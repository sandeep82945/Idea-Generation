{
    "abstractText": "Peer reviews are one of the most important artifacts in scholarly communications. Peer reviews can serve as a rich source of knowledge discovery from texts that are human-generated and also opinionated on the paper under scrutiny. Reviewers comment on several implicit aspects of the paper (Originality, Soundness, Clarity, Appropriateness, etc.) where they sometimes appreciate, sometimes discuss, or sometimes question or criticize the work. Hence, correctly understanding the reviewer\u2019s aspectual perspective on the paper is crucial for chairs/editors to take a stand and also for the authors to respond or revise accordingly. In this paper, we introduce MASEPR, a novel multitask deep neural architecture to jointly discover the aspects and associated sentiments from the peer review texts. Our proposed approach leverages the knowledge sharing between aspect and sentiment lexicons to generate predictions. We outperform the standard baselines by a significant margin. We also make our codes available at https://github.com/cruxieu17/MASEPR.",
    "authors": [
        {
            "affiliations": [],
            "name": "Hardik Arora"
        },
        {
            "affiliations": [],
            "name": "Kartik Shinde"
        },
        {
            "affiliations": [],
            "name": "Tirthankar Ghosal"
        }
    ],
    "id": "SP:7e89bc7812981d859c849997706e2ac2303b07ff",
    "references": [
        {
            "authors": [
                "Md. Shad Akhtar",
                "Tarun Garg",
                "Asif Ekbal"
            ],
            "title": "Multi-task learning for aspect term extraction and aspect sentiment classification",
            "venue": "Neurocomputing",
            "year": 2020
        },
        {
            "authors": [
                "Stefanos Angelidis",
                "Mirella Lapata"
            ],
            "title": "Multiple instance learning networks for fine-grained sentiment analysis",
            "venue": "Transactions of the Association for Computational Linguistics",
            "year": 2018
        },
        {
            "authors": [
                "Iz Beltagy",
                "Kyle Lo",
                "Arman Cohan"
            ],
            "title": "SciBERT: A Pretrained Language Model for Scientific Text",
            "year": 2019
        },
        {
            "authors": [
                "Lutz Bornmann",
                "H-D Daniel"
            ],
            "title": "Reliability of reviewers\u2019 ratings when using public peer review: a case study",
            "venue": "Learned Publishing 23,",
            "year": 2010
        },
        {
            "authors": [
                "Elise S Brezis",
                "Aliaksandr Birukou"
            ],
            "title": "Arbitrariness in the peer review process",
            "venue": "Scientometrics",
            "year": 2020
        },
        {
            "authors": [
                "Pawe\u0142 Budzianowski",
                "Ivan Vulic"
            ],
            "title": "Hello, It\u2019s GPT-2 - How Can I Help You? Towards the Use of Pretrained LanguageModels for Task-Oriented Dialogue Systems",
            "year": 2019
        },
        {
            "authors": [
                "Rich Caruana"
            ],
            "title": "Multitask Learning. In Encyclopedia of Machine Learning and Data Mining",
            "year": 1998
        },
        {
            "authors": [
                "Souvic Chakraborty",
                "Pawan Goyal",
                "Animesh Mukherjee"
            ],
            "title": "Aspectbased Sentiment Analysis of Scientific Reviews",
            "venue": "In JCDL \u201920: Proceedings of the ACM/IEEE Joint Conference on Digital Libraries in 2020,",
            "year": 2020
        },
        {
            "authors": [
                "Domenic V. Cicchetti"
            ],
            "title": "The Reliability of Peer Review for Manuscript and Grant Submissions: A Cross-Disciplinary Investigation",
            "venue": "Behavioral and Brain Sciences",
            "year": 1991
        },
        {
            "authors": [
                "Arman Cohan",
                "Sergey Feldman",
                "Iz Beltagy",
                "Doug Downey",
                "Daniel S. Weld"
            ],
            "title": "SPECTER: Document-level Representation Learning using Citationinformed Transformers",
            "year": 2020
        },
        {
            "authors": [
                "Kevin Cohen",
                "Kar\u00ebn Fort",
                "Margot Mieskes",
                "Aur\u00e9lie N\u00e9v\u00e9ol"
            ],
            "title": "Reviewing Natural Language Processing Research. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts",
            "venue": "Association for Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "year": 2019
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv:1810.04805 [cs.CL] 44 Authorized licensed use limited to: Indian Institute of Technology Patna",
            "venue": "Downloaded on February 13,2024",
            "year": 2019
        },
        {
            "authors": [
                "Crystal Fulton",
                "Claire McGuinness"
            ],
            "title": "Chapter 7 - In Too Deep",
            "venue": "In Digital Detectives, Crystal Fulton and Claire McGuinness (Eds.). Chandos Publishing,",
            "year": 2016
        },
        {
            "authors": [
                "Silvio O. Funtowicz",
                "Jerome R. Ravetz"
            ],
            "title": "Peer Review and Quality Control. In International Encyclopedia of the Social Behavioral Sciences (Second Edition) (second edition ed.), James D",
            "venue": "https: //doi.org/10.1016/B978-0-08-097086-8.85016-3",
            "year": 2015
        },
        {
            "authors": [
                "Yang Gao",
                "Steffen Eger",
                "Ilia Kuznetsov",
                "Iryna Gurevych",
                "Yusuke Miyao"
            ],
            "title": "Does My Rebuttal Matter? Insights from a Major NLP Conference",
            "year": 2019
        },
        {
            "authors": [
                "Tirthankar Ghosal",
                "Rajeev Verma",
                "Asif Ekbal",
                "Pushpak Bhattacharyya"
            ],
            "title": "DeepSentiPeer: Harnessing Sentiment in Review Texts to Recommend Peer Review Decisions",
            "venue": "In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence,",
            "year": 2019
        },
        {
            "authors": [
                "Sepp Hochreiter",
                "J\u00fcrgen Schmidhuber"
            ],
            "title": "Long Short-Term Memory",
            "venue": "Neural Comput. 9,",
            "year": 1997
        },
        {
            "authors": [
                "MinghaoHu",
                "Yuxing Peng",
                "ZhenHuang",
                "Dongsheng Li",
                "Yiwei Lv"
            ],
            "title": "Opendomain targeted sentiment analysis via span-based extraction and classification",
            "year": 2019
        },
        {
            "authors": [
                "Janine Huisman",
                "Jeroen Smits"
            ],
            "title": "Duration and quality of the peer review process: the author\u2019s perspective",
            "venue": "Scientometrics 113,",
            "year": 2017
        },
        {
            "authors": [
                "Qingnan Jiang",
                "Lei Chen",
                "Ruifeng Xu",
                "Xiang Ao",
                "andMin Yang"
            ],
            "title": "A Challenge Dataset and EffectiveModels for Aspect-Based Sentiment Analysis",
            "venue": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP- IJCNLP). Association",
            "year": 2019
        },
        {
            "authors": [
                "Xiaotian Jiang",
                "Quan Wang",
                "Peng Li",
                "Bin Wang"
            ],
            "title": "Relation extraction with multi-instance multi-label convolutional neural networks",
            "venue": "In Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers",
            "year": 2016
        },
        {
            "authors": [
                "Dongyeop Kang",
                "Waleed Ammar",
                "Bhavana Dalvi",
                "Madeleine van Zuylen",
                "Sebastian Kohlmeier",
                "Eduard Hovy",
                "Roy Schwartz"
            ],
            "title": "A dataset of peer reviews (peerread): Collection, insights and nlp applications",
            "year": 2018
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A Method for Stochastic Optimization",
            "year": 2014
        },
        {
            "authors": [
                "Dimitrios Kotzias",
                "Misha Denil",
                "Phil Blunsom",
                "Nando de Freitas"
            ],
            "title": "Deep multi-instance transfer learning",
            "venue": "arXiv preprint arXiv:1411.3128",
            "year": 2014
        },
        {
            "authors": [
                "Kumar",
                "Ghosal",
                "Bharti",
                "Ekbal"
            ],
            "title": "Sharing is Caring! Joint Multitask LearningHelps Aspect-Category Extraction and Sentiment Detection in Scientific Peer Reviews",
            "year": 2021
        },
        {
            "authors": [
                "John Langford",
                "Mark Guzdial"
            ],
            "title": "The arbitrariness of reviews, and advice for school administrators",
            "venue": "Commun. ACM 58,",
            "year": 2015
        },
        {
            "authors": [
                "John Langford",
                "Mark Guzdial"
            ],
            "title": "The arbitrariness of reviews, and advice for school administrators",
            "venue": "Commun. ACM 58,",
            "year": 2015
        },
        {
            "authors": [
                "Zheng Li",
                "Xin Li",
                "Ying Wei",
                "Lidong Bing",
                "Yu Zhang",
                "Qiang Yang"
            ],
            "title": "Transferable end-to-end aspect-based sentiment analysis with selective adversarial learning",
            "year": 2019
        },
        {
            "authors": [
                "Bing Liu"
            ],
            "title": "Sentiment analysis and opinion mining",
            "venue": "Synthesis lectures on human language technologies 5,",
            "year": 2012
        },
        {
            "authors": [
                "Scott M Lundberg",
                "Su-In Lee"
            ],
            "title": "A Unified Approach to Interpreting Model Predictions",
            "venue": "InAdvances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Scott M. Lundberg",
                "Su-In Lee"
            ],
            "title": "A Unified Approach to Interpreting Model Predictions",
            "venue": "In Proceedings of the 31st International Conference on Neural Information Processing Systems (Long",
            "year": 2017
        },
        {
            "authors": [
                "Abby Olena"
            ],
            "title": "How to Make Scientists into Better Peer Reviewers",
            "year": 2018
        },
        {
            "authors": [
                "Bo Pang",
                "Lillian Lee"
            ],
            "title": "Opinion mining and sentiment analysis. Foundations and Trends\u00ae in information retrieval",
            "year": 2008
        },
        {
            "authors": [
                "Nikolaos Pappas",
                "Andrei Popescu-Belis"
            ],
            "title": "Explaining the stars: Weighted multiple-instance learning for aspect-based sentiment analysis",
            "venue": "In Proceedings of the 2014 Conference on Empirical Methods In Natural Language Processing (EMNLP)",
            "year": 2014
        },
        {
            "authors": [
                "Adam Paszke",
                "Sam Gross",
                "Francisco Massa",
                "Adam Lerer",
                "James Bradbury",
                "Gregory Chanan",
                "Trevor Killeen",
                "Zeming Lin",
                "Natalia Gimelshein",
                "Luca Antiga",
                "Alban Desmaison",
                "Andreas Kopf",
                "Edward Yang",
                "Zachary DeVito",
                "Martin Raison",
                "Alykhan Tejani",
                "Sasank Chilamkurthy",
                "Benoit Steiner",
                "Lu Fang",
                "Junjie Bai",
                "Soumith Chintala"
            ],
            "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
            "year": 2019
        },
        {
            "authors": [
                "Jeffrey Pennington",
                "Richard Socher",
                "Christopher Manning"
            ],
            "title": "GloVe: Global Vectors for Word Representation",
            "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics,",
            "year": 2014
        },
        {
            "authors": [
                "Barbara Plank",
                "Reinard van Dalen"
            ],
            "title": "CiteTracked: A Longitudinal Dataset of Peer Reviews and Citations",
            "venue": "(CEUR Workshop Proceedings,",
            "year": 2019
        },
        {
            "authors": [
                "Maria Pontiki",
                "Dimitrios Galanis",
                "Haris Papageorgiou",
                "Ion Androutsopoulos",
                "Suresh Manandhar",
                "Mohammad Al-Smadi",
                "Mahmoud Al-Ayyoub",
                "Yanyan Zhao",
                "Bing Qin",
                "Orph\u00e9e De Clercq"
            ],
            "title": "Semeval-2016 task 5: Aspect based sentiment analysis",
            "venue": "In International workshop on semantic evaluation",
            "year": 2016
        },
        {
            "authors": [
                "Azzurra Ragone",
                "Katsiaryna Mirylenka",
                "Fabio Casati",
                "Maurizio Marchese"
            ],
            "title": "On peer review in computer science: analysis of its effectiveness and suggestions for improvement",
            "venue": "Scientometrics 97,",
            "year": 2013
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych"
            ],
            "title": "Sentence-bert: Sentence embeddings using siamese bert-networks",
            "venue": "arXiv preprint arXiv:1908.10084",
            "year": 2019
        },
        {
            "authors": [
                "Drummond Rennie"
            ],
            "title": "Let\u2019s make peer review scientific",
            "venue": "Nature 535,",
            "year": 2016
        },
        {
            "authors": [
                "Martin Schmitt",
                "Simon Steinheber",
                "Konrad Schreiber",
                "Benjamin Roth"
            ],
            "title": "Joint aspect and polarity classification for aspect-based sentiment analysis with end-to-end neural networks",
            "year": 2018
        },
        {
            "authors": [
                "D Sculley",
                "Jasper Snoek",
                "Alex Wiltschko"
            ],
            "title": "Avoiding a Tragedy of the Commons in the Peer Review Process",
            "year": 2018
        },
        {
            "authors": [
                "Nihar B Shah",
                "Behzad Tabibian",
                "Krikamol Muandet",
                "Isabelle Guyon",
                "Ulrike Von Luxburg"
            ],
            "title": "Design and analysis of the nips 2016 review process",
            "venue": "The Journal of Machine Learning Research 19,",
            "year": 2018
        },
        {
            "authors": [
                "Mihai Surdeanu",
                "Julie Tibshirani",
                "Ramesh Nallapati",
                "Christopher D Manning"
            ],
            "title": "Multi-instance multi-label learning for relation extraction. In Proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning",
            "year": 2012
        },
        {
            "authors": [
                "Yi Tay",
                "Anh Tuan Luu",
                "Aston Zhang",
                "Shuohang Wang",
                "Siu Cheung Hui"
            ],
            "title": "Compositional de-attention networks",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2019
        },
        {
            "authors": [
                "Andrew Tomkins",
                "Min Zhang",
                "William D. Heavlin"
            ],
            "title": "Reviewer bias in single- versus double-blind peer review",
            "venue": "Proc. Natl. Acad. Sci. USA 114,",
            "year": 2017
        },
        {
            "authors": [
                "Verma",
                "Shinde",
                "Arora",
                "Ghosal"
            ],
            "title": "Attend To Your Review: A Deep Neural Network to Extract Aspects from Peer Reviews",
            "year": 2021
        },
        {
            "authors": [
                "Rajeev Verma",
                "Kartik Shinde",
                "Hardik Arora",
                "Tirthankar Ghosal"
            ],
            "title": "Attend to Your Review: A Deep Neural Network to Extract Aspects from Peer Reviews",
            "venue": "In International Conference on Neural Information",
            "year": 2021
        },
        {
            "authors": [
                "Ke Wang",
                "Xiaojun Wan"
            ],
            "title": "Sentiment analysis of peer review texts for scholarly papers",
            "venue": "In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval",
            "year": 2018
        },
        {
            "authors": [
                "Jelte M Wicherts"
            ],
            "title": "Peer review quality and transparency of the peer-review process in open access and subscription journals",
            "venue": "PloS one 11,",
            "year": 2016
        },
        {
            "authors": [
                "Wei Xue",
                "Tao Li"
            ],
            "title": "Aspect based sentiment analysis with gated convolutional networks",
            "venue": "arXiv preprint arXiv:1805.07043",
            "year": 2018
        },
        {
            "authors": [
                "Zhilin Yang",
                "Zihang Dai",
                "Yiming Yang",
                "Jaime G. Carbonell",
                "Ruslan Salakhutdinov",
                "Quoc V. Le"
            ],
            "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding",
            "venue": "NeurIPS",
            "year": 2019
        },
        {
            "authors": [
                "Weizhe Yuan",
                "Pengfei Liu",
                "GrahamNeubig"
            ],
            "title": "CanWeAutomate Scientific Reviewing? CoRR abs/2102.00176 (2021)",
            "venue": "https://arxiv.org/abs/",
            "year": 2021
        },
        {
            "authors": [
                "Min-Ling Zhang",
                "Zhi-Hua Zhou"
            ],
            "title": "M3MIML: Amaximummarginmethod for multi-instance multi-label learning",
            "venue": "In 2008 Eighth IEEE International Conference on Data Mining",
            "year": 2008
        },
        {
            "authors": [
                "Zhi-Hua Zhou",
                "Min-Ling Zhang",
                "Sheng-Jun Huang",
                "Yu-Feng Li"
            ],
            "title": "Multiinstance multi-label learning",
            "venue": "Artificial Intelligence 176,",
            "year": 2012
        }
    ],
    "sections": [
        {
            "text": "CCS CONCEPTS \u2022 Computing methodologies \u2192 Information extraction; \u2022 Information systems \u2192 Information extraction.\nKEYWORDS Peer Reviews, Aspect-based Sentiment Analysis, Deep Neural Network, SHAP (SHapley Additive exPlanations)"
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Peer review is the standard process of evaluating the scientific work of researchers submitted to academic journals or conferences.But recently, peer review system has often been criticized as non transparent, arbitrary to generate the predictions. We outperform the standard base-[5] ,biased [49] and inconsistent[27, 46] which has led researchers to argue over its reliability[9] and quality[43]. This, coupled with the rapid increase of paper submissions, has raised the need to study the paper-vetting system and build proposals towards mitigating its problems. If we consider the example of computer science conferences: The Conference on Neural Information Processing Systems (NeurIPS) and the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) which received 9467 and 6656 submissions in 2020, respectively; the numbers are five times the number of submissions they received in 2010. Hence,\n\u2217Both authors contributed equally to the paper \u2020performed this work while at \u00daFAL, MFF, Charles University, CZ\nto guarantee a minimum number of reviews per paper, these conferences are forced to recruit many reviewers of different expertise, levels, and backgrounds. As a result, the submitted reviews often fail to meet conference conformity standards, such as supporting the claims with sufficient evidence, providing valid arguments (e.g., not self-contradictory), and providing insightful comments.Fortunately, research communities have noted some of these problems and have proposed measures to change reviewing practices. For example, The Association for Computational Linguistics (ACL) recently adopted the Rolling Review system 1 to aid reviewing in a reasonable and timely manner to avoid management issues with a large number of submissions; also conferences like International Conference on Computational Linguistics (COLING) and Conference on Empirical Methods in Natural Language Processing (EMNLP) have adopted the review-rebuttal process which is also an attempt to maintain the integrity of the peer-review process. Conferences in recent years have also organized tutorials[11] for researchers to write good quality reviews [34]. These are commendable efforts to restore faith in the widely accepted method of scholarly communication. However, peer-review suffers from another major problem, i.e., reviewers\u2019 tendency to invest less time in this critical voluntary job [20]. Sometimes the reviewers themselves are unsure of their judgment on the merit of the work, which becomes evident with their reviews [4] [28]. Thus, these reform programs targeted to improve reviewing practices might not be sufficient. They could be frustrating for an enthusiastic prospective author, especially in the double-blind conference model without a rebuttal period.\nIn order to maintain trust in the peer-review process, we believe establishing confidence in peer-review reports is essential. Given this direction\u2019s submission load and urgency, an automatic system would help. As per the rubrics defined in [45], we expect a reviewer to evaluate the work for indicators like novelty, theoretical and empirical soundness of the research methodology, writing, clarity of the work, and impact of the work in a broader academic context. An indicative measure of how exhaustively and critically the reviewer has reviewed the paper could be the answer to the question that how many aspects of the paper has the reviewer touched upon or how thoroughly he has analyzed the paper, under the light of those aspects and in what sense (positive or negative) the reviewer has expressed his views. Hence, in this work, we attempt to automatically extract all the implicit aspect categories and their corresponding sentiments present in the review sentences. This information would\n1https://aclrollingreview.org/\n35\n2575-8152/23/$31.00 \u00a92023 IEEE DOI 10.1109/JCDL57899.2023.00015\n20 23\nA CM\n/I EE\nE Jo\nin t C\non fe\nre nc\ne on\nD ig\nita l L\nib ra\nrie s (\nJC DL\n) | 9\n79 -8\n-3 50\n3- 99\n31 -8\n/2 3/\n$3 1.\n00 \u00a9\n20 23\nIE EE\n| D\nO I:\n10 .1\n10 9/\nJC DL\n57 89\n9. 20\n23 .0\n00 15\nAuthorized licensed use limited to: Indian Institute of Technology Patna. Downloaded on February 13,2024 at 05:01:20 UTC from IEEE Xplore. Restrictions apply.\nhelp the chairs to quickly estimate the informativeness of the review, understand the agreements and disagreements among the reviewers on certain aspects, and initiate discussions to arrive at a consensus on the decision. Even authors could benefit from such an automatic aspect and sentiment extractor, which would help them understand their action points and subsequently pursue appropriate steps in rebuttals or revisions. In this work, we propose MASEPR, a Multi Aspect cum Sentiment Extraction model from Peer Reviews, which does a joint learning of representations for extracting the aspects present in a review sentence along with their associated sentiments (POS,NEG). MASEPR explicitly models that the sentiment of a particular aspect is an aggregation of the sentiment of words indicating the aspect category. Specifically, MASEPR first predicts the instance sentiments, finds the key instances for the aspect categories, and finally aggregates the sentiments to make the final prediction. We conduct our experiments on a recent dataset of peer-reviews [56] and include the following aspect-categories2 in our investigation: Motivation/Impact (MOT), Originality (ORI), Soundness/Correctness (SOU), Substance (SUB), Replicability (REP), Meaningful Comparison (CMP), and Clarity (CLA),a description about them can be found in Table 1. To the best of our knowledge, this is probably the first work to explore multi-aspect sentiment learning on peer reviews. We plan to deploy MASEPR on a web-based interface which chairs and editors can use to discover implicit knowledge from the peer reviews. A demo video of how the interface will look like can be found on our GitHub https://anonymous.4open.science/r/MASEPR-266F."
        },
        {
            "heading": "2 RELATEDWORK",
            "text": "Peer review is the internationally recognized gold standard for scientific quality control. Reviewers examine the manuscript against various aspects and expertise to determine whether the content and style meet the appropriate journal publication standards. Generally, the process is anonymous, but this is not always the case. These anonymizations are processed to minimize bias toward the author(s) during the peer review process,ensuring that the information published in journals is accurate, trustworthy, and original [14]. During the peer review process, both the reviewer and the author recognize each other in an \u2018open\u2019 review. There is growing recognition of the need to alter the traditional peer review process to address the challenges posed by an ever-expanding scientific enterprise, the emergence of new forms of science, and the impact of new technologies. The increasing application of science and science-based technologies in society necessitates the development 2motivated from the ACL review form https://aclrollingreview.org/\nof novel quality control mechanisms and the engagement of new stakeholders. [15].\nPeer-Review and Meta-Research. There have been bodies of work in the meta-research community (Peer Review Week3, Peer Review Congress4, etc.). As one of the first such attempts, Ragone et al. (2013) [41] studied the need to improve the standards and efficiency in the practice of peer review, a study associated explicitly with the fairness, reliability, and validity of the process. Later, in 2016, Wicherts et al.[53] explained how transparency might be seen as an indicator of the quality of the practice of peer-review and also developed a tool enabling different stakeholders to analyze this factor. Wang et al. (2018) [52] proposes a novel multi-instance learning network with an abstract-based memory mechanism (MILAM) for sentiment analysis on peer review texts. As a result of this increased research in the peer-review domain, newer datasets like PeerRead, introduced in Kang et al. (2018) [23], and \u2018CiteTracked\u2019 [39] have also emerged to overcome the persisting shortage of peer-review data.\nIn recent years, various attempts have been made to quantify the qualitative aspects of peer reviews to achieve more comprehensive insights. Gao et al. (2019) [16] assesses a newly introduced corpus of peer reviews and author responses and show how the authors\u2019 rebuttals (especially for borderline papers) influence the paper\u2019s final recommendation scores. While Ghosal et al. (2019) [17] propose a deep neural network to compute the peer-review outcome based on features extracted from the paper and the review text alongside its sentiment. Chakraborty et al. (2020) [8] demonstrates a typical aspect-based sentiment analysis on a peer-review data. Yuan et al. (2021) [56] makes an ambitious attempt to automatically generate peer reviews with aspect, sentiment, and various other features, proposing a dataset consisting of peer-reviews annotated on a phrase-level according to the aspects present in the review text and also show a preclusive result of sentence-level aspect prediction. The study also introduces the \u2018ASAP-Review Dataset\u2019, the experimental test bed for our investigations. The underlying objective in these studies might be coherent with that of ours; however, they essentially cover a different scope in the meta-research domain.\nAspect Extraction andAnalysis. Earlier attempts at sentiment analysis (Pang and Lee, (2008); Liu, (2012)) [30, 35] attracted a lot of attention from the community. As a result, further research gradually opened the doors for investigations in the aspect-based analysis of various types of textual data like reviews from the product, food, service industries, etc. (Pontiki et al., 2014, 2015, 2016)\n3https://peerreviewweek.wordpress.com/ 4https://peerreviewcongress.org/\n36\nAuthorized licensed use limited to: Indian Institute of Technology Patna. Downloaded on February 13,2024 at 05:01:20 UTC from IEEE Xplore. Restrictions apply.\n[40] introduce and foster the research in Aspect-based Sentiment Analysis (ABSA), a sentiment analysis task including subtasks like Aspect-category Detection (ACD), and Aspect-category Sentiment Analysis (ACSA). Studies like (Tay et al. (2018); Hu et al. (2019)) [19, 48] introduce several attention-based models that allocate the appropriate sentiment words for the given aspect category. While Xue & Li et al. (2018) [54] propose a system that generates aspect category-specific representations with the help of convolutional neural networks and gating mechanisms. Several joint-learning models (Schmitt et al., 2018; Wang et al., 2019; Li et al., 2019) [29, 44] that perform tasks like ACD and ACSA have also emerged. In 2020, Akhtar et al. (2020) [1] propose a multi-task framework in the product reviews domain(Restaurant, Laptop, and like) to weigh and analyze aspects and opinion terms simultaneously. And recently, in 2021, Verma et al. (2021) [51] demonstrated an attention-based deep neural architecture to extract the aspects from the review of a given scholarly document.\nMulti-InstanceMulti-Label Learning. Zhou and Zhang, (2012) [58] made one of the very first attempts at defining the task of Multi-Instance Multi-Label Learning (MIML). The saliency of the task lies in representing each training example with multiple instances that are further associated with multiple class labels. Earlier studies revolving around the concept of MIML include its application in several downstream tasks like text categorization (Zhang and Zhou, 2008) [57], relation extraction (Surdeanu et al., 2012; Jiang et al., 2016)[22, 47], etc. In recent years, studies, including (Kotzias et al., 2015; Angelidis and Lapata, 2018)[2, 25], attempted to leverage multi-instance learning for sentiment analysis. Pappas and Popescu-Belis (2014) [36] propose a multiple instance regression (MIR) model to assign sentiment scores to specific aspects of products, thus extending the method to the sub-domain of ABSA.\nTo our knowledge, neither MIL (Multi-Instalce Learning) nor MIML have been used in the peer-review domain. Common product and restaurant reviews are semantically easier to learn as it is guided by explicitly used aspect-category lexicons (e.g., \u2018battery life\u2019 for gadgets, \u2018taste\u2019 of the food, \u2018ambiance\u2019 of the place, and like). At the same time, the sentiment is usually identified as the sentiment of the entire text. A peer review text is very different from what we see in previous reviews and exhibits features that contrast highly from the regular, more straightforward reviews. Peer reviews are often far more comprehensive and detailed. The aspect categories present in a text can rarely be identified with the help of obvious lexicons and like. While the presence of a particular aspect in a sentence is primarily implicit, the overall \u2019sentiment of the sentence\u2019 and the \u2019sentiment related to a particular aspect being talked about in the same sentence\u2019 might differ. In this study, we attempt a joint extraction of aspects-categories and the sentiments associated with each of them from a given peer-review text. Technically, in the above studies, the tasks mainly address a \u2018multi-class singlelabel, OR a \u2018multi-class multi-label\u2019 task limited only to the aspectcategory prediction."
        },
        {
            "heading": "3 PROBLEM DEFINITION",
            "text": "Over the years, numerous studies involving multi-instance, multilabel learning, and aspect-sentiment-based analysis formulated and defined the tasks in MIL, MIML and ACD, ABSA, and ACSA. We combine the two problems and formulate the goals for MASEPR in a\nmulti-instance, multi-label aspect-category cum sentiment-polarity prediction task.\nGiven a review \ud835\udc45 = {\ud835\udc461, \ud835\udc462, ..., \ud835\udc46\ud835\udc5a}, consisting of\ud835\udc5a-number of sentences, where \ud835\udc46\ud835\udc56 \u2208 \ud835\udc45 denotes the i-th sentence in the review. And, the sentence \ud835\udc46\ud835\udc56 = {\ud835\udc641,\ud835\udc642, ...,\ud835\udc64\ud835\udc5b}, comprising a sequence of \ud835\udc5b-number of words, where\ud835\udc64\ud835\udc56 \u2208 \ud835\udc46\ud835\udc56 denotes a single word. Our goal is to predict:\n\u2022 the \ud835\udc58-number of corresponding aspect-categories: \ud835\udc4e\ud835\udc46\ud835\udc56 = {\ud835\udc4e1, \ud835\udc4e2, ..., \ud835\udc4e\ud835\udc58 }, where \ud835\udc4e\ud835\udc46\ud835\udc56 \u2282 \ud835\udc34 associated with the sentence, \ud835\udc46\ud835\udc56 , where\ud835\udc34 denotes set of aspect categories introduced and described in section 1. \u2022 and, the sentiment polarity distributions for all the \ud835\udc58 aspect categories, \ud835\udc5d = {\ud835\udc5d1, \ud835\udc5d2, ...., \ud835\udc5d\ud835\udc58 }, where \ud835\udc5d\ud835\udc58 \u2282 \ud835\udc43 , P denotes the set of sentiments-polarities - (Postitive: POS, Negative: NEG).\nAs a result, a given review line may have multiple aspect categories discussed by its reviewer within its text. Furthermore, comments on each such aspect category may contain opinions of mixed polarity (i.e., positive sentiment, negative sentiment). Because of human thinking and the use of heuristics, we keep a reviewer\u2019s ambivalence in mind and train MASEPR to be flexible and open to all possibilities when making a prediction. Our multi-label learning assumes that for every unobserved word-level sentiment distribution, there exists an unknown function \ud835\udc5d\ud835\udc58 given by:\npk = g\u0302 k \ud835\udf03\ud835\udc46 (s1, s2, ..., sn); (1)\nsj = f\u0302\ud835\udf03\ud835\udc64 (wj) (2)\nwhere \ud835\udc60 \ud835\udc57 is the sentiment distribution for the j-th word\ud835\udc64 \ud835\udc57 from a given review line."
        },
        {
            "heading": "4 DATASET DESCRIPTION AND STATISTICS",
            "text": "For our experimental considerations, we reconstruct the data by creating a sentence-level split across all the reviews using the NLTK library. We assigned each sentence a label based on aspect-sentimentbased phrase-level annotations in the original data. The data thus created is multi-class, having around 200K instances. Table 2 summarizes the different categories of sentences present in the dataset. Each instance has two-dimensional annotations consisting of a sentiment-polarity label corresponding to each aspect-category label. We introduced a new label \u2018no_aspect\u2019 in our reconstructed data for denoting sentences that do not discuss any of the aforementioned aspects. We did some preliminary analysis to show the distribution of different aspect and sentiment categories in the overall reconstructed sentence level dataset. Figure 2 and Figure 3 summarizes our results."
        },
        {
            "heading": "5 METHODOLOGY",
            "text": ""
        },
        {
            "heading": "5.1 Model Description",
            "text": "MASEPR combines the aspect-category task and the sentimentpolarity prediction task in a unified training. The architecture of MASEPR comprises of a few shared representation layers, followed by the two task-specific heads. In essence, MASEPR has two major components: 1. Aspect-category prediction, and, 2. Sentimentpolarity prediction corresponding to each aspect category. For a given review line, we first obtain the contextual embeddings from\n37\nAuthorized licensed use limited to: Indian Institute of Technology Patna. Downloaded on February 13,2024 at 05:01:20 UTC from IEEE Xplore. Restrictions apply.\npretrained transformer-based encoders. Then, we pass these representations to a shared Bi-directional LSTM layer. The first phase in MASEPR\u2019s architecture consists of:\n\u2022 a) an attention layer added on top of the shared layer \u2022 b) an aspect category prediction layer (comprising feedfor-\nward linear layers). While the second phase comprise of:\n\u2022 a) a word-sentiment prediction layer, and \u2022 b) an aspect category sentiment prediction layer which\nmaps the sentiment associated with each aspect category.\n5.1.1 Representation Layer. The input to our model is a review sentence consisting of n words. To represent the words, we use contextual embeddings of these words from various pre-trained transformer models. Alongside being pretrained on large language corpora, these transformer models also leverage a self-attention mechanism to generate contextual representations of input texts. These encoded representations of serve as inputs to the shared BiLSTM layer.\n5.1.2 Shared-BiLSTM Layer. This layer takes input from the shared representation layer. In order to assure the validity of the sentence representations and the weights computed by attention mechanism, we use a single Bi-directional LSTM layer [18]. This LSTM layer takes the word embeddings as input, and returns its hidden states \ud835\udc3b = \u210e1, \u210e2, ..., \u210e\ud835\udc5b . This allows MASEPR to learn over richer contextual information in every representation vector space. At each time step j, the hidden state \u210e \ud835\udc57 is computed by:\n\u210e \ud835\udc53\n\ud835\udc57 = \ud835\udc3f\ud835\udc46\ud835\udc47\ud835\udc40 (\u210e \ud835\udc57\u22121, \u210e \ud835\udc57 ) (3)\n\u210e\ud835\udc4f\ud835\udc57 = \ud835\udc3f\ud835\udc46\ud835\udc47\ud835\udc40 (\u210e \ud835\udc57+1, \u210e \ud835\udc57 ) (4)\n\ud835\udc3b \ud835\udc57 = [\u210e\ud835\udc53\ud835\udc57 ;\u210e \ud835\udc4f \ud835\udc57 ] (5)\nThe size of hidden state of BILSTM is set to\ud835\udc51/2. The hidden state\ud835\udc3b \ud835\udc57 of the Bi-LSTM, refers to the representation of the j-th word. These enhanced representations serve as input to MASEPR\u2019s Attention Layer.\n38\nAuthorized licensed use limited to: Indian Institute of Technology Patna. Downloaded on February 13,2024 at 05:01:20 UTC from IEEE Xplore. Restrictions apply.\n5.1.3 Attention Layer. MASEPR consists of a unique attention mechanism that helps it leverage the sentiment information of each review sentence to better comprehend the semantic structure of a review and the style of reviewers\u2019 language. It combines the tasks of aspect-category and sentiment polarity prediction into a unified training in which the knowledge acquired from each task-specific feature assists the other sub-task and improves the performance on both. This layer takes the enhanced representations from the shared BiLSTM as input and produces an attention weight vector for each word with respect to each aspect category. For \ud835\udc57\ud835\udc61\u210e word in a review sentence, the attention vector is calculated as:\n\ud835\udefc \ud835\udc57 = \ud835\udc46\ud835\udc5c \ud835\udc53 \ud835\udc61\ud835\udc5a\ud835\udc4e\ud835\udc65 ((\ud835\udc4a\ud835\udc3b\ud835\udc47\ud835\udc57 )) (6)\nwhere \ud835\udefc \ud835\udc57 \u2208 \ud835\udc45\ud835\udc5b\u00d71 represents the attention weight vector, \ud835\udc5b is the total number of aspects, and\ud835\udc4a \u2208 \ud835\udc45\ud835\udc5b\u00d7\ud835\udc51 , is a learnable parameter. We utilize these attention weights during the training of the taskspecific heads in subsequent stages. 5.1.4 Aspect-Category Prediction Layer. For the Aspect category prediction layer, we combine the attention weights produced by MASEPR\u2019s Attention Layer with the enhanced representations from the hidden state of the BiLSTM layer. We use this weighted representation vector as the feature representation for each review line. For the \ud835\udc56\ud835\udc61\u210e aspect category, the probability of the presence of the\nrespective aspect is calculated as:\n\ud835\udc66\ud835\udc56 = \ud835\udc60\ud835\udc56\ud835\udc54\ud835\udc5a\ud835\udc5c\ud835\udc56\ud835\udc51 (\ud835\udc4a\ud835\udc56\ud835\udc5f\ud835\udc47\ud835\udc56 + \ud835\udc4f\ud835\udc56 ), \ud835\udc56 = 1, 2, 3.....\ud835\udc5b (7) \ud835\udc5f\ud835\udc56 = \u2211\ufe01 \ud835\udc57 \ud835\udefc\ud835\udc56\ud835\udc57\ud835\udc3b \ud835\udc57 ,\ud835\udc64\u210e\ud835\udc52\ud835\udc5f\ud835\udc52 \ud835\udc56 = 1, 2, 3...\ud835\udc5b (8)\nwhere \ud835\udc66\ud835\udc56 represents the probability of the \ud835\udc56\ud835\udc61\u210e aspect being present in a given review line, \ud835\udc5f\ud835\udc56 is the the attention-infused feature representation vector of the review line, and\ud835\udc4a\ud835\udc57 \u2208 \ud835\udc451\u00d7\ud835\udc51 and \ud835\udc4f \ud835\udc57 is a scalar trainable weight parameter.\n5.1.5 Word Sentiment Prediction Layer. In order to predict the sentiment polarity for j-th word in a given review line, we pass the the enhanced representations from the hidden state \ud835\udc3b \ud835\udc57 of the BiLSTM layer, through two fully connected layers as shown.\n\ud835\udc5d \ud835\udc57 = (\ud835\udc4a (2) )\ud835\udc47\ud835\udc45\ud835\udc52\ud835\udc3f\ud835\udc48 (\ud835\udc4a (1)\ud835\udc3b\ud835\udc47\ud835\udc57 + \ud835\udc4f (1) ) + \ud835\udc4f (2) (9)\nHere, \ud835\udc4a (1) \u2208 \ud835\udc45\ud835\udc51 \u2032\u00d7\ud835\udc51 ,\ud835\udc4a (2) \u2208 \ud835\udc45\ud835\udc51 \u2032\u00d72, \ud835\udc4f (1) \u2208 \ud835\udc45\ud835\udc51 \u2032\u00d71, \ud835\udc4f (2) \u2208 \ud835\udc452\u00d71,and \ud835\udc5d \ud835\udc57 represents the sentiment-polarity prediction label\n5.1.6 Aspect-Category-wise Sentiment Prediction Layer. Finally, to obtain the aspect-category sentiment predictions (i.e. sentimentpolarity corresponding to each aspect-category) by aggregating the word-sentiment predictions from the Word-Sentiment prediction Layer. This is similar to the Aspect-Category Prediction Layer, except that this time, we combine the attention weights produced by MASEPR\u2019s Attention Layer with the aggregated word-sentiment predictions. Formally, for the i-th aspect category, the corresponding sentiment \ud835\udc60\ud835\udc56 is computed as:\n\ud835\udc60\ud835\udc56 = \ud835\udc60\ud835\udc56\ud835\udc54\ud835\udc5a\ud835\udc5c\ud835\udc56\ud835\udc51 ( \u2211\ufe01 \ud835\udc57 \ud835\udc5d \ud835\udc57\ud835\udefc \ud835\udc56 \ud835\udc57 ) (10)\nwhere \ud835\udc60\ud835\udc56 \u2208 \ud835\udc452 , \ud835\udefc\ud835\udc56\ud835\udc57 represents the attention weight of \ud835\udc57 \ud835\udc61\u210e word with respect to \ud835\udc56\ud835\udc61\u210e aspect category in the attention weight vector \ud835\udefc \ud835\udc57 obtained from MASEPR\u2019s Attention Layer."
        },
        {
            "heading": "5.2 Loss Function",
            "text": "We divide our loss function into two parts. the first one corresponds to the aspect category detection problem and is denoted by :\n\ud835\udc3f1 (\ud835\udf031) = \u2212 \ud835\udc5b\u2211\ufe01 \ud835\udc56=1 (\ud835\udc5f\ud835\udc56 log(\ud835\udc5f\ud835\udc56 ) + (1 \u2212 \ud835\udc5f\ud835\udc56 ) log(1 \u2212 \ud835\udc5f\ud835\udc56 )) (11)\nwhere \ud835\udc5f\ud835\udc56 ,is the i-th scalar values in the model output, and \ud835\udc5f\ud835\udc56 is the corresponding target values for aspect respectively. The second part corresponds to the sentiment of the Z aspect categories mentioned in the sentence and is given by\n\ud835\udc3f2 (\ud835\udf032) = \u2212 \ud835\udc4d\u2211\ufe01 \ud835\udc50=1 (\ud835\udc60\ud835\udc56\ud835\udc50 log(\ud835\udc60\ud835\udc56\ud835\udc50 ) + (1 \u2212 \ud835\udc60\ud835\udc56\ud835\udc50 ) log(1 \u2212 \ud835\udc60\ud835\udc56\ud835\udc50 )) (12) where \ud835\udc60\ud835\udc56 is the i-th scalar values in the model output, and \ud835\udc60\ud835\udc56 is the corresponding target value for sentiment corresponding to each aspect category.\nWe jointly train our model for the two tasks and the parameters are updated by minimizing the combined loss function:\n\ud835\udc3f(\ud835\udf03 ) = \ud835\udc3f1 (\ud835\udf031) + \ud835\udc3f2 (\ud835\udf032) (13) where \ud835\udf031 and \ud835\udf032 denotes all the model parameters.\n39\nAuthorized licensed use limited to: Indian Institute of Technology Patna. Downloaded on February 13,2024 at 05:01:20 UTC from IEEE Xplore. Restrictions apply."
        },
        {
            "heading": "6 EXPERIMENTS AND ANALYSIS",
            "text": ""
        },
        {
            "heading": "6.1 Baselines and Comparing Systems",
            "text": "We compare MASEPR with several baseline systems from previous literature. A recent approach proposed in [26] leverages the interdependence by sharing the representations between the aspectcategory and sentiment-category features in an end-to-end deep multitasking model. Although the problem under study was a bit different than ours (a multi class single-label classification one), their experiments show that the two categories help the multitask model in learning each others\u2019 features. We draw a comparison between our method and a few common multi-label classification techniques and have also implemented certain baselines to prove the effectiveness of our proposed architecture. The details of the models are mentioned as follows:\n(1) GloVe Embeddings (GloVe): This baseline uses GloVe[38] representations, instead of the transformer-based encoder models. We again provide the same attention mechanism and the outer architecture is also preserved. The performance on this baseline clearly justifies the use of pretrained transformers encoders in MASEPR.\n(2) CapsNet-BERT and CapsNet-Recurrent [21]: This is a model based on CapsNet and CapsNet-BERT. They combined the strength of BERT and capsule networks by replace the embedding layer and encoding layer of CapsNet with pre-trained BERT. In one setting they feed the sentence and aspect representations into capsule layers and predict the corresponding sentiment polarities while in other they used the bidirectional gated recurrent unit (BiGRU) instead of BERT for their experiments. (3) Shared Feature Extraction (SFE): Introduced in [26], the model uses SciBERT [3] embeddings and feeds the encoded representations directly to a shared layer for feature extraction. This shared layer has two task\u2212specific MLPs (for aspect and sentiment predictions) and a combined loss if computed for training the model. (4) sentence-BERT + attention (sBERT+attn) [50]: This model is designed for review-level aspect prediction and uses s-BERT [42] encoder model fine tuned on STSB (semantic textual similarity) task. Encoded representations of the review text are passed through a BiLSTM layer followed by\n40\nAuthorized licensed use limited to: Indian Institute of Technology Patna. Downloaded on February 13,2024 at 05:01:20 UTC from IEEE Xplore. Restrictions apply.\na DL-based attention mechanism which helps distinguish the aspect-category features during training.\nThe performances of baseline models are compared by evaluating aspect-category wise F1 scores, followed by sentiment-category scores corresponding to each aspect. The distinct features in baselines, and respective performances further support the methods adopted in MASEPR."
        },
        {
            "heading": "6.2 Implementation details",
            "text": "We use Pytorch [37] as the DL framework and pre-trained transformer models from 5hugging face for implementation. For our experimental purposes we split our dataset into 80 % train,5% validation and 15 % test sets. During validation, we experiment with different network configurations and for optimal performance we take the following hyper-parameters :Batch size=32,activation function=ReLU,dropout= 0.5,Learning rate= 1e-3. We used Adam[24] optimizer with a weight_decay=1e-3( for avoiding overfitting) and trained our model for 15 epochs.We run all models for 5 times and report the average results on the test datasets.All our experiments are done on a GPU (GeForce RTX 2080) with 16 GB of memory."
        },
        {
            "heading": "6.3 Result and Analysis",
            "text": "We illustrate the experimental results of our proposed model along with other comparing system in Table 3.We show results for various language models we use in our representation layer namely BERT [12] (trained on Wikipedia and Book Corpus dataset, SCIBERT [3] (trained on a large multi-domain corpus of scientific publications), RoBERTa [31] (trained on CC NEWS dataset), GPT2 [6] (trained on MultiWOZ dataset), XLNET [55] (trained on Wikipedia, Book Corpus, Giga5, ClueWeb 2012-B, and Common Crawl dataset), SPECTRE [10] (trained on SCIDOCS dataset). According to the experimental results, we can come to the following conclusions:\n\u2022 First, MASEPR outperforms all non-BERT baselines on the Review Advisor dataset, which indicates that MASEPR has better ability to detect different multi-sentiment polarities in one sentence toward different aspect categories. \u2022 Second,MASEPR-BERT surpasses all BERT-basedmodels[13], indicating that MASEPR can achieve better performance by using more powerful sentence encoders."
        },
        {
            "heading": "6.4 Ablation Study",
            "text": "Multi-task learning [7] achieves improved performance by exploiting commonalities and differences across tasks. In this section we explore the performance of MASEPR in two settings namely single aspect and joint.The single aspect refers that the model is trained only for aspect category detection while joint refers that model is trained for both aspect category detection and its associated sentiment prediction together. We report the results of the experiment in Table 6,and it is clearly evident that multi task learning performs better than the single task learning.To prove the effectiveness of our model with respect to the real world scenario we tested our model on different data parameters : sentence length and number of aspects in a given sentence.Table 4 refers to the results of the model obtained on different sentence lengths.It clearly show that as the sentence length increases, modeling the information contained in the review sentence becomes complex and hence the model suffers a slight decrease in performance.We also explore the 5https://huggingface.co\neffect of number of aspects on model performance and report the results in Table 5. We can see as the number of aspect increases, aspect categories with similar writing styles such as (originality and motivation), (substance and soundness) which are harder to distinguish, confuses our model. 6.5 Qualitative Analysis We analyze the predictions made by our model by estimating the performance of our model in detecting the key instances (KID: words indicating an aspect category) of the given aspect category using SHAP[32]. Additive explanations by Shapley (SHAP)Machine learning models are commonly thought of as \u2018BlackBox\u2019 algorithms that provide little insight into input-output relationships. A lack of interpretability and formalisms demonstrate the importance of features in supervised learning of labeled data, both globally and locally. Shapley Additive exPlanations (SHAP) are a new development that allows for quantitative model interpretability estimation [33]. SHAP is a family of explanation algorithms that use Shapley values, a principled approach to allocating credit for a feature. Given a sentence \ud835\udc46\ud835\udc56 , we use Shapley values to compute the contribution/importance of each word\ud835\udc64 \ud835\udc57= v in \ud835\udc60\ud835\udc56 using partition explainer. Intuitively, Shapley values compute the marginal contribution for each feature over all possible subsets of features. Computing the exact Shapley value requires exponential time. Hence, the values are computed approximately through sampling. Feature importances computed via Shapley values have several appealing theoretical properties. Figure 5 shows an example of SHAP analysis, formally known as a force diagram for a sentence corresponding to a review. In each force diagram, the base value denotes an aspect\u2019s average predicted probability, while the figure\u2019s f(inputs) denotes the SHAP\u2019s predicted score concerning the model\u2019s prediction. If this score is greater than the base value, it shows that the model has identified that aspect, while if it is less than the base value, the model has not predicted that aspect. Also, if the arrows over the words or phrases shown in the bar area towards the model\u2019s prediction, then they say that these words support the model\u2019s decision and if they are opposite, then they that these words oppose the prediction of that aspect. In this case, the supporting words and phrases are shown in red, while the ones opposing the prediction are shown in blue. Also, the width of the words or phrase in the bar shown first in the diagram denotes the importance of that word or phrase; the more the width, the more the importance of the word.\nTo demonstrate the effectiveness of our model, We would first highlight some of the major roadblocks in the prediction of multiple aspects and their sentiments in a sentence and then use SHAP to demonstrate how our model tackles them.They are as follows :\n6.5.1 Predominance of one aspect over another. Often, a major roadblock in identifying multiple aspects of a sentence is the predominance of one aspect over another. There are usually fewer words devoted to one aspect than to others .For example in the sentence shown in Figure 5, the dominant aspect is motivation, while the suppressed aspect is substance.Our model takes advantage of contextualised word representations generated by transformers, computing the importance of each word relative to every aspect, enabling it to predict the output correctly even if a single word merely represents the aspect. We can see from SHAP that our model focuses on phrases such as \u2019interesting problem\u2019 and \u2019interesting\n41\nAuthorized licensed use limited to: Indian Institute of Technology Patna. Downloaded on February 13,2024 at 05:01:20 UTC from IEEE Xplore. Restrictions apply.\nSUBSTANCE NEGATIVE\nideas worth exploring\u2019 and can infer that the sentence talks about motivation positively. Apart from this, as evident from SHAP, our model establishes a relationship between different parts of the sentence even though the reviewer\u2019s language is confusing and can recognize that the sentence is speaking negatively about the aspect substance.\n6.5.2 Confusing Aspects. The purpose of this section is to raise a concern about the way of writing with regards to certain aspects such as (clarity, replicability), (motivation, originality), and (soundness, substance). We observed that some lexicons are commonly used to describe multiple aspects, creating a conflation of meaning in the model when it attempts to predict them.\n42\nAuthorized licensed use limited to: Indian Institute of Technology Patna. Downloaded on February 13,2024 at 05:01:20 UTC from IEEE Xplore. Restrictions apply.\nThe sentence in the figure 6 indicates that specific clarity lexicons such as clearly might create ambiguity as to what to predict; however, the force diagram shows that our model focuses more on the phrase \u2019not clear\u2019 when predicting replicability. The model\nrecognizes the relationship between the phrase \u2019not clear\u2019 with the words \u2019hinges,\u2019 \u2019rollouts,\u2019 and \u2019how it works\u2019 and infers that the sentence is discussing replicability in a negative sense. Furthermore, as can be seen from the force map, when predicting clarity, our model gives greater weight to the phrase \u2019commented lines in algorithm\u2019 than the phrase \u2019not clear,\u2019 which proves that our model can learn how usage of different words in various settings.\n43\nAuthorized licensed use limited to: Indian Institute of Technology Patna. Downloaded on February 13,2024 at 05:01:20 UTC from IEEE Xplore. Restrictions apply.\nAnother sentence shown in the figure 7 demonstrates one of the most confusing aspect pairs : ( soundness and substance); one talks about whether the experiments/claims in the paper are convincingly supported or not, while the other talks about whether the paper contains substantial experiments to demonstrate the effectiveness of the proposed methods. The main reason for the confusion is the similarity in the reviewer\u2019s tone while he/she refers to these aspects. The phrase \u2019rather weak\u2019 is used both to refer to the lack of adequate evidence supporting the paper\u2019s claims and the absence of any substantive experiments in the paper. Our model MASEPR, as illustrated by explanation maps, can comprehend the deeper underlying meaning of the sentence and correlates the phrase \"rather weak\" with various parts of the sentence while predicting the aspect, soundness, and substance along with their negative sentiment. It completely utilizes complex word representations encoded by transformers for learning such deep relationships. 6.5.3 Unique style of writing. This section intends to talk about those sentences which are too technically written ,with the absence of any sentiment denoting or directly aspect focussed words . One such sentence is shown in Figure 8 ,it talks about the originality of the paper but in a tone and style that is a bit different from the common style of sentence writing used to denote the aspect originality . Our model even struggles to understand such deeply engraved sentences but still is able to weakly identify the underlying aspect present in the sentence by establishing some kind of relationship existing between the words to learn the meaning of the sentence , as evident from the explainability diagram and hence relate the meaning with the aspect originality."
        },
        {
            "heading": "7 ERROR ANALYSIS",
            "text": "\u2022 Implicit semantic-level association of aspect categories:\nMost of the time, the aspect category- Substance is implicitly commented on. The essence lies in identifying specific phrases that talk about the amount of information or the overall presentation of the study. These phrases often closely resemble n-grams that may represent other aspect categories. At times, MASEPR confuses the close association for a different aspect category. In one of such cases where a reviewer wrote \"authors need to conduct more analyses to show why q-mtl is superior to supervised learning \", the phrase \u201canalyses to show why\" (pointing at the aspect category: Substance) resembles common phrases that indicate the presence of aspect category Soundness. Hence, the model has incorrectly predicted the aspect category - Soundness.\nAnother example - \u2018also, they fail at emphasizing the technical contribution of the paper see below as well\u2019, Predicted Label: SUBSTANCE-NEGATIVE, SOUNDNESS-NEGATIVE, True Label: SUBSTANCE-NEGATIVE The reason is reviewer often comments about the motivation with respect to to experimental methods used in a paper.\n\u2022 The presence of complex mathematical structures: For example in the sentence \"however, the theorems only prove bounds on the expected \u2225\ud835\udc5b\ud835\udc4e\ud835\udc4f\ud835\udc59\ud835\udc4e\ud835\udc53 \ud835\udc65\ud835\udc4e\u22252\nwhere a is uniformly sampled in t and x\ud835\udc4e is a random object that depends on s\ud835\udc61 sampled from math.\", Our model sometimes fails to determine correct aspect category or sentiment due to difficulty in understanding the context of a sentence having complex mathematical terms or an equation and predicts it as having no aspect while the true aspect is substance.."
        },
        {
            "heading": "8 CONCLUSION",
            "text": "Here in this work, we present our novel multitasking approach for joint multi-label aspect and sentiment extraction from peer review texts. Our model advances the current state of the art by predicting multiple aspects with varying polarity. In the future, we would like to expand this investigation towards developing a multi-tasking model to estimate the informativeness of the peer reviews. Our result and analysis clearly show that our model is able to understand the co-relations between different aspect categories and is also able to understand the writing style of reviewers.However, we all agree that scholarly language processing is not straightforward. In order to decide the fate of a manuscript or generate a more robust meta-review, pervasive models are required to capture the highlevel interaction between the paper and the peer reviews. We also recommend a larger dataset for peer reviews than what is used for this paper (8,877 papers) to avoid over-fitting during the training process."
        }
    ],
    "title": "Deciphering the Reviewers Aspectual Perspective: A Joint Multitask Framework for Aspect and Sentiment Extraction from Scholarly Peer Reviews",
    "year": 2023
}