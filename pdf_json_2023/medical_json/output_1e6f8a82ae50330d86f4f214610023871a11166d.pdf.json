{
    "abstractText": "The increasing penetration of renewable energy sources tends to redirect the power systems community\u2019s interest from the traditional power grid model towards the smart grid framework. During this transition, load forecasting for various time horizons constitutes an essential electric utility task in network planning, operation, and management. This paper presents a novel mixed power-load forecasting scheme for multiple prediction horizons ranging from 15 min to 24 h ahead. The proposed approach makes use of a pool of models trained by several machine-learning methods with different characteristics, namely neural networks, linear regression, support vector regression, random forests, and sparse regression. The final prediction values are calculated using an online decision mechanism based on weighting the individual models according to their past performance. The proposed scheme is evaluated on real electrical load data sensed from a high voltage/medium voltage substation and is shown to be highly effective, as it results in R2 coefficient values ranging from 0.99 to 0.79 for prediction horizons ranging from 15 min to 24 h ahead, respectively. The method is compared to several state-of-the-art machine-learning approaches, as well as a different ensemble method, producing highly competitive results in terms of prediction accuracy.",
    "authors": [
        {
            "affiliations": [],
            "name": "Nikolaos Giamarelos"
        },
        {
            "affiliations": [],
            "name": "Myron Papadimitrakis"
        },
        {
            "affiliations": [],
            "name": "Marios Stogiannos"
        },
        {
            "affiliations": [],
            "name": "Elias N. Zois"
        },
        {
            "affiliations": [],
            "name": "Alex Alexandridis"
        }
    ],
    "id": "SP:4afc58fcf4153ff9c15afccf0c9cb5155f8e368a",
    "references": [
        {
            "authors": [
                "N.-A.I. Livanos",
                "S. Hammal",
                "N. Giamarelos",
                "V. Alifragkis",
                "C.S. Psomopoulos",
                "E.N. Zois"
            ],
            "title": "OpenEdgePMU: An Open PMU Architecture with Edge Processing for Future Resilient Smart Grids",
            "venue": "Energies",
            "year": 2023
        },
        {
            "authors": [
                "H. Chen",
                "P. Xuan",
                "Y. Wang",
                "K. Tan",
                "X. Jin"
            ],
            "title": "Key Technologies for Integration of Multitype Renewable Energy Sources\u2014Research on Multi-Timeframe Robust Scheduling/Dispatch",
            "venue": "IEEE Trans. Smart Grid 2016,",
            "year": 2016
        },
        {
            "authors": [
                "M. Papadimitrakis",
                "N. Giamarelos",
                "M. Stogiannos",
                "E.N. Zois",
                "N.A. Livanos",
                "A. Alexandridis"
            ],
            "title": "Metaheuristic Search in Smart Grid: A Review with Emphasis on Planning, Scheduling and Power Flow Optimization Applications",
            "venue": "Renew. Sustain. Energy Rev",
            "year": 2021
        },
        {
            "authors": [
                "S.R. Salkuti"
            ],
            "title": "Day-Ahead Thermal and Renewable Power Generation Scheduling Considering Uncertainty",
            "venue": "Renew. Energy",
            "year": 2019
        },
        {
            "authors": [
                "L. Gong",
                "X. Wang",
                "M. Tian",
                "H. Yao",
                "J. Long"
            ],
            "title": "Multi-Objective Optimal Planning for Distribution Network Considering the Uncertainty of PV Power and Line-Switch State",
            "venue": "Sensors 2022,",
            "year": 2022
        },
        {
            "authors": [
                "M. Fotopoulou",
                "D. Rakopoulos",
                "O. Blanas",
                "S. Psomopoulos",
                "R.A. Munteanu",
                "K. Agavanakis"
            ],
            "title": "Day Ahead Optimal Dispatch Schedule in a Smart Grid Containing Distributed Energy Resources and Electric Vehicles",
            "venue": "Sensors 2021,",
            "year": 2021
        },
        {
            "authors": [
                "N. Zhang",
                "C. Kang",
                "Q. Xia",
                "Y. Ding",
                "Y. Huang",
                "R. Sun",
                "J. Huang",
                "J. Bai"
            ],
            "title": "A Convex Model of Risk-Based Unit Commitment for Day-Ahead Market Clearing Considering Wind Power Uncertainty",
            "venue": "IEEE Trans. Power Syst",
            "year": 2015
        },
        {
            "authors": [
                "D. Divenyi",
                "B. Polgari",
                "D. Raisz",
                "A. Sleisz",
                "P. Sores"
            ],
            "title": "Special Session on Proposal of a New European Co-Optimized Energy and Ancillary Service Market Design\u2014Part II",
            "venue": "In Proceedings of the International Conference on the European Energy Market,",
            "year": 2016
        },
        {
            "authors": [
                "U.B. Tayab",
                "A. Zia",
                "F. Yang",
                "J. Lu",
                "M. Kashif"
            ],
            "title": "Short-Term Load Forecasting for Microgrid Energy Management System Using Hybrid HHO-FNN Model with Best-Basis Stationary Wavelet Packet Transform",
            "venue": "Energy",
            "year": 2020
        },
        {
            "authors": [
                "A.A. Abdelsalam",
                "H.A. Zedan",
                "A.A. ElDesouky"
            ],
            "title": "Energy Management of Microgrids Using Load Shifting and Multi-Agent System",
            "venue": "J. Control Autom. Electr. Syst. 2020,",
            "year": 2020
        },
        {
            "authors": [
                "R. Basir",
                "S. Qaisar",
                "M. Ali",
                "M. Aldwairi",
                "M.I. Ashraf",
                "A. Mahmood",
                "M. Gidlund"
            ],
            "title": "Fog Computing Enabling Industrial Internet of Things: State-of-the-Art and Research Challenges",
            "venue": "Sensors",
            "year": 2019
        },
        {
            "authors": [
                "J. Zhang",
                "H. Han"
            ],
            "title": "A Lightweight and Privacy-Friendly Data Aggregation Scheme against Abnormal Data",
            "venue": "Sensors 2022,",
            "year": 2022
        },
        {
            "authors": [
                "M. Papadimitrakis",
                "A. Kapnopoulos",
                "S. Tsavartzidis",
                "A. Alexandridis"
            ],
            "title": "A Cooperative PSO Algorithm for Volt-VAR Optimization in Smart Distribution Grids",
            "venue": "Electr. Power Syst. Res",
            "year": 2022
        },
        {
            "authors": [
                "S.E. Haupt",
                "S. Dettling",
                "J.K. Williams",
                "J. Pearson",
                "T. Jensen",
                "T. Brummet",
                "B. Kosovic",
                "G. Wiener",
                "T. McCandless",
                "C. Burghardt"
            ],
            "title": "Blending Distributed Photovoltaic and Demand Load Forecasts",
            "venue": "Sol. Energy",
            "year": 2017
        },
        {
            "authors": [
                "Y. Pan",
                "J. Zheng",
                "Y. Yang",
                "R. Zhu",
                "C. Zhou",
                "T. Shi"
            ],
            "title": "An Electricity Load Forecasting Approach Combining DBN-Based Deep Neural Network and NAR Model for the Integrated Energy Systems",
            "venue": "In Proceedings of the 2019 IEEE International Conference on Big Data and Smart Computing,",
            "year": 2019
        },
        {
            "authors": [
                "A. Kaur",
                "L. Nonnenmacher",
                "Coimbra"
            ],
            "title": "C.F.M. Net Load Forecasting for High Renewable Energy Penetration Grids",
            "venue": "Energy",
            "year": 2016
        },
        {
            "authors": [
                "D.W. Van der Meer",
                "J. Munkhammar",
                "J. Wid\u00e9n"
            ],
            "title": "Probabilistic Forecasting of Solar Power, Electricity Consumption and Net Load: Investigating the Effect of Seasons, Aggregation and Penetration on Prediction Intervals",
            "venue": "Sol. Energy",
            "year": 2018
        },
        {
            "authors": [
                "J. Qin",
                "Y. Zhang",
                "S. Fan",
                "X. Hu",
                "Y. Huang",
                "Z. Lu",
                "Y. Liu"
            ],
            "title": "Multi-Task Short-Term Reactive and Active Load Forecasting Method",
            "venue": "Based on Attention-LSTM Model. Int. J. Electr. Power Energy Syst. 2022,",
            "year": 2023
        },
        {
            "authors": [
                "S. Theodoridis"
            ],
            "title": "Machine Learning: A Bayesian and Optimization Perspective",
            "year": 2015
        },
        {
            "authors": [
                "C. Wang",
                "G. Grozev",
                "S. Seo"
            ],
            "title": "Decomposition and Statistical Analysis for Regional Electricity Demand Forecasting",
            "venue": "Energy",
            "year": 2012
        },
        {
            "authors": [
                "A. Gerossier",
                "R. Girard",
                "G. Kariniotakis",
                "A. Michiorri"
            ],
            "title": "Probabilistic Day-Ahead Forecasting of Household Electricity Demand",
            "venue": "CIRED Open Access Proc. J. 2017,",
            "year": 2017
        },
        {
            "authors": [
                "Y. Yang",
                "S. Li",
                "W. Li",
                "M. Qu"
            ],
            "title": "Power Load Probability Density Forecasting",
            "venue": "Using Gaussian Process Quantile Regression. Appl. Energy",
            "year": 2018
        },
        {
            "authors": [
                "D.H. Vu",
                "K.M. Muttaqi",
                "A.P. Agalgaonkar"
            ],
            "title": "Assessing the Influence of Climatic Variables on Electricity Demand",
            "venue": "In Proceedings of the 2014 IEEE PES General Meeting|Conference & Exposition, National Harbor, MD,",
            "year": 2014
        },
        {
            "authors": [
                "J. Xie",
                "Y. Chen",
                "T. Hong",
                "T.D. Laing"
            ],
            "title": "Relative Humidity for Load Forecasting Models",
            "venue": "IEEE Trans. Smart Grid 2018,",
            "year": 2018
        },
        {
            "authors": [
                "C. Alzate",
                "M. Sinn"
            ],
            "title": "Improved Electricity Load Forecasting via Kernel Spectral Clustering of Smart Meters",
            "venue": "In Proceedings of the IEEE International Conference on Data Mining,",
            "year": 2013
        },
        {
            "authors": [
                "M. Rossi",
                "D. Brunelli"
            ],
            "title": "Electricity Demand Forecasting of Single Residential Units",
            "venue": "In Proceedings of the 2013 IEEE Workshop on Environmental, Energy and Structural Monitoring Systems, EESMS, Trento, Italy,",
            "year": 2013
        },
        {
            "authors": [
                "P. Damrongkulkamjorn",
                "P. Churueang"
            ],
            "title": "Monthly Energy Forecasting Using Decomposition Method with Application of Seasonal ARIMA",
            "venue": "In Proceedings of the 2005 International Power Engineering Conference, Singapore,",
            "year": 2005
        },
        {
            "authors": [
                "M. Ould Mohamed Mahmoud",
                "F. Mhamdi",
                "M. Ja\u00efdane-Sa\u00efdane"
            ],
            "title": "Long Term Multi-Scale Analysis of the Daily Peak Load Based on the Empirical Mode Decomposition",
            "venue": "In Proceedings of the 2009 IEEE Bucharest PowerTech, Bucharest, Romania,",
            "year": 2009
        },
        {
            "authors": [
                "A.E. Clements",
                "A.S. Hurn",
                "Z. Li"
            ],
            "title": "Forecasting Day-Ahead Electricity Load Using a Multiple Equation Time Series Approach",
            "venue": "Eur. J. Oper. Res",
            "year": 2016
        },
        {
            "authors": [
                "Z.S. Elrazaz",
                "A.A. Mazi"
            ],
            "title": "Unified Weekly Peak Load Forecasting for Fast Growing Power System",
            "venue": "IEE Proc. C Gener. Transm. Distrib",
            "year": 2010
        },
        {
            "authors": [
                "M.H. Amini",
                "A. Kargarian",
                "O. Karabasoglu"
            ],
            "title": "ARIMA-Based Decoupled Time Series Forecasting of Electric Vehicle Charging Demand for Stochastic Power System Operation",
            "venue": "Electr. Power Syst. Res",
            "year": 2016
        },
        {
            "authors": [
                "C.N. Yu",
                "P. Mirowski",
                "T.K. Ho"
            ],
            "title": "A Sparse Coding Approach to Household Electricity Demand Forecasting in Smart Grids",
            "venue": "IEEE Trans. Smart Grid 2017,",
            "year": 2017
        },
        {
            "authors": [
                "D. Yang",
                "L. Xu",
                "S. Gong",
                "H. Li",
                "G.D. Peterson",
                "Z. Zhang"
            ],
            "title": "Joint Electrical Load Modeling and Forecasting Based on Sparse Bayesian Learning for the Smart Grid",
            "venue": "In Proceedings of the 2011 45th Annual Conference on Information Sciences and Systems,",
            "year": 2011
        },
        {
            "authors": [
                "X. Sun",
                "X. Wang",
                "J. Wu",
                "Y. Liu"
            ],
            "title": "Hierarchical Sparse Learning for Load Forecasting in Cyber-Physical Energy Systems",
            "venue": "In Proceedings of the Conference Record\u2014IEEE Instrumentation and Measurement Technology Conference,",
            "year": 2013
        },
        {
            "authors": [
                "Q. Duan",
                "W.X. Sheng",
                "Y. Ma",
                "K. Ma"
            ],
            "title": "Sparse Bayesian Learning Using Combined Kernels for Medium Term Load Forecasting",
            "venue": "In Proceedings of the 2nd IET Renewable Power Generation Conference (RPG 2013),",
            "year": 2013
        },
        {
            "authors": [
                "N. Giamarelos",
                "E.N. Zois",
                "M. Papadimitrakis",
                "M. Stogiannos",
                "N.A.-I. Livanos",
                "A. Alexandridis"
            ],
            "title": "Short-Term Electric Load Forecasting with Sparse Coding Methods",
            "venue": "IEEE Access 2021,",
            "year": 2021
        },
        {
            "authors": [
                "M.K. Azad",
                "S. Uddin",
                "M. Takruri"
            ],
            "title": "Support Vector Regression Based Electricity Peak Load Forecasting",
            "venue": "In Proceedings of the 11th International Symposium on Mechatronics and its Applications,",
            "year": 2018
        },
        {
            "authors": [
                "J. Che",
                "J. Wang"
            ],
            "title": "Short-Term Load Forecasting Using a Kernel-Based Support Vector Regression",
            "venue": "Combination Model. Appl. Energy",
            "year": 2014
        },
        {
            "authors": [
                "Y. Li",
                "J. Che",
                "Y. Yang"
            ],
            "title": "Subsampled Support Vector Regression Ensemble for Short Term Electric Load Forecasting",
            "venue": "Energy",
            "year": 2018
        },
        {
            "authors": [
                "L. Ghelardoni",
                "A. Ghio",
                "D. Anguita"
            ],
            "title": "Energy Load Forecasting Using Empirical Mode Decomposition and Support Vector Regression",
            "venue": "IEEE Trans. Smart Grid 2013,",
            "year": 2013
        },
        {
            "authors": [
                "G. Hafeez",
                "I. Khan",
                "S. Jan",
                "I.A. Shah",
                "F.A. Khan",
                "A. Derhab"
            ],
            "title": "A Novel Hybrid Load Forecasting Framework with Intelligent Feature Engineering and Optimization Algorithm in Smart Grid",
            "venue": "Appl. Energy",
            "year": 2021
        },
        {
            "authors": [
                "N. Korovesis",
                "D. Kandris",
                "G. Koulouras",
                "A. Alexandridis"
            ],
            "title": "Robot Motion Control via an Eeg-Based Brain\u2013Computer Interface by Using Neural Networks and Alpha Brainwaves",
            "venue": "Electronics 2019,",
            "year": 2019
        },
        {
            "authors": [
                "E. Akarslan",
                "F.O. Hocaoglu"
            ],
            "title": "Electricity Demand Forecasting of a Micro Grid Using ANN",
            "venue": "In Proceedings of the 2018 9th International Renewable Energy Congress, IREC, Hammamet, Tunisia,",
            "year": 2018
        },
        {
            "authors": [
                "A.K. Pandey",
                "K.B. Sahay",
                "D. Chandra",
                "M.M. Tripathi"
            ],
            "title": "Day Ahead Load Forecast in ISO New England Market and Ontario Market Using a Novel ANN",
            "venue": "Int. J. Res. Emerg. Sci. Technol. 2015,",
            "year": 2015
        },
        {
            "authors": [
                "M.H.M.R.S. Dilhani",
                "C. Jeenanunta"
            ],
            "title": "Daily Electric Load Forecasting: Case of Thailand",
            "venue": "In Proceedings of the 7th International Conference on Information Communication Technology for Embedded Systems",
            "year": 2016
        },
        {
            "authors": [
                "M.Q. Raza",
                "Z. Baharudin",
                "Badar-Ul-Islam",
                "M. Azman Zakariya",
                "M.H.M. Khir"
            ],
            "title": "Neural Network Based STLF Model to Study the Seasonal Impact of Weather and Exogenous Variables",
            "venue": "Res. J. Appl. Sci. Eng. Technol",
            "year": 2013
        },
        {
            "authors": [
                "K.B. Sahay",
                "M.M. Tripathi"
            ],
            "title": "Day Ahead Hourly Load Forecast of PJM Electricity Market and Iso New England Market by Using Artificial Neural Network",
            "venue": "In Proceedings of the ISGT 2014, Istanbul, Turkiye,",
            "year": 2014
        },
        {
            "authors": [
                "C. Rold\u00e1n-Blay",
                "G. Escriv\u00e1-Escriv\u00e1",
                "C. \u00c1lvarez-Bel",
                "C. Rold\u00e1n-Porta",
                "J. Rodr\u00edguez-Garc\u00eda"
            ],
            "title": "Upgrade of an Artificial Neural Network Prediction Method for Electrical Consumption Forecasting Using an Hourly Temperature",
            "venue": "Curve Model. Energy Build",
            "year": 2013
        },
        {
            "authors": [
                "F. Rodrigues",
                "C. Cardeira",
                "J.M.F. Calado"
            ],
            "title": "The Daily and Hourly Energy Consumption and Load Forecasting Using Artificial Neural Network Method: A Case Study Using a Set of 93 Households in Portugal",
            "venue": "Energy Procedia",
            "year": 2014
        },
        {
            "authors": [
                "K.B. Sahay",
                "S. Sahu",
                "P. Singh"
            ],
            "title": "Short-Term Load Forecasting of Toronto Canada by Using Different ANN Algorithms",
            "venue": "In Proceedings of the 2016 IEEE 6th International Conference on Power Systems (ICPS),",
            "year": 2016
        },
        {
            "authors": [
                "L. Alhmoud",
                "R. Abu Khurma",
                "A.M. Al-Zoubi",
                "I. Aljarah"
            ],
            "title": "A Real-Time Electrical Load Forecasting in Jordan Using an Enhanced Evolutionary Feedforward Neural Network",
            "venue": "Sensors 2021,",
            "year": 2021
        },
        {
            "authors": [
                "Z. Liao",
                "H. Pan",
                "X. Huang",
                "R. Mo",
                "X. Fan",
                "H. Chen",
                "L. Liu",
                "Y. Li"
            ],
            "title": "Short-Term Load Forecasting with Dense Average Network",
            "venue": "Expert Syst. Appl",
            "year": 2021
        },
        {
            "authors": [
                "M. Ding",
                "L. Wang",
                "R. Bi"
            ],
            "title": "An ANN-Based Approach for Forecasting the Power Output of Photovoltaic System",
            "venue": "Procedia Environ. Sci. 2011,",
            "year": 2011
        },
        {
            "authors": [
                "R.M. Salgado",
                "T. Ohishi",
                "R. Ballini"
            ],
            "title": "A Short-Term Bus Load Forecasting System",
            "venue": "In Proceedings of the 2010 10th International Conference on Hybrid Intelligent Systems, Atlanta, GA, USA,",
            "year": 2010
        },
        {
            "authors": [
                "L. Hernandez",
                "C. Baladron",
                "J. Aguiar",
                "B. Carro",
                "A. Sanchez-Esguevillas",
                "J. Lloret",
                "D. Chinarro",
                "J. Gomez-Sanz",
                "D. Cook"
            ],
            "title": "A Multi-Agent System Architecture for Smart Grid Management and Forecasting of Energy Demand in Virtual Power Plants",
            "venue": "IEEE Commun. Mag",
            "year": 2013
        },
        {
            "authors": [
                "Y.I. Alamin",
                "J.D. \u00c1lvarez",
                "M. del Mar Castilla",
                "A. Ruano"
            ],
            "title": "An Artificial Neural Network (ANN) Model to Predict the Electric Load Profile for an HVAC System",
            "venue": "IFAC PapersOnLine",
            "year": 2018
        },
        {
            "authors": [
                "E. Gonz\u00e1lez-Romera",
                "M.\u00c1. Jaramillo-Mor\u00e1n",
                "D. Carmona-Fern\u00e1ndez"
            ],
            "title": "Monthly Electric Energy Demand Forecasting Based on Trend Extraction",
            "venue": "IEEE Trans. Power Syst",
            "year": 2006
        },
        {
            "authors": [
                "N.J. Johannesen",
                "M. Kolhe",
                "M. Goodwin"
            ],
            "title": "Relative Evaluation of Regression Tools for Urban Area Electrical Energy Demand Forecasting",
            "venue": "J. Clean. Prod",
            "year": 2019
        },
        {
            "authors": [
                "W. Zhang",
                "H. Quan",
                "D. Srinivasan"
            ],
            "title": "Parallel and Reliable Probabilistic Load Forecasting via Quantile Regression Forest and Quantile Determination",
            "venue": "Energy",
            "year": 2018
        },
        {
            "authors": [
                "P. Lusis",
                "K.R. Khalilpour",
                "L. Andrew",
                "A. Liebman"
            ],
            "title": "Short-Term Residential Load Forecasting: Impact of Calendar Effects and Forecast Granularity",
            "venue": "Appl. Energy",
            "year": 2017
        },
        {
            "authors": [
                "T. Vantuch",
                "A.G. Vidal",
                "A.P. Ramallo-Gonzalez",
                "A.F. Skarmeta",
                "S. Misak"
            ],
            "title": "Machine Learning Based Electric Load Forecasting for Short and Long-Term Period",
            "venue": "In Proceedings of the 2018 IEEE 4th World Forum on Internet of Things (WF-IoT), Singapore,",
            "year": 2018
        },
        {
            "authors": [
                "M. Bessec",
                "J. Fouquau"
            ],
            "title": "Short-Run Electricity Load Forecasting with Combinations of Stationary Wavelet Transforms",
            "venue": "Eur. J. Oper. Res",
            "year": 2018
        },
        {
            "authors": [
                "N. Mughees",
                "S.A. Mohsin",
                "A. Mughees"
            ],
            "title": "Deep Sequence to Sequence Bi-LSTM Neural Networks for Day-Ahead Peak Load Forecasting",
            "venue": "Expert Syst. Appl",
            "year": 2021
        },
        {
            "authors": [
                "S.P. Adam",
                "S.A.N. Alexandropoulos",
                "P.M. Pardalos",
                "M.N. Vrahatis"
            ],
            "title": "No Free Lunch Theorem: A Review",
            "venue": "Approx. Optim",
            "year": 2019
        },
        {
            "authors": [
                "O. Ahmia",
                "N. Farah"
            ],
            "title": "Multi-Model Approach for Electrical Load Forecasting",
            "venue": "In Proceedings of the 2015 SAI Intelligent Systems Conference (IntelliSys), London, UK,",
            "year": 2015
        },
        {
            "authors": [
                "P. Zeng",
                "M. Jin",
                "M.F. Elahe"
            ],
            "title": "Short-Term Power Load Forecasting Based on Cross Multi-Model and Second Decision Mechanism",
            "venue": "IEEE Access 2020,",
            "year": 2020
        },
        {
            "authors": [
                "C. Peng",
                "Y. Tao",
                "Z. Chen",
                "Y. Zhang",
                "X. Sun"
            ],
            "title": "Multi-Source Transfer Learning Guided Ensemble LSTM for Building Multi-Load Forecasting",
            "venue": "Expert Syst. Appl",
            "year": 2022
        },
        {
            "authors": [
                "S. Li",
                "Y. Zhong",
                "J. Lin"
            ],
            "title": "AWS-DAIE: Incremental Ensemble Short-Term Electricity Load Forecasting Based on Sample Domain Adaptation",
            "venue": "Sustainability 2022,",
            "year": 2022
        },
        {
            "authors": [
                "L. Yu",
                "S. Wang",
                "K.K. Lai"
            ],
            "title": "A Novel Nonlinear Ensemble Forecasting Model Incorporating GLAR and ANN for Foreign Exchange Rates",
            "venue": "Comput. Oper. Res",
            "year": 2005
        },
        {
            "authors": [
                "P.S.G. De Mattos Neto",
                "J.F.L. De Oliveira",
                "P. Bassetto",
                "H.V. Siqueira",
                "L. Barbosa",
                "E. Pereira Alves",
                "M.H.N. Marinho",
                "G.F. Rissi",
                "F. Li",
                "J.F.L Oliveira"
            ],
            "title": "Energy Consumption Forecasting for Smart Meters Using Extreme Learning Machine Ensemble",
            "year": 2021
        },
        {
            "authors": [
                "Y. Hu",
                "B. Qu",
                "J. Wang",
                "J. Liang",
                "Y. Wang",
                "K. Yu",
                "Y. Li",
                "K. Qiao"
            ],
            "title": "Short-Term Load Forecasting Using Multimodal Evolutionary Algorithm and Random Vector Functional Link Network Based Ensemble Learning",
            "venue": "Appl. Energy 2021,",
            "year": 2023
        },
        {
            "authors": [
                "Q. Chen",
                "W. Zhang",
                "K. Zhu",
                "D. Zhou",
                "H. Dai",
                "Q. Wu"
            ],
            "title": "A Novel Trilinear Deep Residual Network with Self-Adaptive Dropout Method for Short-Term Load Forecasting",
            "venue": "Expert Syst. Appl",
            "year": 2021
        },
        {
            "authors": [
                "R. Gao",
                "L. Du",
                "P.N. Suganthan",
                "Q. Zhou",
                "K.F. Yuen"
            ],
            "title": "Random Vector Functional Link Neural Network Based Ensemble Deep Learning for Short-Term Load Forecasting",
            "venue": "Expert Syst. Appl",
            "year": 2022
        },
        {
            "authors": [
                "M. Saviozzi",
                "S. Massucco",
                "F. Silvestro"
            ],
            "title": "Implementation of Advanced Functionalities for Distribution Management Systems: Load Forecasting and Modeling through Artificial Neural Networks Ensembles",
            "venue": "Electr. Power Syst. Res",
            "year": 2019
        },
        {
            "authors": [
                "D.C. Montgomery",
                "E.A. Peck",
                "G.G. Vining"
            ],
            "title": "Introduction to Linear Regression Analysis",
            "venue": "Eds.; John Wiley & Sons: Hoboken, NJ,",
            "year": 2012
        },
        {
            "authors": [
                "Z. Zhang",
                "Y. Xu",
                "J. Yang",
                "X. Li",
                "D. Zhang"
            ],
            "title": "A Survey of Sparse Representation: Algorithms and Applications",
            "venue": "IEEE Access 2015,",
            "year": 2015
        },
        {
            "authors": [
                "Y.C. Pati",
                "R. Rezaiifar",
                "P.S. Krishnaprasad"
            ],
            "title": "Orthogonal Matching Pursuit: Recursive Function Approximation with Applications to Wavelet Decomposition",
            "venue": "In Proceedings of the 27th Asilomar Conference on Signals, Systems and Computers,",
            "year": 1993
        },
        {
            "authors": [
                "A. Beck",
                "M. Teboulle"
            ],
            "title": "A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems",
            "venue": "SIAM J. Imaging Sci. 2009,",
            "year": 2009
        },
        {
            "authors": [
                "S.S. Chen",
                "D.L. Donoho",
                "M.A. Saunders"
            ],
            "title": "Atomic Decomposition by Basis Pursuit",
            "venue": "SIAM Rev",
            "year": 2001
        },
        {
            "authors": [
                "J. Mairal",
                "F. Bach",
                "J. Ponce",
                "G. Sapiro",
                "A. Zisserman"
            ],
            "title": "Supervised Dictionary Learning",
            "venue": "In Proceedings of the Advances in Neural Information Processing Systems,",
            "year": 2009
        },
        {
            "authors": [
                "J. Mairal",
                "F. Bach",
                "J. Ponce",
                "G. Sapiro"
            ],
            "title": "Online Dictionary Learning for Sparse Coding",
            "venue": "In Proceedings of the ACM International Conference Proceeding Series, Athens, Greece,",
            "year": 2009
        },
        {
            "authors": [
                "K. Engan",
                "S.O. Aase",
                "J.H. Husoy"
            ],
            "title": "Method of Optimal Directions for Frame Design",
            "venue": "In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings. ICASSP99,",
            "year": 1999
        },
        {
            "authors": [
                "M. Aharon",
                "M. Elad",
                "A. Bruckstein"
            ],
            "title": "K-SVD: An Algorithm for Designing Overcomplete Dictionaries for Sparse Representation",
            "venue": "IEEE Trans. Signal Process",
            "year": 2006
        },
        {
            "authors": [
                "H. Robbins",
                "S. Monro"
            ],
            "title": "A Stochastic Approximation Method",
            "venue": "Ann. Math. Stat",
            "year": 1951
        },
        {
            "authors": [
                "E. Kyriakides",
                "M. Polycarpou"
            ],
            "title": "Short Term Electric Load Forecasting: A Tutorial",
            "venue": "Trends Neural Comput",
            "year": 2006
        },
        {
            "authors": [
                "W.C. Hong"
            ],
            "title": "Intelligent Energy Demand Forecasting",
            "year": 2013
        },
        {
            "authors": [
                "A. Alexandridis",
                "E. Chondrodima",
                "N. Giannopoulos",
                "H. Sarimveis"
            ],
            "title": "A Fast and Efficient Method for Training Categorical Radial Basis Function Networks",
            "venue": "IEEE Trans. Neural Networks Learn. Syst",
            "year": 2017
        },
        {
            "authors": [
                "L. Breiman",
                "J.H. Friedman",
                "R.A. Olshen",
                "C.J. Stone"
            ],
            "title": "Classification and Regression Trees; John Wiley and Sons: Hoboken, NJ, USA, 2017; ISBN 9781351460491",
            "year": 2017
        },
        {
            "authors": [
                "Z. Tan",
                "J. Zhang",
                "Y. He",
                "Y. Zhang",
                "G. Xiong",
                "Y. Liu"
            ],
            "title": "Short-Term Load Forecasting Based on Integration of SVR and Stacking",
            "venue": "IEEE Access 2020,",
            "year": 2020
        },
        {
            "authors": [
                "M. Zulfiqar",
                "K.A.A. Gamage",
                "M. Kamran",
                "M.B. Rasheed"
            ],
            "title": "Hyperparameter Optimization of Bayesian Neural Network Using Bayesian Optimization and Intelligent Feature Engineering for Load Forecasting",
            "venue": "Sensors 2022,",
            "year": 2022
        },
        {
            "authors": [
                "D.W. Marquardt"
            ],
            "title": "An Algorithm for Least-Squares Estimation of Nonlinear Parameters",
            "venue": "J. Soc. Ind. Appl. Math",
            "year": 1963
        },
        {
            "authors": [
                "A. Alexandridis",
                "E. Chondrodima",
                "H. Sarimveis"
            ],
            "title": "Radial Basis Function Network Training Using a Nonsymmetric Partition of the Input Space and Particle Swarm Optimization",
            "venue": "IEEE Trans. Neural Netw. Learn. Syst",
            "year": 2013
        },
        {
            "authors": [
                "M. Papadimitrakis",
                "A. Alexandridis"
            ],
            "title": "Active Vehicle Suspension Control Using Road Preview Model Predictive Control and Radial Basis Function Networks",
            "venue": "Appl. Soft Comput",
            "year": 2022
        },
        {
            "authors": [
                "D. Karamichailidou",
                "A. Alexandridis",
                "G. Anagnostopoulos",
                "G. Syriopoulos",
                "O. Sekkas"
            ],
            "title": "Modeling Biogas Production from Anaerobic Wastewater Treatment Plants Using Radial Basis Function Networks and Differential Evolution",
            "venue": "Comput. Chem. Eng",
            "year": 2022
        },
        {
            "authors": [
                "E. Chondrodima",
                "H. Georgiou",
                "N. Pelekis",
                "Y. Theodoridis"
            ],
            "title": "Particle Swarm Optimization and RBF Neural Networks for Public Transport Arrival Time Prediction",
            "venue": "Using GTFS Data. Int. J. Inf. Manag. Data Insights 2022,",
            "year": 2022
        },
        {
            "authors": [
                "D. Karamichailidou",
                "V. Kaloutsa",
                "A. Alexandridis"
            ],
            "title": "Wind Turbine Power Curve Modeling Using Radial Basis Function Neural Networks and Tabu Search",
            "venue": "Renew. Energy",
            "year": 2021
        },
        {
            "authors": [
                "L. Sun",
                "K. Zhou",
                "X. Zhang",
                "S. Yang"
            ],
            "title": "Outlier Data Treatment Methods Toward Smart Grid Applications",
            "venue": "IEEE Access 2018,",
            "year": 2018
        },
        {
            "authors": [
                "P. Mart\u00edn",
                "G. Moreno",
                "F.J. Rodr\u00edguez",
                "J.A. Jim\u00e9nez",
                "I. Fern\u00e1ndez"
            ],
            "title": "A Hybrid Approach to Short-Term Load Forecasting Aimed at Bad Data Detection in Secondary Substation Monitoring Equipment",
            "venue": "Sensors 2018,",
            "year": 2018
        },
        {
            "authors": [
                "Y. Wang",
                "S. Sun",
                "X. Chen",
                "X. Zeng",
                "Y. Kong",
                "J. Chen",
                "Y. Guo",
                "T. Wang"
            ],
            "title": "Short-Term Load Forecasting of Industrial Customers Based on SVMD and XGBoost",
            "venue": "Int. J. Electr. Power Energy Syst. 2021,",
            "year": 2023
        },
        {
            "authors": [
                "S. Wang",
                "X. Wang",
                "D. Wang"
            ],
            "title": "Bi-Directional Long Short-Term Memory Method Based on Attention Mechanism and Rolling Update for Short-Term Load Forecasting",
            "venue": "Int. J. Electr. Power Energy Syst",
            "year": 2019
        },
        {
            "authors": [
                "U. Javed",
                "K. Ijaz",
                "M. Jawad",
                "I. Khosa",
                "E. Ahmad Ansari",
                "K. Shabih Zaidi",
                "M. Nadeem Rafiq",
                "N. Shabbir"
            ],
            "title": "A Novel Short Receptive Field Based Dilated Causal Convolutional Network Integrated with Bidirectional LSTM for Short-Term Load Forecasting",
            "venue": "Expert Syst. Appl",
            "year": 2022
        },
        {
            "authors": [
                "A. Rom\u00e1n-Portabales",
                "M. L\u00f3pez-Nores",
                "J.J. Pazos-Arias"
            ],
            "title": "Systematic Review of Electricity Demand Forecast Using ANN-Based Machine Learning Algorithms",
            "venue": "Sensors 2021,",
            "year": 2021
        },
        {
            "authors": [
                "M.M. Rashidi",
                "M. Alhuyi Nazari",
                "I. Mahariq",
                "N. Ali"
            ],
            "title": "Modeling and Sensitivity Analysis of Thermal Conductivity of Ethylene Glycol-Water Based Nanofluids with Alumina Nanoparticles",
            "year": 2023
        },
        {
            "authors": [
                "B. Zhou",
                "Y. Meng",
                "W. Huang",
                "H. Wang",
                "L. Deng",
                "S. Huang",
                "J. Wei"
            ],
            "title": "Multi-Energy Net Load Forecasting for Integrated Local Energy Systems with Heterogeneous Prosumers",
            "venue": "Int. J. Electr. Power Energy Syst",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "Citation: Giamarelos, N.;\nPapadimitrakis, M.; Stogiannos, M.;\nZois, E.N.; Livanos, N.-A.I.;\nAlexandridis, A. A Machine Learning\nModel Ensemble for Mixed Power\nLoad Forecasting across Multiple\nTime Horizons. Sensors 2023, 23, 5436.\nhttps://doi.org/10.3390/s23125436\nAcademic Editors: Yuh-Shyan Chen,\nPaolo Visconti and Wei Yi\nReceived: 24 April 2023\nRevised: 26 May 2023\nAccepted: 2 June 2023\nPublished: 8 June 2023\nCopyright: \u00a9 2023 by the authors.\nLicensee MDPI, Basel, Switzerland.\nThis article is an open access article\ndistributed under the terms and\nconditions of the Creative Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n4.0/).\nKeywords: load forecasting; ensemble learning; neural networks; sparse representation; support vector regression"
        },
        {
            "heading": "1. Introduction",
            "text": "The modernization of the communication infrastructure of the electrical grid, featuring smart sensors, IoT, and edge computing [1], as well as the deregulation of the electric power markets, has enabled the proliferation of distributed generation, mainly from renewable energy sources (RES) [2]. This new paradigm, while actualizing the much sought-after decarbonization of energy generation, has jeopardized the stability of the distribution network due to the intermittency of the aforementioned resources, mainly in isolated power grids, such as non-interconnected islands. The effect of this intermittency is twofold: From the distribution system operator perspective, uncertainty in RES generation compromises the ability to effectively plan short-term unit commitment scheduling [3\u20136], while from the energy market bidder perspective, stochasticity severely constrains bidding strategy and thus, reduces profit margins [7]. An important development in the field of energy transactions is the participation of the energy market in the distribution grid through ancillary services, which is expected to be established in the upcoming years [8]. These shortcomings underline the importance of the application of effective electric load prediction models in the context of multiple operational aspects of the smart grid, such as power stability and security. Especially in the case of micro-grids, storage management is critical and cannot be accomplished without the aid of accurate short-term load forecasts for load shifting and balancing operations [9,10]. Moreover, the grid extension and the\nSensors 2023, 23, 5436. https://doi.org/10.3390/s23125436 https://www.mdpi.com/journal/sensors\nSensors 2023, 23, 5436 2 of 25\nincreasing exploitation of smart meters affect the efficient operation of the grid, leading to a complex and multifaceted framework [11,12]. As regards the distribution network on the substation level, load forecasting up to one day ahead, could be a valuable asset in the grid\u2019s optimization tasks [13]. Such actions can be carried out, not only by controlling the on-load tap changer (OLTC) and capacitor bank movements, which is currently the industry standard, but also by operation scheduling of batteries in the near future. Load forecasting with multiple time horizons participates in different, interdependent levels of operation of a power grid and thus can make a significant contribution to addressing the aforementioned challenges. A pivotal feature of smart grid is the bidirectional power flow and communication through administrators of generation, transmission, distribution, and end-users. As 8a result, the corresponding energy data contain mixed power-load (hereby referred to as \u2019mixed load\u2019). While the majority of the load forecasting models found in the literature predict the electric load production or consumption, the differences between them are frequently studied as well. The forecasting of the so-called net load proves useful for tackling load volatility due to increasing RES penetration [14\u201318]. These net energy load prediction models utilize historic distribution grid load data as well as measurements of weather features on a substation level in order to infer the net active power (AP) demand of the distribution grid. To this end, the field of computational intelligence, and more specifically, the branch of machine-learning (ML) [19], has proved to be an invaluable source providing a multitude of approaches to solving the aforementioned problem. ML methodologies are capable of extracting knowledge from historical data in order to develop black-box models and avoid the computationally intensive use of first-principle equations. Such algorithms can exhibit a number of important advantages like efficiency, increased prediction accuracy, robustness, etc., but require a number of suitable data to do so. The most common ML methodologies used in the context of load forecasting are mentioned in the following literature review. Linear regression (LR) belongs to the methods originally used for load forecasting. An approach based on the regression analysis of decomposed time series is presented in [20] for modeling both voltage and electricity demand volatility. A probabilistic approach for handling the uncertainties of power load data is proposed in [21], employing quantile regression, while in [22], weather conditions and electricity prices are also considered. The influence of climatic variables on electricity demand forecasting is examined thoroughly in references [23,24]. The issue of improving the prediction of load consumption data of a set of smart meters is addressed in [25], featuring a periodic autoregressive model with exogenous variables (PARX), which include calendar and temperature information. An important research matter in load forecasting has been the presence of seasonal effects. To this end, different techniques have been proposed, presenting triple exponential smoothing [26], decomposition methods [27,28], or multiple equation time series [29]. Similarly, the maximum weekly load consumption is forecasted for a one-year horizon in [30], where the different components of the decomposed load are modeled by ARIMAX and ARIMA models. These models incorporate previous forecasting errors in the regression equation and thus outperform the simpler AR models in general. The authors of [31] make use of an ARIMA model for electric vehicle charging demand forecasting, the outputs of which are used to formulate a stochastic day-ahead scheduling problem. Over the last few years, sparsity has emerged as a general principle for signal modeling. Sparse coding refers to the modeling of data signals as the sum of a few basic elements. Although it was primarily used in image processing, recently, there has been considerable interest in electrical signals. The use of sparse coding for modeling and forecasting individual household electricity loads was studied in [32] by implementing the alternating direction method of multipliers (ADMM) algorithm for solving the dictionary learning problem. A number of papers based on sparse Bayesian learning (SBL) have been published during the last decade, featuring weighted SBL [33,34] or combined kernels SBL [35]. More recently, a hierarchical sparsity approach has been proposed [36] for hourly load forecasting,\nSensors 2023, 23, 5436 3 of 25\nachieving remarkable results and outperforming both well-known sparse techniques and rival linear and non-linear methodologies. Load forecasting using support vector machines for regression has been gaining popularity in recent years, due to the ability of this method to model the non-linearities present in the electric load prediction problem. The support vector regression (SVR) model in [37] deals with peak load forecasting. A polynomial kernel function is chosen, the parameters of which are optimized through multiple cross-validation. In contrast to conventional SVR models, the idea of pooling information from different multiple individual models with different kernel functions is introduced in [38,39]. The hourly load consumption prediction is attempted in [40], proposing an SVR model which exploits the empirical mode decomposition method to disaggregate a time series into two sets of components, respectively describing the trend and the local oscillations of the energy consumption measurements. A hybrid model [41] has been proposed recently for halfhourly demand forecasting using a modified fire-fly optimization algorithm for tuning the SVR hyperparameters. Owing to their capability of capturing non-linear correlations, feed-forward neural networks [42] have been extensively applied for the prediction of both load consumption and demand. For the scope of this work, multi-layer perceptron (MLP) and radial basis function (RBF) networks are investigated, and their underlying structure is presented in the next section. The following papers propose the implementation of feed-forward neural networks for hourly load forecasting using historical data on hourly consumption. Particular calendar indices are also used by [43,44], while temperature and humidity data by [45]. These variables are common in most models, as the positive impact of exogenous variables on load forecasting has been confirmed by a number of articles, e.g., [46\u201349]. In many cases, the Levenberg\u2013Marquardt algorithm is selected for artificial neural network training showing a better performance over other algorithms [50,51]. According to [52], the proposed multi-layered feed-forward neural network, which is optimized by grey wolf optimizer, demonstrates superior forecasting accuracy compared to simple LR as well as MLP combined with popular metaheuristic methods. An advanced artificial neural network following a novel connection between layers, called the dense average connection, has been proposed recently [53], showing satisfactory results. Aiming to improve the training procedure of a three-layer feed-forward neural network, [54] proposes an advanced backpropagation algorithm. The authors of [55] propose a methodology for short-term bus load forecasting, where an MLP neural network is combined with a bus-clustering algorithm, achieving reduced computational time. A multi-agent system architecture is designed in [56], including the identification of 24-h demand patterns, the classification of days according to these patterns, and finally, a set of MLPs for demand forecasting. Regarding the use of RBF networks, it occurs on a lesser scale. To be more specific, electric load demand forecasting models have been developed featuring short [57] and long-term [58] prediction horizons. Random forests (RF) constitute an ensemble machine-learning method presenting very good predictive accuracy and have been used in a number of applications, including load forecasting. Several regression models are evaluated in [59], with random forest regressor to provide better short-term load demand predictions than k-nearest neighbor regressor and linear regressor in terms of MAPE. A probabilistic load forecasting model based on quantile regression forest is developed in [60] and is enhanced by recursive feature elimination for the purpose of feature selection. Moreover, the authors propose an alternative quantile determination method to alleviate the reliability issue of direct prediction interval construction. It is apparent from the literature that the problem of electric load forecasting has been addressed by multiple machine-learning methods, but without any of them achieving universal superiority in terms of performance. This observation is confirmed not only by studying the individual research results but also by assessing various benchmark comparisons in the literature [61\u201363]. The inability of universal prediction effectiveness of the\nSensors 2023, 23, 5436 4 of 25\naforementioned models is to be expected taking into account the undesirable characteristics of the load forecasting problem, which include non-linearities and high levels of noise in the associated data. Furthermore, load time series are not statistically static [64], due to the volatile, rapidly changing nature of the weather conditions that affect their power generation component. Different classes of machine-learning methods can cope better with some of these issues but usually underperform with respect to others, e.g., linear models are more robust to noise but cannot capture the non-linearities present in the load forecasting problem. To make things worse, though all of these problems are inherent to load forecasting, their mixture composition changes depending on the time horizon one tries to predict for, making it impossible to single out a unique machine-learning method that could outperform the others across different prediction horizons, e.g., linear methods are often found to perform better in short-term horizons, where data tend to be noisy, but the non-linearities can usually be adequately approximated by linear models, but mostly fail in longer time horizons, where the role of the non-linearities is dominant. It should be noted that the previous observation about the inability of a single method to beat all the others is not only tied to the context of load forecasting, but reveals a more generic concept in machine-learning and optimization, as expressed by the \u201cno free lunch\u201d theorem [65]. To remedy this predicament, one could resort to using a multi-model approach [66], combining multiple machine-learning methods. Unfortunately, in a real-time deployment scenario, an important practical consideration arises for multi-model schemes: How does one select the most suitable model from a pool of trained models for the next prediction timestep? One solution is to employ a rule-based decision system that uses a priori available knowledge, such as the time of day and measured weather conditions at the substation level. This presents a significant impediment. Not only are the rules of such a system difficult to conceptualize, but they also offer no guarantee of continuously optimal model selection. Doing away with a decision system altogether is also problematic since the individually generated predictions do not offer any actionable insight by themselves. Whether a distribution system operator technician or a RES aggregator, a practitioner requires a single forecast value in order to develop their operations strategy. A practical workaround is to discard such selection rules and instead employ a weighting system that assesses models only by using their past prediction performance [67]. The weighting of the output results of basic forecasting LSTM models in [68] is based on the similarity degree between target and identified standard values of load consumption. Two different approaches for determining the weights of multiple forecasters are followed in [69,70], using a novel incremental ensemble weight updating strategy and the minimum-error method, respectively. Alternatively, an extreme learning machine can be employed for combining the outputs of a pool of forecasts, as in [71]. An intelligent decision-making support scheme, including predictive performance evaluation, model properties analysis, structure and fusion strategy optimization, and optimal model preference selection, is incorporated with an evolutionary ensemble learning method proposed in [72] for shortterm load forecasting (STLF) problems. Finally, an automated system is established in [73] based on hidden Markov chains for extracting similar day profiles to obtain the best model from a library of available forecasting models. Differently from the previous works, the output neural network (NN) models result from multiple training cycles based on snapshots [74] or the hidden features of a Random Vector Functional Link network [75]. It has become clear that the necessity of providing mixed load forecasts, and indeed for multiple short-term horizons, is a factor of paramount importance in the upcoming transition to smart electricity grids. Moreover, according to the preceding literature review, it is evident that in order to enhance the predictive capability of a model, it should incorporate more than one machine-learning methodologies, which of course should be able to handle the complex dynamic behavior of the mixed load. Finally, such a methodology is necessary to be applicable in an online implementation, which means that the final predictions should be provided in a reasonable amount of time and respond to the behavior of the load through a dynamic decision mechanism.\nSensors 2023, 23, 5436 5 of 25\nRealizing the aforementioned requirements and seeking to fill the corresponding research gaps, in this work, we present a novel forecasting scheme that is able to efficiently address the diverse and adverse characteristics of the load forecasting problem for various prediction horizons. The proposed method seeks to create an ensemble of prediction models based on multiple machine-learning techniques comprising different beneficial characteristics that have only been used individually for load forecasting before. Indeed, the sparse coding method introduced in the proposed model has been published very recently and used for the first time in ensemble schemes. As the participating techniques excel in different aspects of the load forecasting problem, their combined usage introduced in this work provides the ensemble with the ability to outperform each individual method in all the horizons tested. In order to efficiently combine the different machine-learning techniques, the proposed method employs an error-based metric on a rolling window of past predictions. This approach enhances the novelty of the proposed method as it does away with the adversity exhibited by complex, rule-based model selection systems. By combining the beneficial characteristics of the aforementioned techniques, the proposed scheme demonstrates superior performance in terms of prediction accuracy, compared to all the submodels, as well as a recently proposed MLP model ensemble from the literature [76], through a wide range of different prediction horizons, spanning from 15 min to 24 h-ahead. Thus, reliable forecasts can be obtained for: (a) One hour ahead or less, which are valuable for various applications at the transmission and distribution network, (b) one day ahead, contributing to the scheduling of generation sources and (c) intra-day forecasting, so as to achieve better optimization results. As a result, the introduced model ensemble can become a powerful tool for administrators and participants in the energy market, easily exploitable in both operational and managerial tasks of smart grids. It should be noted that, at least to the authors\u2019 best knowledge, no machine-learning approach that is able to handle this range of prediction horizons has been proposed in the literature. Furthermore, the proposed approach expands the existing literature by using mixed power-load data, i.e., data that include renewable generation measurements. Although there is an abundance of work in forecasting the net power load, the literature on mixed-load forecasting is very scarce. It should be pointed out that the employment of mixed measurements is aligned with the requirements of modern smart grids, where the penetration of renewable resources is a key feature. The paper is structured as follows: Section 2.1 provides a short description of the different ML methods exploited for building the proposed pool of models. In Section 2.2, the proposed approach is presented analytically and then follows the application of the multi-model scheme upon a certain case study in Section 3, where information about the data and the training process and finally results are given in Section 4. Subsequently, in Section 5, the obtained results are discussed and explained. Finally, conclusions and guidelines for future work are outlined in Section 6."
        },
        {
            "heading": "2. Materials and Methods",
            "text": ""
        },
        {
            "heading": "2.1. Machine-Learning Methods Short Description",
            "text": "As mentioned earlier, multiple machine-learning methods are involved in the proposed approach. In this subsection, a short description of each one of these methods is provided. Here, we provide a short description of each one of them.\n2.1.1. Linear Regression\nLR is considered a standard method for addressing problems such as time series prediction, outlier detection, reliability analysis, and feature selection. The regression analysis method is basically a curve-fitting problem. Given a training dataset, (yn, xn), yn \u2208 R, xn \u2208 Rl , n = 1, 2, . . . , l, where yn \u2208 R represents the output or dependent variable and xn \u2208 Rl , n = 1, 2, . . . , l represents the input vector or regressor [77], the aim is to find a function, f , which fits the data. Subsequently, when an unknown data point x\u2217\nSensors 2023, 23, 5436 6 of 25\nappears, we can use this function in order to calculate/predict the respective output y\u2217 [19]. Equation (1) describes the relation between the input and output variables.\ny = \u03b8Tx + \u03b7 (1)\nThe objective of a regression problem is the estimation of regression coefficients vector \u03b8 which arises through the solution of a least squares problem.\n\u03b8\u0302 = ( xTx )\u22121 xTy (2)\nAlthough more modern and advanced methods have been developed, LR is still used due to its simplicity and robustness, which are of great importance, especially in online implementation of load forecasting. However, the inability of the method to extract the non-linear behavior of the load is an important disadvantage.\n2.1.2. Sparse Coding When addressing a linear system, for example x = D\u03b1, x \u2208 Rm\u00d7n, \u03b1 \u2208 Rk\u00d7n, the number of predictors, p, can be extremely large. Thus, it is impossible to fit a linear model when p < m, or even when p \u2248 m without overfitting (depending on the noise level), but it may still be possible to fit a sparse linear model that only depends on a reduced number of predictors s, where s < p. The dictionary D \u2208 Rm\u00d7k is underdetermined, and therefore, the linear system has infinite possible solutions. The sparse regression (SR) problem is defined as the search for the sparsest solution, i.e., the one with the fewest non-zeros, and is described by Equation (3) [78]:\nmin\u03b1\u2208Rk\u00d7n\u2016a\u20160 subject to x = D\u03b1 (3)\nwhere \u2016a\u20160 is the `0 norm, which counts the non-zero components of a. Although the problem in question is NP-hard, it can often be solved using approximation methods, such as greedy algorithms: Orthogonal matching pursuit [79], thresholding algorithm [80], or relaxation algorithms such as basis pursuit [81] are commonly used for this purpose. SR is a methodology of low complexity that carries the disadvantage of linear correlation assumption between mixed load features, but on the other hand is able to prevent overfitting compared to more complicated ML approaches. A key point stage in the sparse representation procedure is the so-called dictionary learning, which consists of finding the elements of the dictionary (atoms). Dictionary learning can be formulated as a joint unconstrained optimization problem [82], given in the form of (4).\nmin D\u2208C,a\u2208R\u03ba\u00d7n\n\u2211ni=1 \u2016xi \u2212 Dai\u2016 2 2 + \u03bb\u2016\u03b1i\u20160 (4)\nwhere C = { D \u2208 Rm\u00d7ks.t.\u2200j = 1, . . . , k, dTj dj \u2264 1 }\n, denotes the feasible space of dictionary D. The role of the parameter \u03bb is to regulate the sparsity level of the coefficient vector. Dictionary learning can be accomplished using several algorithms, like online dictionary learning [83], method of directions [84], K-SVD [85], stochastic gradient descent [86], or LASSO [83].\n2.1.3. Support Vector Regression\nThe next method to be included in the pool of models constitutes an extension of support vector machines in regression and is called SVR. The basic idea of this methodology is the use of a non-linear transformation \u03d5(\u00b7) : Rn \u2192 Rnh that maps the real data into a multi-dimensional space [87] and the subsequent application of LR. According to this approach, a linear function s is supposed to exist in the multi-dimensional space, which\nSensors 2023, 23, 5436 7 of 25\nmodels the non-linear relation between the input and output data of the initial space [88]. Such an equation is given in (5).\ns(x) = wT \u03d5(x) + b (5)\nwhere \u03d5(x) denotes the kernel function and wT \u2208 Rnh , b \u2208 R are the regression coefficients. The problem of calculating the variables w and b is reduced to the minimization of the structural risk functional.\nR = min w 1 2 \u2016w\u201622 + C\u2211 n i=1|s(xi)\u2212 yi|e (6)\nwhere y contains the real measurements and C is a penalty term used to balance between data fitting and overfitting. The employment of the kernel trick allows SVR to acknowledge the presence of non-linearity in mixed load series. However, when a separating hyperplane in a given dimension cannot be found, then it is required to move in a higher dimension. In this case, the computational cost will increase as well. Furthermore, the use of support vectors makes the method sensitive to noisy data and outliers.\n2.1.4. Neural Networks\nNeural networks (NNs) constitute an important family of black-box modeling techniques. NNs are very accurate, robust, fault-tolerant, and flexible to adapt to any process given a suitable number of quality data. The proposed model ensemble includes two representative NN architectures, namely the MLP and the RBF. MLPs identify the process dynamics and form the model by guiding the input data through weighted successive layers of non-linear functions (threshold, sigmoid, etc.) called activation functions or nodes. The input (activity) \u00b5l(x) to each one of the L nodes is the weighted sum of all N input variables to that node.\n\u00b5l(x) = x\u00b7w (7)\nwhere w are the weights corresponding to each variable of the input vector x. The intermediate layers between the input and the output are called hidden layers. The schematic of a typical MLP NN with 2 hidden layers is presented in Figure 1. The prediction produced by an MLP is the weighted sum of the final layer outputs. Due to the existence of non-linear characteristics in mixed-power load data, MLPs certainly seem like a promising method for the problem at hand.\nSensors\u00a02023,\u00a023,\u00a0x\u00a0FOR\u00a0PEER\u00a0REVIEW\u00a0 7\u00a0 of\u00a0 26\u00a0 \u00a0\n\u00a0\n\ud835\udc60 \ud835\udc99 \ud835\udc98 \ud835\udf11 \ud835\udc99 \ud835\udc4f \u00a0 (5)\u00a0 w ere\u00a0 \ud835\udf11 \ud835\udc99 \u00a0 denotes\u00a0 the\u00a0 kernel\u00a0 function\u00a0 and\u00a0 \ud835\udc98 \u2208 \u211d ,\u00a0 \ud835\udc4f \u2208 \u211d \u00a0 are\u00a0 the\u00a0 regression\u00a0 coefficients.\u00a0 The\u00a0 problem\u00a0 of\u00a0 calculating\u00a0 the\u00a0 variables\u00a0 \ud835\udc98 \u00a0 and\u00a0 \ud835\udc4f \u00a0 is\u00a0 reduced\u00a0 to\u00a0 the\u00a0 minimization\u00a0of\u00a0the\u00a0structural\u00a0risk\u00a0functional.\u00a0\n\ud835\udc45 \ud835\udc5a\ud835\udc56\ud835\udc5b\ud835\udc98 \u2016\ud835\udc98\u2016 \ud835\udc36 \u2211 |\ud835\udc60 \ud835\udc65 \ud835\udc66 | \u00a0 (6)\u00a0 ere\u00a0 \ud835\udc9a \u00a0 contains\u00a0 the\u00a0 real\u00a0measurements\u00a0 and\u00a0 \ud835\udc36 \u00a0 is\u00a0 a\u00a0 penalty\u00a0 term\u00a0 used\u00a0 to\u00a0 balance\u00a0\nbetween\u00a0data\u00a0fitting\u00a0and\u00a0overfitting.\u00a0 The\u00a0employment\u00a0of\u00a0the\u00a0kernel\u00a0trick\u00a0allows\u00a0SVR\u00a0to\u00a0acknowledge\u00a0the\u00a0presence\u00a0of\u00a0nonlinearity\u00a0in\u00a0mixed\u00a0load\u00a0series.\u00a0However,\u00a0when\u00a0a\u00a0separating\u00a0hyperplane\u00a0in\u00a0a\u00a0given\u00a0dimension\u00a0cannot\u00a0be\u00a0found,\u00a0then\u00a0it\u00a0is\u00a0required\u00a0to\u00a0move\u00a0in\u00a0a\u00a0higher\u00a0dimension.\u00a0In\u00a0this\u00a0case,\u00a0the\u00a0 computational\u00a0cost\u00a0will\u00a0increase\u00a0as\u00a0well.\u00a0Furthermore,\u00a0the\u00a0use\u00a0of\u00a0support\u00a0vectors\u00a0makes\u00a0 the\u00a0method\u00a0sensitive\u00a0to\u00a0noisy\u00a0data\u00a0and\u00a0outliers.\u00a0\n2.1.4.\u00a0Neural\u00a0Networks\u00a0 Neural\u00a0networks\u00a0(NNs)\u00a0constitute\u00a0an\u00a0important\u00a0family\u00a0of\u00a0black-box\u00a0modeling\u00a0techniques.\u00a0NNs\u00a0are\u00a0very\u00a0accurate,\u00a0robust,\u00a0fault-tolerant,\u00a0and\u00a0flexible\u00a0to\u00a0adapt\u00a0to\u00a0any\u00a0process\u00a0 given\u00a0a\u00a0suitable\u00a0number\u00a0of\u00a0quality\u00a0data.\u00a0The\u00a0proposed\u00a0model\u00a0ensemble\u00a0includes\u00a0two\u00a0representative\u00a0NN\u00a0architectures,\u00a0namely\u00a0the\u00a0MLP\u00a0and\u00a0the\u00a0RBF.\u00a0MLPs\u00a0 identify\u00a0the\u00a0process\u00a0 dynamics\u00a0and\u00a0 form\u00a0 the\u00a0model\u00a0by\u00a0guiding\u00a0 the\u00a0 input\u00a0data\u00a0 through\u00a0weighted\u00a0successive\u00a0 layers\u00a0 of\u00a0 non-linear\u00a0 functions\u00a0 (threshold,\u00a0 sigmoid,\u00a0 etc.)\u00a0 called\u00a0 activation\u00a0 functions\u00a0 or\u00a0 nodes.\u00a0The\u00a0input\u00a0(activity)\u00a0 \ud835\udf07 \ud835\udc99 \u00a0 to\u00a0each\u00a0one\u00a0of\u00a0the\u00a0 \ud835\udc3f\u00a0 nodes\u00a0is\u00a0the\u00a0weighted\u00a0sum\u00a0of\u00a0all\u00a0\ud835\udc41\u00a0 input\u00a0variables\u00a0to\u00a0that\u00a0node.\u00a0\n\ud835\udf07 \ud835\udc99 \ud835\udc99 \u2219 \ud835\udc98 \u00a0 (7)\u00a0 ere\u00a0 \ud835\udc98 \u00a0 are\u00a0 the\u00a0weights\u00a0 co responding\u00a0 to\u00a0 each\u00a0 variable\u00a0 of\u00a0 the\u00a0 input\u00a0 vector\u00a0 \ud835\udc99 .\u00a0 The\u00a0 intermediate\u00a0 layers\u00a0 between\u00a0 the\u00a0 input\u00a0 and\u00a0 the\u00a0 output\u00a0 are\u00a0 called\u00a0 hidden\u00a0 layers.\u00a0 The\u00a0 schematic\u00a0 of\u00a0 a\u00a0 typical\u00a0MLP\u00a0NN\u00a0with\u00a0 2\u00a0 hidden\u00a0 layers\u00a0 is\u00a0 presented\u00a0 in\u00a0 Figure\u00a0 1.\u00a0 The\u00a0 prediction\u00a0produced\u00a0by\u00a0an\u00a0MLP\u00a0is\u00a0t \u00a0weighted\u00a0sum\u00a0of\u00a0the\u00a0final\u00a0layer\u00a0outputs.\u00a0Due\u00a0to\u00a0the\u00a0 existence\u00a0of\u00a0non-linear\u00a0characteristics\u00a0in\u00a0mixed-power\u00a0load\u00a0data,\u00a0MLPs\u00a0certainly\u00a0see \u00a0like\u00a0 a\u00a0promising\u00a0method\u00a0for\u00a0the\u00a0problem\u00a0at\u00a0hand.\u00a0\n\u00a0\nFigure\u00a01.\u00a0A\u00a0typical\u00a0fully\u00a0connected\u00a0multi-layer\u00a0perceptron\u00a0(MLP)\u00a0neural\u00a0network\u00a0(NN)\u00a0structure\u00a0\ncomprising\u00a0of\u00a0\ud835\udc41\u00a0 inputs,\u00a0 \ud835\udc99 , \u2026 ,\ud835\udc99 ,\u00a02\u00a0hidden\u00a0layers\u00a0of\u00a0 \ud835\udc3f\u00a0 neurons\u00a0each,\u00a0and\u00a0one-dimensional\u00a0out-\nput,\u00a0 \ud835\udc9a.\u00a0\nOn\u00a0the\u00a0downside,\u00a0MLP\u00a0training\u00a0is\u00a0usually\u00a0performed\u00a0by\u00a0some\u00a0form\u00a0of\u00a0backpropagation\u00a0technique\u00a0which\u00a0usually\u00a0requires\u00a0more\u00a0than\u00a0one\u00a0intertwined\u00a0iterative\u00a0procedure\u00a0to\u00a0\nFigure 1. A typical fully connected multi-layer perceptron (MLP) neural network (NN) structure comprising of N inputs, x1, . . . , xn, 2 hidden layers of L neurons each, and one-dimensional output, y\u0302.\nOn the downside, MLP training is usually performed by some form of backpropagation technique which usually requires more than one intertwined iterative procedure to\nSensors 2023, 23, 5436 8 of 25\nfully optimize the involved parameters, i.e., the number of layers and nodes, the weights, etc. Depending on the size and architecture of the MLP network, and the input space, this procedure may become computationally intensive, and thus it is commonly performed offline. A quite critical drawback of all backpropagation-based techniques is that they get easily trapped in local minima. In this case, the provided solution may not be satisfactory, a fact which leads to a tedious retraining procedure. RBFs are similar to the MLPs in the sense that data are fed through the input layer and follow a straight path to the output layer but they differ in that there exists only one hidden layer which comprises radially symmetric activation functions (Gaussian, quadratic, thin plate spline, etc.). A typical RBF NN using Gaussian activation functions can be seen in Figure 2. The input layer distributes the data of the N inputs to the L nodes of the hidden layer, which are positioned to a specific point of the input space through a process of training. The activity \u00b5l(uk) of each node is calculated using the Euclidian distance between the input data uk and the center cl of each node.\n\u00b5l(uk) = \u2016uk \u2212 cl\u20162 = \u221a \u2211Ni=1(ui,k \u2212 ci,l) 2, l = 1, 2 . . . , L, k = 1, 2, . . . , K (8)\nwhere K is the number of training data. The chosen RBF gk receives the activity value and calculates the node output. The linear combination of all hidden layer node outputs provides the NNs prediction y\u0302k\ny\u0302k = gk\u00b7w (9)\nSensors\u00a02023,\u00a023,\u00a0x\u00a0FOR\u00a0PEER\u00a0REVIEW\u00a0 8\u00a0 of\u00a0 26\u00a0 \u00a0\n\u00a0\nfully\u00a0optimize\u00a0the\u00a0involved\u00a0parameters,\u00a0i.e.,\u00a0the\u00a0number\u00a0of\u00a0layers\u00a0and\u00a0nodes,\u00a0the\u00a0weights,\u00a0 etc.\u00a0Depending\u00a0on\u00a0the\u00a0size\u00a0and\u00a0architecture\u00a0of\u00a0the\u00a0MLP\u00a0network,\u00a0and\u00a0the\u00a0input\u00a0space,\u00a0this\u00a0 procedure\u00a0may\u00a0become\u00a0computationally\u00a0 intensive,\u00a0and\u00a0 thus\u00a0 it\u00a0 is\u00a0commonly\u00a0performed\u00a0 offline.\u00a0A\u00a0quite\u00a0critical\u00a0drawback\u00a0of\u00a0all\u00a0backpropagation-based\u00a0techniques\u00a0is\u00a0that\u00a0they\u00a0get\u00a0 easily\u00a0trapped\u00a0in\u00a0local\u00a0minima.\u00a0In\u00a0this\u00a0case,\u00a0the\u00a0provided\u00a0solution\u00a0may\u00a0not\u00a0be\u00a0satisfactory,\u00a0 a\u00a0fact\u00a0which\u00a0leads\u00a0to\u00a0a\u00a0tedious\u00a0retraining\u00a0procedure.\u00a0\nRBFs\u00a0are\u00a0similar\u00a0to\u00a0the\u00a0MLPs\u00a0in\u00a0the\u00a0sense\u00a0that\u00a0data\u00a0are\u00a0fed\u00a0through\u00a0the\u00a0input\u00a0layer\u00a0 and\u00a0follow\u00a0a\u00a0straight\u00a0path\u00a0to\u00a0the\u00a0output\u00a0layer\u00a0but\u00a0they\u00a0differ\u00a0in\u00a0that\u00a0there\u00a0exists\u00a0only\u00a0one\u00a0 hidden\u00a0layer\u00a0which\u00a0comprises\u00a0radially\u00a0symmetric\u00a0activation\u00a0functions\u00a0(Gaussian,\u00a0quadratic,\u00a0thin\u00a0plate\u00a0spline,\u00a0etc.).\u00a0A\u00a0typical\u00a0RBF\u00a0NN\u00a0using\u00a0Gaussian\u00a0activation\u00a0functions\u00a0can\u00a0be\u00a0 seen\u00a0in\u00a0Figure\u00a02.\u00a0The\u00a0input\u00a0layer\u00a0distributes\u00a0the\u00a0data\u00a0of\u00a0the\u00a0\ud835\udc41\u00a0 inputs\u00a0to\u00a0the\u00a0 \ud835\udc3f\u00a0 nodes\u00a0of\u00a0 the\u00a0hidden\u00a0 layer,\u00a0which\u00a0are\u00a0positioned\u00a0 to\u00a0a\u00a0specific\u00a0point\u00a0of\u00a0 the\u00a0 input\u00a0space\u00a0 through\u00a0a\u00a0 process\u00a0of\u00a0training.\u00a0The\u00a0activity\u00a0 \ud835\udf07 \ud835\udc96 \u00a0 of\u00a0each\u00a0node\u00a0is\u00a0calculated\u00a0using\u00a0the\u00a0 cli ian\u00a0distance\u00a0b twe n\u00a0the\u00a0input\u00a0data\u00a0 \ud835\udc96 \u00a0 and\u00a0the\u00a0center\u00a0 \ud835\udc84 \u00a0 of\u00a0each\u00a0node.\u00a0\n\ud835\udf07 \ud835\udc96 \u2016\ud835\udc96 \ud835\udc84 \u2016 \u2211 \ud835\udc62 , \ud835\udc50 , , \ud835\udc59 1,2 \u2026 , \ud835\udc3f, \ud835\udc58 1,2, \u2026 ,\ud835\udc3e \u00a0 (8)\u00a0 where\u00a0\ud835\udc3e\u00a0 is\u00a0the\u00a0number\u00a0of\u00a0training\u00a0data.\u00a0The\u00a0chosen\u00a0RBF\u00a0\ud835\udc88 \u00a0 receives\u00a0the\u00a0activity\u00a0value\u00a0 and\u00a0calculates\u00a0the\u00a0node\u00a0output.\u00a0The\u00a0linear\u00a0combination\u00a0of\u00a0all\u00a0hidden\u00a0layer\u00a0node\u00a0outputs\u00a0 provides\u00a0the\u00a0NNs\u00a0prediction\u00a0 \ud835\udc66 \u00a0\n\ud835\udc66 \ud835\udc88 \u2219 \ud835\udc98\u00a0 (9)\u00a0\n\u00a0\nFigure\u00a02.\u00a0A\u00a0typical\u00a0Gaussian-based\u00a0radial\u00a0basis\u00a0function\u00a0(RBF)\u00a0neural\u00a0network\u00a0(NN)\u00a0structure\u00a0com-\nprising\u00a0of\u00a0\ud835\udc41\u00a0 inputs,\u00a0 \ud835\udc99 , \u2026 ,\ud835\udc99 ,\u00a0 \ud835\udc3f\u00a0 neurons\u00a0in\u00a0the\u00a0hidden\u00a0layer,\u00a0and\u00a0one-dimensional\u00a0output,\u00a0 \ud835\udc9a.\u00a0\nThe\u00a0training\u00a0algorithm\u00a0for\u00a0an\u00a0RBF\u00a0NN\u00a0is\u00a0usually\u00a0broken\u00a0down\u00a0into\u00a0two\u00a0phases,\u00a0the\u00a0 first\u00a0of\u00a0which\u00a0discovers\u00a0the\u00a0optimal\u00a0number\u00a0and\u00a0location\u00a0of\u00a0the\u00a0hidden\u00a0node\u00a0centers\u00a0in\u00a0 the\u00a0input\u00a0space,\u00a0while\u00a0the\u00a0second\u00a0one\u00a0calculates\u00a0the\u00a0weights\u00a0\ud835\udc98\u00a0 usually\u00a0through\u00a0simple\u00a0 LR.\u00a0Due\u00a0to\u00a0the\u00a0fact\u00a0that\u00a0the\u00a0training\u00a0process\u00a0is\u00a0broken\u00a0into\u00a0two\u00a0phases,\u00a0RBF\u00a0NNs\u00a0are\u00a0able\u00a0 to\u00a0use\u00a0very\u00a0fast\u00a0algorithms.\u00a0In\u00a0fact,\u00a0some\u00a0of\u00a0the\u00a0current\u00a0RBF\u00a0training\u00a0techniques\u00a0[89]\u00a0are\u00a0 deterministic\u00a0and\u00a0non-iterative,\u00a0requiring\u00a0only\u00a0a\u00a0single\u00a0pass\u00a0of\u00a0the\u00a0data\u00a0to\u00a0converge,\u00a0 in\u00a0 contrast\u00a0to\u00a0other\u00a0NN\u00a0architectures\u00a0which\u00a0(a)\u00a0are\u00a0epoch-based\u00a0requiring\u00a0multiple\u00a0passes\u00a0of\u00a0 the\u00a0data\u00a0and\u00a0 (b)\u00a0are\u00a0stochastic\u00a0requiring\u00a0multiple\u00a0 runs\u00a0 to\u00a0overcome\u00a0 their\u00a0sensitivity\u00a0 to\u00a0 initial\u00a0 conditions.\u00a0 RBF\u00a0 networks\u00a0 provide\u00a0 very\u00a0 strong\u00a0 interpolation\u00a0 tools,\u00a0 usually\u00a0 outperforming\u00a0other\u00a0NN-based\u00a0techniques\u00a0provided\u00a0that\u00a0dense\u00a0and\u00a0good\u00a0quality\u00a0training\u00a0 data\u00a0are\u00a0available.\u00a0In\u00a0the\u00a0absence\u00a0of\u00a0adequate\u00a0training\u00a0data\u00a0though,\u00a0their\u00a0performance\u00a0may\u00a0 become\u00a0rather\u00a0poor.\u00a0Therefore,\u00a0their\u00a0application,\u00a0in\u00a0combination\u00a0with\u00a0the\u00a0other\u00a0models\u00a0of\u00a0 the\u00a0pool,\u00a0can\u00a0make\u00a0a\u00a0significant\u00a0contribution\u00a0to\u00a0mixed-power\u00a0load\u00a0prediction.\u00a0\nFigure 2. A typical Gaussian-based radial basis function (RBF) neural network (NN) structure comprising of N inputs, x1, . . . , xn, L neurons in the hidden layer, and one-dimensional output, y\u0302.\nThe training algorithm for an RBF NN is usually broken down into two phases, the first of which discovers the optimal number and location of the hidden node centers in the input space, while the second one calculates the weights w usually through simple LR. Due to the fact that the training process is broken into two phases, RBF NNs are able to use very fast algorithms. In fact, some of the current RBF training techniques [89] are deterministic and non-iterative, requiring only a single pass of the data to converge, in contrast to other NN architectures which (a) are epoch-based requiring multiple passes of the data and (b) are stochastic requiring multiple runs to overcome their sensitivity to initial conditions. RBF networks provide very strong interpolation tools, usually outperforming other NN-based techniques provided that dense and good quality training data are available. In the absence of adequate training data though, their performance may become rather poor. Therefore, their application, in combination with the other models of the pool, can make a significant contribution to mixed-power load prediction.\nSensors 2023, 23, 5436 9 of 25\n2.1.5. Random Forests\nThe last method involved in the proposed approach is RF [90]. As the name suggests, an RF is a tree-based ensemble, with each tree depending on a collection of random variables. More formally, for a p-dimensional random vector X = ( X1, . . . , Xp )T representing the real-valued input or predictor variables and a random variable Y representing the real-valued response, we assume an unknown joint distribution PXY(X, Y). The goal is to find a prediction function f (X) for predicting Y. The prediction function is determined by a loss function L(Y, f (X)) and defined to minimize the expected value of the loss\nEXY(L(Y, f (X))) (10)\nwhere the subscripts denote expectation with respect to the joint distribution of X and Y. Intuitively, L(Y, f (X)) is a measure of how close f (X) is to Y. It penalizes values of f (X) that are a long way from Y. As for simple LR, squared error loss could be a typical choice of L. Ensembles construct f in terms of a collection of linear estimators of x, the so-called \u201cbase learners\u201d h1(x), h2(x), . . . , hJ(x), where J denotes the number of trees and is user-specified, according to the following iterative procedure. Let D = {(x1, x1), . . . , (xN , xN)} denote the training data, with xi = ( xi,1, . . . , xi,p\n)T , i = 1, . . . N. For each j \u2208 (1, J), a bootstrap sample Dj of size N is extracted from D and a corresponding tree hj ( X, \u0398j ) is derived implementing the binary recursive partitioning [91]. The prediction extraction using a standard RF regressor is depicted in Figure 3. For each unsplit node of the tree, the best binary split among all binary splits on the m \u2208 (1, p) predictors, is found. The component \u0398j is used to inject randomness first by bootstrap sampling and second by the random subset of m predictors. Once the base learners are found, a prediction at a new point x is given by\nf (x) = 1 J \u2211 J j=1 hj(x) (11)\nSensors\u00a02023,\u00a023,\u00a0x\u00a0FOR\u00a0PEER\u00a0REVIEW\u00a0 9\u00a0 of\u00a0 26\u00a0 \u00a0\n\u00a0\n2.1.5.\u00a0Random\u00a0Forests\u00a0 The\u00a0last\u00a0method\u00a0involved\u00a0in\u00a0the\u00a0proposed\u00a0approach\u00a0is\u00a0RF\u00a0[90].\u00a0As\u00a0the\u00a0name\u00a0suggests,\u00a0 an\u00a0RF\u00a0is\u00a0a\u00a0tree-based\u00a0ensemble,\u00a0with\u00a0each\u00a0tree\u00a0depending\u00a0on\u00a0a\u00a0collection\u00a0of\u00a0random\u00a0variables.\u00a0More\u00a0formally,\u00a0for\u00a0a\u00a0 \ud835\udc5d-dimensional\u00a0random\u00a0vector\u00a0 \ud835\udc4b \ud835\udc4b , \u2026 ,\ud835\udc4b \u00a0 representing\u00a0 the\u00a0 real-valued\u00a0 input\u00a0or\u00a0predictor\u00a0variables\u00a0and\u00a0a\u00a0 random\u00a0variable\u00a0 \ud835\udc4c\u00a0 representing\u00a0 the\u00a0 real-valued\u00a0response,\u00a0we\u00a0assume\u00a0an\u00a0unknown\u00a0joint\u00a0distribution\u00a0 \ud835\udc43 \ud835\udc4b,\ud835\udc4c .\u00a0The\u00a0goal\u00a0is\u00a0to\u00a0 find\u00a0a\u00a0prediction\u00a0function\u00a0 \ud835\udc53 \ud835\udc4b \u00a0 for\u00a0predicting\u00a0 \ud835\udc4c.\u00a0The\u00a0prediction\u00a0function\u00a0is\u00a0determined\u00a0 by\u00a0a\u00a0loss\u00a0function\u00a0 \ud835\udc3f \ud835\udc4c, \ud835\udc53 \ud835\udc4b \u00a0 and\u00a0defined\u00a0to\u00a0minimize\u00a0the\u00a0expected\u00a0value\u00a0of\u00a0the\u00a0loss\u00a0\n\ud835\udc38 \ud835\udc3f \ud835\udc4c, \ud835\udc53 \ud835\udc4b \u00a0 (10)\u00a0 where\u00a0the\u00a0subscripts\u00a0denote\u00a0expectation\u00a0with\u00a0respect\u00a0to\u00a0the\u00a0joint\u00a0distribution\u00a0of\u00a0\ud835\udc4b\u00a0 and\u00a0 \ud835\udc4c.\u00a0 Intuitively,\u00a0 \ud835\udc3f \ud835\udc4c,\ud835\udc53 \ud835\udc4b \u00a0 is\u00a0a\u00a0measure\u00a0of\u00a0how\u00a0close\u00a0 \ud835\udc53 \ud835\udc4b \u00a0 is\u00a0to\u00a0 \ud835\udc4c.\u00a0It\u00a0penalizes\u00a0values\u00a0of\u00a0 \ud835\udc53 \ud835\udc4b \u00a0 that\u00a0are\u00a0a\u00a0long\u00a0way\u00a0from\u00a0 \ud835\udc4c.\u00a0As\u00a0for\u00a0simple\u00a0LR,\u00a0squared\u00a0error\u00a0loss\u00a0could\u00a0be\u00a0a\u00a0typical\u00a0choice\u00a0 of\u00a0 \ud835\udc3f.\u00a0Ensembles\u00a0construct\u00a0 \ud835\udc53\u00a0 in\u00a0terms\u00a0of\u00a0a\u00a0collection\u00a0of\u00a0linear\u00a0estimators\u00a0of\u00a0 \ud835\udc65,\u00a0the\u00a0so-called\u00a0 \u201cbase\u00a0learners\u201d\u00a0 \u210e \ud835\udc65 ,\u210e \ud835\udc65 , \u2026 ,\u210e \ud835\udc65 ,\u00a0where\u00a0 \ud835\udc3d\u00a0 denotes\u00a0the\u00a0number\u00a0of\u00a0trees\u00a0and\u00a0 is\u00a0userspecified,\u00a0 according\u00a0 to\u00a0 the\u00a0 following\u00a0 iterative\u00a0procedure.\u00a0Let\u00a0 \ud835\udc9f \ud835\udc65 , \ud835\udc65 , . . , \ud835\udc65 ,\ud835\udc65 \u00a0 denote\u00a0 the\u00a0 training\u00a0 data,\u00a0 with\u00a0 \ud835\udc65 \ud835\udc65 , , \u2026 , \ud835\udc65 , , \ud835\udc56 1, \u2026\ud835\udc41 .\u00a0 For\u00a0 each\u00a0 \ud835\udc57 \u2208 1, \ud835\udc3d ,\u00a0 a\u00a0 bootstrap\u00a0sample\u00a0\ud835\udc9f \u00a0 of\u00a0size\u00a0\ud835\udc41\u00a0 is\u00a0extracted\u00a0from\u00a0\ud835\udc9f\u00a0 and\u00a0a\u00a0corresponding\u00a0tree\u00a0 \u210e \ud835\udc4b,\ud835\udee9 \u00a0 is\u00a0 derived\u00a0 implementing\u00a0 the\u00a0binary\u00a0 recursive\u00a0partitioning\u00a0 [91].\u00a0The\u00a0prediction\u00a0extraction\u00a0 using\u00a0a\u00a0standard\u00a0RF\u00a0regressor\u00a0is\u00a0depicted\u00a0in\u00a0Figure\u00a03.\u00a0For\u00a0each\u00a0unsplit\u00a0node\u00a0of\u00a0the\u00a0tree,\u00a0 the\u00a0best\u00a0binary\u00a0split\u00a0among\u00a0all\u00a0binary\u00a0splits\u00a0on\u00a0the\u00a0\ud835\udc5a \u2208 1,\ud835\udc5d \u00a0 predictors,\u00a0 is\u00a0found.\u00a0The\u00a0 component\u00a0\ud835\udee9 \u00a0 is\u00a0used\u00a0to\u00a0inject\u00a0randomness\u00a0first\u00a0by\u00a0bootstrap\u00a0sampling\u00a0and\u00a0second\u00a0by\u00a0the\u00a0 random\u00a0subset\u00a0of\u00a0\ud835\udc5a\u00a0 predictors.\u00a0Once\u00a0the\u00a0base\u00a0learners\u00a0are\u00a0found,\u00a0a\u00a0prediction\u00a0at\u00a0a\u00a0new\u00a0 point\u00a0 \ud835\udc65\u00a0 is\u00a0given\u00a0by\u00a0\n\ud835\udc53 \ud835\udc65 \u2211 \u210e \ud835\udc65 \u00a0 (11)\u00a0\n\u00a0\nFigure\u00a03.\u00a0A\u00a0typical\u00a0random\u00a0forest\u00a0architecture\u00a0comprising\u00a0of\u00a0\ud835\udc41\u00a0 tree\u00a0learners,\u00a0\ud835\udc3b ,\ud835\udc3b , \u2026 ,\ud835\udc3b .\u00a0The\u00a0pre-\ndiction\u00a0set\u00a0for\u00a0each\u00a0learner\u00a0is\u00a0averaged\u00a0to\u00a0produce\u00a0the\u00a0final\u00a0predictions.\u00a0\nRF\u00a0 is\u00a0a\u00a0simple\u00a0and\u00a0reliable\u00a0 forecasting\u00a0 tool.\u00a0 Its\u00a0main\u00a0 limitation\u00a0 is\u00a0 the\u00a0 trade-off\u00a0between\u00a0performance\u00a0and\u00a0the\u00a0number\u00a0of\u00a0trees.\u00a0Increasing\u00a0this\u00a0parameter\u00a0can\u00a0lead\u00a0to\u00a0more\u00a0 accurate\u00a0predictions\u00a0and\u00a0prevent\u00a0overfitting\u00a0but\u00a0can\u00a0also\u00a0make\u00a0the\u00a0algorithm\u00a0too\u00a0slow\u00a0and\u00a0\nFigure 3. A typical random forest ar hit cture comprising of N t e learners H1, H2, . . . , HN . The prediction set for each learner is averaged to produce the final predictions.\nRF is a si ple and reliable forecasting tool. Its ain limitation is the trade-off between performance a d the number of trees. Increasing this parameter can lead to more accurate predictions and prevent overfitting but can also make the algorithm too slow and ineffective for real-time predictions. It is, therefore, understandable that this methodology may prove suitable in specific areas of the dataset.\nSensors 2023, 23, 5436 10 of 25"
        },
        {
            "heading": "2.2. Machine-Learning Model Ensemble",
            "text": "Recognizing the individual advantages and disadvantages of the machine-learning methods described in Section 2.1, the proposed scheme seeks to create an ensemble that will successfully combine their merits in a single approach. For example, neural-networkbased models such as RBF do exhibit superior prediction performance only as long as the input data point lies well within the domain of the input training dataset, otherwise it fails. On the other hand, linear and sparse prediction models, in general, show much better extrapolative performance, even though they are unable to capture more complex, non-linear dynamics. In other words, by toggling between the robust linear models and the more sensitive but also more effective non-linear ones, a superior approach to load time series prediction can be constructed. In order to obtain the best possible performance of each sub-model, their optimal training configuration has to be determined. Starting with the simpler methods used, a linear and an SR model are trained by least squares and fast iterative shrinkage thresholding algorithm, respectively, the latter being a faster implementation of the corresponding iterative shrinkage thresholding algorithm used for load forecasting [36]. In the case of the sparse coding approach, sparsity is induced by the `2 norm and the regularization parameter was set by trial and error to 0.01. Subsequently, a random forest regressor is employed, where the number of decision trees is selected to be 15 so as to keep the training time at a reasonable level without reducing its predictive ability. As regards the nonlinear methods, an SVR model with Gaussian kernel function was developed [92], using sequential minimal optimization for training and Bayesian optimization to optimize the model\u2019s hyperparameters [93]. Two NN models are also introduced, featuring two different architectures. The first one is a two-layered MLP network trained by the Levenberg\u2013 Marquardt backpropagation algorithm [94], following a 10-fold cross-validation. The neurons of each layer are chosen by trial and error as 20 and 10. It is noted that, in order to compensate for the performance dependence of the MLP training methods to initialization, the training procedure was conducted 10 different times, with different randomly initialized weights of the network. The second NN uses an RBF architecture and is trained using the fuzzy means technique [95], an algorithm that has found many successful applications due to the increased accuracy it provides [96\u201398] combined with fast training times [99]. In this work, the FM algorithm has been tested for a range of fuzzy sets between 4 and 15. When deployed online, the proposed approach evaluates a MAE metric on a rolling window of past predictions coming from a pool of trained models in order to create a weight vector for the next timestep prediction. An important item of the proposed method to be specified is the length of the rolling window. It can be easily inferred that this depends not only on the prediction horizon but also on the statistical properties of the predicted variable (a more volatile, non-stationary time series would require shorter rolling window horizons). Once the model pool has been populated by trained models, the optimum length of the rolling window is calculated in an exhaustive search manner over the same validation data in the range of 3\u201315 regressive timesteps. The proposed method operates as follows: For each timestep k, all trained models in the pool are evaluated concurrently. Their current prediction performance is assessed by applying the MAE metric on their previous predictions up to a rolling time window of length hw\nMAEi(k) = \u2211hw\u22121j=0 |y\u0302i(k\u2212 j)\u2212 y(k\u2212 j)|\nhw (12)\nwhere y\u0302i(k) are the predictions of the i-th model and y are the actual values of the times eries at timestep k. Then, the MAE metric is used to calculate the prediction weight of each model for the next timestep k + 1.\nwi(k + 1) = MAE\u22121i (k)\n\u2211Ni=1 MAE \u22121 i (k)\n(13)\nSensors 2023, 23, 5436 11 of 25\nwhere MAEi is the MAE of the i-th prediction model, N is the total number of models in the model pool, and wi is the prediction weight for the next timestep. The prediction output of the proposed method is calculated as the weighted sum of the model predictions y\u0302i\ny\u0302(k + 1) = \u2211Ni=1 wi(k + 1)y\u0302i(k + 1) (14)\nA snapshot of a two-model example version of the proposed method is shown in Figure 4. Note that the proposed method combines the strengths of the individual models by placing greater weight on the current better-performing model for the time window of length hw. At first, both y\u03021 and y\u03022 models appear ineffective as individual predictors of the y time series. However, after closer inspection, y\u03022 performs better for the first half of y, while y\u03021 for the second half. By placing greater weight on the model with the best past prediction performance within the horizon hw, the proposed method is able to toggle towards the best available model for the current circumstance. The result is an overall superior prediction performance.\nSensors\u00a02023,\u00a023,\u00a0x\u00a0FOR\u00a0PEER\u00a0REVIEW\u00a0 11\u00a0 of\u00a0 26\u00a0 \u00a0\n\u00a0\n\ud835\udc64 \ud835\udc58 1 \u2211 \u00a0 (13)\u00a0 r \u00a0\ud835\udc40\ud835\udc34\ud835\udc38 \u00a0 is\u00a0the\u00a0 AE\u00a0of\u00a0the\u00a0i-th\u00a0prediction\u00a0model,\u00a0N\u00a0is\u00a0the\u00a0total\u00a0number\u00a0of\u00a0models\u00a0in\u00a0 the\u00a0model\u00a0pool,\u00a0and\u00a0\ud835\udc64 \u00a0 is\u00a0 the\u00a0prediction\u00a0weight\u00a0 for\u00a0 the\u00a0next\u00a0 timestep.\u00a0The\u00a0prediction\u00a0 output\u00a0of\u00a0th \u00a0propose \u00a0method\u00a0is\u00a0calculated\u00a0as\u00a0the\u00a0weighted\u00a0sum\u00a0of\u00a0the\u00a0model\u00a0predictions\u00a0 \ud835\udc66 \u00a0\n\ud835\udc66 \ud835\udc58 1 \u2211 \ud835\udc64 \ud835\udc58 1 \ud835\udc66 \ud835\udc58 1 \u00a0 (14)\u00a0 \u00a0snapshot\u00a0of\u00a0a\u00a0 two-model\u00a0 xample\u00a0v rsion\u00a0 f\u00a0 the\u00a0pro osed\u00a0method\u00a0 is\u00a0shown\u00a0 in\u00a0\ni re\u00a04.\u00a0Note\u00a0that\u00a0the\u00a0proposed\u00a0method\u00a0combines\u00a0the\u00a0strengths\u00a0of\u00a0the\u00a0individual\u00a0models\u00a0 \u00a0placing\u00a0greater\u00a0weight\u00a0on\u00a0the\u00a0current\u00a0better-performing\u00a0 odel\u00a0f r\u00a0the\u00a0time\u00a0window\u00a0of\u00a0 length\u00a0hw.\u00a0At\u00a0firs ,\u00a0both\u00a0 \ud835\udc66 \u00a0 and\u00a0 \ud835\udc66 \u00a0models\u00a0appear\u00a0in ffective\u00a0as\u00a0 ndividual\u00a0predictors\u00a0of\u00a0 the\u00a0 \ud835\udc66\u00a0 time\u00a0s ri s.\u00a0However,\u00a0after\u00a0closer\u00a0inspection,\u00a0 \ud835\udc66 \u00a0 performs\u00a0better\u00a0for\u00a0the\u00a0first\u00a0 alf\u00a0of\u00a0 \ud835\udc66,\u00a0while\u00a0 \ud835\udc66 \u00a0 for\u00a0the\u00a0second\u00a0half.\u00a0By\u00a0placing\u00a0greater\u00a0weight\u00a0on\u00a0the\u00a0model\u00a0with\u00a0the\u00a0best\u00a0past\u00a0 prediction\u00a0performance\u00a0within\u00a0the\u00a0horizon\u00a0hw,\u00a0the\u00a0proposed\u00a0method\u00a0is\u00a0able\u00a0to\u00a0toggle\u00a0towards\u00a0the\u00a0best\u00a0available\u00a0model\u00a0for\u00a0the\u00a0current\u00a0circumstance.\u00a0The\u00a0result\u00a0is\u00a0an\u00a0overall\u00a0superior\u00a0prediction\u00a0performance.\u00a0\nFigure\u00a04.\u00a0Schematic\u00a0for\u00a0a\u00a0two-model\u00a0version\u00a0of\u00a0the\u00a0proposed\u00a0method,\u00a0where\u00a0y\u00a0denotes\u00a0the\u00a0real\u00a0load,\u00a0\n\ud835\udc9a \u00a0 the\u00a0prediction\u00a0of\u00a0the\u00a0i-th\u00a0model,\u00a0 \ud835\udc9a\u00a0 the\u00a0weighted\u00a0prediction\u00a0and\u00a0k\u00a0the\u00a0current\u00a0timestep.\u00a0The\u00a0en-\nsemble\u00a0model\u00a0 recognizing\u00a0 the\u00a0 superiority\u00a0of\u00a0 \ud835\udc9a \u00a0 over\u00a0 \ud835\udc9a \u00a0,\u00a0within\u00a0 the\u00a0 rolling\u00a0window\u00a0 adapts\u00a0 its\u00a0\nweights\u00a0accordingly,\u00a0achieving\u00a0highly\u00a0accurate\u00a0prediction\u00a0for\u00a0the\u00a0next\u00a0timestep\u00a0k\u00a0+\u00a01.\u00a0\n3.\u00a0Case\u00a0Study\u00a0 3.1.\u00a0Problem\u00a0and\u00a0Data\u00a0Description\u00a0\nThe\u00a0main\u00a0goal\u00a0of\u00a0this\u00a0paper\u00a0is\u00a0to\u00a0develop\u00a0a\u00a0methodology\u00a0in\u00a0order\u00a0to\u00a0implement\u00a0a\u00a0load\u00a0 forecasting\u00a0 tool\u00a0able\u00a0 to\u00a0provide\u00a0accurate\u00a0mixed\u00a0 load\u00a0predictions\u00a0over\u00a0 several\u00a0different\u00a0 time\u00a0horizons\u00a0and\u00a0in\u00a0particular\u00a015\u00a0min,\u00a01-h,\u00a02-h,\u00a03-h,\u00a06-h,\u00a0and\u00a024-h.\u00a0This\u00a0case\u00a0study\u00a0makes\u00a0 use\u00a0of\u00a0real\u00a0data\u00a0from\u00a0a\u00a0high\u00a0voltage/medium\u00a0voltage\u00a0substation\u00a0located\u00a0in\u00a0mainland\u00a0Europe,\u00a0measured\u00a0during\u00a0the\u00a0years\u00a02017\u20132018.\u00a0The\u00a0MV\u00a0distribution\u00a0network\u00a0contains\u00a0multiple\u00a0photovoltaic\u00a0sites.\u00a0As\u00a0a\u00a0result,\u00a0the\u00a0data\u00a0measurements\u00a0in\u00a0question\u00a0constitute\u00a0mixed\u00a0 power-load\u00a0recordings,\u00a0which\u00a0correspond\u00a0 to\u00a0 the\u00a0mixed\u00a0AP\u00a0demand\u00a0of\u00a0 the\u00a0distribution\u00a0 grid\u00a0from\u00a0the\u00a0transmission\u00a0grid.\u00a0The\u00a0load\u00a0measurements\u00a0have\u00a0been\u00a0recorded\u00a0every\u00a0minute\u00a0 and\u00a0contain\u00a0the\u00a0mixed\u00a0AP\u00a0demand,\u00a0as\u00a0well\u00a0as\u00a0cloud\u00a0coverage,\u00a0wind\u00a0speed,\u00a0humidity,\u00a0and\u00a0 temperature,\u00a0as\u00a0measured\u00a0 from\u00a0 the\u00a0 substation\u2019s\u00a0weather\u00a0 station.\u00a0Due\u00a0 to\u00a0practical\u00a0concerns,\u00a0individual\u00a0power\u00a0generation\u00a0or\u00a0weather\u00a0data\u00a0from\u00a0the\u00a0aforementioned\u00a0photovoltaic\u00a0 sites\u00a0should\u00a0not\u00a0be\u00a0taken\u00a0into\u00a0account\u00a0for\u00a0the\u00a0creation\u00a0of\u00a0the\u00a0input\u00a0dataset\u00a0since\u00a0these\u00a0will\u00a0\n. ase Study"
        },
        {
            "heading": "3.1. Problem and Data Description",
            "text": "The main goal of this paper is to develop a methodology in order to i plement a load f r casting tool le t i t i l i ti s er eral ifferent time horizons a d i particular 15 min, 1-h, 2-h, 3-h, 6-h, and 24- . se study makes use of real d ta from a high voltage/medium voltage s ted in ainland Europe, measured during the years 2017\u20132018. The MV distributi rk contains multiple photov ltaic sites. A a result, the data measurem nts in question constitu e mixed powerl ad ecordings, which correspond t the mix d AP emand of the distribution grid from the transmission grid. The load measurements have been recorded ever inute and contain the mixed AP demand, as ell as cloud coverage, wind speed, humidity, and te perature, as easured fro the substation\u2019s weather station. Due to practical concerns, individual power generation or weather data from the aforementioned photovoltaic sites should not be taken into account for the creation of the input dataset since these will normally not be available for a real-life implementation. In short, in this work, we rely on the substation\u2019s historical measurements of load and weather conditions in order to create a prediction model of the mixed AP demand of the grid.\nSensors 2023, 23, 5436 12 of 25"
        },
        {
            "heading": "3.2. Data Preprocessing and Model Training",
            "text": "Unavoidably, the substation measurements contain large periods of missing or corrupt data owing to sensor downtime or malfunction. For the scope of this case study, no missing data imputation has been performed-instead, corrupted data and outlier removal was the main focus of the preprocessing operation. Due to the sheer size of the dataset, manual preprocessing was impossible, mandating the creation of a bad data detection routine. Corrupted values were decidedly easy to detect since the corresponding AP signal exhibited unusually low variance around a constant value. However, outlier values on mixed load data were a challenge to successfully handle\u2014a review of the challenges of this topic, as well as effective techniques, is available on [100]. The chosen technique must be sufficiently effective at classifying outliers in data, while avoiding false positives. In this case study, a rolling median window threshold approach is used, as it was found to compromise well between the aforementioned points. A two-day snapshot from the application of this algorithm to raw electrical load data is presented in Figure 5. The outliers usually originate from noisy sensor readings [101]. As part of data preprocessing, a resampling step also took place, where each sample was defined as an average of 15 one-minute measurements.\nSensors\u00a02023,\u00a023,\u00a0x\u00a0FOR\u00a0PEER\u00a0REVIEW\u00a0 12\u00a0 of\u00a0 26\u00a0 \u00a0\n\u00a0\nnormally\u00a0not\u00a0be\u00a0available\u00a0for\u00a0a\u00a0real-life\u00a0implementation.\u00a0In\u00a0short,\u00a0in\u00a0this\u00a0work,\u00a0we\u00a0rely\u00a0on\u00a0 the\u00a0substation\u2019s\u00a0historical\u00a0measurements\u00a0of\u00a0load\u00a0and\u00a0weather\u00a0conditions\u00a0in\u00a0order\u00a0to\u00a0create\u00a0 a\u00a0prediction\u00a0model\u00a0of\u00a0the\u00a0mixed\u00a0AP\u00a0demand\u00a0of\u00a0the\u00a0grid.\u00a0\n3.2.\u00a0Data\u00a0Preprocessing\u00a0and\u00a0Model\u00a0Training\u00a0 Unavoidably,\u00a0the\u00a0substation\u00a0measurements\u00a0contain\u00a0large\u00a0periods\u00a0of\u00a0missing\u00a0or\u00a0corrupt\u00a0data\u00a0owing\u00a0to\u00a0sensor\u00a0downtime\u00a0or\u00a0malfunction.\u00a0For\u00a0the\u00a0sco \u00a0of\u00a0this\u00a0case\u00a0study,\u00a0no\u00a0 missing\u00a0data\u00a0imputation\u00a0has\u00a0been\u00a0performed-instead,\u00a0corrupted\u00a0data\u00a0and\u00a0outlier\u00a0rem val\u00a0 w s\u00a0the\u00a0main\u00a0f cus\u00a0of\u00a0th \u00a0pr pr cessing\u00a0operation.\u00a0Due\u00a0to\u00a0the\u00a0sheer\u00a0size\u00a0of\u00a0the\u00a0dat set,\u00a0 anual\u00a0preprocessing\u00a0was\u00a0 impossible,\u00a0mandating\u00a0 the\u00a0cr ation\u00a0of\u00a0a\u00a0bad\u00a0data\u00a0detection\u00a0 routine.\u00a0Corrupted\u00a0values\u00a0were\u00a0decidedly\u00a0easy\u00a0to\u00a0det c \u00a0si ce\u00a0the\u00a0correspon ing\u00a0AP\u00a0signal\u00a0exhibited\u00a0unu ually\u00a0low\u00a0varianc \u00a0aroun \u00a0a\u00a0constant\u00a0value.\u00a0However,\u00a0outlier\u00a0values\u00a0on\u00a0 mixed\u00a0load\u00a0data\u00a0were\u00a0a\u00a0challe ge\u00a0to\u00a0successfully\u00a0handle\u2014a\u00a0review\u00a0of\u00a0the\u00a0challenges\u00a0of\u00a0 this\u00a0topic,\u00a0as\u00a0well\u00a0as\u00a0effective\u00a0techniques,\u00a0is\u00a0available\u00a0on\u00a0[100].\u00a0The\u00a0chosen\u00a0technique\u00a0must\u00a0 be\u00a0sufficiently\u00a0effective\u00a0at\u00a0classifying\u00a0outliers\u00a0 in\u00a0data,\u00a0while\u00a0avoiding\u00a0false\u00a0positives.\u00a0In\u00a0 this\u00a0case\u00a0study,\u00a0a\u00a0rolling\u00a0median\u00a0window\u00a0threshold\u00a0approach\u00a0is\u00a0used,\u00a0as\u00a0it\u00a0was\u00a0found\u00a0to\u00a0 compromise\u00a0well\u00a0between\u00a0the\u00a0aforementioned\u00a0points.\u00a0A\u00a0two-day\u00a0snapshot\u00a0from\u00a0the\u00a0application\u00a0of\u00a0this\u00a0algorithm\u00a0to\u00a0raw\u00a0electrical\u00a0load\u00a0data\u00a0is\u00a0presented\u00a0in\u00a0Figure\u00a05.\u00a0The\u00a0outliers\u00a0 usually\u00a0 originate\u00a0 from\u00a0 noisy\u00a0 sensor\u00a0 readings\u00a0 [101].\u00a0As\u00a0 part\u00a0 of\u00a0 data\u00a0 preprocessing,\u00a0 a\u00a0 resampling\u00a0step\u00a0also\u00a0took\u00a0place,\u00a0where\u00a0each\u00a0sample\u00a0was\u00a0defined\u00a0as\u00a0an\u00a0average\u00a0of\u00a015\u00a0oneminute\u00a0measurements.\u00a0\nFigure\u00a05.\u00a0Operation\u00a0of\u00a0 the\u00a0rolling\u00a0median\u00a0 threshold\u00a0outlier\u00a0detection\u00a0algorithm.\u00a0The\u00a0data\u00a0points\u00a0\nmarked\u00a0as\u00a0outliers\u00a0 exceed\u00a0 the\u00a0median\u00a0value\u00a0of\u00a0 the\u00a0 time\u00a0window\u00a0multiplied\u00a0by\u00a0a\u00a0user-specified\u00a0\nthreshold\u00a0factor.\u00a0\nThe\u00a0task\u00a0of\u00a0 input\u00a0variable\u00a0selection\u00a0is\u00a0closely\u00a0related\u00a0to\u00a0the\u00a0prediction\u00a0horizon.\u00a0All\u00a0 models\u00a0developed\u00a0in\u00a0the\u00a0context\u00a0of\u00a0this\u00a0study\u00a0are\u00a0considered\u00a0autoregressive\u00a0with\u00a0exogenous\u00a0 variables,\u00a0 as\u00a0 they\u00a0 use\u00a0 inputs\u00a0 that\u00a0 consist\u00a0 of\u00a0 previous\u00a0 values\u00a0 of\u00a0 the\u00a0 output\u00a0 and\u00a0 weather\u00a0data.\u00a0A\u00a0set\u00a0of\u00a0inputs\u00a0was\u00a0initially\u00a0constructed\u00a0for\u00a0each\u00a0prediction\u00a0horizon\u00a0based\u00a0 on\u00a0the\u00a0literature.\u00a0Subsequently,\u00a0the\u00a0contribution\u00a0of\u00a0these\u00a0variables\u00a0to\u00a0the\u00a0prediction\u00a0accuracy\u00a0improvement\u00a0was\u00a0examined\u00a0by\u00a0trial\u00a0and\u00a0error,\u00a0sometimes\u00a0leading\u00a0to\u00a0shorter\u00a0input\u00a0 sets\u00a0for\u00a0some\u00a0of\u00a0the\u00a0horizons.\u00a0Alternatively,\u00a0other\u00a0approaches,\u00a0such\u00a0as\u00a0gradient\u00a0boosting\u00a0 decision\u00a0tree\u00a0and\u00a0Pearson\u00a0correlation\u00a0coefficient\u00a0[102],\u00a0attention\u00a0mechanism\u00a0[103],\u00a0or\u00a0Exploratory\u00a0Data\u00a0Analysis\u00a0[104],\u00a0are\u00a0considered\u00a0to\u00a0have\u00a0an\u00a0effective\u00a0contribution\u00a0during\u00a0input\u00a0features\u00a0reduction\u00a0and\u00a0selection.\u00a0However,\u00a0it\u00a0is\u00a0important\u00a0to\u00a0note\u00a0that\u00a0for\u00a0each\u00a0horizon,\u00a0inputs\u00a0remain\u00a0the\u00a0same\u00a0for\u00a0all\u00a0machine-learning\u00a0methods\u00a0used\u00a0in\u00a0the\u00a0present\u00a0study.\u00a0\nThe task of input variable selection is closely related to the prediction horizon. All models developed in the context of this study are considered autoregressive with exogenous variables, as they use inputs that consist of previous values of the output and weather data. A set of inputs was initially constructed for each prediction horizon based on the literature. Subsequently, the contribution of these variables to the prediction accuracy improvement was examined by trial and error, sometimes leading to shorter input sets for some of the horizons. Alternatively, other approaches, such as gradient boosting decision tree and Pearson correlation coefficient [102], attention mechanism [103], or Exploratory Data Analysis [104], are considered to have an effective contribution during input features reduction and selection. However, it is important to note that for each horizon, inputs remain the same for all machine-learning methods used in the present study. The selected input variables which all models accept could be divided into 4 categories, as described in Table 1, namely (a) current and past AP values, (b) difference between current and past AP values, (c) average of past AP values, and (d) weather measurements. It has to be noted that p(t) values contain the current and past, average and difference measures of the AP values, p\u0302(t+s) is the output, i.e., the mixed power load s fifteen-minute intervals ahead, whereas w(t) components contain the respective weather-related inputs of cloud coverage, wind speed, humidity, and temperature, respectively.\nSensors 2023, 23, 5436 13 of 25\nThe choice of the particular set of input variables can be justified as follows: The fact that electric load time series presents a strong dependency on previous values [47] strengthens the selection of such input variables in the form of (a). Trying to capture the trend of electrical load, differences (b) between current and previous AP values are frequently employed [31]. The implementation of past value averages (c) is also quite important, according to the literature [48]. Finally, the introduction of weather data (d) is undoubtedly an improving factor in the predictions [23,24,105]. At this point, it is important to mention that during the training stage of the forecasting model, the weather inputs w(t) are introduced as measured values of actual weather data acquired at the t time index. On the contrary, in an online implementation of the model, future weather data w(t+s) will be unknown and replaced by weather predictions, therefore introducing additional uncertainty. Once the preprocessing stage has been completed and input variables have been selected, the dataset was partitioned in a yearly manner in order to select the training datasets. At this point, an important consideration should be made. As mentioned in the introductory section, the load time series consists of a load and generation component. The statistical properties of both of these components are not static in relation to time, especially on a long-term scale. The network physically expands, incorporating more consumers as well as RES generators, each with different load and generation profiles, respectively. Therefore, it makes sense to select training datasets as close to the actual prediction interval as possible. Since the available data concern two successive years, the data corresponding to 2017 were selected as the training subset, and the data corresponding to 2018 were selected as the testing dataset. A point worth mentioning is that no permutation step is taking place before training. This means that the data used for testing are considered completely unseen for the proposed model, yielding a more reliable forecasting model. Due to confidentiality reasons, the real and predicted mixed load values have been normalized in order to be presented. Finally, it should be noted that models that require a validation step during training, namely models based on MLP and RBF NNs, do so using 10-fold cross-validation, while in the case of models that require multiple training runs for each training seed (see MLP), the best-performing model on the validation data is kept. An overview of the implementation of the proposed model is provided in Figure 6, which illustrates, in the form of a block diagram, the entire sequence of steps that take place, starting from the acquisition of the raw AP data from the substation to the derivation of the final forecasts. It has to be highlighted that this figure is generic and does not refer to a particular prediction horizon.\nSensors 2023, 23, 5436 14 of 25\nSensors\u00a02023,\u00a023,\u00a0x\u00a0FOR\u00a0PEER\u00a0REVIEW\u00a0 14\u00a0 of\u00a0 26\u00a0 \u00a0\n\u00a0\nnormalized\u00a0in\u00a0order\u00a0to\u00a0be\u00a0presented.\u00a0Finally,\u00a0it\u00a0should\u00a0be\u00a0noted\u00a0that\u00a0models\u00a0that\u00a0require\u00a0 a\u00a0validation\u00a0step\u00a0during\u00a0training,\u00a0namely\u00a0models\u00a0based\u00a0on\u00a0MLP\u00a0and\u00a0RBF\u00a0NNs,\u00a0do\u00a0so\u00a0using\u00a0 10-fold\u00a0cross-validation,\u00a0while\u00a0in\u00a0the\u00a0case\u00a0of\u00a0models\u00a0that\u00a0require\u00a0multiple\u00a0training\u00a0runs\u00a0 for\u00a0each\u00a0training\u00a0seed\u00a0(see\u00a0MLP),\u00a0the\u00a0best-performing\u00a0model\u00a0on\u00a0the\u00a0validation\u00a0data\u00a0is\u00a0kept.\u00a0 An\u00a0overview\u00a0of\u00a0the\u00a0implementation\u00a0of\u00a0the\u00a0proposed\u00a0model\u00a0is\u00a0provided\u00a0in\u00a0Figure\u00a06,\u00a0which\u00a0 illustrates,\u00a0 in\u00a0 the\u00a0form\u00a0of\u00a0a\u00a0block\u00a0diagram,\u00a0 the\u00a0entire\u00a0sequence\u00a0of\u00a0steps\u00a0 that\u00a0 take\u00a0place,\u00a0 starting\u00a0from\u00a0the\u00a0acquisition\u00a0of\u00a0the\u00a0raw\u00a0AP\u00a0data\u00a0from\u00a0the\u00a0substation\u00a0to\u00a0the\u00a0derivation\u00a0of\u00a0 the\u00a0final\u00a0forecasts.\u00a0It\u00a0has\u00a0to\u00a0be\u00a0highlighted\u00a0that\u00a0this\u00a0figure\u00a0is\u00a0generic\u00a0and\u00a0does\u00a0not\u00a0refer\u00a0to\u00a0 a\u00a0particular\u00a0prediction\u00a0horizon.\u00a0 \u00a0\nFigure\u00a06.\u00a0Overview\u00a0of\u00a0the\u00a0proposed\u00a0model\u00a0ensemble.\u00a0Its\u00a0application\u00a0in\u00a0mixed\u00a0load\u00a0forecasting\u00a0com-\nprises\u00a0a\u00a0series\u00a0of\u00a0steps,\u00a0i.e.\u00a0raw\u00a0data\u00a0acquisition,\u00a0data\u00a0preprocessing,\u00a0collection\u00a0of\u00a0input\u00a0variables,\u00a0\nsplitting\u00a0of\u00a0the\u00a0dataset\u00a0in\u00a0a\u00a0training\u00a0and\u00a0a\u00a0testing\u00a0subset,\u00a0training\u00a0of\u00a0submodels,\u00a0generation\u00a0of\u00a0the\u00a0\nnext\u00a0AP\u00a0forecast\u00a0by\u00a0each\u00a0submodel,\u00a0weighting\u00a0of\u00a0the\u00a0individual\u00a0predictions,\u00a0and,\u00a0lastly,\u00a0calculation\u00a0\nof\u00a0the\u00a0next\u00a0AP\u00a0final\u00a0forecast.\u00a0\nAt\u00a0 this\u00a0point,\u00a0 it\u00a0should\u00a0be\u00a0mentioned\u00a0 that\u00a0 in\u00a0order\u00a0 to\u00a0evaluate\u00a0 the\u00a0accuracy\u00a0of\u00a0the\u00a0 proposed\u00a0method,\u00a0 it\u00a0was\u00a0considered\u00a0appropriate\u00a0to\u00a0compare\u00a0 it\u00a0with\u00a0a\u00a0model\u00a0ensemble\u00a0 from\u00a0the\u00a0literature.\u00a0To\u00a0be\u00a0more\u00a0specific,\u00a0we\u00a0employed\u00a0a\u00a0method\u00a0proposed\u00a0for\u00a0load\u00a0forecasting\u00a0based\u00a0on\u00a0an\u00a0ensemble\u00a0of\u00a0multiple\u00a0MLP\u00a0neural\u00a0networks\u00a0[76].\u00a0Consequently,\u00a0following\u00a0the\u00a0experimental\u00a0protocol\u00a0described\u00a0in\u00a0this\u00a0work,\u00a0a\u00a0number\u00a0of\u00a0feed-forward\u00a0NNs,\u00a0 with\u00a0 a\u00a0 single\u00a0hidden\u00a0 layer,\u00a0were\u00a0 trained\u00a0 on\u00a0 14\u00a0different\u00a0 random\u00a0 initializations\u00a0 of\u00a0 the\u00a0 weights.\u00a0For\u00a0each\u00a0initialization,\u00a0the\u00a0number\u00a0of\u00a0neurons\u00a0in\u00a0the\u00a0hidden\u00a0layer\u00a0ranged\u00a0from\u00a03\u00a0 to\u00a0 50.\u00a0 The\u00a0 hyperbolic\u00a0 tangent\u00a0 sigmoid\u00a0 function\u00a0was\u00a0 selected\u00a0 as\u00a0 the\u00a0 transfer\u00a0 function\u00a0 among\u00a0the\u00a0NNs\u2019\u00a0layers,\u00a0while\u00a0all\u00a0NNs\u00a0were\u00a0trained\u00a0using\u00a0the\u00a0resilient\u00a0backpropagation\u00a0 algorithm.\u00a0The\u00a0neural\u00a0networks\u00a0were\u00a0arranged\u00a0 in\u00a0ascending\u00a0order\u00a0with\u00a0 respect\u00a0 to\u00a0 the\u00a0 MAPE\u00a0error\u00a0on\u00a0a\u00a0common\u00a0validation\u00a0set,\u00a0which,\u00a0in\u00a0this\u00a0case,\u00a0was\u00a0defined\u00a0as\u00a020%\u00a0of\u00a0the\u00a0 training\u00a0dataset.\u00a0Then,\u00a0the\u00a0networks\u00a0corresponding\u00a0to\u00a0the\u00a0first\u00a05\u00a0MAPE\u00a0errors\u00a0were\u00a0selected,\u00a0and\u00a0the\u00a0final\u00a0forecasts\u00a0were\u00a0obtained\u00a0by\u00a0averaging\u00a0the\u00a0individual\u00a0forecasts\u00a0of\u00a0these\u00a0 5\u00a0models.\u00a0\nAt this point, it should be mentioned that in order to evaluate the accuracy of the proposed method, it was consid red appropriate t compar it with a model ensemble from the literature. T be more specific, we employed a method proposed f r load forecasting based on an ensemble of multiple MLP neural networks [76]. Consequently, following the experimental protocol described in this work, a number of feed-forward NNs, with a single hidden layer, were trained on 14 different random initializations of the weights. For each initialization, the number of neurons in the hidden layer ranged from 3 to 50. The hyperbolic tangent sigmoid function was selected as the transfer function among the NNs\u2019 layers, while all NNs were trained using the resilient backpropagation algorithm. The neural networks were arranged in ascending order with respect to the MAPE error on a common validation set, which, in this case, was defined as 20% of the training dataset. Then, the networks corresponding to the first 5 MAPE errors were selected, and the final forecasts were obtained by averaging the individual forecasts of these 5 models."
        },
        {
            "heading": "4. Results",
            "text": "In this section, the results of extensive simulations of the proposed model are presented. A set of scatterplots is shown in Figure 7a\u2013f, representing the actual versus the predicted values mixed load values for 1, 2, 3, 6, and 24-h-ahead horizons, respectively, through the whole testing dataset. The diagonal line implies a complete match between real values and forecasts. The axes are presented in units of normalized AP.\nSensors 2023, 23, 5436 15 of 25\nSensors\u00a02023,\u00a023,\u00a0x\u00a0FOR\u00a0PEER\u00a0REVIEW\u00a0 15\u00a0 of\u00a0 26\u00a0 \u00a0\n\u00a0\n4.\u00a0Results\u00a0 In\u00a0this\u00a0section,\u00a0the\u00a0results\u00a0of\u00a0extensive\u00a0simulations\u00a0of\u00a0the\u00a0proposed\u00a0model\u00a0are\u00a0presented.\u00a0A\u00a0set\u00a0of\u00a0scatterplots\u00a0 is\u00a0shown\u00a0 in\u00a0Figure\u00a07a\u2013f,\u00a0representing\u00a0 the\u00a0actual\u00a0versus\u00a0the\u00a0 predicted\u00a0values\u00a0mixed\u00a0load\u00a0values\u00a0for\u00a01,\u00a02,\u00a03,\u00a06,\u00a0and\u00a024-h-ahead\u00a0horizons,\u00a0respectively,\u00a0 through\u00a0the\u00a0whole\u00a0testing\u00a0dataset.\u00a0The\u00a0diagonal\u00a0line\u00a0implies\u00a0a\u00a0complete\u00a0match\u00a0between\u00a0 real\u00a0values\u00a0and\u00a0forecasts.\u00a0The\u00a0axes\u00a0are\u00a0presented\u00a0in\u00a0units\u00a0of\u00a0normalized\u00a0AP.\u00a0\n(e)\u00a06-h,\u00a0and\u00a0(f)\u00a024-h\u00a0ahead\u00a0prediction.\u00a0The\u00a0predicted\u00a0values\u00a0residing\u00a0on\u00a0the\u00a0diagonal\u00a0line\u00a0are\u00a0identical\u00a0\nto\u00a0the\u00a0actual\u00a0values.\u00a0Each\u00a0mark\u00a0refers\u00a0to\u00a0a\u00a0data\u00a0point\u00a0and\u00a0shows\u00a0the\u00a0deviation\u00a0of\u00a0its\u00a0predicted\u00a0value\u00a0\nfrom\u00a0its\u00a0actual\u00a0value.\u00a0\nAdditional\u00a0 results\u00a0are\u00a0provided\u00a0 in\u00a0Table\u00a02,\u00a0which\u00a0 contains\u00a0 information\u00a0about\u00a0 the\u00a0 forecasting\u00a0performance\u00a0of\u00a0 the\u00a0proposed\u00a0method\u00a0 in\u00a0comparison\u00a0 to\u00a0 the\u00a0 individual\u00a0machine-learning\u00a0methods\u00a0comprising\u00a0the\u00a0model\u00a0pool.\u00a0In\u00a0order\u00a0to\u00a0distinguish\u00a0the\u00a0results\u00a0for\u00a0 different\u00a0 prediction\u00a0 time\u00a0 horizons,\u00a0 the\u00a0 table\u00a0 is\u00a0divided\u00a0 into\u00a0 sections.\u00a0The\u00a0 accuracy\u00a0 of\u00a0 model\u00a0predictions\u00a0is\u00a0evaluated\u00a0through\u00a0the\u00a0correlation\u00a0coefficient\u00a0(R2),\u00a0RMSE\u00a0and\u00a0MAE,\u00a0 considering\u00a0them\u00a0as\u00a0representative\u00a0and\u00a0efficient\u00a0criteria\u00a0[106].\u00a0For\u00a0comparative\u00a0reasons,\u00a0 the\u00a0table\u00a0also\u00a0contains\u00a0the\u00a0values\u00a0of\u00a0the\u00a0indices\u00a0for\u00a0all\u00a0submodels,\u00a0as\u00a0well\u00a0as\u00a0their\u00a0percentage\u00a0of\u00a0ranking\u00a0in\u00a0the\u00a0first\u00a0place.\u00a0This\u00a0quantity,\u00a0labeled\u00a0as\u00a0\u201cRank\u00a01\u201d\u00a0in\u00a0Table\u00a02,\u00a0denotes\u00a0how\u00a0 many\u00a0times\u00a0each\u00a0submodel\u00a0scored\u00a0the\u00a01st\u00a0rank\u00a0among\u00a0all\u00a0submodels,\u00a0i.e.,\u00a0achieved\u00a0the\u00a0 lowest\u00a0MAE.\u00a0\n\u00a0 \u00a0\nAdditional results are provided in Table 2, which contains information about the forecasting performance of the proposed method in comparison to the individual machinelearning methods comprising t e model pool. I rder to distinguish the results for differe t p ediction time horizons, th table is divided into sections. The accuracy of model predicti ns is evaluated through the correlation coefficient (R2), RMSE and MAE, considering them as representative and efficient criteria [106]. For comparative reasons, the table also contains the values of the indices for all submodels, as well as their percentage of ranking in the first place. This quantity, labeled as \u201cRank 1\u201d in Table 2, denotes how many times each submodel scored the 1st rank among all submodels, i.e., achieved the lowest MAE. The aforementioned form of ranking of the submodels can be seen graphically in Figure 8. More specifically, each one of Figure 8a\u2013f refers to 15 min, 1, 2, 3, 6, and 24-h prediction horizons, respectively. Each one of these subfigures contains 6 pie charts, denoting 1st to 6th rank for the models. To be more specific, each pie chart shows the percentages corresponding to how many times each submodel ranked in the respective place, according to its weighted MAE. For example, the 2nd pie of Figure 8a implies that for 15 min-ahead forecasting, the MLP submodel ranked in the 2nd place among all models with a percentage of 17%, the SR submodel with a percentage of 21%, etc. Finally, analytical graphs are provided for each prediction time horizon, with Figure 9a1\u2013f1 to depict forecasts of 15 min, 1, 2, 3, 6, and 24 h-ahead, respectively, where a randomly chosen 12-h time window (from 09:00 to 21:00) of real AP values and the respective predictions are shown for an arbitrarily chosen day belonging to the testing subset (the same day and the same window is used for all horizons). These graphs are accompanied by Figure 9a2\u2013f2, which indicates which submodel has the largest weight for every predicted data point using a bar plot.\nSensors 2023, 23, 5436 16 of 25\nNote: The quality metrics of the proposed methodology are denoted by text in colour.\nSensors 2023, 23, 5436 17 of 25\nSensors\u00a02023,\u00a023,\u00a0x\u00a0FOR\u00a0PEER\u00a0REVIEW\u00a0 17\u00a0 of\u00a0 26\u00a0 \u00a0\n\u00a0\nProposed\u00a0 0.78474\u00a0 1.1835\u00a0 1.8174\u00a0 -\u00a0 MLP\u00a0ensemble\u00a0 0.7827\u00a0 1.2372\u00a0 1.8468\u00a0 -\u00a0 MLP\u00a0 0.78073\u00a0 1.2313\u00a0 1.8553\u00a0 21.93%\u00a0 RBF\u00a0 0.73576\u00a0 1.4119\u00a0 2.0367\u00a0 21.83%\u00a0 LR\u00a0 0.75712\u00a0 1.3188\u00a0 1.9526\u00a0 11.16%\u00a0 SVR\u00a0 0.73669\u00a0 1.3031\u00a0 2.0331\u00a0 16.82%\u00a0 RF\u00a0 0.76487\u00a0 1.2694\u00a0 1.9212\u00a0 16.71%\u00a0 SR\u00a0 0.74761\u00a0 1.3419\u00a0 1.9905\u00a0 11.56%\u00a0 Note:\u00a0The\u00a0quality\u00a0metrics\u00a0of\u00a0the\u00a0proposed\u00a0methodology\u00a0are\u00a0denoted\u00a0by\u00a0text\u00a0in\u00a0colour.\u00a0\nThe\u00a0aforementioned\u00a0 form\u00a0of\u00a0 ranking\u00a0of\u00a0 the\u00a0submodels\u00a0can\u00a0be\u00a0seen\u00a0graphically\u00a0 in\u00a0 Figure\u00a08.\u00a0More\u00a0specifically,\u00a0each\u00a0one\u00a0of\u00a0Figure\u00a08a\u2013f\u00a0refers\u00a0to\u00a015\u00a0min,\u00a01,\u00a02,\u00a03,\u00a06,\u00a0and\u00a024-h\u00a0 prediction\u00a0horizons,\u00a0respectively.\u00a0Each\u00a0one\u00a0of\u00a0these\u00a0subfigures\u00a0contains\u00a06\u00a0pie\u00a0charts,\u00a0denoting\u00a01st\u00a0to\u00a06th\u00a0rank\u00a0for\u00a0the\u00a0models.\u00a0To\u00a0be\u00a0more\u00a0specific,\u00a0each\u00a0pie\u00a0chart\u00a0shows\u00a0the\u00a0percentages\u00a0corresponding\u00a0to\u00a0how\u00a0many\u00a0times\u00a0each\u00a0submodel\u00a0ranked\u00a0in\u00a0the\u00a0respective\u00a0place,\u00a0 according\u00a0to\u00a0its\u00a0weighted\u00a0MAE.\u00a0For\u00a0example,\u00a0the\u00a02nd\u00a0pie\u00a0of\u00a0Figure\u00a08a\u00a0implies\u00a0that\u00a0for\u00a015\u00a0 min-ahead\u00a0forecasting,\u00a0the\u00a0MLP\u00a0submodel\u00a0ranked\u00a0in\u00a0the\u00a02nd\u00a0place\u00a0among\u00a0all\u00a0models\u00a0with\u00a0 a\u00a0percentage\u00a0of\u00a017%,\u00a0the\u00a0SR\u00a0submodel\u00a0with\u00a0a\u00a0percentage\u00a0of\u00a021%,\u00a0etc.\u00a0Finally,\u00a0analytical\u00a0 graphs\u00a0are\u00a0provided\u00a0for\u00a0each\u00a0prediction\u00a0time\u00a0horizon,\u00a0with\u00a0Figure\u00a09a1\u2013f1\u00a0to\u00a0depict\u00a0forecasts\u00a0of\u00a015\u00a0min,\u00a01,\u00a02,\u00a03,\u00a06,\u00a0and\u00a024\u00a0h-ahead,\u00a0respectively,\u00a0where\u00a0a\u00a0randomly\u00a0chosen\u00a012-h\u00a0time\u00a0 window\u00a0(from\u00a009:00\u00a0to\u00a021:00)\u00a0of\u00a0real\u00a0AP\u00a0values\u00a0and\u00a0the\u00a0respective\u00a0predictions\u00a0are\u00a0shown\u00a0 for\u00a0an\u00a0arbitrarily\u00a0chosen\u00a0day\u00a0belonging\u00a0to\u00a0the\u00a0testing\u00a0subset\u00a0(the\u00a0same\u00a0day\u00a0and\u00a0the\u00a0same\u00a0 window\u00a0is\u00a0used\u00a0for\u00a0all\u00a0horizons).\u00a0These\u00a0graphs\u00a0are\u00a0accompanied\u00a0by\u00a0Figure\u00a09a2\u2013f2,\u00a0which\u00a0 indicates\u00a0which\u00a0submodel\u00a0has\u00a0the\u00a0largest\u00a0weight\u00a0for\u00a0every\u00a0predicted\u00a0data\u00a0point\u00a0using\u00a0a\u00a0 bar\u00a0plot.\u00a0\n\u00a0\n\u00a0 \u00a0 (a)\u00a0 (b)\u00a0\nMLP RBF LR SVR RF SR\n12%\n24%\n14%10%\n20%\n19% 21%\n7%\n15% 22%\n17%\n17% 24%\n6%\n20%26%\n11%\n13%\n21%\n7%\n20%23%\n13%\n15% 15%\n11%\n19%\n14%\n20%\n21% 6%\n45%\n12% 4%\n19%\n14%\n1st Ranking 2nd Ranking 3rd Ranking\n4th Ranking 5th Ranking 6th Ranking\n17%\n21%\n11%9%\n21%\n22% 17%\n13%\n14% 20%\n17%\n19% 18%\n11%\n20%22%\n14%\n14%\n17%\n11%\n21%24%\n13%\n13% 15%\n14%\n18% 20%\n17%\n15%\n1st Ranking 2nd Ranking 3rd Ranking\n4th Ranking 5th Ranking 6th Ranking\n15%\n30%\n17%\n4%\n17%\n16%\nSensors\u00a02023,\u00a023,\u00a0x\u00a0FOR\u00a0PEER\u00a0REVIEW\u00a0 18\u00a0 of\u00a0 26\u00a0 \u00a0\n\u00a0\n\u00a0 (c)\u00a0\n\u00a0 (e)\u00a0\n\u00a0 (d)\u00a0\n\u00a0 (f)\u00a0\nFigure\u00a08.\u00a0Pie\u00a0charts\u00a0depicting\u00a0the\u00a0ranking\u00a0of\u00a0the\u00a0submodels\u00a0included\u00a0in\u00a0the\u00a0proposed\u00a0model\u00a0ensemble\u00a0for\u00a0(a)\u00a015-min,\u00a0(b)\u00a01-h,\u00a0(c)\u00a02-h,\u00a0(d)\u00a03-h,\u00a0(e)\u00a06-h,\u00a0and\u00a0(f)\u00a024-h\u00a0ahead\u00a0prediction.\u00a0Each\u00a0pie\u00a0chart\u00a0refers\u00a0 to\u00a0a\u00a0ranking\u00a0position\u00a0and\u00a0shows\u00a0the\u00a0percentage\u00a0that\u00a0each\u00a0submodel\u00a0was\u00a0ranked\u00a0in\u00a0that\u00a0position.\u00a0 Each\u00a0submodel\u00a0is\u00a0represented\u00a0by\u00a0a\u00a0different\u00a0color\u00a0and\u00a0pattern.\u00a0\n\u00a0 (a2)\u00a0\n\u00a0 (b2)\u00a0\n16%\n23%\n11%11%\n18%\n20% 17%\n15%\n19%17%\n15%\n16% 20%\n12%\n21% 24%\n11%\n13%\n19%\n11%\n21%24%\n13%\n13% 15%\n15%\n18%16%\n19%\n16%\n1st Ranking 2nd Ranking 3rd Ranking\n4th Ranking 5th Ranking 6th Ranking\n13%\n24%\n9% 9%\n23%\n22%\n10%\n21%\n16% 10%\n21%\n21% 16%\n19%\n13%17%\n17%\n20% 18%\n17%\n13%21%\n14%\n17%\n23%\n14%\n13%23%\n12%\n16% 22%\n15%\n14% 20%\n14%\n15%\n1st Ranking 2nd Ranking 3rd Ranking\n4th Ranking 5th Ranking 6th Ranking\n11%\n14%\n30%10%\n23%\n12%\n18%\n24%\n12%9%\n20%\n18% 17%\n16%\n15%20%\n18%\n15% 19%\n12%\n21% 24%\n13%\n12%\n19%\n13%\n20% 24%\n13%\n11% 15%\n14%\n17% 19%\n19%\n17%\n1st Ranking 2nd Ranking 3rd Ranking\n4th Ranking 5th Ranking 6th Ranking\n13%\n22%\n15%5% 17%\n28%\n12%\n17%\n17% 11%\n22%\n22% 15%\n19%\n16%17%\n13%\n20% 18%\n18%\n19%\n19%\n10%\n16%\n20%\n15%\n19% 21%\n11%\n15% 20%\n18%\n14% 21%\n13%\n14%\n1st Ranking 2nd Ranking 3rd Ranking\n4th Ranking 5th Ranking 6th Ranking\n16%\n14%\n15% 10%\n32%\n13%\n09:00 11:00 13:00 15:00 17:00 19:00 21:00\n09:00 11:00 13:00 15:00 17:00 19:00 21:00\nSensors 2023, 23, 5436 18 of 25\nSensors\u00a02023,\u00a023,\u00a0x\u00a0FOR\u00a0PEER\u00a0REVIEW\u00a0 18\u00a0 of\u00a0 26\u00a0 \u00a0\n\u00a0\n\u00a0 (c)\u00a0\n\u00a0 (e)\u00a0\n\u00a0 (d)\u00a0\n\u00a0 (f)\u00a0\nFigure\u00a08.\u00a0Pie\u00a0charts\u00a0depicting\u00a0the\u00a0ranking\u00a0of\u00a0the\u00a0submodels\u00a0included\u00a0in\u00a0the\u00a0proposed\u00a0model\u00a0ensemble\u00a0for\u00a0(a)\u00a015-min,\u00a0(b)\u00a01-h,\u00a0(c)\u00a02-h,\u00a0(d)\u00a03-h,\u00a0(e)\u00a06-h,\u00a0and\u00a0(f)\u00a024-h\u00a0ahead\u00a0prediction.\u00a0Each\u00a0pie\u00a0chart\u00a0refers\u00a0 to\u00a0a\u00a0ranking\u00a0position\u00a0and\u00a0shows\u00a0the\u00a0percentage\u00a0that\u00a0each\u00a0submodel\u00a0was\u00a0ranked\u00a0in\u00a0that\u00a0position.\u00a0 Each\u00a0submodel\u00a0is\u00a0represented\u00a0by\u00a0a\u00a0different\u00a0color\u00a0and\u00a0pattern.\u00a0\n\u00a0 \u00a0 (a1)\u00a0\n\u00a0 (a2)\u00a0\n(b2)\u00a0\n\u00a0 (b2)\u00a0\n16% 23% 11%11% 18% 20% 17% 15% 19%17% 15% 16% 20% 12% 21% 24% 11% 13% 19% 11% 21%24% 13% 13% 15% 15% 18%16% 19% 16% 1st Ranking 2nd Ranking 3rd Ranking 4th Ranking 5th Ranking 6th Ranking 13% 24% 9% 9% 23% 22% 10% 21% 16% 10% 21% 21% 16% 19% 13%17% 17% 20% 18% 17% 13%21% 14% 17% 23% 14%\n13%23%\n12% 16% 22% 15%\n14% 20%\n14% 15% 1st Ranking 2nd Ranking 3rd Ranking\n4th Ranking 5th Ranking 6th Ranking\n11% 14%\n30%10%\n23% 12%\n18% 24% 12%9% 20% 18% 17% 16% 15%20% 18% 15% 19% 12% 21% 24% 13% 12% 19% 13% 20% 24% 13% 11% 15% 14% 17% 19% 19% 17% 1st Ranking 2nd Ranking 3rd Ranking 4th Ranking 5th Ranking 6th Ranking 13% 22% 15%5% 17% 28% 12% 17% 17% 11% 22% 22% 15% 19% 16%17% 13% 20% 18% 18% 19% 19% 10% 16% 20% 15%\n19% 21%\n11% 15% 20% 18%\n14% 21%\n13% 14% 1st Ranking 2nd Ranking 3rd Ranking\n4th Ranking 5th Ranking 6th Ranking\n16% 14%\n15% 10%\n32% 13%\n-0.7\n-0.5\n-0.3\n-0.1\n0.1\n0.3\n0.5\n0.7\n09:00 11:00 13:00 15:00 17:00 19:00 21:00\nM ixe\nd Lo\nad (n\nor m\nal ize\nd) Real Predictions\n-0.7\n-0.5\n-0.3\n-0.1\n0.1\n0.3\n0.5\n0.7\n09:00 11:00 13:00 15:00 17:00 19:00 21:00\nM ixe\nd Lo\nad (n\nor m\nal ize\nd) Real Predictions\n09:00 11:00 13:00 15:00 17:00 19:00 21:00\nBe st\nsu\nbm od\nel SR RF\nSVR LR RBF MLP\n09:00 11:00 13:00 15:00 17:00 19:00 21:00\nBe st\nsu\nbm od\nel\nSR RF SVR LR\nRBF MLPSensors\u00a02023,\u00a023,\u00a0x\u00a0FOR\u00a0PEER\u00a0REVIEW\u00a0 19\u00a0 of\u00a0 26\u00a0 \u00a0\n\u00a0\n\u00a0 (c1)\u00a0\n\u00a0 (c2)\u00a0\n\u00a0 (e1)\u00a0\n\u00a0 (e2)\u00a0\n\u00a0 (d1)\u00a0\n\u00a0 (d2)\u00a0\n\u00a0 (f1)\u00a0\n\u00a0 (f2)\u00a0\nFigure\u00a09.\u00a0Results\u00a0for\u00a0a\u00a0randomly\u00a0selected\u00a012-h\u00a0window\u00a0for\u00a0(a)\u00a015-min,\u00a0(b)\u00a01-h,\u00a0(c)\u00a02-h,\u00a0(d)\u00a03-h,\u00a0(e)\u00a06h,\u00a0and\u00a0(f)\u00a024-h\u00a0ahead\u00a0predictions.\u00a0Subgraphs\u00a0labeled\u00a01\u00a0depict\u00a0actual\u00a0and\u00a0predicted\u00a0value\u00a0results,\u00a0 whereas\u00a0subgraphs\u00a0labeled\u00a02\u00a0depict\u00a0the\u00a0best\u00a0submodel\u00a0performance\u00a0results.\u00a0\n5.\u00a0Discussion\u00a0 In\u00a0the\u00a0context\u00a0of\u00a0the\u00a0case\u00a0study,\u00a0multiple\u00a0experiments\u00a0were\u00a0conducted,\u00a0and\u00a0the\u00a0results\u00a0 are\u00a0explained\u00a0and\u00a0discussed\u00a0here.\u00a0At\u00a0this\u00a0point,\u00a0we\u00a0should\u00a0point\u00a0out\u00a0that\u00a0providing\u00a0accurate\u00a0predictions\u00a0is\u00a0indeed\u00a0a\u00a0challenging\u00a0task\u00a0due\u00a0to\u00a0both\u00a0grid\u00a0and\u00a0data-related\u00a0reasons.\u00a0 First,\u00a0the\u00a0system\u2019s\u00a0expandability\u00a0can\u00a0be\u00a0a\u00a0limiting\u00a0factor\u00a0for\u00a0the\u00a0accuracy\u00a0of\u00a0future\u00a0forecasts.\u00a0At\u00a0the\u00a0same\u00a0time,\u00a0this\u00a0is\u00a0reinforced\u00a0by\u00a0inherent\u00a0characteristics\u00a0of\u00a0the\u00a0load\u00a0time\u00a0series,\u00a0 such\u00a0 as\u00a0 non-linearity\u00a0 and\u00a0 uncertainty.\u00a0 In\u00a0 the\u00a0 face\u00a0 of\u00a0 these\u00a0 challenges,\u00a0 the\u00a0 proposed\u00a0 method\u00a0seems\u00a0to\u00a0be\u00a0quite\u00a0effective,\u00a0providing\u00a0reliable\u00a0predictions.\u00a0From\u00a0Figure\u00a07a\u2013f,\u00a0we\u00a0 can\u00a0draw\u00a0conclusions\u00a0about\u00a0the\u00a0quality\u00a0of\u00a0predictions.\u00a0When\u00a0the\u00a0prediction\u00a0time\u00a0horizon\u00a0 is\u00a0too\u00a0short\u00a0(Figure\u00a07a),\u00a0the\u00a0forecast\u00a0error\u00a0is\u00a0distributed\u00a0close\u00a0to\u00a0the\u00a0diagonal\u00a0line,\u00a0which\u00a0 implies\u00a0quite\u00a0accurate\u00a0predictions.\u00a0While\u00a0we\u00a0are\u00a0trying\u00a0to\u00a0increase\u00a0the\u00a0prediction\u00a0horizon,\u00a0 the\u00a0 forecasts\u00a0are\u00a0getting\u00a0 less\u00a0accurate\u00a0 (Figure\u00a07b\u2013f),\u00a0as\u00a0obviously,\u00a0 the\u00a0pairs\u00a0of\u00a0real\u00a0and\u00a0 predicted\u00a0values\u00a0are\u00a0scattered\u00a0further\u00a0from\u00a0the\u00a0ideal\u00a0line.\u00a0\nLooking\u00a0at\u00a0Table\u00a02,\u00a0we\u00a0observe\u00a0that\u00a0the\u00a0proposed\u00a0model\u00a0outmatches\u00a0all\u00a0individual\u00a0 submodels,\u00a0 and\u00a0 the\u00a0 competitive\u00a0MLP\u00a0model\u00a0 ensemble\u00a0 in\u00a0 terms\u00a0 of\u00a0MAE,\u00a0 and\u00a0R2\u00a0 and\u00a0 RMSE.\u00a0Moreover,\u00a0this\u00a0conclusion\u00a0applies\u00a0to\u00a0all\u00a0prediction\u00a0time\u00a0horizons.\u00a0As\u00a0the\u00a0prediction\u00a0 horizon\u00a0gets\u00a0longer,\u00a0the\u00a0forecasting\u00a0error\u00a0increases,\u00a0which\u00a0is\u00a0absolutely\u00a0reasonable.\u00a0The\u00a0 only\u00a0exceptions\u00a0are\u00a0the\u00a0R2\u00a0and\u00a0RMSE\u00a0values\u00a0obtained\u00a0by\u00a0the\u00a0MLP\u00a0model\u00a0ensemble\u00a0for\u00a02\u00a0h\u00a0 prediction\u00a0horizon,\u00a0which\u00a0slightly\u00a0exceeds\u00a0those\u00a0of\u00a0the\u00a0proposed\u00a0model.\u00a0However,\u00a0these\u00a0 differences\u00a0cannot\u00a0be\u00a0considered\u00a0significant\u00a0as\u00a0they\u00a0are\u00a0marginal,\u00a0while\u00a0on\u00a0the\u00a0other\u00a0hand,\u00a0\n-0.7\n-0.5\n-0.3\n-0.1\n0.1\n0.3\n0.5\n0.7\n09:00 11:00 13:00 15:00 17:00 19:00 21:00\nM ixe\nd Lo\nad (n\nor m\nal ize\nd) Real Predictions\n09:00 11:00 13:00 15:00 17:00 19:00 21:00\nBe st\nsu\nbm od\nel SR RF\nSVR LR RBF MLP\n-0.7\n-0.5\n-0.3\n-0.1\n0.1\n0.3\n0.5\n0.7\n09:00 11:00 13:00 15:00 17:00 19:00 21:00\nM ixe\nd Lo\nad (n\nor m\nal ize\nd) Real Predictions\n09:00 11:00 13:00 15:00 17:00 19:00 21:00\nBe st\nsu\nbm od\nel SR RF\nSVR LR RBF MLP\n-0.7\n-0.5\n-0.3\n-0.1\n0.1\n0.3\n0.5\n0.7\n09:00 11:00 13:00 15:00 17:00 19:00 21:00\nM ixe\nd Lo\nad (n\nor m\nal ize\nd) Real Predictions\n09:00 11:00 13:00 15:00 17:00 19:00 21:00\nBe st\nsu\nbm od\nel SR RF\nSVR LR RBF MLP\n-0.7\n-0.5\n-0.3\n-0.1\n0.1\n0.3\n0.5\n0.7\n09:00 11:00 13:00 15:00 17:00 19:00 21:00\nM ixe\nd Lo\nad (n\nor m\nal ize\nd) Real Predictions\n09:00 11:00 13:00 15:00 17:00 19:00 21:00\nBe st\nsu\nbm od\nel SR RF\nSVR LR RBF MLP\nFigure 9. Results for a randomly selected 12-h window for (a) 15-min, (b) 1-h, (c) 2-h, (d) 3-h, (e) 6-h, and (f) 24-h ahead predictions. Subgraphs labeled 1 depict actual and predicted value results, whereas subgraphs labeled 2 depict the best submodel performance results."
        },
        {
            "heading": "5. Discussion",
            "text": "In the context of the case study, multiple experiments were conducted, and the results are explained and discussed here. At this point, we should point out that providing accurate predictions is indeed a challenging task due to both grid and data-related reasons. First, the system\u2019s expandability can be a limiting factor for the accuracy of future forecasts. At the same time, this is reinforced by inherent characteristics of the load time series, such as non-linearity and uncertainty. In the face of these challenges, the proposed method\nSensors 2023, 23, 5436 19 of 25\nseems to be quite effective, providing reliable predictions. From Figure 7a\u2013f, we can draw conclusions about the quality of predictions. When the prediction time horizon is too short (Figure 7a), the forecast error is distributed close to the diagonal line, which implies quite accurate predictions. While we are trying to increase the prediction horizon, the forecasts are getting less accurate (Figure 7b\u2013f), as obviously, the pairs of real and predicted values are scattered further from the ideal line. Looking at Table 2, we observe that the proposed model outmatches all individual submodels, and the competitive MLP model ensemble in terms of MAE, and R2 and RMSE. Moreover, this conclusion applies to all prediction time horizons. As the prediction horizon gets longer, the forecasting error increases, which is absolutely reasonable. The only exceptions are the R2 and RMSE values obtained by the MLP model ensemble for 2 h prediction horizon, which slightly exceeds those of the proposed model. However, these differences cannot be considered significant as they are marginal, while on the other hand, the corresponding value of the MAE index clearly favors the proposed method. A result worth mentioning is the improvement of the multi-model performance over the current best sub-model that occurs in most cases while the horizon is getting longer. More specifically, the reduction of MAE that the proposed approach achieves over the best of the individual models ranges from 0.03411 to 0.3156. Such an improvement in performance could be partly explained by the occurrence of uncertainty in the load time series. As the prediction horizon is getting longer, the level of uncertainty is also increased, which is better addressed by the ensemble model than each individual submodel alone. Regarding the efficiency of the individual models of the pool, the results of MAE, RMSE, and R2 show that there is not just one model to prevail over the others in all cases. For the shorter prediction horizons and, more specifically, up to 3 h, LR and SR appear to achieve marginally smaller forecasting errors than their non-linear counterparts. Although the non-linearities are an intrinsic characteristic of mixed load [107], this behavior becomes more apparent as the prediction horizon is getting longer. As a result, models which are based on LR are able to provide robust results for very short-term forecasts. On the other hand, one major advantage of neural networks is their capability of modelling non-linear systems. An important observation is that neural networks appear to perform better for longer prediction horizons, and this can be attributed to the fact that, as the prediction horizon is getting longer, the non-linear properties of the load are becoming more dominant. Therefore, when predictions for longer horizons are required, MLP neural networks take the lead. However, the same does not apply to RBF networks. As stated above, in order for RBF networks to perform well, dense and suitable data are required. Consequently, their performance is reduced for 24-h prediction horizons, where the input information is poorer due to the resampling process. Although the remaining models of the pool, SVR and RF networks, present a moderate predictive capability, they contribute positively to the overall performance of the proposed model. This conclusion confirms our claim of the need to use multiple models in order to enhance the reliability of load predictions. Looking at the results of actual and predicted values in Figure 9 we confirm that, as the time horizon increases, accurate load forecasting becomes more and more difficult. Continuing with the subfigures of Figure 9 that show the alternation between submodels in order to maintain the accuracy of predictions, we conclude that the weighting mechanism of the proposed model seems to perform adequately regardless of the time horizon. It can easily be seen that quite reliable forecasts are obtained during the steady rise or fall of the actual AP values. On the contrary, predictions become less accurate when the AP presents great fluctuation. Several quite interesting conclusions can also be drawn from the pie charts in Figure 8. Each percentage in the pies represents the degree to which the respective model yielded the highest weight or equivalently the lowest MAE. The highest percentages of the first rank (above 18%) belong to MLP, RBF, and RF, and this applies for all horizons except that of 24 h, where SVR takes the place of RF. RF, in particular, scores lower MAE most of the time when the prediction horizon does not exceed 3 h. Beyond that point, RBF neural networks\nSensors 2023, 23, 5436 20 of 25\noutperform the rest of the submodels. An interesting observation is that the aforementioned models have equally high percentages in the sixth rank. Thus, these methods either achieve very good or poor performance. This observation is quite significant and strongly enhances the usefulness and effectiveness of our proposed method. The percentages of the rest of the pool models are, in most cases, divided into the intermediate rankings, with the exception of the high percentage of SVR in the sixth rank for the 6-h horizon."
        },
        {
            "heading": "6. Conclusions and Prospects",
            "text": "Achieving reliable electric load forecasts is of paramount importance for the smooth operation of electric power grids. However, the intrinsic volatility of the electric load makes its prediction particularly hard. Therefore, we assume that the load behavior is influenced by multiple input variables, which differ depending on the data to be predicted. The mixed load forecasting task has been addressed by a variety of machine-learning methods, and it has been observed that none is able to provide equally accurate results for any testing dataset. In the present study, a mixed power-load forecasting model is introduced, which employs the predictions coming from several individual models, namely MLP and RBF neural networks, LR, SVR, RF, and SR. These forecasts are weighted based on how accurate they have been and then added to calculate the final forecast value. The proposed model provides predictions for different time horizons, spanning from 15 min to 24 h. The extended results presented using real data sensed from a high voltage/medium voltage substation show the superiority of this novel approach compared to all the individual models as well as an MLP model ensemble for every prediction horizon tested. Thus, the proposed multi-model forecasting scheme constitutes a powerful method capable of greatly enhancing the operation of the modern electricity grid, with potential practical applications in network planning, operation, and management. It should be noted here that a limitation of the present study is that it did not involve predictions for long-term horizons. Although investigating longer prediction horizons is outside the scope of this work, we believe the proposed model ensemble could serve as the basis for designing such a tool. On the other hand, it is quite probable that a different set of input variables, presenting higher correlation with the long-term evolution of the mixed load would be needed in this case. Driven by the remarkable performance of the proposed methodology in mixed load forecasting, its application could be extended to other critical sectors of the smart grid, such as forecasting the electricity price and the production from RES, in order to more efficiently schedule conventional sources. A fruitful application would also be to forecast the residential demand or the aggregated load corresponding to several substations. Another promising direction for future research towards this direction includes the integration of Graph Neural Networks, which have been proved to be a promising candidate due to their ability to successfully interpret spatiotemporal features of the input data.\nAuthor Contributions: Conceptualization, E.N.Z., N.-A.I.L. and A.A.; methodology, N.G., M.P., E.N.Z., N.-A.I.L. and A.A.; software, N.G. and M.P.; validation, N.G., M.P., M.S. and N.-A.I.L.; formal analysis, E.N.Z. and A.A.; investigation, N.G., M.P. and M.S.; resources, N.-A.I.L., E.N.Z. and A.A.; data curation, N.G. and M.P.; writing\u2014original draft preparation, N.G., M.P., M.S., E.N.Z., N.-A.I.L. and A.A.; writing\u2014review and editing, N.G., M.P., M.S., E.N.Z., N.-A.I.L. and A.A.; visualization, N.G. and M.S.; supervision, E.N.Z. and A.A.; project administration, A.A.; funding acquisition, N.-A.I.L. and A.A. All authors have read and agreed to the published version of the manuscript.\nFunding: This research has been co-financed by the European Union and Greek national funds through the Operational Program Competitiveness, Entrepreneurship and Innovation, under the call RESEARCH\u2014CREATE\u2014INNOVATE (project code: T1EDK-00244).\nInstitutional Review Board Statement: Not applicable.\nInformed Consent Statement: Not applicable.\nData Availability Statement: The data are not publicly available due to confidentiality and privacy reasons.\nSensors 2023, 23, 5436 21 of 25\nConflicts of Interest: The authors declare no conflict of interest.\nNomenclature\nAP active power LR linear regression ML Machine-learning MAE mean absolute error MLP multi-layer perceptron NN neural network RBF radial basis function RF random forests RES renewable energy sources SBL sparse Bayesian learning SR sparse regression SVR support vector regression"
        }
    ],
    "title": "A Machine Learning Model Ensemble for Mixed Power Load Forecasting across Multiple Time Horizons",
    "year": 2023
}