{
    "abstractText": "We present an interactive music AI system that enables users to co-create expressive performances of notated music using speech and gestures. The system provides multi-modal interactive dialog-based control of performance rendering via smartphones and is accessible to people regardless of their musical background. We train a deep learning music performance rendering model on sheet music and associated performances with notated performance directions and user-system interaction data. Users have the opportunity to actively participate in the performance process. A speechand gesture-based feedback loop with interactive learning improve the accuracy of performance rendering control. We believe that many people can express aspects of music performance using natural human expressions such as speech, voice, and gestures, and that by hearing the music follow their communicated intent, they can achieve deeper immersion and enjoyment of music than otherwise possible. With this work we pursue the goal of developing novel, fulfilling, and accessible music making experiences for large numbers of people who are not currently musically active.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ilya BOROVIK"
        },
        {
            "affiliations": [],
            "name": "Vladimir VIRO"
        }
    ],
    "id": "SP:75f227efceb1ac039b22e557c1c2589aebc65b74",
    "references": [
        {
            "authors": [
                "M Gillies",
                "R Fiebrink",
                "A Tanaka",
                "J Garcia",
                "F Bevilacqua",
                "A Heloir"
            ],
            "title": "Human-Centred Machine Learning",
            "venue": "Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems;",
            "year": 2016
        },
        {
            "authors": [
                "T Kaluarachchi",
                "A Reis",
                "S. Nanayakkara"
            ],
            "title": "A Review of Recent Deep Learning Approaches in Human- Centered Machine Learning. Sensors",
            "venue": "https://www.mdpi.com/",
            "year": 2021
        },
        {
            "authors": [
                "E Mosqueira-Rey",
                "E Hern\u00e1ndez-Pereira",
                "D Alonso-R\u0131\u0301os",
                "J Bobes-Bascar\u00e1n",
                "\u00c1. Fern\u00e1ndez-Leal"
            ],
            "title": "Humanin-the-loop machine learning: a state of the art",
            "venue": "Artificial Intelligence Review",
            "year": 2022
        },
        {
            "authors": [
                "J. Rink"
            ],
            "title": "Musical Performance: A Guide to Understanding",
            "year": 2002
        },
        {
            "authors": [
                "de Mantaras RL",
                "Arcos JL"
            ],
            "title": "AI and Music: From Composition to Expressive Performance",
            "venue": "AI Magazine",
            "year": 2002
        },
        {
            "authors": [
                "Kirke A",
                "Miranda ER"
            ],
            "title": "Guide to Computing for Expressive Music Performance",
            "year": 2013
        },
        {
            "authors": [
                "C Hernandez-Olivan",
                "J Hernandez-Olivan",
                "JR. Beltran"
            ],
            "title": "A Survey on Artificial Intelligence for Music Generation: Agents, Domains and Perspectives",
            "venue": "arXiv preprint arXiv:2210.13944",
            "year": 2022
        },
        {
            "authors": [
                "S Dadman",
                "BA Bremdal",
                "B Bang",
                "R. Dalmo"
            ],
            "title": "Toward Interactive Music Generation: A Position Paper",
            "venue": "IEEE Access",
            "year": 2022
        },
        {
            "authors": [
                "T Cochrane",
                "B Fantini",
                "KR. Scherer"
            ],
            "title": "The Emotional Power of Music: Multidisciplinary perspectives on musical arousal, expression, and social control",
            "year": 2013
        },
        {
            "authors": [
                "L. Pipe"
            ],
            "title": "The role of gesture and non-verbal communication in popular music performance, and its application to curriculum and pedagogy [Doctoral thesis",
            "venue": "University of West London;",
            "year": 2018
        },
        {
            "authors": [
                "Kelly SN"
            ],
            "title": "Using Conducting Gestures to Teach Music Concepts A Review of Research",
            "venue": "Update: Applications of Research in Music Education",
            "year": 1999
        },
        {
            "authors": [
                "R Dannenberg",
                "D Siewiorek",
                "N. Zahler"
            ],
            "title": "Exploring Meaning And Intention In Music Conducting",
            "venue": "Proceedings of the International Computer Music Conference,",
            "year": 2010
        },
        {
            "authors": [
                "A Vaswani",
                "N Shazeer",
                "N Parmar",
                "J Uszkoreit",
                "L Jones",
                "AN Gomez"
            ],
            "title": "Attention is All you Need",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2017
        },
        {
            "authors": [
                "DP Kingma",
                "M. Welling"
            ],
            "title": "Auto-Encoding Variational Bayes",
            "venue": "arXiv preprint arXiv:1312.6114",
            "year": 2013
        },
        {
            "authors": [
                "S Zhao",
                "J Song",
                "S. Ermon"
            ],
            "title": "InfoVAE: Information Maximizing Variational Autoencoders",
            "venue": "arXiv preprint arXiv:1706.02262",
            "year": 2017
        },
        {
            "authors": [
                "T Brown",
                "B Mann",
                "N Ryder",
                "M Subbiah",
                "JD Kaplan",
                "P Dhariwal"
            ],
            "title": "Language Models are Few- Shot Learners",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2020
        },
        {
            "authors": [
                "A Radford",
                "JW Kim",
                "C Hallacy",
                "A Ramesh",
                "G Goh",
                "S Agarwal"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "Proceedings of the International conference on machine learning",
            "year": 2021
        },
        {
            "authors": [
                "A Radford",
                "JW Kim",
                "T Xu",
                "G Brockman",
                "C McLeavey",
                "I. Sutskever"
            ],
            "title": "Robust Speech Recognition via Large-Scale Weak Supervision",
            "venue": "arXiv preprint arXiv:2212.04356",
            "year": 2022
        },
        {
            "authors": [
                "CJ Cai",
                "S Winter",
                "D Steiner",
                "L Wilcox",
                "M. Terry"
            ],
            "title": "Hello AI\u201d: Uncovering the Onboarding Needs of Medical Practitioners for Human\u2013AI Collaborative Decision-Making",
            "venue": "Proceedings of the ACM on Humancomputer Interaction",
            "year": 2019
        },
        {
            "authors": [
                "E Jensen",
                "M Dale",
                "PJ Donnelly",
                "C Stone",
                "S Kelly",
                "A Godley"
            ],
            "title": "Toward Automated Feedback on Teacher Discourse to Enhance Teacher Learning",
            "venue": "Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems;",
            "year": 2020
        },
        {
            "authors": [
                "A Ramesh",
                "P Dhariwal",
                "A Nichol",
                "C Chu",
                "M. Chen"
            ],
            "title": "Hierarchical Text-Conditional Image Generation with CLIP Latents",
            "venue": "arXiv preprint arXiv:2204.06125",
            "year": 2022
        },
        {
            "authors": [
                "M Chen",
                "J Tworek",
                "H Jun",
                "Q Yuan",
                "HPdO Pinto",
                "J Kaplan"
            ],
            "title": "Evaluating Large Language Models Trained on Code",
            "venue": "arXiv preprint arXiv:2107.03374",
            "year": 2021
        },
        {
            "authors": [
                "L Ouyang",
                "J Wu",
                "X Jiang",
                "D Almeida",
                "CL Wainwright",
                "P Mishkin"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "arXiv preprint arXiv:2203.02155",
            "year": 2022
        },
        {
            "authors": [
                "R Fiebrink",
                "D Trueman",
                "PR. Cook"
            ],
            "title": "A Meta-Instrument for Interactive, On-the-Fly Machine Learning",
            "venue": "Proceedings of the International Conference on New Interfaces for Musical Expression;",
            "year": 2009
        },
        {
            "authors": [
                "N\u00e6ss TR",
                "Martin CP"
            ],
            "title": "A Physical Intelligent Instrument using Recurrent Neural Networks",
            "venue": "Proceedings of the International Conference on New Interfaces for Musical Expression",
            "year": 2019
        },
        {
            "authors": [
                "CZA Huang",
                "C Hawthorne",
                "A Roberts",
                "M Dinculescu",
                "J Wexler",
                "L Hong"
            ],
            "title": "The Bach Doodle: Approachable music composition with machine learning at scale",
            "year": 1907
        },
        {
            "authors": [
                "R Louie",
                "A Coenen",
                "CZ Huang",
                "M Terry",
                "CJ. Cai"
            ],
            "title": "Novice-AI Music Co-Creation via AI-Steering Tools for Deep Generative Models",
            "venue": "Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems",
            "year": 2020
        },
        {
            "authors": [
                "Y Zhang",
                "G Xia",
                "M Levy",
                "S. Dixon"
            ],
            "title": "COSMIC: A Conversational Interface for Human-AI Music Co- Creation",
            "venue": "Proceedings of the International Conference on New Interfaces for Musical Expression. Shanghai,",
            "year": 2021
        },
        {
            "authors": [
                "CZA Huang",
                "A Vaswani",
                "J Uszkoreit",
                "N Shazeer",
                "C Hawthorne",
                "AM Dai"
            ],
            "title": "Music Transformer: Generating Music with Long-Term Structure",
            "venue": "arXiv preprint arXiv:1809.04281",
            "year": 2018
        },
        {
            "authors": [
                "K Choi",
                "C Hawthorne",
                "I Simon",
                "M Dinculescu",
                "J. Engel"
            ],
            "title": "Encoding Musical Style with Transformer Autoencoders",
            "year": 1912
        },
        {
            "authors": [
                "B Yu",
                "P Lu",
                "R Wang",
                "W Hu",
                "X Tan",
                "W Ye"
            ],
            "title": "Museformer: Transformer with Fine-and Coarse-Grained Attention for Music Generation",
            "venue": "arXiv preprint arXiv:2210.10349",
            "year": 2022
        },
        {
            "authors": [
                "A Roberts",
                "J Engel",
                "C Raffel",
                "C Hawthorne",
                "D. Eck"
            ],
            "title": "A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music",
            "venue": "Proceedings of the International Conference on Machine Learning",
            "year": 2018
        },
        {
            "authors": [
                "G Brunner",
                "A Konrad",
                "Y Wang",
                "R. Wattenhofer"
            ],
            "title": "MIDI-VAE: Modeling Dynamics and Instrumenta- tion of Music with Applications to Style Transfer",
            "venue": "arXiv preprint arXiv:1809.07600",
            "year": 2018
        },
        {
            "authors": [
                "Wu SL",
                "Yang YH"
            ],
            "title": "MuseMorphose: Full-Song and Fine-Grained Piano Music Style Transfer with One Transformer VAE",
            "venue": "arXiv preprint arXiv:2105.04090",
            "year": 2021
        },
        {
            "authors": [
                "D von R\u00fctte",
                "L Biggio",
                "Y Kilcher",
                "T. Hoffman"
            ],
            "title": "FIGARO: Generating Symbolic Music with Fine-Grained Artistic Control",
            "venue": "arXiv preprint arXiv:2201.10936",
            "year": 2022
        },
        {
            "authors": [
                "A Agostinelli",
                "TI Denk",
                "Z Borsos",
                "J Engel",
                "M Verzetti",
                "A Caillon"
            ],
            "title": "MusicLM: Generating Music From Text",
            "venue": "arXiv preprint arXiv:2205.05448",
            "year": 2023
        },
        {
            "authors": [
                "CE Cancino-Chac\u00f3n",
                "M Grachten",
                "W Goebl",
                "G. Widmer"
            ],
            "title": "Computational Models of Expressive Music Performance: A Comprehensive and Critical Review",
            "venue": "Frontiers in Digital Humanities",
            "year": 2018
        },
        {
            "authors": [
                "G Widmer",
                "W. Goebl"
            ],
            "title": "Computational Models of Expressive Music Performance: The State of the Art",
            "venue": "Journal of New Music Research",
            "year": 2004
        },
        {
            "authors": [
                "A Friberg",
                "R Bresin",
                "J. Sundberg"
            ],
            "title": "Overview of the KTH rule system for musical performance",
            "venue": "Advances in cognitive psychology",
            "year": 2006
        },
        {
            "authors": [
                "D Jeong",
                "T Kwon",
                "Y Kim",
                "K Lee",
                "J. Nam"
            ],
            "title": "VirtuosoNet: A Hierarchical RNN-based System for Modeling Expressive Piano Performance",
            "venue": "Proceedings of the 20th International Society for Music Information Retrieval Conference;",
            "year": 2019
        },
        {
            "authors": [
                "A Maezawa",
                "K Yamamoto",
                "T. Fujishima"
            ],
            "title": "Rendering Music Performance With Interpretation Variations Using Conditional Variational RNN",
            "venue": "Proceedings of the 20th International Society for Music Information Retrieval Conference;",
            "year": 2019
        },
        {
            "authors": [
                "S Rhyu",
                "S Kim",
                "K. Lee"
            ],
            "title": "Sketching the Expression: Flexible Rendering of Expressive Piano Performance with Self-Supervised Learning",
            "venue": "arXiv preprint arXiv:2208.14867",
            "year": 2022
        },
        {
            "authors": [
                "H Salehinejad",
                "J Baarbe",
                "S Sankar",
                "J Barfett",
                "E Colak",
                "S. Valaee"
            ],
            "title": "Recent Advances in Recurrent Neural I",
            "venue": "Borovik and V. Viro / Co-Performing Music with AI 349 Networks. arXiv preprint",
            "year": 2017
        },
        {
            "authors": [
                "A Gretton",
                "K Borgwardt",
                "M Rasch",
                "B Sch\u00f6lkopf",
                "A. Smola"
            ],
            "title": "A Kernel Method for the Two-Sample- Problem",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2006
        },
        {
            "authors": [
                "F Foscarin",
                "A Mcleod",
                "P Rigaux",
                "F Jacquemard",
                "M. Sakai"
            ],
            "title": "ASAP: a Dataset of Aligned Scores and Performances for Piano Transcription",
            "venue": "Proceedings of the 21st International Society for Music Information Retrieval Conference;",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "Keywords. expressive music performance, human-computer interaction, mobile interface, deep learning, interactive learning"
        },
        {
            "heading": "1. Introduction",
            "text": "The development of artificial intelligence and deep learning models has led to the creation of a new paradigm of human-centered machine learning, which aims to improve user experience and enhance human capabilities in various domains [1,2,3]. Music performance is an art form that requires expertise, practice, and physical ability [4,5]. The traditional paradigm of music interpretation and performance, where the musician interprets a score and translates the intended expression into the control of the musical instrument, can be difficult for those without musical training. Artificial intelligence and machine learning offer new approaches to music creation and performance [6,7], and interactive music creation systems have become increasingly popular [8,9].\nIn this work, we present a system for interactive co-creation of expressive performances of notated music using speech and gestures, which provides multi-modal inter-\n1Corresponding Author: ilya.borovik@skoltech.ru 2Corresponding Author: vladimir@peachnote.de\nactive dialog-based control of performance rendering via smartphones and is accessible to people regardless of their musical background. Our system is designed to allow people to actively participate in the performance process by using their natural human expressions such as speech, voice, and gestures to control the performance of existing music. By hearing the music follow their communicated intent, they can achieve a deeper immersion and enjoyment of music than otherwise possible [10,11].\nTo accomplish this, we draw inspiration from the practice of music conducting, in which musicians translate the score and the conductor\u2019s gestures, facial expressions, and speech direction into music performance [12,13]. We focus on performance rendering for notated music, which provides written performance direction markings that composers use to communicate aspects of intended articulation to the musicians. We use existing music performances to ground these labels in music performance practices and learn their interpretations through a deep learning model to intuitively control performances. Our system uses state-of-the-art transformers for modeling sequential musical patterns [14] and variational autoencoders [15,16] that operate at different levels of the music hierarchy to advance research in expressive music performance rendering. Building on recent advances in multi-modal representation learning [17,18,19], we link user expression data in multiple modalities with music performance features and provide real-time or near real-time interactive music co-performance.\nThis paper presents the ongoing development of a deep learning based system for the interactive co-creation of expressive music performances for written music with:\n1. real-time interactive music performance rendering; 2. human expression as performance control modalities; 3. accessibility to people without professional musical training and background; 4. maximum accessibility through inexpensive smartphone devices.\nOur goal is to provide a new kind of fulfilling, engaging, and accessible music making experience, allowing people to perform great musical works using natural human expression. Our main contributions:\n1. We develop a method to interactively participate in and control music performance rendering in real-time using speech and facial expressions;\n2. We employ transformer models for controllable expressive music performance rendering and build interpretable connections between learned performance embeddings and notated music performance directions;\n3. We implement a mobile web interface with low hardware requirements for interactive co-creation of music performances.\nThe paper is organized as follows. In Section 2, we present related work on interactive and deep learning-based music performance creation systems and how our work relates to them. Then, in Section 3, we present the design of our system with a description of its main parts: the music performance model, the interaction backend, and the mobile web application. Section 4 describes the examples of interactivity implemented in the system. Finally, we outline future work in Section 5 and make a conclusion in Section 6."
        },
        {
            "heading": "2. Related Work",
            "text": ""
        },
        {
            "heading": "2.1. Human-Centered Machine Learning",
            "text": "Human-centered machine learning is an actively emerging research field that explores the methods of aligning machine learning systems with human needs to make humans more effective and efficient [1,2,3]. The applications include medicine [20], education [21], music [9], art [22], software development [23], interactive and assistive technologies [24]. Recent work on tuning large-scale language models to build human-friendly assistants shows the great potential of using deep learning as a human companion in everyday tasks [24]. Our primary goal is to make the music-making experience accessible to more people through computational models. We develop a deep learning model with the focus on interactive inference and fine-tuning based on user feedback."
        },
        {
            "heading": "2.2. Interactive Music Performance",
            "text": "Interactive music performance systems contribute to the emerging field of humancentered machine learning [2,9]. They introduce new instruments for musical expression [25,26,27] and interfaces for controlling a generative model [28,29,30]. Wekinator [25] is a user-friendly computer application that learns to map camera-scanned sample input, such as gestures or facial expressions, to specific music performance actions. Piano Genie [27] is a machine learning controller that allows non-musicians to improvise on the piano. CoCoCo [29] provides multi-example sampling with revision and AI steering tools to control the diversity and high-level directions of a generative model. COSMIC [30] provides a novel way to create music through a textual dialog system. Our work follows the Wekinator approach and focuses on expressive performance rendering for a fixed score and accessibility through a multi-modal mobile web interface."
        },
        {
            "heading": "2.3. Music Generation with Deep Learning",
            "text": "Deep learning is commonly used for music generation [8]. The most popular architectures are transformers for learning long-term sequential musical patterns [31,32,33] and variational autoencoders for unsupervised style encoding and control [34,35,36,37]. The models provide offline control over performance style [35,32] or performance parameters [36,37]. Recent works aim to generate music from descriptions [37] and text [38], as an intuitive way for humans to express themselves musically. Our work builds on these advances, focusing on real-time and interactive music performance generation that allows humans to participate directly in the creative process."
        },
        {
            "heading": "2.4. Expressive Music Performance Rendering",
            "text": "Expressive music performance models render performances for written musical scores [7,39], either through rule-based [40,41] or machine learning approaches [42,43,44]. The latter mainly consist of variational autoencoders [15] for performance style encoding and control, and recurrent neural networks [45] for expressive performance representation. Our music performance model uses a transformer architecture [14] to improve long-term music dependency modeling and maps learned style representations to human expressive control inputs. In addition, in contrast to offline control of performance generation, we aim for interactive controllable generation and model fine-tuning."
        },
        {
            "heading": "3. Interactive Music Performance Rendering System",
            "text": "We develop an interactive learning system for real-time co-creation of expressive music performances, shown in Figure 1. Its main components are:\n1. Music Performance Model; 2. Interaction Backend; 3. Mobile Web Application.\nThe Music Performance Model enables the controllable creation of expressive performances for written music. The model learns from examples of human performance and is fine-tuned with user feedback to provide human-like musical expressiveness satisfying user requests. By automatically performing the written notes, the model removes the need for a user to play a musical instrument in order to perform a piece of music.\nThe Interaction Backend and Mobile Application connect users to the computational music performance model and allow interactive manipulation of the performance. By offering real-time performance rendering, we provide users with online interaction and immediate response. The speech and gestures allow users to express creative ideas using intuitive concepts such as text and emotion.\nThe following subsections describe the technical details behind the performance rendering model, interaction backend and mobile application."
        },
        {
            "heading": "3.1. Music Performance Model",
            "text": "Music Performance Model is a deep learning model for controllable expressive music performance rendering illustrated in Figure 2. It combines transformers [14] for sequential data modeling and variational autoencoders [15] for encoding performance style at different levels of the musical hierarchy. The building blocks are: performance encoder, performance decoder, and performance direction classifier.\nPerformance Encoder computes performance style representations at the note, beat, and bar levels. The transformer encoder [14] takes a sequence of score and performance features as input and outputs an embedding for each note. The embeddings are\naveraged over bars, beats, and onsets (chords, or notes at the same position) and passed through a linear layer to compute latent bar-, beat-, and onset-level performance style embeddings, optimized using maximum mean discrepancy objective [16,46].\nPerformance Decoder operates with the score features (notes to play), the previous performance context (performance history), and the combined multi-level performance style embeddings computed by the Performance Encoder (style input). The decoder is a decoder transformer model autoregressively predicting the expressive performance features of the currently played note. The model is trained by maximizing the likelihood of the performance features.\nPerformance Direction Classifier learns an association between the performance embeddings and performance directions written in musical scores. It aims to provide an intuitive interpretation of the learned control space to be used to control the performance generation. The Direction Classifier classifies a local context of bar, beat and onset embeddings into performance direction classes:\n\u2022 dynamic: degrees of piano and forte; \u2022 dynamic changes: crescendo and diminuendo; \u2022 tempo: adagio, largo, presto, etc.; \u2022 tempo changes: accelerando, ritardando, a tempo, etc.; \u2022 articulations: legato, staccato, fermata, etc.\nThe classifier predicts the likelihood of a direction being performed in a given performance context. Differences between embeddings with high and low likelihoods provide a direction for moving the generation toward a specified performance marking. We map these quantified per-direction embedding differences to natural language commands such as \u201cplay more piano here\u201d or \u201cswitch to largo\u201d to control performance rendering. This interaction is implemented in the interaction backend and application interface.\nFor model training, we preprocess the MIDI files from the ASAP dataset of aligned scores and piano performances [47]. The score features are note pitch, duration, bar, position in bar, time signature, and inter-onset position shift. The performance features are local performance tempo, note onset deviation, performed duration, and velocity.\nDuring inference, the randomly sampled or modified existing performance embeddings can be used to generate and control music performances. Since the embedding space is optimized with the decoder performance generation objective, the latent space encodes features relevant for performance reconstruction. By connecting the classifier interpretation of the performance embedding with natural language instructions, we enable an intuitive controllable music performance rendering.\nThe initial model is trained isolated from the target user and generated music performances might be far from the user needs. To improve the quality of the model, we fine-tune it on the user-model interaction data. Currently, the active learning framework involves the offline periodic fine-tuning of the model on feedback scores. Given a set of performance-feedback score pairs, the performance decoder is optimized with an additional loss function maximizing the positive feedback per input performance sequence. In the future, we plan to implement fully interactive and online model updates."
        },
        {
            "heading": "3.2. Interaction Backend",
            "text": "Interaction Backend comprises multiple micro-services responsible for different tasks, such as handling the client connection, audio transcription, video analysis, audio rendering, etc. Services are implemented in different languages (Python, C++ and Go) and may be restarted, updated and rolled back independently. They all communicate via a messaging bus. A database stores scores, past performances, user feedback and performance directions which we use to optimize the performance model.\nThe JavaScript client connects to a multi-user WebRTC backend and establishes a bidirectional data channel and audio stream as well as a video stream from the client. Audio and video streams are analyzed in real-time. Audio is transcribed to text using Whisper [19], which is forwarded to GPT-3 [17] for intent extraction. The intent extractions works with input in multiple languages.\nCurrently, the system is sequencing and rendering MIDI performances to audio. In the future, we plan to generate audio directly through a deep learning model. The MIDI sequencer gets its cues from the gesture and intent recognition services and renders MIDI performances live. The MIDI stream is sent to the audio rendering node. Its audio output is sent back to the WebRTC server process that handles the client connection, and from there the music audio is sent back to the web interface and the user."
        },
        {
            "heading": "3.3. Mobile Web Application",
            "text": "Mobile Web Application3 connects users to the interaction backend and the deep learning based music performance rendering system. The application requires a smartphone with a stable internet connection, camera and voice recorder. The web interface welcomes users and prompts them to turn on their camera and microphone to start interactive communication with the music performance model. Once the permissions are granted and the WebRTC connection is established, the user sees the camera image in the top half of the screen and the interaction button in the bottom half. The button allows the user to indicate that the system should pay attention to their input: audio, or video. The next section provides an overview of the mechanics of interacting with the system.\n3Demo: https://d3dbzxyywswxzm.cloudfront.net."
        },
        {
            "heading": "4. Interaction with the System",
            "text": "Our application uses two primary intuitive interaction modalities which can be combined to create a highly expressive and dynamic musical performance:\n1. Speech: the system analyzes the input audio stream and recognizes speechspecific phrases. The text embeddings of speech transcription are mapped to performance direction control embeddings as described in Section 3.1. 2. Gestures: the system processes the video stream and extracts facial expressions. The expression embeddings are mapped to performance direction classes and pre-defined user-system actions.\nThe web interface greets the user and prompts them to turn on their camera and microphone to begin interacting with the music performance model. Once permissions are granted, the user sees the camera image in the top half of the screen and the interaction button in the middle of the bottom half. Initially, the system selects a random musical composition from the database and begins rendering an arbitrary performance for this written music. The user can press the button and ask the system to do any of the following within a single phrase:\n1. select a composition - \u201cLet\u2019s play Chopin\u2019s Mazurka in D major\u201d 2. pause or stop the performance - \u201cPlease stop\u201d 3. navigate to a different place in the score - \u201cLet\u2019s play again from the beginning\u201d 4. provide feedback on the current performance - \u201cThat was still a bit too slow and too much staccato\u201d 5. ask the system to play in a particular way - \u201cCould you play this like a mother singing a lullaby to her child?\u201d 6. show the system non-verbally how to play using facial gestures, for example making a blissful expression.\nThe interaction button relieves us of the need to continuously evaluate user input and judge whether it is intentional or accidental (the user does not intend to direct the performance, but moves or says something). While the button is pressed, the system continuously evaluates the video input and applies the analysis results to the performance. Audio input is not evaluated until the button is released. However, if a voice activity detector (VAD) detects speech, we immediately lower the volume of the performance to let the user know that we are listening and to make it easier to understand the speech.\nThe information we are looking for, such as navigation directions, feedback on past performance, and directions for future performance, is extracted from the transcribed speech using GPT-3. This allows us to successfully process free-form speech in multiple languages and provides great flexibility during development, at a cost in reliability and latency that we are currently willing to accept.\nThe interaction data and feedback are stored in the database for tuning the music performance model in subsequent iterations. Specifically, the backend stores the compressed video frame representations, the verbal commands and their embeddings, and the rendered performances. These features are then used to fine-tune the performance rendering model according to the desired input control as discussed in Section 3.1."
        },
        {
            "heading": "5. Future Work",
            "text": "As discussed throughout the paper, our main goal is to make musical expression accessible to people with no prior musical experience by designing simple interfaces and machine learning models. Currently, we support limited modalities for controlling music performance, namely speech and gesture. Our long-term goal is to integrate all forms of human expression used in musical contexts, such as conducting, teaching, and playing together, into our system. For example, using vocalization and full-body gestures to enhance interaction, allowing the user to modulate tempo, dynamics, and articulations.\nAnother important aspect of the human-computer interaction paradigm is personalization, the alignment of generated results with human requests and wishes. Understanding user intent, whether or not they provide control input, whether they are engaged in following the music rather than trying to direct its performance, is critical. We will study how different people describe and demonstrate music in relation to their expectations of the system\u2019s behavior. Our goal is to provide a personalized experience for each user, while collecting music descriptors that will help improve the system over time. Extensive human evaluation of the system is an important part of future research.\nRegarding the technical solution, there are several points to consider. Currently, there are rare glitches in the rendered music performances coming from the trained deep learning model. We plan to solve them by collecting more data and using feedback to tune the models. Other shortcomings include support for piano music only and no control over the acoustic sound properties of the music, as the backend synthesizes rendered MIDI performances. We plan to perform music performance rendering in the audio domain using a deep learning model. Finally, we intend to improve the user interface to make it more user-friendly, robust, and inviting while keeping it simple. Various visualization options will complement and frame the musical functionality of the application."
        },
        {
            "heading": "6. Conclusion",
            "text": "In this work, we have presented a user-friendly interactive system for the co-creation of expressive music performances using speech and gestures. Through a mobile web application, it allows users to interact with an autonomous deep learning-based performance rendering model in real time. Our approach incorporates both verbal and non-verbal human expressive capabilities, allowing individuals to project emotions and affects through music using natural expressive language. This makes our system accessible to people without musical training or the ability to play musical instruments, and makes complex musical works more widely available for performance and interpretation.\nWe believe that this work will contribute to the fields of human-computer interaction, human-centered machine learning, and interactive music creation and performance, and will allow a greater number of people to experience the joy of musical expression. We hope that the system can be used in educational contexts to make musical tradition and practice more accessible, tangible, and engaging for young people. Since the web application does not require any setup on the part of the user, our system is easy to try out. If it produces interesting results right away, it has a chance of being used by many people. In future work, we will continue to incorporate different ways of interacting with the system to provide a complete, intuitive, and accessible musical experiences."
        }
    ],
    "title": "Co-Performing Music with AI: Real-Time Performance Control Using Speech and Gestures",
    "year": 2023
}