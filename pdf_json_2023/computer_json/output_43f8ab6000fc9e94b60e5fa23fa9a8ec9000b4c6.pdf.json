{
    "abstractText": "Multimodal-driven talking face generation refers to animating a portrait with the given pose, expression, and gaze transferred from the driving image and video, or estimated from the text and audio. However, existing methods ignore the potential of text modal, and their generators mainly follow the source-oriented feature rearrange paradigm coupled with unstable GAN frameworks. In this work, we first represent the emotion in the text prompt, which could inherit rich semantics from the CLIP, allowing flexible and generalized emotion control. We further reorganize these tasks as the target-oriented texture transfer and adopt the Diffusion Models. More specifically, given a textured face as the source and the rendered face projected from the desired 3DMM coefficients as the target, our proposed Texture-Geometry-aware Diffusion Model decomposes the complex transfer problem into multi-conditional denoising process, where a Texture Attention-based module accurately models the correspondences between appearance and geometry cues contained in source and target conditions, and incorporate extra implicit information for high-fidelity talking face generation. Additionally, TGDM can be gracefully tailored for face swapping. We derive a novel paradigm free of unstable seesaw-style optimization, resulting in simple, stable, and effective training and inference schemes. Extensive experiments demonstrate the superiority of our method.",
    "authors": [
        {
            "affiliations": [],
            "name": "Chao Xu"
        },
        {
            "affiliations": [],
            "name": "Shaoting Zhu"
        },
        {
            "affiliations": [],
            "name": "Junwei Zhu"
        },
        {
            "affiliations": [],
            "name": "Tianxin Huang"
        },
        {
            "affiliations": [],
            "name": "Jiangning Zhang"
        },
        {
            "affiliations": [],
            "name": "Ying Tai"
        },
        {
            "affiliations": [],
            "name": "Yong Liu"
        }
    ],
    "id": "SP:80234c46c9588cb61dcec394fa4a2570ceb89b7a",
    "references": [
        {
            "authors": [
                "H. Zhou",
                "Y. Sun",
                "W. Wu",
                "C.C. Loy",
                "X. Wang",
                "Z. Liu"
            ],
            "title": "Posecontrollable talking face generation by implicitly modularized audiovisual representation",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2021, pp. 4176\u20134186. JOURNAL OF LTEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 12",
            "year": 2021
        },
        {
            "authors": [
                "B. Liang",
                "Y. Pan",
                "Z. Guo",
                "H. Zhou",
                "Z. Hong",
                "X. Han",
                "J. Han",
                "J. Liu",
                "E. Ding",
                "J. Wang"
            ],
            "title": "Expressive talking head generation with granular audio-visual control",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 3387\u20133396.",
            "year": 2022
        },
        {
            "authors": [
                "A. Siarohin",
                "S. Lathuili\u00e8re",
                "S. Tulyakov",
                "E. Ricci",
                "N. Sebe"
            ],
            "title": "First order motion model for image animation",
            "venue": "Advances in Neural Information Processing Systems, vol. 32, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "H. Kim",
                "Y. Choi",
                "J. Kim",
                "S. Yoo",
                "Y. Uh"
            ],
            "title": "Exploiting spatial dimensions of latent in gan for real-time image editing",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 852\u2013861.",
            "year": 2021
        },
        {
            "authors": [
                "X. Zeng",
                "Y. Pan",
                "M. Wang",
                "J. Zhang",
                "Y. Liu"
            ],
            "title": "Realistic face reenactment via self-supervised disentangling of identity and pose",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, no. 07, 2020, pp. 12 757\u201312 764.",
            "year": 2020
        },
        {
            "authors": [
                "E. Burkov",
                "I. Pasechnik",
                "A. Grigorev",
                "V. Lempitsky"
            ],
            "title": "Neural head reenactment with latent pose descriptors",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp. 13 786\u201313 795.",
            "year": 2020
        },
        {
            "authors": [
                "X. Huang",
                "S. Belongie"
            ],
            "title": "Arbitrary style transfer in real-time with adaptive instance normalization",
            "venue": "Proceedings of the IEEE international conference on computer vision, 2017, pp. 1501\u20131510.",
            "year": 2017
        },
        {
            "authors": [
                "Y. Ren",
                "G. Li",
                "Y. Chen",
                "T.H. Li",
                "S. Liu"
            ],
            "title": "Pirenderer: Controllable portrait image generation via semantic neural rendering",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 13 759\u201313 768.",
            "year": 2021
        },
        {
            "authors": [
                "J. Zhao",
                "H. Zhang"
            ],
            "title": "Thin-plate spline motion model for image animation",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 3657\u20133666.",
            "year": 2022
        },
        {
            "authors": [
                "J. Tao",
                "B. Wang",
                "B. Xu",
                "T. Ge",
                "Y. Jiang",
                "W. Li",
                "L. Duan"
            ],
            "title": "Structure-aware motion transfer with deformable anchor model",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 3637\u20133646.",
            "year": 2022
        },
        {
            "authors": [
                "X. Ji",
                "H. Zhou",
                "K. Wang",
                "Q. Wu",
                "W. Wu",
                "F. Xu",
                "X. Cao"
            ],
            "title": "Eamm: One-shot emotional talking face via audio-based emotion-aware motion model",
            "venue": "ACM SIGGRAPH 2022 Conference Proceedings, 2022, pp. 1\u201310.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Zhou",
                "X. Han",
                "E. Shechtman",
                "J. Echevarria",
                "E. Kalogerakis",
                "D. Li"
            ],
            "title": "Makelttalk: speaker-aware talking-head animation",
            "venue": "ACM Transactions On Graphics (TOG), vol. 39, no. 6, pp. 1\u201315, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "C. Zhang",
                "Y. Zhao",
                "Y. Huang",
                "M. Zeng",
                "S. Ni",
                "M. Budagavi",
                "X. Guo"
            ],
            "title": "Facial: Synthesizing dynamic talking face with implicit attribute learning",
            "venue": "Proceedings of the IEEE/CVF international conference on computer vision, 2021, pp. 3867\u20133876.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Guo",
                "K. Chen",
                "S. Liang",
                "Y.-J. Liu",
                "H. Bao",
                "J. Zhang"
            ],
            "title": "Adnerf: Audio driven neural radiance fields for talking head synthesis",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 5784\u20135794.",
            "year": 2021
        },
        {
            "authors": [
                "I. Goodfellow",
                "J. Pouget-Abadie",
                "M. Mirza",
                "B. Xu",
                "D. Warde-Farley",
                "S. Ozair",
                "A. Courville",
                "Y. Bengio"
            ],
            "title": "Generative adversarial networks",
            "venue": "Communications of the ACM, vol. 63, no. 11, pp. 139\u2013144, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "L. Li",
                "S. Wang",
                "Z. Zhang",
                "Y. Ding",
                "Y. Zheng",
                "X. Yu",
                "C. Fan"
            ],
            "title": "Writea-speaker: Text-based emotional and rhythmic talking-head generation",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, vol. 35, no. 3, 2021, pp. 1911\u20131920.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Deng",
                "J. Yang",
                "S. Xu",
                "D. Chen",
                "Y. Jia",
                "X. Tong"
            ],
            "title": "Accurate 3d face reconstruction with weakly-supervised learning: From single image to image set",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops, 2019, pp. 0\u20130.",
            "year": 2019
        },
        {
            "authors": [
                "D. Bigioi",
                "S. Basak",
                "H. Jordan",
                "R. McDonnell",
                "P. Corcoran"
            ],
            "title": "Speech driven video editing via an audio-conditioned diffusion model",
            "venue": "arXiv preprint arXiv:2301.04474, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "S. Shen",
                "W. Zhao",
                "Z. Meng",
                "W. Li",
                "Z. Zhu",
                "J. Zhou",
                "J. Lu"
            ],
            "title": "Difftalk: Crafting diffusion models for generalized talking head synthesis",
            "venue": "arXiv preprint arXiv:2301.03786, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "K. Kim",
                "Y. Kim",
                "S. Cho",
                "J. Seo",
                "J. Nam",
                "K. Lee",
                "S. Kim",
                "K. Lee"
            ],
            "title": "Diffface: Diffusion-based face swapping with facial guidance",
            "venue": "arXiv preprint arXiv:2212.13344, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "W. Wu",
                "Y. Zhang",
                "C. Li",
                "C. Qian",
                "C.C. Loy"
            ],
            "title": "Reenactgan: Learning to reenact faces via boundary transfer",
            "venue": "Proceedings of the European conference on computer vision (ECCV), 2018, pp. 603\u2013619.",
            "year": 2018
        },
        {
            "authors": [
                "P.-H. Huang",
                "F.-E. Yang",
                "Y.-C.F. Wang"
            ],
            "title": "Learning identityinvariant motion representations for cross-id face reenactment",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 7084\u20137092.",
            "year": 2020
        },
        {
            "authors": [
                "J. Zhang",
                "X. Zeng",
                "M. Wang",
                "Y. Pan",
                "L. Liu",
                "Y. Liu",
                "Y. Ding",
                "C. Fan"
            ],
            "title": "Freenet: Multi-identity face reenactment",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 5326\u20135335.",
            "year": 2020
        },
        {
            "authors": [
                "S. Ha",
                "M. Kersner",
                "B. Kim",
                "S. Seo",
                "D. Kim"
            ],
            "title": "Marionette: Few-shot face reenactment preserving identity of unseen targets",
            "venue": "Proceedings of the AAAI conference on artificial intelligence, vol. 34, no. 07, 2020, pp. 10 893\u201310 900.",
            "year": 2020
        },
        {
            "authors": [
                "Z. Chen",
                "C. Wang",
                "B. Yuan",
                "D. Tao"
            ],
            "title": "Puppeteergan: Arbitrary portrait animation with semantic-aware appearance transformation",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 13 518\u201313 527.",
            "year": 2020
        },
        {
            "authors": [
                "E. Zakharov",
                "A. Shysheya",
                "E. Burkov",
                "V. Lempitsky"
            ],
            "title": "Few-shot adversarial learning of realistic neural talking head models",
            "venue": "Proceedings of the IEEE/CVF international conference on computer vision, 2019, pp. 9459\u20139468.",
            "year": 2019
        },
        {
            "authors": [
                "O. Wiles",
                "A. Koepke",
                "A. Zisserman"
            ],
            "title": "X2face: A network for controlling face generation using images, audio, and pose codes",
            "venue": "Proceedings of the European conference on computer vision (ECCV), 2018, pp. 670\u2013686.",
            "year": 2018
        },
        {
            "authors": [
                "A. Siarohin",
                "S. Lathuili\u00e8re",
                "S. Tulyakov",
                "E. Ricci",
                "N. Sebe"
            ],
            "title": "Animating arbitrary objects via deep motion transfer",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 2377\u20132386.",
            "year": 2019
        },
        {
            "authors": [
                "Z. Zhang",
                "L. Li",
                "Y. Ding",
                "C. Fan"
            ],
            "title": "Flow-guided one-shot talking face generation with a high-resolution audio-visual dataset",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 3661\u20133670.",
            "year": 2021
        },
        {
            "authors": [
                "M.C. Doukas",
                "S. Zafeiriou",
                "V. Sharmanska"
            ],
            "title": "Headgan: One-shot neural head synthesis and editing",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 14 398\u201314 407.",
            "year": 2021
        },
        {
            "authors": [
                "F.-T. Hong",
                "L. Zhang",
                "L. Shen",
                "D. Xu"
            ],
            "title": "Depth-aware generative adversarial network for talking head video generation",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 3397\u20133406.",
            "year": 2022
        },
        {
            "authors": [
                "C. Xu",
                "J. Zhang",
                "Y. Han",
                "G. Tian",
                "X. Zeng",
                "Y. Tai",
                "Y. Wang",
                "C. Wang",
                "Y. Liu"
            ],
            "title": "Designing one unified framework for high-fidelity face reenactment and swapping",
            "venue": "Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XV. Springer, 2022, pp. 54\u201371.",
            "year": 2022
        },
        {
            "authors": [
                "P. Garrido",
                "L. Valgaerts",
                "H. Sarmadi",
                "I. Steiner",
                "K. Varanasi",
                "P. Perez",
                "C. Theobalt"
            ],
            "title": "Vdub: Modifying face video of actors for plausible visual alignment to a dubbed audio track",
            "venue": "Computer graphics forum, vol. 34, no. 2. Wiley Online Library, 2015, pp. 193\u2013204.",
            "year": 2015
        },
        {
            "authors": [
                "S. Suwajanakorn",
                "S.M. Seitz",
                "I. Kemelmacher-Shlizerman"
            ],
            "title": "Synthesizing obama: learning lip sync from audio",
            "venue": "ACM Transactions on Graphics (ToG), vol. 36, no. 4, pp. 1\u201313, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "K. Prajwal",
                "R. Mukhopadhyay",
                "V.P. Namboodiri",
                "C. Jawahar"
            ],
            "title": "A lip sync expert is all you need for speech to lip generation in the wild",
            "venue": "Proceedings of the 28th ACM International Conference on Multimedia, 2020, pp. 484\u2013492.",
            "year": 2020
        },
        {
            "authors": [
                "D. Aneja",
                "W. Li"
            ],
            "title": "Real-time lip sync for live 2d animation",
            "venue": "arXiv preprint arXiv:1910.08685, 2019.",
            "year": 1910
        },
        {
            "authors": [
                "S. Biswas",
                "S. Sinha",
                "D. Das",
                "B. Bhowmick"
            ],
            "title": "Realistic talking face animation with speech-induced head motion",
            "venue": "Proceedings of the Twelfth Indian Conference on Computer Vision, Graphics and Image Processing, 2021, pp. 1\u20139.",
            "year": 2021
        },
        {
            "authors": [
                "S.E. Eskimez",
                "R.K. Maddox",
                "C. Xu",
                "Z. Duan"
            ],
            "title": "Generating talking face landmarks from speech",
            "venue": "Latent Variable Analysis and Signal Separation: 14th International Conference, LVA/ICA 2018, Guildford, UK, July 2\u20135, 2018, Proceedings 14. Springer, 2018, pp. 372\u2013381.",
            "year": 2018
        },
        {
            "authors": [
                "X. Ji",
                "H. Zhou",
                "K. Wang",
                "W. Wu",
                "C.C. Loy",
                "X. Cao",
                "F. Xu"
            ],
            "title": "Audiodriven emotional video portraits",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2021, pp. 14 080\u201314 089.",
            "year": 2021
        },
        {
            "authors": [
                "L. Chen",
                "G. Cui",
                "C. Liu",
                "Z. Li",
                "Z. Kou",
                "Y. Xu",
                "C. Xu"
            ],
            "title": "Talking-head generation with rhythmic head motion",
            "venue": "Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part IX. Springer, 2020, pp. 35\u201351.",
            "year": 2020
        },
        {
            "authors": [
                "D. Cudeiro",
                "T. Bolkart",
                "C. Laidlaw",
                "A. Ranjan",
                "M.J. Black"
            ],
            "title": "Capture, learning, and synthesis of 3d speaking styles",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 10 101\u201310 111.",
            "year": 2019
        },
        {
            "authors": [
                "T. Karras",
                "T. Aila",
                "S. Laine",
                "A. Herva",
                "J. Lehtinen"
            ],
            "title": "Audio-driven facial animation by joint end-to-end learning of pose and emotion",
            "venue": "ACM Transactions on Graphics (TOG), vol. 36, no. 4, pp. 1\u201312, 2017. JOURNAL OF LTEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 13",
            "year": 2017
        },
        {
            "authors": [
                "A. Richard",
                "M. Zollh\u00f6fer",
                "Y. Wen",
                "F. De la Torre",
                "Y. Sheikh"
            ],
            "title": "Meshtalk: 3d face animation from speech using cross-modality disentanglement",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 1173\u20131182.",
            "year": 2021
        },
        {
            "authors": [
                "S. Shen",
                "W. Li",
                "Z. Zhu",
                "Y. Duan",
                "J. Zhou",
                "J. Lu"
            ],
            "title": "Learning dynamic facial radiance fields for few-shot talking head synthesis",
            "venue": "Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XII. Springer, 2022, pp. 666\u2013682.",
            "year": 2022
        },
        {
            "authors": [
                "D. Min",
                "M. Song",
                "S.J. Hwang"
            ],
            "title": "Styletalker: One-shot stylebased audio-driven talking head video generation",
            "venue": "arXiv preprint arXiv:2208.10922, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "F. Yin",
                "Y. Zhang",
                "X. Cun",
                "M. Cao",
                "Y. Fan",
                "X. Wang",
                "Q. Bai",
                "B. Wu",
                "J. Wang",
                "Y. Yang"
            ],
            "title": "Styleheat: One-shot high-resolution editable talking face generation via pre-trained stylegan",
            "venue": "Computer Vision\u2013 ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u2013 27, 2022, Proceedings, Part XVII. Springer, 2022, pp. 85\u2013101.",
            "year": 2022
        },
        {
            "authors": [
                "K. Wang",
                "Q. Wu",
                "L. Song",
                "Z. Yang",
                "W. Wu",
                "C. Qian",
                "R. He",
                "Y. Qiao",
                "C.C. Loy"
            ],
            "title": "Mead: A large-scale audio-visual dataset for emotional talking-face generation",
            "venue": "Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XXI. Springer, 2020, pp. 700\u2013717.",
            "year": 2020
        },
        {
            "authors": [
                "J. Ho",
                "A. Jain",
                "P. Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "Advances in Neural Information Processing Systems, vol. 33, pp. 6840\u20136851, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "A.Q. Nichol",
                "P. Dhariwal"
            ],
            "title": "Improved denoising diffusion probabilistic models",
            "venue": "International Conference on Machine Learning. PMLR, 2021, pp. 8162\u20138171.",
            "year": 2021
        },
        {
            "authors": [
                "M. Stypulkowski",
                "K. Vougioukas",
                "S. He",
                "M. Zieba",
                "S. Petridis",
                "M. Pantic"
            ],
            "title": "Diffused heads: Diffusion models beat gans on talking-face generation",
            "venue": "arXiv preprint arXiv:2301.03396, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "V. Blanz",
                "K. Scherbaum",
                "T. Vetter",
                "H.-P. Seidel"
            ],
            "title": "Exchanging faces in images",
            "venue": "Computer Graphics Forum, vol. 23, no. 3. Wiley Online Library, 2004, pp. 669\u2013676.",
            "year": 2004
        },
        {
            "authors": [
                "D. Bitouk",
                "N. Kumar",
                "S. Dhillon",
                "P. Belhumeur",
                "S.K. Nayar"
            ],
            "title": "Face swapping: automatically replacing faces in photographs",
            "venue": "ACM SIGGRAPH 2008 papers, 2008, pp. 1\u20138.",
            "year": 2008
        },
        {
            "authors": [
                "Y.-T. Cheng",
                "V. Tzeng",
                "Y. Liang",
                "C.-C. Wang",
                "B.-Y. Chen",
                "Y.-Y. Chuang",
                "M. Ouhyoung"
            ],
            "title": "3d-model-based face replacement in video",
            "venue": "SIGGRAPH\u201909: Posters, 2009, pp. 1\u20131.",
            "year": 2009
        },
        {
            "authors": [
                "Y. Lin",
                "S. Wang",
                "Q. Lin",
                "F. Tang"
            ],
            "title": "Face swapping under large pose variations: A 3d model based approach",
            "venue": "2012 IEEE International Conference on Multimedia and Expo. IEEE, 2012, pp. 333\u2013338.",
            "year": 2012
        },
        {
            "authors": [
                "I. Perov",
                "D. Gao",
                "N. Chervoniy",
                "K. Liu",
                "S. Marangonda",
                "C. Um\u00e9",
                "M. Dpfks",
                "C.S. Facenheim",
                "L. RP",
                "J. Jiang"
            ],
            "title": "Deepfacelab: Integrated, flexible and extensible face-swapping framework",
            "venue": "arXiv preprint arXiv:2005.05535, 2020.",
            "year": 2005
        },
        {
            "authors": [
                "R. Natsume",
                "T. Yatagawa",
                "S. Morishima"
            ],
            "title": "Rsgan: face swapping and editing using face and hair representation in latent spaces",
            "venue": "arXiv preprint arXiv:1804.03447, 2018.",
            "year": 1804
        },
        {
            "authors": [
                "J. Bao",
                "D. Chen",
                "F. Wen",
                "H. Li",
                "G. Hua"
            ],
            "title": "Towards open-set identity preserving face synthesis",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 6713\u20136722.",
            "year": 2018
        },
        {
            "authors": [
                "L. Li",
                "J. Bao",
                "H. Yang",
                "D. Chen",
                "F. Wen"
            ],
            "title": "Advancing high fidelity identity swapping for forgery detection",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp. 5074\u20135083.",
            "year": 2020
        },
        {
            "authors": [
                "R. Chen",
                "X. Chen",
                "B. Ni",
                "Y. Ge"
            ],
            "title": "Simswap: An efficient framework for high fidelity face swapping",
            "venue": "Proceedings of the 28th ACM International Conference on Multimedia, 2020, pp. 2003\u20132011.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Wang",
                "X. Chen",
                "J. Zhu",
                "W. Chu",
                "Y. Tai",
                "C. Wang",
                "J. Li",
                "Y. Wu",
                "F. Huang",
                "R. Ji"
            ],
            "title": "Hififace: 3d shape and semantic prior guided high fidelity face swapping",
            "venue": "arXiv preprint arXiv:2106.09965, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "J. Li",
                "Z. Li",
                "J. Cao",
                "X. Song",
                "R. He"
            ],
            "title": "Faceinpainter: High fidelity face adaptation to heterogeneous domains",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2021, pp. 5089\u20135098.",
            "year": 2021
        },
        {
            "authors": [
                "T. Karras",
                "S. Laine",
                "T. Aila"
            ],
            "title": "A style-based generator architecture for generative adversarial networks",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp. 4401\u2013 4410.",
            "year": 2019
        },
        {
            "authors": [
                "T. Karras",
                "S. Laine",
                "M. Aittala",
                "J. Hellsten",
                "J. Lehtinen",
                "T. Aila"
            ],
            "title": "Analyzing and improving the image quality of stylegan",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp. 8110\u20138119.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Zhu",
                "Q. Li",
                "J. Wang",
                "C.-Z. Xu",
                "Z. Sun"
            ],
            "title": "One shot face swapping on megapixels",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2021, pp. 4834\u20134844.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Xu",
                "B. Deng",
                "J. Wang",
                "Y. Jing",
                "J. Pan",
                "S. He"
            ],
            "title": "High-resolution face swapping via latent semantics disentanglement",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 7642\u20137651.",
            "year": 2022
        },
        {
            "authors": [
                "C. Xu",
                "J. Zhang",
                "M. Hua",
                "Q. He",
                "Z. Yi",
                "Y. Liu"
            ],
            "title": "Region-aware face swapping",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 7632\u20137641.",
            "year": 2022
        },
        {
            "authors": [
                "E. Richardson",
                "Y. Alaluf",
                "O. Patashnik",
                "Y. Nitzan",
                "Y. Azar",
                "S. Shapiro",
                "D. Cohen-Or"
            ],
            "title": "Encoding in style: a stylegan encoder for imageto-image translation",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2021, pp. 2287\u20132296.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Luo",
                "J. Zhu",
                "K. He",
                "W. Chu",
                "Y. Tai",
                "C. Wang",
                "J. Yan"
            ],
            "title": "Styleface: Towards identity-disentangled face generation on megapixels",
            "venue": "Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XVI. Springer, 2022, pp. 297\u2013312.",
            "year": 2022
        },
        {
            "authors": [
                "Z. Xu",
                "H. Zhou",
                "Z. Hong",
                "Z. Liu",
                "J. Liu",
                "Z. Guo",
                "J. Han",
                "J. Liu",
                "E. Ding",
                "J. Wang"
            ],
            "title": "Styleswap: Style-based generator empowers robust face swapping",
            "venue": "Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XIV. Springer, 2022, pp. 661\u2013677.",
            "year": 2022
        },
        {
            "authors": [
                "G. Gao",
                "H. Huang",
                "C. Fu",
                "Z. Li",
                "R. He"
            ],
            "title": "Information bottleneck disentanglement for identity swapping",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2021, pp. 3404\u2013 3413.",
            "year": 2021
        },
        {
            "authors": [
                "P. Dhariwal",
                "A. Nichol"
            ],
            "title": "Diffusion models beat gans on image synthesis",
            "venue": "Advances in Neural Information Processing Systems, vol. 34, pp. 8780\u20138794, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "J. Ho",
                "T. Salimans"
            ],
            "title": "Classifier-free diffusion guidance",
            "venue": "arXiv preprint arXiv:2207.12598, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "J. Song",
                "C. Meng",
                "S. Ermon"
            ],
            "title": "Denoising diffusion implicit models",
            "venue": "arXiv preprint arXiv:2010.02502, 2020.",
            "year": 2010
        },
        {
            "authors": [
                "R. Rombach",
                "A. Blattmann",
                "D. Lorenz",
                "P. Esser",
                "B. Ommer"
            ],
            "title": "Highresolution image synthesis with latent diffusion models",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 10 684\u201310 695.",
            "year": 2022
        },
        {
            "authors": [
                "A. Radford",
                "J.W. Kim",
                "C. Hallacy",
                "A. Ramesh",
                "G. Goh",
                "S. Agarwal",
                "G. Sastry",
                "A. Askell",
                "P. Mishkin",
                "J. Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "International conference on machine learning. PMLR, 2021, pp. 8748\u20138763.",
            "year": 2021
        },
        {
            "authors": [
                "O. Avrahami",
                "D. Lischinski",
                "O. Fried"
            ],
            "title": "Blended diffusion for textdriven editing of natural images",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 18 208\u201318 218.",
            "year": 2022
        },
        {
            "authors": [
                "W.-C. Fan",
                "Y.-C. Chen",
                "D. Chen",
                "Y. Cheng",
                "L. Yuan",
                "Y.-C.F. Wang"
            ],
            "title": "Frido: Feature pyramid diffusion for complex scene image synthesis",
            "venue": "arXiv preprint arXiv:2208.13753, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "N. Ruiz",
                "Y. Li",
                "V. Jampani",
                "Y. Pritch",
                "M. Rubinstein",
                "K. Aberman"
            ],
            "title": "Dreambooth: Fine tuning text-to-image diffusion models for subjectdriven generation",
            "venue": "arXiv preprint arXiv:2208.12242, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "C. Saharia",
                "W. Chan",
                "S. Saxena",
                "L. Li",
                "J. Whang",
                "E. Denton",
                "S.K.S. Ghasemipour",
                "B.K. Ayan",
                "S.S. Mahdavi",
                "R.G. Lopes"
            ],
            "title": "Photorealistic text-to-image diffusion models with deep language understanding",
            "venue": "arXiv preprint arXiv:2205.11487, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "J. Ho",
                "T. Salimans",
                "A. Gritsenko",
                "W. Chan",
                "M. Norouzi",
                "D.J. Fleet"
            ],
            "title": "Video diffusion models",
            "venue": "arXiv preprint arXiv:2204.03458, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "J. Ho",
                "W. Chan",
                "C. Saharia",
                "J. Whang",
                "R. Gao",
                "A. Gritsenko",
                "D.P. Kingma",
                "B. Poole",
                "M. Norouzi",
                "D.J. Fleet"
            ],
            "title": "Imagen video: High definition video generation with diffusion models",
            "venue": "arXiv preprint arXiv:2210.02303, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "J.Z. Wu",
                "Y. Ge",
                "X. Wang",
                "W. Lei",
                "Y. Gu",
                "W. Hsu",
                "Y. Shan",
                "X. Qie",
                "M.Z. Shou"
            ],
            "title": "Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation",
            "venue": "arXiv preprint arXiv:2212.11565, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "E. Molad",
                "E. Horwitz",
                "D. Valevski",
                "A.R. Acha",
                "Y. Matias",
                "Y. Pritch",
                "Y. Leviathan",
                "Y. Hoshen"
            ],
            "title": "Dreamix: Video diffusion models are general video editors",
            "venue": "arXiv preprint arXiv:2302.01329, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "R. Huang",
                "J. Huang",
                "D. Yang",
                "Y. Ren",
                "L. Liu",
                "M. Li",
                "Z. Ye",
                "J. Liu",
                "X. Yin",
                "Z. Zhao"
            ],
            "title": "Make-an-audio: Text-to-audio generation with prompt-enhanced diffusion models",
            "venue": "arXiv preprint arXiv:2301.12661, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "H. Liu",
                "Z. Chen",
                "Y. Yuan",
                "X. Mei",
                "X. Liu",
                "D. Mandic",
                "W. Wang",
                "M.D. Plumbley"
            ],
            "title": "Audioldm: Text-to-audio generation with latent diffusion models",
            "venue": "arXiv preprint arXiv:2301.12503, 2023. JOURNAL OF LTEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 14",
            "year": 2023
        },
        {
            "authors": [
                "B. Poole",
                "A. Jain",
                "J.T. Barron",
                "B. Mildenhall"
            ],
            "title": "Dreamfusion: Textto-3d using 2d diffusion",
            "venue": "arXiv preprint arXiv:2209.14988, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "J. Xu",
                "X. Wang",
                "W. Cheng",
                "Y.-P. Cao",
                "Y. Shan",
                "X. Qie",
                "S. Gao"
            ],
            "title": "Dream3d: Zero-shot text-to-3d synthesis using 3d shape prior and textto-image diffusion models",
            "venue": "arXiv preprint arXiv:2212.14704, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "M. Li",
                "Y. Duan",
                "J. Zhou",
                "J. Lu"
            ],
            "title": "Diffusion-sdf: Text-to-shape via voxelized diffusion",
            "venue": "arXiv preprint arXiv:2212.03293, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "S. Park",
                "X. Zhang",
                "A. Bulling",
                "O. Hilliges"
            ],
            "title": "Learning to find eye region landmarks for remote gaze estimation in unconstrained settings",
            "venue": "Proceedings of the 2018 ACM symposium on eye tracking research & applications, 2018, pp. 1\u201310.",
            "year": 2018
        },
        {
            "authors": [
                "S. Hochreiter",
                "J. Schmidhuber"
            ],
            "title": "Long short-term memory",
            "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.",
            "year": 1997
        },
        {
            "authors": [
                "Y. Lu",
                "J. Chai",
                "X. Cao"
            ],
            "title": "Live speech portraits: real-time photorealistic talking-head animation",
            "venue": "ACM Transactions on Graphics (TOG), vol. 40, no. 6, pp. 1\u201317, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "K. Cho",
                "B. Van Merri\u00ebnboer",
                "C. Gulcehre",
                "D. Bahdanau",
                "F. Bougares",
                "H. Schwenk",
                "Y. Bengio"
            ],
            "title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
            "venue": "arXiv preprint arXiv:1406.1078, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "A. Vaswani",
                "N. Shazeer",
                "N. Parmar",
                "J. Uszkoreit",
                "L. Jones",
                "A.N. Gomez",
                "\u0141. Kaiser",
                "I. Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems, vol. 30, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "J. Ho",
                "A. Jain",
                "P. Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "Advances in Neural Information Processing Systems, vol. 33, pp. 6840\u20136851, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "R. Zhang",
                "P. Isola",
                "A.A. Efros",
                "E. Shechtman",
                "O. Wang"
            ],
            "title": "The unreasonable effectiveness of deep features as a perceptual metric",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 586\u2013595.",
            "year": 2018
        },
        {
            "authors": [
                "K. Simonyan",
                "A. Zisserman"
            ],
            "title": "Very deep convolutional networks for large-scale image recognition",
            "venue": "arXiv preprint arXiv:1409.1556, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "C. Yu",
                "J. Wang",
                "C. Peng",
                "C. Gao",
                "G. Yu",
                "N. Sang"
            ],
            "title": "Bisenet: Bilateral segmentation network for real-time semantic segmentation",
            "venue": "Proceedings of the European conference on computer vision (ECCV), 2018, pp. 325\u2013341.",
            "year": 2018
        },
        {
            "authors": [
                "Y. Huang",
                "Y. Wang",
                "Y. Tai",
                "X. Liu",
                "P. Shen",
                "S. Li",
                "J. Li",
                "F. Huang"
            ],
            "title": "Curricularface: adaptive curriculum learning loss for deep face recognition",
            "venue": "proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp. 5901\u20135910.",
            "year": 2020
        },
        {
            "authors": [
                "A. Nagrani",
                "J.S. Chung",
                "A. Zisserman"
            ],
            "title": "Voxceleb: a large-scale speaker identification dataset",
            "venue": "arXiv preprint arXiv:1706.08612, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "C.-H. Lee",
                "Z. Liu",
                "L. Wu",
                "P. Luo"
            ],
            "title": "Maskgan: Towards diverse and interactive facial image manipulation",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020.",
            "year": 2020
        },
        {
            "authors": [
                "A. Rossler",
                "D. Cozzolino",
                "L. Verdoliva",
                "C. Riess",
                "J. Thies",
                "M. Nie\u00dfner"
            ],
            "title": "Faceforensics++: Learning to detect manipulated facial images",
            "venue": "Proceedings of the IEEE/CVF international conference on computer vision, 2019, pp. 1\u201311.",
            "year": 2019
        },
        {
            "authors": [
                "J. Deng",
                "J. Guo",
                "N. Xue",
                "S. Zafeiriou"
            ],
            "title": "Arcface: Additive angular margin loss for deep face recognition",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp. 4690\u2013 4699.",
            "year": 2019
        },
        {
            "authors": [
                "M. Heusel",
                "H. Ramsauer",
                "T. Unterthiner",
                "B. Nessler",
                "S. Hochreiter"
            ],
            "title": "Gans trained by a two time-scale update rule converge to a local nash equilibrium",
            "venue": "Advances in neural information processing systems, vol. 30, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "L. Chen",
                "Z. Li",
                "R.K. Maddox",
                "Z. Duan",
                "C. Xu"
            ],
            "title": "Lip movements generation at a glance",
            "venue": "Proceedings of the European conference on computer vision (ECCV), 2018, pp. 520\u2013535.",
            "year": 2018
        },
        {
            "authors": [
                "J.S. Chung",
                "A. Zisserman"
            ],
            "title": "Out of time: automated lip sync in the wild",
            "venue": "Computer Vision\u2013ACCV 2016 Workshops: ACCV 2016 International Workshops, Taipei, Taiwan, November 20-24, 2016, Revised Selected Papers, Part II 13. Springer, 2017, pp. 251\u2013263.",
            "year": 2016
        },
        {
            "authors": [
                "A. Mollahosseini",
                "B. Hasani",
                "M.H. Mahoor"
            ],
            "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
            "venue": "IEEE Transactions on Affective Computing, vol. 10, no. 1, pp. 18\u201331, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "T.-C. Wang",
                "A. Mallya",
                "M.-Y. Liu"
            ],
            "title": "One-shot free-view neural talking-head synthesis for video conferencing",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2021, pp. 10 039\u201310 049.",
            "year": 2021
        },
        {
            "authors": [
                "X. Ji",
                "H. Zhou",
                "K. Wang",
                "W. Wu",
                "C.C. Loy",
                "X. Cao",
                "F. Xu"
            ],
            "title": "Audiodriven emotional video portraits",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2021, pp. 14 080\u201314 089.",
            "year": 2021
        },
        {
            "authors": [
                "A. Radford",
                "J. Wu",
                "R. Child",
                "D. Luan",
                "D. Amodei",
                "I. Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog, vol. 1, no. 8, p. 9, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "H. Zeng",
                "W. Zhang",
                "C. Fan",
                "T. Lv",
                "S. Wang",
                "Z. Zhang",
                "B. Ma",
                "L. Li",
                "Y. Ding",
                "X. Yu"
            ],
            "title": "Flowface: Semantic flow-guided shape-aware face swapping",
            "venue": "arXiv preprint arXiv:2212.02797, 2022.",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "Multimodal-driven Talking Face Generation via a Unified Diffusion-based Generator\nChao Xu, Shaoting Zhu, Junwei Zhu, Tianxin Huang, Jiangning Zhang, Ying Tai, Yong Liu\nAbstract\u2014Multimodal-driven talking face generation refers to animating a portrait with the given pose, expression, and gaze transferred from the driving image and video, or estimated from the text and audio. However, existing methods ignore the potential of text modal, and their generators mainly follow the source-oriented feature rearrange paradigm coupled with unstable GAN frameworks. In this work, we first represent the emotion in the text prompt, which could inherit rich semantics from the CLIP, allowing flexible and generalized emotion control. We further reorganize these tasks as the target-oriented texture transfer and adopt the Diffusion Models. More specifically, given a textured face as the source and the rendered face projected from the desired 3DMM coefficients as the target, our proposed Texture-Geometry-aware Diffusion Model decomposes the complex transfer problem into multi-conditional denoising process, where a Texture Attention-based module accurately models the correspondences between appearance and geometry cues contained in source and target conditions, and incorporate extra implicit information for high-fidelity talking face generation. Additionally, TGDM can be gracefully tailored for face swapping. We derive a novel paradigm free of unstable seesaw-style optimization, resulting in simple, stable, and effective training and inference schemes. Extensive experiments demonstrate the superiority of our method.\nIndex Terms\u2014Multimodal-driven Talking Face Generation, Face Swapping, Diffusion Model\nF"
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Talking face generation aims to synthesize talking video from the source face according to the given emotion, mouth movement, and head rotation, which is relevant to several applications, including video production and virtual avatars. Multimodal information could guide the animation in the real scenario, such as text, audio, image, and video.\nRecently, many attempts [1], [2], [3] have achieved significant progress in these tasks, most of them share the same paradigm, i.e., extracting the intermediate structural representation from given conditions first and then manipulating the source face to the desired expression and pose, which mainly follows the sourceoriented pipeline, as shown in Fig. 1 (a). Specifically, among these approaches, some image- and video-driven [4], [5], [6] methods employ AdaIN-based [7] generators that take vectors as input, which inevitably lead to the information loss and fail to preserve the source identity and background. Others [3], [8], [9], [10], [11] warp the source feature to the target by the explicit motion flows for better visual results, but they appear warping artifacts when the source and driving conditions encompass significant appearance variation. Consequently, the above generators tend to suffer from image degradation when rearranging the source features. Recent audio-driven tasks require more authentic results, subsequent works [12], [13], [14] adopt identity-specific training but cannot generalize across different persons. Besides, existing pipelines generally adopt GANs [15], and its unstable adversarial min-max objective training process further exacerbates unrealistic textures. Due to these constraints of the generator, each task\n\u2022 C. Xu, S. Zhu, T. Huang and Y. Liu are with APRIL Lab, Zhejiang University, Hangzhou, China (e-mail: 21832066@zju.edu.cn; zhust@zju.edu.cn; 21725129@zju.edu.cn; yongliu@iipc.zju.edu.cn). \u2022 J. Zhu, J. Zhang, and Y. Tai are with YouTu Lab, Tencent, Shanghai, China (e-mail: junweizhu@tencent.com; vtzhang@tencent.com; yingtai@tencent.com). \u2022 Y. Liu is the corresponding author. Manuscript received April 19, 2005; revised August 26, 2015.\nneeds a specific design and is unfriendly for practical applications. Thus, one challenge arises, how to accomplish a robust and stable generator for all driving modals to achieve high-fidelity talking face generation. In addition, existing work [16] simply reflects the text on the mouth movements, ignoring the potential of text when under the large-scale pre-trained models. Thus another challenge arises, how to sufficiently use the text modal in this task.\nTo address the above challenges, we first represent the emotion style in the text prompt inspired by the zero-shot CLIP-guided image manipulation, which could inherit rich semantic knowledge and allow flexible emotion control, i.e., unseen emotions could be specified using the text description and precisely reflected on the synthesized faces. Furthermore, to unify the multimodal-driven tasks into the same generator, we frame the talking face generation as a target-oriented texture transfer, instead of the source-oriented feature rearrange, and adopt a multi-conditional diffusion model to avoid unstable training of GANs, termed Texture-Geometry-aware Diffusion Model (TGDM), as shown in Fig. 1 (b). In particular, benefiting from the explainable and disentangled parameter space of 3DMMs [17], we combine the texture-related coefficients from the source face with the geometry-related ones from the driving conditions to construct 3D descriptors, which are projected to the image domain and serve as the target pivot. To further supplement source texture to rendered face, we employ cross attention that accurately models the correspondences between source and target appearance. To this end, TGDM is dedicated to transferring the source texture to the target rendered face, which preserves explicit structural information but avoids complex texture deformations. In contrast to recent diffusion-based methods [18], [19] that only handle mouth area generation, our approach can generate realistic faces with various expressions and poses.\nConsidering the characteristics of the TGDM to model complex texture and semantic transfer, we further connect TGDM with another popular task, face swapping, which aims to transfer the source identity to the target face while preserving the target\nar X\niv :2\n30 5.\n02 59\n4v 2\n[ cs\n.C V\n] 9\nM ay\n2 02\n3\nattributes. Recent developments are stuck due to unstable GANbased training schemes and seesaw-style optimization goals. DiffFace [20] first avoids GANs but is still sensitive to identity-related and identity-unrelated hyperparameter settings when sampling. Borrowing the idea from the aforementioned driving framework, we derive a novel paradigm for face swapping built upon the TGDM, which inherits the merits of the diffusion model and requires only reconstruction loss during training, with no extra tricks for sampling either, as shown in Fig. 1 (d).\nIn summary, we make the following four contributions:\n\u2022 We adopt the text modal as the talking face emotion representation, inheriting rich semantics from large-scale pre-trained models, which allows flexible emotion control and unseen emotion generalization. \u2022 We propose a novel TGDM pipeline based on the multiconditional diffusion model to afford complex texture and identity transfer, generating high-quality talking face generation for all driven modals. \u2022 We transfer the TGDM to face swapping task and propose a novel training and inference paradigm that is simple, stable, and effective. \u2022 Abundant experiments are conducted to demonstrate the superiority of TGDM for several face manipulation tasks over SOTA methods."
        },
        {
            "heading": "2 RELATED WORKS",
            "text": ""
        },
        {
            "heading": "2.1 Talking Face Generation",
            "text": "Face reenactment involves taking the source face and replicating its pose and expression as the target. This can be achieved through two main techniques: instruction-based methods animate the source face instructed by the target structure. Various works [21], [22], [23], [24], [25] adopt landmarks and segmentation maps to indicate the facial attribute. Recently, with the success of AdaIN [7], subsequent works [5], [26] encode the target attributes in the vectorized information and then inject them into the source face. However, the above methods fail to explicitly indicate the movements between the source and target faces. Subsequently,\nwarping-based methods learn to warp and synthesize the target faces based on the estimated motion fields. These methods [27], [28] usually separate motion estimation and warped source face refinement into two stages. The most representative work is FOMM [3], which uses relative key-point locations to predict flow fields for source appearance driving. Other follow-up works [9], [10] focus on improving the motion flows and warping operation accuracy. Some works [8], [29], [30], [31] introduce 3D information as structure guidance for flow field generation. However, they still suffer from identity degradation under some extreme conditions. Recent UniFace [32] proposes a unified framework to boost the model\u2019s robustness with the help of face swapping.\nAudio-driven talking head synthesis, a special form of face reenactment, aims to create talking videos with lip movements corresponding to the driving audio [33], [34], [35]. The traditional approaches could be roughly divided into 2D-based and 3D-based ones. 2D-based methods [36], [37], [38], [39] generate a series of 2D points on the face based on audio inputs, while 3D-based methods [14], [40], [41], [42], [43], [44] use audio to predict expression parameters or facial radiance fields. Some works [12], [31], [45] further improve geometry learning and take talking style into account. After achieving the intermediate structure, PCAVS [1] injects the pose and lip information into the generator by implicit modulation. StyleHeat [46] generates high-resolution driven faces with the help of StyleGAN. Besides, emotion is a factor that plays a critical role in realistic animation. MEAD [47] releases a high-quality talking head video dataset with annotations of emotion category and intensity. Subsequent works [2], [11] follow the framework of PC-AVS or FOMM and take emotion as another condition. However, due to the limitations of the generator and unstable GAN-based training, the above methods need to design the specific intermediate representation and generator for each driving modal, thus making it impossible to share the same structure. Nowadays, some attempts based on diffusion models [48], [49] have been made. Stypu\u0142kowski et al. [50] leverage a pre-trained audio encoder to add audio embeddings during the denoising process. DiffTalk [19] and Bigioi et al. [18] present crafted conditional diffusion models for generalized talking head synthesis. However, they fail to model the distinct expression and pose variations."
        },
        {
            "heading": "2.2 Face Swapping",
            "text": "Face swapping aims to change the target identity according to the given source but keep other facial attributes constant. Early face swap works [51], [52], [53], [54] mainly focus on 3Dbased methods but suffer from poor visual quality. Recently, GAN-based [15] methods [55], [56], [57] have made significant progress. Specifically, Faceshifter [58] integrates identity and attribute embeddings adaptively from a well-designed learning model. SimSwap [59] introduces a feature matching loss hoping to preserve more attribute embeddings at the cost of sacrificing identity similarity. Hififace [60] and FaceInpainter [61] take 3D face descriptor into consideration for better geometry structure of swapped results. With the success of StyleGAN [62], [63], many works have emerged as a solution for high-resolution face swap. MegaFS [64] first exploits StyleGAN2 as the decoder. The followup works [65], [66] also adopt the pSp [67] framework and design the fusion strategy for better attribute preservation. However, they lack flexibility in application due to the fixed StyleGAN generator. Consequently, some attempts have been made to solve\nthis problem. StyleFace [68] redesigns the StyleGAN2 module and opens parameters for training. StyleSwap [69] introduces a mask branch and an ID inversion strategy to empower high-fidelity and robust face swapping. As the diffusion model shows excellent performance in many fields, DiffFace [20] makes the first attempt to apply the diffusion model to the face swapping task. Despite the impressive progress achieved by the above methods, it is still a struggle to fully transfer the face identity from the source face while preserving identity-unrelated attributes of the target images due to seesaw-style training losses. One solution [70] is to fully disentangle identity-related and identity-unrelated information, but it is almost impossible in the current implementation scheme. In this paper, we propose a new training paradigm only guided by the reconstruction loss to solve this challenge."
        },
        {
            "heading": "2.3 Diffusion Model and Multimodal Generation",
            "text": "Diffusion models [48], [49] are recently proposed generative models that can synthesize high-quality images. They are a type of generative probabilistic model that consists of two steps. Firstly, data is destroyed by successively adding small amounts of Gaussian noise to it over a series of time steps. Secondly, a learning algorithm is trained to recover the data by gradually removing the noise over a series of time steps. Diffusion models are trained without discriminators, so they are more reliable and robust during training compared to GANs. Additionally, they do not suffer from common issues such as mode collapse or vanishing gradients, which are inevitable in the training process of GANs. After achieving great success in the unconditional generation, diffusion models are adapted to enable conditional generation. Dhariwal et al. [71] introduce classifier-guided diffusion, which forces the produced noise to approach the desired condition. Ho et al. further [72] develop a Classifier-Free Guidance approach that allows conditional editing without having to pretrain classifiers. Despite these advantages, diffusion models are hindered by their slow sampling speed due to the thousands of times on one sample for complete pixel space-based denoising. To address this issue, Song et al. [73] propose DDIM reduce sample time, and Rombach et al. [74] propose the Latent Diffusion Models (LDMs), which transfer the training and inference processes to a compressed lower-dimension latent space for more efficient computing.\nDiffusion models have become increasingly popular in multimodal generation incorporated with CLIP [75] due to their ability to generate data with desirable qualities while covering a wide range of distributions. Application fields of the diffusion model vary from text-based image generation [76], [77], [78], [79], textbased video generation [12], [80], [81], [82], [83], text-based audio generation [84], [85], text-based 3D representation generation [86], [87], [88], and many others. In this paper, we build our framework on the diffusion model and focus on animating the source face by multimodal geometry guidance, i.e., text, audio, image, and video, reflecting on facial expression and pose."
        },
        {
            "heading": "3 PRELIMINARIES",
            "text": ""
        },
        {
            "heading": "3.1 Denoising Diffusion Probabilistic Models (DDPMs)",
            "text": "DDPMs follow the idea of latent variable models that consist of a forward diffusion process and a reverse diffusion process. Specifically, a diffusion process gradually adds noise to the data sampled from the target distribution x0 \u223c q(x0) as a Markov chain. Each step q (xt | xt\u22121) (for t \u2208 {0, . . . , T}) is defined as\na Gaussian distribution with a fixed or learned variance schedule \u03b2t \u2208 (0, 1):\nq (xt | xt\u22121) := N (\u221a 1\u2212 \u03b2txt\u22121, \u03b2tI ) . (1)\nBy the Bayes\u2019 rules and Markov property, the latent variable xt can be expressed as:\nq (xt | x0) = N ( xt; \u221a \u03b1\u0304tx0, (1\u2212 \u03b1\u0304t) I ) , (2)\nwhere \u03b1\u0304t = \u220ft s=1 \u03b1s, and \u03b1t = 1\u2212\u03b2t. Then, the reverse process q (xt\u22121 | xt) can be parametrized by another Gaussian transition:\np\u03b8 (xt\u22121 | xt) := N (xt\u22121;\u00b5\u03b8 (xt, t) , \u03c3\u03b8 (xt, t)) , (3)\nwhere \u00b5\u03b8 (\u00b7) and \u03c3\u03b8 (\u00b7) are predicted by the trained deep neural networks \u03b8 , which is optimized under the objective Ex, \u223cN (0,1),t [ \u2016 \u2212 \u03b8 (xt, t)\u201622 ] . Thus, given xt, xt\u22121 can be sampled by using:\nxt\u22121 = 1\u221a\n1\u2212 \u03b2t\n( xt \u2212\n\u03b2t\u221a 1\u2212 \u03b1t \u03b8 (xt, t)\n) + \u03c3tz, (4)\nwhere z \u2208 N (0, I). Furthermore, according to [73], x0 can be approximate derived by xt and \u03b8 (xt, t):\nx\u03020 := xt \u2212 \u221a 1\u2212 \u03b1t \u03b8 (xt, t)\u221a\n\u03b1t . (5)\nThis facilitates the use of pixel-level and perceptual losses during the training stage in Sec. 4.3."
        },
        {
            "heading": "3.2 3D Morphable Models (3DMMs)",
            "text": "Recent methods estimate the 3D face descriptors of 2D images by optimizing a neural network to extract 3D parameters from a face image. Thus we follow the previous work D3DFR [17] that adopts ResNet50 as the backbone to predict 3DMM coefficients, which consists of identity \u03b1 \u2208 R80, expression \u03b2 \u2208 R64, texture \u03b4 \u2208 R80, illumination \u03b3 \u2208 R27, and pose p \u2208 R6. Note that the original 3DMM fails to control gaze direction, we explicitly model the gaze like [89], providing the normalized direction vector from the center of the eye to the pupil in four dimensions \u03c9 \u2208 R4. Therefore, given an input face I , the output coefficients \u03c1 \u2208 R261:\n\u03c1 = D(I) = {\u03b1,\u03b2, \u03b4,\u03b3,p,\u03c9} . (6)\nWith 3DMM, the 3D shape S and albedo texture T could be parameterized as:\nS = S\u0304 + Bid\u03b1+ Bexp\u03b2, T = T\u0304 + Bt\u03b4, (7)\nwhere S\u0304 and T\u0304 denote the mean face shape and albedo texture. Bid, Bexp, and Bt are the bases of identity, expression, and texture computed via PCA. We project the reconstructed 3D face onto the 2D image plane with a differentiable renderer R according to its illumination \u03b3 and pose p:\nI3d = R(S,T,\u03b3,p). (8)\nWe naturally choose the rendered image I3d as the intermediate geometry condition in Sec. 4.2 due to its several appealing properties: 1) Compared with other structural representations, e.g., landmarks and segmentation maps, 3DMMs provide an explainable and disentangled parameter space, which enables direct recombine corresponding factors when conducting the specific face manipulation task. Besides, mapping other cues to 3DMMs is\nmuch easier since no additional spatial information is required. 2) Rendered face images provide more detailed semantic and explicit geometry than vectorized parameters, thus reducing the training difficulty. We conduct extensive experiments in Sec. 5.2."
        },
        {
            "heading": "4 METHOD",
            "text": ""
        },
        {
            "heading": "4.1 Overview",
            "text": "As shown in Fig. 2, multimodal-driven talking face generation aims to produce realistic videos according to given source identity and multimodal geometry conditions, i.e., text, audio, image, and video. In Multimodal-driven Geometry Condition, we employ rendered faces projected from 3DMMs as the intermediate structural representation. To further exploit the potential of text in this task, we represent the emotion style in text prompts, which could inherit rich semantics from the large-scale pre-trained models for flexible and generalized emotion control (Sec. 4.2). To enable multimodal conditions to share the same generator, we propose a powerful paradigm, termed Texture-Geometry-aware Diffusion Model (TGDM), which is based on the multi-conditional diffusion model, allowing complex texture transfer for high-fidelity face generation, and avoids unstable GAN-based training (Sec. 4.3). Finally, we extend TGDM to face swapping and derive a new paradigm for stable and effective training and inference (Sec. 4.4). In the following, we will supply more details."
        },
        {
            "heading": "4.2 Multimodal-driven Geometry Condition",
            "text": "Image-driven and Video-driven Conditions. For image driving, we combine the appearance-related 3DMM coefficients (identity, texture, and illumination) from the source image Is with the motion-related coefficients (expression, pose, and gaze) from the driving image Id to construct the desired 3D face descriptors \u03c1\u0302 = {\u03b1s,\u03b2d, \u03b4s,\u03b3s,pd,\u03c9d}, along with its rendered face I3d as the geometry conditions. For video driving {I1,d, I2,d, \u00b7 \u00b7 \u00b7 , IN,d}, we can treat them as isolated N images for processing. However, parameters from a single input frame will cause jitter and instability in the final generated video due to the inevitable prediction errors between consecutive frames. To alleviate this problem, like [8], we introduce a windowing strategy for better temporal consistency, i.e., the parameters of the adjacent frames are also used as descriptors of the central frame to smooth the motion trajectory. In practice, the coefficients of a window with continuous frames \u03c1\u0302 \u2261 \u03c1\u0302i\u2212k:i+k and the rendered frame of the central frame as the geometry conditions, where k is the radius of the window and set to 1 experimentally. Audio-driven and Text-driven Conditions. Audio-driven talking face generation is expected to maintain lip movements synchronized with input speech contents and synthesize natural facial motion simultaneously. Consequently, it raises two challenges, one is precise audio-to-lip mapping, and the other is highly temporal consistent. Unlike previous works that adopt LSTM [8], [90] or GRU [91], [92] to autoregressively deduce expression coefficients, we adopt the non-autoregressive Transformer [93] to capture the short- and long-term audio context and provide the sequencelevel representations for more accurate and temporal-coherent coefficients regression. Besides, emotion style also plays a crucial role in generating vivid talking face. For emotion representation, the one-hot coding [47] is in a fixed pattern and fails to convey the semantics cues contained in the label, while recent methods [2], [11] extract emotion embedding from given images and audio,\nlacking generalizing to unseen styles due to the limited semantics. In contrast, we represent the emotion style in the text prompt and borrow help from CLIP to deliver the semantic cues. Thus our method inherits rich semantic knowledge and convenient interaction after various emotion styles are encoded by CLIP.\nTo this end, we propose the Emotional Audio to Expression (EmoA2E) module. Specifically, as shown in Fig. 3, the Mel-frequency Cepstral Coefficients (MFCC) clips A = {A1, . . . ,AN} provide the cues of lip movement, a non-learnable extended token takes identity information \u03b1 as input to connect the expression motion to the specific person, and the emotion embedding zemo produced from the fixed CLIP text encoder as the emotion condition. Besides, instead of utilizing a one-hot coding [47] to control the emotion intensity, we further prepend a learnable intensity token \u03d5, which is the product of the base learnable intensity vector and intensity scalar:\n\u03d5 = \u03b7\u03d5base, (9)\nwhere \u03b7 \u2208 {1, 2, 3} at the training phase, and it can be a continuous random value range from 1 to 3 during the testing phase. Typically, the audio sequence and identity token are first embedded into the hidden dimension, then together with the prefix token \u03d5 to be added with standard positional PE and emotion embeddings:\n\u03b2\u0304 = T ([\u03d5,MLP(\u03b1),MLP(A)] + PE + MLP(zemo)), (10)\n\u03b2\u0302 = MLP(\u03b2\u0304). (11)\nWe train EmoA2E independently by two losses. First, we define an expression Reconstruction Loss Lrec to calculate the distance between the predicted \u03b2\u0302 and ground truth \u03b2:\nLrec = \u2225\u2225\u2225\u03b2 \u2212 \u03b2\u0302\u2225\u2225\u2225\n2 . (12)\nBesides, we further select 68 points from the original 3DMM \u03c1 and modified one \u03c1\u0302, obtaining l and l\u0302, respectively. We define Landmark Loss to measure the similarity between them:\nLlm = \u2225\u2225\u2225l\u2212 l\u0302\u2225\u2225\u2225\n2 . (13)\nThus, the total loss is defined as follows:\nL = \u03bbrecLrec + \u03bblmLlm, (14)\nwhere \u03bbrec = 100 and \u03bblm = 0.1."
        },
        {
            "heading": "4.3 Texture-Geometry-aware Diffusion Model",
            "text": "Most recent GAN-based methods are source-oriented that explicitly model the deformation to animate the source into the driving pose and expression. However, it is still quite challenging to achieve the accurate desired geometry and capture the complex identity appearance when under various extreme conditions, such as large pose, yielding noticeable artifacts and degradation problems. Thus, we revisit this task and propose the target-oriented Texture-Geometry-aware Diffusion Model (TGDM), which focuses on transferring the source texture to the rendered geometry face and inherits the flexibility and fidelity of diffusion models. In this part, we give the descriptions of the network structure and the training details for the denoising process. Architecture. Following the [94], our conditional denoising model \u03b8 is designed by the UNet-based backbone, consisting of the encoder \u03a8E and decoder \u03a8D . As shown in Fig. 2, TGDM\n\ud835\udc70\ud835\udc56\u22121,\ud835\udc60\n\ud835\udc700,\ud835\udc60\n\ud835\udc81\ud835\udc56,\ud835\udc61\u22121\n\ud835\udf50\ud835\udf03(\ud835\udc81\ud835\udc56,\ud835\udc61, \ud835\udc703\ud835\udc51 \ud835\udc56\u22121,\ud835\udc56, \ud835\udc6d\ud835\udc56,\ud835\udc60, \u0ddd\ud835\udf46 \ud835\udc56\u22121,\ud835\udc56, \ud835\udc61) \uff0c\n\ud835\udf50\ud835\udf03(\ud835\udc81\ud835\udc56,\ud835\udc61, \u2205, \u2205, \u2205, \ud835\udc61)\n\ud835\udf50\ud835\udf03(\ud835\udc81\ud835\udc56,\ud835\udc61, \ud835\udc703\ud835\udc51 0,\ud835\udc56 , \ud835\udc6d0,\ud835\udc60,\u0ddd\ud835\udf46 0,\ud835\udc56, \ud835\udc61) \uff0c \ud835\udc81\ud835\udc56,\ud835\udc61\n\u2026 \u2026\n\ud835\udc70\ud835\udc56\u22121,\ud835\udc60 \ud835\udc70\ud835\udc56\n\ud835\udc70\ud835\udc56\u22121,\ud835\udc60\n\ud835\udf50\ud835\udf03(\ud835\udc81\ud835\udc56,\ud835\udc61, \ud835\udc703\ud835\udc51 \ud835\udc56\u22121,\ud835\udc56, \ud835\udc6d\ud835\udc56,\ud835\udc60, \u0ddd\ud835\udf46 \ud835\udc56\u22121,\ud835\udc56,, \ud835\udc61) \uff0c \u2026 \u2026\n\ud835\udc70\ud835\udc56\u22121,\ud835\udc60 \ud835\udc81\ud835\udc56,\ud835\udc61 \ud835\udc81\ud835\udc56,\ud835\udc61\u22121 \ud835\udc70\ud835\udc56\nNext frame \ud835\udc70\ud835\udc56:= \ud835\udc70\ud835\udc56\u22121,\ud835\udc60\nTransformer \ud835\udce3\nLinear\nLinear\nPE\n\ud835\udc681 \ud835\udc68\ud835\udc41\ud835\udf4b\n\ud835\udf371 \ud835\udf37\ud835\udc41\u2026\n\u2026\nCLIP\n\ud835\udc9b\ud835\udc86\ud835\udc8e\ud835\udc90\n\u201cAngry\u201d\nLinear\n\ud835\udf36\nLearned\nFixed Next frame \ud835\udc70\ud835\udc56:= \ud835\udc70\ud835\udc56\u22121,\ud835\udc60\n\ud835\udc700,\ud835\udc60\n\ud835\udf50\ud835\udf03(\ud835\udc81\ud835\udc56,\ud835\udc61, \ud835\udc703\ud835\udc51 0,\ud835\udc56 , \ud835\udc6d0,\ud835\udc60, \u0ddd\ud835\udf46 0,\ud835\udc56,, \ud835\udc61) \uff0c \u2026 \u2026\n\ud835\udc81\ud835\udc56,\ud835\udc61 \ud835\udc81\ud835\udc56,\ud835\udc61\u22121\ud835\udc700,\ud835\udc60\nDenoising Process\n\ud835\udc700,\ud835\udc60\n\uff08a\uff09\n\uff08b\uff09\n\uff08c\uff09\n\ud835\udc70\ud835\udc56\nResample\nFig. 3. The architecture of EmoA2E module. Removing the emotion embedding zemo and intensity \u03d5 inputs, this structure is used for emotion-free audio-to-expression learning.\nis conditioned on three external inputs. First, the texture encoder \u03a6E provides the multiscale features F s = { F 0s, . . . ,F k s } to provide the desired texture patterns, where k is 1, i.e., we adopt two resolution texture features in 16 \u00d7 16 and 32 \u00d7 32. To mix the source texture within the noise prediction branch and eliminate the effects of misalignment, we design the Texture Attention-based (TexAtt) module that employs the cross-attention mechanism for better integration. As shown in Fig. 2, each TexAtt receives the source texture feature F is and the noise feature F i d, the query is extracted by one convolution from F id, and the key and value are extracted from F is in the same way, obtaining Qd,Ks,V s \u2208 RCi/4\u00d7Hi\u00d7Wi with reduced channel numbers. Then Qd and Ks are used to calculate the correlation matrix M , which further multiplies V s to obtain F i s\u2192d. A zero-initialized learned scale parameter \u03c4 is applied onF is\u2192d to control the source\ntexture transfer flow when added to the F id:\nF is\u2192d = softmax(Qd(Ks) T )V s = MV s, (15)\nF\u0302 id = \u03c4F s\u2192d + F i d. (16)\nThen, the spatially aligned rendered face I3d is concatenated channel-wise with the noisy faceZT , which is obtained by adding noise to Id according to Eq. 2. They are fed to the first layer of the network to guide the denoising process, ensuring the intermediate noise and the output face follow the given facial geometry. In addition, the modified coefficients \u03c1\u0302 further supplement the implicit geometry cues, especially the gaze direction not included in the rendered face. It added with embedded time, forming the last condition C = Linear(\u03c1\u0302) + Linear(t), which is injected into the noise predictor via the adaptive instance normalization (AdaIN) [7]:\nAdaIN(F id,C) = \u03c3c(C) F id \u2212 \u00b5(F i d)\n\u03c3(F id) + \u00b5c(C), (17)\nwhere \u00b5(\u00b7) and \u03c3(\u00b7) is the average and variance operation of the input feature F id respectively. \u00b5c(\u00b7) and \u03c3c(\u00b7) are used to estimate the adapted mean and bias according to the given condition. To this end, all condition information is properly integrated into the network \u03b8(Zt,F s, I3d, \u03c1\u0302, t) to predict the noise for talking face generation. Objectives. We first adopt the regular simple Denoising Loss:\nLsimple = \u2016 \u2212 \u03b8(Zt,F s, I3d, \u03c1\u0302, t)\u20162 , (18)\nwhere is an added noise on Id. Besides, we estimate the fully denoised face Z\u03020 according to the Eq. 5, which enables further constraints on the image level. Concretely, we measure the difference between Z\u03020 and Id at the pixel and perceptual level by\na Reconstruction Loss Lrec as L2 distance and a Perceptual Loss as the LPIPS loss [95]:\nLrec = \u2225\u2225\u2225Z\u03020 \u2212 Id\u2225\u2225\u2225\n2 , (19) Lp = \u2225\u2225\u2225\u03c6vgg(Z\u03020)\u2212 \u03c6vgg(Id)\u2225\u2225\u2225\n2 , (20)\nwhere \u03c6vgg(\u00b7) represents the pre-trained VGG16 [96] network. Thus, the total loss is defined as follows:\nL = \u03bbsimpleLsimple + \u03bbrecLrec + \u03bbpLp, (21)\nwhere \u03bbsimple = 10, \u03bbrec = 1, and \u03bbp = 1."
        },
        {
            "heading": "4.4 A Novel Face Swapping Paradigm Built on TGDM",
            "text": "Despite the impressive progress of recent methods, GAN- and diffusion-based face swapping methods still suffer from the dilemma that the improvement of source face identity consistency at the expense of sacrificing target attribute preservation. For example, DiffFace [20] employs identity and attribute expert models to guide the noise prediction, and the balance between them is critical to producing high-quality swapped faces. However, it is complex and needs many experimental attempts. We attribute this phenomenon to the training phase playing the seesaw-style game, which struggles to balance all identity-unrelated attributes preservation and the source identity fusion. Since our proposed method for talking face is able to transfer complex textures, we derive a novel paradigm for face swapping built upon the TGDM.\nSpecifically, as shown in the top of Fig. 4, there are two modifications. First, we completely mask the face region of the source texture image with the help of the mask predictor M [97] to ensure that the ground truth identity information is not visible to the network. Second, because of the low-dimensional linear representation of 3DMMs, the rendered images often lack\nphoto-realism and fine texture details like wrinkles. We further supplement the identity embedding from the expert identity model G [98]. In this way, the renderer image I3d, identity embedding zid, and Linear(\u03c1) focused on affording identity cues and identity-unrelated attributes of the face region, while Im makes up for the absence of hair and background. Notably, the mouth area is also served as the background, which is discussed in the Sec. 5.4. During training, as Eq. 21, our scheme does not require complex losses. Instead, the reconstruction loss is sufficient. The hyperparameter setting is the same as Eq. 21 either. For inference, given the source Is and the target It, we first render the I3d with the identity factor of the source and the remaining parameters of the target. As shown in the bottom of Fig. 4, I3d is sensitive to the geometric structure, exhibiting the exact desired face shape, and zid contains source identity semantics. Combining both of them guarantees identity similarity. To this end, following the standard denoising process, our method successfully transfers the source geometry- and semantic-aware identity information to the target, while fully keeping the identity-unrelated attributes without any complex sampling tricks."
        },
        {
            "heading": "5 EXPERIMENT",
            "text": ""
        },
        {
            "heading": "5.1 Datasets and Implementation Details",
            "text": "Datasets. For talking face generation, we leverage the VoxCeleb1 [99] dataset, which contains over 20K videos. Among them, We select the high-resolution (720P) ones and follow the preprocessing method in FOMM [3] to crop the videos and resize them to 256 \u00d7 256, obtaining 17,927 training videos and 491 testing videos. For emotional talking face generation, we adopt the MEAD [47] dataset, which contains eight emotion types (neutral, angry, contempt, disgusted, fear, happy, sad, and surprised) and three intensity levels (levels 1, 2, 3). We randomly select 36 identities of front-view video clips for training and the rest for testing. For face swapping, we utilize the high-quality CelebAMask-HQ [100] dataset, which has 30,000 images with fine-grained mask annotation. FaceForensics++ [101] is used for testing, which is a forensics dataset consisting of 1000 videos. Metrics. For face reenactment, we use PSNR and LPIPS [95] to evaluate reconstruction quality. Exp, Angle, and Gaze are used to calculate the average Euclidean Distances of corresponding coefficients between the generated and target faces. We employ ID embeddings extracted by Curricularface [98] (ID-C) and Arcface [102] (ID-A) to measure identity cosine similarity. We further use FID [103] to evaluate the realism of the generated faces. For talking face generation, in addition to the metrics mentioned above, we use Landmarks Distance (LMD) [104] around the mouth, and the confidence score (Sync) proposed in SyncNet [105] to measure the accuracy of mouth shapes and lip synchronization. We further use Emotion Feature Distance (EFD) to measure the accuracy of the emotion representation, which is extracted by [106]. For face swapping, we adopt Exp, Angle, ID-A, and FID for evaluation. We do not adopt ID-C since Curricularface has been used in training and inference. Implementation Details. For EmoA2E, we randomly sample consecutive K = 32 clips for emotion-condition training in MEAD and emotion-free training in VoxCeleb1. We use a learning rate of 0.0002 and 128 batch sizes with the Adam optimizer on one V100 GPU for 200K iterations. For TGDM, we randomly sample the source and target faces from the same video in MEAD and VoxCeleb1 for training. It takes about 4 days by using 4\nV100 GPUs with 8 batch sizes and a 0.0002 learning rate for 200K iterations. For face swapping, we train its model as the aforementioned setting for approximately 3 days. For the diffusion model, the length of the denoising step T is set to 1000, and a linear noise schedule is adopted for both the training and inference process. Notably, to stale the training procedure, only MSE loss of noise is used at the beginning of the training. Only when it has been decreased below 0.05, MSE loss of image, and LIPIS loss then start to work. Besides, the UNet of TGDM receives 256\u00d7256 resolution images and performs 16 down-sample ratios."
        },
        {
            "heading": "5.2 Face Reenactment",
            "text": ""
        },
        {
            "heading": "5.2.1 Comparison with Baselines",
            "text": "Qualitative Results. We perform qualitative comparisons with FOMM [3], PIRenderer [8], NTHS [107], HifiHead, TPSM [9], and DAM [10] in the Cross-Identity setting, where the source and the target are of different identities. We do not compare with StyleHeat [46] since it requires the aligned inputs due to the fixed StyleGAN generator. As shown in Fig. 5, we sample nine pairs from VoxCeleb1 for visualization. First, the top three pairs have a significant difference in face size. It can be seen that FOMMbased methods, e.g., TPSM and DAM, produce over-smooth facial textures and suffer from noticeable warping artifacts. HifiHead could generate realistic faces, but their poses are inconsistent with the target. By contrast, the results of our method are of high quality and with the desired attributes. Second, the target faces of the middle ones show rich micro-expressions. Recent methods just imitate mouth shape and head direction, and they ignore the emotion embodied in the target. For example, the target of the fourth row is surprised, and the sixth is contempt. For comparison, our results exhibit accurate emotion styles, i.e., surprised forehead lines, delighted mouth corners, and disdainful eyes. Finally, the bottom pairs suffer from occlusions in the source or the target. It is difficult for FOMM-based methods to estimate the precise key\nInput Source\nFig. 7. Attention visualization of TextAtt. The color bars indicate activation values. The points in the input rendered face could correctly match similar semantic and geometrical areas in the source.\npoints, thus usually resulting in extremely distorted facial shapes (the head area of row 7). Other methods also struggle to animate the occluded objects to fit the desired pose. Benefiting from the effective cross-attention mechanism, our method is not sensitive to occlusion and reasonably preserves the non-facial parts in the generated results (the headphones of row 8 and the hat of row 9). Moreover, these cases are all under large-pose conditions, which convincingly demonstrate that our method successfully transfers the source texture to the target rendered image, providing more realistic results with accurate pose and detailed expression while preserving the source identity. Quantitative Results. We quantitatively compare the proposed method with several aforementioned SOTA methods both in SameIdentity and Cross-Identity settings. We randomly sample 200 identities from the test set and set 5 random seeds to generate\n1K pairs in total. The results are summarized in Tab. 1. Benefiting from the explicit facial representation contained in the rendered face, our methods achieve an impressive performance of facial attributes, indicating that our model can animate the source face that is highly faithful to the given structure cues. Furthermore, our method is favorable against other methods regarding the reconstruction metrics PSNR and LPIPS and image quality metric FID. HifiHead obtains the lowest FID due to the StyleGAN-based generator. Besides, it also shows the best identity consistent but suffers from severe pose error, which can be concluded from rows 2 and 8 of Fig 5 either. Overall, the above observations are consistent with the qualitative results in Fig. 5."
        },
        {
            "heading": "5.2.2 Ablation Study and Analysis",
            "text": "Ablation Study. We perform qualitative and quantitative experiments to validate the merits of the proposed designs. Specifically, we design two variations to evaluate the effectiveness of TextAtt. Specifically, we adopt the image-level and feature-level concatenation for feature injection as two baselines. For a fair comparison, we train our method and two baselines with the same setting, e.g., same batch sizes and training iterations. As shown in columns 3 and 4 of Fig. 6, these two baselines are able to generate the desired pose and expression, but they have a limited ability to retain the source appearance, exhibiting severe color jitting, especially the feature-level concatenation. To further verify the necessity of the rendered face I3d, we only use the 3D face descriptors \u03c1 to supply the facial geometry information. Comparing the results of columns 5 and 6, we can observe that the implicit representation is insufficient to effectively support geometry alignment. Contrary to the above competitors, our results show higher quality, which illustrates the effectiveness of cross attention as the feature transfer module and rendered image as the explicit geometry condition, both reducing the difficulty of training and speeding up the convergence of the model. Besides, the above observations could also be summarized from Tab. 2, our proposed method improves all metrics by a large margin. Interpretability of TextAtt. To better understand the crossattention mechanism, we visualize the attention maps of the TextAtt in UNet middle block, which is 16 \u00d7 16 resolution. As\nS o u r c e\nHappy \u2193\nContempt\nHappy \u2193\nDisgusted\nHappy \u2193\nSurprised\n\ud835\udf3c = \ud835\udfcf\n\ud835\udf3c = \ud835\udfd0 .\ud835\udfd3\nS o u r c e\nH a t r e d\nS a d l y s u r p r i s e d\nH a p p y + S u r p r i s e d\n\uff08a\uff09 \uff08b\uff09\nS o u r c e\n\ud835\udf07 = 2.5\nHappy \u2193\nContempt\nHappy \u2193\nDisgusted\nHappy \u2193\nSurprised\n\ud835\udf3c = \ud835\udfcf\n\ud835\udf3c = \ud835\udfd0 .\ud835\udfd3\nS o u r c e\nH a t r e d\nS a d l y s u r p r i s e d\nH a p p y + S u r p r i s e d\n\uff08a\uff09 \uff08b\uff09\nFig. 9. (a) The visualization of unseen emotion style. Rows 2 and 3 (in Red) are the compound styles, and row 4 (in Blue) is a new style. (b) Results of different emotion styles and intensity levels.\nshown in Fig. 7, we select three points from different regions in the noise feature, i.e., head, face, and background. The visualized attention maps indicate that each location pays more attention to the geometrically and semantically similar areas, e.g., the red point is sampled from the head region, which has a higher response with the corresponding region of the source feature. Consequently, the such attention-based design allows the explicit texture transfer to achieve photo-realistic and identity-consistent face generation."
        },
        {
            "heading": "5.3 Talking Face Generation",
            "text": ""
        },
        {
            "heading": "5.3.1 Comparison with Baselines",
            "text": "Qualitative Results. We perform qualitative comparisons with Wav2Lip [35], PC-AVS [1], and EAMM [11] for talking face generation. Fig. 8 (a) visualizes the generated frames of these methods. It can be seen that all methods could produce synchronized lip shapes with given audio signals. However, Wav2Lip fails to change the head pose and artifacts appear around the mouth area due to the inevitable blending mismatch. PC-AVS and EAMM only support the aligned faces as inputs. Thus such preprocess\nMethod EFD \u2193 LMD \u2193 Sync \u2191 ID-C \u2191 FID \u2193\nWav2Lip - 3.09 4.86 - - PC-AVS - 3.21 4.65 0.81 30.72 EAMM-Neutral - 3.22 4.60 0.79 37.90 Ours - 3.09 4.91 0.83 26.54\nMEAD 0.084 2.62 3.09 0.81 30.69 EVP 0.106 2.54 3.21 0.70 12.83 EAMM-Emo 0.092 2.50 3.26 0.74 29.01 Ours 0.061 2.36 3.52 0.81 16.33\nTABLE 3. The top part is the quantitative comparison of emotion-free on VoxCeleb1 and the bottom part is the emotion-condition on MEAD dataset.\noperation destroys the original facial structure, reflecting on the misalignment pose with the target in the animated faces. Besides, their results are blurred and lose the sharp source textures. By contrast, our results are highly faithful to the given pose from the images and mouth movements from the audio, while maintaining the source texture well. For emotional talking face generation, we select three frames of two emotion styles in MEAD for comparison. As shown in Fig. 8 (b), Wav2Lip and PC-AVS struggle to generate desired emotions with synchronized lip shapes in this task, while the synthesized images from MEAD are of poor quality. EVP [108] and EAMM suffer identity inconsistency with the source and show less rich expression due to lacking intensity modeling. Benefiting from sufficient emotion semantics learning and the powerful generative capabilities of diffusion models, our method produces more accurate expressions and realistic textures. Quantitative Results. We conduct a quantitative comparison in the reconstruction setting that guarantees access to ground truth for evaluation. For a fair comparison, we align the cropping manner of all the methods. For talking face generation, as shown in the top part of Tab. 3, we do not calculate the ID-C and FID of Wav2Lip since it only generates the mouth region and copies other regions from input faces. Contrary to other methods, our method yields the best motion control, temporal coherence, identity consistency, and\nSource Target Normal Small Dilated Coeff\nimage quality in terms of LMD, Sync, ID-C, and FID. We further compare our emotion-condition pipeline with other emotional talking face generation methods. As shown in the bottom part of the Tab. 3, our method outperforms most metrics except for the FID. EVP achieves higher FID due to the vid2vid-based generator, but it exhibits a weak manipulated ability, which can be inferred from the lower ID-C and Sync, and higher LMD. Moreover, compared with the performance of emotion-free talking face generation and the emotion-condition one, the latter achieves better mouth shape accuracy (lower LMD) due to the limited corpus of MEAD, but the emotions introduce the irregular talking rhythm, leading to the poor synchronization (lower Sync)."
        },
        {
            "heading": "5.3.2 Ablation Study and Analysis",
            "text": "Ablation Study. To verify the effectiveness of the Transformer encoder in EmoA2E, we replace it with stacked fully-connected layers of GRU-based recurrent neural networks. Our method outperforms the above two architectures on LMD metric: 3.54 vs. 2.47 vs. 2.36 of MLPs, GRUs, and Transformers. To further explore the effect of different emotion encoding manners on the unseen emotion style, we use one-hot encoding and language pretrained model GPT2 [109] for evaluation. It is obvious that one-hot fails to represent a new style due to the fixed pattern. GPT2 is not available to the visual cues and struggles to reflect the unseen textual semantics to the image domain. We conduct a quantitative experiment that measures the cosine similarity of the attached sequences in Fig. 9 with the corresponding text prompts when encoded by GPT2 and CLIP. Our method achieves better results: 0.621 vs. 0.430, which demonstrates the superiority of CLIP in handling multimodal information. Generalizing to Unseen Emotion Styles. Unseen emotion styles include compound and totally new styles. As shown in Fig. 9 (a), row 2 shows the results of the given Sadly surprised, and row 3 of the average embedding of Happy and Surprised, which indicates the flexible manipulation for compound emotion. We further present the new style Hatred in the fourth row. The correct\nexhibition of these unseen styles verifies the flexibility and rich semantic priors of the CLIP feature space. Continuous Emotion Style Control. We conduct a qualitative experiment to evaluate the effectiveness of our method for controlling emotion style. As shown in Fig. 9 (b), our approach could change the emotion representation between two distinct styles, rather than previous techniques only taking a neutral face as the source. We increase the intensity value from 1 to 2.5, which shows continuous and accurate expression changes. Please pay attention to the mouth and eyes regions."
        },
        {
            "heading": "5.4 Expanded Application of Face Swapping",
            "text": ""
        },
        {
            "heading": "5.4.1 Comparison with Baselines",
            "text": "We first conduct qualitative experiments to compare our method with DiffFace [20], High-Res [65], InfoSwap [70], MegaFS [107], HifiFace [60], Simswap [59], and FaceShifter [58] on the FaceForensics++ [101] dataset. As shown in Fig. 10, our model outperforms other models in changing identity-related geometry, especially the face shape, and preserving non-identity-related attributes. For example, in the third row, the generated face shape is more similar to the source, while other methods almost contain the same face shape as the target. Also, in the fourth and fifth rows, we totally preserve the non-identity-related attributes like hair and backgrounds. In the first row, our result is more similar to the source than others. Compared with another diffusion-based method DiffFace, our results obviously show the superiority of generating both identity-consistent and attributes-preserving faces, but the visual quality reduces to some degree. This is because our synthesized faces are more faithful to the target, while DiffFace produces clear but inconsistent textures. Moreover, Fig. 11 presents more qualitative comparisons with other SOTA methods that without officially released codes, e.g., StyleFace [68], StyleSwap [69], and FlowFace [110]. Please attention to the area indicated by the red arrow. We further report quantitative results compared to a part of the above method with officially released codes. The results in Tab. 4 also prove that our method is better\nSource Target Ours Diffface High-Res SimSwap HifiFace InfoSwap MegaFS FaceShifter\nconsidering both identity consistency with the source and attribute preservation with the target."
        },
        {
            "heading": "5.4.2 Ablation Study and Analysis",
            "text": "The critical operation of our reconstruction-based face swapping paradigm is to mask the source face to avoid identity information leaking. Thus we report a visualization to explore the effect of the mask area. As depicted in Fig. 12, we design three variations, i.e., the Normal mask covers the all face area, the Small treats the mouth area as the background, and the Dilated mask dilates the Normal mask to cover more areas. There is no apparent difference between the Normal and Small types in terms of identity and attributes by comparing columns 3 and 4, but the Small obtains the more realistic mouth area since it can learn information from the Small masked source. Please pay attention to the red rectangle of row 2. The results of Dilated show the artifacts around the face contour and lead to image degradation. On the basis of these phenomenons, we choose Small mask experimentally. Besides, as shown in column 6 in Fig. 12, we observe that without the rendered face I3d, the color of the swapped results are prone\nto be similar to the source rather than the target, which further demonstrate the necessity of the rendered face as the condition."
        },
        {
            "heading": "6 LIMITATIONS AND FUTURE WORKS",
            "text": "First, almost all generators are based on a single image, and TGDM is no exception, which inevitably introduces temporal inconsistency. To boost the coherence of the generated talking videos, previous works [19] exploit the synthesized image as the source face for the next time step, resulting in a smoother transition between frames since the adjacent frames share the most consistent texture. However, such a frame-by-frame strategy has the problem of error propagation when encountering sudden movements, resulting in face degradation in all subsequent frames. In future work, we will be working on addressing the temporal incoherence of diffusion-based video generation.\nBesides, our method retains some disadvantages of the diffusion model. For example, it takes about 45 ms on one V100 GPU to generate a single face under the T = 1000 DDPM setting, which is unacceptable in the real application. We also do not train the model for a longer time, considering the high consumption of the diffusion model. For efficiency, our model only supports 256\u00d7 256 image generation. Although DDIM [73] and LDMs [74] have alleviated the above problems, we hope to propose an intuitive design like StyleGAN to allow efficient highresolution face generation."
        },
        {
            "heading": "7 CONCLUSION",
            "text": "In this paper, we propose a diffusion-based model to complete multimodal-driven talking face generation, which shows several appealing properties: 1) We adopt the text modal as the talking face emotion representation, which inherits rich semantics from the CLIP, allowing flexible and generalized emotion control. 2) We treat talking face generation as a target-oriented texture transfer task. Our proposed TGDM maintains the faithful textures and undistorted appearance details from the source face and preserves explicit structural information but avoids complex texture deformations, which allows all modals to share the same generator. 3) Our proposed TGDM is also suitable for face swapping, which enables a novel reconstruction-based training paradigm and gets rid of seesaw-style optimization during inference. Our extensive results demonstrate the superiority of the proposed pipeline for various face manipulation tasks."
        }
    ],
    "title": "Multimodal-driven Talking Face Generation via a Unified Diffusion-based Generator",
    "year": 2023
}