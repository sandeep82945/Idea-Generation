{
    "abstractText": "One of the major impediments to the development of new task-oriented dialogue (TOD) systems is the need for human evaluation at multiple stages and iterations of the development process. In an effort to move toward automated evaluation of TOD, we propose a novel user simulator built using recently developed large pretrained language models (LLMs). In order to increase the linguistic diversity of our system relative to the related previous work, we do not fine-tune the LLMs used by our system on existing TOD datasets; rather we use in-context learning to prompt the LLMs to generate robust and linguistically diverse output with the goal of simulating the behavior of human interlocutors. Unlike previous work, which sought to maximize goal success rate (GSR) as the primary metric of simulator performance, our goal is a system which achieves a GSR similar to that observed in human interactions with TOD systems. Using this approach, our current simulator is effectively able to interact with several TOD systems, especially on single-intent conversational goals, while generating lexically and syntactically diverse output relative to previous simulators that rely upon fine-tuned models. Finally, we collect a Human2Bot dataset of humans interacting with the same TOD systems with which we experimented in order to better quantify these achievements.",
    "authors": [
        {
            "affiliations": [],
            "name": "Sam Davidson"
        },
        {
            "affiliations": [],
            "name": "Salvatore Romeo"
        },
        {
            "affiliations": [],
            "name": "Raphael Shu"
        },
        {
            "affiliations": [],
            "name": "James Gung"
        },
        {
            "affiliations": [],
            "name": "Arshit Gupta"
        },
        {
            "affiliations": [],
            "name": "Saab Mansour"
        },
        {
            "affiliations": [],
            "name": "Yi Zhang"
        }
    ],
    "id": "SP:380fd8a5bdf8dadc6a8fa815945f0eabc224bbbf",
    "references": [
        {
            "authors": [
                "Samuel Weinbach."
            ],
            "title": "GPT-NeoX-20B: An opensource autoregressive language model",
            "venue": "Proceed-",
            "year": 2022
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "2020b. Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Pawe\u0142 Budzianowski",
                "Tsung-Hsien Wen",
                "Bo-Hsiang Tseng",
                "I\u00f1igo Casanueva",
                "Stefan Ultes",
                "Osman Ramadan",
                "Milica Gasic."
            ],
            "title": "Multiwoz - a largescale multi-domain wizard-of-oz dataset for taskoriented dialogue modelling",
            "venue": "Conference on Em-",
            "year": 2018
        },
        {
            "authors": [
                "Moya Chen",
                "Paul A Crook",
                "Stephen Roller."
            ],
            "title": "Teaching models new apis: Domain-agnostic simulators for task oriented dialogue",
            "venue": "arXiv preprint arXiv:2110.06905.",
            "year": 2021
        },
        {
            "authors": [
                "Qinyu Cheng",
                "Linyang Li",
                "Guofeng Quan",
                "Feng Gao",
                "Xiaofeng Mou",
                "Xipeng Qiu."
            ],
            "title": "Is multiwoz a solved task? an interactive tod evaluation framework with user simulator",
            "venue": "ArXiv, abs/2210.14529.",
            "year": 2022
        },
        {
            "authors": [
                "Kimmo Kettunen"
            ],
            "title": "Can type-token ratio be used to show morphological complexity of languages",
            "venue": "Journal of Quantitative Linguistics,",
            "year": 2014
        },
        {
            "authors": [
                "Rie Koizumi",
                "Yo In\u2019nami"
            ],
            "title": "Effects of text length on lexical diversity measures: Using short texts with less than 200",
            "venue": "tokens. System,",
            "year": 2012
        },
        {
            "authors": [
                "Takeshi Kojima",
                "Shixiang Shane Gu",
                "Machel Reid",
                "Yutaka Matsuo",
                "Yusuke Iwasawa."
            ],
            "title": "Large language models are zero-shot reasoners",
            "venue": "arXiv preprint arXiv:2205.11916.",
            "year": 2022
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "Bart: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Jiwei Li",
                "Michel Galley",
                "Chris Brockett",
                "Jianfeng Gao",
                "William B. Dolan."
            ],
            "title": "A diversity-promoting objective function for neural conversation models",
            "venue": "North American Chapter of the Association for Computational Linguistics.",
            "year": 2016
        },
        {
            "authors": [
                "Xinmeng Li",
                "Wansen Wu",
                "Long Qin",
                "Quanjun Yin."
            ],
            "title": "How to evaluate your dialogue models: A review of approaches",
            "venue": "arXiv preprint arXiv:2108.01369.",
            "year": 2021
        },
        {
            "authors": [
                "Junying Liang",
                "Yuanyuan Fang",
                "Qianxi Lv",
                "Haitao Liu."
            ],
            "title": "Dependency distance differences across interpreting types: implications for cognitive demand",
            "venue": "Frontiers in Psychology, 8:2132.",
            "year": 2017
        },
        {
            "authors": [
                "Hsien-Chin Lin",
                "Christian Geishauser",
                "Shutong Feng",
                "Nurul Lubis",
                "Carel van Niekerk",
                "Michael Heck",
                "Milica Gasic."
            ],
            "title": "Gentus: Simulating user behaviour and language in task-oriented dialogues with generative transformers",
            "venue": "Proceedings of the 23rd",
            "year": 2022
        },
        {
            "authors": [
                "Ram\u00f3n L\u00f3pez-C\u00f3zar",
                "Zoraida Callejas Carri\u00f3n",
                "Michael F. McTear."
            ],
            "title": "Testing the performance of spoken dialogue systems by means of an artificially simulated user",
            "venue": "Artificial Intelligence Review, 26:291\u2013323.",
            "year": 2007
        },
        {
            "authors": [
                "Philip M McCarthy",
                "Scott Jarvis."
            ],
            "title": "Mtld, vocdd, and hd-d: A validation study of sophisticated approaches to lexical diversity assessment",
            "venue": "Behavior research methods, 42(2):381\u2013392.",
            "year": 2010
        },
        {
            "authors": [
                "Biswesh Mohapatra",
                "Gaurav Pandey",
                "Danish Contractor",
                "Sachindra Joshi."
            ],
            "title": "Simulated chats for building dialog systems: Learning to generate conversations from instructions",
            "venue": "EMNLP.",
            "year": 2021
        },
        {
            "authors": [
                "Jinghui Ouyang",
                "Jingyang Jiang",
                "Haitao Liu."
            ],
            "title": "Dependency distance measures in assessing l2 writing proficiency",
            "venue": "Assessing Writing, 51:100603.",
            "year": 2022
        },
        {
            "authors": [
                "Masanori Oya."
            ],
            "title": "Syntactic dependency distance as sentence complexity measure",
            "venue": "Proceedings of the 16th International Conference of Pan-Pacific Association of Applied Linguistics, volume 1.",
            "year": 2011
        },
        {
            "authors": [
                "Baolin Peng",
                "Chunyuan Li",
                "Jinchao Li",
                "Shahin Shayandeh",
                "Lars Liden",
                "Jianfeng Gao."
            ],
            "title": "Soloist: Few-shot task-oriented dialog with a single pretrained auto-regressive model",
            "venue": "arXiv preprint arXiv:2005.05298.",
            "year": 2020
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu"
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2020
        },
        {
            "authors": [
                "Abhinav Rastogi",
                "Xiaoxue Zang",
                "Srinivas Sunkara",
                "Raghav Gupta",
                "Pranav Khaitan."
            ],
            "title": "Towards scalable multi-domain conversational agents: The schema-guided dialogue dataset",
            "venue": "AAAI Conference on Artificial Intelligence.",
            "year": 2020
        },
        {
            "authors": [
                "Brian Richards"
            ],
            "title": "Type/token ratios: What do they really tell us",
            "venue": "Journal of child language,",
            "year": 1987
        },
        {
            "authors": [
                "Brian Roark",
                "Margaret Mitchell",
                "Kristy Hollingshead."
            ],
            "title": "Syntactic complexity measures for detecting mild cognitive impairment",
            "venue": "Biological, translational, and clinical language processing, pages 1\u20138.",
            "year": 2007
        },
        {
            "authors": [
                "Teven Le Scao",
                "Angela Fan",
                "Christopher Akiki",
                "Ellie Pavlick",
                "Suzana Ili\u0107",
                "Daniel Hesslow",
                "Roman Castagn\u00e9",
                "Alexandra Sasha Luccioni",
                "Fran\u00e7ois Yvon",
                "Matthias Gall\u00e9"
            ],
            "title": "Bloom: A 176bparameter open-access multilingual language model",
            "year": 2022
        },
        {
            "authors": [
                "Jost Schatzmann",
                "Karl Weilhammer",
                "Matt Stuttle",
                "Steve Young."
            ],
            "title": "A survey of statistical user simulation techniques for reinforcement-learning of dialogue management strategies",
            "venue": "The knowledge engineering review, 21(2):97\u2013126.",
            "year": 2006
        },
        {
            "authors": [
                "Weiyan Shi",
                "Kun Qian",
                "Xuewei Wang",
                "Zhou Yu."
            ],
            "title": "How to build user simulators to train RL-based dialog systems",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference",
            "year": 2019
        },
        {
            "authors": [
                "Saleh Soltan",
                "Shankar Ananthakrishnan",
                "Jack FitzGerald",
                "Rahul Gupta",
                "Wael Hamza",
                "Haidar Khan",
                "Charith Peris",
                "Stephen Rawls",
                "Andy Rosenbaum",
                "Anna Rumshisky"
            ],
            "title": "Alexatm 20b: Few-shot learning using a large-scale multilingual seq2seq",
            "year": 2022
        },
        {
            "authors": [
                "Yixuan Su",
                "Lei Shu",
                "Elman Mansimov",
                "Arshit Gupta",
                "Deng Cai",
                "Yi-An Lai",
                "Yi Zhang."
            ],
            "title": "Multi-task pre-training for plug-and-play task-oriented dialogue system",
            "venue": "arXiv preprint arXiv:2109.14739.",
            "year": 2021
        },
        {
            "authors": [
                "Weiwei Sun",
                "Shuyu Guo",
                "Shuo Zhang",
                "Pengjie Ren",
                "Zhumin Chen",
                "M. de Rijke",
                "Zhaochun Ren."
            ],
            "title": "Metaphorical user simulators for evaluating taskoriented dialogue systems",
            "venue": "ArXiv, abs/2204.00763.",
            "year": 2022
        },
        {
            "authors": [
                "Ben Wang",
                "Aran Komatsuzaki"
            ],
            "title": "Gpt-j-6b: A 6 billion parameter autoregressive language model",
            "year": 2021
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Ed Chi",
                "Quoc Le",
                "Denny Zhou."
            ],
            "title": "Chain of thought prompting elicits reasoning in large language models",
            "venue": "arXiv preprint arXiv:2201.11903.",
            "year": 2022
        },
        {
            "authors": [
                "Thomas Wolf",
                "Lysandre Debut",
                "Victor Sanh",
                "Julien Chaumond",
                "Clement Delangue",
                "Anthony Moi",
                "Pierric Cistac",
                "Tim Rault",
                "R\u00e9mi Louf",
                "Morgan Funtowicz"
            ],
            "title": "Transformers: State-of-the-art natural language processing",
            "year": 2020
        },
        {
            "authors": [
                "Qi Zhu",
                "Zheng Zhang",
                "Yan Fang",
                "Xiang Li",
                "Ryuichi Takanobu",
                "Jinchao Li",
                "Baolin Peng",
                "Jianfeng Gao",
                "Xiaoyan Zhu",
                "Minlie Huang."
            ],
            "title": "Convlab2: An open-source toolkit for building, evaluating, and diagnosing dialogue systems",
            "venue": "arXiv preprint",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Automated evaluation of newly developed TaskOriented Dialogue (TOD) systems remains a largely unsolved task in the NLP community. Two primary methods of evaluation have historically been used to test the quality of new TOD systems: dataset-based evaluation and interactive evaluation. Dataset-based evaluation, which is far more common in the literature, consists of using a TOD\n\u2217Work conducted during an internship at Amazon. 3Corresponding author.\nsystem to generate responses to annotated conversation logs from a fixed dataset, such as MultiWOZ (Budzianowski et al., 2018) or the SchemaGuided Dialogue (SGD) dataset (Rastogi et al., 2020). While long used as a benchmark in the TOD community, this type of evaluation fails to capture one key aspect of TOD system usage: the interactive exchange between a TOD system and its users. That is, when testing using benchmark data, the user utterances are fixed and thus cannot adapt to reasonable, though unanticipated, system responses. As discussed in Cheng et al. (2022), the key weakness of this type of evaluation is the possibility of a policy mismatch between the generated TOD system utterance and the benchmark user utterance. In such a scenario, even reasonable and coherent responses by the TOD system may be penalized as they do not match the output expected in the benchmark dataset. Additionally, available TOD benchmark datasets are created using a predefined dialogue schema that limits the domains and intents on which a new TOD system can be tested using benchmark data. For example, the SGD Dataset Rastogi et al. (2020) is limited to twenty common domains, such as banking and weather. This shortcoming reduces the utility of dataset-based evaluation when testing TOD systems developed for novel domains.\nInteractive evaluation, on the other hand, consists of a user simulator interacting with the target TOD system in a turn-by-turn manner, similar to a real conversation with a human user. Interactive evaluation benefits from the ability of the user simulator to adaptively respond to TOD-generated utterances, thereby reducing the likelihood of a policy mismatch between the system-side and simulated user-side utterances. This evaluation setup, while potentially more representative of the type of interaction a TOD system is likely to see in deployment, has heretofore been limited by the difficulty of creating an effective user simulator. However,\nar X\niv :2\n30 9.\n13 23\n3v 1\n[ cs\n.C L\n] 2\n3 Se\np 20\n23\nrecent work, such as that of Sun et al. (2022) and Cheng et al. (2022), has substantially advanced the art in TOD user simulator development. But this recent work still depends on large language models (LLMs) that have been fine-tuned on specific TOD benchmark datasets, thereby limiting the generalizability of the current generation of user simulators to unseen domains.\nIn this paper, we present a user simulator which generates user-side utterances in a TOD conversation using only in-context learning with LLMs, thereby avoiding the fine-tuning stage of system development and removing the requirement of domain-specific conversation data for fine-tuning. In the context of developing a TOD system for a novel domain, the availability of such conversation data cannot be taken for granted. By removing the fine-tuning stage from our proposed user simulator\u2019s development pipeline, we are able to avoid any dependencies on extant domain-specific data. To engage in a conversations, our proposed user simulator requires only a set of predefined goals aligned to the schema of the target TOD system; these goals can be presented in either logical or naturallanguage form, and thus can be hand-written or automatically generated from the defined dialogue schema. While no fine-tuning of the underlying LLM is required in our system setup, simulator performance will depend on choice of LLM; we primarily experiment using GPTNeoX-20B (Black et al., 2022) as the basis for our user simulator, but have also tested our proposed in-context learning setup using GPT3 (Brown et al., 2020a), BLOOM (Scao et al., 2022), and AlexaTM-20B (Soltan et al., 2022).\nWe ask three primary research questions in this paper:\n\u2022 Can in-context learning be effectively used to leverage the power of LLMs for simulating a user interacting with a TOD system?\n\u2022 How do we evaluate such a user simulator to demonstrate that it is generating diverse, \u201chuman-like\u201d utterances, and that it is adaptable across domains?\n\u2022 How do these metrics, as calculated on a user simulator, compare to those metrics calculated on data of humans interacting with deployed TOD systems?\nAs stated above, our goal is to develop a user\nsimulator that generates linguistically diverse, complex utterances that approximate real-world user input to TOD systems. We use automated and human evaluation to demonstrate the user simulator\u2019s ability to generate appropriate and diverse output in many conversational settings, which may be used to test in-development and deployed TOD systems. We collect a dataset of humans interacting with two state-of-the-art TOD systems, which we use to establish a baseline for human-level diversity and goal success. Finally, we present and discuss the shortcomings and limitations of the proposed in-context learning-based user simulator, most importantly the decline in system performance with more complex conversational goals, and discuss possible future directions to improve upon and mitigate these limitations."
        },
        {
            "heading": "2 Related Work",
            "text": "Much of the previous work in user and dialogue simulation for TOD systems has focused on data augmentation for system training. For example, Mohapatra et al. (2021) uses dialogues fully generated by fine-tuned LLMs for data augmentation in low-resource training scenarios. Similarly, Chen et al. (2021) generates complete dialogues in an effort to bootstrap training on unseen domains. This focus on using user simulators to generate training data follows a trend in past user-simulator work, much of which focused on creating new data for reinforcement learning (for example Shi et al. (2019). Thus, despite work such as L\u00f3pez-C\u00f3zar et al. (2007) and Schatzmann et al. (2006), the use of user simulators to test extant dialogue systems is a major gap in the literature, that may be due to the fact that existing models have until recently struggled to generate output that is both reasonable and linguistically diverse (Chen et al., 2021).\nMore recent work, such as Sun et al. (2022), Cheng et al. (2022), and Lin et al. (2022), has begun to close the this gap in the development of end-to-end user simulators for the evaluation of TOD systems. Sun et al. (2022) presents an effective user simulator based on the T5 language model (Raffel et al., 2020), as well as a framework for testing TOD user simulators. Similarly, Cheng et al. (2022) build a user simulator by fine-tuning T5 on data from MultiWOZ. Likewise, Lin et al. (2022) used a fine-tuned version of BART (Lewis et al., 2020) as the basis of its TOD user simulator, which is further refined using reinforcement\nlearning. While these simulators achieve impressive goal-success rates (GSR), their high GSR may actually be a negative in the context of user simulation, as discussed further below. In addition, none of the recent works harness the power of the more recent generation of LLMs (such as GPT3, ChatGPT, OPT, and GPTNeoX), nor do they take advantage of recent work in in-context learning (for example, Kojima et al. (2022); Wei et al. (2022)). Rather these models rely on fine-tuning pretrained language models on annotated, domain-specific dialogue data, thereby limiting their applicability in the testing of new TOD systems for previously unseen domains. While Lin et al. (2022) does report a reasonable degree of adaptability to unseen domains, the need for annotated data to fine-tune the system remains a potential issue when dealing with new dialogue schemata that diverge significantly from the schema used in creating the training data.\nAs discussed in Li et al. (2021), user simulators, if done right, can be a powerful means of evaluating new and in-development dialogue systems. In the present paper, our goal is to harness the power of the newer generation of LLMs along with recent developments in in-context learning to generate linguistically diverse, human-like user-side utterances that can be used to test and evaluate extant dialogue systems in a realistic setting. While we achieve this goal to a limited extent, we also find specific difficulties in the use of in-context learning for TOD user simulation, especially with more complex grounding goals."
        },
        {
            "heading": "3 Evaluation Metrics",
            "text": "We use several metrics to quantify the interaction of both human users and user simulators with extant TOD systems. In all cases, our goal in developing our user simulator is not maximizing these metrics, but rather building a system that simulates humanlevel performance."
        },
        {
            "heading": "3.1 Goal Success Rate",
            "text": "In recent work using user simulators to evaluate TOD systems, such as Cheng et al. (2022), Sun et al. (2022), and Chen et al. (2021), the primary metric used to gauge the performance of user simulators is Goal Success Rate (GSR) also know as Task Success Rate (TSR). In this metric, every dialogue is scored with a 1 or 0 depending on whether or not the final dialogue state satisfies all of the slot requests in the original goal. While this is a rea-\nsonable metric for evaluating the performance of TOD systems, as we want TOD systems to be able to complete all of a user\u2019s requested tasks, it is an imperfect metric for measuring the performance of user simulators, especially when maximizing GSR becomes a measure of system success. Specifically, a user simulator that is trained to maximize GSR may result in a simulator which communicates with a TOD system in an artificial manner designed to maximize the efficiency of information exchange. When designing a system to simulate real-world human interaction with a TOD system, our goal is not to maximize the ability of the simulator to convey information to the TOD system with which it is interacting. Rather, we wish to build a simulator that communicates in a naturalistic manner. As such, our goal is to achieve a GSR comparable to that of human interlocutors speaking to the TOD system, as this \u201cGSR correlation\u201d indicates a simulator that provides information to the TOD system with efficiency similar to humans. After all, our end goal in developing a user simulator is creating a system that is able to test the abilities of a deployed or in-development TOD system in a manner similar to how the system will be challenged by human users \u201cin the wild.\u201d For this reason, in this work we do not seek to maximize GSR; rather we aim to create a system that achieves a reasonable GSR that is in line with what we see in our collected human data."
        },
        {
            "heading": "3.2 Diversity Metrics",
            "text": "We propose using a set of metrics to quantify the lexical and syntactic diversity of our proposed user simulator. Sun et al. (2022) has proposed using Distinct score (Li et al., 2016) to measure lexical diversity in user simulator output; however, this measure is essentially an n-gram version of typetoken ratio (TTR), and thus is likely to suffer from the well-known issues with TTR, namely that TTR is not normalized for text length and is known to be unreliable for comparing dialogues of varying length (Richards, 1987; Kettunen, 2014). To avoid these issues relative to lexical diversity, and to take into account issues of syntactic complexity and diversity, we adopt a set of metrics frequently used in the language acquisition and linguistic analysis research, namely:\n\u2022 MTLD: Measure of Textual Lexical Diversity, or MTLD, (McCarthy and Jarvis, 2010), is a modified version of TTR which is normalized for length effects. As mentioned above,\nTTR is well known to be impacted by length of the target text; that is, as texts increase in length the TTR begins to level out or even drop (Richards, 1987). MTLD addresses this issue and has been shown effective in measuring the lexical diversity of texts as short as 100 tokens (Koizumi and In\u2019nami, 2012). Thus, MTLD allows us to consistently measure lexical diversity at both dialogue and dataset levels.\n\u2022 Mean dependency distance: Mean dependency distance, the average number of words spanned by dependency links in a given text, is a well-established measure of syntactic diversity (Oya, 2011), used for measuring cognitive load (Roark et al., 2007) as well as language development (Ouyang et al., 2022).\n\u2022 Dependency distance standard deviation: We report the standard deviation in the dependency distance of our user utterances as a proposed metric of syntactic diversity, as used in Liang et al. (2017)."
        },
        {
            "heading": "4 Human2Bot Dataset",
            "text": "As previously mentioned, GSR alone is not a reasonable metric for evaluating the performance of user simulators. Rather, we argue that GSR must be considered relative to the goal success seen in human interaction with deployed TOD systems. Additionally, we wished to have a baseline against which to compare the lexical and syntactic diversity of output from our user simulator as well as those presented in previous work. To that end, we engaged eight human subjects to converse with two deployed TOD systems: PPTOD (Su et al., 2021) and Soloist (Peng et al., 2020). As with our user simulator, we provided the participants with goal information drawn from MultiWOZ, and asked them to use these goals to engage in conversations with the target TOD system. In addition to the goal information, subjects were asked to include a series of conversational complexities, such as providing two values for a single slot or changing a previously stated slot value at a later point in the conversation. These complexities encourage diversity in our collected dataset and serve to provide reference data for similar human behavior \u201cin the wild\u201d.\nAll human participants are native English speakers who regularly work in data collection efforts for this work\u2019s sponsor. Conversations were collected using a text-based dialogue interface with\nthe target TOD system. In this way, we collected 1165 human2bot dialogues, 410 interacting with Soloist and 755 interacting with PPTOD. We use these collected dialogues to calculate the humanlevel GSR, lexical diversity, and syntactic diversity reported in the Human column of Table 3."
        },
        {
            "heading": "5 Interactive User Simulator",
            "text": "One of our key goals in the present work is to test the ability of large pretrained language models (LLMs) to generate reasonable, linguistically diverse, and \u201chuman-like\u201d user-side utterances using only in-context learning in a few-shot setting. The primary motivation for our choice to use incontext learning lies in the fact that, for many novel TOD domains, no annotated human-created dialogue datasets are available for model fine-tuning. Thus, we want to create a user simulator that is able to function without the need for fine-tuning, and that is able to use only a handful of example dialogues to ground its generation. To that end, we take inspiration from the approaches described in recent work on in-context learning with LLMs, such as Kojima et al. (2022) and Wei et al. (2022) to build a user simulator which is able to effectively engage with extant TOD systems, especially in settings with less complex conversational goals.\nWe have developed an interactive system which allows our user simulator to converse directly with deployed TOD systems, including end-to-end systems like PPTOD (Su et al., 2021) and Soloist (Peng et al., 2020), as well as modular systems such as the ConvLab2 (Zhu et al., 2020) pipeline TOD system. Additionally, we are able to run our user simulator in a \u201cgold\u201d setting wherein the user simulator generates only a single user turn given gold dialogue history data from a target TOD evaluation dataset (such as MultiWOZ (Budzianowski et al., 2018) or SGD (Rastogi et al., 2020)), thereby requiring no direct interaction with a deployed TOD system. This latter approach is useful in testing the language generation capabilities of our simulator in isolation.\nOur system design, as shown in Figure 2, consists of a context accumulator and simulator module which formats the provided example dialogues and current conversational goal prompt and queries an LLM to generate a user utterance. This utterance and dialogue history are then provided to the target TOD system to generate a system response. The generated utterance and the system response\nprovided by the TOD system are then added to the context accumulator, and the process repeats until an end of dialogue token is generated by the user simulator. We provide two example conversational goals with their corresponding dialogue histories to the LLM for in-context learning, along with a new goal prompt to initiate the generation of userside utterances in the target conversation. While we believe that additional examples for in-context learning may improve system performance, we are constrained by the token input limits of the LLM models we are using. An example initial prompt is shown in Figure 1.\nAs the user simulator code is based on Huggingface Transformers (Wolf et al., 2020), we are able to use a wide variety of generative LLMs as the basis for the simulator. We have primarily tested our system using GPTJ-6B (Wang and Komatsuzaki, 2021) and GPTNeoX-20B (Black et al., 2022), though we have additionally run limited experiments with GPT-3 davinci (Brown et al., 2020b) (using the OpenAI API) and AlexaTM-20B (Soltan et al., 2022)."
        },
        {
            "heading": "6 Experiments",
            "text": "Our driving goal in the present work is to build a user simulator which requires no domain-specific training data and which is able to generate \u201chumanlike\u201d utterances as measured by goal success, lexical diversity and syntactic diversity. In pursuit of this goal, we tested multiple combinations of\nprompt design and model selection. In terms of GSR, we found our best performing LLM to be GPT-NeoX-20B, as shown in Table 1. As such, we settled on GPT-NeoX-20B to proceed with experiments comparing our user simulator to both humanlevel performance in TOD interaction and to previous user simulator work. This and all reported experiments were conducted using data from MultiWOZ 2.2 (Budzianowski et al., 2018)\nWe similarly tested variation in system performance based on the form of the grounding goal: logical, natural language, and parsed logical form (automatically generated from logical form) using goals drawn from MultiWOZ. Examples of goal grounding settings provided in Appendix B, and results are shown in Table 2. Given these results, we chose to use the parsed logical form in our subsequent experiments. The parsed logical form achieves very similar performance to the natural language form without the need for human-written natural language goals; when creating goals with which to test a TOD system for a new domain, the fact that parsed logical form goals can be automatically generated from the dialogue schema is a distinct advantage.\nOnce we established both a model and prompting method for our user simulator, we proceeded to the key aspect of our project experiments: comparing the performance of a user simulator which is im-\nplemented using in-context learning in the method described to both human-level performance and the performance of recent works using user simulators based on fine-tuned LLMs to evaluate and test TOD systems. To do this, we first ran all diversity metrics, as described in Section 3.2 on our Human2bot dataset; these evaluation scripts are being made public along with our user simulator code. We also calculated GSR for the Human2bot data using the MultiWOZ evaluation script provided by Budzianowski et al. (2018).\nNext we extracted all goals from the MultiWOZ 2.2 test set to use as grounding goals to prompt user-side utterances in our interactive dialogue tests. We then ran our user simulator, as described in 5, using PPTOD to generate system-side utterances; PPTOD is implemented using the parameters described in Su et al. (2021). For our user simulator, we provided two randomly selected dialogues from the MultiWOZ training set as in-context learning data along with the grounding goal, as shown in Figure 1. Generation was conducted using GPTNeoX-20B as implemented by Huggingface Transformers. We use the generate method for GPTNeoX-20B with a temperature of 0.5 and a max length of 2048 (the maximum allowed by the model). We tested a range of temperature settings and found 0.5 to offer a reasonable balance between diversity and successful goal grounding. All other parameters for the GPTNeoX model are set to default values.\nWe repeated the above process using Soloist as our target TOD system, but with all other experimental parameters the same. We implement Soloist using the parameters described in Peng et al. (2020).\nWe also setup interactions between the ConvLab2 TUS user simulator and our two target TOD systems (PPTOD and Soloist). We wrote a simple wrapper to allow the target TOD systems to interact with the ConvLab2 framework, and ran the interaction with the ConvLab2 TUS system using the method described in the ConvLab2 GitHub\nrepository1. The GSR number reported for MetaSim is taken from Sun et al. (2022), while the lexical and syntactic diversity measures were calculated using our diveristy metric scripts run on MetaSim system output data provided by the MetaSim authors. We were unable to run MetaSim using PPTOD as the target TOD system due to technical issues, thus we do not report PPTOD numbers for the MetaSim user simulator."
        },
        {
            "heading": "7 Experimental Results",
            "text": "Our primary experimental results are shown in Table 3. These results show a comparison between the GSR, lexical and syntactic diversity of human participants (tested using our collected Human2Bot data) and three TOD user simulators (our user simulator, MetaSim, and ConvLab TUS) when interacting with PPTOD and Soloist, respectively, as the target TOD system.\nTable 4 shows a detailed breakdown of our user simulator\u2019s performance when grounded on goals with different numbers of intents. We include this information to demonstrate the large variation in model performance based on the number of intents in the grounding goal provided to the user simulator.\nFinally, we wished to test the generation abilities of our proposed user simulator at the utterance level. To do this, we test our system in a \u201cgold\u201d setting, wherein the system is provided with a dialogue history taken from MultiWOZ and asked to generate the next user turn. Thus, in this setting, there is no interaction with a deployed TOD system. This approach eliminates errors caused by the TOD system (rather than the user simulator) as well as accumulated error in the user simulator. We find that the user simulator is able to achieve a GSR of 77.1% in this setting, demonstrating that errors from the TOD system side are a major contributing factor to relatively low interactive system performance."
        },
        {
            "heading": "8 Human Evaluation",
            "text": "While our experimental results show that our proposed user simulator is able to generate lexically and syntactically diverse output, as measured by MTLD and dependency length, we need to further analyze our output to ensure that this diversity is\n1gitlab.cs.uni-duesseldorf.de/general/dsml/tus_public\nnot due to poorly grounded output. In order to further test our proposed user simulator\u2019s ability to generate coherent, human-like utterances given a dialogue history, we conduct human evaluation of our simulator output. The goal of this evaluation is to determine how many of the user-side utterances generated by our simulator are considered \u201creasonable\u201d responses by human annotators given the dialogue history. Our annotators indicate that 73.0% of utterances generated by our system are reasonable given the dialogue history. This high degree of reasonable utterance generation indicates that our increases in diversity and complexity do not come at the cost of coherent generation, further supporting the utility of our proposed system as a reasonable method to test TOD systems, especially those in which the developer does not expect complex, multi-intent goals from users.\nAdditionally, this qualitative analysis of our model output reveals the primary causes of failed goal completion in the user simulator side of our interactive setup.\n\u2022 Premature termination of dialogues: The user simulator is prone to generating an end dialogue token prior to the successful fulfillment of all intents in the grounding goal. While the user simulator generally starts strong, it frequently terminates the dialogue after the TOD system has fulfilled one or two intents, thereby reducing GSR. This tendency toward partial goal completion is reflected in the Inform rates reported in Table 4, which remain relatively stable across intent counts even while GSR rates drop precipitously.\n\u2022 Conversational loops: in the interactive setting, the user simulator and TOD system are also prone to getting stuck in loops with one another, wherein both systems repeatedly respond with the utterance from their respective previous turn. Given that we append both simulator and TOD system generated utterances to the dialogue history to prompt the next round of generation, this problem tends to build upon itself as the in-context learning prompt begins to contain repetitious dialogue, thus encouraging the generation of further repetitious dialogue.\n\u2022 LLM hallucination: While less of a problem than the aforementioned, model hallucination is another source of error created by the user simulator. In this situation, the model attempts to achieve intents which are not included in the grounding goal.\nExample dialogues generated using our interactive user simulator setup, including those exhibiting the degenerate behaviors listed above, are shown in Appendix A."
        },
        {
            "heading": "9 Discussion",
            "text": "One immediate observation when reviewing our primary results presented above is the fact that our incontext learning based user simulator is not achieving a GSR score nearly as high as that achieved by MetaSim and the ConvLab2 TUS simulator. While at first glance this may seem to indicate the the other two systems are better user simulators, this is not necessarily the case. First, as discussed in Section 3.1, we argue that maximizing absolute\nGSR is not an appropriate method of evaluating the performance of a TOD user simulator. The goal of a user simulator is not to convey information to the TOD system as effectively as possible, but rather to replicate the behavior of humans interacting with a TOD system. That is, we want the simulator to be \u201chuman-like\u201d, not just to do a good job talking to a TOD system. As anyone who has worked with TOD systems in actual deployment will tell you, human behavior interacting with these systems is often far from optimal.\nWe argue that the high GSR figures reported by the recently developed TOD user simulators, such as MetaSim (reported above) and Cheng et al. (2022) (86.2 GSR interacting with PPTOD), indicate that these systems are not performing in a truly \u201chuman-like\u201d manner. Our proposed system, while not achieving human-level performance when presented with more complex goals (see Table 4, does does perform well relative to human-level performance on single-intent conversations without any model fine-tuning. Given that a large portion of user goals in real-world interactions with deployed TOD systems consist of single intents, our proposed system is promising as a platform for testing and evaluating newly developed commercial TOD systems. Additionally, our system is able to generate utterances which show near-human level lexical and syntactic diversity and syntactic complexity, furthering its potential utility for testing in-development TOD systems.\nAs previously mentioned, one of the key benefits of our proposed method is the fact that our approach does not require large amounts of in-domain training data to fine-tune its component language model. This near data-independence makes our proposed user simulator a reasonable choice when testing TOD systems for new domains, for which domain-specific (or at least similar) training data does not exist. To test a system using our user simulator, a developer need only write or collect a few example dialogues and create a set of grounding goals for testing. In the context of a new domain, these goals can either be hand-written or automatically generated from the underlying dialogue schema.\nFinally, our proposed user simulator is relatively straightforward and easy to implement, especially compared with systems which consist of multiple interacting modules, such as MetaSim. This makes our proposed method attractive as a way to quickly\ngenerate conversations with a new TOD system."
        },
        {
            "heading": "10 Future work",
            "text": "While the present work makes significant progress towards effectively simulating user-side dialogue, especially for less complex conversational goals, the use of in-context learning for TOD user simulation remains an open question. As with other in-context learning approaches, the solution space for prompt design is infinite. That said, there are several potential improvements to our proposed prompt design which we believe are worth exploring in future work. These include:\n\u2022 Prompting the user simulator to repeat the subgoal on which it is grounding its output along with the NLG output itself. We believe this may provide additional grounding, thereby improving the system\u2019s fidelity to the stated grounding goal.\n\u2022 Chain-of-thought prompting: Recasting the dialogue as a chain-of-thought reasoning problem to take advantage of the properties of GPTstyle models discussed in Wei et al. (2022).\n\u2022 Providing belief state information in the prompts to indicate which goals have been completed during generation. We believe this approach may result in fewer prematurely terminated conversations.\nThere are many other generative LLMs with which we could have tested our approach. For example, we have conducted limited testing with GPT-3 davinci using the OpenAI API, which showed promising results. In future work, we would like to conduct further experiments using larger LLMs, in particular those using reinforcement learning from human feedback, such as ChatGPT, as well as smaller models such as LLaMA."
        },
        {
            "heading": "11 Conclusion",
            "text": "In this paper, we have presented a proposed method of using in-context learning with large pretrained language models to create a user simulator for the TOD domain. By conducting extensive tests of the proposed system using data from MultiWOZ, as well as human evaluation, we have shown that our proposed user simulator is capable of generating lexically diverse and syntactically complex output. Additionally, we demonstrate the the proposed system can successfully interact with extant\nTOD systems, especially when the provided goal is limited to a single intent. While there remains significant room for improvement in the reported results, our system demonstrates that in-context learning alone shows promise in developing effective user simulators for the TOD domain."
        },
        {
            "heading": "12 Limitations",
            "text": "One limitation of this paper, and the method proposed herein, is the fact that individuals wishing to use the system to test a new dialogue system must create a goal set for their target domain, which can be a time-consuming process. This fact somewhat limits the \u201cfew-shot\u201d nature of the proposed method. Additionally, the present paper discusses only a small number of large language models that could be used to implement the proposed method. We understand that there are alternative LLMs that we could have used to test our approach, and we plan to test additional LLMs in future work. That said, we acknowledge that the prompting method we describe may require substantial modification when applied to different LLMs, depending on the target model\u2019s training method."
        },
        {
            "heading": "13 Ethical Considerations",
            "text": "Whenever using LLMs to generate text, one must consider the fact that such models may be prone to generating biased, erroneous, and harmful content. Given that the method proposed in this paper uses LLMs to test TOD systems in an unsupervised manner, there exists the possibility that the LLM could generate such harmful content in its user simulation. However, users themselves may use harmful and inflammatory language when interacting with TOD systems \u201cin the wild\u201d, so the fact that our LLM-based user simulator is capable of generating such language may better reflect real-world user behavior. However, anyone who is interacting with or reviewing data generated by our proposed method should be aware of the potential exposure to harmful content within the generated data."
        },
        {
            "heading": "A Appendix A - Example dialogues",
            "text": "\u2022 Conversational Loop & LLM Hallucination: Goal: {\u2019restaurant\u2019: {\u2019info\u2019: {\u2019food\u2019: \u2019european\u2019, \u2019pricerange\u2019: \u2019expensive\u2019}, \u2019reqt\u2019: [\u2019address\u2019, \u2019phone\u2019, \u2019area\u2019]}}\nUser Sim.: I am looking for a place to dine in the expensive price range which serves european food. TOD Sys.: I have [value_choice] restaurant -s that meet your criteria . would you like to narrow it down by area ? User Sim.: yes, thank you. a table for 3 at 12:30 on saturday. TOD Sys.: I have booked you at [value_name] . your reference number is [value_reference] . User Sim.: I also want to go to a cinema in the centre. TOD Sys.: I have [value_choice] [value_type] in the [value_area] . [value_name] is a great 1 . User Sim.: I will go with that. TOD Sys.: would you like me to book that for you ? User Sim.: yes, thank you. a table for 3 at 12:30 on saturday. TOD Sys.: I have booked you at [value_name] . your reference number is [value_reference] . User Sim.: I also want to go to a cinema in the centre. TOD Sys.: I have [value_choice] [value_type] in the [value_area] . would you like me to book 1 for you ? User Sim.: yes, thank you. a table for 3 at 12:30 on saturday. TOD Sys.: I have booked you at [value_name] . your reference number is [value_reference] . User Sim.: I also want to go to a cinema in the centre. TOD Sys.: I have [value_choice] [value_type] in the [value_area] . would you like me to book 1 for you ? User Sim.: yes, thank you. a table for 3 at 12:30 on saturday. TOD Sys.: I have booked you at [value_name] . your reference number is [value_reference] . User Sim.: I also want to go to a cinema in the centre. TOD Sys.: I have [value_choice] [value_type] in the [value_area] . would you like me to book 1 for you ?\n\u2022 Premature Termination: Goal: {\u2019hotel\u2019: {\u2019info\u2019: {\u2019name\u2019: \u2019city centre north b and b\u2019}, \u2019reqt\u2019: [\u2019parking\u2019]}, \u2019train\u2019: {\u2019info\u2019: {\u2019destination\u2019: \u2019cambridge\u2019, \u2019day\u2019: \u2019sunday\u2019, \u2019arriveBy\u2019: \u201914:00\u2019, \u2019departure\u2019: \u2019broxbourne\u2019}, \u2019reqt\u2019: [\u2019duration\u2019, \u2019leaveAt\u2019, \u2019price\u2019]}}\nUser Sim.: I am looking for a hotel in cambridge. TOD Sys.: there are [value_choice] [value_type] in cambridge . do you have a price range or area in mind ? User Sim.: I would prefer to stay in the centre, anything in the moderate price range would be fine. TOD Sys.: I have [value_choice] [value_type] that meet your criteria . would you like me to book 1 for you ? User Sim.: yes please. TOD Sys.: I would be happy to book you a room . first , can you tell me how many people will be staying , what day you will be arriving , and how many nights you will be staying ? User Sim.: I am travelling with my wife and I will be arriving on sunday and staying for 1 night. TOD Sys.: I have booked you at the [value_name] . your reference number is [value_reference] . is there anything else I can help you with ? User Sim.: no that seems to be everything. thank you! TOD Sys.: you are welcome . have a great day . User Sim.: bye <end_dialog> TOD Sys.: thank you for using our service today .\n\u2022 Successful Dialogue: Goal: {\u2019train\u2019: {\u2019info\u2019: {\u2019leaveAt\u2019: \u201920:30\u2019, \u2019destination\u2019: \u2019cambridge\u2019, \u2019day\u2019: \u2019friday\u2019, \u2019departure\u2019: \u2019leicester\u2019}, \u2019reqt\u2019: [\u2019duration\u2019]}}\nUser Sim.: I would like to book a train from leicester to cambridge on friday. TOD Sys.: there are [value_choice] trains that meet your criteria . do you have a time you would like to leave or arrive by ? User Sim.: I would like to leave at 20:30. TOD Sys.: I have train [value_id] leaving at [value_leave] and arriving at [value_arrive] . would you like me to book that for you ? User Sim.: yes, please. TOD Sys.: how many tickets do you need ? User Sim.: 2 TOD Sys.: I have booked you [value_people] ticket on [value_id] . your reference number is [value_reference] . User Sim.: thank you. <end_dialog> TOD Sys.: is there anything else I can help you with today ?"
        },
        {
            "heading": "B Appendix B - Goal grounding settings",
            "text": "Examples of our three tested goal grounding settings for the same goal taken from MultiWOZ 2.1\n\u2022 Logical Form: {\"hotel\": {\"info\": {\"type\": \"hotel\", \"parking\": \"yes\", \"pricerange\": \"cheap\", \"internet\": \"yes\"},\"book\": {\"stay\": \"3\", \"day\": \"tuesday\", \"people\": \"6\"}, \"fail_book\": {\"stay\": \"2\"}}}\n\u2022 Parsed Logical Form: - hotel info type hotel. - hotel info parking yes. - hotel info pricerange cheap. - hotel info internet yes. - hotel book stay 3. - hotel book day tuesday. - hotel book people 6. - hotel fail_book stay 2.\n\u2022 Natural Language Form: You are looking for a place to stay. The hotel should be in the cheap price range and should be in the type of hotel. The hotel should include free parking and should include free wifi. Once you find the hotel you want to book it for 6 people and 3 nights starting from tuesday. If the booking fails how about 2 nights"
        }
    ],
    "title": "User Simulation with Large Language Models for Evaluating Task-Oriented Dialogue",
    "year": 2023
}