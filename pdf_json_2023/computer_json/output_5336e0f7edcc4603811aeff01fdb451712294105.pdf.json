{
    "abstractText": "This paper outlines the first KSAA-RD shared task, which aims to develop a Reverse Dictionary (RD) system for the Arabic language. RDs allow users to find words based on their meanings or definition. This shared task, KSAA-RD, includes two subtasks: Arabic RD and cross-lingual reverse dictionaries (CLRD). Given a definition (referred to as a \u201cgloss\u201d) in either Arabic or English, the teams compete to find the most similar word embeddings of their corresponding word. The winning team achieved 24.20 and 12.70 for RD and CLRD, respectively in terms of rank metric. In this paper, we describe the methods employed by the participating teams and offer an outlook for KSAA-RD.",
    "authors": [],
    "id": "SP:5afb5403acef2631833487ba0ede5f20f2c36933",
    "references": [
        {
            "authors": [
                "Muhammad Abdul-Mageed",
                "AbdelRahim A. Elmadany",
                "El Moatez Billah Nagoudi."
            ],
            "title": "ARBERT & MARBERT: Deep Bidirectional Transformers for Arabic",
            "venue": "CoRR, abs/2101.01785. arXiv: 2101.01785.",
            "year": 2021
        },
        {
            "authors": [
                "Aarchi Agrawal",
                "Athulkumar R",
                "K S Ashin Shanly",
                "Kavita Vaishnaw",
                "Mayank Singh."
            ],
            "title": "Reverse Dictionary Using an Improved CBoW Model",
            "venue": "Proceedings of the 3rd ACM India Joint International Conference on Data Science &",
            "year": 2021
        },
        {
            "authors": [
                "Almutawa",
                "Abdullah Alfifi."
            ],
            "title": "Toward an Automatic Semantic Order of Word Meanings Based on Lexically Tagged Corpus",
            "venue": "Manuscript submitted for publication. Wissam Antoun, Fady Baly, and Hazem Hajj. 2020.",
            "year": 2022
        },
        {
            "authors": [
                "European Language Resource Association. Wissam Antoun",
                "Fady Baly",
                "Hazem Hajj."
            ],
            "title": "AraELECTRA: Pre-Training Text Discriminators for Arabic Language Understanding",
            "venue": "Proceedings of the Sixth Arabic Natural Language Processing",
            "year": 2021
        },
        {
            "authors": [
                "Hiram Calvo",
                "Oscar M\u00e9ndez",
                "Marco A. MorenoArmend\u00e1riz."
            ],
            "title": "Integrated concept blending with vector space models",
            "venue": "Computer Speech & Language, 40:79\u201396. Kevin Clark, Minh-Thang Luong, Quoc V. Le, and",
            "year": 2016
        },
        {
            "authors": [
                "Christopher D. Manning."
            ],
            "title": "ELECTRA: Pretraining Text Encoders as Discriminators Rather Than Generators",
            "venue": "CoRR, abs/2003.10555. arXiv: 2003.10555. H. Vance Crawford and Joel Crawford. 1997.",
            "year": 2020
        },
        {
            "authors": [
                "Kareem Darwish",
                "Hamdy Mubarak."
            ],
            "title": "Farasa: A New Fast and Accurate Arabic Word Segmenter",
            "venue": "International Conference on Language Resources and Evaluation. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and",
            "year": 2016
        },
        {
            "authors": [
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "venue": "Proceedings of the 2019 Conference of the North, pages 4171\u20134186, Minneapolis, Minnesota. Association for",
            "year": 2019
        },
        {
            "authors": [
                "Computational Linguistics. Dominique Dutoit",
                "Pierre Nugues."
            ],
            "title": "A lexical database and an algorithm to find words from definitions | Proceedings of the 15th European Conference on Artificial Intelligence",
            "venue": "ECAI\u201902:",
            "year": 2002
        },
        {
            "authors": [
                "Kawin Ethayarajh."
            ],
            "title": "How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language",
            "year": 2019
        },
        {
            "authors": [
                "Klakow",
                "Gerard de Melo."
            ],
            "title": "Using MultiSense Vector Embeddings for Reverse Dictionaries",
            "venue": "Proceedings of the 13th International Conference on Computational Semantics - Long Papers, pages 247\u2013258, Gothenburg, Sweden. Association for",
            "year": 2019
        },
        {
            "authors": [
                "Computational Linguistics. Felix Hill",
                "Kyunghyun Cho",
                "Anna Korhonen",
                "Yoshua Bengio."
            ],
            "title": "Learning to Understand Phrases by Embedding the Dictionary",
            "venue": "Transactions of the Association for Computational Linguistics,",
            "year": 2016
        },
        {
            "authors": [
                "4:17\u201330. Go Inoue",
                "Bashar Alhafni",
                "Nurpeiis Baimukan",
                "Houda Bouamor",
                "Nizar Habash"
            ],
            "title": "The Interplay of Variant, Size, and Task Type in Arabic Pre-trained Language Models",
            "year": 2021
        },
        {
            "authors": [
                "Oscar M\u00e9ndez",
                "Hiram Calvo",
                "Marco A. Moreno"
            ],
            "title": "Kyiv, Ukraine (Virtual). Association for Computational Linguistics. king Salman global academy for Arabic language KSAA",
            "year": 2023
        },
        {
            "authors": [
                "Armend\u00e1riz."
            ],
            "title": "A Reverse Dictionary Based on Semantic Analysis Using WordNet",
            "venue": "F\u00e9lix Castro, Alexander Gelbukh, and Miguel Gonz\u00e1lez, editors, Advances in Artificial Intelligence and Its Applications, pages 275\u2013285, Berlin, Heidelberg.",
            "year": 2013
        },
        {
            "authors": [
                "Springer. Timothee Mickus",
                "Kees Van Deemter",
                "Mathieu Constant",
                "Denis Paperno."
            ],
            "title": "Semeval-2022 Task 1: CODWOE \u2013 Comparing Dictionaries and Word Embeddings",
            "venue": "Proceedings of the 16th",
            "year": 2022
        },
        {
            "authors": [
                "Tomas Mikolov",
                "Kai Chen",
                "Greg Corrado",
                "Jeffrey Dean"
            ],
            "title": "Efficient Estimation of Word",
            "venue": "International Workshop on Semantic Evaluation",
            "year": 2022
        },
        {
            "authors": [
                "cs]. Yuya Morinaga",
                "Kazunori Yamaguchi"
            ],
            "title": "Improvement of Reverse Dictionary by Tuning Word Vectors and Category Inference",
            "venue": "In Robertas Damas\u030cevic\u030cius and Giedre\u0307 Vasiljeviene\u0307,",
            "year": 2018
        },
        {
            "authors": [
                "Kazunori Yamaguchi"
            ],
            "title": "Improvement of Neural Reverse Dictionary by Using Cascade Forward Neural Network",
            "venue": "Information and Software Technologies,",
            "year": 2020
        },
        {
            "authors": [
                "Ossama Obeid",
                "Nasser Zalmout",
                "Salam Khalifa",
                "Dima Taji",
                "Mai Oudah",
                "Bashar Alhafni",
                "Go Inoue",
                "Fadhl Eryani",
                "Alexander Erdmann",
                "Nizar Habash"
            ],
            "title": "CAMeL Tools: An Open Source",
            "venue": "Information Processing,",
            "year": 2020
        },
        {
            "authors": [
                "Ahmed Mukhtar Omar."
            ],
            "title": "Dictionary of Contemporary Arabic Language",
            "venue": "Volume One, 1st Edition, Cairo, Egypt:2109. Mohammad Taher Pilehvar. 2019. On the Importance of Distinguishing Word Meaning",
            "year": 2008
        },
        {
            "authors": [
                "Abed Qaddoumi."
            ],
            "title": "Abed at KSAA-RD Shared Task: Enhancing Arabic Word Embedding with Modified BERT Multilingual",
            "venue": "In",
            "year": 2023
        },
        {
            "authors": [
                "Fanchao Qi",
                "Lei Zhang",
                "Yanhui Yang",
                "Zhiyuan Liu",
                "Maosong Sun."
            ],
            "title": "WantWords: An Opensource Online Reverse Dictionary System",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System",
            "year": 2020
        },
        {
            "authors": [
                "Jorge C. Reyes Maga\u00f1a",
                "Gemma Bel Enguix",
                "Gerardo Sierra",
                "Helena G\u00f3mez-Adorno."
            ],
            "title": "Designing an Electronic Reverse Dictionary Based on Two Word Association Norms of English Language",
            "venue": "Electronic lexicography in the 21st",
            "year": 2019
        },
        {
            "authors": [
                "Ryan Shaw",
                "Anindya Datta",
                "Debra VanderMeer",
                "Kaushik Dutta."
            ],
            "title": "Building a Scalable DatabaseDriven Reverse Dictionary",
            "venue": "IEEE Transactions on Knowledge and Data Engineering, 25(3):528\u2013540.",
            "year": 2013
        },
        {
            "authors": [
                "Serry Sibaee",
                "Samar Ahmad",
                "Ibrahim Khurfan",
                "Vian Sabeeh",
                "Ahmed Bahaaulddin",
                "Hanan M. Belhaj",
                "Abdullah I. Alharbi."
            ],
            "title": "Qamosy at KSAA-RD shared task: Semi Decoder Architecture for Reverse Dictionary with SBERT Encoder",
            "venue": "In",
            "year": 2023
        },
        {
            "authors": [
                "Bushra Siddique",
                "Mirza Mohd Sufyan Beg."
            ],
            "title": "A Review of Reverse Dictionary: Finding Words from Concept Description",
            "venue": "Manish Prateek, Durgansh Sharma, Rajeev Tiwari, Rashmi Sharma, Kamal Kumar, and Neeraj Kumar, editors, Next",
            "year": 2019
        },
        {
            "authors": [
                "Bilac Slaven",
                "Wataru Watanabe",
                "Taiichi Hashimoto",
                "Takenobu Tokunaga",
                "Hozumi Tanaka."
            ],
            "title": "Dictionary search based on the target word description",
            "venue": "pages 556\u2013559.",
            "year": 2004
        },
        {
            "authors": [
                "Abu Bakr Soliman",
                "Kareem Eissa",
                "Samhaa R. El-Beltagy."
            ],
            "title": "AraVec: A set of Arabic Word Embedding Models for use in Arabic NLP",
            "venue": "Procedia Computer Science, 117:256\u2013265.",
            "year": 2017
        },
        {
            "authors": [
                "Stephen Taylor."
            ],
            "title": "UWB at KSAA-RD shared task: Computing the meaning of a gloss",
            "venue": "In",
            "year": 2023
        },
        {
            "authors": [
                "Sushrut Thorat",
                "Varad Choudhari."
            ],
            "title": "Implementing a Reverse Dictionary, based on word definitions, using a Node-Graph Architecture",
            "venue": "In",
            "year": 2016
        },
        {
            "authors": [
                "William Timkey",
                "Marten van Schijndel."
            ],
            "title": "All Bark and No Bite: Rogue Dimensions in Transformer Language Models Obscure Representational Quality",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural",
            "year": 2021
        },
        {
            "authors": [
                "Lukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is All you Need",
            "venue": "ArXiv, abs/1706.03762. Hang Yan, Xiaonan Li, Xipeng Qiu, and Bocao Deng. 2020. BERT for Monolingual and CrossLingual Reverse Dictionary. In Findings of the",
            "year": 2017
        },
        {
            "authors": [
                "Fabio Massimo Zanzotto",
                "Ioannis Korkontzelos",
                "Francesca Fallucchi",
                "Suresh Manandhar"
            ],
            "title": "Association for Computational Linguistics: EMNLP 2020, pages 4329\u20134338",
            "year": 2010
        },
        {
            "authors": [
                "Lei Zhang",
                "Fanchao Qi",
                "Zhiyuan Liu",
                "Yasheng Wang",
                "Qun Liu",
                "Maosong Sun."
            ],
            "title": "MultiChannel Reverse Dictionary Model",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, 34(01):312\u2013319.",
            "year": 2020
        },
        {
            "authors": [
                "Michael Zock",
                "Slaven Bilac."
            ],
            "title": "Word Lookup on the Basis of Associations : from an Idea to a Roadmap",
            "venue": "Proceedings of the Workshop on Enhancing and Using Electronic Dictionaries, pages 29\u201335, Geneva, Switzerland. COLING.",
            "year": 2004
        },
        {
            "authors": [
                "Michael Zock",
                "Didier Schwab."
            ],
            "title": "Lexical access based on underspecified input",
            "venue": "Coling 2008: Proceedings of the Workshop on Cognitive Aspects of the Lexicon (COGALEX 2008), pages 9\u2013 17, Manchester, United Kingdom. Coling 2008",
            "year": 2008
        }
    ],
    "sections": [
        {
            "text": "Proceedings of the The First Arabic Natural Language Processing Conference (ArabicNLP 2023), pages 450\u2013460 December 7, 2023 \u00a92023 Association for Computational Linguistics\nThis paper outlines the first KSAA-RD shared task, which aims to develop a Reverse Dictionary (RD) system for the Arabic language. RDs allow users to find words based on their meanings or definition. This shared task, KSAA-RD, includes two subtasks: Arabic RD and cross-lingual reverse dictionaries (CLRD). Given a definition (referred to as a \u201cgloss\u201d) in either Arabic or English, the teams compete to find the most similar word embeddings of their corresponding word. The winning team achieved 24.20 and 12.70 for RD and CLRD, respectively in terms of rank metric. In this paper, we describe the methods employed by the participating teams and offer an outlook for KSAA-RD."
        },
        {
            "heading": "1 Introduction",
            "text": "A Reverse Dictionary (RD) is a type of dictionaries that allows users to find words based on their meanings or definitions. Unlike a traditional dictionary, where users search for a word by its spelling, a RD allow users to enter a description of a word or a phrase, and the RD will generate a list of words that match that description. RDs can be useful for writers, crossword puzzle enthusiasts, non-native language learners, and anyone looking to expand their vocabulary. Specifically, RD addresses the Tip-of-Tongue (TOT) phenomenon (Brown and McNeill, 1966), which refers to the situation where a person is aware of a word they want to say but is unable to express it accurately (Siddique and Sufyan Beg, 2019).\n* Equal Contribution\nVarious approaches have been proposed in the literature to develop RDs, including Information Retrieval (IR) System-based (Slaven et al., 2004; Crawford and Crawford, 1997; El-Kahlout and Oflazer, 2004; Shaw et al., 2013), Graph-based (Dutoit and Nugues, 2002; Reyes Maga\u00f1a et al., 2019; Thorat and Choudhari, 2016), Mental Dictionary-based (Zock and Schwab, 2008; Zock and Bilac, 2004), Vector Space Model-based Semantic Analysis (Calvo et al., 2016; M\u00e9ndez et al., 2013), and Neural Language Model-based approaches (Agrawal et al., 2021; Hedderich et al., 2019; Hill et al., 2016; Morinaga and Yamaguchi, 2018; Morinaga and Yamaguchi, 2020; Pilehvar, 2019; Qi et al., 2020; Yan et al., 2020; Zhang et al., 2020; Devlin et al., 2019).\nHowever, to the best of our knowledge, there is no available Arabic RD system that allows the user to find the best matching word for a gloss in a specific dictionary, while most of the Arabic available digital dictionaries allow users to search for the definition by words (Siddique and Sufyan Beg, 2019).\nWe ran this shared task as a part of the first Arabic Natural Language Processing (ArabicNLP) conference collocated with EMNLP 2023, featuring the KSAA-RD (King Salman Global Academy for Arabic Language) with two subtasks: Arabic RD (Arabic to Arabic) and Cross-lingual Reverse Dictionary (CLRD) (Arabic to English). CLRD task aims to assist translating systems in selecting the best Arabic translation for new terms and definitions.\nThe dataset for both tasks used Arabic and English available dictionaries. Also, we provide manually annotated mapped dictionary between Arabic and English words to be used for supervised learning in the second task.\n450\nA total of four papers submitted for the shared task. Three teams surpassed the RD task baseline, while all four teams exceeded the CLRD task baseline. We provide a description of all submitted systems and the approaches they use. All the datasets created for this shared task are publicly available to support further research in a GitHub repository1.\nThe rest of this paper is organized as follows: Section 2 defines related work that tackled the RD problem. Section 3 presents shared task description and the subtasks included in KSAARD. Section 4 describes the data given in the task. Section 5 presents the methodology that is used to evaluate the performance of the systems. Section 6 provides the baseline system and its results, in addition to discussing the participating systems and their results in the shared task. Section 7 draws conclusions."
        },
        {
            "heading": "2 Related work",
            "text": "Various approaches have been proposed to develop RD systems, including Information Retrieval (IR) System-based Approach, Graphbased Approach, Mental Dictionary-based Approach, Vector Space Model-based Semantic Analysis Approach, and Neural Language Modelbased Approach. The four subsections provide a preview for each approach respectively."
        },
        {
            "heading": "2.1 Information Retrieval (IR) Systembased Approach",
            "text": "The traditional IR systems retrieve a ranked list of the most relevant words, and it has a long-standing tradition in computational semantics. An earliest work addressing reverse dictionary by (Crawford and Crawford, 1997) is a patented work that uses synonyms to enhance search capabilities, and provide a broader range of relevant words based on user queries. Rather than searching the actual word, a study from (Slaven et al., 2004) analyzes the descriptions of target words and convert them into a structured representation. This structured representation allows for efficient matching and retrieval of words that closely match the given descriptions. Another work from (El-Kahlout and Oflazer, 2004) explores a lexical database for retrieving\n1 https://github.com/Waadtss/ArReverseDictionary\nwords based on their meanings. The method of extracting words based on their \"meaning\" involves comparing the user's definition with each entry in the Turkish database, without taking into account any semantic or grammatical information. The study from (Shaw et al., 2013) presents the development of a system that relies on a scalable database for efficient word retrieval. The system takes a user input phrase describing the desired concept and returns a word that satisfies the input phrase."
        },
        {
            "heading": "2.2 Graph-based Approach",
            "text": "A graph-based approach involves using a graph structure to represent the connections between words or concepts. This graph is built by considering semantic associations like synonyms, antonyms, hypernyms, and other related semantic links, to establish the relationships between the nodes representing the words or concepts. (Dutoit and Nugues, 2002) explores the connectivity and associations within the lexical database; by leveraging the graph structure to retrieve relevant words that align with the given definitions. Another approach focuses on utilizing the graph structure of a dictionary (Thorat and Choudhari, 2016) by investigating the sub-graph that surrounds each content word in a user query. They then prioritize and rank all the nodes encountered during the exploration, aiming to retrieve the most probable target word based on the given query. Another study from (Reyes Maga\u00f1a et al., 2019) uses word association norms to establish semantic connections between words in the context of designing an electronic RD. The authors used the corpus of human-definitions and graph-based techniques, specifically a measure of betweenness centrality, to perform searches in the knowledge graph."
        },
        {
            "heading": "2.3 Mental Dictionary-based Approach",
            "text": "The mental dictionary-based approach depends on an individual's internal knowledge or mental lexicon to find words based on their meanings or descriptions. Instead of relying on external resources such as dictionaries or databases, this approach emphasizes using the individual's own mental representation of words and their\nassociations. The study from (Zock and Bilac, 2004) proposes the concept of accessing words in an electronic dictionary by utilizing associations. This involves categorizing words based on the associations they evoke and identifying and labeling the most common or valuable associations within the dictionary. The proposal has been taken further in (Zock and Schwab, 2008) by implementing a user-guided search to the desired word that simulates human word synthesis, in order to gain a quick and intuitive access to that word."
        },
        {
            "heading": "2.4 Vector Space Model-based Semantic Analysis Approach",
            "text": "This approach attempts to use vector space models to transform the human-written queries into a vector by utilizing a semantic relations to improve the effectiveness of RD lookup. Another study that utilize semantic analysis with WordNet (M\u00e9ndez et al., 2013) to generate vectors by identifying synsets that maximize a similarity measure. They then conduct a neighborhood search to extract the most relevant word. (Calvo et al., 2016) obtain vectors using LDA instead of WordNet."
        },
        {
            "heading": "2.5 Neural Language Model-based Approach",
            "text": "The neural language approach relies on encoding each input gloss into a vector representation, the output is a group of words whose embeddings are most similar to the corresponding gloss embedding. (Agrawal et al., 2021) enhance the traditional CBoW model by incorporating additional contextual information, such as word relationships and semantic associations, to better capture the nuances of word meanings. Another study that utilizes multi-sense embeddings (Hedderich et al., 2019) based on attention mechanism to enhance the representation of input queries in sentences. Focusing on eliminating the need for manually designed features, (Hill et al., 2016) propose Recurrent Neural Networks (RNN) to the RD task by encoding the definition of a word into a vector representation. The model then searches for the nearest neighbor word based on this vector. The performance was comparable to OneLook commercial RD. Another study from (Morinaga and Yamaguchi, 2018) improved the embedding accuracy by selecting better word vectors and employing category inference that\neliminate irrelevant results using Convolutional Neural Network (CNN). An approach from (Morinaga and Yamaguchi, 2020) aim to better capture the nuances of a word meaning and tackle the problem of sufficient capacities by combining the bidirectional long short-term memory (BiLSTM) with Cascade Forward Neural Network (CFNN). A neural model from (Zhang et al., 2020) which is a multi-channel RD model (MRDM) that consists of BiLSTM and attention as sentence encoder. The model can help find the target words by utilizing four characteristic predictors that predict the POS, morphemes, word category and sememes. (Pilehvar, 2019) incorporating more fine-grained representations by adopting sense embeddings to disambiguate senses of polysemous target words.\nThe BERT models were incorporated in RD tasks as well. (Devlin et al., 2019) employs BERT that capture the bidirectional contextual information of words and sentences, allowing it to better comprehend the context and meaning of language. (Qi et al., 2020) develop an online RD that enhanced multi-channel RD model from (Zhang et al., 2020). The model uses BERT instead of BiLSTM as a sentence encoder. Another model from (Yan et al., 2020) use BERT in both monolingual and cross-lingual RD system.\nTo the best of our knowledge, this shared task is the first to target Arabic RD problem and there is no available Arabic RD system that allows the user to find the best word in a specific dictionary, while most of the Arabic available digital dictionaries allow the users to search for the definition by words (Siddique and Sufyan Beg, 2019).\nBased on the previous studies and approaches, the neural language model-based approach gives promising results compared to other approaches due to its ability to map word embeddings for an input definition into an embedding of the word defined by the definition using neural networks. Such a function encodes phrasal semantics and bridges the gap between them and lexical semantics. Therefore, we applied it in our baseline."
        },
        {
            "heading": "3 Task Description",
            "text": "This section describes the two subtasks in detail: RD and CLRD. The former converts Arabic word definitions into Arabic embeddings, while the\nlatter converts English word definitions into Arabic embeddings."
        },
        {
            "heading": "3.1 Task 1: Reverse Dictionary",
            "text": "The structure of RDs (sequence-to-vector) is the opposite of traditional dictionaries lookup. This task focuses on the learning of how to convert human readable definitions into vector representation of the Arabic word.\nIn this task, the input for the model is an Arabic word definition (gloss) and the output is the\ncorresponding Arabic word embedding. For instance, given the Arabic gloss \" \u0627\u0644\u0645\u0633\u06cc\u0631 \u0644\u06cc\u0644\u0627\u064b ,\" the model would generate an embedding for the Arabic word \" \u0627\u0644\u0625\u0633\u0631\u0627\u0621 \" which is the word corresponding to the gloss.\nThe task involves reconstructing the word embedding vector of the defined word, rather than simply finding the target word that is similar to the approach used by (Mickus et al., 2022; Zanzotto et al., 2010; Hill et al., 2016). This would enable the users to search for words based on the definition or meanings they anticipate.\nThe training data collection contains a source word vector representation \u201celectra and sgns\u201d and its corresponding word definition \u201cgloss\u201d, as illustrated in Figure 1 (a) and (b). The baseline model described in section 6.1 is designed to generate new word vector representations for the target unseen readable definitions."
        },
        {
            "heading": "3.2 Task 2: Cross-lingual Reverse",
            "text": "Dictionary\nThe objective of the CLRDs task (sequence-tovector) is to acquire the ability to transform readable definitions in the English language into a vector representation of the Arabic word. The main objective of this task is to identify the most accurate and suitable Arabic word vector that can efficiently express the identical semantic interpretation as the provided English language definition or gloss, which is commonly known as Arabicization \"\u062a\u0639\u0652\u064e\u0631\u0650\u06cc\u0628\" . In this task, the input for the model is an English word definition (gloss) and the output is the Arabic word embeddings corresponding to the gloss. For instance, given the English gloss \"Travelling at night,\" the model would generate an embedding for the Arabic word \" \u0627\u0644\u0625\u0633\u0631\u0627\u0621 .\"\nThe task involves reconstructing the word embedding vector that represents the Arabic word to its corresponding English definition. This approach enables users to search for words in other languages based on their anticipated meanings or definitions in English. This task facilitates cross-lingual search, language understanding, and language translation. The data collection includes the word, source word vector representation \u201celectra and sgns\u201d, and the definition \u201cgloss\u201d in both Arabic and English languages, as demonstrated in Figure 1 (d)."
        },
        {
            "heading": "4 Data",
            "text": "This section discusses the data used in the shared task. The dataset includes two main components: the dictionary data, which is presented in section 4.1, and the word embedding vectors, which is presented in section 4.2. Section 4.3 describes further details of the dataset."
        },
        {
            "heading": "4.1 Dictionary data",
            "text": "To achieve the aim of the first task, known as RD, which seeks to develop a model capable of conducting reverse searches for Arabic words based on their meanings rather than their roots or lemmas, we utilized the Contemporary Arabic Language dictionary authored by Ahmed Mokhtar Omar (Omar, 2008). More specifically, we utilized the transferred version of this lexicon that adheres to the ISO standard, the Lexical Markup Framework (LMF) (Aljasim et al., 2022). It is worth mentioning that the KSAA team conducted this work. The dictionary relies on lemmas rather than roots, as discussed in the referenced study. The dataset comprises 58,000 words, commonly referred to as the lemmas that can have glosses with non-relevant information (e.g., morphological, and syntactic properties).\nIn the second task, our approach involved using a supplementary English dictionary, namely the English dictionary version employed in the SemEval 2022 Shared task on RD. It has a total of 63,596 lemmas that can have a different number of glosses (polysemy), and vice versa, a gloss can belong to more than one word (synonymy) (Mickus et al., 2022). This enabled us to construct a model that could effectively forecast the appropriate Arabic lemma matching to a given English meaning.\nConsequently, there are two distinct datasets available: the dataset containing the Arabic dictionary and the dataset including the English dictionary. Each dataset consists of six components, including word form, part of speech, gloss, word ID, Electra embedding (Clark et al., 2020), and word2vec embedding (Mikolov et al., 2013). Within the realm of linguistic analysis, the \u201cword\u201d component encompasses the words. Additionally, the \u201cpart of speech\u201d serves to denote the grammatical category to which the lemma belongs, namely noun, verb, adjective, adverb, or particle. The \u201cgloss\u201d serves the purpose of\n2 https://pypi.org/project/deep-translator/\nconveying the semantic content or meaning of a word, with the intention of excluding any phonetic, morphological, or syntactic aspects. The subsequent sections in the paper will provide explanations for the \"electra\u201d embedding and word2vec embedding components.\nIn order to fulfil the goals of the second task, we integrated the datasets in Arabic and English. The manual annotation procedure entailed a meticulous examination of the English gloss alongside its equivalent Arabic gloss, with the aim of attaining a thorough alignment between the two glosses across several linguistic dimensions.\nTo provide an instance, the English Dictionary defines the verb \"cloud\" as \"to make obscure\". This concept can be annotated with Arabic lemmas such as ' \u0623\u063a\u0645\u0636 ' which signifies the act of concealing, or ' \u0623\u062e\u0641\u0649 ', which denotes the act of covering, among other examples. The establishment of a correspondence between Arabic and English languages can be accomplished by assigning the Arabic gloss ID \u201cid\u201d to their corresponding English glosses \u201cenId\u201d. Note that a word can have other irrelevant glosses (or meanings), but they will not be assigned.\nThe dictionary annotation process employs a systematic approach to facilitate manual annotation. For each entry in the English dictionary, deep-translator2 library is employed to provide word translation \u201cwt\u201d. Leveraging AraVec word embeddings (Soliman et al., 2017), the top ten similar word candidates are identified for wt. If any of these candidates align with lemmas in the Arabic dictionary, their corresponding IDs are integrated along with POS and gloss. The annotators then select the best candidate ID based on the corresponding POS and gloss that match the English dictionary.\nIn the manual phase, annotators meticulously select English lemmas, cross-referencing them with candidate IDs. This involves instances of confirmed matches, where corresponding Arabic lemma are included. In other cases, the process entails identifying the most suitable Arabic lemma translations within the Arabic dictionary and incorporating them. This meticulous process ensures data coherence, with lemma encompassing the word, POS, and gloss."
        },
        {
            "heading": "4.2 Embedding data",
            "text": "Our objective is to employ two distinct word embedding techniques, specifically contextualized word embedding and fixed word embedding. To efficiently attain this objective, we employ AraELECTRA (Antoun et al., 2021) for contextualized word embedding (referred to as \u201cElectra\u201d). AraELECTRA is an Arabic language representation model built upon the ELECTRA model. Unlike training a model to restore masked tokens, AraELECTRA focuses on training a discriminator model to distinguish original input tokens from replaced tokens, which have been substituted by a generator network. For single entries \u201cword\u201d, we use the token's embedding; for multi-token entries, we average the token embeddings. This approach leverages the substantial volumes of high-quality language models that have already been trained, enabling us to harness the contextualized representation of existing large pretrained models. For fixed word embedding (referred to as skip-gram with negative sampling \u201csgns\u201d), we employ the AraVec skip-gram architecture from (Soliman et al., 2017). During the word2vec embedding extraction, a two-step approach is applied. 1. For a single-token word, a Unigrams skip-\ngram model is used to generate the embedding. When a word lacks representation (out-of-vocabulary) in the skip-gram model, the average embedding is obtained from the gloss associated with that word. 2. For a multi-token word, an N-Grams skipgram model is employed to generate the embedding. When a multi-token word lacks representation (out-of-vocabulary) in the skip-gram model, the average embedding is obtained from the gloss associated with that word. When a gloss lacks representation in the skipgram model, the embedding is obtained from the stemmed gloss. The stem of each word in gloss is extracted using CAMEL tool (Obeid et al., 2020). When representation remains elusive, the Farasa stemmer (Darwish and Mubarak, 2016) is employed instead of CAMEL. The two-step approach is then employed for generating embeddings. When there is no representation, the stemmer is then employed at the word level."
        },
        {
            "heading": "4.3 Dataset description",
            "text": "The datasets are provided in JSON format comprising nearly 58k Arabic entry data points and 63k English data entry points. The dataset is divided into three splits, including a training split that consists of almost 78% of the data points, a validation split that consists of 11% of the data points, and a test split that consists of 11% of the data points. Refer to Table 1 for data statistics."
        },
        {
            "heading": "5 Evaluation",
            "text": "The primary objective of our tasks is to find the most similar Arabic embedding for an Arabic or English definition. Thus, we consider three different approaches to measure vector similarity. The first approach is a Mean Squared Error (MSE), which calculates the average squared difference between the generated reconstructed embedding and target embeddings. The second approach is using the cosine similarity measure, where a perfect reconstructed embedding would result in a cosine similarity of 1 with the target embeddings. The challenge with the cosine measure is that language models utilizing the Transformer architecture can produce anisotropic output. Hence, it is not reasonable to use it alone to anticipate that two random contextualized embeddings will be orthogonal (Ethayarajh, 2019; Timkey and van Schijndel, 2021).\nTo complement the limitations of both MSE and cosine measure, a third approach known as the ranking metric has been utilized. The ranking evaluation metrics proposed in the CODWOE SemEval competition (Mickus et al., 2022). As shown in equation (1), the ranking metric is concerned with comparing and evaluating the proportion between the reconstructed embedding cosine \ud835\udc5d! and the target embedding cosine \ud835\udc61! to the reconstruction embedding cosine \ud835\udc5d! with all other targets embedding \ud835\udc61\" in the test set. The proportion of targets with a higher correlation is determined by identifying the number of cosine values greater than cos(\ud835\udc5d! , \ud835\udc61! ) (Mickus et al., 2022). The ranking metric can be described as:\nRanking(\ud835\udc5d!) = \u2211 $!\"#(%& ,)* ),!\"#(%& ,)& ))* -./) /.)\n# '()* )(* (1)\nTo select the top-performing and well-rounded model, the submitted systems evaluation process follows a hierarchy of metrics. The primary metric is the ranking metric, which is used to assess how well the model ranks predictions compared to ground truth values. If models have similar rankings, the secondary metric, mean squared error (MSE), is considered. Lastly, if further differentiation is needed, the tertiary metric, cosine similarity, provides additional insights."
        },
        {
            "heading": "6 Shared Task Teams & Results",
            "text": "In this section, we present our baseline model and, participating teams, and results and description of submitted systems."
        },
        {
            "heading": "6.1 Our Baseline system",
            "text": "The baseline architecture proposed by (Mickus et al., 2022) is based on the Transformer model introduced by (Vaswani et al., 2017). The architecture involves feeding the input gloss, which is represented as a sequence starting with a special token \u2018bos\u2019 and ending with another special token \u2018eos\u2019, into a straightforward Transformer encoder. The encoder generates hidden representations, which are then summed to produce the prediction. Additionally, a small nonlinear feed-forward module is used to further refine the prediction. The evaluation of both tasks will be based on three different metrics including MSE, cosine similarity measure, and ranking metric."
        },
        {
            "heading": "6.2 Participating Teams",
            "text": "A total of 31 unique team registrations were received. A total of 39 valid submissions from 5 unique teams were received. During the testing phase, we received 5 submissions for the RD Subtask and 3 submissions for the CLRD Subtask from 4 different teams. You can find the details of these 4 teams in Table 2. Additionally, a total of 4 description papers were submitted and accepted.\nUWB (Taylor, 2023) University of West Bohemia 1,2 Qamosy (Sibaee et al., 2023) Prince Sultan University 1 Abed (Qaddoumi, 2023) NYU 1,2"
        },
        {
            "heading": "6.3 Results and Description of Submitted Systems",
            "text": "Three teams participate in both RD and CLRD tasks, Rosetta Stone team, UWB teams, and Abed team. In the other hand, Qamosy team only submit the RD task. Results for both tasks are presented in Table 3 and Table 4. The top team for both RD and CLRD tasks is the Rosetta Stone team (ElBakry et al., 2023). They employ an ensemble of fine-tuned Arabic BERTbased models, including camelBERT-MSA, camelBERT-Mix (Inoue et al., 2021), MARBERTv2 (Abdul-Mageed et al., 2021), and AraBERTv2 (Antoun et al., 2020). For RD, averaging the output embeddings from camelBERT-MSA and MARBERTv2 yielded a ranking of 24.20 using ELECTRA embeddings. For the CLRD task, they translate English glosses into Arabic, using the same models as in RD, achieving a rank of 12.70 with ELECTRA embeddings. Qamosy team (Sibaee et al., 2023) methodology for RD task involves two phases: transforming the gloss into multidimensional vector representations using SBERT encoding, followed by training these vectors using the Simi-Decoder model. Their system achieved 2nd place in the RD task with a score of 28.10 in the Rank metric, utilizing ELECTRA embeddings. Abed team (Qaddoumi, 2023) employs a modified multilingual BERT model for both RD and CLRD tasks, using data augmentation techniques like synonym replacement, random word insertion, deletion, and swapping in English, and random word deletion and swapping in Arabic. They achieved the 2nd and 3rd place in the RD and CLRD tasks, respectively, with a scores of 28.50 and 28.10 in the Rank metric with ELECTRA. UWB teams (Taylor, 2023) utilize a rule-based approach for RD and CLRD tasks. They build a dataset-based dictionary and expand it using gloss. The dictionary-based approach with SGNS embeddings achieves 43.8 within RD task, lower than the baseline model, and 48.87 within the CLRD task.\nIt's evident that three teams utilizing BERTbased models outperformed our RD task Baseline, except for the dictionary-based approach by the UWB teams. However, the Rosetta Stone team's ensemble of different BERT architectures surpassed the performance of other methods. Abed team demonstrated exceptional performance with ELECTRA embeddings, underscoring the substantial impact of data manipulation techniques on results. Surprisingly, the UWB teams, despite using a dictionary-based technique, outperformed our baseline that based on transformer model in the CLRD task."
        },
        {
            "heading": "7 Conclusion",
            "text": "In this paper, we present the first Arabic RD shared task, KSAA-RD, encompassing two subtasks: Arabic RD and Cross-Lingual RD, CLRD. The KSAA-RD task received 31 unique team registrations, resulting in 39 valid submissions and 4 submitted description papers. The outcomes from various teams underscore the persistent challenges posed by both RD and\nCLRD tasks, emphasizing the need for continued research in the field of Arabic RD tasks.\nOur experience with KSAA-RD emphasize the significant impact of data manipulation techniques. Furthermore, employing an ensemble of diverse transformer architectures proved superior to other methods, highlighting the importance of model diversity in enhancing performance.\nThe Arabic dictionary used in this shared task is limited, compared to the newly released dictionary of the Arabic contemporary language: \u201cAlriyadh Dictionary\u201d (KSAA, 2023), which contains more than 120K terms compared to 58K and it is manually verified by groups of experts in the KSAA. Other aspects of future work include exploring advanced embedding techniques that might be more suitable to the semantic notion of the problem. Future work includes employing these techniques in a search engine and analyzing the user behavior of the search results. Further investigation is needed to examine whether dictionary definitions (which are usually written in a formal style) are a good representation of the users\u2019 inquiries."
        }
    ],
    "year": 2023
}