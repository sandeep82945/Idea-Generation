{
    "abstractText": "The adoption of pre-trained language models in task-oriented dialogue systems has resulted in significant enhancements of their text generation abilities. However, these architectures are slow to use because of the large number of trainable parameters and can sometimes fail to generate diverse responses. To address these limitations, we propose two models with auxiliary tasks for response selection (1) distinguishing distractors from ground truth responses and (2) distinguishing synthetic responses from ground truth labels. They achieve state-of-the-art results on the MultiWOZ 2.1 dataset with combined scores of 107.5 and 108.3 and outperform a baseline with three times more parameters. We publish reproducible code and checkpoints and discuss the effects of applying auxiliary tasks to T5based architectures.",
    "authors": [
        {
            "affiliations": [],
            "name": "Radostin Cholakov"
        },
        {
            "affiliations": [],
            "name": "Todor Kolev"
        }
    ],
    "id": "SP:4305b9f685df3857463d2dd5867990c98cf6fc88",
    "references": [
        {
            "authors": [
                "Pawe\u0142 Budzianowski",
                "Ivan Vuli\u0107."
            ],
            "title": "Hello, it\u2019s gpt-2\u2013how can i help you? towards the use of pretrained language models for task-oriented dialogue systems",
            "venue": "arXiv preprint arXiv:1907.05774.",
            "year": 2019
        },
        {
            "authors": [
                "Sumanth Dathathri",
                "Andrea Madotto",
                "Janice Lan",
                "Jane Hung",
                "Eric Frank",
                "Piero Molino",
                "Jason Yosinski",
                "Rosanne Liu."
            ],
            "title": "Plug and play language models: A simple approach to controlled text generation",
            "venue": "arXiv preprint arXiv:1912.02164.",
            "year": 2019
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805.",
            "year": 2018
        },
        {
            "authors": [
                "Li Dong",
                "Nan Yang",
                "Wenhui Wang",
                "Furu Wei",
                "Xiaodong Liu",
                "Yu Wang",
                "Jianfeng Gao",
                "Ming Zhou",
                "Hsiao-Wuen Hon."
            ],
            "title": "Unified language model pre-training for natural language understanding and generation",
            "venue": "Advances in Neural Information",
            "year": 2019
        },
        {
            "authors": [
                "Mihail Eric",
                "Rahul Goel",
                "Shachi Paul",
                "Abhishek Sethi",
                "Sanchit Agarwal",
                "Shuyag Gao",
                "Dilek HakkaniTur"
            ],
            "title": "Multiwoz 2.1: Multi-domain dialogue state corrections and state tracking baselines. arXiv preprint arXiv:1907.01669",
            "year": 2019
        },
        {
            "authors": [
                "Shuyang Gao",
                "Sanchit Agarwal",
                "Tagyoung Chung",
                "Di Jin",
                "Dilek Hakkani-Tur."
            ],
            "title": "From machine reading comprehension to dialogue state tracking: Bridging the gap",
            "venue": "arXiv preprint arXiv:2004.05827.",
            "year": 2020
        },
        {
            "authors": [
                "Ian Goodfellow",
                "Jean Pouget-Abadie",
                "Mehdi Mirza",
                "Bing Xu",
                "David Warde-Farley",
                "Sherjil Ozair",
                "Aaron Courville",
                "Yoshua Bengio."
            ],
            "title": "Generative adversarial nets",
            "venue": "Advances in neural information processing systems, 27.",
            "year": 2014
        },
        {
            "authors": [
                "Claudio Greco",
                "Barbara Plank",
                "Raquel Fern\u00e1ndez",
                "Raffaella Bernardi."
            ],
            "title": "Psycholinguistics meets continual learning: Measuring catastrophic forgetting in visual question answering",
            "venue": "arXiv preprint arXiv:1906.04229.",
            "year": 2019
        },
        {
            "authors": [
                "Emil Julius Gumbel."
            ],
            "title": "Statistical theory of extreme values and some practical applications: a series of lectures, volume 33",
            "venue": "US Government Printing Office.",
            "year": 1954
        },
        {
            "authors": [
                "Wanwei He",
                "Yinpei Dai",
                "Yinhe Zheng",
                "Yuchuan Wu",
                "Zheng Cao",
                "Dermot Liu",
                "Peng Jiang",
                "Min Yang",
                "Fei Huang",
                "Luo Si"
            ],
            "title": "Galaxy: A generative pre-trained model for task-oriented dialog with semisupervised learning and explicit policy injection",
            "year": 2022
        },
        {
            "authors": [
                "Ehsan Hosseini-Asl",
                "Bryan McCann",
                "Chien-Sheng Wu",
                "Semih Yavuz",
                "Richard Socher."
            ],
            "title": "A simple language model for task-oriented dialogue",
            "venue": "Advances in Neural Information Processing Systems, 33:20179\u201320191.",
            "year": 2020
        },
        {
            "authors": [
                "Eric Jang",
                "Shixiang Gu",
                "Ben Poole."
            ],
            "title": "Categorical reparameterization with gumbel-softmax",
            "venue": "arXiv preprint arXiv:1611.01144.",
            "year": 2016
        },
        {
            "authors": [
                "Jon\u00e1\u0161 Kulh\u00e1nek",
                "Vojt\u011bch Hude\u010dek",
                "Tom\u00e1\u0161 Nekvinda",
                "Ond\u0159ej Du\u0161ek."
            ],
            "title": "Augpt: Auxiliary tasks and data augmentation for end-to-end dialogue with pre-trained language models",
            "venue": "arXiv preprint arXiv:2102.05126.",
            "year": 2021
        },
        {
            "authors": [
                "Yohan Lee."
            ],
            "title": "Improving end-to-end task-oriented dialog system with a simple auxiliary task",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1296\u20131303, Punta Cana, Dominican Republic. Association for Compu-",
            "year": 2021
        },
        {
            "authors": [
                "Wenqiang Lei",
                "Xisen Jin",
                "Min-Yen Kan",
                "Zhaochun Ren",
                "Xiangnan He",
                "Dawei Yin."
            ],
            "title": "Sequicity: Simplifying task-oriented dialogue systems with single sequence-to-sequence architectures",
            "venue": "Proceedings of the 56th Annual Meeting of the Association",
            "year": 2018
        },
        {
            "authors": [
                "Zhaojiang Lin",
                "Andrea Madotto",
                "Genta Indra Winata",
                "Pascale Fung."
            ],
            "title": "Mintl: Minimalist transfer learning for task-oriented dialogue systems",
            "venue": "arXiv preprint arXiv:2009.12005.",
            "year": 2020
        },
        {
            "authors": [
                "Chris J Maddison",
                "Andriy Mnih",
                "Yee Whye Teh."
            ],
            "title": "The concrete distribution: A continuous relaxation of discrete random variables",
            "venue": "arXiv preprint arXiv:1611.00712.",
            "year": 2016
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318, Philadelphia,",
            "year": 2002
        },
        {
            "authors": [
                "Baolin Peng",
                "Chunyuan Li",
                "Jinchao Li",
                "Shahin Shayandeh",
                "Lars Liden",
                "Jianfeng Gao."
            ],
            "title": "Soloist: Few-shot task-oriented dialog with a single pretrained auto-regressive model",
            "venue": "arXiv preprint arXiv:2005.05298.",
            "year": 2020
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu"
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2020
        },
        {
            "authors": [
                "Yixuan Su",
                "Lei Shu",
                "Elman Mansimov",
                "Arshit Gupta",
                "Deng Cai",
                "Yi-An Lai",
                "Yi Zhang"
            ],
            "title": "Multitask pre-training for plug-and-play task-oriented dialogue system",
            "year": 2022
        },
        {
            "authors": [
                "Thomas Wolf",
                "Victor Sanh",
                "Julien Chaumond",
                "Clement Delangue."
            ],
            "title": "Transfertransfo: A transfer learning approach for neural network based conversational agents",
            "venue": "arXiv preprint arXiv:1901.08149.",
            "year": 2019
        },
        {
            "authors": [
                "Chien-Sheng Wu",
                "Steven C.H. Hoi",
                "Richard Socher",
                "Caiming Xiong."
            ],
            "title": "TOD-BERT: Pre-trained natural language understanding for task-oriented dialogue",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
            "year": 2020
        },
        {
            "authors": [
                "Shiquan Yang",
                "Xinting Huang",
                "Jey Han Lau",
                "Sarah Erfani."
            ],
            "title": "Robust task-oriented dialogue generation with contrastive pre-training and adversarial filtering",
            "venue": "arXiv preprint arXiv:2205.10363.",
            "year": 2022
        },
        {
            "authors": [
                "Yunyi Yang",
                "Yunhao Li",
                "Xiaojun Quan."
            ],
            "title": "Ubar: Towards fully end-to-end task-oriented dialog system with gpt-2",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 14230\u201314238.",
            "year": 2021
        },
        {
            "authors": [
                "Steve Young",
                "Milica Ga\u0161i\u0107",
                "Blaise Thomson",
                "Jason D. Williams."
            ],
            "title": "Pomdp-based statistical spoken dialog systems: A review",
            "venue": "Proceedings of the IEEE, 101(5):1160\u20131179.",
            "year": 2013
        },
        {
            "authors": [
                "Yichi Zhang",
                "Zhijian Ou",
                "Zhou Yu."
            ],
            "title": "Taskoriented dialog systems that consider multiple appropriate responses under the same context",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 9604\u20139611.",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Task-oriented dialogue (TOD) systems are developed to lead conversations with users and assist them with the completion of various tasks. Unlike traditional solutions which rely on natural language understanding, state tracking, language generation, and other modules, end-to-end systems utilize a single network for all required functionality (Young et al., 2013). The recent research in the field has concentrated on leveraging language models pre-trained on general-domain corpora (Devlin et al., 2018; Radford et al., 2019; Raffel et al., 2020) to produce more robust architectures finetuned specifically for TOD generation. This has bridged the gap between production-ready modularized pipelines and single-network models in terms of accuracy and human-sounding results. However, such architectures are big and computationally expensive; they are also prone to overfitting on the final task and \"forgetting\" useful capabilities from the pre-training phase (Greco et al., 2019; Kulh\u00e1nek et al., 2021). Multiple studies (Section 2)\nhave demonstrated that learning related auxiliary tasks can improve the generation performance of a model while making it less affected by the overfitting issue.\nIn this paper, we study the effects of learning auxiliary response selection tasks together with an architecture based on the T5 (Raffel et al., 2020) text-to-text transformer. We use MTTOD (Lee, 2021), trained on the MultiWOZ 2.1 (Eric et al., 2019) dataset, as a baseline and evaluate two main approaches for response selection:\n\u2022 Binary classifier to distinguish between encodings of ground truth responses and encodings of distractor sentences sampled from the dataset.\n\u2022 Binary classifier to tell apart ground truth responses from decoder-generated sequences.\nReproducible code and model checkpoints are available at https://github.com/ radi-cho/RSTOD."
        },
        {
            "heading": "2 Related Work",
            "text": "TOD sequence-to-sequence models usually generate a belief state based on the dialogue history and then use the belief state (in addition to the previous context) to generate a response (Lei et al., 2018).\nPre-trained Language Models such as BERT (Devlin et al., 2018), GPT-2 (Radford et al., 2019), and T5 (Raffel et al., 2020) significantly enhance dialogue systems when they are fine-tuned for sequence tasks. The first study to validate this on GPT-2 is (Budzianowski and Vulic\u0301, 2019). SOLOIST (Peng et al., 2020), UBAR (Yang et al., 2021), and SimpleTOD (Hosseini-Asl et al., 2020) further develop the end-to-end setting of the problem by considering database results and generated responses during training. MinTL (Lin et al., 2020) provides a minimalistic transfer learning dialogue system with multiple backbones. TOD-BERT (Wu\nar X\niv :2\n20 8.\n07 09\n7v 2\n[ cs\n.C L\n] 1\n2 Fe\nb 20\n23\net al., 2020) utilizes a contrastive objective function to mimic a response selection task. (Yang et al., 2022) augments data by ignoring nonessential tokens and also adversarially filters \u201ceasy\u201d samples to enhance model robustness.\nAuxiliary Learning - training additional tasks which improve the performance of the primary text generation task - is increasingly applied in TOD systems. AuGPT (Kulh\u00e1nek et al., 2021) demonstrates that response selection tasks are helpful on top of GPT-2. MTTOD (Lee, 2021) has a span selection auxiliary task. GALAXY (He et al., 2022) (with UniLM (Dong et al., 2019) as a backbone) optimizes four objectives, one of which is a selection between ground truth responses and randomly sampled responses. PPTOD (Su et al., 2022) is also trained for multiple tasks in a plug-and-play fashion (Dathathri et al., 2019)."
        },
        {
            "heading": "3 Method",
            "text": ""
        },
        {
            "heading": "3.1 Dialogue System Baseline",
            "text": "As a baseline, we use the end-to-end system setting introduced in (Lee, 2021) (Figure 1) with T5 encoder-decoder backbone. The encoder input consists of a dialogue history concatenated with a user utterance. A belief state decoder generates a sequence of a domain name, slot names, and slot values. There is an option for querying a domainspecific database based on the belief state to generate a DB state which is then used to condition a final response decoder. The response output contains a system action state - a sequence of a domain name, action types and slots - and a system response. Since the decoder works autoregressively1, response generation is automatically conditioned on the system action.\nAs shown in figure 1, MTTOD utilizes a classifier as an auxiliary task for span matching, inspired by recent dialogue state tracking approaches. Labels for this task are the extractive informable slots defined in (Gao et al., 2020).\nThe loss to be jointly minimized is\nL = \u03b1Lbelief +\u03b2 Lresp+\u03b3 Lspan (1)\nwhere Lbelief and Lresp are negative loglikelihood language modeling losses for the two decoders and Lspan is a cross-entropy loss for the span task. For compatibility with (Lee, 2021) the\n1An autoregressive decoder uses information from previous time steps to generate the value at the current time step.\ncoefficients \u03b1, \u03b2 and \u03b3 are set to 1.0, 1.0 and 0.5 respectively. Refer to section 5 for baseline benchmarks."
        },
        {
            "heading": "3.2 Response Selection as an Auxiliary Task",
            "text": "Our study aims to evaluate the effects of using response selection as an additional auxiliary task for the presented T5-based dialogue system. We propose two variants for such a task (Figure 2) and modify the full objective to\nL = \u03b1Lbelief +\u03b2 Lresp+\u03b3 Lspan+\u03b4Lselect (2)\nIn our experiments \u03b4 is also set to 0.5."
        },
        {
            "heading": "3.2.1 Distinguishing distractor encodings",
            "text": "The first proposal for a response selection task in our system is a binary classifier head - a linear layer or a simple multilayer perceptron - distinguishing randomly sampled distractor responses from ground truth responses. During training, the dialogue context Ct at time step t (consisting of history Ht and user utterance Ut) is concatenated with both the ground truth labels Tt - forming a sequence (Ct, Tt) - and a distractor response Dt sampled from the dataset - forming a sequence (Ct, Dt). Encodings for both sequences are generated by the already implemented T5 encoder and are then fed to the response selection head. The class label is 0 for (Ct, Dt) and 1 for (Ct, Tt). The binary cross entropy loss to be minimized is defined as\nLselect = \u2212 log p (l = 1 | Ct, Tt) \u2212 log p (l = 0 | Ct, Dt)\n(3)\np (l = 1 | Ct, Tt) = sigmoid (\u03c6a(\u03c6E(Ct, Tt))) \u2208 R1\np (l = 0 | Ct, Dt) = 1\u2212 sigmoid (\u03c6a(\u03c6E(Ct, Dt))) \u2208 R1\n(4)\nwhere \u03c6E denotes the encoder and \u03c6a - the final classifier.\nOptimizing the auxiliary response selection task affects the gradients of the encoder parameters. We empirically prove that this is beneficial for the overall score improvements on multiple metrics."
        },
        {
            "heading": "3.2.2 Distinguishing generated sequences",
            "text": "We also propose another independent auxiliary task for response selection inspired by Generative Adversarial Networks (Goodfellow et al., 2014). Its goal is to distinguish between responses from the transformer Rt and ground truth sequences Tt.\nThe baseline response decoder generates a sequence of token logits \u03c01, \u03c02, ..., \u03c0k, where \u03c0i is a vector of unnormalized class outputs over a vocabulary with size v. To obtain token ids we usually apply\nargmax j\n[log \u03c0ij ], j \u2208 [1, v \u2212 1] (5)\nfor every \u03c0i. However, such a step is not differentiable, and when subsequent layers are optimized, transformer gradients won\u2019t be affected, making the auxiliary task useless. One way to overcome the limitation is to re-encode the sequences as previously described in 3.2.1 and thus backpropagate knowledge to the T5 encoder. Instead, we propose a classifier that works with differentiably sampled token representations and backpropagates knowledge to the whole architecture during training.\nWe sample vocabulary class probabilities yi1, yi2, ..., yiv for every token representation \u03c0i from a Gumbel-Softmax distribution (Jang et al., 2016; Maddison et al., 2016; Gumbel, 1954):\nyij = exp((log(\u03c0ij) + gj)/\u03c4)\u2211v k=1 exp((log(\u03c0ik) + gk)/\u03c4)\n(6)\nwhere \u03c4 is a temperature, treated as a hyperparameter, and gj is a noise sample from a Gumbel(0, 1) distribution which can be computed by drawing a u \u223c Uniform(0, 1) and applying\ng = \u2212 log(\u2212 log(u)) (7)\nFor consistency with ground truth response sequences which are represented with v-dimensional one-hot vectors y\u0302i, we programmatically2 convert the probabilities yi to one-hot vectors but compute gradients with their continuous values.\n2Refer to the hard flag in http://pytorch. org/docs/stable/generated/torch.nn. functional.gumbel_softmax\nFinally, both y and y\u0302 are fed to the binary classifier \u03c6b and the loss to be minimized is computed as\nLselect = \u2212 log p (l = 1 | y\u0302) \u2212 log p (l = 0 | y)\n(8)\np (l = 1 | y\u0302) = sigmoid (\u03c6b(y\u0302)) \u2208 R1\np (l = 0 | y) = 1\u2212 sigmoid (\u03c6b(y)) \u2208 R1 (9)"
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Dataset",
            "text": "In our workflow, we use the large-scale TOD dataset MultiWOZ 2.1 (Eric et al., 2019) for benchmarks and comparisons with baselines. We follow the preprocessing techniques from (Zhang et al., 2020; Lee, 2021) to replace the specific slot values with placeholders. Table 1 presents more in-depth details and statistics on the contents of the dataset."
        },
        {
            "heading": "4.2 Training procedure",
            "text": "Train/development/test sets are generated with 80%/10%/10% of the samples. We optimize the objectives from section 3 for 15 epochs and report the results from the best performing checkpoint on the development set. In our experiments, we tested different learning rate schedule strategies and found the best results to be achieved with a constant learning rate initialized as 5\u00d7 10\u22124 with liner warmup for the first 10% of the samples.\nFor variant 2 of our architecture, a scheduler is used to linearly decrease the Gumbel-Softmax temperature \u03c4 with each training iteration. The optimal initial value for \u03c4 used to derive the results in Table 2 is 4 and is gradually decreased to 0.8."
        },
        {
            "heading": "4.3 Evaluation Metrics",
            "text": "During inference, the response selection head is not used and the model performs the same way in terms of speed as the T5-small baseline. We compute BLEU (Papineni et al., 2002), Inform and Success metrics for both architecture variants. Inform validates whether system entities are correct and Success checks whether relevant information is given for all user inquiries. A combined score is derived as 0.5 \u00d7 (Inform + Success) + BLEU which is consistent with previous studies."
        },
        {
            "heading": "5 Results",
            "text": ""
        },
        {
            "heading": "5.1 MultiWOZ Benchmarks",
            "text": "Table 2 compares the calculated benchmarks for the two proposed variants of our auxiliary task. As a baseline, we present the results of MTTOD with T5 base backbone, which has more than 360 million trainable parameters. In contrast, our models, which use T5 small as a backbone, have 102.2 and 105.5 million parameters but achieve higher overall results with total scores of 107.4 and 108.3, respectively."
        },
        {
            "heading": "6 Discussion",
            "text": "Response selection tasks similar to variant 1 of our architecture have been previously applied in models for chit-chatting and question answering (Wolf et al., 2019). For TOD systems such tasks are used in architectures with GPT-2 (AuGPT) and UniLM (GALAXY) backbones resulting in responses with higher text-generation metrics. Our study is the first to provide an in-depth analysis of whether a T5-based model in a task-oriented setting would\nbenefit from selection tasks. The results we present are consistent with related literature since we also observe an increase in generation performance.\nMost of the solutions relying on pre-trained language models have big amounts of trainable parameters making them slow to train. In our study, we use a modification of the baseline with T5-small instead of T5-base, reducing the parameters more than 3 times. In variant 1 the shared encoder is responsible for processing more sequences than the baseline - it is slower to train but identical in terms of inference speed and amount of storage space required for its weights. Variant 2 is comparable in terms of train-time and inference-time speed to the baseline but is able to achieve a higher overall score. It employs techniques for overcoming backpropagation issues with the discrete token representations of a generated response sequence3.\n3Usually text is generated by picking the most likely tokens"
        },
        {
            "heading": "7 Future Work",
            "text": "Directions for further research on the topic of TOD systems include testing our proposals on bigger backbone models to evaluate their effectiveness against overfitting, experimenting with additional auxiliary tasks for the current baseline, and introducing data augmentations. Also, whether our classifier heads could be used during inference to perform real-time response selection should be explored.\nAs a long-term development in the field, we consider various possibilities for building productionready end-to-end dialogue systems by employing reinforcement learning or semi-supervised learning methods. More experimentally, a generative adversarial network for creative text generation could\nfrom a probability distribution over the token space. This is not a differentiable operation and prevents gradient computations.\nalso be tested."
        },
        {
            "heading": "8 Conclusion",
            "text": "In this paper, we propose two independent auxiliary tasks for response selection on top of a TOD system transformer baseline. Both tasks demonstrate state-of-the-art results on multiple text generation metrics despite having 3+ times less trainable parameters. The first variant involves a classifier, distinguishing between distractor and ground truth responses, which affects the transformer encoder during training and achieves results consistent with related literature. The second variant applies a novel technique for the TOD problem and involves a classifier, distinguishing between synthetic and ground truth responses. We publish reproducible code implementations of our proposals and present potential directions for future research."
        }
    ],
    "title": "Efficient Task-Oriented Dialogue Systems with Response Selection as an Auxiliary Task",
    "year": 2023
}