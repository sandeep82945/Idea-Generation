{
    "abstractText": "It is common that entity mentions can contain other mentions recursively. This paper introduces a scalable transition-based method to model the nested structure of mentions. We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest. Our shiftreduce based system then learns to construct the forest structure in a bottom-up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length. Based on Stack-LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space, our system is further incorporated with a character-based component to capture letterlevel patterns. Our model achieves the stateof-the-art results on ACE datasets, showing its effectiveness in detecting nested mentions.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Bailin Wang"
        },
        {
            "affiliations": [],
            "name": "Wei Lu"
        },
        {
            "affiliations": [],
            "name": "Yu Wang"
        },
        {
            "affiliations": [],
            "name": "Hongxia Jin"
        }
    ],
    "id": "SP:def1fed499a03fffd74eb7c62b969b2548d14544",
    "references": [
        {
            "authors": [
                "Steven Abney",
                "Michael Collins",
                "Amit Singhal."
            ],
            "title": "Answer extraction",
            "venue": "Proc. of the sixth conference on applied natural language processing.",
            "year": 2000
        },
        {
            "authors": [
                "Beatrice Alex",
                "Barry Haddow",
                "Claire Grover."
            ],
            "title": "Recognising nested named entities in biomedical text",
            "venue": "Proc. of BioNLP.",
            "year": 2007
        },
        {
            "authors": [
                "Kai-Wei Chang",
                "Rajhans Samdani",
                "Dan Roth."
            ],
            "title": "A constrained latent variable model for coreference resolution",
            "venue": "Proc. of EMNLP.",
            "year": 2013
        },
        {
            "authors": [
                "Jason PC Chiu",
                "Eric Nichols."
            ],
            "title": "Named entity recognition with bidirectional lstm-cnns",
            "venue": "TACL.",
            "year": 2016
        },
        {
            "authors": [
                "Ronan Collobert",
                "Jason Weston",
                "L\u00e9on Bottou",
                "Michael Karlen",
                "Koray Kavukcuoglu",
                "Pavel Kuksa."
            ],
            "title": "Natural language processing (almost) from scratch",
            "venue": "JMLR.",
            "year": 2011
        },
        {
            "authors": [
                "George R Doddington",
                "Alexis Mitchell",
                "Mark A Przybocki",
                "Lance A Ramshaw",
                "Stephanie Strassel",
                "Ralph M Weischedel."
            ],
            "title": "The automatic content extraction (ace) program-tasks, data, and evaluation",
            "venue": "Proc. of LREC.",
            "year": 2004
        },
        {
            "authors": [
                "Chris Dyer",
                "Miguel Ballesteros",
                "Wang Ling",
                "Austin Matthews",
                "Noah A. Smith."
            ],
            "title": "Transitionbased dependency parsing with stack long shortterm memory",
            "venue": "Proc. of ACL.",
            "year": 2015
        },
        {
            "authors": [
                "Jenny Rose Finkel",
                "Christopher D Manning."
            ],
            "title": "Nested named entity recognition",
            "venue": "Proc. of EMNLP.",
            "year": 2009
        },
        {
            "authors": [
                "R. Florian",
                "H. Hassan",
                "A. Ittycheriah",
                "H. Jing",
                "N. Kambhatla",
                "X. Luo",
                "N. Nicolov",
                "S. Roukos."
            ],
            "title": "A statistical model for multilingual entity detection and tracking",
            "venue": "Proc. of HLT-NAACL.",
            "year": 2004
        },
        {
            "authors": [
                "Yoav Goldberg",
                "Joakim Nivre."
            ],
            "title": "A dynamic oracle for arc-eager dependency parsing",
            "venue": "Proceedings of COLING 2012, pages 959\u2013976.",
            "year": 2012
        },
        {
            "authors": [
                "Alex Graves",
                "J\u00fcrgen Schmidhuber."
            ],
            "title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures",
            "venue": "Neural Networks.",
            "year": 2005
        },
        {
            "authors": [
                "Zhiheng Huang",
                "Wei Xu",
                "Kai Yu."
            ],
            "title": "Bidirectional lstm-crf models for sequence tagging",
            "venue": "arXiv preprint arXiv:1508.01991.",
            "year": 2015
        },
        {
            "authors": [
                "Meizhi Ju",
                "Makoto Miwa",
                "Sophia Ananiadou."
            ],
            "title": "A neural layered model for nested named entity recognition",
            "venue": "Proc. of NAACL-HLT.",
            "year": 2018
        },
        {
            "authors": [
                "Arzoo Katiyar",
                "Claire Cardie."
            ],
            "title": "Nested named entity recognition revisited",
            "venue": "Proc. of NAACLHLT.",
            "year": 2018
        },
        {
            "authors": [
                "J-D Kim",
                "Tomoko Ohta",
                "Yuka Tateisi",
                "Jun\u2019ichi Tsujii"
            ],
            "title": "Genia corpus\u2014a semantically annotated corpus for bio-textmining",
            "year": 2003
        },
        {
            "authors": [
                "Diederik Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "Proc. of ICLR.",
            "year": 2014
        },
        {
            "authors": [
                "John Lafferty",
                "Andrew McCallum",
                "Fernando CN Pereira."
            ],
            "title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
            "venue": "Proc. of ICML.",
            "year": 2001
        },
        {
            "authors": [
                "Guillaume Lample",
                "Miguel Ballesteros",
                "Sandeep Subramanian",
                "Kazuya Kawakami",
                "Chris Dyer."
            ],
            "title": "Neural architectures for named entity recognition",
            "venue": "Proc. of NAACL-HLT.",
            "year": 2016
        },
        {
            "authors": [
                "Qi Li",
                "Heng Ji",
                "Liang Huang."
            ],
            "title": "Joint event extraction via structured prediction with global features",
            "venue": "Proc. of ACL.",
            "year": 2013
        },
        {
            "authors": [
                "Liyuan Liu",
                "Xiang Ren",
                "Qi Zhu",
                "Shi Zhi",
                "Huan Gui",
                "Heng Ji",
                "Jiawei Han."
            ],
            "title": "Heterogeneous supervision for relation extraction: A representation learning approach",
            "venue": "Proc. of EMNLP.",
            "year": 2017
        },
        {
            "authors": [
                "Yinxia Lou",
                "Yue Zhang",
                "Tao Qian",
                "Fei Li",
                "Shufeng Xiong",
                "Donghong Ji."
            ],
            "title": "A transition-based joint model for disease named entity recognition and normalization",
            "venue": "Bioinformatics.",
            "year": 2017
        },
        {
            "authors": [
                "Wei Lu",
                "Dan Roth."
            ],
            "title": "Joint mention extraction and classification with mention hypergraphs",
            "venue": "Proc. of EMNLP.",
            "year": 2015
        },
        {
            "authors": [
                "Xuezhe Ma",
                "Eduard Hovy."
            ],
            "title": "End-to-end sequence labeling via bi-directional lstm-cnns-crf",
            "venue": "Proc. of ACL.",
            "year": 2016
        },
        {
            "authors": [
                "Ryan McDonald",
                "Koby Crammer",
                "Fernando Pereira."
            ],
            "title": "Flexible text segmentation with structured multilabel classification",
            "venue": "Proc. of HLTEMNLP.",
            "year": 2005
        },
        {
            "authors": [
                "Mike Mintz",
                "Steven Bills",
                "Rion Snow",
                "Dan Jurafsky."
            ],
            "title": "Distant supervision for relation extraction without labeled data",
            "venue": "Proc. of ACL-IJCNLP.",
            "year": 2009
        },
        {
            "authors": [
                "Aldrian Obaja Muis",
                "Wei Lu."
            ],
            "title": "Learning to recognize discontiguous entities",
            "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 75\u201384.",
            "year": 2016
        },
        {
            "authors": [
                "Aldrian Obaja Muis",
                "Wei Lu."
            ],
            "title": "Labeling gaps between words: Recognizing overlapping mentions with mention separators",
            "venue": "Proc. of EMNLP.",
            "year": 2017
        },
        {
            "authors": [
                "Vincent Ng",
                "Claire Cardie."
            ],
            "title": "Improving machine learning approaches to coreference resolution",
            "venue": "Proc. of ACL.",
            "year": 2002
        },
        {
            "authors": [
                "Jeffrey Pennington",
                "Richard Socher",
                "Christopher Manning."
            ],
            "title": "Glove: Global vectors for word representation",
            "venue": "Proc. of EMNLP.",
            "year": 2014
        },
        {
            "authors": [
                "S. Riedel",
                "A. McCallum."
            ],
            "title": "Fast and robust joint models for biomedical event extraction",
            "venue": "Proc. of EMNLP.",
            "year": 2011
        },
        {
            "authors": [
                "Kenji Sagae",
                "Alon Lavie."
            ],
            "title": "A classifier-based parser with linear run-time complexity",
            "venue": "IWPT.",
            "year": 2005
        },
        {
            "authors": [
                "Richard Socher",
                "Alex Perelygin",
                "Jean Wu",
                "Jason Chuang",
                "Christopher D Manning",
                "Andrew Ng",
                "Christopher Potts."
            ],
            "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
            "venue": "Proc. of EMNLP.",
            "year": 2013
        },
        {
            "authors": [
                "Wee Meng Soon",
                "Hwee Tou Ng",
                "Daniel Chung Yong Lim."
            ],
            "title": "A machine learning approach to coreference resolution of noun phrases",
            "venue": "Computational linguistics, 27(4):521\u2013544.",
            "year": 2001
        },
        {
            "authors": [
                "Nitish Srivastava",
                "Geoffrey Hinton",
                "Alex Krizhevsky",
                "Ilya Sutskever",
                "Ruslan Salakhutdinov."
            ],
            "title": "Dropout: A simple way to prevent neural networks from overfitting",
            "venue": "JMLR.",
            "year": 2014
        },
        {
            "authors": [
                "Taro Watanabe",
                "Eiichiro Sumita."
            ],
            "title": "Transitionbased neural constituent parsing",
            "venue": "Proc. of ACL.",
            "year": 2015
        },
        {
            "authors": [
                "Jie Zhang",
                "Dan Shen",
                "Guodong Zhou",
                "Jian Su",
                "Chew-Lim Tan."
            ],
            "title": "Enhancing hmm-based biomedical named entity recognition by studying special phenomena",
            "venue": "Journal of Biomedical Informatics, 37(6):411\u2013422.",
            "year": 2004
        },
        {
            "authors": [
                "Yue Zhang",
                "Stephen Clark."
            ],
            "title": "Transition-based parsing of the chinese treebank using a global discriminative model",
            "venue": "Proc. of IWPT.",
            "year": 2009
        },
        {
            "authors": [
                "Guodong Zhou."
            ],
            "title": "Recognizing names in biomedical texts using mutual information independence model and svm plus sigmoid",
            "venue": "International Journal of Medical Informatics, 75(6):456\u2013467.",
            "year": 2006
        },
        {
            "authors": [
                "Guodong Zhou",
                "Jie Zhang",
                "Jian Su",
                "Dan Shen",
                "Chewlim Tan."
            ],
            "title": "Recognizing names in biomedical texts: a machine learning approach",
            "venue": "Bioinformatics, 20(7):1178\u20131190.",
            "year": 2004
        }
    ],
    "sections": [
        {
            "text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1011\u20131017 Brussels, Belgium, October 31 - November 4, 2018. c\u00a92018 Association for Computational Linguistics\n1011"
        },
        {
            "heading": "1 Introduction",
            "text": "There has been an increasing interest in named entity recognition or more generally recognizing entity mentions2 (Alex et al., 2007; Finkel and Manning, 2009; Lu and Roth, 2015; Muis and Lu, 2017) that the nested hierarchical structure of entity mentions should be taken into account to better facilitate downstream tasks like question answering (Abney et al., 2000), relation extraction (Mintz et al., 2009; Liu et al., 2017), event extraction (Riedel and McCallum, 2011; Li et al., 2013), and coreference resolution (Soon et al., 2001; Ng and Cardie, 2002; Chang et al., 2013). Practically, the mentions with nested structures frequently exist in news (Doddington et al., 2004) and biomedical documents (Kim et al., 2003). For example in\n1We make our implementation available at https:// github.com/berlino/nest-trans-em18.\n2Mentions are defined as references to entities that could be named, nominal or pronominal (Florian et al., 2004).\nFigure 1, \u201cUN Secretary General\u201d of type Person also contains \u201cUN\u201d of type Organization.\nTraditional sequence labeling models such as conditional random fields (CRF) (Lafferty et al., 2001) do not allow hierarchical structures between segments, making them incapable to handle such problems. Finkel and Manning (2009) presented a chart-based parsing approach where each sentence with nested mentions is mapped to a rooted constituent tree. The issue of using a chart-based parser is its cubic time complexity in the number of words in the sentence.\nTo achieve a scalable and effective solution for recognizing nested mentions, we design a transition-based system which is inspired by the recent success of employing transition-based methods for constituent parsing (Zhang and Clark, 2009) and named entity recognition (Lou et al., 2017), especially when they are paired with neural networks (Watanabe and Sumita, 2015). Generally, each sentence with nested mentions is mapped to a forest where each outermost mention forms a tree consisting of its inner mentions. Then our transition-based system learns to construct this forest through a sequence of shift-reduce actions. Figure 1 shows an example of such a forest. In contrast, the tree structure by Finkel and Manning (2009) further uses a root node to connect all tree elements. Our forest representation eliminates the root node so that the number of actions required to\nconstruct it can be reduced significantly. Following (Dyer et al., 2015), we employ StackLSTM to represent the system\u2019s state, which consists of the states of input, stack and action history, in a continuous space incrementally. The (partially) processed nested mentions in the stack are encoded with recursive neural networks (Socher et al., 2013) where composition functions are used to capture dependencies between nested mentions. Based on the observation that letter-level patterns such as capitalization and prefix can be beneficial in detecting mentions, we incorporate a characterlevel LSTM to capture such morphological information. Meanwhile, this character-level component can also help deal with the out-of-vocabulary problem of neural models. We conduct experiments in three standard datasets. Our system achieves the state-of-the-art performance on ACE datasets and comparable performance in GENIA dataset."
        },
        {
            "heading": "2 Related Work",
            "text": "Entity mention recognition with nested structures has been explored first with rule-based approaches (Zhang et al., 2004; Zhou et al., 2004; Zhou, 2006) where the authors first detected the innermost mentions and then relied on rule-based postprocessing methods to identify outer mentions. McDonald et al. (2005) proposed a structured multi-label model to represent overlapping segments in a sentence. but it came with a cubic time complexity in the number of words. Alex et al. (2007) proposed several ways to combine multiple conditional random fields (CRF) (Lafferty et al., 2001) for such tasks. Their best results were obtained by cascading several CRF models in a specific order while each model is responsible for detecting mentions of a particular type. However, such an approach cannot model nested mentions of the same type, which frequently appear.\nLu and Roth (2015) and Muis and Lu (2017) proposed new representations of mention hypergraph and mention separator to model overlapping mentions. However, the nested structure is not guaranteed in such approaches since overlapping structures additionally include the crossing structures3, which rarely exist in practice (Lu and Roth, 2015). Also, their representations did not model the dependencies between nested mentions\n3For example, in a four-word sentence ABCD, the phrase ABC and BCD together form a crossing structure.\nexplicitly, which may limit their performance. In contrast, the chart-based parsing method (Finkel and Manning, 2009) can capture the dependencies between nested mentions with composition rules which allow an outer entity to be influenced by its contained entities. However, their cubic time complexity makes them not scalable to large datasets.\nAs neural network based approaches are proven effective in entity or mention recognition (Collobert et al., 2011; Lample et al., 2016; Huang et al., 2015; Chiu and Nichols, 2016; Ma and Hovy, 2016), recent efforts focus on incorporating neural components for recognizing nested mentions. Ju et al. (2018) dynamically stacked multiple LSTM-CRF layers (Lample et al., 2016), detecting mentions in an inside-out manner until no outer entities are extracted. Katiyar and Cardie (2018) used recurrent neural networks to extract features for a hypergraph which encodes all nested mentions based on the BILOU tagging scheme."
        },
        {
            "heading": "3 Model",
            "text": "Specifically, given a sequence of words {x0, x1, . . . , xn}, the goal of our system is to output a set of mentions where nested structures are allowed. We use the forest structure to model the nested mentions scattered in a sentence, as shown in Figure 1. The mapping is straightforward: each outermost mention forms a tree where the mention is the root and its contained mentions correspond to constituents of the tree.4"
        },
        {
            "heading": "3.1 Shift-Reduce System",
            "text": "Our transition-based model is based on the shiftreduce parser for constituency parsing (Watan-\n4Note that words that are not contained in any mention each forms a single-node tree.\nabe and Sumita, 2015), which adopts (Zhang and Clark, 2009; Sagae and Lavie, 2005). Generally, our system employs a stack to store (partially) processed nested elements. The system\u2019s state is defined as [S, i, A] which denotes stack, buffer front index and action history respectively. In each step. an action is applied to change the system\u2019s state.\nOur system consists of three types of transition actions, which are also summarized in Figure 2:\n\u2022 SHIFT pushes the next word from buffer to the stack.\n\u2022 REDUCE-X pops the top two items t0 and t1 from the tack and combines them as a new tree element {X \u2192 t0t1} which is then pushed onto the stack.\n\u2022 UNARY-X pops the top item t0 from the stack and constructs a new tree element {X \u2192 t0} which is pushed back to the stack.\nSince the shift-reduce system assumes unary and binary branching, we binarize the trees in each forest in a left-branching manner. For example, if three consecutive words A,B,C are annotated as Person, we convert it into a binary tree {Person \u2192 {Person\u2217 \u2192 A,B}, C} where Person\u2217 is a temporary label for Person. Hence, the X in reduce- actions will also include such temporary labels.\nNote that since most words are not contained in any mention, they are only shifted to the stack and not involved in any reduce- or unary- actions. An example sequence of transitions can be found in Figure 3. Our shift-reduce system is different from previous parsers in terms of the terminal state. 1) It does not require the terminal stack to be a rooted tree. Instead, the final stack should be a forest consisting of multiple nested elements with tree structures. 2) To conveniently determine the ending of our transition process, we add an auxiliary symbol $ to each sentence. Once it is pushed to the stack, it implies that all deductions of actual words are finished. Since we do not allow unary rules between labels like X1 \u2192 X2, the length of maximal action sequence is 3n.5"
        },
        {
            "heading": "3.2 Action Constraints",
            "text": "To make sure that each action sequence is valid, we need to make some hard constraints on the ac-\n5In this case, each word is shifted (n) and involved in a unary action (n). Then all elements are reduced to a single node (n\u2212 1). The last action is to shift the symbol $.\ntion to take. For example, reduce- action can only be conducted when there are at least two elements in the stack. Please see the Appendix for the full list of restrictions. Formally, we use V(S, i, A) to denote the valid actions given the parser state. Let us denote the feature vector for the parser state at time step k as pk. The distribution of actions is computed as follows:\np(zk | pk) = exp\n( w>zkpk + bzk )\u2211 z\u2032\u2208V(S,i,A) exp ( w>z\u2032pk + bz\u2032\n) (1)\nwhere wz is a column weight vector for action z, and bz is a bias term."
        },
        {
            "heading": "3.3 Neural Transition-based Model",
            "text": "We use neural networks to learn the representation of the parser state, which is pk in (1)."
        },
        {
            "heading": "Representation of Words",
            "text": "Words are represented by concatenating three vectors:\nexi = [ewi , epi , cwi ] (2)\nwhere ewi and epi denote the embeddings for i-th word and its POS tag respectively. cwi denotes the representation learned by a character-level model\nusing a bidirectional LSTM. Specifically, for character sequence s0, s1, . . . , sn in the i-th word, we use the last hidden states of forward and backward LSTM as the character-based representation of this word, as shown below:\ncwi = [ \u2212\u2212\u2212\u2212\u2192 LSTMc(s0, . . . , sn), \u2190\u2212\u2212\u2212\u2212 LSTMc(s0, . . . , sn)]\n(3)"
        },
        {
            "heading": "Representation of Parser States",
            "text": "Generally, the buffer and action history are encoded using two vanilla LSTMs (Graves and Schmidhuber, 2005). For the stack that involves popping out top elements, we use the Stack-LSTM (Dyer et al., 2015) to efficiently encode it.\nFormally, if the unprocessed word sequence in the buffer is xi, xi+1, . . . , xn and action history sequence is a0, a1, . . . , ak\u22121, then we can compute buffer representation bk and action history representation ak at time step k as follows:\nbk = \u2190\u2212\u2212\u2212\u2212\u2212 LSTMb[exi , . . . , exn ] (4) ak = \u2212\u2212\u2212\u2212\u2212\u2192 LSTMa[ea0 , . . . , eak\u22121 ] (5)\nwhere each action is also mapped to a distributed representation ea.6 For the state of the stack, we also use an LSTM to encode a sequence of tree elements. However, the top elements of the stack are updated frequently. Stack-LSTM provides an efficient implementation that incorporates a stackpointer.7 Formally, the state of the stack bk at time step k is computed as:\nsk = Stack-LSTM[htm , . . . ,ht0 ] (6)\nwhere hti denotes the representation of the i-th tree element from the top, which can be computed recursively similar to Recursive Neural Network (Socher et al., 2013) as follows:\nhparent = W > u,lhchild + bu,l (7) hparent = W > b,l[hlchild,hrchild] + bu,l (8)\nwhere Wu,l and Wb,l denote the weight matrices for unary(u) and binary(b) composition with parent node being label(l). Note that the composition function is distinct for each label l. Recall that the leaf nodes of each tree element are raw words. Instead of representing them with their original embeddings introduced in Section 3.3, we found that\n6Note that LSTMb runs in a right-to-left order such that the output can represent the contextual information of xi.\n7Please refer to Dyer et al. (2015) for details.\nconcatenating the buffer state in (5) are beneficial during our initial experiments. Formally, when a word xi is shifted to the stack at time step k, its representation is computed as:\nhleaf = W > leaf [exi ,bk] + bleaf (9)\nFinally, the state of the system pk is the concatenation of the states of buffer, stack and action history:\npk = [bk, sk,ak] (10)"
        },
        {
            "heading": "Training",
            "text": "We employ the greedy strategy to maximize the log-likelihood of the local action classifier in (1). Specifically, let zik denote the k-th action for the i-th sentence, the loss function with `2 norm is:\nL(\u03b8) = \u2212 \u2211 i \u2211 k log p(zik) + \u03bb 2 \u2016\u03b8\u20162 (11)\nwhere \u03bb is the `2 coefficient."
        },
        {
            "heading": "4 Experiments",
            "text": "We mainly evaluate our models on the standard ACE-04, ACE-05 (Doddington et al., 2004), and GENIA (Kim et al., 2003) datasets with the same splits used by previous research efforts (Lu and Roth, 2015; Muis and Lu, 2017). In ACE datasets, more than 40% of the mentions form nested structures with some other mention. In GENIA, this number is 18%. Please see Lu and Roth (2015) for the full statistics."
        },
        {
            "heading": "4.1 Setup",
            "text": "Pre-trained embeddings GloVe (Pennington et al., 2014) of dimension 100 are used to initialize the\n8Note that in ACE2005, Ju et al. (2018) did their experiments with a different split from Lu and Roth (2015) and Muis and Lu (2017) which we follow as our split.\nword vectors for all three datasets.9 The embeddings of POS tags are initialized randomly with dimension 32. The model is trained using Adam (Kingma and Ba, 2014) and a gradient clipping of 3.0. Early stopping is used based on the performance of development sets. Dropout (Srivastava et al., 2014) is used after the input layer. The `2 coefficient \u03bb is also tuned during development process."
        },
        {
            "heading": "4.2 Results",
            "text": "The main results are reported in Table 1. Our neural transition-based model achieves the best results in ACE datasets and comparable results in GENIA dataset in terms of F1 measure. We hypothesize that the performance gain of our model compared with other methods is largely due to improved performance on the portions of nested mentions in our datasets. To verify this, we design an experiment to evaluate how well a system can recognize nested mentions."
        },
        {
            "heading": "Handling Nested Mentions",
            "text": "The idea is that we split the test data into two portions: sentences with and without nested mentions. The results of GENIA are listed in Table 2. We can observe that the margin of improvement is more significant in the portion of nested mentions, revealing our model\u2019s effectiveness in handling nested mentions. This observation helps explain why our model achieves greater improvement in ACE than in GENIA in Table 1 since the former has much more nested structures than the latter. Moreover, Ju et al. (2018) performs better when it comes to non-nested mentions possibly due to the CRF they used, which globally normalizes each stacked layer."
        },
        {
            "heading": "Decoding Speed",
            "text": "Note that Lu and Roth (2015) and Muis and Lu (2017) also feature linear-time complexity, but with a greater constant factor. To compare the decoding speed, we re-implemented their model with the same platform (PyTorch) and run them on the same machine (CPU: Intel i5 2.7GHz). Our model turns out to be around 3-5 times faster than theirs, showing its scalability.\n9We also additionally tried using embeddings trained on PubMed for GENIA but the performance was comparable."
        },
        {
            "heading": "Ablation Study",
            "text": "To evaluate the contribution of neural components including pre-trained embeddings, the characterlevel LSTM and dropout layers, we test the performances of ablated models. The results are listed in Table 1. From the performance gap, we can conclude that these components contribute significantly to the effectiveness of our model in all three datasets."
        },
        {
            "heading": "5 Conclusion and Future Work",
            "text": "In this paper, we present a transition-based model for nested mention recognition using a forest representation. Coupled with Stack-LSTM for representing the system\u2019s state, our neural model can capture dependencies between nested mentions efficiently. Moreover, the character-based component helps capture letter-level patterns in words. The system achieves the state-of-the-art performance in ACE datasets.\nOne potential drawback of the system is the greedy training and decoding. We believe that alternatives like beam search and training with exploration (Goldberg and Nivre, 2012) could further boost the performance. Another direction that we plan to work on is to apply this model to recognizing overlapping and entities that involve discontinuous spans (Muis and Lu, 2016) which frequently exist in the biomedical domain."
        },
        {
            "heading": "Acknowledgements",
            "text": "We would like to thank the anonymous reviewers for their valuable comments. We also thank Meizhi Ju for providing raw predictions and helpful discussions. This work was done after the first author visited Singapore University of Technology and Design. This work is supported by Singapore Ministry of Education Academic Research Fund (AcRF) Tier 2 Project MOE2017-T2-1-156."
        },
        {
            "heading": "Appendix",
            "text": "The action constraints are listed as follows:\n\u2022 The SHIFT action is valid only when the buffer is not empty.\n\u2022 The UNARY-X actions are valid only when the stack is not empty.\n\u2022 The REDUCE-X actions are valid only when the stack has two or more elements.\n\u2022 If the top element of the stack is labeled, then unary actions are not valid. That is, {X1 \u2192 X2} is not allowed.\n\u2022 If only one of the top two elements of the stack is temporary, say X*, then among all reduce actions, only REDUCE-X* and REDUCE-X are valid.\n\u2022 If the top two elements of the stack are both temporary, then all reduce actions are not allowed.\n\u2022 If one of the elements in the stack is temporary, say X*, which means it is not finished, then last terminal symbol $ cannot be shifted until it is reduced to X."
        }
    ],
    "title": "A Neural Transition-based Model for Nested Mention Recognition",
    "year": 2018
}