{
    "abstractText": "Sound source separation is an essential aspect in auditory scene analysis, which is still an urgent challenge for machine hearing. In this paper, a fully convolutional time-domain audio separation network (ConvTasNet) is trained for universal two-source separation, consisting of speech, environmental sounds, and music. Besides the separation performance of the network, the underlying separation mechanisms are our main concern. Through a series of classic auditory segregation experiments, we systematically explore the principles learned by the network for simultaneous and sequential organization. The results show that without prior knowledge of auditory scene analysis imparted on the network, it spontaneously learns the separation mechanisms from raw waveforms that are similar to those which have developed over many years in humans. The Gestalt principles for separation in the human auditory system are shown to be effective in our network: harmonicity, onset synchrony and common fate (coherent modulation in amplitude and frequency), proximity, continuity, similarity. The universal sound source separation network following Gestalt principles is not limited to specific sources and can be applied to various acoustic situations like human hearing, providing new directions for solving the problem of auditory scene analysis.",
    "authors": [
        {
            "affiliations": [],
            "name": "Han Li"
        },
        {
            "affiliations": [],
            "name": "Bernhard U. Seeber"
        }
    ],
    "id": "SP:f543ba66cb8dbd44e50a13a951e219d96305428b",
    "references": [
        {
            "authors": [
                "A.S. Bregman"
            ],
            "title": "Auditory Scene Analysis: The Perceptual Organization of Sound",
            "year": 1990
        },
        {
            "authors": [
                "D. Wang",
                "G.J. Brown"
            ],
            "title": "Fundamentals of computational auditory scene analysis",
            "venue": "Computational Auditory Scene Analysis: Principles, Algorithms, and Applications, D. Wang and G. J. Brown, Eds., Hoboken, NJ, USA: Wiley, 2006, pp. 1\u201337.",
            "year": 2006
        },
        {
            "authors": [
                "D. Wang",
                "G.J. Brown"
            ],
            "title": "Separation of speech from interfering sounds based on oscillatory correlation",
            "venue": "IEEE Trans. Neural Netw., vol. 10, no. 3, pp. 684\u2013697, May 1999.",
            "year": 1999
        },
        {
            "authors": [
                "G. Hu",
                "D. Wang"
            ],
            "title": "Monaural speech segregation based on pitch tracking and amplitude modulation",
            "venue": "IEEE Trans. Neural Netw., vol. 15, no. 5, pp. 1135\u20131150, Sep. 2004.",
            "year": 2004
        },
        {
            "authors": [
                "G. Hu",
                "D. Wang"
            ],
            "title": "Auditory segmentation based on onset and offset analysis",
            "venue": "IEEE Trans. Audio, Speech Lang. Process., vol. 15, no. 2, pp. 396\u2013405, Feb. 2007.",
            "year": 2007
        },
        {
            "authors": [
                "D. Wang",
                "J. Chen"
            ],
            "title": "Supervised speech separation based on deep learning: An overview",
            "venue": "IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 26, no. 10, pp. 1702\u20131726, Oct. 2018.",
            "year": 2018
        },
        {
            "authors": [
                "K. Tan",
                "J. Chen",
                "D. Wang"
            ],
            "title": "Gated residual networks with dilated convolutions for monaural speech enhancement",
            "venue": "IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 27, no. 1, pp. 189\u2013198, Jan. 2019.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Liu",
                "D. Wang"
            ],
            "title": "Divide and conquer: A deep CASA approach to talker-independent monaural speaker separation",
            "venue": "IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 27, no. 12, pp. 2092\u20132102, Dec. 2019.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Luo",
                "N. Mesgarani"
            ],
            "title": "Conv-TasNet: Surpassing ideal time-frequency magnitude masking for speech separation",
            "venue": "IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 27, no. 8, pp. 1256\u20131266, Aug. 2019.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Luo",
                "Z. Chen",
                "T. Yoshioka"
            ],
            "title": "Dual-Path RNN: Efficient long sequence modeling for time-domain single-channel speech separation",
            "venue": "Proc. IEEE Int. Conf. Acoust., Speech, Signal Process., 2020, pp. 46\u201350.",
            "year": 2020
        },
        {
            "authors": [
                "A. Francl",
                "J.H. McDermott"
            ],
            "title": "Deep neural network models of sound localization reveal how perception is adapted to real-world environments",
            "venue": "Nature Hum. Behav., vol. 6, no. 1, pp. 111\u2013133, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "D. Chakrabarty",
                "M. Elhilali"
            ],
            "title": "A Gestalt inference model for auditory scene segregation",
            "venue": "PLoS Comput. Biol., vol. 15, no. 1, pp. 1\u201333, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "H. Li",
                "K. Chen",
                "B.U. Seeber"
            ],
            "title": "Auditory filterbanks benefit universal sound source separation",
            "venue": "Proc. IEEE Int. Conf. Acoust., Speech, Signal Process., 2021, pp. 181\u2013185.",
            "year": 2021
        },
        {
            "authors": [
                "H. Li",
                "K. Chen",
                "R. Li",
                "J. Liu",
                "B. Wan",
                "B. Zhou"
            ],
            "title": "Auditory-like simultaneous separation mechanisms spontaneously learned by a deep source separation network",
            "venue": "Appl. Acoust., vol. 188, 2022, Art. no. 108591.",
            "year": 2022
        },
        {
            "authors": [
                "M. Wertheimer"
            ],
            "title": "Untersuchungen zur Lehre von der Gestalt. II",
            "venue": "Psychol. Forsch., vol. 4, no. 1, pp. 301\u2013350, Jan. 1923.",
            "year": 1923
        },
        {
            "authors": [
                "C.J. Darwin",
                "R.P. Carlyon"
            ],
            "title": "Auditory grouping",
            "venue": "Hearing, 2nd ed., B. C. J. Moore, Ed., San Diego, CA, USA: Academic, 1995, pp. 387\u2013424.",
            "year": 1995
        },
        {
            "authors": [
                "C. Micheyl",
                "A.J. Oxenham"
            ],
            "title": "Pitch, harmonicity and concurrent sound segregation: Psychoacoustical and neurophysiological findings",
            "venue": "Hear. Res., vol. 266, no. 1\u20132, pp. 36\u201351, 2010.",
            "year": 2010
        },
        {
            "authors": [
                "J.F. Culling",
                "C.J. Darwin"
            ],
            "title": "Perceptual and computational separation of simultaneous vowels: Cues arising from low-frequency beating",
            "venue": "J. Acoust. Soc. Amer., vol. 95, no. 3, pp. 1559\u20131569, 1994.",
            "year": 1994
        },
        {
            "authors": [
                "G.J. Sandell",
                "C.J. Darwin"
            ],
            "title": "Recognition of concurrently-sounding musical instruments with different fundamental frequencies",
            "venue": "J. Acoust. Soc. Amer., vol. 100, no. 4, Oct. 1996, Art. no. 2683.",
            "year": 1996
        },
        {
            "authors": [
                "C.J. Darwin"
            ],
            "title": "Pitch and auditory grouping",
            "venue": "Pitch: Neural Coding and Perception. New York, NY, USA: Springer, 2005, pp. 278\u2013305.",
            "year": 2005
        },
        {
            "authors": [
                "N. Grimault",
                "S.P. Bacon",
                "C. Micheyl"
            ],
            "title": "Auditory stream segregation on the basis of amplitude-modulation rate",
            "venue": "J. Acoust. Soc. Amer., vol. 111, no. 3, pp. 1340\u20131348, 2002.",
            "year": 2002
        },
        {
            "authors": [
                "C.J. Darwin",
                "V. Ciocca"
            ],
            "title": "Grouping in pitch perception: Effects of onset asynchrony and ear of presentation of a mistuned component",
            "venue": "J. Acoust. Soc. Amer., vol. 91, no. 6, pp. 3381\u20133390, 1992.",
            "year": 1992
        },
        {
            "authors": [
                "A.S. Bregman",
                "S. Pinker"
            ],
            "title": "Auditory streaming and the building of timbre",
            "venue": "Can. J. Psychol., vol. 32, no. 1, pp. 19\u201331, 1978. Authorized licensed use limited to: Technische Universitaet Muenchen. Downloaded on June 17,2022 at 13:59:27 UTC from IEEE Xplore. Restrictions apply. LI et al.: GESTALT PRINCIPLES EMERGE WHEN LEARNING UNIVERSAL SOUND 1891",
            "year": 1978
        },
        {
            "authors": [
                "J.H. Lee",
                "L.E. Humes"
            ],
            "title": "Effect of fundamental-frequency and sentenceonset differences on speech-identification performance of young and older adults in a competing-talker background",
            "venue": "J. Acoust. Soc. Amer., vol. 132, no. 3, pp. 1700\u20131717, 2012.",
            "year": 2012
        },
        {
            "authors": [
                "A.S. Bregman",
                "J. Abramson",
                "P. Doehring",
                "C.J. Darwin"
            ],
            "title": "Spectral integration based on common amplitude modulation",
            "venue": "Percep. Psychophys., vol. 37, no. 5, pp. 483\u2013493, 1985.",
            "year": 1985
        },
        {
            "authors": [
                "R.P. Carlyon"
            ],
            "title": "The psychophysics of concurrent sound segregation",
            "venue": "Philos. Trans. Roy. Soc. London. Ser. B: Biol. Sci., vol. 336, no. 1278, pp. 347\u2013355, Jun. 1992.",
            "year": 1992
        },
        {
            "authors": [
                "R.P. Carlyon"
            ],
            "title": "Discriminating between coherent and incoherent frequency modulation of complex tones",
            "venue": "J. Acoust. Soc. Amer., vol. 89, no. 1, pp. 329\u2013340, Jan. 1991.",
            "year": 1991
        },
        {
            "authors": [
                "M.H. Chalikia",
                "A.S. Bregman"
            ],
            "title": "The perceptual segregation of simultaneous vowels with harmonic, shifted, or random components",
            "venue": "Percep. Psychophys., vol. 53, no. 2, pp. 125\u2013133, 1993.",
            "year": 1993
        },
        {
            "authors": [
                "N. Itatani",
                "G.M. Klump"
            ],
            "title": "Animal models for auditory streaming",
            "venue": "Philos. Trans. Roy. Soc. B: Biol. Sci., vol. 372, no. 1714, 2017, Art. no. 20160112.",
            "year": 2017
        },
        {
            "authors": [
                "L.P.A.S. van Noorden"
            ],
            "title": "Temporal coherence in the perception of tone sequences",
            "venue": "Ph.D. dissertation, Inst. Perception Res., Technische Hogeschool Eindhoven, Eindhoven, 1975.",
            "year": 1975
        },
        {
            "authors": [
                "P.G. Singh"
            ],
            "title": "Perceptual organization of complex-tone sequences: A tradeoff between pitch and timbre",
            "venue": "J. Acoust. Soc. Amer., vol. 82, no. 3, pp. 886\u2013899, 1987.",
            "year": 1987
        },
        {
            "authors": [
                "R. Cusack",
                "B. Roberts"
            ],
            "title": "Effects of differences in timbre on sequential grouping",
            "venue": "Percep. Psychophys., vol. 62, no. 5, pp. 1112\u20131120, 2000.",
            "year": 2000
        },
        {
            "authors": [
                "D.L. Wessel"
            ],
            "title": "Timbre space as a musical control structure",
            "venue": "Comput. Music J., vol. 3, no. 2, pp. 45\u201352, 1979.",
            "year": 1979
        },
        {
            "authors": [
                "I. Kavalerov"
            ],
            "title": "Universal sound separation",
            "venue": "Proc. IEEE Workshop Appl. Signal Process. Audio Acoust., 2019, pp. 175\u2013179.",
            "year": 2019
        },
        {
            "authors": [
                "H. Li",
                "K. Chen",
                "B.U. Seeber"
            ],
            "title": "ConvTasNet-based anomalous noise separation for intelligent noise monitoring",
            "venue": "Proc. INTERNOISE, 2021, pp. 2044\u20132051.",
            "year": 2021
        },
        {
            "authors": [
                "V. Panayotov",
                "G. Chen",
                "D. Povey",
                "S. Khudanpur"
            ],
            "title": "Librispeech: An ASR corpus based on public domain audio books",
            "venue": "Proc. IEEE Int. Conf. Acoust., Speech, Signal Process., 2015, pp. 5206\u20135210.",
            "year": 2015
        },
        {
            "authors": [
                "D. Snyder",
                "G. Chen",
                "D. Povey"
            ],
            "title": "MUSAN: A music, speech, and noise corpus",
            "venue": "2015, arXiv:1510.08484.",
            "year": 2015
        },
        {
            "authors": [
                "J.L. Roux",
                "S. Wisdom",
                "H. Erdogan",
                "J.R. Hershey"
            ],
            "title": "SDR - Half-baked or well done",
            "venue": "Proc. IEEE Int. Conf. Acoust., Speech, Signal Process., 2019, pp. 626\u2013630.",
            "year": 2019
        },
        {
            "authors": [
                "D. Yu",
                "M. Kolbaek",
                "Z.H. Tan",
                "J. Jensen"
            ],
            "title": "Permutation invariant training of deep models for speaker-independent multi-talker speech separation",
            "venue": "Proc. IEEE Int. Conf. Acoust., Speech, Signal Process., 2017, pp. 241\u2013245.",
            "year": 2017
        },
        {
            "authors": [
                "M. Pariente"
            ],
            "title": "Asteroid: The pytorch-based audio source separation toolkit for researchers",
            "venue": "Proc. INTERSPEECH, 2020, pp. 2637\u20132641.",
            "year": 2020
        },
        {
            "authors": [
                "A. Van Den Oord"
            ],
            "title": "WaveNet: A generative model for raw audio",
            "venue": "2016, arXiv:1609.03499.",
            "year": 2016
        },
        {
            "authors": [
                "G. Wichern"
            ],
            "title": "WHAM!: Extending speech separation to noisy environments",
            "venue": "Proc. INTERSPEECH, 2019, pp. 1368\u20131372.",
            "year": 2019
        },
        {
            "authors": [
                "A.J. Oxenham"
            ],
            "title": "Pitch perception and auditory stream segregation: Implications for hearing loss and cochlear implants",
            "venue": "Trends Amplification, vol. 12, no. 4, pp. 316\u2013331, 2008.",
            "year": 2008
        },
        {
            "authors": [
                "A.S. Bregman",
                "G.L. Dannenbring"
            ],
            "title": "The effect of continuity on auditory stream segregation",
            "venue": "Percep. Psychophys., vol. 13, no. 2, pp. 308\u2013312, 1973.",
            "year": 1973
        },
        {
            "authors": [
                "R.P. Carlyon",
                "H.E. Gockel"
            ],
            "title": "Effects of harmonicity and regularity on the perception of sound sources",
            "venue": "Auditory Perception of Sound Sources, Boston, MA, USA: Springer, 2008, pp. 191\u2013213.",
            "year": 2008
        },
        {
            "authors": [
                "N. Jacoby",
                "E.A. Undurraga",
                "M.J. McPherson",
                "J. Vald\u00e9s",
                "T. Ossand\u00f3n",
                "J.H. McDermott"
            ],
            "title": "Universal and non-universal features of musical pitch perception revealed by singing",
            "venue": "Curr. Biol., vol. 29, no. 19, pp. 3229\u20133243, Oct. 2019.",
            "year": 2019
        },
        {
            "authors": [
                "E. Terhardt"
            ],
            "title": "Calculating virtual pitch",
            "venue": "Hear. Res., vol. 1, pp. 155\u2013182, 1979.",
            "year": 1979
        },
        {
            "authors": [
                "R. Plomp",
                "W.J.M. Levelt"
            ],
            "title": "Tonal consonance and critical bandwidth",
            "venue": "J. Acoust. Soc. Amer., vol. 38, no. 4, pp. 548\u2013560, Oct. 1965.",
            "year": 1965
        },
        {
            "authors": [
                "W.A. Sethares"
            ],
            "title": "Local consonance and the relationship between timbre and scale",
            "venue": "J. Acoust. Soc. Amer., vol. 94, no. 3, pp. 1218\u20131228, Sep. 1993.",
            "year": 1993
        },
        {
            "authors": [
                "G.L. Dannenbring",
                "A.S. Bregman"
            ],
            "title": "Streaming vs. fusion of sinusoidal components of complex tones",
            "venue": "Percep. Psychophys., vol. 24, no. 4, pp. 369\u2013376, 1978.",
            "year": 1978
        },
        {
            "authors": [
                "S. Popham",
                "D. Boebinger",
                "D.P.W. Ellis",
                "H. Kawahara",
                "J.H. McDermott"
            ],
            "title": "Inharmonic speech reveals the role of harmonicity in the cocktail party problem",
            "venue": "Nature Commun., vol. 9, no. 2122, pp. 1\u201313, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "J.H. McDermott",
                "D.P.W. Ellis",
                "H. Kawahara"
            ],
            "title": "Inharmonic speech: A tool for the study of speech perception and separation",
            "venue": "Proc. SAPA- SCALE, 2012, pp. 114\u2013117.",
            "year": 2012
        },
        {
            "authors": [
                "H. Kawahara",
                "M. Morise"
            ],
            "title": "Technical foundations of TANDEM- STRAIGHT, a speech analysis, modification and synthesis framework",
            "venue": "SADHANA, vol. 36, no. 5, pp. 713\u2013727, 2011.",
            "year": 2011
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014Gestalt principles, separation mechanisms, universal source separation.\nI. INTRODUCTION\nIN OUR daily lives, auditory scenes with multiple soundsources are ubiquitous. One of the most remarkable abilities of the human auditory system is to separate and track one source from complex scenes seemingly without effort. According to the seminal book of Bregman [1], auditory scene analysis (ASA) is based on two mechanisms, primitive and schema-driven grouping. The primitive grouping mechanism relies on intrinsic sound\nManuscript received June 25, 2021; revised December 25, 2021 and April 12, 2022; accepted May 2, 2022. Date of publication May 27, 2022; date of current version June 6, 2022. This work was supported by the TUM AIP through a 2-year Ph.D. scholarship for Han Li by the China Scholarship Council. The computer infrastructure was supported by the Bernstein Center for Computational Neuroscience, under Grant BMBF 01 GQ 1004B, and the Titan V card used was donated by the NVIDIA Corporation. The associate editor coordinating the review of this manuscript and approving it for publication was Prof. Jun Du. (Corresponding author: Han Li.)\nHan Li is with the School of Marine Science and Technology, Northwestern Polytechnical University, Xi\u2019an 710072, China, and also with the Audio Information Processing group, Department of Electrical and Computer Engineering, Technische Universit\u00e4t M\u00fcnchen, 80333 Munich, Germany (e-mail: lihan@mail.nwpu.edu.cn).\nKean Chen is with the School of Marine Science and Technology, Northwestern Polytechnical University, Xi\u2019an 710072, China (e-mail: kachen@nwpu.edu.cn).\nBernhard U. Seeber is with the Audio Information Processing group, Department of Electrical and Computer Engineering, Technische Universit\u00e4t M\u00fcnchen, 80333 Munich, Germany (e-mail: seeber@tum.de).\nDigital Object Identifier 10.1109/TASLP.2022.3178233\nattributes (or cues) such as fundamental frequency, onset, loudness, etc., and is regarded as an innate, bottom-up process for simultaneous grouping and for binding components over time. Components in mixtures are separated and arranged into streams according to the Gestalt principles, including the principle of proximity, similarity, continuation, and common fate. On the other hand, the schema-driven mechanism represents top-down processing. Listeners exploit learned knowledge and attention to the further processing of complex auditory scenes.\nComputational auditory scene analysis (CASA) models are technical source separation systems based on human auditory segregation principles. Many CASA systems model auditory scene analysis as a two-stage process: segmentation of timefrequency elements and grouping into auditory objects and streams [2]. Segmentation relies on the estimation of intrinsic, bottom-up sound attributes, such as pitch (e.g., Wang-Brown, 1999 [3]), amplitude modulation (Hu-Wang, 2004 [4]), or onsets (Hu-Wang, 2007 [5]). Then, according to continuity, synchrony, or other primitive grouping principles, these time-frequency segments are next grouped simultaneously across frequency and sequentially across time and frequency to produce auditory objects. Although these typical CASA models attempt to extract meaningful and biologically plausible cues, the accuracy of these cues, such as pitch or onset estimation in complex acoustic conditions, cannot be guaranteed, resulting in limited model performance.\nWith the development of neural networks, the approach to explicitly extract features has been gradually weakened. Also, the biological rational for modeling the process of auditory scene analysis is often no longer the main concern, but the improvement of separation performance for technical applications. Since these approaches do not mimic the auditory system\u2019s operation, they are not CASA models, but rather acoustic source separation or acoustic scene analysis approaches. Various supervised networks have been used in source separation and made great progress, especially for speech separation [6]\u2013[8], such as convolutional neural networks (CNNs), recurrent neural networks (RNNs), and generative adversarial networks (GANs). Recently proposed end-to-end time-domain speech separation systems, such as Conv-TasNet [9] and DPRNN-TasNet [10], even surpassed the performance of ideal time-frequency masks.\nAlthough performance has improved when using deep learning approaches, the underlying separation mechanisms of the network are more obscure. It is still unknown whether the separation is based on general primitive grouping principles, like in human hearing, or the pattern modeling of specific sound\n2329-9290 \u00a9 2022 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information.\nAuthorized licensed use limited to: Technische Universitaet Muenchen. Downloaded on June 17,2022 at 13:59:27 UTC from IEEE Xplore. Restrictions apply.\nsources. If the network separates sounds based on general Gestalt principles like human hearing, this could be instructive for developing generalizing networks that do not depend on specific sound sources. In addition, it helps to explain the \u201cblack box\u201d of deep networks, which is also an important issue that hinders the development of deep learning.\nFew attempts have been made to interpret networks in biological terms. Francl and McDermott [11] demonstrated that trained networks can replicate key properties of mammalian spatial hearing, such as the sensitivity to monaural spectral cues and interaural time and level differences. Elhilali et al. [12] trained a hierarchical inference model to mimic the human auditory system for separation, demonstrating that some grouping principles are effective in this hierarchical inference model, such as harmonicity or frequency separation. However, due to the unsupervised learning, the model was not optimized for separation tasks, the accuracy for separation of actual complex sounds was not clear.\nTo our knowledge, except for our previous attempt to test some separation principles learned by the network [13], [14], there is no evidence that a supervised deep network for source separation can learn the Gestalt principles spontaneously like human hearing. In this study, we focus on the separation of two arbitrary sources in a monaural mixture consisting of a wide variety of sounds (speech, environmental sounds, music). A state-of-the-art end-to-end time-domain source separation framework, ConvTasNet, is adapted for separating arbitrary sources in Section III. In Section IV, the separation mechanisms are then explored through a series of classic auditory segregation experiments to test Gestalt principles. The experiments demonstrate, to our knowledge for the first time, that the Gestalt principles are intrinsically learned with supervised deep learning from unrelated natural sounds \u2013 a process and network that does not directly imitate the biological processing stages of the auditory system. The approach paves the way to establishing a universal separation network that can adapt to all scenes and achieve a segregation performance like in human hearing."
        },
        {
            "heading": "II. AUDITORY GROUPING PRINCIPLES",
            "text": "The \u2018Gestalt\u2019 concept originated in the 1920s [15] to explain visual object perception and was later extended to the auditory domain, c.f. the review by Bregman [1]. Combined with more psychological and physiological experiments, the Gestalt principles evolved into more specific principles for auditory scene analysis, such as harmonicity and onset synchrony. In the following, these specific Gestalt principles are introduced for simultaneous and sequential grouping.\nFor auditory perception, scene analysis can be divided into simultaneous and sequential organization, which depicts the processes for fusing and separating components across frequency and across time into one or multiple auditory objects, or sources. For simultaneous organization, there is a consensus that harmonicity and onset synchrony are the most important principles for segregating concurrent sounds in the auditory system [16]. In addition, common fate is also an important principle addressing the dynamic changes of concurrent sounds.\nHarmonicity is a strong, common spectral regularity in natural sounds, especially in speech and music. It refers to the situation that frequencies of components are integer multiples of a common fundamental frequency (F0), which typically results from one single source. A wide range of psychoacoustic experiments on harmonicity for segregation and pitch perception has been conducted [17], showing that F0 differences aid concurrent sound segregation. Taking the identification of two sounds with different F0 as an example, experiments with double-vowels [18] and with orchestral instruments [19] show consistently that identification performance improves gradually as the F0 difference increases to two semitones and then asymptotes for further F0 difference increases [20].\nIf different frequency components change in the same way at the same time, they probably arise from the same source [1], [21] - the principle of \u201ccommon fate\u201d proposed by Gestalt psychologists [15]. Common fate in auditory scene analysis can be defined in terms of correlated changes in their amplitudes (amplitude modulation, AM) and their frequencies (frequency modulation, FM). AM refers to slow temporal fluctuations of the sound\u2019s intensity. Synchrony of the onset (common onset) is a special and critical example of AM, which has been shown to be one of the most powerful temporal principle for simultaneous component grouping [22]. When components share a common onset, it is likely that they have originated from the same source. On the contrary, components that start at sufficiently different times tend to be heard as separated sources. An onset asynchrony of about 30\u201350 ms is enough for affecting auditory grouping of pure tones [23] or the identification of double-vowels [24]. This principle is taken out from the common fate principle separately for detailed analysis.\nThe common fate principle here refers to coherent modulation in amplitude (AM) and frequency (FM). The role of AM for simultaneous grouping is common and useful; the modulation in speech caused by the opening and closing of the vocal cords contributes to the fusion of acoustic components [25]. When two tones are amplitude modulated by the same rate, they tended to be fused more strongly than when modulated with different rates. Moreover, components from the same source often share a common pattern of frequency modulation [26], which is thought to also contribute to fusion. Small fluctuations in frequency are common in speech and music instrument sounds, ranging from less than 1 percent to 10 percent of the carrier frequency, which is called \u201cmicromodulation\u201d [1], [27], [28]. The micromodulation affects all frequency components that stem from one source, causing them to move in parallel and group into one coherent object.\nThe sequential organization is the process that assigns auditory time-frequency elements arriving sequentially over time to appropriate sources, which is often regarded as auditory streaming for the human auditory system [1]. For components in sequences, proximity, similarity, and continuity of their attributes are the most important principles for their sequential organization [29].\nProximity is the most intuitive and widely investigated Gestalt principle. It plays an essential role in auditory scene analysis, which refers to the proximity in frequency, time, loudness,\nAuthorized licensed use limited to: Technische Universitaet Muenchen. Downloaded on June 17,2022 at 13:59:27 UTC from IEEE Xplore. Restrictions apply.\nand other source attributes. In 1975, van Noorden [30] proposed the well-known temporal coherence boundary for auditory streaming based on frequency and temporal proximity. It shows that if the frequency and temporal distance between successive frequency components is large, they are more likely assigned to two streams by the auditory system.\nThe law of good continuation is a further Gestalt principle, which refers to the acoustic properties of components that are continuous or with a smooth transition, such as frequency or loudness. Any sequence that exhibits acoustic contiguity has probably come from one source. Abrupt changes in these attributes often mean the emergence of new sound sources.\nThe principle of similarity usually refers to a multidimensional sound attribute, timbre. Timbre is a complex auditory attribute relating to the spectro-temporal composition of stimuli that otherwise do not differ in pitch and loudness. It has been demonstrated that timbre dissimilarity can serve segregation [31]\u2013[33].\nAfter training the network, a series of experiments are conducted in Section IV to explore whether these specific Gestalt principles have been learned by the network."
        },
        {
            "heading": "III. SEPARATION NETWORK MODEL",
            "text": ""
        },
        {
            "heading": "A. Framework",
            "text": "With the development of deep learning, many source separation networks have made significant progress, especially for speech separation. However, few attempts have been made to separate arbitrary sources in monaural recordings [34], [35]. In this study, one end-to-end fully convolutional time-domain separation network (ConvTasNet) proposed by Luo et al. [9] is adapted to separate universal sound sources.\nConvTasNet follows the unified separation framework: encoder-separator-decoder. First, an encoder transforms the mixture waveform into intermediate representations by convolving with the framed mixture x with N encoding filters {hEncn (t)}n=0,...,N\u22121 of length L:\nX(k, n) = L\u22121\u2211\nt=0\nx(t+ kH)hEncn (t), (1)\nwhere k \u2208 {0, . . . ,K \u2212 1}is the frame index of the waveform and H is the hop size. In this study, the encoder is freely learned through the training process by a 1-D convolutional layer. A rectified linear unit (RELU) layer is next applied to obtain nonnegative X+(k, n) for the following separator.\nThe separator is used to estimate weighting functions (masks) for two sources through the time-dilated convolutional network (TDCN). It is chosen as three 1-D convolutional modules, and each module contains eight stacked 1-D convolutional blocks with different dilation factors. Other parameters are the same as the best non-causal model reported by Luo et al. [9]. The mask for the i-th source (Mi(k, n)) is then multiplied with the mixture:\nYi(k, n) = X +(k, n) Mi(k, n), (2)\nwhere indicates point-wise multiplication. Reconstructed waveforms are calculated by transposed convolution with N decoding filters {hDecn (t)}n=0,...,N\u22121 and overlap-add operation:\ns\u0302i(t) = K\u22121\u2211\nk=0\nN\u22121\u2211\nn=0\nYi(k, n)h Dec n (t\u2212 kH). (3)"
        },
        {
            "heading": "B. Dataset",
            "text": "In this study, we attempt to train a universal network to adapt to various acoustic scenes. Therefore, we used a universal dataset to train ConvTasNet, including environmental sounds (e.g., vehicle noise, bells, animal calls, etc.) from the BBC sound effects dataset [36], speech from the LibriSpeech database [37], and music without vocals from the musan database [38]. For data pre-processing, files only with background noise or with multiple overlapping sounds were excluded. All segments were downsampled to 16 kHz and cut to 3 s length. Environmental sounds, speech, and music had the same proportion in the dataset.\nTo create mixtures, two source clips were chosen randomly and mixed with random signal-to-noise ratios (SNRs) between \u22125 dB and +5 dB. To avoid confusion, mixing from the same sound source was not allowed, such as the same speaker, the same music track, or the same class (e.g., cars) in the environmental sound. Overall, the dataset included 180000 clips (150 hours), of which 70% were randomly selected for training (105 hours), 20% for cross-validation (30 hours), and 10% for testing (15 hours)."
        },
        {
            "heading": "C. Training and Evaluation Setup",
            "text": "The scale-invariant source-to-distortion ratio (SI-SDR) [39] is used as an objective training target and measure of separation accuracy. It directly calculates the fidelity in the time domain by comparing the given true source s and the estimated source s\u0302 , which can be expressed as\nSI\u2212 SDR(s, s\u0302) = 10 log10 \u2016\u03b1s\u20162\n\u2016\u03b1s\u2212 s\u0302\u20162 , (4)\nwhere\u03b1 = < s, s\u0302 >/\u2016s\u20162 , and<> indicates the inner product. SI-SDR improvement (SI-SDRi) is the difference between output SI-SDR and input SI-SDR, where the network output signal and input mixture signal are regarded as s\u0302 to calculate output SI-SDR and input SI-SDR through (4), respectively.\nPermutation invariant training (PIT) [40] is adopted to address the source label permutation problem, which aligns the network output and the given true source during training. All possible assignments between estimated and clean sources (s\u03021 \u223c s1, s\u03021 \u223c s2, s\u03022 \u223c s1, s\u03022 \u223c s2) are listed. Then the SI-SDR is calculated for each assignment to get the pairwise scores. The maximum score for different assignments is chosen as the training objective.\nAll experiments are implemented with the Asteroid toolkit [41] and are trained through the Adam optimizer for 100 epochs.\nAuthorized licensed use limited to: Technische Universitaet Muenchen. Downloaded on June 17,2022 at 13:59:27 UTC from IEEE Xplore. Restrictions apply."
        },
        {
            "heading": "D. Results",
            "text": "The separation performance for the test dataset is shown in Table I. As mentioned before, our model is used for a universal dataset, which includes speech, environmental sounds, and music. In the following, not only the average results but also the results for different source types are presented.\nThe encoder in ConvTasNet is framewise, where the kernel size (also called window size) in the 1-D convolutional layer controls the frame rate, which determines the context viewed by the network and is an important parameter for separation performance. The results of the ideal ratio mask (IRM) are calculated for comparison. IRM indicates the ratio of the target source energy to mixture energy in spectro-temporal units. It is one commonly used indicator of the dataset difficulty. Because the IRM is based on the spectrogram, the STFT is also calculated with different window sizes for comparison.\nFor ConvTasNet, the best average SI-SDRi is 11.70 dB and obtained when the window size is 2 ms. Compared with the IRM, the network shows promising results. In general, the performance is different for different source types. The separation of speech outperforms others, where speech and music separation is 14.28 dB, followed by speech and speech separation (13.45 dB), then speech and environmental sounds separation (13.41 dB). They are comparable or even surpass corresponding best IRMs. This may be due to the unique harmonic structure of speech which is more easily learned by the network. In contrast to speech separation, the performance for other mixture types is not satisfactory, where separation of music from music is the worst. A piece of music in the dataset is not played by only one instrument. It includes many tracks with various instruments, such as piano, drums, bass et al. When different music pieces are mixed, especially for similar music genres, it will be hard to separate all different instruments into the respective source signal mixtures. The IRM results of music and music separation are also the worst among other mixtures, indicating greater difficulty. Different types of sound sources have their own unique characteristics and may affect the learning of principles, which could be interesting for future study.\nThe performance of ConvTasNet decreases as window size increases, which is opposite to the trend of the IRM. The\nintroduction of dilated convolution with increasing dilation factors in the network ensures that neurons in the highest layer of ConvTasNet can be affected by a long enough context [42]. A smaller window size allows the network to have a higher temporal resolution for each frame, which improves separation performance. The effect of window size on performance varies with the type of source. The separation of speech and other sources is sensitive to the window size, while environmental sounds and music are less so.\nIn addition to the average values in Table I, scatter plots are provided to see the distribution of model separation results. Fig. 1 shows scatter plots of input SI-SDR and SI-SDRi (dB) of the results of ConvTasNet with 2 ms window size for mixtures from different source types in the test dataset. The color scale is the density estimated by Gaussian kernel density estimation. The results generally show a downward trend, indicating that for lower input SI-SDR it is easier to obtain a larger SI-SDRi, which was previously observed [43]. In addition, the distribution of speech and other sources separation is more compact with a higher cluster center and fewer failure cases, while it is less concentrated for separations of environment sounds and music.\nTaking the separation of speech and dog barking as an example, Fig. 2 shows the spectrogram of the mixture, both sources, and the two separated outputs of the network. The SI-SDRi for both sources are 12.11 dB and 9.81 dB, respectively. It can be seen from the figure that except for some instantaneous components, these two sources are well separated and reconstructed."
        },
        {
            "heading": "IV. SEPARATION MECHANISMS FOR SIMULTANEOUS AND SEQUENTIAL ORGANIZATION",
            "text": "The trained ConvTasNet model achieved good performance for universal source separation. But what are the underlying separation mechanisms? Is separation based on modeled patterns of specific sound sources or on generalized primitive grouping principles?\nIn perception, auditory scene analysis can be divided into simultaneous and sequential organization. Simultaneous organization forms an object from concurrent components across frequency, while sequential organization links components across time. For the auditory system, the main grouping principles\nAuthorized licensed use limited to: Technische Universitaet Muenchen. Downloaded on June 17,2022 at 13:59:27 UTC from IEEE Xplore. Restrictions apply.\nfor simultaneous organization are harmonicity, onset synchrony, and common fate (AM and FM).\nThe proximity in frequency and time, the similarity in timbre, and continuity are predominant in the sequential organization. In this paper, we chose a set of classic segregation experiments probing simultaneous and sequential organization to test the model\u2019s functioning."
        },
        {
            "heading": "A. Methods and Stimuli",
            "text": "The Gestalt principles are often investigated one by one through artificial stimuli composed of discrete frequency components in most research on ASA [1]. We follow this approach\nwith a series of experiments from classic ASA literature using two sound stimuli summed in the one input channel of the trained network.\nBecause the network is trained with a universal sound source dataset and has been verified that the network is capable of separating complex natural sources. Artificial stimuli that the network has not seen before are used to test the network\u2019s underlying mechanisms. These artificial stimuli completely differ in their kind and their spectral and temporal composition from the training dataset. Only when networks generalize segregation principles, the separation performance of untrained artificial stimuli may follow that of human auditory scene analysis.\nAll experiments are conducted on the best model (ConvTasNet with 2 ms window size) trained by the universal dataset from Section III-B without any other adjustment. The network separation result (SI-SDRi) is used as a performance indicator to analyze the principle\u2019s effect.\nTwo types of stimuli are used in the simultaneous and sequential organization experiments, respectively. The stimuli used for probing simultaneous organization are two harmonic complexes with different fundamental frequencies (F0) with or without common onset, as shown in Fig. 3(a). The duration of the stimulus is 3 s, including 200 ms raised-cosine onset and offset ramps to reduce transient effects. Each component in the mixture has equal amplitude, which means that the SNR of the two sources is equal to 0. There are three harmonics in one source, and F01 is fixed at 110 Hz. Here, four simultaneous organization experiments are conducted to test harmonicity, onset synchrony, and common fate (AM and FM), respectively.\nFor experiment 1, the stimuli are two groups of harmonics with different F0 and with common onset. F0 differences (\u0394F0s) are varied from 0 to 12 semitones in steps of 0.1 semitones. The semitone scale is adopted in this paper because it is commonly used in psychoacoustic experiments [44], and the perceived pitch of complex tones is generally proportional to the logarithm of the frequency. Each semitone is one-twelfth of an octave, and an\nAuthorized licensed use limited to: Technische Universitaet Muenchen. Downloaded on June 17,2022 at 13:59:27 UTC from IEEE Xplore. Restrictions apply.\noctave is an interval between one tone and another with double its frequency. When f2 is one semitone higher than f1 , f2 = f1 \u00d7 21/12.\nThen we introduce onset asynchrony into the harmonicity experiment to study the contribution of onset asynchrony to segregation. For experiment 2, \u0394onset is varied from 0 s to 1.5 s (half of the duration of source 1) in steps of 0.01 s, where \u0394onset indicates the delay of source 2 from source 1.\nFinally, we conducted two experiments to test whether the introduction of AM (experiment 3) and FM (experiment 4) with different modulation depths and rates contribute to separation. The parameters (\u0394F0 and \u0394onset) that have been tested in the above experiments are fixed, and a case with partial segregation based on harmonicity is chosen here (\u0394F0 = 1.5 semitones, and \u0394onset = 0 s). Source 1 is unmodulated as before and all three components in source 2 are sinusoidally modulated in amplitude or in frequency.\nIn the AM experiment, the modulation depth is varied from 0 to 100% in steps of 2%, and the modulation rate is changed from 0 to 5 Hz in steps of 0.1 Hz. In the FM experiment, because micromodulation is the common pattern of frequency modulation in natural sounds and can be perceived by the auditory system, the modulation depth here is varied from 0 to 10% in steps of 0.2%. The modulation rate is varied from 0 to 5 Hz in steps of 0.1 Hz.\nFor sequential organization experiments, a classic stimulus paradigm proposed by van Noorden [30] for auditory streaming is used, including two alternating components A and B with different frequency and tone repetition time (TRT(ms), the onset to onset time for two adjacent tones), as shown in Fig. 3(b). A and B are 40 ms in duration, including 5 ms raised-cosine onset and offset ramps to reduce transient effects. Each sequence consists of 10 A-B components in total. A and B can be pure tones or\nharmonic complex tones. In the following, three experiments are conducted to investigate whether proximity, continuity, and similarity principles emerge through model training.\nFor the proximity experiment, the proximity in frequency and time is investigated. A and B are pure tones. B-tones are fixed at 1 kHz, and A-tones are varied from 0 to 15 semitones below B tones in steps of 0.2 semitones. TRT varies from 50 to 200 ms in steps of 2 ms. These parameters are replicated from the classic psychoacoustic experiment of van Noorden [30] to compare the model\u2019s behavior with human performance.\nFor the continuity experiment, we introduce smooth frequency transitions between successive tones on the basis of the proximity experiment, which changes \u201cdiscrete\u201d tones A and B to be \u201cconnected\u201d. The samples in frequency transitions are generated through a logarithmic swept-frequency cosine signal [45], where the start and end frequency are frequency of tone A and B, respectively.\nFor similarity in the timbre experiment, due to the multidimensional nature of timbre, it is difficult to quantify the similarity in timbre space since the relationship of components in amplitude, temporal, and spectral spacing contribute. Here, the timbre differences stem from using different sets of three adjacent harmonics. Two alternating sources A and B with three harmonics to the same fundamental frequency (F0 = 110 Hz) and with equal amplitude are presented which provide the same pitch. A set of 10 timbres (T1\u2013T10) with different harmonic numbers are created, where T1 uses harmonics 1, 2, and 3, T2 uses harmonics 2, 3, and 4, and T10 uses harmonics 10, 11, and 12. The harmonics have a duration of 250 ms with 20 ms raised-cosine onsets and offsets ramps and the TRT is 350 ms."
        },
        {
            "heading": "B. Results of Simultaneous Organization Experiments",
            "text": "1) Harmonicity: For the first experiment probing segregation of simultaneous harmonic complex tones by differences in fundamental frequency, results are shown in Fig. 4 as SI-SDRi (dB) as a function of F0 differences, where\u0394F0 (F02\u2013F01, semitones) is marked on the bottom x-axis, and the ratio of F02 to F01 is marked on the upper x-axis. When \u0394F0 < 2 semitones, the network separation performance increases as\u0394F0 increases. When\nAuthorized licensed use limited to: Technische Universitaet Muenchen. Downloaded on June 17,2022 at 13:59:27 UTC from IEEE Xplore. Restrictions apply.\n\u0394F0 is at 2\u201310 semitones, as \u0394F0 increases, the separation performance fluctuates with one semitone period. When \u0394F0 is at 10\u201312 semitones, the first harmonic of F02 is close enough to the second harmonic of F01, and it tends to be perceived as a unitary source again.\nThe symmetric increase and decrease at 0\u20132 and 10\u201312 semitones result in the same conclusion that \u0394F0 contributes to the separation of the sound source, which is consistent with the auditory system [46]. Within a certain range, the increase of \u0394F0 significantly promotes segregation, such as the 0\u20132 semitones range for double-vowel recognition [18], and fewer further improvements beyond this range can be obtained.\nWhen \u0394F0 exceeds that range, that is, for 2\u201310 semitones difference, the difficulties to separate two sources when \u0394F0 is an integer multiple of semitones are also evidence for the role of harmonicity for component grouping. In these cases, all the components from sources 1 and 2 are integer multiples of semitones, they are more harmonic, resulting in grouping into one source. It is evidence for supporting that the semitone is the smallest interval commonly used in Western tonal music [47]. If tones are not in the semitone scale, they tend to be segregated and perceived as dissonant and unpleasant. Further, the result is consistent with the theory of musical consonance [48]\u2013[50], which is an auditory perceptual phenomenon that simple frequency ratios between two tones give much higher consonance than other ratios. The most consonant intervals are with ratios 1:2 (F01 = 110 Hz, F02 = 220 Hz, \u0394F0 = 12 semitones), 2:3 (F01 = 110 Hz, F02 = 165 Hz, \u0394F0 = 7 semitones), 3:4 (F01 = 110 Hz, F02 = 146.8 Hz, \u0394F0 = 5 semitones), 4:5 (F01 = 110 Hz, F02 = 138.6 Hz, \u0394F0 = 4 semitones), which correspond to the local minima of SI-SDRi in Fig. 4 that are more difficult to separate.\nThe input and outputs of the network for two stimuli with different a) \u0394F0 = 7 semitones and b) 7.7 semitones are shown in Fig. 5 to explain how the network exploits the harmonic constraints to assign these six frequency components to one or two sources. The worst separation performance in 2\u201310 semitones is reached for \u0394F0 = 7 semitones, a fifth in music and a highly consonant tone combination widely used in music (\u201cpower chord\u201d). As shown in Fig. 5(a), the 3rd harmonic of source 1 is identical to the 2nd harmonic of source 2, where F01 = 110 Hz and F02 = 165 Hz. All components with the most energy in the mixture are assigned to one estimate. The energy in the other estimate is small, but some components emerge that do not exist in the stimuli, a series of harmonics of 55 Hz. It indicates that components are estimated by the network based on harmonic constraints. When the component frequencies of two sources share harmonic relationships, they are likely be regarded as the same sound source and even evoke virtual fundamental frequencies [48].\nAnother example that harmonicity is exploited to separate two sources is \u0394F0 = 7.7 semitones, as shown in Fig. 5(b). When \u0394F0 is not an integer multiple of semitones, there is no specific harmonic relationship between F0 of the two sound sources. Then, according to the harmonicity between the three components of each sound source, all components can be correctly assigned to two sources.\nIn general, for concurrent components, harmonicity is learned by the network and used effectively for separation. The harmonicity principle exploited by the network is consistent with the auditory system in the following aspects. First, within the range of 2 semitones, \u0394F0 contributes to source separation. Second, components are always assigned due to harmonic constraints. Based on whether the harmonicity is within each source or across two sources, the harmonicity has two effects on separation, beneficial or hindering. When the fundamental frequencies of the two sound sources have no harmonic relationship, that is, only components belonging to the same source are harmonic, they will be correctly separated into two sound sources due to the harmonicity within each source. When \u0394F0 is an integer multiple of semitones, all components from two sources share a harmonic relationship. At this time, the harmonicity helps to combine all components into a single source and hinders the separation of the two sound sources.\n2) Onset Synchrony: Results of SI-SDRi (dB) as a function of \u0394onset for \u0394F0 = 1.6, 7.0, 7.7 semitones are shown in Fig. 6. For \u0394F0 = 1.6 semitones, a condition which shows some segregation already for simultaneous onsets, SI-SDRi increases with the increase of \u0394onset. Onset asynchrony is segregationpromoting, consistent with the auditory system. Specifically, SI-SDRi increases rapidly when a slight delay (<100 ms) breaks the synchronization of both sources, and then increases gradually until it approaches the asymptote. Asynchrony of more than\nAuthorized licensed use limited to: Technische Universitaet Muenchen. Downloaded on June 17,2022 at 13:59:27 UTC from IEEE Xplore. Restrictions apply.\n30 ms has been shown to be helpful for auditory separation [51], and similarly for ConvTasNet, the impact of 10\u201320 ms \u0394onset is also relatively small while for \u0394onset of 30\u201350 ms a rapid increase in segregation occurs. For \u0394F0 = 7.7 semitones, a condition that is already well segregated with simultaneous onsets, the contribution of \u0394onset is limited. However, when \u0394F0 is in integer multiples of semitones (\u0394F0= 7.0 semitones), onset asynchrony-based segregation is not sufficiently powerful to overcome grouping due to harmonicity with ConvTasNet. Regardless of \u0394onset, the network tends to fuse all components into one source. In this case, harmonicity has a greater weight than onset asynchrony for the network.\nHaving seen that both harmonicity and onset synchrony contribute to segregation by the network, we are interested in their detailed interaction. Fig. 7 shows separation performance as a function of both parameters. As visible in Fig. 7(b), the introduction of \u0394onset does not change the trend between SI-SDRi and \u0394F0. The separation performance is dominated by \u0394F0, whereas onset asynchrony contributes only in cases with partial segregation based on F0, as one would expect. The\ncontribution of \u0394onset is different for different F0, and three typical cases have been analyzed in Fig. 6. The projection on the \u0394F0\u2013\u0394onset plane in Fig. 7(d) indicates that harmonicity and onset asynchrony contribute almost independently.\nIn summary, for the network, harmonicity is the overall dominant principle in inducing simultaneous segregation, but onset asynchrony also facilitates segregation, especially for conditions with partial separation by harmonicity. These two principles are processed almost independently.\n3) Common Fate: Results of experiment 3 (AM) and experiment 4 (FM) are given in Fig. 8(a) and (b), respectively. For the AM experiment, the introduction of low-rate AM into one of the two sources helps to group these three harmonic components that share the same modulation, resulting in increased separation. The thresholds of AM depth and rate that effectively promote separation are about 30% and 0.3 Hz for this experiment. Beyond this threshold, separation performance increases with the increase of modulation depth and rate. Segregation appears to peak at around 3\u20134 Hz, the syllable rate of speech, which also agrees with the maximum perceived fluctuation strength [52].\nFor the FM experiment, the introduction of micromodulation bring obvious benefits for separation. It provides new support for grouping three harmonics that share the same pattern of fluctuation in frequency in addition to the harmonicity principle. Segregation peaks at a modulation depth of 3%\u20135%. For the auditory system, it is the effective range of FM in voiced portions of speech [27].\nAuthorized licensed use limited to: Technische Universitaet Muenchen. Downloaded on June 17,2022 at 13:59:27 UTC from IEEE Xplore. Restrictions apply.\nIn general, a difference in amplitude and frequency modulation assists source separation. These phenomena are consistent with the auditory system, and it is plausible to believe that common fate is learned effectively by the network."
        },
        {
            "heading": "C. Results of Sequential Organization Experiments",
            "text": "1) Proximity: Results of the experiment with alternating tone sequences are given as SI-SDRi (dB) as a function of \u0394F and TRT in Fig. 9. When \u0394F is large and TRT is short (top left corner of Fig. 9), tone sequences A and B are more likely to be separated. On the contrary, the proximity in frequency and time will hinder model separation. It is consistent with the temporal coherence boundary presented by van Noorden [30]: when the tone interval is higher than the temporal coherence boundary (the white dashed line in Fig. 9), listeners tend to perceive two sound sources. It appears that the proximity principle in frequency and time is learned automatically by the network with a similar parametric outcome as in humans.\n2) Continuity: Results of continuity experiment are shown in Fig. 10, again as SI-SDRi (dB) as a function of \u0394F and TRT. The contribution of continuity, introduced in the experiment by linking A and B tones by logarithmic sweeps, is understood by comparison against the proximity experiment with discrete\nfrequency jumps. Compared with Fig. 9, the results in Fig. 10 share the same general trend that when \u0394F is large and TRT is short (top left corner of Fig. 10), mixtures are more likely to be separated. More importantly, regardless of the interval in the time-frequency domain, the separation performance significantly dropped after the introduction of a smooth transition. The good continuation hinders sequential segregation effectively.\nThe spectrogram of the mixture and the two sources estimated by the network for \u0394F = 5.6 semitones and TRT = 0.08 s are shown in Fig. 11 for direct comparison without (left column, proximity experiment) and with (right column, continuity experiment) continuation. Without continuation, the model entirely separates the mixture into two sources of high and low frequencies. On the contrary, mixtures are grouped into one source by the network when consecutive tones are connected with the frequency transitions.\n3) Similarity: Results for sequential segregation based on timbre are shown in Fig. 12. When source A and B are more similar in timbre, that is, harmonics of source A and B with the same F0 are in a closer frequency region, they are more likely to integrate into one stream. On the contrary, when the differences in spectral spacing are larger (upper-left and lower-right corner), they are separated into two sources by the network despite sharing the same fundamental frequency and intensity. It indicates that the timbre differences, here from differences in the spectral centroid, can be learned by the model and are used effectively for separation as an additional cue."
        },
        {
            "heading": "V. COMPETITION AND COOPERATION OF SIMULTANEOUS AND SEQUENTIAL ORGANIZATION",
            "text": "If a group of sound components can be regarded as arising from the same physical source, they should have a simultaneous or sequential relationship. As mentioned in the above sections,\nAuthorized licensed use limited to: Technische Universitaet Muenchen. Downloaded on June 17,2022 at 13:59:27 UTC from IEEE Xplore. Restrictions apply.\nthere are various factors that promote the appropriate separation of mixtures. Harmonicity, onset synchrony, and common fate (AM and FM) are predominant for the simultaneous organization. The sequential organization is affected by proximity and continuity in frequency and time and the similarity in timbre. These factors are cooperative and competitive, and the relative importance of these factors for separation is a matter of debate and likely situation-dependent.\nFor the human auditory system, the competition and cooperation of simultaneous and sequential organization was explored by Bregman and Pinker [23]. They presented one well-known organization principle, the old-plus-new heuristic. It can be stated that \u201cIf any part of a sound can be plausibly interpreted as being a continuation of an earlier sound, then it should be.\u201d, which gives priority to sequential grouping. In this section, following their classical experiment, we give an example of how different principles compete and cooperate to control the separation of mixtures."
        },
        {
            "heading": "A. Methods and Stimuli",
            "text": "As shown in Fig. 13, for alternating tones A, B, and C, the simultaneous organization tends to integrate B and C into one source, which is accompanied by a series of pure tones A. However, the sequential grouping facilitates the integration of A\nand B and destroys the integration of the complex tone B-C. It can be thought that tones A and C compete to decide which one can be combined with B. In this experiment, the calculation of SI-SDRi is based on the separation of tones A vs. complex tones B-C, where source 1 is sequence A, and source 2 is complex tones B and C. For the other possible separation pattern, that source 1 is the alternating tones A and B, while source 2 is tone C, the results are similar and will not be repeated here.\nIn our experiment, four factors and their interactions are considered to analyze the dominant factors for fusion. The frequency of B-tones is fixed at 220 Hz, and tone duration is 100 ms. A-tones separate from B in frequency by 0 to 12 semitones in steps of 1 semitone and TRT between A and B is varied from 100 ms to 300 ms in steps of 20 ms. These two parameters are used to control the proximity in frequency and time for the strength of sequential grouping. On the contrary, the harmonicity and onset synchrony between tones B and C determine simultaneous organization. C tones are varied from 10 semitones to 14 semitones below B tones in steps of 0.5 semitones, when \u0394F is \u221212 semitones means that C tones (110 Hz) and B tones (220 Hz) are harmonically related. The onset of C tones is varied from 0 to 50 ms behind B in steps of 5 ms to investigate the contribution of common onsets to fusion."
        },
        {
            "heading": "B. Results",
            "text": "The separation results as a function of the four factors are shown in Fig. 14. There is a total of 13\u221711 subgraphs, which represent the results under different \u0394F between A and B (\u0394FAB) and different conditions of TRT. These two parameters control sequential grouping. In each subgraph, SI-SDRi (dB) is given as function of \u0394F between B and C (\u0394FBC) and \u0394onset between tones B and C (\u0394onsetBC), which control simultaneous integration.\nWe start with the overall analysis of the effect of \u0394FAB and TRT for sequential grouping. For all subgraphs under different \u0394FAB and TRT, they show one generally that when \u0394FAB is larger and TRT is shorter, the separation between tones A and complex tones B-C is better. This demonstrates that the proximity principle in frequency and time for sequential separation is effective. When the tone interval in frequency and time is higher than the temporal coherence boundary, they tend to be separated.\nFig. 14 is divided into panels (a)\u2013(d) according to separation performance. For panels (a) and (b), \u0394FAB is large enough (\u22656 semitones) for sequential organization to separate tones A and B according to the proximity principle. The presence of tones C also facilitates the grouping of tones C and B and the separation of tones A from tones B. In this condition, sequential and simultaneous organization are cooperative to separate tones A from the complex of tones B-C. As shown in the spectrogram (a1) and (b1), sources 1 and 2 are correctly separated.\nFor panel (c), \u0394FAB is less than 6 semitones and the time interval is less than 200 ms (TRT \u2264 200 ms). There is fierce competition between sequential and simultaneous grouping. The sequential organization here tends to combine tones A and B because\u0394FAB<6 semitones, while tones B and C are also affected by simultaneous organization. The spectrograms of three typical\nAuthorized licensed use limited to: Technische Universitaet Muenchen. Downloaded on June 17,2022 at 13:59:27 UTC from IEEE Xplore. Restrictions apply.\ncases (c1) (c2) (c3) are shown in Fig. 14. For (c1), it is a special case that tones A and B are continuous (frequency of tones A and B is 220 Hz and TRT = 100 ms), resulting in a strong sequential grouping of tones A and B, which destroys the simultaneous grouping of tones B and C. For the other subgraphs of panel (c), if tones B and C are harmonic (\u0394FBC = \u221212 semitones) and synchronous (\u0394onsetBC = 0 ms), they will integrate into one source regardless of the proximity between tones A and B, as visible in panel (c2). In this situation, it is plausible to believe that harmonicity and synchrony for simultaneous grouping are stronger than effects of sequential grouping. With the introduction of onset asynchrony or mistuning of harmonics, the force of simultaneous grouping between B and C weakens. As shown in panel (c3), tones B combine with A in sequence again, and tones C are separated.\nFor panel (d), especially for \u0394FAB \u2264 3 semitones, regardless of the relationship between tones B and C, the separation performance is very poor. As shown in panel (d1), three tones are grouped into one source. According to the proximity principle, for large TRTs, sequential organization will force the components to integrate into one stream, i.e., the separation process is dominated by sequential organization. In summary, the analysis shows that using ConvTasNet for segregation, principles for simultaneous and sequential separation compete and cooperate in dealing with segregation of acoustic scenes similar to the\nauditory system, and the relative importance of principles for separation depends on the specific situation."
        },
        {
            "heading": "VI. GROUPING BASED ON HARMONICITY IN COMPLEX STIMULI",
            "text": "In the above sections, simple artificial stimuli commonly used in psychoacoustic experiments are adopted to illustrate that Gestalt principles have been acquired by the network. We now explore whether these segregation principles generalize to more complex stimuli, such as speech.\nHarmonicity is a prominent characteristic of the voiced parts of speech and it has been established in many psychoacoustic experiments to play a critical role in natural sound source separation [17]. In Section IV we have demonstrated the segregation of two complex tones with different F0s based on the harmonicity principle. We now explore the effectiveness of harmonicity in the separation of speech by destroying the harmonicity. McDermott et al. [53] used the STRAIGHT with sinusoidal modeling [54] to manipulate the harmonicity in speech and conducted psychoacoustic experiments to reveal the role of harmonicity for natural speech separation. The results showed that inharmonic speech was less intelligible for concurrent sentences, indicating that harmonicity contributes to auditory segregation.\nAuthorized licensed use limited to: Technische Universitaet Muenchen. Downloaded on June 17,2022 at 13:59:27 UTC from IEEE Xplore. Restrictions apply."
        },
        {
            "heading": "A. Methods and Stimuli",
            "text": "STRAIGHT [55] is a speech analysis and synthesis vocoder. For a speech utterance, STRAIGHT estimates speech parameters of voiced excitation (the time-varying F0), unvoiced excitation (the time-varying spectral parameters of aperiodic components), and vocal tract filter (the time-varying spectral envelope). Then these estimated parameters can be manipulated to synthesize more altered speech signals. Here, STRAIGHT with sinusoidal modeling is used to generate speech with inharmonic carrier components while preserving other attributes. It models the voiced excitation as a sum of sinusoids, permitting frequency components to be manipulated individually.\nIn this experiment, the random jittering manipulation to each component is adopted rather than shifting by fixed frequencies to ensure that components no longer have any spectra regularity.\nThe first 30 harmonics of sources are randomly jittered individually. The n-th inharmonic carrier component fn is generated by jittering the n-th harmonic with a random proportion of F0,\nfn = nF0 + cnF0,\nwhere cn is a random value that follows a uniform distribution between \u2212c and c. The jitter magnitude (c) is used to control the degree of inharmonicity, which is fixed as 0.1, 0.2, 0.3, 0.4, and 0.5.\nTaking one sentence spoken by a female speaker as an example, the spectrogram of the original speech sentence, synthetic harmonic speech, and synthetic inharmonic speech jittered by 0.1, 0.3, 0.5 are shown in Fig. 15(a1)\u2013(a5), respectively. It can be seen that the synthetic harmonic speech is highly similar to the original speech, demonstrating the high accuracy of STRAIGHT for analysis and synthesis. For inharmonic speech, each component is randomly shifted upwards or downwards with a random proportion of F0, while the spectrotemporal envelope that conveys the information is preserved. In quiet, it sounds like harmonic speech accompanied by some whistle or some reverberation.\nIn this experiment, 1000 speech sentences are selected from the LibriSpeech dataset [37], which are not included in the network training dataset. For each sentence, 5 jitter magnitudes (c = 0.1, 0.2, 0.3, 0.4, 0.5) are explored, and 100 random jitter patterns for each jitter magnitude are created to avoid accidental harmonicity in a random jitter pattern. To create mixtures, two synthetic sentences with the same jitter degree are selected randomly and mixed with equivalent energy. The cross mixture of different jitter degrees is not tested, such as 0.1 jitter for one sentence and 0.2 jitter for another sentence. There is a total of 1000\u00d75\u00d7100 mixture clips, and each clip is 3 s in length with a 16 kHz sample frequency."
        },
        {
            "heading": "B. Results",
            "text": "Before the analysis of experiment results, the spectrograms of one example sentence spoken by a female speaker are shown in Fig. 15. When the example sentence and another sentence spoken by a male speaker are mixed correspondingly (the same degree of inharmonicity), the spectrograms of the network estimation are shown in (b1)\u2013(b5). It is visually apparent that the example sentence is well separated and reconstructed for both original and synthetic harmonic cases in (b1) and (b2). However, when the harmonic components are perturbed by random jitters, even by 0.1 of the F0, the separated sentence is filled with other interferences and loses some necessary components. If frequency components are not subject to strict harmonic constraints, their assignment to appropriate sources is more difficult for the network and the separation performance decreases with the increase of the degree of inharmonicity.\nResults of separating two concurrent sentences with different degrees of inharmonicity are given by SI-SDRi (dB) in Fig. 16. For mixtures of 1000 speech sentences from the LibriSpeech dataset (\u2018original\u2019), the average SI-SDRi is 14.47 dB, which is comparable with the result of speech and speech separation reported in Section III. For synthetic harmonic mixtures, the average SI-SDRi is 12.46 dB. This reduction of about 2 dB is due to the accuracy of estimation and synthesis of STRAIGHT,\nAuthorized licensed use limited to: Technische Universitaet Muenchen. Downloaded on June 17,2022 at 13:59:27 UTC from IEEE Xplore. Restrictions apply.\nwhere some instantaneous components are not accurately synthesized. For the effect of harmonicity, once the harmonicity is destroyed, even jittered by 0.1 of the F0 will make the average separation performance drop sharply to 4.13 dB. As the degree of inharmonicity further increases, the separation performance decreases gently and tends to level off when the jitter magnitude is larger than 0.3 (c > 0.3). The performances in our network are consistent with that in psychoacoustic experiments obtained by McDermott et al. [53], which illustrated that the intelligibility of concurrent words or sentences decreased with the degree of inharmonicity for the human auditory system.\nIn summary, for concurrent sentences, inharmonicity hinders the grouping of frequency components and the separation performance decreases with the degree of inharmonicity increases. This suggests that the network uses harmonicity principles to separate speech. The harmonicity principle is still effective in the separation of complex sound sources."
        },
        {
            "heading": "VII. DISCUSSION",
            "text": "Two general approaches have been followed to solve sound source separation problems: one is to separate sources through the imitation of the auditory system, while the other is to base it solely on statistical signal processing.\nThe first approach develops a model with biological rational based on knowledge from psychoacoustics and auditory neuroscience (CASA models) [2]. Conceptually, CASA models operate as a two-stage process: segmentation and grouping. Segmentation is based on the representation of sound attributes. Hu and Wang [4] estimated attributes (pitch and AM) through the imitation of auditory peripheral and mid-level processing. Recently, more auditory central processes have appeared in CASA models. Elhilali et al. [56] mimicked human cortical processing to segregate auditory objects, which mapped the acoustic waveform into a 4-D cortical representation. Elhilali et al. [12] learned spectro-temporal representations through a stochastic neural network with two layers, including a local analysis layer\nand a long-range analysis layer to mimic the simultaneous and sequential organization in the auditory system respectively.\nAfter obtaining the effective representation of sound attributes, CASA models tend to group segments on the basis of Gestalt principles, such as the proximity in pitch explored by Hu and Wang [4], dynamic similarity reflected by an integrative and clustering stage [56], and temporal coherence via Hebbian learning [12]. One or few specific grouping principles, rather than all principles, are implemented in traditional CASA models and dominate the process of separation. If CASA models are tested with the simple stimuli in our study, they are likely to have similar behavior characteristics with humans in some principles but not all aspects.\nTraditional CASA models separate sound sources by carefully modeling the auditory system, while the current understanding of auditory neuroscience is not sufficient to develop a system as intelligent as humans. The opportunities to learn principles through task optimization are lost in those unsupervised models, which makes them particularly effective for simple stimuli and cannot be generalized to natural sources in complex scenes.\nThe second approach treats source separation as a supervised learning problem. In recent years, these statistical deep network models have achieved excellent performance in natural source separation. However, few attempts have been made to investigate the biological plausibility of this kind of network because these models do not appear to mimic the auditory system in a particular way and network weights are hard to interpret. In this paper, we demonstrate that similar separation mechanisms emerge in our statistical network as they are present in human hearing. This means that without accurate biological modeling, a network that follows Gestalt rules can be obtained.\nIs the ultimate destination of deep learning to be able to spontaneously learn the same optimization criteria like human beings? Francl and McDermott [11] showed that for localization, trained networks can spontaneously operate similarly to human spatial hearing. In our study, the trained network has also been shown to behave similarly to human hearing \u2013 in this case the more complex auditory scene analysis. We believe that observing the emergence of (segregation) mechanisms purely on the basis of statistical signal processing has a profound influence on the study of deep learning and auditory neuroscience.\nThe trained network is not limited to some specific sound sources but depends on generalized primitive grouping principles. The underlying generalization suggests a general source separation network that can adapt to all scenes and achieve selective hearing like the human auditory system. The work also provides a new perspective on network interpretation: the underlying mechanisms are explored through Gestalt experiments following those developed in many years of auditory research, which goes beyond the visualization of features or filter activation and can be used to probe specific hypotheses while building on a wealth of previous experience. The hypothesis testing helps explain the \u201cblack box\u201d of the network and in turn guides further network optimization.\nOn the other hand, the emergence of separation mechanisms through network learning can also help our understanding of the processes in the human auditory system. As an ideal observer,\nAuthorized licensed use limited to: Technische Universitaet Muenchen. Downloaded on June 17,2022 at 13:59:27 UTC from IEEE Xplore. Restrictions apply.\nthe network can nonetheless test a lot of stimuli, which could be instructive for future psychoacoustic experiments."
        },
        {
            "heading": "VIII. CONCLUSION",
            "text": "In this study, a convolutional deep neural network, ConvTasNet, is developed to separate arbitrary sounds in the time domain, including speech, music, and environmental sounds. The SI-SDRi of the best-performing network is 11.70 dB, which is comparable or even surpasses the result of IRMs. This demonstrates that our network has an excellent ability to separate natural complex sound sources.\nFor this network, that is capable of solving actual separation problems, the underlying separation mechanisms are investigated. At first, the method of Gestalt psychologists is adopted, where simple stimuli are used to explore principles one by one on simultaneous and sequential organization. There are fundamental differences between the training dataset (natural sources) and the highly specific and abstracted artificial stimuli made of tones. These differences make sure that only when networks generalize segregation principles, the separation performance of untrained artificial stimuli may follow that of human auditory scene analysis. Then, speech stimuli are generated with inharmonic carrier components while preserving other attributes to explore whether the harmonicity principle can be generalized to complex stimuli.\nTo our knowledge, it is the first demonstration that Gestalt principles underlying human auditory scene analysis are learned by supervised deep learning from unrelated sound sources with a completely statistical model that does not have any particular auditory-related process. The experiments probing simultaneous organization demonstrate that harmonicity, onset asynchrony, and coherent AM and FM assist the segregation. For sequential grouping, proximity in time and frequency is in a consistent manner with the emergence of a temporal coherence boundary like in the auditory system. A good continuation in frequency also exerts a strong force to integrate components into sequences. The similarity in timbre, as studied with varying spectral centroid, contributes to separation beyond the effects of fundamental frequency and intensity. These principles for simultaneous and sequential organization are shown to be cooperative and competitive, and the relative importance of these principles for separation is situation dependent. In addition, the experiment of concurrent sentence separation illustrates that the harmonicity principle is still effective in the separation of complex sound sources.\nIn summary, without prior knowledge about auditory scene analysis principles imparted on the network, it learns separation mechanisms similar to those in the human auditory system, which provides a new perspective for the problem of auditory scene analysis. Since ConvTasNet is a purely statistical model aiming to optimally segregate sound sources, results suggest that the mechanisms developed in the auditory system over many years have evolved for optimal segregation based on statistical characteristics of the acoustical signal.\nOur study is the first step for exploring auditory-like mechanisms learned by deep networks. In the future, the comparison\nof network results and psychoacoustic experiments may extend to other experiments and objective measures. In addition, how the networks\u2019 specific structure contributes to performance over data statistics is also worth exploring. It is not yet clear whether other kinds of networks can learn similar separation mechanisms from unrelated natural sounds.\nREFERENCES\n[1] A. S. Bregman, Auditory Scene Analysis: The Perceptual Organization of Sound. Cambridge, MA, USA: MIT Press, 1990. [2] D. Wang and G. J. Brown, \u201cFundamentals of computational auditory scene analysis,\u201d in Computational Auditory Scene Analysis: Principles, Algorithms, and Applications, D. Wang and G. J. Brown, Eds., Hoboken, NJ, USA: Wiley, 2006, pp. 1\u201337. [3] D. Wang and G. J. Brown, \u201cSeparation of speech from interfering sounds based on oscillatory correlation,\u201d IEEE Trans. Neural Netw., vol. 10, no. 3, pp. 684\u2013697, May 1999. [4] G. Hu and D. Wang, \u201cMonaural speech segregation based on pitch tracking and amplitude modulation,\u201d IEEE Trans. Neural Netw., vol. 15, no. 5, pp. 1135\u20131150, Sep. 2004. [5] G. Hu and D. Wang, \u201cAuditory segmentation based on onset and offset analysis,\u201d IEEE Trans. Audio, Speech Lang. Process., vol. 15, no. 2, pp. 396\u2013405, Feb. 2007. [6] D. Wang and J. Chen, \u201cSupervised speech separation based on deep learning: An overview,\u201d IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 26, no. 10, pp. 1702\u20131726, Oct. 2018. [7] K. Tan, J. Chen, and D. Wang, \u201cGated residual networks with dilated convolutions for monaural speech enhancement,\u201d IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 27, no. 1, pp. 189\u2013198, Jan. 2019. [8] Y. Liu and D. Wang, \u201cDivide and conquer: A deep CASA approach to talker-independent monaural speaker separation,\u201d IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 27, no. 12, pp. 2092\u20132102, Dec. 2019. [9] Y. Luo and N. Mesgarani, \u201cConv-TasNet: Surpassing ideal time-frequency magnitude masking for speech separation,\u201d IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 27, no. 8, pp. 1256\u20131266, Aug. 2019. [10] Y. Luo, Z. Chen, and T. Yoshioka, \u201cDual-Path RNN: Efficient long sequence modeling for time-domain single-channel speech separation,\u201d in Proc. IEEE Int. Conf. Acoust., Speech, Signal Process., 2020, pp. 46\u201350. [11] A. Francl and J. H. McDermott, \u201cDeep neural network models of sound localization reveal how perception is adapted to real-world environments,\u201d Nature Hum. Behav., vol. 6, no. 1, pp. 111\u2013133, 2022. [12] D. Chakrabarty and M. Elhilali, \u201cA Gestalt inference model for auditory scene segregation,\u201d PLoS Comput. Biol., vol. 15, no. 1, pp. 1\u201333, 2019. [13] H. Li, K. Chen, and B. U. Seeber, \u201cAuditory filterbanks benefit universal sound source separation,\u201d in Proc. IEEE Int. Conf. Acoust., Speech, Signal Process., 2021, pp. 181\u2013185. [14] H. Li, K. Chen, R. Li, J. Liu, B. Wan, and B. Zhou, \u201cAuditory-like simultaneous separation mechanisms spontaneously learned by a deep source separation network,\u201d Appl. Acoust., vol. 188, 2022, Art. no. 108591. [15] M. Wertheimer, \u201cUntersuchungen zur Lehre von der Gestalt. II,\u201d Psychol. Forsch., vol. 4, no. 1, pp. 301\u2013350, Jan. 1923. [16] C. J. Darwin and R. P. Carlyon, \u201cAuditory grouping,\u201d in Hearing, 2nd ed., B. C. J. Moore, Ed., San Diego, CA, USA: Academic, 1995, pp. 387\u2013424. [17] C. Micheyl and A. J. Oxenham, \u201cPitch, harmonicity and concurrent sound segregation: Psychoacoustical and neurophysiological findings,\u201d Hear. Res., vol. 266, no. 1\u20132, pp. 36\u201351, 2010. [18] J. F. Culling and C. J. Darwin, \u201cPerceptual and computational separation of simultaneous vowels: Cues arising from low-frequency beating,\u201d J. Acoust. Soc. Amer., vol. 95, no. 3, pp. 1559\u20131569, 1994. [19] G. J. Sandell and C. J. Darwin, \u201cRecognition of concurrently-sounding musical instruments with different fundamental frequencies,\u201d J. Acoust. Soc. Amer., vol. 100, no. 4, Oct. 1996, Art. no. 2683. [20] C. J. Darwin, \u201cPitch and auditory grouping,\u201d in Pitch: Neural Coding and Perception. New York, NY, USA: Springer, 2005, pp. 278\u2013305. [21] N. Grimault, S. P. Bacon, and C. Micheyl, \u201cAuditory stream segregation on the basis of amplitude-modulation rate,\u201d J. Acoust. Soc. Amer., vol. 111, no. 3, pp. 1340\u20131348, 2002. [22] C. J. Darwin and V. Ciocca, \u201cGrouping in pitch perception: Effects of onset asynchrony and ear of presentation of a mistuned component,\u201d J. Acoust. Soc. Amer., vol. 91, no. 6, pp. 3381\u20133390, 1992. [23] A. S. Bregman and S. Pinker, \u201cAuditory streaming and the building of timbre,\u201d Can. J. Psychol., vol. 32, no. 1, pp. 19\u201331, 1978.\nAuthorized licensed use limited to: Technische Universitaet Muenchen. Downloaded on June 17,2022 at 13:59:27 UTC from IEEE Xplore. Restrictions apply.\n[24] J. H. Lee and L. E. Humes, \u201cEffect of fundamental-frequency and sentenceonset differences on speech-identification performance of young and older adults in a competing-talker background,\u201d J. Acoust. Soc. Amer., vol. 132, no. 3, pp. 1700\u20131717, 2012. [25] A. S. Bregman, J. Abramson, P. Doehring, and C. J. Darwin, \u201cSpectral integration based on common amplitude modulation,\u201d Percep. Psychophys., vol. 37, no. 5, pp. 483\u2013493, 1985. [26] R. P. Carlyon, \u201cThe psychophysics of concurrent sound segregation,\u201d Philos. Trans. Roy. Soc. London. Ser. B: Biol. Sci., vol. 336, no. 1278, pp. 347\u2013355, Jun. 1992. [27] R. P. Carlyon, \u201cDiscriminating between coherent and incoherent frequency modulation of complex tones,\u201d J. Acoust. Soc. Amer., vol. 89, no. 1, pp. 329\u2013340, Jan. 1991. [28] M. H. Chalikia and A. S. Bregman, \u201cThe perceptual segregation of simultaneous vowels with harmonic, shifted, or random components,\u201d Percep. Psychophys., vol. 53, no. 2, pp. 125\u2013133, 1993. [29] N. Itatani and G. M. Klump, \u201cAnimal models for auditory streaming,\u201d Philos. Trans. Roy. Soc. B: Biol. Sci., vol. 372, no. 1714, 2017, Art. no. 20160112. [30] L. P. A. S. van Noorden, \u201cTemporal coherence in the perception of tone sequences,\u201d Ph.D. dissertation, Inst. Perception Res., Technische Hogeschool Eindhoven, Eindhoven, 1975. [31] P. G. Singh, \u201cPerceptual organization of complex-tone sequences: A tradeoff between pitch and timbre,\u201d J. Acoust. Soc. Amer., vol. 82, no. 3, pp. 886\u2013899, 1987. [32] R. Cusack and B. Roberts, \u201cEffects of differences in timbre on sequential grouping,\u201d Percep. Psychophys., vol. 62, no. 5, pp. 1112\u20131120, 2000. [33] D. L. Wessel, \u201cTimbre space as a musical control structure,\u201d Comput. Music J., vol. 3, no. 2, pp. 45\u201352, 1979. [34] I. Kavalerov et al., \u201cUniversal sound separation,\u201d in Proc. IEEE Workshop Appl. Signal Process. Audio Acoust., 2019, pp. 175\u2013179. [35] H. Li, K. Chen, and B. U. Seeber, \u201cConvTasNet-based anomalous noise separation for intelligent noise monitoring,\u201d in Proc. INTERNOISE, 2021, pp. 2044\u20132051. [36] \u201cThe BBC sound effects library,\u201d [Online]. Available: http://bbcsfx. acropolis.org.uk/ [37] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, \u201cLibrispeech: An ASR corpus based on public domain audio books,\u201d in Proc. IEEE Int. Conf. Acoust., Speech, Signal Process., 2015, pp. 5206\u20135210. [38] D. Snyder, G. Chen, and D. Povey, \u201cMUSAN: A music, speech, and noise corpus,\u201d 2015, arXiv:1510.08484. [39] J. L. Roux, S. Wisdom, H. Erdogan, and J. R. Hershey, \u201cSDR - Half-baked or well done?,\u201d in Proc. IEEE Int. Conf. Acoust., Speech, Signal Process., 2019, pp. 626\u2013630. [40] D. Yu, M. Kolbaek, Z. H. Tan, and J. Jensen, \u201cPermutation invariant training of deep models for speaker-independent multi-talker speech separation,\u201d in Proc. IEEE Int. Conf. Acoust., Speech, Signal Process., 2017, pp. 241\u2013245. [41] M. Pariente et al., \u201cAsteroid: The pytorch-based audio source separation toolkit for researchers,\u201d in Proc. INTERSPEECH, 2020, pp. 2637\u20132641. [42] A. Van Den Oord et al., \u201cWaveNet: A generative model for raw audio,\u201d 2016, arXiv:1609.03499. [43] G. Wichern et al., \u201cWHAM!: Extending speech separation to noisy environments,\u201d in Proc. INTERSPEECH, 2019, pp. 1368\u20131372. [44] A. J. Oxenham, \u201cPitch perception and auditory stream segregation: Implications for hearing loss and cochlear implants,\u201d Trends Amplification, vol. 12, no. 4, pp. 316\u2013331, 2008. [45] A. S. Bregman and G. L. Dannenbring, \u201cThe effect of continuity on auditory stream segregation,\u201d Percep. Psychophys., vol. 13, no. 2, pp. 308\u2013312, 1973. [46] R. P. Carlyon and H. E. Gockel, \u201cEffects of harmonicity and regularity on the perception of sound sources,\u201d in Auditory Perception of Sound Sources, Boston, MA, USA: Springer, 2008, pp. 191\u2013213. [47] N. Jacoby, E. A. Undurraga, M. J. McPherson, J. Vald\u00e9s, T. Ossand\u00f3n, and J. H. McDermott, \u201cUniversal and non-universal features of musical pitch perception revealed by singing,\u201d Curr. Biol., vol. 29, no. 19, pp. 3229\u20133243, Oct. 2019. [48] E. Terhardt, \u201cCalculating virtual pitch,\u201d Hear. Res., vol. 1, pp. 155\u2013182, 1979. [49] R. Plomp and W. J. M. Levelt, \u201cTonal consonance and critical bandwidth,\u201d J. Acoust. Soc. Amer., vol. 38, no. 4, pp. 548\u2013560, Oct. 1965. [50] W. A. Sethares, \u201cLocal consonance and the relationship between timbre and scale,\u201d J. Acoust. Soc. Amer., vol. 94, no. 3, pp. 1218\u20131228, Sep. 1993. [51] G. L. Dannenbring and A. S. Bregman, \u201cStreaming vs. fusion of sinusoidal components of complex tones,\u201d Percep. Psychophys., vol. 24, no. 4, pp. 369\u2013376, 1978. [52] H. Fastl and E. Zwicker, Psychoacoustics: Facts and Models, 3rd ed., New York, NY, USA: Springer, 2007, pp. 247\u2013253. [53] S. Popham, D. Boebinger, D. P. W. Ellis, H. Kawahara, and J. H. McDermott, \u201cInharmonic speech reveals the role of harmonicity in the cocktail party problem,\u201d Nature Commun., vol. 9, no. 2122, pp. 1\u201313, 2018. [54] J. H. McDermott, D. P. W. Ellis, and H. Kawahara, \u201cInharmonic speech: A tool for the study of speech perception and separation,\u201d in Proc. SAPASCALE, 2012, pp. 114\u2013117. [55] H. Kawahara and M. Morise, \u201cTechnical foundations of TANDEMSTRAIGHT, a speech analysis, modification and synthesis framework,\u201d SADHANA, vol. 36, no. 5, pp. 713\u2013727, 2011. [56] M. Elhilali and S. A. Shamma, \u201cA cocktail party with a cortical twist: How cortical mechanisms contribute to sound segregation,\u201d J. Acoust. Soc. Amer., vol. 124, no. 6, pp. 3751\u20133771, 2008. Han Li received the B.Eng. degree in environmental engineering in 2015 from Northwestern Polytechnical University, Xi\u2019an, China, where she is currently working toward the Ph.D. degree in acoustics. From 2018 to 2020, she was the recipient of the two -year Ph.D. scholarship from the China Scholarship Council to do research with the Technical University of Munich, Munich, Germany. Her research interests include source separation, sound event detection, and auditory perception. Kean Chen received the Ph.D. degree from Northwestern Polytechnical University, Xi\u2019an, China, in 1992. He is currently a Professor with the School of Marine Science and Technology, Northwestern Polytechnical University. His research interests include active control of noise and vibration, auditory perception and its applications. Prof. Chen is a member of the Acoustical Society of China, a Member of Chinese Society for Vibration Engineering, the President of the Acoustical Society of Shaanxi Province, the Vice Director of the Noise Control Chapter of Chinese Society for Vibration Engineering, the Vice Director of the Environmental Acoustics Chapter of the Acoustical Society of China, a Committee Member of National Acoustics Standardization Committee, and an Editorial Board Member of the Journal of Vibration Engineering, Noise and Vibration Control, and Journal of Marine Engineering. Bernhard U. Seeber received the Dipl.-Ing. degree in electrical engineering and information technology and the Dr.-Ing. degree (with distinction) from the Technical University of Munich (TUM), Munich, Germany, in 1999 and 2003, respectively. He was a Postdoc with the Department of Psychology, University of California, Berkeley, Berkeley, CA, USA. In 2007, he joined the MRC Institute of Hearing Research, Nottingham, U.K., to lead the Spatial Hearing Laboratory. Since 2012, he is the Head of the Audio Information Processing Lab and a Professor with the Department of Electrical and Computer Engineering, TUM. His research interests include signal processing for hearing aids and cochlear implants, on virtual acoustics, spatial hearing, auditory modeling and acoustic nondestructive testing. Prof. Seeber is a Member of the German Acoustical Society (DEGA), Association for Electrical, Electronic and Information Technologies, Acoustical Society of America, Association for Research in Audiology, and Bernstein Network for Computational Neuroscience. He heads the Technical Committee on hearing acoustics in the Society for Information Technology (ITG/VDE) and was a Member of the Executive Board of the DEGA from 2016 to 2022. He was the recipient of the Lothar-Cremer Award of the DEGA, Doctoral Thesis Award of the ITG, and ITG Publication Award.\nAuthorized licensed use limited to: Technische Universitaet Muenchen. Downloaded on June 17,2022 at 13:59:27 UTC from IEEE Xplore. Restrictions apply."
        }
    ],
    "title": "Gestalt Principles Emerge When Learning Universal Sound Source Separation",
    "year": 2022
}