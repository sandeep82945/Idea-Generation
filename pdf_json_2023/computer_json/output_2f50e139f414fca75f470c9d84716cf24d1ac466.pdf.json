{
    "abstractText": "Semi-supervised semantic segmentation (SSS) is an important task that utilizes both labeled and unlabeled data to reduce expenses on labeling training examples. However, the effectiveness of SSS algorithms is limited by the difficulty of fully exploiting the potential of unlabeled data. To address this, we propose a dual-level Siamese structure network (DSSN) for pixel-wise contrastive learning. By aligning positive pairs with a pixel-wise contrastive loss using strong augmented views in both low-level image space and high-level feature space, the proposed DSSN is designed to maximize the utilization of available unlabeled data. Additionally, we introduce a novel class-aware pseudo-label selection strategy for weak-to-strong supervision, which addresses the limitations of most existing methods that do not perform selection or apply a predefined threshold for all classes. Specifically, our strategy selects the top high-confidence prediction of the weak view for each class to generate pseudo labels that supervise the strong augmented views. This strategy is capable of taking into account the class imbalance and improving the performance of long-tailed classes. Our proposed method achieves state-of-the-art results on two datasets, PASCAL VOC 2012 and Cityscapes, outperforming other SSS algorithms by a significant margin.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zhibo Tian"
        },
        {
            "affiliations": [],
            "name": "Xiaolin Zhang"
        },
        {
            "affiliations": [],
            "name": "Peng Zhang"
        },
        {
            "affiliations": [],
            "name": "Kun Zhan"
        }
    ],
    "id": "SP:0b42aa2052ba63f8d7200702d5a66dcf85306b23",
    "references": [
        {
            "authors": [
                "David Berthelot",
                "Nicholas Carlini",
                "Ian Goodfellow",
                "Nicolas Papernot",
                "Avital Oliver",
                "Colin A Raffel"
            ],
            "title": "MixMatch: A holistic approach to semi-supervised learning",
            "venue": "NeurIPS",
            "year": 2019
        },
        {
            "authors": [
                "Liang-Chieh Chen",
                "Yukun Zhu",
                "George Papandreou",
                "Florian Schroff",
                "Hartwig Adam"
            ],
            "title": "Encoder-decoder with atrous separable convolution for semantic image segmentation",
            "year": 2018
        },
        {
            "authors": [
                "Xiaokang Chen",
                "Yuhui Yuan",
                "Gang Zeng",
                "Jingdong Wang"
            ],
            "title": "Semisupervised semantic segmentation with cross pseudo supervision",
            "venue": "In CVPR",
            "year": 2021
        },
        {
            "authors": [
                "S. Chopra",
                "R. Hadsell",
                "Y. LeCun"
            ],
            "title": "Learning a similarity metric discriminatively, with application to face verification",
            "venue": "In CVPR",
            "year": 2005
        },
        {
            "authors": [
                "Marius Cordts",
                "Mohamed Omran",
                "Sebastian Ramos",
                "Timo Rehfeld",
                "Markus Enzweiler",
                "Rodrigo Benenson",
                "Uwe Franke",
                "Stefan Roth",
                "Bernt Schiele"
            ],
            "title": "The cityscapes dataset for semantic urban scene understanding",
            "venue": "In CVPR",
            "year": 2016
        },
        {
            "authors": [
                "Antonia Creswell",
                "Tom White",
                "Vincent Dumoulin",
                "Kai Arulkumaran",
                "Biswa Sengupta",
                "Anil A Bharath"
            ],
            "title": "Generative adversarial networks: An overview",
            "venue": "IEEE Signal Processing Magazine 35,",
            "year": 2018
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "In CVPR. IEEE,",
            "year": 2009
        },
        {
            "authors": [
                "Terrance DeVries",
                "Graham W Taylor"
            ],
            "title": "Improved regularization of convolutional neural networks with cutout",
            "venue": "arXiv preprint arXiv:1708.04552",
            "year": 2017
        },
        {
            "authors": [
                "Mark Everingham",
                "SM Ali Eslami",
                "Luc Van Gool",
                "Christopher KI Williams",
                "John Winn",
                "Andrew Zisserman"
            ],
            "title": "The pascal visual object classes challenge: A retrospective",
            "year": 2015
        },
        {
            "authors": [
                "Zhengyang Feng",
                "Qianyu Zhou",
                "Qiqi Gu",
                "Xin Tan",
                "Guangliang Cheng",
                "Xuequan Lu",
                "Jianping Shi",
                "Lizhuang Ma"
            ],
            "title": "Dmt: Dynamic mutual training for semi-supervised learning",
            "venue": "Pattern Recognition",
            "year": 2022
        },
        {
            "authors": [
                "Geoff French",
                "Samuli Laine",
                "Timo Aila",
                "Michal Mackiewicz",
                "Graham Finlayson"
            ],
            "title": "Semi-supervised semantic segmentation needs strong, varied perturbations",
            "year": 2019
        },
        {
            "authors": [
                "Jean-Bastien Grill",
                "Florian Strub",
                "Florent Altch\u00e9",
                "Corentin Tallec",
                "Pierre Richemond",
                "Elena Buchatskaya",
                "Carl Doersch",
                "Bernardo Avila Pires",
                "Zhaohan Guo",
                "Mohammad Gheshlaghi Azar"
            ],
            "title": "Bootstrap your own latent-a new approach to self-supervised learning",
            "venue": "In NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Raia Hadsell",
                "Sumit Chopra",
                "Yann LeCun"
            ],
            "title": "Dimensionality reduction by learning an invariant mapping",
            "venue": "In CVPR",
            "year": 2006
        },
        {
            "authors": [
                "Bharath Hariharan",
                "Pablo Arbel\u00e1ez",
                "Lubomir Bourdev",
                "Subhransu Maji",
                "Jitendra Malik"
            ],
            "title": "Semantic contours from inverse detectors",
            "venue": "In ICCV",
            "year": 2011
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In CVPR",
            "year": 2016
        },
        {
            "authors": [
                "R Devon Hjelm",
                "Alex Fedorov",
                "Samuel Lavoie-Marchildon",
                "Karan Grewal",
                "Phil Bachman",
                "Adam Trischler",
                "Yoshua Bengio"
            ],
            "title": "Learning deep representations by mutual information estimation and maximization",
            "venue": "In ICLR",
            "year": 2019
        },
        {
            "authors": [
                "Wei-Chih Hung",
                "Yi-Hsuan Tsai",
                "Yan-Ting Liou",
                "Yen-Yu Lin",
                "Ming-Hsuan Yang"
            ],
            "title": "Adversarial learning for semi-supervised semantic segmentation",
            "year": 2018
        },
        {
            "authors": [
                "Ying Jin",
                "Jiaqi Wang",
                "Dahua Lin"
            ],
            "title": "Semi-Supervised Semantic Segmentation via Gentle Teaching Assistant",
            "venue": "In NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Zhanghan Ke",
                "Di Qiu",
                "Kaican Li",
                "Qiong Yan",
                "Rynson WH Lau"
            ],
            "title": "Guided collaborative training for pixel-wise semi-supervised learning",
            "year": 2020
        },
        {
            "authors": [
                "Samuli Laine",
                "Timo Aila"
            ],
            "title": "Temporal ensembling for semi-supervised learning",
            "venue": "In ICLR",
            "year": 2017
        },
        {
            "authors": [
                "Shikun Liu",
                "Shuaifeng Zhi",
                "Edward Johns",
                "Andrew J Davison"
            ],
            "title": "Bootstrapping semantic segmentation with regional contrast",
            "venue": "In ICLR",
            "year": 2022
        },
        {
            "authors": [
                "Yuyuan Liu",
                "Yu Tian",
                "Yuanhong Chen",
                "Fengbei Liu",
                "Vasileios Belagiannis",
                "Gustavo Carneiro"
            ],
            "title": "Perturbed and strict mean teachers for semi-supervised semantic segmentation",
            "venue": "In CVPR",
            "year": 2022
        },
        {
            "authors": [
                "Aditya Krishna Menon",
                "Sadeep Jayasumana",
                "Ankit Singh Rawat",
                "Himanshu Jain",
                "Andreas Veit",
                "Sanjiv Kumar"
            ],
            "title": "Long-tail learning via logit adjustment",
            "venue": "In ICLR",
            "year": 2021
        },
        {
            "authors": [
                "Sudhanshu Mittal",
                "Maxim Tatarchenko",
                "Thomas Brox"
            ],
            "title": "Semi-supervised semantic segmentation with high-and low-level consistency",
            "venue": "TPAMI 43,",
            "year": 2019
        },
        {
            "authors": [
                "Aaron van den Oord",
                "Yazhe Li",
                "Oriol Vinyals"
            ],
            "title": "Representation learning with contrastive predictive coding",
            "venue": "arXiv preprint arXiv:1807.03748",
            "year": 2018
        },
        {
            "authors": [
                "Yassine Ouali",
                "C\u00e9line Hudelot",
                "andMyriam Tami"
            ],
            "title": "Semi-supervised semantic segmentation with cross-consistency training",
            "venue": "In CVPR",
            "year": 2020
        },
        {
            "authors": [
                "Kihyuk Sohn",
                "David Berthelot",
                "Nicholas Carlini",
                "Zizhao Zhang",
                "Han Zhang",
                "Colin A Raffel",
                "Ekin Dogus Cubuk",
                "Alexey Kurakin",
                "Chun-Liang Li"
            ],
            "title": "FixMatch: Simplifying semi-supervised learning with consistency and confidence",
            "venue": "In NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Antti Tarvainen",
                "Harri Valpola"
            ],
            "title": "Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results",
            "venue": "In NeurIPS,",
            "year": 2017
        },
        {
            "authors": [
                "A.M. Walker"
            ],
            "title": "On the Asymptotic Behaviour of Posterior Distributions",
            "venue": "Journal of the Royal Statistical Society: Series B (Methodological) 31,",
            "year": 1969
        },
        {
            "authors": [
                "Yuchao Wang",
                "Haochen Wang",
                "Yujun Shen",
                "Jingjing Fei",
                "Wei Li",
                "Guoqiang Jin",
                "Liwei Wu",
                "Rui Zhao",
                "Xinyi Le"
            ],
            "title": "Semi-supervised semantic segmentation using unreliable pseudo-labels",
            "venue": "In CVPR",
            "year": 2022
        },
        {
            "authors": [
                "Qizhe Xie",
                "Zihang Dai",
                "Eduard Hovy",
                "Thang Luong",
                "Quoc Le"
            ],
            "title": "Unsupervised data augmentation for consistency training",
            "venue": "NeurIPS",
            "year": 2020
        },
        {
            "authors": [
                "Haiming Xu",
                "Lingqiao Liu",
                "Qiuchen Bian",
                "Zhen Yang"
            ],
            "title": "Semi-supervised Semantic Segmentation with Prototype-based Consistency Regularization",
            "venue": "In NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Lihe Yang",
                "Wei Zhuo",
                "Lei Qi",
                "Yinghuan Shi",
                "Yang Gao"
            ],
            "title": "St++: Make self-training work better for semi-supervised semantic segmentation",
            "venue": "In CVPR",
            "year": 2022
        },
        {
            "authors": [
                "Sangdoo Yun",
                "Dongyoon Han",
                "Seong Joon Oh",
                "Sanghyuk Chun",
                "Junsuk Choe",
                "Youngjoon Yoo"
            ],
            "title": "Cutmix: Regularization strategy to train strong classifiers with localizable features",
            "venue": "In ICCV",
            "year": 2019
        },
        {
            "authors": [
                "Yuanyi Zhong",
                "Bodi Yuan",
                "Hong Wu",
                "Zhiqiang Yuan",
                "Jian Peng",
                "Yu-Xiong Wang"
            ],
            "title": "Pixel contrastive-consistent semi-supervised semantic segmentation",
            "venue": "In ICCV",
            "year": 2021
        },
        {
            "authors": [
                "Yuliang Zou",
                "Zizhao Zhang",
                "Han Zhang",
                "Chun-Liang Li",
                "Xiao Bian",
                "Jia-Bin Huang",
                "Tomas Pfister"
            ],
            "title": "PseudoSeg: Designing pseudo labels for semantic segmentation",
            "venue": "In ICLR",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "CCS CONCEPTS \u2022 Computing methodologies\u2192 Image segmentation.\nKEYWORDS Semi-supervised semantic segmentation, pixel-wise contrastive learning, class-aware pseudo-label generation\n\u2217Corresponding author.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. MM \u201923, October 28, 2023 \u2013 November 3, 2023, Ottawa, Canada \u00a9 2023 Association for Computing Machinery. ACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00 https://doi.org/XXXXXXX.XXXXXXX\nACM Reference Format: Zhibo Tian, Xiaolin Zhang, Peng Zhang, and Kun Zhan. 2023. Improving Semi-Supervised Semantic Segmentation with Dual-Level Siamese Structure Network. In Proceedings of the 31st ACM International Conference on Multimedia (MM \u201923), October 28, 2023 \u2013 November 3, 2023, Ottawa, Canada. ACM, New York, NY, USA, 9 pages. https://doi.org/XXXXXXX.XXXXXXX"
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Deep learning methods for supervised segmentation have shown remarkable performance. However, they heavily rely on a large amount of annotated images, which is labor cost and time-consuming. Alternatively, semi-supervised semantic segmentation (SSS) offers a viable solution to address this fundamental weakness by exploiting the readily available unlabeled data to improve model performance.\nar X\niv :2\n30 7.\n13 93\n8v 1\n[ cs\n.C V\n] 2\n6 Ju\nl 2 02\nExisting semi-supervised learning methods typically use unlabeled samples in two ways: pseudo supervision [1, 27] and consistency regularization [20, 28, 31]. Pseudo supervision is to generate pseudo labels for the unlabeled images and gradually incorporates them into the training process to supervise model learning. For example, preliminary works [17, 24] in SSS tend to utilize the generative adversarial networks [6] as auxiliary supervision for unlabeled images. Consistency regularization promotes agreement among model predictions on unlabeled samples that are subjected to various perturbations, thus improving model generalization by ensuring that different views of the same unlabeled image are consistent. Modern SSS algorithms combine pseudo supervision and consistency regularization into a two-view network architecture, where one view generates pseudo labels to supervise the other view for prediction consistency. For instance, the intuition of CPS [3] is that using one view generates pseudo labels of unlabeled images to expand the training set of the other view. PseudoSeg [36] generates pseudo labels in a weak augmented view to supervise the other strong augmented view. PS-MT [22] employs higher-confidence pseudo labels than CPS by averaging the predictions of two views. To search for high-quality pseudo labels, CCT [26] employs a fixed threshold for all classes and pixels with confidence scores above the threshold to participate in network updates. In CCT [26], it mainly uses consistency learning between one weak view and two strong augmented views of a high-level feature.\nHowever, many existing SSS algorithms do not fully exploit the potential of unlabelled data. To address this issue, we propose a Dual-level Siamese structure network (DSSN) to fully exploit feature diversities. In addition to the two strategies commonly used in most algorithms, we introduce a new variant of contrastive learning. Fig. 1(b) illustrates a typical structure of the vanilla contrastive learning, which excels at providing extraordinary generalization abilities for unlabeled samples [4, 13]. Specifically, the proposed DSSN simultaneously employs pixel-wise contrastive learning and two-level strong augmented views. Accordingly, contrastive objectives in terms of image-level and feature-level augmentations are introduced to guide the network training. Such structure guarantees fully exploiting the potential of unlabeled data. As shown in Fig. 1(a), at the image level, two different views of unlabeled samples are obtained with different strong augmentations, and a pixel-wise contrastive objective is added to train DSSN using the corresponding predictions. At the feature level, high-level latent features from the encoder produce two strong augmented views and also conduct a contrastive loss. This DSSN design enables us to fully exploit the available unlabeled data.\nGiven that most real-world datasets exhibit imbalanced or longtailed label distributions [23], we propose a class-aware pseudo label generation (CPLG) strategy that selects class-specific highconfidence pseudo labels from weak views to supervise the strong views. Our CPLG strategy differs from previous approaches [11, 26], which apply a fixed threshold to all categories. By treating each class differently, our method aims to improve the performance of long-tailed categories. Without any selection, low-quality pseudo labels generated from the weak augmented view are used to supervise the strong augmented view, which could negatively affect the model training. Using a constant threshold for all classes may result in long-tailed classes being poorly trained, as their confidence may\nbe lower than the threshold and thus not involved in training. Using a fixed threshold may also result in useful pseudo-labels being ignored in some classes that fall below the predefined threshold. For each class has pseudo labels, we select top high-confidence pixels in each class since most segments in an image are imbalances and also it is imbalances in the whole dataset. A schematic illustrating this strategy is presented in Fig. 1(c). This approach increases the contribution of long-tailed classes and addresses the learning difficulties of different classes.\nIn summary, the proposed DSSN method makes the following contributions:\n(1) DSSN offers a novel approach to leverage unlabeled data in training SSS models by utilizing dual-level pixel-wise contrastive learning. This approach is a valuable addition to the existing techniques of exploiting unlabeled data, such as pseudo-supervision and consistency regularization.\n(2) DSSN\u2019s design enables the maximal utilization of available unlabeled data. The dual-level structure is not only utilized in contrastive learning but also in weak-to-strong pseudo-supervision.\n(3) We introduce a novel class-aware pseudo-label selection strategy for weak-to-strong supervision, known as CPLG. This strategy effectively improves the performance of long-tailed classes."
        },
        {
            "heading": "2 RELATEDWORK",
            "text": "SSS has two mainstream methods, pseudo supervision and consistency regularization. Preliminary works [17, 24] use the generative adversarial networks [6] to generate pseudo supervision. Specifically, consistency regularization methods encourage consistency prediction of unlabeled samples with various perturbation. The CutMix-Seg [11] approach incorporates the CutMix [34] augmentation into semantic segmentation in order to supply consistency restrictions on unlabeled data and also revealed Cutout [8] and CutMix [34] are critical to the success of consistency regularization. Alternatively, CCT [26] proposes a feature-level perturbation and a cross-consistency training method that enforce consistency between the main decoder predictions and auxiliary decoders. By using two segmentation models with the same structure but different initialization, GCT [19] conducts network perturbation and promotes consistency between the predictions from the two models. In the meantime, CPS [3] constructs two parallel networks to provide cross-pseudo labels for one another. DMT [10] re-weights the loss on different regions based on the disagreement of two different initialized models. Self-training by pseudo labeling is a classic technique that dates back about a decade, taking the most likely class as a pseudo label and training models on unlabeled data is a common method for achieving minimum entropy. Concurrently ST++ [33] also demonstrates that employing suitable data perturbations on unlabeled samples is really quite beneficial for self-training. Contrastive learning is one of the alternative methods that stands out. RoCo [21] and U2PL [30] use InfoNCE loss [25] on the predicted logits, but they not use Siamese structure network as shown in Fig. 1(b). DSSN obtains better performance than them, which can be seen in the experiment section."
        },
        {
            "heading": "3 METHOD",
            "text": ""
        },
        {
            "heading": "3.1 Preliminaries",
            "text": "Following the previous SSS works [3, 21, 33], we use both a small fraction of labeled data D\ud835\udc59 = {(\ud835\udc4b\ud835\udc56 , \ud835\udc7b\ud835\udc56 )}\ud835\udc40\ud835\udc56=1 and a large fraction of unlabeled data D\ud835\udc62 = {\ud835\udc4b\ud835\udc56 }\ud835\udc41+\ud835\udc40\ud835\udc56=1+\ud835\udc40 . \ud835\udc4b\ud835\udc56 denotes an image, and \ud835\udc7b\ud835\udc8a represents its ground-truth label if \ud835\udc4b\ud835\udc56 is a labeled image. \ud835\udc41 and \ud835\udc40 indicate the number of labeled images and unlabeled images, respectively, where \ud835\udc41 \u226b \ud835\udc40 in most cases. To facilitate the calculation of loss functions, we represent each pixel in an image as a vector \ud835\udc99 since a pixel has values in different channels. Thus, in subsequent sections, we represent each pixel as a vector \ud835\udc99 with \ud835\udc95 as its one-hot ground-truth label. Given an image \ud835\udc4b = [\ud835\udc99\ud835\udc56 ] with the size of\ud835\udc4a \u00d7\ud835\udc3b where\ud835\udc4a and \ud835\udc3b are the width and height, we denote the pixel by \ud835\udc99\ud835\udc56 , \ud835\udc56 \u2208 {1, ...,\ud835\udc4a \u00d7 \ud835\udc3b }. The latent high-level feature \ud835\udc9b corresponding to \ud835\udc99 is obtained by an encoder \ud835\udc53 (\ud835\udc99 |\ud835\udf03 ) where \ud835\udf03 is the learnable parameters of the encoder. We yield the predicted logits \ud835\udc89 by feeding the latent representations \ud835\udc9b into a decoder \ud835\udc54(\ud835\udc9b |\ud835\udf11) where \ud835\udf11 is the learnable parameters of the decoder. Finally, a softmax layer is added to obtain the ultimate probability for each class, i.e., \ud835\udc9a = softmax(\ud835\udc89).\nGiven a labeled image, it is straightforward to use a supervised cross-entropy loss for learning,\nLsup = \u2212 \u2211\ufe01 \ud835\udc56 \u2211\ufe01 \ud835\udc57\u2208C \ud835\udc61\ud835\udc56 \ud835\udc57 log\ud835\udc66\ud835\udc56 \ud835\udc57 (1)\nwhere C = {1, . . . ,\ud835\udc36} and \ud835\udc36 is the total number of classes. For images without annotated masks, the most straightforward way to generate their pseudo labels \ud835\udc95\ud835\udc56 is to apply a one-hot operation to the predictions, i.e., \ud835\udc9a\ud835\udc56 . For the \ud835\udc56-th pixel of an unlabeled image, we represent the predicted probability of the \ud835\udc56-th pixel belonging to the \ud835\udc57-th class as \ud835\udc66\ud835\udc56 \ud835\udc57 . Specifically, we use the following operation to generate pseudo labels:\n\ud835\udc50 = arg max \ud835\udc57\u2208C (\ud835\udc66\ud835\udc56 \ud835\udc57 ), (2)\n\ud835\udc61\ud835\udc56 \ud835\udc57 = { 1, if \ud835\udc57 = \ud835\udc50 0, otherwise\n(3)\nwhere \ud835\udc50 denotes the maximal probability within the class \ud835\udc57 \u2208 C, the \ud835\udc95\ud835\udc56 = [\ud835\udc61\ud835\udc56 \ud835\udc57 ] is the one-hot pseudo label."
        },
        {
            "heading": "3.2 Dual-Level Contrastive Learning",
            "text": "To fully exploit the potential of available unlabeled data, we propose to use DSSN for extracting pixel-wise contrastive positive pairs in different abstraction levels. The low-level image is subjected to two-view strong augmentations,\n\ud835\udc99\ud835\udc59\ud835\udc601\ud835\udc56 = AugL\ud835\udc60 (\ud835\udc99\ud835\udc56 ), (4) \ud835\udc99\ud835\udc59\ud835\udc602\ud835\udc56 = AugL\ud835\udc60 (\ud835\udc99\ud835\udc56 ) (5)\nwhere \ud835\udc99\ud835\udc59\ud835\udc601 \ud835\udc56 denotes the strong augmented low-level pixel in the first view. The output, AugL\ud835\udc60 (\u00b7), is random. AugL\ud835\udc60 (\u00b7) generates varying outputs using the same input to augment the data diversity. This increases the diversity, resulting in an improvement in the robustness and generalization ability of the training model.\nWe can use the two-view augmented images to obtain its decoded logits,\n\ud835\udc89\ud835\udc59\ud835\udc601\ud835\udc56 = \ud835\udc54(\ud835\udc53 (\ud835\udc99 \ud835\udc59\ud835\udc601 \ud835\udc56 |\ud835\udf03 ) |\ud835\udf11) . (6) \ud835\udc89\ud835\udc59\ud835\udc602\ud835\udc56 = \ud835\udc54(\ud835\udc53 (\ud835\udc99 \ud835\udc59\ud835\udc602 \ud835\udc56 |\ud835\udf03 ) |\ud835\udf11) . (7)\nAnalogous to [16], we apply the contrastive objective, i.e., Lcl to pairwise pixels for learning better representations:\nLcl = \u2212 1 |P | \u2211\ufe01 (\ud835\udc56,\ud835\udc56 ) \u2208P log\ud835\udc51 (\ud835\udc89\ud835\udc59\ud835\udc601\ud835\udc56 ,\ud835\udc89 \ud835\udc59\ud835\udc602 \ud835\udc56 )\n\u2212 1|N | \u2211\ufe01\n(\ud835\udc56, \ud835\udc57 ) \u2208N log\n( 1 \u2212 \ud835\udc51 (\ud835\udc89\ud835\udc59\ud835\udc601\ud835\udc56 ,\ud835\udc89 \ud835\udc59\ud835\udc602 \ud835\udc57 ) ) (8)\nwhere \ud835\udc51 (\u00b7, \u00b7) is a similarity score of a pair of logits. \ud835\udc89\ud835\udc59\ud835\udc601 \ud835\udc56 and \ud835\udc89\ud835\udc59\ud835\udc602 \ud835\udc56 are belong to positive pairs (\ud835\udc56, \ud835\udc56) \u2208 P while \ud835\udc89\ud835\udc59\ud835\udc601\n\ud835\udc56 and \ud835\udc89\ud835\udc59\ud835\udc602 \ud835\udc57 are negative\npairs (\ud835\udc56, \ud835\udc57) \u2208 N ,\u2200 \ud835\udc56 \u2260 \ud835\udc57 . We use P and N to denote the sets of positive and negative pairs, respectively.\nInspired by BYOL [12], we only use the positive pairs in this paper. The similarity score \ud835\udc51 (\u00b7, \u00b7) of positive logits is defined by a\nGaussian function,\n\ud835\udc51 (\ud835\udc89\ud835\udc59\ud835\udc601\ud835\udc56 ,\ud835\udc89 \ud835\udc59\ud835\udc602 \ud835\udc56 ) = exp ( \u2212 \ud835\udc89\ud835\udc59\ud835\udc601\ud835\udc56 \u2212 \ud835\udc89\ud835\udc59\ud835\udc602\ud835\udc56 22) . (9)\nThe similarity defined by Eq. (9) implies the similarity core is 1 if the pairwise logits are the same while it tends to 0 if their distance is very far from each other. From a different perspective, the error \u2225\ud835\udc89\ud835\udc59\ud835\udc601\n\ud835\udc56 \u2212\ud835\udc89\ud835\udc59\ud835\udc602 \ud835\udc56 2 2 of two-view logits is governed by the Gaussian distribution due to the central limit theorem [29], so we can also obtain Eq. (9).\nSubstituting Eq. (9) into Eq. (8) obtains the following form of the loss function.\nL\ud835\udc59\ud835\udc60cl = 1 \ud835\udc4a \u00d7 \ud835\udc3b \u2211\ufe01 \ud835\udc56\n\ud835\udc89\ud835\udc59\ud835\udc601\ud835\udc56 \u2212 \ud835\udc89\ud835\udc59\ud835\udc602\ud835\udc56 22 (10) where we only use pixel-wise positive pairs.\nFor the high-level feature contrastive learning, we obtain the high-level latent feature with the encoder,\n\ud835\udc9b\u210e\ud835\udc64\ud835\udc56 = \ud835\udc53 (AugL\ud835\udc64 (\ud835\udc99\ud835\udc56 ) |\ud835\udf03 ) (11)\nwhere AugL\ud835\udc64 (\u00b7) denotes a weak augmentation for the low-level pixel. The high-level feature is subjected to two-view strong augmentations,\n\ud835\udc9b\u210e\ud835\udc601\ud835\udc56 = AugH\ud835\udc60 (\ud835\udc9b \u210e\ud835\udc64 \ud835\udc56 ), (12) \ud835\udc9b\u210e\ud835\udc602\ud835\udc56 = AugH\ud835\udc60 (\ud835\udc9b \u210e\ud835\udc64 \ud835\udc56 ) (13)\nWe use the two-view augmented features to obtain its decoded logits, \ud835\udc89\u210e\ud835\udc601\n\ud835\udc56 = \ud835\udc54(\ud835\udc9b\u210e\ud835\udc601 \ud835\udc56 |\ud835\udf11) and \ud835\udc89\u210e\ud835\udc602 \ud835\udc56 = \ud835\udc54(\ud835\udc9b\u210e\ud835\udc6012 \ud835\udc56 |\ud835\udf11) . Then, we use them\nto construct the contrastive loss,\nL\u210e\ud835\udc60cl = 1 \ud835\udc4a \u00d7 \ud835\udc3b \u2211\ufe01 \ud835\udc56\n\ud835\udc89\u210e\ud835\udc601\ud835\udc56 \u2212 \ud835\udc89\u210e\ud835\udc602\ud835\udc56 22 . (14)"
        },
        {
            "heading": "3.3 Weak-to-Strong Pseudo Supervision",
            "text": "To leverage the four predictions generated by a strongly augmented image, we feed the corresponding weakly augmented image into DSSN. Next, we use the prediction of the weak view to generate its pseudo label and supervise the four strong views. Given our dual-level structure, weak-to-strong pseudo supervision is also performed in both levels. Specifically, we use the pseudo labels of the weak view, denoted as \ud835\udc95 , to supervise the predictions of the strong views, denoted as \ud835\udc9a,.\nThe weak pseudo supervisions are obtained by\n\ud835\udc9a\ud835\udc59\ud835\udc64 = softmax(\ud835\udc54(\ud835\udc53 (\ud835\udc99 |\ud835\udf03 \u2032) |\ud835\udf11 \u2032)) (15)\n\ud835\udc9a\u210e\ud835\udc64 = softmax(\ud835\udc54(\ud835\udc9b\u210e\ud835\udc64 |\ud835\udf11)) (16)\nwhere the parameters (\ud835\udf03 \u2032, \ud835\udf11\u2032) of the teacher model are updated from the student model (\ud835\udf03, \ud835\udf11) by the exponential moving average (EMA) method\n(\ud835\udf03 \u2032, \ud835\udf11\u2032) = \ud835\udefc (\ud835\udf03 \u2032, \ud835\udf11\u2032) + (1 \u2212 \ud835\udefc) (\ud835\udf03, \ud835\udf11) (17)\nwhere \ud835\udefc is a momentum parameter, with \ud835\udefc \u2208 [0, 1]. The pseudo labels \ud835\udc95\ud835\udc59\ud835\udc64 and \ud835\udc95\u210e\ud835\udc64 of \ud835\udc9a\ud835\udc59\ud835\udc64 and \ud835\udc9a\u210e\ud835\udc64 are calculated by using Eqs. (2) and (3), respectively. The output probability of the strong augmented views,\ud835\udc9a\ud835\udc59\ud835\udc601\n\ud835\udc56 ,\ud835\udc9a\ud835\udc59\ud835\udc602 \ud835\udc56 ,\n\ud835\udc9a\u210e\ud835\udc601 \ud835\udc56 , and \ud835\udc9a\u210e\ud835\udc602 \ud835\udc56 , are attained by the softmax layer.\nThe weak-to-strong pseudo-supervision loss functions are given by\nL (\ud835\udc59 )w2s = \u2212 \u2211\ufe01 \ud835\udc56 \u2211\ufe01 \ud835\udc57 \ud835\udc5a\ud835\udc59\ud835\udc64\ud835\udc56 \ud835\udc57 ( \ud835\udc61\ud835\udc59\ud835\udc64\ud835\udc56 \ud835\udc57 log\ud835\udc66 \ud835\udc59\ud835\udc601 \ud835\udc56 \ud835\udc57 + \ud835\udc61 \ud835\udc59\ud835\udc64 \ud835\udc56 \ud835\udc57 log\ud835\udc66 \ud835\udc59\ud835\udc602 \ud835\udc56 \ud835\udc57 ) (18)\nL (\u210e)w2s = \u2212 \u2211\ufe01 \ud835\udc56 \u2211\ufe01 \ud835\udc57 \ud835\udc5a\u210e\ud835\udc64\ud835\udc56 \ud835\udc57 ( \ud835\udc61\u210e\ud835\udc64\ud835\udc56 \ud835\udc57 log\ud835\udc66 \u210e\ud835\udc601 \ud835\udc56 \ud835\udc57 + \ud835\udc61 \u210e\ud835\udc64 \ud835\udc56 \ud835\udc57 log\ud835\udc66 \u210e\ud835\udc602 \ud835\udc56 \ud835\udc57 ) (19)\nwhere\ud835\udc5a\ud835\udc56 \ud835\udc57 is a class-wise binary mask to select the pixel with highconfidence score and we show how to obtain it in the next section."
        },
        {
            "heading": "3.4 Class-aware pseudo-label generation",
            "text": "As shown in Fig. 1(c), we show the class-aware pseudo-label generation (CPLG) method in this section.\nFor the \ud835\udc56-th pixel, it has different probabilities belonging to different classes.\ud835\udc66\ud835\udc56 \ud835\udc57 denotes the probability of the \ud835\udc56-th pixel belonging to the \ud835\udc57-th class. We observe all pixels in the same class, i.e., in the same channel of network output.\nFirst, we find the pixel class-wisely that has the largest probability in the \ud835\udc57-th class,\n\ud835\udc66max\ud835\udc57 = max \ud835\udc56 (\ud835\udc66\ud835\udc56 \ud835\udc57 ) ,\u2200 \ud835\udc57 \u2208 C . (20)\nSecond, we establish a class-wise threshold \ud835\udf0f \ud835\udc57 by multiplying the maximum probability by \ud835\udc5f%. Pixels exceeding this class-wise threshold are selected. Additionally, we restrict the maximum probability by \ud835\udf0flow and exclude pixels with a low maximum probability since they indicate lower prediction confidence. Thus, the class-wise threshold \ud835\udf0f \ud835\udc57 is determined by\n\ud835\udf0f \ud835\udc57 =\n{ \ud835\udc66max \ud835\udc57 \u00b7 \ud835\udc5f%, if \ud835\udc66max \ud835\udc57 > \ud835\udf0flow\n\ud835\udc66max \ud835\udc57\n, otherwise (21)\nwhere the ratio \ud835\udc5f and the low bound \ud835\udf0flow are parameters. Third, we select pixels in each class by \ud835\udf0f \ud835\udc57 , i.e., pixels exceeding \ud835\udf0f \ud835\udc57 are selected:\n\ud835\udc5a\ud835\udc56 \ud835\udc57 = { 1, if \ud835\udc66\ud835\udc56 \ud835\udc57 > \ud835\udf0f \ud835\udc57 0, otherwise .\n(22)\nThe generation of the pseudo label is straightforward by using Eqs. (2) and (3). The refined class-aware pseudo labels are attained by multiplying them, i.e.,\ud835\udc5a\ud835\udc56 \ud835\udc57 \ud835\udc61\ud835\udc56 \ud835\udc57 , as used in Eqs. (18) and (19). Our CPLG strategy considers the learning status and difficulties of different classes by adjusting thresholds for each class. As a result, we select useful pixels with low thresholds for training, which enhances the accuracy of challenging classes."
        },
        {
            "heading": "3.5 Overall Algorithm",
            "text": "We propose a novel approach to train a model with both labeled and unlabeled images. Training with labeled images is straightforward using a supervised cross-entropy loss function. However, our primary focus is on training with unlabeled images. Fig. 2 illustrates how we combine two distinct learning strategies for the unlabeled images: contrastive learning and weak-to-strong pseudo supervision. These two strategies are implemented at both the low-level image and high-level feature. To ensure weak-to-strong consistency, we use similar model parameters in each level, which is either obtained through EMA or fully shared. During training, dual-level\nSiamese Structure Network (DSSN) architecture receives low-level and high-level augmentations of the unlabeled data.\nIn this section, we present the DSSN algorithm, which is illustrated in Algorithm 1. It takes a small fraction of labeled data and a large fraction of unlabeled data as input to train the model. The supervised loss between the model prediction on labeled data and the ground truth is computed using Eq. (1). Subsequently, the lowlevel and high-level contrastive learning losses are calculated using Eqs. (10) and (14), respectively. We then compute the weak-tostrong pseudo-supervision loss using Eqs. (18) and (19). The overall loss term is formulated as follows:\nL = Lsup + \ud835\udefe1 ( L\ud835\udc59\ud835\udc60cl + L \u210e\ud835\udc60 cl ) + \ud835\udefe2 ( L (\ud835\udc59 )w2s + L (\u210e) w2s ) , (23)\nwhere \ud835\udefe1 and \ud835\udefe2 are the trade-off weight. Finally, we update the student model and the teacher model by using the error backpropagation algorithm and EMA, respectively.\nAlgorithm 1 The DSSN algorithm.\n1: Input: D = {D\ud835\udc62 ,D\ud835\udc59 }, and batch size \ud835\udc4f. 2: Output: (\ud835\udf03 \u2032, \ud835\udf11\u2032) . 3: Initialization: \ud835\udc52\ud835\udc5d\ud835\udc5c\ud835\udc50\u210e = 0, \ud835\udc52\ud835\udc5d\ud835\udc5c\ud835\udc50\u210emax, and (\ud835\udf03, \ud835\udf11) . 4: while \ud835\udc52\ud835\udc5d\ud835\udc5c\ud835\udc50\u210e \u2264 \ud835\udc52\ud835\udc5d\ud835\udc5c\ud835\udc50\u210emax do 5: for mini-batch samples in D do 6: Feed the samples into DSSN for forward propagation ; 7: Update Lsup by Eq. (1) ; 8: Update L\ud835\udc59\ud835\udc60cl and L \u210e\ud835\udc60 cl by Eqs. (10) and (14) ; 9: Update L (\ud835\udc59 )w2s and L (\u210e) w2s by Eqs. (18) and (19) ;\n10: Update L by Eq. (23) ; 11: Update (\ud835\udf03, \ud835\udf11) by back propagation of \u2211\ud835\udc4f L ; 12: Update (\ud835\udf03 \u2032, \ud835\udf11\u2032) by Eq. (17) ; 13: \ud835\udc52\ud835\udc5d\ud835\udc5c\ud835\udc50\u210e = \ud835\udc52\ud835\udc5d\ud835\udc5c\ud835\udc50\u210e + 1 ; 14: end for 15: end while"
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "In this section, we first present the details of the experiments. Second, we compare the proposed DSSN method to the recent stateof-the-art (SOTA) approaches to the SSS task. Third, we conduct extensive ablation experiments to demonstrate the effectiveness and robustness of the proposed method."
        },
        {
            "heading": "4.1 Experimental setup",
            "text": "Datasets. We evaluate the proposed method on two classical semantic segmentation datasets, i.e., PASCAL VOC 2012 [9] and Cityscapes [5]. In particular, PASCAL VOC 2012 [9] has 20 classes of objects and 1 class of background. The standard training, validation and test sets consist of 1,464, 1449 and 1,456 images, respectively. Following the previous work [3, 19, 33], we also use augmented set SBD [14] (9,118 images) and original training set (1,464 images) as our full training set (10,582 images). The labels from the SBD [14] dataset are noise-prone and of low quality. Cityscapes [5] has 19 semantic classes and is mostly intended for understanding urban scenes. It consists of 500 validation images, 1,525 test images, and 2,975 training images. All of the images have well-annotated masks.\nFor a fair comparison with the benchmarks, we follow the partition procedure of CPS [3]. Specifically, the training set is divided into two partitions by randomly sampling 1/2, 1/4, 1/8, and 1/16 of the total set as the labeled samples and the remaining images as the unlabeled for the blended set.\nImplementation details. Following the previous benchmarks CPS [3], we adopt DeepLab v3+ [2] based on ResNet [15] as the segmentation network for a fair comparison. The backbone i.e., ResNet, is initialized with the weights pre-trained on ImageNet [7]. The segmentation heads are randomly initialized. During training, each mini-batch contains eight labeled and eight unlabeled images. The stochastic gradient descent (SGD) optimizer is used, and the initial learning rates are set to 0.002 and 0.005 for the PASCAL VOC 2012 and Cityscapes, respectively. In accordance with other works [3, 26], we employ the following polynomial to decrease the learning rate while training: (1 \u2212 \ud835\udc52\ud835\udc5d\ud835\udc5c\ud835\udc50\u210e/\ud835\udc52\ud835\udc5d\ud835\udc5c\ud835\udc50\u210emax)0.9. The model is trained for 100 epochs on PASCAL VOC 2012 and 240 epochs for Cityscapes. For weak augmentations, we adopt the same operation as ST++[33], where the training images are random flipping and resizing (between 0.5 and 2.0 times), followed by a random crop operation to the resolutions of 513 \u00d7 513 and 801 \u00d7 801 for the two datasets, respectively. We employ several strong augmentation, including random color-jitter, grayscale, Gaussian blur, etc. For strong feature augmentation, we apply a random dropout of 50% on features from the encoder. The unsupervised trade-off weights \ud835\udefe1 and \ud835\udefe2 are set as 0.01 and 0.25. In CPLG, \ud835\udc5f is set to 96% and \ud835\udf0flow is 0.92, respectively.\nAdditionally, we also apply CutMix [34] data augmentation to the student model images. The EMA smoothing factor \ud835\udefc is set as 0.996. We follow U2PL [30], the supervised loss is cross-entropy on PASCAL, and for Cityscapes the cross-entropy loss is replaced by the online hard example mining loss.\nEvaluation.We use the mean of Intersection-over-Union(mIoU) for the validation set to evaluate the segmentation performance for both datasets. Following the previous works [3, 33], we employ the sliding evaluation to examine the efficacy of our model on the validation images from Cityscapes with a resolution of 1024\u00d72048."
        },
        {
            "heading": "4.2 Comparison to SOTA Methods",
            "text": "To demonstrate the superiority of our proposed DSSN method, we conduct a comparison with the current state-of-the-art methods across various settings. All results are reported on the validation set for both PASCAL VOC and Cityscapes datasets. Additionally, we present the corresponding baseline at the top of the table, representing the results of purely supervised learning trained on the same labeled data. To ensure a fair comparison, all methods employed the DeepLab v3+ architecture.\nPASCAL VOC 2012.We report results of our experiments on the PASCAL VOC 2012 validation dataset in Tables 1 and 2, where we evaluate the mean Intersection over Union (mIoU) for different proportions of labeled samples. Additionally, we present the corresponding baseline at the top of the table, representing the results of purely supervised learning trained on the same labeled data.\nTable 1 presents results on the classic PASCAL VOC 2012 dataset. It shows our method significantly outperforms current state-of-theart methods. When employing ResNet-101 as the backbone, DSSN\nattains a 5.18% performance gain on the 1/16(92) split which surpass the performance obtain by the (1/3)183 data split in the prior study. Even with more labeled data, the performance differences become less evident; however, the proposed method still demonstrates performance improvements of 2.21% with 1/2 fine annotations over the previous SOTAs.\nTable 2 illustrates the results on blender PASCAL VOC 2012 Dataset. Our method shows significant improvement on the 1/16, 1/8, 1/4, and 1/2 splits with ResNet-50, compared to the baseline, with improvements of 15.51%, 10.1%, 6.73%, and 3.57%, respectively. Similarly, with ResNet-101, our method achieves improvements of 12.9%, 9.18%, 6.65%, and 3.01% under the same partitions. Especially, our method shows significant improvements when the ratio of labeled data becomes smaller, such as under 1/8 or 1/16\npartition protocols. In particular, when the labeled data is extremely limited,e.g., on the 1/16 partitions, our method achieves remarkable increases of 15.51% and 12.9% compared to the baseline with ResNet-50 and ResNet-101 as the backbone networks, respectively. Furthermore, our method demonstrates a considerable improvement over the previous state-of-the-art PS-MT [22], achieving a margin of 3.88% with ResNet-50 as the backbone, and 1.7% under the 1/8 partition protocol.\nCityscapes. In Table 3, we can see that our method consistently outperforms the supervised baseline by a significant margin, achieving improvements of 12.11%, 7.11%, 4.95%, and 2.13%with ResNet-50 under 1/16, 1/8, 1/4, and 1/2 partition protocols, respectively. Similarly, with ResNet-101, our method shows improvements of 10.22%, 5.38%, 3.62%, and 1.58% under 1/16, 1/8, 1/4, and 1/2 partition protocols, respectively. Furthermore, our method outperforms all other state-of-the-art methods across various settings. Specifically, under 1/8, 1/4, and 1/2 partitions, DSSN achieves a 1.55%, 1.13%, and 1.09% improvement over the previous state-of-the-art PS-MT [22] using ResNet-50, and a 1.29%, 1.02%, and 0.49% improvement using ResNet-101, respectively.\nWe evaluate DSSN using ResNet-50 on a 1/30 data split, which contained only 100 labeled images. As illustrated in Fig. 3, DSSN outperforms the current state-of-the-art significantly. This result indicates that our method effectively utilizes the unlabeled data through contrastive learning and the class-aware pseudo-label selection strategy (CPLG). Besides, although ReCo [21] and U2PL[30] try to construct positive and negative pairs to use contrastive learning, the result shows our DSSN outperform them significantly.\nUpon comparing performance on classic PASCAL VOC 2012 and blended training set, we observe that the quality of labeled samples is important. For example, DSSN achieves an exceptional performance of 80.61% by utilizing only 732 high-quality labels. However, even with significantly more labels (5291) from the blended dataset, a comparable score of 80.61% cannot be achieved."
        },
        {
            "heading": "4.3 Ablation Studies",
            "text": "In this subsection, we discuss the contribution of each component to our framework using ResNet-101 and a 1/8 labeled ratio on PASCAL VOC 2012 dataset.\nEffectiveness of theDSSN components.We conduct a step-bystep ablation study of each component to comprehensively assess their effectiveness. Table 4 presents the results of our study.Without our proposed dual-Level contrastive learning and CPLG, applying a plain consistency method yields an accuracy of 76.12%. However, employing dual-level contrastive learning leads to an accuracy of 78.33%, while the proposed CPLG results in 78.70%. Combining both dual-level contrastive learning and CPLG produces the highest accuracy of 79.58%, demonstrating the effectiveness of each component in the proposed DSSN method.\nEffectiveness of contrastive Learning. In our study, we incorporate both low-level and high-level contrastive learning in our dual-level contrastive learning approach. Table 5 presents the results of our study. Without the use of both low-level contrastive and high-level contrastive, the accuracy was 78.33%. Using low-level contrastive alone results in a 0.57% improvement, while using highlevel contrastive alone improves the accuracy by 0.86%. Notably,\nusing both low-level and high-level contrastive further improves the accuracy by 1.25%, which shows the efficacy of our method.\nEffectiveness of CPLG. As discussed in \u00a73.4, the CPLG strategy considers difficulties of different classes and long-tailed classes, instead of using a fixed threshold during the pseudo-label generation. To test our method against a fixed threshold, we conduct experiments using a fixed threshold. Fig. 4 shows that our strategy outperforms using a fixed threshold of 0.96 and 0.92 since we set \ud835\udc5f to 0.96 and \ud835\udf0flow to 0.92 in CPLG. This finding further highlights\nthe effectiveness of our proposed DSSN method. We chose these specific thresholds because, following our experiments, we establish 0.92 as the lowest threshold and used 0.96 as the factor for the maximum probability value. Additionally, Fig. 5 presents the mean Intersection over Union (mIoU) values of classes with long tails and those that are hard to learn during training, which demonstrates the effectiveness of our CPLG strategy.\nQualitative Results. In Figs. 6 and 7, we present the qualitative results of our study on the PASCAL VOC 2012 validation set.\nDSSN is based on the DeepLab v3+ with ResNet-101 network and a 1/8 ratio. The integration of contrastive learning into our method improve the performance of our model for contour and ambiguous regions, while also enhancing the accuracy of some scenarios, as illustrated in Fig. 6. Furthermore, our proposed CPLG achieved substantial precision in certain classes that are typically challenging to learn, as illustrated in Fig. 7.\nIn summary, DSSN has shown significant improvements in various scenarios. By utilizing pixel-wise constrative learning for two levels during training, DSSN can effectively handle large amounts of unlabeled data while still delivering strong performance."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "In this paper, we introduce DSSN, a novel method that utilizes pixel-wise contrastive learning to address the SSS problem. DSSN is equipped with a dual-level structure that can effectively leverage unlabeled data. In DSSN, both contrastive learning and weak-tostrong consistency learning are utilized to maximize the utilization of available unlabeled data. Furthermore, we propose a class-aware\npseudo label selection strategy that generates high-quality pseudo labels and significantly improves performance on long-tailed classes without incurring additional computation. Our proposed DSSN achieves state-of-the-art performance on two benchmarks, and the effectiveness of our proposed novelties is confirmed by the ablation study."
        }
    ],
    "title": "Improving Semi-Supervised Semantic Segmentation with Dual-Level Siamese Structure Network",
    "year": 2023
}