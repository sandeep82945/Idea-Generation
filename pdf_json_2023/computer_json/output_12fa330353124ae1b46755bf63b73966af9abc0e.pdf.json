{
    "abstractText": "Training energy-based models (EBMs) on discrete spaces is challenging because sampling over such spaces can be difficult. We propose to train discrete EBMs with energy discrepancy (ED), a novel type of contrastive loss functional which only requires the evaluation of the energy function at data points and their perturbed counter parts, thus not relying on sampling strategies like Markov chain Monte Carlo (MCMC). Energy discrepancy offers theoretical guarantees for a broad class of perturbation processes of which we investigate three types: perturbations based on Bernoulli noise, based on deterministic transforms, and based on neighbourhood structures. We demonstrate their relative performance on lattice Ising models, binary synthetic data, and discrete image data sets.",
    "authors": [
        {
            "affiliations": [],
            "name": "Tobias Schr\u00f6der"
        },
        {
            "affiliations": [],
            "name": "Zijing Ou"
        },
        {
            "affiliations": [],
            "name": "Yingzhen Li"
        },
        {
            "affiliations": [],
            "name": "Andrew Duncan"
        }
    ],
    "id": "SP:6e8386d67a58ccd4a78c4fdb1d62eea037158d94",
    "references": [
        {
            "authors": [
                "M.A. Carreira-Perpinan",
                "G. Hinton"
            ],
            "title": "On contrastive divergence learning",
            "venue": "In International workshop on artificial intelligence and statistics,",
            "year": 2005
        },
        {
            "authors": [
                "C. Ceylan",
                "M.U. Gutmann"
            ],
            "title": "Conditional noise-contrastive estimation of unnormalised models",
            "venue": "In Dy,",
            "year": 2018
        },
        {
            "authors": [
                "H. Dai",
                "R. Singh",
                "B. Dai",
                "C. Sutton",
                "D. Schuurmans"
            ],
            "title": "Learning discrete energy-based models via auxiliary-variable local exploration",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "B. Eikema",
                "G. Kruszewski",
                "C.R. Dance",
                "H. Elsahar",
                "M. Dymetman"
            ],
            "title": "An approximate sampler for energy-based models with divergence diagnostics",
            "venue": "Transactions of Machine Learning Research,",
            "year": 2022
        },
        {
            "authors": [
                "P. Emami",
                "A. Perreault",
                "J. Law",
                "D. Biagioni",
                "P.S. John"
            ],
            "title": "Plug & play directed evolution of proteins with gradient-based discrete MCMC",
            "venue": "Machine Learning: Science and Technology,",
            "year": 2023
        },
        {
            "authors": [
                "W. Grathwohl",
                "K. Swersky",
                "M. Hashemi",
                "D. Duvenaud",
                "C.J. Maddison"
            ],
            "title": "Oops I took a gradient: Scalable sampling for discrete distributions",
            "venue": "arXiv preprint arXiv:2102.04509,",
            "year": 2021
        },
        {
            "authors": [
                "F. Gray"
            ],
            "title": "Pulse code communication",
            "venue": "United States Patent Number 2632058,",
            "year": 1953
        },
        {
            "authors": [
                "A. Gretton",
                "K.M. Borgwardt",
                "M.J. Rasch",
                "B. Sch\u00f6lkopf",
                "A. Smola"
            ],
            "title": "A kernel two-sample test",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2012
        },
        {
            "authors": [
                "M. Gutmann",
                "A. Hyv\u00e4rinen"
            ],
            "title": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models",
            "venue": "In Proceedings of the thirteenth international conference on artificial intelligence and statistics,",
            "year": 2010
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "G.E. Hinton"
            ],
            "title": "Training products of experts by minimizing contrastive divergence",
            "venue": "Neural computation,",
            "year": 2002
        },
        {
            "authors": [
                "A. Hyv\u00e4rinen"
            ],
            "title": "Some extensions of score matching",
            "venue": "Computational statistics & data analysis,",
            "year": 2007
        },
        {
            "authors": [
                "D.P. Kingma",
                "J. Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980,",
            "year": 2014
        },
        {
            "authors": [
                "M. Lazaro-Gredilla",
                "A. Dedieu",
                "D. George"
            ],
            "title": "Perturb-and-max-product: Sampling and learning in discrete energy-based models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "M. Liu",
                "H. Liu",
                "S. Ji"
            ],
            "title": "Gradient-guided importance sampling for learning binary energy-based models",
            "year": 2023
        },
        {
            "authors": [
                "S. Lyu"
            ],
            "title": "Unifying non-maximum likelihood learning objectives with minimum KL contraction",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2011
        },
        {
            "authors": [
                "S. Lyu"
            ],
            "title": "Interpretation and generalization of score matching",
            "venue": "arXiv preprint arXiv:1205.2629,",
            "year": 2012
        },
        {
            "authors": [
                "D. Majerek",
                "W. Nowak",
                "W. Ziba"
            ],
            "title": "Conditional strong law of large number",
            "venue": "International Journal of Pure and Applied Mathematics, 20,",
            "year": 2005
        },
        {
            "authors": [
                "C. Meng",
                "K. Choi",
                "J. Song",
                "S. Ermon"
            ],
            "title": "Concrete score matching: Generalized score matching for discrete data",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "R.M. Neal"
            ],
            "title": "Annealed importance sampling",
            "venue": "Statistics and computing,",
            "year": 2001
        },
        {
            "authors": [
                "E. Nijkamp",
                "M. Hill",
                "Zhu",
                "S.-C",
                "Y.N. Wu"
            ],
            "title": "Learning non-convergent non-persistent short-run MCMC toward energy-based model",
            "year": 1904
        },
        {
            "authors": [
                "G. Papandreou",
                "A.L. Yuille"
            ],
            "title": "Perturb-and-map random fields: Using discrete optimization to learn and sample from energy models",
            "venue": "In 2011 International Conference on Computer Vision,",
            "year": 2011
        },
        {
            "authors": [
                "P. Ramachandran",
                "B. Zoph",
                "Q.V. Le"
            ],
            "title": "Searching for activation functions",
            "venue": "arXiv preprint arXiv:1710.05941,",
            "year": 2017
        },
        {
            "authors": [
                "B. Rhodes",
                "M. Gutmann"
            ],
            "title": "Enhanced gradient-based MCMC in discrete spaces",
            "venue": "arXiv preprint arXiv:2208.00040,",
            "year": 2022
        },
        {
            "authors": [
                "T. Schr\u00f6der",
                "Z. Ou",
                "J.N. Lim",
                "Y. Li",
                "S.J. Vollmer",
                "A.B. Duncan"
            ],
            "title": "Energy discrepancies: A score-independent loss for energy-based models, 2023",
            "venue": "URL https://arxiv.org/abs/2307",
            "year": 2023
        },
        {
            "authors": [
                "H. Sun",
                "H. Dai",
                "D. Schuurmans"
            ],
            "title": "Optimal scaling for locally balanced proposals in discrete spaces",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "H. Sun",
                "H. Dai",
                "W. Xia",
                "A. Ramamurthy"
            ],
            "title": "Path auxiliary proposal for MCMC in discrete space",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "H. Sun",
                "H. Dai",
                "B. Dai",
                "H. Zhou",
                "D. Schuurmans"
            ],
            "title": "Discrete Langevin samplers via Wasserstein gradient flow",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2023
        },
        {
            "authors": [
                "T. Tieleman"
            ],
            "title": "Training restricted boltzmann machines using approximations to the likelihood gradient",
            "venue": "In Proceedings of the 25th international conference on Machine learning,",
            "year": 2008
        },
        {
            "authors": [
                "A. van den Oord",
                "Y. Li",
                "O. Vinyals"
            ],
            "title": "Representation learning with contrastive predictive coding",
            "venue": "CoRR, abs/1807.03748,",
            "year": 2018
        },
        {
            "authors": [
                "G. Zanella"
            ],
            "title": "Informed proposals for local MCMC in discrete spaces",
            "venue": "Journal of the American Statistical Association,",
            "year": 2020
        },
        {
            "authors": [
                "D. Zhang",
                "N. Malkin",
                "Z. Liu",
                "A. Volokhova",
                "A. Courville",
                "Y. Bengio"
            ],
            "title": "Generative flow networks for discrete probabilistic modeling",
            "venue": "arXiv preprint arXiv:2202.01361,",
            "year": 2022
        },
        {
            "authors": [
                "R. Zhang",
                "X. Liu",
                "Q. Liu"
            ],
            "title": "A Langevin-like sampler for discrete distributions",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "y/Kx\u2032 . C"
            ],
            "title": "Consistency of our Approximation The following proof is similar to Schr\u00f6der et al. (2023)",
            "venue": "We first restate the consistency result: Theorem",
            "year": 2023
        },
        {
            "authors": [
                "EDq(pdata"
            ],
            "title": "Lq,M(N),w(U)| < \u03b5 almost surely. D Related Work Contrastive loss functions Our work is based on an unpublished work on energy discrepancies in the continuous case (Schr\u00f6der et al., 2023)",
            "year": 2023
        },
        {
            "authors": [
                "van den Oord"
            ],
            "title": "A similar loss has been suggested before as KL contraction divergence (Lyu, 2011), however, only for its theoretical properties. Interestingly, the structure of the stabilised energy discrepancy loss shares similarities with other contrastive losses such as Ceylan",
            "year": 2018
        },
        {
            "authors": [
                "Dai"
            ],
            "title": "Another sampling free approach for training discrete EBMs is ratio matching (Hyv\u00e4rinen",
            "venue": "Eikema et al",
            "year": 2012
        },
        {
            "authors": [
                "Zhang"
            ],
            "title": "Training Ising Models Table 3: Mean negative log-RMSE (higher is better) between the learned connectivity matrix J\u03c6 and the true matrix J for different values of D and \u03c3. The results of baselines",
            "year": 2022
        },
        {
            "authors": [
                "Grathwohl"
            ],
            "title": "Experimental Details",
            "year": 2022
        },
        {
            "authors": [
                "Grathwohl"
            ],
            "title": "mizer with a learning rate of 0.0001 and a batch size of 256. Following Zhang et al. (2022a), all models are trained with an l1 regularization with a coefficient",
            "venue": "in {10,",
            "year": 2021
        },
        {
            "authors": [
                "Zhang"
            ],
            "title": "2022a), which adopts the exponential Hamming kernel with 0.1 bandwidth. Moreover, the reported performances are averaged over 10 repeated estimations, each with 4, 000 samples, which are drawn from the learned energy function via Gibbs sampling",
            "year": 2022
        },
        {
            "authors": [],
            "title": "ED-Pool and ED-Grid), energy discrepancy demonstrates rapid divergence and fails to converge. Additionally, we find that increasing M can address this issue to some extent and introducing a non-zero value for w can significantly stabilize the convergence, even with M = 1. Moreover, larger w tends to produce a flatter estimated energy landscapes, which also aligns with the findings in continuous scenarios of energy discrepancy",
            "year": 2023
        },
        {
            "authors": [
                "Zhang"
            ],
            "title": "Discrete Image Modelling Experimental Details",
            "venue": "We choose M = 32,",
            "year": 2017
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Building large-scale probabilistic models for discrete data is a critical challenge in machine learning for its broad applicability to perform inference and generation tasks on images, text, or graphs. Energy-based models (EBMs) are a class of particularly flexible models pebm \u221d exp(\u2212U), where the modelling of the energy function U through a neural network function can be taylored to the data set of interest. However, EBMs are notoriously difficult to train due to the intractability of their normalisation.\nThe most popular paradigm for the training of EBMs is the contrastive divergence (CD) algorithm (Hinton, 2002) which performs approximate maximum likelihood estimation by using short-run Markov Chain Monte Carlo (MCMC) to approximate intractable expectations with respect to pebm. The success of CD has lead to rich research results on sampling from discrete distributions to enable fast and accurate estimation of the EBM (Zanella, 2020; Grathwohl et al., 2021; Zhang et al., 2022b; Sun et al., 2022b,a, 2023; Emami et al., 2023). However, training EBMs with CD remains challenging: Firstly, discrete probabilistic models often exhibit a large number of spurious modes which are difficult to explore even for the most advanced sampling algorithms. Secondly, CD lacks theoretical guarantees due to short run MCMC (Carreira-Perpinan & Hinton, 2005) and often times leads to malformed energy landscapes (Nijkamp et al., 2019).\nWe propose the usage of a new type of loss function called Energy Discrepancy (ED) (Schr\u00f6der et al., 2023) for the training of energy-based models on discrete spaces. The definition of ED only requires the evaluation of the EBM on positive and contrasting, negative samples. Unlike CD,\n\u2217Correspondence to: Tobias Schr\u00f6der, t.schroeder21@imperial.ac.uk \u2020Code: https://github.com/J-zin/discrete-energy-discrepancy, z.ou22@imperial.ac.uk\nICML 2023 Workshop: Sampling and Optimization in Discrete Space (SODS 2023).\nar X\niv :2\n30 7.\n07 59\n5v 1\n[ st\nat .M\nL ]\n1 4\nJu l 2\nenergy discrepancy does not require sampling from the model during training, thus allowing for fast training with theoretical guarantees. We demonstrate the effectiveness of ED by training Ising models, estimating discrete densities, and modelling discrete images in high-dimensions (see Figure 1 for an illustration)."
        },
        {
            "heading": "2 Energy Discrepancies",
            "text": "Energy discrepancies are based on the idea that if information is processed through a channel Q then information will be lost. Mathematically, this is expressed through the data processing inequality KL(Qpdata \u2225 Qpebm) \u2264 KL(pdata \u2225 pebm). Consequently, the difference of the two KL divergences forms a valid loss for density estimation (Lyu, 2011). Retaining only terms that depend on the energy function U results in the energy discrepancy (Schr\u00f6der et al., 2023):\nDefinition 1 (Energy Discrepancy). Let pdata be a positive density on a measure space (X , dx)3and let q(y|x) be a conditional probability density. Define the contrastive potential induced by q as\nUq(y) := \u2212 log \u2211 x\u2032\u2208X q(y|x\u2032) exp(\u2212U(x\u2032)) (1)\nWe define the energy discrepancy between pdata and U induced by q as\nEDq(pdata, U) := Epdata(x)[U(x)]\u2212 Epdata(x)Eq(y|x)[Uq(y)].\nThe validity of this loss functional is given by the following non-parametric estimation result, previously stated in Schr\u00f6der et al. (2023):\nTheorem 1. Let pdata be a positive probability density on (X , dx). Assume that for all x \u223c pdata and y \u223c q(y|x), Var(x|y) > 0. Then, the energy discrepancy EDq is functionally convex in U and has, up to additive constants, a unique global minimiser U\u2217 = argminEDq(pdata, U). Furthermore, this minimiser is the Gibbs potential for the data distribution, i.e. pdata \u221d exp(\u2212U\u2217).\nWe give the proof of Theorem 1 in Appendix A.1. The perturbation q can be chosen quite generally as long as it can be guaranteed that computing y comes at a loss of information which mathematically is expressed through the variance of recovering x from y \u223c q(y|x) being positive. In the next section, we propose some practical choices for q."
        },
        {
            "heading": "2.1 Training Discrete Energy-Based Models with Energy Discrepancy",
            "text": "The perturbation process q needs to be chosen under the following considerations: 1) The contrastive potential Uq(y) has a numerically tractable approximation. 2) The negative samples obtained through q are informative for training the EBM when only finite amounts of data are available. We propose three categories for constructing perturbative processes:\nBernoulli Perturbation. For \u03b5 \u2208 (0, 1), let \u03be \u223c Bernoulli(\u03b5)d. On X = {0, 1}d, consider the perturbation y = x+ \u03bemod(2) which induces a symmetric transition density q(y \u2212 x) on {0, 1}d. Due to the symmetry of q, we can then write the contrastive potential as\nUbernoulli(y) = \u2212 log \u2211 x\u2032\u2208X q(y \u2212 x\u2032) exp(\u2212U(x\u2032)) = \u2212 logEx\u2032\u223cq(y\u2212x\u2032)[exp(\u2212U(x\u2032))]\nThe expectation on the right hand side can now be approximated via sampling M Bernoulli random variables \u03bej and taking the remainder of (y + \u03bej)/2. We denote this method as ED-Bern.\nDeterministic Transformation. The perturbation q can also be defined through a deterministic information loosing map g : X \u2192 Y , where the space Y may or may not be equal to X depending on the choice of g. The contrastive potential can be expressed in terms of the preimage of g, i.e.\nUg(y) = \u2212 log \u2211\n{x\u2032:g(x\u2032)=y}\nexp(\u2212U(x\u2032)) = \u2212 logEx\u2032\u223cU({g\u22121(y)})[exp(\u2212U(x\u2032))]\u2212 c\n3On discrete spaces dx is assumed to be a counting measure. On continuous spaces X , the appearing sums and expectations turn into integrals with respect to the Lebesgue measure\nwith c = log |{g\u22121(y)}|. Again, the contrastive potential can be approximated through sampling M instances from the uniform distribution over the set {x\u2032 : g(x\u2032) = y}. In our numerical experiments, we focus on the mean-pooling transform gpool whose inverse are block-wise permutations. For details, see Appendix C.2. We denote this method as ED-Pool.\nNeighbourhood-based Transformation. Finally, inspired from concrete score matching (Meng et al., 2022), we may define energy discrepancies based on neighbourhood maps x 7\u2192 N (x) \u2208 XK which assign each point x \u2208 X a set of K neighbours4. We define the forward perturbation q(y|x) by selecting neighbours y \u223c U(N (x)) uniformly at random. Conversely, the contrastive potential can be expressed in terms of the inverse neighbourhood y 7\u2192 N\u22121(y) \u2208 XK , i.e. the set of points that have y to their neighbour. We then obtain for the contrastive potential\nUN (y) = \u2212 log 1\nK \u2211 x\u2032\u2208X :y\u2208N (x\u2032) exp(\u2212U(x\u2032)) = \u2212 logEx\u2032\u223cU({N\u22121(y)})[exp(\u2212U(x\u2032))] .\nIn practice, we choose the grid neighbourhood (Appendix C.3) and denote this method by ED-Grid.\nStabilising Training. Above schemes permit the approximation of the contrastive potential from M samples which are generated by first sampling y \u223c q(y|x), after which we compute M approximate recoveries xj\u2212. The full loss can then be constructed for each data point x+ \u223c pdata by calculating log \u2211M\nj=1 exp(U(x+)\u2212 U(x j \u2212))\u2212 log(M) using the numerically stabilised logsumexp function. In\npractice, however, we find that this estimator for energy discrepancy is biased due to the logarithm and can exhibit high variance. To stabilise training, we introduce an offset for the logarithm which introduces a deterministic lower bound for the loss. This yields the energy discrepancy loss function\nLq,M,w(U) := 1\nN N\u2211 i=1 log w + M\u2211 j=1 exp(U(xi+)\u2212 U(x i,j \u2212 )) \u2212 log(M) (2) with xi+ \u223c pdata. In Appendix C.5 we proof that this approximation is consistent for any fixed w: Theorem 2. For every \u03b5>0 there exist N,M \u2208N such that |Lq,M,w(U)\u2212EDq(pdata, U)|<\u03b5 a.s..\n3 Experiments\nTraining Ising Models. We evaluate the proposed methods on the lattice Ising model, which has the form of\np(x) \u221d exp(xTJx), x \u2208 {\u22121, 1}D, where J = \u03c3AD with \u03c3 \u2208 R and AD being the adjacency matrix of a D\u00d7D\n4We are making the assumption that the numbers of neighbours is the same for each point. A more general case is discussed in Appendix C.4.\ngrid. Following Zhang et al. (2022a), we generate training data through Gibbs sampling and use the generated data to fit a symmetric matrix J via energy discrepancy. In Figure 2, we consider D = 10\u00d7 10 grids with \u03c3 = 0.2 and illustrate the learned matrix J using a heatmap. It can be seen that the variants of energy discrepancy can identify the pattern of the ground truth, confirming the effectiveness of our methods. We defer experimental details and quantitative results comparing with baselines to Appendix E.1.\nDiscrete Density Estimation. In this experiment, we follow the experimental setting of Dai et al. (2020); Zhang et al. (2022a), which aims to model discrete densities over 32-dimensional binary data that are discretisations of continuous densities on the plane (see Figure 4). Specifically, we convert each planar data point x\u0302 \u2208 R2 to a binary data point x \u2208 {0, 1}32 via Gray code (Gray, 1953). Consequently, the models face the challenge of modeling data in a discrete space, which is particularly difficult due to the non-linear transformation from x\u0302 to x.\nWe compare our methods to three baselines: PCD (Tieleman, 2008), ALOE+ (Dai et al., 2020), and EB-GFN (Zhang et al., 2022a). The experimental details are given in Appendix E.2. For qualitative evaluation, we visualise the energy landscapes learned by our methods in Figure 3. It shows that energy discrepancy is able to faithfully model multi-modal distributions and accurately learn the sharp edges present in the data support. For further qualitative comparisons, we refer to the energy landscapes of baseline methods presented in Figure C.2 of Zhang et al. (2022a). Moreover, we quantitatively evaluate different methods in Table 1 by showing the negative log-likelihood (NLL) and the exponential Hamming MMD (Gretton et al., 2012). Perhaps surprisingly, we find that energy discrepancy outperforms the baselines on most settings, despite not requiring MCMC simulation like PCD or training an additional variational network like ALOE and EB-GFN. A possible explanation for this are biases introduced by short-run MCMC sampling in the case of PCD or non-converged variational proposals in ALOE. By definition, ED transforms the data distribution as well as the energy function which corrects for such biases.\nDiscrete Image Modelling. Here, we evaluate our methods in discrete high-dimensional spaces. Following the settings in Grathwohl et al. (2021); Zhang et al. (2022b), we conduct experiments on four different binary image datasets. Training details are given in Appendix E.3. After training, we adopt Annealed Importance Sampling (Neal, 2001) to estimate the log-likelihoood.\nThe baselines include persistent contrastive divergence with vanilla Gibbs sampling, Gibbs-WithGradient (Grathwohl et al., 2021, GWG), Generative-Flow-Network (Zhang et al., 2022a, GFN), and Discrete-Unadjusted-Langevin-Algorithm (Zhang et al., 2022b, DULA). The NLLs on the test set are reported in Table 2. We see that energy discrepancy yields comparable performances to the baselines, while ED-Pool is unable to capture the data distribution. We emphasise that energy discrepancy only requires M (here, M = 32) evaluations of the energy function per data point in parallel. This is notably fewer than contrastive divergence, which requires simulating multiple MCMC steps without parallelisation. We also visualise the generated samples in Figure 11, which showcase the diversity and high quality of the images generated by ED-Bern and ED-Grid. However, we observed that ED-Pool suffers from mode collapse."
        },
        {
            "heading": "4 Conclusion and Outlook",
            "text": "In this paper we demonstrate how energy discrepancy can be used for efficient and competitive training of energy-based models on discrete data without MCMC. The loss can be defined based on a large class of perturbative processes of which we introduce three types: noise, determinstic transform, and neighbourhood-based transform. Our results show that the choice of perturbation matters and motivates further research on effective choices depending on the data structure of interest.\nWe observe empirically that similarly to other contrastive losses, energy discrepancy shows limitations when the ambient dimension of X is significantly larger than the intrinsic dimension of the data. In these cases, training is aided significantly by a base distribution that models the lower-dimensional space populated by data. For this reason, the adoption of ED on new data sets or different data structures may require adjustments to the methodology such as learning appropriate base distributions and finding more informative perturbative transforms.\nFor future work, we are interested in how this work extends to highly structured data such as graphs or text. These settings may require a deeper understanding of how the perturbation influences the performance of ED and what is gained from gradient information in CD (Zhang et al., 2022b; Grathwohl et al., 2021) or ratio matching (Liu et al., 2023)."
        },
        {
            "heading": "Acknowledgements",
            "text": "TS would like to thank G.A. Pavliotis for insightful discussions leading up to the presented work. TS was supported by the EPSRC-DTP scholarship partially funded by the Ddepartment of Mathematics, Imperial College London. ZO was supported by the Lee Family Scholarship. ABD was supported by Wave 1 of The UKRI Strategic Priorities Fund under the EPSRC Grant EP/T001569/1 and EPSRC Grant EP/W006022/1, particularly the \u201cEcosystems of Digital Twins\u201d theme within those grants and The Alan Turing Institute. We thank the anonymous reviewer for their comments."
        },
        {
            "heading": "Appendix for \u201cTraining Discrete EBMs with Energy",
            "text": "Discrepancy\u201d"
        },
        {
            "heading": "Contents",
            "text": ""
        },
        {
            "heading": "A Abstract Proofs and Derivations 8",
            "text": "A.1 Proof of the Non-Parametric Estimation Theorem 1 . . . . . . . . . . . . . . . . . . . . . . . 8"
        },
        {
            "heading": "B Connections to other Methods 10",
            "text": "B.1 Connections of Energy Discrepancy with Contrastive Divergence . . . . . . . . . . . . . . . 10\nB.2 Derivation of Energy Discrepancy from KL Contractions . . . . . . . . . . . . . . . . . . . . 10"
        },
        {
            "heading": "C Sample Approximations of Energy Discrepancies 11",
            "text": "C.1 General Strategy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\nC.2 Mean Pooling Transform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\nC.3 Grid Neighborhood . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\nC.4 Directed Neighbourhood Structures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\nC.5 Consistency of our Approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12"
        },
        {
            "heading": "D Related Work 13",
            "text": ""
        },
        {
            "heading": "E More about Experiments 14",
            "text": "E.1 Training Ising Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\nE.2 Discrete Density Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\nE.3 Discrete Image Modelling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16"
        },
        {
            "heading": "A Abstract Proofs and Derivations",
            "text": ""
        },
        {
            "heading": "A.1 Proof of the Non-Parametric Estimation Theorem 1",
            "text": "In this subsection we give a formal proof for the uniqueness of minima of EDq(pdata, U) as a functional in the energy function U . We first reiterate the theorem as stated in the paper: Theorem 1. Let pdata be a positive probability density on (X , dx). Assume that for all x \u223c pdata and y \u223c q(y|x), Var(x|y) > 0. Then, the energy discrepancy EDq is functionally convex in U and has, up to additive constants, a unique global minimiser U\u2217 = argminEDq(pdata, U). Furthermore, this minimiser is the Gibbs potential for the data distribution, i.e. pdata \u221d exp(\u2212U\u2217).\nWe test energy discrepancy on the first and second order optimality conditions, i.e. we test that the first functional derivative of ED vanishes in U\u2217 and that the second functional derivative is positive definite. For uniqueness and well-definedness, we constrain the optimisation domain to the following set:\nG := { U : X 7\u2192 R such that exp(\u2212U) \u2208 L1(X ,dx) , U \u2208 L1(pdata) , and min\nx\u2208X U(x) = 0 } and require that there exists a U\u2217 \u2208 G such that exp(\u2212U\u2217) \u221d pdata. We now start with the following lemmata and then complete the proof of Theorem 1 in Corollary 1. Lemma 1. Let h \u2208 G be arbitrary. The first variation of EDq is given by\nd\nd\u03f5 EDq(pdata, U + \u03f5h) \u2223\u2223\u2223\u2223 \u03f5=0 = Epdata(x)[h(x)]\u2212 Epdata(x)Eq(y|x)EpU (z|y)[h(z)] (3)\nwhere pU (z|y) = q(y|z) exp(\u2212U(z))\u2211 z\u2032\u2208X q(y|z\u2032) exp(\u2212U(z\u2032)) .\nProof. We define the short-hand notation U\u03f5 := U + \u03f5h. The energy discrepancy at U\u03b5 reads\nEDq(pdata, U\u03f5) = Epdata(x)[U\u03f5(x)] + Epdata(x)Eq(y|x) [ log \u2211 z\u2208X q(y|z) exp(\u2212U\u03f5(z)) ] .\nFor the first functional derivative, we only need to calculate\nd d\u03f5 log \u2211 z\u2208X q(y|z) exp(\u2212U\u03f5(z)) = \u2211 z\u2208X \u2212q(y|z)h(z) exp(\u2212U\u03f5(z))\u2211 z\u2032\u2208X q(y|z\u2032) exp(\u2212U\u03f5(z\u2032)) = \u2212EpU\u03f5 (z|y)[h(z)]. (4)\nPlugging this expression into EDq(pdata, U\u03f5) and setting \u03f5 = 0 yields the first variation of EDq .\nLemma 2. The second variation of EDq is given by\nd2\nd\u03f52 EDq(pdata, U + \u03f5h) \u2223\u2223\u2223\u2223 \u03f5=0 = Epdata(x)Eq(y|x)VarpU (z|y)[h(z)].\nProof. For the second order term, we have based on equation 4 and the quotient rule for derivatives:\nd2 d\u03f52 log \u2211 z\u2208X q(y|z) exp(\u2212U\u03f5(z))\n=\n\u2211 z\u2208X q(y|z) exp(U\u03f5(z))h2(z) \u2211 z\u2032\u2208X q(y|z\u2032) exp(\u2212U\u03f5(z\u2032))(\u2211\nz\u2032\u2208X q(y|z\u2032) exp(\u2212U\u03f5(z\u2032)) )2\n\u2212 \u2211 z\u2208X q(y|z) exp(U\u03f5(z))h(z) \u2211\nz\u2032\u2208X q(y|z\u2032) exp(\u2212U\u03f5(z\u2032))h(z\u2032)(\u2211 z\u2032\u2208X q(y|z\u2032) exp(\u2212U\u03f5(z\u2032)) )2 = EpU\u03f5 (z|y)[h 2(z)]\u2212 EpU\u03f5 (z|y)[h(z)] 2 = VarpU\u03f5 (z|y)[h(z)] .\nWe obtain the desired result by interchanging the outer expectations with the derivatives in \u03f5.\nCorollary 1. Let c = minx\u2208X (\u2212 log pdata(x)). For U\u2217 = \u2212 log(pdata)\u2212 c \u2208 G it holds that d\nd\u03f5 EDq(pdata, U\n\u2217 + \u03f5h) \u2223\u2223\u2223\u2223 \u03f5=0 = 0\nd2\nd\u03f52 EDq(pdata, U\n\u2217 + \u03f5h) \u2223\u2223\u2223\u2223 \u03f5=0 > 0 for all h ,\nFurthermore, U\u2217 is the unique global minimiser of EDq(pdata, \u00b7) in G.\nProof. By definition, the variance is non-negative, i.e. for every h \u2208 G: d2\nd\u03f52 EDq(pdata, U + \u03f5h) \u2223\u2223\u2223\u2223 \u03f5=0 = VarpU (z|y)[h(z)] \u2265 0 .\nConsequently, the energy discrepancy is convex and an extremal point of EDq(pdata, \u00b7) is a global minimiser. We are left to show that the minimiser is obtained at U\u2217 and unique. First of all, we have for U\u2217:\nEpU\u2217 (z|y)[h(z)] = \u2211 z\u2208X q(y|z) exp(\u2212U\u2217(z))\u2211 z\u2032\u2208X q(y|z\u2032) exp(\u2212U\u2217(z\u2032)) h(z)\n= \u2211 z\u2208X q(y|z)pdata(z)\u2211 z\u2032\u2208X q(y|z\u2032)pdata(z\u2032) h(z).\nBy applying the outer expectations we obtain Epdata(x)Eq(y|x)EpU\u2217 (z|y)[h(z)] = \u2211 x\u2208X pdata(x) \u2211 y\u2208Y ( q(y|x) \u2211 z\u2208X ( q(y|z)pdata(z)\u2211 z\u2032\u2208X q(y|z\u2032)pdata(z\u2032) h(z) )) = \u2211 z\u2208X \u2211 y\u2208Y q(y|x)pdata(z)h(z)\n= Epdata(z)[h(z)],\nwhere we used that the marginal distributions \u2211\nx\u2208X pdata(x)q(y|x) cancel out and the conditional probability density integrates to one. This implies\nd\nd\u03f5 EDq(pdata, U\n\u2217 + \u03f5h) \u2223\u2223\u2223\u2223 \u03f5=0 = Epdata(z)[h(z)]\u2212 Epdata(z)[h(z)] = 0.\nfor all h \u2208 G. We now show that d2\nd\u03f52 EDq(pdata, U\n\u2217 + \u03f5h) \u2223\u2223\u2223\u2223 \u03f5=0 = Epdata(x)Eq(y|x)Varpdata(z|y)[h(z)] > 0 .\nAssume that the second variation was zero. Since the perturbed data distribution\u2211 x\u2208X pdata(x)q(y|x) is positive, the second variation at U\u2217 is zero if and only if the conditional variance Varpdata(z|y)[h(z)] = 0. Since U \u2217+\u03b5h \u2208 G, the function h can not be constant. By definition of\nthe conditional variance, h(z) must then be a deterministic function of y \u223c \u2211\nx\u2208X q(y|x)pdata(x). Since h was arbitrary, there exists a measurable map g such that z = g(y) and Varpdata(z|y)[z] = 0 which is a contradiction to our assumptions. Consequently, U\u2217 is the unique global minimiser of EDq which completes the statement in Theorem 1."
        },
        {
            "heading": "B Connections to other Methods",
            "text": "In this section, we follow Schr\u00f6der et al. (2023)."
        },
        {
            "heading": "B.1 Connections of Energy Discrepancy with Contrastive Divergence",
            "text": "The contrastive divergence update can be derived from an energy discrepancy when, for E\u03b8 fixed, q satisfies the detailed balance relation\nq(y|x) exp(\u2212E\u03b8(x)) = q(x|y) exp(\u2212E\u03b8(y)) . To see this, we calculate the contrastive potential induced by q: We have\n\u2212 log \u2211 x\u2032\u2208X q(y|x\u2032) exp(\u2212E\u03b8(x\u2032)) = \u2212 log \u2211 x\u2032\u2208X q(x\u2032|y) exp(\u2212E\u03b8(y)) = E\u03b8(y) .\nConsequently, the energy discrepancy induced by q is given by EDq(pdata, E\u03b8) = Epdata(x)[E\u03b8(x)]\u2212 Epdata(x)Eq(y|x)[E\u03b8(y)] . Updating \u03b8 based on a sample approximation of this loss leads to the contrastive divergence update\n\u2206\u03b8 \u221d 1 N N\u2211 i=1 \u2207\u03b8E\u03b8(xi)\u2212 1 N N\u2211 i=1 \u2207\u03b8E\u03b8(yi) yi \u223c q(\u00b7|xi)\nIt is important to notice that the distribution q depends on E\u03b8 and needs to adjusted in each step of the algorithm. For fixed q, EDq(pdata, E\u03b8) satisfies Theorem 1. This means that each step of contrastive divergence optimises a loss with minimiser E\u2217\u03b8 = \u2212 log pdata + c. However, the loss function changes in each step of contrastive divergence. The connection also highlights the importance Metropolis-Hastings adjustment to ensure that the implied q distribution satisfies the detailed balance relation."
        },
        {
            "heading": "B.2 Derivation of Energy Discrepancy from KL Contractions",
            "text": "A Kullback-Leibler contraction is the divergence function KL(pdata \u2225 pebm)\u2212KL(Qpdata \u2225 Qpebm) (Lyu, 2011) for the convolution operator Qp(y) = \u2211 x\u2032\u2208X q(y|x\u2032)p(x\u2032). The linearity of the convolution operator retains the normalisation of the measure, i.e. for the energy-based distribution pebm we have\nQpebm = 1\nZU \u2211 x\u2032\u2208X q(y|x\u2032) exp(\u2212U(x\u2032)) with ZU = \u2211 x\u2032\u2208X exp(\u2212U(x\u2032)) .\nThe KL divergences then become with Uq := \u2212 logQ exp(\u2212U(x)) KL(pdata \u2225 pebm) = Epdata(x)[log pdata(x)] + Epdata(x)[U(x)] + logZU\nKL(Qpdata \u2225 Qpebm) = EQpdata(y)[logQpdata(y)] + EQpdata(y) [Uq(y)] + logZU Since the normalisation cancels when subtracting the two terms we find\nKL(pdata \u2225 pebm)\u2212KL(Qpdata \u2225 Qpebm) = EDq(pdata, U) + c where c is a constant that contains the U -independent entropies of pdata and Qpdata."
        },
        {
            "heading": "C Sample Approximations of Energy Discrepancies",
            "text": "In this section, we discuss practical implementations of the mean-pooling transform as an information destroying deterministic process and the grid-neighbourhood as a neighbourhood-based transformation."
        },
        {
            "heading": "C.1 General Strategy",
            "text": "As a general strategy, the contrastive potential has to be written as an expectation over an appropriate to be determined distribution pneg,q,y that depends on the chosen perturbation process and on the point where the contrastive potential is evaluated, i.e.\nUq(y) = \u2212 logEpneg,q,y(x\u2032) exp(\u2212U(x \u2032)) (5)\nwhich allows the evaluation of the contrastive potential via sampling from pneg,q,y. The energy discrepancy can then be written as\nEDq(pdata, U) = Epdata(x)Eq(y|x) [ logEpneg,q,y(x\u2032) [exp(U(x)\u2212 U(x \u2032))] ]\n(6)\nby using properties of the logarithm and exponential and the fact that U(x) does not depend on the expectations taken in y and x\u2032. The loss can then be approximated via ancestral sampling. We first sample a batch xi+ \u223c pdata, subsequently sample its perturbed counter part yi \u223c q(\u00b7|xi+), and finally sample M negative samples xi,j\u2212 \u223c pneg,q,yi . Sometimes, the perturbed sample yi is never explicitely computed in the process. As described in Equation (2), the approximation is always stabilised through tunable hyper-parameter w which finally yields the loss function\nLq,M,w(U) := 1\nN N\u2211 i=1 log w + M\u2211 j=1 exp(U(xi+)\u2212 U(x i,j \u2212 )) \u2212 log(M) The justification for the stabilisation is two-fold. Firstly, the logarithm makes the Monte-Carlo approximation of the contrastive potential biased due to Jensens inequality. The bias is negative, given to leading order by the variance of the approximation, and depends on the energy function U . Thus, the optimiser may start to optimise for a high bias and high variance estimator of the contrastive potential rather than learning the data distribution. While this issue can be alleviated by significantly large choices for M , it is much more practical to introduce a deterministic lower bound to the loss-functional through the stabilisation w, which prevents the bias and logarithm from diverging. Secondly, the effect of the stabilisation goes to zero as M increases. Thus, the asymptotic limit for M and N large is retained through the stabilisation. For more details and analogous arguments in the continuous case, see Schr\u00f6der et al. (2023)."
        },
        {
            "heading": "C.2 Mean Pooling Transform",
            "text": "We describe the mean-pooling transform on the example of image data which takes values in the space {0, 1}h\u00d7w. We fix a window size s and reshape each data-point into blocks of size s\u00d7 s, i.e.\n{0, 1}h\u00d7w \u2192 {0, 1}s\u00d7s\u00d7hs \u00d7ws , x 7\u2192 x\u0304\nThe mean pooling transform gpool computes the average over each block x\u0304\u2022,\u2022,i,j for i = 1, 2, . . . , h/s and j = 1, 2, . . . , w/s. The corresponding preimage of the mean pooling transform is given by the set of points which are identical to x up to block-wise permutation, i.e.\ng\u22121(gpool(x)) = {x\u2032 \u2208 X : there exist \u03c0i,j \u2208 Ss\u00d7s s.t. x\u0304\u2032l,k,i,j = x\u0304\u2032\u03c0i,j(l,k),i,j for all l, k, i, j}\nwhere Ss\u00d7s denotes the permutation group for matrices of size s\u00d7 s. In practice, the mean-pooled data point has to never be computed, only the block wise permutations of the data point are required. Consequently, we obtain negative samples through xi,j\u2212 \u223c U(g\u22121(gpool(xi))), i.e. via block wise permutation of the entries of each data point xi.\nStrictly speaking, this transformation violates the assumptions of Theorem 1 for data points that only consist of blocks that average to 1 or 0. Since this is only the case for a small set of the state space, we assume this violation to be negligible."
        },
        {
            "heading": "C.3 Grid Neighborhood",
            "text": "The grid neighbourhood for x \u2208 {0, 1}d is constructed as\nNgrid(x) = {y \u2208 {0, 1}d : y \u2212 x = \u00b1ek, k = 1, 2, . . . , d}\nwhere ek is a vector of zeros with a one in the k-th entry. This neighbourhood structure is symmetric, i.e. N\u22121grid(y) = Ngrid(y). Consequently, the negative samples are created by sampling from\nxi,j\u2212 \u223c U(Ngrid(yi)) with yi \u223c U(Ngrid(xi))\nNotice that each negative sample is the second neighbour of the positive sample, and with a small chance the positive sample itself."
        },
        {
            "heading": "C.4 Directed Neighbourhood Structures",
            "text": "More generally, the neighbourhood structure may form a non-symmetric directed graph for which the neighbourhood maps N\u22121 and N don\u2019t coincide. In this case, an additional weighting-term is introduced. We denote the number of neighbours of x as Kx = |N (x)| and the number of elements of which y is a neighbour as K \u2032y = |N\u22121(y)|. The forward transition density is given by the uniform distribution, i.e.\nq(y|x) = { 1/Kx if y \u2208 N (x) 0 else (7)\nWe then have\nUN (y) = log \u2211 x\u2032\u2208X q(y|x\u2032) exp(\u2212U(x\u2032))\n= log \u2211\nx\u2032\u2208N\u22121(y)\n1\nKx\u2032 exp(\u2212U(x\u2032))\n= log 1\nK \u2032y \u2211 x\u2032\u2208N\u22121(y) K \u2032y Kx\u2032 exp(\u2212U(x\u2032))\n= logEx\u2032\u223cU({N\u22121(y)})[\u03c9yx\u2032 exp(\u2212U(x\u2032))]\nwhere we introduced the weighting term \u03c9yx\u2032 = K \u2032y/Kx\u2032 ."
        },
        {
            "heading": "C.5 Consistency of our Approximation",
            "text": "The following proof is similar to Schr\u00f6der et al. (2023). We first restate the consistency result:\nTheorem 2. For every \u03b5>0 there exist N,M \u2208N such that |Lq,M,w(U)\u2212EDq(pdata, U)|<\u03b5 a.s..\nProof. For N data points xi+ \u223c pdata and perturbed points yi \u223c q(\u00b7|xi+) denote the M corresponding negative samples by xi,j\u2212 \u223c pneg,q,yi . Notice that the distribution of the negative samples depends on yi. Using the triangle inequality, we can upper bound the difference |EDq(pdata, U)\u2212 Lq,M,w(U)| by upper bounding the following two terms, individually:\u2223\u2223\u2223\u2223\u2223EDq(pdata, U)\u2212 1N N\u2211 i=1 logE [ exp(U(xi+)\u2212 U(x i,j \u2212 ) \u2223\u2223\u2223xi+,yi]\n\u2223\u2223\u2223\u2223\u2223 +\n\u2223\u2223\u2223\u2223\u2223 1N N\u2211 i=1 logE [ exp(U(xi+)\u2212 U(x i,j \u2212 ) \u2223\u2223\u2223xi+,yi]\u2212 Lq,M,w(U) \u2223\u2223\u2223\u2223\u2223 The conditioning expresses that the expectation is only taken in xi,j\u2212 \u223c pneg,q,yi while keeping the values of the random variables xi+ and y\ni fixed. The first term can be bounded by a sequence \u03b5N\na.s.\u2212\u2212\u2192 0 due to the normal strong law of large numbers. For the second term one needs to consider that the distribution pneg,q,yi depends on the random variable yi. For this reason, we notice that x i,j \u2212\nare conditionally indepedent given xi+,y i and employ a conditional version of the strong law of large numbers (Majerek et al., 2005, Theorem 4.2) to obtain\n1\nM M\u2211 j=1 exp ( U(xi+)\u2212 U(x i,j \u2212 ) ) a.s.\u2212\u2212\u2192 E [ exp(U(xi+)\u2212 U(x i,j \u2212 ) \u2223\u2223\u2223xi+,yi]\nNext, we have that the deterministic sequence w/M \u2192 0. Thus, adding the stabilisation w/M does not change the limit in M . Furthermore, since the logarithm is continuous, the limit also holds after applying the logarithm. Finally, the estimate translates to the sum by another application of the triangle inequality: For each i = 1, 2, . . . , N there exists a sequence \u03b5i,M\na.s.\u2212\u2212\u2192 0 such that\u2223\u2223\u2223\u2223\u2223 1N N\u2211 i=1 logE [ exp(U(xi+)\u2212 U(x i,j \u2212 ) \u2223\u2223\u2223xi+,yi]\u2212 Lq,M,w(U) \u2223\u2223\u2223\u2223\u2223 \u2264 1\nN N\u2211 i=1 \u2223\u2223\u2223\u2223\u2223\u2223logE [ exp(U(xi+)\u2212 U(x i,j \u2212 ) \u2223\u2223\u2223xi+,yi]\u2212 log 1M M\u2211 j=1 exp ( U(xi+)\u2212 U(x i,j \u2212 ) )\u2223\u2223\u2223\u2223\u2223\u2223\n< 1\nN N\u2211 i=1 \u03b5i,M \u2264 max(\u03b51,M , . . . , \u03b5N,M ) .\nHence, for each \u03b5 > 0 there exists an N \u2208 N and an M(N) \u2208 N such that |EDq(pdata, U) \u2212 Lq,M(N),w(U)| < \u03b5 almost surely."
        },
        {
            "heading": "D Related Work",
            "text": "Contrastive loss functions Our work is based on an unpublished work on energy discrepancies in the continuous case (Schr\u00f6der et al., 2023). The motivation for such constructed loss functions lies in the data processing inequality. A similar loss has been suggested before as KL contraction divergence (Lyu, 2011), however, only for its theoretical properties. Interestingly, the structure of the stabilised energy discrepancy loss shares similarities with other contrastive losses such as Ceylan & Gutmann (2018); Gutmann & Hyv\u00e4rinen (2010); van den Oord et al. (2018). This poses the question of possible classification-based interpretations of energy discrepancy and of the w-stabilisation.\nContrastive divergence and Sampling. Discrete training methods for energy-based models largely rely on contrastive divergence methods, thus motivating a lot of work on discrete sampling and proposal methods. Improvements of the standard Gibbs method were proposed by Zanella (2020) through locally informed proposals. The method was extended to include gradient information (Grathwohl et al., 2021) to drastically reduce the computational complexity of flipping bits of binary valued data and to flipping bits in several places (Sun et al., 2022b; Emami et al., 2023; Sun et al., 2022a). Finally, discrete versions of Langevin sampling have been introduced based on this idea (Zhang et al., 2022b; Rhodes & Gutmann, 2022; Sun et al., 2023). Consequently, most current implementations of contrastive divergence use multiple steps of a gradient based discrete sampler. Alternatively, energy-based models can be trained using generative flow networks which learns a Markov chain to construct data by optimising a given reward function. The Markov chain can be used to obtain samples for contrastive divergence without MCMC from the EBM (Zhang et al., 2022a).\nOther training methods for discrete EBMs. There also exist some MCMC free approaches for training discrete EBMs. Our work is most similar to concrete score matching (Meng et al., 2022) which uses neighbourhood structures to define a replacement of the continuous score function. Another sampling free approach for training discrete EBMs is ratio matching (Hyv\u00e4rinen, 2007; Lyu, 2012). However is has been found that also for ratio matching, gradient information drastically improves the performance (Liu et al., 2023). Moreover, Dai et al. (2020) proposed to apply variational approaches to train discrete EBMs instead of MCMC. Eikema et al. (2022) replaced the widelyused Gibbs algorithms with quasi-rejection sampling to trade off the efficiency and accuracy of the sampling procedure. The perturb-and-map (Papandreou & Yuille, 2011) is also recently utilised to sample and learn in discrete EBMs (Lazaro-Gredilla et al., 2021)."
        },
        {
            "heading": "E More about Experiments",
            "text": ""
        },
        {
            "heading": "E.1 Training Ising Models",
            "text": "ED-Grid (ours) 4.6 4.0 3.1 2.6 2.3 4.5 4.0\nExperimental Details. As in Grathwohl et al. (2021); Zhang et al. (2022a,b), we train a learnable connectivity matrix J\u03d5 to estimate the true matrix J in the Ising model. To generate the training data, we simulate Gibbs sampling with 1, 000, 000 steps for each instance to construct a dataset of 2, 000 samples. For energy discrepancy, we choose w = 1,M = 32 for all variants, \u03f5 = 0.1 in ED-Bern, and the window side is \u221a D \u00d7 \u221a D in ED-\nPool. The parameter J\u03d5 is learned by the Adam (Kingma & Ba, 2014) optimizer with a learning rate of 0.0001 and a batch size of 256. Following Zhang et al. (2022a), all models are trained with an l1 regularization with a coefficient in {10, 5, 1, 0.1, 0.01} to encourage sparsity. The other setting is basically the same as Section F.2 in Grathwohl et al. (2021). We report the best result for each setting using the same hyperparameter searching protocol for all methods.\nQuantitative Results. We consider D = 10\u00d7 10 grids with \u03c3 = 0.1, 0.2, . . . , 0.5 and D = 9\u00d7 9 grids with \u03c3 = \u22120.1,\u22120.2. The methods are evaluated by computing the negative log-RMSE between the estimated J\u03d5 and the ture matrix J . As shown in Table 3, our methods demonstrate comparable results to the baselines and, in certain settings, even outperform Gibbs and GWG, indicating that energy discrepancy is able to discover the underlying structure within the data."
        },
        {
            "heading": "E.2 Discrete Density Estimation",
            "text": "Experimental Details. This experiment keeps a consistent setting with Dai et al. (2020). We first generate 2D floating-points from a continuous distribution p\u0302 which lacks a closed form but can be easily sampled. Then, each sample x\u0302 := [x\u03021, x\u03022] \u2208 R2 is converted to a discrete data point x \u2208 {0, 1}32 using Gray code. To be specific, given x\u0302 \u223c p\u0302, we quantise both x\u03021 and x\u03022 into 16-bits binary representations via Gray code (Gray, 1953), and concatenate them together\nto obtain a 32-bits vector x. As a result, the probabilistic mass function in the discrete space is p(x) \u221d p\u0302 ([GrayToFloat(x1:16),GrayToFloat(x17:32)]). It is noteworthy that learning on this discrete space presents challenges due to the highly non-linear nature of the Gray code transformation.\nThe energy function is parameterised by a 4 layer MLP with 256 hidden dimensions and Swish (Ramachandran et al., 2017) activation. We train the EBM for 105 steps and adopt an Adam optimiser with a learning rate of 0.002 and a batch size of 128 to update the parameter. For the energy discrepancy, we choose w = 1,M = 32 for all variants, \u03f5 = 0.1 in ED-Bern, and the window size is 32\u00d7 1 in ED-Pool. After training, we quantitatively evaluate all methods using the negative log-likelihood (NLL) and the maximum mean discrepancy (MMD). To be specific, the NLL metric is computed based on 4, 000 samples drawn from the data distribution, and the normalisation constant is estimated using importance sampling with 1, 000, 000 samples drawn from a variational Bernoulli distribution with p = 0.5. For the MMD metric, we follow the setting in Zhang et al. (2022a), which adopts the exponential Hamming kernel with 0.1 bandwidth. Moreover, the reported performances are averaged over 10 repeated estimations, each with 4, 000 samples, which are drawn from the learned energy function via Gibbs sampling.\nQualitative Results. We qualitatively visualise the learned energy functions of our proposed approaches in Figure 3. To provide further insights into the oracle energy landscape, we also plot the ground truth samples in Figure 4. The results clearly demonstrate that energy discrepancy effectively fits the data distribution, validating the efficacy of our methods.\nThe Effect of \u03f5 in Bernoulli Perturbation. Perhaps surprisingly, we find that the proposed energy discrepancy loss with Bernoulli perturbation is very robust to the noise scalar \u03f5.\nIn Figure 6, w visualise the learned energy landscapes with different \u03f5. The results demonstrate that ED-Bern is able to learn faithful energy functions, even with extreme values of \u03f5, such as \u03f5 \u2208 {0.999, 0.001}. This high-\nlights the robustness and effectiveness of our approach. In Figure 5, we further show that, with \u03f5 \u2208 {0.9999, 0.0001}, ED-Bern can still learn a faithful energy landscape using a large value of M . However, when \u03f5 \u2208 {1, 0}, ED-Bern fails to work. It is noteworthy that the choice of \u03f5 is highly dependent on the specific structure of the dataset. While ED-Bern exhibits robustness to different values of \u03f5 in the synthetic data, we have observed that a large value of \u03f5 (\u03f5 \u2265 0.1) is not effective for discrete image modeling.\nThe Effect of Window Size in Deterministic Transformation. To investigate the effectiveness of the window size in ED-Pool, we conduct experiments in Figure 7 with different window sizes. The results indicate that employing a small window size (e.g., 2\u00d7 1) does not provide sufficient information for energy discrepancy to effectively learn the underlying data structure. Furthermore, our empirical findings suggest that solely increasing the value of M is not a viable solution to address this issue. Again, the choice of the window size should depend on the underlying data structure. In the discrete image modelling, we find that even with a small window size\n(i.e., 4 \u00d7 4), energy discrepancy yields an energy with low values on the data-support but rapidly diverging values outside of it. Therefore, it fails to learn a faithful energy landscape.\nQualitatively Understanding the Effect of w and M . The hyperparameters w and M play a crucial role in the estimation of energy discrepancy. Increasing M can reduce the variance of the Monte Carlo estimation of the contrastive potential in (1), while a proper value of w can improve the stabilisation of training. Here, we evaluate the effect of w and M on the variants of energy discrepancy in Figures 8 to 10. Based on empirical observations, we observe that when w = 0 and M is small (e.g., M \u2264 32 for ED-Bern and M \u2264 64 for ED-Pool and ED-Grid), energy discrepancy demonstrates rapid divergence and fails to converge. Additionally, we find that increasing M can address this issue to some extent and introducing a non-zero value for w can significantly stabilize the convergence, even with M = 1. Moreover, larger w tends to produce a flatter estimated energy landscapes, which also aligns with the findings in continuous scenarios of energy discrepancy Schr\u00f6der et al. (2023)."
        },
        {
            "heading": "E.3 Discrete Image Modelling",
            "text": "Experimental Details. In this experiment, we parametrise the energy function using ResNet (He et al., 2016) following the settings in Grathwohl et al. (2021); Zhang et al. (2022b), where the network has 8 residual blocks with 64 feature maps. Each residual block has 2 convolutional layers and uses Swish activation function (Ramachandran et al., 2017). We choose M = 32, w = 1 for all variants of energy discrepancy, \u03f5 = 0.001 for ED-Bern, and the window size is 2 \u00d7 2 for ED-Pool. Note that here we choose a relatively small \u03f5 and window size, since we empirically find that the loss of energy discrepancy converges to a constant rapidly with larger \u03f5 and window size, which can not\nprovide meaningful gradient information to update the parameters. All models are trained with Adam optimiser with a learning rate of 0.0001 and a batch size of 100 for 50, 000 iterations. We perform model evaluation every 5, 000 iterations by conducting Annealed Importance Sampling (AIS) with a discrete Langevin sampler for 10, 000 steps. The reported results are obtained from the model that achieves the best performance on the validation set. After training, we finally report the negative log-likelihood by running 300, 000 iterations of AIS.\nQualitative Results. We show the generated images in Figure 11, which are the samples in the final step of AIS. We see that our methods can generate realistic images on the Omniglot dataset but mediocre images on Caltech Silhouette. We hypothesise that improving the design of the affinity structure in the neighborhood-based transformation can lead to better results. On both the static and dynamic MNIST datasets, ED-Bern and ED-Grid generate diverse and high-quality images. However, ED-Pool experiences mode collapse, resulting in limited variation in the generated samples."
        }
    ],
    "title": "Training Discrete Energy-Based Models with Energy Discrepancy",
    "year": 2023
}