{
    "abstractText": "The material removal rate (MRR) is an important variable but difficult to measure in the chemical\u2013mechanical planarization (CMP) process. Most data-based virtual metrology (VM) methods ignore the large number of unlabeled samples, resulting in a waste of information. In this paper, the semi-supervised deep kernel active learning (SSDKAL) model is proposed. Clustering-based phase partition and phase-matching algorithms are used for the initial feature extraction, and a deep network is used to replace the kernel of Gaussian process regression so as to extract hidden deep features. Semi-supervised regression and active learning sample selection strategies are applied to make full use of information on the unlabeled samples. The experimental results of the CMP process dataset validate the effectiveness of the proposed method. Compared with supervised regression and co-training-based semi-supervised regression algorithms, the proposed model has a lower mean square error with different labeled sample proportions. Compared with other frameworks proposed in the literature, such as physics-based VM models, Gaussian-process-based regression models, and stacking models, the proposed method achieves better prediction results without using all the",
    "authors": [
        {
            "affiliations": [],
            "name": "Yuchun Xu"
        },
        {
            "affiliations": [],
            "name": "Chunhua Yang"
        },
        {
            "affiliations": [],
            "name": "Chao Liu"
        },
        {
            "affiliations": [],
            "name": "Chunpu Lv"
        },
        {
            "affiliations": [],
            "name": "Jingwei Huang"
        },
        {
            "affiliations": [],
            "name": "Ming Zhang"
        },
        {
            "affiliations": [],
            "name": "Huangang Wang"
        },
        {
            "affiliations": [],
            "name": "Tao Zhang"
        }
    ],
    "id": "SP:492db6c2f0f7b51234cd63cd8789c44ceaab06f0",
    "references": [
        {
            "authors": [
                "F. Preston"
            ],
            "title": "The theory and design of plate glass polishing machines",
            "venue": "J. Glass Technol. 1927,",
            "year": 1927
        },
        {
            "authors": [
                "H.M. Yeh",
                "K.S. Chen"
            ],
            "title": "Development of a pad conditioning simulation module with a diamond dresser for CMP applications",
            "venue": "Int. J. Adv. Manuf. Technol",
            "year": 2010
        },
        {
            "authors": [
                "C. Shin",
                "A. Kulkarni",
                "K. Kim",
                "H. Kim",
                "S. Jeon",
                "E. Kim",
                "Y. Jin",
                "T. Kim"
            ],
            "title": "Diamond structure-dependent pad and wafer polishing performance during chemical mechanical polishing",
            "venue": "Int. J. Adv. Manuf. Technol",
            "year": 2018
        },
        {
            "authors": [
                "D. Liu",
                "G. Chen",
                "Q. Hu"
            ],
            "title": "Material removal model of chemical mechanical polishing for fused silica using soft nanoparticles",
            "venue": "Int. J. Adv. Manuf. Technol",
            "year": 2017
        },
        {
            "authors": [
                "S. Zhao",
                "Y. Huang"
            ],
            "title": "A stack fusion model for material removal rate prediction in chemical-mechanical planarization process",
            "venue": "Int. J. Adv. Manuf. Technol",
            "year": 2018
        },
        {
            "authors": [
                "Z. Li",
                "D. Wu",
                "T. Yu"
            ],
            "title": "Prediction of material removal rate for chemical mechanical planarization using decision tree-based ensemble learning",
            "venue": "J. Manuf. Sci. Eng",
            "year": 2019
        },
        {
            "authors": [
                "H. Cai",
                "J. Feng",
                "Q. Yang",
                "W. Li",
                "X. Li",
                "J. Lee"
            ],
            "title": "A virtual metrology method with prediction uncertainty based on Gaussian process for chemical mechanical planarization",
            "year": 2020
        },
        {
            "authors": [
                "H. Cai",
                "J. Feng",
                "F. Zhu",
                "Q. Yang",
                "X. Li",
                "J. Lee"
            ],
            "title": "Adaptive virtual metrology method based on Just-in-time reference and particle filter for semiconductor manufacturing",
            "venue": "Measurement",
            "year": 2021
        },
        {
            "authors": [
                "K.B. Lee",
                "C.O. Kim"
            ],
            "title": "Recurrent feature-incorporated convolutional neural network for virtual metrology of the chemical mechanical planarization process",
            "venue": "J. Intell. Manuf",
            "year": 2020
        },
        {
            "authors": [
                "T. Yu",
                "Z. Li",
                "D. Wu"
            ],
            "title": "Predictive modeling of material removal rate in chemical mechanical planarization with physics-informed machine learning",
            "year": 2019
        },
        {
            "authors": [
                "K.L. Lim",
                "R. Dutta"
            ],
            "title": "Material removal rate prediction using the classification-regression approach",
            "venue": "In Proceedings of the 2020 IEEE 22nd Electronics Packaging Technology Conference (EPTC), Singapore,",
            "year": 2020
        },
        {
            "authors": [
                "M. Zhang",
                "N. Amaitik",
                "Z. Wang",
                "Y. Xu",
                "A. Maisuradze",
                "M. Peschl",
                "D. Tzovaras"
            ],
            "title": "Predictive maintenance for remanufacturing based on hybrid-driven remaining useful life prediction",
            "venue": "Appl. Sci. 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Q. Ma",
                "M. Zhang",
                "Y. Xu",
                "J. Song",
                "T. Zhang"
            ],
            "title": "Remaining Useful Life Estimation for Turbofan Engine with Transformer-based Deep Architecture",
            "venue": "In Proceedings of the 2021 26th International Conference on Automation and Computing (ICAC),",
            "year": 2021
        },
        {
            "authors": [
                "K.L. Lim",
                "R. Dutta"
            ],
            "title": "Prognostics and Health Management of Wafer Chemical-Mechanical Polishing System using Autoencoder",
            "venue": "In Proceedings of the 2021 IEEE International Conference on Prognostics and Health Management (ICPHM), Detroit, MI, USA,",
            "year": 2021
        },
        {
            "authors": [
                "L. Xia",
                "P. Zheng",
                "X. Huang",
                "C. Liu"
            ],
            "title": "A novel hypergraph convolution network-based approach for predicting the material removal rate in chemical mechanical planarization",
            "venue": "J. Intell. Manuf",
            "year": 2022
        },
        {
            "authors": [
                "M. Maggipinto",
                "M. Terzi",
                "C. Masiero",
                "A. Beghi",
                "G.A. Susto"
            ],
            "title": "A computer vision-inspired deep learning architecture for virtual metrology modeling with 2-dimensional data",
            "venue": "IEEE Trans. Semicond. Manuf",
            "year": 2018
        },
        {
            "authors": [
                "X. Wu",
                "J. Chen",
                "L. Xie",
                "L.L.T. Chan",
                "C.I. Chen"
            ],
            "title": "Development of convolutional neural network based Gaussian process regression to construct a novel probabilistic virtual metrology in multi-stage semiconductor processes",
            "venue": "Control. Eng. Pract",
            "year": 2020
        },
        {
            "authors": [
                "C.E. Rasmussen"
            ],
            "title": "Gaussian processes in machine learning",
            "venue": "In Proceedings of the Summer School on Machine Learning, Tubingen,",
            "year": 2003
        },
        {
            "authors": [
                "C. Rasmussen",
                "Z. Ghahramani"
            ],
            "title": "Infinite mixtures of Gaussian process experts",
            "venue": "In Proceedings of the Advances in Neural Information Processing Systems",
            "year": 2001
        },
        {
            "authors": [
                "A.G. Wilson",
                "Z. Hu",
                "R. Salakhutdinov",
                "E.P. Xing"
            ],
            "title": "Deep kernel learning",
            "venue": "In Proceedings of the Artificial Intelligence and Statistics, Cadiz, Spain,",
            "year": 2016
        },
        {
            "authors": [
                "B. Settles"
            ],
            "title": "Active Learning Literature Survey",
            "year": 2009
        },
        {
            "authors": [
                "D. Wu"
            ],
            "title": "Pool-based sequential active learning for regression",
            "venue": "IEEE Trans. Neural Networks Learn. Syst. 2018,",
            "year": 2018
        },
        {
            "authors": [
                "H.S. Seung",
                "M. Opper",
                "H. Sompolinsky"
            ],
            "title": "Query by committee",
            "venue": "In Proceedings of the Fifth Annual Workshop on Computational Learning Theory, Pittsburgh, Pennsylvania,",
            "year": 1992
        },
        {
            "authors": [
                "Y. Freund",
                "H.S. Seung",
                "E. Shamir",
                "N. Tishby"
            ],
            "title": "Selective sampling using the query by committee",
            "year": 1997
        },
        {
            "authors": [
                "A. Krogh",
                "J. Vedelsby"
            ],
            "title": "Neural network ensembles, cross validation, and active learning",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 1994
        },
        {
            "authors": [
                "T. RayChaudhuri",
                "L. Hamey"
            ],
            "title": "Minimisation of data collection by active learning",
            "venue": "In Proceedings of the ICNN\u201995-International Conference on Neural Networks, Perth, WA, Australia,",
            "year": 1995
        },
        {
            "authors": [
                "W. Cai",
                "Y. Zhang",
                "J. Zhou"
            ],
            "title": "Maximizing expected model change for active learning in regression",
            "venue": "In Proceedings of the 2013 IEEE 13th International Conference on Data Mining, Dallas, TX, USA,",
            "year": 2013
        },
        {
            "authors": [
                "H. Yu",
                "S. Kim"
            ],
            "title": "Passive sampling for regression",
            "venue": "In Proceedings of the 2010 IEEE International Conference on Data Mining, Sydney, NSW, Australia,",
            "year": 2010
        },
        {
            "authors": [
                "D. Wu",
                "C.T. Lin",
                "J. Huang"
            ],
            "title": "Active learning for regression using greedy sampling",
            "venue": "Inf. Sci",
            "year": 2019
        },
        {
            "authors": [
                "J. Camacho",
                "J. Pic\u00f3"
            ],
            "title": "Online monitoring of batch processes using multi-phase principal component analysis",
            "venue": "J. Process. Control",
            "year": 2006
        },
        {
            "authors": [
                "C. Zhao",
                "Y. Sun"
            ],
            "title": "Step-wise sequential phase partition (SSPP) algorithm based statistical modeling and online process monitoring",
            "venue": "Chemom. Intell. Lab. Syst",
            "year": 2013
        },
        {
            "authors": [
                "L.A. Leiva",
                "E. Vidal"
            ],
            "title": "Warped k-means: An algorithm to cluster sequentially-distributed data",
            "venue": "Inf. Sci",
            "year": 2013
        },
        {
            "authors": [
                "G. Kostopoulos",
                "S. Karlos",
                "S. Kotsiantis",
                "O. Ragos"
            ],
            "title": "Semi-supervised regression: A recent review",
            "venue": "J. Intell. Fuzzy Syst",
            "year": 2018
        },
        {
            "authors": [
                "Z. Ge",
                "B. Huang",
                "Z. Song"
            ],
            "title": "Mixture semisupervised principal component regression model and soft sensor application",
            "venue": "AIChE J",
            "year": 2014
        },
        {
            "authors": [
                "J. Zheng",
                "Z. Song"
            ],
            "title": "Semisupervised learning for probabilistic partial least squares regression model and soft sensor application",
            "venue": "J. Process. Control",
            "year": 2018
        },
        {
            "authors": [
                "W. Shao",
                "Z. Ge",
                "Z. Song"
            ],
            "title": "Semi-supervised mixture of latent factor analysis models with application to online key variable estimation",
            "venue": "Control Eng. Pract",
            "year": 2019
        },
        {
            "authors": [
                "N. Jean",
                "S.M. Xie",
                "S. Ermon"
            ],
            "title": "Semi-supervised deep kernel learning: Regression with unlabeled data by minimizing predictive variance",
            "venue": "In Proceedings of the Neural Information Processing Systems (NeurIPS), Montreal, QC, Canada,",
            "year": 2018
        },
        {
            "authors": [
                "M. Zhao",
                "T.W. Chow",
                "Z. Wu",
                "Z. Zhang",
                "B. Li"
            ],
            "title": "Learning from normalized local and global discriminative information for semi-supervised regression and dimensionality reduction",
            "venue": "Inf. Sci",
            "year": 2015
        },
        {
            "authors": [
                "T. Cover",
                "P. Hart"
            ],
            "title": "Nearest neighbor pattern classification",
            "venue": "IEEE Trans. Inf. Theory",
            "year": 1967
        },
        {
            "authors": [
                "P. Geurts",
                "D. Ernst",
                "L. Wehenkel"
            ],
            "title": "Extremely randomized trees",
            "venue": "Mach. Learn",
            "year": 2006
        },
        {
            "authors": [
                "Z.H. Zhou",
                "M. Li"
            ],
            "title": "Semisupervised regression with cotraining-style algorithms",
            "venue": "IEEE Trans. Knowl. Data Eng",
            "year": 2007
        },
        {
            "authors": [
                "J. Gardner",
                "G. Pleiss",
                "K.Q. Weinberger",
                "D. Bindel",
                "A.G. Wilson"
            ],
            "title": "Gpytorch: Blackbox matrix-matrix gaussian process inference with gpu acceleration",
            "venue": "In Proceedings of the Neural Information Processing Systems (NeurIPS), Montreal, QC, Canada,",
            "year": 2018
        },
        {
            "authors": [
                "P. Wang",
                "R.X. Gao",
                "R. Yan"
            ],
            "title": "A deep learning-based approach to material removal rate prediction in polishing",
            "venue": "CIRP Ann",
            "year": 2017
        }
    ],
    "sections": [
        {
            "text": "Citation: Lv, C.; Huang, J.; Zhang, M.;\nWang, H.; Zhang, T. Semi-Supervised\nDeep Kernel Active Learning for\nMaterial Removal-Rate Prediction in\nChemical Mechanical Planarization.\nSensors 2023, 23, 4392. https://\ndoi.org/10.3390/s23094392\nAcademic Editors: Pai Zheng, Tao\nPeng, Yuchun Xu, Chunhua Yang,\nChao Liu and Xi Wang\nReceived: 1 February 2023\nRevised: 11 April 2023\nAccepted: 20 April 2023\nPublished: 29 April 2023\nCopyright: \u00a9 2023 by the authors.\nLicensee MDPI, Basel, Switzerland.\nThis article is an open access article\ndistributed under the terms and\nconditions of the Creative Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n4.0/).\nKeywords: semi-supervised regression; active learning; deep kernel learning; virtual metrology; phase partition; phase match"
        },
        {
            "heading": "1. Introduction",
            "text": "Today, semiconductor manufacturing represents the highest level of microfabrication in the world. Wafer production is a key step in semiconductor manufacturing, which includes many complex processes, such as lithography, etching, deposition, chemical mechanical planarization (CMP), oxidation, ion implantation, diffusion, etc. Processes such as etching, deposition, and oxidation result in wafers with uneven surfaces, including higher steps and larger trenches. CMP technology is an important way to build wafer structures. The material removal rate (MRR) is a critical variable in the CMP process. Each wafer needs to be polished to the target thickness; thus, an accurate MRR estimation is required to accurately set the polishing time. However, the MRR is difficult and costly to measure, and at the same time, it may cause damage to the wafers. In this stage, the MRR relies on engineering experience and actual measurements with a very low frequency (e.g., only one wafer is selected for measurement in one lot, or one wafer is measured at fixed time intervals) as a reference to complete the setup and adjustment. In industrial production, to accurately estimate an important but difficult-to-measure variable, it is customary to model the relationship between the variable and easily accessible environmental variables, equipment parameters, intermediate variables, etc., and complete the prediction with physical laws or historical data. This process is called virtual metrology (VM). VM consists of two main forms: physics-based models and data-based models. Physics-based models establish mathematical expressions by analyzing the physical and\nSensors 2023, 23, 4392. https://doi.org/10.3390/s23094392 https://www.mdpi.com/journal/sensors\nchemical reactions of the process. Preston proposed that the MRR is related to pressure and the relative velocities of wafers and polishing pads and established the Preston equation [1]. Teh and Chen et al. investigated the dressing process of polishing pads by designing pad simulation modules to investigate the effect of key geometrics [2]. Shin and Kulkarni et al. tested the effects of different diamond structures on the polishing pad and polishing performance [3]. Liu et al. established the relationship of the MRR of fused silica with the polishing pressure, chemical reagents, and their concentrations, as well as the relative velocity between the wafer and the polishing pad [4]. Some of the existing physics-based models build global-based mathematical expressions by analyzing the physical and chemical reactions of the CMP process. Other models focus on the role of certain variables in the CMP process and the effects on the MRR. However, the CMP process involves so many variables that explicit mathematical expressions and physics-based models can hardly cover all of them. At the same time, the duration of the CMP process is very long. In an operating cycle, the CMP process passes through multiple operating points, dividing the process into multiple phases. The different relationships between variables in different phases also lead to continuous changes in the physical models. The global-based physical models are difficult to describe in detail in terms of process changes. With the continuous development of data science, researchers are increasingly placing emphasis on mining knowledge and information from large amounts of historical data, and therefore, data-based VM models are increasingly favored. Since individual machine learning models do not suitably fit the CMP process, some studies have used stacking models. Zhao and Huang performed one-hot encoding of the \u201cstage\u201d and \u201cchamber\u201d variables in the feature creation and feature encoding stages to transform the process data into multidimensional information, and the stacking integration model was chosen for the regression [5]. Li and Wu et al. also used a stacking integrated learning regression model with primary learners including random forest (RF), gradient boosting tree (GBT), and extreme random tree (ERT), and meta-learner as extreme learning machine (ELM) and classification and regression tree (CART), and the features in the model included the frequency domain features of the three rotation variables and feature selection using RF [6]. The CMP process runs continuously, and the dynamic prediction of the MRR is necessary. Cai and Feng et al. combined k-nearest neighbors (kNN) and Gaussian process regression (GPR) to dynamically predict the MRR, where kNN searched historical data as reference samples and a multi-task Gaussian process (MTGP) model incorporated the training data and reference sample information [7]. The authors proposed another dynamic prediction model, in which reference samples from historical data were fused with data samples through support vector regression (SVR), and a particle filter (PF) estimated and updated the prediction results and ensured that the model could track changes in the CMP process [8]. Lee and Kim addressed the problem of the degradation of the model\u2019s prediction performance due to process drift and proposed a VM model combining the recurrent neural network (RNN) and the convolutional neural network (CNN), extracting time-dependent and time-independent features with a two-stage training method to alternately update the network weights [9]. The existing stacking models and dynamic prediction models in the literature focus on extracting global features and are insensitive to the phase changes in the CMP process. Therefore, a more adequate feature extraction method is needed. Several researchers have proposed prediction methods based on a combination of physical models and data models [10]. Yu and Li et al. derived a formula for MRR using a pad and conditioner. This model considers the contact between the polishing pads, abrasives, and wafers, and the polishing pad surface topology term in the formula is estimated using RF [11]. Lim and Dutta proposed that the kinetics and contact between the wafer and polishing equipment are applied based on knowledge obtained a priori and a physical model for wafer classification, and they also proposed classification and regression methods, where the feature extraction is obtained by downsampling and expanding the\ndosage and pressure variables [12]. These methods add physical analysis of the CMP process to the data models. However, similar to the physics-based VM model, the physical analysis focuses on some important variables but cannot cover all the variables\u2019 effects on the MRR. Research on neural networks and deep learning has provided new ideas [13,14]. Lim and Dutta used autoencoder and k-means clustering to determine the feature space of the training samples and combined the reconstruction loss of the encoder\u2013decoder and the clustering error to form a loss function [15]. Xia and Zheng et al. applied hypergraph convolutional networks to MRR prediction [16]. In the data preprocessing stage, the piecewise aggregate approximation (PAA) method was used for time series alignment and dimensionality reduction. In the prediction stage, a hypergraph model was used to represent the complex relationships between the devices and variables. Maggipinto et al. used deep learning methods to model images of optical emission spectra (OES) with spatial and temporal evolution to build VM models of visual 2D data [17]. Wu et al. designed a VM model combining CNN and Gaussian process regression (GPR) to address the mismatch between VM models and features, using CNN for feature extraction and GPR for the prediction of quality variables [18]. The deep learning methods can fully explore the process features and the relationships between variables but require a large amount of data; otherwise, there is a risk of overfitting. However, due to the difficulty of measuring the MRR, there are less data for the CMP process. There are some points for improvement regarding the established MRR prediction methods. First, the CMP process is a batch process, and its process data have three dimensions: the batch, variable, and time. Meanwhile, the CMP process involves several phases, and the work points change over time. The different batches and phases are not equal in duration. Most of the existing prediction methods extract the overall features of the process and ignore the phase features. Second, feature extraction with machine learning only involves statistical features, as well as other features such as the duration and nearest neighbors, and the mining of deep features and deep information in the CMP process data is not sufficient. Third, it is worth noting that a large number of wafers lack an MRR and become unlabeled samples because the MRR is not easy to measure. If labeled samples alone are used to build VM models, this will cause information wastage. In this paper, a semi-supervised deep kernel active learning (SSDKAL) method is proposed for MRR prediction, with the following main contributions: First, the semi-supervised active learning framework can fully exploit and utilize information regarding unlabeled samples. In this framework, semi-supervised regression (SSR) incorporates the uncertainty of the unlabeled samples into the loss function, while active learning (AL) can compensate for the weakness of the insufficient representativeness and diversity of samples in SSR. Second, clustering-based phase partition and phasematching algorithms are used to extract phase features. Third, neural networks provide more in-depth identification and fitting of complex multi-phase processes through complex network connections. Our experiments based on CMP process data show that the proposed method achieves good results in terms of the prediction accuracy for the MRR. In actual production, many batch processes have the characteristics of multiple phases and unequal lengths, and there are a large number of unlabeled samples. Therefore, the proposed method can be extended to these processes. In the feature extraction stage, feature data are extracted using phase partition and phase matching, and in the prediction stage, a combination of DKL, SSR, and AL are used for prediction. The rest of this paper is organized as follows. Chapter 2 presents the related works, including deep kernel learning and active learning regression. Chapter 3 describes the procedure of the SSDKAL method in detail. Chapter 4 presents the experimental results and the discussion. Chapter 5 concludes the paper."
        },
        {
            "heading": "2. Related Works",
            "text": ""
        },
        {
            "heading": "2.1. Deep Kernel Learning",
            "text": "Gaussian process regression (GPR) was first proposed by Rasmussen [19]. As an efficient statistical learning method, GPR computes optimal hyperparameters by learning prior functions of historical data to obtain predictive models. With its good interpretability, GPR is widely used in system modeling for industrial processes, especially complex systems with high dimensionality, small samples, and nonlinear characteristics [20]. GPR can provide rich statistical representation, accurate prediction, and new insights into modeling using large datasets. A Gaussian process (GP) is a stochastic process in which observations appear in a continuous domain (e.g., time or space). In a Gaussian process, each point in the continuous input space is associated with a Gaussian-distributed random variable. Any finite number of random variables obey the joint Gaussian distribution, denoted as Equation (1):\nf (X) \u223c N(\u00b5(X), \u03a3X,X) (1)\nwhere \u00b5(X) denotes the mean function and \u03a3X,X = [ \u03c3ij ] = [ \u03c3(xi, xj) ] denotes the covariance function. GPR is a typical nonparametric model based on a Bayesian framework. Suppose that we obtain a set of n independently and identically distributed observation samples that constitute the input matrix X = (x1, x2, \u00b7 \u00b7 \u00b7 , xn)T, xi \u2208 Rd, where d denotes the dimensionality of the input samples. The output values corresponding to the samples constitute the vector y = (y1, y2, \u00b7 \u00b7 \u00b7 , yn)T, yi \u2208 R. Let the matrix of the test samples to be X\u2217 = (x\u22171 , x\u22172 , \u00b7 \u00b7 \u00b7 , x\u2217m)T. From the definition and properties of GPR, it is known that the training samples and the test samples obey the joint Gaussian distribution, and y and the model prediction of the test samples f (X\u2217) obey the joint Gaussian distribution, as shown in Equation (2):[\ny f (X\u2217)\n] \u223c N ([ \u00b5(X) \u00b5(X\u2217) ] , [ \u03a3X,X + \u03c32 I \u03a3X,X\u2217 \u03a3X\u2217 ,X \u03a3X\u2217 ,X\u2217 ]) (2)\nwhere \u00b5(X) and \u00b5(X\u2217) are the mean vectors of the training and test samples, respectively, \u03a3X,X and \u03a3X\u2217 ,X\u2217 are the self-covariance matrices of the training and test samples, respectively, and \u03a3X,X\u2217 = \u03a3TX\u2217 ,X is the covariance matrix between the training and test samples. This can be deduced as shown in Equations (3)\u2013(5):\nf (X\u2217)|y, X, X\u2217 \u223c N(\u00b5 f , \u03a3 f ) (3)\n\u00b5 f = \u00b5(X \u2217) + \u03a3X\u2217 ,X(\u03a3X,X + \u03c32 I)\u22121(y\u2212 \u00b5(X)) (4)\n\u03a3 f = \u03a3X\u2217 ,X\u2217 \u2212 \u03a3X\u2217 ,X(\u03a3X\u2217 ,X\u2217 + \u03c32 I)\u22121\u03a3X,X\u2217 (5)\nwhere \u00b5 f is the predicted mean vector, i.e., the predicted value of the GPR model, while \u03a3 f denotes the error region of the predicted output. Assuming that the parameters of \u03a3X,X are \u03b8, and \u03a3X,X is replaced by \u03a3\u03b8 , while the input data are considered to have completed normalization with a mean of 0, according to Equations (3)\u2013(5), y conforms to a Gaussian distribution with a mean of 0 and a variance of \u03a3\u03b8 + \u03c32 I. The probability distribution function of y is shown in Equation (6):\np(y|\u03b8, X) = 1 (2\u03c0) n 2 |\u03a3\u03b8 + \u03c32 I|\nexp ( \u22121\n2 yT(\u03a3\u03b8 + \u03c32 I)\u22121y\n) (6)\nAfter taking the logarithm, we can obtain Equation (7):\nlog p(y|\u03b8, X) \u221d \u2212 [ yT(\u03a3\u03b8 + \u03c32 I)\u22121y + log |\u03a3\u03b8 + \u03c32 I| ] (7)\nWilson and Hu et al. introduced deep networks into the Gaussian process by replacing the kernel and proposed deep kernel learning (DKL) method, which combines deep learning with existing kernel functions using techniques such as deep feedforward, convolutional architectures, and structure utilization algebra to transform deep architectures into inputs for spectral hybrid-based kernels [21]. The deep kernel is as shown in Equation (8):\n\u03c3(xi, xj|\u03b8)\u2192 \u03c3(g(xi, w), g(xj, w)|\u03b8, w) (8)\nwhere g(x, w) is a nonlinear mapping of the deep network, and w is a network parameter; therefore, the parameters of DKL become \u03b7 = (\u03b8, w). Similarly, the deep kernel model learns the parameters by maximizing the log marginal likelihood function L of the Gaussian process, in which the chain rule is used to calculate the derivatives of the log marginal likelihood function with respect to the parameters and update the parameters through backpropagation. The derivatives are calculated as shown in Equations (9) and (10):\n\u2202L \u2202\u03b8 = \u2202L \u2202\u03a3\u03b7 \u2202\u03a3\u03b7 \u2202\u03b8\n(9)\n\u2202L \u2202w = \u2202L \u2202\u03a3\u03b7 \u2202\u03a3\u03b7 \u2202g(x, w) \u2202g(x, w) \u2202w\n(10)\nwhere \u03a3\u03b7 is the depth kernel covariance matrix, and the KISS-GP matrix can be used instead in the calculation."
        },
        {
            "heading": "2.2. Active Learning Regression",
            "text": "The purpose of active learning (AL) is to manually label fewer samples from unlabeled samples to achieve the target accuracy when both labeled and unlabeled samples are available. AL is an iterative framework that actively selects the most useful samples in the unlabeled sample set during each iteration and passes them on to experts for labeling, after which they are added to the training set and the current model is updated. AL greatly reduces the amount of data needed to train the model by prioritizing the expert labeling, thus reducing the cost while improving accuracy [22]. In AL, the strategy of selecting valuable unlabeled samples is the core of the algorithm. For active learning classification (ALC), there are numerous strategies, while there are fewer strategies for active learning regression (ALR) [23]. Wu proposed three criteria for pool-based ALR strategies, namely, informativeness, representativeness, and diversity. Informativeness indicates that the queried samples need to contain rich information, which, after labeling, can improve the prediction performance of the model. Representativeness can be measured based on the number of nearest neighbors of the sample, and if the number is high, the sample is more representative. Diversity indicates that the samples need to be dispersed throughout the input space rather than concentrated in a category or a region [23]. Seung and Opper, et al. proposed the query by committee (QBC) method for ALC, in which a set of classifiers are trained on labeled data and then predict unlabeled data, from among which the samples with the most divergent committee members are selected for labeling [24]. Freund et al. demonstrated that the method is applicable when the samples are not labeled as discrete [25]. Krogh and Vedelsby defined the ambiguity of a sample in terms of prediction variance in a committee composed of neural networks and queried the sample label with the largest variance [26]. RayChaudhuri and Hamey used a similar approach but employed bagging in the design of the models [27]. Cai et al. proposed an AL framework for expected model change maximization (EMCM), which aims to select samples from an unlabeled sample set that maximizes the change in the model before and after labeling [28]. The model change is quantified according to the difference in the model parameters before and after the addition of the given sample.\nYu and Kim proposed a passive sampling technique for ALR to identify informative samples based on their geometric features in the feature space [29]. They argued that the samples selected for queries based on loss functions have relatively large errors and are likely to be noisy. They proposed four passive sampling methods, including the Grid approach, k-center algorithms, greedy sampling (GS) algorithm, and incremental k-medoids algorithm. Wu and Lin et al. argued that the above GS algorithm takes into account the diversity of samples in the input space and proposed two other GS methods. The first method, GSy, takes into account the diversity of samples in the output space, and the second method, iGS, takes into account the diversity of samples in both the input and output spaces [30]. Wu proposed a new ALR method using passive sampling that incorporates the idea of clustering in the initialization and iteration while considering informativeness, representativeness, and diversity. The method can complete the process of ALR without a single labeled sample. This method can also be combined with QBC, EMCM, and greedy sampling [23]."
        },
        {
            "heading": "3. Methods",
            "text": ""
        },
        {
            "heading": "3.1. Feature Extraction",
            "text": "The data for the CMP process have three dimensions: the batch, variable, and time. Usually, the data of a batch represent the production process of a wafer, i.e., the data matrix of variable\u00d7 time, corresponding to an MRR value. The main processing method for 3D data is to expand the data in the variable direction or batch direction and recover them as 2D data. Multiple principal component analysis (MPCA) and multiple partial least squares (MPLS) are then used for early batch process statistical monitoring and regression analysis, respectively [31,32]. Due to variations in the equipment and environment, the duration of each wafer is different. The above-mentioned MPCA, MPLS, and most other methods are not feasible for a CMP process that is unequal in length. At the same time, the CMP process is divided into several phases, and the variable relationships are different in different phases, with different operating points of the devices. The extraction of global features will only lead to the loss of information. In this paper, we adopt a feature extraction method based on phase partition and phase matching. In phase partition, a combination of wrapped k-means (WKM) [33] and the phase partition combination index (PPCI) are used. The dimensionality of each wafer data matrix is different because the wafers have different operation times. The purpose of phase partition is to obtain more accurate phase features; thus, phase partition is performed separately for each wafer. The data of the I-th batch are X = (x1, x2, \u00b7 \u00b7 \u00b7 , xKI )T, where KI denotes the sampling length, i.e., the number of sample points contained in the batch, and xk \u2208 RJ , k = 1, 2, \u00b7 \u00b7 \u00b7 , KI , where J is the number of variables contained in each sample point. The basic idea of phase partition is that the sample vectors in the batch are clustered, and different categories correspond to different phases. However, it is worth noting that the process data need to satisfy the temporal order constraint. Given the number of clusters, i.e., the number of phases C, the batch data are initially divided according to the cumulative error, and then the sampling points are moved. Here, the sum of squared error (SSE) is used to calculate the clustering error and acts as the basis for the sampling point movement. The SSE is calculated as shown in Equation (11):\nSSE = C\n\u2211 c=1 \u2211 xk\u2208Phase c\n(xk \u2212 sc)T(xk \u2212 sc) (11)\nwhere sc is the center point of the c-th phase. The following rule is used for sample point movement, when the movement condition is met, the first half of the sample points of each phase can only move to the previous phase, and the second half can only move to the latter phase. If a sample point does not meet the move condition, the remaining sample points in\nthat half will no longer move. After a sample point xk moves from phase c to phase b, the centroids will change, as shown in Equations (12) and (13):\ns\u0302c = sc \u2212 xk \u2212 sc lc \u2212 1\n(12)\ns\u0302b = sb + xk \u2212 sb lb + 1\n(13)\nwhere lc and lb are the numbers of sampling points in phase c and phase b before the move, respectively. After that, the SSE is recalculated, and if the SSE decreases, xk will be moved to phase b. Otherwise, the move will be rejected. The CMP process contains stable phases with small changes in variable relationships and transition phases with fast changes in variable relationships. In the WKM algorithm, given the number of clusters C, a wafer can be divided into C phases, but the effectiveness of the partition is related to the size of C. When C is larger, the partition is detailed, and more transition phases are divided. When C is small, more transition phases are merged into the stable phases. We weigh the phase number C and the clustering error SSE and use PPCI for the metric. With the increase in C, PPCI shows a trend of decreasing and then increasing. The phase number corresponding to the smallest PPCI is chosen as the phase partition number of the wafer. Due to the presence of PPCI, the optimal phase number differs from wafer to wafer. In the CMP process, many wafers have more transition phases with drastic changes, and the PPCI still decreases when the cluster number is large. The ultimate goal of phase partition is to extract the features of key phases; therefore, the phases of different wafers need to be matched. First, we must eliminate the effect of transition phases. Given a minimum length LS, the phase with a number of sampling points no lower than LS is considered as a stable phase; otherwise, it is a transition phase. Next, the standard stable phase number (SSN) and the standard stable phase center (SSPC) are determined. For all wafers, the plural of the stable phase number is selected as SSN. All wafers with stable phase numbers equal to SSN are selected, the mean value of the centroid of each stable phase is calculated, the wafers far from the mean value are discarded, and the process is iterated to finally obtain the SSPC. After that, all wafers that have completed phase partition are matched with the SSPC. The basic method of phase matching is to calculate each phase center and identify the SSPC with the closest distance to it. However, at the same time, the time series constraint needs to be maintained. Two phases with backward and forward orders of time cannot be matched with SSPCs with opposite time orders. The optimization problem of phase matching is as shown in Equation (14):\nmin \u2211SSNh=1 [ dist(s\u2217h, s\u0303 i h)\u2212 \u03c1L\u0303h ] s.t. L\u0303h = endih \u2212 start i h\ns\u0303ih = 1\nL\u0303h \u2211 endih l=startih\nxl startih+1 \u2265 end i h, h = 1, 2, \u00b7 \u00b7 \u00b7 , SSN \u2212 1\ndist(s\u2217h, s i t) \u2264 \u03b8, t = Jh, Jh + 1, \u00b7 \u00b7 \u00b7 , Jh + Th \u2212 1\n(14)\nwhere i denotes the matching process of the i-th wafer, s\u2217h denotes the h-th SSPC, and s\u0303 i h denotes the h-th phase center of the i-th wafer after phase matching is completed. startih, endih, and L\u0303h denote the start point, end point, and the length of the h-phase after the matching, respectively. \u03b8 is the distance threshold of the two centers, \u03c1 denotes the balance parameter of the distance and phase length, and Jh and Th denote the starting and ending phases of the original wafer that match the h-th standard phase. We use a greedy algorithm to continuously compare the distance between the center of the phases and the SSPC to complete the phase matching."
        },
        {
            "heading": "3.2. Semi-Supervised Deep Kernel Active Learning",
            "text": "It is worth noting that, due to the sampling detection of the MRR, a large number of samples are missing the MRR. Attention should be paid to semi-supervised regression (SSR) methods using both labeled and unlabeled samples. Kostopoulos et al. divided SSR into four categories: semi-supervised kernel regression, multi-view regression, graph regularization regression, and semi-supervised GPR [34]. Among them, the semi-supervised GPR model consists of two components in the log-likelihood function: the joint distribution of independent and dependent variables with labeled samples based on the requested parameters, and the joint distribution likelihood function with unlabeled samples based on the requested parameters. The parameter value that maximizes the log-likelihood function is obtained via derivation [35\u201337]. The semi-supervised deep kernel learning (SSDKL) method combining deep learning and the Gaussian process uses the deep neural network (DNN) as a kernel function, and the optimization objective function includes both the log-likelihood function and the prediction variance of the unlabeled samples, the latter being used as a regularization term to suppress the overfitting of the model [38]. The loss function is adjusted to Equation (15):\nLSS(\u03b7) = 1 L Llikelihood(\u03b7) + \u03b1 U Luncertainty(\u03b7) (15)\nwhere \u03b1 is the equilibrium parameter controlling the two types of losses. In SSR, information on unlabeled samples is captured by adding the prediction variance as the regularization term of the loss function. In contrast toSSR based on cotraining [39], the unlabeled samples and pseudo-labels in SSDKL are not added to the labeled sample set, but by changing the loss function, they have an impact on the parameters of the model. The unlabeled samples provide an informative basis for model improvement but do not take into account the representativeness and diversity. At the same time, the small number of labeled samples in SSR are likely to be unevenly distributed and cannot fully cover the overall distribution of the samples. Active learning (AL) can compensate for this weakness. AL allows the model to select the important samples itself, thus obtaining a higher performance with less training samples. Considering the three sample selection criteria, this paper uses a sample query strategy based on a combination of query by committee (QBC) and greedy sampling (GS). Suppose that there are N samples in the training set, constituting X = {xi}Ni=1, where the set of labeled samples XL = {xi}Li=1 contains L labeled samples, and the set of unlabeled samples XU = {xi}L+Ui=L+1 contains U unlabeled samples, L + U = N. According to QBC, M regressors are trained according to XL, and the prediction variance of the unlabeled samples is as shown in Equation (16):\nsQBCi = 1 M\nM\n\u2211 j=1\n(yji \u2212 yi) 2 (16)\nwhere yi is the average of the predicted values of the M regressors for the i-th unlabeled sample. QBC selects the sample with the largest prediction variance for the query. Here, the prediction variances of all unlabeled samples are sorted from largest to smallest, and the set of unlabeled samples with the largest variance are selected and denoted as XQBC. Next, from the set XQBC, GS selects the sample farthest from the existing labeled sample for the query. The distance is calculated as shown in Equation (17), and the unlabeled sample corresponding to the largest sGSi will become the queried sample.\nsGSi = minxl\u2208XL \u2016xi \u2212 xl\u2016 (17)\nThe semi-supervised deep kernel active learning (SSDKAL) method is proposed. In each round of iterations, a combination of QBC and the GS sample selection strategy are used. QBC focuses on the performance improvement of the model with respect to the\nquery sample, i.e., informativeness, while GS takes into account the representativeness and diversity. The training process starts by training a set of SSDKL regressors using a small number of labeled samples. All regressors make predictions based on the unlabeled samples, select a set of samples with the largest variance, and choose a subset of unlabeled samples from this set, which are farthest from the existing labeled samples. The sample labels in this subset are queried and added to the labeled sample set. The learning, prediction, and sample selection process are repeated until the accuracy constraint or the upper limit of the query capability is reached. The algorithm of SSDKAL is shown in Algorithm 1. Without loss of generality, we assume that there are only two regressors in the AL committee, fA(x|\u03b7A) and fB(x|\u03b7B).\nAlgorithm 1 SSDKAL\nInput: After phase partition and phase matching, the input data X, which have completed phase feature extraction, feature selection, and normalization, include the labeled dataset XL = {(x1, y1), \u00b7 \u00b7 \u00b7 , (xL, yL))}, unlabeled dataset XU = {(xL+1), \u00b7 \u00b7 \u00b7 , (xL+U))}, learning rate lr, unlabeled sample loss term weight parameter \u03b1, maximum number of iterations Maxep, maximum AL query sample number MAL, and error threshold e.\nOutput: Final model parameters \u03b7\u2217A = { \u03b8\u2217A, w \u2217 A }\n, \u03b7\u2217B = {\u03b8\u2217B, w\u2217B}. 1: Initializing model parameters \u03b70A = { \u03b80A, w 0 A } , \u03b70B = { \u03b80B, w 0 B }\n. 2: The query number of AL qey = 0. 3: while qey \u2264 MAL do 4: Construct semi-supervised deep kernel regression models fA(x|\u03b70A), fB(x|\u03b70B) using XL. 5: Iteration number i = 0. 6: while i \u2264 Maxep do 7: Calculate the loss function of the models based on the labeled and unlabeled datasets. 8: Lilabeled = 1 L \u2211x\u2208XL Llikelihoood(\u03b7 i A/B) 9: Liunlabeled = 1 U \u2211x\u2208XU Luncertainty(\u03b7 i A/B)\n10: LiSS = L i labeled + L i unlabeled 11: Compute the derivative of the loss function LiSS with respect to the parameter\n\u03b7iA/B = { (\u03b8iA/B, w i A/B) } and update the parameter using stochastic gradient\ndescent (SGD). 12: \u03b8i+1A/B \u2190 SGD(L i SS, lr, \u03b8 i A/B) 13: wi+1A/B \u2190 SGD(L i SS, lr, w i A/B) 14: if abs(LiSS, L i\u22121 SS ) \u2264 e then 15: End the training. 16: end if 17: i = i + 1 18: end while 19: Calculate the outputs of the models for unlabeled data, fA(x|x \u2208 XU), fB(x|x \u2208 XU).\n20: Sort ( fA(x)\u2212 fB(x))2 and take the first K maxima to form the unlabeled data subset. 21: Calculate the sample farthest from XL in the subset, query its label, add it to the labeled dataset XL, and remove it from XU . 22: qey = qey + 1 23: end while 24: return: Model parameters \u03b7\u2217A = { \u03b8\u2217A, w \u2217 A } , \u03b7\u2217B = {\u03b8\u2217B, w\u2217B}\nFigure 1 shows the algorithm flow of SSDKAL. First, the training samples pass through the steps of phase partition, phase matching, and feature extraction to obtain the initial dataset, which includes the labeled sample set and the unlabeled sample set. According to the SSDKAL algorithm, the DKL model is built using the labeled sample set, and the model parameters are updated based on the log marginal likelihood function of the\nlabeled samples and the uncertainty of the unlabeled samples. The model predicts the outputs of the unlabeled samples and selects the samples that satisfy the informativeness, representativeness, and diversity according to the AL strategy of combining QBC and GS, querying the labels, and adding them to the labeled sample set. For the test set, the phase partition, phase matching, and feature extraction are similar to the training set. After the phase partition for each wafer, the SSN and SSPC obtained from the training set are applied to complete the phase matching, and the final model is used for the prediction."
        },
        {
            "heading": "4. Experiments and Discussion",
            "text": ""
        },
        {
            "heading": "4.1. Datasets",
            "text": "The dataset is from the 2016 Prognostics and Health Management (PHM16) Data Challenge and contains a set of CMP process data. The dataset is divided into a training set, a validation set, and a test set, each of which contains process data from several wafers. The number of sampling points for each wafer is between roughly 200 and 400, and each sampling point contains 25 variables, as shown in Table 1. Among them, 6 variables are device and wafer information, and the remaining 19 variables are process variables, which are divided into 5 groups, including usage, pressure, flow, rotation, and status. Each wafer is identified based on two variables, \u201cWafer_ID\u201d and \u201cStage\u201d, and corresponds to an MRR value. Figure 2 shows the trajectories of some variables of a wafer, including six pressure variables, three flow variables, and three rotation variables. It can be seen these variables have different relationships in different phases, which also indicates that the CMP process is a multi-phase process.\nAll wafers can be divided into three modes based on the \u201cChamber\u201d and \u201cStage\u201d. Table 2 shows the distribution of three modes, including the chamber, stage, and the range of the MRR, as well as the numbers of samples in the training set, validation set, and test set."
        },
        {
            "heading": "4.2. Feature Extraction",
            "text": "For each wafer, we perform phase partition using the WKM-PPCI algorithm. During the CMP process, different variables have different trends. Among them, the usage variables have an increasing trend and do not conform to the phase change characteristics. The pressure, flow, and rotation variables have phase change characteristics, and the pressure variables have more obvious changes and coincide with the switching of chambers at certain change time points. Therefore, six pressure variables are used for clustering and phase partition. Given the phase number C, integers of [4, 15] are taken, and the number corresponding to the smallest PPCI is chosen as the phase number of this wafer. After the phase partition, the minimum number of sampling points LS = 5 contained in the stable phase is selected\naccording to the phase alignment algorithm to distinguish the stable phases and the transition phases for each wafer. The plural of all the wafers\u2019 stable phases is chosen as the SSN, and in this experiment, the SSN = 5. The SSPC is calculated iteratively, and the phases of all the wafers are matched with the SSPC using a greedy algorithm. Finally, all the wafers are divided into five phases. Figure 3 shows the phase partition and phase match of a wafer. In Figure 3, the horizontal axis indicates the time order of the sampling points, and the vertical axis indicates the normalized variable values. The colored solid lines indicate the variation of the 6 pressure variables along time. The vertical dashed line indicates the optimal phase partition results after using the phase partition algorithm with the combination of WKM and PPCI. It can be seen that due to the presence of the transition phases, the phase partition is very detailed in the transition phases, and the optimal number of phases is large, which is 15 in the case of this wafer. The solid line on the vertical axis and the number between the solid lines indicate the results of the phase match. After the phase match, 15 phases are matched with 5 standard stable phase centers.\nAs shown in Table 3, for each phase, the pressure, flow, and rotation variables coincide with the change in the phases, and five statistical features are extracted, including the mean, standard deviation (std), median, peak-to-peak (PtP), and area under the curve (AUC). The status variable has only two states of 0 and 1, the median and PtP are meaningless, and the mean and AUC are redundant variables; only the mean and std features are retained. The usage variable is an incremental variable independent of the phase change, and only the initial value feature is extracted. In addition, the start time and duration are extracted from the timestamp variable. Seventy variables are extracted for each phase, forming a large feature set. The filtered feature selection method is used, which can filter out single numerical features, low-variance features, high-linear-correlation features, etc."
        },
        {
            "heading": "4.3. Prediction Results of Different Regression Models",
            "text": "After obtaining the phase features, we apply different models to the PHM16 Dataset. Tables 4 and 5 show the prediction results of different types of regression models for Mode I and Mode II. The prediction results of each regression model are the average predicted values obtained after running several experiments. In this paper, the criterion for comparing the prediction results is the mean square error (MSE), as shown in Equation (18):\nMSE = 1\nNtest\nNtest \u2211 i=1 (y\u0302i \u2212 yi)2 (18)\nwhere Ntest is the number of samples in the test set, and y\u0302i and yi are the prediction and true values of the i-th test sample, respectively. The bold numbers in Tables 4 and 5 indicate the prediction error values with the best prediction performance. In our experiments, we chose four supervised regression models, including k-nearest neighbor (kNN) regression [40], support vector regression (SVR) [41], ExtraTree (ET) [42], and Gaussian process regression (GPR), and three SSR models, including Coreg-kNN, Coreg-SVR, and SSDKL, in addition to the proposed method, SSDKAL. \u201cGlobal Features\u201d indicate that the models do not use phase partition or phase match methods, and only global features are extracted from the process data. The regression models used for the \u201cGlobal Features\u201d are four supervised regression models, including kNN, SVR, ET, and GPR. The models use multiple experiments to obtain the mean value as the prediction results. Among them, Coreg is a semi-supervised method based on co-training and involves multiple regression models [43]. Coreg-kNN and Coreg-SVR denote the regression models of kNN regression and SVR, respectively. \u201cLabel_Ratio\u201d indicates the proportion of labeled samples in the training set, and the formula is shown in Equation (19):\nLabel_Ratio = L\nL + U \u00d7 100% (19)\nwhere L and U denote the numbers of labeled samples and unlabeled samples in the training set, respectively. In the supervised regression models, the number of nearest neighbors of kNN is 15, the penalty parameter of SVR is 1.0, the minimum number of samples of split points in ET is 2, and the minimum number of samples of leaf nodes is 1. In SSR, the numbers of nearest neighbors of the two kNN models chosen by Coreg-kNN are 10 and 15, and the penalty parameters of the two SVR models chosen by Coreg-SVR are 0.1 and 1.0, respectively. The front-end network of SSDKL uses multiple fully connected layers, the parameter of the activation function LeakyReLU is 0.2, and the optimizer chooses stochastic gradient descent (SGD) with a learning rate of 0.01. The main parameters of SSDKAL are the same as those of SSDKL, but multiple models need to be included as committees of AL, and the main difference between the models is the number of layers and nodes of the fully connected layers.\nThe results in Tables 4 and 5 show that the prediction errors of the different models basically show a decreasing trend as the proportion of labeled samples increases. This is an expected result, because the increase in the amount of labeled data causes the models to obtain more accurate information, and the models can fit the real variable relationships more accurately. However, there are also some models for which the prediction results fluctuate as the proportion of labeled samples increases. On the one hand, supervised regression is more prone to fluctuations, because when there is less valid information, the fitting effect of the model is far from the real situation, and an effective prediction model cannot be built. On the other hand, the labeled samples were obtained by random sampling in these experiments; thus, the informativeness and representativeness of the extracted samples vary with different proportions, leading to a situation in which the prediction results fluctuate with the proportion of labeled samples. In Tables 4 and 5, the comparisons of the prediction results of the \u201cGlobal Features\u201d and supervised regression models based on phase features demonstrate the role of the phase features in the proposed framework. For the same regression model and label ratio, the phase features contribute to the accuracy of the prediction results. The proposed phase partition and phase match methods can more fully exploit the trend of the time series data in the CMP process and obtain accurate phase features. Meanwhile, Tables 4 and 5 show that the SSDKAL algorithm, which incorporates SSR, GPR, and DKL together with AL, obtains more accurate prediction results than supervised regression and SSR for different labeled sample proportions in both Mode I and Mode II. Compared with the corresponding supervised regression methods (kNN and SVR), the semi-supervised Coreg algorithm, which obtains pseudo-labels of unlabeled data, leads to\na substantial improvement in the prediction performance. The addition of DKL leads to a further reduction in the prediction error. The SSDKAL algorithm takes the features initially extracted after phase partition and phase matching and, through information mining via DNN, fitting with a Gaussian process kernel function, and adding the uncertainty information of the unlabeled data, with a very small amount of AL query information, achieves a more accurate MRR prediction. For comparison, Table 6 shows the prediction results for Mode III. Mode III involves fewer samples, and the multi-phase characteristic is not obvious. For Mode III, we only extracted global features and did not use phase partition or phase matching. The feature number is small. Table 6 shows that the simple model (e.g., kNN) has better prediction results, and the addition of other modules increases the model\u2019s complexity and the risk of overfitting, which prevents the extraction of more feature information. Therefore, the SSDKAL algorithm is more suitable for datasets with a larger data volume and a larger number of features.\nTaking Mode I as an example, Table 7 shows the time consumption of different regression models, and the unit of data is seconds. The time for the \u201cGlobal Features\u201d and supervised regression models is the average of a single run time. The time for the SSR models and SSDKAL is the time for each training round, and the number of training rounds is set to 30. Each round of Coreg-kNN and Coreg-SVR includes the training of multiple models based on labeled data. The models select unlabeled data with high confidence, as well as their pseudo-labels, and add them to the labeled dataset. Each training round of SSDKL consists of training DKL models based on labeled data, predicting unlabeled data, calculating a loss function containing the uncertainty of the unlabeled data, and performing backpropagation. Each round of SSDKAL includes the forward computation and backpropagation of multiple models and query sample selection based on a combination of QBC and GS. Bold numbers indicate the time required by the SSDKAL model. The bold numbers in Table 7 indicate the time consumption of SSDKAL. The time consumption in Table 7 shows that the simple supervised regression models take very little time but have a lower prediction accuracy. The SSR models and SSDKAL require the prediction, selection, and training of unlabeled samples, and the time required for each training round is long. SSDKAL incorporates deep neural networks for feature extraction and applies the results of multiple regression models to the sample selection strategy of combining QBC and GS, and therefore, the time consumption is higher. However, the SSDKAL method can fully exploit the data features and improve the prediction accuracy by using unlabeled samples."
        },
        {
            "heading": "4.4. Ablation Experiments",
            "text": "In the ablation experiments, we split SSDKAL to test the prediction performance of the GPR, DNN, and DKL models separately. DNN uses multiple fully connected layers, with the top-layer features as input to the DKL model. DKL, SSDKL, and SSDKAL optimize all the hyperparameters of the deep kernel and train the network using the log marginal likelihood function to derive the parameters and perform backpropagation. The models are experimentally implemented using GPytorch [44]. The experimental results of the ablation experiments are shown in Table 8 and Figure 4 for Mode I.\nAs shown in Table 8 and Figure 4, the ablation experiments based on Mode I demonstrate the prediction performance of each model when the proportion of labeled samples is consistent, and the approximate performance ranking is SSDKAL > SSDKL > DKL > GPR > DNN. The GPR model is a typical machine learning model, which requires additional data cleaning and feature engineering. When the modeled data distribution is too complex, the availability of a priori knowledge regarding the data distribution directly determines the accuracy of the data feature engineering, which, in turn, affects the model\u2019s performance. DNN can reduce manual involvement to a greater extent via multi-level feature extraction. DKL gives full play to the advantages of both models by organically combining DNN and GPR and maintains the strong generalization performance with respect to the data, keeping the model performance high while maintaining strong generalization in regard to the data. The model prediction performance of SSDKAL is best when the proportion of labeled samples is kept the same. In the semi-supervised scenario, pure machine learning has a performance limitation. SSDKAL further enhances the model\u2019s utilization of unlabeled data by incorporating AL methods into the training mechanism of DKL by combining the prediction uncertainty of unlabeled samples with actively learned expert queries and thus improves the comprehensive modeling capacity of the dataset."
        },
        {
            "heading": "4.5. Comparisons of Different AL Strategies and Kernel Functions",
            "text": "Tables 9 and 10 show the prediction performance of the models with different AI query strategies and different kernel functions. The models used in the experiments are all SSDKAL, and the proportion of labeled samples is 0.3.\nIn Tables 9 and 10, \u201cQBC + GS\u201d indicates the query strategy using a combination of QBC and GS, i.e., the group of unlabeled samples with the largest prediction variance between different models is selected first; then, that which is farthest from the labeled samples is selected from among them and the label is queried. \u201cQBC\u201d, \u201cGS\u201d indicates that only the QBC or the GS strategy is used, respectively, and only one sample is queried. The \u201cQBC Group\u201d indicates the querying of a group of samples in an epoch using the QBC strategy, in which case the query is faster. The experimental results show that the\nstrategy combining QBC and GS, which considers informativeness, representativeness, and diversity, has the best prediction effect. RBF, cosine, Matern, and RQ are the four kernel functions in the DKL framework, and the tables show that the cosine kernel has a poor prediction performance. The CMP process is a nonlinear, multi-batch complex production process, and there is no significant correlation between the data features, which is a key factor limiting the performance of the cosine kernel. In comparison, RBF has a better fitting effect. For different application scenarios, the kernel functions and related hyperparameters need to be flexibly selected based on domain knowledge in order to achieve the maximum performance of the model."
        },
        {
            "heading": "4.6. Effects of the SSDKAL Method",
            "text": "Table 11 presents the results of existing methods for MRR prediction and compares them with the proposed SSDKAL method. The data shown are the MSE. The ratio of labeled samples for the SSDKAL method is 0.7. SSDKAL achieves good prediction results without using all the labeled samples. In the feature extraction stage, SSDKAL takes into account the unequal length and multi-phase characteristics of the CMP process and extracts detailed process features through phase partition, phase matching, and phase feature extraction. DKL obtains the phase features through deep neural networks and further mines the depth features and the deep associations between variables. SSR adds the uncertainty of unlabeled samples to the loss function. The AL sample selection strategy compensates for the lack of representativeness and diversity of samples in SSR and makes full use of unlabeled sample information.\nFigure 5 demonstrates the prediction accuracy of SSDKAL for a labeled sample ratio of 0.7. Figure 5a shows the predicted values compared with the actual values, Figure 5b shows the distribution of the residuals, and Figure 5c shows the linear analysis of the predicted and actual values. It can be seen that SSDKAL can predict the MRR more accurately."
        },
        {
            "heading": "4.7. Limitations",
            "text": "The proposed method, SSDKAL, still has some limitations and drawbacks. Firstly, due to the small number of samples, the query samples for AL are still affected by the problem of insufficient representativeness and diversity. Since the query samples have an incomplete coverage of the overall distribution of the samples, the model\u2019s estimation of the sample distribution is biased, which affects the prediction accuracy. Secondly, in the SSDKAL model, the deep kernel mapping of the Gaussian DKL model uses fully connected layers, which is insufficient for the extraction and mining of the depth features of the sample data. Finally, the AL sample selection strategy is based on the data features of the samples. However, the CMP process is complex, and the utilization of information from the process data is incomplete if knowledge obtained a priori, such as the process mechanism, is completely absent."
        },
        {
            "heading": "5. Conclusions",
            "text": "In this paper, we proposed a VM method known as SSDKAL for MRR prediction in the CMP process. In the feature extraction stage, the phase information is fully mined, and the phase features are extracted using phase partition and phase-matching algorithms. In the modeling stage, the combination of deep neural networks and Gaussian process regression enable the deep mining of feature information. Semi-supervised regression and active learning sample query strategies make full use of the information of unlabeled samples. The proposed method was validated based on the CMP process dataset and achieved a better prediction accuracy than supervised regression and semi-supervised regression. Our future work will focus on applying deep learning methods to data in the CMP process. To date, deep learning has yielded rich research results for both image and time series data, while more accurate and deep feature extraction and processing could be accomplished. The 3D data can be considered as both image and time series data. Therefore, deep learning will play an important role in data research on the CMP process.\nAuthor Contributions: Conceptualization, C.L. and J.H.; methodology, C.L. and M.Z.; software, C.L., J.H. and M.Z.; validation, H.W., and T.Z.; formal analysis, C.L. and M.Z.; data curation, C.L.; writing\u2014original draft preparation, C.L.; writing\u2014review and editing, H.W. and T.Z.; supervision, T.Z. All authors have read and agreed to the published version of the manuscript.\nFunding: This research received no external funding.\nInstitutional Review Board Statement: Not applicable.\nInformed Consent Statement: Not applicable.\nData Availability Statement: Not applicable.\nConflicts of Interest: The authors declare no conflict of interest.\nReferences 1. Preston, F. The theory and design of plate glass polishing machines. J. Glass Technol. 1927, 11, 214\u2013256. 2. Yeh, H.M.; Chen, K.S. Development of a pad conditioning simulation module with a diamond dresser for CMP applications. Int. J. Adv. Manuf. Technol. 2010, 50, 1\u201312. [CrossRef] 3. Shin, C.; Kulkarni, A.; Kim, K.; Kim, H.; Jeon, S.; Kim, E.; Jin, Y.; Kim, T. Diamond structure-dependent pad and wafer polishing performance during chemical mechanical polishing. Int. J. Adv. Manuf. Technol. 2018, 97, 563\u2013571. [CrossRef] 4. Liu, D.; Chen, G.; Hu, Q. Material removal model of chemical mechanical polishing for fused silica using soft nanoparticles. Int. J. Adv. Manuf. Technol. 2017, 88, 3515\u20133525. [CrossRef] 5. Zhao, S.; Huang, Y. A stack fusion model for material removal rate prediction in chemical-mechanical planarization process. Int. J. Adv. Manuf. Technol. 2018, 99, 2407\u20132416. [CrossRef] 6. Li, Z.; Wu, D.; Yu, T. Prediction of material removal rate for chemical mechanical planarization using decision tree-based ensemble learning. J. Manuf. Sci. Eng. 2019, 141, 031003. [CrossRef] 7. Cai, H.; Feng, J.; Yang, Q.; Li, W.; Li, X.; Lee, J. A virtual metrology method with prediction uncertainty based on Gaussian process for chemical mechanical planarization. Comput. Ind. 2020, 119, 103228. [CrossRef] 8. Cai, H.; Feng, J.; Zhu, F.; Yang, Q.; Li, X.; Lee, J. Adaptive virtual metrology method based on Just-in-time reference and particle filter for semiconductor manufacturing. Measurement 2021, 168, 108338. [CrossRef] 9. Lee, K.B.; Kim, C.O. Recurrent feature-incorporated convolutional neural network for virtual metrology of the chemical mechanical planarization process. J. Intell. Manuf. 2020, 31, 73\u201386. [CrossRef] 10. Zhang, M.; Wang, D.; Amaitik, N.; Xu, Y. A Distributional Perspective on Remaining Useful Life Prediction With Deep Learning and Quantile Regression. IEEE Open J. Instrum. Meas. 2022, 1, 1\u201313. [CrossRef] 11. Yu, T.; Li, Z.; Wu, D. Predictive modeling of material removal rate in chemical mechanical planarization with physics-informed machine learning. Wear 2019, 426, 1430\u20131438. [CrossRef] 12. Lim, K.L.; Dutta, R. Material removal rate prediction using the classification-regression approach. In Proceedings of the 2020 IEEE 22nd Electronics Packaging Technology Conference (EPTC), Singapore, 2\u20134 December 2020; pp. 172\u2013175. 13. Zhang, M.; Amaitik, N.; Wang, Z.; Xu, Y.; Maisuradze, A.; Peschl, M.; Tzovaras, D. Predictive maintenance for remanufacturing based on hybrid-driven remaining useful life prediction. Appl. Sci. 2022, 12, 3218. [CrossRef] 14. Ma, Q.; Zhang, M.; Xu, Y.; Song, J.; Zhang, T. Remaining Useful Life Estimation for Turbofan Engine with Transformer-based\nDeep Architecture. In Proceedings of the 2021 26th International Conference on Automation and Computing (ICAC), Portsmouth, UK, 2\u20134 September 2021; pp. 1\u20136.\n15. Lim, K.L.; Dutta, R. Prognostics and Health Management of Wafer Chemical-Mechanical Polishing System using Autoencoder. In Proceedings of the 2021 IEEE International Conference on Prognostics and Health Management (ICPHM), Detroit, MI, USA, 7\u20139 June 2021; pp. 1\u20138. 16. Xia, L.; Zheng, P.; Huang, X.; Liu, C. A novel hypergraph convolution network-based approach for predicting the material removal rate in chemical mechanical planarization. J. Intell. Manuf. 2022, 33, 2295\u20132306. [CrossRef] 17. Maggipinto, M.; Terzi, M.; Masiero, C.; Beghi, A.; Susto, G.A. A computer vision-inspired deep learning architecture for virtual metrology modeling with 2-dimensional data. IEEE Trans. Semicond. Manuf. 2018, 31, 376\u2013384. [CrossRef] 18. Wu, X.; Chen, J.; Xie, L.; Chan, L.L.T.; Chen, C.I. Development of convolutional neural network based Gaussian process regression to construct a novel probabilistic virtual metrology in multi-stage semiconductor processes. Control. Eng. Pract. 2020, 96, 104262. [CrossRef] 19. Rasmussen, C.E. Gaussian processes in machine learning. In Proceedings of the Summer School on Machine Learning, Tubingen, Germany, 4\u201316 August 2003; Springer: Berlin/Heidelberg, Germany, 2003; pp. 63\u201371. 20. Rasmussen, C.; Ghahramani, Z. Infinite mixtures of Gaussian process experts. In Proceedings of the Advances in Neural Information Processing Systems 14 (NIPS 2001), Vancouver, BC, Canada, 3\u20138 December 2001; Volume 14. 21. Wilson, A.G.; Hu, Z.; Salakhutdinov, R.; Xing, E.P. Deep kernel learning. In Proceedings of the Artificial Intelligence and Statistics, Cadiz, Spain, 9\u201311 May 2016; pp. 370\u2013378. 22. Settles, B. Active Learning Literature Survey. 2009. Available online: http://digital.library.wisc.edu/1793/60660 (accessed on 10 March 2023). 23. Wu, D. Pool-based sequential active learning for regression. IEEE Trans. Neural Networks Learn. Syst. 2018, 30, 1348\u20131359. [CrossRef] 24. Seung, H.S.; Opper, M.; Sompolinsky, H. Query by committee. In Proceedings of the Fifth Annual Workshop on Computational Learning Theory, Pittsburgh, Pennsylvania, 27\u201329 July 1992; pp. 287\u2013294. 25. Freund, Y.; Seung, H.S.; Shamir, E.; Tishby, N. Selective sampling using the query by committee algorithm. Mach. Learn. 1997, 28, 133\u2013168. [CrossRef] 26. Krogh, A.; Vedelsby, J. Neural network ensembles, cross validation, and active learning. In Advances in Neural Information Processing Systems 7; MIT Press: Cambridge, MA, USA, 1994. 27. RayChaudhuri, T.; Hamey, L. Minimisation of data collection by active learning. In Proceedings of the ICNN\u201995-International Conference on Neural Networks, Perth, WA, Australia, 27 November\u20131 December 1995; Volume 3, pp. 1338\u20131341.\n28. Cai, W.; Zhang, Y.; Zhou, J. Maximizing expected model change for active learning in regression. In Proceedings of the 2013 IEEE 13th International Conference on Data Mining, Dallas, TX, USA, 7\u201310 December 2013; pp. 51\u201360. 29. Yu, H.; Kim, S. Passive sampling for regression. In Proceedings of the 2010 IEEE International Conference on Data Mining, Sydney, NSW, Australia, 13\u201317 December 2010; pp. 1151\u20131156. 30. Wu, D.; Lin, C.T.; Huang, J. Active learning for regression using greedy sampling. Inf. Sci. 2019, 474, 90\u2013105. [CrossRef] 31. Camacho, J.; Pic\u00f3, J. Online monitoring of batch processes using multi-phase principal component analysis. J. Process. Control 2006, 16, 1021\u20131035. [CrossRef] 32. Zhao, C.; Sun, Y. Step-wise sequential phase partition (SSPP) algorithm based statistical modeling and online process monitoring. Chemom. Intell. Lab. Syst. 2013, 125, 109\u2013120. [CrossRef] 33. Leiva, L.A.; Vidal, E. Warped k-means: An algorithm to cluster sequentially-distributed data. Inf. Sci. 2013, 237, 196\u2013210. [CrossRef] 34. Kostopoulos, G.; Karlos, S.; Kotsiantis, S.; Ragos, O. Semi-supervised regression: A recent review. J. Intell. Fuzzy Syst. 2018, 35, 1483\u20131500. [CrossRef] 35. Ge, Z.; Huang, B.; Song, Z. Mixture semisupervised principal component regression model and soft sensor application. AIChE J. 2014, 60, 533\u2013545. [CrossRef] 36. Zheng, J.; Song, Z. Semisupervised learning for probabilistic partial least squares regression model and soft sensor application. J. Process. Control 2018, 64, 123\u2013131. [CrossRef] 37. Shao, W.; Ge, Z.; Song, Z. Semi-supervised mixture of latent factor analysis models with application to online key variable estimation. Control Eng. Pract. 2019, 84, 32\u201347. [CrossRef] 38. Jean, N.; Xie, S.M.; Ermon, S. Semi-supervised deep kernel learning: Regression with unlabeled data by minimizing predictive\nvariance. In Proceedings of the Neural Information Processing Systems (NeurIPS), Montreal, QC, Canada, 3\u20138 December 2018; Volume 31.\n39. Zhao, M.; Chow, T.W.; Wu, Z.; Zhang, Z.; Li, B. Learning from normalized local and global discriminative information for semi-supervised regression and dimensionality reduction. Inf. Sci. 2015, 324, 286\u2013309. [CrossRef] 40. Cover, T.; Hart, P. Nearest neighbor pattern classification. IEEE Trans. Inf. Theory 1967, 13, 21\u201327. [CrossRef] 41. Cortes, C.; Vapnik, V. Support-vector networks. Mach. Learn. 1995, 20, 273\u2013297. [CrossRef] 42. Geurts, P.; Ernst, D.; Wehenkel, L. Extremely randomized trees. Mach. Learn. 2006, 63, 3\u201342. [CrossRef] 43. Zhou, Z.H.; Li, M. Semisupervised regression with cotraining-style algorithms. IEEE Trans. Knowl. Data Eng. 2007, 19, 1479\u20131493. [CrossRef] 44. Gardner, J.; Pleiss, G.; Weinberger, K.Q.; Bindel, D.; Wilson, A.G. Gpytorch: Blackbox matrix-matrix gaussian process inference\nwith gpu acceleration. In Proceedings of the Neural Information Processing Systems (NeurIPS), Montreal, QC, Canada, 3\u20138 December 2018; Volume 31.\n45. Wang, P.; Gao, R.X.; Yan, R. A deep learning-based approach to material removal rate prediction in polishing. CIRP Ann. 2017, 66, 429\u2013432. [CrossRef]\nDisclaimer/Publisher\u2019s Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content."
        }
    ],
    "title": "Semi-Supervised Deep Kernel Active Learning for Material Removal Rate Prediction in Chemical Mechanical Planarization",
    "year": 2023
}