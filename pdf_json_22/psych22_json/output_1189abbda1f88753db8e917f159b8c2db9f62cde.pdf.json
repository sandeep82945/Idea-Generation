{
    "abstractText": "This paper describes the contribution of team PHG to the WASSA 2022 shared task on Empathy Prediction and Emotion Classification. The broad goal of this task was to model an empathy score, a distress score, and the type of emotion associated with the person who had reacted to the essay written in response to a newspaper article. We have used the RoBERTa model for training, and on top of it, five layers are added to finetune the transformer. We also use a few machine learning techniques to augment and upsample the data. Our system achieves a Pearson Correlation Coefficient of 0.488 on Task 1 (Average of Empathy 0.470 and Distress 0.506) and Macro F1-score of 0.531 on Task 2.",
    "authors": [
        {
            "affiliations": [],
            "name": "Himil Vasava"
        },
        {
            "affiliations": [],
            "name": "Pramegh Uikey"
        },
        {
            "affiliations": [],
            "name": "Gaurav Wasnik"
        },
        {
            "affiliations": [],
            "name": "Raksha Sharma"
        }
    ],
    "id": "SP:18b5ccacad385134d7cfc2d34baf3dec4a26bb95",
    "references": [
        {
            "authors": [
                "C Daniel Batson",
                "Jim Fultz",
                "Patricia A Schoenrade."
            ],
            "title": "Distress and empathy: Two qualitatively distinct vicarious emotions with different motivational consequences",
            "venue": "Journal of personality, 55(1):19\u201339.",
            "year": 1987
        },
        {
            "authors": [
                "Jacob Benesty",
                "Jingdong Chen",
                "Yiteng Huang",
                "Israel Cohen."
            ],
            "title": "Pearson correlation coefficient",
            "venue": "Noise reduction in speech processing, pages 1\u20134. Springer.",
            "year": 2009
        },
        {
            "authors": [
                "Sven Buechel",
                "Anneke Buffone",
                "Barry Slaff",
                "Lyle Ungar",
                "Jo\u00e3o Sedoc."
            ],
            "title": "Modeling empathy and distress in reaction to news stories",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4758\u20134765, Brussels,",
            "year": 2018
        },
        {
            "authors": [
                "Sergey Edunov",
                "Myle Ott",
                "Michael Auli",
                "David Grangier."
            ],
            "title": "Understanding back-translation at scale",
            "venue": "arXiv preprint arXiv:1808.09381.",
            "year": 2018
        },
        {
            "authors": [
                "Paul Ekman",
                "Wallace V Friesen."
            ],
            "title": "Constants across cultures in the face and emotion",
            "venue": "Journal of personality and social psychology, 17(2):124.",
            "year": 1971
        },
        {
            "authors": [
                "Sarah Fabi",
                "Lydia Anna Weber",
                "Hartmut Leuthold."
            ],
            "title": "Empathic concern and personal distress depend on situational but not dispositional factors",
            "venue": "PloS one, 14(11):e0225102.",
            "year": 2019
        },
        {
            "authors": [
                "Justin M Johnson",
                "Taghi M Khoshgoftaar."
            ],
            "title": "Survey on deep learning with class imbalance",
            "venue": "Journal of Big Data, 6(1):1\u201354.",
            "year": 2019
        },
        {
            "authors": [
                "Jungo Kasai",
                "Hao Peng",
                "Yizhe Zhang",
                "Dani Yogatama",
                "Gabriel Ilharco",
                "Nikolaos Pappas",
                "Yi Mao",
                "Weizhu Chen",
                "Noah A Smith."
            ],
            "title": "Finetuning pretrained transformers into rnns",
            "venue": "arXiv preprint arXiv:2103.13076.",
            "year": 2021
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980.",
            "year": 2014
        },
        {
            "authors": [
                "Meera Komarraju",
                "Steven J Karau",
                "Ronald R Schmeck",
                "Alen Avdic."
            ],
            "title": "The big five personality traits, learning styles, and academic achievement",
            "venue": "Personality and individual differences, 51(4):472\u2013477.",
            "year": 2011
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Alejandro Moreo",
                "Andrea Esuli",
                "Fabrizio Sebastiani."
            ],
            "title": "Distributional random oversampling for imbalanced text classification",
            "venue": "Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval,",
            "year": 2016
        },
        {
            "authors": [
                "Adam Paszke",
                "Sam Gross",
                "Francisco Massa",
                "Adam Lerer",
                "James Bradbury",
                "Gregory Chanan",
                "Trevor Killeen",
                "Zeming Lin",
                "Natalia Gimelshein",
                "Luca Antiga"
            ],
            "title": "Pytorch: An imperative style, high-performance deep learning library",
            "year": 2019
        },
        {
            "authors": [
                "Amane Sugiyama",
                "Naoki Yoshinaga."
            ],
            "title": "Data augmentation using back-translation for contextaware neural machine translation",
            "venue": "Proceedings of the Fourth Workshop on Discourse in Machine Translation (DiscoMT 2019), pages 35\u201344.",
            "year": 2019
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems, 30.",
            "year": 2017
        },
        {
            "authors": [
                "Jonathan J Webster",
                "Chunyu Kit."
            ],
            "title": "Tokenization as the initial phase in nlp",
            "venue": "COLING 1992 Volume 4: The 14th International Conference on Computational Linguistics.",
            "year": 1992
        }
    ],
    "sections": [
        {
            "text": "Proceedings of the 12th Workshop on Computational Approaches to Subjectivity, Sentiment & Social Media Analysis, pages 261 - 264 May 26, 2022 c\u00a92022 Association for Computational Linguistics"
        },
        {
            "heading": "1 Introduction",
            "text": "Empathy and Distress are quite important regarding human health. Emotion classification in natural languages has been studied for over two decades, and many applications successfully used emotion as their principal component. Empathy utterances can be emotional. Therefore, examining emotion in text-based empathy has a significant impact on predicting empathy. Empathic concern and personal distress are empathic responses that may result when observing someone in discomfort (Fabi et al., 2019). Some news stories are also displayed in this task, and people have reacted to them. The news is disturbing or discomforting to some people. And hence, regarding that, their empathy and distress are noted. This paper presents the WASSA 2022 Shared Task: Predicting Empathy and Emotion in Reaction to News Stories. This shared task included four individual tasks where teams developed models to predict Emotions, empathy, and personality in essays in which people expressed their empathy and distress in reaction to news articles in which an individual or a group of people were harmed. Additionally, the dataset also included the demographic information of the authors\nof the essays, such as age, gender, ethnicity, income, education level, and personality information. The shared task consisted of four tracks (optional): Track 1: Empathy Prediction (EMP) task consists of predicting both the empathy concern and the personal distress. (Evaluation based on an average of Pearson correlation (Benesty et al., 2009) of empathy and distress). Track 2: Emotion Classification (EMO) consists of predicting the emotion (sadness, joy, disgust, surprise, anger, or fear, taken from the six basic emotions (Ekman and Friesen, 1971) also including neutral) at the essay-level (Evaluation based on the macro F1-score). Track 3: Personality Prediction (PER), which consists in predicting the personality of the essay writer, knowing all their essays and the news article from which they reacted (Evaluation based on the average of Pearson correlation over Personality values (Komarraju et al., 2011) - conscientiousness, Openness, Extraversion, Agreeableness, and Stability). Track 4: Interpersonal Reactivity Index Prediction (IRI) consists of predicting the personality of the essay writer. (Evaluation based on an average of Pearson correlation over IRI values - fantasy, perspective taking, empathetic concern, personal distress). We participated in only the first two tasks."
        },
        {
            "heading": "2 Related Work",
            "text": "Over the last few years, earnest endeavors have been made in the NLP community to analyze empathy and distress. For text-based empathy prediction, (Buechel et al., 2018) laid a firm foundation for predicting Batson\u2019s (Batson et al., 1987) empathic concern and personal distress scores in reaction to news articles. They present the first publicly available gold-standard dataset for text-based empathy and distress prediction. To annotate emotions in text, classical studies in NLP suggest categorical\n261\ntagsets, and most studies are focussed on basic emotion models that psychological emotion models offer. The most popular one is the Ekman 6 basic emotions (Ekman and Friesen, 1971). The emotions presented in this dataset are the same six emotions by Ekman plus one extra emotion (neutral)."
        },
        {
            "heading": "3 Dataset",
            "text": "The dataset is an extension to the one provided by (Buechel et al., 2018). For all the tasks, a traindev-test split was provided. The dataset consists of essays collected from participants who had read news articles about a person, a group of people, or disturbing situations. The dataset had an essay (300-800 characters), empathy score, a distress score, emotion label, and other demographic information (age, gender, race, education, income) as well as personality information (conscientiousness, openness, extraversion, agreeableness, stability) and interpersonal reactivity index (IRI) scores (fantasy, perspective taking, empathetic concern, personal distress)."
        },
        {
            "heading": "3.1 Data Augmentation",
            "text": "A single sentence does not always convey the information required to translate it into other languages; we sometimes need to specialize words that are ambiguous in the source languages (Sugiyama and Yoshinaga, 2019). So, we used back translation (Edunov et al., 2018) for text augmentation. The\nidea here was to have different sentences having the same meaning for training. Step 1: Select the essay (English). Step 2: Select a random language and convert the essay to that language. Step 3: Now translate that converted essay back to English.\nWe used Google translate API for translating essays back and forth. Every example was translated to one other language, and hence after back translation, the total number of samples was doubled (3720). Data augmentation improved the performance, as shown in the Table 2."
        },
        {
            "heading": "4 System Description",
            "text": ""
        },
        {
            "heading": "4.1 Empathy Prediction",
            "text": "Transformers (Vaswani et al., 2017) have outperformed recurrent neural networks (RNNs) in natural language generation (Kasai et al., 2021). For this task, we had to predict empathy and distress scores which had been done by training the same model by keeping the targets different (empathy for model 1 and distress for model 2). The approach used is based on fine-tuning RoBERta model (Liu et al., 2019) separately for empathy and distress. To take the essay as input to the RoBERTa model, initially, tokenization (Webster and Kit, 1992) was required. The input tokens were made using the Roberta Tokenizer imported from the transformer library. The loss function used was Mean Squared Error (MSE). No parameters were frozen (all of them were trainable), and on top of it, five layers were trained (to make the network deeper). Four layers were linear, while one was a dropout layer (to prevent overfitting). In the pre-final layer, five additional demographic features were taken as input.\nThe model was trained on both the augmented\ndata and original data. Still, the final submission was made using the model trained on the augmented data as it resulted in a higher Pearson Correlation Coefficient."
        },
        {
            "heading": "4.2 Emotion Classification",
            "text": "This was a multi-classification task, i.e., to classify the emotions into seven labels. Here also, we fine-tuned RoBERTa model (Liu et al., 2019) with the same five layers, just changing the output neurons to 7 instead of 1. We had used Cross-Entropy Loss as the loss function (which already has a softmax layer). We also upsampled the dataset as it was imbalanced. Highly imbalanced data poses added difficulty, as most learners will exhibit bias towards the majority class and, in extreme cases, may ignore the minority class altogether (Johnson and Khoshgoftaar, 2019). Random over-sampling (Moreo et al., 2016) was performed using the imblearn library. The imbalanced dataset can be seen in figure 2, the minority class being the emotion labeled \"joy\"."
        },
        {
            "heading": "4.3 Hyperparameters and other settings",
            "text": "For all the tasks, the learning rate was set to 10\u22125, and the models were trained using Adam (Kingma and Ba, 2014) as optimizer. The parameters of\nAdam were Beta(0.9, 0.999) and weight decay as 0. The batch size was set to 8. The dataset was shuffled using Pytorch (Paszke et al., 2019) data loader. All the models were trained on the GPUs provided by Google Colab."
        },
        {
            "heading": "5 Results",
            "text": "Our system achieved a Pearson Correlation Coefficient (Benesty et al., 2009) of 0.488 on Task 1. Empathy Pearson Correlation was 0.470, and Distress Pearson Correlation was 0.506. Hence, the average of both was taken as the final score. In the development set, the empathy score was 0.4583 (after the 8th epoch), and the distress score was 0.4415 (after the 4th epoch, as after the score was decreased due to overfitting). Although the empathy score was slightly high, it yielded less score in the test set due to overfitting. While due to early stopping, distress yielded a better score.\nWe had two different submissions for the emotion classification, one with augmentation and upsampling and one without altering the data. The test scores of both submissions are mentioned in\nTable 2. Also, the results of the development set are plotted in figure 4. We tested until ten epochs but decided to submit the model, trained only up to eight epochs as it was overfitting. Hence, the macro F1-score decreased on the development set despite accuracy increasing on the training set."
        },
        {
            "heading": "6 Conclusion",
            "text": "This paper describes our submission to the WASSA 2022 shared task, where we have used the already trained RoBERTa model on a large dataset and then used its power by just finetuning on the given dataset. By the approach we have used, it can also be deduced that text augmentation and upsampling helped in emotion classification and predicting the empathy and distress scores as most of the time, the larger amount of data helps improve the training process of a model."
        }
    ],
    "title": "Transformer-based Architecture for Empathy Prediction and Emotion Classification",
    "year": 2022
}