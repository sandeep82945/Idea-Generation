{
    "abstractText": "In this paper we explore the use of selectional preferences for detecting noncompositional verb-object combinations. To characterise the arguments in a given grammatical relationship we experiment with three models of selectional preference. Two use WordNet and one uses the entries from a distributional thesaurus as classes for representation. In previous work on selectional preference acquisition, the classes used for representation are selected according to the coverage of argument tokens rather than being selected according to the coverage of argument types. In our distributional thesaurus models and one of the methods using WordNet we select classes for representing the preferences by virtue of the number of argument types that they cover, and then only tokens under these classes which are representative of the argument head data are used to estimate the probability distribution for the selectional preference model. We demonstrate a highly significant correlation between measures which use these \u2018typebased\u2019 selectional preferences and compositionality judgements from a data set used in previous research. The type-based models perform better than the models which use tokens for selecting the classes. Furthermore, the models which use the automatically acquired thesaurus entries produced the best results. The correlation for the thesaurus models is stronger than any of the individual features used in previous research on the same dataset.",
    "authors": [
        {
            "affiliations": [],
            "name": "Diana McCarthy"
        },
        {
            "affiliations": [],
            "name": "Sriram Venkatapathy"
        },
        {
            "affiliations": [],
            "name": "Aravind K. Joshi"
        }
    ],
    "id": "SP:081d0c66d5647ce05ab789eeb99ed10616e57658",
    "references": [
        {
            "authors": [
                "Steven Abney",
                "Marc Light."
            ],
            "title": "Hiding a semantic class hierarchy in a Markov model",
            "venue": "Proceedings of the ACL Workshop on Unsupervised Learning in Natural Language Processing, pages 1\u20138.",
            "year": 1999
        },
        {
            "authors": [
                "Timothy Baldwin",
                "Colin Bannard",
                "Takaaki Tanaka",
                "Dominic Widdows."
            ],
            "title": "An empirical model of multiword expression decomposability",
            "venue": "Proceedings of the ACL Workshop on multiword expressions: analysis, acquisition and treatment, pages 89\u201396.",
            "year": 2003
        },
        {
            "authors": [
                "Colin Bannard",
                "Timothy Baldwin",
                "Alex Lascarides."
            ],
            "title": "A statistical approach to the semantics of verb-particles",
            "venue": "Proceedings of the ACL Workshop on multiword expressions: analysis, acquisition and treatment, pages 65\u201372.",
            "year": 2003
        },
        {
            "authors": [
                "Colin. Bannard."
            ],
            "title": "Statistical techniques for automatically inferring the semantics of verbparticle constructions",
            "venue": "Technical Report WP-200206, University of Edinburgh, School of Informatics. http://lingo.stanford.edu/pubs/WP-2002-06.pdf.",
            "year": 2002
        },
        {
            "authors": [
                "Colin Bannard."
            ],
            "title": "Learning about the meaning of verb-particle constructions from corpora",
            "venue": "Computer Speech and Language, 19(4):467\u2013478.",
            "year": 2005
        },
        {
            "authors": [
                "Daniel M. Bikel."
            ],
            "title": "A distributional analysis of a lexicalized statistical parsing model",
            "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 2004), Barcelona, Spain, July. Association for Computational Linguistics.",
            "year": 2004
        },
        {
            "authors": [
                "Don Blaheta",
                "Mark Johnson."
            ],
            "title": "Unsupervised learning of multi-word verbs",
            "venue": "Proceedings of the ACL Workshop on Collocations, pages 54\u201360, Toulouse, France.",
            "year": 2001
        },
        {
            "authors": [
                "Thorsten Brants",
                "Alex Franz"
            ],
            "title": "Web 1T 5-gram corpus version 1.1",
            "venue": "Technical Report",
            "year": 2006
        },
        {
            "authors": [
                "Edward Briscoe",
                "John Carroll."
            ],
            "title": "Robust accurate statistical annotation of general text",
            "venue": "Proceedings of the Third International Conference on Language Resources and Evaluation (LREC), pages 1499\u20131504, Las Palmas, Canary Islands, Spain.",
            "year": 2002
        },
        {
            "authors": [
                "Kenneth Church",
                "Patrick Hanks."
            ],
            "title": "Word association norms, mutual information and lexicography",
            "venue": "Computational Linguistics, 19(2):263\u2013312.",
            "year": 1990
        },
        {
            "authors": [
                "Stephen Clark",
                "David Weir."
            ],
            "title": "Class-based probability estimation using a semantic hierarchy",
            "venue": "Computational Linguistics, 28(2):187\u2013206.",
            "year": 2002
        },
        {
            "authors": [
                "Afsaneh Fazly",
                "Suzanne Stevenson."
            ],
            "title": "Automatically constructing a lexicon of verb phrase idiomatic combinations",
            "venue": "Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL-2006), pages 337\u2013344,",
            "year": 2006
        },
        {
            "authors": [
                "Christiane Fellbaum",
                "editor"
            ],
            "title": "WordNet, An Electronic Lexical Database",
            "year": 1998
        },
        {
            "authors": [
                "Ralph Grishman",
                "John Sterling."
            ],
            "title": "Generalizing automatically generated selectional patterns",
            "venue": "Proceedings of the 15th International Conference of Computational Linguistics. COLING-94, volume I, pages 742\u2013747.",
            "year": 1994
        },
        {
            "authors": [
                "Frank Keller",
                "Mirella Lapata."
            ],
            "title": "Using the web to obtain frequencies for unseen bigrams",
            "venue": "Computational Linguistics, 29(3):459\u2013484.",
            "year": 2003
        },
        {
            "authors": [
                "Brigitte Krenn",
                "Stefan Evert."
            ],
            "title": "Can we do better than frequency? A case study on extracting PP-verb collocations",
            "venue": "Proceedings of the ACL Workshop on Collocations, pages 39\u201346, Toulouse, France.",
            "year": 2001
        },
        {
            "authors": [
                "Geoffrey Leech."
            ],
            "title": "100 million words of English: the British National Corpus",
            "venue": "Language Research, 28(1):1\u201313.",
            "year": 1992
        },
        {
            "authors": [
                "Hang Li",
                "Naoki Abe."
            ],
            "title": "Generalizing case frames using a thesaurus and the MDL principle",
            "venue": "Computational Linguistics, 24(2):217\u2013244.",
            "year": 1998
        },
        {
            "authors": [
                "Dekang Lin."
            ],
            "title": "Automatic retrieval and clustering of similar words",
            "venue": "Proceedings of COLING-ACL 98, Montreal, Canada.",
            "year": 1998
        },
        {
            "authors": [
                "Dekang Lin."
            ],
            "title": "Automatic identification of noncompositional phrases",
            "venue": "Proceedings of ACL-99, pages 317\u2013324, Univeristy of Maryland, College Park, Maryland.",
            "year": 1999
        },
        {
            "authors": [
                "Diana McCarthy",
                "Bill Keller",
                "John Carroll."
            ],
            "title": "Detecting a continuum of compositionality in phrasal verbs",
            "venue": "Proceedings of the ACL 03 Workshop: Multiword expressions: analysis, acquisition and treatment, pages 73\u201380.",
            "year": 2003
        },
        {
            "authors": [
                "Diana McCarthy."
            ],
            "title": "Using semantic preferences to identify verbal participation in role switching alternations",
            "venue": "Proceedings of the First Conference of the North American Chapter of the Association for Computational Linguistics. (NAACL), pages 256\u2013263,",
            "year": 2000
        },
        {
            "authors": [
                "Fernando Pereira",
                "Nattali Tishby",
                "Lillian Lee."
            ],
            "title": "Distributional clustering of English words",
            "venue": "Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, pages 183\u2013190.",
            "year": 1993
        },
        {
            "authors": [
                "Philip Resnik."
            ],
            "title": "Selection and Information: A Class-Based Approach to Lexical Relationships",
            "venue": "Ph.D. thesis, University of Pennsylvania.",
            "year": 1993
        },
        {
            "authors": [
                "Jorma Rissanen."
            ],
            "title": "Modelling by shortest data description",
            "venue": "Automatica, 14:465\u2013471.",
            "year": 1978
        },
        {
            "authors": [
                "Ivan Sag",
                "Timothy Baldwin",
                "Francis Bond",
                "Ann Copestake",
                "Dan Flickinger."
            ],
            "title": "Multiword expressions: A pain in the neck for NLP",
            "venue": "Proceedings of the Third International Conference on Intelligent Text Processing and Computational Linguistics (CICLing",
            "year": 2002
        },
        {
            "authors": [
                "Patrick Schone",
                "Daniel Jurafsky"
            ],
            "title": "Is knowledge-free induction of multiword unit dictionary headwords a solved problem",
            "venue": "In Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2001
        },
        {
            "authors": [
                "Hinrich Sch\u00fctze."
            ],
            "title": "Automatic word sense discrimination",
            "venue": "Computational Linguistics, 24(1):97\u2013123.",
            "year": 1998
        },
        {
            "authors": [
                "Sidney Siegel",
                "N. John Castellan."
            ],
            "title": "NonParametric Statistics for the Behavioral Sciences",
            "venue": "McGraw-Hill, New York.",
            "year": 1988
        },
        {
            "authors": [
                "Suzanne Stevenson",
                "Afsaneh Fazly",
                "Ryan North."
            ],
            "title": "Statistical measures of the semi-productivity of light verb constructions",
            "venue": "Proceedings of the ACL 2004 Workshop on Multiword Expressions: Integrating Processing, Barcelona, Spain.",
            "year": 2003
        },
        {
            "authors": [
                "Vivian Tsang",
                "Suzanne Stevenson."
            ],
            "title": "Using selectional profile distance to detect verb alternations",
            "venue": "In",
            "year": 2004
        },
        {
            "authors": [
                "Sriram Venkatapathy",
                "Aravind K. Joshi."
            ],
            "title": "Measuring the relative compositionality of verb-noun (v-n) collocations by integrating features",
            "venue": "Proceedings of",
            "year": 2005
        },
        {
            "authors": [
                "Andreas Wagner."
            ],
            "title": "Learning thematic role relations for wordnets",
            "venue": "Proceedings of ESSLLI-2002 Work-",
            "year": 2002
        }
    ],
    "sections": [
        {
            "text": "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp. 369\u2013379, Prague, June 2007. c\u00a92007 Association for Computational Linguistics\nual features used in previous research on the same dataset."
        },
        {
            "heading": "1 Introduction",
            "text": "Characterising the semantic behaviour of phrases in terms of compositionality has particularly attracted attention in recent years (Lin, 1999; Schone and Jurafsky, 2001; Bannard, 2002; Bannard et al., 2003; Baldwin et al., 2003; McCarthy et al., 2003; Bannard, 2005; Venkatapathy and Joshi, 2005). Typically the phrases are putative multiwords and noncompositionality is viewed as an important feature of many such \u201cwords with spaces\u201d (Sag et al., 2002). For applications such as paraphrasing, information extraction and translation, it is essential to take the words of non-compositional phrases together as a unit because the meaning of a phrase cannot be obtained straightforwardly from the constituent words. In this work we are investigate methods of determining semantic compositionality of verb-object 1 combinations on a continuum following previous research in this direction (McCarthy et al., 2003; Venkatapathy and Joshi, 2005).\nMuch previous research has used a combination of statistics and distributional approaches whereby distributional similarity is used to compare the constituents of the multiword with the multiword itself. In this paper, we will investigate the use of selectional preferences of verbs. We will use the preferences to find atypical verb-object combinations as we anticipate that such combinations are more likely to be non-compositional.\n1We use object to refer to direct objects.\n369\nSelectional preferences of predicates have been modelled using the man-made thesaurus WordNet (Fellbaum, 1998), see for example (Resnik, 1993; Li and Abe, 1998; Abney and Light, 1999; Clark and Weir, 2002). There are also distributional approaches which use co-occurrence data to cluster distributionally similar words together. The cluster output can then be used as classes for selectional preferences (Pereira et al., 1993), or one can directly use frequency information from distributionally similar words for smoothing (Grishman and Sterling, 1994).\nWe used three different types of probabilistic models, which vary in the classes selected for representation over which the probability distribution of the argument heads 2 is estimated. Two use WordNet and the other uses the entries in a thesaurus of distributionally similar words acquired automatically following (Lin, 1998). The first method is due to Li and Abe (1998). The classes over which the probability distribution is calculated are selected according to the minimum description length principle (MDL) which uses the argument head tokens for finding the best classes for representation. This method has previously been tried for modelling compositionality of verb-particle constructions (Bannard, 2002).\nThe other two methods (we refer to them as \u2018typebased\u2019) also calculate a probability distribution using argument head tokens but they select the classes over which the distribution is calculated using the number of argument head types (of a verb in a corpus) in a given class, rather than the number of argument head tokens in contrast to previous WordNet models (Resnik, 1993; Li and Abe, 1998; Clark and Weir, 2002). For example, if the object slot of the verb park contains the argument heads { car, car, car, car, van, jeep } then the type-based models use the word type \u201ccar\u201d only once when determining the classes over which the probability distribution is to be estimated. Classes are selected which maximise the number of types that they cover, rather than the number of tokens. This is done to avoid the selectional preferences being heavily influenced by noise from highly frequent arguments which may be polysemous and some or all of their meanings may not be\n2Argument heads are the nouns occurring in the object slot of the target verb.\nsemantically related to the \u2018prototypical\u2019 arguments of the verb. For example car has a gondola sense in WordNet.\nThe third method uses entries in a distributional thesaurus rather than classes from WordNet. The entries used as classes for representation are selected by virtue of the number of argument types they encompass. As with the WordNet models, the tokens are used to estimate a probability distribution over these entries.\nIn the next section, we discuss related work on identifying compositionality. In section 3, we describe the methods we are using for acquiring our models of selectional preference. In section 4, we test our models on a dataset used in previous research. We compare the three types of models individually and also investigate the best performing model when used in combination with other features used in previous research. We conclude in section 5."
        },
        {
            "heading": "2 Related Work",
            "text": "Most previous work using distributional approaches to compositionality either contrasts distributional information of candidate phrases with constituent words (Schone and Jurafsky, 2001; Bannard et al., 2003; Baldwin et al., 2003; McCarthy et al., 2003) or uses distributionally similar words to detect nonproductive phrases (Lin, 1999).\nLin (1999) used his method (Lin, 1998) for automatic thesaurus construction. He identified candidate phrases involving several open-class words output from his parser and filtered these by the loglikelihood statistic. Lin proposed that if there is a phrase obtained by substitution of either the head or modifier in the phrase with a \u2018nearest neighbour\u2019 from the thesaurus then the mutual information of this and the original phrase must be significantly different for the original phrase to be considered noncompositional. He evaluated the output manually.\nAs well as distributional similarity, researchers have used a variety of statistics as indicators of non-compositionality (Blaheta and Johnson, 2001; Krenn and Evert, 2001). Fazly and Stevenson (2006) use statistical measures of syntactic behaviour to gauge whether a verb and noun combination is likely to be a idiom. Although they are not specifically detecting compositionality, there is a strong corre-\nlation between syntactic rigidity and semantic idiosyncrasy.\nVenkatapathy and Joshi (2005) combine different statistical and distributional methods using support vector machines (SVMs) for identifying noncompositional verb-object combinations. They explored seven features as measures of compositionality:\n1. frequency\n2. pointwise mutual information (Church and Hanks, 1990),\n3. least mutual information difference with similar collocations, based on (Lin, 1999) and using Lin\u2019s thesaurus (Lin, 1998) for obtaining the similar collocations.\n4. The distributed frequency of an object, which takes an average of the frequency of occurrence with an object over all verbs occurring with the object above a threshold.\n5. distributed frequency of an object, using the verb, which considers the similarity between the target verb and the verbs occurring with the target object above the specified threshold.\n6. a latent semantic approach (LSA) based on (Schu\u0308tze, 1998; Baldwin et al., 2003) and considering the dissimilarity of the verb-object pair with its constituent verb\n7. the same LSA approach, but considering the similarity of the verb-object pair with the verbal form of the object (to capture support verb constructions e.g. give a smile\nVenkatapathy and Joshi (2005) produced a dataset of verb-object pairs with human judgements of compositionality. We say more about this dataset and Venkatapathy and Joshi\u2019s results in section 4 since we use the dataset for our experiments.\nIn this paper, we investigate the use of selectional preferences to detect compositionality. Bannard (2002) did some pioneering work to try and establish a link between the compositionality of verb particle constructions and the selectional preferences of the multiword and its constituent verb.\nHis results were hampered by models based on (Li and Abe, 1998) which involved rather uninformative models at the roots of WordNet. There are several reasons for this. The classes for the model are selected using MDL by compromising between a simple model with few classes and one which explains the data well. The models are particularly affected by the quantity of data available (Wagner, 2002). Also noise from frequent but idiosyncratic or polysemous arguments weakens the signal. There is scope for experimenting with other approaches such as (Clark and Weir, 2002), however, we feel a type-based approach is worthwhile to avoid the noise introduced from frequent but polysemous arguments and bias from highly frequent arguments which might be part of a multiword rather than a prototypical argument of the predicate in question, for example eat hat. In contrast to Bannard, our experiments are with verb-object combinations rather than verb particle constructions. We compare Li and Abe models with WordNet models which use the number of argument types to obtain the classes for representation of the selectional preferences. In addition to experiments with these WordNet models, we propose models using entries in distributional thesauruses for representing preferences."
        },
        {
            "heading": "3 Three Methods for Acquiring Selectional Preferences",
            "text": "All models were acquired from verb-object data extracted using the RASP parser (Briscoe and Carroll, 2002) from the 90 million words of written English from the BNC (Leech, 1992). We extracted verb and common noun tuples where the noun is the argument head of the object relation. The parser was also used to extract the grammatical relation data used for acquisition of the thesaurus described below in section 3.3.\n3.1 TCMs\nThis approach is a reimplementation of Li and Abe (1998). Each selectional preference model (referred to as a tree cut model, or TCM) comprises a set of disjunctive noun classes selected from all the possibilities in the WordNet hyponym hierarchy 3 using MDL (Rissanen, 1978). The TCM covers all the\n3We use WordNet version 2.1 for the work in this paper.\nnoun senses in the WordNet hierarchy and is associated with a probability distribution over these noun senses in the hierarchy reflecting the argument head data occurring in the given grammatical relationship with the specified verb. MDL finds the classes in the TCM by considering the cost measured in bits of describing both the model and the argument head data encoded in the model. A compromise is made by having as simple a model as possible using classes further up the hierarchy whilst also providing a good model for the set of argument head tokens (TK).\nThe classes are selected by recursing from the top of the WordNet hierarchy comparing the cost (or description length) of using the mother class to the cost of using the hyponym daughter classes. In any path, the mother is preferred unless using the daughters would reduce the cost. If using the daughters for the model is less costly than the mother then the recursion continues to compare the cost of the hyponyms beneath.\nThe cost (or description length) for a set of classes is calculated as the model description length (mdl) and the data description length (ddl) 4 :-\nmdl + ddl k 2 \u00d7 log |TK| +\u2212 \u2211 tk\u2208TK log p(tk) (1)\nk, is the number of WordNet classes being currently considered for the TCM minus one. The MDL method uses the size of TK on the assumption that a larger dataset warrants a more detailed model. The cost of describing the argument head data is calculated using the log of the probability estimate from the classes currently being considered for the model. The probability estimate for a class being considered for the model is calculated using the cumulative frequency of all the hyponym nouns under that class that occur in TK , divided by the number of noun senses that these nouns have, to account for their polysemy. This cumulative frequency is also divided by the total number of noun hyponyms under that class in WordNet to obtain a smoothed estimate for all nouns under the class. The probability of the class is obtained by dividing this frequency estimate by the total frequency of the argument heads. The algorithm is described fully by Li and Abe (1998).\n4See (Li and Abe, 1998) for a full explanation.\nA small portion of the TCM for the object slot of park is shown in figure 1. WordNet classes are displayed in boxes with a label which best reflects the meaning of the class. The probability estimates are shown for the classes on the TCM. Examples of the argument head data are displayed below the WordNet classes with dotted lines indicating membership at a hyponym class beneath these classes. We cannot show the full TCM due to lack of space, but we show some of the higher probability classes which cover some typical nouns that occur as objects of park. Note that probability under the classes abstract entity, way and location arise because of a systematic parsing error where adverbials such as distance in park illegally some distance from the railway station are identified by the parser as objects. Systematic noise from the parser has an impact on all the selectional preference models described in this paper.\n3.2 WNPROTOs\nWe propose a method of acquiring selectional preferences which instead of covering all the noun senses in WordNet, just gives a probability distribution over a portion of prototypical classes, we refer to these models as WNPROTOs. A WNPROTO consists of classes within the noun hierarchy which have the highest proportion of word types occurring in the argument head data, rather than using the number of tokens, or frequency, as is used for the TCMs. This allows less frequent, but potentially informative arguments to have some bearing on the models acquired to reduce the impact of highly frequent but polysemous arguments. We then used the frequency data to populate these selected classes.\nThe classes (C) in the WNPROTO are selected from those which include at least a threshold of 2 argument head types 5 occurring in the training data. Each argument head in the training data is disambiguated according to whichever of the WordNet classes it occurs at or under which has the highest \u2018type ratio\u2019. Let TY be the set of argument head types in the object slot of the verb for which we are acquiring the preference model. The type ratio for a class (c) is the ratio of noun types (ty \u2208 TY ) occurring in the training data also listed at or beneath that class in WordNet to the total number of noun types listed at or beneath that particular class in WordNet (wnty \u2208 c). The argument types attested in the training data are divided by the number of WordNet classes that the noun (classes(ty)) belongs to, to account for polysemy in the training data.\ntype ratio(c) =\n\u2211 ty\u2208TY \u2208c 1 |classes(ty)|\n|wnty \u2208 c| (2)\nIf more than one class has the same type ratio then the argument is not used for calculating the probability of the preference model. In this way, only arguments that can be disambiguated are used for calculating the probability distribution. The advantage of using the type ratio to determine the classes used to represent the model and to disambiguate the arguments is that it prevents high frequency verb noun combinations from masking the information from prototypical but low frequency arguments. We wish to use classes which are as representative of the argument head types as possible to help detect when an argument head is not related to these classes and is therefore more likely to be non-compositional.\nFor example, the class motor vehicle is selected for the WNPROTO model of the object slot of park even though there are 5 meanings of car in WordNet including elevator car and gondola. There are 174 occurrences of car which overwhelms the frequency of the other objects (e.g. van 11, vehicle 8) but by looking for classes with a high proportion of types (rather than word tokens) car is disambiguated appropriately and the class motor vehicle is selected for representation.\n5We have experimented with a threshold of 3 and obtained similar results.\nThe relative frequency of each class is obtained from the set of disambiguated argument head tokens and used to provide the probability distribution over this set of classes. Note that in WNPROTO, classes can be subsumed by others in the hyponym hierarchy. The probability assigned to a class is applicable to any descendants in the hyponym hierarchy, except those within any hyponym classes within the WNPROTO. The algorithm for selecting C and calculating the probability distribution is shown as Algorithm 1. Note that we use brackets for comments.\nIn figure 2 we show a small portion of the WNPROTO for park. Again, WordNet classes are displayed in boxes with a label which best reflects the meaning of the class. The probability estimates are shown in the boxes for all the classes included in the WNPROTO. The classes in the WNPROTO model are shown with dashed lines. Examples of the argument head data are displayed below the WordNet classes with dotted lines indicating membership at a hyponym class beneath these classes. We cannot show the full WNPROTO due to lack of space, but we show some of the classes with higher probability which cover some typical nouns that occur as objects of park.\nAlgorithm 1 WNPROTO algorithm C = (){classes in WNPROTO} D = () {disambiguated ty \u2208 TY } fD = 0 {frequency of disambiguated items} TY = argument head types {nouns occurring as objects of verb, with associated frequencies} C1 \u2208 WordNet where |ty \u2208 TY occurring in c \u2208 C1| > 1 for all ty \u2208 TY do\nfind c \u2208 classes(ty) \u2208 C1 where c = argmaxc typeratio(c) if c & c /\u2208 C then\nadd c to C add ty \u2194 c to D {Disambiguated ty with c}\nend if end for for all c \u2208 C do\nif |ty \u2194 c \u2208 D| > 1 then fD = fD + frequency(ty){sum frequencies of types under classes to be used in model} else remove c from C {classes with less than two disambiguated nouns are removed}\nend if end for for all c \u2208 C do\np(c) = frequency-of-all-tys-disambiguated-to-class(c,D) fD\n{calculating class probabilities} end for\nAlgorithm 2 DSPROTO algorithm C = (){classes in DSPROTO} D = () {disambiguated ty \u2208 TY } fD = 0 {frequency of disambiguated items} TY = argument head types {nouns occurring as objects of verb, with associated frequencies} C1 = cty \u2208 TY where num-types-in-thesaurus(cty, TY ) > 1 order C1 by num-types-in-thesaurus(cty, TY ) {classes ordered by coverage of argument head types} for all cty \u2208 ordered C1 do\nDcty = () {disambiguated for this class} for all ty \u2208 TY where in-thesaurus-entry(cty, ty) do\nif ty /\u2208 D then add ty to Dcty {types disambiguated to this class only if not disambiguated by a class used already}\nend if end for if |Dcty| > 1 then\nadd cty to C for all ty \u2208 Dcty do\nadd ty \u2194 cty to D {Disambiguated ty with cty} fD = fD + frequency(ty)\nend for end if\nend for for all cty \u2208 C do\np(cty) = frequency-of-all-tys-disambiguated-to-class(cty,D) fD\n{calculating class probabilities} end for\n3.3 DSPROTOs\nWe use a thesaurus acquired using the method proposed by Lin (1998). For input we used the grammatical relation data from automatic parses of the BNC. For each noun we considered the cooccurring verbs in the object and subject relation, the modifying nouns in noun-noun relations and the modifying adjectives in adjective-noun relations. Each thesaurus entry consists of the target noun and the 50 most similar nouns, according to Lin\u2019s measure of distributional similarity, to the target.\nThe argument head noun types (TY ) are used to find the entries in the thesaurus as the \u2018classes\u2019 (C) of the selectional preference for a given verb. As with WNPROTOs, we only cover argument types which form coherent groups with other argument types since we wish i) to remove noise and ii) to be able to identify argument types which are not related with the other types and therefore may be noncompositional. As our starting point we only consider an argument type as a class for C if its entry in the thesaurus covers at least a threshold of 2 types. 6\nTo select C we use a best first search. This method processes each argument type in TY in order of the number of the other argument types from TY that it has in its thesaurus entry of 50 similar nouns. An argument head is selected as a class for C (cty \u2208 C) 7 if it covers at least 2 of the argument heads that are not in the thesaurus entries of any of the other classes already selected for C . Each argument head is disambiguated by whichever class in C under which it is listed in the thesaurus and which has the largest number of the TY in its thesaurus entry. When the algorithm finishes processing the ordered argument heads to select C , all argument head types are disambiguated by C apart from those which after disambiguation occur in isolation in a class without other argument types. Finally a probability distribution over C is estimated using the frequency (tokens) of argument types that occur in the thesaurus entries for any cty \u2208 C . If an argument type occurs in the entry of more than one cty then it is assigned to whichever of these has the largest number\n6As with the WNPROTOs, we experimented with a value of 3 for this threshold and obtained similar results.\n7We use cty for the classes of the DSPROTO. These classes are simply groups of nouns which occur under the entry of a noun (ty) in the thesaurus.\nof disambiguated argument head types and its token frequency is attributed to that class. We show the algorithm as Algorithm 2.\nThe algorithms for WNPROTO algorithm 1 and DSPROTO (algorithm 2) differ because of the nature of the inventories of candidate classes (WordNet and the distributional thesaurus). There are a great many candidate classes in WordNet. The WNPROTO algorithm selects the classes from all those that the argument heads belong to directly and indirectly by looping over all argument types to find the class that disambiguates each by having the largest type ratio calculated using the undisambiguated argument heads. The DSPROTO only selects classes from the fixed set of argument types. The algorithm loops over the argument types with at least two argument heads in the thesaurus entry and ordered by the number of undisambiguated argument heads in the thesaurus entry. This is a best first search to minimise the number of argument heads used in C but maximise the coverage of argument types.\nIn figure 3, we show part of a DSPROTO model for the object of park. 8 Note again that the class mile arises because of a systematic parsing error where adverbials such as distance in park illegally some distance from the railway station are identified by the parser as objects."
        },
        {
            "heading": "4 Experiments",
            "text": "Venkatapathy and Joshi (2005) produced a dataset of verb-object pairs with human judgements of compositionality. They obtained values of rs between 0.111 and 0.300 by individually applying the 7 features described above in section 2. The best correlation was given by feature 7 and the second best was feature 3. They combined all 7 features using SVMs and splitting their data into test and training data and achieve a rs of 0.448, which demonstrates\n8We cannot show the full model due to lack of space.\nsignificantly better correlation with the human goldstandard than any of the features in isolation\nWe evaluated our selectional preference models using the verb-object pairs produced by Venkatapathy and Joshi (2005). 9 This dataset has 765 verbobject collocations which have been given a rating between 1 and 6, by two annotators (both fluent speakers of English). Kendall\u2019s Tau (Siegel and Castellan, 1988) was used to measure agreement, and a score of 0.61 was obtained which was highly significant. The ranks of the two annotators gave a Spearman\u2019s rank-correlation coefficient (rs) of 0.71.\nThe Verb-Object pairs included some adjectives (e.g. happy, difficult, popular), pronouns and complements e.g. become director. We used the subset of 638 verb-object pairs that involved common nouns in the object relationship since our preference models focused on the object relation for common nouns. For each verb-object pair we used the preference models acquired from the RASP parses of the BNC to obtain the probability of the class that this object occurs under. Where the object noun is a member of several classes (classes(noun) \u2208 C) in the model, the class with the largest probability is used. Note though that for WNPROTOs we have the added constraint that a hyponym class from C is selected in preference to a hypernym in C . Compositionality of an object noun and verb is computed as:-\ncomp(noun, verb) = maxc\u2208classes(noun)\u2208C p(c|verb) (3)\nWe use the probability of the class, rather than an estimate of the probability of the object, because we want to determine how likely any word belonging to this class might occur with the given verb, rather than the probability of the specific noun which may be infrequent, yet typical, of the objects that occur with this verb. For example, convertible may be an infrequent object of park, but it is quite likely given its membership of the class motor vehicle. We do not want to assume anything about the frequency of non-compositional verb-object combinations, just that they are unlikely to be members of classes which represent prototypical objects. We\n9This verb-object dataset is available from http://www.cis.upenn.edu/\u02dcsriramv/mywork.html.\nwill contrast these models with a baseline frequency feature used by Venkatapathy and Joshi.\nWe use our selectional preference models to provide the probability that a candidate is representative of the typical objects of the verb. That is, if the object might typically occur in such a relationship then this should lessen the chance that this verb-object combination is non-compositional. We used the probability of the classes from our 3 selectional preference models to rank the pairs and then used Spearman\u2019s rank-correlation coefficient (rs) to compare these ranks with the ranks from the goldstandard.\nOur results for the three types of preference models are shown in the first section of table 1. 10 All the correlation values are significant, but we note that using the type based selectional preference models achieves a far greater correlation than using the TCMs. The DSPROTO models achieve the best results which is very encouraging given that they only require raw data and an automatic parser to obtain the grammatical relations.\nWe applied 4 of the features used by Venkatapathy and Joshi (2005) 11 and described in section 2 to our subset of 638 items. These features were ob-\n10We show absolute values of correlation following (Venkatapathy and Joshi, 2005).\n11The other 3 features performed less well on this dataset so we do not report the details here. This seems to be because they worked particularly well with the adjective and pronoun data in the full dataset.\ntained using the same BNC dataset used by Venkatapathy and Joshi which was obtained using Bikel\u2019s parser (Bikel, 2004). We obtained correlation values for these features as shown in table 1 under V&J. These features are feature 1 frequency, feature 2 pointwise mutual information, feature 3 based on (Lin, 1999) and feature 7 LSA feature which considers the similarity of the verb-object pair with the verbal form of the object. Pointwise mutual information did surprisingly well on this 84% subset of the data, however the DSPROTO preferences still outperformed this feature. We combined the DSPROTO and V&J features with an SVM ranking function and used 10 fold cross validation as Venkatapathy and Joshi did. We contrast the result with the V&J features without the preference models. The results in the bottom section of table 1 demonstrate that the preference models can be combined with other features to produce optimal results."
        },
        {
            "heading": "5 Conclusions and Directions for Future Work",
            "text": "We have demonstrated that the selectional preferences of a verbal predicate can be used to indicate if a specific combination with an object is noncompositional. We have shown that selectional preference models which represent prototypical arguments and focus on argument types (rather than tokens) do well at the task. Models produced from distributional thesauruses are the most promising which is encouraging as the technique could be applied to a language without a man-made thesaurus. We find that the probability estimates from our models show a highly significant correlation, and are very promising for detecting non-compositional verb-object pairs, in comparison to individual features used previously.\nFurther comparison of WNPROTOs and DSPROTOs to other WordNet models are warranted to contrast the effect of our proposal for disambiguation using word types with iterative approaches, particularly those of Clark and Weir (2002). A benefit of the DSPROTOs is that they do not require a hand-crafted inventory. It would also be worthwhile comparing the use of raw data directly, both from the BNC and from google\u2019s Web 1T corpus (Brants and Franz, 2006) since\nweb counts have been shown to outperform the Clark and Weir models on a pseudo-disambiguation task (Keller and Lapata, 2003).\nWe believe that preferences should NOT be used in isolation. Whilst a low preference for a noun may be indicative of peculiar semantics, this may not always be the case, for example chew the fat. Certainly it would be worth combining the preferences with other measures, such as syntactic fixedness (Fazly and Stevenson, 2006). We also believe it is worth targeting features to specific types of constructions, for example light verb constructions undoubtedly warrant special treatment (Stevenson et al., 2003)\nThe selectional preference models we have proposed here might also be applied to other tasks. We hope to use these models in tasks such as diathesis alternation detection (McCarthy, 2000; Tsang and Stevenson, 2004) and contrast with WordNet models previously used for this purpose."
        },
        {
            "heading": "6 Acknowledgements",
            "text": "We acknowledge support from the Royal Society UK for a Dorothy Hodgkin Fellowship to the first author. We thank the anonymous reviewers for their constructive comments on this work."
        }
    ],
    "title": "Detecting Compositionality of Verb-Object Combinations using Selectional Preferences",
    "year": 2007
}