{
    "abstractText": "Amodal instance segmentation aims to infer the amodal mask, including both the visible part and occluded part of each object instance. Predicting the occluded parts is challenging. Existing methods often produce incomplete amodal boxes and amodal masks, probably due to lacking visual evidences to expand the boxes and masks. To this end, we propose a prior-guided expansion framework, which builds on a two-stage segmentation model (i.e., Mask R-CNN) and performs box-level (resp., pixel-level) expansion for amodal box (resp., mask) prediction, by retrieving regression (resp., flow) transformations from a memory bank of expansion prior. We conduct extensive experiments on KINS, D2SA, and COCOA cls datasets, which show the effectiveness of our method.",
    "authors": [
        {
            "affiliations": [],
            "name": "Junjie Chen"
        },
        {
            "affiliations": [],
            "name": "Li Niu"
        },
        {
            "affiliations": [],
            "name": "Jianfu Zhang"
        },
        {
            "affiliations": [],
            "name": "Jianlou Si"
        },
        {
            "affiliations": [],
            "name": "Chen Qian"
        },
        {
            "affiliations": [],
            "name": "Liqing Zhang"
        }
    ],
    "id": "SP:7a429d7f3608217781a0c817e7fe5f228af897c8",
    "references": [
        {
            "authors": [
                "H. Chen",
                "K. Sun",
                "Z. Tian",
                "C. Shen",
                "Y. Huang",
                "Y. Yan"
            ],
            "title": "BlendMask: Top-down meets bottom-up for instance segmentation",
            "venue": "CVPR.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Chen",
                "G. Lin",
                "S. Li",
                "O. Bourahla",
                "Y. Wu",
                "F. Wang",
                "J. Feng",
                "M. Xu",
                "X. Li"
            ],
            "title": "BANet: Bidirectional aggregation network with occlusion handling for panoptic segmentation",
            "venue": "CVPR.",
            "year": 2020
        },
        {
            "authors": [
                "X. Chu",
                "A. Zheng",
                "X. Zhang",
                "J. Sun"
            ],
            "title": "Detection in crowded scenes: One proposal, multiple predictions",
            "venue": "CVPR.",
            "year": 2020
        },
        {
            "authors": [
                "A. Dosovitskiy",
                "P. Fischer",
                "E. Ilg",
                "P. Hausser",
                "C. Hazirbas",
                "V. Golkov",
                "P. Van Der Smagt",
                "D. Cremers",
                "T. Brox"
            ],
            "title": "FlowNet: Learning optical flow with convolutional networks",
            "venue": "ICCV.",
            "year": 2015
        },
        {
            "authors": [
                "Z. Fang",
                "A. Jain",
                "G. Sarch",
                "A.W. Harley",
                "K. Fragkiadaki"
            ],
            "title": "Move to See Better: Self-Improving Embodied Object Detection",
            "venue": "arXiv preprint arXiv:2012.00057.",
            "year": 2020
        },
        {
            "authors": [
                "P. Follmann",
                "R. K\u00f6nig",
                "P. H\u00e4rtinger",
                "M. Klostermann",
                "T. B\u00f6ttger"
            ],
            "title": "Learning to see the invisible: End-to-end trainable amodal instance segmentation",
            "venue": "WACV.",
            "year": 2019
        },
        {
            "authors": [
                "T. Gao",
                "B. Packer",
                "D. Koller"
            ],
            "title": "A segmentationaware object detection model with occlusion handling",
            "venue": "CVPR.",
            "year": 2011
        },
        {
            "authors": [
                "G. Ghiasi",
                "Y. Cui",
                "A. Srinivas",
                "R. Qian",
                "T.-Y. Lin",
                "E.D. Cubuk",
                "Q.V. Le",
                "B. Zoph"
            ],
            "title": "Simple copy-paste is a strong data augmentation method for instance segmentation",
            "venue": "CVPR.",
            "year": 2021
        },
        {
            "authors": [
                "A. Gupta",
                "P. Dollar",
                "R. Girshick"
            ],
            "title": "LVIS: A dataset for large vocabulary instance segmentation",
            "venue": "CVPR.",
            "year": 2019
        },
        {
            "authors": [
                "K. He",
                "G. Gkioxari",
                "P. Doll\u00e1r",
                "R. Girshick"
            ],
            "title": "Mask R-CNN",
            "venue": "ICCV.",
            "year": 2017
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "CVPR.",
            "year": 2016
        },
        {
            "authors": [
                "X. Huang",
                "Z. Ge",
                "Z. Jie",
                "O. Yoshie"
            ],
            "title": "NMS by representative region: Towards crowded pedestrian detection by proposal pairing",
            "venue": "CVPR.",
            "year": 2020
        },
        {
            "authors": [
                "L. Ke",
                "Y.-W. Tai",
                "C.-K. Tang"
            ],
            "title": "Deep OcclusionAware Instance Segmentation with Overlapping BiLayers",
            "venue": "CVPR.",
            "year": 2021
        },
        {
            "authors": [
                "A. Kortylewski",
                "Q. Liu",
                "A. Wang",
                "Y. Sun",
                "A. Yuille"
            ],
            "title": "Compositional convolutional neural networks: A robust and interpretable model for object recognition under occlusion",
            "venue": "International Journal of Computer Vision, 129(3): 736\u2013760.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Lee",
                "J. Park"
            ],
            "title": "Centermask: Real-time anchorfree instance segmentation",
            "venue": "CVPR.",
            "year": 2020
        },
        {
            "authors": [
                "K. Li",
                "J. Malik"
            ],
            "title": "Amodal instance segmentation",
            "venue": "ECCV.",
            "year": 2016
        },
        {
            "authors": [
                "T.-Y. Lin",
                "M. Maire",
                "S. Belongie",
                "J. Hays",
                "P. Perona",
                "D. Ramanan",
                "P. Doll\u00e1r",
                "C.L. Zitnick"
            ],
            "title": "Microsoft coco: Common objects in context",
            "venue": "ECCV.",
            "year": 2014
        },
        {
            "authors": [
                "P.O. Pinheiro",
                "R. Collobert",
                "P. Doll\u00e1r"
            ],
            "title": "Learning to segment object candidates",
            "venue": "NeurIPS.",
            "year": 2015
        },
        {
            "authors": [
                "L. Qi",
                "L. Jiang",
                "S. Liu",
                "X. Shen",
                "J. Jia"
            ],
            "title": "Amodal instance segmentation with KINS dataset",
            "venue": "CVPR.",
            "year": 2019
        },
        {
            "authors": [
                "S. Ren",
                "K. He",
                "R. Girshick",
                "J. Sun"
            ],
            "title": "Faster RCNN: Towards real-time object detection with region proposal networks",
            "venue": "NeurIPS.",
            "year": 2015
        },
        {
            "authors": [
                "R. Roscher",
                "M. Volpi",
                "C. Mallet",
                "L. Drees",
                "J.D. Wegner"
            ],
            "title": "SemCity Toulouse: A benchmark for building instance segmentation in satellite images",
            "venue": "ISPRS Annals of Photogrammetry, Remote Sensing and Spatial Information Sciences, 5: 109\u2013116.",
            "year": 2020
        },
        {
            "authors": [
                "H. Su",
                "S. Wei",
                "S. Liu",
                "J. Liang",
                "C. Wang",
                "J. Shi",
                "X. Zhang"
            ],
            "title": "HQ-ISNet: High-quality instance segmentation for remote sensing imagery",
            "venue": "Remote Sensing, 12(6): 989.",
            "year": 2020
        },
        {
            "authors": [
                "Z. Tian",
                "C. Shen",
                "H. Chen",
                "T. He"
            ],
            "title": "FCOS: Fully convolutional one-stage object detection",
            "venue": "ICCV.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Tu",
                "L. Niu",
                "J. Chen",
                "D. Cheng",
                "L. Zhang"
            ],
            "title": "Learning from web data with self-organizing memory module",
            "venue": "CVPR.",
            "year": 2020
        },
        {
            "authors": [
                "A. Wang",
                "Y. Sun",
                "A. Kortylewski",
                "A.L. Yuille"
            ],
            "title": "Robust object detection under occlusion with context-aware CompositionalNets",
            "venue": "CVPR.",
            "year": 2020
        },
        {
            "authors": [
                "S. Wei",
                "X. Zeng",
                "Q. Qu",
                "M. Wang",
                "H. Su",
                "J. Shi"
            ],
            "title": "HRSID: A high-resolution SAR images dataset for ship detection and instance segmentation",
            "venue": "IEEE Access, 8: 120234\u2013120254.",
            "year": 2020
        },
        {
            "authors": [
                "J. Winn",
                "J. Shotton"
            ],
            "title": "The layout consistent random field for recognizing and segmenting partially occluded objects",
            "venue": "CVPR.",
            "year": 2006
        },
        {
            "authors": [
                "M. Xiao",
                "A. Kortylewski",
                "R. Wu",
                "S. Qiao",
                "W. Shen",
                "A. Yuille"
            ],
            "title": "TDMPNet: Prototype network with recurrent top-down modulation for robust object classification under partial occlusion",
            "venue": "ECCV.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Xiao",
                "Y. Xu",
                "Z. Zhong",
                "W. Luo",
                "J. Li",
                "S. Gao"
            ],
            "title": "Amodal Segmentation Based on Visible Region Segmentation and Shape Prior",
            "venue": "AAAI.",
            "year": 2021
        },
        {
            "authors": [
                "E. Xie",
                "P. Sun",
                "X. Song",
                "W. Wang",
                "X. Liu",
                "D. Liang",
                "C. Shen",
                "P. Luo"
            ],
            "title": "Polarmask: Single shot instance segmentation with polar representation",
            "venue": "CVPR.",
            "year": 2020
        },
        {
            "authors": [
                "M. Yang",
                "Y. Liu",
                "L. Wen",
                "Z. You",
                "S.Z. Li"
            ],
            "title": "A probabilistic framework for multitarget tracking with mutual occlusions",
            "venue": "CVPR.",
            "year": 2014
        },
        {
            "authors": [
                "X. Yuan",
                "A. Kortylewski",
                "Y. Sun",
                "A. Yuille"
            ],
            "title": "Robust Instance Segmentation through Reasoning about MultiObject Occlusion",
            "venue": "CVPR.",
            "year": 2021
        },
        {
            "authors": [
                "X. Zhan",
                "X. Pan",
                "B. Dai",
                "Z. Liu",
                "D. Lin",
                "C.C. Loy"
            ],
            "title": "Self-supervised scene de-occlusion",
            "venue": "CVPR.",
            "year": 2020
        },
        {
            "authors": [
                "R. Zhang",
                "Z. Tian",
                "C. Shen",
                "M. You",
                "Y. Yan"
            ],
            "title": "Mask encoding for single shot instance segmentation",
            "venue": "CVPR.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Zhang",
                "L. Niu",
                "Z. Pan",
                "M. Luo",
                "J. Zhang",
                "D. Cheng",
                "L. Zhang"
            ],
            "title": "Exploiting motion information from unlabeled videos for static image action recognition",
            "venue": "AAAI.",
            "year": 2020
        },
        {
            "authors": [
                "Z. Zhang",
                "A. Chen",
                "L. Xie",
                "J. Yu",
                "S. Gao"
            ],
            "title": "Learning semantics-aware distance map with semantics layering network for amodal instance segmentation",
            "venue": "ACM MM. Zhu, Y.; Tian, Y.; Metaxas, D.; and Doll\u00e1r, P. 2017. Semantic",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Instance segmentation (e.g., Mask R-CNN (He et al. 2017)) focuses on segmenting visible pixels for each object instance. In real-world images, the object instances usually partially occlude each other. To better parse the complex scene, amodal instance segmentation (Xiao et al. 2021; Li and Malik 2016) requires to infer the complete amodal mask, including both the visible region and the occluded region for each object instance. Such capacity could greatly benefit intelligent systems in extensive real-world applications, e.g., facilitating the moving decision in complex traffic or living environment in autonomous driving (Qi et al. 2019) or robotics (Fang et al. 2020; Follmann et al. 2019).\nRecent years have witnessed promising progress in the amodal instance segmentation area. Former methods (Qi et al. 2019; Zhu et al. 2017; Follmann et al. 2019) directly infer both the visible and the amodal regions from images, while recent methods infer depth order information (Zhang et al. 2019; Zhan et al. 2020) or introduce prior information (Xiao et al. 2021) to help amodal instance segmentation. Despite the great progress of previous works, predicting amodal masks is still challenging because of lacking visible evidences for the occluded regions.\nIn practice, we found that the inferred amodal box and amodal mask are often incomplete, probably due to lacking evidences for expanding to complete amodal region. Re-\n*Corresponding author Copyright \u00a9 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\ncently, Xiao et al. (2021) employs several ground-truth (GT) amodal masks of training instances (i.e., instances with GT annotations in the training set) similar to the initially estimated amodal mask as shape prior to benefit the mask refinement. We also consider exploiting prior information to support amodal inference. In contrast, we propose to exploit the prior information of expanding visible region to amodal region, based on which prior-guided expansion is performed for amodal instance segmentation. Specifically, the amodal mask is represented by a box and a mask within it (He et al. 2017; Xiao et al. 2021). Undersize box inherently limits the subsequent amodal mask prediction, which remains ignored in recent works (Xiao et al. 2021; Follmann et al. 2019). Thus, we perform box-level (resp., pixel-level) expansion for amodal box (resp., mask) prediction.\nFollowing (Xiao et al. 2021; Follmann et al. 2019), our framework is built upon Mask R-CNN and further includes an expansion prior memory bank, a prior-guided amodal box head, and a prior-guided amodal mask head. The general pipeline firstly employs Mask R-CNN to infer the object class, original amodal box, and visible mask. After that, we search expansion prior for box in the bank to help the priorguided amodal box head obtain the expanded amodal box, as shown in Fig. 1 (a). Finally, we search expansion prior for mask in the bank to assist the prior-guided amodal mask\nhead in obtaining the amodal mask, as shown in Fig. 1 (b). Specifically, we construct one sub-bank for each class, where each slot in bank stores the information of a training instance. For each slot, the key is GT visible mask and the value is a tuple of GT visible box, GT amodal box, and GT amodal mask. We use the estimated visible mask to query the class-specific sub-bank, considering that objects having similar visible parts are likely to have similar amodal parts. Then, we can derive regression/flow transformations according to the retrieved values. Based on the expansion prior that how GT visible boxes are expanded to GT amodal boxes, we derive regression transformations to guide the box-level expansion. Based on the expansion prior that how the estimated visible mask is expanded to GT amodal masks, we derive flow transformations to guide the pixel-level expansion. Thanks to the expansion prior, our model can produce more complete amodal boxes and amodal masks.\nWe conduct extensive experiments on three datasets: KINS (Qi et al. 2019), D2SA (Follmann et al. 2019), and COCOA cls (Zhu et al. 2017). The in-depth analysis could demonstrate the effectiveness of our framework by performing box-level expansion and pixel-level expansion guided by expansion prior. Our contributions can be summarized as: 1) we propose a prior-guided expansion framework for amodal instance segmentation to address the incomplete box/mask issue; 2) technically, we propose to exploit prior-based regression (resp., flow) transformations to facilitate box-level (resp., pixel-level) expansion; 3) extensive experiments on three benchmark datasets indicate the effectiveness of our method against state-of-the-art baselines."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Visual Occlusion Learning",
            "text": "In practice, occlusion is an inevitable problem and dramatically increases the learning difficulty, which has been researched in extensive applications including image classification (Kortylewski et al. 2021; Xiao et al. 2020), object detection (Wang et al. 2020; Chu et al. 2020), tracking (Yang et al. 2014), and segmentation (Gao, Packer, and Koller 2011; Winn and Shotton 2006). For example, BANet (Chen et al. 2020b) introduced an occlusion handling algorithm to model the occlusion between object instances for panoptic segmentation. Huang et al. (Huang et al. 2020) proposed to leverage the less occluded visible parts for effectively removing the redundant boxes in crowded pedestrian detection. Zhan et al. (Zhan et al. 2020) conducted ordering recovery, amodal completion, and content completion subtasks in self-supervised manner for scene de-occlusion task. Recently, Yuan et al. (Yuan et al. 2021) proposed a generative model of multiple objects to reason about multi-object occlusion under box-level supervision for the robust instance segmentation task. In this paper, we focus on the occlusion problem in amodal instance segmentation."
        },
        {
            "heading": "2.2 Amodal Instance Segmentation",
            "text": "The standard instance segmentation has achieved prominent progress in recent years (Ghiasi et al. 2021; Tian et al. 2019;\nChen et al. 2020a; Xie et al. 2020) and has derived various richer tasks, including efficient instance segmentation (Zhang et al. 2020a; Lee and Park 2020), high-resolution instance segmentation (Wei et al. 2020; Su et al. 2020), and so on (Gupta, Dollar, and Girshick 2019; Roscher et al. 2020). In this paper, we focus on amodal instance segmentation (Li and Malik 2016), which considers the problem of segmenting instances with the occluded region.\nRecently, amodal instance segmentation has drawn increasing research interest, probably due to its practical application in extensive complex scenes. The earliest work on amodal instance segmentation was proposed by (Li and Malik 2016), which iteratively predicts the amodal bounding box based on amodal segmentation heatmap and trains the model using occlusion data synthesized by overlapping cropped image patches. Afterwards, Zhu et al. (Zhu et al. 2017) employed SharpMask (Pinheiro, Collobert, and Dolla\u0301r 2015) to predict the object amodal mask from coarse to fine. Qi et al. (Qi et al. 2019) proposed to ensemble the features from box and class head by multi-level coding for the occluded instances, which are determined by an occlusion classifier. SLN (Zhang et al. 2019) introduced a depth order representation to facilitate the inference of amodal mask. Considering the relationship between the visible region and amodal region, ORCNN (Follmann et al. 2019) proposed to further predict the occlusion mask by subtracting the visible mask from the amodal mask. BCNet (Ke, Tai, and Tang 2021) added an occlusion perception branch parallel to the traditional instance segmentation pipeline to consider the interactions between objects. Xiao et al. (Xiao et al. 2021) proposed to use cross-task attention together with GT training masks similar to coarse prediction for refinement. Considering that using memory bank of prior knowledge (Xiao et al. 2021; Tu et al. 2020; Zhang et al. 2020b) is also an effective method, we exploit expansion transformations from prior knowledge to perform both box-level and pixellevel expansion for better amodal inference."
        },
        {
            "heading": "3 Method",
            "text": "For the input image I, amodal instance segmentation aims to infer the object class y and amodal mask Ma for each object instance. As illustrated in Fig 1 (b), the amodal mask consists of visible mask and occluded mask.\nOur overall framework is shown in Fig 2, which mainly consists of four modules: Mask R-CNN, expansion prior memory bank, prior-guided amodal box head, and priorguided amodal mask head. Firstly, Mask R-CNN produces N target instances from the input image, and the i-th instance has three estimations: object class yi, original amodal box Boi , and visible mask M v i . Based on the estimated class and visible mask, the memory bank searches and provides regression transformations Tri for subsequent box-level expansion. After that, the visible box Bvi is derived by clipping the visible mask, and the prior-guided amodal box head estimates the expanded amodal box Bai according to B v i and T r i . Based on Bai , the original region feature map and estimated visible mask are also expanded accordingly via resampling (e.g., ROIAlign (He et al. 2017)). Then, the expanded visible mask is employed to query the bank to retrieve the flow\ntransformations Tfi . Finally, the prior-guided amodal mask head predicts the amodal mask Mai on the expanded region feature map supported by Tfi .\nFor Mask R-CNN, we follow the setup in previous works (Xiao et al. 2021; Follmann et al. 2019), e.g., predicting amodal box in its box head, using ResNet-50 (He et al. 2016) as backbone, and using channel size C = 256 and spatial size 14\u00d7 14 for the region feature maps of mask heads. The training objective of Mask R-CNN is summarized as\nLmrcnn = Lcls + Lobox + Lvmask, (1)\nwhere Lcls, Lobox, and Lvmask are the training objectives for object class, original amodal box, and visible mask, consistent with these loss terms in (Xiao et al. 2021; Follmann et al. 2019; He et al. 2017). The architectures and procedures of other modules are introduced as follows."
        },
        {
            "heading": "3.1 Expansion Prior Memory Bank",
            "text": "The memory bank stores the expansion prior for box-level expansion and pixel-level expansion.\nConstruction The memory bank is constructed before the formal training according to training samples. For each class, we construct a sub-bank having Ks slots (Ks = 1000 in our experiments), with each slot corresponding to a training instance. The key of each slot is the visible mask of this instance, denoted as Mv . The value of each slot is a tuple of visible box Bv , amodal box Ba, and amodal mask Ma of this instance. For the classes with fewer than Ks instances, we perform augmentation (i.e., spatial transformation) to obtain Ks instances. For the classes with more than Ks instances, we perform K-Means and use Ks cluster centers.\nSearch For the i-th target instance, we employ its estimated visible mask Mvi as query to search the sub-bank belonging to its predicted class, and find K (K = 8 in our experiments) nearest slots using the distance function d(Mquery,Mkey) = \u2225E(Mquery)\u2212 E(Mkey)\u22252, where E(\u00b7) is a pre-trained encoder used in the distance computation (we reuse the mask encoder in (Xiao et al. 2021) and freeze it during training). After that, we can extract two types of expansion prior from the K retrieved values. Considering that the expansion prior is derived per value individually, we will take the k-th value as an example in the following description.\nDeriving Regression Transformation To exploit expansion prior to guide the box-level expansion, we derive regression transformation according to the GT visible box Bvk and GT amodal box Bak, as shown in Fig. 3 (a). That is,\nTrk = S r(Bvk,B a k), (2)\nwhere the regression transformation Trk accounts for translating and scaling the box Bvk to match B a k. Function S\nr(\u00b7, \u00b7) is implemented based on the coordinates of two boxes as in (He et al. 2017; Ren et al. 2015). The details of Sr(\u00b7, \u00b7) are trivial and omitted here.\nDeriving Flow Transformation To exploit expansion prior\nto guide the pixel-level expansion, we derive flow transformation according to the estimated visible mask Mv and GT amodal mask Mak, as shown in Fig. 3 (b). That is,\nTfk = S f (Mv,Mak), (3)\nwhere Tfk is the derived flow transformation and S f (\u00b7, \u00b7) is a function solving the transformation from Mv to Mak, i.e., 2-D spatial offsets for moving pixels in the visible mask to reconstruct the amodal mask. Unlike the close-form solver Sr(\u00b7, \u00b7) for box, we have to employ a lightweight network (similar to FlowNet (Dosovitskiy et al. 2015)) as Sf (\u00b7, \u00b7) to derive the flow transformations. Sf (\u00b7, \u00b7) is pre-trained on paired GT visible masks and GT amodal masks of training instances and frozen during the formal training.\nIn the following, we introduce the details that how the prior regression (resp., flow) transformations in memory bank guide the box-level (resp., pixel-level) expansion."
        },
        {
            "heading": "3.2 Prior-guided Amodal Box Head",
            "text": "The original amodal box Bo predicted by Mask R-CNN is usually incomplete. Therefore, the prior-guided box head is proposed to perform box-level expansion guided by the searched regression transformations Tr, as shown in the right-upper subfigure of Fig. 2.\nFirstly, guided by regression transformations, we expand visible box Bv (derived via clipping visible mask Mv) by\nBpk = T r k(B v), (4)\nwhere Trk(\u00b7) is the k-th regression transformation and B p k is the k-th prior-expanded box. Secondly, K prior-expanded boxes perform ROIAlign on the pyramid feature maps of Mask R-CNN to obtain K expanded region feature maps, in spatial size 4 considering computation complexity. After that, the channel size of concatenated expanded region feature map is squeezed from K \u00d7 C to C. Meanwhile, the region feature map within visible box is obtained by ROIAlign. Then, the two feature maps are concatenated and fed into a 3 \u00d7 3 convolution layer outputting C channels. Finally, the flattened feature vector is fed into three fullyconnected layers to predict the expanded amodal box Ba. The loss term of this module w.r.t N target instances is\nLabox = 1\nN N\u2211 i \u2225Bai \u2212 Bai \u2217\u22251, (5)\nwhere Bai is specifically a vector representing the 4 normalized coordinate (He et al. 2017; Ren et al. 2015) of the predicted amodal box for the i-th instance, and Bai\n\u2217 represents the corresponding ground-truth amodal box.\nOverall, the expansion prior in the training instances is exploited as regression transformations and employed to expand visible box to facilitate the prediction of amodal box. In addition, we employ the expansion prior to expand the visible box derived by clipping visible mask, but it may be more intuitive to directly predict visible box by the box head in Mask R-CNN and then expand it. However, this intuitive manner degrades the performance of amodal box and\namodal mask (see experiments in Sec. 4.4), probably because of lacking occlusion context which could have been exploited by backbone or region proposal network implicitly. Therefore, we predict amodal box in the box head of Mask R-CNN following (Xiao et al. 2021; Follmann et al. 2019), and obtain visible box by clipping visible mask."
        },
        {
            "heading": "3.3 Prior-guided Amodal Mask Head",
            "text": "Within the expanded amodal box Ba, directly predicting the amodal mask Ma is still difficult. Therefore, the priorguided amodal mask head is proposed to perform pixel-level expansion guided by the searched flow transformations Tf , as shown in the bottom right subfigure of Fig. 2.\nFirstly, guided by the flow transformations, we expand the feature map of visible region by\nFpk = T f k(F e \u00b7 Mv,e), (6) where \u00b7 means dot-product, and Fe and Mv,e are the region feature map and estimated visible mask within the expanded amodal box respectively. Fpk is the k-th expanded region feature map which spatially transforms visible region feature map via 2D offsets in Tfk(\u00b7). After that, the channel size of concatenated expanded region feature map is squeezed from K\u00d7C to C. The squeezed feature map is concatenated with Fe and then fed into a 1 \u00d7 1 convolution layer outputting C channels. Finally, 4 convolution layers, 1 deconvolution layer, and 1 convolution layer are employed to predict the amodal mask Ma, following the mask head in (He et al. 2017; Xiao et al. 2021; Follmann et al. 2019). The loss term of this module w.r.t N instances could be formulated as\nLamask = 1\nN N\u2211 i Lbce(Mai ,Mai \u2217), (7)\nwhere Mai \u2217 is the associated ground-truth amodal mask of i-th target instance, and Lbce(\u00b7, \u00b7) is the binary cross-entropy loss used in (He et al. 2017; Xiao et al. 2021).\nOverall, the expansion prior in the training instances is exploited as flow transformations, which are used to expand the feature map of visible region to facilitate the prediction of amodal mask. Compared with the directly concatenated GT amodal masks in (Xiao et al. 2021), the concatenated expanded region feature maps could implicitly encode more structural and contextual information, and thus better benefit the amodal inference (see experiments in Sec. 4.4)."
        },
        {
            "heading": "3.4 The Total Training Objective",
            "text": "Overall, our total training objective can be formulated as\nL = Lmrcnn + Labox + Lamask, (8) where Lmrcnn, Labox and Lamask are the training objectives of Mask R-CNN in Sec. 3, prior-guided amodal box head in Sec. 3.2, and prior-guided amodal mask head in Sec. 3.3."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Datasets and Implementation Details",
            "text": "As in (Xiao et al. 2021), we investigate the performance of our method on three public datasets: the KINS dataset (Qi\net al. 2019), the D2SA (D2S amodal) dataset (Follmann et al. 2019), and the COCOA cls dataset (Zhu et al. 2017). We implement the proposed method on the codebase of previous work (Xiao et al. 2021), which builds on Detectron2 using Python 3.7 and PyTorch 1.4.0 framework. We conducted the experiments on Ubuntu 18.04 system with 32 GB Intel 9700K CPU and two NVIDIA 1080ti GPU cards."
        },
        {
            "heading": "4.2 Evaluation",
            "text": "We employ the mean average precision (AP) for the evaluation following previous works (Xiao et al. 2021; Zhu et al. 2017), and we evaluate the performance for both amodal box and amodal mask to investigate the effectiveness of boxlevel expansion and pixel-level expansion. We also follow (Xiao et al. 2021) to focus on the performance of occluded instances via AP-occ, which only computes the performance on the instances having visible rate (i.e., IoU between visible mask and amodal mask) not larger than 85%. We employ the evaluation API in (Xiao et al. 2021) for fair comparisons, which inherits the API of COCO dataset (Lin et al. 2014)."
        },
        {
            "heading": "4.3 Ablation Study",
            "text": "We conduct ablation study on the KINS dataset, considering that it is the largest real-world dataset for amodal instance segmentation. We investigate the performances of various combination sets of modules, and summarize the results in Tab. 1. The most basic set is Set#1, which totally obsoletes the prior-guided amodal box head and directly predicts amodal mask within the original amodal box (i.e., Mask R-CNN with additional amodal mask head). Firstly, simply adding a box head without prior (i.e., Set#1 v.s. Set#2) just slightly improves the performances of mask and\nbox. Solely utilizing the prior-guided amodal box head (i.e., Set#1 v.s. Set#3) dramatically promotes the performance of box and thus benefits the downstream amodal mask prediction. While solely utilizing the prior-guided amodal mask head (i.e., Set#1 v.s. Set#4) dramatically promotes the performance of amodal mask. Finally, the full-fledged combination of prior-guided amodal box head and mask head (i.e., Set#6) achieves the optimal performance indicating the effectiveness of prior-guided expansion."
        },
        {
            "heading": "4.4 Expansion Prior Analysis",
            "text": "For the number of used expansion prior, we summarize the results in Fig. 4 (a) to show the effects. Guided by only 2 expansion prior, the performances of amodal box and amodal box both are obviously improved. The performances of box are gradually saturated around 4, while the performances of mask are gradually saturated around 8. Considering the uniformity, we employ K = 8 for both amodal box and amodal mask in Sec. 3.1. For the size of constructed memory bank, we summarize the results in Fig. 4 (b) to show the impacts. The overall performance is relatively saturated around the default value (i.e., 1000). In addition, large bank size is still efficient relative to the dominated backbone, and the default value is a reasonable choice."
        },
        {
            "heading": "4.5 Qualitative Analysis",
            "text": "We visualize the regression transformations and flow transformations to better understand the process of box-level expansion and pixel-level expansion, as shown in Fig. 5. The regression transformations serve as prior knowledge indicating the expansion directions and scopes, and results in a more complete expanded amodal box. Within the expanded amodal box (zoomed to spatial size 14\u00d714 via ROIAlign), the flow transformations serve as prior knowledge to guide the expanding of visible region feature map. Thanks to the expansion prior of regression transformations and flow transformations, our model is guided to produce more complete results."
        },
        {
            "heading": "4.6 Comparison with Previous Works",
            "text": "Comparative Baselines We compare our method (dubbed as PGExp) with the following state-of-the-art methods: 1) MRCNN (He et al. 2017), which uses the network architecture of Mask R-CNN to predict amodal box and amodal mask. 2) MRCNN8, which is a deeper Mask R-CNN used in (Xiao et al. 2021). 3) ORCNN (Follmann et al. 2019), which parallelly predicts visible mask and amodal mask, and further predicts the occlusion mask by subtracting visible mask from amodal mask to model the relationship between them. 4) Qi et al. (2019), which estimates occluded parts by multitask framework with multi-level coding. 5) BCNet (Ke, Tai, and Tang 2021), which employs bilayer structure to consider the interaction between occluding and occluded instances.\n6) Xiao et al. (2021), which parallelly predicts coarse visible mask and amodal mask and employs cross-task attention and concatenating searched GT masks for refinement.\nResults and Analysis All the results are summarized in Tab. 2, where the values marked by \u2217 are directly copied from (Xiao et al. 2021) or the corresponding paper and the values without \u2217 are obtained via reproductions with comparable and fair implementation. Firstly, simply employing a deeper mask head only slightly improves the mask performances, e.g., 30.71 v.s. 30.01 AP on KINS dataset. Our method outperforms all baselines by a large margin in terms of amodal box (e.g., 35.77 v.s. 33.40 AP on KINS dataset) by prior-guided box-level expansion, which also enables producing more complete amodal mask in the downstream process. Our method also shows superior performance against all baselines dramatically in term of amodal mask (e.g., 33.82 v.s. 32.20 AP on KINS dataset), thanks to the priorguided pixel-level expansion. In addition, the improvement in amodal box prediction of our method leads to generally less improvement in downstream amodal mask prediction, which may due to the neglecting of amodal box in previous works. We also conjecture that mask prediction is notably more difficult than box prediction, the improvement on box would be inevitably compromised on the downstream segmentation. Overall, our model achieves the optimal performances for both amodal box and mask, demonstrating the effectiveness of our prior-guided expansion framework."
        },
        {
            "heading": "4.7 Qualitative Comparison",
            "text": "We conduct the qualitative comparison with the two representative baselines, i.e., BCNet (Ke, Tai, and Tang 2021) and Xiao et al. (Xiao et al. 2021). As shown in Fig. 6, our method could predict more complete amodal boxes and amodal masks, by performing prior-guided box-level expansion and prior-guided pixel-level expansion. For example, in the complex scene of the first row (i.e., the two bicycles are occluded by cars), BCNet and Xiao et al. both fail to produce complete amodal boxes and amodal masks, and our method could estimate more precise results by virtue of the expansion priors. In the last row, for the cucumber occluded by the cabbage and border padding, our model could produce more favorable amodal mask, especially for the two ends."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this paper, we have developed a prior-guided expansion framework for amodal instance segmentation. Specifically, we exploit expansion prior from training instances and derive regression (resp., flow) transformations to facilitate boxlevel (resp., pixel-level) expansion. Extensive experiments and in-depth analyses on KINS, D2SA, and COCOA cls datasets have demonstrated the effectiveness of our proposed framework for amodal instance segmentation."
        },
        {
            "heading": "Acknowledgements",
            "text": "The work was supported by the National Science Foundation of China (62076162), and the Shanghai Municipal Science and Technology Major/Key Project, China (2021SHZDZX0102, 20511100300)."
        }
    ],
    "title": "Amodal Instance Segmentation via Prior-Guided Expansion",
    "year": 2023
}