{
    "abstractText": "Convolutional neural networks (CNNs) have been widely applied in motor fault diagnosis. However, to obtain high recognition accuracy, massive training data are typically required and transmitted to the cloud/local server for training, which may suffer from security and privacy problems. In this study, a noise-boosted CNN (NBCNN) model is developed to achieve accelerated training and improved recognition accuracy with limited training samples. First, the NBCNN model with a noise-injection fully connected layer is established. Then, a strategy for noise selection and injection is proposed to obtain an optimal matching among the data, model, and noise. Finally, the optimal injected noise accelerates the convergence of model training and improves the accuracy of motor fault diagnosis. Compared with the conventional CNN without noise injection and the state-of-the-art models, the effectiveness and superiority of the proposed NBCNN model are validated by two benchmark datasets. In addition, the algorithm is deployed onto an edge device and the results show that the training speed of the developed NBCNN can reach nine times faster than the conventional CNN. The proposed method shows remarkable potential for distributed model training, federal learning, and real-time motor fault diagnosis.",
    "authors": [],
    "id": "SP:431de8f21a17fc742e5074c5acd52cb15250a820",
    "references": [
        {
            "authors": [
                "C. Ding",
                "M. Zhao",
                "J. Lin",
                "J. Jiao",
                "K. Liang"
            ],
            "title": "Sparsity-Based Algorithm for Condition Assessment of Rotating Machinery Using Internal Encoder Data",
            "venue": "IEEE Transactions on Industrial Electronics, vol. 67, no. 9, pp. 7982-7993, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "H. Shao",
                "H. Jiang",
                "H. Zhang",
                "T. Liang"
            ],
            "title": "Electric Locomotive Bearing Fault Diagnosis Using a Novel Convolutional Deep Belief Network",
            "venue": "IEEE Transactions on Industrial Electronics, vol. 65, no. 3, pp. 2727-2736, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "Q. Guo",
                "Y. Li",
                "Y. Song",
                "D. Wang",
                "W. Chen"
            ],
            "title": "Intelligent Fault Diagnosis Method Based on Full 1-D Convolutional Generative Adversarial Network",
            "venue": "IEEE Transactions on Industrial Informatics, vol. 16, no. 3, pp. 2044-2053, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Z. Long",
                "X. Zhang",
                "M. He",
                "S. Huang",
                "G. Qin"
            ],
            "title": "Motor Fault Diagnosis Based on Scale Invariant Image Features",
            "venue": "IEEE Transactions on Industrial Informatics, vol. 18, no. 3, pp. 1605-1617, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "W. Yin",
                "K. Kann",
                "M. Yu",
                "H. Sch\u00fctze"
            ],
            "title": "Comparative study of CNN and RNN for natural language processing",
            "venue": "arXiv preprint arXiv:1702.01923, 2017. 10",
            "year": 1923
        },
        {
            "authors": [
                "A. Krizhevsky",
                "I. Sutskever",
                "G.E. Hinton"
            ],
            "title": "ImageNet classification with deep convolutional neural networks",
            "venue": "Communications of the ACM, vol. 60, no. 6, pp. 84-90, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "Z. Liu",
                "Z. Wu",
                "T. Li",
                "J. Li",
                "C. Shen"
            ],
            "title": "GMM and CNN Hybrid Method for Short Utterance Speaker Recognition",
            "venue": "IEEE Transactions on Industrial Informatics, vol. 14, no. 7, pp. 3244-3252, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "R. Socher",
                "Y. Bengio",
                "C.D. Manning"
            ],
            "title": "Deep learning for NLP (without magic)",
            "venue": "Tutorial Abstracts of ACL 2012, 2012, pp. 5-5.",
            "year": 2012
        },
        {
            "authors": [
                "G. Hinton",
                "L. Deng",
                "D. Yu",
                "G.E. Dahl",
                "A.-r. Mohamed"
            ],
            "title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups",
            "venue": "IEEE Signal processing magazine, vol. 29, no. 6, pp. 82-97, 2012.",
            "year": 2012
        },
        {
            "authors": [
                "M. Ma",
                "C. Sun",
                "X. Chen"
            ],
            "title": "Deep Coupling Autoencoder for Fault Diagnosis With Multimodal Sensory Data",
            "venue": "IEEE Transactions on Industrial Informatics, vol. 14, no. 3, pp. 1137-1145, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "W. Du",
                "M. Kang",
                "M. Pecht"
            ],
            "title": "Fault Diagnosis Using Adaptive Multifractal Detrended Fluctuation Analysis",
            "venue": "IEEE Transactions on Industrial Electronics, vol. 67, no. 3, pp. 2272-2282, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "J. Song",
                "J. Zhao",
                "X. Zhang",
                "F. Dong",
                "J. Zhao"
            ],
            "title": "Accurate Demagnetization Faults Detection of Dual-Sided Permanent Magnet Linear Motor Using Enveloping and Time-Domain Energy Analysis",
            "venue": "IEEE Transactions on Industrial Informatics, vol. 16, no. 10, pp. 6334-6346, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "J. Zhang",
                "Y. Wang",
                "K. Zhu",
                "Y. Zhang",
                "Y. Li"
            ],
            "title": "Diagnosis of Interturn Short-Circuit Faults in Permanent Magnet Synchronous Motors Based on Few-Shot Learning Under a Federated Learning Framework",
            "venue": "IEEE Transactions on Industrial Informatics, vol. 17, no. 12, pp. 8495-8504, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Z.-H. Liu",
                "B.-L. Lu",
                "H.-L. Wei",
                "L. Chen",
                "X.-H. Li"
            ],
            "title": "Deep Adversarial Domain Adaptation Model for Bearing Fault Diagnosis",
            "venue": "IEEE Transactions on Systems, Man, and Cybernetics: Systems, vol. 51, no. 7, pp. 4217-4226, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "H. Wang",
                "J. Xu",
                "C. Sun",
                "R. Yan",
                "X. Chen"
            ],
            "title": "Intelligent Fault Diagnosis for Planetary Gearbox Using Time-Frequency Representation and Deep Reinforcement Learning",
            "venue": "IEEE/ASME Transactions on Mechatronics, vol. 27, no. 2, pp. 985-998, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "F. Husari",
                "J. Seshadrinath"
            ],
            "title": "Incipient Interturn Fault Detection and Severity Evaluation in Electric Drive System Using Hybrid HCNN-SVM Based Model",
            "venue": "IEEE Transactions on Industrial Informatics, vol. 18, no. 3, pp. 1823-1832, 2022.",
            "year": 1823
        },
        {
            "authors": [
                "T. Xie",
                "X. Huang",
                "S.-K. Choi"
            ],
            "title": "Intelligent Mechanical Fault Diagnosis Using Multisensor Fusion and Convolution Neural Network",
            "venue": "IEEE Transactions on Industrial Informatics, vol. 18, no. 5, pp. 3213-3223, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "J. Zhang",
                "Y. Sun",
                "L. Guo",
                "H. Gao",
                "X. Hong"
            ],
            "title": "A new bearing fault diagnosis method based on modified convolutional neural networks",
            "venue": "Chinese Journal of Aeronautics, vol. 33, no. 2, pp. 439-447, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "L. Ren",
                "J. Dong",
                "X. Wang",
                "Z. Meng",
                "L. Zhao"
            ],
            "title": "A Data-Driven Auto-CNN-LSTM Prediction Model for Lithium-Ion Battery Remaining Useful Life",
            "venue": "IEEE Transactions on Industrial Informatics, vol. 17, no. 5, pp. 3478-3487, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "H. Shao",
                "M. Xia",
                "G. Han",
                "Y. Zhang",
                "J. Wan"
            ],
            "title": "Intelligent Fault Diagnosis of Rotor-Bearing System Under Varying Working Conditions With Modified Transfer Convolutional Neural Network and Thermal Images",
            "venue": "IEEE Transactions on Industrial Informatics, vol. 17, no. 5, pp. 3488-3496, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "D. Peng",
                "H. Wang",
                "Z. Liu",
                "W. Zhang",
                "M.J. Zuo"
            ],
            "title": "Multibranch and Multiscale CNN for Fault Diagnosis of Wheelset Bearings Under Strong Noise and Variable Load Condition",
            "venue": "IEEE Transactions on Industrial Informatics, vol. 16, no. 7, pp. 4949-4960, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "M.-Q. Tran",
                "M.-K. Liu",
                "Q.-V. Tran",
                "T.-K. Nguyen"
            ],
            "title": "Effective Fault Diagnosis Based on Wavelet and Convolutional Attention Neural Network for Induction Motors",
            "venue": "IEEE Transactions on Instrumentation and Measurement, vol. 71, pp. 1-13, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "F. Wang",
                "R. Liu",
                "Q. Hu",
                "X. Chen"
            ],
            "title": "Cascade Convolutional Neural Network With Progressive Optimization for Motor Fault Diagnosis Under Nonstationary Conditions",
            "venue": "IEEE Transactions on Industrial Informatics, vol. 17, no. 4, pp. 2511-2521, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "J. Xiong",
                "C. Li",
                "C.-D. Wang",
                "J. Cen",
                "Q. Wang"
            ],
            "title": "Application of Convolutional Neural Network and Data Preprocessing by Mutual Dimensionless and Similar Gram Matrix in Fault Diagnosis",
            "venue": "IEEE Transactions on Industrial Informatics, vol. 18, no. 2, pp. 1061-1071, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "E. Sisinni",
                "A. Saifullah",
                "S. Han",
                "U. Jennehag",
                "M. Gidlund"
            ],
            "title": "Industrial Internet of Things: Challenges, Opportunities, and Directions",
            "venue": "IEEE Transactions on Industrial Informatics, vol. 14, no. 11, pp. 4724-4734, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "Y. Lu",
                "X. Huang",
                "Y. Dai",
                "S. Maharjan",
                "Y. Zhang"
            ],
            "title": "Blockchain and Federated Learning for Privacy-Preserved Data Sharing in Industrial IoT",
            "venue": "IEEE Transactions on Industrial Informatics, vol. 16, no. 6, pp. 4177-4186, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "C. Yin",
                "J. Xi",
                "R. Sun",
                "J. Wang"
            ],
            "title": "Location Privacy Protection Based on Differential Privacy Strategy for Big Data in Industrial Internet of Things",
            "venue": "IEEE Transactions on Industrial Informatics, vol. 14, no. 8, pp. 3628-3636, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "M. Zhao",
                "S. Zhong",
                "X. Fu",
                "B. Tang",
                "M. Pecht"
            ],
            "title": "Deep Residual Shrinkage Networks for Fault Diagnosis",
            "venue": "IEEE Transactions on Industrial Informatics, vol. 16, no. 7, pp. 4681-4690, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "K. Audhkhasi",
                "O. Osoba",
                "B. Kosko"
            ],
            "title": "Noise-enhanced convolutional neural networks",
            "venue": "Neural Netw, vol. 78, pp. 15-23, Jun 2016.",
            "year": 2016
        },
        {
            "authors": [
                "Y.M. Yeap",
                "N. Geddada",
                "K. Satpathi",
                "A. Ukil"
            ],
            "title": "Time- and Frequency-Domain Fault Detection in a VSC-Interfaced Experimental DC Test System",
            "venue": "IEEE Transactions on Industrial Informatics, vol. 14, no. 10, pp. 4353-4364, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "J. Yu",
                "X. Zhou"
            ],
            "title": "One-Dimensional Residual Convolutional Autoencoder Based Feature Learning for Gearbox Fault Diagnosis",
            "venue": "IEEE Transactions on Industrial Informatics, vol. 16, no. 10, pp. 6347-6358, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "S. Lu",
                "G. Qian",
                "Q. He",
                "F. Liu",
                "Y. Liu"
            ],
            "title": "In Situ Motor Fault Diagnosis Using Enhanced Convolutional Neural Network in an Embedded System",
            "venue": "IEEE Sensors Journal, vol. 20, no. 15, pp. 8287-8296, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "P. Peng",
                "J. Lu",
                "T. Xie",
                "S. Tao",
                "H. Wang"
            ],
            "title": "Open-Set Fault Diagnosis via Supervised Contrastive Learning with Negative Out-of-Distribution Data Augmentation",
            "venue": "IEEE Transactions on Industrial Informatics, doi:10.1109/tii.2022.3149935,2022.",
            "year": 2022
        },
        {
            "authors": [
                "L. Xiao",
                "J.X. Tang",
                "X.H. Zhang",
                "E. Bechhoefer",
                "S.Y. Ding"
            ],
            "title": "Remaining useful life prediction based on intentional noise injection and feature reconstruction",
            "venue": "Reliability Engineering & System Safety, vol. 215, Nov 2021, Art no. 107871.",
            "year": 2021
        },
        {
            "authors": [
                "J. Tang",
                "N. Wu",
                "L. Xiao",
                "J. Zhao"
            ],
            "title": "The Improvement of Remaining Useful Life Prediction for Aero-engines by Injecting Noise",
            "venue": "2020 Global Reliability and Prognostics and Health Management (PHM-Shanghai), 2020: IEEE, pp. 1-6.",
            "year": 2020
        },
        {
            "authors": [
                "W.A. Smith",
                "R.B. Randall"
            ],
            "title": "Rolling element bearing diagnostics using the Case Western Reserve University data: A benchmark study",
            "venue": "Mechanical systems and signal processing, vol. 64, pp. 100-131, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "C. Wu",
                "P. Jiang",
                "C. Ding",
                "F. Feng",
                "T. Chen"
            ],
            "title": "Intelligent fault diagnosis of rotating machinery based on one-dimensional convolutional neural network",
            "venue": "Computers in Industry, vol. 108, pp. 53-61, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "L. Liao",
                "Y. Zhao",
                "S. Wei",
                "Y. Wei",
                "J. Wang"
            ],
            "title": "Parameter Distribution Balanced CNNs",
            "venue": "IEEE Trans Neural Netw Learn Syst, vol. 31, no. 11, pp. 4600-4609, Nov 2020.",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "widely applied in motor fault diagnosis. However, to obtain high recognition accuracy, massive training data are typically required and transmitted to the cloud/local server for training, which may suffer from security and privacy problems. In this study, a noise-boosted CNN (NBCNN) model is developed to achieve accelerated training and improved recognition accuracy with limited training samples. First, the NBCNN model with a noise-injection fully connected layer is established. Then, a strategy for noise selection and injection is proposed to obtain an optimal matching among the data, model, and noise. Finally, the optimal injected noise accelerates the convergence of model training and improves the accuracy of motor fault diagnosis. Compared with the conventional CNN without noise injection and the state-of-the-art models, the effectiveness and superiority of the proposed NBCNN model are validated by two benchmark datasets. In addition, the algorithm is deployed onto an edge device and the results show that the training speed of the developed NBCNN can reach nine times faster than the conventional CNN. The proposed method shows remarkable potential for distributed model training, federal learning, and real-time motor fault diagnosis.\nIndex Terms\u2014CNN, noise injection, motor fault diagnosis,\nlimited samples, edge device\nI. INTRODUCTION\notors provide mechanical kinetic energy and have been generally used and play an indispensable role in household and industrial equipment [1, 2]. Motors\nthat work in extreme environments are prone to failure after a long period of operational wear and environmental erosion. These failures can accelerate equipment damage and cause\nManuscript received Oct. 2, 2022; accepted Dec. 07, 2022. Paper no. TII-22-4132. This work was supported in part by the National Natural Science Foundation of China under Grants 52075002 and 62203010, and the Project of the Outstanding Young Talents in Colleges and Universities of Anhui Province under Grant gxyqZD2022006. (Corresponding authors: Xiaoxian Wang; Min Xia; Siliang Lu)\nLv Chen, Kang An, and Siliang Lu are with the College of Electrical Engineering and Automation, Anhui University, Hefei 230601, China. (e-mail: lusliang@mail.ustc.edu.cn)\nDali Huang is with the Beijing Research Institute of Mechanical & Electrical Technology, Beijing 102299, China. Xiaoxian Wang is with the College of Electronics and Information Engineering, Anhui University, Hefei, 230601, China, and also with the Department of Precision Machinery and Precision Instrumentation, University of Science and Technology of China, Hefei 230027, China. (e-mail: xiaoxian@ahu.edu.cn)\nMin Xia is with the Department of Engineering, Lancaster University, Lancaster LA1 4YW, U.K. (e-mail: m.xia3@lancaster.ac.uk)\nsafety hazards, or directly lead to severe safety accidents, resulting in huge losses of life and property [3, 4]. Therefore, to maintain the safe and reliable operating environment of motor equipment, the study of motor fault detection and diagnosis technologies is particularly important.\nThe rapid development of computing power has led to a new wave of deep learning (DL), which has shown great application potential and superiority in natural language processing, speech recognition, and image recognition [5-9]. Given the shortcomings of traditional fault diagnosis that heavily rely on expert knowledge and human experience, the application of DL to mechanical fault diagnosis has become a hot topic in current research. For example, Ma et al. used enhanced stacked auto-encoder for fault determination of gears and bearings [10]. Du et al. used the adaptive multifractal detrended fluctuation analysis and doubly iterative empirical mode decomposition to examine the fault diagnosis of bearings, gears, and piston pumps [11]. Song et al. investigated the accurate detection of motor faults based on envelope and time domain energy analyses [12]. Zhang et al. used stacked sparse autoencoders for interturn short-circuit fault in permanent magnet motors [13]. Liu et al. proposed a deep adversarial domain adaptation model to address the issue where the distribution of the source and target domains is inconsistent [14]. Wang et al. investigated a new fault diagnosis method using time-frequency representation and deep reinforcement learning in varying working conditions [15]. These studies show the capability of DL and further promote its application in fault diagnosis.\nOne of the most commonly used models for machine fault diagnosis is the convolutional neural network (CNN). Many CNN-based machine fault diagnosis methods can extract representative features from the raw input and have achieved outstanding classification results. For instance, Husari et al. proposed a two-stage network based on CNN and support vector machine, and this method can not only identify the fault category of the induction motor, but also the severity of failure [16]. Xie et al. used CNN to implement intelligent fault diagnosis of multi-sensor data [17]. Zhang et al. directly extracted features from the original time domain model of data through CNN model with two dropout layers and two fully connected layers [18].Ren et al. combined CNN and long-short term memory network to design a residual life prediction model for lithium batteries [19]. Based on an improved CNN with transfer learning, Shao et al. designed a new framework for fault diagnosis of rotor bearing systems under different operating conditions [20]. Peng et al. proposed a multibranch\nM\nand multiscale CNN for multiple signal components and time scale fault information [21]. The method proposed by Tran et al. achieved higher fault diagnosis accuracy for induction motor diagnosis by combining continuous wavelet transform with convolutional attention neural network (CANN) [22]. The new progressive cascading CNN model designed by Wang et al. achieved better performance of fault diagnosis under nonstationary conditions [23]. Xiong et al. developed a fault diagnosis data preprocessing method based on mutual dimensionless and similar Gram matrix [24].\nWith the rapid growth of data and its transmission, data security and privacy are becoming more concerning [25-27]. Thus, the leakage during the transmission and processing of data caused by malicious or unintentional attacks needs to be prevented. To mitigate the risks, federated learning and distributed machine learning are developing fast in many applications. Data no longer need a uniform transfer to the central processor, but rather partially or fully calculated locally. Given the reduction in data transmission, the possibility of data leakage is also considerably reduced. However, distributed devices such as miniature instruments or handheld machines are often limited in computational resources and power supply due to their compact size and remote location. Therefore, optimizing the existing CNN model for distributed deployment is necessary to ensure satisfactory performance under resource-constrained conditions.\nAnother critical issue that prevents the practical adoption of the above approaches is the large amount of data needed for training. In real-world engineering applications, equipment operates in a healthy or normal state most of the time, which causes an extremely unbalance between data collected from fault conditions and normal condition. Unbalanced data with limited fault condition data brings significant difficulties to train the conventional CNN [28]. According to literature [29], backpropagation algorithms are a special case of generalized expectation maximization, and injecting appropriate noise can speed up the relevant training of CNN. In the Mixed National Institute of Standards and Technology database (MNIST), CNN with noise injection can effectively reduce the convergence speed of the first few trainings in handwritten digit recognition. Simulation results show that the maximum noise gain can be obtained in a small dataset.\nCNN methods with noise injection show potential in improving the training efficiency. However, noise injection parameters have not been effectively optimized such that the noise gain is positive only in the first few rounds of training. For different samples, the noise yield hyperplane also needs to change accordingly, which hinders the generalizability of the model. To address these issues, we propose a novel noise-boosted convolutional neural network (NBCNN) model to achieve fault diagnosis of motor faults with limited data samples. Besides significantly accelerating the training convergence of CNN, the proposed method achieves higher prediction accuracy than other conventional models. The developed method achieves superior classification accuracy and is suitable for edge solutions where computational resources are limited and massive fault samples are difficult to be obtained. The main contributions of this paper are summarized in the following: 1) This study indicates that noise is beneficial to improve the\nCNN model performance. To our knowledge, this topic has rarely been studied in the field of CNN-based motor fault diagnosis.\n2) The proposed NBCNN method can not only effectively\nimprove the convergence speed of model training, but also improve the prediction accuracy. The average accuracy of the NBCNN model can reach 99.72%.\n3) The effectiveness and superiority of the proposed method\nhave been verified in comparison with several state-of-the-art models. The flexibility and robustness are also validated by two kinds of datasets including the Case West Reserve University (CWRU) motor bearing dataset and the motor dataset from our laboratory.\n4) The designed NBCNN has a small model size with fewer\nparameters, and can achieve high accuracy and fast fault prediction on resource-constrained edge devices such as NVIDIA Jetson TX1. The remainder of the paper is organized as follows. Section II introduces the theoretical model of NBCNN. Section III describes the experimental setup and datasets. The performance of the NBCNN model is validated in Section IV in comparison with the state-of-the-art models. Section V verifies the deployment of the NBCNN algorithm onto an edge device. Section VI presents the conclusions and future work."
        },
        {
            "heading": "II. PROPOSED METHOD",
            "text": "This section introduces a new motor fault recognition method for limited samples based on NBCNN. The flowchart of the method is shown in Fig. 1. The raw vibration signal acquired from the motor is pre-processed to enhance the fault characteristics. Subsequently, the one-dimensional (1D) signal is stacked into a two-dimensional (2D) grayscale image after removing half of the repetitive information. Finally, the converted image is processed by the NBCNN model and the fault type of the motor can be determined."
        },
        {
            "heading": "A. Data Preprocessing",
            "text": "CNN can efficiently extract feature information from a 2D image. The typical machine condition monitoring data (e.g., vibration signals) are 1D data and hence they need to be\npreprocessed to accommodate the CNN input. Converting a 1D vibration signal into a 2D image signal can be done in various methods, such as time\u2013frequency conversion and 1D stacking method [30, 31] . In this study, a two-step data preprocessing method is adopted. The first step is a segmented sampling of the original vibration data, and the second is to use two logarithmic spectra (B2LS) [32] to process the segmented signal. This approach has been proven effective in extracting distinct features of fault data in Ref. [27]. Fig. 2 illustrates the workflow of the signal preprocessing. The final result is a set of low-resolution grayscale atlases, which corresponds to the NBCNN input resolution.\nIn the segmented collection of raw data, augmentation is used to enhance the characterization ability of the small training samples [33]. Suppose that the original vibration signal length is L, each sampled segment has a length of M, and the sample coincident part length is O. The final sample size S that can be obtained is\n[ ] L O\nS M O\n\u2212 =\n\u2212 , (1)\nwhere [\u2219] means rounding down.\nThe B2LS method can balance the difference in frequency components on the amplitude scale in the original data, weaken the significant influence of larger frequency components on the classification results, and enable CNN to extract comprehensive features. For the sampling segment P[s], s=1,2, \u2026 , S mentioned above, this step can be described as\n2\n2 | ( [ ]) | [ ] log , 1,2,..., FFT P s Q i i S\nM\n  = = \n  , (2)\nwhere FFT(\u2219) denotes the fast Fourier transform, and Q[i] is the calculated symmetric sequence. Normalization to obtain the desired input grayscale image X of m row n column leads to\n[1] [2] ... [ ]\n[ 1] [ 2] ... [2 ] min( [ ])\n... ... ... ...\n[( 1) 1] ... ... [ ] 255\nmax( [ ])\nQ Q Q n\nQ n Q n Q n Q i\nQ m n Q mn X\nQ i\n    + +   \u2212    \n\u2212 + =  , (3)\nwhere max(Q[i]) and min(Q[i]) represent the maximum and minimum values of sequence Q[i], respectively. This step ensures that all points of different amplitudes in the original vibration signal are fully mapped to pixels of varying brightness in the image."
        },
        {
            "heading": "B. CNN",
            "text": "The designed CNN architecture consists of one layer each for input, convolution, max-pooling, full connection, noise injection, and output.\nWhen the gray-scale image dataset is fed into the input layer, the convolution layer extracts the information in the graph, described as\n1\nC ( * ) J\nj\nj X W B = = + , (4)\nwhere X and C denote the input and feature matrices of a single grayscale map in the convolutional step, respectively; j=1, 2, \u2026 , J, is the index of the filter W; and the symbol * represents the 2D discrete convolution between the input X and the jth filter Wj. Then, the bias matrix B is added to the summation. Later, the rectified linear unit acts on the feature matrices to prevent the gradient from disappearing. This step can be described as\n' max(0, )c c= , (5)\nwhere c and c' are the before and after elements of the output matrix C of Equation (4), respectively.\nThe max-pooling layer can down-sample the feature matrices, helping to reduce the model parameters, remove redundant information, and avoid overfitting. The max-pooling layer formula is , ,' max( : ' ' , ' ' )x y x yg g x x x m y y y n=   +   + , (6)\nwhere m and n are the width and height of the pooling window, respectively; and gx,y and g\u2019x,y are the eigenmatrix elements before and after the update, respectively.\nThe fully connected layer acts as a linear classifier for the features extracted from the upper layer. Then noise is added to speed up the training and improve prediction performance.\nThe CNN is trained using a small batch gradient descent algorithm. The weight parameters are updated by calculating and minimizing the cross-entropy loss of the predicted values, calculated by a logarithmic softmax operation as\nlog ( )i is h x= \u2212 , (7)\n( ) i\nk\nx\ni x\nk\ne h x\ne =  , (8)\nwhere xi is the value of the ith output neuron and si is the corresponding result. Equation (8) shows the calculation of h(\u2219), which is the softmax operator."
        },
        {
            "heading": "C. NBCNN",
            "text": "Conditional noise injection has been applied to CNNs [29] and validated on handwritten digit recognition in MNIST datasets. However, noise in hypercubes corresponding to different sample sets is not always beneficial. The noise gains also diminish as the training progresses. Experiments on conditional noise injection [34, 35] show that the number of noise injections account for approximately half of the total training times in each dataset trained alone. Clearly, the potential of the noise injected CNN remains to be further explored, and its gain in classification accuracy can be enhanced.\nIn this study, the cross-entropy function is used to measure the difference between the real and predicted samples. The convergence and the weight matrix update faster in fault\nclassification training than using mean squared error. The cross-entropy E(\u0398) is calculated as\n1\n( ) ln( ) I\ni i\ni E y z =  = \u2212 , (9)\nwhere zi represents the result of the output value calculated by softmax and zi=h(xi); and yi is the true value corresponding to zi. The label vector y is a binary one-hot encoding of the fault type, meaning yi=1 only if i is the correct label classification and 0 otherwise. I is the total fault type number. Thus, Equation (9) can also be written as\n( ) ln iE z = \u2212 . (10)\nEquation (10) is the deformation of the yi term with a value of 0 discarded in Equation (9). The likelihood function L(\u0398) of the network can be written as\n( ) ln ( | , )L p y z =  . (11)\nThe conditional probability density function p(y|z,\u0398) is the probability of observation y of input x at the output layer in a CNN with parameters \u0398. Continue deriving Equation (11) to obtain\n1\n( ) ln ( | , ) I\ni\ni L p y z =  =  , (12)\n1\nln i I y\ni\ni z = =  , (13)\n1\nln I\ni i\ni y z = =  , (14)\n( )E= \u2212  . (15)\nThe likelihood function of the network L(\u0398) is then equal to\nthe value of negative cross-entropy -E(\u0398), such as\n( ) ( )L E = \u2212  . (16)\nAs seen from Equation (16), minimizing E(\u0398) can maximize the likelihood L(\u0398). To explore the noise component that can reduce cross-entropy E(\u0398), we check that the cross-entropy of NBCNN meets the conditions\n( ) ( ) 0nE E \u2212   , (17)\nwhere En(\u0398) represents the cross-entropy of NBCNN. Substitute Equation (10) into Equation (17) to obtain\n1 1\nln( ) ln( ' ) 0 I I\ni i i i\ni i y z y z = = \u2212   , (18)\nwhere z\u2019i represents the predicted value of NBCNN. This case involves the mapping of noise added in the fully connected layer at the output layer, which is expanded in detail as\n1\n' ( ) J i ij j j i\nj z h U n b =\n  =  + + \n   , (19)\nwhere U is the output layer neuron matrix, \u03c9j is the output value of the previous layer, nj is the added noise, bi is biased number, and h(.) is the softmax operator. Substitute Equation (19) into Equation (18) to obtain\n1 1 1 1\nln ln ( ) 0 I J I J\ni ij j i i ij j j i\ni j i j y h U b y h U n b  = = = =\n        + \u2212  + +          \n           .(20)\nSimplified, the conditions with positive noise gain are\n( ) ( )h u h u n + , (21)\nwhere\n1\nJ\nij j i\nj u U b =\n  =  + \n   , (22)\n1\nJ\nij j\nj n U n =\n  =  \n   . (23)\nAs shown in Equations (22) and (23), respectively, u is the sample component and n is the noise component.\nOutput\nlayer\nConvolutional\nlayer Input layer\nRelu Max\nFlatten\nAdd noise\nRelu Max\nFlatten\nRelu Max\nFlatten\nMax-pooling\nlayer\nFully connected\nlayer Noise injection\nlayer\nAdd noise\nVibration\nsignal\nP rep ro ce ssin g\nFig. 3. Schematic diagram of NBCNN.\nNoise can be used to improve CNN and save resource waste when judging such conditions. When choosing the noise type, random uniform noise can be avoided and the noise that is biased and is relative to the input can be selected. On this basis, and verified by experimentation, a parametric Gaussian noise associated with the input is applied to the present method and can be expressed as\n2 22\n22\ni\ni\ne n\n \n\n\u2212\n= , (24)\nwhere \u03c9i is the influencing factor of the noise component and the output of the last fully connected layer; \u03c3 is the noise parameter and set to 0.1, and can adjust the noise amplitude and affect the performance of the CNN; and ni is the resulting noise\nto be injected before the output layer. This noise is unconditionally injected into the CNN and affects the parameter update at each iteration. The noise injection method is given in Algorithm 1. Fig. 3 shows the schematic diagram.\nAlgorithm 1. Fault diagnosis based on NBCNN\nResult: Trained CNN weight matrices Initialize network weights \u03a9 For: epoch i: 1\u2192I do\nFor: iteration j: 1\u2192J do\nDetermine a batch of K samples {xk, yk} K k=1 Forward propagation\nConvolution of the input matrix using (4) Nonlinear rectification using (5) and (6) Down-sample by max-pooling layer A 1D vector a is obtained by a fully connected layer\nInject noise Produce noise vector n using (24)\nInject noise\nBackpropagation calculation Calculate the cross-entropy error E(\u03b8)\nCalculate the error gradient (t ) ( )E  Update the parameter set \u03a9\nEnd End\nIn this study, we set the learning rate, minimum batch size, and maximum epoch number of all CNNs as 0.01, 20 and 1000, respectively. The noise parameter of NBCNN is set to 0.1 by default."
        },
        {
            "heading": "III. EXPERIMENTAL SETUP AND DATA DESCRIPTION",
            "text": "Two datasets are used to verify the proposed method; one is the CWRU bearing data [36] and the other is collected from experiments on our test bench, as shown in Fig. 4. For the convenience of distinction, these two are called datasets 1 and 2, respectively."
        },
        {
            "heading": "A. Description of Dataset 1",
            "text": "The CWRU experimental setup includes a motor, a torque transducer/decoder, a dynamometer, and an electronic controller. The bearings that support the motor hinge are monitored (the drive end bearing is SKF6205 and fan end bearing is SKF6203). Single point damage is created by electrical discharge machining for different fault conditions. Table \u2160 shows the details of dataset 1.\nTABLE \u2160\nFAULT TYPE AND LABEL OF DATASET 1\nFault label Fault type Fault depth(inch) Fault name\n1 B1 0.007 Ball fault 2 B3 0.014 Ball fault 3 B5 0.021 Ball fault 4 IR1 0.007 Inner race 5 IR3 0.014 Inner race 6 IR5 0.021 Inner race 7 OR1 0.007 Outer race 8 OR3 0.014 Outer race 9 OR5 0.021 Outer race\n10 Normal 0 Normal"
        },
        {
            "heading": "B. Description of Dataset 2",
            "text": "Fig. 4 shows the experimental setup for motor fault data collection. The test rig consists of 8 brushless direct current motors (BLDCMs) with different healthy or fault states.\nSpecifically, 2 kinds of bearing inner ring faults (BIRF) and 3 kinds of bearing outer ring faults (BORF) are generated by electrical discharge machining. The left bottom and right bottom subfigures in Fig. 4 show the BIRF and BORF with fault width of 1 mm, respectively. The rotor unbalance fault (RUF) is set by attaching an unbalanced mass to the rotor. The Hall sensor fault (HSF) is set by disconnecting one of the Hall sensor wires from the motor controller. Dataset 2 is collected from 7 faulty motors and one healthy motor, as shown in Table II. The motor is driven by the motor driver with a rated voltage of 48 VDC. The motor speed is set to around 1500 rpm. The motor vibration signal is collected by an accelerometer mounted on the motor housing, and amplified by an operational amplifier, and finally sampled by an analog-to-digital converter (ADC). The sampling frequency and duration are set as 20 kHz and 120 seconds, respectively. The raw vibration signal is transmitted from a micro controller unit (MCU) to a computer for storage and analysis. The detailed specifications of the instrument systems are summarized in Table III.\nDevice Model Manufacturer\nBLDCM 80BL110S50 SDCQ Inc.\nAccelerometer CA-YD-1182 SINOCERA Inc.\nOperational amplifier MAX9632 Maxim Inc.\nADC MAX1300 Maxim Inc. MCU STM32F407 STMicroelectronics Inc.\nWhen the training sample size is comparatively small, the sensitivity of the sample size to the final performance can be more obvious. To investigate the effects of the different sample sizes on the NBCNN, we selected different numbers of samples from each dataset to form 10 subsets, namely, subsets A\u2013E for dataset 1 and subsets F\u2013J for dataset 2. Table IV shows the details of these subsets. Each subset is used for two tests using NBCNN and CNN. Although sufficient raw data can be easily obtained in the laboratory, the fault data are usually difficult to be accessed in practical applications. Given this, this study considers the extreme conditions where only a small volume of data can be used for model training. To evaluate the effect of dataset size on the model training and prediction, the subsets with different sizes are generated."
        },
        {
            "heading": "IV. EXPERIMENTAL RESULT",
            "text": "To reduce the occasionality of experiments, 10 times of the comparative experiments for the subsets are repeated and the average accuracy is calculated. The algorithm is run on the\nPyCharm Community Edition 2021 software and implemented on a computer with GPU of RTX 3060 with 12GB memory, and CPU of AMD 5600X with 6-Cores."
        },
        {
            "heading": "A. Results of Dataset 1",
            "text": "1) NBCNN accelerates Model Training The five subsets of dataset 1 are inputted into both the NBCNN and the conventional CNN without noise injection. Fig. 5 shows the training accuracy and cross-entropy loss. In this experiment, differences in the first 500 epochs of training are highlighted.\nFig. 5 shows the training accuracy and cross-entropy loss w.r.t. the epoch of subsets A, B, C, D, and E, respectively. It can be seen that the performance of NBCNN is superior to that of conventional CNN without noise injection in both accuracy and cross-entropy loss. Meanwhile, with the increase in sample size (from subset A to E), the performance of the two CNNs improves accordingly. In this study, the number of epochs needed for a stable accuracy of over 90% is used to measure the convergence speed of CNN. Table V records the number of epochs needed for each subset of dataset 1 to reach 90% training accuracy. The results show that with the same subset, the convergence speed of NBCNN is 6\u20139 times that of CNN. The performance boosted by noise increases as the sample size decreases, which demonstrates the strong capability of the developed method in applications with a small-size dataset. Meanwhile, the convergence speed and cross-entropy loss of subsets A and B by NBCNN are similar to those of subsets D and E in CNN, respectively. Subsets A and B have only one-tenth samples as subsets D and E. The benefit of noise for CNN with small samples is similar to that with ten times samples, which is highly advantageous to industrial fault diagnosis where sufficient fault data can hardly be obtained.\n0 100 200 300 400 500 0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n100 200 300 400 500 100 200 300 400 500 100 200 300 400 500\n0%\n20%\n40%\n60%\n80%\n100%\n100 200 300 400 500\n0 2 4 6 8 10 12 14 0%\n20%\n40%\n60%\n80%\n100%\n0 2 4 6 8 10 12 14 0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n(A)\nL o\nss\n10515\n(B)\n557\n(C)\nEpoch\n294\n(D)\n22827\nCNN NBCNN\nA cc\nu ra\ncy\n122\n(E)\nFig. 5. Training accuracy and cross-entropy loss of subsets A, B, C, D, and E.\n2) NBCNN Improves Fault Diagnosis Accuracy It can be seen that the proposed NBCNN method effectively accelerates the model training procedure. This subsection\nvalidates the fault diagnosis performance of the NBCNN model in comparison with three conventional methods, i.e., CNN without noise injection, 1D-CNN [37], and CANN [22]. In the 1D-CNN method, the raw vibration signal is directly inputted into the model. In the CANN method, the 1D vibration signal is transformed to a 2D image using continuous wavelet transform, and then the 2D image is inputted into the model. The key hyperparameters of the 1D-CNN and CANN are configured according to that in Refs. [22, 37], respectively.\nThe fault diagnosis accuracies under different numbers of training samples are evaluated and compared, and the results are shown in Fig. 6. It can be seen that when the number of training samples is 5000, the accuracy of the proposed method is 99.68%, which is higher than that of other compared models. When the number of training samples decreased to 200, the accuracy of all the models decreases accordingly. Nevertheless, the NBCNN still maintains the highest accuracy (92.75%), and the accuracy of the CNN, CANN, and 1D-CNN model is 85.25%, 73.00%, and 50.75%, respectively.\nWhen the number of samples is limited, the capacity of feature learning will affect the fault diagnosis accuracy. The proposed NBCNN model firstly pre-processes the original signal to make the signal features easier to be discriminated, and then injects approximate noise in the fully connected layer to avoid the over dependency on certain features. The randomness enhances the generalization ability and robustness of the model to some extent. In contrast, the traditional DL models require sufficient training samples to extract the fault features. When the number of samples is limited, the insufficient information embedding in the sample prevents an effective extraction of features. In this case, the model is difficult to converge during training. Subsequently, the performance of the DL models will degenerate significantly, which finally leads to a decrease in fault diagnosis accuracy. The distinct feature of the NBCNN model, i.e., requires fewer samples for training, makes it suitable for practical applications where the labeled data is hard to be obtained.\nTo further compare the effects of NBCNN and CNN on the accuracy of fault classification, the output features of subset A are visualized as a 2D distribution via t-SNE. Fig. 7 shows that the features extracted by NBCNN can effectively distinguish different types of faults. Different faults also have apparent\ndecomposition bands. For CNN, except for normal features, samples with the same fault are clustered loosely, and more overlaps are observed between different faults, such as B1 and B3 in Fig. 7(b)."
        },
        {
            "heading": "B. Results of Dataset 2",
            "text": "In this subsection, dataset 2 is processed to further verify the robustness and generalization ability of the proposed NBCNN model. Dataset 1 from CWRU is composed of the motor bearing data with different fault types. Dataset 2 is composed of both the mechanical and electrical faults of the test motors.\nThe vibration data is collected from the experimental apparatus as shown in Fig. 4. Table VI shows the epochs required to reach training accuracy of 90% of dataset 2. In subset F, noise improves the training speed by up to 10 times. The convergence speeds of subsets F and G using NBCNN are comparable to subsets I and J using CNN, which shows significant improvement in convergence speed of NBCNN.\nFig. 8 shows the prediction accuracy of different models including NBCNN, CNN, 1-D CNN, and CANN. The proposed NBCNN still shows considerable improvement when trained with a small number of samples. For instance, when the number of samples decreases to 160, the accuracy of NBCNN, CNN, 1D-CNN, and CANN is 97.50%, 93.75%, 93.75%, and 86.25%, respectively. The comparative results of the four models on dataset 1 and dataset 2 confirm the effectiveness and robustness of the proposed method. The NBCNN model can effectively diagnose both the mechanical and electrical faults in a motor, and also shows potential to be extended and applied to other datasets and fault diagnosis applications."
        },
        {
            "heading": "C. Parameter Evaluation",
            "text": "The performance of the DL model is affected by both the data and model structure. The methods and strategies for determination of the proper CNN parameters have been widely investigated in the literature. For instance, Liao et al. investigated the relationship between CNN parameter distribution and CNN discrimination performance [38]. Zhang et al. proposed a high-precision CNN model with successive small kernels, and the parameters of the dropout and fully connected layers were evaluated and discussed [18]. Wen et al. firstly investigated the effect of learning rate on classification accuracy, and then proposed a new learning rate adjustment\nstrategy for CNN-based fault classification [39]. The NBCNN is an improved version of the classical CNN, and hence the optimization of the conventional parameters of the CNN can be referred to the references. The most distinct feature of the NBCNN is that a proper noise is injected into the specific layer of the model. Given this, the effects of the injected noise on the prediction accuracy are evaluated and discussed in this study.\nThe above experiments show that adding suitable noise can effectively improve the training and prediction performance of CNN. To investigate the influence of noise parameter \u03c3 on training results, we inject different noises into CNN. Equation (24) shows the sign of \u03c3 does not affect the magnitude of the noise, and thus only the case where \u03c3 > 0 needs discussion. The gain varies with the change of noise parameter on CNN training. Fig. 5 shows that the effect of noise on CNN training is mainly concentrated in the first 500 epochs of training, thus this gain can be measured with Equation (25)\n( ) 500\ni 1= 500\ni\ni iB A\nDIF\n=\n= \u2212 , (25)\nwhere Ai represents the accuracy of the noiseless CNN at ith training; Bi indicates the NBCNN accuracy at ith training; and DIF measures the degree of influence of noise on the training. Fig. 9 shows the relationship between DIF and \u03c3 in subset A.\nThe dots in the upper half of Fig. 9 indicate that the NBCNN performs better than the CNN, while those in the lower half indicate that the noise reduces the convergence speed of CNN. From the results of the multiple tests in subset A, it can be seen that NBCNN can achieve the best performance when the noise parameter \u03c3 is set to 0.1. When \u03c3 exceeds 0.1, DIF decreases and converges to 0. This result is reasonable because as \u03c3 increases, the noise amplitude also approaches 0. This is equivalent to a noiseless CNN, and thus the accuracy difference DIF is 0. When \u03c3 is less than 0.1 and greater than 0.05, the noise has positive feedback on CNN. As \u03c3 continues to decrease and approach 0, the noise amplitude continues to increase and impairs the performance of CNN.\nIn addition, in all of the above experiments, although the noise waveforms are different in each test, the NBCNN can still yield the optimal result when \u03c3 = 0.1. For both dataset 1 and dataset 2, the relationship between the performance and noise parameter in each subset follows the pattern shown in Fig. 9. When the \u03c3 value is set around 0.1, the performance of NBCNN exceeds that of CNN, which means that CNN is sensitive to\nnoise with a certain intensity. When the noise intensity is too large, the original signal features would be masked which leads to a decrease in prediction accuracy. Besides the noise intensity, different types of noises such as colored noise, broadband noise, and narrow band noise, could also affect the performance of NBCNN. Additionally, the noises injected into different model layers can also affect the performance of the NBCNN method. These topics remain a further study in the future."
        },
        {
            "heading": "V. PERFORMANCE ON EDGE COMPUTING PLATFORM",
            "text": "The above experimental results indicate that the NBCNN has remarkable performance in both training and prediction procedures. Besides, the NBCNN model also has a small size and can be easily deployed onto resource-constrained edge devices for online and real-time motor fault recognition. For example, the total number of model parameters trained from subset A is less than 40,000 and the model size is less than 200 Kbytes, and this model is implemented onto the edge platform to evaluate its actual performance.\nThe effectiveness of the proposed method on edge computing platforms is verified by 10 independent comparative experiments of subset A run on an NVIDIA Jetson TX1 edge device with 4G memory. Fig. 10 shows the experimental setup. Apart from the comparisons on training and prediction performance, the actual time the algorithm runs on edge computing platforms is also evaluated.\nFig. 11 plots the accuracy and cross-entropy loss variation of the CNN and NBCNN training in Jetson TX1. The epochs required for the accuracies of model training to reach 90% are 255 and 25, respectively. This result is consistent with the conclusion of the convergence speed gap of up to 10 times in the previous subsections on experiments on the desktop GPU. In addition, the overall accuracy and cross-entropy loss of NBCNN are better than those of CNN.\nDeploying CNN in resource-constrained edge devices must also consider algorithm execution time in addition to basic performance requirements. Fig. 12(a) shows the time spent for each epoch of CNN and NBCNN training. NBCNN has a higher training time cost than CNN in each epoch, a reasonable result because NBCNN is realized by adding noise on the basis of the CNN model. However, to consider only the time spent on each epoch of training has no practical significance. The actual training time to reach 90% accuracy can be calculated by considering the number of training epochs, and the results are shown in Fig. 12(b). Both 6.6 s on edge device and 1.5 s on the computer for NBCNN are more than nine times faster than CNN\u2019s 60.5 s and 14.5 s respective results.\nThe comparison in fault prediction performance between the two models is also tested 10 times and the results are shown in Fig. 13. The inference time of the NBCNN model is slightly higher than that of the CNN method due to the fact that external noise is injected into the model, as shown in Fig. 13(a). Besides, the inference time on the edge device is also higher than that on the desktop GPU, as the edge device has lower GPU frequency and memory. Nevertheless, the average time for processing each frame of signal is only 36.5 ms. Such a time consumption is fast enough for real-time fault diagnosis.\nThe prediction accuracy of NBCNN is significantly higher than that of CNN, with an average of 93.25% for NBCNN compared with only 84.75% for CNN, and their difference is 6.5%. It should be noticed that Subset A has only 200 training samples, which is the smallest sample size among the subsets of dataset 1. Based on the previous experimental results, the prediction results of this model can represent the worst performance of NBCNN. Even in this extreme situation, the NBCNN can still obtain a considerable accuracy, which demonstrates its robustness and reliability.\nIn addition, the TX1 has much fewer computational resources in comparison with the typical DL device such as a desktop GPU. Under this circumstance, the above experimental results indicate that the NBCNN model can still easily execute the computation on this edge device. In the future, the NBCNN can be tested on cheaper devices with less computational resources, which could further improve the scalability and flexibility for real-time motor fault diagnosis."
        },
        {
            "heading": "VI. CONCLUSIONS",
            "text": "This paper designs an improved CNN model to solve the problem of limited training samples in industrial applications. Adding the proper amount of noise to the CNN can speed up training and improve inference accuracy. Experiments on two datasets show that NBCNN could significantly improve the convergence speed compared with the conventional CNN without noise injection. In addition, the prediction accuracy is also significantly improved with NBCNN in comparison with several state-of-the-art models. Experiments in resource-constrained edge platforms also show that the proposed method achieves better performance. The proposed method is widely applicable to machine fault diagnosis where fault data is typically difficult to be collected."
        }
    ],
    "year": 2022
}