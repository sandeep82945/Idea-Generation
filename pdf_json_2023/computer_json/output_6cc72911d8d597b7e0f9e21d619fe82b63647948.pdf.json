{
    "abstractText": "Complementary label learning (CLL) requires annotators to give irrelevant labels instead of relevant labels for instances. Currently, CLL has shown its promising performance on multi-class data by estimating a transition matrix. However, current multi-class CLL techniques cannot work well on multi-labeled data since they assume each instance is associated with one label while each multi-labeled instance is relevant to multiple labels. Here, we show theoretically how the estimated transition matrix in multi-class CLL could be distorted in multi-labeled cases as they ignore co-existing relevant labels. Moreover, theoretical findings reveal that calculating a transition matrix from label correlations in multi-labeled CLL (ML-CLL) needs multi-labeled data, while this is unavailable for ML-CLL. To solve this issue, we propose a two-step method to estimate the transition matrix from candidate labels. Specifically, we first estimate an initial transition matrix by decomposing the multi-label problem into a series of binary classification problems, then the initial transition matrix is corrected by label correlations to enforce the addition of relationships among labels. We further show that the proposal is classifier-consistent, and additionally introduce an MSE-based regularizer to alleviate the tendency of BCE loss overfitting to noises. Experimental results have demonstrated the effectiveness of the proposed method.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yi Gao"
        },
        {
            "affiliations": [],
            "name": "Miao Xu"
        }
    ],
    "id": "SP:275e896e2c7ccb39a4882b36bfe64eddf926fcf5",
    "references": [
        {
            "authors": [
                "M.-L. Zhang",
                "Z.-H. Zhou"
            ],
            "title": "A review on multi-label learning algorithms",
            "venue": "IEEE Trans. Knowl. Data Eng., vol. 26, no. 8, pp. 1819\u2013 1837, 2014.",
            "year": 1819
        },
        {
            "authors": [
                "M.-L. Zhang",
                "L. Wu"
            ],
            "title": "Lift: Multi-label learning with labelspecific features",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 37, no. 1, pp. 107\u2013120, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "T.N. Rubin",
                "A. Chambers",
                "P. Smyth",
                "M. Steyvers"
            ],
            "title": "Statistical topic models for multi-label document classification",
            "venue": "Mach. Learn., vol. 88, no. 1-2, pp. 157\u2013208, 2012.",
            "year": 2012
        },
        {
            "authors": [
                "P.-J. Tang",
                "M. Jiang",
                "B.N. Xia",
                "J.W. Pitera",
                "J. Welser",
                "N.V. Chawla"
            ],
            "title": "Multi-label patent categorization with non-local attention-based graph convolutional network",
            "venue": "Proceedings of the 34th Conference on Artificial Intelligence, York, NY, 2020, pp. 9024\u2013 9031.",
            "year": 2020
        },
        {
            "authors": [
                "A. Lambrecht",
                "C. Tucker"
            ],
            "title": "When does retargeting work? information specificity in online advertising",
            "venue": "Journal of Marketing research, vol. 50, no. 5, pp. 561\u2013576, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "T. Ishida",
                "G. Niu",
                "W.-H. Hu",
                "M. Sugiyama"
            ],
            "title": "Learning from complementary labels",
            "venue": "Advances in Neural Information Processing Systems 30, Long Beach, CA, 2017, pp. 5639\u20135649.",
            "year": 2017
        },
        {
            "authors": [
                "T. Ishida",
                "G. Niu",
                "A.K. Menon",
                "M. Sugiyama"
            ],
            "title": "Complementary-label learning for arbitrary losses and models",
            "venue": "Proceedings of the 36th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research, Long Beach, CA, 2019, pp. 2971\u20132980.",
            "year": 2019
        },
        {
            "authors": [
                "X.-Y. Yu",
                "T.-L. Liu",
                "M.-M. Gong",
                "D.-C. Tao"
            ],
            "title": "Learning with biased complementary labels",
            "venue": "Proceedings of the 15th European Conference on Computer Vision, Munich, Germany, 2018, pp. 69\u201385.",
            "year": 2018
        },
        {
            "authors": [
                "Y.-T. Chou",
                "G. Niu",
                "H.-T. Lin",
                "M. Sugiyama"
            ],
            "title": "Unbiased risk estimators can mislead: A case study of learning with complementary labels",
            "venue": "Proceedings of the 37th International Conference on Machine Learning, Virtual Event, 2020, pp. 1929\u20131938.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Gao",
                "M.-L. Zhang"
            ],
            "title": "Discriminative complementary-label learning with weighted loss",
            "venue": "Proceedings of the 38th International Conference on Machine Learning, Virtual Event, 2021, pp. 3587\u20133597.",
            "year": 2021
        },
        {
            "authors": [
                "D.-B. Wang",
                "L. Feng",
                "M.-L. Zhang"
            ],
            "title": "Learning from complementary labels via partial-output consistency regularization",
            "venue": "Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, Virtual Event, 2021, pp. 3075\u20133081.",
            "year": 2021
        },
        {
            "authors": [
                "L. Feng",
                "T. Kaneko",
                "B. Han",
                "G. Niu",
                "B. An",
                "M. Sugiyama"
            ],
            "title": "Learning with multiple complementary labels",
            "venue": "Proceedings of the 37th International Conference on Machine Learning, Virtual Event, 2020, pp. 3072\u20133081.",
            "year": 2020
        },
        {
            "authors": [
                "A. Ghosh",
                "H. Kumar",
                "P.S. Sastry"
            ],
            "title": "Robust loss functions under label noise for deep neural networks",
            "venue": "Proceedings of the 31st AAAI Conference on Artificial Intelligence, San Francisco, CA, 2017, pp. 1919\u20131925.",
            "year": 2017
        },
        {
            "authors": [
                "T.S. Sindlinger"
            ],
            "title": "Crowdsourcing: why the power of the crowd is driving the future of business",
            "venue": "2010.",
            "year": 2010
        },
        {
            "authors": [
                "M.-L. Zhang",
                "Z.-H. Zhou"
            ],
            "title": "A review on multi-label learning algorithms",
            "venue": "IEEE Trans. Knowl. Data Eng., vol. 26, no. 8, pp. 1819\u2013 1837, 2014.",
            "year": 1819
        },
        {
            "authors": [
                "F. Wu",
                "Z.-H. Wang",
                "Z.-F. Zhang",
                "Y. Yang",
                "J.-B. Luo",
                "W.-W. Zhu",
                "Y.-T. Zhuang"
            ],
            "title": "Weakly semi-supervised deep learning for multilabel image annotation",
            "venue": "IEEE Trans. Big Data, vol. 1, no. 3, pp. 109\u2013122, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "S.S. Bucak",
                "R. Jin",
                "A.K. Jain"
            ],
            "title": "Multi-label learning with incomplete class assignments",
            "venue": "The 24th IEEE Conference on Computer Vision and Pattern Recognition, Colorado Springs, CO, 2011, pp. 2801\u20132808.",
            "year": 2011
        },
        {
            "authors": [
                "W.-W. Liu",
                "I.W. Tsang",
                "K. M\u00fcller"
            ],
            "title": "An easy-to-hard learning paradigm for multiple classes and multiple labels",
            "venue": "J. Mach. Learn. Res., vol. 18, pp. 94:1\u201394:38, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "M.-L. Zhang",
                "Y.-K. Li",
                "X.-Y. Liu",
                "X. Geng"
            ],
            "title": "Binary relevance for multi-label learning: an overview",
            "venue": "Frontiers Comput. Sci., vol. 12, no. 2, pp. 191\u2013202, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "M.R. Boutell",
                "J.-B. Luo",
                "X.-P. Shen",
                "C.M. Brown"
            ],
            "title": "Learning multi-label scene classification",
            "venue": "Pattern Recognit., vol. 37, no. 9, pp. 1757\u20131771, 2004.",
            "year": 2004
        },
        {
            "authors": [
                "M.-L. Zhang",
                "Z.-H. Zhou"
            ],
            "title": "ML-KNN: A lazy learning approach to multi-label learning",
            "venue": "Pattern Recognit., vol. 40, no. 7, pp. 2038\u20132048, 2007.",
            "year": 2007
        },
        {
            "authors": [
                "A. Elisseeff",
                "J. Weston"
            ],
            "title": "A kernel method for multi-labelled classification",
            "venue": "Advances in Neural Information Processing Systems 14, Vancouver, Canada, 2001, pp. 681\u2013687.",
            "year": 2001
        },
        {
            "authors": [
                "J. Read",
                "B. Pfahringer",
                "G. Holmes",
                "E. Frank"
            ],
            "title": "Classifier chains for multi-label classification",
            "venue": "Mach. Learn., vol. 85, no. 3, pp. 333\u2013 359, 2011.",
            "year": 2011
        },
        {
            "authors": [
                "G. Tsoumakas",
                "I. Katakis",
                "I.P. Vlahavas"
            ],
            "title": "Random k-labelsets for multilabel classification",
            "venue": "IEEE Trans. Knowl. Data Eng., vol. 23, no. 7, pp. 1079\u20131089, 2011.",
            "year": 2011
        },
        {
            "authors": [
                "M.-L. Zhang",
                "Z.-H. Zhou"
            ],
            "title": "Multilabel neural networks with applications to functional genomics and text categorization",
            "venue": "IEEE transactions on Knowledge and Data Engineering, vol. 18, no. 10, pp. 1338\u20131351, 2006.",
            "year": 2006
        },
        {
            "authors": [
                "Y.-C. Li",
                "Y. Song",
                "J.-B. Luo"
            ],
            "title": "Improving pairwise ranking for multi-label image classification",
            "venue": "Proceedings of 2017 IEEE conference on computer vision and pattern recognition, Honolulu, HI, 2017, pp. 3617\u20133625.",
            "year": 2017
        },
        {
            "authors": [
                "S.-W. Ji",
                "L. Tang",
                "S.-P. Yu",
                "J.-P. Ye"
            ],
            "title": "A shared-subspace learning framework for multi-label classification",
            "venue": "ACM Trans. Knowl. Discov. Data, vol. 4, no. 2, pp. 8:1\u20138:29, 2010.",
            "year": 2010
        },
        {
            "authors": [
                "W. Gerych",
                "T. Hartvigsen",
                "L. Buquicchio",
                "E. Agu",
                "E.A. Rundensteiner"
            ],
            "title": "Recurrent bayesian classifier chains for exact multilabel classification",
            "venue": "Advances in Neural Information Processing Systems 34, virtual event, 2021, pp. 15 981\u201315 992.",
            "year": 2021
        },
        {
            "authors": [
                "W.-T. Zhao",
                "S.-F. Kong",
                "J.-W. Bai",
                "D. Fink",
                "C.P. Gomes"
            ],
            "title": "HOT-VAE: learning high-order label correlation for multi-label classification via attention-based variational autoencoders",
            "venue": "Proceedings of 35th AAAI Conference on Artificial Intelligence, Virtual Event, 2021, pp. 15 016\u201315 024.",
            "year": 2021
        },
        {
            "authors": [
                "L.-C. Wang",
                "Z.-M. Ding",
                "S.-J. Han",
                "J.-J. Han",
                "C. Choi",
                "Y. Fu"
            ],
            "title": "Generative correlation discovery network for multi-label learning",
            "venue": "Proceedings of 2019 IEEE International Conference on Data Mining, Beijing, China, 2019, pp. 588\u2013597.",
            "year": 2019
        },
        {
            "authors": [
                "G.-X. Xun",
                "K. Jha",
                "J.-H. Sun",
                "A.-D. Zhang"
            ],
            "title": "Correlation networks for extreme multi-label text classification",
            "venue": "Proceedings of 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, 2020, pp. 1074\u20131082.",
            "year": 2020
        },
        {
            "authors": [
                "L. Sun",
                "S. Feng",
                "J. Liu",
                "G. Lyu",
                "C. Lang"
            ],
            "title": "Global-local label correlation for partial multi-label learning",
            "venue": "IEEE Transactions on Multimedia, vol. PP, no. 99, pp. 1\u20131, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Z.-H. Zhou"
            ],
            "title": "A brief introduction to weakly supervised learning",
            "venue": "National science review, vol. 5, no. 1, pp. 44\u201353, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "M.-K. Xie",
                "S.-J. Huang"
            ],
            "title": "Partial multi-label learning",
            "venue": "Proceedings of the 32nd AAAI Conference on Artificial Intelligence, New Orleans, LA, 2018, pp. 4302\u20134309.",
            "year": 2018
        },
        {
            "authors": [
                "M.-K. Xie",
                "S.-J. Huang"
            ],
            "title": "Partial multi-label learning with noisy label identification",
            "venue": "Proceedings of 34th AAAI Conference on Artificial Intelligence, New York, NY, 2020, pp. 6454\u20136461.",
            "year": 2020
        },
        {
            "authors": [
                "L.-J. Sun",
                "S.-H. Feng",
                "T. Wang",
                "C.-Y. Lang",
                "Y. Jin"
            ],
            "title": "Partial multi-label learning by low-rank and sparse decomposition",
            "venue": "Proceedings of the 33rd AAAI Conference on Artificial Intelligence, Honolulu, HI, 2019, pp. 5016\u20135023.",
            "year": 2019
        },
        {
            "authors": [
                "G.-X. Yu",
                "X. Chen",
                "C. Domeniconi",
                "J. Wang",
                "Z. Li",
                "Z.-L. Zhang",
                "X.-D. Wu"
            ],
            "title": "Feature-induced partial multi-label learning",
            "venue": "Proceedings of 2018 IEEE International Conference on Data Mining, Singapore, 2018, pp. 1398\u20131403.",
            "year": 2018
        },
        {
            "authors": [
                "Y.-W. Xu",
                "M.-M. Gong",
                "J.-X. Chen",
                "T.-L. Liu",
                "K. Zhang",
                "K. Batmanghelich"
            ],
            "title": "Generative-discriminative complementary learning",
            "venue": "Proceedings of the 34th AAAI Conference on Artificial Intelligence, New York, NY, 2020, pp. 6526\u20136533.",
            "year": 2020
        },
        {
            "authors": [
                "S. Diplaris",
                "G. Tsoumakas",
                "P.A. Mitkas",
                "I.P. Vlahavas"
            ],
            "title": "Protein classification with multiple algorithms",
            "venue": "Advances in 10th Panhellenic Conference on Informatics, vol. 3746, Volos, Greece, 2005, pp. 448\u2013456.",
            "year": 2005
        },
        {
            "authors": [
                "G. Patrini",
                "A. Rozza",
                "A.K. Menon",
                "R. Nock",
                "L.-Z. Qu"
            ],
            "title": "Making deep neural networks robust to label noise: A loss correction approach",
            "venue": "Proceedings of 2017 IEEE Conference on Computer Vision and Pattern Recognition, Honolulu, HI, 2017, pp. 2233\u20132241.",
            "year": 2017
        },
        {
            "authors": [
                "X.-B. Xia",
                "T.-L. Liu",
                "N.-N. Wang",
                "B. Han",
                "C. Gong",
                "G. Niu",
                "M. Sugiyama"
            ],
            "title": "Are anchor points really indispensable in labelnoise learning?",
            "venue": "Advances in Neural Information Processing Systems 32,",
            "year": 2019
        },
        {
            "authors": [
                "J.-Q. Lv",
                "M. Xu",
                "L. Feng",
                "G. Niu",
                "X. Geng",
                "M. Sugiyama"
            ],
            "title": "Progressive identification of true labels for partial-label learning",
            "venue": "Proceedings of the 37th International Conference on Machine Learning, Virtual Event, 2020, pp. 6500\u20136510.",
            "year": 2020
        },
        {
            "authors": [
                "Z.-L. Zhang",
                "M.R. Sabuncu"
            ],
            "title": "Generalized cross entropy loss for training deep neural networks with noisy labels",
            "venue": "Advances in Neural Information Processing Systems 31, Montr\u00e9al, Canada, 2018, pp. 8792\u20138802.",
            "year": 2018
        },
        {
            "authors": [
                "Y. Katsura",
                "M. Uchida"
            ],
            "title": "Bridging ordinary-label learning and complementary-label learning",
            "venue": "Proceedings of the 12th Asian Conference on Machine Learning, ser. Proceedings of Machine Learning Research, Bangkok, Thailand, 2020, pp. 161\u2013176.",
            "year": 2020
        },
        {
            "authors": [
                "A. Paszke",
                "S. Gross",
                "F. Massa",
                "A. Lerer",
                "J. Bradbury",
                "G. Chanan",
                "T. Killeen",
                "Z.-M. Lin",
                "N. Gimelshein",
                "L. Antiga",
                "A. Desmaison",
                "A. K\u00f6pf",
                "E. Yang",
                "Z. DeVito",
                "M. Raison",
                "A. Tejani",
                "S. Chilamkurthy",
                "B. Steiner",
                "L. Fang",
                "J.-J. Bai",
                "S. Chintala"
            ],
            "title": "Pytorch: An imperative style, high-performance deep learning library",
            "venue": "Advances in Neural Information Processing Systems 32, Vancouver, Canada, 2019, pp. 8024\u20138035.",
            "year": 2019
        },
        {
            "authors": [
                "M.-L. Zhang",
                "L. Wu"
            ],
            "title": "Lift: Multi-label learning with labelspecific features",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 37, no. 1, pp. 107\u2013120, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "D.P. Kingma",
                "J. Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "Proceedings of the 3rd International Conference on Learning Representations, San Diego, CA, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "L. Feng",
                "J. Huang",
                "S.-L. Shu",
                "B. An"
            ],
            "title": "Regularized matrix factorization for multilabel learning with missing labels",
            "venue": "IEEE Trans. Cybern., vol. 52, no. 5, pp. 3710\u20133721, 2022.",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014Complementary label learning, multi-label learning, transition matrix, label correlations.\nF"
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "In multi-label learning (MLL), each instance is associated with a set of relevant labels, where the learned classifier aims to predict all relevant labels of unseen instances [1], [2]. MLL is widely used in many real-world applications, such as text categorization [3], [4], image retrieval [5], etc. However, collecting precisely multi-labeled data is laborious because of the unknown number of relevant labels per instance and the existence of complex semantic labels. For the example image in Fig. 1, besides the label Architecture, there exist other relevant labels whose accurate annotation needs one-by-one checking of the whole label space; in addition, annotators need special geographical and cultural domain knowledge to accurately label the image as Paris.\nTo release the laborious of annotating multi-labeled data, we explore the problem setting of multi-labeled CLL (MLCLL), where each instance is associated with a single complementary label (an irrelevant label of the instance) instead of multiple relevant labels. Providing such weakly supervised information will ease the labeling process in large label space because selecting one complementary label is low-cost and requires less domain knowledge than selecting all relevant labels. One example of ML-CLL is given in Fig. 1 when selecting desert as the complementary label. Given the complementary label, the goal of ML-CLL is still the\n\u2022 Yi Gao is with the School of Cyber Science and Engineering, Southeast University, Nanjing 210096, China and the Key Laboratory of Computer Network and Information Integration (Southeast University), Ministry of Education, China. E-mail: gao yi@seu.edu.cn \u2022 Miao Xu is with The University of Queensland, Australia. E-mail: miao.xu@uq.edu.au \u2022 Min-Ling Zhang (corresponding author) is with the School of Computer Science and Engineering,Southeast University, Nanjing 210096, China and the Key Laboratory of Computer Network and Information Integration (Southeast University), Ministry of Education, China. E-mail: zhangml@seu.edu.cn\nManuscript received April 19, 2005; revised August 26, 2015.\nsame as fully supervised MLL, i.e., learning a model that can accurately predict multiple relevant labels for unseen instances.\nThe setting of CLL was initially applied in the multi-class learning task [6], [7], [8], [9], [10], [11], [12]. Previous multiclass CLL approaches are based on an estimated transition matrix that summarizes the probability of a label being selected as a complementary label [6], [7], [8]. Although they have achieved a promising performance on multi-class data, they are restricted to the case where an instance is associated with only one relevant label. In this case, multi-class CLL approaches only consider the exclusive relationship among labels, while these approaches ignore that labels can bear other relationships in the multi-labeled case, especially the co-occurrence of labels. In fact, relationships among labels are crucial to solving ML-CLL problems since the selection of a complementary label of an instance in MLL is the combined result against multiple relevant labels rather than against only a single relevant label. Misusing a technique targeting against a single relevant label to the multiple relevant labels case will result in a wrongly estimated transition\nar X\niv :2\n30 2.\n12 98\n7v 1\n[ cs\n.L G\n] 2\n5 Fe\nb 20\n23\nmatrix. In this paper, we first theoretically analyze how the estimation of the transition matrix using the current multi-class CLL techniques could be distorted in multi-labeled cases. According to these findings, we observe that estimating the transition matrix in ML-CLL from label correlations needs to know relevant labels of instances, while these are unavailable. To remove this pain, we propose a two-step method to estimate the transition matrix in ML-CLL from candidate labels which are the complement of complementary labels. Our strategy includes: (1) estimating an initial transition matrix by decomposing the multi-label problem into binary classification problems; (2) using label correlations to correct the initial transition matrix by enforcing the addition of relationships among labels. The fast convergence of CrossEntropy (CE) loss benefits from focusing on instances that are difficult to classify, which may result in CE loss overfitting to noisy labeled data. As a type of CE loss, Binary CE (BCE) loss has the same problem. The study of [13] indicates that Mean square error (MSE) loss is less sensitive to noisy labels than CE loss. As Binary CE (BCE) loss is a benchmark of our approach, an MSE-based regularizer is further introduced to alleviate the tendency of it overfitting to noises.\nIn addition, we show that our proposed ML-CLL can be easily combined with learning from relevant labels, which significantly extends the application scenario of the proposed algorithm. This combination is particularly useful, e.g. when labels are collected via crowdsourcing [14] where crowdworkers are asked to randomly select a complementary label and one or more relevant labels for an instance. Experimental results on various datasets demonstrate the effectiveness of the proposed approach. Especially in situation when each instance is only equipped with a complementary label and a relevant label, our proposal has superior performance, even comparable with the performance on fully supervised data. Our main contributions are summarized as follows:\n\u2022 We theoretically analyze the distortion of the transition matrix estimated by multi-class CLL in multilabeled cases, because multi-class CLL techniques ignore the co-existence of relevant labels. Theoretical findings reveal that multi-labeled data is indispensable for calculating the transition matrix from label correlations. \u2022 To solve the problem of unavailable multi-labeled data, we propose a two-step method to estimate the transition matrix from candidate labels. Moreover, we show theoretically that the proposed approach is classifier-consistent under a mild assumption. \u2022 We introduce a practical strategy \u2013 MSE-based regularization \u2013 to alleviate the overfitting tendency of BCE loss. Our empirical study shows that the proposal obtains comparable performance with state-ofthe-art baselines, which proves the effectiveness of our approach.\nThe rest of this paper are organized as follows. Section 2 briefly reviews related work of ML-CLL. Then we formalize the ML-CLL problem in Section 3, analyze it theoretically and describe our approach in Section 4. In Section 5, we introduce an MSE-based regularization and show how to\nadapt our method to bear an additional small amount of relevant labels. The experimental results are given in Section 6 and we conclude in Section 7."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "In this section, we will give a brief review of related work of ML-CLL, including MLL, partial multi-label learning (PML) and multi-class CLL."
        },
        {
            "heading": "2.1 Multi-Label Learning",
            "text": "MLL problems aim to train a classifier that can predict a set of relevant labels for an unseen instance, where each training instance is associated with multiple relevant labels simultaneously. With the complexity of label correlation, the previous studies can be grouped into three categories [15], [16], [17], [18]: first-order approach [19], [20], [21], second-order approach [22], [23] and high-order approach [24], [25]. To solve MLL problems, the first-order approach decomposes MLL problems into a set of binary classification problems [19], [20]. However, these approaches ignore label correlations among labels, which play a crucial role in MLL [15]. After realizing the importance of label correlation, more and more studies attempt to exploit it to improve MLL performance. Among them, the second-order approach considers the pairwise label correlations that refer to the relationship between two labels. The kind of these approaches generally transform MLL problems into bipartite ranking problems by enforcing that relevant labels should be ranked higher than irrelevant labels [23], [26], [27]. Beyond second-order relationship, there exists more complex relationship between labels in many real-world scenarios. Therefore, many approaches begin to exploit high-order label correlations to handle the MLL problems recently [24], [28], [29], [30]. For example, Zhao et al. [30] leverage variational autoencoder to facilitate the learning process via exploiting high-order correlations among labels, while Wang et al. and Xun et al. [31], [32] both design special neural network blocks to automatically extract label correlations to improve the label prediction performance. Although high-order approaches have the ability of stronger label correlation-modeling, they may suffer from high computational cost comparing to first and second-orders approaches [33]."
        },
        {
            "heading": "2.2 Partial Multi-Label Learning",
            "text": "Due to that the fully supervised data is difficult to collect, many reseachers tend to explore the weakly supervision data form to alleviate the heavy load of labeled data collection [34]. PML is a recently emerging weakly supervised approch firstly proposed by Xie et al. [35]. In PML, each training instance is associated with a set of candidate labels that consist of relevant labels and irrelevant (noisy) labels and the goal is to learn a classifier assigning a set of labels accurately for unseen instances.\nAt the first glance, it seems that ML-CLL is an extreme case of PML, such that all PML methods are also applicable to ML-CLL. However, existing PML methods assume that noisy only composes a small portion in the candidate labels [33], [36], [37], [38], such that many approaches [33], [37], [38] adopt matrix factorization matrix factorization to\ntackle PML problems, which decompose the candidate label matrix into the low-rank multi-label matrix and the sparse noisy label matrix. Compared to PML, the studied MLCLL problem in this paper are target at the problem with only one complementary label, resulting in a high-noise PML problem on which the existing approaches can not be applicable. We will demonstrate the performance difference in the experimental part."
        },
        {
            "heading": "2.3 Multi-Class Complementary Label Learning",
            "text": "Currently, CLL problem is only considered in multi-class learning, whose goal is to predict a single relevant label per instance precisely from complementary labeled data. Previous approaches can be roughly grouped into two categories: (1) modeling the generative relationship between the complementary label and the relevant label [6], [7], [8], [12], [39]; (2) modeling the probability of complementary labels from the learned discriminative classifier directly [9], [10], [11].\nThe first multi-class CLL method belongs to category one. It models the generative relationship between complementary labels and relevant labels, and uses a such generative process to rewrite one-versus-all and pairwise comparison loss functions to derive an unbiased risk estimator [6]. Ishida et al. [7] realize that the method of [6] is restricted to loss functions and propose a new method which can use arbitrary losses and models. A typical way to make use of the modeled generative process is through a transition matrix, which summarizes the probabilities of a label being complementary labels when relevant labels are given. Then, approaches apply a transition matrix to recover relevant labels from complementary labels [7], [8], [39]. Compared with [6], [7], transition matrix-based methods can map more complex generative relationship rather than uniform one only. Therefore, we tend to design a transition matrix-based method to solve ML-CLL problem with a different estimating way.\nDiffer from category one, approaches residing in category two directly model the probabilities of complementary labels from the learned classifier without the generative relationship [9], [10], [11]. Chou et al. propose a surrogate complementary loss framework based on complementary labels providing negative feedback during the training process [9]. Although its losses fail to derive an unbiased risk estimator, it achieves good performance on the multi-class CLL. In light of the property of the complementary label that the predictive probability of the complementary label is expected to approach zero, [10] and [11] propose a discriminative solution by directly modeling the probabilities of complementary labels from learned classifier to avoid the generative assumption. Due to that multi-class CLL approaches are designed for a single relevant label case, which are not suitable for the ML-CLL case that an instance is associated with multiple labels simultaneously. We will demonstrate that in the experimental part."
        },
        {
            "heading": "3 PROBLEM SETUP",
            "text": "In MLL, let X be the feature space and Y = {l1, l2, . . . , lK} be the finite label space withK possible class labels (K > 2).\nA multi-label instance x \u2208 X is equipped with a set of relevant labels Y \u2286 Y . (x, Y ) is independently sampled from an unknown joint probability distribution p(x, Y ). Here we exclude the special cases of Y = \u2205 nor Y to ensure relevant labels and complementary labels both exist. For convenience, we use a binary vector y = [y1, y2, . . . , yK ] \u2208 {0, 1}K to denote Y , where yk = 1 indicates that lk \u2208 Y is relevant to x and 0 otherwise. Suppose D = {(xi,yi)}ni=1\ni.i.d.\u223c p(x, Y ) is the training set with n instances. The goal of MLL is to learn a multi-label classifier h : X \u2192 2Y , which can predict a set of relevant labels for any unseen instance. Instead of learning h directly, most MLL methods tend to learn a realvalued decision function f : X \u2192 RK via minimizing the expected risk\nRL(f) = Ep(x,Y )[L(f(x),y)], (1)\nwhere L is a proper MLL loss function [30], such as BCE loss. f(x) is usually interpreted as a probability vector: fk(x) is the k-th entry of f(x) and predicts the confidence score that label lk is relevant to x, i.e., if properly normalized then p(yk = 1|x). Due to that p(x, Y ) is unknown, the expected risk is usually approximated by the empirical risk R\u0302L(f) = 1n \u2211n i=1 L(f(xi),yi). If denoting the optimal classifier learned from the expected risk as f\u2217, i.e., f\u2217 = argminf RL(f), then f\u0302\n\u2217 denotes the optimal classifier learned by minimizing the empirical risk, i.e., f\u0302\u2217 = argminf R\u0302L(f).\nIn ML-CLL studied in this paper, each training instance is equipped with a single complementary label. The complementary labeled instance (x, y\u0304) \u2208 (X ,Y) is drawn from an unknown joint probability distribution p(x, y\u0304), where y\u0304 \u2208 Y \\Y is a complementary label of x. y\u0304 can be presented as a K-dimensional vector y\u0304 = [y\u03041, y\u03042, . . . , y\u0304K ]. If label lj is selected as the complementary label to x (y\u0304 = lj), then y\u0304j is one and all other elements are zero in y\u0304. We utilize Y\u0302 = Y \\ y\u0304 to denote the candidate label set of x. Let a K-dimension vector y\u0302 = [y\u03021, y\u03022, . . . , y\u0302K ] to be the corresponding vector representation of subset Y\u0302 , where all elements are one except that the one corresponding to the complementary label is set to be zero (y\u0302 = 1\u2212 y\u0304).\nLet D\u0304 = {(xi, y\u0304i)}ni=1 i.i.d.\u223c p(x, y\u0304) be the ML-CLL training set with n instances. The expected risk of multilabeled CLL is defined over p(x, y\u0304):\nRL\u0304(f) = Ep(x,y\u0304)[L\u0304(f(x), y\u0304)], (2)\nwhere L\u0304 denotes a ML-CLL loss, which will be proposed later this paper. Similarly, the corresponding empirical risk is described as R\u0302L\u0304(f) = 1 n \u2211n i=1 L\u0304(f(xi), y\u0304i)."
        },
        {
            "heading": "4 THE PROPOSED APPROACH",
            "text": "In this section, we first introduce the definition of the transition matrix in MLL and analyze why the estimated transition matrix using multi-class techniques is unsuitable for ML-CLL. Then, we describe an advanced two-step way to estimate the transition matrix in the MLL case. Finally, we prove our approach is classifier-consistent with a mild assumption."
        },
        {
            "heading": "4.1 Transition Matrix for ML-CLL",
            "text": "In ML-CLL, we start by introducing a transition matrix T\u0303 that summarizes the probabilities for a complementary label given a set of relevant labels. More specifically, the transition matrix T\u0303 is defined as T\u0303kj = p(y\u0304j = 1|Y = Ck) where Ck \u2208 Y \u2032 = {2Y \u2212 \u2205 \u2212 Y} (k \u2208 [2K \u2212 2]) is the k-th label subset. If lj \u2208 Ck, then T\u0303kj = 0 because the label lj has no chance to be selected as the complementary label. In this paper, we employ the same class-dependent assumption as the multi-class CLL approach [8]: p(y\u0304|Y,x) = p(y\u0304|Y ) as y\u0304 and x are conditionally independent given Y . Then we can obtain the following equation:\np(y\u0304j = 1|x) = \u2211\nC\u2208Y\u2032,lj /\u2208C\np(y\u0304j = 1|Y = C)p(Y = C|x),\n(3)\nwhere we assume the label lj is a complementary label of x. Then, according to Eq.(3), p(y\u0304|x) can be approximated by p(Y |x) when the transition matrix T\u0303 is known. If considering all possible label subsets of Y \u2032 as C , we have T\u0303 \u2208 R(2K\u22122)\u00d7K , i.e., the size of T\u0303 depends on the size of the power set of Y \u2032. Practically, the power set of Y \u2032 would be computationally prohibitive and even impossible to store, since 2K\u22122 is an extremely large number when the number of possible labels K is large. To solve this combinatorial explosion problem, we explore a more practical way to use an alternative lower-dimensional transition matrix to replace the higher-dimensional one. We start investigating the feasibility of the alternative lower-dimensional matrix from Theorem 1.\nTheorem 1. Given an instance x, suppose Y is the relevant label set and the label lj is the complementary label which is randomly selected. Then the following equality holds:\np(y\u0304j = 1|x) = \u2211\nC\u2208Y\u2032,lj /\u2208C\np(y\u0304j = 1|Y = C)p(Y = C|x)\n\u2265 K\u2211\nk=1,k 6=j p(y\u0304j = 1|yk = 1)p(yk = 1|x).\nThe second inequality holds because of addition rule of probability. The detailed proof is in Appendix A. Theorem 1 shows that using T to approximate p(y\u0304|x) is a lower bound of using T\u0303 to approximate p(y\u0304|x). Observed by Eq.(3), we find that our main goal transforms from precisely predicting the relevant label set Y of x to precisely predicting its complementary label y\u0304 via the transition matrix T\u0303. This means that we need to maximize the predictive probability of the complementary label of x, i.e., maximizing p(y\u0304|x). From this point of view, Theorem 1 theoretically shows the feasibility of using a low-dimension transition matrix to replace the high-dimension T\u0303, because we optimize by maximizing the lower bound of Eq.(3). Let T \u2208 [0, 1]K\u00d7K denote the lower-dimensional transition matrix, where the (k, j)-th element of T is Tkj = p(y\u0304j = 1|yk = 1), and Tkj = 0 when k = j. Thus, we adopt the K \u00d7 K matrix T as the transition matrix in the following of the paper to avoid the pain in computation and storage brought up by the (2K \u2212 2)\u00d7K matrix T\u0303."
        },
        {
            "heading": "4.2 Distortion in Estimating the Transition Matrix",
            "text": "Before exploring how the transition matrix estimated by multi-class CLL is distorted from that of ML-CLL, we first introduce the transition matrix estimated by multi-class CLL techniques. Suppose Q \u2208 [0, 1]K\u00d7K be the transition matrix estimated in multi-class CLL. Recalling the approach [8], it estimates the transition matrix under a special assumption: for each label lk, existing an anchor set Sx|lk \u2282 X such that p(yk = 1|x) = 1 and p(yk\u2032 = 1|x) = 0 (lk\u2032 \u2208 Y \\ {lk}). With this assumption and regardless of label correlations, the estimation of Qkj is p(y\u0304j = 1|yk = 1) = p(y\u0304j = 1|x) iff x is sampled from Sx|lk , where Qkj is the k-th row and j-th column element of Q.\nTo measure the distortion between T calculated in MLCLL and the estimated Q, we define their difference on the complementary label lj of x as follows\n`j = K\u2211 k=1 |Tkj \u2212Qkj |. (4)\nThe larger value of \u2211K j=1 `j indicates that T deviates further from Q. As we know, label correlations and co-occurred multiple labels are key properties of MLL. Due to that the correlations among labels are intricate, directly calculating T from all label correlations will bring high computational cost. For convenience, we give a simple case of MLL including label correlations \u2013 at most two labels can co-occur for an instance, and the rest of labels are mutually exclusive \u2013 to facilitate us calculating T from label correlations and explore the distortion of T and Q. We start to study the above contents from the definition of mutually exclusive.\nDefinition 2. For any x \u2208 X , only a label is relevant to x, i.e. |Y | = 1, which labels are mutually exclusive.\nUnder the simple case in MLL, in Theorem 3, we state how to estimate T directly from label correlations, and the distortion of T and Q.\nTheorem 3. Under a MLL scenario: suppose the labels lz1 , lz2 \u2208 Y (z1, z2 \u2208 [K], z1 6= z2) are dependent, and the labels belonging to Y \\ {lz1 , lz2} are mutually exclusive. For any x, its label set Y \u2286 {lz1 , lz2} and Y 6= \u2205. Let the label lj (j \u2208 [K], j 6= z1, z2) be the complementary label of x \u2208 X . Tz1j and Tz2j calculated from label correlations satisfy\nTz1j = p(y\u0304j = 1|x)\np(yz2 = 1|y\u0304j = 1, yz1 = 1,x)p(yz1 = 1|x) ,\nTz2j = p(y\u0304j = 1|x)\np(yz1 = 1|y\u0304j = 1, yz2 = 1,x)p(yz2 = 1|x) ,\nwhere [K] denotes the integer set {1, 2, . . . ,K}. The difference of T and Q on the complementary label lj is\n`j \u2265 2( 1\n\u03be2 \u2212 1)p(y\u0304j = 1|x),\nwhere \u03be = max{p(yz2 = 1|y\u0304j = 1, yz1 = 1,x), p(yz1 = 1|y\u0304j = 1, yz2 = 1,x)}.\nThe proof is provided in Appendix B. From Theorem 3, we can see that calculating the transition matrix from label correlations is more complex than estimating one without label correlations, and the relevant label sets of instances\nneed to be known. Moreover, Theorem 3 shows that there is a distortion between T and Q, which widely exists in multilabeled cases since each multi-label instance is relevant to multiple labels. The above learning scenario only considers the pairwise label correlations, while there exists a more complex relationship among labels. Similarly, under a realizable computational cost, we construct another simple MLL scenario with more complex label relationships to explore factors that affect `j in Corollary 4.\nCorollary 4. Under a MLL scenario: there are m (m \u2265 2) labels lz1 , lz2 , . . . , lzm \u2208 Y (z1, . . . , zm \u2208 [K]) that are dependent, while the labels belong to Y \\ {lz1 , lz2 , . . . , lzm} are mutually exclusive. For any x \u2208 X , its relevant set Y \u2286 {lz1 , lz2 . . . , lzm} and Y 6= \u2205. Suppose the label lj is the complementary label of x. The difference `j between T and Q has\n`j \u2265 m( 1\n\u03bem \u2212 1)p(y\u0304j = 1|x),\nwhere \u03be = max{p(yzm = 1|y\u0304j = 1, yz1 = 1, . . . , yzm\u22121 = 1,x), p(yzm\u22121 = 1|y\u0304j = 1, yz1 = 1, . . . , yzm\u22122 = 1, yzm = 1,x), . . . , p(yz1 = 1|y\u0304j = 1, yz2 = 1, . . . , yzm = 1,x)} (\u03be \u2208 (0, 1]).\nThe proof is shown in Appendix C. According to Corollary 4, when label correlations are more complex, the distortion of the transition matrix estimated by the multi-class CLL approach is more serious as m increases. Meanwhile, it demonstrates that the ML-CLL problem cannot be solved by current techniques in multi-class CLL."
        },
        {
            "heading": "4.3 Estimation T with Label Correlations",
            "text": "As discussed above, calculating the transition matrix T from label correlations needs instances whose relevant label sets are known. Moreover, calculating T is more and more difficult as relationships among labels become more complex by observing the results of T in Theorem 3 and Corollary 4. Due to that multi-labeled data are unavailable for our setting, we propose a two-step method to estimate T from candidate labels, and it can reduce the complexities in calculating T from label correlations. This two-step method includes: (1) computing an initial transition matrix S \u2208 [0, 1]K\u00d7K from candidate labels by decomposing the multi-label problem into a series of binary classification problem; (2) obtaining the final estimation of T by using label correlations to correct S.\nComputing an initial transition matrix S. Let Skj = p(y\u0304j = 1|y\u0302k = 1) be an initial transition probability, which is a (k, j)-th element of S. We caulculate S from candidate labels of instances. Multiplication theorem of probability 1 is applied to calculate Skj and ensure that the following equation holds:\nSkj = p(y\u0304 j = 1|y\u0302k = 1) (5) = p(y\u0304j = 1|y\u0302k = 1) \u222b p(x|y\u0304j = 1, y\u0302k = 1)dx\n= \u222b p(y\u0304j = 1|y\u0302k = 1,x)p(x|y\u0302k = 1)dx\n1. p(x, y\u0304j = 1, y\u0302k = 1) = p(y\u0304j = 1|y\u0302k = 1,x)p(x|y\u0302k = 1)p(y\u0302k = 1) = p(y\u0304j = 1|y\u0302k = 1)p(x|y\u0304j = 1, y\u0302k = 1)p(y\u0302k = 1)\u21d2 p(y\u0304j = 1|y\u0302k = 1,x)p(x|y\u0302k = 1) = p(y\u0304j = 1|y\u0302k = 1)p(x|y\u0304j = 1, y\u0302k = 1)\n= Ep(x|y\u0302k=1)[p(y\u0304j = 1|y\u0302k = 1,x)],\nwhere j, k \u2208 [K] and j 6= k. In practice, Ep(x|y\u0302k=1)[p(y\u0304j = 1|y\u0302k = 1,x)] can be approximated by the expectation of p(y\u0304j = 1|y\u0302k = 1,x) over the conditional distribution p(x|y\u0302k = 1). Assuming y\u0304 and Y\u0302 are conditionally independent given x, so p(y\u0304j = 1|y\u0302k = 1,x) = p(y\u0304j = 1|x). Intuitively, p(y\u0304j = 1|x) can be approximated by the classifier learned from D\u0304 to predict the probability of complementary labels. Let Ak denote the subset of x in D\u0304 with y\u0302k = 1, which satisfies the conditional distribution p(x|y\u0302k = 1). Thus, Skj can be estimated by\nSkj = 1 |Ak| \u2211\nx\u2208Ak\np(y\u0304j = 1|y\u0302k = 1,x) (6)\n= 1 |Ak| \u2211\nx\u2208Ak\np(y\u0304j = 1|x).\nEstimating T with label correlations. The calculating procedure of S lacks exactly supervised data. Observed by the transition probabilities of T calculated from label correlations in subsection 4.2, we can find that they are affected by label correlations. Moreover, a label that is lowco-occurred to the relevant labels could be preferentially selected as the complementary label from the view of label correlations. For example, considering water as the relevant label; in this case, desert (low-co-occurred label) will have a larger chance to be selected as the complementary label compared to fish (high-co-occurred label). Motivated by these findings, we use label correlations to correct the initial matrix S to estimate T by enforcing the addition of relationships among labels.\nSuppose C \u2208 [0, 1]K\u00d7K be a label correlation matrix, where the element Ckj represents the correlation between labels lk and lj . The value of Ckj is larger when the correlation of labels lk and lj is stronger. Following [35], [40], we adopt the co-occurrence rate of two candidate labels as their correlations. Finally, the transition matrix T can be estimated by T\u0302 = SCT , where T\u0302kj = 0 if k = j, and normalizing T by row.\nFig. 2 is an example of refining procedure. As can be seen from the Fig. 2, though the estimated initial probability of p(y\u03042 = 1|y\u03021 = 1) is higher than p(y\u03043 = 1|y\u03021 = 1) in S, the value of p(y\u03042 = 1|y1 = 1) is lower than p(y\u03043 = 1|y1 = 1) in T\u0302. This is because the labels l1 and l2 have a strong correlation as shown in C, so the label l2 has a lower chance to be selected as the complementary label for the label l1. The corrected initial transition matrix S agrees with our expectation on the low-co-occurred labels that tend to be selected as complementary labels preferentially. In practice, the estimation of T depends on p(y\u0304|x), where the classifier should perfectly model the probability of complementary\nlabels. When data equipped with complementary labels is sufficiently, the perfect model is capable of modeling p(y\u0304|x)."
        },
        {
            "heading": "4.4 A Classifier-Consistent Approach",
            "text": "According to the transition matrix T, we can derive the probability of complementary labels from multi-label classifier. Let f\u0304(x) \u2208 RK be a complementary label classifier, which is defined as\nf\u0304(x) = TTf(x), (7)\nwhere f\u0304(x) is applied to approximate p(y\u0304|x), f\u0304 j(x) refers to the j-th element of f\u0304(x). ML-CLL problems aim to recover a set of relevant labels per instance from a complementary label. Since training instances are associated with complementary labels, the common loss functions of MLL are unsuitable for ML-CLL. Therefore, we define a complementary loss function L\u0304 as\nL\u0304(f(x), y\u0304) = L(f\u0304(x), y\u0304) = L(TTf(x), y\u0304). (8)\nDenote by f\u2217CL the minimizer of RL\u0304(f), the minimizer f\u0302\u2217CL of R\u0302L\u0304(f) is used to approximated f \u2217 CL. Recalling the definition of classifier-consistent, if a classifier learned by an approach finally converges to the optimal classifier f\u2217 learned in MLL as the number of instances increases, then this approach is classifier-consistent [41], [42], [43]. We derive our proposal is classifier-consistent based on a mild assumption:\nAssumption 5. Suppose the transition matrix T is invertible and can perfectly recover the relationship between relevant labels of x and its complementary label. Then, we have y\u0304 = TTy.\nWith Assumption 5, our approach trained on L\u0304 can be inferred to be classifier-consistent, which is stated in Theorem 6. Naturally, Theorem 6 guarantees that the optimal classifier learned from complementary labeled data converges to the optimal one learned from fully supervised MLL.\nTheorem 6. With Assumption 5, suppose the transition matrix T is invertible, then the ML-CLL optimal classifier f\u2217CL converges to the MLL optimal classifier f\u2217, i.e., f\u2217CL = f \u2217.\nThe proof is represented in Appendix D. Thanks to BCE loss is a popular loss function in MLL, we adopt BCE loss as the base in this paper, then L\u0304 is expressed as\nL\u0304(f(x), y\u0304) =\u2212 y\u0304log(TTf(x))\u2212 (1\u2212 y\u0304)log(1\u2212TTf(x))), (9)\nwhere 1 denotes a K-dimensional vector with 1 for all elements."
        },
        {
            "heading": "5 REGULARIZATION-BASED ENHANCEMENT",
            "text": "In this section, an MSE-based regularization of our approach is described. And we attempt to combine a small amount of relevant labels to explore more possibilities of our proposal."
        },
        {
            "heading": "5.1 An MSE-Based Regularization",
            "text": "Previous works indicate that CE loss always makes the model focus on hard instances that are difficult to be classified precisely, while MSE loss and Mean Absolute Error (MAE) loss are less sensitive to hard instances since they\nAlgorithm 1: MLCL Algorithm Input: D\u0304: the complementary-label training set {(xi, y\u0304i)}ni=1; E: the number of epochs; A: an external stochastic optimization algorithm; Output: \u03b8: model parameter for f(x; \u03b8); 1 if T is unknown then 2 Train a classifier f\u0304(x) with the softmax output layer and Cross-Entropy loss on D\u0304; 3 Fill S \u2208 [0, 1]K\u00d7K with zeros; 4 for k = 1 to K do 5 num = 0; 6 for (xi, y\u0304i) \u2208 D\u0304 such that y\u0304ki = 0 do 7 num += 1; 8 Sk\u00b7+ = f\u0304(xi); //add f\u0304(xi) to k-th row of S 9 end\n10 Sk\u00b7/ = num; 11 end 12 T\u0302 = SCT ; 13 end 14 for t = 1 to E do 15 Let L be the risk, L = 1n \u2211n i=1 L\u0304(f(xi), y\u0304i) =\n1 n \u2211n i=1(L(T\u0302 Tf(xi), y\u0304i) + \u2225\u2225\u2225y\u0304i \u2212 T\u0302Tf(xi)\u2225\u2225\u22252\nF );\n16 Set gradient \u2212\u2207\u03b8L; 17 Update \u03b8 by A; 18 end\ntreat per instance coequally [13], [44]. As this property, the convergence rate of CE loss is superior to MSE loss and MAE loss, whereas this property makes CE loss more prone to the overfitting problem than MSE loss and MAE loss when noisy labels present at training data [13], [44]. Actually, an excellent approach can converge quickly during the training process, and shows good generalization ability and robustness for unseen instances [11].\nObviously, BCE loss has a similar property to CE loss, which results in an excellent convergence rate of approaches. Meanwhile, approaches based on BCE loss are easy to suffer from the overfitting problem when using noisy labeled data to learn. In fact, ML-CLL is a problem setting with dense noisy labels, BCE loss may cause the overfitting problem of a model in ML-CLL. To cope with this problem, we introduce an MSE-based regularizer based on MSE loss (i.e. `2-norm regularization) to balance the robust and convergence requirement of the proposed approach. Hence, the MSE-based regularizer is defined as:\nL\u0304mse(f(x), y\u0304) = \u2225\u2225\u2225y\u0304 \u2212TTf(x)\u2225\u2225\u22252\nF . (10)\nFinally, we combine the complementary loss and the MSE-based regularizer term, which leads to our target loss:\nL\u0304(f(x), y\u0304) = L\u0304(f(x), y\u0304) + \u03b2L\u0304mse(f(x), y\u0304), (11)\nwhere \u03b2 is the trade-off parameter and set as 1 (the selection shown in Section 6). The all procedure of the proposed approach (called MLCL) is shown in Algorithm 1."
        },
        {
            "heading": "5.2 Incorporation of Relevant Labels",
            "text": "In many practical situations, we can use complementary labels and relevant labels to learn more accurate classifiers, which is highly practical implementation. To this end, motivated by [6], [45], let us design a reasonable combination of the loss derived from complementary labeled data and relevant labeled data:\nL\u0303(f(x), y\u0304, y\u0303) = L\u0304(f(x), y\u0304) + \u2016y\u0303 \u2212 f(x)\u20162F , (12)\nwhere y\u0303 = [y\u03031, . . . , y\u03031] \u2208 {0, 1}K denotes a binary vector of relevant labels Y\u0303 of x, in which y\u03031 = 1 when the label lk \u2208 Y\u0303 . To provide more practicability, we do not restrict given relevant labels Y\u0303 to must be equal to the set of relevant labels Y , which means Y\u0303 \u2286 Y and Y\u0303 6= \u2205.\nAs explained in the instruction, we can naturally collect data associated with complementary labels and relevant labels via crowdsourcing [14]. Our loss function Eq.(12) can leverage both kinds of labeled data to learn better classifiers. We will experimentally show the usefulness of this combination method in Section 6."
        },
        {
            "heading": "6 EXPERIMENTS",
            "text": "In this section, we will evaluate the effectiveness of MLCL, where five common MLL criteria, including ranking loss, hamming loss, one error, coverage and average precision, are employed in this paper. The values of first four criteria are smaller, the performance of approach is better. While the value of average precision is greater, the better the performance. The label set of x is predicted by Y = {lk|fk(x) > 0.5, 1 \u2264 k \u2264 K}. All experiments use PyTorch [46] and NVIDIA TESLA K80 GPU to implement. The code will be released after this paper has been accepted."
        },
        {
            "heading": "6.1 Experimental Settings",
            "text": "Datasets. We use eight widely-used MLL datasets, namely corel5k, corel16k, delicious, eurlex dc, eurlex sm, yeast, bookmarks and scene, to our experiments2. Following [35], [36], we adopt the same pre-processing to deal with the datasets. More specifically, rare class labels are filtered out for datasets with more than 15 class labels, whose class labels are kept under 15. Accordingly, instances that are relevant with removed class labels are filtered out as well. Detailed characteristics of these datasets are shown in Table 1.\nBase models. The linear model is used as the base model. Baselines. Two typical MLL approaches, ML-KNN [21] and LIFT [47], are utilized as baselines, which deal with\n2. Publicly available at http://mulan.sourceforge.net/datasets.\nML-CLL via regarding all possible labels in the candidate label set as relevant labels for a training instance. Similarly, three recent PML approaches are employed as comparing approaches, including PML-lc [35], fpml [38] and PML-LRS [37], which learn from training instances associated with candidate labels. In addition, we employ a multi-class CLL approach, called L-UW [10], as a baseline, which uses BEC loss and sigmoid output layer instead of CE loss and softmax output layer respectively to make L-UW suit for multilabeled data."
        },
        {
            "heading": "6.2 Comparison on Uniform Complementary Labels",
            "text": "Setup. Weight-decay is set as 1e \u2212 4 and learning rate is selected from {1e \u2212 1, 1e \u2212 2, 1e \u2212 3} for all data sets. We employ Adam [48] optimization method, and set the number of batch-size and epoch as 256 and 200 respectively. L-UW applies the same model and hyper-parameters as ours. Here, we estimate T with a linear model. We use Ten-fold cross-validation to evaluate experiments, where training data is associated with complementary labels that are generated by randomly selecting one of possible labels excepting relevant labels (uniform complementary labels), and test data is equipped with the set of relevant labels. The mean metrics value and standard deviation (std) will be reported as final experimental results for all approaches.\nResults. Table 2 is utilized to report experimental results of various approaches on eight data sets equipped with uniform complementary labels. \u2191 / \u2193 indicates the larger/smaller the value, the better the performance.\nAccording to reported results in Table 2, we can observe that results of MLCL are superior or comparable performance against baselines out of different data sets on five criteria. Our approach achieves the best performance in most cases. Specifically, the proposed approach outperforms LIFT on eight datasets across all metrics. This is because our approach is better at tackling the issue that training data is associated with relevant labels and irrelevant labels simultaneously than fully supervised MLL algorithms. Furthermore, experimental results of PML-lc and PML-LRS are inferior to ours in most cases, which demonstrate that PML approaches are indeed inferior to our approach in cases of dense noisy labels. Similarly, based on the results of L-UW shown in Table 2, we observe that our approach outperforms L-UW on almost all datasets and metrics other than ranking loss and coverage on the delicious dataset. This reflects that label correlations are important to solve MLCLL problems, which leads to the proposed approach taking label correlations into account surpasses L-UW that ignores label correlations."
        },
        {
            "heading": "6.3 Comparison on Biased Complementary Labels",
            "text": "Setup. To evaluate the effectiveness of our approach in different situations, we utilize training data with biased complementary labels that are generated via the co-occurrence rate of relevant labels. Specifically, we select a complementary label of an instance x from Y \\Y , and the selecting rule follows: the class label with a lower co-occurrence rate has a higher probability to be selected as a complementary label. We adopt training data with biased complementary labels to train the model, while test data is equipped with relevant\nlabel sets to evaluate the effectiveness of our approach. For other experimental settings, we apply same settings with Subsection 5.2.\nResults. The mean and std of results on test data are shown in Table 3. According to results shown in Table 3, we can summarize the following impressive observations: (1) MLCL achieves superior or comparable performance to LIFT, fpml, PML-lc, PML-LRS and L-UW on different data sets, which proves that the proposed approach can predict the set of proper labels for unseen instances from complementary labeled data; (2) Although MLCL fails to achieve the best result on the scene dataset, our approach\nis better than other baselines in the rest of datasets, which indicates that our approach can effectively deal with MLCLL problems than others. These observations demonstrate that the proposed method can both hold for the situation of data with uniform and biased complementary labels."
        },
        {
            "heading": "6.4 Additional Experiments",
            "text": "Ablation experiments. We then explore the effect of different learning components on MLCL performance. Table 4 summarizes results of MLCL without the different component, which trains on the data with uniform complementary labels. In Table 4, without C refers to MLCL directly use the\nestimated initial transition matrix S to train, and without L\u0304mse indicates that MLCL only utilizes Eq.(9) to optimaze.\nFrom results reported in Table 4, the performance of MLCL surpasses that without different components in most cases, which shows that two components, including using label correlations to correct and an MSE-based regularizer, are beneficial for our approach to improve the performance. Especially, estimating T based on label correlations pushes the proposed approach performance forward significantly compared with that without C on most cases. Similarly, an MSE-based regularizer brings significant benefits for our approach, which demonstrates that an MSE-based regular-\nizer balances the robustness and convergence rate of BCE loss. These indicate that using label correlations to estimate the transition matrix T and an MSE-based regularizer are effective strategies to alleviate ML-CLL problems.\nTrade-off parameter \u03b2. Table 5 reports the performance of MLCL with varying \u03b2 values that trade-off the complementary loss function L\u0304 and an MSE-based regularization L\u0304mse. Here, average precision is regarded as the criterion, and the training data is with uniform complementary labels. \u03b2 is selected from the candidate value list {0.1, 0.3, 0.5, 0.8, 1}. We can observe the best results of most datasets is achieved at \u03b2 = 1 and the performance drops when \u03b2 takes a smaller\nvalue. In general, a relatively large \u03b2 (\u03b2 \u2264 1) usually leads to better performance than a small value. Therefore, we set \u03b2 = 1 for MLCL."
        },
        {
            "heading": "6.5 Combination of Complementary Labels and Relevant Labels",
            "text": "Setup. Finally, we demonstrate the effectiveness of combining relevant labeled data and complementary labeled one. The training data is associated with uniform complementary labels and relevant labels simultaneously. More specifically, an instance x is associated with a complementary label y\u0304 and relevant labels Y\u0303 , where y\u0304 is uniformly selected and Y\u0303 is randomly selected from the relevant label set Y of x (i.e., Y\u0303 \u2286 Y ). Here, we set |Y\u0303 | = 1 that means each instance only associated with a complementary label and a relevant label. The other experimental settings are the same with Subsection 5.2.\nResults. We compare three methods: (1) the \u201cFully supervised\u201d method uses the linear model to train with the fully supervised data, which is fully supervised MLL; (2) the \u201cCL\u201d method refers to MLCL training with the uniform complementary-label data; (3) the combination (\u201cCL & RL\u201d) method adopts the linear model with the loss function Eq.(12) to train, where the training data is equipped with the combination of complementary labels and relevant labels.\nTable 6 reports the experimental results on five criteria. We can see that the performance of \u201cCL& RL\u201d method is much superior to \u201cCL\u201d method on all datasets over hamming loss, ranking loss, one error, coverage and average precision, such as \u201cCL& RL\u201d method outperforms \u201cCL\u201d method by a large margin over average precision (+0.436 on eurlex dc and +0.425 on eurlex sm). This demonstrates that the ML-CLL is easily applied to fully supervised MLL scenarios, MLL with missing labels [49], [50] or other MLL scenarios. Moreover, \u201cCL & RL\u201d method achieves comparable performance to \u201cFully supervised\u201d method, which illustrates that ML-CLL can get excellent results just via increasing a few additional information. This is useful for application in the real world, because ML-CLL can obtain good performance through less expensive labeled data."
        },
        {
            "heading": "7 CONCLUSION",
            "text": "In this paper, we theoretically analyze the reason causing why the estimated transition matrix in multi-class CLL is distorted in ML-CLL. To alleviate the pain in directly calculating the transition matrix from complex label correlations under multi-labeled data is unknown, we propose a twostep method to estimate the transition matrix T in MLCLL, which adopts label correlations to correct an initial transition matrix. Furthermore, we theoretically show that\nthe proposed approach is classifier-consistent. Additionally, due to MSE loss achieving a prominent robust, an MSEbased regularizer is introduced to alleviate the tendency of the fast convergent BCE loss overfitting to noises. Finally, we show that our proposed ML-CLL can be easily combined with relevant labels and the proposed method can achieve a comparable performance to fully supervised MLL through a few additional information."
        },
        {
            "heading": "APPENDIX A THE PROOF OF THEOREM 1",
            "text": "Theorem 1. Given an instance x, suppose Y is the relevant label set and the label lj is the complementary label which is randomly selected. Then the following equality holds:\np(y\u0304j = 1|x) = \u2211\nC\u2208Y\u2032,lj /\u2208C\np(y\u0304j = 1|Y = C)p(Y = C|x) \u2265 K\u2211\nk=1,k 6=j p(y\u0304j = 1|yk = 1)p(yk = 1|x).\nProof. Firstly, we should introduce addition rule of probability: p(AB) = p(A) + p(B) \u2212 p(A \u222a B), so we have p(AB) \u2265 p(A) + p(B). We start to prove the above inequlity. According to the assumption: p(y\u0304|Y ) = p(y\u0304|Y,x), we have\np(y\u0304j = 1|x) = \u2211\nC\u2208Y\u2032,lj /\u2208C\np(y\u0304j = 1|Y = C)p(Y = C|x)\n= \u2211\nC\u2208Y\u2032,lj /\u2208C\np(y\u0304j = 1|Y = C,x)p(Y = C|x)\n= \u2211\nC\u2208Y\u2032,lj /\u2208C\np(y\u0304j = 1, Y = C|x)\n= \u2211\nC\u2208Y\u2032,lj /\u2208C\np(Y = C|y\u0304j = 1,x)p(y\u0304j = 1|x).\nAccording to addition rule of probability, so we have\np(y\u0304j = 1|x) \u2265 \u2211\nC\u2208Y\u2032,lj /\u2208C  K\u2211 k=1,k 6=j,lk\u2208C p(yk = 1|y\u0304j = 1,x) + K\u2211 k=1,lk /\u2208C p(yk = 0|y\u0304j = 1,x)  p(y\u0304j = 1|x) \u2265\n\u2211 C\u2208Y\u2032,lj /\u2208C K\u2211 k=1,k 6=j,lk\u2208C p(yk = 1|y\u0304j = 1,x)p(y\u0304j = 1|x) \u2235 K\u2211 k=1,lk /\u2208C p(yk = 0|y\u0304j = 1,x) \u2265 0\n= \u2211\nC\u2208Y\u2032,lj /\u2208C K\u2211 k=1,k 6=j,lk\u2208C p(y\u0304j = 1|yk = 1,x)p(yk = 1|x)\n= \u2211\nC\u2208Y\u2032,lj /\u2208C\nK\u2211 k=1,k 6=j p(y\u0304j = 1|yk = 1,x)p(yk = 1|x) \u2235 p(yk = 1|x) = 0 if lk /\u2208 Y\n= K\u2211\nk=1,k 6=j \u2211 C\u2208Y\u2032,lj /\u2208C p(y\u0304j = 1|yk = 1,x)p(yk = 1|x)\n= K\u2211\nk=1,k 6=j (2K\u22121 \u2212 1)p(y\u0304j = 1|yk = 1,x)p(yk = 1|x)\n\u2265 K\u2211\nk=1,k 6=j p(y\u0304j = 1|yk = 1,x)p(yk = 1|x)\n= K\u2211\nk=1,k 6=j p(y\u0304j = 1|yk = 1)p(yk = 1|x)."
        },
        {
            "heading": "APPENDIX B THE PROOF OF THEOREM 3",
            "text": "Theorem 3. Under a MLL scenario: suppose the labels lz1 , lz2 \u2208 Y (z1, z2 \u2208 [K], z1 6= z2) are dependent, and the labels belonging to Y \\ {lz1 , lz2} are mutually exclusive. For any x \u2208 X , its label set Y \u2286 {lz1 , lz2} and Y 6= \u2205. Let the label lj (j \u2208 [K], j 6= z1, z2) be the complementary label of x. Tz1j and Tz2j calculated from label correlations satisfy\nTz1j = p(y\u0304j = 1|x)\np(yz2 = 1|y\u0304j = 1, yz1 = 1,x)p(yz1 = 1|x) ,\nTz2j = p(y\u0304j = 1|x)\np(yz1 = 1|y\u0304j = 1, yz2 = 1,x)p(yz2 = 1|x) ,\nwhere [K] denotes the integer set {1, 2, . . . ,K}. The difference of T and Q on the complementary label lj is\n`j \u2265 2( 1\n\u03be2 \u2212 1)p(y\u0304j = 1|x),\nwhere \u03be = max{p(yz2 = 1|y\u0304j = 1, yz1 = 1,x), p(yz1 = 1|y\u0304j = 1, yz2 = 1,x)}.\nProof. We start calculating the difference `j from estimating the transition probabilities Tz1j and Tz1j . According to Definition 2 and the description of Theorem 3, we have p(y\u0304j = 1|x) = K\u2211\nk=1,k 6=j,z1,z2\np(y\u0304j = 1|yk = 1,x)p(yk = 1|x) + p(y\u0304j = 1|yz1 = 1, yz2 = 1,x)p(yz1 = 1, yz2 = 1|x)\n+ p(y\u0304j = 1|yz1 = 1, yz2 = 0,x)p(yz1 = 1, yz2 = 0|x) + p(y\u0304j = 1|yz1 = 0, yz2 = 1,x)p(yz1 = 0, yz2 = 1|x) + p(y\u0304j = 1|yz1 = 0, yz2 = 0,x)p(yz1 = 0, yz2 = 0|x) = K\u2211\nk=1,k 6=j,z1,z2\np(y\u0304j = 1|yk = 1,x)p(yk = 1|x) + p(yz2 = 1|y\u0304j = 1, yz1 = 1,x)p(y\u0304j = 1|yz1 = 1,x)p(yz1 = 1|x)\n+ p(yz1 = 1|y\u0304j = 1, yz2 = 0,x)p(y\u0304j = 1|yz2 = 0,x)p(yz2 = 0|x) + p(yz2 = 1|y\u0304j = 1, yz1 = 0,x)p(y\u0304j = 1|yz1 = 0,x)p(yz1 = 0|x) + p(yz2 = 0|y\u0304j = 1, yz1 = 0,x)p(y\u0304j = 1|yz1 = 0,x)p(yz1 = 0|x).\nBased on the assumption of that y\u0304 and x are conditionally independent given Y , then we can have\np(y\u0304j = 1|x) = K\u2211\nk=1,k 6=j,z1,z2\np(y\u0304j = 1|yk = 1)p(yk = 1|x) + p(yz2 = 1|y\u0304j = 1, yz1 = 1,x)p(y\u0304j = 1|yz1 = 1)p(yz1 = 1|x)\n+ p(yz1 = 1|y\u0304j = 1, yz2 = 0,x)p(y\u0304j = 1|yz2 = 0)p(yz2 = 0|x) + p(yz2 = 1|y\u0304j = 1, yz1 = 0,x)p(y\u0304j = 1|yz1 = 0)p(yz1 = 0|x) + p(yz2 = 0|y\u0304j = 1, yz1 = 0,x)p(y\u0304j = 1|yz1 = 0)p(yz1 = 0|x).\nSince p(y\u0304j = 1|yz1 = 0) and p(y\u0304j = 1|yz2 = 0) do not hold according to the definition of the transition matrix, and then we can obtain\np(y\u0304j = 1|x) = K\u2211\nk=1,k 6=j,z1,z2\np(y\u0304j = 1|yk = 1)p(yk = 1|x) + p(yz2 = 1|y\u0304j = 1, yz1 = 1,x)p(y\u0304j = 1|yz1 = 1)p(yz1 = 1|x)\n= p(yz2 = 1|y\u0304j = 1, yz1 = 1,x)p(y\u0304j = 1|yz1 = 1)p(yz1 = 1|x) \u2235 p(yk = 1|x) = 0 if lk /\u2208 Y\n\u21d2 Tz1j = p(y\u0304j = 1|yz1 = 1) = p(y\u0304j = 1|x)\np(yz2 = 1|y\u0304j = 1, yz1 = 1,x)p(yz1 = 1|x) .\nSimilarly, we can get\nTz2j = p(y\u0304 j = 1|yz2 = 1) = p(y\u0304 j = 1|x) p(yz1 = 1|y\u0304j = 1, yz2 = 1,x)p(yz2 = 1|x) .\nNext, we calculate the difference `j . The rest elements of T\u00b7j are same as that estimated by multi-class CLL. According the definition of `j , we have\n`j = K\u2211 k=1 |Tkj \u2212Qkj |\n= \u2223\u2223Tz1j + Tz2j \u2212 2p(y\u0304j = 1|x)\u2223\u2223\n= \u2223\u2223\u2223\u2223 p(y\u0304j = 1|x)p(yz2 = 1|y\u0304j = 1, yz1 = 1,x)p(yz1 = 1|x) + p(y\u0304j = 1|x)p(yz1 = 1|y\u0304j = 1, yz2 = 1,x)p(yz2 = 1|x) \u2212 2p(y\u0304j = 1|x) \u2223\u2223\u2223\u2223\n\u2265 \u2223\u2223\u2223\u22232( 1\u03be2 \u2212 1)p(y\u0304j = 1|x) \u2223\u2223\u2223\u2223 = 2( 1\n\u03be2 \u2212 1)p(y\u0304j = 1|x). \u2235 1 \u03be2 \u2265 1\nBecause 0 \u2264 p(yz1 = 1|x) \u2264 p(yz1 = 1|y\u0304j = 1, yz2 = 1,x) \u2264 1 and 0 \u2264 p(yz2 = 1|x) \u2264 p(yz2 = 1|y\u0304j = 1, yz1 = 1,x) \u2264 1, \u03be is defined as \u03be = max{p(yz2 = 1|y\u0304j = 1, yz1 = 1,x), p(yz1 = 1|y\u0304j = 1, yz2 = 1,x)}, the above inequation holds."
        },
        {
            "heading": "APPENDIX C THE PROOF OF COROLLARY 4",
            "text": "Corollary 4. Under a MLL scenario: there are m (m \u2265 2) labels lz1 , lz2 , . . . , lzm \u2208 Y (z1, . . . , zm \u2208 [K]) that are dependent, while the labels belong to Y \\ {lz1 , lz2 , . . . , lzm} are mutually exclusive. For any x \u2208 X , its relevant set Y \u2286 {lz1 , lz2 . . . , lzm} and Y 6= \u2205. Suppose the label lj is the complementary label of x. The difference `j between T and Q has\n`j \u2265 m( 1\n\u03bem \u2212 1)p(y\u0304j = 1|x),\nwhere \u03be = max{p(yzm = 1|y\u0304j = 1, yz1 = 1, . . . , yzm\u22121 = 1,x), p(yzm\u22121 = 1|y\u0304j = 1, yz1 = 1, . . . , yzm\u22122 = 1, yzm = 1,x), . . . , p(yz1 = 1|y\u0304j = 1, yz2 = 1, . . . , yzm = 1,x)} (\u03be \u2208 (0, 1]).\nProof. Here, we apply induction to get the difference as m increases. We start by computing the difference in the case of m = 3. Suppose class labels lz1 , lz2 , lz3 \u2208 Y are dependent, while the rest of labels in the label space are mutually exclusive. x is associated with Y \u2286 {lz1 , lz2 , lz3} and Y 6= \u2205. Then we calculate transition probabilities in T from label correlations according to Theorem 3 as:\np(y\u0304j = 1|x) = K\u2211\nk=1,k 6=j,z1,z2,z3\np(y\u0304j = 1|yk = 1,x)p(yk = 1|x) + p(y\u0304j = 1, yz1 = 1, yz2 = 1, yz3 = 1|x)\n= p(y\u0304j = 1, yz1 = 1, yz2 = 1, yz3 = 1|x) = p(yz3 = 1|y\u0304j = 1, yz1 = 1, yz2 = 1,x)p(yz2 = 1|y\u0304j = 1, yz1 = 1,x)p(y\u0304j = 1|yz1 = 1,x)p(yz1 = 1|x) = p(yz3 = 1|y\u0304j = 1, yz1 = 1, yz2 = 1,x)p(yz2 = 1|y\u0304j = 1, yz1 = 1,x)p(y\u0304j = 1|yz1 = 1)p(yz1 = 1|x)\n\u21d2 Tz1j = p(y\u0304j = 1|yz1 = 1) = p(y\u0304j = 1|x)\np(yz3 = 1|y\u0304j = 1, yz1 = 1, yz2 = 1,x)p(yz2 = 1|y\u0304j = 1, yz1 = 1,x)p(yz1 = 1|x) .\nTz2j and Tz3j use the same way to estimate. Due to 0 \u2264 p(yz1 = 1|x) \u2264 p(yz1 = 1|y\u0304j = 1, yz2 = 1,x) \u2264 p(yz1 = 1|y\u0304j = 1, yz2 = 1, yz3 = 1,x) \u2264 1, let \u03be = max{p(yz3 = 1|y\u0304j = 1, yz1 = 1, yz2 = 1,x), p(yz2 = 1|y\u0304j = 1, yz1 = 1, yz3 = 1,x), p(yz1 = 1|y\u0304j = 1, yz2 = 1, yz3 = 1,x)}, we can obtain\nTz1j = p(y\u0304 j = 1|yz1 = 1) \u2265 1\n\u03be3 p(y\u0304j = 1|x).\nSimilarly, we can compute Tz2j ,Tz3j \u2265 1\u03be3 p(y\u0304 j = 1|x). Then the difference `j is\n`j = K\u2211 k=1 |Tkj \u2212Qkj |\n= \u2223\u2223Tz1j + Tz2j + Tz3j \u2212 3p(y\u0304j = 1|x)\u2223\u2223 \u2265 3( 1 \u03be3 \u2212 1)p(y\u0304j = 1|x).\nSimilarly, for any m (0 < m < K), suppose class labels lz1 , lz2 , . . . , lzm \u2208 Y are strongly dependent, while the rest of labels in the label space are mutually exclusive. x is associated with Y \u2286 {lz1 , lz2 , lz3} and Y 6= \u2205. Then we calculate transition probabilities from label correlations: p(y\u0304j = 1|x) = K\u2211\nk=1,k 6=j,z1,...,zm\np(y\u0304j = 1|yk = 1,x)p(yk = 1|x) + p(y\u0304j = 1, yz1 = 1, . . . , yzm = 1|x)\n= p(y\u0304j = 1, yz1 = 1, . . . , yzs = 1|x) = p(yz2 = 1|y\u0304j = 1, yz1 = 1,x)p(y\u0304j = 1|yz1 = 1)p(yz1 = 1|x)\u03a0mi=3p(yzi = 1|y\u0304j = 1, yz1 = 1, . . . , yzi\u22121 = 1,x)\n\u21d2 Tz1j = p(y\u0304j = 1|yz1 = 1) = p(y\u0304j = 1|x)\np(yz2 = 1|y\u0304j = 1, yz1 = 1,x)p(yz1 = 1|x)\u03a0mi=3p(yzi = 1|y\u0304j = 1, yz1 = 1, . . . , yzi\u22121 = 1,x) .\nAs discussed above, Tz1j \u2265 1\u03bem p(y\u0304 j = 1|x) since \u03be = max{p(yzm = 1|y\u0304j = 1, yz1 = 1, . . . , yzm\u22121 = 1,x), p(yzm\u22121 = 1|y\u0304j = 1, yz1 = 1, . . . , yzm\u22122 = 1, yzm = 1,x), . . . , p(yz1 = 1|y\u0304j = 1, yz2 = 1, . . . , yzm = 1,x)} (\u03be \u2208 (0, 1]). By the same calculation way, we can obtain Tz2j , . . . ,Tzmj \u2265 1\u03bem p(y\u0304\nj = 1|x). Based on induction, we can summarize the difference `j = \u2211K k=1 |Tkj \u2212Qkj | \u2265 m( 1\u03bem \u2212 1)p(y\u0304 j = 1|x)."
        },
        {
            "heading": "APPENDIX D THE PROOF OF THEOREM 6",
            "text": "Theorem 6. With Assumption 5, suppose the transition matrix T is invertible, then the ML-CLL optimal classifier f\u2217CL converges to the MLL optimal classifier f\u2217, i.e., f\u2217CL = f \u2217.\nProof. We prove f\u2217 is also the optimal classifier for ML-CLL via substituting f\u2217 into the ML-CLL risk:\nRL\u0304(f \u2217) = Ep(x,y\u0304)[L\u0304(f\u2217(x), y\u0304)]\n= \u222b \u2211 y\u0304\u2208Y L\u0304(f\u2217(x), y\u0304)p(x, y\u0304)dx\n= \u222b \u2211 y\u0304\u2208Y L(TTf\u2217(x), y\u0304) \u2211 Y \u2208Y p(Y |y\u0304,x)p(y\u0304,x)dx\n= \u222b \u2211 y\u0304\u2208Y \u2211 Y \u2208Y L(TTf\u2217(x), y\u0304)p(y\u0304|Y,x)p(Y,x)dx\n= \u222b \u2211 Y \u2208Y L(TTf\u2217(x), y\u0304)p(Y,x)dx\n= \u222b \u2211 Y \u2208Y L(TTf\u2217(x), TTy)p(Y,x)dx = R(TTf\u2217)\nAccording to the proof of [8], f\u2217CL = T Tf\u2217. So we find the optimal f\u2217 ensuring f\u2217CL = f \u2217 when the transition matrix T is invertible and Assumption 5 is satisfied."
        }
    ],
    "title": "Complementary to Multiple Labels: A Correlation-Aware Correction Approach",
    "year": 2023
}