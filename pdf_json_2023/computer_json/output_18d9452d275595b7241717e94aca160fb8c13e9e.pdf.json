{
    "abstractText": "Novel view synthesis aims to render unseen views given a set of calibrated images. In practical applications, the coverage, appearance or geometry of the scene may change over time, with new images continuously being captured. Efficiently incorporating such continuous change is an open challenge. Standard NeRF benchmarks only involve scene coverage expansion. To study other practical scene changes, we propose a new dataset, World Across Time (WAT), consisting of scenes that change in appearance and geometry over time. We also propose a simple yet effective method, CLNeRF, which introduces continual learning (CL) to Neural Radiance Fields (NeRFs). CLNeRF combines generative replay and the Instant Neural Graphics Primitives (NGP) architecture to effectively prevent catastrophic forgetting and efficiently update the model when new data arrives. We also add trainable appearance and geometry embeddings to NGP, allowing a single compact model to handle complex scene changes. Without the need to store historical images, CLNeRF trained sequentially over multiple scans of a changing scene performs on-par with the upper bound model trained on all scans at once. Compared to other CL baselines CLNeRF performs much better across standard benchmarks and WAT. The source code, a demo, and the WAT dataset are available at https: //github.com/IntelLabs/CLNeRF.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zhipeng Cai"
        },
        {
            "affiliations": [],
            "name": "Matthias M\u00fcller"
        }
    ],
    "id": "SP:17fb15f8ec9ed4500b6366c4c0a16674f2207af6",
    "references": [
        {
            "authors": [
                "Zhipeng Cai",
                "Ozan Sener",
                "Vladlen Koltun"
            ],
            "title": "Online continual learning with natural distribution shifts: An empirical study with visual data",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Arslan Chaudhry",
                "Marcus Rohrbach",
                "Mohamed Elhoseiny",
                "Thalaiyasingam Ajanthan",
                "Puneet K Dokania",
                "Philip HS Torr",
                "Marc\u2019Aurelio Ranzato"
            ],
            "title": "On tiny episodic memories in continual learning",
            "year": 1902
        },
        {
            "authors": [
                "Jaeyoung Chung",
                "Kanggeon Lee",
                "Sungyong Baik",
                "Kyoung Mu Lee"
            ],
            "title": "Meil-nerf: Memory-efficient incremental learning of neural radiance fields",
            "venue": "arXiv preprint arXiv:2212.08328,",
            "year": 2022
        },
        {
            "authors": [
                "Matthias De Lange",
                "Rahaf Aljundi",
                "Marc Masana",
                "Sarah Parisot",
                "Xu Jia",
                "Ale\u0161 Leonardis",
                "Gregory Slabaugh",
                "Tinne Tuytelaars"
            ],
            "title": "A continual learning survey: Defying forgetting in classification tasks",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Kyle Gao",
                "Yina Gao",
                "Hongjie He",
                "Denning Lu",
                "Linlin Xu",
                "Jonathan Li"
            ],
            "title": "Nerf: Neural radiance field in 3d vision, a comprehensive review",
            "venue": "arXiv preprint arXiv:2210.00379,",
            "year": 2022
        },
        {
            "authors": [
                "Ian Goodfellow",
                "Jean Pouget-Abadie",
                "Mehdi Mirza",
                "Bing Xu",
                "David Warde-Farley",
                "Sherjil Ozair",
                "Aaron Courville",
                "Yoshua Bengio"
            ],
            "title": "Generative adversarial networks",
            "venue": "Communications of the ACM,",
            "year": 2020
        },
        {
            "authors": [
                "Richard Hartley",
                "Andrew Zisserman"
            ],
            "title": "Multiple view geometry in computer vision",
            "venue": "Cambridge university press,",
            "year": 2003
        },
        {
            "authors": [
                "Alain Hore",
                "Djemel Ziou"
            ],
            "title": "Image quality metrics: Psnr vs. ssim",
            "venue": "In 2010 20th international conference on pattern recognition,",
            "year": 2010
        },
        {
            "authors": [
                "James Kirkpatrick",
                "Razvan Pascanu",
                "Neil Rabinowitz",
                "Joel Veness",
                "Guillaume Desjardins",
                "Andrei A Rusu",
                "Kieran Milan",
                "John Quan",
                "Tiago Ramalho",
                "Agnieszka Grabska-Barwinska"
            ],
            "title": "Overcoming catastrophic forgetting in neural networks",
            "venue": "Proceedings of the national academy of sciences,",
            "year": 2017
        },
        {
            "authors": [
                "Arno Knapitsch",
                "Jaesik Park",
                "Qian-Yi Zhou",
                "Vladlen Koltun"
            ],
            "title": "Tanks and temples: Benchmarking large-scale scene reconstruction",
            "venue": "ACM Transactions on Graphics (ToG),",
            "year": 2017
        },
        {
            "authors": [
                "Ruilong Li",
                "Matthew Tancik",
                "Angjoo Kanazawa"
            ],
            "title": "Nerfacc: A general nerf acceleration toolbox",
            "venue": "arXiv preprint arXiv:2210.04847,",
            "year": 2022
        },
        {
            "authors": [
                "Zhizhong Li",
                "Derek Hoiem"
            ],
            "title": "Learning without forgetting",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2017
        },
        {
            "authors": [
                "Zhiqiu Lin",
                "Jia Shi",
                "Deepak Pathak",
                "Deva Ramanan"
            ],
            "title": "The clear benchmark: Continual learning on real-world imagery",
            "venue": "In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round",
            "year": 2021
        },
        {
            "authors": [
                "Lingjie Liu",
                "Jiatao Gu",
                "Kyaw Zaw Lin",
                "Tat-Seng Chua",
                "Christian Theobalt"
            ],
            "title": "Neural sparse voxel fields",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "David Lopez-Paz",
                "Marc\u2019Aurelio Ranzato"
            ],
            "title": "Gradient episodic memory for continual learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Arun Mallya",
                "Svetlana Lazebnik"
            ],
            "title": "Packnet: Adding multiple tasks to a single network by iterative pruning",
            "venue": "In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Ricardo Martin-Brualla",
                "Noha Radwan",
                "Mehdi SM Sajjadi",
                "Jonathan T Barron",
                "Alexey Dosovitskiy",
                "Daniel Duckworth"
            ],
            "title": "Nerf in the wild: Neural radiance fields for unconstrained photo collections",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Nelson Max"
            ],
            "title": "Optical models for direct volume rendering",
            "venue": "IEEE Transactions on Visualization and Computer Graphics,",
            "year": 1995
        },
        {
            "authors": [
                "Michael McCloskey",
                "Neal J Cohen"
            ],
            "title": "Catastrophic interference in connectionist networks: The sequential learning problem",
            "venue": "In Psychology of learning and motivation,",
            "year": 1989
        },
        {
            "authors": [
                "Ben Mildenhall",
                "Pratul P Srinivasan",
                "Matthew Tancik",
                "Jonathan T Barron",
                "Ravi Ramamoorthi",
                "Ren Ng"
            ],
            "title": "Nerf: Representing scenes as neural radiance fields for view synthesis",
            "venue": "Communications of the ACM,",
            "year": 2021
        },
        {
            "authors": [
                "Thomas M\u00fcller",
                "Alex Evans",
                "Christoph Schied",
                "Alexander Keller"
            ],
            "title": "Instant neural graphics primitives with a multiresolution hash encoding",
            "venue": "ACM Transactions on Graphics (ToG),",
            "year": 2022
        },
        {
            "authors": [
                "Ameya Prabhu",
                "Philip HS Torr",
                "Puneet K Dokania"
            ],
            "title": "Gdumb: A simple approach that questions our progress in continual learning",
            "venue": "In Computer Vision\u2013ECCV 2020: 16th European Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Christian Reiser",
                "Songyou Peng",
                "Yiyi Liao",
                "Andreas Geiger"
            ],
            "title": "Kilonerf: Speeding up neural radiance fields with thousands of tiny mlps",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Tom Schaul",
                "John Quan",
                "Ioannis Antonoglou",
                "David Silver"
            ],
            "title": "Prioritized experience replay",
            "venue": "arXiv preprint arXiv:1511.05952,",
            "year": 2015
        },
        {
            "authors": [
                "Johannes Lutz Sch\u00f6nberger",
                "Jan-Michael Frahm"
            ],
            "title": "Structure-from-motion revisited",
            "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2016
        },
        {
            "authors": [
                "Joan Serra",
                "Didac Suris",
                "Marius Miron",
                "Alexandros Karatzoglou"
            ],
            "title": "Overcoming catastrophic forgetting with hard attention to the task",
            "venue": "In International conference on machine learning,",
            "year": 2018
        },
        {
            "authors": [
                "Hanul Shin",
                "Jung Kwon Lee",
                "Jaehong Kim",
                "Jiwon Kim"
            ],
            "title": "Continual learning with deep generative replay",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Cheng Sun",
                "Min Sun",
                "Hwann-Tzong Chen"
            ],
            "title": "Direct voxel grid optimization: Super-fast convergence for radiance fields reconstruction",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Matthew Tancik",
                "Vincent Casser",
                "Xinchen Yan",
                "Sabeek Pradhan",
                "Ben Mildenhall",
                "Pratul P Srinivasan",
                "Jonathan T Barron",
                "Henrik Kretzschmar"
            ],
            "title": "Block-nerf: Scalable large scene neural view synthesis",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "Novel view synthesis aims to render unseen views given a set of calibrated images. In practical applications, the coverage, appearance or geometry of the scene may change over time, with new images continuously being captured. Efficiently incorporating such continuous change is an open challenge. Standard NeRF benchmarks only involve scene coverage expansion. To study other practical scene changes, we propose a new dataset, World Across Time (WAT), consisting of scenes that change in appearance and geometry over time. We also propose a simple yet effective method, CLNeRF, which introduces continual learning (CL) to Neural Radiance Fields (NeRFs). CLNeRF combines generative replay and the Instant Neural Graphics Primitives (NGP) architecture to effectively prevent catastrophic forgetting and efficiently update the model when new data arrives. We also add trainable appearance and geometry embeddings to NGP, allowing a single compact model to handle complex scene changes. Without the need to store historical images, CLNeRF trained sequentially over mul-\ntiple scans of a changing scene performs on-par with the upper bound model trained on all scans at once. Compared to other CL baselines CLNeRF performs much better across standard benchmarks and WAT. The source code, a demo, and the WAT dataset are available at https: //github.com/IntelLabs/CLNeRF."
        },
        {
            "heading": "1. Introduction",
            "text": "Neural Radiance Fields (NeRFs) have emerged as the preeminent method for novel view synthesis. Given images of a scene from multiple views, NeRFs can effectively interpolate between them. However, in practical applications (e.g., city rendering [32]), the scene may change over time, resulting in a gradually revealed sequence of images with new scene coverage (new city blocks), appearance (lighting or weather) and geometry (new construction). Learning continually from such sequential data is an important problem.\nNaive model re-training on all revealed data is expensive, millions of images may need to be stored for large scale systems [32]. Meanwhile, updating the model only on new\nar X\niv :2\n30 8.\n14 81\ndata leads to catastrophic forgetting [22], i.e., old scene geometry and appearances can no longer be recovered (see\nFig. 1). Inspired by the continual learning literature for image classification [7], this work studies continual learning in the context of NeRFs to design a system that can learn from a sequence of scene scans without forgetting while requiring minimal storage.\nReplay is one of the most effective continual learning algorithms; it trains models on a blend of new and historical data. Experience replay [5] explicitly stores a tiny portion of the historical data for replay, while generative replay [30] synthesizes replay data using a generative model (e.g., a GAN [9]) trained on historical data. Experience replay is more widely used in image classification, since generative models are often hard to train, perform poorly on high resolution images, and introduce new model parameters. In contrast, NeRFs excel at generating high-resolution images, making them ideal candidates for generative replay.\nMotivated by this synergy between advanced NeRF models and generative replay, we propose CLNeRF which combines generative replay with Instant Neural Graphics Primitives (NGP) [24] to enable efficient model updates and to prevent forgetting without the need to store historical images. CLNeRF also introduces trainable appearance and geometry embeddings into NGP so that various scene changes can be handled by a single model. Unlike classification-based continual learning methods whose performance gap to the upper bound model is still non-negligible [30], the synergy between continual learning and advanced NeRF architectures allows CLNeRF to achieve a similar rendering quality as the upper bound model (see Fig. 1).\nContributions: (1) We study the problem of continual learning in the context of NeRFs. We present World Across Time (WAT), a practical continual learning dataset for NeRFs that contains scenes with real-world appearance and geometry changes over time. (2) We propose CLNeRF, a simple yet effective continual learning system for NeRFs with minimal storage and memory requirements. Extensive experiments demonstrate the superiority of CLNeRF over other continual learning approaches on standard NeRF datasets and WAT."
        },
        {
            "heading": "2. Related Work",
            "text": "NeRF. Learning Neural Radiance Fields (NeRFs) is arguably the most popular technique for novel view synthesis (see [8] for a detailed survey). Vanilla NeRF [23] represents a scene implicitly using neural networks, specifically, multi-layer perceptrons (MLPs). These MLPs map a 3D location and a view direction to their corresponding color and opacity. An image of the scene is synthesized by casting camera rays into 3D space and performing volume rendering. Though effective at interpolating novel views, vanilla NeRF has several limitations, for example, the slow training/inference speed.\nThis problem is addressed by using explicit scene representations [31, 24], or spatially-distributed small MLPs [26]. CLNeRF applies these advanced architectures to ensure efficient model updates during continual learning. Vanilla NeRF only considers static scenes; to handle varied lightning or weather conditions, trainable appearance embeddings are introduced [20, 32]. Transient objects in in-the-wild photos are handled by either introducing a transient MLP [20] or using segmentation masks [32]. CLNeRF adopts these techniques to allow a single model to handle complex scene changes. Concurrent to this work, Chung et al. [6] also study NeRFs in the context of continual learning. However, they only consider static scenes and the vanilla NeRF architecture. We consider scenes with changing appearance/geometry, and introduce a new dataset to study such scenarios. The proper combination of continual learning and more advanced architectures also makes CLNeRF simpler (no extra hyperparameters) and much more effective at mitigating forgetting. Continual Learning. Continual learning aims to learn from a sequence of data with distribution shifts, without storing historical data (see [7] for a detailed survey). Naive training over non-IID data sequences suffers from catastrophic forgetting [12] and performs poorly on historical data. A popular line of work regularizes the training objective to prevent forgetting [12, 15]. However, since the regularization does not rely on historical data it is less effective in practice. Parameter isolation methods prevent forgetting by freezing (a subset of) neurons from previous tasks and use new neurons to learn later tasks [19, 29]. Though remembrance can be guaranteed [19], these methods have a limited capacity or grow the network significantly given a large number of tasks. Replay-based approaches use historical data to prevent forgetting. This historical data is either stored in a small replay buffer [5, 18], or synthesized by a generative model [30]. Generative replay [30], i.e., synthesizing historical data, is less effective for image classification, since the generative model introduces extra parameters, and performs poorly on high resolution images. In contrast, this work shows that advanced NeRF models and generative replay benefit from each other, since high quality replay data can be rendered without introducing new model parameters."
        },
        {
            "heading": "3. Method",
            "text": ""
        },
        {
            "heading": "3.1. Preliminaries",
            "text": "Before introducing CLNeRF, we first review the basics of NeRFs, and formulate the problem of continual learning. NeRF. Given a set of images, NeRFs train a model parameterized by \u03b8 that maps a 3D location x \u2208 R3 and a view direction d \u2208 S3 (a unit vector from the camera center to x) to the corresponding color c(x,d|\u03b8) \u2208 [0, 1]3 and opacity \u03c3(x|\u03b8) \u2208 [0, 1]. Given a target image view, we render the color for each pixel independently. For each pixel, we cast a\nray from the camera center o \u2208 R3 towards the pixel center, and sample a set of 3D points X = {xi|xi = o+\u03c4id} along the ray, where \u03c4i is the euclidean distance from the camera center to the sampled point. Then, we render the color C\u0302(X) of the ray following the volume rendering [21] equation:\nC\u0302(X) = \u2211 i wic(xi,d|\u03b8), (1)\nwhere wi = e\u2212 \u2211i\u22121\nj=1 \u03c3(xj |\u03b8)(\u03c4j+1\u2212\u03c4j)(1 \u2212 e\u2212\u03c3(xi|\u03b8)(\u03c4i+1\u2212\u03c4i)). Intuitively, Equation (1) computes the weighted sum of the colors on all sampled points. The weights wi are computed based on the opacity and the distance to the camera center.\nContinual NeRF. Throughout this paper, we refer to the continual learning problem for NeRFs as continual NeRF. At each time step t of continual NeRF:\n1. A set of training images along with their camera parameters (intrinsics and extrinsics) St are generated.\n2. The current model \u03b8t and the replay buffer Mt (for storing historical data) are updated by:\n{Mt,\u03b8t} \u2190 update(St,\u03b8t\u22121,Mt\u22121) (2)\n3. \u03b8t is deployed for rendering novel views until t+ 1.\nThis process simulates the practical scenario where the model \u03b8t is deployed continually. Once in a while, a set of new images arrives, potentially containing new views of the scene and changes in appearance or geometry. The goal is to update \u03b8t; ideally storage\n(to maintain historical data in Mt) and memory (to deploy \u03b8t) requirements are small.\nAs shown in Fig. 2, CLNeRF addresses three major problems of continual NeRF: (1) effectively updating \u03b8t using minimal storage, (2) updating Mt during optional experience replay, and (3) handling various scene changes with a single compact model. We provide further details on each of these components below."
        },
        {
            "heading": "3.2. Model Update",
            "text": "CLNeRF applies replay-based methods [5, 30] to prevent catastrophic forgetting. To enable applications with extreme storage limits, CLNeRF combines generative replay [30] with advanced NeRF architectures so that it is effective even when no historical image can be stored.\nFig. 2(a) depicts the model update process of CLNeRF at each time step t. The camera parameters of all historical images are stored in the replay buffer Mt\u22121 for generative replay. A small number of images IER are optionally maintained when the storage is sufficient for experience replay [5]. At each training iteration of \u03b8t, CLNeRF generates a batch of camera rays X = XER \u22c3 XGR \u22c3 Xt\nuniformly from Pt \u22c3 PER \u22c3\nPGR, where Pt, PGR and PER are respectively the camera parameters of new data St, generative replay data and experience replay data. The training objective is:\nminimize \u03b8t\n\u2211 X\u2208X LNeRF(C(X), C\u0302(X|\u03b8t))\n|X | , (3)\nwhere LNeRF is the loss for standard NeRF training, C(\u00b7) is the supervision signal from new data or replay, and C\u0302(\u00b7|\u03b8t) is the color rendered by \u03b8t. For the rays X \u2208 XGR sampled from PGR, we\nperform generative replay, i.e., we set the supervision signal C(X) as the image colors C\u0302(X|\u03b8t\u22121) generated by \u03b8t\u22121. For the other rays, C(X) is the ground-truth image color. After the model update, we replace the previously deployed model \u03b8t\u22121 with \u03b8t and update the replay buffer Mt (see Sec. 3.3 for more details). Only \u03b8t and Mt are maintained until the next set of data St+1 arrives.\nAlthough all camera parameters are stored in Mt\u22121, they only consume a small amount of storage, at most Nt\u22121(dpose + dint), where Nt\u22121 is the number of historical images, and dpose and dint are the dimensions of camera poses and intrinsic parameters respectively; dpose = 6 and dint \u2264 5 for common camera models [10]. dint is shared if multiple images are captured by the same camera. As a concrete example, storing the parameters for 1000 samples each captured with a different camera requires roughly 45KB of storage in our experiment, much less than storing a single high resolution image. This guarantees the effectiveness of CLNeRF (see Sec. 5.1) even for applications with extreme storage limits.\nWe also emphasize the importance of random sampling. CLNeRF assigns uniform sampling weights between all views revealed so far. Some image-classification-based continual learning methods [5] and the cocurrent work for NeRFs [6] propose biased sampling strategies, where a fixed and large percentage ( 1\n2 to 2 3 ) of\nthe rays are sampled from new data (Pt). This strategy not only introduces new hyperparameters (e.g., loss weights of old data or the proportion of rays from new data [6]), but also performs worse than uniform random sampling, as shown in Sec. 5."
        },
        {
            "heading": "3.3. Replay Buffer Update",
            "text": "In the extreme case where no image can be stored for experience replay, we only store the camera parameters of historical data in Mt to make CLNeRF flexible and effective for practical systems with various storage sizes. When the storage is sufficient to maintain a subset of historical images for experience replay, we use a reservoir buffer [5]. Specifically, current data is added to Mt as long as the storage limit is not reached. Otherwise, as shown in Fig. 2 (b), given Mt\u22121 capable of storing K images, we first generate for each image Ii \u2208 St a random integer ji \u2208 {1, 2, ..., Ni}, where Ni represents the order of Ii in the continual learning data sequence. If ji > K, we do not add Ii into Mt. Otherwise, we replace the ji\u2019th image in Mt\u22121 with Ii. Note that Mt stores all camera parameters regardless of if the corresponding image is stored or not.\nWe also experiment with the prioritized replay buffer [27], where Mt keeps images with the lowest rendering quality. Specifically, after updating \u03b8t, we iterate over all images in Mt\u22121 and St and keep the K images with the lowest rendering PSNR [11] from \u03b8t. Though widely used in reinforcement learning, prioritized replay does not perform better than reservoir sampling in continual NeRF (see Sec. 5.2.2). A reservoir buffer is also simpler to implement and more efficient (no need to compare the rendering quality) to update; hence CLNeRF applies it by default."
        },
        {
            "heading": "3.4. Architecture",
            "text": "CLNeRF by default uses the Instant Neural Graphics Primitives (NGP) [24] architecture. This not only enables efficient model updates during continual NeRF, but also ensures the low overhead and effectiveness of generative replay. As shown in Sec. 5.2.3, using NGP as the backbone for CLNeRF results in better performance and efficiency compared to vanilla NeRF [23].\n(a) Test view rendered by NGP with transient MLP. (b) Test view rendered by NGP with masked transient objects.\n(c) Training view rendered by NGP with transient MLP. (d) GT training view.\nFigure 3. Result of NGP with transient MLPs. Similar to [20], we add artificial transient objects and lightning changes to the Lego scene. NGP with transient MLPs overfits to the training views and fails to filter transient objects automatically.\nA compact continual NeRF system should use a single model to incorporate scene changes, so that the model size does not increase significantly over time. We achieve this by adding trainable appearance and geometry embeddings to the base architecture (Fig. 2 (c)). Given a spatial location x and a viewing direction d, we first encode x into a feature vector f (using the grid-based hash encoder for NGP, and an MLP for vanilla NeRF). Then, we generate the color and opacity respectively by c = Dc(f ,d, ea) and \u03c3 = D\u03c3(f , eg), where Dc and D\u03c3 are the color and opacity decoders (MLP for both NGP and vanilla NeRF); ea is the trainable appearance embedding and eg is the geometry embedding. Given a sequence of scans of the same scene, with appearance and geometry changes between different scans, we add one appearance embedding and one geometry embedding for each scan, i.e., for each time step t of continual NeRF. We set the dimension of appearance and geometry embeddings to 48 and 16 respectively, which ensures minimal model size increase during continual NeRF and is sufficient to encode complex real-world scene changes as shown in Sec. 5.\nCLNeRF uses segmentation masks (from [3]) to filter transient objects. As shown in Fig. 3, we also explored using the transient MLP and robust training objectives [20] but found empirically that NGP is not compatible with this strategy \u2013 the non-transient network overfits to the transient objects and fails to filter them automatically. Note that we only remove transient/dynamic objects within a single scan, e.g., moving pedestrians. Scene changes between different scans, e.g., newly constructed buildings, are handled by geometry embeddings."
        },
        {
            "heading": "4. WAT: A Continual NeRF Benchmark",
            "text": "Most continual learning methods in the literature are evaluated on datasets synthesized from standard image classification benchmarks [7]. Although a similar strategy can be used on standard\nNeRF benchmarks, it is not practical as it only considers static scenes with a gradually expanding rendering range. However, this does not model the real-world distribution shifts introduced by the change of time [4, 16], such as the change of scene appearance (e.g., lighting and weather) and geometry (e.g., new decoration of a room). To solve this problem, we propose World Across Time (WAT), a new dataset and benchmark for practical continual NeRF.\nAs shown in Fig. 4, WAT consists of images captured from 6 different scenes (both indoor and outdoor). For each scene, we capture 5-10 videos at different real-world time to generate natural appearance or geometry changes across videos. We extract a subset of the video frames (200-400 images for each scene), and use colmap [28] to compute the camera parameters. For each scene, we hold out 1\n8 of the images for testing and use the remaining images\nfor training. We order the images naturally based on the time that the corresponding videos were captured. At each time step t of continual NeRF, all images belonging to a new video are revealed to the model. Compared to standard NeRF datasets, WAT has diverse scene types, scene changes, and a realistic data order based on real-world time. As shown in Sec. 5.1, the natural time-based order makes WAT much more challenging than randomly dividing standard in-the-wild datasets into subsets (e.g., as in the case of phototourism, which has similar appearance and pose distributions between different subsets). WAT enables us to study the importance of the model design for changing scenes. As shown in Sec. 5.1, methods designed only for static scenes perform poorly on WAT."
        },
        {
            "heading": "5. Experiments",
            "text": "In the experiments, we first compare CLNeRF against other continual learning approaches (Sec. 5.1). Then, we analyse different continual NeRF components in detail (Sec. 5.2). Although NGP is used by default in CLNeRF, we also experiment with vanilla NeRF to demonstrate the effect of architectures.\nImplementation Details. Our implementation of the vanilla NeRF\nand NGP backbones is based on NeRFAcc [14] and NGP-PL [2] respectively. Following the base implementations, we allow 50K training iterations for vanilla NeRF and 20K for NGP whenever we train or update the model. We find that NGP produces \u201cNaN\u201d losses and exploding gradients when trained for too long , making it hard to initialize \u03b8t with \u03b8t\u22121. Hence, we randomly initialize \u03b8t and train the model from scratch at each time step, as done in GDumb [25]. Empirical results (Sec. 5.2.3) on vanilla NeRF show that initializing \u03b8t with \u03b8t\u22121 can help continual NeRF, and we leave further investigation of this issue on NGP for future work. Unless otherwise stated, all hyperparameters strictly follow the base code. See Appendix A for further implementation details of different continual learning methods. Training one model with either NGP or vanilla NeRF backbone takes 5-20 minutes or 1-2 hours on a single RTX6000 GPU respectively.\nDatasets. Besides WAT, we also evaluate methods on datasets derived from standard NeRF benchmarks. Specifically, we uniformly divide the training data of standard benchmarks into 10-20 subsets and reveal them sequentially during continual NeRF. For synthetic data, we use the dataset proposed in [23] (referred to as Synth-NeRF), resulting in 8 scenes, each with 10 time steps and 20 training images (with consecutive image IDs) per time step. For real-world data, we use two Tanks and Temples [13] subsets proposed in [17] (with background filter, referred to as NSVF) and [33] (without background filter, referred to as NeRF++). Both datasets are divided into multiple sub-trajectories with consecutive video frames. NeRF++ has 4 scenes, 10 time steps and 60-80 images per time step. For NSVF, we mimic the construction process of the concurrent work [6], and divide the first 100 training images of each scene into 20 subsets, resulting in 5 scenes, each with 20 time steps and 5 images per time step. Finally, we use 4 Phototourism scenes (Brandenburg Gate, Sacre Coeur, Trevi Fountain and Taj Mahal) along with the train-test split from [20]. Due to the lack of time stamps, we randomly divide each scene into 20 time steps and 42-86 images per time step (with consecutive image IDs).\nEvaluation protocol. To evaluate a continual NeRF approach, we first train it sequentially over all time steps, then compute the mean PSNR/SSIM [11] on the held-out test images for all time steps."
        },
        {
            "heading": "5.1. Main Results",
            "text": "To evaluate CLNeRF, we compare it against: (1) Naive Training (NT), where we train a model sequentially on new data without continual learning. NT represents the lower-bound performance that we can achieve on continual NeRF. (2) Elastic Weight Consolidation (EWC) [12], a widely-used regularization-based continual learning method. (3) Experience Replay (ER) [5], one of the most effective continual learning methods. (4) MEIL-NeRF [6], a concurrent work that also uses generative replay. For fair comparison, we use the ground-truth camera parameters to generate replay camera rays for MEIL-NeRF as done in CLNeRF, rather than using a small MLP to learn the rays of interests. This strategy makes the implementation simpler and performs better, as also demonstrated in the original paper. (5) The upper bound model (UB) trained on all data at once, representing the upper-bound performance of continual NeRF. For all methods that involve experience replay (ER and CLNeRF), we allow 10 images to be stored in the replay buffer to simulate the case of highly limited storage (see Sec. 5.2.2 for the\neffect of the replay buffer size). For fair comparison, we choose the best-performing architecture for each method. See Sec. 5.2 for the effect of architectures, and Appendix D for NGP-only results.\nAs shown in Tab. 1, CLNeRF, even without storing historical data for experience replay (CLNeRF-noER), performs much better than other continual NeRF approaches across all datasets (see Appendix C for the results of individual scenes). With only 10 images (2.5%-10% of the complete dataset size) stored in the replay buffer, CLNeRF achieves comparable performance as UB, which requires storing all historical data. Although MEIL-NeRF also applies generative replay, the biased sampling strategy (towards new data) and the complex loss designed for vanilla NeRF are not suitable for advanced architectures like NGP. As a result, there is a significant performance gap compared to UB which is consistent with the results in the original paper. As shown later in Sec. 5.2.3, CLNeRF also performs better than MEIL-NeRF with a vanilla NeRF backbone. Interestingly, methods without generative replay (NT, EWC, ER) work better with vanilla NeRF. We analyze this phenomenon in detail in Sec. 5.2.3. For a fairer comparison on WAT, we use the trainable embeddings of CLNeRF also for the other methods in Tab. 1. The results without embeddings are reported in the caption; the gap to CLNeRF increases significantly.\nWe also apply CLNeRF to in-the-wild images from Phototourism. Since the NeRFAcc-based vanilla NeRF implementation does not perform well on Phototourism, and NeRFW [20] is slow (> 1 week per time step with 8 GPUs) in continual NeRF, we instead report the performance using NGP for NT and ER, and the UB version of NeRFW based on the implementation of [1]. EWC cannot be applied to NGP since we need to perform re-initialization at each time step. We assign 1 appearance embedding to each image (rather than each time step) to handle per-image appearance change. As shown in Tab. 1 the NGP-based upper bound model performs better than NeRFW in terms of PSNR and worse in terms of SSIM. CLNeRF performs close to the upper bound model. Due to the lack of time stamps, we do not have a natural data order for Phototourism. This simplifies the problem since images from different (artificially created) time steps have similar pose and ap-\npearance distributions. As a result, the performance gap between different methods is much smaller compared to other datasets, even with a much larger number of images (800-1700 in Phototourism versus 100-400 in other datasets).\nFig. 5 shows qualitative results (see Appendix C for more). Each two rows show the novel views rendered for the current and past time steps. CLNeRF provides similar rendering quality as UB, with a much lower storage consumption. Without using continual learning, NT overfits to the current data, resulting in wrong geometry (the redundant lamp and the wrongly closed curtain in Living room - past), lightning and severe artifacts for past time steps. Due to the lack of historical data, EWC not only fails to recover past scenes, but also hinders the model from learning on new data. ER stores a small amount of historical data to prevent forgetting. However, the limited replay buffer size makes it hard to recover details in past time steps (see Sec. 5.2.2 for the effect of replay buffer sizes). The biased sampling strategy and complex loss design make MEIL-NeRF not only more complex (e.g., extra hyperparameters), but also underfit on historical data. As a result, it loses detail from past time steps, even when equipped with the same trainable embeddings of CLNeRF.\nThese results show the importance of WAT on benchmarking continual NeRF under practical scene changes. They also show the superiority of CLNeRF over other baselines to robustly handle appearance and geometry changes."
        },
        {
            "heading": "5.2. Analysis",
            "text": ""
        },
        {
            "heading": "5.2.1 Ablation",
            "text": "This section analyzes the effectiveness of individual CLNeRF components. Specifically, we remove each component and report the performance drop. As shown in Tab. 2, the performance drops only slightly without experience replay (No ER). However, without generative replay (No GenRep), the performance dropps significantly. Note that NoGenRep is ER with NGP instead of vanilla NeRF. Hence, generative replay is more important in CLNeRF than experience replay to prevent forgetting. Without using NGP, i.e., when applied to vanilla NeRF, CLNeRF also performs much worse,\nand sometimes (e.g., on Synth-NeRF) worse than ER (NeRF) in Tab. 1. This result shows the importance of advanced architectures for guaranteeing the effectiveness of generative replay. Without the trainable embeddings (No Embed), CLNeRF cannot adapt well to changing appearance and geometry of WAT."
        },
        {
            "heading": "5.2.2 Effect of Replay Buffer",
            "text": "Replay Buffer Size. To mimic the case of highly limited storage, we only allow 10 historical images to be stored for experience replay in the main experiment. Here, we investigate the effect of replay buffer size. Specifically, we vary the replay buffer size of ER (with NGP) and CLNeRF in the pattern of {0, 10, 10%, ..., 100%} and report the performance change. 0 and 10 (roughly 2.5%\u2212 5% on WAT) are the number of stored images. The percentages are\nwith respect to all images across time steps. As shown in Fig. 6, CLNeRF does not require any samples stored in the replay buffer to perform well but it also does not hurt performance. ER requires a large replay buffer size (80%) to perform on-par with CLNeRF. This interesting result shows that widely-used CL methods designed for image classification can be sub-optimal for other problems.\nReplay Buffer Update Strategy. CLNeRF applies reservoir sampling to update the replay buffer at each time step. Here, we analyze the effect of different replay buffer update strategies. Specifically, we compare the performance of CLNeRF using reservoir sampling [5] and prioritized replay [27]. As shown in Tab. 3, changing the reservoir buffer to a prioritized replay buffer does not improve CLNeRF. Hence, a uniform coverage of the whole scene (changes) is sufficient for effective experience replay."
        },
        {
            "heading": "5.2.3 Effect of Architecture",
            "text": "To show the effectiveness of CLNeRF across architectures, we compare it against UB and MEIL-NeRF using vanilla NeRF as a back-\nbone. We do not use experience replay for either CLNeRF or MEILNeRF, and we equip all methods with the trainable embeddings proposed in Sec. 3.4. As shown in Tab. 4, CLNeRF still performs slightly better than MEIL-NeRF, even though MEIL-NeRF was specifically designed based on vanilla NeRF. The performance gap between CLNeRF/MEIL-NeRF and UB on SynthNeRF is larger with vanilla NeRF than with NGP, highlighting the importance of advanced architectures for generative replay.\nAs shown in Tab. 1, both ER and NT benefit more from vanilla NeRF. To reveal the underlying reason, we compare NT, ER, CLNeRF under 3 different training strategies: (1) Trained using vanilla NeRF without re-initialization (NeRF). (2) Trained using vanilla NeRF with re-initialization at each time step t (NeRFReinit). (3) Trained using NGP (NGP). As shown in Tab. 5, a large portion of the performance gap lies in the inheritance of model parameters from previous time steps, i.e., not performing reinitialization for vanilla NeRF. When both are re-initialized, vanilla NeRF still performs slightly better than NGP for methods without generative replay. We conjecture that this is because NGP overfits more to the training data given sparse views (which is the case for NT and ER), and generalizes poorly on novel views. Performing generative replay allows NGP to overcome the sparse training view issue and exceed the performance of vanilla NeRF."
        },
        {
            "heading": "6. Conclusion",
            "text": "This work studies continual learning for NeRFs. We propose a new dataset \u2013 World Across Time (WAT) \u2013 containing natural scenes with appearance and geometry changes over time. We also propose CLNeRF, an effective continual learning system that performs close to the upper bound model trained on all data at once. CLNeRF uses generative replay and performs well even without storing any historical images. While our current experiments only cover scenes with hundreds of images, they are an important step toward deploying practical NeRFs in the real world. There are many interesting future research directions for CLNeRF. For example, solving the NaN loss problem of NGP to make model inheritance more effective during continual learning. Extending CLNeRF to the scale of Block-NeRF [32] is also an interesting future work."
        },
        {
            "heading": "A. Further Implementation Details for Differ-",
            "text": "ent Methods\nEWC. EWC [12] regularizes the current model \u03b8t to be close to \u03b8t\u22121. The training loss of EWC at time step t is\nLEWC(\u03b8t) = LNeRF(\u03b8t) + \u2211 i \u03bb 2 Fi(\u03b8t,i \u2212 \u03b8t\u22121,i)2, (4)\nwhere LNeRF(\u00b7) is the original NeRF training loss, Fi is the i-th element of the diagnal of the Fisher information matrix, \u03b8t,i is the i-th element of \u03b8t, and \u03bb is the hyper-parameter that controls the regularization strength. As in the original paper, the regularization term from EWC has a much smaller magnitude than LNeRF. To make regularization effective, we tune \u03bb by grid search from 1e3 to 1e9 on the Synth-NeRF dataset, and picking 1e5 that provides the best performance (making the regularization term roughly 10% of LNeRF). ER. Experience replay (ER) [5] uses the loss on both new and historical data to update the model \u03b8t. Due to the ineffectiveness of biased sampling for continual NeRF, we uses random sampling for ER to produce the best performance and uniformly weight the losses of all ray samples. MEIL-NeRF. MEIL-NeRF [6] forms each mini-batch of training data by sampling 2\n3 of the rays from new data, and the rest from\nold ones. The training loss of MEIL-NeRF is\n1 |Xc| \u2211 Xc LNeRF(Xc,\u03b8t) + \u03bbMEIL|Xo| \u2211 Xo \u03c1(C\u0302(Xo|\u03b8t\u22121)\u2212 C\u0302(Xo|\u03b8t)), (5)\nwhere Xc and Xo are respectively the rays from new and old data, LNeRF(\u00b7) is the original loss for NeRF training, \u03c1(\u00b7) is the Charbonnier penalty function, C\u0302(X|\u03b8) is the color of ray X generated by \u03b8, and \u03bbMEIL is the hyper-parameter that controls the regularization strength from old data. Following S2 of the original paper, \u03bbMEIL is scheduled as\n\u03bbMEIL = cos(\u03c0(1 + r)) + 1\n2 , (6)\nwhere r is the training progress rate (from 0 to 1) in each time step. Note that we correct the typo of the original paper and add a \u201c+1\u201d after cos(\u00b7) to ensure that \u03bb grows gradually from 0 to 1 (consistent with Figure S5 of the original paper). CLNeRF Depending on the resource limit, the generative replay of CLNeRF can be implemented in both online and offline fashion. For applications where new data are generated only once a while (e.g., city scans are uploaded once a couple of days), the model is mostly in the deploy mode. Hence during the infrequent case of model update, we can assign a temporal storage to store all images generated by \u03b8t\u22121, and then use them to update \u03b8t. The benefit of this implementation is that we only need to load 1 model into the GPU memory, and the temporal storage can be released once we finish updating \u03b8t (which takes only 5-20 mins for CLNeRF). For applications where no temporal storage can be used, we load \u03b8t\u22121 (evaluation mode to same memory) and \u03b8t into the GPU at the same time, and generate the replay supervision signal on-the-fly, which requires an extra forward pass per training iteration. In our test, such implementation can still fit into a single RTX6000 GPU, and increased the training time by roughly 60%, which is still fast for NGP. We use the first implementation in our experiments due to its simplicity."
        },
        {
            "heading": "B. Further Qualitative Results on WAT",
            "text": "Here, we show further qualitative results on individual scenes of WAT. To better demonstrate the advantage of CLNeRF in terms of architecture design and continual learning strategies, we show the results of other continual NeRF methods with (Fig. 8) and without (Fig. 7) using the proposed trainable embeddings (please refer to the video demo in our github repo for close-view comparisons). Methods without trainable embeddings cannot recover the geometry and appearance of past time steps, resulting in severe artifacts. Even with trainable embeddings, severe artifacts (NT, EWC) and detail lost (ER, MEIL-NeRF) still exist in other baselines."
        },
        {
            "heading": "C. Quantitative Results on Individual Scenes",
            "text": "This section shows the quantitative results (Tab. 6 to 10) on individual scenes of each used dataset. CLNeRF performs better than other continual NeRF approaches on all individual scenes, even without storing any historical image. The performance gap between CLNeRF and UB is also small for all scenes. Due to the training noise and the close practical performance, the best model of each scene changes between CLNeRF and CLNeRF-noER and UB."
        },
        {
            "heading": "D. Baseline Results with NGP",
            "text": "In the main experiments (Tab. 1), we choose the best architecture for different baselines and observe that methods like ER and NT perform better with vanilla NeRF. Here, we further report the baseline results using NGP architecture. Due to the NaN loss issue of NGP, we cannot perform EWC on it. Hence, we only report the results for NT and ER. Due to the resource limit, we only report the results on Synth-NeRF, NeRF++ and WAT. As discussed in Sec. 5.2.3, NGP without generative replay overfits to the training views and fails to generalize to novel views and past time steps."
        },
        {
            "heading": "E. More WAT Scenes",
            "text": "In this section, we add 4 more scenes to WAT, making the total number of scenes from 6 to 10. We use the same data creation process as described in Sec. 4, and call this enlarged dataset WAT+ (available in our code repository). As shown in Tab. 12, the results on these new scenes are consistent with Tab. 1."
        }
    ],
    "title": "CLNeRF: Continual Learning Meets NeRF",
    "year": 2023
}