{
    "abstractText": "Privacy risk assessments have been touted as an objective, principled way to encourage organizations to implement privacy-by-design. They are central to a new regulatory model of collaborative governance, as embodied by the GDPR. However, existing guidelines and methods remain vague, and there is little empirical evidence on privacy harms. In this paper we conduct a close analysis of US NIST\u2019s Privacy Risk Assessment Methodology, highlighting multiple sites of discretion that create countless opportunities for adversarial organizations to engage in performative compliance. Our analysis shows that the premises on which the success of privacy risk assessments depends do not hold, particularly in regard to organizations\u2019 incentives and regulators auditing capabilities. We highlight the limitations and pitfalls of what is essentially a utilitarian and technocratic approach, leading us to discuss alternatives and a realignment of our policy and research objectives.",
    "authors": [],
    "id": "SP:55b4cb0a507a07ef14fcb1ace2ca1993ce0f6e7d",
    "references": [
        {
            "authors": [
                "Alessandro Acquisti",
                "Laura Brandimarte",
                "George Loewenstein"
            ],
            "title": "Privacy and human behavior in the age of information",
            "year": 2015
        },
        {
            "authors": [
                "Alessandro Acquisti",
                "Laura Brandimarte",
                "George Loewenstein"
            ],
            "title": "Secrets and likes: The drive for privacy and the difficulty of achieving it in the digital age",
            "venue": "Journal of Consumer Psychology,",
            "year": 2020
        },
        {
            "authors": [
                "Sushant Agarwal"
            ],
            "title": "Developing a structured metric to measure privacy risk in privacy impact assessments. Privacy and Identity Management. Time for a Revolution",
            "venue": "IFIP WG 9.2,",
            "year": 2015
        },
        {
            "authors": [
                "Michael Barbaro",
                "Tom Zeller",
                "Saul Hansell"
            ],
            "title": "A face is exposed for aol searcher no. 4417749",
            "year": 2008
        },
        {
            "authors": [
                "Solon Barocas",
                "Helen Nissenbaum"
            ],
            "title": "On notice: The trouble with notice and consent. In Proceedings of the engaging data forum: The first international forum on the application and management",
            "year": 2009
        },
        {
            "authors": [
                "Robin M Bayley",
                "Colin J Bennett"
            ],
            "title": "Privacy impact assessments in canada",
            "venue": "Privacy Impact Assessment,",
            "year": 2012
        },
        {
            "authors": [
                "Colin J Bennett",
                "Charles D Raab"
            ],
            "title": "Revisiting the governance of privacy: Contemporary policy instruments in global perspective",
            "venue": "Regulation & Governance,",
            "year": 2020
        },
        {
            "authors": [
                "Reuben Binns"
            ],
            "title": "Data protection impact assessments: a meta-regulatory approach",
            "venue": "International Data Privacy Law,",
            "year": 2017
        },
        {
            "authors": [
                "Sarah Bird",
                "Solon Barocas",
                "Kate Crawford",
                "Fernando Diaz",
                "Hanna Wallach"
            ],
            "title": "Exploring or exploiting? social and ethical implications of autonomous experimentation in ai",
            "venue": "In Workshop on Fairness, Accountability, and Transparency in Machine Learning,",
            "year": 2016
        },
        {
            "authors": [
                "Sean Brooks",
                "Michael Garcia",
                "Naomi Lefkovitz",
                "Suzanne Lightman",
                "Ellen Nadeau"
            ],
            "title": "An introduction to privacy engineering and risk management in federal systems",
            "venue": "NISTIR 8062,",
            "year": 2017
        },
        {
            "authors": [
                "Ryan Calo"
            ],
            "title": "The boundaries of privacy harm",
            "venue": "Ind. LJ,",
            "year": 2011
        },
        {
            "authors": [
                "Amanda Chisholm",
                "J\u00falio Jesus"
            ],
            "title": "Cultural heritage. In Methods of Environmental and Social Impact Assessment, pages 432\u2013474",
            "year": 2017
        },
        {
            "authors": [
                "Roger Clarke"
            ],
            "title": "Privacy impact assessment: Its origins and development",
            "venue": "Computer law & security review,",
            "year": 2009
        },
        {
            "authors": [
                "Julie E Cohen"
            ],
            "title": "What privacy is for",
            "venue": "Harvard law review,",
            "year": 2013
        },
        {
            "authors": [
                "Lorrie Faith Cranor",
                "Joseph Reagle",
                "Mark S Ackerman"
            ],
            "title": "Beyond concern: Understanding net users\u2019 attitudes about online privacy. The Internet upheaval: raising questions, seeking answers in communications",
            "year": 2000
        },
        {
            "authors": [
                "R Jason Cronk",
                "Stuart S Shapiro"
            ],
            "title": "Quantitative privacy risk analysis",
            "venue": "IEEE European Symposium on Security and Privacy Workshops (EuroS&PW),",
            "year": 2021
        },
        {
            "authors": [
                "Sourya Joyee De",
                "Daniel Le M\u00e9tayer"
            ],
            "title": "PRIAM: a privacy risk analysis methodology",
            "venue": "In Data privacy management and security assurance,",
            "year": 2016
        },
        {
            "authors": [
                "Cynthia Dwork",
                "Moni Naor"
            ],
            "title": "On the difficulties of disclosure prevention in statistical databases or the case for differential privacy",
            "venue": "Journal of Privacy and Confidentiality,",
            "year": 2010
        },
        {
            "authors": [
                "Francesco Feri",
                "Caterina Giannetti",
                "Nicola Jentzsch"
            ],
            "title": "Disclosure of personal information under risk of privacy shocks",
            "venue": "Journal of Economic Behavior & Organization,",
            "year": 2016
        },
        {
            "authors": [
                "Baruch Fischhoff"
            ],
            "title": "The realities of risk-cost-benefit analysis",
            "venue": "Science, 350(6260):aaa6516,",
            "year": 2015
        },
        {
            "authors": [
                "Baruch Fischhoff",
                "Stephen RWatson",
                "Chris Hope"
            ],
            "title": "Defining risk",
            "venue": "Policy sciences,",
            "year": 1984
        },
        {
            "authors": [
                "Rapha\u00ebl Gellert"
            ],
            "title": "Data protection: a risk regulation? between the risk management of everything and the precautionary alternative",
            "venue": "International Data Privacy Law,",
            "year": 2015
        },
        {
            "authors": [
                "Raphael Gellert"
            ],
            "title": "Understanding the notion of risk in the general data protection regulation",
            "venue": "Computer Law & Security Review,",
            "year": 2018
        },
        {
            "authors": [
                "Rapha\u00ebl Gellert"
            ],
            "title": "The risk-based approach to data protection",
            "year": 2020
        },
        {
            "authors": [
                "John Glasson"
            ],
            "title": "Socio-economic impacts 1: overview and economic impacts",
            "venue": "Methods of Environmental and Social Impact Assessment,",
            "year": 2017
        },
        {
            "authors": [
                "Ben Green"
            ],
            "title": "The false promise of risk assessments: epistemic reform and the limits of fairness",
            "venue": "In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency,",
            "year": 2020
        },
        {
            "authors": [
                "Seda G\u00fcrses",
                "Jose M Del Alamo"
            ],
            "title": "Privacy engineering: Shaping an emerging field of research and practice",
            "venue": "IEEE Security & Privacy,",
            "year": 2016
        },
        {
            "authors": [
                "Seda G\u00fcrses",
                "Carmela Troncoso",
                "Claudia Diaz"
            ],
            "title": "Engineering privacy by design reloaded",
            "venue": "In Amsterdam Privacy Conference,",
            "year": 2015
        },
        {
            "authors": [
                "Woodrow Hartzog"
            ],
            "title": "What is privacy? that\u2019s the wrong question",
            "venue": "U. Chi. L. Rev.,",
            "year": 2021
        },
        {
            "authors": [
                "Basileal Imana",
                "Aleksandra Korolova",
                "John Heidemann"
            ],
            "title": "Auditing for discrimination in algorithms delivering job ads",
            "venue": "In Proceedings of the Web Conference",
            "year": 2021
        },
        {
            "authors": [
                "Irene Kamara"
            ],
            "title": "Co-regulation in eu personal data protection: the case of technical standards and the privacy by design standardisation\u2019mandate",
            "venue": "European journal of law and technology,",
            "year": 2017
        },
        {
            "authors": [
                "Margot E. Kaminski"
            ],
            "title": "Binary governance: Lessons from the gdpr\u2019s approach to algorithmic accountability",
            "venue": "S. Cal. L. Rev.,",
            "year": 2018
        },
        {
            "authors": [
                "Margot E. Kaminski",
                "Gianclaudio Malgieri"
            ],
            "title": "Algorithmic impact assessments under the gdpr: producing multi-layered explanations",
            "venue": "International Data Privacy Law,",
            "year": 2020
        },
        {
            "authors": [
                "Daniel M Kammen",
                "David M Hassenzahl"
            ],
            "title": "Should we risk it? In Should We Risk It",
            "year": 2018
        },
        {
            "authors": [
                "Jack Lewis"
            ],
            "title": "The birth of epa",
            "venue": "EPA J.,",
            "year": 1985
        },
        {
            "authors": [
                "Alessandro Mantelero"
            ],
            "title": "Human rights impact assessment and ai",
            "year": 2022
        },
        {
            "authors": [
                "Sandy Mccarthy"
            ],
            "title": "Google\u2019s announces timeline for new privacy policy",
            "venue": "UWIRE Text,",
            "year": 2020
        },
        {
            "authors": [
                "M. Granger Morgan"
            ],
            "title": "Probing the question of technology-induced risk",
            "venue": "Readings in risk,",
            "year": 1990
        },
        {
            "authors": [
                "Arvind Narayanan",
                "Vitaly Shmatikov"
            ],
            "title": "Robust de-anonymization of large sparse datasets",
            "venue": "IEEE Symposium on Security and Privacy (sp",
            "year": 2008
        },
        {
            "authors": [
                "Alfred Ng"
            ],
            "title": "Can auditing eliminate bias from algorithms? Online at https://themarkup.org/the-breakdown/2021/02/23/can-auditing-eliminate-bias-from-algorithms",
            "venue": "Last retrieved on Feb",
            "year": 2023
        },
        {
            "authors": [
                "Helen Nissenbaum"
            ],
            "title": "Privacy in context",
            "year": 2009
        },
        {
            "authors": [
                "Marie Caroline Oetzel",
                "Sarah Spiekermann"
            ],
            "title": "A systematic methodology for privacy impact assessments: a design science approach",
            "venue": "European Journal of Information Systems,",
            "year": 2014
        },
        {
            "authors": [
                "Charles D Raab"
            ],
            "title": "The future of privacy protection",
            "venue": "Technical report, Office of Science and Technology (UK),",
            "year": 2004
        },
        {
            "authors": [
                "Charles D Raab",
                "Colin J Bennett"
            ],
            "title": "Taking the measure of privacy: can data protection be evaluated",
            "venue": "International Review of Administrative Sciences,",
            "year": 1996
        },
        {
            "authors": [
                "Marvin Rausand"
            ],
            "title": "Risk assessment: theory, methods, and applications, volume 115",
            "year": 2013
        },
        {
            "authors": [
                "Priscilla M Regan"
            ],
            "title": "Privacy as a common good in the digital world",
            "venue": "Information, Communication & Society,",
            "year": 2002
        },
        {
            "authors": [
                "Antoaneta Roussi"
            ],
            "title": "Resisting the rise of facial recognition",
            "venue": "Nature, 587(7834):350\u2013354,",
            "year": 2020
        },
        {
            "authors": [
                "Sam Schechner"
            ],
            "title": "Meta\u2019s targeted ad model faces restrictions in Europe",
            "venue": "Online at https://www.wsj.com/articles/metas-targeted-ad-model-faces-restrictions-in-europe-11670335772. Last retrieved on Feb",
            "year": 2023
        },
        {
            "authors": [
                "Evan Selinger",
                "Woodrow Hartzog"
            ],
            "title": "Obscurity and privacy. In Spaces for the Future, pages 119\u2013129",
            "year": 2017
        },
        {
            "authors": [
                "Lei Shen"
            ],
            "title": "The NIST cybersecurity framework: Overview and potential impacts",
            "venue": "Scitech Lawyer,",
            "year": 2014
        },
        {
            "authors": [
                "Laurens Sion",
                "Dimitri Van Landuyt",
                "Kim Wuyts",
                "Wouter Joosen"
            ],
            "title": "Privacy risk assessment for data subject-aware threat modeling",
            "venue": "IEEE Security and Privacy Workshops (SPW),",
            "year": 2019
        },
        {
            "authors": [
                "Robert H Sloan",
                "Richard Warner"
            ],
            "title": "Beyond notice and choice: Privacy, norms, and consent",
            "venue": "J. High Tech. L.,",
            "year": 2014
        },
        {
            "authors": [
                "J Daniel"
            ],
            "title": "Solove. A taxonomy of privacy",
            "venue": "University of Pennsylvania law review,",
            "year": 2006
        },
        {
            "authors": [
                "Daniel J. Solove",
                "Danielle Keats Citron"
            ],
            "title": "Risk and anxiety: A theory of data-breach harms",
            "venue": "Tex. L. Rev.,",
            "year": 2017
        },
        {
            "authors": [
                "Till Speicher",
                "Muhammad Ali",
                "Giridhari Venkatadri",
                "Filipe Nunes Ribeiro",
                "George Arvanitakis",
                "Fab\u0155\u0131cio Benevenuto",
                "Krishna P Gummadi",
                "Patrick Loiseau",
                "Alan Mislove"
            ],
            "title": "Potential for discrimination in online targeted advertising",
            "venue": "In Conference on Fairness, Accountability and Transparency,",
            "year": 2018
        },
        {
            "authors": [
                "Sarah Spiekermann"
            ],
            "title": "The challenges of privacy by design",
            "venue": "Communications of the ACM,",
            "year": 2012
        },
        {
            "authors": [
                "Andrew Stirling"
            ],
            "title": "Risk, precaution and science: towards a more constructive policy debate: talking point on the precautionary principle",
            "venue": "EMBO reports,",
            "year": 2007
        },
        {
            "authors": [
                "Daniel Susser"
            ],
            "title": "Notice after notice-and-consent: Why privacy disclosures are valuable even if consent frameworks aren\u2019t",
            "venue": "Journal of Information Policy,",
            "year": 2019
        },
        {
            "authors": [
                "Latanya Sweeney"
            ],
            "title": "k-anonymity: A model for protecting privacy",
            "venue": "International journal of uncertainty, fuzziness and knowledge-based systems,",
            "year": 2002
        },
        {
            "authors": [
                "Peter P Swire"
            ],
            "title": "Efficient confidentiality for privacy, security, and confidential business information",
            "venue": "Brookings-Wharton Papers on Financial Services,",
            "year": 2003
        },
        {
            "authors": [
                "Catherine E Tucker"
            ],
            "title": "The economics of advertising and privacy",
            "venue": "International journal of Industrial organization,",
            "year": 2012
        },
        {
            "authors": [
                "Frank Vanclay"
            ],
            "title": "Reflections on social impact assessment in the 21st century",
            "venue": "Impact Assessment and Project Appraisal,",
            "year": 2020
        },
        {
            "authors": [
                "Inder M Verma"
            ],
            "title": "Editorial expression of concern: Experimental evidence of massive-scale emotional contagion through social networks",
            "venue": "Proceedings of the National Academy of Sciences of the United States of America,",
            "year": 2014
        },
        {
            "authors": [
                "Salome Viljoen"
            ],
            "title": "A relational theory of data governance",
            "venue": "Yale LJ,",
            "year": 2021
        },
        {
            "authors": [
                "Ari Ezra Waldman"
            ],
            "title": "Industry Unbound: The Inside Story of Privacy, Data, and Corporate Power",
            "year": 2021
        },
        {
            "authors": [
                "Georgia Wells",
                "Jeff Horwitz",
                "Deepa Seetharaman"
            ],
            "title": "Facebook knows instagram is toxic for teen girls, company documents show",
            "venue": "The Wall Street Journal,",
            "year": 2021
        },
        {
            "authors": [
                "Richard Wilson",
                "Edmund A.C. Crouch"
            ],
            "title": "Risk-benefit analysis",
            "year": 2001
        },
        {
            "authors": [
                "Langdon Winner"
            ],
            "title": "Do artifacts have politics? In Computer Ethics, pages 177\u2013192",
            "year": 2017
        },
        {
            "authors": [
                "David Wright",
                "Paul De Hert"
            ],
            "title": "Introduction to privacy impact assessment",
            "venue": "Privacy impact assessment,",
            "year": 2012
        },
        {
            "authors": [
                "David Wright",
                "Paul De Hert"
            ],
            "title": "Privacy impact assessment, volume",
            "year": 2012
        }
    ],
    "sections": [
        {
            "text": "ar X\niv :2\n31 0.\n05 93\n6v 1\n[ cs\n.C R\n] 2\n4 A\nPrivacy risk assessments have been touted as an objective, principled way to encourage organizations to implement privacy-by-design. They are central to a new regulatory model of collaborative governance, as embodied by the GDPR. However, existing guidelines and methods remain vague, and there is little empirical evidence on privacy harms. In this paper we conduct a close analysis of US NIST\u2019s Privacy Risk Assessment Methodology, highlighting multiple sites of discretion that create countless opportunities for adversarial organizations to engage in performative compliance. Our analysis shows that the premises on which the success of privacy risk assessments depends do not hold, particularly in regard to organizations\u2019 incentives and regulators auditing capabilities. We highlight the limitations and pitfalls of what is essentially a utilitarian and technocratic approach, leading us to discuss alternatives and a realignment of our policy and research objectives."
        },
        {
            "heading": "1 Introduction",
            "text": "Privacy risk assessments (PRAs) have steadily gained prominence as an instrument to help and encourage organizations to develop privacy-preserving systems and services. Touted as a much needed departure from the rigid command-andcontrol regulatory model, PRAs are said to embody a more agile and dynamic approach to privacy regulation. This model, variously referred to as metaregulation or collaborative governance, seeks to leverage knowledge and expertise from organizations, as well as push responsibility onto them, encouraging cooperation and trust with the regulator in a light supervisory role [9, 36, 37] At the same time, by encouraging organizations to assess privacy risks before they implement and deploy their services, PRAs are considered a key tool in implementing privacy-by-design [13, 60]. Indeed, PRAs have recently gained renewed support across both sides of the Atlantic, with the EU enshrining risk assessments in the GDPR, and the US\u2019s NIST proposing risk assessment as the key tool in their privacy engineering program [22, 27, 28, 1, 11].\n\u2217Working draft. A version of this paper was presented at the 16th International Conference on Computers, Privacy & Data Protection, May 24\u201326, 2023 in Brussels (Belgium).\nHowever, in spite of the institutional endorsement, the actual benefits of adopting PRAs remains unclear. Recent empirical work as well as the constant trickle of privacy breaches, scandals and litigation, suggests that privacy impact assessments (PIAs) may not be as effective as expected [52, 70]. Legal scholars and political scientists have long theorized about the necessary conditions for PIAs to succeed [8, 75] Factors such as transparency, accountability, and a systematic, principled approach to risk assessment, e.g. recital 76 of the GDPR specifies that \u201c[r]isk should be evaluated on the basis of an objective assessment\u201d [22].\nThis emphasis in objectivity further hints at a technocratic approach to privacy regulation that pervades PIA scholarship [28]. Early on, PIA proponents lamented the lack of standardized guidelines and trained professionals, as well as the vagueness and arbitrariness of existing PIA [48, 75]. Privacy scholars argued that a systematic and (somewhat) objective approach to risk assessment could however turn the tide and not only help organizations manage privacy risks, but render PIAs more successful overall [48]. Yet this ambition has also failed to materialize. Privacy harms remain elusive to model and quantify.1 We do not seem today any closer to having something even resembling a science of privacy risks or actuarial models for privacy than 30 years ago. PIA has remained more of an art than a science, learned by craft and practice rather than formal methods.\nThis paper revisits and re-examines PRA as a tool in the regulatory arsenal, providing the following contributions. Firstly, we question whether PRAs can, today, deliver meaningful results in terms of increased privacy protections. We investigate the extent to which PRAs (1) may be weaponized by adversarial organizations as a tool of performative compliance [70], and (2) are amenable to the kind of systematic audit processes that collaborative governance requires. We observe that existing guidelines introduce (to borrow Green\u2019s terminology) countless sites of discretion that organizations can weaponize to escape meaningful privacy intervention and rendering them, as a result, hard to audit [30].\nSecondly, we examine the role of risk assessments within the field of privacy engineering [31]. Privacy engineers have become adept at minimizing privacy risk through privacy enhancing technologies [32]. However, privacy engineering does not support\u2014cannot support\u2014the broader scope of risk assessment that methodologies such as NIST\u2019s Privacy risk assessment methodology (PRAM) and regulatory efforts such as the GDPR promote. When it comes to assessing the legitimacy of a service or functionality, privacy engineering is mute.\nThirdly, we resort to Nissenbaum\u2019s theory of Contextual integrity (CI) to show that PRAs have a disruptive potential insofar as they can weaponize the lack of empirical evidence on privacy harms to upend existing norms and expectations around privacy [45]. We further argue how CI is in fact a PRA of sorts, even if devoid of the pseudoscientific claims and technocratic ideals that PRA embody, and may offer hints to a better deliberation framework to guide privacy design.\nFurthermore, to support and illustrate the points above, we provide a stepby-step analysis of NIST\u2019s PRAM, While NIST\u2019s PRAM is just one among many guidelines and methods of PRA, NIST\u2019s normative authority as a standard-\n1While recent scholarship and discourse has increasingly recognized and widened the scope of privacy harms, privacy risk remains absent from the mainstream risk analysis literature [12, 15, 58].\nsetting agency makes it a perfect candidate for evaluation and reflection. Moreover, we argue that similar lessons can be drawn from existing alternatives [4, 19, 20, 46, 55, 69]. At the same time, our goal is not to dismiss privacy risk assessments as an instrument; rather, we aim to identify and be more systematic about the challenges and sites of discretion that one may consider for PRAs to become an effective tool of regulation."
        },
        {
            "heading": "2 Background, preliminaries and related work",
            "text": "While the terms privacy risk assessment and privacy impact assessment are often conflated and used interchangeably in the literature [75], in this paper we adopt the following convention. We use the term privacy risk assessment (PRA) to refer to the explicit evaluation of privacy risk, understood as the product of two components: the likelihood and impact of those privacy harms.2 We use the term privacy impact assessment (PIA) to refer to a broader category of assessments that may or may not rely on a explicit PRA. Hence, PIAs include both PRA and more casual, informal assessments that do not formally identify risk.3 Our focus in this paper is on PRA, but we begin by tracing some of the earlier literature on PIAs.\nClarke traced the early origins and development of PIAs, finding that PIAs increasingly gained popularity since the mid-1990s as a response to the disillusionment of decades of pro-business fair information practice principles (FIPPs) and OECD guidelines and the weak privacy legislation that they had spurred [16]. Clarke advances two possible explanations for the emergence of PIAs: increasing pressure from the public, disappointed with privacy protections at the time, and a natural development of rational management techniques, by adopting privacy as an additional component in risk management frameworks. Clarke sees this emergence in \u201cthe context of larger trends in advanced industrial societies to manage risk and to impose the burden of proof for [harmlessness] on its promoters\u201d [16]. This notion reverberates in more recent work that has interpreted the EU\u2019s regulator adoption of risk assessments, standardization and certification as a shift from command-and-control to what various authors have referred to as co-regulation, meta-regulation, binary governance or collaborative governance [8, 9, 35, 36, 37]. In essence, this approach espouses a philosophy of \u201cmonitored self-regulation\u201d [37].\nPIAs are said to encourage organizations to consider privacy from the inception of their projects (in line with privacy-by-design principles) and inspire a sense of responsibility, enabling in turn more fully informed decision-making processes and cost-effective solutions [75]. Privacy scholars have long theorized\n2We note however that, as several authors have pointed out, both within the expert risk assessment community and outside of, terminology is wildly inconsistent, with authors variously referring to risk as either chance, probability or possibility; or hazard, threat or danger, among other terms [49, 75].\n3E.g. we would refer to a document that lists potential privacy harms a system may cause without a explicit determination of the likelihood and impact of those harms as a PIA, not a PRA.Indeed, various authors have noted that risk assessment lies at the heart of any impact assessment, particularly as the former conveys the notion of a more systematic and objective approach [8, 47, 75]. Moreover, the GDPR refers to a data protection impact assessment (DPIA) which, strictly speaking, is not a PIA. However, a DPIA must consider the rights and freedoms of data subjects, among which the right to privacy. Hence, we consider that our analysis extends to DPIAs as envisioned in the GDPR.\nabout the reasons why and conditions under which PIAs, as an instrument of monitored self-regulation, would be successful [8]. Among other factors, PIAs should engage relevant stakeholders, conducted in a systematic way, be made publicly available and subjected to audits by the regulator. Failure to abide by these principles could render them meaningless, enabling organizations to simply revert to self-regulation [9].\nThe lack of formalism and rigorousness has haunted privacy scholarship for decades now. Clarke observed that, at the time, many PIA guidelines were \u201cchecklist-like\u201d and would not offer much beyond a compliance check [16]. Raab aimed to \u201ctake the practice and theory of privacy protection beyond the stage of the merely casual use of the term \u2018risk\u2019, namely, \u201ca more nuanced understanding\u201d that would enable us to \u201cestimate different degrees of privacy risk\u201d beyond vague statements such as \u201ccookies pose a threat to privacy\u201d [47]. Wright and De Hert observed that existing guidelines gave \u201cconsiderable discretion to organizations and were \u201cperfunctory at best, as short as two pages\u201d, while Bayley and Bennett found \u201cmeaningless statements that risks were identified and appropriate mitigations implemented or planned, with no hint to what those might be\u201d [7, 74].4 The lack of a systematic methodology further hampers the auditability requirement. Rather than having an established methodology or canon to evaluate a PIA, auditors face a plethora of unruly ad-hoc criteria and interpretations that make their jobs significantly harder.\nAnd yet, to this day, privacy has remained largely ignored in scholarship devoted to the development of risk models. No formal methods exist to model and calculate privacy risks, i.e. to understand not only what needs to be measured, but also how. Whereas a rich scholarship exists to evaluate health, environmental and technological risks, with well-established disciplines such as toxicology, epidemiology or exposure assessment [38, 49], no such models have been developed for privacy. PIA guidelines remain vague, ad-hoc and seemingly arbitrary, reflecting the disciplinary and political biases of their authors and institutions [4, 19, 20, 46, 55, 69].\nPart of what may explain the dearth of privacy risk modeling is privacy\u2019s conceptual complexity, multifaceted-nature and the elusiveness of its harms [15, 57, 58]. Scholars have debated the meaning of privacy for decades, linking privacy to theories of access, control or appropriate flow of information as well as its relationship to other values [45]. This lack of consensus over a formal, universal definition of privacy has led others, most notably Solove, to opt for focusing on identifying privacy-related harms, regardless of whether those harms would fit under a single theory of privacy [12, 57].\nOn the other hand, much of the empirical research on privacy has focused on understanding users\u2019 privacy attitudes and behavior [2, 18, 23, 65]. An increasingly rich literature on the behavioral economics of privacy has offered empirical evidence for factors that may explain the failure of notice-and-consent as a regulatory mechanism for privacy: end-users are often unable to estimate privacy risks because of incomplete and asymmetric information, present-bias and the ease with which the selective disclosure of information may skew their perception of privacy risks [2].\nAt the same time, there has been ample theoretical work on economic the-\n4A concern for the lack of \u201cformal methodologies\u201d has similarly crept in the context of AI-regulation [40].\nories of privacy, yet again very little in terms of empirical research on privacy harms. Acquisti et al. trace three waves of (theoretical) economic analysis of privacy, from its origins in the 70s and early 80s and the Chicago School of economics and their general economic arguments informed by market fundamentalism, to more recent work providing formal economic models and empirical analyses. And yet, as Acquisti et al. note, \u201cthe economics of privacy focuses on measurable, or at least assessable, privacy components. Some (perhaps, many) of the consequences of privacy protection, and its erosion, go beyond the economic dimensions\u2013for instance, the intrinsic value of privacy as a human right, or individuals\u2019 innate desires for privacy regardless of the associated economic benefits or lack thereof\u201d.5 In other words, there remains little to no empirical evidence on the more elusive harms that result from privacy violations, such as loss of autonomy, trust and dignity. Even in what is one of the most decried privacy-infringing practices, behavioral advertising, there is little conclusive evidence about the privacy harms\u2014or even benefits, for that matter\u2014it causes [3]. Moreover, as Swire argues, the efficiency analysis of standard economic theory is impoverished compared to the concerns considered by other approaches. Economists often do not recognize an individual\u2019s right to privacy of personal information. Even where they do, the mere violation of a right, in the eyes of the economist, does not generally change the utilitarian calculus. [...] The efficiency analysis leaves out much of what people actually fear in the area of privacy protection [64]. Acquisti et al. refer to this as \u201ceconomic \u2018dark matter\u2019: We know it is there, but cannot directly quantify it\u201d [3].\nAt the same time, a rich literature on risk analysis in other domains has revealed the limitations and politics of risk assessment as a policy and decisionmaking tool. Fischhoff et al. provide an illustrative overview of the limitations and shortcomings of addressing societal problems through the lens of risk [25]. They remind us that the notion of risk is inherently controversial and political. Riskiness depends on the underlying definition of what we are deeming risky. Notions of objectivity are problematic: it assumes a consensus or a rational approach towards estimating risk. It also occludes politics, e.g. when prioritizing individual as opposed to societal risk, time-effects, scope and many other factors. Other authors explore the contentious nature of settling trade-offs between risks and benefits [24, 42].\nPerhaps closer to privacy is the field of social impact assessments. Chisholm and Jesus observe that these assessments in the context of cultural heritage are qualitative and subjective, there is no objectivity, and there is an implicit assumption that whoever performs the assessment does so in good faith [14]. Similarly, Glasson observes that when assessing socio-economic impacts, there is often no data, no models, when, in fact \u201call prediction methods should be some clarification of the cause-effect relationships between variables involved\u201d [29]. Both Fischhoff and Vanclay remind us that risk creates anxiety before harm actually materializes, imposing a burden on people to manage risk before they materialize [24, 66].\n5Acquisti et al. define the economics of privacy as hitherto develop in scholarship as \u201cthe analysis of the relationships between personal data and dynamic pricing\u201d. Most of the work reviewed by the authors centers around how knowing information about consumers affects pricing and the distribution of consumer surplus. In this body of work, consumer surplus is the ultimate measure of welfare, omitting other, less tangible social values."
        },
        {
            "heading": "3 NIST\u2019s PRAM",
            "text": "Part of its cybersecurity division, NIST\u2019s Privacy engineering program\u2019s (PEP) mission is to \u201csupport the development of trustworthy information systems by applying measurement science and system engineering principles to [...] protect privacy\u201d.6 Within this program, NIST has made available several resources, among which three main guidelines: First, a Privacy Framework, which describes at high-level the sort of organizational processes and structures that organizations deploy to integrate a privacy risk assessment within their risk management strategy. The Framework heavily borrows from NIST\u2019s Cybersecurity Framework [54] and it does not provide a definition of privacy or risk models for privacy harms; its focus is entirely on organizational measures.7 Second, an Internal Report (NISTIR) 8062, that introduces the concepts of privacy engineering and risk management. NISTIR 8062 addresses both the differences and overlap between information security and privacy, introduces the notion of risk as the product of likelihood and impact of adverse privacy effects and emphasizes that \u201cmeasurability matters, so that agencies can demonstrate the effectiveness of privacy controls in addressing identified privacy risks\u201d. Thirdly, and our focus in this paper, a PRAM, which instantiates the risk management approach introduced in NISTIR 8062 by providing set of concrete steps that an organization may follow to perform a risk assessment.\nPRAM is comprised of 4 worksheets each proposing several tasks, as well as a catalog of problematic data actions and problems. In the following, we examine each of these tasks in order, identifying along the way the sites of discretion that illustrate the challenges that PRAs pose for auditors and regulators.\nWS1: Framing organizational objectives and privacy governance\nWorksheet 1 (WS1) addresses the elicitation of privacy requirements. In software engineering, requirements elicitation is the first stage in the design process, where the designer determines what the system must do (functional requirements) and which constraints may be put in the design space. (non-functional requirements, among which privacy). 8\nWS1 requires the analyst to \u201cframe organizational objectives\u201d such as \u201cmission/business\u201d needs, the system\u2019s functional needs and the \u201cprivacy-preserving goals\u201d that the organization may \u201cplan to highlight or market to users or customers\u201d. Next, PRAM suggests to \u201cidentif[y] privacy-related legal obligations \u201d as well as \u201cany privacy goals[...] explicit or implicit in the organization\u2019s vision and/or mission\u201d and the \u201corganization\u2019s risk tolerance with respect to privacy\u201d.\nOf relevance in WS1, in line with the conceptual foundation set in NIST\u2019s Privacy Framework and NISTIR 8062, is that NIST avoids committing to a\n6NIST Privacy engineering program (PEP), https://www.nist.gov/itl/applied-cybersecurity/privacy-engineering 7NIST distinguishes between privacy risk management, which relates to the processes that\nenable an organization to manage privacy, i.e. \u201cthe connection between business or mission drivers, organizational roles and responsibilities, and privacy protection activities\u201d to \u201chelp organizations build better privacy foundations by bringing privacy risk into parity with their broader enterprise risk portfolio\u201d, and privacy risk assessment as the subprocess of privacy risk management that involves the actual estimation of risk.\n8Functional requirements define what the system must do, whereas non-functional requirements, among which legal and privacy requirements, constrain how one must design the system to fulfill the functional requirements.\nparticular definition of privacy.9 While this choice reveals NIST\u2019s strategy to remain flexible, and avoid alienating organizations to get them to incorporate privacy into their risk management processes, the refusal is essentially political and normative in itself, for beyond the minimum legal requirements it portrays privacy as a marketable commodity, reinforcing the logic of self-regulation and notice-and-choice.10 At the same time, implicit in this approach is an assumption of universality in PRAM as a methodology, i.e. PRAM is a sound methodology regardless of the definition of privacy one considers.\nWS2: Assessing system design\nWorksheet 2 (WS2) focuses on threat modeling instructing analysts to produce a system model or data diagram flow of the system (Task 2 \u2013 Supporting data map) and project the privacy requirements elicited in WS1 onto it. This requires the operationalization privacy goals, privacy-related policies, principles and requirements in WS1 to concrete constraints on data operations within the system, e.g. defining which parties must have access to which types of data. Task 3 (Contextual factors) encourages the analyst to consider contextual factors such as the nature of the organizations engaged in the system (public, private or regulated industry), or the public\u2019s perception and privacy expectations of these organizations. Finally, Task 4 (Data action analysis) requires the analyst to fill a table to list data actions in the system, the data involved in such data actions, relevant contextual factors (as per Task 3) and a summary of issues which is meant to document any relevant observations related to privacy.\nAll in all, WS2 closely resembles guidance commonly found in existing threat modeling methodologies. As such, none of these tasks are inherently flawed. It is their application and interpretation in the context of a privacy risk assessment that makes them problematic. Let us consider two particular issues. Task 2 requires the production of a data diagram flow, a common practice in software engineering and threat modeling, which implicitly defines the scope of the system under consideration. Hence, what the organization considers to be the \u2018system\u2019 will have important consequences for risk assessment. An intuitive, obvious definition of \u2018system\u2019 would comprise the processes that the organization directly controls or interacts with, and leave as out of scope the processes and \u2018systems\u2019 of other organizations. After all, an organization may have little knowledge and control over other organizations\u2019 operational processes\u2014even if commercial relationships may confer certain leverage to prime or discourage certain practices. However, to the extent that an organization\u2019s (data) operations are embedded within broader, more complex systems, taking such an approach may lead to a myopic view of what is at stake, in turn underestimating the role that the organization\u2019s subsystem plays within the greater system as a whole.\n9NIST\u2019s Privacy Framework highlights the \u201cbroad and shifting nature of privacy\u201d, stating that privacy is an \u201call-encompassing concept that helps safeguard important values such as human autonomy and dignity\u201d while cautioning that \u201chuman autonomy and dignity are not fixed, quantifiable constructs\u201d [1]. NIST points instead to \u201dpublications that provide an indepth treatment\u201d, citing both Solove\u2019s and Selinger and Hartzog\u2019s works as two sources of normative guidance [11, 53, 57].\n10As largely explored in the privacy literature, notice-and-choice has largely failed at tackling current privacy issues as it pushes all responsibility and burden to end-users themselves, who face a largely adversarial market landscape, devoid of privacy-preserving alternatives [6, 56, 62].\nOnline advertising is a case in point. Media publishers that wish to monetize their audiences may ponder over whether to serve contextual or behavioral advertising. Let us consider, informally, the privacy risk of installing a thirdparty cookie to enable advertising networks to track readers across the web. From the point of view of an online news organization, estimation of privacy risk will be wildly different depending on what the organization considers as system. The organization could consider as \u2018system\u2019 its own websites and the content its readers see, i.e. the organization\u2019s system proper. It may thus compute the risk of collecting and providing information about a user\u2019s visits to its websites, which could be innocuous enough, leading to no obvious harms by themselves, in isolation. However, the organization\u2019s system proper only plays a small role in the advertising ecosystem as a whole. To truly evaluate the impact of behavioral advertising on users, the organization would have to consider the vast network of cookies and other tracking mechanisms across the web, and ultimately evaluate the impact of targeted advertising on society as a whole. An organization may also exert some control over the kind of ads it may display to its readers. However, the cookies it places on its websites have an impact on the ads web users see in another publisher\u2019s website. Hence, constraining a system\u2019s definition to the operations within an organization may not be truly representative of the privacy risks to which an organization contributes.\nAt the same time, disentangling the contribution of a single publisher to the whole targeted ads ecosystem is far from trivial. Much of the value that advertisers derive from tracking users stems from the relationality of data, i.e. what people that visit a particular set of websites are interested in [68]. The tracking data points from any single website, by themselves, would be of little value. Similarly, these data points, by themselves, may not lead to any obvious harms, in part because a single point of data may have little predictive value: it is the aggregation of points across websites and users, the emergence of patterns, that provide utility to advertisers and represent a privacy threat to users.\nSimilarly, Tasks 1 and 2\u2019s focus on granular data operations may also miss the forest for the trees. A system model or data diagram flow is important to understand how the system works and how it may need to be redesigned.11 However, breaking a system into atomic data operations and their individualized risk scores may not necessarily capture the combined risk of considering these data operations as a whole. Behavioral advertising as described above is one example of this forest-for-the-trees problem. Another classic, illuminating example comes from the field of database privacy. In a groundbreaking paper, Sweeney showed that whereas knowing each date of birth, gender or ZIP-code alone would rarely lead to the re-identification of any individual within a database, the combination of all three pieces of information would enable to uniquely identify up to 87% of individuals [63]. This illustrates how, whereas the risk of revealing any one single attribute may be low (very low), the combination may be high and not equivalent to the sum of each of these attributes separately.\n11In conversations the author had with NIST representatives, this was one of the reported outcomes from pilot projects. In one instance, as reported by NIST, a CEO would deny the company was using a particular type of data, to which engineers would reply, armed with a data diagram flow, that they did process those data, thus triggering a more informed discussion around their operations.\nWS3: Prioritizing risk.\nWorksheet 3 (WS3), \u201cPrioritizing risk\u201d, represents the core of the privacy risk assessment, providing guidelines on how to estimate the two components of risk: likelihood and impact. WS3 defines four tasks: (1) assess likelihood, (2) assess impact, (3) calculate risk, and (4) prioritize risk, which we review below.\nLikelihood. PRAM defines likelihood as the \u201cprobability that a data action will become problematic for representative or typical individuals whose data is being processed or is interacting with the system/product/service\u201d. This definition raises three main questions.\nFirstly, what it means for data actions to \u201cbecome problematic\u201d. PRAM provides a Catalog of Problematic Data Actions and Problems. Problematic actions include appropriation, re-identification, surveillance or unanticipated revelation, among others. According to PRAM, these actions become problematic when they lead to problems such as dignity loss, discrimination, economic loss, loss of self-determination (which includes autonomy, liberty and physical harm) as well as loss of trust. Thus PRAM offers to bypass the contentious definition of privacy to focus on harms that derive from privacy loss. In other words, there is no specific privacy violation that an analyst is meant to identify; rather, the focus is on these problems or harms, widely recognized as byproducts of privacy loss.12 This strategy, namely, conceptualizing privacy through a taxonomy of harms rather through a formal definition, is not new, and has been previously adopted in privacy scholarship, most prominently by Solove [33, 57]. However, such a strategy only seems to kick the can down the road, opening new challenges for the privacy analyst.13\nWhile it is hard to contend with the argument that privacy is a slippery and hard to pin down concept, values such as dignity or autonomy are equally slippery and open to interpretation, thus representing yet another point for an organization to exercise discretion, i.e. encoding their own, self-interested version of privacy by cherry-picking how they decide to incorporate these proxy values into the analysis, e.g. giving them more or less weight, or simply ignoring them.\nMoreover, the space of potential problems is far too big for an analyst to consider in its totality, a problem compounded by the nonexistence of actuarial models for privacy. One problematic action may lead to a wide range of consequences, from the most banal to the catastrophic, and estimating their probability depends on a large number of factors that is not easy to model or anticipate. Consider the AOL search log anonymization blunder, where two journalists at The New York Times managed to re-identify several searchers [5]. While a (better) privacy risk analysis may have anticipated the risk of re-identification, the\n12NIST representatives confirmed this was the case in conversations with one of the authors. Mentions of privacy threats or privacy harms were deemed too ambiguous and vague in NIST\u2019s consultations with stakeholders. Hence, NIST saw benefit in avoiding privacy as a concept, choosing instead to favor a panoply of privacy-related values.\n13Critiques of the taxonomization approach point out that by grounding privacy problems in social recognition rather than a formal definition, the taxonomical approach tends to identify harms as privacy-related insofar as social commentary identifies them as so, arbitrarily excluding harms that may later become accepted as privacy-related, while failing to account for the tensions among harms that fall under the privacy umbrella [12].\nconsequences and their likelihood would have been far harder to determine.14 Privacy harms are notoriously elusive. The loss of trust in search engines as a result of loose data sharing practices cannot be easily quantified. How individuals in the AOL dataset may have approached online search from that moment on, or how other individuals not included in the dataset, whether using AOL or another search engine, may have become mistrustful of search engines has an impact on society overall that, to this day, we do not know how to measure. The expectation that one\u2019s searches may be exposed or monitored may restrain people\u2019s self-expression, leading to self-censorship or anxiety over \u201crisky searches\u201d.\nCitron and Solove have identified numerous factors that illustrate the elusiveness of privacy harms, explaining why US courts have consistently failed to acknowledge them [15, 58]. Harms may be small but numerous, often repeated over time and perpetrated by multiple entities. Or they may be tiny but affect a very large number of people. While any of these harms may be small enough to dismiss and even fail to register in a risk analysis, the combined effect could be significant and harder to ignore. In many cases, the harm is not even knowable, the loss of trust in search engines as described above a case in point, but also the chains of transmission that go beyond the initial disclosure, e.g. an organization A that discloses data to organization B may not be able to account for B\u2019s subsequent uses of data. Harms are also easy to overlook due to the relational and contextual value of data. Seemingly innocuous pieces of information may become sensitive in a different context, or when combined with additional data [43, 45]. Finding causality between privacy breaches and privacy harms is also challenging, because misuses are not widely known or advertised, i.e. malicious entities have incentives to keep their practices secret, e.g. which data they are using, what they are combining it with. More generally, it was the realization that it is impossible to provide any absolute guarantees of privacy risk or protection that motivated the push for differential privacy [21]. This should indeed give us pause when attempting to measure the likelihood of a privacy harm.\nIn short, a privacy analyst cannot be realistically expected to account for every possible situation and harm, let alone be able to measure their impact as Task 2 below requires. This incommensurable complexity forces analysts to engage in two types of simplification. First, the analyst must select a strategy to scope the problem space within which problems are considered at all, leaving other problems out. Second, the analyst must select a threshold beyond which certain events become problematic. Both strategies lead to bias in the selection of which types of problems make it into the risk assessment, further reinforcing the idea that organizations can mold risks assessments to fit their own agenda, rather than aiming for socially beneficial outcomes. This is how risk assessments may become arbitrary and biased, in spite of the veneer of legitimacy that (pseudo-)statistic methods may confer to it.\nSecondly, who the representative or typical individual is. We may consider two possible approaches to determine who this typical individual is. One, in terms of demographics relevant to the organization\u2019s bottom-line (e.g. in terms of age, revenue, loyalty). Another, in terms of actual risk, i.e. the pool of individuals who would typically bear the brunt of privacy harms. Intuitively, the\n14Even if the journalists followed ethical disclosure practices\u2014as they did, requesting permission from re-identified searchers to publish their results\u2014 other parties may not have been so careful.\nsecond approach would be the most rigorous. However, this is a chicken and egg problem: to determine who the representative individuals are, we need to perform an assessment for all possible representative individuals, thereby defeating the very purpose of this simplifying strategy. Not only the likelihood and impact of suffering a privacy problem may vary widely for different individuals, but also their ability to deal with them, as people may have vastly different abilities and resources. Affluent or tech-savvy people are likely better equipped to prevent or address privacy harms, e.g. by their ability to hire lawyers or have measures in place that shield them from the organization\u2019s missteps. Thus a distributional inequity problem surfaces [72].\nOrganizations, even those conducting PRAs in good faith, are likely to prioritize their own users or clients, thus introducing a bias in who they calculate risk for. Consider the secret collection of images of people from social media and the web to develop facial recognition technology, as in the Clearview scandal [51]. While the most immediate privacy violation that led to widespread condemnation relates to those individuals whose images were scraped, the privacy harms for individuals who are not even in the dataset and upon whom face recognition technology is later deployed could be disastrous and yet more easily overlooked. 15\nThirdly, the actual computation of likelihood. PRAM suggests to measure \u201cthe probability of occurrence for each potential problem\u201d on a scale from 1 to 10, while conceding that \u201corganizations can use any scale they prefer as long as they use the same scale throughout the process\u201d. The elusiveness of privacy harms means that computing their probability of occurrence is a very complex task. The absence of actuarial models means that likelihood estimation must be done from scratch, with no existing references to compare and gauge, and therefore to readily challenge estimations. Similarly, giving organizations\u2019 the freedom to choose their own measurement scale not only represents yet another site of discretion, it also leads to auditing challenges. Auditors (as well as the broader public or whoever intends to examine an organization\u2019s PRA) must grapple with different estimation methods and scales, a significant auditing burden that ultimately hinders accountability.\nImpact. Task 2 (\u201cAssess impact\u201d) gives organizations even more discretion. In recognizing that \u201cit may be difficult for an organization to assess the impact of these problems\u201d, NIST suggests that \u201cshould [the analyst] be unable to [assess direct impact on individuals], organizational impact factors as secondary costs absorbed by the organization can be used in lieu of or in addition to direct impact assessment\u201d.16\nAllowing organizations to consider organizational rather than societal impact has been touted as a strategy to encourage them to embrace PIAs. After all, it makes sense that organizations will be more willing to address privacy problems\n15This is yet another example of how the relational nature of data challenges the notion of a representative individual. While the social, relational and interdependent nature of privacy has been amply theorized, an individualistic perspective still dominates public discourse, thereby sweeping under the rug the kind of networked, relational harms that a more capacious notion of privacy would surface [15, 17, 50, 68].\n16As organizational impact factors, PRAM proposes non compliance costs such as fines or litigation expenses; direct business costs such as loss of revenue; reputational costs such as brand damage or loss of consumers\u2019 trust; and internal culture costs such as impact on employee morale, as well as \u201cother costs an organization wants to consider\u201d.\nif they realize these affect their bottom-line. Besides, as privacy harms are often elusive, tying them to more tangible \u201cproxy\u201d costs could make their estimation easier.\nYet such a rationale can easily backfire if an organization\u2019s incentives are misaligned with society\u2019s. First, privacy harms may simply not translate into costs for an organization; organizations may also be able to externalize these costs and thus ignore privacy. Tech companies like Google or Facebook may be able to externalize most of the societal cost stemming from the loss of opportunity that results from (discriminative) targeted advertising, due to the highly lucrative duopoly they hold over the online advertising market [34, 59]. Secondly, such an approach implicitly legitimizes risk management strategies that do not seek to remediate privacy risks per se but, rather, simply mitigate the impact of such risks on the organization, e.g. in mitigating reputational costs, an organization may invest in PR rather than privacy; in avoiding regulatory fines, an organization may lobby public officials to weaken privacy regulations [70]. And even if an organization acts in good faith and attempts to measure the impact on individuals and society, similar challenges arise as discussed for the estimation of likelihood above, such as the incommensurability of all the potential avenues for harm or the diversity and breadth of the affected population.\nThe empirical estimation of privacy harms raises additional conundrums, as it must to some extent involve experimentation, potentially subjecting people to privacy harms we wish to identify and model. To the extent that these experiments are run under controlled conditions and strict ethical supervision, it may be possible to avert or minimize some of these harms. Cases such as Instagram\u2019s internal study of teens\u2019 mental health, or Facebook\u2019s experiments on emotional contagion, however, show that those organizations that have the data and manpower to perform such experiments may not always do so with sufficiently robust guardrails in place [10, 67, 71]\nAnother conundrum relates to the measure of privacy impact. Economic analyses as reviewed in Sect. 2 would likely resort to dollars as a measure of consumer welfare. Other risk measures such as fatal accident rate, lost-time injury or reduction in life expectancy make sense in contexts such as health or occupational safety [49].However, such measures cannot capture the wide range of harms that privacy encompasses. As Acquisti et al. argue, \u201ccosts of privacy across scenarios are arguably impossible to combine into a definitive, aggregate estimation of the \u2018loss\u2019 caused by a generalized lack of privacy\u201d [3].\nPRAM bypasses this problem by suggesting that the analyst determines \u201con a scale from 1-10 the estimated effect of each potential problem\u201d. Those values are then \u201cadded to calculate organizational impact per potential problem\u201d. Hence, PRAM assumes a linear cumulative relationship between individual problems. There is however no evidence of a linear relationship between these values, i.e., a loss of trust of magnitude 7 plus a loss of autonomy of magnitude 5 would result in an impact of magnitude 12.17 Such losses may discount or reinforce each other, leading to over and underestimation of aggregate effects. Assuming linearity also prompts questions about what the resulting impact represents, since we are combining different types of losses, i.e. what does a combined impact of magnitude 12 represent? Similarly, it also assumes\n17Moreover, such operations may not even make sense, mathematically, when using an ad-hoc scale.\nlinearity of the scale across different types of privacy problems, i.e. it assumes a dignity loss of value 5 is equally severe as a liberty loss of value 5. To the best of our knowledge, no evidence for any of these assumptions currently exists. These issues propagate on to the next task \u201cCalculate risk\u201d.\nRisk. Task 3 (\u201cCalculate risk\u201d) simply prompts the analyst to multiply likelihood and impact and add the resulting risk scores up per problematic data action. Our observations mirror those we have made above about linearity and composability. In short, PRAM computes a total score operating on individual likelihoods and impact scores in a way that is both conceptually and mathematically incorrect.\nManagement. Lastly, Task 4 (\u201cPrioritize Risk\u201d) provides suggestions for the organization to prioritize risks, as part of a risk management strategy. PRAM suggests that an organization may choose \u201cprioritization methods that provide the best communication tool for their organization and that best support decisionmaking\u201d, once again legitimizing methods that may align with the organization\u2019s bottom-line as opposed to societal good. At the same time, PRAM proposes two sample prioritization methods, both roughly implying that those actions with the highest risk score be dealt with first.\nWhile prioritizing the riskiest operations makes sense\u2014even if, at this point, we hope to have shown how the accumulation of conceptual errors and indeterminacies may render these values virtually meaningless or, worse still, the result of a biased analysis that seeks to legitimize the organization\u2019s operations\u2014 it is far from trivial that this is the right prioritization strategy or that it should simply boil down to simple math ordering or mapping, as PRAM suggests.\nThere are a number of elements that PRAM does not even surface. Firstly, the quintessential question: how much risk is too much? PRAM offers no guidance to determine whether an operation is simply too risky and not worth pursuing at all. There is no mention of balancing risks and benefits, and how to perform such balancing. Ideally, we would expect that risky operations should only be pursued if they can be justified for the greater good and, even then, minimizing the risks to the extent that is viably possible. PRAM does not address the estimation of benefit, or the need to perform a benefit-risk-cost analysis [24]. Some operations may entail risks which are too expensive to manage, other operations\u2019 benefits may be too slim. Both should dissuade an organization from carrying such operations. Equity issues resurface when we consider the distribution of risks of benefits. The benefits may be great, yet for a selected few at the expense of the majority, who face increased risks. Benefits may also be immediately enjoyable, as opposed to risks expected in the future, rendering the latter easy to discount. 18 PRAM bypasses this complexity by suggesting that organizations should manage these risks at their own discretion. The risks of trusting organizations with this kind of assessment are well known. As reported by the WSJ, Facebook would argue that the risks of using Instagram \u201care manageable and can be outweighed by the app\u2019s utility\u201d, while a mounting body of independent research may call this assessment into question [71].\n18A classic example is climate risk, where the benefits of extracting oil can be immediately enjoyed, while future risks, even if already apparent at present, are easily discounted."
        },
        {
            "heading": "4 Lessons and discussion",
            "text": "In this section we summarize and discuss some of the lessons that we have drawn from our analysis of PRAM.\nSites of discretion\nPrivacy risk assessment guidelines such as PRAM introduce multiple sites of discretion that adversarial organizations\u2014namely, those whose interests are misaligned with privacy\u2014 can use to engage in performative compliance. We have identified the following in PRAM.\nDefining privacy. It goes without saying that to assess privacy risk we need a definition of privacy, or at least some notion of what it encompasses. A narrow definition of privacy is likely to underestimate legitimate risk, while a biased, interested definition of privacy is likely to downplay risks that do not fit the analyst\u2019s agenda. The lack of consensus and standards represents a challenge for auditors, who must grapple with potentially unique definitions of privacy across organizations.\nYet this is an inescapable problem. Even under universal definitions of privacy, such as Contextual Integrity, the operationalization of privacy varies across contexts, e.g. information flows which are appropriate in one context may not be in another. As a result, there is no easy template or checklist that auditors can use to verify the validity of a PRA. A subtle understanding of contextual factors\u2014e.g. informational norms and expectations, goals and values\u2014is necessary to calculate risk. This implicitly represents an endorsement of the metaregulatory approach insofar as it acknowledges that organizations understand the context on which they operate better than the regulator. It also highlights how given too much latitude, adversarial organizations may weaponize such epistemic asymmetry. Hence, guidelines should steer analysts towards socially meaningful notions of privacy, while auditors must place extra care to scrutinize the privacy notion that organizations operationalize in their PRA.\nSystem definition. PRAM\u2019s WS2 illustrates how an organization\u2019s definition of service or system may not represent a meaningful unit of observation to assess of privacy risk. Understanding that the organization may only play a small role or represent a component of a greater system is important and likely to be overlooked. Because data is a non-rivalrous and non-excludable good, as well as the relational nature of privacy, constraining privacy risk to an organization\u2019s system or operations may lead to unreliable assessments. As we review below, each organization\u2019s operations within a larger system may pose very small risk in itself; yet altogether accrue to significant risks.\nLack of empirical evidence, actuarial models and cognitive biases. The lack of empirical evidence of privacy harms means that PRAs today are more of an art and practice than a science. They are performed based on intuition and principles rather than actuarial models. Because of this, there is a large element of subjectivity in PRAs that make this instrument particularly vulnerable to analysts\u2019 biases, whether they are steered by the organization,\ntheir own personal beliefs, or both.19 Consequently, not only are they hard to audit, but may also fall victim to auditors\u2019 capriciousness, as it is unlikely that they share the same subjectivity and values as the analyst.\nRisk accretion. Some risks may be deemed negligibly low on an individual basis. By systematically failing to address them across individuals or through time, we incur in risk accretion that steadily and insidiously erodes privacy, a sort of privacy death by a thousand cuts.20\nLinearity and composability. In addition to the lack of empirical evidence, PRAM\u2019s WS3 illustrates how baking assumptions of linearity and composability leads to pseudoscientific assessments. While risk analysis is not an exact science and often relies on approximate divide-and-conquer strategies to deal with complex systems, those assumptions must follow evidence-based model approximations. Failing to do so leads to arbitrary, meaningless assessments. As Stirling observes, \u201cunder uncertainty, attempts to assert a single aggregated picture of risk are neither rational nor \u2018science based\u2019\u201d [61].\nDistributional effects. As our analysis of PRAM\u2019s WS3 illustrates, privacy harms may not only be unevenly distributed, but the affected population may be unequally equipped to cope with those harms. Organizations are likely to prioritize their own clients or potential users.21\nBenefit-cost-analysis. Guidelines like PRAM do not address the trade-off between risk, cost and benefits inherent in risk management. In other words, such guidelines do not offer guidance as to what is too great a risk, or when benefits outweigh risks, leaving the legitimacy question unanswered. This has been a long-standing issue in the field, with Clarke observing that \u201c[q]uite simply, there is no vehicle for answering the question: \u2018Should a particular [practice] or system exist at all?\u2019\u201d [16]. The PRA approach gives organizations free rein to settle these considerations, assuming that such a comparison is somehow rational\u2014i.e. do benefits outweigh risks according to estimated values?\u2014when it is, in fact, political.\n19Early accounts reveal how external analysts cannot be realistically expected to exercise full discretion in their recommendations and become subjected to their employers\u2019 pressures. As Waters reports: \u201cClients will rarely welcome a recommendation that an entire project be taken back to the drawing board and fundamentally be re-designed [...] it is unrealistic to expect PIA practitioners to make such recommendations [75].\n20As Wilson and Crouch note, \u201cFor [most] risk analysis [...] a great deal of precision in calculation is not necessary: if large it is obvious that the risk exceeds the benefit and if negligible it is obvious that the benefit exceeds the risk\u201d [72].\n21Acquisti et al. observe that \u201cprivacy trade-offs are inherently redistributive\u201d, as decisions affect stakeholders differently, and stakeholders\u2019 interests \u201care rarely aligned\u201d. \u201cThe theoretical goal of achieving desirable societal outcomes inevitably forces policymakers to tackle thorny questions regarding whose welfare they want to prioritize. Either by intervening with regulation, or by letting market outcomes determine levels of privacy across domains, they inescapably favor one or the other stakeholder\u201d [3]."
        },
        {
            "heading": "On contextual integrity and utilitarianism.",
            "text": "PRAs encode a utilitarian stance to privacy, as an organization may legitimize its operations if it can show that the benefits outweigh the privacy risks.22\nThe theory of CI enables us to see how this may be problematic. According to CI, privacy is defined by long-standing norms and expectations about information flows in a particular context, with context understood as a social sphere with defined goals, ends and values, such as health care, the workplace, or education [45]. Informational norms are supposed to have evolved to promote such ends, goals and values, e.g. doctors ask their patients very sensitive questions under strict confidentiality conditions; confidentiality supports the overall goal of promoting patients\u2019 health. Patients are (at least in theory) reassured to speak freely and frankly about their symptoms, enabling in turn better diagnosing.\nWhile PRAs should be informed by norms and expectations, they implicitly upend them. Consider doctors performing a PRA for each patient and piece of data, rather than abiding by the principle of confidentiality. Doctors may deem benefits may outweigh a negligible risk to patients, e.g. an employee\u2019s doctor may disclose to a mistrustful employer, in exchange for a sum of money, that the employee has the flu, considering that such a disclosure would seldom cause the patient much harm. While benefits may outweigh risks in many situations, engaging in such ad-hoc analyses opens a Pandora\u2019s box, subverting long-established informational norms that support the fabric of society. This is not to say that a principled PRA cannot faithfully align with informational norms and expectations; rather, it is the multiple sites of discretion we have outlined above that threaten the legitimization of deviations from such longestablished principles, particularly in case of new technologies, where norms may not be readily apparent [45].\nIronically, informational norms underpinning CI are the result of a PRA of sorts. Many norms have been established through time by trial and error, the result of a sort of social and historical deliberation process in pursuit of certain values.23 As these norms represent the result of a subtle process of political consolidation into normative systems that purportedly support socially desirable outcomes, they encode a trade-off between the benefits and risks of certain information flows.24 Hence, we may argue that these norms are the result of an implicit socially-elicited PRA.25\n22As we have argued earlier, given the elusiveness of privacy harms, and the often partial role that an organization may play in a larger privacy-infringing system, it is easier to showcase the benefits and downplay the risks, e.g. social media platforms can trumpet the rich interactions that advertising revenue subsidizes while easily downplaying insidious privacy harms related to hypertargeting and personalization \u2014harms that we currently do not understand well in spite of worrying evidence from cases of political hypertargeting and emotional manipulation [67].\n23Confidentiality is one of the Hippocratic Oath\u2019s core principles, dating all the way back to ancient Greece.\n24Not all norms are solidly established or have a long tradition. As Nissenbaum observes, some norms are in flux, while others may be the result of recent but profound social changes. [45].\n25Nissenbaum acknowledges that CI is implicitly conservative, and that some norms may not support the ideal values in a context, resulting from asymmetrical power struggles that do not maximize social welfare [45].\nPrivacy engineering, legitimacy and the technocratization of privacy.\nPRAs embody a technocratic approach to privacy because they rely on the premise that they \u201coffer a comprehensively rigorous basis for informing decisionmaking\u201d [61], i.e. a PRA is assumed to be scientific, technical and objective; an analyst that performs a sound PRA must arrive at the correct decision and design.26 To include PRAs as part of the privacy engineering toolkit, as NIST does, further reinforces this idea. And while privacy engineers play an important role in the design of privacy-friendly systems, as amply documented elsewhere [32], portraying PRAs as a purely technical affair is both misleading and misguided. Examining a privacy engineer\u2019s role in establishing a system\u2019s legitimacy helps us illustrate why.\nIn a nutshell, a privacy engineer implements data minimization strategies. As such, privacy engineers play a key role in minimizing privacy risks, preventing mission creep and illegitimate uses of data. In assessing a system\u2019s legitimacy, let us simplify and consider two factors. Firstly, the functional purpose itself, i.e. what the system is supposed to do or its functional requirements. Secondly, its implementation, i.e. how the system is implemented, which depends on a set of non-functional requirements, such as security and privacy. On the one hand, a prima-facie legitimate system may be deemed illegitimate because its implementation causes undue privacy risks or concerns, such as e-mail services where users\u2019 messages are routinely scanned for profiling or an instant messaging service where messages are sent unencrypted, so that third parties can eavesdrop users\u2019 conversations. On the other hand, a system may be deemed illegitimate because its very purpose is privacy-infringing, such as (arguably) behavioral advertising or indiscriminate surveillance.\nA privacy engineer may redesign these systems to collect or expose the bare minimum amount of information to fulfill the desired functionality, e.g. implementing end-to-end encryption so that only sender and recipients are able to read exchanged messages. However, when the system itself poses a fundamental risk to privacy, there is nothing a privacy engineer can do to make the system legitimate. A case in point is Google\u2019s ongoing efforts to phase out thirdparty cookies, addressing the most egregious privacy concerns in how behavioral advertising is implemented to legitimize what is still arguably, an illegitimate secondary use of user data [41].\nIn short, privacy engineering enables us to mitigate privacy threats insofar as the functionality we wish to implement does not lead to privacy risks in itself. However, it is not equipped to answer whether a system is legitimate or not because that is, in essence, a political and ethical question that lies outside the realm of engineering. 27 That is why trying to pass privacy risk assessments as part of privacy engineering practice is so insidiously perilous: it could help\n26The privacy literature is littered with statements that reinforce this notion. To mention a couple of examples: NIST states that \u201c[m]easurability matters, so that agencies can demonstrate the effectiveness of privacy controls in addressing identified privacy risks\u201d [11]. Oetzel and Spierkermann\u2019s state that \u201cbecause [a PIA] offers a risk management approach that includes standardised procedures[, it] should lead to concrete technical improvements, [overcoming] the largely qualitative approach of the legal compliance domain\u201d [46]. Gellert refers to risk as \u201can instrument that allows for the transformation of uncertain future dangers into certain future dangers. Such transformation is possible because of the use of statistics and probabilities. [26]\n27That is not to say that engineering practice is not political, as a rich literature on the politics of technology has amply documented elsewhere [73].\nlegitimize, through privacy technologies, privacy invasive systems at heart."
        },
        {
            "heading": "On (collaborative) governance.",
            "text": "Our analysis raises numerous questions about governance. For those organizations whose economic imperatives are fundamentally misaligned with privacy, PRAs offer an opportunity to engage in performative compliance. Under the model of collaborative governance, organizations are meant to conduct PRAs under light supervision, even if they may be potentially subjected to audits.\nThis governance model seems to be set up to fail. In the same way that we would not trust a mining company to conduct an environmental impact assessment or a chemical plant to decide pollution levels, the lack of clear guidelines on what is too much risk and what benefits may justify it pose a challenge for governance. While the US\u2019s Environmental protection agency (EPA) sets mandatory federal standards for drinking water and air quality, there is no such standard or thresholds for privacy risk [39]. Pushing for the adoption of PRAs would require an independent body subjected to democratic authority (e.g. a DPA) to set and oversee acceptable privacy risk levels. Organizations could then run their own assessments to determine whether or not their activities fall beyond those levels. However with no empirical models on privacy harms, it is unclear that PRAs are viable and a good idea in the first place. 28\nCollaborative governance also seeks to leverage organizations\u2019 expertise, relegating the regulator to a mere auditing role. Paradoxically, the lack empirical models means that, to test an organization\u2019s assessment, auditors would likely need to perform each PRA from scratch, as they have no actuarial models to use as a reference. As recent experience with algorithmic impact assessmentss (AIAs) show, \u201c[it] requires a lot of energy to bridge the gap between getting the audit results and then translating that into accountability\u201d [44]. Hence, the purported benefit of leveraging organizations\u2019 expertise seem flimsy.\nTakeaways and future work\nOur analysis has hopefully shown that we should tame our expectations as to what we can expect from PRAs. Whereas as a heuristic or brainstorming exercise they may bring value, we should be critical and careful of how much credit we give them and how they guide our decisions. The potential for misuse is vast, while the regulator\u2019s ability to prevent such misuse is doubtful, in light of the lack of resources for regulatory agencies.\nA \u201cscience\u201d of privacy does not currently exist. This does not mean that we should not attempt to develop it, try to understand privacy harms, gather empirical evidence. Yet to the extent that our current governance approach depends on such models, we must find alternative guidance.\nAs stated earlier, CI relies on existing norms and expectations, implicitly the result of a PRA of sorts. CI encodes a preference for the precautionary principle, honoring expectations and existing principles rather than taking an all-is-up-for-grabs approach which, as similarly to Stirling\u2019s arguments in the context of human health and the environment, \u201cprovides a general normative\n28Granted that they may provide a tool to diligent and good-willing organizations to brainstorm about the privacy implications of their operations.\nguide, to the effect that policy making under uncertainty, ambiguity and ignorance should give \u2018the benefit of the doubt\u2019 to the protection of human health and the environment, rather than to competing organizational or economic interests\u201d [61]. Yet the implementation of CI is not without challanges of its own. Often values are in flux and norms not readily apparent for new technologies. Similarly, new or updated informational norms may support contextual ends and values, in which case an overhaul of entrenched norms would be in order. As Stirling argues, \u201cif what we seek [is] to remove the need for subjectivity, argument, deliberation and politics, then precaution offers no such promise. Instead, it points to a rich array of methods that reveal more explicitly and accountably the intrinsically normative and contestable basis for decisions, and the different ways in which our knowledge is so often incomplete. This is as good a \u2018rule\u2019 for decision-making, as we can reasonably get\u201d [61]. Interestingly, CI offers such a normative and contestable basis for decisions. Hence, in the spirit of CI, finding ways to integrate a deliberative framework around information flows that support contextual ends and values represents an alternative worth pursuing in the future. Stirling points to several methods that may support such an approach while avoiding the pitfalls of the pseudoscientific approach implicit in PRA [61]."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this paper we have examined privacy risk assessment (PRA) as an instrument of collaborative governance, taking NIST\u2019s privacy risk assessment methodology (PRAM) as a case study. We have shown that multiple sites of discretion and a lack of empirical models on privacy harms threaten their applicability and undermine the very premises of their utility. PRAs are touted as technical, objective, essentially technocratic means of governance, occluding the ability of adversarial organizations to use them as instruments of performative compliance.\nWe have conducted a detailed analysis of where PRA may fail or be misused, the challenges auditors face, and the existence of alternative deliberative frameworks such as Nissenbaum\u2019s theory of Contextual Integrity, that may offer a much needed counterpoint and additional tool of governance."
        }
    ],
    "year": 2023
}