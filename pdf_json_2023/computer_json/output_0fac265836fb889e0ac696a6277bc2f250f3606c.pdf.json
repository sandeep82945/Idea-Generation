{
    "abstractText": "We provide the first online algorithm for spectral hypergraph sparsification. In the online setting, hyperedges with positive weights are arriving in a stream, and upon the arrival of each hyperedge, we must irrevocably decide whether or not to include it in the sparsifier. Our algorithm produces an (\u03b5, \u03b4)-spectral sparsifier with multiplicative error \u03b5 and additive error \u03b4 that has O(\u03b5n log n log r log(1 + \u03b5W/\u03b4n)) hyperedges with high probability, where \u03b5, \u03b4 \u2208 (0, 1), n is the number of nodes, r is the rank of the hypergraph, and W is the sum of edge weights. The space complexity of our algorithm is O(n), while previous algorithms require the space complexity of \u03a9(m), where m is the number of hyperedges. This provides an exponential improvement in the space complexity since m can be exponential in n.",
    "authors": [
        {
            "affiliations": [],
            "name": "Tasuku Soma"
        }
    ],
    "id": "SP:2a03abb6e46e0428dba6ac9c7d23469c251f1c61",
    "references": [
        {
            "authors": [
                "Joshua Batson",
                "Daniel A Spielman",
                "Nikhil Srivastava",
                "Shang-Hua Teng"
            ],
            "title": "Spectral sparsification of graphs: Theory and algorithms",
            "venue": "Communications of the ACM,",
            "year": 2013
        },
        {
            "authors": [
                "Nikhil Bansal",
                "Ola Svensson",
                "Luca Trevisan"
            ],
            "title": "New notions and constructions of sparsification for graphs and hypergraphs",
            "venue": "In Proceedings of the IEEE 60th Annual Symposium on Foundations of Computer Science (FOCS),",
            "year": 2019
        },
        {
            "authors": [
                "Fan Chung",
                "Linyuan Lu"
            ],
            "title": "Concentration inequalities and martingale inequalities: a survey",
            "venue": "Internet mathematics,",
            "year": 2006
        },
        {
            "authors": [
                "T.-H. Hubert Chan",
                "Anand Louis",
                "Zhihao Gavin Tang",
                "Chenzi Zhang"
            ],
            "title": "Spectral properties of hypergraph laplacian and approximation algorithms",
            "venue": "Journal of the ACM,",
            "year": 2018
        },
        {
            "authors": [
                "Daniele Calandriello",
                "Alessandro Lazaric",
                "Michal Valko"
            ],
            "title": "Analysis of Kelner and Levin graph sparsification algorithm for a streaming setting",
            "venue": "arXiv preprint arXiv:1609.03769,",
            "year": 2016
        },
        {
            "authors": [
                "Michael B. Cohen",
                "Cameron Musco",
                "Jakub Pachocki"
            ],
            "title": "Online row sampling",
            "venue": "Theory of Computing,",
            "year": 2020
        },
        {
            "authors": [
                "Arun Jambulapati",
                "James R Lee",
                "Yang P Liu",
                "Aaron Sidford"
            ],
            "title": "Sparsifying sums of norms",
            "venue": "In Proceedings of the IEEE 64th Annual Symposium on Foundations of Computer Science (FOCS),",
            "year": 2023
        },
        {
            "authors": [
                "Arun Jambulapati",
                "Yang P Liu",
                "Aaron Sidford"
            ],
            "title": "Chaining, group leverage score overestimates, and fast spectral hypergraph sparsification",
            "venue": "In Proceedings of the 55th Annual ACM Symposium on Theory of Computing (STOC),",
            "year": 2023
        },
        {
            "authors": [
                "Michael Kapralov",
                "Robert Krauthgamer",
                "Jakab Tardos",
                "Yuichi Yoshida"
            ],
            "title": "Towards tight bounds for spectral sparsification of hypergraphs",
            "venue": "In Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing (STOC),",
            "year": 2021
        },
        {
            "authors": [
                "Michael Kapralov",
                "Robert Krauthgamer",
                "Jakab Tardos",
                "Yuichi Yoshida"
            ],
            "title": "Spectral hypergraph sparsifiers of nearly linear size",
            "venue": "In Proceedings of the 62nd IEEE Annual Symposium on Foundations of Computer Science (FOCS),",
            "year": 2022
        },
        {
            "authors": [
                "Jonathan A Kelner",
                "Alex Levin"
            ],
            "title": "Spectral sparsification in the semi-streaming setting",
            "venue": "Theory of Computing Systems,",
            "year": 2013
        },
        {
            "authors": [
                "Michael Kapralov",
                "Yin Tat Lee",
                "CN Musco",
                "Christopher Paul Musco",
                "Aaron Sidford"
            ],
            "title": "Single pass spectral sparsification in dynamic streams",
            "venue": "SIAM Journal on Computing,",
            "year": 2017
        },
        {
            "authors": [
                "James R. Lee"
            ],
            "title": "Spectral hypergraph sparsification via chaining",
            "venue": "In Proceedings of the 55th Annual ACM Symposium on Theory of Computing (STOC),",
            "year": 2023
        },
        {
            "authors": [
                "Kazusato Oko",
                "Shinsaku Sakaue",
                "Shin-ichi Tanigawa"
            ],
            "title": "Nearly tight spectral sparsification of directed hypergraphs",
            "venue": "In The proceedings of the 50th International Colloquium on Automata, Languages, and Programming (ICALP),",
            "year": 2023
        },
        {
            "authors": [
                "Akbar Rafiey",
                "Yuichi Yoshida"
            ],
            "title": "Sparsification of decomposable submodular functions",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Daniel A. Spielman",
                "Nikhil Srivastava"
            ],
            "title": "Graph sparsification by effective resistances",
            "venue": "SIAM Journal on Computing,",
            "year": 1913
        },
        {
            "authors": [
                "Daniel A Spielman",
                "Shang-Hua Teng"
            ],
            "title": "Spectral sparsification of graphs",
            "venue": "SIAM Journal on Computing,",
            "year": 2011
        },
        {
            "authors": [
                "Tasuku Soma",
                "Yuichi Yoshida"
            ],
            "title": "Spectral sparsification of hypergraphs",
            "venue": "In Proceedings of the 30th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA),",
            "year": 2019
        },
        {
            "authors": [
                "Michel Talagrand"
            ],
            "title": "Upper and lower bounds for stochastic processes",
            "year": 2014
        },
        {
            "authors": [
                "Nisheeth K. Vishnoi"
            ],
            "title": "Lx = b, laplacian solvers and their algorithmic applications",
            "venue": "Foundations and Trends\u00ae in Theoretical Computer Science,",
            "year": 2013
        }
    ],
    "sections": [
        {
            "text": "ar X\niv :2\n31 0.\n02 64\n3v 2\n[ cs\n.D S]\n7 N"
        },
        {
            "heading": "1 Introduction",
            "text": "Spectral sparsification is a cornerstone of modern algorithm design. The studies of spectral sparsification date back to the seminal work of Spielman and Teng [ST11] for undirected graphs. Let G = (V,E,w) be an undirected graph with positive edge weight w : E \u2192 R>0. Let \u03b5 \u2208 (0, 1) be an arbitrary constant. A weighted graph G\u0303 = (V,E, w\u0303) on the same node set V is called an \u03b5-spectral sparsifier of G if\n(1\u2212 \u03b5)z\u22a4LGz \u2264 z\u22a4LG\u0303z \u2264 (1 + \u03b5)z\u22a4LGz\nfor all z \u2208 RV , where w\u0303 is a nonnegative edge weight, and LG and LG\u0303 denote the Laplacian matrices of G and G\u0303, respectively. The number of nonzeros in w\u0303 is called the size of a spectral sparsifier G\u0303. Spielman Teng [ST11] showed that one can find an \u03b5-spectral sparsifier with O(\u03b5\u22122n logn) edges in nearly linear time in the size of the input graph. Since then, there has been a series of works on spectral sparsification of graphs and various applications in the design of fast algorithms; see [BSST13, Vis13] for survey.\nRecently, the notion of spectral sparsification was extended to undirected hypergraphs and has been actively studied in the literature [SY19, BST19, KKTY21, KKTY22, Lee23, JLS23]. For a weighted hypergraph H = (V,E,w) with a positive edge weight w : E \u2192 R>0, the energy function QH : RV \u2192 R of H is given by\nQH(z) := \u2211\ne\u2208E w(e) max u,v\u2208e (z(u)\u2212 z(v))2.\nThis is a generalization of the quadratic form of the Laplacian matrix of a graph. For \u03b5 \u2208 (0, 1), a weighted hypergraph H\u0303 = (V,E, w\u0303) on the same node set V is called an \u03b5-spectral sparsifier of H if\n(1\u2212 \u03b5)QH(z) \u2264 QH\u0303(z) \u2264 (1 + \u03b5)QH(z) (1)\nfor all z \u2208 RV . Again, the number of nonzeros in w\u0303 is called the size of a spectral sparsifier H\u0303 . Since the number of hyperedges can be exponential, the existence of polynomial-size spectral sparsifiers is nontrivial. This concept was first introduced by Soma and Yoshida [SY19], and they showed that there exists an \u03b5spectral sparsifier with O(\u03b5\u22122n3) hyperedges and it can be found in time polynomial in the size of the input hypergraph. The current best upper bound on the size of spectral sparsifiers is O(\u03b5\u22122n logn log r) by [Lee23, JLS23], where r is the rank of the hypergraph, i.e., the maximum size of a hyperedge.\nHowever, all known algorithms for hypergraph spectral sparsification are offline, i.e., they first store the entire hypergraph in the working memory and then construct a spectral sparsifier. This is somewhat unreasonable because the space complexity (e.g., the size of input hypergraphs) could be exponentially larger than the size of the output sparsifier. So we are naturally led to the following question: Can we construct a spectral sparsifier of hypergraphs with smaller space complexity?\nTo formalize this, we study online spectral sparsification in this paper. In the online setting, the hyperedges e1, . . . , em arrive in a stream fashion together with their weights. When edge ei arrives, we must decide immediately whether or not to include it in the sparsifier H\u0303 . Our goal is for H\u0303 to have a small number of edges and for the algorithm to use little working memory."
        },
        {
            "heading": "1.1 Our Contribution",
            "text": "We provide the first algorithm for online spectral hypergraph sparsification. We say H\u0303 is an (\u03b5, \u03b4)-spectral sparsifier of H , if (1\u2212 \u03b5)QH(z)\u2212 \u03b4\u2016z\u201622 \u2264 QH\u0303(z) \u2264 (1 + \u03b5)QH(z) + \u03b4\u2016z\u201622 for all z \u2208 RV . Our main contribution is the following.\nTheorem 1.1 (Main). There exists an online algorithm (Algorithm 1) with the following performance guarantees:\n\u2022 The amount of working memory required is O(n2) assuming the word RAM model;\n\u2022 With high probability (i.e., probability at least 1 \u2212 1/n), it finds an (\u03b5, \u03b4)-spectral sparsifier H\u0303 of a rank-r hypergraph H with\nO\n(\nn logn log r\n\u03b52 \u00b7 log\n(\n1 + \u03b5W\n\u03b4n\n))\nmany hyperedges, where W = \u2211\ne\u2208E w(e).\nRemark 1.2 (Lower Bound). We remark that the upper bound on the number of hyperedges is tight up to logarithmic factors. In fact, in [CMP20, Theorem 5.1] it is shown that even in the graph case it is necessary to sample \u2126(n log(1 + \u03b5W/(\u03b4n))/\u03b52) edges.\nWe can also obtain an \u03b5-spectral sparsifier for rank-r hypergraphs if the range of edge weights is known in advance.\nCorollary 1.3 (\u03b5-spectral sparsifier). Suppose that H = (V,E,w) is a rank-r hypergraph and that Wmin \u2264 w(e) \u2264Wmax for every e \u2208 E for some 0 < Wmin \u2264Wmax. Then, Algorithm 1 with \u03b4 = O(\u03b5W 2minn\u22122r) finds an \u03b5-spectral sparsifier with\nO\n(\nnr logn log r\n\u03b52 \u00b7 log\n(\nnWmax Wmin\n))\nhyperedges with high probability."
        },
        {
            "heading": "1.2 Our Techniques",
            "text": "We outline our algorithms below. Our starting point is the work of spectral hypergraph sparsification via generic chaining [Lee23]. He showed that if we sample each hyperedge with probability proportional to the effective resistance of an auxiliary (ordinary) graph, then the resulting hypergraph is a spectral sparsifier with\nhigh probability. Here, the auxiliary graph is a weighted clique-graph G = (V, F ), where F is the multiset of undirected edges obtained by replacing every hyperedge e \u2208 E with the clique on V (e). The weight of edge (u, v) coming from hyperedge e is given by w(e)ce,u,v , where c : F \u2192 R\u22650 is a special reweighting satisfying the following conditions [KKTY22]: For each e \u2208 E, (i)\u2211u,v\u2208e ce,u,v = 1 for e \u2208 E and (ii) ce,u,v > 0 implies ru,v = maxu\u2032,v\u2032\u2208e ru\u2032,v\u2032 , where ru,v denotes the effective resistance between u and v in G. Such a reweighting can be found by solving the following convex optimization problem\nmaximize log det\n(\n\u2211\ne\u2208E\n\u2211\nu,v\u2208e w(e)ce,u,vLuv + J\n)\nsubject to \u2211\nu,v\u2208e ce,u,v = 1 (e \u2208 E) ce,u,v \u2265 0 (e \u2208 E, u, v \u2208 e),\nwhere Luv is the Laplacian of edge (u, v) and J is the all-one matrix [Lee23]. Now we set the edge sampling probability pe \u221d w(e)maxu,v\u2208e ru,v. Using the generic chaining technique, [Lee23] showed that this yields an \u03b5-spectral sparsifier with a constant probability.\nIn the online setting, the entire hypergraph is not available, so we have to estimate the edge sampling probability on the fly. Inspired by the online row sampling algorithm [CMP20], we introduce the ridged edge sampling probability. Let \u03b7 = \u03b5/\u03b4. For i = 1, . . . ,m, we iteratively compute a sequence of auxiliary graphs Gi. Initially, G0 is the empty graph on V . For each i > 0, we construct a graph Gi from Gi\u22121 by adding an edge (u, v) with weight wici,u,v for every pair of vertices (u, v) in ei, where wi is the weight of ei and ci,u,v is an optimal solution of the following convex optimization problem:\nmaximize log det\n(\nLGi\u22121 + \u2211\nu,v\u2208ei wici,u,vLuv + \u03b7I\n)\nsubject to \u2211\nu,v\u2208ei ci,u,v = 1 ci,u,v \u2265 0 (u, v \u2208 ei)\nThen, we define the \u03b7-ridged edge sampling probability by pi \u221d maxu,v\u2208ei\u2016(LGi +\u03b7I)\u22121/2(\u03c7u\u2212\u03c7v)\u201622, where \u03c7u denotes the u-th standard unit vector. Using the techniques from [JLLS23], we show that this gives an (\u03b5, \u03b4)-spectral sparsifier having the desired number of hyperedges with high probability. Since we only need to maintain the Laplacian of Gi, the space complexity is O(n\n2) as required. Note that the above convex optimization problem can be solved by projected gradient descent (up to desired accuracy), which only requires space of linear in the dimension, i.e., O(n2)."
        },
        {
            "heading": "1.3 Related Work",
            "text": "The literature on spectral sparsification is vast. We refer the readers to [BSST13, Vis13] for technical details and various applications. Spectral sparsification of hypergraphs can be used to speed up semi-supervised learning with hypergraph regularizers and hypergraph network analysis; see discussion in [SY19, KKTY22] for further applications.\nSpectral sparsification for graphs in the semi-streaming setting is also well-studied. This is almost identical to our online setting, but the algorithm can change the weights of edges in the output that have already been sampled. Kelner and Levin [KL13] initiated this line of research and provided a natural extension of the celebrated effective resistance sampling sparsification algorithm of [SS11].1 Cohen et al. [CMP20] devised an online row sampling algorithm for general tall and skinny matrices in the online setting, which includes online\n1As pointed out in [CMP20], the original analysis has a subtle dependency issue. Later, [CLV16] provided a complete analysis of their algorithm.\nspectral sparsification of graphs. Kapralov et al. [KLM+17] devised a fully dynamic streaming algorithm for spectral sparsification of graphs, that supports both insertion and deletion.\nBeyond undirected hypergraphs, there are several works on spectral sparsification for more complex objects such as directed hypergraphs [SY19, OST23], submodular functions [RY22], and the sum of norms [JLLS23]."
        },
        {
            "heading": "2 Preliminaries",
            "text": ""
        },
        {
            "heading": "2.1 Notations",
            "text": "We use R\u22650 and R>0 to denote the set of nonnegative and positive real numbers, respectively. Given a positive integer N , we use [N ] to denote the set {1, 2, . . . , N}. All logarithms are natural logarithms unless otherwise specified. Given a function f : X \u2192 R, its support supp(f) is the set {x \u2208 X : f(x) 6= 0}. For any (finite) set X and element u \u2208 X , the vector \u03c7u \u2208 RX is the vector whose u-entry is 1 and all other entries are 0, and the vector 1X (or simply 1) is the vector whose entries are all 1. For x, x\n\u2032 \u2208 RX , x \u22a5 x\u2032 denotes \u3008x, x\u2032\u3009 = 0. For 0 \u2264 p \u2264 1, Ber(p) denotes the random variable that takes value 1 with probability p and 0 with probability 1\u2212 p."
        },
        {
            "heading": "2.2 Linear Algebra",
            "text": "Let M \u2208 Rk\u00d7k be a symmetric matrix. By the spectral theorem, M has the following decomposition M = V \u039bV \u22a4, where V is orthogonal and \u039b = diag(\u03bb1, . . . , \u03bbk) is the diagonal matrix consisting of the eigenvalues of M . It is known that \u039b is unique up to permutation. We say that M is positive definite (denoted M \u227b 0) if \u03bbi > 0 for all i, and we say that M is positive semidefinite (PSD) (denoted M 0) if \u03bbi \u2265 0 for all i.\nGiven a PSD matrix M = V \u039bV \u22a4, its pseudoinverse M \u2020 is V \u039b\u2020V \u22a4 where \u039b\u2020i,i = \u03bb \u22121 i if \u03bbi > 0 and\n\u039b\u2020i,i = 0 if \u03bbi \u2265 0. Accordingly, M \u2020/2 = \u221a M \u2020 = ( \u221a M)\u2020 where \u221a\u00b7 is the usual matrix square root defined on PSD matrices."
        },
        {
            "heading": "2.3 Hypergraphs",
            "text": "Basic definitions. A hypergraph H = (V,E,w) is defined with a vertex set V , a (multi-)set of hyperedges E = {e1, e2, . . . , e|E|}, where each ei \u2286 V and |ei| \u2265 2, and a weight function w : e \u2208 E 7\u2192 w(e) \u2208 R\u22650. We use wi as a shorthand for w(ei). Unless otherwise specified, n := |V | denotes the number of vertices and m := |E| denotes the number of hyperedges of H . Denote by r = rank(H) := maxi\u2208[m] |ei| the rank of H , i.e., the size of the largest hyperedge in H . We say that H is unweighted if wi = 1 for all i \u2208 [m].\nEnergy and sparsification. Given a hyperedge e \u2286 V , its energy Qe : RV \u2192 R is defined as:\nQe(z) :=\n[\nmax u,v\u2208e\n(z(u)\u2212 z(v)) ]2\n= max u,v\u2208e\n(z(u)\u2212 z(v))2.\nThe energy QH : R V \u2192 R of a hypergraph H is the weighted sum of its edge energies:\nQH(z) := \u2211\ni\u2208[m] wiQei(z) =\n\u2211\ni\u2208[m] wi max u,v\u2208ei (z(u)\u2212 z(v))2.\nGiven a hypergraph H = (V,E,w) and error parameters \u03b5, \u03b4 \u2265 0 (\u03b5 for relative error, \u03b4 for absolute error), an (\u03b5, \u03b4)-spectral sparsifier of H is a hypergraph H\u0303 = (V,E, w\u0303) that is supported on the hyperedge set E and its energy QH\u0303 satisfies\n(1\u2212 \u03b5)QH(z)\u2212 \u03b4z\u22a4z \u2264 QH\u0303(z) \u2264 (1 + \u03b5)QH(z) + \u03b4z\u22a4z \u2200z \u2208 RV .\nThe size of the sparsifier H\u0303 is simply | supp(w\u0303)|. The term \u201c\u03b5-spectral sparsifier\u201d refers to (\u03b5, 0)-spectral sparsifiers, i.e., additive error is not allowed. It corresponds to the well-established notion of spectral sparsifiers introduced in [ST11]."
        },
        {
            "heading": "2.4 Reweighting",
            "text": "A reweighting of a hyperedge e \u2286 V is a set of weights {cu,v}u,v\u2208e such that cu,v \u2265 0 and \u2211\nu,v\u2208e cu,v = 1. The corresponding reweighted clique-graph G is the (ordinary) graph on V with edges (u, v) having weight cu,v (and other edges having zero weight). A reweighting of a hypergraph H is the weighted sum of the reweightings of its hyperedges. The corresponding reweighted clique-graph G is the graph on V with edges (u, v) having weight w(u, v) = \u2211\ni\u2208[m]:u,v\u2208ei wici,u,v, where {ci,u,v}i\u2208[m]:u,v\u2208ei is a reweighting of the edge ei. The reason for considering reweighted clique-graphs is that the Laplacian\n[LG(z)]u := \u2211\nv\u2208V w(u, v)(z(u)\u2212 z(v))\nof an ordinary graph is a linear operator. More importantly, it is PSD. Its energy QG is then\nQG(z) := \u3008z, LG(z)\u3009 = \u2211\nu,v\u2208V\n\u2211\ni\u2208[m],u,v\u2208ei\nwici,u,v(z(u)\u2212 z(v))2 = \u2211\ni\u2208[m] wi\n\u2211\nu,v\u2208ei ci,u,v(z(u)\u2212 z(v))2.\nThe following property follows from the fact that the energy of a reweighted clique-graph of H is at most that of H .\nProposition 2.1 (Energy Comparison). Let H be a hypergraph and G be a reweighted clique-graph of H. Let x \u2208 RV such that x \u22a5 1. Then, QH(L\u2020/2G x) \u2265 \u2016x\u201622.\nProof. By the definition of a reweighting,\nQH(x) =\nm \u2211\ni=1\nwi max u,v\u2208e\n(x(u)\u2212 x(v))2\n=\nm \u2211\ni=1\n\u2211\nu,v\u2208e wici,u,v max u,v\u2208e (x(u)\u2212 x(v))2\n\u2265 m \u2211\ni=1\n\u2211\nu,v\u2208e wici,u,v(x(u)\u2212 x(v))2\n= x\u22a4LGx.\nSo, QH(L \u2020/2 G x) \u2265 \u2016x\u201622."
        },
        {
            "heading": "2.5 Generic Chaining",
            "text": "Our sparsifier H\u0303 will be an unbiased random sample of H . Since the hypergraph energies QH and QH\u0303 in (1) are not quadratic forms, i.e. not of the form z 7\u2192 z\u22a4Mz where M is a matrix, we cannot use matrix concentration inequalities to control the deviation |QH\u0303(z) \u2212 QH(z)|/QH(z) at all points z. Instead, we prove pointwise concentration and extend it to a uniform bound over the entire domain using Talagrand\u2019s generic chaining [Tal14]. We summarize here certain useful facts about generic chaining. Let (Vx)x\u2208X be a real-valued stochastic process and d be a semi-metric on X . We say (Vx)x\u2208X is a subgaussian process with respect to d if\nPr(Vx \u2212 Vy > \u03b5) \u2264 exp ( \u2212 \u03b5 2\n2d(x, y)2\n)\nfor all x, y \u2208 X and \u03b5 > 0. Talagrand\u2019s generic chaining relates the supremum of the process (Vx) with the following geometric quantity, called the \u03b3-functionals:\n\u03b32(X, d) := inf X=(Xh) sup x\u2208X\n\u2211 h\u22650 2h/2d(x,Xh),\nwhere the infimum is taken over all collections (Xh) of admissible sequences, meaning that Xh \u2286 X and |Xh| \u2264 22 h\nfor all h \u2265 0. Intuitively, X is a successively finer net over which the union bound is applied. The following lemma is key to obtaining a tail bound on the supremum of the process (Vx) and will yield\nhigh-probability success guarantees for our streaming algorithm.\nLemma 2.2 ([Tal14, Theorem 2.2.27]). Let (Vx)x\u2208X be a subgaussian process on a semi-metric space with respect to a semi-metric d and let \u2206(X, d) := supx,y\u2208X d(x, y) be the d-diameter of X. If Z = supx\u2208X |Vx|, then for any \u03bb > 0,\nlogE[e\u03bbZ ] . \u03bb2\u2206(X, d)2 + \u03bb\u03b32(X, d)."
        },
        {
            "heading": "2.6 Concentration Inequalities",
            "text": "We shall use Azuma\u2019s inequality to establish the subgaussian bound required for our chaining argument.\nProposition 2.3 (Azuma\u2019s Inequality). Let \u03c80 = 0, \u03c81, . . . , \u03c8T be a martingale. Suppose that\n|\u03c8i \u2212 \u03c8i\u22121| \u2264Mi \u2200i \u2208 [T ].\nThen,\nPr [|\u03c8T | \u2265 \u03b2] \u2264 2 exp ( \u2212\u03b22 2 \u2211\niM 2 i\n)\n.\nThe proof and the requisite background can be found in standard treatises such as [CL06]. The following special case of Chernoff bound is also useful.\nProposition 2.4 (Chernoff bound). Let X1, . . . , XT be independent Bernoulli random variables where Xi = Ber(pi). Let \u00b5 := \u2211 i pi. Then, for any M > 0,\nPr\n[\n\u2211\ni\nXi > (\u00b5+M)\n]\n\u2264 inf \u03bb>0 exp ( \u00b5(e\u03bb \u2212 1\u2212 \u03bb)\u2212M\u03bb ) ."
        },
        {
            "heading": "3 Algorithm Description",
            "text": "In order to obtain an unbiased estimator H\u0303 in this setting, our options are limited. We consider the class of algorithms where the current edge ei is sampled with probability pi, where pi depends on all edge arrivals and decisions so far. If sampled, the edge is added to the sparsifier H\u0303 with weight wi/pi.\nOur proposed algorithm, Algorithm 1, has the following features:\n\u2022 The sampling probability does not depend on previous decisions, but only on previous edge arrivals.\n\u2022 It requires maintaining a reweighted graph Gi of the hypergraph Hi at all times, and uses Gi to define sampling probabilities.\nUsing the reweighted graph Gi, the algorithm produces overestimates of the \u201cimportance\u201d of the hyperedge ei in the entire hypergraph H , using the effective resistances of the clique edges in the reweighted graph. By virtue of it being an overestimate, it is relatively easy to analyze the success probability. The difficulty lies in choosing an appropriate reweighting, so that the number of selected hyperedges remains well-controlled. This is why it is helpful to use the log-determinant potential function to guide the search for a suitable reweighting.\nAlgorithm 1 Online Hypergraph Sparsification\nInput: Hypergraph H = (V,E) given as a stream, \u03b5, \u03b4 > 0. Let \u03b7 := \u03b4/\u03b5. Initialization: let G0 be the empty graph on V , L \u03b7 0 := \u03b7In + LG0 = \u03b7In. For i = 1, . . . ,m\n1. Edge ei arrives with weight wi.\n2. Compute a reweighting ci,u,v of edge ei, so that\nlog det\n(\nL\u03b7i\u22121 + \u2211\nu,v\u2208ei wici,u,vLuv\n)\nis maximized.\n3. Let Gi be the graph obtained from Gi\u22121 by adding an edge (u, v) with weight wici,u,v for every pair of vertices (u, v) in ei, and let L \u03b7 i := \u03b7In + LGi = L \u03b7 i\u22121 + \u2211 u,v\u2208ei wici,u,vLuv.\n4. Let ri := maxu,v\u2208ei\u2016(L\u03b7i )\u22121/2(\u03c7u \u2212 \u03c7v)\u201622 be the maximum ridged effective resistance across a pair of vertices in ei.\n5. Sample edge ei with probability pi := min(1, criwi), where c = O(\u03b5 \u22122 logn log r)."
        },
        {
            "heading": "4 Analyzing the Success Probability",
            "text": "In this section, we prove that Algorithm 1 succeeds with high probability. Given a hypergraph H , let\nQ\u03b7H(z) := QH(z) + \u03b7z \u22a4z\nbe the \u03b7-ridged energy of z. We would like to control the probability that Q\u03b7 H\u0303 (z) is within a multiplicative factor of 1\u00b1 \u03b5 from Q\u03b7H(z) for all z \u2208 RV . Note that the event\nsup z:Q\u03b7\nH (z)\u22641\n|Q\u03b7 H\u0303 (z)\u2212Q\u03b7H(z)| \u2264 \u03b5\nis the same as (1\u2212 \u03b5)Q\u03b7H(z) \u2264 Q \u03b7 H\u0303 (z) \u2264 (1 + \u03b5)Q\u03b7H(z) \u2200z \u2208 RV , which by the choice of \u03b7 implies that\n(1\u2212 \u03b5)QH(z)\u2212 \u03b4z\u22a4z \u2264 QH\u0303(z) \u2264 (1 + \u03b5)QH(z) + \u03b4z\u22a4z \u2200z \u2208 RV ,\ni.e., that H\u0303 is an (\u03b5, \u03b4)-spectral sparsifier of H .\nOur plan is as follows. For the desired concentration bound, we will bound the exponential moment generating function (MGF) EH\u0303 [exp(\u03bb \u00b7 supz:Q\u03b7H(z)\u22641 |Q \u03b7 H\u0303 (z) \u2212Q\u03b7H(z)|)] of the energy discrepancy function. By Markov\u2019s inequality and a suitable choice of \u03bb, we can then conclude that Q\u03b7H and Q \u03b7 H\u0303 are \u03b5-close with high probability. The following is the main technical result of the section:\nLemma 4.1 (Exponential MGF Bound). Let H be a hypergraph stream and H\u0303 be the sampled hypergraph obtained from Algorithm 1. Let Z := supz:Q\u03b7 H (z)\u22641 |Q\u03b7H\u0303(z)\u2212Q \u03b7 H(z)|. Then, for any \u03bb > 0,\nE H\u0303 [exp(\u03bbZ)] \u2264 E H\u0303\n[\nexp\n(\n\u03bb2\nc (1 + Z) + \u03bb\n\u221a\nlogn log r\nc (1 + Z)1/2\n)]\n.\nThis is an implicit bound on the exponential MGF of Z := supz:Q\u03b7 H (z)\u22641 |Q\u03b7H\u0303(z) \u2212 Q \u03b7 H(z)| because Z itself appears in the bound. The proof of Lemma 4.1 follows closely the chaining proofs in [Lee23] and [JLLS23], with a few modifications. For completeness, we present the proof in full detail in Appendix A. In the remainder of the section, we will show how Lemma 4.1 implies that Z \u2264 \u03b5 with high probability."
        },
        {
            "heading": "4.1 High Probability Guarantee from Lemma 4.1",
            "text": "In order to obtain a high probability guarantee on the success probability, we must derive an explicit upper bound on EH\u0303 exp(\u03bbZ).\nSuppose c = (\u03ba1 logn log r)/\u03b5 2 and take \u03bb := (\u03ba2 \u221a logn log r)/\u03b5, where \u03ba1 is an absolute constant and \u03ba2\ndepends only on \u03b5. Let \u03ba := \u03ba2/ \u221a \u03ba1. We will ensure that the parameters satisfy \u03ba 2 + \u03ba \u221a logn log r \u2264 \u03bb/2. Then,\nE H\u0303 [exp(\u03bbZ)] \u2264 E H\u0303\n[\nexp\n(\n\u03bb2\nc (1 + Z) + \u03bb\n\u221a\nlogn log r\nc (1 + Z)1/2\n)]\n(Lemma 4.1)\n= E H\u0303\n[ exp ( \u03ba2(1 + Z) + \u03ba \u221a logn log r(1 + Z)1/2 )]\n\u2264 E H\u0303\n[ exp ( \u03ba2(1 + Z) + \u03ba \u221a logn log r(1 + Z) )]\n= exp ( \u03ba2 + \u03ba \u221a logn log r )\n\u00b7 E H\u0303\n[ exp ( (\u03ba2 + \u03ba \u221a logn log r)Z )]\n\u2264 exp ( \u03ba2 + \u03ba logn ) \u00b7 E H\u0303 [exp (\u03bbZ)]\n\u03ba2+\u03ba \u221a\nlog n log r \u03bb (Jensen\u2019s inequality and \u03ba 2+\u03ba \u221a logn log r \u03bb \u2264 1/2)\n\u2264 exp ( \u03ba2 + \u03ba logn ) \u00b7 E H\u0303 [exp (\u03bbZ)] 1/2 ,\nwhich resolves to E H\u0303 [exp(\u03bbZ)] \u2264 exp(2\u03ba2 + 2\u03ba logn).\nMarkov\u2019s inequality then implies that\nPr H\u0303 [Z > \u03b5] \u2264 EH\u0303 [exp(\u03bbZ)] e\u03bb\u03b5 \u2264 exp ( 2\u03ba2 + 2\u03ba logn\u2212 \u03bb\u03b5 ) = exp ( 2\u03ba2 \u2212 (\u03ba2 \u2212 2\u03ba) logn ) .\nWe can take \u03ba1 \u2265 16 and \u03ba2 . min( \u221a \u03ba1, 1/\u03b5) for the above analysis to go through, and for large enough n or small enough \u03b5 (or both) satisfying (2\u03ba2 \u2212 (\u03ba2\u2212 2\u03ba) logn) \u2264 \u2212 logn. We arrive at the following conclusion.\nLemma 4.2 (Success probability). For Algorithm 1, let Z := supz:Q\u03b7 H (z)\u22641 |Q\u03b7H\u0303(z)\u2212Q \u03b7 H(z)|. Then, we have PrH\u0303 [Z > \u03b5] \u2264 1/n. As a corollary, with probability \u2265 1\u2212 1/n, H\u0303 is an (\u03b5, \u03b4)-spectral sparsifier of H."
        },
        {
            "heading": "5 Bounding the Sample Size",
            "text": "The expected number of edges in H\u0303 is simply \u2211 i pi \u2264 \u2211\ni cwiri, but it is not easy to bound each ri directly. To bound this more easily, we use a potential function\n\u03a6i := log det (L \u03b7 i ) .\nWe will show that, every time an edge gets sampled, \u03a6i increases substantially. Then, we bound the value of \u03a6m \u2212 \u03a60, which will in turn give a bound on the number of edges.\nSince the update to LGi is no longer rank-1, but rank-|ei|, we will make use of concavity of the logdeterminant function:\nProposition 5.1. The function X 7\u2192 log det(X) is concave on the set of positive definite matrices.\nProof. Let\u2019s just check concavity over all rays X+\u03bbY , where X is positive definite, Y is a symmetric matrix and \u03bb is in a small enough open interval containing 0. We have\nlog det(X + \u03bbY )\u2212 log det(X) = log det(I + \u03bbX\u22121/2Y X\u22121/2)\n= n \u2211\ni=1\nlog(1 + \u03bbci),\nwhere ci \u2208 R are the eigenvalues of X\u22121/2Y X\u22121/2. Note that log det(X) is constant; the result then follows from the concavity of the function \u03bb 7\u2192 log(1 + \u03bbci) near \u03bb = 0 for each i.\nProposition 5.2 (Potential Increase). We have\n\u03a6i \u2212 \u03a6i\u22121 \u2265 pi log 2\nc .\nProof. Let us write L\u03b7i := L \u03b7 i\u22121 + wi \u2211\nu,v\u2208ei ci,u,vLuv. Let Ri(u, v) denote the maximum ridged effective resistance between u and v. By a KKT-condition argument similar to [Lee23, Section 3.3], an optimal solution ci,u,v satisfies that ci,u,v > 0 implies Ri(u, v) = ri := maxu\u2032,v\u2032\u2208ei Ri(u \u2032, v\u2032). Then,\n\u03a6i \u2212 \u03a6i\u22121 = log det(L\u03b7i\u22121 + wi \u2211 u,v\u2208ei ci,u,vLuv)\u2212 log det(L\u03b7i\u22121)\n\u2265 \u2211\nu,v\u2208ei ci,u,v\n(\nlog det(L\u03b7i\u22121 + wiLuv)\u2212 log det(L \u03b7 i\u22121)\n)\n(by Proposition 5.1)\n= \u2211\nu,v\u2208ei ci,u,v log det\n(\nI + wi(L \u03b7 i\u22121) \u22121Luv )\n= \u2211\nu,v\u2208ei ci,u,v log(1 + wiRi\u22121(u, v))\n\u2265 \u2211\nu,v\u2208ei ci,u,v log(1 + wiRi(u, v)) (Eff. resistance only decreases with edge addition)\n= \u2211\nu,v\u2208ei ci,u,v log(1 + wiri)\n\u2265 log ( 1 + pi c ) \u2265 pi log 2 c .\nThe final inequality uses the fact that log(1 + x) \u2265 x log 2 for x \u2208 [0, 1]. We know pi \u2264 1 and we can easily set c \u2265 1. This concludes the proof.\nLemma 5.3 (Number of sampled edges). We have\nE[|H\u0303 |] . cn log ( 1 + 2W\n\u03b7n\n)\n,\nwhere W = \u2211m\ni=1 wi. Moreover,\n|H\u0303 | . cn log ( 1 + 2W\n\u03b7n\n)\nwith probability at least 1\u2212 1/n.\nProof. We first bound the expected number of sampled edges. It follows rather straight-forwardly from Proposition 5.2.\nE[|H\u0303 |] = \u2211\ni\u2208[m] pi . c \u00b7 (\u03a6m \u2212 \u03a60)\n= c \u00b7 (log det(Lm + \u03b7I)\u2212 log det(\u03b7I)) = c \u00b7 log det(I + \u03b7\u22121Lm).\nBy the AM-GM inequality, we have\ndet(I + \u03b7\u22121Lm) = n \u220f\ni=1\n(1 + \u03b7\u22121\u03bbi(Lm)) \u2264 ( 1 + 1\n\u03b7n\nn \u2211\ni=1\n\u03bbi(Lm)\n)n\n=\n(\n1 + trLm \u03b7n\n)n\n.\nPlugging this into the previous inequality, we get\nE[|H\u0303 |] . cn \u00b7 log ( 1 + trLm \u03b7n ) .\nFinally,\ntrLm = m \u2211\ni=1\nwi \u2211 u,v\u2208ei ci,u,v tr ((\u03c7u \u2212 \u03c7v)(\u03c7u \u2212 \u03c7v)\u22a4) = 2\nm \u2211\ni=1\nwi \u2211\nu,v\u2208ei ci,u,v = 2W,\nand the desired bound on E |H\u0303 | is established. The high probability guarantee then follows from a standard application of Proposition 2.4.\nCombining Lemma 4.2 and Lemma 5.3 yields Theorem 1.1.\nProof of Corollary 1.3. Now we prove Corollary 1.3. By the hypergraph Cheeger inequality [CLTZ18], QH(x) & W 2 minn\n\u22122r\u2016x\u201622 for x \u2208 Rn with x \u22a5 1. Since \u03b4 = O(\u03b5W 2minn\u22122r), we have \u03b5QH(x) \u2265 \u03b4\u2016x\u201622. So an (\u03b5, \u03b4)-spectral sparsifier is indeed a (2\u03b5, 0)-spectral sparsifier. Therefore, Algorithm 1 outputs an (2\u03b5, 0)-spectral sparsifier with a constant probability. The expected number of hyperedges is immediate from Theorem 1.1."
        },
        {
            "heading": "6 Conclusion",
            "text": "To summarize, we designed and analyzed the first online algorithm for hypergraph spectral sparsification, showing that it uses significantly less space than the number of edges. We leave open the following questions concerning the performance of the algorithm:\nQuestion 6.1. Can we derive a matching lower bound on the space complexity of any (online) streaming algorithm for spectral hypergraph sparsification?\nQuestion 6.2. Can we improve the space complexity from O(n2) to O(nr polylogm), or even better? Such an algorithm would perform better when the rank of the hypergraph is small.\nWhile this paper focused on the insertion-only setting, the fully dynamic setting is also of interest.\nQuestion 6.3. Can we obtain an efficient fully dynamic algorithm (i.e., one that supports both hyperedge insertion and deletion) for spectral hypergraph sparsification?"
        },
        {
            "heading": "Acknowledgements",
            "text": "TS is supported by JSPS KAKENHI Grant Number JP19K20212. A part of this work was done during KT\u2019s visit to National Institute of Informatics. YY is supported by JSPS KAKENHI Grant Number JP20H05965 and JP22H05001."
        },
        {
            "heading": "A Proof of Lemma 4.1",
            "text": "A.1 Symmetrization\nChaining works best when the random variables involved are symmetric. Therefore, we first make the quantity to be controlled more symmetric. Since Q\u03b7H(z) = EH\u0302 Q \u03b7 H\u0302 (z), by Jensen\u2019s inequality we have\nE H\u0303 [exp(\u03bbZ)] \u2264 E H\u0303,H\u0302\n[\nexp\n(\n\u03bb sup Q\u03b7\nH (z)\u22641\n|Q\u03b7 H\u0303 (z)\u2212Q\u03b7 H\u0302 (z)| )] .\nHere H\u0302 is an independent copy of H\u0303 . We can write the inside term as\n\u22a4 \u2211\ni=1\nwi\n(\n\u03be\u0303i pi \u2212 \u03be\u0302i pi\n)\n\u00b7Qei(z)\nsince the \u03b7z\u22a4z terms cancel out one another. Here, \u03be\u0303i and \u03be\u0302i are independent Ber(pi) random variables corresponding to H\u0303 and H\u0302 , respectively.\nWe would like to deal with the special case pi = 1 separately. Notice that when pi = 1, \u03be\u0303i \u2212 \u03be\u0302i always equals 0. Therefore, if we set\n\u03b6\u0303i :=\n{\n\u03be\u0303i if pi < 1; 0 otherwise and \u03b6\u0302i :=\n{\n\u03be\u0302i if pi < 1;\n0 otherwise,\nthen \u03be\u0303i \u2212 \u03be\u0302i is distributed the same as \u03b6\u0303i \u2212 \u03b6\u0302i. Next, notice that \u03be\u0303i \u2212 \u03be\u0302i is symmetrically distributed, which in turn implies that \u03b6\u0303i \u2212 \u03b6\u0302i is symmetrically distributed as well. Therefore, \u03b6\u0303i \u2212 \u03b6\u0302i is distributed the same as \u03b5i(\u03b6\u0303i \u2212 \u03b6\u0302i) where \u03b5i takes values +1, \u22121 with equal probability and is independent of all other random variables. To summarize,\nE H\u0303,H\u0302\n[\nexp\n(\n\u03bb sup Q\u03b7\nH (z)\u22641\n|Q\u03b7 H\u0303 (z)\u2212Q\u03b7 H\u0302 (z)| )]\n= E H\u0303,H\u0302 E (\u03b5i)\n[\nexp\n(\n\u03bb sup Q\u03b7\nH (z)\u22641\n\u2223 \u2223 \u2223 \u2223 \u2223 m \u2211\ni=1\nwi pi\n\u00b7 \u03b5i(\u03b6\u0303i \u2212 \u03b6\u0302i) \u00b7Qei(z) \u2223 \u2223 \u2223 \u2223\n\u2223\n)]\n.\nWrite G = Gm. By triangle inequality and rearrangement inequality,\nE H\u0303,H\u0302 E (\u03b5i)\n[\nexp\n(\n\u03bb sup Q\u03b7\nH (z)\u22641\n\u2223 \u2223 \u2223 \u2223 \u2223 m \u2211\ni=1\nwi pi\n\u00b7 \u03b5i(\u03b6\u0303i \u2212 \u03b6\u0302i) \u00b7Qei(z) \u2223 \u2223 \u2223 \u2223\n\u2223\n)]\n\u2264 E H\u0303 E (\u03b5i)\n[\nexp\n(\n2\u03bb sup Q\u03b7\nH (z)\u22641\n\u2223 \u2223 \u2223 \u2223 \u2223 m \u2211\ni=1\nwi pi\n\u00b7 \u03b5i\u03b6\u0303i \u00b7Qei(z) \u2223 \u2223 \u2223 \u2223\n\u2223\n)]\n= E H\u0303 E (\u03b5i)\n[\nexp\n(\n2\u03bb sup x\u2208X\n\u2223 \u2223 \u2223 \u2223 \u2223 m \u2211\ni=1\nwi pi\n\u00b7 \u03b5i\u03b6\u0303i \u00b7Qei((L\u03b7G)\u22121/2x) \u2223 \u2223 \u2223 \u2223\n\u2223\n)]\n,\nwhere we applied the change of variables x := (L\u03b7G) 1/2z and X := {x \u2208 RV : Q\u03b7H((L \u03b7 G) \u22121/2x) \u2264 1}. Note that by Proposition 2.1, X \u2286 Bn2 where Bn2 is the closed unit ball in Rn.\nA.2 Setting up the Metric\nConsider the inner expectation\nE (\u03b5i)\n[\nexp\n(\n2\u03bb sup x\u2208X\n\u2223 \u2223 \u2223 \u2223 \u2223 m \u2211\ni=1\nwi pi\n\u00b7 \u03b5i\u03b6\u0303i \u00b7Qei((L\u03b7G)\u22121/2x) \u2223 \u2223 \u2223 \u2223\n\u2223\n)]\n.\nFor each fixed H\u0303 (which means fixing the \u03b6\u0303i\u2019s), let\nVx :=\nm \u2211\ni=1\nwi pi \u00b7 \u03b5i\u03b6\u0303i \u00b7Qei((L\u03b7G)\u22121/2x).\nWe would like to apply Proposition 2.3 to control the difference between Vx and Vy for any two points x, y \u2208 X . Set\n\u03c8i :=\ni \u2211\nj=1\nwj pj \u00b7 \u03b5j \u03b6\u0303j \u00b7 (Qej ((L\u03b7G)\u22121/2x)\u2212Qej ((L \u03b7 G) \u22121/2y)).\nNote that \u03c80 = 0 and \u03c8m = Vx \u2212 Vy , and that (\u03c8i) is a martingale. The difference |\u03c8i \u2212 \u03c8i\u22121| is always bounded by Mi := (wi/pi) \u00b7 \u03b6\u0303i \u00b7 (Qei((L\u03b7G)\u22121/2x)\u2212Qei((L \u03b7 G) \u22121/2y)). Then, Proposition 2.3 gives\nPr [|Vx \u2212 Vy| \u2265 \u03b2] \u2264 2 exp ( \u2212\u03b22 2d(x, y)2 ) ,\nwhere\nd(x, y) :=\n\u221a \u221a \u221a \u221a m \u2211\ni=1\n(\nwi pi\n)2\n\u00b7 \u03b6\u0303i 2 \u00b7 (Qei((L\u03b7G)\u22121/2x)\u2212Qei((L \u03b7 G) \u22121/2y))2.\nSince d(x, y) is the \u21132 distance between the images of x and y under a mapping, it is a semi-metric on X .\nA.3 Bounding Chaining Functional\nIn order to apply Lemma 2.2, we would like to upper bound both \u03b32(X, d) and \u2206(X, d). We first bound \u03b32(X, d). The following chaining guarantee is the key bound in [Lee23].\nProposition A.1 (Bound on \u03b32(X, d); see [Lee23, Corollary 2.13]). Suppose X \u2286 Bn2 and that a metric d(\u00b7, \u00b7) of the form\nd(x, x\u2032) :=\n(\nm \u2211\ni=1\n\u2223 \u2223\u03c6i(Ax) 2 \u2212 \u03c6i(Ax\u2032)2 \u2223 \u2223 2\n)1/2\n,\nwhere A : Rn \u2192 RM is a linear map and \u03c61, . . . , \u03c6m : RM \u2192 R are semi-norms in the form of\n\u03c6i(z) = max j\u2208Si\n\u03c9j |(Az)j |\nfor some Si \u2286 [M ] and \u03c9j \u2265 0 (j \u2208 Si). Let \u03b1 > 0 be a constant such that |\u03c6i(z)\u2212 \u03c6i(z\u2032)| \u2264 \u03b1\u2016z \u2212 z\u2032\u2016\u221e for all i \u2208 [m]. Let r = maxi\u2208[m]|Si|. Then,\n\u03b32(X, d) . \u03b1 \u221a log(M + n) \u00b7 log r\u2016A\u20162\u2192\u221e \u00b7 sup x\u2208X\n(\nm \u2211\ni=1\n\u03c6i(Ax) 2\n)1/2\n.\nIn order to apply Proposition A.1, we define A : RV \u2192 RV \u00d7V as\n(Ax)uv :=\n\u2329 x, (L\u03b7G) \u22121/2(\u03c7u \u2212 \u03c7v) \u2016(L\u03b7G)\u22121/2(\u03c7u \u2212 \u03c7v)\u20162 \u232a\nand\n\u03c6i(z) :=\n\u221a\nwi\u03b6\u0303i pi max u,v\u2208ei ( \u2016(L\u03b7G)\u22121/2(\u03c7u \u2212 \u03c7v)\u20162 \u00b7 |zuv| ) .\nThen,\n\u03c6i(Ax) 2 = wi\u03b6\u0303i pi \u00b7 max u,v\u2208ei \u2329 x, (L\u03b7G) \u22121/2(\u03c7u \u2212 \u03c7v)\n\u232a2\n= wi\u03b6\u0303i pi \u00b7Qei((L\u03b7G)\u22121/2x),\nso our metric does take the form in the proposition. Using the fact that pi = criwi when \u03b6\u0303i 6= 0, and that\nri := max u\u2032,v\u2032\u2208ei\n\u2016(L\u03b7Gi) \u22121/2(\u03c7u\u2032 \u2212 \u03c7v\u2032)\u201622 \u2265 \u2016(L\u03b7G)\u22121/2(\u03c7u \u2212 \u03c7v)\u201622,\nwe can see that |\u03c6i(z)\u2212\u03c6i(z\u2032)| \u2264 1/ \u221a c \u00b7 \u2016z\u2212 z\u2032\u2016\u221e. Next, since each row of Ax is formed by taking the inner product of x with a unit vector, we have that \u2016A\u20162\u2192\u221e \u2264 1. Since M = |V \u00d7 V | = n2, Proposition A.1 then gives\n\u03b32(X, d) .\n\u221a\nlogn log r\nc \u00b7 sup x\u2208X\n(\nm \u2211\ni=1\nwi\u03b6\u0303i pi Qei((L \u03b7 G) \u22121/2x)\n)1/2\n\u2264 \u221a logn log r\nc \u00b7 sup z:QH (z)\u22641 QH\u0303(z) 1/2.\nA.4 Bounding Diameter\nNext, we upper bound the diameter \u2206(X, d), which amounts to upper bounding 2 supx\u2208X d(x,~0). Indeed,\nsup x\u2208X d(x,~0) = sup x\u2208X\n\u221a \u221a \u221a \u221a m \u2211\ni=1\n(\nwi pi\n)2\n\u00b7 \u03b6\u0303i 2 \u00b7 (Qei((L\u03b7G)\u22121/2x))2\n\u2264 sup x\u2208X\n\u221a \u221a \u221a \u221a (\nsup i\u2208[m] wi\u03b6\u0303i pi\n\u00b7Qei((L\u03b7G)\u22121/2x) ) \u00b7 ( m \u2211\ni=1\nwi\u03b6\u0303i pi Qei((L \u03b7 G) \u22121/2x)\n)\n\u2264\n\u221a \u221a \u221a \u221a (\nsup x\u2208X sup i\u2208[m] wi\u03b6\u0303i pi \u00b7 sup u,v\u2208ei\n\u3008x, (L\u03b7G)\u22121/2(\u03c7u \u2212 \u03c7v)\u30092 )\n\u00b7 sup z:QH(z)\u22641 QH\u0303(z)\n\u2264 1\u221a c \u00b7 sup z:QH (z)\u22641 QH\u0303(z) 1/2,\nwhere we used the fact that pi = cwiri if \u03b6\u0303i 6= 0 and\n\u3008x, (L\u03b7G)\u22121/2(\u03c7u \u2212 \u03c7v)\u30092 \u2264 \u2016x\u201622 \u00b7 \u2016(L \u03b7 G) \u22121/2(\u03c7u \u2212 \u03c7v)\u201622 \u2264 \u2016(L\u03b7G)\u22121/2(\u03c7u \u2212 \u03c7v)\u201622 \u2264 ri.\nThe second inequality is because X \u2286 Bn2 and the third inequality is because ri \u2265 \u2016(L\u03b7Gi)\u22121/2(\u03c7u \u2212\u03c7v)\u201622 \u2265 \u2016(L\u03b7G)\u22121/2(\u03c7u \u2212 \u03c7v)\u201622 for all i \u2208 [m] and u, v \u2208 ei.\nWe thus conclude that \u2206(X, d) \u2264 O(1/\u221ac).\nA.5 Conclusion\nFor each fixed H\u0303 , apply Lemma 2.2 with the previously obtained upper bounds on \u03b32(X, d) and \u2206(X, d), noting that\nsup z:QH(z)\u22641\nQH\u0303(z) 1/2 \u2264\n(\n1 + sup z:QH (z)\u22641\n|QH(z)\u2212QH\u0303(z)| )1/2 = (1 + Z)1/2.\nNow, apply the outer expectation over H\u0303, and Lemma 4.1 follows."
        }
    ],
    "title": "Online Algorithms for Spectral Hypergraph Sparsification",
    "year": 2023
}