{
    "abstractText": "7 The hippocampal subfield CA3 is thought to function as an autoassociative network that stores 8 experiences as memories. Information from these experiences arrives via the entorhinal cortex (EC), 9 which projects to CA3 directly as well as indirectly through the dentate gyrus (DG). DG sparsifies and 10 decorrelates the information before also projecting to CA3. The computational purpose for receiving 11 two encodings of the same sensory information has not been firmly established. We model CA3 as a 12 Hopfield-like network that stores both correlated and decorrelated encodings and retrieves them at low 13 and high inhibitory tone, respectively. As more memories are stored, the dense, correlated encodings 14 merge along shared features while the sparse, decorrelated encodings remain distinct. In this way, the 15 model learns to transition between concept and example representations by controlling inhibitory tone. To 16 experimentally test for the presence of these complementary encodings, we analyze the theta-modulated 17 tuning of place cells in rat CA3. In accordance with our model\u2019s prediction, these neurons exhibit more 18 precise spatial tuning and encode more detailed task features during theta phases with sparser activity. 19 Finally, we generalize the model beyond hippocampal architecture and find that feedforward neural 20 networks trained in multitask learning benefit from a novel loss term that promotes hybrid encoding 21 using correlated and decorrelated representations. Thus, the complementary encodings that we have 22 found in CA3 can provide broad computational advantages for solving complex tasks. 23",
    "authors": [
        {
            "affiliations": [],
            "name": "Louis Kang"
        },
        {
            "affiliations": [],
            "name": "Taro Toyoizumi"
        }
    ],
    "id": "SP:ead3554ba6f45441cfb038db4fd787e96062650e",
    "references": [
        {
            "authors": [
                "J.R. Anderson"
            ],
            "title": "The adaptive nature of human categorization",
            "venue": "Psychol. Rev.,",
            "year": 1991
        },
        {
            "authors": [
                "F.G. Ashby",
                "W.T. Maddox"
            ],
            "title": "Human category learning",
            "venue": "Annu. Rev. Psychol.,",
            "year": 2005
        },
        {
            "authors": [
                "S.M. Attili",
                "M.F.M. Silva",
                "T.-v. Nguyen",
                "G.A. Ascoli"
            ],
            "title": "Cell numbers, distribution, shape, and regional variation 748 throughout the murine hippocampal formation from the adult brain Allen Reference Atlas",
            "venue": "Brain Struct. Funct.,",
            "year": 2019
        },
        {
            "authors": [
                "A.R. Backus",
                "J.-M. Schoffelen",
                "S. Szeb\u00e9nyi",
                "S. Hanslmayr",
                "C.F. Doeller"
            ],
            "title": "Hippocampal-prefrontal theta oscilla- 751 tions support memory integration",
            "venue": "Curr. Biol.,",
            "year": 2016
        },
        {
            "authors": [
                "Y. Bengio",
                "N. L\u00e9onard",
                "A. Courville"
            ],
            "title": "Estimating or propagating gradients through stochastic neurons for 753 conditional computation",
            "venue": "arXiv 1308.3432,",
            "year": 2013
        },
        {
            "authors": [
                "S.D. Berry",
                "R.F. Thompson"
            ],
            "title": "Prediction of learning rate from the hippocampal",
            "venue": "electroencephalogram. Science,",
            "year": 1978
        },
        {
            "authors": [
                "G.-q. Bi",
                "M.-m. Poo"
            ],
            "title": "Synaptic modifications in cultured hippocampal neurons: Dependence on spike timing, 757 synaptic strength, and postsynaptic cell",
            "venue": "type. J. Neurosci.,",
            "year": 1998
        },
        {
            "authors": [
                "M. Borzello",
                "S. Ramirez",
                "A. Treves",
                "I. Lee",
                "H. Scharfman",
                "C. Stark",
                "J.J. Knierim",
                "L.M. Rangel"
            ],
            "title": "Assessments of 759 dentate gyrus function: discoveries and debates",
            "venue": "Nat. Rev. Neurosci.,",
            "year": 2023
        },
        {
            "authors": [
                "C.R. Bowman",
                "D. Zeithamova"
            ],
            "title": "Abstract memory representations in the ventromedial prefrontal cortex and 761 hippocampus support concept generalization",
            "venue": "J. Neurosci.,",
            "year": 2018
        },
        {
            "authors": [
                "M. Carandini",
                "D.J. Heeger"
            ],
            "title": "Normalization as a canonical neural computation",
            "venue": "Nat. Rev. Neurosci.,",
            "year": 2012
        },
        {
            "authors": [
                "N.A. Cayco-Gajic",
                "C. Clopath",
                "R.A. Silver"
            ],
            "title": "Sparse synaptic connectivity is required for decorrelation and 765 pattern separation in feedforward networks",
            "venue": "Nat. Commun.,",
            "year": 2017
        },
        {
            "authors": [
                "Y. Chen",
                "D. Paiton",
                "B. Olshausen"
            ],
            "title": "The Sparse Manifold Transform",
            "venue": "Advances in Neural Information Processing",
            "year": 2018
        },
        {
            "authors": [
                "M. Cogswell",
                "F. Ahmed",
                "R. Girshick",
                "L. Zitnick",
                "D. Batra"
            ],
            "title": "Reducing overfitting in deep networks by decorrelating 769 representations",
            "venue": "arXiv 1511.06068,",
            "year": 2015
        },
        {
            "authors": [
                "L.L. Colgin"
            ],
            "title": "Mechanisms and functions of theta rhythms",
            "venue": "Annu. Rev. Neurosci.,",
            "year": 2013
        },
        {
            "authors": [
                "N.V. Covington",
                "S. Brown-Schmidt",
                "M.C. Duff"
            ],
            "title": "The necessity of the hippocampus for statistical learning",
            "venue": "J. 772 Cognit. Neurosci.,",
            "year": 2018
        },
        {
            "authors": [
                "N.M. Dotson",
                "M.M. Yartsev"
            ],
            "title": "Nonlocal spatiotemporal representation in the hippocampus of freely flying bats",
            "year": 2021
        },
        {
            "authors": [
                "M.C. Duff",
                "N.V. Covington",
                "C. Hilverman",
                "N.J. Cohen"
            ],
            "title": "Semantic memory and the hippocampus: Revisiting, 776 reaffirming, and extending the reach of their critical relationship",
            "venue": "Front. Hum. Neurosci.,",
            "year": 2020
        },
        {
            "authors": [
                "\u00c9. Duvelle",
                "R.M. Grieves",
                "M.A. van der Meer"
            ],
            "title": "Temporal context and latent state inference in the hippocampal 778 splitter",
            "venue": "signal. eLife,",
            "year": 2023
        },
        {
            "authors": [
                "E. Engin",
                "E.D. Zarnowska",
                "D. Benke",
                "E. Tsvetkov",
                "M. Sigal",
                "R. Keist",
                "V.Y. Bolshakov",
                "R.A. Pearce",
                "U. Rudolph"
            ],
            "title": "Tonic inhibitory control of dentate gyrus granule cells by \u03b15-containing GABAA receptors reduces memory inter- 781 ference",
            "venue": "The Journal of Neuroscience,",
            "year": 2015
        },
        {
            "authors": [
                "K.A. Ferguson",
                "J.A. Cardin"
            ],
            "title": "Mechanisms underlying gain modulation in the cortex",
            "venue": "Nat. Rev. Neurosci.,",
            "year": 2020
        },
        {
            "authors": [
                "J.F. Fontanari"
            ],
            "title": "Generalization in a Hopfield network",
            "venue": "J. Phys.,",
            "year": 1990
        },
        {
            "authors": [
                "L.M. Frank",
                "E.N. Brown",
                "M. Wilson"
            ],
            "title": "Trajectory encoding in the hippocampus and entorhinal",
            "venue": "cortex. Neuron,",
            "year": 2000
        },
        {
            "authors": [
                "M.E. Hasselmo"
            ],
            "title": "The role of acetylcholine in learning and memory",
            "venue": "Curr. Opin. Neurobiol.,",
            "year": 2006
        },
        {
            "authors": [
                "M.E. Hasselmo",
                "C. Bodeln",
                "B.P. Wyble"
            ],
            "title": "A proposed function for hippocampal theta rhythm: Separate phases 789 of encoding and retrieval enhance reversal of prior learning",
            "venue": "Neural Comput.,",
            "year": 2002
        },
        {
            "authors": [
                "D.A. Henze",
                "L. Wittner",
                "G. Buzs\u00e1ki"
            ],
            "title": "Single granule cells reliably discharge targets in the hippocampal CA3 791 network in vivo",
            "venue": "Nat. Neurosci.,",
            "year": 2002
        },
        {
            "authors": [
                "N.A. Herweg",
                "E.A. Solomon",
                "M.J. Kahana"
            ],
            "title": "Theta oscillations in human memory",
            "venue": "Trends in Cognitive Sciences,",
            "year": 2020
        },
        {
            "authors": [
                "J.J. Hopfield"
            ],
            "title": "Neural networks and physical systems with emergent collective computational abilities",
            "venue": "Proc. Natl. 795 Acad. Sci. U.S.A.,",
            "year": 1982
        },
        {
            "authors": [
                "J. Isaacson",
                "M. Scanziani"
            ],
            "title": "How inhibition shapes cortical activity",
            "venue": "Neuron, 72(2):231\u2013243,",
            "year": 2011
        },
        {
            "authors": [
                "J. Jacobs",
                "G. Hwang",
                "T. Curran",
                "M.J. Kahana"
            ],
            "title": "EEG oscillations and recognition memory: Theta correlates of 798 memory retrieval and decision making",
            "year": 2006
        },
        {
            "authors": [
                "P. Kaifosh",
                "A. Losonczy"
            ],
            "title": "Mnemonic functions for nonlinear dendritic integration in hippocampal pyramidal 800",
            "venue": "circuits. Neuron,",
            "year": 2016
        },
        {
            "authors": [
                "L. Kang",
                "M.R. DeWeese"
            ],
            "title": "Replay as wavefronts and theta sequences as bump oscillations in a grid cell attractor 802",
            "venue": "network. eLife,",
            "year": 2019
        },
        {
            "authors": [
                "L. Kang",
                "T. Toyoizumi"
            ],
            "title": "A Hopfield-like model with complementary encodings of memories",
            "venue": "arXiv 2302.04481,",
            "year": 2023
        },
        {
            "authors": [
                "M. Karlsson",
                "M. Carr",
                "L.M. Frank"
            ],
            "title": "Simultaneous extracellular recordings from hippocampal areas CA1 and 806 CA3 (or MEC and CA1) from rats performing an alternation task in two W-shapped tracks that are geometrically 807 identically but visually distinct",
            "venue": "CRCNS.org,",
            "year": 2015
        },
        {
            "authors": [
                "K. Kay",
                "J.E. Chung",
                "M. Sosa",
                "J.S. Schor",
                "M.P. Karlsson",
                "M.C. Larkin",
                "D.F. Liu",
                "L.M. Frank"
            ],
            "title": "Constant 809 sub-second cycling between representations of possible futures in the hippocampus",
            "year": 2020
        },
        {
            "authors": [
                "R. Kempter",
                "C. Leibold",
                "G. Buzs\u00e1ki",
                "K. Diba",
                "R. Schmidt"
            ],
            "title": "Quantifying circular\u2013linear associations: Hippocampal 811 phase precession",
            "venue": "J. Neurosci. Methods,",
            "year": 2012
        },
        {
            "authors": [
                "S. Kim",
                "S.J. Guzman",
                "H. Hu",
                "P. Jonas"
            ],
            "title": "Active dendrites support efficient initiation of dendritic spikes in 813 hippocampal CA3 pyramidal neurons",
            "venue": "Nat. Neurosci.,",
            "year": 2012
        },
        {
            "authors": [
                "B.J. Knowlton",
                "L.R. Squire"
            ],
            "title": "The learning of categories: Parallel brain systems for item memory and category 815 knowledge",
            "year": 1993
        },
        {
            "authors": [
                "G. Kowadlo",
                "A. Ahmed",
                "D. Rawlinson"
            ],
            "title": "AHA! an \u2019Artificial Hippocampal Algorithm\u2019 for episodic machine 817 learning",
            "year": 1909
        },
        {
            "authors": [
                "A. Krizhevsky",
                "G. Hinton"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "venue": "Technical Report 0,",
            "year": 2009
        },
        {
            "authors": [
                "S. Kunec",
                "M.E. Hasselmo",
                "N. Kopell"
            ],
            "title": "Encoding and retrieval in the CA3 region of the hippocampus: A model 821 of theta-phase separation",
            "venue": "J. Neurophysiol.,",
            "year": 2005
        },
        {
            "authors": [
                "Q.V. Le",
                "A. Karpenko",
                "J. Ngiam",
                "A.Y. Ng"
            ],
            "title": "ICA with reconstruction cost for efficient overcomplete feature 823 learning",
            "venue": "Adv. Neural Inf. Process. Syst",
            "year": 2011
        },
        {
            "authors": [
                "Y. LeCun",
                "L. Bottou",
                "Y. Bengio",
                "P. Haffner"
            ],
            "title": "Gradient-based learning applied to document recognition",
            "venue": "Proc. 825 IEEE,",
            "year": 1998
        },
        {
            "authors": [
                "H. Lee",
                "A. Battle",
                "R. Raina",
                "A.Y. Ng"
            ],
            "title": "Efficient sparse coding algorithms",
            "venue": "Adv. Neural Inf. Process. Syst.,",
            "year": 2006
        },
        {
            "authors": [
                "B.C. Lega",
                "J. Jacobs",
                "M. Kahana"
            ],
            "title": "Human hippocampal theta oscillations and the formation of episodic memories",
            "venue": "Hippocampus,",
            "year": 2012
        },
        {
            "authors": [
                "L.S. Leung",
                "C.S.H. Law"
            ],
            "title": "Phasic modulation of hippocampal synaptic plasticity by theta rhythm",
            "venue": "Behav. 832 Neurosci.,",
            "year": 2020
        },
        {
            "authors": [
                "J.K. Leutgeb",
                "S. Leutgeb",
                "M.-B. Moser",
                "E.I. Moser"
            ],
            "title": "Pattern separation in the dentate gyrus and CA3 of the 834 hippocampus",
            "year": 2007
        },
        {
            "authors": [
                "B.C. Love",
                "D.L. Medin",
                "T.M. Gureckis"
            ],
            "title": "SUSTAIN: A network model of category learning",
            "venue": "Psychol. Rev.,",
            "year": 2004
        },
        {
            "authors": [
                "M.L. Mack",
                "B.C. Love",
                "A.R. Preston"
            ],
            "title": "Dynamic updating of hippocampal object representations reflects new 838 conceptual knowledge",
            "venue": "Proc. Natl. Acad. Sci. U.S.A.,",
            "year": 2016
        },
        {
            "authors": [
                "J. Makara",
                "J. Magee"
            ],
            "title": "Variable dendritic integration in hippocampal CA3",
            "venue": "pyramidal neurons. Neuron,",
            "year": 2013
        },
        {
            "authors": [
                "J.R. Manns",
                "R.O. Hopkins",
                "L.R. Squire"
            ],
            "title": "Semantic memory and the human",
            "venue": "hippocampus. Neuron,",
            "year": 2003
        },
        {
            "authors": [
                "D. Marr"
            ],
            "title": "Simple memory: a theory for archicortex",
            "venue": "Philos. Trans. R. Soc. B,",
            "year": 1971
        },
        {
            "authors": [
                "C.J. McAdams",
                "J.H.R. Maunsell"
            ],
            "title": "Effects of attention on orientation-tuning functions of single neurons in 846 Macaque cortical area V4",
            "venue": "J. Neurosci.,",
            "year": 1999
        },
        {
            "authors": [
                "J.L. McClelland",
                "N.H. Goddard"
            ],
            "title": "Considerations arising from a complementary learning systems perspective on 848 hippocampus and neocortex. Hippocampus",
            "year": 1996
        },
        {
            "authors": [
                "B. McNaughton",
                "R. Morris"
            ],
            "title": "Hippocampal synaptic enhancement and information storage within a distributed 850 memory system",
            "venue": "Trends Neurosci.,",
            "year": 1987
        },
        {
            "authors": [
                "M.R. Mehta",
                "A.K. Lee",
                "M.A. Wilson"
            ],
            "title": "Role of experience and oscillations in transforming a rate code into a 852 temporal code",
            "year": 2002
        },
        {
            "authors": [
                "R.K. Mishra",
                "S. Kim",
                "S.J. Guzman",
                "P. Jonas"
            ],
            "title": "Symmetric spike timing-dependent plasticity at CA3\u2013CA3 854 synapses optimizes storage and recall in autoassociative networks",
            "venue": "Nat. Commun.,",
            "year": 2016
        },
        {
            "authors": [
                "K. Mizuseki",
                "G. Buzs\u00e1ki"
            ],
            "title": "Preconfigured, skewed distribution of firing rates in the hippocampus and entorhinal 856 cortex",
            "venue": "Cell Rep.,",
            "year": 2013
        },
        {
            "authors": [
                "K. Mizuseki",
                "A. Sirota",
                "E. Pastalkova",
                "K. Diba",
                "G. Buzs\u00e1ki"
            ],
            "title": "Multiple single unit recordings from different rat 858 hippocampal and entorhinal regions while the animals",
            "year": 2013
        },
        {
            "authors": [
                "K. Mizuseki",
                "K. Diba",
                "E. Pastalkova",
                "J. Teeters",
                "A. Sirota",
                "G. Buzs\u00e1ki"
            ],
            "title": "Neurosharing: large-scale data sets (spike, 861 LFP) recorded from the hippocampal-entorhinal system in behaving rats",
            "year": 2014
        },
        {
            "authors": [
                "T.C. Murakami",
                "T. Mano",
                "S. Saikawa",
                "S.A. Horiguchi",
                "D. Shigeta",
                "K. Baba",
                "H. Sekiya",
                "Y. Shimizu",
                "K.F. Tanaka",
                "863 H. Kiyonari",
                "M. Iino",
                "H. Mochizuki",
                "K. Tainaka",
                "H.R. Ueda"
            ],
            "title": "A three-dimensional single-cell-resolution 864 whole-brain atlas using CUBIC-X expansion microscopy and tissue clearing",
            "venue": "Nat. Neurosci.,",
            "year": 2018
        },
        {
            "authors": [
                "Y. Norman",
                "O. Raccah",
                "S. Liu",
                "J. Parvizi",
                "R. Malach"
            ],
            "title": "Hippocampal ripples and their coordinated dialogue with 866 the default mode network during recent and remote recollection",
            "year": 2021
        },
        {
            "authors": [
                "J. O\u2019Keefe",
                "M.L. Recce"
            ],
            "title": "Phase relationship between hippocampal place units and the EEG theta rhythm",
            "venue": "Hippocampus,",
            "year": 1993
        },
        {
            "authors": [
                "B.A. Olshausen",
                "D.J. Field"
            ],
            "title": "Emergence of simple-cell receptive field properties by learning a sparse code for 870 natural images",
            "year": 1996
        },
        {
            "authors": [
                "B.A. Olshausen",
                "D.J. Field"
            ],
            "title": "Sparse coding of sensory inputs",
            "venue": "Curr. Opin. Neurobiol.,",
            "year": 2004
        },
        {
            "authors": [
                "R.C. O\u2019Reilly",
                "J.L. McClelland"
            ],
            "title": "Hippocampal conjunctive encoding, storage, and recall: Avoiding a trade-off",
            "venue": "Hippocampus,",
            "year": 1994
        },
        {
            "authors": [
                "R.C. O\u2019Reilly",
                "J.W. Rudy"
            ],
            "title": "Conjunctive representations in learning and memory: Principles of cortical and 875 hippocampal function",
            "venue": "Psychol. Rev.,",
            "year": 2001
        },
        {
            "authors": [
                "G.I. Parisi",
                "R. Kemker",
                "J.L. Part",
                "C. Kanan",
                "S. Wermter"
            ],
            "title": "Continual lifelong learning with neural networks: A 877 review",
            "venue": "Neural Networks,",
            "year": 2019
        },
        {
            "authors": [
                "S.C. Penley",
                "J.R. Hinman",
                "L.L. Long",
                "E.J. Markus",
                "M.A. Escab\u0301\u0131",
                "J.J. Chrobak"
            ],
            "title": "Novel space alters theta and 879 gamma synchrony across the longitudinal axis of the hippocampus",
            "venue": "Front. Syst. Neurosci.,",
            "year": 2013
        },
        {
            "authors": [
                "B.E. Pfeiffer",
                "D.J. Foster"
            ],
            "title": "Autoassociative dynamics in the generation of sequences of hippocampal place cells",
            "year": 2015
        },
        {
            "authors": [
                "X. Pitkow",
                "M. Meister"
            ],
            "title": "Decorrelation and efficient coding by retinal ganglion cells",
            "venue": "Nat. Neurosci.,",
            "year": 2012
        },
        {
            "authors": [
                "S.E. Qasim",
                "I. Fried",
                "J. Jacobs"
            ],
            "title": "Phase precession in the human hippocampus and entorhinal",
            "venue": "cortex. Cell,",
            "year": 2021
        },
        {
            "authors": [
                "R. Quian Quiroga",
                "L. Reddy",
                "G. Kreiman",
                "C. Koch",
                "I. Fried"
            ],
            "title": "Invariant visual representation by single neurons 887 in the human brain",
            "year": 2005
        },
        {
            "authors": [
                "R. Quian Quiroga",
                "A. Kraskov",
                "C. Koch",
                "I. Fried"
            ],
            "title": "Explicit encoding of multimodal percepts by single neurons in 889 the human brain",
            "venue": "Curr. Biol.,",
            "year": 2009
        },
        {
            "authors": [
                "L. Reddy",
                "M.W. Self",
                "B. Zoefel",
                "M. Poncet",
                "J.K. Possel",
                "J.C. Peters",
                "J.C. Baayen",
                "S. Idema",
                "R. VanRullen",
                "891 P.R. Roelfsema"
            ],
            "title": "Theta-phase dependent neuronal coding during sequence learning in human single neurons",
            "year": 2021
        },
        {
            "authors": [
                "E.T. Rolls",
                "R.P. Kesner"
            ],
            "title": "A computational theory of hippocampal function, and empirical tests of the theory",
            "venue": "Prog. Neurobiol.,",
            "year": 2006
        },
        {
            "authors": [
                "G. Rosen",
                "A. Williams",
                "J. Capra",
                "M. Connolly",
                "B. Cruz",
                "L. Lu",
                "D. Airey",
                "K. Kulkarni",
                "R. Williams"
            ],
            "title": "The Mouse 896 Brain Library @ www.mbl.org",
            "venue": "Int Mouse Genome Conference,",
            "year": 2000
        },
        {
            "authors": [
                "A.C. Schapiro",
                "E. Gregory",
                "B. Landau",
                "M. McCloskey",
                "N.B. Turk-Browne"
            ],
            "title": "The necessity of the medial temporal 898 lobe for statistical learning",
            "venue": "J. Cognit. Neurosci.,",
            "year": 2014
        },
        {
            "authors": [
                "A.C. Schapiro",
                "N.B. Turk-Browne",
                "M.M. Botvinick",
                "K.A. Norman"
            ],
            "title": "Complementary learning systems within 900 the hippocampus: a neural network modelling approach to reconciling episodic memory with statistical learning",
            "venue": "Philos. Trans. R. Soc. B,",
            "year": 2017
        },
        {
            "authors": [
                "W.B. Scoville",
                "B. Milner"
            ],
            "title": "Loss of recent memory after bilateral hippocampal lesions",
            "venue": "J. Neurol. Neurosurg. 903 Psychiatry,",
            "year": 1957
        },
        {
            "authors": [
                "M.A. Seager",
                "L.D. Johnson",
                "E.S. Chabot",
                "Y. Asaka",
                "S.D. Berry"
            ],
            "title": "Oscillatory brain states and learning: Impact 905 of hippocampal theta-contingent training",
            "venue": "Proc. Natl. Acad. Sci. U.S.A.,",
            "year": 2002
        },
        {
            "authors": [
                "J.H. Siegle",
                "M.A. Wilson"
            ],
            "title": "Enhancement of encoding and retrieval functions through theta phase-specific 907 manipulation of hippocampus",
            "venue": "eLife, 3:e03061,",
            "year": 2014
        },
        {
            "authors": [
                "W.E. Skaggs",
                "B.L. McNaughton",
                "K.M. Gothard",
                "E.J. Markus"
            ],
            "title": "An information-theoretic approach to deciphering 909 the hippocampal code",
            "venue": "Adv. Neural Inf. Process. Syst.,",
            "year": 1993
        },
        {
            "authors": [
                "W.E. Skaggs",
                "B.L. McNaughton",
                "M.A. Wilson",
                "C.A. Barnes"
            ],
            "title": "Theta phase precession in hippocampal neuronal 911 populations and the compression of temporal sequences. Hippocampus",
            "year": 1996
        },
        {
            "authors": [
                "B.C. Souza",
                "A.B.L. Tort"
            ],
            "title": "Asymmetry of the temporal code for space by hippocampal place cells",
            "venue": "Sci. Rep.,",
            "year": 2017
        },
        {
            "authors": [
                "L.R. Squire"
            ],
            "title": "Memory and the hippocampus: A synthesis from findings with rats, monkeys, and humans",
            "venue": "Psychol. 915 Rev.,",
            "year": 1992
        },
        {
            "authors": [
                "J. Su\u010devi\u0107",
                "A.C. Schapiro"
            ],
            "title": "A neural network model of hippocampal contributions to category learning",
            "venue": "bioRxiv",
            "year": 2022
        },
        {
            "authors": [
                "V. Sze",
                "Y.-H. Chen",
                "T.-J. Yang",
                "J.S. Emer"
            ],
            "title": "Efficient processing of deep neural networks: A tutorial and survey",
            "venue": "Proc. IEEE,",
            "year": 2017
        },
        {
            "authors": [
                "S. Terada",
                "Y. Sakurai",
                "H. Nakahara",
                "S. Fujisawa"
            ],
            "title": "Temporal and rate coding for discrete event sequences in the 921",
            "venue": "hippocampus. Neuron,",
            "year": 2017
        },
        {
            "authors": [
                "T.J. Teyler",
                "P. DiScenna"
            ],
            "title": "The hippocampal memory indexing theory",
            "venue": "Behav. Neurosci.,",
            "year": 1986
        },
        {
            "authors": [
                "A. Treves",
                "E.T. Rolls"
            ],
            "title": "Computational constraints suggest the need for two distinct input systems to the hip- 926 pocampal CA3",
            "venue": "network. Hippocampus,",
            "year": 1992
        },
        {
            "authors": [
                "M.V. Tsodyks",
                "M.V. Feigel\u2019man"
            ],
            "title": "The enhanced storage capacity in neural networks with low activity level",
            "venue": "Europhys. Lett.,",
            "year": 1988
        },
        {
            "authors": [
                "B.B. Ujfalussy",
                "G. Orb\u00e1n"
            ],
            "title": "Sampling motion trajectories during hippocampal theta sequences",
            "venue": "eLife, 11:e74058,",
            "year": 2022
        },
        {
            "authors": [
                "W.E. Vinje",
                "J.L. Gallant"
            ],
            "title": "Sparse coding and decorrelation in primary visual cortex during natural vision",
            "year": 2000
        },
        {
            "authors": [
                "N.P. Vyleta",
                "C. Borges-Merjane",
                "P. Jonas"
            ],
            "title": "Plasticity-dependent, full detonation at hippocampal mossy fiber\u2013CA3 934 pyramidal neuron",
            "venue": "synapses. eLife,",
            "year": 2016
        },
        {
            "authors": [
                "J.C. Whittington",
                "T.H. Muller",
                "S. Mark",
                "G. Chen",
                "C. Barry",
                "N. Burgess",
                "T.E. Behrens"
            ],
            "title": "The Tolman-Eichenbaum 936 Machine: Unifying space and relational memory through generalization in the hippocampal formation",
            "year": 2020
        },
        {
            "authors": [
                "B. Willmore",
                "D.J. Tolhurst"
            ],
            "title": "Characterizing the sparseness of neural codes",
            "venue": "Netw. Comput. Neural Syst.,",
            "year": 2001
        },
        {
            "authors": [
                "E.R. Wood",
                "P.A. Dudchenko",
                "R. Robitsek",
                "H. Eichenbaum"
            ],
            "title": "Hippocampal neurons encode information about 941 different types of memory episodes occurring in the same location",
            "year": 2000
        },
        {
            "authors": [
                "H. Xiao",
                "K. Rasul",
                "R. Vollgraf"
            ],
            "title": "Fashion-MNIST: a novel image dataset for benchmarking machine learning 943 algorithms",
            "venue": "arXiv 1708.07747,",
            "year": 2017
        },
        {
            "authors": [
                "D. Zeithamova",
                "W.T. Maddox",
                "D.M. Schnyer"
            ],
            "title": "Dissociable prototype learning systems: Evidence from brain 945 imaging and behavior",
            "venue": "J. Neurosci.,",
            "year": 2008
        },
        {
            "authors": [
                "K. Zhang",
                "I. Ginzburg",
                "B.L. McNaughton",
                "T.J. Sejnowski"
            ],
            "title": "Interpreting neuronal population activity by 947 reconstruction: Unified framework with application to hippocampal place cells",
            "venue": "J. Neurophysiol.,",
            "year": 1998
        },
        {
            "authors": [
                "J. Zheng",
                "R.F. Stevenson",
                "B.A. Mander",
                "L. Mnatsakanyan",
                "F.P. Hsu",
                "S. Vadera",
                "R.T. Knight",
                "M.A. Yassa",
                "J.J. Lin"
            ],
            "title": "Multiplexing of theta and alpha rhythms in the amygdala-hippocampal circuit supports pattern 951 separation of emotional",
            "venue": "information. Neuron,",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "The hippocampal subfield CA3 is thought to function as an autoassociative network that stores 8 experiences as memories. Information from these experiences arrives via the entorhinal cortex (EC), 9 which projects to CA3 directly as well as indirectly through the dentate gyrus (DG). DG sparsifies and 10 decorrelates the information before also projecting to CA3. The computational purpose for receiving 11 two encodings of the same sensory information has not been firmly established. We model CA3 as a 12 Hopfield-like network that stores both correlated and decorrelated encodings and retrieves them at low 13 and high inhibitory tone, respectively. As more memories are stored, the dense, correlated encodings 14 merge along shared features while the sparse, decorrelated encodings remain distinct. In this way, the 15 model learns to transition between concept and example representations by controlling inhibitory tone. To 16 experimentally test for the presence of these complementary encodings, we analyze the theta-modulated 17 tuning of place cells in rat CA3. In accordance with our model\u2019s prediction, these neurons exhibit more 18 precise spatial tuning and encode more detailed task features during theta phases with sparser activity. 19 Finally, we generalize the model beyond hippocampal architecture and find that feedforward neural 20 networks trained in multitask learning benefit from a novel loss term that promotes hybrid encoding 21 using correlated and decorrelated representations. Thus, the complementary encodings that we have 22 found in CA3 can provide broad computational advantages for solving complex tasks. 23\nIntroduction 24\nThe hippocampus is believed to underlie our ability to form episodic memories, through which we can 25 recount personally experienced events from our daily lives (Scoville and Milner, 1957). In particular, the 26 subfield CA3 is thought to provide this capability as an autoassociative network (McNaughton and Morris, 27 1987; O\u2019Reilly and Rudy, 2001; Rolls and Kesner, 2006). Its pyramidal cells contain abundant recurrent 28 connections exhibiting spike-timing-dependent plasticity (Bi and Poo, 1998; Mishra et al., 2016). These 29 features allow networks to perform pattern completion and recover stored patterns of neural activity from 30 noisy cues. Sensory information to be stored as memories arrives to CA3 via the entorhinal cortex (EC), 31 which serves as the major gateway between hippocampus and neocortex (Fig. 1A). Neurons from layer II of 32 EC project to CA3 via two different pathways (Amaral and Pierre, 2006). First, they synapse directly onto 33 the distal dendrites of CA3 pyramidal cells through the perforant path (PP). Second, before reaching CA3, 34 perforant path axons branch towards the dentate gyrus (DG) and synapse onto granule cells. Granule cell 35 axons form the mossy fibers (MF) that also synapse onto CA3 pyramidal cells, though at more proximal 36 dendrites. 37 \u2217louis.kang@riken.jp\nAlong these pathways, information is transformed by each projection in addition to being simply relayed. 38 DG sparsifies encodings from EC by maintaining high inhibitory tone across its numerous neurons (Engin 39 et al., 2015). Sparsification in feedforward networks generally decorrelates activity patterns as well (Marr, 40 1971; O\u2019Reilly and McClelland, 1994; Vinje and Gallant, 2000; Pitkow and Meister, 2012; Cayco-Gajic 41 et al., 2017). The sparse, decorrelated nature of DG encodings is preserved by the MF pathway because 42 its connectivity is also sparse; each CA3 pyramidal cell receives input from only \u224850 granule cells (Amaral 43 et al., 1990). In contrast, PP connectivity is dense with each CA3 pyramidal cell receiving input from 44 \u22484000 EC neurons (Amaral et al., 1990), so natural correlations between similar sensory stimuli should 45 be preserved. Thus, CA3 appears to receive two encodings of the same sensory information with different 46 properties: one sparse and decorrelated through MF and the other dense and correlated through PP. What 47 is the computational purpose of this dual-input architecture? Previous theories have proposed that the MF 48 pathway is crucial for pattern separation during memory storage, but retrieval is predominantly mediated by 49 the PP pathway and can even be hindered by MF inputs (Treves and Rolls, 1992; McClelland and Goddard, 50 1996; Kaifosh and Losonczy, 2016). In these models, MF and PP encodings merge during storage and one 51 hybrid pattern per memory is recovered during retrieval. 52\nInstead, we consider the possibility that CA3 can store both MF and PP encodings for each memory and 53 retrieve either of them. Inhibitory tone selects between the two; with a higher activity threshold, sparser MF 54 patterns are more likely to be recovered, and the opposite holds for denser PP patterns. By encoding the same 55\nmemory in two different ways, each can be leveraged for a different computational purpose. Conceptually, 56 in terms of energy landscapes, sparser patterns have narrower attractor basins than denser patterns because 57 fewer neurons actively participate (Fig. 1B). Moreover, less correlated patterns are located farther apart 58 compared to more correlated patterns. Thus, MF energy basins tend to remain separate with barriers 59 between them, a property called pattern separation that maintains distinctions between similar memories 60 and is known to exist in DG (Leutgeb et al., 2007; Aimone et al., 2011; Borzello et al., 2023). In contrast, PP 61 energy basins tend to merge, which enables the clustering of similar memories into concepts. This proposed 62 ability for CA3 to recall both individual experiences and generalizations across them would explain observed 63 features of hippocampal function (Fig. 1C). For instance, remembering the details of a recent visit with an 64 acquaintance is an example of hippocampus-dependent episodic memory (Scoville and Milner, 1957; Squire, 65 1992). Meanwhile, hippocampal neurons can also generalize over your visits and respond to many different 66 representations of your acquaintance, including previously unseen photographs or her name in spoken or 67 written form (Quian Quiroga et al., 2005, 2009). 68\nTo instantiate these ideas, we constructed a model for EC, DG, and CA3 in which CA3 stores both MF 69 and PP encodings of each memory. We observe that MF encodings remain distinct, whereas PP encodings 70 perform concept learning by merging similar memories. Our model predicts relationships between coding 71 properties and network sparsity across phases of the theta oscillation, which modulates inhibitory tone in 72 the hippocampal region. We tested these predictions across two publicly available datasets (Mizuseki et al., 73 2013; Karlsson et al., 2015), and each analysis reveals that tuning of CA3 neurons is sharper during sparse 74 theta phases and broader during dense phases. This supports our model and enriches our understanding of 75 phase coding in hippocampus. While our model does not include CA1, we present comparative experimental 76 analyses for this subfield in various Supplementary Figures. Beyond asserting the presence of complementary 77 encodings in CA3, we demonstrate that they can offer functional advantages. Applying inspiration from our 78 CA3 model and data analysis toward machine learning, we introduced a novel plug-and-play loss function that 79 endows artificial neural networks with both correlated, PP-like and decorrelated, MF-like representations. 80 These networks can perform better in multitask learning compared to networks with single representation 81 types, which suggests a promising strategy for helping neural networks to solve complex tasks. While 82 the essential components of our networks are explained in the Results section, their full descriptions and 83 justifications are provided in the Methods section with parameter values in Table 1. 84\nResults 85"
        },
        {
            "heading": "MF encodings remain distinct while PP encodings build concepts in our model 86",
            "text": "for CA3 87\nWe model how representations of memories are transformed along the two pathways from EC to CA3 and 88 then how the resultant encodings are stored and retrieved in CA3. First, we focus on the transformations 89 between memories and their CA3 encodings. The sensory inputs whose encodings serve as memories in 90 our model are FashionMNIST images (Xiao et al., 2017), each of which is an example belonging to one 91 of three concepts: sneakers, trousers, and coats (Fig. 2A). They are converted to neural activity patterns 92 along each projection from EC to CA3 (Fig. 2B). Our neurons are binary with activity values of 0 or 1. 93 Each image i\u00b5\u03bd representing example \u03bd in concept \u00b5 is first encoded by EC using a binary autoencoder, 94 whose middle hidden layer activations represent the patterns xEC\u00b5\u03bd (Fig. 2C). Only 10% of the neurons are 95\nallowed to be active, so the representation is sparse, and there are more EC neurons than image pixels, so 96 the representation is overcomplete; sparse, overcomplete encoding models and autoencoder neural networks 97 are common unsupervised models for natural image processing (Olshausen and Field, 1996; Lee et al., 2006; 98 Makhzani and Frey, 2014; Chen et al., 2018). 99\nFrom EC, we produce DG, MF, and PP encodings with random, binary, and sparse connectivity matrices 100 between presynaptic and postsynaptic regions (Fig. 2D), i.e., from EC to DG, from DG to CA3 via MF, and 101 from EC to CA3 via PP. Each matrix transforms presynaptic patterns xpre\u00b5\u03bd into postsynaptic inputs, which 102 are converted into postsynaptic patterns xpost\u00b5\u03bd at a desired density using a winners-take-all approach. That 103 is, the postsynaptic neurons receiving the largest inputs are set to 1 and the others are set to 0. We define 104 density to be the fraction of active neurons, so lower values correspond to sparser patterns. Enforcing a 105 desired postsynaptic pattern density is equivalent to adjusting an activity threshold. At CA3, two encodings 106 for each image converge: xMF\u00b5\u03bd with density 0.02 and x PP \u00b5\u03bd with density 0.2. Not only are MF patterns 107 sparser, they are less correlated with average correlation 0.01, compared to a corresponding value of 0.09 for 108 PP patterns. Such an association between sparsification and decorrelation has been widely reported across 109 many theoretical models and brain regions (O\u2019Reilly and McClelland, 1994; Vinje and Gallant, 2000; Pitkow 110 and Meister, 2012; Cayco-Gajic et al., 2017), and it is also captured by our model. Decreasing postsynaptic 111 pattern density (sparsification) correspondingly decreases the postsynaptic correlation (decorrelation) for any 112 presynaptic statistics (Fig. 2E). We contribute further insight by deriving an explicit mathematical formula 113\nthat connects densities and correlations of patterns in presynaptic and postsynaptic networks: 114\n\u03c1post = \u0393 [\u221a 2 erfc\u22121(2apost), apre + \u03c1pre \u2212 apre\u03c1pre ] \u2212 a2post\napost(1\u2212 apost) ,\nwhere \u0393[\u03d5, \u03c3] \u2261 1 2\u03c0 \u222b \u03c0 arccos\u03c3 d\u03c8 exp [ \u2212 \u03d5 2 1 + cos\u03c8 ] (1)\nand erfc\u22121 is the inverse complementary error function. In other words, given the density apre and cor- 115 relation \u03c1pre of the presynaptic patterns and the desired density apost of the postsynaptic patterns, the 116 postsynaptic correlation \u03c1post is determined. Equation 1 is remarkable in that only these four quantities are 117 involved, revealing that at least in some classes of feedforward networks, other parameters such as network 118 sizes, synaptic density, and absolute threshold values do not contribute to decorrelation. It is derived in 119 Supplementary Methods, and its behavior is further depicted in Fig. S2B, C. 120\nUltimately, the encoding pathways in Fig. 2C\u2013E provide CA3 with a sparse, decorrelated xMF\u00b5\u03bd and a 121 dense, correlated xPP\u00b5\u03bd for each memory, in accordance with our biological understanding (Fig. 1A, B). Next, 122 we aim to store these patterns in an autoassociative model of CA3. Before doing so, we develop visualization 123 pathways that decode CA3 representations back into images, so memory retrieval can be intuitively evaluated. 124 This is accomplished by training a continuous-valued feedforward network to associate each MF and PP 125 pattern with its corresponding EC pattern (Fig. 2F). From there, the reconstructed EC pattern can be fed 126 into the decoding half of the autoencoder in Fig. 2C to recover the image encoded by CA3. These decoding 127 pathways are for visualization only and are not designed to mimic biology, although there may be parallels 128 with the neocortical output pathway from CA3 to CA1 and deep layers of EC (Amaral and Pierre, 2006). 129 The neuroanatomical connectivity of CA1 is more complex and includes temporoammonic inputs from EC 130 as well as strong secondary outputs through the subiculum, which also reciprocally connects with EC. 131\nNow, we model memory storage in the CA3 autoassociative network. For each example \u03bd in concept \u00b5, its 132 MF encoding xMF\u00b5\u03bd arrives at the proximal dendrites and its PP encoding x PP \u00b5\u03bd arrives at the distal dentrites 133 of CA3 pyramidal cells (Fig. 3A). The relative strength of PP inputs is weaker because PP synapses are 134 located more distally and are observed to be much weaker than MF synapses, which are even called detonator 135 synapses (Amaral and Pierre, 2006; Henze et al., 2002; Vyleta et al., 2016). The inputs are linearly summed 136 and stored in a Hopfield-like network (Hopfield, 1982), with connectivity 137\nWij \u223c \u2211 \u00b5\u03bd ( 0.9xMF\u00b5\u03bdi + 0.1x PP \u00b5\u03bdi )( 0.9xMF\u00b5\u03bdj + 0.1x PP \u00b5\u03bdj ) , (2)\nwhere i and j are respectively postsynaptic and presynaptic neurons. Equation 2 captures the most crucial 138 terms in Wij ; see Methods for the full expression. While we assume linear summation between x MF \u00b5\u03bd and 139 xPP\u00b5\u03bd for simplicity, integration of inputs across CA3 dendritic compartments is known to be nonlinear (Kim 140 et al., 2012; Makara and Magee, 2013; Kaifosh and Losonczy, 2016). Moreover, sublinear summation can 141 also arise from a temporal offset between MF and PP inputs, in which case changes in synaptic weights 142 across pathways could be weaker than those within the same pathway according to spike-timing-dependent 143 plasticity (Bi and Poo, 1998; Mishra et al., 2016). In Fig. S3F, we show that network behavior can be 144 maintained when nonlinearity is introduced. 145\nIn previous models, CA3 would retrieve only MF encodings, only PP encodings, or only the activity com- 146 mon between MF\u2013PP pairs (Treves and Rolls, 1992; McClelland and Goddard, 1996; Kaifosh and Losonczy, 147 2016). We assess the ability of the network to retrieve either xMF\u00b5\u03bd or x PP \u00b5\u03bd using either encoding as a cue 148\n(Fig. 3B). Each cue is corrupted by flipping randomly chosen neurons between active and inactive and is 149 set as the initial network activity. During retrieval, the network is asynchronously updated via Glauber 150 dynamics (Amit et al., 1985). That is, at each simulation timestep, one neuron is randomly selected to be 151\nupdated (Fig. 3C). If its total input from other neurons exceeds a threshold \u03b8, then it is more likely to become 152 active; conversely, subthreshold total input makes silence more likely. The width of the sigmoid function in 153 Fig. 3C determines the softness of the threshold. A large width implies that activation and silence are almost 154 equally likely for recurrent input near threshold. A small width implies that activation is almost guaranteed 155 for recurrent input above threshold and almost impossible for input below threshold. See Methods for the 156 full expression of this update rule. 157\nThe threshold \u03b8 represents the general inhibitory tone of CA3 and plays a key role in retrieval. At high 158 \u03b8, neural activity is disfavored, so we expect the network to retrieve the sparser, more strongly stored MF 159 encoding of the cue. Upon lowering \u03b8, more neurons are permitted to activate, so those participating in 160 the denser, more weakly stored PP encoding should become active as well. Because our neurons are binary, 161 active neurons in either the MF or the PP encoding would have the same activity level of 1, even though 162 their connectivity strengths differ. This combined activity of both encodings is almost the same as the PP 163 encoding alone, which contains many more active neurons. Thus, we expect the network to approximately 164 retrieve the PP encoding at low \u03b8. 165\nFigure 3D\u2013G illustrates the central behavior of our CA3 model; see Fig. S3A, B for trouser and coat 166 visualizations, which behave similarly to the sneaker visualizations shown here. First using MF encodings 167 as cues, we seek to retrieve either MF or PP encodings by respectively setting a high or low threshold. 168 As we load the network with increasingly more stored examples, distinct MF examples can consistently be 169 retrieved with high threshold (Fig. 3D). Meanwhile, retrieval of PP examples with low threshold fails above 170 1\u20132 examples stored per concept. At large example loads, the network again retrieves a sneaker memory 171 when cued with sneaker examples. However, this memory is the same for all sneaker cues and appears 172 similar to the average image over all sneaker examples (Fig. 2A), which captures common sneaker features 173 (Fig. S2A). Thus, the network is retrieving a representation of the sneaker concept. Notably, concepts 174 are never directly presented to the network; instead, the network builds them through the unsupervised 175 accumulation of correlated examples. The retrieval properties visualized in Fig. 3D are quantified in Fig. 3E 176 by computing the overlap between retrieved and target patterns. Across all example loads shown, retrieved 177 MF patterns overlap with target examples. As example load increases, retrieved PP patterns transition 178 from encoding examples to representing concepts. We define the target pattern xPP\u00b5 for a PP concept \u00b5 by 179 activating the most active neurons across PP examples within that concept until the PP density is reached, 180 and the dotted line in Fig. 3E coarsely estimates the largest overlap achievable (Methods). 181\nThe network capabilities observed for MF cues are preserved when we instead use PP cues (Fig. 3F, G) 182 or cues combining the neurons active in either encoding (Fig. S3C\u2013E); again, these latter two are similar 183 because MF encodings are sparse. Thus, retrieval behavior is driven largely by the level of inhibition rather 184 than the encoding type of cues. This feature implies that our model is agnostic to whether memory retrieval 185 in the hippocampus is mediated by the MF pathway, the PP pathway, or both. Computationally, it implies 186 that our model can not only retrieve two encodings for each memory but also perform heteroassociation 187 between them. Autoassociation and heteroassociation are preserved over large ranges in model parameters 188 (Fig. S3F). 189\nTo show that concept target patterns xPP\u00b5 and average images within concepts are indeed valid repre- 190 sentations of concepts for our image dataset, we plot them in image space after transforming xPP\u00b5 through 191 the visualization pathway (Fig. S2A). We observe that these two representation types appear similar to each 192 other and lie near the centers of well-separated clusters of examples for each concept. In machine learning, 193 clustering around central points is a common paradigm for unsupervised category learning, with k-means 194\nclustering as an example. In cognitive science as well, clustering has been used as model for unsupervised 195 category learning (Anderson, 1991; Love et al., 2004), and central representations called prototypes can be 196 used for category assignment (Ashby and Maddox, 2005). Thus, we conclude that xPP\u00b5 and average im- 197 ages can indeed serve as concept representations. With more complex image datasets, such as CIFAR10 198 (Krizhevsky and Hinton, 2009), examples may not be clustered in image space or in encoding space with our 199 model\u2019s simple autoencoder. To learn concepts, nonlinear decision boundaries can identified using super- 200 vised algorithms, but these complicated partitions of space may not admit central prototypes that accurately 201 represent concepts. Alternatively, we can employ more sophisticated feature extraction techniques to map 202 examples into an encoding space that exhibits clusters with simple boundaries between concepts. If that is 203 achieved, then central features such as averages within clusters in that space can again serve as concept rep- 204 resentations. More powerful feature extraction can be incorporated in our model by substituting our simple 205 autoencoder with, for example, an unsupervised variational autoencoder or a supervised deep classifier. 206\nWe investigate retrieval more comprehensively by randomly generating MF and PP patterns across 207 a broader range of statistics instead of propagating images along the hippocampal pathways in Fig. 2B 208 (Methods). For simplicity, we take MF examples to be uncorrelated. In Fig. 3H, I, we show regimes for 209 successful retrieval of MF examples, PP examples, and PP concepts. For MF and PP examples, the network 210 has a capacity for stored patterns above which they can no longer be retrieved (Fig. 3H). For PP concepts, 211 the network requires storage of a minimum number of examples below which concepts cannot be built. As 212 expected intuitively, fewer examples are needed if they are more correlated, since common features can be 213 more easily deduced. Figure 3I overlays retrieval regimes for MF examples and PP concepts. When the 214 number of concepts is low, there exists a regime at intermediate numbers of stored examples in which both 215 examples and concepts can be retrieved. This multiscale retrieval regime corresponds to the network behavior 216 observed in Fig. 3D\u2013G, and it is larger for more correlated PP encodings. On the other hand, its size does 217 not substantially change with the sparsity of MF patterns (Fig. S3G, H). Our capacity values agree with 218 theoretical formulas calculated using techniques from statistical physics (Kang and Toyoizumi, 2023). In all, 219 our networks with randomly generated patterns demonstrate that our results generalize to larger networks 220 that store more examples in more concepts and are not idiosyncratic to the pattern generation process in 221 Fig. 2. 222\nTo further explore the heteroassociative capability of our network, we cue the network with an MF pattern 223 and apply a time-varying threshold during retrieval. The network representation can then alternate between 224 the PP concept of the original cue during oscillation phases with low threshold and various MF examples 225 of that concept during phases with high threshold (Fig. 4A, B). Sharply and sinusoidally varying threshold 226 values both produce this behavior. From one oscillation cycle to the next, the MF encoding can hop among 227 different examples because concept information is preferentially preserved over example information during 228 low-threshold phases. If we weakly apply the MF cue as additional neural input throughout the simulation 229 (Methods), the network will only alternate between the target MF example and the target PP concept. This 230 condition can represent memory retrieval with ongoing sensory input. If we decrease the amplitude of the 231 oscillation, alternation between examples and concepts is disrupted and the network favors one encoding type 232 over the other. We quantify the distribution of network behaviors during high- and low-threshold phases 233 in Fig. 4C. The proportion of simulations in which single MF patterns are retrieved, the persistence of the 234 target PP concept, and other retrieval properties vary with network parameters. In Fig. S4A, B, we present 235 analogous results for randomly generated MF and PP patterns demonstrating that these retrieval properties 236 also depend on MF pattern sparsity. All in all, while our network can represent either examples or concepts 237\nat each moment in time, an oscillating threshold provides access to a range of representations over every 238 oscillation cycle. 239\nPlace cell data reveals predicted relationships between encoding properties and 240\ntheta phase 241\nThe central feature of our CA3 model is that an activity threshold determines whether the network retrieves 242 example or concept encodings. We claim that the theta oscillation in CA3 physiologically implements this 243 threshold and drives changes in memory scale. To be specific, our model predicts that single neurons should 244 convey more information per spike about example identity during epochs of sparser activity (Fig. 5A). This 245 single-neuron prediction can be tested by analyzing publicly available datasets of CA3 place cells. Figure 5B 246 shows one example place cell recorded while a rat traverses a linear track (Mizuseki et al., 2013, 2014). 247 During locomotion, single-neuron activity in CA3 is strongly modulated by the theta oscillation (Fig. 5C); 248 we use this activity as an indicator of network sparsity since a relationship between the two has been 249 observed (Fig. 3 in Skaggs et al., 1996). We assume an equivalence between the encoding of images by our 250 CA3 model and the encoding of spatial positions by CA3 place cells (Fig. 5D). Examples are equivalent to 251 fine positions along the linear track. Just as similar examples merge into concepts, nearby positions can 252 aggregate into coarser regions of space. Through this equivalence, we can translate the prediction about 253 example information per spike (Fig. 5A) into a prediction about spatial tuning (Fig. 5E). During denser 254 theta phases, place fields should be broader, which corresponds to lower position information per spike. 255 This prediction relies on our claim that the theta oscillation in CA3 acts as the inhibitory threshold of our 256 model. A priori, the alternative prediction that place fields are sharper during dense theta phases is an 257 equally valid hypothesis. Higher activity may result from strong drive by external stimuli that the neuron 258 serves to encode, while lower activity may reflect noise unrelated to neural tuning. The sharpening of visual 259 tuning curves by attention is an example of this alternative prediction (McAdams and Maunsell, 1999). 260 From a more general perspective, the model and alternative predictions in Fig. 5E roughly correspond to 261 subtractive and divisive modulation of firing rates, respectively. Both kinds of inhibitory effects are found 262 in cortical circuits (Isaacson and Scanziani, 2011; Carandini and Heeger, 2012; Ferguson and Cardin, 2020). 263 We will now test whether experimental data reflect our model prediction of sharper place field tuning with 264 higher spatial information during sparser theta phases, which would support a subtractive role of theta as 265 an oscillating inhibitory threshold over a divisive one. 266\nFirst, we investigate the encoding of fine, example-like positions by analyzing phase-dependent tuning 267 within single place fields. We use the Collaborative Research in Computational Neuroscience (CRCNS) hc-3 268 dataset contributed by Gyo\u0308rgy Buzsa\u0301ki and colleagues (Mizuseki et al., 2013, 2014). Figure 5F shows one 269 extracted field that exhibits phase precession (for others, see Fig. S5A). At each phase, we compute the total 270 activity as well as the information per spike conveyed about position within the field (Fig. 5G and Fig. S5B). 271 It is well known that the estimation of information per spike is strongly biased by sparsity. Consider the 272 null data in Fig. 5H that is matched in spike phases; spike positions, however, are randomly chosen from a 273 uniform distribution. In the large spike count limit, uniformly distributed activity should not convey any 274 information. Yet, the null data show more position information per spike during sparser phases (Fig. 5I). 275 To correct for this bias, we follow previous protocols and subtract averages over many null-matched samples 276 from position information (Dotson and Yartsev, 2021). In all of our comparisons of information between 277 sparse and dense phases, including the model prediction in Fig. 5A, we report sparsity-corrected information. 278 For further validation, we generate a shuffled dataset that disrupts any relationship between spike positions 279\nand phases found in the original data (Fig. 5J). Figure 5J, K illustrates a second place field whose tuning 280 also depends on theta phase but does not exhibit precession. For each theta-modulated CA3 place field, 281 we partition phases into sparse and dense halves based on activity, and we average the sparsity-corrected 282\nposition information per spike across each partition. CA3 place fields convey significantly more information 283 during sparse phases than dense phases (Fig. 5L). This relationship is present in both phase-precessing 284 and other fields (although slightly non-significantly in the latter) and is absent in the shuffled data. Thus, 285 experimental data support our model\u2019s prediction that CA3 encodes information in a finer, example-like 286 manner during sparse theta phases. Notably, CA1 place fields do not convey more information per spike 287 during sparse phases, which helps to show that our prediction is nontrivial and demonstrates that the phase 288 behavior in CA3 is not just simply propagated forward to CA1 (Fig. S5C). 289\nTo characterize the relationship between information and theta phase more precisely, we aggregate spikes 290 over phase-precessing fields in CA3 and in CA1 (Fig. S5D\u2013G). This process implicitly assumes that each 291 phase-precessing field is a sample of a general distribution characteristic to each region. These aggregate fields 292 recapitulate the single-neuron results that CA3 spikes are uniquely more informative during sparse phases 293 (Fig. S5H). They also reveal how position information varies with other field properties over theta phases 294 (Fig. S5I, J). For example, information is inversely correlated with field width, confirming the interpretation 295 that more informative phases have sharper tuning curves (Fig. 5E). In CA3, information is greatest during 296 early progression through the field, which corresponds to future locations, with a smaller peak during late 297 progression, which corresponds to past locations. In contrast, past locations are more sharply tuned in CA1. 298 Thus, different hippocampal subfields may differentially encode past and future positions across the theta 299 cycle; we will return to this topic in the Discussion. 300\nNext, we turn our attention to the representation of concepts instead of examples. Our model predicts 301 that single neurons exhibit more concept information per spike during dense activity regimes (Fig. 6A). To 302 test this prediction using the same CRCNS hc-3 dataset, we invoke the aforementioned equivalence between 303 concepts in our model and coarser positions along a linear track (Fig. 6B). Thus, single CA3 neurons should 304 encode more information per spike about coarse positions during dense theta phases. Previously, to test 305 for finer position encoding in Fig. 5, we divided single place fields into multiple position bins during the 306 computation of information. Here, we analyze encoding of coarser positions by choosing large position bins 307 across the whole track (Fig. 6C, D and Fig. S6A). We consider different bin sizes to characterize at which 308 scale the merging of examples into concepts occurs. When we again compute the average difference in 309 sparsity-corrected position information per spike between sparse and dense theta phases, we find that dense 310 phases are the most preferentially informative at the coarsest scales (Fig. 6E). CA1 place cells also exhibit 311 this property (Fig. S6B). Crucially, differences between sparse and dense phases are not seen in shuffled 312 data, which supports the validity of our analysis methods (Fig. 6F). Our results are further bolstered by 313 their preservation under a different binning procedure (Fig. S6C\u2013E). Thus, coarse positions along a linear 314 track can be best distinguished during dense theta phases, in agreement with our model. Note that we 315 always consider 4 bins at a time even for track scales smaller than 1/4, because changing the number of bins 316 across scales introduces a bias in the shuffled data (Fig. S6F\u2013H). At finer scales, this process sometimes fails 317 to capture entire fields and artificially partitions them (Fig. 6C, left). Here, we adopt a neutral approach 318 and do not adjust our partitions to avoid these cases; the opposite approach was adopted in Fig. 5 where 319 intact fields were explicitly extracted. These different approaches may explain why at the finest scales in 320 Fig. 6E, sparse phases do not convey more information like they do in Fig. 5L. 321\nIn our model, concepts are formed by merging examples across all correlated features. While track 322 position can be one such feature, we now assess whether our predictions also apply to another one. In 323 the CRCNS hc-6 dataset contributed by Loren Frank and colleagues, CA3 place cells are recorded during 324 a W-maze alternation task in which mice must alternately visit left and right arms between runs along 325\n0 250 0\u00b0\n360\u00b0\n720\u00b0\nposition [cm]\nth et\na ph\nas e\nBA model prediction\ntrack scale: 1/4track scale: 1/16 track scale: 1/8\nD\nbroader concepts\nexamples\nconcepts\nfiner positions\ncoarser positions\nCA3 theta\n0\u00b0 360\u00b0 720\u00b0 0\n43\n0\n1.1\ntheta phase\nac tiv\nity [s\npk /s\n]\nposition info.[bit/spk] nul lm atched [bit /spk ] 0\u00b0 360\u00b0 720\u00b0 0 9 0 2.0\ntheta phase\nac tiv\nity [s\npk /s\n]\nposition info.[bit/spk] nullm atched [bit / spk]\n0 250 0\u00b0\n360\u00b0\n720\u00b0\nposition [cm]\nth et\na ph\nas e\n0\u00b0 360\u00b0 720\u00b0 0\n21\n0\n1.2\ntheta phase\nac tiv\nity [s\npk /s\n] position info.[bit/spk] nullm atched [bit / spk]\n0 250 0\u00b0\n360\u00b0\n720\u00b0\nposition [cm] th\net a\nph as\ne\n-1\n0\n1\nsp ar\nse -\nde ns e co nc ep ti nf o. [b\nit/ sp k] C\nthe center arm (Karlsson et al., 2015). It is known that place cells along the center arm can encode the 326 turn direction upon entering or leaving the center arm in addition to position (Frank et al., 2000; Wood 327 et al., 2000). Again, our model predicts that sparse theta phases preferentially encode specific information 328 (Fig. 7A), so they should be more tuned to a particular turn direction (Fig. 7B). During dense phases, they 329 should generalize over turn directions and solely encode position. 330\nFigure 7C shows spikes from one CA3 place cell accumulated over outward runs along the center arm 331\nfollowed by either left or right turns (Fig. S7A). For each theta phase, we compute the total activity, the 332 turn information per spike (ignoring position), and the mean information of null-matched samples used for 333 sparsity correction (Fig. 7D). Figure 7E, F show similar results for inward runs (for others, see Fig. S7B, C). 334 For both outward and inward runs, sparsity-corrected turn information per spike is greater during sparse 335 theta phases compared to dense phases (Fig. 7G). This finding is not observed in data in which theta phase 336 and turn direction are shuffled (Fig. 7G, H). Not only do these results support our model, they also reveal 337 that in addition to splitter cells that encode turn direction over all theta phases (Duvelle et al., 2023), CA3 338 contains many more place cells that encode it only at certain phases (Fig. S7D). The difference between 339 sparse and dense phases is significantly greater in CA3 than it is in CA1 (Fig. S7E, F). Thus, our subfield- 340 specific results for example encoding are consistent across position and turn direction. Aggregate neurons, 341 formed by combining spikes from more active turn directions and those from less active turn directions, 342 demonstrate similar tuning properties to individual neurons (Fig. S7G\u2013I). 343\nBeyond the single-neuron results presented above, we seek to test our predictions at the population 344 level. To do so, we perform phase-dependent Bayesian population decoding of turn direction during runs 345 along the center arm (Fig. 7I). This analysis requires multiple neurons with sufficiently sharp tuning to 346 be simultaneously active across all theta phases; it can be used to decode left versus right turns, whereas 347 an analogous decoding of track position, which spans a much broader range of values, is intractable with 348 our datasets. We find that the CA3 population likelihood exhibits greater confidence during sparse phases 349 (Fig. 7J). From a Bayesian perspective, the population expresses stronger beliefs about turn direction during 350 sparse phases and is more agnostic during dense phases. If pressed to choose the direction with higher 351 likelihood as its estimate, CA3 is also more accurate during sparse phases (Fig. 7K, L). These results match 352 our predictions in Fig. 7A, B and bolster our single-neuron results. Moreover, they are specific to CA3, as 353 similar conclusions cannot be made about the CA1 place cell population (Fig. S7J\u2013L). 354\nIn summary, extensive data analysis reveals experimental support for our CA3 model over two datasets 355 collected by different research groups, across two encoding modalities, for both example and concept repre- 356 sentations, and at both the single-neuron and population level. 357\nCA3-like complementary encodings improve neural network performance in mul- 358\ntitask machine learning 359\nWe have observed how CA3 encodes behaviorally relevant information at different scales across theta phases. 360 Can these different types of encodings be useful for solving different types of tasks? Can they even benefit 361 neural networks designed for machine learning, abstracting away from the hippocampus? To address these 362 questions through one framework, we turn to a classic paradigm in machine learning: a multilayer perceptron 363 trained on MNIST digits (LeCun et al., 1998). First, we augment the MNIST dataset by randomly assigning 364 an additional label to each image: a set number (Fig. 8A). We train the fully connected feedforward network 365 to perform one of two tasks: classification of the written digit or identification of the assigned set (Fig. 8B). 366 The former requires clustering of images based on common features, which resembles concept learning in 367 our CA3 model, and the latter requires discerning differences between similar images, which resembles 368 example learning in our CA3 model (Fig. 8C). We use a held-out test dataset to evaluate digit classification 369 performance and corrupted images from the train dataset to evaluate set identification performance. 370\nIn our CA3 model, we found that examples were preferentially encoded by the decorrelated MF pathway 371 and concepts by the correlated PP pathway (Fig. 3). In an analogous fashion, we seek to manipulate the 372 correlation properties within the final hidden layer of our perception, whose activations s\u03b1 serve as encodings 373\nof the input images i\u03b1. In particular, we apply a novel DeCorr loss function, which penalizes correlations in 374 s\u03b1 between every pair of items \u03b1, \u03b2 in a training batch (Fig. 8D): 375\nLDeCorr \u2248 1\n2 \u2211 \u03b1,\u03b2\u2208 batch Pearson(s\u03b1, s\u03b2) 2. (3)\nDeCorr mimics the MF pathway; the equation is approximate due to a slight modification of the Pearson 376 correlation formula to aid numerical convergence (Methods). Alternatively, we consider the baseline condition 377 with no loss function on hidden layer activations, which preserves natural correlations between similar images 378 and mimics the PP pathway. Indeed, we observe that different encoding properties are suited for different 379 tasks. Baseline networks perform better in concept learning (Fig. 8E) while DeCorr networks perform better 380 in example learning (Fig. 8F), and these effects vary consistently with the strength of the DeCorr loss 381 function (Fig. S8A, B). Thus, DeCorr allows us to tune encoding correlations in neural networks to highlight 382 input features at either broader or finer scales. Tasks can be solved more effectively by matching their 383 computational requirements with the appropriate encoding scale. Note that DeCorr is different from the 384 DeCov loss function previously developed to reduce overfitting (Cogswell et al., 2015). DeCorr decorrelates 385 pairs of inputs across all neurons in the specified layer, whereas DeCov decorrelates pairs of neurons across 386 all inputs. As a regularizer that promotes generalization, DeCov improves digit classification and does not 387 substantially improve set identification, which contrasts with the effect of DeCorr (Fig. S8C, D). 388\nComplex tasks, including those performed by biological systems, may require information to be processed 389 at different scales of correlation. In CA3, a spectrum of encodings is available during each theta cycle. Can 390 neural networks take advantage of multiple encodings? We tackle this question by asking a perceptron to 391 simultaneously perform digit classification and set identification (Fig. 8G). In addition to the baseline and 392 DeCorr networks, we define a HalfCorr loss function (Fig. 8H): 393\nLHalfCorr \u2248 1\n2 \u2211 \u03b1,\u03b2\u2208 batch Pearson(shalf\u03b1 , s half \u03b2 ) 2, (4)\nwhere shalf\u03b1 represents the second half of neurons in the final hidden layer. After training with this loss 394 function, the neural representation consists of both a correlated, PP-like component in the first half and 395 a decorrelated, MF-like component in the second half. When we evaluate these networks on both digit 396 classification and set identification, we see that baseline and DeCorr networks behave similarly to how they 397 did on single tasks. Compared to baseline, DeCorr networks perform better in example learning at the 398 cost of poorer concept learning (Fig. 8I). However, HalfCorr networks do not suffer from this tradeoff and 399 perform well at both tasks. Their superior performance is maintained over a variety of network and dataset 400 parameters (Fig. S8E). Moreover, HalfCorr networks learn to preferentially use each type of encoding for the 401 task to which it is better suited. We use the decrease in task accuracy upon silencing a neuron as a metric 402 for its influence on the task. Correlated neurons are more influential in concept learning and decorrelated 403 neurons in example learning (Fig. 8J). 404\nNote that we do not manipulate pattern sparsity in these artificial networks. Sparsification can be useful 405 in the hippocampus because it provides a biologically tractable means of achieving decorrelation. It also 406 allows biological networks to access both less and more correlated representations by changing the level of 407 inhibition. Instead, we can directly manipulate correlation through the DeCorr and HalfCorr loss functions. 408 Under some conditions, the decorrelated half of the final hidden layer in HalfCorr networks indeed exhibits 409\nsparser activation than the correlated half (Fig. S8F). It is possible that directly diversifying sparsity can also 410 improve machine learning performance, especially since sparse coding is known to offer certain computational 411 advantages as well as greater energy efficiency (Olshausen and Field, 1996, 2004; Sze et al., 2017). 412\nDiscussion 413\nThe hippocampus is widely known to produce our ability to recall specific vignettes as episodic memories. 414 This process has been described as indexing every sensory experience with a unique neural barcode so that 415 separate memories can be independently recovered (Teyler and DiScenna, 1986; Teyler and Rudy, 2007). 416 Recently, research has shown that the hippocampus is also important in perceiving commonalities and 417 regularities across individual experiences, which contribute to cognitive functions such as statistical learning 418 (Schapiro et al., 2014; Covington et al., 2018), category learning (Knowlton and Squire, 1993; Zeithamova 419 et al., 2008; Mack et al., 2016; Bowman and Zeithamova, 2018), and semantic memory (Manns et al., 2003; 420 Duff et al., 2020; Norman et al., 2021). Evidence for this has been obtained largely through human studies, 421 which can present and probe memories in controlled settings. However, the detailed circuit mechanisms used 422 by the hippocampus to generalize across experiences while also indexing them separately are not known. 423\nOur analysis of rodent place cell recordings reveals that single CA3 neurons alternate between finer, 424 example-like representations and broader, concept-like representations of space across the theta cycle (Figs. 5, 425 6, and 7). These single-neuron results extend to the network level, which alternatively encodes more specific 426 and more general spatial features in a corresponding manner (Fig. 7). If we accept that place cells store 427 these features as spatial memories, then our experimental analysis reveals that CA3 can access memories of 428 different scales at different theta phases. We propose that the computational mechanism underlying these 429 observations is the multiplexed encoding of each memory at different levels of correlation (Figs. 2 and 8). We 430 show that this mechanism can be biologically implemented through the storage of both sparse, decorrelated 431 MF and dense, correlated PP inputs to CA3 and their alternating retrieval by the theta oscillation, which 432 acts as an activity threshold and subtractively modulates neural activity (Figs. 2, 3, and 4). Our model 433 performs successful pattern completion for both types of encodings, suggesting that patterns across the theta 434 cycle can truly function as memories that can be recovered from partial cues. 435\nAlone, our secondary analyses of experimental data contribute to a large set of observations on how 436 coding properties vary with theta phase in the hippocampus. Of note is phase precession, in which different 437 phases preferentially encode different segments within a firing field as it is traversed, with later phases 438 tuned to earlier segments (O\u2019Keefe and Recce, 1993). Phase precession is most widely reported for place 439 cells and traversals of physical space, but it also appears during the experience of other sequences, such 440 as images and tasks (Terada et al., 2017; Qasim et al., 2021; Reddy et al., 2021). Our analysis implies 441 that the sharpness of tuning is not constant throughout traversals. In particular, CA3 neurons are more 442 sharply tuned at early positions in place fields, while CA1 neurons are more sharply tuned at late positions 443 (Fig. S5I, J). Transforming these conclusions about position into those about time through the concept of 444 theta sequences, CA3 represents the future more precisely, while CA1 represents the future more broadly. 445 The latter is consistent with the idea that CA1 may participate in the exploration of multiple possible future 446 scenarios (Kay et al., 2020). Furthermore, our W-maze analysis reveals that certain hippocampal neurons 447 which do not obviously encode an external modality across all theta phases, such as turn direction, may do 448 so only during sparse phases (Fig. S7D). This observation adds to the subtleties with which the hippocampus 449 represents the external world. 450\nOther groups have investigated the variation of place field sharpness with theta phase in CA1, not CA3, 451 and their results are largely in agreement with our CA1 analyses. Skaggs et al. (1996) partitioned theta 452 phases into halves, one of which with higher activity than the other, and find more information per spike 453 during the less active half. We observe no difference at the single-neuron level, though our W-maze results are 454 only slightly non-significant (Figs. S5C and S7E). Their partitions differ from ours by 30\u25e6 and they employ 455 a different binning technique, both of which can influence the results. The more informative phases in their 456 work correspond to earlier field positions, which we also observe (Fig. S5J). Note that their computation of 457 sparsity is performed along a different axis compared to ours; using terms fromWillmore and Tolhurst (2001), 458 they use the lifetime density while we compute the population density, which fundamentally differ. Ujfalussy 459 and Orba\u0301n (2022) also found that phases with smaller field sizes correspond to earlier field positions. Mehta 460 et al. (2002) considered phase-dependent tuning within CA1 place fields, but they calculate field width 461 over theta phase as a function of field progress, whereas we do the opposite. Similarly, Souza and Tort 462 (2017) considered phase tuning at various field progresses. Both sets of results appear to be compatible with 463 ours, but a direct comparison cannot be made. Overall, our work offers original insights into hippocampal 464 phase coding not only by focusing on CA3, which behaves differently from CA1, but also by elucidating 465 a connection between tuning width and network sparsity. Intriguingly, Pfeiffer and Foster (2015) found a 466 relationship between CA1 replay speed and slow gamma oscillation phase, which modulates network activity 467 during quiescence. This observation opens the possibility that other oscillations can leverage the connection 468 between tuning and sparsity when the dominant theta rhythm is absent. 469\nBetter memory performance has been associated with greater theta power during both encoding (Berry 470 and Thompson, 1978; Seager et al., 2002; Lega et al., 2012; Penley et al., 2013; Backus et al., 2016; Herweg 471 et al., 2020) and retrieval (Jacobs et al., 2006; Lega et al., 2012; Zheng et al., 2019; Herweg et al., 2020). Our 472 model cannot explain the former because it does not contain a theta oscillation during encoding. It does, 473 however, offer a possible explanation for the latter observation. We simulated retrieval under an oscillating 474 threshold with lower amplitude and observed that the network stalls on MF example encodings instead of 475 alternating with PP concept encodings (Fig. 4A). Thus, the biological processes that produce larger theta 476 amplitudes, such as stronger medial septum inputs or changes in neuromodulator concentrations (Colgin, 477 2013), may promote memory recall by granting access to wider ranges of pattern sparsities and, consequently, 478 representational scales. 479\nThe temporal coordination between memory storage and retrieval is also biologically significant. We 480 make the major simplification of separately simulating memory storage and retrieval. These two operating 481 regimes can represent different tones of a neuromodulator such as acetylcholine, which is thought to bias 482 the network towards storage (Hasselmo, 2006). Another proposal is that storage and retrieval preferentially 483 occur at different theta phases, motivated by the variation in long-term potentiation (LTP) strength at CA1 484 synapses across the theta cycle (Hasselmo et al., 2002; Kunec et al., 2005; Siegle and Wilson, 2014). Although 485 this idea focuses on plasticity in CA1, it is possible that storage and retrieval also occur at different phases 486 in CA3. Note that our experimental analysis reveals a sharp dip in position information around the sparsest 487 theta phase in both CA3 and CA1 (Fig. S5E, G). This phase may coincide with the storage of new inputs, 488 during which the representation of existing memories is momentarily disrupted; the rest of the theta cycle 489 may correspond to retrieval. This interpretation could motivate excluding the sparsest theta phase from 490 further analysis, since our model predictions only regard memory retrieval. However, we take a conservative 491 approach and include all phases. Intriguingly, recent work reported that the strength of LTP in CA1 peaks 492 twice per theta cycle (Leung and Law, 2020), suggesting for our model that MF and PP patterns could have 493\ntheir own storage and retrieval intervals during each theta cycle. 494\nOur work connects hippocampal anatomy and physiology with foundational attractor theory. Among 495 others, Tsodyks and Feigel\u2019man (1988) observed that sparse, decorrelated patterns can be stored at high 496 capacity, and Fontanari (1990) found that dense, correlated patterns can merge into representations of 497 common features. We demonstrate that both types of representations can be stored and retrieved in the 498 same network, using a threshold to select between them. This capability can be given solid theoretical 499 underpinnings using techniques from statistical mechanics (Kang and Toyoizumi, 2023). The convergence of 500 MF and PP pathways in CA3 has also been the subject of previous computational investigations (Treves and 501 Rolls, 1992; McClelland and Goddard, 1996; Kaifosh and Losonczy, 2016). In these models, CA3 stores and 502 retrieves one encoding per memory, while our model asserts that multiple encodings for the same memory 503 alternate across the theta cycle. Another series of models proposes, like we do, that the hippocampus can 504 simultaneously maintain both decorrelated, example-like encodings and correlated, concept-like encodings 505 (Schapiro et al., 2017; Suc\u030cevic\u0301 and Schapiro, 2022). These encodings converge at CA1 and each type is 506 not independently retrieved there, which differs from our model. In a related hippocampal model, the PP 507 pathway was shown to be crucial for learning cue-target associations in the presence of additional context 508 inputs to EC that drift over time (Antony et al., 2023). Successful learning requires the network to perform 509 decontextualization and abstract away the slowly varying context inputs, which is, like concept learning, a 510 form of generalization. EC has also been hypothesized to differentially encode inputs upstream of CA3, with 511 specific sensory information conveyed by lateral EC and common structural representations by medial EC 512 (Whittington et al., 2020). Further experimental investigation into the contributions of various subregions 513 would help to clarify how the hippocampus participates in memory generalization. 514\nFinally, our work addresses how CA3-like complementary encodings can be computationally leveraged 515 by neural networks to solve complex tasks. We conceptually extend our results about CA3 and introduce 516 a novel HalfCorr loss function that diversifies hidden layer representations to include both correlated and 517 decorrelated components (Fig. 8). HalfCorr networks can better learn tasks that involve both distinction 518 between similar inputs and generalization across them. They are simultaneously capable of pattern separation 519 and categorization even based on small datasets, demonstrating a possible advantage of brain computation 520 over conventional deep learning. Yet, we deliberately chose a neural architecture that differs from that of 521 the CA3 network to test the scope over which complementary encodings can improve learning. Instead of 522 a recurrent neural network storing patterns of different sparsities through unsupervised Hopfield learning 523 rules, we implemented a feedforward multilayer perceptron, a workhorse of supervised machine learning. The 524 success of HalfCorr networks in this scenario supports the possibility that HalfCorr can be broadly applied 525 as a plug-and-play loss function to improve computational flexibility. 526\nFunctional heterogeneity is commonly invoked in the design of modern neural networks. It can be 527 implemented in the form of deep or modular neural networks in which different subnetworks perform different 528 computations in series or parallel, respectively (LeCun et al., 2015; Amer and Maul, 2019). Of note, Kowadlo 529 et al. (2020) constructed a deep, modular network inspired by the hippocampus for one-shot machine learning 530 of both concepts and examples. As an aside, their architecture shares similarities with our hippocampus 531 model in Fig. 2, but their Hopfield-like network for CA3 only stores MF patterns and inactivating these 532 recurrent connections does not affect network performance. In contrast to these networks with specialized 533 subnetworks, we propose a different paradigm in which a loss function applied differentially across neurons 534 promotes heterogeneity within a single layer. This idea can be extended from the two components of 535 HalfCorr networks, correlated and decorrelated, by assigning a different decorrelation strength to each neuron 536\nand thereby producing a true spectrum of representations. Furthermore, heterogeneity in other encoding 537 properties such as mean activation, variance, and sparsity may also improve performance in tasks with 538 varying or unclear computational requirements. Such tasks are not limited to multitask learning, but also 539 include continual learning (Parisi et al., 2019), reinforcement learning (Arulkumaran et al., 2017), and natural 540 learning by biological brains. 541\nMethods 542\nTransformation of memories along hippocampal pathways 543\nBinary autoencoder from images to EC 544\nOur memories are 256 images from each of the sneaker, trouser, and coat classes in the FashionMNIST dataset (Xiao 545 et al., 2017). We train a fully connected linear autoencoder on these images with three hidden layers of sizes 128, 1024, 546 and 128. Batch normalization is applied to each hidden layer, followed by a rectified linear unit (ReLU) nonlinearity 547 for the first and third hidden layers and a sigmoid nonlinearity for the output layer. Activations in the middle hidden 548 layer are binarized by a Heaviside step function with gradients backpropagated by the straight-through estimator 549 (Bengio et al., 2013). The loss function is 550\nL = \u2211 \u00b5\u03bd\u2208 batch ||\u0302i\u00b5\u03bd \u2212 i\u00b5\u03bd ||2 + \u03bb \u2211 \u00b5\u03bd\u2208 batch KL ( 1 NEC \u2211 i x EC \u00b5\u03bdi \u2225\u2225\u2225 aEC), (5)\nwhere i\u00b5\u03bd is the image with pixel values between 0 and 1, i\u0302\u00b5\u03bd is its reconstruction, x EC \u00b5\u03bd represents the binary 551 activations of the middle hidden layer with NEC = 1024 units indexed by i, and aEC = 0.1 is its desired density 552 (Fig. 2C). Sparsification with strength \u03bb = 10 is achieved by computing the Kullback-Leibler (KL) divergence between 553 the hidden layer density and aEC (Le et al., 2011). Training is performed over 150 epochs with batch size 64 using 554 the Adam optimizer with learning rate 10\u22123 and weight decay 10\u22125. 555\nBinary feedforward networks from EC to CA3 556\nTo propagate patterns from EC to DG, from DG to MF inputs, and from EC to PP inputs, we compute 557\nxpost\u00b5\u03bdi = \u0398 [\u2211 j Wijx pre \u00b5\u03bdj \u2212 \u03b8 ] , (6)\nwhere xpre\u00b5\u03bd and x post \u00b5\u03bd are presynaptic and postsynaptic patterns, Wij is the connectivity matrix, and \u03b8 is a threshold. 558 Each postsynaptic neuron receives l excitatory synapses of equal strength from randomly chosen presynaptic neurons. 559 \u03b8 is implicitly set through a winners-take-all (WTA) process that enforces a desired postsynaptic pattern density apost. 560 \u0398 is the Heaviside step function, and N is the network size. 561\nEC patterns have NEC = 1024 and aEC = 0.1. To determine N , a, and l for each subsequent region, we turn to 562 estimated biological values and loosely follow their trends. Rodents have approximately 5-10 times more DG granule 563 cells and 2-3 times more CA3 pyramidal cells compared to medial EC layer II principal neurons (Amaral et al., 1990; 564 Murakami et al., 2018; Attili et al., 2019). Thus, we choose NDG = 8192 and NCA3 = 2048. During locomotion, DG 565 place cells are approximately 10 times less active than medial EC grid cells (Mizuseki and Buzsa\u0301ki, 2013), and MF 566 inputs are expected to be much sparser than PP inputs (Treves and Rolls, 1992). Thus, we choose aDG = 0.005, 567 aMF = 0.02, and aPP = 0.2. We do not directly enforce correlation within concepts, which take values \u03c1EC = 0.15, 568 \u03c1DG = 0.02, \u03c1MF = 0.01, and \u03c1PP = 0.09. Each DG neuron receives approximately 4000 synapses from EC and each 569 CA3 neuron receives approximately 50 MF and 4000 PP synapses (Amaral et al., 1990). Thus, we choose lDG = 205, 570 lMF = 8, and lPP = 205. Note that for each feedforward projection, postsynaptic statistics apost and \u03c1post are not 571 expected to depend on l (Eq. 1). 572\nIn Fig. 2E, for the case of xpre\u00b5\u03bd = x EC \u00b5\u03bd , we use Npost = 2048 and l = 205. \u03c1post is obtained by computing 573 correlations between examples within the same concept and averaging over 3 concepts and 8 connectivity matrices. 574 For the case of randomly generated xpre\u00b5\u03bd , we use Npre = Npost = 10 000, a single concept, and a single connectivity 575 matrix with l = 2000. See Supplementary Methods for further details, including the derivation of Eq. 1. 576\nVisualization pathway from CA3 to EC 577 We train a fully connected linear feedforward network with one hidden layer of size 4096 to map inputs xMF\u00b5\u03bd to targets 578 xEC\u00b5\u03bd and inputs x PP \u00b5\u03bd also to targets x EC \u00b5\u03bd . Batch normalization and a ReLU nonlinearity is applied to the hidden layer 579 and a sigmoid nonlinearity is applied to the output layer. The loss function is 580\nL = \u2211 \u00b5\u03bd\u2208 batch ||x\u0302EC\u00b5\u03bd \u2212 xEC\u00b5\u03bd ||2. (7)\nTraining is performed over 100 epochs with batch size 128 using the Adam optimizer with learning rate 10\u22124 and 581 weight decay 10\u22125. 582\nHopfield-like model for CA3 583\nPattern storage 584\nOur Hopfield-like model for CA3 stores linear combinations q\u00b5\u03bd of MF and PP patterns: 585\nq\u00b5\u03bdi = (1 \u2212 \u03b6) \u00b7 (xMF\u00b5\u03bdi \u2212 aMF) + \u03b6 \u00b7 (xPP\u00b5\u03bdi \u2212 aPP), (8)\nwhere \u03b6 = 0.1 is the relative strength of the PP patterns (Fig. 3A). The subtraction of densities from each pattern is 586 typical of Hopfield networks with neural states 0 and 1 (Tsodyks and Feigel\u2019man, 1988). The synaptic connectivity 587 matrix is 588\nWij = 1\nNCA3 \u2211 \u00b5\u03bd q\u00b5\u03bdiq\u00b5\u03bdj . (9)\nPattern retrieval 589\nCues are formed from target patterns by randomly flipping the activity of a fraction 0.01 of all neurons (Fig. 3B). 590 This quantity is termed cue inaccuracy ; in Fig. S3F, we also consider cue incompleteness, which is the fraction of 591 active neurons that are randomly inactivated to form the cue. During retrieval, neurons are asynchronously updated 592 in cycles during which every neuron is updated once in random order (Fig. 3C). The total synaptic input to neuron 593 i at time t is 594\ngi(t) = \u2211 j WijSj(t) + hi(t), (10)\nwhere Sj(t) is the activity of presynaptic neuron j and hi(t) is an external input. The external input is zero except 595 for the cue-throughout condition in Fig. 4, in which h(t) = \u03c3x for noisy MF cue x and strength \u03c3 = 0.2. 596\nThe activity of neuron i at time t is probabilistically updated via the Glauber dynamics 597\nP [Si(t+ 1) = 1] = 1\n1 + e\u2212\u03b2[gi(t)\u2212\u03b8(t)] , (11)\nwhere \u03b8 is the threshold and \u03b2 is inverse temperature, with higher \u03b2 implying a harder threshold. Motivated by 598 theoretical arguments, we define rescaled variables \u03b8\u2032 and \u03b2\u2032 such that \u03b8 = \u03b8\u2032 \u00b7 (1 \u2212 \u03b6)2aMF and \u03b2 = \u03b2\u2032/(1 \u2212 \u03b6)2aMF 599 (Kang and Toyoizumi, 2023). Unless otherwise indicated, we run simulations for 10 update cycles, use \u03b2\u2032 = 100, 600 and use \u03b8\u2032 = 0.5 to retrieve MF patterns and \u03b8\u2032 = 0 to retrieve PP patterns. The rescaled \u03b8\u2032 is the threshold value 601 illustrated in Fig. 4A and Fig. S4A. 602\nRetrieval evaluation 603\nThe overlap between the network activity S and a target pattern x is 604\nm = 1 NCA3a(1 \u2212 a) \u2211 i Si(xi \u2212 a), (12)\nwhere a is the density of the target pattern. This definition is also motivated by theory (Kang and Toyoizumi, 2023). 605 The target pattern x\u0304PP\u00b5 for PP concept \u00b5 is 606\nx\u0304PP\u00b5i = \u0398 [\u2211 \u03bd xPP\u00b5\u03bdi \u2212 \u03d5 ] , (13)\nwhere \u03d5 is a threshold implicitly set by using winners-take-all to enforce that x\u0304PP\u00b5 has density aPP. The theoretical 607 maximum overlap between the network and x\u0304PP\u00b5 is the square root of the correlation \u221a \u03c1PP (Kang and Toyoizumi, 608 2023). Because this estimate is derived for random binary patterns in the large network limit, it can be exceeded in 609 our simulations. 610\nTo visualize S, we first recall that the inverse of a dense stored pattern x with every neuron flipped can also be 611 an equivalent stable state (Hopfield, 1982). Thus, we invert S if we are retrieving PP patterns at low \u03b8 and if m < 0. 612 Then, we decode its EC representation by passing S through the feedforward visualization network and binarizing 613 the output with threshold 0.5. Finally, we pass the EC representation through the decoding layers of the image 614 autoencoder. 615\nSee Supplementary Methods for the determination of network capacity with random binary patterns (Fig. 3H, I 616 and Fig. S3G, H) and the definition of oscillation behaviors (Fig. 4C and Fig. S4B). 617\nExperimental data analysis 618\nGeneral considerations 619\nTo calculate activity, we tabulate spike counts c(r, \u03d5) over spatial bins r (position or turn direction) and theta phase 620 bins \u03d5, and we tabulate trajectory occupancy u(r, \u03d5) over the same r and distribute them evenly across \u03d5. Activity 621 as a function of theta phase, the spatial variable, and both variables are respectively 622\nf(\u03d5) = \u2211 r c(r, \u03d5)\u2211 r u(r, \u03d5) , f(r) = \u2211 \u03d5 c(r, \u03d5)\u2211 \u03d5 u(r, \u03d5) , and f(r, \u03d5) = c(r, \u03d5) u(r, \u03d5) . (14)\nInformation per spike as a function of theta phase is calculated by 623\nI(\u03d5) = \u2211 r c(r, \u03d5) c(\u03d5) log2 f(r, \u03d5) f(\u03d5) , (15)\nwhere c(\u03d5) = \u2211 r c(r, \u03d5) (Skaggs et al., 1993). To perform sparsity correction for each neuron, we generate 100 null- 624 matched neurons in which the spatial bin of each spike is replaced by a random value uniformly distributed across 625 spatial bins. We subtract the mean I(\u03d5) over the null matches from the I(\u03d5) for the true data. To calculate the 626 average difference in information between sparse and dense phases, we first f(\u03d5) to partition \u03d5 into sparse and dense 627 halves. We then average the sparsity-corrected I(\u03d5) over each half, apply a ReLU function to each half to prevent 628 negative information values, and compute the difference between halves. 629\nSee Supplementary Methods for dataset preprocessing details. 630\nModel prediction 631\nFor the example prediction in Fig. 5A, we choose one concept from Fig. 2A and find 50 neurons that are active in 632 at least one MF example and one PP example within it. For each neuron, we convert each active response to one 633 spike and assign equal occupancies across all examples. We calculate the information per spike across MF examples 634 and across PP examples using example identity \u03bd as the spatial bin r. These values are sparsity-corrected with 50 635 null-matched neurons, and their difference becomes our example prediction, associating MF encodings with sparse 636 phases and PP with dense. 637\nFor the concept prediction in Fig. 6A, we find 50 neurons that are active in at least one MF example and one 638 PP example within any concept. For each neuron, we convert each active response to one spike and collect MF and 639 PP concept responses by summing spikes within each concept. We assign equal occupancies across all concepts. We 640 calculate the information per spike across MF concepts and across PP concepts using concept identity \u00b5 as the spatial 641 bin r. We then proceed as in the example case to produce our concept prediction. 642\nLinear track data 643\nSingle neurons in Fig. 6 are preprocessed from the CRCNS hc-3 dataset as described in Supplementary Methods 644 (Mizuseki et al., 2013). To identify place cells, we compute the phase-independent position information per spike 645 using 1 cm-bins across all theta phases, and we select neurons with values greater than 0.5. For each place cell, we 646 bin spikes into various position bins as illustrated and phase bins of width 30\u25e6. Since our prediction compares sparse 647 and dense information conveyed by the same neurons, we require at least 8 spikes within each phase value to allow 648 for accurate estimation of position information across all theta phases. To ensure theta modulation, we also require 649 the most active phase to contain at least twice the number of spikes as the least active phase. In Fig. 6E, these 650 constraints yield 47, 49, and 56 valid CA3 neurons respectively for track scales 1/16, 1/8, and 1/4, and in Fig. S6B, 651 they yield 122, 137, and 144 valid CA1 neurons. 652\nPlace fields in Fig. 5 are extracted as described in Supplementary Methods. Processing occurs similarly to the 653 whole-track case above, except we do not enforce a phase-independent information constraint, we use 5 progress bins, 654 and we require at least 5 spikes within each phase value. These constraints yield 35 valid CA3 fields and 47 valid 655 CA1 fields. Phase precession is detected by performing circular\u2013linear regression between spike progresses and phases 656 (Kempter et al., 2012; Kang and DeWeese, 2019). The precession score and precession slope are respectively defined 657 to be the mean resultant length and regression slope. Precessing neurons have score greater than 0.3 and negative 658 slope steeper than \u221272 \u25e6/field. 659\nW-maze data 660\nSingle neurons in Fig. 7A\u2013H are preprocessed from the CRCNS hc-6 dataset as described in Supplementary Methods 661 (Karlsson et al., 2015). For each neuron, we bin spikes into 2 turn directions and phase bins of width 45\u25e6. Since our 662 prediction compares sparse and dense information conveyed by the same neurons, we require at least 5 spikes within 663 each phase value to allow for accurate estimation of position information across all theta phases. To ensure theta 664 modulation, we also require the most active phase to contain at least twice the number of spikes as the least active 665 phase. These constraints yield 99 valid CA3 neurons and 187 valid CA1 neurons. 666\nBayesian population decoding in Fig. 7I\u2013L involves the same binning as in the single-neuron case above, and we 667 enforce a minimum spike count of 30 across all phases instead of a minimum for each phase value. We do not ensure 668 theta modulation on a single-neuron basis. We consider all sessions in which at least 5 neurons are simultaneously 669 recorded; there are 8 valid CA3 sessions and 25 valid CA1 sessions. For each session, we compute the total activity 670 across neurons and turn directions as a function of theta phase to determine the sparsest and densest half of phases 671 (similarly to Eq. 14). We then compute activities fi(r, \u03c8) over each half, indexed by \u03c8 \u2208 {sparse, dense}, for neurons 672 i and turn directions r. For each neuron, we rectify all activity values below 0.02 times its maximum. 673\nWe decode turn direction during runs along the center arm using sliding windows of width \u2206t = 0.5 s and stride 674 0.25 s. In each window at time t, we tabulate the population spike count c(t, \u03c8) over sparse and dense phases \u03c8. The 675\nlikelihood that it arose from turn direction r is 676\np(c(t, \u03c8)|r) = \u220f i p(ci(t, \u03c8)|r) \u221d (\u220f i fi(r, \u03c8) ci(t,\u03c8) ) exp ( \u2212\u2206t \u2211 i fi(r, \u03c8) ) . (16)\nThis formula assumes that spikes are independent across neurons and time and obey Poisson statistics (Zhang et al., 677 1998). We only decode with at least 2 spikes. By Bayes\u2019s formula and assuming a uniform prior, the likelihood is 678 proportional to the posterior probability p(r|c(t, \u03c8)) of turn direction r decoded from spikes c(t, \u03c8). Consider one 679 decoding that yields p(R) as the probability of a right turn. Its confidence is |2p(R)\u22121|. Its accuracy is 1 if p(R) > 0.5 680 and the true turn direction is right or if p(R) < 0.5 and the true direction is left; otherwise, its accuracy is 0. 681\nMachine learning with multilayer perceptrons 682\nDataset 683\nWe use the MNIST dataset of handwritten digits (LeCun et al., 1998). Each image i\u03b1 is normalized by subtracting 684 the mean value and dividing by the standard deviation across all images and pixels. In addition to its digit class label, 685 we randomly assign a set number. We train networks on a subset of images from the train dataset. To test concept 686 learning through digit classification, we use all held-out images from the test dataset. To test example learning 687 through set identification, we use all train images corrupted by randomly setting 20% of normalized pixel values to 0. 688\nSingle-task learning 689\nWe train a fully-connected two-layer perceptron with a hyperbolic tangent (tanh) activation function applied to each 690 hidden layer and a softmax activation function applied to the output layer. Each hidden layer contains 50 neurons, 691 and the output layer contains 10 neurons for digit classification and as many neurons as sets for set identification. 692\nLet s\u03b1 be the activations of the final hidden layer for image \u03b1. The loss is composed of a cross-entropy loss 693 function between reconstructed labels y\u0302\u03b1 and true labels y\u03b1, which are one-hot encodings of either digit class or set 694 number, and the DeCorr loss function: 695\nL = \u2212 \u2211 \u03b1\u2208\nbatch\nN\u22121\u2211 i=0 [ y\u03b1i log y\u0302\u03b1i + (1 \u2212 y\u03b1i) log(1 \u2212 y\u0302\u03b1i) ] + \u03bbLDeCorr, (17)\nwhere 696\nLDeCorr = 1\n2 \u2211 \u03b1\u0338=\u03b2\u2208 batch\n[\u2211N\u22121 i=0 (s\u03b1i \u2212 s\u0304\u03b1)(s\u03b2i \u2212 s\u0304\u03b2) ]2[\u2211N\u22121 i=0 (s\u03b1i \u2212 s\u0304\u03b1)2 +N\u03f5 ][\u2211N\u22121 i=0 (s\u03b2i \u2212 s\u0304\u03b2)2 +N\u03f5\n] . (18) We introduce \u03f5 = 0.001, which is scaled by the number of hidden layer neurons N , to aid numerical convergence. 697 Mean activations are s\u0304\u03b1 = (1/N) \u2211N\u22121 i=0 s\u03b1i. The DeCorr strength is \u03bb; except for Fig. S8A, B, we use \u03bb = 0 for the 698 baseline case and \u03bb = 1 for the DeCorr case. 699 We train the network using stochastic gradient descent (SGD) with batch size 50, learning rate 10\u22124, and mo- 700 mentum 0.9. In general, we train until the network reaches >99.9% accuracy with the train dataset. For example, 701 we use 40, 100, and 200 epochs respectively for digit classification and set identification with 10 and 50 sets. 702\nIn contrast to DeCorr, the DeCov loss function formulated to reduce overfitting is 703\nLDeCov = 1\n2 N\u22121\u2211 i\u0338=j=0 [\u2211 \u03b1\u2208\nbatch\n(s\u03b1i \u2212 s\u0304i)(s\u03b1j \u2212 s\u0304j) ]2 , (19)\nwhere mean activations are now taken over batch items: s\u0304i = (1/Nbatch) \u2211 \u03b1\u2208batch s\u03b1i (Cogswell et al., 2015). 704\nMultitask learning 705\nWe train a fully-connected two-layer perceptron with a hyperbolic tangent (tanh) activation function applied to each 706 hidden layer. In Fig. S8E, F, we also consider applying a ReLU activation function to each hidden layer, or a ReLU 707 to the first hidden layer and no nonlinearity to the second. The final hidden layer is fully connected to two output 708 layers, one for digit classification and the other for set identification. A softmax activation function applied to each 709 layer. Each hidden layer contains 100 neurons, the concept output layer contains 10 neurons, and the example output 710 layer contains as many neurons as sets. 711\nThe loss is composed of a cross-entropy loss function between reconstructed y\u0302\u03b1 and true y\u03b1 digit labels, a 712 cross-entropy loss function between reconstructed z\u0302\u03b1 and true z\u03b1 set labels, and either the DeCorr or HalfCorr loss 713 function: 714\nL = \u2212 \u2211 \u03b1\u2208\nbatch\nN\u22121\u2211 i=0 [ y\u03b1i log y\u0302\u03b1i+(1\u2212y\u03b1i) log(1\u2212y\u0302\u03b1i) ] \u2212 \u2211 \u03b1\u2208\nbatch\nN\u22121\u2211 i=0 [ z\u03b1i log z\u0302\u03b1i+(1\u2212z\u03b1i) log(1\u2212z\u0302\u03b1i) ] +\u03bbLDeCorr/HalfCorr, (20)\nwhere 715\nLHalfCorr = 1\n2 \u2211 \u03b1\u0338=\u03b2\u2208 batch\n[\u2211N\u22121 i=N/2(s\u03b1i \u2212 s\u0304\u03b1)(s\u03b2i \u2212 s\u0304\u03b2) ]2[\u2211N\u22121 i=N/2(s\u03b1i \u2212 s\u0304\u03b1)2 +N\u03f5/2 ][\u2211N\u22121 i=N/2(s\u03b2i \u2212 s\u0304\u03b2)2 +N\u03f5/2\n] . (21) Mean activations are s\u0304\u03b1 = (2/N) \u2211N\u22121 i=N/2 s\u03b1i. The DeCorr/HalfCorr strength is \u03bb; we use \u03bb = 1 with a tanh activation 716 function, \u03bb = 0.04 with a ReLU, \u03bb = 2 with no nonlinearity, and \u03bb = 0 for the baseline case with any nonlinearity. 717 We train the network using stochastic gradient descent (SGD) with batch size 50 and learning rate 10\u22124. In 718 general, we train until the network reaches >99.9% accuracy in both tasks with the train dataset. For example, we 719 use 100 epochs for the results in Fig. 8I, J. 720\nCode and data availability 721\nAll network training and simulation code is available at https://louiskang.group/repo. All experimental data are 722 taken from the Collaborative Research in Computational Neuroscience (CRCNS) hc-3 dataset contributed by Gyo\u0308rgy 723 Buzsa\u0301ki and colleagues (Mizuseki et al., 2013) and the hc-6 dataset contributed by Loren Frank and colleagues 724 (Karlsson et al., 2015). They are publicly available at https://crcns.org/data-sets/hc. 725\nAcknowledgments 726\nWe thank Tom McHugh and Lukasz Kus\u0301mierz for helpful ideas. LK is supported by JSPS KAKENHI for Early- 727 Career Scientists (22K15209) and has been supported by the Miller Institute for Basic Research in Science and a 728 Burroughs Wellcome Fund Collaborative Research Travel Grant. TT is supported by Brain/MINDS from AMED 729 (JP19dm0207001) and JSPS KAKENHI (JP18H05432). 730\nReferences 731\nJ. B. Aimone, W. Deng, and F. H. Gage. Resolving new memories: A critical look at the dentate gyrus, adult 732 neurogenesis, and pattern separation. Neuron, 70(4):589\u2013596, 2011. 733\nD. Amaral and L. Pierre. Hippocampal neuroanatomy. In P. Andersen, R. Morris, D. Amaral, T. Bliss, and J. O\u2019Keefe, 734 editors, The Hippocampus Book, The Hippocampus Book, pages 37\u2013114. Oxford University Press, 2006. 735\nD. G. Amaral, N. Ishizuka, and B. Claiborne. Neurons, numbers and the hippocampal network. Prog. Brain Res., 736 83:1\u201311, 1990. 737\nM. Amer and T. Maul. A review of modularization techniques in artificial neural networks. Artif. Intell. Rev., 52(1): 738 527\u2013561, 2019. 739\nD. J. Amit, H. Gutfreund, and H. Sompolinsky. Spin-glass models of neural networks. Phys. Rev. A, 32(2):1007\u20131018, 740 1985. 741\nJ. R. Anderson. The adaptive nature of human categorization. Psychol. Rev., 98(3):409\u2013429, 1991. 742\nJ. Antony, X. L. Liu, Y. Zheng, C. Ranganath, and R. C. O\u2019Reilly. Memory out of context: Spacing effects and 743 decontextualization in a computational model of the medial temporal lobe. bioRxiv 2022.12.01.518703, 2023. 744\nK. Arulkumaran, M. P. Deisenroth, M. Brundage, and A. A. Bharath. Deep reinforcement learning. IEEE Signal 745 Process. Mag., 34(6):26\u201338, 2017. 746\nF. G. Ashby and W. T. Maddox. Human category learning. Annu. Rev. Psychol., 56(1):149\u2013178, 2005. 747\nS. M. Attili, M. F. M. Silva, T.-v. Nguyen, and G. A. Ascoli. Cell numbers, distribution, shape, and regional variation 748 throughout the murine hippocampal formation from the adult brain Allen Reference Atlas. Brain Struct. Funct., 749 224(8):2883\u20132897, 2019. 750\nA. R. Backus, J.-M. Schoffelen, S. Szebe\u0301nyi, S. Hanslmayr, and C. F. Doeller. Hippocampal-prefrontal theta oscilla- 751 tions support memory integration. Curr. Biol., 26(4):450\u2013457, 2016. 752\nY. Bengio, N. Le\u0301onard, and A. Courville. Estimating or propagating gradients through stochastic neurons for 753 conditional computation. arXiv 1308.3432, 2013. 754\nS. D. Berry and R. F. Thompson. Prediction of learning rate from the hippocampal electroencephalogram. Science, 755 200(4347):1298\u20131300, 1978. 756\nG.-q. Bi and M.-m. Poo. Synaptic modifications in cultured hippocampal neurons: Dependence on spike timing, 757 synaptic strength, and postsynaptic cell type. J. Neurosci., 18(24):10464\u201310472, 1998. 758\nM. Borzello, S. Ramirez, A. Treves, I. Lee, H. Scharfman, C. Stark, J. J. Knierim, and L. M. Rangel. Assessments of 759 dentate gyrus function: discoveries and debates. Nat. Rev. Neurosci., 24(8):502\u2013517, 2023. 760\nC. R. Bowman and D. Zeithamova. Abstract memory representations in the ventromedial prefrontal cortex and 761 hippocampus support concept generalization. J. Neurosci., 38(10):2605\u20132614, 2018. 762\nM. Carandini and D. J. Heeger. Normalization as a canonical neural computation. Nat. Rev. Neurosci., 13(1):51\u201362, 763 2012. 764\nN. A. Cayco-Gajic, C. Clopath, and R. A. Silver. Sparse synaptic connectivity is required for decorrelation and 765 pattern separation in feedforward networks. Nat. Commun., 8(1):1116, 2017. 766\nY. Chen, D. Paiton, and B. Olshausen. The Sparse Manifold Transform. Advances in Neural Information Processing 767 Systems, pages 10513 \u2013 10524. Curran Associates, Inc., 2018. 768\nM. Cogswell, F. Ahmed, R. Girshick, L. Zitnick, and D. Batra. Reducing overfitting in deep networks by decorrelating 769 representations. arXiv 1511.06068, 2015. 770\nL. L. Colgin. Mechanisms and functions of theta rhythms. Annu. Rev. Neurosci., 36(1):295\u2013312, 2013. 771\nN. V. Covington, S. Brown-Schmidt, and M. C. Duff. The necessity of the hippocampus for statistical learning. J. 772 Cognit. Neurosci., 30(5):680\u2013697, 2018. 773\nN. M. Dotson and M. M. Yartsev. Nonlocal spatiotemporal representation in the hippocampus of freely flying bats. 774 Science, 373(6551):242\u2013247, 2021. 775\nM. C. Duff, N. V. Covington, C. Hilverman, and N. J. Cohen. Semantic memory and the hippocampus: Revisiting, 776 reaffirming, and extending the reach of their critical relationship. Front. Hum. Neurosci., 13:471, 2020. 777\nE\u0301. Duvelle, R. M. Grieves, and M. A. van der Meer. Temporal context and latent state inference in the hippocampal 778 splitter signal. eLife, 12:e82357, 2023. 779\nE. Engin, E. D. Zarnowska, D. Benke, E. Tsvetkov, M. Sigal, R. Keist, V. Y. Bolshakov, R. A. Pearce, and U. Rudolph. 780 Tonic inhibitory control of dentate gyrus granule cells by \u03b15-containing GABAA receptors reduces memory inter- 781 ference. The Journal of Neuroscience, 35(40):13698\u201313712, 2015. 782\nK. A. Ferguson and J. A. Cardin. Mechanisms underlying gain modulation in the cortex. Nat. Rev. Neurosci., 21(2): 783 80\u201392, 2020. 784\nJ. F. Fontanari. Generalization in a Hopfield network. J. Phys., 51(21):2421\u20132430, 1990. 785\nL. M. Frank, E. N. Brown, and M. Wilson. Trajectory encoding in the hippocampus and entorhinal cortex. Neuron, 786 27(1):169\u2013178, 2000. 787\nM. E. Hasselmo. The role of acetylcholine in learning and memory. Curr. Opin. Neurobiol., 16(6):710\u2013715, 2006. 788\nM. E. Hasselmo, C. Bodeln, and B. P. Wyble. A proposed function for hippocampal theta rhythm: Separate phases 789 of encoding and retrieval enhance reversal of prior learning. Neural Comput., 14(4):793\u2013817, 2002. 790\nD. A. Henze, L. Wittner, and G. Buzsa\u0301ki. Single granule cells reliably discharge targets in the hippocampal CA3 791 network in vivo. Nat. Neurosci., 5(8):790\u2013795, 2002. 792\nN. A. Herweg, E. A. Solomon, and M. J. Kahana. Theta oscillations in human memory. Trends in Cognitive Sciences, 793 24(3):208\u2013227, 2020. 794\nJ. J. Hopfield. Neural networks and physical systems with emergent collective computational abilities. Proc. Natl. 795 Acad. Sci. U.S.A., 79(8):2554\u20132558, 1982. 796\nJ. Isaacson and M. Scanziani. How inhibition shapes cortical activity. Neuron, 72(2):231\u2013243, 2011. 797\nJ. Jacobs, G. Hwang, T. Curran, and M. J. Kahana. EEG oscillations and recognition memory: Theta correlates of 798 memory retrieval and decision making. Neuroimage, 32(2):978\u2013987, 2006. 799\nP. Kaifosh and A. Losonczy. Mnemonic functions for nonlinear dendritic integration in hippocampal pyramidal 800 circuits. Neuron, 90(3):622\u2013634, 2016. 801\nL. Kang and M. R. DeWeese. Replay as wavefronts and theta sequences as bump oscillations in a grid cell attractor 802 network. eLife, 8:e46351, 2019. 803\nL. Kang and T. Toyoizumi. A Hopfield-like model with complementary encodings of memories. arXiv 2302.04481, 804 2023. 805\nM. Karlsson, M. Carr, and L. M. Frank. Simultaneous extracellular recordings from hippocampal areas CA1 and 806 CA3 (or MEC and CA1) from rats performing an alternation task in two W-shapped tracks that are geometrically 807 identically but visually distinct. CRCNS.org, 2015. 808\nK. Kay, J. E. Chung, M. Sosa, J. S. Schor, M. P. Karlsson, M. C. Larkin, D. F. Liu, and L. M. Frank. Constant 809 sub-second cycling between representations of possible futures in the hippocampus. Cell, 180(3):552\u2013567, 2020. 810\nR. Kempter, C. Leibold, G. Buzsa\u0301ki, K. Diba, and R. Schmidt. Quantifying circular\u2013linear associations: Hippocampal 811 phase precession. J. Neurosci. Methods, 207(1):113\u2013124, 2012. 812\nS. Kim, S. J. Guzman, H. Hu, and P. Jonas. Active dendrites support efficient initiation of dendritic spikes in 813 hippocampal CA3 pyramidal neurons. Nat. Neurosci., 15(4):600\u2013606, 2012. 814\nB. J. Knowlton and L. R. Squire. The learning of categories: Parallel brain systems for item memory and category 815 knowledge. Science, 262(5140):1747\u20131749, 1993. 816\nG. Kowadlo, A. Ahmed, and D. Rawlinson. AHA! an \u2019Artificial Hippocampal Algorithm\u2019 for episodic machine 817 learning. arXiv 1909.10340, 2020. 818\nA. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Technical Report 0, University 819 of Toronto, Toronto, Ontario, 2009. 820\nS. Kunec, M. E. Hasselmo, and N. Kopell. Encoding and retrieval in the CA3 region of the hippocampus: A model 821 of theta-phase separation. J. Neurophysiol., 94(1):70\u201382, 2005. 822\nQ. V. Le, A. Karpenko, J. Ngiam, and A. Y. Ng. ICA with reconstruction cost for efficient overcomplete feature 823 learning. Adv. Neural Inf. Process. Syst. 1017\u20131025, 2011. 824\nY. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proc. 825 IEEE, 86(11):2278\u20132324, 1998. 826\nY. LeCun, Y. Bengio, and G. Hinton. Deep learning. Nature, 521(7553):436\u2013444, 2015. 827\nH. Lee, A. Battle, R. Raina, and A. Y. Ng. Efficient sparse coding algorithms. Adv. Neural Inf. Process. Syst., 19, 828 2006. 829\nB. C. Lega, J. Jacobs, and M. Kahana. Human hippocampal theta oscillations and the formation of episodic memories. 830 Hippocampus, 22(4):748\u2013761, 2012. 831\nL. S. Leung and C. S. H. Law. Phasic modulation of hippocampal synaptic plasticity by theta rhythm. Behav. 832 Neurosci., 134(6):595\u2013612, 2020. 833\nJ. K. Leutgeb, S. Leutgeb, M.-B. Moser, and E. I. Moser. Pattern separation in the dentate gyrus and CA3 of the 834 hippocampus. Science, 315(5814):961\u2013966, 2007. 835\nB. C. Love, D. L. Medin, and T. M. Gureckis. SUSTAIN: A network model of category learning. Psychol. Rev., 111 836 (2):309\u2013332, 2004. 837\nM. L. Mack, B. C. Love, and A. R. Preston. Dynamic updating of hippocampal object representations reflects new 838 conceptual knowledge. Proc. Natl. Acad. Sci. U.S.A., 113(46):13203\u201313208, 2016. 839\nJ. Makara and J. Magee. Variable dendritic integration in hippocampal CA3 pyramidal neurons. Neuron, 80(6): 840 1438\u20131450, 2013. 841\nA. Makhzani and B. Frey. k-sparse autoencoders. arXiv 1312.5663, 2014. 842\nJ. R. Manns, R. O. Hopkins, and L. R. Squire. Semantic memory and the human hippocampus. Neuron, 38(1): 843 127\u2013133, 2003. 844\nD. Marr. Simple memory: a theory for archicortex. Philos. Trans. R. Soc. B, 262(841):23\u201381, 1971. 845\nC. J. McAdams and J. H. R. Maunsell. Effects of attention on orientation-tuning functions of single neurons in 846 Macaque cortical area V4. J. Neurosci., 19(1):431\u2013441, 1999. 847\nJ. L. McClelland and N. H. Goddard. Considerations arising from a complementary learning systems perspective on 848 hippocampus and neocortex. Hippocampus, 6(6):654\u2013665, 1996. 849\nB. McNaughton and R. Morris. Hippocampal synaptic enhancement and information storage within a distributed 850 memory system. Trends Neurosci., 10(10):408\u2013415, 1987. 851\nM. R. Mehta, A. K. Lee, and M. A. Wilson. Role of experience and oscillations in transforming a rate code into a 852 temporal code. Nature, 417(6890):741\u2013746, 2002. 853\nR. K. Mishra, S. Kim, S. J. Guzman, and P. Jonas. Symmetric spike timing-dependent plasticity at CA3\u2013CA3 854 synapses optimizes storage and recall in autoassociative networks. Nat. Commun., 7:11552, 2016. 855\nK. Mizuseki and G. Buzsa\u0301ki. Preconfigured, skewed distribution of firing rates in the hippocampus and entorhinal 856 cortex. Cell Rep., 4(5):1010\u20131021, 2013. 857\nK. Mizuseki, A. Sirota, E. Pastalkova, K. Diba, and G. Buzsa\u0301ki. Multiple single unit recordings from different rat 858 hippocampal and entorhinal regions while the animals were performing multiple behavioral tasks. CRCNS.org, 859 2013. 860\nK. Mizuseki, K. Diba, E. Pastalkova, J. Teeters, A. Sirota, and G. Buzsa\u0301ki. Neurosharing: large-scale data sets (spike, 861 LFP) recorded from the hippocampal-entorhinal system in behaving rats. F1000Research, 3:98, 2014. 862\nT. C. Murakami, T. Mano, S. Saikawa, S. A. Horiguchi, D. Shigeta, K. Baba, H. Sekiya, Y. Shimizu, K. F. Tanaka, 863 H. Kiyonari, M. Iino, H. Mochizuki, K. Tainaka, and H. R. Ueda. A three-dimensional single-cell-resolution 864 whole-brain atlas using CUBIC-X expansion microscopy and tissue clearing. Nat. Neurosci., 21(4):625\u2013637, 2018. 865\nY. Norman, O. Raccah, S. Liu, J. Parvizi, and R. Malach. Hippocampal ripples and their coordinated dialogue with 866 the default mode network during recent and remote recollection. Neuron, 109(17):2767\u20132780, 2021. 867\nJ. O\u2019Keefe and M. L. Recce. Phase relationship between hippocampal place units and the EEG theta rhythm. 868 Hippocampus, 3(3):317\u2013330, 1993. 869\nB. A. Olshausen and D. J. Field. Emergence of simple-cell receptive field properties by learning a sparse code for 870 natural images. Nature, 381(6583):607\u2013609, 1996. 871\nB. A. Olshausen and D. J. Field. Sparse coding of sensory inputs. Curr. Opin. Neurobiol., 14(4):481\u2013487, 2004. 872\nR. C. O\u2019Reilly and J. L. McClelland. Hippocampal conjunctive encoding, storage, and recall: Avoiding a trade-off. 873 Hippocampus, 4(6):661\u2013682, 1994. 874\nR. C. O\u2019Reilly and J. W. Rudy. Conjunctive representations in learning and memory: Principles of cortical and 875 hippocampal function. Psychol. Rev., 108(2):311\u2013345, 2001. 876\nG. I. Parisi, R. Kemker, J. L. Part, C. Kanan, and S. Wermter. Continual lifelong learning with neural networks: A 877 review. Neural Networks, 113:54\u201371, 2019. 878\nS. C. Penley, J. R. Hinman, L. L. Long, E. J. Markus, M. A. Escab\u0301\u0131, and J. J. Chrobak. Novel space alters theta and 879 gamma synchrony across the longitudinal axis of the hippocampus. Front. Syst. Neurosci., 7:20, 2013. 880\nB. E. Pfeiffer and D. J. Foster. Autoassociative dynamics in the generation of sequences of hippocampal place cells. 881 Science, 349(6244):180\u2013183, 2015. 882\nX. Pitkow and M. Meister. Decorrelation and efficient coding by retinal ganglion cells. Nat. Neurosci., 15(4):628\u2013635, 883 2012. 884\nS. E. Qasim, I. Fried, and J. Jacobs. Phase precession in the human hippocampus and entorhinal cortex. Cell, 184 885 (12):3242\u20133255, 2021. 886\nR. Quian Quiroga, L. Reddy, G. Kreiman, C. Koch, and I. Fried. Invariant visual representation by single neurons 887 in the human brain. Nature, 435(7045):1102\u20131107, 2005. 888\nR. Quian Quiroga, A. Kraskov, C. Koch, and I. Fried. Explicit encoding of multimodal percepts by single neurons in 889 the human brain. Curr. Biol., 19(15):1308\u20131313, 2009. 890\nL. Reddy, M. W. Self, B. Zoefel, M. Poncet, J. K. Possel, J. C. Peters, J. C. Baayen, S. Idema, R. VanRullen, and 891 P. R. Roelfsema. Theta-phase dependent neuronal coding during sequence learning in human single neurons. Nat. 892 Commun., 12(1):4839, 2021. 893\nE. T. Rolls and R. P. Kesner. A computational theory of hippocampal function, and empirical tests of the theory. 894 Prog. Neurobiol., 79(1):1\u201348, 2006. 895\nG. Rosen, A. Williams, J. Capra, M. Connolly, B. Cruz, L. Lu, D. Airey, K. Kulkarni, and R. Williams. The Mouse 896 Brain Library @ www.mbl.org. Int Mouse Genome Conference, 14:166, 2000. 897\nA. C. Schapiro, E. Gregory, B. Landau, M. McCloskey, and N. B. Turk-Browne. The necessity of the medial temporal 898 lobe for statistical learning. J. Cognit. Neurosci., 26(8):1736\u20131747, 2014. 899\nA. C. Schapiro, N. B. Turk-Browne, M. M. Botvinick, and K. A. Norman. Complementary learning systems within 900 the hippocampus: a neural network modelling approach to reconciling episodic memory with statistical learning. 901 Philos. Trans. R. Soc. B, 372(1711):20160049, 2017. 902\nW. B. Scoville and B. Milner. Loss of recent memory after bilateral hippocampal lesions. J. Neurol. Neurosurg. 903 Psychiatry, 20(1):11, 1957. 904\nM. A. Seager, L. D. Johnson, E. S. Chabot, Y. Asaka, and S. D. Berry. Oscillatory brain states and learning: Impact 905 of hippocampal theta-contingent training. Proc. Natl. Acad. Sci. U.S.A., 99(3):1616\u20131620, 2002. 906\nJ. H. Siegle and M. A. Wilson. Enhancement of encoding and retrieval functions through theta phase-specific 907 manipulation of hippocampus. eLife, 3:e03061, 2014. 908\nW. E. Skaggs, B. L. McNaughton, K. M. Gothard, and E. J. Markus. An information-theoretic approach to deciphering 909 the hippocampal code. Adv. Neural Inf. Process. Syst., 1993. 910\nW. E. Skaggs, B. L. McNaughton, M. A. Wilson, and C. A. Barnes. Theta phase precession in hippocampal neuronal 911 populations and the compression of temporal sequences. Hippocampus, 6(2):149\u2013172, 1996. 912\nB. C. Souza and A. B. L. Tort. Asymmetry of the temporal code for space by hippocampal place cells. Sci. Rep., 7 913 (1):8507, 2017. 914\nL. R. Squire. Memory and the hippocampus: A synthesis from findings with rats, monkeys, and humans. Psychol. 915 Rev., 99(2):195\u2013231, 1992. 916\nJ. Suc\u030cevic\u0301 and A. C. Schapiro. A neural network model of hippocampal contributions to category learning. bioRxiv 917 2022.01.12.476051, 2022. 918\nV. Sze, Y.-H. Chen, T.-J. Yang, and J. S. Emer. Efficient processing of deep neural networks: A tutorial and survey. 919 Proc. IEEE, 105(12):2295\u20132329, 2017. 920\nS. Terada, Y. Sakurai, H. Nakahara, and S. Fujisawa. Temporal and rate coding for discrete event sequences in the 921 hippocampus. Neuron, 94(6):1248\u20131262, 2017. 922\nT. J. Teyler and P. DiScenna. The hippocampal memory indexing theory. Behav. Neurosci., 100(2):147\u2013154, 1986. 923\nT. J. Teyler and J. W. Rudy. The hippocampal indexing theory and episodic memory: Updating the index. Hip- 924 pocampus, 17(12):1158\u20131169, 2007. 925\nA. Treves and E. T. Rolls. Computational constraints suggest the need for two distinct input systems to the hip- 926 pocampal CA3 network. Hippocampus, 2(2):189\u2013199, 1992. 927\nM. V. Tsodyks and M. V. Feigel\u2019man. The enhanced storage capacity in neural networks with low activity level. 928 Europhys. Lett., 6(2):101\u2013105, 1988. 929\nB. B. Ujfalussy and G. Orba\u0301n. Sampling motion trajectories during hippocampal theta sequences. eLife, 11:e74058, 930 2022. 931\nW. E. Vinje and J. L. Gallant. Sparse coding and decorrelation in primary visual cortex during natural vision. 932 Science, 287(5456):1273\u20131276, 2000. 933\nN. P. Vyleta, C. Borges-Merjane, and P. Jonas. Plasticity-dependent, full detonation at hippocampal mossy fiber\u2013CA3 934 pyramidal neuron synapses. eLife, 5:3386, 2016. 935\nJ. C. Whittington, T. H. Muller, S. Mark, G. Chen, C. Barry, N. Burgess, and T. E. Behrens. The Tolman-Eichenbaum 936 Machine: Unifying space and relational memory through generalization in the hippocampal formation. Cell, 183 937 (5):1249\u20131263, 2020. 938\nB. Willmore and D. J. Tolhurst. Characterizing the sparseness of neural codes. Netw. Comput. Neural Syst., 12(3): 939 255\u2013270, 2001. 940\nE. R. Wood, P. A. Dudchenko, R. Robitsek, and H. Eichenbaum. Hippocampal neurons encode information about 941 different types of memory episodes occurring in the same location. Neuron, 27(3):623\u2013633, 2000. 942\nH. Xiao, K. Rasul, and R. Vollgraf. Fashion-MNIST: a novel image dataset for benchmarking machine learning 943 algorithms. arXiv 1708.07747, 2017. 944\nD. Zeithamova, W. T. Maddox, and D. M. Schnyer. Dissociable prototype learning systems: Evidence from brain 945 imaging and behavior. J. Neurosci., 28(49):13194\u201313201, 2008. 946\nK. Zhang, I. Ginzburg, B. L. McNaughton, and T. J. Sejnowski. Interpreting neuronal population activity by 947 reconstruction: Unified framework with application to hippocampal place cells. J. Neurophysiol., 79(2):1017\u20131044, 948 1998. 949\nJ. Zheng, R. F. Stevenson, B. A. Mander, L. Mnatsakanyan, F. P. Hsu, S. Vadera, R. T. Knight, M. A. Yassa, 950 and J. J. Lin. Multiplexing of theta and alpha rhythms in the amygdala-hippocampal circuit supports pattern 951 separation of emotional information. Neuron, 102(4):887\u2013898, 2019. 952"
        }
    ],
    "title": "Distinguishing examples while building concepts in hippocampal and artificial networks",
    "year": 2023
}