{
    "abstractText": "Entity relation extraction consists of two subtasks: entity recognition and relation extraction. Existing methods either tackle these two tasks separately or unify them with word-byword interactions. In this paper, we propose HIORE, a new method for unified entity relation extraction. The key insight is to leverage the high-order interactions, i.e., the complex association among word pairs, which contains richer information than the first-order wordby-word interactions. For this purpose, we first devise a W-shape DNN (WNet) to capture coarse-level high-order connections. Then, we build a heuristic high-order graph and further calibrate the representations with a graph neural network (GNN). Experiments on three benchmarks (ACE04, ACE05, SciERC) show that HIORE achieves the state-of-the-art performance on relation extraction and an improvement of 1.1 \u223c 1.8 F1 points over the prior best unified model.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yijun Wang"
        },
        {
            "affiliations": [],
            "name": "Changzhi Sun"
        },
        {
            "affiliations": [],
            "name": "Yuanbin Wu"
        },
        {
            "affiliations": [],
            "name": "Lei Li"
        },
        {
            "affiliations": [],
            "name": "Junchi Yan"
        },
        {
            "affiliations": [],
            "name": "Hao Zhou"
        }
    ],
    "id": "SP:4197f04f3af5af8251b9e86271ccc6f7428bde7c",
    "references": [
        {
            "authors": [
                "Jasmijn Bastings",
                "Ivan Titov",
                "Wilker Aziz",
                "Diego Marcheggiani",
                "Khalil Sima\u2019an"
            ],
            "title": "Graph convolutional encoders for syntax-aware neural machine translation",
            "year": 2017
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "ACL.",
            "year": 2019
        },
        {
            "authors": [
                "George Doddington",
                "Alexis Mitchell",
                "Mark Przybocki",
                "Lance Ramshaw",
                "Stephanie Strassel",
                "Ralph Weischedel."
            ],
            "title": "The automatic content extraction (ACE) program \u2013 tasks, data, and evaluation",
            "venue": "LREC.",
            "year": 2004
        },
        {
            "authors": [
                "Timothy Dozat",
                "Christopher D. Manning."
            ],
            "title": "Deep biaffine attention for neural dependency parsing",
            "venue": "ICLR.",
            "year": 2017
        },
        {
            "authors": [
                "Arzoo Katiyar",
                "Claire Cardie."
            ],
            "title": "Investigating lstms for joint extraction of opinion entities and relations",
            "venue": "ACL.",
            "year": 2016
        },
        {
            "authors": [
                "Arzoo Katiyar",
                "Claire Cardie."
            ],
            "title": "Going out on a limb: Joint extraction of entity mentions and relations without dependency trees",
            "venue": "ACL.",
            "year": 2017
        },
        {
            "authors": [
                "Qi Li",
                "Heng Ji."
            ],
            "title": "Incremental joint extraction of entity mentions and relations",
            "venue": "ACL.",
            "year": 2014
        },
        {
            "authors": [
                "Xiaoya Li",
                "Fan Yin",
                "Zijun Sun",
                "Xiayu Li",
                "Arianna Yuan",
                "Duo Chai",
                "Mingxin Zhou",
                "Jiwei Li."
            ],
            "title": "Entity-relation extraction as multi-turn question answering",
            "venue": "ACL.",
            "year": 2019
        },
        {
            "authors": [
                "Ying Lin",
                "Heng Ji",
                "Fei Huang",
                "Lingfei Wu."
            ],
            "title": "A joint neural model for information extraction with global features",
            "venue": "ACL.",
            "year": 2020
        },
        {
            "authors": [
                "Qian Liu",
                "Bei Chen",
                "Jian-Guang Lou",
                "Bin Zhou",
                "Dongmei Zhang."
            ],
            "title": "Incomplete utterance rewriting as semantic segmentation",
            "venue": "EMNLP.",
            "year": 2020
        },
        {
            "authors": [
                "Yi Luan",
                "Luheng He",
                "Mari Ostendorf",
                "Hannaneh Hajishirzi."
            ],
            "title": "Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction",
            "venue": "EMNLP.",
            "year": 2018
        },
        {
            "authors": [
                "Yi Luan",
                "Dave Wadden",
                "Luheng He",
                "Amy Shah",
                "Mari Ostendorf",
                "Hannaneh Hajishirzi."
            ],
            "title": "A general framework for information extraction using dynamic span graphs",
            "venue": "ACL.",
            "year": 2019
        },
        {
            "authors": [
                "Diego Marcheggiani",
                "Ivan Titov."
            ],
            "title": "Encoding sentences with graph convolutional networks for semantic role labeling",
            "venue": "EMNLP.",
            "year": 2017
        },
        {
            "authors": [
                "Makoto Miwa",
                "Mohit Bansal."
            ],
            "title": "End-to-end relation extraction using LSTMs on sequences and tree structures",
            "venue": "ACL.",
            "year": 2016
        },
        {
            "authors": [
                "Tapas Nayak",
                "Hwee Tou Ng."
            ],
            "title": "Effective modeling of encoder-decoder architecture for joint entity and relation extraction",
            "venue": "AAAI.",
            "year": 2020
        },
        {
            "authors": [
                "O. Ronneberger",
                "P.Fischer",
                "T. Brox."
            ],
            "title": "U-net: Convolutional networks for biomedical image segmentation",
            "venue": "MICCAI.",
            "year": 2015
        },
        {
            "authors": [
                "Victor Sanh",
                "Thomas Wolf",
                "Sebastian Ruder."
            ],
            "title": "A hierarchical multi-task approach for learning embeddings from semantic tasks",
            "venue": "AAAI.",
            "year": 2019
        },
        {
            "authors": [
                "Changzhi Sun",
                "Yeyun Gong",
                "Yuanbin Wu",
                "Ming Gong",
                "Daxin Jiang",
                "Man Lan",
                "Shiliang Sun",
                "Nan Duan."
            ],
            "title": "Joint type inference on entities and relations via graph convolutional networks",
            "venue": "ACL.",
            "year": 2019
        },
        {
            "authors": [
                "Changzhi Sun",
                "Yuanbin Wu",
                "Man Lan",
                "Shiliang Sun",
                "Wenting Wang",
                "Kuang-Chih Lee",
                "Kewen Wu."
            ],
            "title": "Extracting entities and relations with joint minimum risk training",
            "venue": "EMNLP.",
            "year": 2018
        },
        {
            "authors": [
                "Christopher Walker",
                "Stephanie Strassel",
                "Julie Medero",
                "Kazuaki Maeda."
            ],
            "title": "Ace 2005 multilingual training corpus",
            "venue": "LDC.",
            "year": 2006
        },
        {
            "authors": [
                "Jue Wang",
                "Wei Lu."
            ],
            "title": "Two are better than one: Joint entity and relation extraction with tablesequence encoders",
            "venue": "EMNLP.",
            "year": 2020
        },
        {
            "authors": [
                "Shaolei Wang",
                "Yue Zhang",
                "Wanxiang Che",
                "Ting Liu."
            ],
            "title": "Joint extraction of entities and relations based on a novel graph scheme",
            "venue": "IJCAI.",
            "year": 2018
        },
        {
            "authors": [
                "Yijun Wang",
                "Changzhi Sun",
                "Yuanbin Wu",
                "Junchi Yan",
                "Peng Gao",
                "Guotong Xie."
            ],
            "title": "Pre-training entity relation encoder with intra-span and inter-span information",
            "venue": "EMNLP.",
            "year": 2020
        },
        {
            "authors": [
                "Yijun Wang",
                "Changzhi Sun",
                "Yuanbin Wu",
                "Hao Zhou",
                "Lei Li",
                "Junchi Yan."
            ],
            "title": "UniRE: A unified label space for entity relation extraction",
            "venue": "ACL.",
            "year": 2021
        },
        {
            "authors": [
                "Yucheng Wang",
                "Bowen Yu",
                "Yueyang Zhang",
                "Tingwen Liu",
                "Hongsong Zhu",
                "Limin Sun."
            ],
            "title": "TPLinker: Single-stage joint extraction of entities and relations through token pair linking",
            "venue": "COLING.",
            "year": 2020
        },
        {
            "authors": [
                "Bishan Yang",
                "Claire Cardie."
            ],
            "title": "Joint inference for fine-grained opinion extraction",
            "venue": "ACL.",
            "year": 2013
        },
        {
            "authors": [
                "Xiangrong Zeng",
                "Daojian Zeng",
                "Shizhu He",
                "Kang Liu",
                "Jun Zhao."
            ],
            "title": "Extracting relational facts by an end-to-end neural model with copy mechanism",
            "venue": "ACL.",
            "year": 2018
        },
        {
            "authors": [
                "Meishan Zhang",
                "Yue Zhang",
                "Guohong Fu."
            ],
            "title": "End-to-end neural relation extraction with global optimization",
            "venue": "ACL.",
            "year": 2017
        },
        {
            "authors": [
                "Ningyu Zhang",
                "Xiang Chen",
                "Xin Xie",
                "Shumin Deng",
                "Chuanqi Tan",
                "Mosha Chen",
                "Fei Huang",
                "Luo Si",
                "Huajun Chen."
            ],
            "title": "Document-level relation extraction as semantic segmentation",
            "venue": "IJCAI.",
            "year": 2021
        },
        {
            "authors": [
                "Ranran Haoran Zhang",
                "Qianying Liu",
                "Aysa Xuemo Fan",
                "Heng Ji",
                "Daojian Zeng",
                "Fei Cheng",
                "Daisuke Kawahara",
                "Sadao Kurohashi."
            ],
            "title": "Minimize exposure bias of Seq2Seq models in joint entity and relation extraction",
            "venue": "EMNLP.",
            "year": 2020
        },
        {
            "authors": [
                "Yuhao Zhang",
                "Peng Qi",
                "Christopher D. Manning."
            ],
            "title": "Graph convolution over pruned dependency trees improves relation extraction",
            "venue": "EMNLP.",
            "year": 2018
        },
        {
            "authors": [
                "Suncong Zheng",
                "Feng Wang",
                "Hongyun Bao",
                "Yuexing Hao",
                "Peng Zhou",
                "Bo Xu."
            ],
            "title": "Joint extraction of entities and relations based on a novel tagging scheme",
            "venue": "ACL.",
            "year": 2017
        },
        {
            "authors": [
                "Zexuan Zhong",
                "Danqi Chen."
            ],
            "title": "A frustratingly easy approach for entity and relation extraction",
            "venue": "ACL.",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Automatically extracting entities and relations from the free text is a fundamental task of NLP. It aims to identify typed spans (entities) and assign a semantic relation for each entity pair (relations). As shown in Fig. 1, a person (PER) entity \u201cJordena Ginsberg\u201d and an organization (ORG) entity \u201cNews 12 Westchester\u201d have an affiliation (ORG-AFF) relation.1\nMost mainstream approaches fall into one of two classes: pipeline or joint. Pipeline approaches use two independent models to predict entities and relations, respectively, while joint approaches build connections between the two sub-models by parameters sharing (Miwa and Bansal, 2016; Katiyar and\n\u2020Corresponding Author. 1In Fig. 1, a special symbol \u22a5 indicates that there is no\nsemantic relation.\nCardie, 2017; Sanh et al., 2019; Luan et al., 2019; Wang and Lu, 2020) or joint decoding (Yang and Cardie, 2013; Li and Ji, 2014; Katiyar and Cardie, 2016; Zhang et al., 2017; Sun et al., 2018, 2019; Lin et al., 2020). Due to adopting two separate label spaces for entity and relation, most joint methods can only learn shallow interactions between the two sub-models, leaving the deep interactions unexplored. To address this issue, a recent joint paradigm UNIRE (Wang et al., 2021) proposes a unified label space and tackle the two sub-tasks (i.e., entity recognition and relation extraction) by a single model, achieving the state-of-the-art performance with faster speed.\nThe key insight of UNIRE is leveraging wordby-word interactions to construct a 2D table, where each cell corresponds to a word pair in the sentence. In this way, the two sub-tasks are unified\nar X\niv :2\n30 5.\n04 29\n7v 1\n[ cs\n.C L\n] 7\nM ay\n2 02\n3\ninto one table-filling problem. However, considering that it only encodes the first-order interactions between two words with the biaffine attention mechanism (Dozat and Manning, 2017), we wonder whether there exist high-order interactions that may further advance unified entity relation extraction. Revisiting the 2D table, we find that the connections among multiple cells provide more complementary information than an isolated cell. For example in Fig. 1, neighboring cells (marked by the blue rectangle) potentially share the same label and can provide references for each other. Besides, cells across different regions (marked by dashed lines) imply some specific constraints on multi-entity, multi-relation, and entity-relation, e.g., the ORG-AFF relation usually indicates the existence of PER and ORG entity. Here the connections among multiple word pairs (cells) are built on top of the first-order word-by-word interactions and hence are regarded as high-order interactions. To this end, we attempt to leverage the high-order interactions for unified entity relation extraction.\nIn this paper, we follow UNIRE to formulate entity relation extraction as a table filling problem and propose HIORE, which incorporates highorder interactions in a coarse-to-fine manner. We first devise a W-shape DNN (WNet), which aggregates the first-order (word-by-word) representations around local neighbors to model the coarselevel high-order interactions among word pairs. Note that we also utilize the attention information from pre-trained language models in the WNet, which provide additional guidance concerning the word-by-word similarity. After that, we define a heuristic high-order graph upon the table to learn fine-grained high-order information. Specifically, we assign a node for each cell and design two edgelinking strategies: the static graph and the dynamic graph. The static graph is defined according to hand-crafted principles, which build connections among some specific cells, i.e., the diagonal cells and cells in the same row (column). The dynamic graph is learned with a binary classification task, which further prunes some redundant edges. Based on the heuristic graph, we adopt a graph neural network (GNN) to calibrate the representation of each node and bring global constraints into the final decision.\nWe summarize our contributions as follows:\n\u2022 We propose HIORE for unified entity relation extraction, which leverages high-order interactions\namong word pairs. We first develop a WNet to learn coarse-level information. Then we employ a GNN on top of the heuristic high-order graph to calibrate the final representations.\n\u2022 Experiments on three benchmarks (ACE04, ACE05, and SciERC) demonstrate the effectiveness of the high-order information and show that HIORE achieves state-of-the-art relation performance on the three benchmarks."
        },
        {
            "heading": "2 Background",
            "text": "Given an input sentence s = {wi}ni=1 of length n (wi is a word), it aims to extract an entity set E and a relation set R. An entity e \u2208 E is a text span {wi}i\u2208e 2 with an pre-defined entity type ye \u2208 Ye, e.g., person (PER), organization (ORG). A relation r \u2208 R is a triplet (e1, e2, yr), where e1, e2 are two entities and yr \u2208 Yr is a pre-defined relation type describing the semantic relation among two entities, e.g., organization affiliation relation (ORG-AFF). Ye,Yr denote the set of possible entity types and relation types, respectively.\nUNIRE formulates the entity relation extraction as a table filling task. For the input sentence s, it maintains a 2D table T n\u00d7n, where each cell is assigned a label yi,j \u2208 Y (Y = Ye \u222a Yr \u222a {\u22a5}, \u22a5 denotes no relation). For each entity e, the corresponding cells yi,j(i \u2208 e, j \u2208 e) are filled with ye.3 For each relation (e1, e2, yr), the corresponding cells yi,j(i \u2208 e1, j \u2208 e2) are filled with yr. Lastly, the remaining cells are filled with \u22a5.\nNext, we will briefly describe overall UNIRE architecture in four parts: the sentence encoder, the table encoder, table filling, and table decoding. Due to space limitations, please refer to (Wang et al., 2021) for more details.\nSentence Encoder For the input sentence s, UNIRE adopts a pre-trained language model (PLM) like BERT (Devlin et al., 2019) as the sentence encoder to obtain contextual representations. The output is calculated via\n{h1, . . . ,hn} = PLM({x1, . . . ,xn}),\nwhere xi sums corresponding word, position, and segmentation embeddings of word wi. To better model the directional information of word-by-word\n2i \u2208 e denotes start(e) \u2264 i \u2264 end(e), where start(e) is the start word offset of e in the sentence s and end(e) is similar.\n3We do not consider nested entities in this paper.\ninteractions, a head multi-layer perceptron (MLP) and a tail MLP are applied on each hi:\nhheadi = MLPhead(hi), h tail i = MLPtail(hi).\nTable Encoder UNIRE adopts the deep biaffine attention mechanism (Dozat and Manning, 2017) to obtain the scoring vector gi,j \u2208 R|Y| of each cell (i, j) in the table T :\ngi,j = Biaff(h head i ,h tail j ).\nTable Filling With the scoring vector gi,j , UNIRE yields a categorical probability distribution over the unified label space Y by the softmax function. Given the gold label y\u2217i,j , the objective function is to minimize\nLentry = \u2212 1\nn2 \u2211 i,j log p(yi,j = y \u2217 i,j |s),\np(yi,j |s) = Softmax(gi,j).\nIn the training phase, it imposes two structural constraints (symmetry objective Lsym and implication objective Limp) and jointly optimized them with Lentry. To some extent, these two constraints can be viewed as artificial high-order features. In this work, we further investigate the high-order information in both coarse and fine level.\nTable Decoding In the testing phase, UNIRE proposes an approximate decoding algorithm to compute the final extraction results. The main idea is to firstly decode spans, then decode the entity label of each span, and lastly decode the relation label of each entity pair."
        },
        {
            "heading": "3 HIORE",
            "text": "Following the unified paradigm UNIRE (Wang et al., 2021), we propose HIORE and formulate the entity relation extraction as a table filling problem. Unlike UNIRE that considers only the first-order word-by-word interactions, HIORE further leverages the high-order information among word-pair in a coarse-to-fine manner. The schematic description of HIORE is shown in Fig. 2. With the vanilla table representations of word pairs, we first devise a W-shape convolutional network to aggregate the coarse-level high-order information among local neighbors (Section 3.1). After that, we build a heuristic high-order graph and use a GNN to propagate and calibrate the fine-grained information (Section 3.2). Next, we will explain the proposed model in detail."
        },
        {
            "heading": "3.1 Coarse-level High-order Information",
            "text": "Unlike UNIRE that processes each cell (representing a word pair) independently when computing the table representation, we desire to aggregate the neighboring information within the table. For this purpose, we treat the 2D table as an image and apply convolution to capture the neighboring information. For the network design, we find that the U-shaped convolutional network (Ronneberger et al., 2015) combines the encoder-decoder architecture with skip connections, which facilitates the extraction of both structural and semantic information. Inspired by the classical U-shape architecture, we devise a W-shape (i.e., double U-shape) convolutional network (WNet) to capture the coarse-level high-order information. Next, we will detail the construction of the image-like input and the WNet.\nInput Table Construction On top of the contextual representation of each word in the sentence s, we construct two types of input tables to encode word-to-word information.\n\u2022 For each cell (i, j) of the table T , we construct a vanilla cell representation vi,j based on the word representations hheadi and h tail j as\nVi,j = [h head i ;h tail j ;h head i \u2212 htailj ;hheadi htailj ; c|i\u2212j|],\nwhere c|i\u2212j| is the distance embedding, denotes element-wise multiplication and [\u00b7; \u00b7] is the concatenation operation. We denote the input matrix as V \u2208 Rn\u00d7n\u00d7v and Vi,j \u2208 Rv (v = 750).\n\u2022 Besides, pre-trained language models (PLM) provide not only the contextual representations for each token but also the attention scores that reflect the word-to-word similarity. To take advantage of the attention information, we extract the attention matrix from PLM as the table input Kn\u00d7n\u00d7k, which is collected from all heads of each layer (e.g., k = 12 \u00d7 12 for the BERT base model) in the pretrained model. 4\nWNet Given the word-by-word representation V and the attention signal K, we carefully devise a W-shape DNN, namely WNet, to aggregate the high-order interactions among the neighboring\n4We take the first sub-word for each token to collect attention scores. Since the number of attention matrix channels is too large for ALBERT-xxlarge model (k = 768), we omit the attention matrix input in ALBERT-xxlarge-based model for efficiency.\nword pairs. WNet is a variant of UNet that follows an encoder-decoder schema, with multiple shortcuts connecting the encoded features to the decoding process. Unlike the conventional UNet that consists of one encoder and one decoder, WNet contains two encoders to process V and K separately but share one decoder. As shown in Fig. 3, the two types of information are compacted by the corresponding encoder and are lately integrated during the decoding process. In this way, the output U \u2208 Rn\u00d7n\u00d7u summarizes both the neighboring high-order interactions and word-by-word correlations.\nU = WNet(V ,K)."
        },
        {
            "heading": "3.2 Fine-grained High-order Information",
            "text": "After the coarse-level representation U aggregates the high-order interactions among neighboring\ncells, we desire to further incorporate the finegrained information of entity-to-entity, relation-torelation, and entity-to-relation. For this purpose, we build a heuristic graph G upon the 2D table, with each cell (i, j) corresponding to a graph node and Ui,j as the initial node embedding. Thus, the total number of nodes in the graph G is n2. Next, we establish the cell-to-cell connections as the edges between two nodes. The edges in the graph G are linked by two strategies, resulting the static graph and the dynamic graph.\nStatic Graph The static graph is defined by the following two hand-crafted principles:\n\u2022 We connect any two cells on the diagonal, namely, (i, i) and (j, j) where i 6= j. This type of edge may reveal the interactions between entities and advance entity prediction with more other entities\u2019 features.\n\u2022 For each off-diagonal cell (i, j), we connect it to its two corresponding diagonal cells (i, i) and (j, j). This type of edge may reveal the interactions between entities and relations. For example, the edges between the pair (Jordena, Westchester) and (Jordena, Jordena), (Westchester, Westchester) in Fig. 1, which may help (Jordena, Westchester) learn the information from its argument entities, and vice versa.\nBesides, the two principles can also build connections between relations by multi-hopping, providing other relations\u2019 feature for relation prediction.\nThis strategy uses the diagonal cells as pivots and outputs a sparse graph. Nevertheless, the static graph possibly contain some redundant edges:\n\u2022 If the label of a diagonal cell (i, i) is\u22a5, the edges connecting (i, i) to other diagonal cells (j, j) are redundant edges.\n\u2022 If the label of an off-diagonal cell (i, j) is \u22a5, the edges connecting (i, j) to (i, i) and (i, j) to (j, j) are redundant edges.\nTo prune these redundant edges, we further investigate the dynamic graph.\nDynamic Graph We introduce a binary classification task to predict whether the label of each cell is \u22a5. Given the initial embedding Ui,j for the cell (i, j), we adopt a classifier to compute the probability of the binary label bi,j .\np(bi,j |s) = Softmax(WbinUi,j),\nwhere Wbin is a learnable parameter. The training objective is to minimize\nLbin = \u2212 1\nn2 \u2211 i,j log p(bi,j = b \u2217 i,j |s),\nwhere the gold binary label b\u2217i,j are transformed from the table annotations y\u2217i,j .\n5 According to the predicted label, we build the edges of the dynamic graph by the following rules:\n\u2022 If b\u0302i,j = 1 where i 6= j, we connect (i, j) to (i, i) and (i, j) to (j, j);\n\u2022 If b\u0302i,i = b\u0302j,j = 1 where i 6= j, we connect (i, i) to (j, j).\nIn practice, we adopt either the static or the dynamic strategy to build a heuristic high-order graph G, with the node embedding initialized by U . After that, a multi-layer Graph Neural Network (GNN) is used to obtain the final representation G:\nG = GNN(U ,G) \u2208 Rn\u00d7n\u00d7d."
        },
        {
            "heading": "3.3 Training and Decoding",
            "text": "To predict each label of the table T , the cell representation Gi,j is fed into the softmax function, yielding a categorical probability distribution over the unified label space Y as\np(yi,j |s) = Softmax(WyGi,j), 5b\u2217i,j = 1 if y \u2217 i,j 6= \u22a5, and b\u2217i,j = 0 if y\u2217i,j = \u22a5.\nwhere Wy is a learnable parameter. Given the input sentence s and the gold label of each cell y\u2217i,j , the training objective is to minimize\nLentry=\u2212 1\nn2 \u2211 i,j log p(yi,j = y \u2217 i,j |s).\nWith the static graph, we only optimize Lentry. And with the dynamic graph, we jointly optimize Lentry and binary classification objective Lbin as Lentry + Lbin.\nIn the decoding phase, we use the same algorithm as UNIRE to extract all entities and relations based on table predictions."
        },
        {
            "heading": "4 Experiments",
            "text": "Datasets We evaluate our model on three common used datasets, i.e., ACE04 (Doddington et al., 2004), ACE05 (Walker et al., 2006), and SciERC (Luan et al., 2018). We use the same data splits and pre-processing as (Li and Ji, 2014; Miwa and Bansal, 2016; Wang et al., 2021).\nEvaluation Following prior works, we report F1 score with Micro-averaging strategy for performance comparison. Specifically, i) we consider the entity is correctly labeled if its type and boundaries match those of a gold entity, and ii) the relation is correctly labeled when its type and two-argument entities (including the entity type and boundaries) both match those of a gold relation, i.e., the Strict Evaluation criterion.\nExperimental Settings We set the hidden size of head/tail MLP as d = 150 and the activation function as GELU. Following (Wang et al., 2021), the batch size is 32, the learning rate is 5e-5 with weight decay 1e-5, and the optimizer is AdamW with \u03b21 = 0.9 and \u03b22 = 0.9. We train the model with a maximum of 300 epochs and employ an early stop strategy.\nWe tune all hyper-parameters on the development set of ACE05, then directly use the same hyper-parameters on ACE04 and SciERC. For each experiment, we run our model 3 runs and report the averaged scores. We pick the best model based on the averaged F1 score of the entity and relation on the development set. All experiments are conducted on an NVIDIA Tesla V100 GPU (32G)."
        },
        {
            "heading": "4.1 Performance Comparison",
            "text": "We compare the performance of our model with previous methods in Table 1. In general, our\nmodel achieves the best relation performance on three benchmarks with different pre-trained models. Specifically, with the base encoder (BERTBASE and SciBERT), HIORE increases the relation performance by +1.7 (ACE04), +1.0 (ACE05), +1.4 (SciERC) absolute F1 points over the previous best results.\nNote that though HIORE is slightly inferior to the previous best model in the entity detection, it significantly advances both entity and relation performances upon UNIRE. Specifically, HIORE improves the relation performance by 1.1 \u223c 1.8 F1 points on three datasets and entity performance by 0.5 and 0.4 points on ACE04 and ACE05, respectively. These results demonstrate that HIORE effectively mines the high-order information among word pairs."
        },
        {
            "heading": "4.2 Ablation Study",
            "text": "In this section, we evaluate the different components of HIORE on ACE05 and SciERC. From the results in Table 2, we have the following observations:\n\u2022 When the attention matrix input K is removed (line 2), both entity and relation performances decline on two datasets. It implies that the word-to-\nSettings ACE05 SciERCEnt F1 Rel F1 Ent F1 Rel F1\nDefault 89.6 65.8 68.2 38.3\nw/o K 89.3 65.2 67.5 38.1 w/o WNet 88.3 63.3 66.5 33.1 w/o GNN 89.0 64.5 67.0 37.4\ndynamic graph 89.2 64.8 66.9 37.5 biaffine attention 88.8 65.0 67.2 36.4 constrained objectives 89.4 65.5 67.6 35.9\nsingle UNet 89.6 65.6 66.3 34.6 two separate UNets 88.8 63.8 67.9 37.6\nthat HIORE has captured more effective high-order interactions.\n\u2022 For the architecture of WNet, , we try some alternatives, i.e., a single UNet (line 8) and two separate UNets (line 9). When replacing the WNet with the UNet and concatenating V and K as one input, the performance on ACE05 is comparable but that on SciERC drops sharply. Besides, adopting two separate UNets for two input tables (i.e., V and K) is also inferior to WNet. These results show that the WNet is more effective and stable on both datasets."
        },
        {
            "heading": "4.3 Impact of Different Settings for GNN",
            "text": "We evaluate our model under different settings of GNN, i.e., different numbers of GNN layers and different graph schemas, on the development set of ACE05 and SciERC. As shown in Fig. 4, when increasing the number of GNN layers from 1 to 4, the relation performance gradually decreases while the entity performance almost keeps still. The result shows that the simplest 1-layer GNN is more effective, so we adopt 1-layer GNN in the final HIORE design. Next, the entity performance is still stable when changing the graph schema from the static graph to the dynamic graph. But the relation performance of the dynamic graph is slightly inferior to that of the static graph (-0.3 points). We think that the dynamic graph introduces some errors in the binary relation classification procedure, which subsequently hazards the relation performance."
        },
        {
            "heading": "4.4 Inference Speed",
            "text": "In this section, we evaluate the inference speed of our model on ACE05 and SciERC (Table 3). We use the same pre-trained language model and batch size as (Wang et al., 2021) and obtain the results of other models under the same machine configuration. Compared with UNIRE, the inference speed of our model drops by 30% due to the more complex network architecture. But HIORE achieves\nsignificantly superior relation performance (+1.5 and +1.4 absolute F1 score on ACE05 and SciERC), and we consider it an acceptable trade-off between performance and speed. Besides, it is worth noting that the speed of HIORE is still quite competitive when compared with prior work (Zhong and Chen, 2021). Note that when adopting the dynamic graph in our model, the inference speed drops dramatically, i.e., a quarter of that of UNIRE on ACE05, and one-eighth of that of UNIRE on SciERC. Therefore, we adopt the static graph in the final model, which strikes a good balance between accuracy and efficiency.\nTo study the impact of the number of model parameters, we tried to increase the number of UniRE\u2019s parameters by increasing the hidden size of the biaffine model (from 150 to 512), achieving UNIRE+ (Table 3). But the performance of UniRE+ did not improve further or even decreased on ACE05 and SciERC. Thus, we attribute the performance improvement of HIORE mainly to the novel network architecture. Moreover, compared with prior work (Zhong and Chen, 2021), HIORE is still quite compact."
        },
        {
            "heading": "4.5 Error Analysis",
            "text": "In this section, we compare HIORE with UNIRE (Wang et al., 2021) in three hard situations (Fig. 5): i) isolated entity (IE), i.e., an entity that does not participate in any relation; ii) multi-relation (MR), that multiple relations exist in one sentence; and iii) long-distance relation (LDR), that the distance between a relation\u2019s two argument entities exceeds 4. Some specific examples of the typical error are presented in Table 4.\n\u2022 In the \u201cIE\u201d case, HIORE achieves better entity performance (+1.4 F1 score) than UNIRE. An example is that HIORE detects the isolated WEA en-\ntity \u201cbag\u201d while UNIRE does not. It shows that with entity connection on the table graph, HIORE can capture more inter-entity interactions and detect more semantic information. Though outperforming UNIRE, performance in the \u201cIE\u201d situation is significantly lower than that on the complete test set, raising a crucial problem for further exploration.\n\u2022 For relation extraction, HIORE significantly outperforms UNIRE on both \u201cMR\u201d (+ 2.4 points) and \u201cLDR\u201d (+1.7 points), showing that HIORE makes the best of the interactions between multiple relations and effectively builds long-distance dependency between words. As shown in Table 4, HIORE successfully extracts two overlapped relations (line 6), while UNIRE fails to detect the PART-WHOLE relation (line 5). In addition, UNIRE misses a longer distance relation PHYS (line 8) while HIORE correctly recognizes it. On the whole, both HIORE and UNIRE can handle the \u201cMR\u201d situation but are not satisfying in the \u201cLDR\u201d case. The performance of detecting long-distance relations is only twothirds of that on the complete test set, indicating that the long-distance relation problem is another challenge that waits to be solved."
        },
        {
            "heading": "5 Related Work",
            "text": "Unified Entity Relation Extraction is first introduced by (Zheng et al., 2017), which formulates entity relation extraction as a sequence labeling problem. Then (Wang et al., 2018) convert this task into a graph generation problem and solve it with a transition-based parsing system. Besides, (Zeng et al., 2018; Zhang et al., 2020; Nayak and Ng, 2020) adopt a Seq2Seq model to generate relation triplets. (Wang et al., 2020b) propose a token pair linking problem to extract relation triplets, which is also based on table filling. Unlike thse work that do not model entity types, both UNIRE (Wang et al., 2021) and our model HIORE jointly model entity types and relation types and extract entity and relation in a unified label space. Comparing with UNIRE, our model further considers more high-order interactions among word pairs.\nUNet is first used to biomedical image segmentation and gradually becomes a popular backbone in the image segmentation. Interestingly, recent work (Liu et al., 2020; Zhang et al., 2021) try to introduce UNet into some NLP tasks, e.g., (Liu et al., 2020) successfully tackle the incomplete utterance rewriting with the UNet. Thus, we also borrow ideas from UNet to learn high-order interactions.\nGraph Neural Network has been widely used in many NLP tasks, e.g., semantic role labeling (Marcheggiani and Titov, 2017), machine translation (Bastings et al., 2017), relation extraction (Zhang et al., 2018; Sun et al., 2019). (Sun et al., 2019) first introduce GNN into joint entity relation extraction, which regards each entity and relation as node and constructs a bipartite graph for joint type inference."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this paper, we focus on leveraging high-order interactions to advance unified entity relation extraction. We capture coarse-level high-order information with the WNet and adopt a GNN on the heuristic graph to further calibrate representations. Experimental results show that our model HIORE achieves state-of-the-art performance on three benchmarks. The unified extraction for overlapped entities we leave for future work."
        }
    ],
    "title": "HIORE: Leveraging High-order Interactions for Unified Entity Relation Extraction",
    "year": 2023
}