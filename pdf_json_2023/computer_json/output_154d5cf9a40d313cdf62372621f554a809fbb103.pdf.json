{
    "abstractText": "We propose a multi-dimensional structured state space (S4) approach to speech enhancement. To better capture the spectral dependencies across the frequency axis, we focus on modifying the multi-dimensional S4 layer with whitening transformation to build new small-footprint models that also achieve good performance. We explore several S4-based deep architectures in time (T) and time-frequency (TF) domains. The 2D S4 layer can be considered a particular convolutional layer with an infinite receptive field although it utilizes fewer parameters than a conventional convolutional layer. Evaluated on the VoiceBank-DEMAND data set, when compared with the conventional U-net model based on convolutional layers, the proposed TF-domain S4-based model is 78.6% smaller in size, yet it still achieves competitive results with a PESQ score of 3.15 with data augmentation. By increasing the model size, we can even reach a PESQ score of 3.18.",
    "authors": [
        {
            "affiliations": [],
            "name": "Pin-Jui Ku"
        },
        {
            "affiliations": [],
            "name": "Chao-Han Huck Yang"
        },
        {
            "affiliations": [],
            "name": "Sabato Marco Siniscalchi"
        },
        {
            "affiliations": [],
            "name": "Chin-Hui Lee"
        }
    ],
    "id": "SP:abad67dee208ffc0f998039fda4edf72dc425b50",
    "references": [
        {
            "authors": [
                "B. Wu",
                "K. Li",
                "F. Ge",
                "Z. Huang",
                "M. Yang",
                "S.M. Siniscalchi",
                "C.- H. Lee"
            ],
            "title": "An end-to-end deep learning approach to simultaneous speech dereverberation and acoustic modeling for robust speech recognition",
            "venue": "IEEE Journal of Selected Topics in Signal Processing, vol. 11, pp. 1289\u20131300, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "J.H. Hansen",
                "T. Hasan"
            ],
            "title": "Speaker recognition by machines and humans: A tutorial review",
            "venue": "IEEE Signal Processing Magazine, vol. 32, no. 6, pp. 74\u201399, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "S. Boll"
            ],
            "title": "Suppression of acoustic noise in speech using spectral subtraction",
            "venue": "IEEE Trans. on Acoustics, Speech, and Signal Processing, vol. 27, no. 2, pp. 113\u2013120, 1979.",
            "year": 1979
        },
        {
            "authors": [
                "J. Lim",
                "A. Oppenheim"
            ],
            "title": "Enhancement and bandwidth compression of noisy speech",
            "venue": "Proceedings of the IEEE, vol. 67, no. 12, pp. 1586\u20131604, 1979.",
            "year": 1979
        },
        {
            "authors": [
                "K. Paliwal",
                "A. Basu"
            ],
            "title": "A speech enhancement method based on kalman filtering",
            "venue": "Proc. ICASSP, 1987.",
            "year": 1987
        },
        {
            "authors": [
                "V. Krishnan",
                "S. Siniscalchi",
                "D. Anderson",
                "M. Clements"
            ],
            "title": "Noise robust aurora-2 speech recognition employing a codebook-constrained kalman filter preprocessor",
            "venue": "Proc. ICASSP, 2006.",
            "year": 2006
        },
        {
            "authors": [
                "M. Berouti",
                "R. Schwartz",
                "J. Makhoul"
            ],
            "title": "Enhancement of speech corrupted by acoustic noise",
            "venue": "Proc. ICASSP, 1979.",
            "year": 1979
        },
        {
            "authors": [
                "Y. Xu",
                "J. Du",
                "L.-R. Dai",
                "C.-H. Lee"
            ],
            "title": "An experimental study on speech enhancement based on deep neural networks",
            "venue": "IEEE Signal Processing Letters, vol. 21, no. 1, pp. 65\u201368, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "\u2014\u2014"
            ],
            "title": "A regression approach to speech enhancement based on deep neural networks",
            "venue": "IEEE/ACM Trans. Audio, Speech, Language Process, vol. 23, no. 1, pp. 7\u201319, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "D. Wang"
            ],
            "title": "Time-frequency masking for speech separation and its potential for hearing aid design",
            "venue": "Trends in amplification, vol. 12, no. 4, pp. 332\u2013353, 2008.",
            "year": 2008
        },
        {
            "authors": [
                "Y. Luo",
                "N. Mesgarani"
            ],
            "title": "Conv-tasnet: Surpassing ideal time\u2013 frequency magnitude masking for speech separation",
            "venue": "IEEE/ACM Trans. Audio, Speech, Language Process, vol. 27, no. 8, pp. 1256\u2013 1266, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "S. Pascual",
                "A. Bonafonte",
                "J. Serr\u00e0"
            ],
            "title": "Segan: Speech enhancement generative adversarial network",
            "venue": "Proc. Interspeech, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "S.-W. Fu",
                "C.-F. Liao",
                "Y. Tsao",
                "S.-D. Lin"
            ],
            "title": "MetricGAN: Generative adversarial networks based black-box metric scores optimization for speech enhancement",
            "venue": "Proc. ICML, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "S.-W. Fu",
                "C. Yu",
                "T.-A. Hsieh",
                "P. Plantinga",
                "M. Ravanelli",
                "X. Lu",
                "Y. Tsao"
            ],
            "title": "MetricGAN+: An Improved Version of MetricGAN for Speech Enhancement",
            "venue": "Proc. Interspeech, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Bando",
                "K. Sekiguchi",
                "K. Yoshii"
            ],
            "title": "Adaptive Neural Speech Enhancement with a Denoising Variational Autoencoder",
            "venue": "Proc. Interspeech, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "H. Fang",
                "G. Carbajal",
                "S. Wermter",
                "T. Gerkmann"
            ],
            "title": "Variational autoencoder for speech enhancement with a noise-aware encoder",
            "venue": "Proc. ICASSP, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Y.-J. Lu",
                "Z.-Q. Wang",
                "S. Watanabe",
                "A. Richard",
                "C. Yu",
                "Y. Tsao"
            ],
            "title": "Conditional diffusion probabilistic model for speech enhancement",
            "venue": "Proc. ICASSP, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "J. Richter",
                "S. Welker",
                "J.-M. Lemercier",
                "B. Lay",
                "T. Gerkmann"
            ],
            "title": "Speech enhancement and dereverberation with diffusion-based generative models",
            "venue": "arXiv preprint arXiv:2208.05830, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "H. Yen",
                "F.G. Germain",
                "G. Wichern",
                "J.L. Roux"
            ],
            "title": "Cold diffusion for speech enhancement",
            "venue": "arXiv preprint arXiv:2211.02527, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "E.M. Grais",
                "M.U. Sen",
                "H. Erdogan"
            ],
            "title": "Deep neural networks for single channel source separation",
            "venue": "Proc. ICASSP, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "Y. Wang",
                "A. Narayanan",
                "D. Wang"
            ],
            "title": "On training targets for supervised speech separation",
            "venue": "IEEE/ACM Trans. Audio, Speech, Language Process, vol. 22, no. 12, pp. 1849\u20131858, 2014.",
            "year": 1849
        },
        {
            "authors": [
                "T. Gao",
                "J. Du",
                "L.-R. Dai",
                "C.-H. Lee"
            ],
            "title": "Densely connected progressive learning for lstm-based speech enhancement",
            "venue": "Proc. ICASSP, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "P.-S. Huang",
                "M. Kim",
                "M. Hasegawa-Johnson",
                "P. Smaragdis"
            ],
            "title": "Deep learning for monaural speech separation",
            "venue": "Proc. ICASSP, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "F. Weninger",
                "H. Erdogan",
                "S. Watanabe",
                "E. Vincent",
                "J.L. Roux",
                "J.R. Hershey",
                "B. Schuller"
            ],
            "title": "Speech enhancement with LSTM recurrent neural networks and its application to noise-robust ASR",
            "venue": "Proc. LVA/ICA, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "R. Giri",
                "U. Isik",
                "A. Krishnaswamy"
            ],
            "title": "Attention wave-u-net for speech enhancement",
            "venue": "Proc. WASPAA, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "A. D\u00e9fossez",
                "G. Synnaeve",
                "Y. Adi"
            ],
            "title": "Real Time Speech Enhancement in the Waveform Domain",
            "venue": "Proc. Interspeech, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "R. Pascanu",
                "T. Mikolov",
                "Y. Bengio"
            ],
            "title": "On the difficulty of training recurrent neural networks",
            "venue": "Proc. ICML, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "A. Gu",
                "I. Johnson",
                "K. Goel",
                "K.K. Saab",
                "T. Dao",
                "A. Rudra",
                "C. Re"
            ],
            "title": "Combining recurrent, convolutional, and continuous-time models with linear state space layers",
            "venue": "Proc. NeuIPS, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "A. Gu",
                "K. Goel",
                "C. Re"
            ],
            "title": "Efficiently modeling long sequences with structured state spaces",
            "venue": "Proc. ICLR, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "K. Goel",
                "A. Gu",
                "C. Donahue",
                "C. Re"
            ],
            "title": "It\u2019s raw! Audio generation with state-space models",
            "venue": "Proc. ICML, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "K. Miyazaki",
                "M. Murata",
                "T. Koriyama"
            ],
            "title": "Structured state space decoder for speech recognition and synthesis",
            "venue": "arXiv preprint arXiv:2210.17098, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "E. Nguyen",
                "K. Goel",
                "A. Gu",
                "G. Downs",
                "P. Shah",
                "T. Dao",
                "S. Baccus",
                "C. R\u00e9"
            ],
            "title": "S4ND: Modeling images and videos as multidimensional signals with state spaces",
            "venue": "Proc. NeuIPS, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "A. Tustin"
            ],
            "title": "A method of analysing the behaviour of linear systems in terms of time series",
            "venue": "Journal of the Institution of Electrical Engineers-Part IIA: Automatic Regulators and Servo Mechanisms, vol. 94, no. 1, pp. 130\u2013142, 1947.",
            "year": 1947
        },
        {
            "authors": [
                "I. Markovsky"
            ],
            "title": "Structured low-rank approximation and its applications",
            "venue": "Automatica, vol. 44, no. 4, pp. 891\u2013909, 2008.",
            "year": 2008
        },
        {
            "authors": [
                "J.R. Bunch"
            ],
            "title": "A note on the stable decompostion of skewsymmetric matrices",
            "venue": "Mathematics of Computation, vol. 38, no. 158, pp. 475\u2013479, 1982.",
            "year": 1982
        },
        {
            "authors": [
                "R. Yamamoto",
                "E. Song",
                "J.-M. Kim"
            ],
            "title": "Parallel wavegan: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram",
            "venue": "Proc. ICASSP, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "H.-S. Choi",
                "J.-H. Kim",
                "J. Huh",
                "A. Kim",
                "J.-W. Ha",
                "K. Lee"
            ],
            "title": "Phase-aware speech enhancement with deep complex u-net",
            "venue": "Proc. ICLR, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Hu",
                "Y. Liu",
                "S. Lv",
                "M. Xing",
                "S. Zhang",
                "Y. Fu",
                "J. Wu",
                "B. Zhang",
                "L. Xie"
            ],
            "title": "DCCRN: Deep Complex Convolution Recurrent Network for Phase-Aware Speech Enhancement",
            "venue": "Proc. Interspeech, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "J. Qi",
                "J. Du",
                "S.M. Siniscalchi",
                "X. Ma",
                "C.-H. Lee"
            ],
            "title": "On mean absolute error for deep neural network based vector-to-vector regression",
            "venue": "IEEE Signal Processing Letters, vol. 27, pp. 1485\u2013 1489, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "A. Kessy",
                "A. Lewin",
                "K. Strimmer"
            ],
            "title": "Optimal whitening and decorrelation",
            "venue": "The American Statistician, vol. 72, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "C. Valentini-Botinhao"
            ],
            "title": "Noisy speech database for training speech enhancement algorithms and tts models",
            "venue": "2017.",
            "year": 2017
        },
        {
            "authors": [
                "A. Rix",
                "J. Beerends",
                "M. Hollier",
                "A. Hekstra"
            ],
            "title": "Perceptual evaluation of speech quality (pesq)-a new method for speech quality assessment of telephone networks and codecs",
            "venue": "Proc. ICASSP, 2001.",
            "year": 2001
        },
        {
            "authors": [
                "C. Macartney",
                "T. Weyde"
            ],
            "title": "Improved speech enhancement with the wave-u-net",
            "venue": "arXiv preprint arXiv:1811.11307, 2018.",
            "year": 1811
        },
        {
            "authors": [
                "S. Lv",
                "Y. Fu",
                "M. Xing",
                "J. Sun",
                "L. Xie",
                "J. Huang",
                "Y. Wang",
                "T. Yu"
            ],
            "title": "S-dccrn: Super wide band dccrn with learnable complex feature for speech enhancement",
            "venue": "Proc. ICASSP, 2022.",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Speech enhancement aims at reducing noise to improve the quality and intelligibility of noisy speech. It also serves as a front-end in robust speech recognition [1] and speaker recognition [2] in adverse environments. Conventional approaches to speech enhancement usually rely on using statistical models to predict speech and noise, e.g., spectral subtraction [3], Wiener filtering [4], and Kalman filtering [5, 6]. In recent years, deep neural network (DNN) based approaches have demonstrated superiority over conventional signal processing techniques, especially for alleviating problems caused by musical and nonstationary noise [7]. They can be roughly categorized according to the features used. In the time-frequency (TF) domain, two distinct types exist: (i) spectral mapping [8, 9], by establishing a regression function directly from mapping the noisy speech spectrogram to the clean one; and (ii) masking [10, 11] by estimating a 2-D ratio mask that describes the TF relationships between clean speech and background noise. More recently, generative models, aiming at learning a prior distribution over clean speech data, have been proposed, e.g., generative adversarial networks (GAN) [12, 13, 14], variational autoencoders (VAE) [15, 16], and diffusion models [17, 18, 19].\nDNNs [9, 20, 21], recurrent neural networks (RNNs) [22, 23, 24], and convolutional neural networks (CNNs) [25, 26] have been proposed for speech enhancement. While RNNs are a natural choice for sequential inputs, their recursive nature makes them slow to train and suffer from optimization difficulties, such as the \u201cvanishing gradient\u201d problem [27], limiting their ability to handle long sequences. In contrast, CNNs\nencode local context and enable parallel training, but they are fundamentally constrained by the size of their receptive field and may not achieve global coherence. Recently, deep state space models (SSM) [28], in particular S4-based ones [29], have achieved state-of-the-art results in modeling sequence data with extremely long-range dependencies. The S4 layer unifies the strengths of both RNN and CNN layers: (i) it allows parallel computing like CNN layers, and (ii) it can capture global information across the whole input sequence like RNNs. S4-based systems have been deployed for raw audio generation [30] and speech recognition [31].\nIn this study, we propose a structured state-space-based architecture in the multi-dimensional TF domain for speech enhancement. We explore 2-D extensions since the original S4-based layer was designed to handle only 1-D input signals. However, to adequately capture the spectral dependencies across the frequency axis, we first compute the covariance matrix of each frequency bin and apply a whitening technique to make the frequency bins statistically uncorrelated. Unfortunately, while whitening was effective under specific scenarios, the overall performance improvement was limited. We therefore utilize the multi-dimensional S4 layer proposed in [32] and adapt it to the TF domain. Experimental evidence shows that by injecting the multi-dimensional S4 layer inside a U-net-like architecture, we were able to deploy a novel deep enhancement model that attained better PESQ scores than the original timedomain model while reducing the model size by 78.6% (with only 0.75M parameters). Since S4 layer directly handles 1-D inputs, we have also built a sequence-to-sequence regression model in the time-domain. Furthermore, we have also evaluated the effect of data augmentation on speech enhancement.\nIn the following sections, we will demonstrate three main contributions. We first conduct an initial investigation on how to best incorporate an S4 layer into speech enhancement models. To the best of our knowledge, this is the first attempt in this field. Next, we find that TF-domain models can be improved using a small hop-length to extract spectrograms with long time lengths through short-time Fourier transform (STFT) mainly because S4 can easily handle long time-series inputs. Finally, by unifying the strengths of RNN and CNN layers, we achieve competitive enhancement results even with a compact TF-domain S4-based U-Net architecture"
        },
        {
            "heading": "2. Related Work",
            "text": ""
        },
        {
            "heading": "2.1. SSM - State Space Modeling",
            "text": "The S4 layer is based on the linear state space layer (LSSL) proposed in [28], where u(t) \u2208 C, and v(t) \u2208 C are 1-dimensional input/output patterns in state space modeling, respectively, and x(t) \u2208 CN is an implicit N-dimensional state vector. LSSL de-\nar X\niv :2\n30 6.\n00 33\n1v 1\n[ ee\nss .A\nS] 1\nJ un\n2 02\n3\nfines a 1-D mapping function u(t) \u2192 v(t) leveraging upon the following ordinary differential equations (ODEs):\nx\u0307(t) = Ax(t) +Bu(t) v(t) = Cx(t) +Du(t) (1)\nEq. (1) describes a state space model (SSM). The goal of LSSL is to treat the SSM as a black-box representation in a deep model, where the state-transition matrix A \u2208 CN\u00d7N and the projection matrices B \u2208 C1\u00d7N ,C \u2208 CN\u00d71,D \u2208 C1\u00d71 are parameters that could be learned by gradient descent. To use LSSL in a discrete sequence-to-sequence model, the bilinear method [33] can be used to discretize all continuous-time signals as follows:\nxk = Axk\u22121 +Buk\nvk = Cxk +Duk\nA = (I \u2212\u2206/2 \u00b7A)\u22121(I +\u2206/2 \u00b7A) B = (I \u2212\u2206/2 \u00b7A)\u22121\u2206B\n(2)\nwhere \u2206 is a trainable time-step size parameter. Since Eq. (2) becomes a recurrence in the hidden state vector xk, LSSL can now be considered a special RNN with linearity. By setting the initial state to be x\u22121 = 0 and unfolding Eq. refeq:continuousSSM):\nvk = CA k Bu0 +CA k\u22121 Bu1 + \u00b7 \u00b7 \u00b7+CBuk +Duk (3)\nThat is, the output sequence v = (v0, v1, ..., vL\u22121) with length L could be computed as a convolution v = K\u2217u+Du, where u = (u0, u1, ..., uL\u22121) and K is called an SSM convolution kernel defined as a 1-D sequence:\nK := (K0,K1, ...,KL\u22121) = ( CB,CAB, . . . ,CA L\u22121 B )\n(4) Therefore, LSSL can also be viewed as a convolutional layer with a global receptive field [28, 29], and it can be computed very efficiently with fast Fourier transform once the SSM convolutional kernel, K, is known.\nOne major bottleneck of LSSL is that treating A as trainable parameters means computing K as many times as it is necessary in the training stage. However, it is challenging to compute A i efficiently enough for practical usage without making the whole model unstable. The S4 layer overcomes this critical issue by decomposing the state transition matrix A into a sum of a low-rank matrix [34] and a skew-symmetric matrix [35]. In practice, S4 re-parameterizes the state-transition matrices A as A = \u039b \u2212 PP \u2217, where \u039b \u2208 CN is a diagnoal matrix and P \u2208 CN . In S4, D is set equal to 0 since it could be replaced by a residual connection. Therefore, an S4 layer is comprised of 4N trainable matrix parameters: \u039b, P,B, and C. The interested reader is referred to [29, 30] for more details on S4."
        },
        {
            "heading": "2.2. SSM with Multi-dimensional Patterns",
            "text": "The S4 layer was developed for 1-D inputs, which limited its applicability. In [28, 29, 30], that limitation was overcome by (i) running H independent copies of the S4 layer on a 2-D input features with a shape of (H,L), and (ii) mixing all output features by a position-wise feedforward layer. In practice, the 2-D input is assumed to consist of H independent signals, analogous to a 1-D CNN layer with H channels. However, most of the 2-D inputs are correlated with each other in both axes. For example, a 2-D spectrogram is not only time-dependent but also has strong spectral dependencies across the frequency axis. In [32],\nthe conventional S4 layer was extended to multi-dimensional signals by turning the standard SSM (1-D ODEs) into multidimensional partial differential equations (PDEs) governed by an independent SSM in each dimension. To make it clear, let u = u(t1, t2) \u2208 R2 \u2192 C and v = v(t1, t2) \u2208 R2 \u2192 C be the input and output signals, and x(t1, t2) be the SSM state with dimension N1 \u00d7N2, which equals to the outer product of x1(t1, t2) \u2208 CN1 and x2(t1, t2) \u2208 CN2 . A 2-D SSM thus can be represented by the following two-variable PDEs:\n\u2202\n\u2202t1 x(t1, t2) = A1x1(t1, t2)\u2297 x2(t1, t2) +B1u(t1, t2)\n\u2202\n\u2202t2 x(t1, t2) = x1(t1, t2)\u2297A2x2 (t1, t2) +B2u (t1, t2)\nv(t1, t2) = \u27e8C, x(t1, t2)\u27e9 (5)\nwhere \u2297 is the outer-product operator, \u27e8\u00b7, \u00b7\u27e9 is the point-wise inner-product operators for two matrices. Factoring that matrix C as a low-rank tensor and using the standard 1-D S4 layer as a black box, a 2D-version S4 layer is equivalent to a multi-dimensional convolution with an infinite receptive field. The multi-dimensional S4 layer is referred to as S4ND [32]. The same bilinear method mentioned in Sec. 2.1 can be applied when handling 2D discrete-time inputs."
        },
        {
            "heading": "3. Proposed Deep Structured State Space Modeling for Speech Enhancement",
            "text": ""
        },
        {
            "heading": "3.1. Time-domain S4-based Model",
            "text": "The original S4-based model in [30] can be used as a timedomain regression SE model, consisting of repeated S4-block combined with a U-net architecture. This model is referred to as Time-domain S4 U-Net. A 1-D convolutional layer with kernel size = 1 will first turn the single-channel noisy speech into a hidden feature with 64 channels before it goes through the whole U-net path. Another single-kernel 1-D convolutional layer will instead shrink the channel size back to 1, which represents the enhanced speech. More details can be found [30].\nThe time-domain S4 U-Net is optimized by the loss function proposed in [26], which is the L1 loss over the waveform together with a multi-resolution short-time Fourier transform (STFT) loss over the spectrogram magnitude [36]:\nlossSTFT (y, y\u0303) = | |STFT (y)| \u2212 |STFT (y\u0303)| |F\n| |STFT (y)| |F\n+ 1\nT | log(|STFT (y)|)\u2212 log(|STFT (y\u0303)|) |1 (6)\nloss(y, y\u0303) = 1\nT | y \u2212 y\u0303 |1 +\n1\nM M\u2211 i=1 loss (i) STFT (y, y\u0303) (7)\nwhere STFT stands for short-time Fourier transform, | \u00b7 |F is the Forbenious norm, y, y\u0303 are the clean and estimated speech, respectively, and M means the total number of different STFT settings when computing lossSTFT (y, y\u0303). We use the same three STFT settings to compute loss(i)STFT (y, y\u0303) in [26]."
        },
        {
            "heading": "3.2. TF-domain S4-based Model",
            "text": "We deploy two S4-based SE models in the TF-domain, namely (i) only the magnitude of the spectrogram is enhanced and then the noisy phase is used to reconstruct the speech waveform, or (ii) the complex noisy spectrogram is used to directly estimate both the magnitude and phase information simultaneously. In\nthe magnitude-only scenario, two variants are put forth: a regression model, which maps noisy spectrograms to clean spectrograms, and a masking model which predicts a spectrogram mask. In the complex scenario, we adopt the masking method for the complex spectrogram proposed in [37, 38] since the regression-based SE model led to an unstable training process in the complex case. In sum, there are three training scenarios: mag-regression, mag-masking, and complex-masking.\nL1 loss is employed to optimize our TF-domain deep architecture, as suggested in [39]. More specifically, the following loss is used when only the magnitude information is taken into account (i.e., mag-regression and mag-masking):\nLossmag = 1\nTF T\u22121\u2211 t=0 F\u22121\u2211 f=0 | S(t, f)\u2212 S\u0303(t, f) | (8)\nwhere S and S\u0303 are the magnitude spectrograms of the clean speech and enhanced speech, respectively, and T and F are the numbers of frames and frequency bins, respectively.\nWhen the complex spectrogram is used as the input/output, the loss function becomes:\nLosscomplex = 1\nTF T\u22121\u2211 t=0 F\u22121\u2211 f=0 | Sr(t, f)\u2212 S\u0303r(t, f) |\n+ | Si(t, f)\u2212 S\u0303i(t, f) | +Lossmag\n(9)\nwhere r, and i denote the real and imaginary parts of the complex-valued spectrogram, respectively."
        },
        {
            "heading": "3.2.1. 1-D S4-based Model with Whitening Transform",
            "text": "As discussed in Section 2.2, we could directly adopt the 1-D S4-based layer to build a deep SE model working on 2-D spectrograms by treating frequency bins as a group of independent 1-D signals. In doing so, the deep SE model can not exploit dependencies across the frequency axis. However, we introduced a whitening transformation [40] to the input spectrogram so that the frequency bins can be considered statistically uncorrelated to one another. This system is called TF-domain S4 U-Net."
        },
        {
            "heading": "3.2.2. Multi-dimensional S4-based Model",
            "text": "Although the whitening transformation can theoretically alleviate the frequency-dependent problem, it doesn\u2019t really help the model capture the 2-D information as well as a 2-D CNN layer does. Our experiments (see Sec. 4.2) show that while adding the whitening transformation is helpful, it doesn\u2019t perform better than the time-domain S4-based model but with more model parameters. For that reason, we build a new SE model based on the S4ND layer, which is named S4ND U-Net.\nFig. 1 illustrates how the proposed S4ND U-Net is built. S4ND U-Net is similar to the Deep Complex Convolutional Recurrent Network (DCCRN) model [38], which is a U-netlike architecture with several down-layers and up-layers, where we have however replaced all complex CNN/RNN layers with S4ND layers. Furthermore, we have reduced the model complexity by shrinking the number of down-layers and up-layers to 2 (the original DCCRN model had 6 layers), since it is unnecessary to stack too many S4ND layers to increase the receptive field. With complex spectrograms, we stack the real and imaginary parts as a real-value spectrogram with two channels."
        },
        {
            "heading": "4. Experiments and Result Analyses",
            "text": ""
        },
        {
            "heading": "4.1. Experimental Setup",
            "text": ""
        },
        {
            "heading": "4.1.1. Dataset",
            "text": "We choose the VoiceBank-DEMAND dataset [41], a common SE benchmark data set providing recordings from 30 speakers with 10 types of noise, to assess our models. The data set is split into a training and a testing set with 28 and 2 speakers, respectively. Four types of signal-to-noise ratios (SNRs) are used to mix clean samples with noise samples in the dataset, [0, 5, 10, 15] dB for training and [2.5, 7.5, 12.5, 17.5] dB for testing. We follow the approach used by Lu et al. [17] to form the validation set by excerpting two speakers from the training set. This resulted in 10,802 utterances for training and 770 for validation. The testing set included a total of 824 utterances. All recordings were downsampled from 48 kHz to 16 kHz. Our evaluation metrics include the wide-band perceptual evaluation of speech quality (PESQ) [42], prediction of the signal distortion (CSIG), prediction of the background intrusiveness (CBAK), and prediction of the overall speech quality (COVL)."
        },
        {
            "heading": "4.1.2. STFT feature transformation and data augmentation",
            "text": "During training, an amplitude transformation as in [18] is used to normalize the amplitude of all complex coefficients of both noisy and clean spectrograms. Two out of four data augmentation methods in [26], namely, Remix and BandMask are used. Remix shuffles the noises within one batch to form new noisy mixtures. BandMask is a band-stop filter that removes 20% of the frequencies starting from f0, a random frequency sampled uniformly in the mel scale."
        },
        {
            "heading": "4.2. Experimental Results",
            "text": "In Table 1, we first assess our baseline time-domain S4 U-Net discussed in Sec. 3.1 and compare it with other time-domain techniques. We build time-domain S4 U-Net according to the default hyper-parameter setting in [29]. A visual inspection of\nTable 1 shows that when compared with other models, timedomain S4 U-Net achieves a good PESQ (with PESQ=3.02 in the bottom row) and the best CSIG, CBAK, and COVL scores. Although DEMUCS (with PESQ=3.07 in the fourth row) has a better PESQ score than time-domain S4 U-Net, its performance degrades to 2.93 when its model size shrinks from 33.5 million to 18.9 million. This result verifies that the S4 layers can replace CNN/RNN layers in a DNN-based model to reduce the model sizes while maintaining competitive performances.\nNext in Table 2, we compare the proposed TF-domain S4 U-Net and S4ND U-Net with previously proposed TF-domain models. All networks use a U-Net structure and the same complex-masking approach [37] except Metric GAN+. From Table 2, we can see that S4ND U-Net attains the best results among all models on PESQ/CSIG/COVL metrics with relatively good CBAK scores. It is worth noting that Metric GAN+ optimizes the PESQ score directly (with the best PESQ=3.15 in the sixth row) at the cost of getting worse results on the other metrics. Additionally, DCUnet-20 attains the best CBAK of 4.00 with PESQ=3.13 in the fifth row. However, its evaluation results significantly drop when halving the number of layers. In contrast, S4ND U-Net uses only 0.75M parameters, corresponding to a 78.6% reduction in model size compared to DCUnet-20. It also achieves a PESQ score of 3.15 (equal to the best GAN+) as shown in the bottom row.\nTo have a fair comparison, we retrain DCCRN from scratch with the same data augmentation and amplitude transformation (see Sec. 4.1.2) used to build S4ND U-Net. Two STFT settings (i.e., subscripts \u201d1\u201d and \u201d2\u201d in Table 3) are adopted to investigate how DCCRN and S4ND U-Net react when the input STFT spectrograms have different parameter settings. Furthermore, DCCRN parameters were limited to 1.21M by halving the number of down/up layers from 6 to 3. As shown in Table 3, S4ND U-Net outperforms both the small and large DCCRN under the first STFT setting, with PESQ scores of 3.15 and 3.18 in the fifth\nand sixth rows, respectively. When a bigger hop-length=255 is chosen and the time-length of input spectrograms decreases correspondingly (i.e., the second STFT setting), the S4ND UNet\u2019s performance drops a little bit (with PESQ=3.09 and 3.11 in the bottom two rows) but remains superior to DCCRN. This suggests our S4ND U-Net works particularly well when dealing with long input sequences. With a large S4ND U-Net with 4.4M parameters by increasing the number of down/up layers from 2 to 5, We only improve PESQ slightly from 3.15 to 3.18. This confirms there is no need to stack too many S4ND layers to increase the receptive field. Even without data augmentation, S4ND U-Net (with PESQ=2.99 shown in the second last row of Table 2) still outperforms both DCCRN and DCUnet-16.\nFinally, we assess S4ND U-Net under three different TFdomain scenarios, namely mag-regression, mag-masking, and complex-masking. Results in Table 4 show that complexmasking attains a better PESQ score of 3.15 as shown in the bottom row when compared to 3.05 in the top and 3.12 in the middle rows. It is noted that although complex masking manages to achieve the best result among the three TF-domain scenarios compared here, models with only enhanced magnitudes plus noisy phase information for waveform reconstruction also perform well using the proposed S4ND U-net architecture."
        },
        {
            "heading": "5. Conclusion",
            "text": "We have conducted a series of experiments to investigate new uses of S4 layers for speech enhancement. We first develop an S4-based deep SE neural model to enhance speech in the time domain and attain promising results. To extend the approach to multi-dimensional inputs, we propose two techniques: a whitening transformation and an S4ND U-Net architecture. We found that our proposed S4ND U-Net not only outperforms the original time-domain S4 U-Net but also attains comparable or better scores in four evaluation metrics when contrasted with other time-domain and TF-domain U-Net models. Our results also empirically verify that fewer S4ND layers can be used for compact model design, resulting in a robust TF-domain SE model with a limited size. Codes used in this work are released at https://github.com/Kuray107/ S4ND-U-Net_speech_enhancement"
        },
        {
            "heading": "6. References",
            "text": "[1] B. Wu, K. Li, F. Ge, Z. Huang, M. Yang, S. M. Siniscalchi, and C.-\nH. Lee, \u201cAn end-to-end deep learning approach to simultaneous speech dereverberation and acoustic modeling for robust speech recognition,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 11, pp. 1289\u20131300, 2017.\n[2] J. H. Hansen and T. Hasan, \u201cSpeaker recognition by machines and humans: A tutorial review,\u201d IEEE Signal Processing Magazine, vol. 32, no. 6, pp. 74\u201399, 2015.\n[3] S. Boll, \u201cSuppression of acoustic noise in speech using spectral subtraction,\u201d IEEE Trans. on Acoustics, Speech, and Signal Processing, vol. 27, no. 2, pp. 113\u2013120, 1979.\n[4] J. Lim and A. Oppenheim, \u201cEnhancement and bandwidth compression of noisy speech,\u201d Proceedings of the IEEE, vol. 67, no. 12, pp. 1586\u20131604, 1979.\n[5] K. Paliwal and A. Basu, \u201cA speech enhancement method based on kalman filtering,\u201d in Proc. ICASSP, 1987.\n[6] V. Krishnan, S. Siniscalchi, D. Anderson, and M. Clements, \u201cNoise robust aurora-2 speech recognition employing a codebook-constrained kalman filter preprocessor,\u201d in Proc. ICASSP, 2006.\n[7] M. Berouti, R. Schwartz, and J. Makhoul, \u201cEnhancement of speech corrupted by acoustic noise,\u201d in Proc. ICASSP, 1979.\n[8] Y. Xu, J. Du, L.-R. Dai, and C.-H. Lee, \u201cAn experimental study on speech enhancement based on deep neural networks,\u201d IEEE Signal Processing Letters, vol. 21, no. 1, pp. 65\u201368, 2014.\n[9] \u2014\u2014, \u201cA regression approach to speech enhancement based on deep neural networks,\u201d IEEE/ACM Trans. Audio, Speech, Language Process, vol. 23, no. 1, pp. 7\u201319, 2015.\n[10] D. Wang, \u201cTime-frequency masking for speech separation and its potential for hearing aid design,\u201d Trends in amplification, vol. 12, no. 4, pp. 332\u2013353, 2008.\n[11] Y. Luo and N. Mesgarani, \u201cConv-tasnet: Surpassing ideal time\u2013 frequency magnitude masking for speech separation,\u201d IEEE/ACM Trans. Audio, Speech, Language Process, vol. 27, no. 8, pp. 1256\u2013 1266, 2019.\n[12] S. Pascual, A. Bonafonte, and J. Serra\u0300, \u201cSegan: Speech enhancement generative adversarial network,\u201d in Proc. Interspeech, 2017.\n[13] S.-W. Fu, C.-F. Liao, Y. Tsao, and S.-D. Lin, \u201cMetricGAN: Generative adversarial networks based black-box metric scores optimization for speech enhancement,\u201d in Proc. ICML, 2019.\n[14] S.-W. Fu, C. Yu, T.-A. Hsieh, P. Plantinga, M. Ravanelli, X. Lu, and Y. Tsao, \u201cMetricGAN+: An Improved Version of MetricGAN for Speech Enhancement,\u201d in Proc. Interspeech, 2021.\n[15] Y. Bando, K. Sekiguchi, and K. Yoshii, \u201cAdaptive Neural Speech Enhancement with a Denoising Variational Autoencoder,\u201d in Proc. Interspeech, 2020.\n[16] H. Fang, G. Carbajal, S. Wermter, and T. Gerkmann, \u201cVariational autoencoder for speech enhancement with a noise-aware encoder,\u201d in Proc. ICASSP, 2021.\n[17] Y.-J. Lu, Z.-Q. Wang, S. Watanabe, A. Richard, C. Yu, and Y. Tsao, \u201cConditional diffusion probabilistic model for speech enhancement,\u201d in Proc. ICASSP, 2022.\n[18] J. Richter, S. Welker, J.-M. Lemercier, B. Lay, and T. Gerkmann, \u201cSpeech enhancement and dereverberation with diffusion-based generative models,\u201d arXiv preprint arXiv:2208.05830, 2022.\n[19] H. Yen, F. G. Germain, G. Wichern, and J. L. Roux, \u201cCold diffusion for speech enhancement,\u201d arXiv preprint arXiv:2211.02527, 2022.\n[20] E. M. Grais, M. U. Sen, and H. Erdogan, \u201cDeep neural networks for single channel source separation,\u201d in Proc. ICASSP, 2014.\n[21] Y. Wang, A. Narayanan, and D. Wang, \u201cOn training targets for supervised speech separation,\u201d IEEE/ACM Trans. Audio, Speech, Language Process, vol. 22, no. 12, pp. 1849\u20131858, 2014.\n[22] T. Gao, J. Du, L.-R. Dai, and C.-H. Lee, \u201cDensely connected progressive learning for lstm-based speech enhancement,\u201d in Proc. ICASSP, 2018.\n[23] P.-S. Huang, M. Kim, M. Hasegawa-Johnson, and P. Smaragdis, \u201cDeep learning for monaural speech separation,\u201d in Proc. ICASSP, 2014.\n[24] F. Weninger, H. Erdogan, S. Watanabe, E. Vincent, J. L. Roux, J. R. Hershey, and B. Schuller, \u201cSpeech enhancement with LSTM recurrent neural networks and its application to noise-robust ASR,\u201d in Proc. LVA/ICA, 2015.\n[25] R. Giri, U. Isik, and A. Krishnaswamy, \u201cAttention wave-u-net for speech enhancement,\u201d in Proc. WASPAA, 2019.\n[26] A. De\u0301fossez, G. Synnaeve, and Y. Adi, \u201cReal Time Speech Enhancement in the Waveform Domain,\u201d in Proc. Interspeech, 2020.\n[27] R. Pascanu, T. Mikolov, and Y. Bengio, \u201cOn the difficulty of training recurrent neural networks,\u201d in Proc. ICML, 2013.\n[28] A. Gu, I. Johnson, K. Goel, K. K. Saab, T. Dao, A. Rudra, and C. Re, \u201cCombining recurrent, convolutional, and continuous-time models with linear state space layers,\u201d in Proc. NeuIPS, 2021.\n[29] A. Gu, K. Goel, and C. Re, \u201cEfficiently modeling long sequences with structured state spaces,\u201d in Proc. ICLR, 2022.\n[30] K. Goel, A. Gu, C. Donahue, and C. Re, \u201cIt\u2019s raw! Audio generation with state-space models,\u201d in Proc. ICML, 2022.\n[31] K. Miyazaki, M. Murata, and T. Koriyama, \u201cStructured state space decoder for speech recognition and synthesis,\u201d arXiv preprint arXiv:2210.17098, 2022.\n[32] E. Nguyen, K. Goel, A. Gu, G. Downs, P. Shah, T. Dao, S. Baccus, and C. Re\u0301, \u201cS4ND: Modeling images and videos as multidimensional signals with state spaces,\u201d in Proc. NeuIPS, 2022.\n[33] A. Tustin, \u201cA method of analysing the behaviour of linear systems in terms of time series,\u201d Journal of the Institution of Electrical Engineers-Part IIA: Automatic Regulators and Servo Mechanisms, vol. 94, no. 1, pp. 130\u2013142, 1947.\n[34] I. Markovsky, \u201cStructured low-rank approximation and its applications,\u201d Automatica, vol. 44, no. 4, pp. 891\u2013909, 2008.\n[35] J. R. Bunch, \u201cA note on the stable decompostion of skewsymmetric matrices,\u201d Mathematics of Computation, vol. 38, no. 158, pp. 475\u2013479, 1982.\n[36] R. Yamamoto, E. Song, and J.-M. Kim, \u201cParallel wavegan: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram,\u201d in Proc. ICASSP, 2020.\n[37] H.-S. Choi, J.-H. Kim, J. Huh, A. Kim, J.-W. Ha, and K. Lee, \u201cPhase-aware speech enhancement with deep complex u-net,\u201d in Proc. ICLR, 2019.\n[38] Y. Hu, Y. Liu, S. Lv, M. Xing, S. Zhang, Y. Fu, J. Wu, B. Zhang, and L. Xie, \u201cDCCRN: Deep Complex Convolution Recurrent Network for Phase-Aware Speech Enhancement,\u201d in Proc. Interspeech, 2020.\n[39] J. Qi, J. Du, S. M. Siniscalchi, X. Ma, and C.-H. Lee, \u201cOn mean absolute error for deep neural network based vector-to-vector regression,\u201d IEEE Signal Processing Letters, vol. 27, pp. 1485\u2013 1489, 2020.\n[40] A. Kessy, A. Lewin, and K. Strimmer, \u201cOptimal whitening and decorrelation,\u201d The American Statistician, vol. 72, 2015.\n[41] C. Valentini-Botinhao, \u201cNoisy speech database for training speech enhancement algorithms and tts models,\u201d 2017.\n[42] A. Rix, J. Beerends, M. Hollier, and A. Hekstra, \u201cPerceptual evaluation of speech quality (pesq)-a new method for speech quality assessment of telephone networks and codecs,\u201d in Proc. ICASSP, 2001.\n[43] C. Macartney and T. Weyde, \u201cImproved speech enhancement with the wave-u-net,\u201d arXiv preprint arXiv:1811.11307, 2018.\n[44] S. Lv, Y. Fu, M. Xing, J. Sun, L. Xie, J. Huang, Y. Wang, and T. Yu, \u201cS-dccrn: Super wide band dccrn with learnable complex feature for speech enhancement,\u201d in Proc. ICASSP, 2022."
        }
    ],
    "title": "A Multi-dimensional Deep Structured State Space Approach to Speech Enhancement Using Small-footprint Models",
    "year": 2023
}