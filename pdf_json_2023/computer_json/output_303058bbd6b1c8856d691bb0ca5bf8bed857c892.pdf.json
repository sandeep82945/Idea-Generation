{
    "abstractText": "Graphs are widely used to encapsulate a variety of data formats, but real-world networks often involve complex node relations beyond only being pairwise. While hypergraphs and hierarchical graphs have been developed and employed to account for the complex node relations, they cannot fully represent these complexities in practice. Additionally, though many Graph Neural Networks (GNNs) have been proposed for representation learning on higher-order graphs, they are usually only evaluated on simple graph datasets. Therefore, there is a need for a unified modelling of higher-order graphs, and a collection of comprehensive datasets with an accessible evaluation framework to fully understand the performance of these algorithms on complex graphs. In this paper, we introduce the concept of hybrid graphs, a unified definition for higher-order graphs, and present the Hybrid Graph Benchmark (HGB). HGB contains 23 real-world hybrid graph datasets across various domains such as biology, social media, and e-commerce. Furthermore, we provide an extensible evaluation framework and a supporting codebase to facilitate the training and evaluation of GNNs on HGB. Our empirical study of existing GNNs on HGB reveals various research opportunities and gaps, including (1) evaluating the actual performance improvement of hypergraph GNNs over simple graph GNNs; (2) comparing the impact of different sampling strategies on hybrid graph learning methods; and (3) exploring ways to integrate simple graph and hypergraph information. We make our source code and full datasets publicly available at https://zehui127.github.io/hybrid-graph-benchmark/.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zehui Li"
        },
        {
            "affiliations": [],
            "name": "Xiangyu Zhao"
        },
        {
            "affiliations": [],
            "name": "Mingzhu Shen"
        },
        {
            "affiliations": [],
            "name": "Guy-Bart Stan"
        },
        {
            "affiliations": [],
            "name": "Pietro Li\u00f2"
        },
        {
            "affiliations": [],
            "name": "Yiren Zhao"
        }
    ],
    "id": "SP:38f0644087cf34828b52c922c1369f6984e11a25",
    "references": [
        {
            "authors": [
                "I. Amburg",
                "N. Veldt",
                "A. Benson"
            ],
            "title": "Clustering in graphs and hypergraphs with categorical edge labels",
            "venue": "In Proceedings of The Web Conference",
            "year": 2020
        },
        {
            "authors": [
                "S. Bai",
                "F. Zhang",
                "P.H. Torr"
            ],
            "title": "Hypergraph convolution and hypergraph attention",
            "venue": "Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "M. Ben Guebila",
                "C.M. Lopes-Ramos",
                "D. Weighill",
                "A.R. Sonawane",
                "R. Burkholz",
                "B. Shamsaei",
                "J. Platig",
                "K. Glass",
                "M.L. Kuijjer",
                "J. Quackenbush"
            ],
            "title": "GRAND: a database of gene regulatory network models across human conditions",
            "venue": "Nucleic Acids Research,",
            "year": 2022
        },
        {
            "authors": [
                "S. Brody",
                "U. Alon",
                "E. Yahav"
            ],
            "title": "How attentive are graph attention networks",
            "venue": "In The 10th International Conference on Learning Representations. OpenReview.net",
            "year": 2022
        },
        {
            "authors": [
                "Chan",
                "T.-H. H",
                "Z. Liang"
            ],
            "title": "Generalizing the hypergraph laplacian via a diffusion process with mediators",
            "venue": "Theoretical Computer Science,",
            "year": 2020
        },
        {
            "authors": [
                "E. Chien",
                "C. Pan",
                "J. Peng",
                "O. Milenkovic"
            ],
            "title": "You are AllSet: A multiset function framework for hypergraph neural networks",
            "venue": "In The 10th International Conference on Learning Representations,. OpenReview.net",
            "year": 2022
        },
        {
            "authors": [
                "P.S. Chodrow",
                "N. Veldt",
                "A.R. Benson"
            ],
            "title": "Generative hypergraph clustering: From blockmodels to modularity",
            "venue": "Science Advances,",
            "year": 2021
        },
        {
            "authors": [
                "M. Choe",
                "J. Yoo",
                "G. Lee",
                "W. Baek",
                "U. Kang",
                "K. Shin"
            ],
            "title": "MiDaS: Representative sampling from real-world hypergraphs",
            "venue": "In Proceedings of the ACM Web Conference",
            "year": 2022
        },
        {
            "authors": [
                "Y. Dong",
                "W. Sawin",
                "Y. Bengio"
            ],
            "title": "HNHN: Hypergraph networks with hyperedge neurons",
            "venue": "ICML 2020 Graph Representation Learning and Beyond Workshop",
            "year": 2020
        },
        {
            "authors": [
                "M. Dyer",
                "C. Greenhill",
                "P. Kleer",
                "J. Ross",
                "L. Stougie"
            ],
            "title": "Sampling hypergraphs with given degrees",
            "venue": "Discrete Mathematics,",
            "year": 2021
        },
        {
            "authors": [
                "G. Eraslan",
                "\u017d. Avsec",
                "J. Gagneur",
                "F.J. Theis"
            ],
            "title": "Deep learning: new computational modelling techniques for genomics",
            "venue": "Nature Reviews Genetics,",
            "year": 2019
        },
        {
            "authors": [
                "Y. Fang",
                "S. Sun",
                "Z. Gan",
                "R. Pillai",
                "S. Wang",
                "J. Liu"
            ],
            "title": "Hierarchical graph network for multi-hop question answering",
            "venue": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2020
        },
        {
            "authors": [
                "Y. Feng",
                "H. You",
                "Z. Zhang",
                "R. Ji",
                "Y. Gao"
            ],
            "title": "Hypergraph neural networks",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "H. Gao",
                "S. Ji"
            ],
            "title": "Graph u-nets",
            "venue": "In Proceedings of the 36th International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "L. Getoor"
            ],
            "title": "Link-based classification. In Advanced Methods for Knowledge Discovery from Complex Data, pages 189\u2013207",
            "year": 2005
        },
        {
            "authors": [
                "A. Gotmare",
                "N.S. Keskar",
                "C. Xiong",
                "R. Socher"
            ],
            "title": "A closer look at deep learning heuristics: Learning rate restarts, warmup and distillation",
            "venue": "In The 7th International Conference on Learning Representations. OpenReview.net",
            "year": 2019
        },
        {
            "authors": [
                "W. Hamilton",
                "Z. Ying",
                "J. Leskovec"
            ],
            "title": "Inductive representation learning on large graphs",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "R. He",
                "J. McAuley"
            ],
            "title": "Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering",
            "venue": "In Proceedings of the 25th International Conference on World Wide Web,",
            "year": 2016
        },
        {
            "authors": [
                "D.P. Kingma",
                "J. Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "In The 3rd International Conference on Learning Representations,",
            "year": 2015
        },
        {
            "authors": [
                "T.N. Kipf",
                "M. Welling"
            ],
            "title": "Semi-supervised classification with graph convolutional networks",
            "venue": "In The 5th International Conference on Learning Representations, Conference Track Proceedings. OpenReview.net",
            "year": 2017
        },
        {
            "authors": [
                "J. Lee",
                "I. Lee",
                "J. Kang"
            ],
            "title": "Self-attention graph pooling",
            "venue": "In Proceedings of the 36th International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "N.W. Lemons",
                "B. Hu",
                "W.S. Hlavacek"
            ],
            "title": "Hierarchical graphs for rule-based modeling of biochemical systems",
            "venue": "BMC Bioinformatics,",
            "year": 2011
        },
        {
            "authors": [
                "G. Li",
                "M. Muller",
                "A. Thabet",
                "B. Ghanem"
            ],
            "title": "DeepGCNs: Can GCNs go as deep as CNNs",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Y. Long",
                "X. Wang",
                "D.T. Youmans",
                "T.R. Cech"
            ],
            "title": "How do lncRNAs regulate transcription",
            "venue": "Science Advances,",
            "year": 2017
        },
        {
            "authors": [
                "G.A. Maston",
                "S.K. Evans",
                "M.R. Green"
            ],
            "title": "Transcriptional regulatory elements in the human genome",
            "venue": "Annual Review of Genomics and Human Genetics,",
            "year": 2006
        },
        {
            "authors": [
                "J. McAuley",
                "C. Targett",
                "Q. Shi",
                "A. Van Den Hengel"
            ],
            "title": "Image-based recommendations on styles and substitutes",
            "venue": "In Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval,",
            "year": 2015
        },
        {
            "authors": [
                "A.K. McCallum",
                "K. Nigam",
                "J. Rennie",
                "K. Seymore"
            ],
            "title": "Automating the construction of Internet portals with machine learning",
            "venue": "Information Retrieval,",
            "year": 2000
        },
        {
            "authors": [
                "J. Ni",
                "J. Li",
                "J. McAuley"
            ],
            "title": "Justifying recommendations using distantly-labeled reviews and fine-grained aspects",
            "venue": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing,",
            "year": 2019
        },
        {
            "authors": [
                "P. Pati",
                "G. Jaume",
                "A. Foncubierta-Rodr\u00edguez",
                "F. Feroce",
                "A.M. Anniciello",
                "G. Scognamiglio",
                "N. Brancati",
                "M. Fiche",
                "E. Dubruc",
                "D. Riccio",
                "M. Di Bonito",
                "G. De Pietro",
                "G. Botti",
                "Thiran",
                "J.-P",
                "M. Frucci",
                "O. Goksel",
                "M. Gabrani"
            ],
            "title": "Hierarchical graph representations in digital pathology",
            "venue": "Medical Image Analysis,",
            "year": 2022
        },
        {
            "authors": [
                "A. Radford",
                "J.W. Kim",
                "C. Hallacy",
                "A. Ramesh",
                "G. Goh",
                "S. Agarwal",
                "G. Sastry",
                "A. Askell",
                "P. Mishkin",
                "J Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In Proceedings of the 38th International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "B. Rozemberczki",
                "C. Allen",
                "R. Sarkar"
            ],
            "title": "Multi-scale attributed node embedding",
            "venue": "Journal of Complex Networks,",
            "year": 2021
        },
        {
            "authors": [
                "P. Sen",
                "G. Namata",
                "M. Bilgic",
                "L. Getoor",
                "B. Galligher",
                "T. Eliassi-Rad"
            ],
            "title": "Collective classification in network data",
            "year": 2008
        },
        {
            "authors": [
                "O. Shchur",
                "M. Mumme",
                "A. Bojchevski",
                "S. G\u00fcnnemann"
            ],
            "title": "Pitfalls of graph neural network evaluation",
            "venue": "NeurIPS",
            "year": 2018
        },
        {
            "authors": [
                "P. Veli\u010dkovi\u0107",
                "G. Cucurull",
                "A. Casanova",
                "A. Romero",
                "P. Li\u00f2",
                "Y. Bengio"
            ],
            "title": "Graph attention networks",
            "venue": "In The 6th International Conference on Learning Representations, Conference Track Proceedings. OpenReview.net",
            "year": 2018
        },
        {
            "authors": [
                "N. Yadati",
                "M. Nimishakavi",
                "P. Yadav",
                "V. Nitin",
                "A. Louis",
                "P. Talukdar"
            ],
            "title": "HyperGCN: A new method for training graph convolutional networks on hypergraphs",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Z. Ying",
                "J. You",
                "C. Morris",
                "X. Ren",
                "W. Hamilton",
                "J. Leskovec"
            ],
            "title": "Hierarchical graph representation learning with differentiable pooling",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "H. Zeng",
                "H. Zhou",
                "A. Srivastava",
                "R. Kannan",
                "V. Prasanna"
            ],
            "title": "GraphSAINT: Graph sampling based inductive learning method",
            "venue": "In The 8th International Conference on Learning Representations. OpenReview.net",
            "year": 2020
        },
        {
            "authors": [
                "Adam (Kingma",
                "Ba"
            ],
            "title": "optimiser, and CosineAnnealingLR (Gotmare et al., 2019) is used as the learning rate scheduler for all training. All models are trained for 50 epochs. For each experiment, the nodes of the used hybrid graph are split into the train, validation, and test sets with a split ratio of 6:2:2",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Graphs are powerful tools for capturing relationships between objects, ranging from social networks, biology (Rozemberczki et al., 2021) to e-commerce (McAuley et al., 2015; He and McAuley, 2016; Ni et al., 2019). However, real-world large-scale networks often involve complex node relations beyond only being pairwise. While hypergraphs and hierarchical graphs (McCallum et al., 2000; Getoor, 2005; Sen et al., 2008; Chien et al., 2022) have been introduced to capture these complex node relations, they still cannot fully represent the real-world scenarios. Although hypergraphs can describe clustering relations between multiple nodes, they are still limited in their ability to represent hierarchical relations. On the other hand, while hierarchical graphs can make up for the limitation of hypergraphs, they are not flexible enough to represent multi-node relations.\nExisting Graph Neural Networks (GNNs) have been proposed for representation learning on simple graphs (Kipf and Welling, 2017; Hamilton et al., 2017; Velic\u030ckovic\u0301 et al., 2018; Li et al., 2019; Brody et al., 2022) and higher-order graphs (Feng et al., 2019; Yadati et al., 2019; Chan and Liang, 2020;\n\u2217Equal contribution. Correspondence to {zehui.li22,x.zhao22}@imperial.ac.uk.\nPreprint. Under review.\nar X\niv :2\n30 6.\n05 10\n8v 1\n[ cs\n.L G\n] 8\nJ un\n2 02\nDong et al., 2020; Zeng et al., 2020; Bai et al., 2021; Brody et al., 2022). However, they are usually only evaluated on simple graph datasets, which do not fully reflect the complexity of real-world graph structures. Besides, without a unified modeling of higher-order graphs, nor a collection of comprehensive datasets with an accessible evaluation framework, we cannot fully uncover the underlying performance of these GNNs on complex graphs.\nTo address these limitations, we endeavour to construct the Hybrid Graph Benchmark (HGB), with a unified view of complex graphs including both datasets and an evaluation framework. Firstly, we introduce the concept of hybrid graphs, which provides a unified definition for higher-order graphs. Hybrid graphs extend the notion of simple graphs by allowing nodes to be connected to sets of other nodes, rather than just individual nodes. This enables hybrid graphs to capture more complex node interactions, including simple and higher-order interactions. Based on this unified definition, we build 23 real-world hybrid graph datasets across various domains such as biology, social media, and e-commerce. Compared with other existing hypergraph datasets (McCallum et al., 2000; Getoor, 2005; Sen et al., 2008; Chien et al., 2022) that are on a relatively small scale and do not facilitate hierarchical node relations, we believe our HGB represents a significant step forward in the development of comprehensive datasets for evaluating higher-order graph learning algorithms.\nFurthermore, to facilitate fair evaluation of GNNs on our proposed hybrid graph datasets, we provide an extensible evaluation framework and a supporting codebase. Our evaluation framework includes several common graph prediction tasks using their corresponding evaluation metrics, such as node classification and regression tasks. We benchmark seven widely-used GNN models (Kipf and Welling, 2017; Hamilton et al., 2017; Velic\u030ckovic\u0301 et al., 2018; Brody et al., 2022; Zeng et al., 2020; Bai et al., 2021; Brody et al., 2022) on HGB, and introduce three novel baselines that incorporate simple and higher-order graph information, thereby enabling the researchers to conveniently evaluate their own models and compare the results. In addition, our empirical study of existing GNNs on HGB reveals various research opportunities and gaps, which are elaborated in later sections.\nTo summarise, we provide a comprehensive and unified framework for modelling and evaluating hybrid graph methods, in the hope of stimulating further research in this field. The source code and complete datasets of HGB are publicly available. Our main contributions in this paper are as follows:\n\u2022 We introduce the hybrid graphs, a unified view of high-order graphs for fostering further study in representation learning on complex graphs.\n\u2022 Inspired by this unified mathematical framework, we construct the Hybrid Graph Benchmark (HGB) consisting of 23 datasets covering a wide range of real-world applications. We then extend HGB with an easy-to-use and extensible evaluation framework.\n\u2022 Through extensive experimentation, we have verified both the necessity and superiority of our proposed datasets and benchmarking tool. We also draw insights for the graph representation learning community, such as (1) existing hypergraph GNNs may not keep outperforming simple graph GNNs on large-scale networks; (2) appropriate sampling strategies improve the performance of GNNs on higher-order graphs; and (3) integrating simple and higher-order graph information can significantly enhance the prediction performance on complex graphs."
        },
        {
            "heading": "2 Related Work",
            "text": "Graph Neural Networks on Simple Graphs Graph Neural Networks (GNNs) on simple graphs encode the nodes through neural networks, and learn the representations of the nodes through messagepassing within the graph structure. GCNs (Kipf and Welling, 2017) incorporate the convolution operation into GNNs. GAT (Velic\u030ckovic\u0301 et al., 2018) and GATv2 (Brody et al., 2022) are another family of GNN variants that improves the expressive power of GNNs through attention mechanisms. GraphSAGE (Hamilton et al., 2017) is a general inductive framework that leverages node information to efficiently generate node embeddings for previously unseen data. GraphSAINT (Zeng et al., 2020) emphasises the importance of graph sampling-based inductive learning method to improve training efficiency, especially for large graphs. While these models succeed in simple graph datasets, it is also of great research interest to test their performance on higher-order graphs. However, there lacks a systematic evaluation of these models on higher-order graph datasets.\nGraph Neural Networks for Higher-Order Graphs Higher-order graphs are designed to capture more complex node relations, with hypergraphs being the dominant practice in deep graph representation learning. Hypergraphs refer to graphs in which an edge can connect two or more nodes. In general, GNNs for hypergraphs optimise the node representation through a two-step process. Initially, the node embeddings within each hyperedge are aggregated to form a hidden embedding of each hyperedge. Subsequently, the hidden embeddings of hyperedges with common nodes are aggregated to update the representations of their common nodes. Both HGNN (Feng et al., 2019) and HyperConv (Bai et al., 2021) precisely follow this process. The expressiveness of hypergraph GNNs could be enhanced by modifying this procedure. For instance, HyperGCN (Yadati et al., 2019) refines the node aggregation within hyperedges using mediators (Chan and Liang, 2020); HyperAtten (Bai et al., 2021) proposes to use real value to measure the degree to which a vertex belongs to a hyperedge; HNHN (Dong et al., 2020) applies nonlinear functions to both node and edge aggregation processes.\nHierarchical graphs refer to graphs where nodes are organised into multiple levels, and exhibit hierarchical relations. Compared to hypergraphs, hierarchical graphs are less explored in higher-order graph representation learning. To integrate node information across different levels in the hierarchy, HACT-Net (Pati et al., 2022) treats nodes in different levels as separate subgraphs. In HACT-Net, the input features of the lower-level nodes are first passed through a GNN to produce the lower-level node embeddings. Then, the computed lower-level node embeddings are concatenated with the input features of the higer-level nodes, and are passed through another GNN to produce the higher-level node embeddings. In comparison, HGN (Fang et al., 2020) treats the entire hierarchical graph as a whole, by concatenating the precomputed encodings of all levels into the node features, and passing them into a single GNN. There is also a group of simple graph methods (Ying et al., 2018; Lee et al., 2019; Gao and Ji, 2019) that propose to learn hierarchical relations in their GNN layers through graph pooling, but they do not use hierarchical graph modelling in their input graph structures.\nExisting Higher-Order Graph Datasets All above-mentioned hypergraph learning methods are evaluated on hypergraphs constructed from citation networks, such as Cora (McCallum et al., 2000), CiteSeer (Getoor, 2005), PubMed (Sen et al., 2008) and DBLP1. Hyperedges are constructed using either of the two ways: co-citation (i.e., articles are grouped in the same hyperedge if they cite the same article) and co-authorship (i.e., articles are grouped in the same hyperedge if they share the same author). However, hypergraphs constructed from citation networks suffer from two severe disadvantages, making them inadequate evaluation metrics: (1) citation networks are too small in size and are prone to overfitting; and (2) the way hyperedges are constructed leads to substantial overlaps between hyperedges, thereby limiting their effectiveness in capturing multi-node relations. Chien et al. (2022) propose several more hypergraph datasets including adaptations of Yelp2, Walmart (Amburg et al., 2020) and House (Chodrow et al., 2021), but these datasets are still relatively small-scale and have not been widely adopted by the hypergraph learning community."
        },
        {
            "heading": "3 Hybrid Graph Datasets",
            "text": ""
        },
        {
            "heading": "3.1 Definition of Hybrid Graphs",
            "text": "Formally, a simple graph G = (V, E) is a collection of nodes V and edges E \u2286 V \u00d7 V between pairs of nodes. And this graph abstraction assumes that each edge only connects two nodes. However, as discussed in the previous sections, many real-world networks have more complex node relations than just pairwise relations. Attempts made to capture such complex relations include hypergraphs (McCallum et al., 2000; Getoor, 2005; Sen et al., 2008; Chien et al., 2022), where a hyperedge can connect more than two nodes, and hierarchical graphs (Lemons et al., 2011), where nodes are organised into multiple levels. A hypergraph G = (V, E) is defined by a set of nodes V and a set of hyperedges E , where edges e \u2208 E are arbitrary subsets of V . A hierarchical graph is a graph G = (V, E) with an acyclic parent function fp : V \u2192 V , which defines the node hierarchy. Both hypergraph and hierarchical graph capture more complex node relations than a simple graph, but they still cannot fully capture the relations in real-world applications. Therefore, there is a need for a more general complex graph abstraction that combines simple graphs, hypergraphs and hierarchical graphs.\n1https://dblp.org/xml/release/ 2https://www.yelp.com/dataset\nWe define a hybrid graph as G = (V, E , fp), where V is the set of nodes, E is the set of hyperedges, and fp : V \u2192 V is an acyclic parent function. Each hyperedge e \u2208 E is a non-empty subset of V , which may contain two or more nodes, and is assigned a positive weight w(e). The parent node fp(v) of a node v \u2208 V is at the next level up in the node hierarchy. This unified hybrid graph definition encloses the traditional definitions of simple graphs, hypergraphs and hierarchical graphs, making each of them a special type of hybrid graph with some extra constraints:\n\u2022 Simple graph: a hybrid graph is a simple graph if and only if\n1. \u2200e \u2208 E . |e| = 2, i.e., a simple graph can only contain simple edges; 2. fp is a null function, i.e., a simple graph can only contain a single node level. Therefore,\nfp can also be omitted in the simple graph notation.\n\u2022 Hypergraph: a hybrid graph is a hypergraph if and only if\n1. \u2203e \u2208 E . |e| \u2265 3, i.e., a hypergraph contains at least one hyperedge; 2. fp is a null function, i.e., a hypergraph can only contain a single node level. Therefore,\nfp can also be omitted in the hypergraph notation.\n\u2022 Hierarchical graph: a hybrid graph is a hierarchical graph if and only if\n1. \u2200e \u2208 E . |e| = 2, i.e., a hierarchical graph can only contain simple edges; 2. \u2203v \u2208 V. fp(v) \u0338= v, i.e., a hierarchical graph contains at least two node levels; 3. In a hierarchical graph, except for the nodes at the highest level, every node must be\nconnected to another node in the next level up in the node hierarchy.\nFigure 1 illustrates the properties of a hybrid graph. In graph representation learning, a hybrid graph can be represented as a tuple of features (X,E,H,W,R). X \u2208 R|V|\u00d7dv is the node feature matrix of the hybrid graph, with each row xv \u2208 Rdv being the dv-dimensional features of node v. E \u2208 R|E|\u00d7de is the hyperedge feature matrix of the hybrid graph, with each row ee \u2208 Rde being the de-dimensional features of hyperedge e. H \u2208 {0, 1}|V|\u00d7|E| is the node-hyperedge incidence matrix of the hybrid graph, with each entry Hve = 1 if v \u2208 e and 0 otherwise. We can optionally separate out the simple edges from H and form an adjacency matrix A \u2208 {0, 1}|V|\u00d7|V|, where Auv = 1 if {u, v} \u2208 E . Note that simple and hierarchical graphs only contain adjacency matrices. W \u2208 R|E|\u00d7|E| is a diagonal matrix containing the weights of the hyperedges, with each on-diagonal entry Wee = w(e). R \u2208 {0, 1}|V|\u00d7|V| is the parent relation matrix of the hybrid graph, with each entry Ruv = 1 if fp(u) = v and 0 otherwise. If the nodes are arranged in ascending order of their levels in the node hierarchy, then R becomes an upper-triangular matrix. Note that W = I|E| and R = I|V| for simple graphs and hypergraphs, therefore they can be omitted in their representations."
        },
        {
            "heading": "3.2 Dataset Construction",
            "text": "The hybrid graph, as a versatile data structure, presents a more comprehensive view of graph data. It not only captures simple pairwise relationships as a simple graph does but also retains complex multi-node interactions similar to hypergraphs and hierarchical graphs. Therefore, it outperforms traditional graph structures in capturing nuanced data relationships. In light of this, we introduce the Hybrid Graph Benchmark (HGB), a novel collection of hybrid graph datasets consisting of 23 hybrid graphs derived from real-world networks across varied domains, including biology, social media, and e-commerce. Care has been taken to ensure that the datasets do not contain any personally identifiable information. Most of these datasets are one-level hybrid graphs containing both simple edges and hyperedges. Meanwhile, if we treat the hyperedges as higher-level virtual nodes representing collections of nodes, then our constructed datasets can also be interpreted as hybrid graphs with a two-level node hierarchy. Figure 2 provides an overview of these graphs and their construction mechanisms. Table 1 reports the key hybrid graph statistics for each dataset group. The HGB datasets can be splitted into three groups according to their construction processes:\nMUSAE We build eight social networks derived from the Facebook pages, GitHub developers and Twitch gamers, plus three English Wikipedia page-page networks on specific topics (chameleons, crocodiles and squirrels) based on the MUSAE (Rozemberczki et al., 2021) datasets. Nodes represent users or articles, and edges are mutual followers relationships between the users, or mutual links between the articles. In addition to the original MUSAE datasets, we construct the hyperedges to be mutually connected sub-groups that contain at least three nodes (i.e., maximal cliques with sizes of at least 3). We also enable each dataset to have an option to use either the raw node features, or the preprocessed node embeddings as introduced in MUSAE.\nGRAND We select and build ten gene regulatory networks in different tissues and diseases from GRAND (Ben Guebila et al., 2022), a public database for gene regulation. Nodes represent gene regulatory elements (Maston et al., 2006) with three distinct types: protein-encoding gene, lncRNA gene (Long et al., 2017), and other regulatory elements. Edges are regulatory effects between genes. The task is a multi-class classification of gene regulatory elements. We train a CNN (Eraslan et al., 2019) and use it to take the gene sequence as input and create a 4,651-dimensional embedding for each node. The hyperedges are constructed by grouping nearby genomic elements on the chromosomes, i.e., the genomic elements within 200k base pair distance are grouped as hyperedges.\nAmazon Following existing works on graph representation learning on e-commerce networks (Shchur et al., 2018; Zeng et al., 2020), we further build two e-commerce hypergraph datasets based on the Amazon Product Reviews dataset (McAuley et al., 2015; He and McAuley, 2016; Ni et al., 2019). Nodes represent products, and an edge between two products is established if a user buys these two products or writes reviews for both. However, unlike those existing datasets, we introduce the image modality into the construction of hyperedge. To be specific, the raw images are fed into a CLIP (Radford et al., 2021) classifier, and a 512-dimensional feature embedding for each image is returned to assist the clustering. The hyperedges are then constructed by grouping products whose image embeddings\u2019 pairwise distances are within a certain threshold."
        },
        {
            "heading": "4 Evaluation Framework",
            "text": ""
        },
        {
            "heading": "4.1 Overview",
            "text": "We create an extensible evaluation framework in HGB, simplifying the process of training and assessing GNNs for both simple graphs and hypergraphs. Figure 3 illustrates the key components of HGB. 23 hybrid graphs are used to train and evaluate the seven GNNs, including four GNNs for simple graphs: GCN (Kipf and Welling, 2017), GraphSAGE (Hamilton et al., 2017), GAT (Velic\u030ckovic\u0301 et al., 2018), GATv2 (Brody et al., 2022); two hypergraph GNNs: HyperConv, HyperAtten (Bai et al., 2021); and one sampling-based training strategy: GraphSAINT (Zeng et al., 2020). To fairly compare these models, we evaluate them under the same hyperparameter settings, which are listed in Appendix B. We repeat each experiment 5 times with different random seeds, and report their means and standard deviations in Appendix C. The experiments are evaluated with the accuracy for the node classification task and the mean square error (MSE) for node regression tasks. Our results highlight the challenges and research gaps in developing effective GNNs for real-world hybrid graphs:\n\u2022 Existing hypergraph GNNs may not outperform simple graph GNNs, even when the hyperedges provide meaningful information to the task. (Section 4.2)\n\u2022 The performance of conventional hypergraph GNNs can be improved with graph samplers that samples the node mainly based on the simple edge information. (Section 4.3)\n\u2022 Combining both simple and higher-order graph information can be substantially beneficial for node classification and regression tasks in higher-order graphs. (Section 4.4)"
        },
        {
            "heading": "4.2 Assessing Performance: Hypergraph GNNs vs. Simple Graph GNNs on HGB",
            "text": "We report the mean accuracies of four simple graph GNNs (GCN, GraphSAGE, GAT, GATv2) and two hypergraph GNNs (HyperConv, HyperAtten) on HGB in Appendix C. For MUSAE and GRAND, the information embedded in the hypergraph space and simple graph space do not contribute to the task objective. For the Amazon datasets, since the hyperedges are constructed using the embedding of actual product images, nodes within a hyperedge can provide meaningful information to the task. Therefore, we expect hypergraph GNNs to outperform simple graph GNNs on Amazon. However, our experiments show that the performance gain from hypergraph GNNs on Amazon is only marginal.\nFigure 4a shows a pairwise comparison of two types of GNNs. In both MUSAE and GRAND, hypergraph GNNs perform equally or less well than the simple graph methods, while they perform better than simple graph GNNs on two Amazon Review graphs. However, Figure 4b, the aggregated mean accuracy across all graphs with the standard deviation, indicates that the performance gain of using hypergraph GNNs may not be significant."
        },
        {
            "heading": "4.3 Optimised Sampling in Hybrid Graph",
            "text": "Various hypergraph sampling strategies are proposed for sampling subgraphs with the purpose of preserving the graph statistics (Choe et al., 2022; Dyer et al., 2021). However, there is a lack of practical implementations of hypergraph samplers and evaluations of their efficacy. Following the work by GraphSAINT (Zeng et al., 2020) on simple graph sampling, we propose HybridGraphSAINT, a class of hybrid graph samplers employing GraphSAINT\u2019s graph sampling approaches. In HybridGraphSAINT, we adopt the same sampling strategies in GraphSAINT for sampling the simple graph components in a hybrid graph, making three different types of samplers: node sampler (HybridGraphSAINT-Node), edge sampler (HybirdGraphSAINT-Edge), and random walk sampler (HybridGraphSAINT-RW). As for the hyperedges, we use an intuitive procedure that any hyperedges containing at least one node in the sampled subgraph are retained, but all nodes not in the subgraph are masked out from those hyperedges. By using this method, we preserve the original hybrid graph characteristics in the sampled subgraphs to the maximum extent. We also construct two na\u00efve random samplers as baselines for evaluation: random node sampler and random hyperedge sampler, which randomly sample a subset of nodes/hyperedges from the original hybrid graph according to a uniform sampling distribution. However, subgraphs sampled using the random node sampler can be very sparse, while subgraphs sampled using the random hyperedge sampler can be very dense.\nWe evaluate these samplers on MUSAE-GitHub and MUSAE-Facebook, the two largest graphs in HGB. Firstly, we sample subgraphs with various samplers to examine how they preserve the structure of the original graphs. This is measured by three graph statistics: average node/hyperedge degree, and clustering coefficient. We sample subgraphs multiple times and report the average graph statistics in Table 11 of Appendix C. Among all constructed samplers, HybirdGraphSAINT-Edge and HybirdGraphSAINT-RW perform the best in preserving the graph-level statistics.\nWe also evaluate HyperConv performance when paired with three different HybridGraphSAINT samplers. Figure 5 shows distinct patterns regarding how the accuracy varied with different subgraph sizes. Evaluations were carried out on two distinct datasets, yielding the following key observations (1) There exists a positive correlation between the size of sampled subgraphs and model accuracy across both datasets. (2) At subgraph sizes of 5000 or larger, both HybridGraphSAINT-RW and HybridGraphSAINT-Node outperform vanilla HyperConv (trained on the whole graph without sampling). This highlights the potential for improved performance of hypergraph GNNs when integrating our HybridGraphSAINT samplers. This performance gain could be explained by the reduction of over-smoothing brought by sampling subgraphs.\nOverall, the HybridGraphSAINT samplers, which mostly use simple edge information, can both effectively preserve hypergraph statistics and improve the performance of the hypergraph learning algorithms. This again underpins the advantage of hybrid graphs over conventional hypergraphs and simple graphs, emphasising the need to preserve multi-level information."
        },
        {
            "heading": "4.4 Integrating Simple Graph and Hypergraph Information",
            "text": "To test whether combining simple and higher-order graph information could improve the performance of GNNs, we propose a simple algorithm called Linear Probe Graph Neural Networks (LP-GNNs). LP-GNN consists of two GNNs fGNN1 , fGNN2 , plus a linear layer f(x) = \u03b8x+b. For a given hybrid graph G = (X,E,H,W,R), where x denotes the input features of a node, LP-GNN is defined as\nLP-GNN(x) = \u03b8 \u00b7 CONCATENATE(fGNN1(x), fGNN2(x)) + b (1) For node regression tasks, LP-GNN(x) is directly used as the final output. For node classification tasks, a log softmax function is applied to LP-GNN(x) to produce the final output.\nTable 2 shows the performance of three LP-GNNs with the GNN pair being GCN+GAT, GCN+HyperConv, and GAT+HyperConv. These models are benchmarked across five different HGB datasets, and their performances are contrasted with those of simple graph GNNs. Our focus lies on the performance of LP-GCN+HyperConv and LP-GAT+HyperConv, which combine simple edge and hyperedge information. In three of the datasets (Wiki-Chameleon, Breast, and Leukemia), LP-GAT+HyperConv and LP-GCN+HyperConv demonstrate the highest efficacy. In GitHub and\nComputers, those two models also tend to match other GNNs\u2019 performances. LP-GNNs employ a very simple strategy to combine information from two levels and show its advantage. This evidents the potential advantages of integrating information across multiple levels."
        },
        {
            "heading": "5 Conclusion and Discussion",
            "text": "We introduce the concept of hybrid graphs, a unified representation of various complex graph structures. Additionally, we present the Hybrid Graph Benchmark (HGB), a collection of real-world hybrid graph datasets accompanied with an extensible GNN evaluation framework. Through extensive experiments, we demonstrate that existing hypergraph GNNs are not guaranteed to outperform simple graph GNNs on large-scale complex networks. In light of this, we propose a simple model called Linear Probe Graph Neural Networks (LP-GNNs), which integrates simple graph and hypergraph information, and leverages the node prediction performance on the hybrid graphs. We believe that HGB can significantly aid the research in complex graph representation learning, and provide valuable insights to future research directions for the graph representation learning community.\nLimitations We define hybrid graphs as graphs with multiple levels. However, the current collection of datasets in HGB primarily consists of shallow hybrid graphs with a two-level node hierarchy, and networks with deeper node hierarchies are desired. In MUSAE, hyperedges are constructed using the maximal cliques of a graph, which might lead to an overlap of information between the simple edges and hyperedges. In GRAND, the majority of gene regulatory graphs exhibit bipartite-like characteristics, which may be limited in connectivity and clustering for hyperedge constructions. In Amazon, hyperedges are created based on a pre-defined threshold. Our preliminary experiments show that the selection of this threshold value can influence the structure of the resultant hybrid graphs, and a more thorough investigation is required to find the range of the most appropriate threshold values.\nFuture Work HGB will be updated on a regular basis, and we welcome inputs from the community. In the future versions of HGB, we plan to to enable multiple versions of Amazon datasets by adjusting various pairwise distance thresholds, and include more real-world large-scale networks featuring multilevel node hierarchies. Additionally, as simply concatenating simple and hypergraph information followed by a linear transformation can already enhance the node prediction performance on hybrid graphs, we will also seek to further improve the performance following this idea, by incorporating more fine-grained operations to combine simple graph and higher-order graph information.\nAcknowledgments and Disclosure of Funding\nThis work was performed using the Sulis Tier 2 HPC platform hosted by the Scientific Computing Research Technology Platform at the University of Warwick, and the JADE Tier 2 HPC facility. Sulis is funded by EPSRC Grant EP/T022108/1 and the HPC Midlands+ consortium. JADE is funded by EPSRC Grant EP/T022205/1. Zehui Li acknowledges the funding from the UKRI 21EBTA: EB-AI Consortium for Bioengineered Cells & Systems (AI-4-EB) award, Grant BB/W013770/1. Xiangyu Zhao acknowledges the funding from the Imperial College London Electrical and Electronic Engineering PhD Scholarship. Mingzhu Shen acknowledges the funding from the Imperial College London President\u2019s PhD Scholarship. For the purpose of open access, the authors have applied a Creative Commons Attribution (CC BY) licence to any Author Accepted Manuscript version arising."
        },
        {
            "heading": "A Full Statistics of HGB Datasets",
            "text": ""
        },
        {
            "heading": "B Experimental Details",
            "text": "B.1 Training Details\nWe run all the experiments on NVIDIA A100 PCIe GPU with 40GB RAM (Sulis) and NVIDIA V100 NVLink GPU with 32GB RAM (JADE), with each experiment taking less than 2 minutes. Adam (Kingma and Ba, 2015) is used as the optimiser, and CosineAnnealingLR (Gotmare et al., 2019) is used as the learning rate scheduler for all training. All models are trained for 50 epochs. For each experiment, the nodes of the used hybrid graph are split into the train, validation, and test sets with a split ratio of 6:2:2. For node classification tasks, BCEWithLogitsLoss is used as the loss function, which is defined as:\nLBCEWithLogits(y, y\u0302) = \u2212 1\nn n\u2211 i=1 [ yi \u00b7 log(\u03c3(y\u0302i)) + (1\u2212 yi) \u00b7 log(1\u2212 \u03c3(y\u0302i)) ] (2)\nwhere n is the total number of elements in y and y\u0302, yi is the i-th element of y, the batch of true values, and y\u0302i is the i-th element of y\u0302, the batch of raw (i.e., non-sigmoid-transformed) predicted values. \u03c3 denotes the sigmoid function, which transforms the raw predictions into the range (0, 1). For node regression tasks, MSELoss is used as the loss function, which is defined as:\nLMSE(y, y\u0302) = 1\nn n\u2211 i=1 (yi \u2212 y\u0302i)2 (3)\nwhere n is the total number of elements in y and y\u0302, yi is the i-th element of y, the batch of true values, and y\u0302i is the i-th element of y\u0302, the batch of predicted values.\nB.2 Hyperparameter Settings\nWe perform a hyperparameter search for the learning rate and keep the hidden layer dimension the same for different models, the hyperparameters used for training each architecture are listed in Table 4. All seven GNNs (GCN, GraphSAGE, GAT, GATv2, HyperConv, HyperAtten, and GraphSAINT) share the same learning rate, hidden dimension, and dropout rate. HyperAtten has an additional hyperparameter, which is the hyperedge aggregation function. This function determines how the hyperedge is constructed from the nodes within it. The possible options for this function are \u2018sum\u2019 and \u2018concatenate\u2019. In this work, we have selected \u2018sum\u2019 as the hyperedge aggregation function. For GraphSAINT, the additional hyperparameters are subgraph size, measured by the number of nodes in the subgraph, and the batch size, which is the number of subgraphs to sample in each epoch. Different subgraph sizes are applied according to the sizes of the original hybrid graphs."
        },
        {
            "heading": "C Results",
            "text": "We evaluate the performance of seven GNNs on all 23 HGB datasets. Each experiment is repeated five times with different random seeds, and the results are summarised in Tables 5 to 10. In the node classification tasks of MUSAE, GCN and GraphSAGE perform the best in the on GitHub and Facebook, as shown in Table 5, while all GNNs perform roughly the same on Twitch, as shown in Table 6. In the three node regression tasks on MUSAE-Wiki, GraphSAINT stands out among other GNNs, as shown in Table 10. Table 7 shows that HyperConv and HyperAtten outperform other simple graph GNNs on five of the six hybrid graphs in GRAND-Tissues. For hybrid graphs in GRANDDiseases as shown in Table 8, GraphSAGE exhibits a superior performance. For the two Amazon hybrid graph datasets, as shown in Table 9, GraphSAINT consistently achieves the best performance. Overall, hypergraph GNNs tend to outperform simple graph GNNs on GRAND-Tissues and Amazon, perform equally as simple graph GNNs on MUSAE-Twitch, MUSAE-Wiki and GRAND-Diseases, and underperform simple graph GNNs on MUSAE-GitHub and MUSAE-Facebook.\nTable 11 show the statistics of the subgraphs obtained by five different samplers. We sample subgraphs from both MUSAE-GitHub and MUSAE-Facebook, and then compute the average statistics of the sampled subgraphs. HybridGraphSAINT-Edge and HybridGraphSAINT-RW perform the best in preserving the graph-size agnostic statistics. The two random samplers tend to sample subgraphs with distinct structures from the original hybrid graphs."
        },
        {
            "heading": "D Data Accessibility",
            "text": "The source code and full datasets of HGB is publicly available at https://zehui127.github. io/hybrid-graph-benchmark/. While the raw dataset in JSON format is hosted at https: //zenodo.org/record/7982540, we recommend the users to access the datasets through our Python library hybrid-graph, which is installable via pip. This would allow the users to read the hybrid graphs in the format of PyTorch Geometric Data objects."
        },
        {
            "heading": "E Licence",
            "text": "The raw data for the MUSAE datasets are licenced under the the GNU General Public Licence, version 3 (GPLv3)3. The raw data for the GRAND datasets are licenced under the Creative Commons Attribution-ShareAlike 4.0 International Public Licence (CC BY-SA 4.0)4. The raw data for the Amazon datasets are licenced under the Amazon Service licence5. Having carefully observed the licence requirements of all data sources and code dependencies, we apply the following licence to our source code and datasets:\n\u2022 The source code of HGB is licenced under the MIT licence6; \u2022 The MUSAE and GRAND datasets are licenced under the GPLv3 licence3; \u2022 The Amazon datasets are licenced under the Amazon Service licence5."
        },
        {
            "heading": "F Ethics Statement",
            "text": "All datasets constructed in HGB are generated from public open-source datasets, and the original raw data downloaded from the data sources do not contain any personally identifiable information or other sensitive contents. The node prediction tasks for the HGB datasets are designed to ensure that they do not, by any means, lead to discriminations against any social groups. Therefore, we are not aware of any social or ethical concern of HGB. Since HGB is a general benchmarking tool for representation learning on complex graphs, we also do not forsee any direct application of HGB to malicious purposes. However, the users of HGB should be aware of any potential negative social and ethical impacts that may arise from their chosen downstream datasets or tasks outside of HGB, if they intend to use the HGB datasets as pre-training datasets to perform trasnfer learning.\n3https://www.gnu.org/licenses/gpl-3.0.html 4https://creativecommons.org/licenses/by-sa/4.0/ 5https://s3.amazonaws.com/amazon-reviews-pds/LICENSE.txt 6https://opensource.org/license/mit/"
        }
    ],
    "title": "Hybrid Graph: A Unified Graph Representation with Datasets and Benchmarks for Complex Graphs",
    "year": 2023
}