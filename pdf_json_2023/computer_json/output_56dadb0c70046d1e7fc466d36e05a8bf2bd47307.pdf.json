{
    "abstractText": "Recently developed pretrained models can encode rich world knowledge expressed in multiple modalities, such as text and images. However, the outputs of these models cannot be integrated into algorithms to solve sequential decision-making tasks. We develop an algorithm that utilizes the knowledge from pretrained models to construct and verify controllers for sequential decisionmaking tasks, and to ground these controllers to task environments through visual observations. In particular, the algorithm queries a pretrained model with a user-provided, text-based task description and uses the model\u2019s output to construct an automaton-based controller that encodes the model\u2019s task-relevant knowledge. It then verifies whether the knowledge encoded in the controller is consistent with other independently available knowledge, which may include abstract information on the environment or userprovided specifications. If this verification step discovers any inconsistency, the algorithm automatically refines the controller to resolve the inconsistency. Next, the algorithm leverages the vision and language capabilities of pretrained models to ground the controller to the task environment. It collects image-based observations from the task environment and uses the pretrained model to link these observations to the text-based control logic encoded in the controller (e.g., actions and conditions that trigger the actions). We propose a mechanism to ensure the controller satisfies the user-provided specification even when perceptual uncertainties are present. We demonstrate the algorithm\u2019s ability to construct, verify, and ground automaton-based controllers through a suite of real-world tasks, including daily life and robot manipulation tasks.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yunhao Yanga"
        },
        {
            "affiliations": [],
            "name": "Cyrus Nearya"
        },
        {
            "affiliations": [],
            "name": "Ufuk Topcua"
        }
    ],
    "id": "SP:6668a371b25aa91626353d9f531739a875e6aa64",
    "references": [
        {
            "authors": [
                "Y. Yang",
                "J.-R. Gaglione",
                "C. Neary",
                "U. Topcu"
            ],
            "title": "Automaton-Based Representations of Task Knowledge from Generative Language Models, arXiv preprint arXiv:2212.01944",
            "year": 1944
        },
        {
            "authors": [
                "P. West",
                "C. Bhagavatula",
                "J. Hessel",
                "J.D. Hwang",
                "L. Jiang",
                "R.L. Bras",
                "X. Lu",
                "S. Welleck",
                "Y. Choi"
            ],
            "title": "Symbolic Knowledge Distillation: from General Language Models to Commonsense Models",
            "venue": "in: Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
            "year": 2022
        },
        {
            "authors": [
                "N. Rezaei",
                "M.Z. Reformat"
            ],
            "title": "Utilizing Language Models to Expand Vision-Based Commonsense Knowledge Graphs",
            "venue": "Symmetry 14 ",
            "year": 2022
        },
        {
            "authors": [
                "Y. Lu",
                "W. Feng",
                "W. Zhu",
                "W. Xu",
                "X.E. Wang",
                "M. Eckstein",
                "W.Y. Wang"
            ],
            "title": "Neuro-Symbolic Procedural Planning with Commonsense Prompting",
            "venue": "in: International Conference on Learning Representations",
            "year": 2023
        },
        {
            "authors": [
                "W. Huang",
                "P. Abbeel",
                "D. Pathak",
                "I. Mordatch"
            ],
            "title": "Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents",
            "venue": "in: International Conference on Machine Learning, Vol. 162 of Proceedings of Machine Learning Research",
            "year": 2022
        },
        {
            "authors": [
                "S. Vemprala",
                "R. Bonatti",
                "A. Bucker",
                "A. Kapoor"
            ],
            "title": "ChatGPT for Robotics: Design Principles and Model Abilities",
            "venue": "Microsoft Autonomous Systems and Robotics Research 2 ",
            "year": 2023
        },
        {
            "authors": [
                "B. Ichter",
                "A. Brohan",
                "Y. Chebotar",
                "C. Finn",
                "K. Hausman",
                "A. Herzog",
                "D. Ho",
                "J. Ibarz",
                "A. Irpan",
                "E. Jang",
                "R. Julian",
                "D. Kalashnikov",
                "S. Levine",
                "Y. Lu",
                "C. Parada",
                "K. Rao",
                "P. Sermanet",
                "A. Toshev",
                "V. Vanhoucke",
                "F. Xia",
                "T. Xiao",
                "P. Xu",
                "M. Yan",
                "N. Brown",
                "M. Ahn",
                "O. Cortes",
                "N. Sievers",
                "C. Tan",
                "S. Xu",
                "D. Reyes",
                "J. Rettinghouse",
                "J. Quiambao",
                "P. Pastor",
                "L. Luu",
                "K. Lee",
                "Y. Kuang",
                "S. Jesmonth",
                "N.J. Joshi",
                "K. Jeffrey",
                "R.J. Ruano",
                "J. Hsu",
                "K. Gopalakrishnan",
                "B. David",
                "A. Zeng",
                "C.K. Fu"
            ],
            "title": "Do As I Can",
            "venue": "Not As I Say: Grounding Language in Robotic Affordances, in: Conference on Robot Learning, Vol. 205 of Proceedings of Machine Learning Research",
            "year": 2022
        },
        {
            "authors": [
                "D. Shah",
                "B. Osinski",
                "B. Ichter",
                "S. Levine"
            ],
            "title": "LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language",
            "venue": "Vision, and Action, in: Conference on Robot Learning, Vol. 205 of Proceedings of Machine Learning Research",
            "year": 2022
        },
        {
            "authors": [
                "W. Huang",
                "F. Xia",
                "T. Xiao",
                "H. Chan",
                "J. Liang",
                "P. Florence",
                "A. Zeng",
                "J. Tompson",
                "I. Mordatch",
                "Y. Chebotar",
                "P. Sermanet",
                "T. Jackson",
                "N. Brown",
                "L. Luu",
                "S. Levine",
                "K. Hausman",
                "B. Ichter"
            ],
            "title": "Inner Monologue: Embodied Reasoning through Planning with Language Models",
            "venue": "in: Conference on Robot Learning, Vol. 205 of Proceedings of Machine Learning Research",
            "year": 2022
        },
        {
            "authors": [
                "A. Radford",
                "J.W. Kim",
                "C. Hallacy",
                "A. Ramesh",
                "G. Goh",
                "S. Agarwal",
                "G. Sastry",
                "A. Askell",
                "P. Mishkin",
                "J. Clark",
                "G. Krueger",
                "I. Sutskever"
            ],
            "title": "Learning Transferable Visual Models From Natural Language Supervision",
            "venue": "in: M. Meila, T. Zhang (Eds.), International Conference on Machine Learning, Vol. 139 of Proceedings of Machine Learning Research",
            "year": 2021
        },
        {
            "authors": [
                "J. Redmon",
                "S.K. Divvala",
                "R.B. Girshick",
                "A. Farhadi"
            ],
            "title": "You Only Look Once: Unified",
            "venue": "Real-Time Object Detection, in: Conference on Computer Vision and Pattern Recognition, IEEE Computer Society",
            "year": 2016
        },
        {
            "authors": [
                "S. Ren",
                "K. He",
                "R.B. Girshick",
                "J. Sun"
            ],
            "title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
            "venue": "IEEE Transaction Pattern Analysis Machine Intelligence 39 (6) ",
            "year": 2017
        },
        {
            "authors": [
                "A. Kirillov",
                "E. Mintun",
                "N. Ravi",
                "H. Mao",
                "C. Rolland",
                "L. Gustafson",
                "T. Xiao",
                "S. Whitehead",
                "A.C. Berg",
                "W.-Y. Lo",
                "P. Doll\u00e1r",
                "R. Girshick"
            ],
            "title": "Segment Anything",
            "venue": "arXiv preprint arXiv:2304.02643. 23 Y. Yang et al. / Manuscript 00 ",
            "year": 2023
        },
        {
            "authors": [
                "X. Gu",
                "T. Lin",
                "W. Kuo",
                "Y. Cui"
            ],
            "title": "Open-vocabulary Object Detection via Vision and Language Knowledge Distillation",
            "venue": "in: International Conference on Learning Representations, OpenReview.net",
            "year": 2022
        },
        {
            "authors": [
                "L.H. Li",
                "P. Zhang",
                "H. Zhang",
                "J. Yang",
                "C. Li",
                "Y. Zhong",
                "L. Wang",
                "L. Yuan",
                "L. Zhang",
                "J. Hwang",
                "K. Chang",
                "J. Gao"
            ],
            "title": "Grounded Language-Image Pre-training",
            "venue": "in: Conference on Computer Vision and Pattern Recognition, IEEE",
            "year": 2022
        },
        {
            "authors": [
                "J. Devlin",
                "M. Chang",
                "K. Lee",
                "K. Toutanova"
            ],
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "venue": "in: J. Burstein, C. Doran, T. Solorio (Eds.), Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics",
            "year": 2019
        },
        {
            "authors": [
                "T.B. Brown",
                "B. Mann",
                "N. Ryder",
                "M. Subbiah",
                "J. Kaplan",
                "P. Dhariwal",
                "A. Neelakantan",
                "P. Shyam",
                "G. Sastry",
                "A. Askell",
                "S. Agarwal",
                "A. Herbert- Voss",
                "G. Krueger",
                "T. Henighan",
                "R. Child",
                "A. Ramesh",
                "D.M. Ziegler",
                "J. Wu",
                "C. Winter",
                "C. Hesse",
                "M. Chen",
                "E. Sigler",
                "M. Litwin",
                "S. Gray",
                "B. Chess",
                "J. Clark",
                "C. Berner",
                "S. McCandlish",
                "A. Radford",
                "I. Sutskever",
                "D. Amodei"
            ],
            "title": "Language Models are Few-shot Learners",
            "venue": "Advances in Neural Information Processing Systems 33 ",
            "year": 2020
        },
        {
            "authors": [
                "O. Biggar",
                "M. Zamani"
            ],
            "title": "A Framework for Formal Verification of Behavior Trees with Linear Temporal Logic",
            "venue": "IEEE Robotics and Automation Letters 5 (2) ",
            "year": 2020
        },
        {
            "authors": [
                "A. Cimatti",
                "E.M. Clarke",
                "E. Giunchiglia",
                "F. Giunchiglia",
                "M. Pistore",
                "M. Roveri",
                "R. Sebastiani",
                "A. Tacchella"
            ],
            "title": "NuSMV 2: An OpenSource Tool for Symbolic Model Checking",
            "venue": "in: Computer Aided Verification, Vol. 2404 of Lecture Notes in Computer Science",
            "year": 2002
        },
        {
            "authors": [
                "M. Chang",
                "J. Lambert",
                "P. Sangkloy",
                "J. Singh",
                "S. Bak",
                "A. Hartnett",
                "D. Wang",
                "P. Carr",
                "S. Lucey",
                "D. Ramanan",
                "J. Hays"
            ],
            "title": "Argoverse: 3D Tracking and Forecasting With Rich Maps",
            "venue": "in: IEEE Conference on Computer Vision and Pattern Recognition, Computer Vision Foundation / IEEE",
            "year": 2019
        },
        {
            "authors": [
                "H. Ali"
            ],
            "title": "How to Cross the Road Safely (Mar 2023)",
            "venue": "URL https://www.hseblog.com/cross-road-safely/",
            "year": 2023
        }
    ],
    "sections": [
        {
            "text": "Recently developed pretrained models can encode rich world knowledge expressed in multiple modalities, such as text and images. However, the outputs of these models cannot be integrated into algorithms to solve sequential decision-making tasks. We develop an algorithm that utilizes the knowledge from pretrained models to construct and verify controllers for sequential decisionmaking tasks, and to ground these controllers to task environments through visual observations. In particular, the algorithm queries a pretrained model with a user-provided, text-based task description and uses the model\u2019s output to construct an automaton-based controller that encodes the model\u2019s task-relevant knowledge. It then verifies whether the knowledge encoded in the controller is consistent with other independently available knowledge, which may include abstract information on the environment or userprovided specifications. If this verification step discovers any inconsistency, the algorithm automatically refines the controller to resolve the inconsistency. Next, the algorithm leverages the vision and language capabilities of pretrained models to ground the controller to the task environment. It collects image-based observations from the task environment and uses the pretrained model to link these observations to the text-based control logic encoded in the controller (e.g., actions and conditions that trigger the actions). We propose a mechanism to ensure the controller satisfies the user-provided specification even when perceptual uncertainties are present. We demonstrate the algorithm\u2019s ability to construct, verify, and ground automaton-based controllers through a suite of real-world tasks, including daily life and robot manipulation tasks.\nKeywords: Multimodal pre-trained model, sequential decision-making, automaton-based representation, formal verification"
        },
        {
            "heading": "1. Introduction",
            "text": "While the rapidly emerging capabilities of multimodal pretrained models (also referred to as foundation models or base models) in question answering, code synthesis, and image generation offer new opportunities for autonomous systems, a gap exists between the text-based and image-based outputs of these models and algorithms for solving sequential decision-making tasks. Additional methods are required to integrate the outputs of these pretrained models into autonomous systems that can perceive and react to an environment in order to fulfill a task. Additionally, it is hard, if not impossible, to formally verify whether autonomous systems implementing such pretrained models satisfy user-provided specifications.\nTowards filling the gap between multimodal pretrained models and sequential decision-making, we develop a pipeline that integrates the outputs of pretrained models into downstream design steps, e.g., control policy synthesis or reinforcement learning, and provides a systematic way to ground the knowledge from such models. Specifically, we develop an algorithm to construct automaton-based controllers representing the knowledge from the pretrained\nEmail addresses: yunhaoyang234@utexas.edu (Yunhao Yang), cneary@utexas.edu (Cyrus Neary), utopcu@utexas.edu (Ufuk Topcu)\nar X\niv :2\n30 8.\n05 29\n5v 1\n[ cs\n.A I]\n1 0\nA ug\nmodels. Such representations can be formally verified against knowledge from other independently available sources, such as abstract information on the environment or user-provided specifications. This verification step ensures consistency between the knowledge encoded in the pretrained model and the knowledge from other independent sources. To implement the controllers in their task environments, we leverage the multimodal capabilities of the pretrained models, i.e., simultaneous vision and language understanding, to ground these controllers through visual perception.\nFigure 1 illustrates the proposed pipeline for constructing, verifying, and grounding the automaton-based controllers. The proposed method Automata2Env links image-based observations from the task environment to the controller\u2019s textbased propositions representing the environment\u2019s conditions. Specifically, Automata2Env first collects visual observations and uses the vision and language capabilities of the employed pretrained models to evaluate the truth values of conditions from the controller, given the observations. The controller then uses these truth values to select its next action. Furthermore, we propose a mechanism that allows Automata2Env to account for perceptual uncertainties, i.e., potential misclassifications raised by the pretrained model. The mechanism guarantees the controller will not take actions under uncertainties. It thus ensures the autonomous agent\u2019s safety with respect to mission specifications, even when such perceptual uncertainties exist. To formally verify whether these mission specifications are satisfied, we use finite state automata (FSAs) to represent the controllers.\nTo construct these FSA-based controllers, we develop an algorithm named LLM2Automata that constructs a controller encoding the task knowledge obtained from the pretrained model. LLM2Automata builds upon the authors\u2019 recently presented algorithm, GLM2FSA [1]: It similarly queries the pretrained model to obtain text-based task knowledge, parses the text to extract actions, and defines a set of rules (grammar) to transform these actions into an FSA. In contrast to GLM2FSA, LLM2Automata explicitly queries the pretrained model for the environment conditions before and after each action is taken and encodes them into the constructed controller. This dis-\ntinction of LLM2Automata is proposed to facilitate the grounding method Automata2Env, which connects these\nconditions to image-based observations of the task environment. We leverage the automaton-based representations to develop a procedure to verify whether the knowledge encoded in the controller is consistent with knowledge available from other independently available sources. If the verification step fails, we present an algorithm to automatically refine the controller to resolve any potential inconsistencies. The independently available knowledge may be explicitly given by the user in an automaton-based form that is compatible with the verification step, or it may be provided in textual form (e.g., user manuals, online information, or natural-language descriptions). In the latter case, we develop an algorithm Text2Model that automatically builds such automaton-based representations encoding the text-based independently available knowledge.\nWe demonstrate the algorithms\u2019 capabilities on sequential decision-making tasks through a variety of case studies. We provide proof-of-concept examples of commonsense tasks (e.g., cross the road) and real-robot tasks (e.g., robot arm manipulation). Figure 2 illustrates the major components of the proposed pipeline when it is applied to a robot arm manipulation task. These examples show the algorithms\u2019 ability to construct verifiable knowledge representations and to ground these representations in real-world environments through visual observations with perceptual uncertainties."
        },
        {
            "heading": "2. Related Work",
            "text": ""
        },
        {
            "heading": "2.1. Formal Representations of Textual Knowledge",
            "text": "Many works have developed methods to construct symbolic representations of task knowledge from natural language descriptions. Several works construct knowledge graphs from textual descriptions of given tasks [2, 3, 4], or analyze causalities between the textual step descriptions and build causal graphs [5]. However, the graphs resulting from these works are not directly useful in algorithms for sequential decision-making, nor are they formally verifiable. Another work builds automaton-based representations of task-relevant knowledge from text-based descriptions of tasks [1]. These representations are both formally verifiable and directly applicable to algorithms for sequential decision-making. However, in contrast with [1], we not only generate automaton-based representations but also ground the generated representations to the task environment through image-based perceptions."
        },
        {
            "heading": "2.2. Multimodal Models in Sequential Decision-Making",
            "text": "A work [6] generates static high-level plans and matches them to the closest admissible action. Some other works [7, 8, 9, 10] generate zero-shot plans for sequential decision-making tasks from querying generative language models. These works require a set of pre-defined actions, which limit their generalization capability. Another work [11] uses large language models to generate executable code or API for robots. The existing works lack a discussion on grounding the generated plans to real-world or simulated task environments while perceiving the environment. In contrast, we do not need to define admissible actions prior to querying the generative model, and we can ground the abstract representations constructed from the generative model to the task environments."
        },
        {
            "heading": "2.3. Multimodal Models Grounding and Perceptions",
            "text": "Several works [12, 13, 14, 15] match textual plans to image observations and perform actions based on the perceptual outputs. A work [16] recursively generates plans based on visual observations. Another work [17] matches texts to images by generating visual-grounded textual plans from generative models and images. However, none of these works consider perceptual uncertainties. They assume the vision models can correctly classify the content within the image and make plans or actions accordingly. In contrast, we consider the uncertainties in the image observations from the environments and can verify the automaton-based representations over the task environment with uncertainties.\nThere are multimodal pretrained models with vision and image capabilities that can interpret the content within the images and connect images to natural language. CLIP [18] measures the text-image consistency. Many other models [19, 20] can detect objects described in text from a given image. However, they have a fixed set of vocabularies to define objects. Open-vocabulary object detection models [21, 22, 23, 24] remove the constraints on vocabularies, which we will use for connecting the automaton-based representations to the task environment."
        },
        {
            "heading": "3. Preliminaries",
            "text": ""
        },
        {
            "heading": "3.1. Multimodal Pretrained Models",
            "text": "Multimodal pretrained models (also referred to as foundation models [25] or base models [18]) are capable of processing, understanding, and generating data across multiple formats, such as images, text, and audio. These models are pretrained on large training datasets, and they have demonstrated strong empirical performance across a variety of tasks, such as question-answering and next-word prediction, even without further task-specific fine-tuning [26].\nThe Generative Pretrained Transformer (GPT) series of models [27, 26] consists of the most well-known multimodal pretrained models that can generate natural language or other data formats. In addition to GPT, pretrained models such as PaLM [28], BLOOM [29], Codex [30], and Megatron [31] also have the capability of generating outputs in natural language or other formats. Language generation is the core capability of these models, which we will use in the rest of the paper. Hence we denote this category of multimodal pretrained models as Large-scale generative language models (GLMs).\nVision-language models such as CLIP [18], Yolo [19], and the Segment Anything Model [22] are another type of multimodal pretrained model. CLIP takes an image and a set of texts as inputs, and measures the image-text consistency. Yolo, R-CNN [20] and Segment Anything Model are object detection models, which take an image and a set of words that describe objects, and classify whether the objects appear in the image. These models are capable of processing and understanding texts and images but are not capable of content generation."
        },
        {
            "heading": "3.2. Finite State Automata",
            "text": "A finite state automaton (FSA) is a tuple A = \u27e8\u03a3,\u0393,Q, q0, \u03b4, \u03c9\u27e9 where \u03a3 is the input alphabet (the set of input symbols), \u0393 is the output alphabet (the set of output symbols), q0 \u2208 Q is the initial state, \u03b4 : Q \u00d7 \u03a3 \u00d7 Q\u2192 {0, 1} is the transition function, and \u03c9 : Q \u00d7 \u03a3 \u00d7 Q\u2192 \u0393 is the output function.\nWe use P to denote the set of atomic propositions, which we use to define the input alphabet, \u03a3 B 2P. In words, any given input symbol \u03c3 \u2208 \u03a3 consists of a set of atomic propositions from P that currently evaluate to True. A propositional logic formula is based on one or more atomic propositions in P. A transition from qi to q j exists if \u03b4(qi, \u03c6, q j) = 1, the current state is qi, and the propositional logic formula \u03c6 is true. Note that we define the FSA transitions to possibly be non-deterministic\u2014mulitple transitions are possible under the same input symbol, from a given FSA state. Figure 3 depicts an example FSA."
        },
        {
            "heading": "3.3. Controllers and Models",
            "text": "In this work, we refer to the automaton-based representation of task knowledge as a controller: a system component responsible for making decisions and taking actions based on the system\u2019s state. A controller is represented as mapping the system\u2019s current state to an action, which can be interpreted as a control input or a setpoint. Mathematically, we use an FSA \u27e8\u03a3,\u0393,Q, q0, \u03b4, \u03c9\u27e9 to represent the controller, whose input alphabet \u03a3 indicates all possible observations of the environment and output alphabet \u0393 indicates all possible actions. We additionally allow for a \u201cno operation\u201d action \u03f5 \u2208 \u0393.\nThe controller\u2019s goal is to adjust the control input so that the system\u2019s state evolves in a way that satisfies externally provided requirements or properties. These requirements or properties are often specified using formal languages, such as linear temporal logic (LTL) [32].\nA model is a transition system that may represent either the dynamics of the task environment, or knowledge from other independent sources. We formally define a model as M B \u27e8\u03a3M,\u0393M,QM, \u03b4M, \u03c9M\u27e9, which consists of input alphabet \u03a3M B 2PM is a set of input symbols, where PM is defined as the actions. QM is a finite set of states,\n\u03b4M : QM \u00d7 \u03a3M \u00d7 QM \u2192 {0, 1} is a non-deterministic transition function, and \u03c9M : QM \u2192 \u0393M is a labeling function, where \u0393M = 2P and P is a set of atomic propositions representing conditions of the environment."
        },
        {
            "heading": "3.4. Product Automata",
            "text": "LetM B \u27e8QM,\u03a3M,\u0393M, \u03b4M, \u03c9M\u27e9 be a model and let C B \u27e8Q,\u03a3,\u0393, q0, \u03b4, \u03c9\u27e9 be a controller, we define the product automaton as an FSA P =M\u2297C B \u27e8QP, \u03b4P, qPinit, \u03c9P\u27e9 as follows:\nQP B QM \u00d7 Q \u03b4P((p, q)) B { (p\u2032, q\u2032) \u2208 QP \u2223\u2223\u2223\u03b4(q, c, q\u2032) = 1 \u2227 \u03b4M(p, a, p\u2032) = 1} where a = \u03c9(q, \u03c3, q\u2032) and c = \u03c9M(p)\nqPinit B (p, q0) where p can be any state inM \u03c9P((p, q)) B \u03c9M(p) \u222a \u03c9(q, \u03c9M(p), q\u2032) where q\u2032 \u2208 Q and \u03b4(q, \u03c9M(p), q\u2032) = 1.\nThe trajectories from P are in the form (2PM\u222aP)\u2217, i.e. \u03c80, \u03c81, \u03c82 . . . where \u03c8i = \u03c9P(qi, pi)."
        },
        {
            "heading": "3.5. The Planning Domain Definition Language",
            "text": "A Planning Domain Definition Language (PDDL) [33] is a formal language used in artificial intelligence and automated planning to define a planning problem. We use PDDL to describe the possible initial states of a problem, the desired goal, and the actions that can be taken to transform the initial state into the goal state. PDDL provides a standardized syntax for specifying a set of predicates\u2014atomic propositions\u2014describing the states of the task, the actions, and the goal specification.\nEach action a in PDDL has a name, a precondition that must be satisfied before the action can be performed, and an of effect that describes how the state of the environment will change after the action is performed. The preconditions and effects are expressed as sets of atomic propositions."
        },
        {
            "heading": "4. Task Controller Construction and Refinement",
            "text": "We develop an algorithm, LLM2Automata, takes a brief task description in textual form from the task designer and returns an FSA representing the task controller that satisfies the specifications given by the task designer.\nWe then design a verification-refinement procedure for the controller constructed through LLM2Automata. We first obtain a model representing the dynamics of the task environment, or other side information from external knowledge sources other than the GLM. This model could be created by a human designer, or it can be automatically constructed from the textual information contained in external knowledge sources such as websites, operation manuals, blog posts, or books. We present the algorithm for such automatic construction in Section 4.2 below. Next, we verify whether the controller, when implemented against the model, satisfies the user-defined specifications. If the verification step fails, we use the verification outcomes, e.g., counter-examples, to refine the controller until the controller passes the verification step."
        },
        {
            "heading": "4.1. Controller Construction",
            "text": "The algorithm LLM2Automata takes a brief text description of a task and constructs an FSA to represent the controller of the given task. Specifically, the algorithm sends the text description as the input prompt (in blue) to a GLM and obtains the GLM\u2019s response (in red), which is a list of steps for achieving the task in textual form:\n1 Steps for task description 2 step_number_1. step description 3 step_number_2. step description 4 ...\nThe algorithm uses the semantic parsing method introduced in GLM2FSA [1] to parse each step description into verb phrases (VP) and connective keywords. A list of pre-defined keywords is provided in Table 1. A verb phrase consists of a verb and its noun dependencies. Each step corresponds to a state in the FSA. Meanwhile, each verb phrase VP in the step description represents an action, and the algorithm queries the GLM to extract the precondition and effect of this action in the form of PDDL:\n1 Define an action \"action name\" in PDDL 2 Action: action name 3 Precondition: a set of propositions 4 Effect: a set of propositions\nWe use the extracted verb phrase VPi to define the action name, and we use VPCi and VP E i to denote the precondition and effect of the action, respectively. Then, the algorithm follows the rules illustrated in Table 1 to transform natural language into propositions or automaton transitions. Each step description is translated into a state in the FSA and a set of outgoing transitions from this state.\nWe note that in contrast to the GLM2FSA algorithm presented in [1], we query the GLM for the preconditions and effects of each action and encode them into the constructed controller. These preconditions and effects are descriptions of the task environment prior to and after taking some actions. This explicit representation of the actions\u2019 preconditions and effects is required to the methodology we propose to ground the constructed automaton-based controllers to their task environments via image-based observations, described in Section 5."
        },
        {
            "heading": "4.2. Automated Model Extraction from External Knowledge",
            "text": "A model is a transition system that encodes the dynamics of the task environment or the task-relevant knowledge from external knowledge sources. If we obtain an automaton-based model, we can use the model directly for the verification steps.\nHowever, in many cases, external knowledge is only available in textual form, such as websites and operation manuals. Again, such textual information is not formally verifiable. To address this problem, we develop an algorithm that automatically constructs a model that encodes the textual information from external sources. The algorithm takes natural language sentences as inputs, extracts propositions corresponding to actions and effects, and returns a transition system representing the model. Note that the algorithm automates the procedure for model construction. However, we do allow users to modify the constructed models to represent additional information.\nExtracting Actions and Effects from Text-Based Information Sources. Given text-based encodings of externally available knowledge, we apply semantic parsing again to extract all the verb phrases from the text. We consider each verb phrase as an action. Note that some of these verb phrases may have identical meanings, such as \u201ccross the road\u201d and \u201cwalk across the road.\u201d We align these verb phrases to the output symbols (actions) from the controller by querying GLM in the following format:\n1 Do the actions \"action 1\" and \"action 2\" lead to the same effect? 2 Yes/No.\nAlgorithm 1: Model Construction from Side Information\n1: procedure Text2Model(Set[String] actions, Set[String] effects) 2: QM = every combination of effects 3: \u03c9M := QM \u2192 2effects 4: \u03a3M = [VP for VP in actions] 5: \u03b4M = [] 6: for VP in actions do 7: for each pair of states (S 1, S 2) do 8: Prop = \u03c9M(S 1) \u2295 \u03c9M(S 2) \u25b7 \u2295 is XOR operation 9: if VPE \u2208 2Prop and VPE = \u03c9M(S 2) over the symbols 2Prop then 10: \u03b4M.append(S 1 \u00d7 VP\u2192 S 2) 11: end if 12: end for 13: end for 14: for each pair of neighbor states (S 1, S 2) do \u25b7 Optional 15: if there is no transition between S 1, S 2 then 16: e = \u03c9M(S 1) \u2295 \u03c9M(S 2) 17: if e < effects and \u00ace < effects then 18: \u03b4M.append(S 1 \u00d7 \u03f5 \u2192 S 2) 19: \u03b4M.append(S 2 \u00d7 \u03f5 \u2192 S 1) 20: end if 21: end if 22: end for 23: for S in QM do 24: \u03b4M.append(S \u00d7 \u03f5 \u2192 S ) 25: end for 26: return QM,\u03a3M, \u03b4M, \u03c9M 27: end procedure\nIf the response is \u201cyes,\u201d we align the action extracted from the textual information to the controller\u2019s output symbol. For instance, we align \u201cwalk across the road\u201d to \u201ccross the road\u201d, where \u201ccross the road\u201d is an output symbol from the controller. Since we already obtained the effects of the controller\u2019s actions (output symbols), we have a list of aligned actions and corresponding effects.\nIf an extracted action is not aligned with any of the controller\u2019s output symbols, we consider it as a new action and query the GLM to obtain the PDDL definition. The result of this process is a list of new actions and the corresponding effects from their PDDL definitions, which we merge with the aligned actions and effects to get a complete list.\nConstructing the Model from the Actions and their Effects. We send the lists of actions and effects to Algorithm 1 for model construction. Recall that the model consists of states, input and output symbols, a transition function, and a label function. The model has 2M states, where M is the number of atomic propositions in the set of effects. Each state S \u2208 QM is associated with a unique label \u03c9M(S ) that is a conjunction of all the atomic propositions or their negations from the set of effects. For instance, if effects={A, B}, then there will be four states with labels A \u2227 B, A \u2227 \u00acB,\u00acA \u2227 B,\u00acA \u2227 \u00acB.\nDefinition 1. Let S 1, S 2 \u2208 QM be the states from the model. We make pairwise comparisons of each atomic proposition in the labels of the two states \u03c9M(S 1) and \u03c9M(S 2). If only one of the atomic propositions differs, then states S 1, S 2 are considered neighbors.\nMathematically, we use the symbol \u2295 to denote this elementwise XOR operation. If only one proposition in \u03c9M(S 1) \u2295 \u03c9M(S 2) is evaluated to true, then states S 1, S 2 are considered neighbors.\nFor any two states S 1 and S 2, if there is an action whose effect fills the gap between the labels \u03c9M(S 1) and \u03c9M(S 2), then there is a transition between them with this action as the input symbol. For example, suppose S 1 has\na label A \u2227 \u00acB and S 2 has a label A \u2227 B, if there exists an action \u03b1 whose effect is B, then we can build a transition \u03b4(S 1, \u03b1) = S 2. Next, we build self-transitions with the action \u201cno operation\u201d for every state and build transitions with the action \u201cno operation\u201d between every two neighbor states if no other transition between them already exists.\nThose \u201cno operation\u201d transitions capture potential environmental changes that are not caused by the controller\u2019s behaviors. These transitions provide a conservative approximation of what could potentially happen in the environment. During the verification procedure, these transitions will lead to more failures. We can decide whether to add the \u201cno operation\u201d transitions depending on the stability of the environment and the task requirement. We present the algorithm for constructing models in Algorithm 1."
        },
        {
            "heading": "4.3. Controller Refinement",
            "text": "Once we have the controller and the model, we use the model to formally verify whether the controller satisfies user-provided specifications and design procedures to refine the controller if the verification step fails. As in [1], these procedures could be manual or automatic. Beyond the methods presented in [1], we present an automatic refinement method that makes more explicit use of the model.\nAutomated Verification. In the verification procedure, we build a product automaton P = M \u2297 C describing the interactions of the controller C with the modelM. Then, we obtain a specification \u03a6 expressed in linear temporal logic from the task designer or whoever wants to verify the controller. We run a model checker (e.g., NuSMV [34]) to verify if the product automaton satisfies the specification,\nM\u2297C |= \u03a6. (1)\nWe verify the product automaton against the specification for all the possible initial states. If the verification fails, the model checker returns a counter-example, which is a sequence of states of the product automaton (p1, q1), (p2, q2), ... where pi \u2208 QM, qi \u2208 Q.\nIf the verification fails, we iteratively refine the controller until the specification holds for all the initial states. The refinement procedure starts from the counter-example returned by the model checker and eventually produces a refined controller that will never violate the specification.\nAutomatic Refinement. Once we obtain a counter-example, we use the model to synthesize a new FSA that represents a sub-controller. Specifically, we run the model checker on the model against the negation of the specification and set the initial state to the first state in the counter-example. Mathematically, the model checker verifies\nM |= \u00ac\u03a6. (2)\nThe model checker returns a counter-example Q = {q1, ..., qn} to the negated specifications, which may be interpreted as an example that satisfies the specification. This counter-example is a sequence of states qi from the model. Such state sequence can be converted into a trajectory T = (p1, a1), ..., (pn, an), where pi \u2208 2P are the state labels and ai \u2208 \u03a3M are the input symbols. We construct an FSA that has an identical number of states as the number of steps in Q. The states for the new FSA are qi for i \u2208 [0, ..., |Q|]. The transition function of the FSA takes pi \u2208 2P as input symbols and adds a transition from qi\u22121 to qi. The label function takes the states and input symbols (qi\u22121, pi, qi) and outputs ai \u2208 \u03a3M. We have now constructed an FSA representing the desired sub-controller.\nNext, we merge the new sub-controller into the original controller. We begin by creating a \u201cno operation\u201d transition whose input symbols are the labels of the first state in the negated counter-example. Then, we modify the input of the self-transition in the controller\u2019s initial state to the conjunction of the negations of the input symbols of its outgoing transitions. Finally, if the last state of the sub-controller does not have any outgoing transitions, we add a \u201cno operation\u201d transition from it to any of the sink states in the controller. Note that a sink state is a state that can only transit to itself. Now, the controller and the sub-controller together form a refined controller.\nThe details of the refinement procedure are in Algorithm 2. We iteratively repeat this verification and refinement procedure until the specification is satisfied for all the initial states.\nAlgorithm 2: Controller Refinement through Synthesizing from the Model\n1: procedure Refine(Controller C, ModelM, Specification \u03a6) \u25b7 C = \u27e8\u03a3,\u0393,Q, q0, \u03b4, \u03c9\u27e9,M = \u27e8QM,\u03a3M, \u03b4M, \u03c9M\u27e9 2: if C \u2297M |= \u03a6 then 3: return C 4: end if 5: find a counter-example Q = {(p1, q1), (p2, q2), ...|pi \u2208 \u03a3M, qi \u2208 \u03a3} forM |= \u00ac\u03a6 6: convert Q to a trajectory T = {(\u03c31 \u2227 a1), ((\u03c32 \u2227 a2), ...|\u03c3i \u2208 \u03c9M, ai \u2208 \u03c9)} 7: Construct a new state q1, Q.append(q1) 8: \u03b4.append((qinit, \u03c31)\u2192 q1) 9: \u03c9.append((qinit, \u03c31)\u2192 a1)\n10: for i in [2, n] do 11: Q.append(qi) 12: \u03b4.append((qi\u22121, \u03c3i)\u2192 qi) 13: \u03c9.append((qi\u22121, \u03c3i)\u2192 ai) 14: end for 15: \u03b4.append((qn,True)\u2192 qn) 16: \u03c9.append((qn,True)\u2192 \u03f5) 17: \u03b4.replace((qinit,\u00acc1 \u2227 ... \u2227 \u00acck)\u2192 qinit) 18: return C 19: end procedure\nManual Refinement. We can also manually refine the controller by modifying the input prompt to the GLM based on a human\u2019s interpretation of the counter-example [1]. After the model checker returns a counter-example in a sequence of states, we convert it to a trajectory consisting of a sequence of symbols and actions. Then, a human user may interpret the reasons that lead to the counter-example and describes the reasons in natural language. Next, we modify the input prompt to include the reasons and ask GLM to refine the steps in natural language:\n1 Refine the following steps for task description to avoid/ensure reasons leading to the counter -example : 2 step_number_1. step description 3 step_number_2. step description 4 5 step_number_1. refined step description 6 step_number_2. refined step description 7 ...\nLastly, we construct a new controller from the refined steps output by GLM."
        },
        {
            "heading": "5. Grounding and Perception",
            "text": "We also develop a method named Automata2Env to connect the controller to the real-world task environment with a specified level of confidence. Automata2Env takes visual observations from the task environment and uses a vision-language model to determine the truth values of the atomic propositions that are relevant to the conditions specified in the controller.\n5.1. The Pipeline of Automata2Env\nTo operate in the task environment, an agent starts from the initial state of the controller. The agent collects all the propositions P from the controller and gets an image observation from the task environment. It then feeds the image and all the propositions as text into a vision-language model. Automata2Env requires vision-language models that can output normalized scores indicating how each proposition matches the image (e.g., CLIP [18]). We refer to such scores as confidence scores, which are commonly provided as outputs of vision-language models. A higher score means the vision-language model is more confident that the context of the proposition is within the content of the image.\nPractically, the vision-language models may not always capture the image features that are needed to make decisions due to perceptual limitations such as image artifacts or object misclassifications. Such perceptual limitations may cause uncertainties in the outputs of visual observations. The high-confidence grounding method, Automata2Env, incorporates the confidence scores from the vision-language model in order to ensure that the agent does not take actions when the conditions are uncertain.\nThe overall pipeline of Automata2Env is as follows:\nModifying the Controller to Handle Uncertainties. We first add uncertain as an additional atomic proposition and modify the controller by adding a self-transition \u03b4(qi, uncertain, qi) = 1 to each state qi. Intuitively, the controller will stay in the current state, and it will not perform any action if it gets uncertain observations.\nEvaluating Atomic Propositions. Second, we propose an algorithm to evaluate the truth values of propositions in image observations. The algorithm takes an atomic proposition in textual form, a vision-language model that can return confidence scores, and numerical thresholds as inputs. Recall that an input symbol is a set of atomic propositions.\nAs opposed to ordinary binary evaluation, the algorithm evaluates an atomic proposition and assigns one of the three values: true, false, and uncertain. Algorithm 3 shows how we evaluate the propositions using the confidence scores from the vision-language model. We define two thresholds to divide confidence scores into three categories. Since the confidence scores are dependent on the set of propositions that are provided as input (e.g., the visionlanguage model is more confident in detecting one set of objects than another set), we may set different thresholds for different controllers.\nTaking Actions. Third, after evaluating the set of atomic propositions, the agent chooses one transition whose input symbol (which itself is a logical formula over the atomic propositions) evaluates to true and takes corresponding actions. A demonstration of this pipeline is in Figure 4."
        },
        {
            "heading": "5.2. Procedure of Determining True and False Thresholds",
            "text": "Selecting the Vision-Language Model. We use the current state-of-the-art vision-language model called GroundedSegment-Anything (Grounded-SAM) [22, 21] to evaluate the propositions from image observations. The GroundedSAM is an open-domain object detection model, which can take any text as input and determine whether the object or scene described in the text appears in the image. The Grounded-SAM returns a confidence score for each detected object and the score will be zero if it does not find the object in the image.\nAlgorithm 3: Proposition Evaluation under Uncertainty\n1: procedure EvalProp(Atomic Proposition p, Observation I, Vision-Language Model VM, true threshold t, false threshold f ) \u25b7 p is a string and I is an image 2: score = VM(p, I) 3: if score \u2265 t then return p (p = true) 4: end if 5: if score \u2264 f then return \u00acp (p = false) 6: end if 7: return uncertain 8: end procedure\nValidating the Vision-Language Model. Once the vision-language model is selected, we validate the selected model on an externally provided dataset to determine the values of the true and false thresholds to be used in Algorithm 3. We assume the performance of a vision-language model on data outside its training dataset is consistent. The detection accuracy of the validation dataset should be close to the accuracy of the real data.\nFor validation, we use an image dataset called Argoverse [35] that contains driving scenes. We use the GroundedSAM to detect driving-relevant objects (e.g., crosswalks, traffic lights, cars) and check whether the detection results are correct. Figure 7 shows some examples of the Grounded-SAM\u2019s detection results on images from the Argoverse. We collect the confidence scores of the detection results from Grounded-SAM and the corresponding ground truth labels from the dataset. We count the number of detection results whose confidence scores fall into each range and plot them in the left figure (confidence score count) of Figure 8.\nDetermining Thresholds. From the confidence score histogram figure, we observe a gap between the confidence values of 0 and 0.25, which means there is no detection result whose confidence score falls within this range. A confidence score has to be greater than 0.25 (object detected) or equal to 0 (not detected). Hence we select 0.25 as the false threshold for Algorithm 3.\nTo determine the true threshold, we create a plot on the true threshold vs. prediction accuracy, which we presented in the right figure of Figure 8. For each true threshold t, we evaluate all the detection results whose confidence scores are greater than t to true and compute the percentage of \u201cthe number of results correctly be evaluated to true/the number of results be evaluated to true.\u201d We expect this percentage to be as close to 1 as possible, which means everything that evaluated to true is actually being true. Using the empirical results illustrated in the figure, we accordingly select 0.45 as the true threshold."
        },
        {
            "heading": "6. Empirical Demonstration",
            "text": "We illustrate the LLM2Automata algorithm and the grounding method Automata2Env with three proof-of-concept examples on different domains. We present the outputs of the algorithm step by step to show how it constructs, verifies, and refines the task controller. We also collect image observations from real-world environments to demonstrate the controller\u2019s behaviors in realistic deployment settings. We use the current state-of-the-art multimodal pretrained models GPT-4 [27] and Grounded-SAM [22, 21] to produce all the results in this section."
        },
        {
            "heading": "6.1. Crossing Road Example",
            "text": "We start the demonstration on a daily life task: cross the road. In this example, we construct a controller that can handle the crossing-road task both at the traffic light and without a traffic light.\nController Construction. First, we query GPT-4 for the steps of crossing the road at the crosswalk (without a traffic light) and obtain a list of steps in textual form:\n1 Steps for \"cross the road at the crosswalk\" 2 1. Look for a crosswalk. 3 2. Wait for a safe gap in traffic. 4 3. Cross the road.\nWe parse the steps to extract the keywords and verb phrases: 1 1. <Look for a crosswalk>. 2 2. <Wait> <safe gap in traffic>. 3 3. <Cross the road>.\nNext, we query GPT-4 again to define the verb phrases in PDDL and extract the preconditions and effects:\n1 Define an action \"Look for a crosswalk\" in PDDL 2 Action: Look_for_crosswalk 3 Precondition: (not (at crosswalk))\n4 Effect: (at crosswalk) 5 6 Define an action \"Safe gap in traffic\" in PDDL 7 Action: Safe_gap_in_traffic 8 Precondition: (and (vehicle_coming) (at crosswalk) ) 9 Effect: (not (vehicle_coming))\n10 11 Define an action \"cross road\" in PDDL 12 Action: Cross_road 13 Precondition: (and (not (vehicle_coming)) (at crosswalk) ) 14 Effect: (at other_side_of_road)\nWe repeat the steps above to get the verb phrases with their preconditions and effects for the task of crossing the road at a traffic light:\n1 Steps for \"cross the road at a traffic light\" 2 1. Approach the pedestrian crossing. 3 2. Wait for the traffic light to turn green. 4 3. Cross the road. 5 6 Define an action \"approach pedestrian crossing\" in PDDL 7 Action: Approach_pedestrian_crossing 8 Precondition: (not (at_pedestrian_crossing)) 9 Effect: (at_pedestrian_crossing)\n10 11 Define an action \"Traffic light turn green\" in PDDL 12 Action: Traffic_light_turn_green 13 Precondition: (not (traffic_light_is_green)) 14 Effect: (traffic_light_is_green) 15 16 Define an action \"cross road\" in PDDL 17 Action: Cross_road 18 Precondition: (traffic_light_is_green) (at_pedestrian_crossing) 19 Effect: (at other_side)\nAfter we have the verb phrases with preconditions and effects in textual form, we follow the grammar in Table 1 to transform each step into a state and its outgoing transitions. We get an FSA that represents the controller by connecting all the states with the transitions. Note that we get two controllers for crossing the road at the traffic light and at the crosswalk. For presentation purposes, we manually create a new initial state with two outgoing transitions pointing to the two controllers\u2019 original initial states. Finally, we merge the last states of the two controllers and obtain an integrated controller that can handle both scenarios, which we present in Figure 9.\nModel Construction. After constructing the controller, we want to check whether the knowledge from GPT-4 encoded by the controller is consistent with external knowledge. To do so, we collect information for crossing the road at the traffic light from a tutorial blog [36]. The information is in textual form:\n1 1. Reach painted lines on the roadway that indicate a safe crosswalk. 2 2. Cross the road only when the traffic light turns green.\nWe extract two verb phrases \u201creach painted lines indicating crosswalk\u201d and \u201ccross the road when the light turns green\u201d from the external textual information.\nNext, we check whether the extracted verb phrases are aligned with the existing actions by querying GPT-4:\n1 Are the actions \"reach painted lines indicating crosswalk\" and \"approach pedestrian - crossing\" lead to the same effect? 2 Yes. Both actions lead to reaching one side of the crosswalk. 3 Are the actions \"cross the road when the light turns green\" and \"cross road\" lead to the same effect? 4 Yes. Both actions lead to reaching the other side of the road.\nIn practice, we ask for every pair of combinations. However, we only present the aligned pairs for brevity. Note that both verb phrases extracted from the external information are aligned with the controller\u2019s output symbols (actions). Hence the model we will construct shares the same set of vocabularies with the controller.\nSince the two verb phrases are aligned with two of the actions from the controller, we directly use the aligned actions and their effects. We send a set of actions {cross road, approach pedestrian-crossing} and a set of corresponding effects {at pedestrian-crossing, at other side of road } as input to Algorithm 1 to construct a model representing the external information, which we present in Figure 10. The Text2Model algorithm builds a model that preserves all the possible environment dynamics and possible actions the agent can take. Some of the actions and environment dynamics may be nonsensical and can be manually pruned by users.\nVerification and Automatic Refinement. For a concise presentation, we present the verification and refinement procedure for the controller that the \u201ctraffic light\u201d proposition is fixed to true. That means we verify and refine the top half of the controller in Figure 9, which we also present as the automaton on the left of Figure 11.\nWe use the constructed model to verify whether the controller satisfies a specification\n\u03d5 = \u25a1\u2662 green \u2192(\u2662 other side of road \u2227 \u00ac( cross road \u2227 \u00ac green )).\nIntuitively, we want the agent to eventually reach the other side of the road but never cross the road when the traffic light is not green. We perform the verification step and get a counter-example. Hence we need to refine the controller.\nIn the first iteration of refinement, we get a counter-example whose trajectory starts from [at Pedestrian Crossing \u2227\u00ac green \u2227\u00ac the other side of the road], which is the \u201c100\u201d state in Figure 10. This counter-example means that if the agent is already at the pedestrian crossing when the controller is instantiated, it will get trapped in the initial state. This is because the precondition for the outgoing transition is \u201cnot at the pedestrian crossing\u201d. We follow Algorithm 2 to refine the controller before verifying it against the model again.\nIn the second iteration, we get another counter-example whose trajectory starts from [at Pedestrian Crossing \u2227 green \u2227\u00ac the other side of the road], which is the \u201c110\u201d state. We apply the refinement algorithm again and obtain the final controller that satisfies all the specifications. We present the final controller as the bottom automaton in Figure 11. The top and bottom branches of the final controller are synthesized during the first and second iterations, respectively.\nGrounding and Perception. We use the Grounded-SAM to evaluate the input symbols and implement the control logic in the real-world task environment. The Grounded-SAM takes an image and a set of propositions in textual form as inputs and classifies which propositions match the image. A proposition matches an image if the object or scenario described by the proposition appears in the image.\nKeep in mind that the Grounded-SAM can mistakenly classify objects in the images. We consider the confidence scores returned by the Grounded-SAM for implementing the control logic. In this experiment, we apply Algorithm 3 with a true threshold of 0.45 and a false threshold of 0.25. A proposition will be evaluated as uncertain if the score is between 0.25 and 0.45. We also adjust the controller to adapt to the real-world environment with perceptual uncertainties, as presented in Figure 12.\nWe again use the model in Figure 10 to verify the controller with uncertainties against the specification \u03a6 = \u00ac(crossroad \u2227 \u00acgreen) to ensure safety. The controller with uncertainties satisfies the specification. Therefore, with the assumption that the Grounded-SAM is reliable, the controller will never perform the cross-road action when the traffic light is not green. Hence we can ensure the safety of the controller.\nFigure 12 shows an example of grounding the controller to the real-world environment with perceptual uncertainties. We use the Grounded-SAM to evaluate propositions, which lead to the controller\u2019s state transitions. The controller returns the actions to take during the state transitions. We highlight the second image from the left in the observation sequence in Figure 12. Due to a confidence score of 0.4, the proposition \u201ctraffic light is green\u201d is evaluated to uncertain, which triggers a self-transition at state q22, and no action is taken. Note that the Grounded-SAM misclassified the red light to the green light. If we do not consider perceptual uncertainties, the cross-road action may be triggered at the red light."
        },
        {
            "heading": "6.2. Robot Arm Maniputation",
            "text": "We follow LLM2Automata to construct a controller for the task \u201cuse a robot arm to remove all the red blocks off the table.\u201d We show how we use the Grounded-SAM to perceive the operating environment and make decisions accordingly.\nController Construction. In this example, we assume the user has some prior knowledge of the task, such as some basic knowledge of the environment and the admissible actions of the robot arm. The user thus queries GPT-4 with\nthe following prompt:\n1 Task: place all the red blocks off the table. 2 Environment: there are unknown numbers of red blocks and yellow blocks on the table initially. Someone may randomly add a red block or yellow block to the table. 3 Steps for achieving the task: 4 1. Target one block on the table. 5 2. Classify the color of the targeted block. 6 3. If the block is red , place it from the table to an off -table location (B). If the block is yellow , leave it on the table. 7 4. Go to step 1. 8 9 1. Define an action \"target one block\" in PDDL.\n10 Action: target -one -block 11 Parameters: () 12 Precondition: (block_on_table) 13 Effect: (and (block_targeted)) 14 15 2. Define an action \"classify the color of the targeted block\" in PDDL. 16 Action: classify -color 17 Parameters: () 18 Precondition: (block_targeted) 19 Effect: (or (block_red) (block_yellow)) 20 21 3. Define an action \"place the block\" in PDDL. 22 Action: place -block 23 Parameters: () 24 Precondition: (and (block_targeted) (block_red)) 25 Effect: (not (block_targeted)) 26 27 3. Define an action \"leave the block\" in PDDL. 28 Action: leave -block 29 Parameters: (?b - block ?t - table)\n30 Precondition: (and (block_targeted) (block_yellow)) 31 Effect: (not (block_targeted))\nWe present the constructed controller in Figure 13.\nVerification of the Controller for the Manipulation Task. Next, we verify the controller in Figure 13. We begin by following Algorithm 1 to construct a model. We send the set of all actions (target block, classify color, leave, place) and the subset of effects (targeted, red, yellow) as the parameters for constructing the model. Then, we manually remove the states that are not reachable by any single action (i.e., the state label is not equal to the effect of any action) and obtain a model as presented in Figure 15.\nAs an added task specification, we want to guarantee the robot arm never accidentally places a yellow block outside the table. Hence we define the logical specification\n\u03a6 = \u00acplace \u2227 yellow.\nRecall that the trajectory is defined over the union of the set of actions and the set of effects. We use this environment model to verify that the robot arm controller satisfies the specification \u03a6.\nGrounding and Perception. We again use the Grounded-SAM as the perception model to ground the controller from Figure 13 to the operating environment. We set the true threshold and false threshold in Algorithm 3 to 0.45 and 0.25, respectively. Figure 14 shows a full iteration of the controller (q1 \u2192 q2 \u2192 q3 \u2192 q4). The robot arm grabs a block and places it outside the table only if the Grounded-SAM returns a confidence score above 0.45 on a red block.\nThe Effect of Changes to the Visual Confidence Threshold Value.. Figure 16 shows how the thresholds for evaluating propositions affect decision-making. In the first row of Figure 16, we set the true threshold to 0.55. Before we target a block, we search whether there are any blocks on the table. Due to the high value of the true threshold, there is only one block is evaluated to be true (a confidence score of 0.55) before we target a block. Then, we stay in state q3 without performing any action because the color of the targeted block is evaluated to be uncertain\u2014it is evaluated as \u201cred\u201d with a confidence score of 0.45.\nIn the second row of Figure 16, we set the true threshold to 0.45. Under this threshold, three blocks are evaluated to be true, with confidence scores of 0.46, 0.54, and 0.55. Therefore, we randomly target one block and classify its color. The color of the targeted block is classified as red, with a confidence score of 0.45. Hence we perform an action \u201cplace the block outside the table\u201d following the controller.\nIn the third row of Figure 16, we set the true threshold to 0.35. Three blocks are detected and evaluated to be true. We randomly target a block and classify its color. The classification result is red, with a confidence score of 0.38.\nHence we place this block outside the table. This operation violates the specification \u03a6 = \u00acplace \u2227 yellow due to the Grounded-SAM misclassifying a yellow block as red.\nIn conclusion, setting a higher true threshold or a lower false threshold leads to more conservative decisionmaking. The autonomous agent will spend more time on perceptions to raise the confidence score before taking an action. A more conservative decision-making process is less likely to violate the used-provided specifications."
        },
        {
            "heading": "6.3. Computer System Operation",
            "text": "In this example, we demonstrate our algorithms\u2019 capabilities on tasks involving interfacing with a computer system. The task is to restore a folder from the trash bin in the Apple MacOS operating system.\nController and Model Construction. We start constructing a controller for the task \u201crestoring a folder from the trash in MacBook\u201d by querying GPT-4 and get responses as follows:\n1 Steps for restoring one folder from Trash in MacBook: 2 1. Open Trash. 3 2. Find folder. 4 3. Restore folder. 5 6 1. Define an action \"Open Trash\" in PDDL. 7 Action: open -trash 8 Parameters: () 9 Precondition: (trash_bin)\n10 Effect: (trash_opened) 11\np1 : TBstart p2 : opened\np3 : \u00ac found\np4 : found p5 : restored\n\u00ac open Trash\nopen Trash\n\u00ac view Trash\nview Trash\nview Trash\n(True, \u03f5)\n\u00ac restore folder\nrestore folder (True, \u03f5)\nWe get a list of steps and a set of actions with preconditions from the responses. Then, we follow the grammar and transition rules defined in Table 1 to construct an automaton-based controller, as presented in Figure 18.\nAfter we construct the controller, we want to verify whether this controller is consistent with information from another knowledge source. We collect external information from a MacOS tutorial blog [37]:\n1 1. Navigate to the Trash folder. 2 2. View the content of the Trash folder. 3 3. Move a specific folder to your home folder.\nWe extract verb phrases \u201cnavigate to Trash folder,\u201d \u201cView the content of Trash folder,\u201d and \u201cMove a folder to home folder\u201d from the external information. Then, we align these verb phrases to the controller\u2019s actions:\n1 Are the actions \"navigate to Trash folder\" and \"open Trash\" lead to the same effect? 2 Yes. 3 Are the actions \"Move a folder to home folder\" and \"recover folder\" lead to the same effect? 4 Yes.\nWe align \u201cnavigate to Trash folder\u201d to \u201copen Trash\u201d and \u201cMove a folder to home folder\u201d to \u201crecover folder.\u201d However,\nthe verb phrase \u201cView the content of Trash folder\u201d is not aligned with any existing actions. Hence we consider it as a new action and define the action in PDDL and we rename the action to \u201cview Trash\u201d for presentation purposes:\n1 Define an action \"View Trash\" in PDDL. 2 Action: view -trash 3 Parameters: () 4 Precondition: (trash_opened) 5 Effect: (or (folder_found) (not (folder_found)) )\nNow we have a list of actions [\u201copen Trash,\u201d \u201cview Trash,\u201d \u201crecover folder\u201d] and their corresponding effects. We build a model accordingly following Algorithm 1. Note that we are not adding \u201cno operation\u201d transitions in this example.\nVerification and Refinement. We obtain a product automaton of the controller and the model and verify the product automaton against a specification\n\u03a6 = trash bin \u2227 \u00actrash opened \u2227 folder found\u2192 \u2662 folder restored.\nThe model-checking procedure fails and returns the following counter-example: (p1, q1), (p2, q2), (p2, q3), infinite loop in (p2, q4). This counter-example indicates that the model expects an action \u201cview Trash\u201d but the controller never takes such action.\nWe first apply Algorithm 2 to refine the controller automatically and present this automatically refined controller in the middle of Figure 18. In the second iteration, we verify the refined controller again and get the same counterexample. The same counter-example leads to the same refinement, which means the refined controller after the second iteration remains unchanged. Then we will continue to get the same counter-example in the following iterations because the controller remains unchanged. In this scenario, the automatic refinement procedure will never return a controller that satisfies the specification. This scenario indicates a drawback of our automatic refinement procedure: Such procedure only works when all the verb phrases (actions) from the external source are aligned with the controller\u2019s output symbols (actions).\nSince the automatic refinement fails to address this counter-example, we manually refine the controller through human interpretation of the counter-example. We find that the reason for failing the verification is missing an action \u201cview Trash.\u201d Hence we refine the input prompt to the language model and get a new list of steps from its responses:\n1 Refine the following steps to ensure an action \"view Trash\" is taken: 2 1. Open Trash 3 2. Find folder 4 3. Restore folder 5 1. Open Trash. 6 2. View Trash. 7 3. Restore folder.\nNow we construct a new controller from the refined list of steps and present it at the bottom of Figure 18. This new controller satisfies the specification.\nA Step Toward Low-Level Control. We have constructed an automaton-based controller that satisfies provided specifications. However, the controller only encodes high-level task knowledge. Therefore, we move a step forward to connect high-level knowledge to low-level control implementations, such as to an API or terminal commands that can directly control the system.\nWe again take advantage of the rich knowledge encoded in the GLM and ask the terminal command for each action from the controller:\n1 Give the command line on MacBook for each of the following actions: 2 1. Open Trash 3 2. View Trash 4 3. Restore folder 5 1. open ~/. Trash 6 2. ls -al ~/. Trash 7 3. mv ~/. Trash/folder_name destination_path\nWe then ground the controller to the MacOS by using the Grounded-SAM to perceive the task environment, i.e., images of the OS GUI. Once we determine the transition to take based on the observation, we execute the terminal command corresponding to the action of this transition. This example indicates the potential of our algorithm to connect abstract representations or high-level control logic to system-operable commands. However, this potential\nhas not been fully developed yet, as we currently only use knowledge from the language model to connect high-level actions to operable commands. This approach is incapable if the language model does not encode the knowledge of the operable commands, e.g., unique APIs of a particular robot."
        },
        {
            "heading": "7. Conclusion",
            "text": "We provide a proof-of-concept for the automatic construction of an automaton-based task controller of task knowledge from GLMs and the grounding of the controller to physical task environments. We propose an algorithm named LLM2Automata that fills the gap between the textual outputs of generative models and sequential decision-making in the aspects of synthesis, verification, grounding, and perception. The algorithm synthesizes automaton-based controllers from the text-based descriptions of task-relevant knowledge that are obtained from a GLM. Such automatonbased controllers can be verified against user-provided specifications over models representing the task environments or task knowledge from other independent sources. The algorithm provides a method to iteratively refine the controller until all the specifications are satisfied. Controller synthesis and refinement are highly automated, requiring only a short task description as the input prompt to the GLM. Additionally, we develop a high-confidence grounding method Automata2Env that grounds the automaton-based controllers to physical environments, uses vision-language models to interpret visual perceptions, and implements control logic based on the perceptions. Automata2Env utilizes the confidence scores returned by the vision-language models to ensure safety under perceptual uncertainties. Experimental results demonstrate the capabilities of LLM2Automata and Automata2Env on synthesis, verification, grounding, and perception."
        },
        {
            "heading": "7.1. Limitations",
            "text": "The grounding method Automata2Env uses a vision-language model to detect the objects in the images that match the propositions. Current state-of-the-art object detection models perform well in detecting static scenes but cannot capture dynamic scenes, such as motion or action detection. Although some up-to-date image question-answering models and image captioning models can capture dynamic scenes, these models cannot capture all the details in the image. Therefore, due to the limitation of the current vision-language models, Automata2Env can exhibit its full potential only if all the propositions describe static scenes."
        },
        {
            "heading": "7.2. Future Directions",
            "text": "So far, we have developed the algorithm to create formal representations of textual task knowledge and to ground those abstract representations in the physical environment through visual perceptions. As one future direction, we can\nexpand our perception method to an active perception algorithm, which actively seeks observations of the objects or scenes that are relevant to the task specifications.\nAlthough we can ground and verify the formal representations in the physical environment, the presented grounding and verification procedures operate at a relatively high level of abstraction and they do not yield controllers that are capable of low-level control. For instance, the autonomous agent performs an action \u201ccross-road\u201d based on the controller, but we do not specify how the \u201ccross-road\u201d action will be performed. As another future direction, we will develop an algorithm that can map those high-level actions into low-level operations (e.g., APIs provided by the robot) through further queries to the GLM."
        }
    ],
    "title": "Multimodal Pretrained Models for Sequential Decision-Making: Synthesis, Verification, Grounding, and Perception",
    "year": 2023
}