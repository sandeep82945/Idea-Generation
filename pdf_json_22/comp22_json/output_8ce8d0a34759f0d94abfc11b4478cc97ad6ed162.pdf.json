{
    "abstractText": "This work presents \u2018BanglaNLG,\u2019 a comprehensive benchmark for evaluating natural language generation (NLG) models in Bangla, a widely spoken yet low-resource language. We aggregate six challenging conditional text generation tasks under the BanglaNLG benchmark, introducing a new dataset on dialogue generation in the process. Furthermore, using a clean corpus of 27.5 GB of Bangla data, we pretrain \u2018BanglaT5\u2019, a sequenceto-sequence Transformer language model for Bangla. BanglaT5 achieves state-of-the-art performance in all of these tasks, outperforming several multilingual models by up to 9% absolute gain and 32% relative gain. We are making the new dialogue dataset and the BanglaT5 model publicly available at https://github. com/csebuetnlp/BanglaNLG in the hope of advancing future research on Bangla NLG.",
    "authors": [
        {
            "affiliations": [],
            "name": "Abhik Bhattacharjee"
        },
        {
            "affiliations": [],
            "name": "Tahmid Hasan"
        },
        {
            "affiliations": [],
            "name": "Wasi Uddin Ahmad"
        },
        {
            "affiliations": [],
            "name": "Rifat Shahriyar"
        }
    ],
    "id": "SP:1be2454ddbeb6fb23249036a54d2ce646a6eca38",
    "references": [
        {
            "authors": [
                "Abhik Bhattacharjee",
                "Tahmid Hasan",
                "Wasi Uddin Ahmad",
                "Yuan-Fang Li",
                "Yong bin Kang",
                "Rifat Shahriyar."
            ],
            "title": "Crosssum: Beyond english-centric crosslingual abstractive text summarization for 1500+ language pairs",
            "venue": "CoRR, abs/2112.08804.",
            "year": 2021
        },
        {
            "authors": [
                "Abhik Bhattacharjee",
                "Tahmid Hasan",
                "Wasi Ahmad Uddin",
                "Kazi Mubasshir",
                "Md. Saiful Islam",
                "Anindya Iqbal",
                "M. Sohel Rahman",
                "Rifat Shahriyar"
            ],
            "title": "BanglaBERT: Lagnuage model pretraining and benchmarks for low-resource language",
            "year": 2022
        },
        {
            "authors": [
                "Prithwiraj Bhattacharjee",
                "Avi Mallick",
                "Md. Saiful Islam",
                "Marium-E-Jannat."
            ],
            "title": "Bengali abstractive news summarization (bans): A neural attention approach",
            "venue": "Proceedings of International Conference on Trends in Computational and Cognitive Engineer-",
            "year": 2021
        },
        {
            "authors": [
                "Terra Blevins",
                "Luke Zettlemoyer."
            ],
            "title": "Language contamination explains the cross-lingual capabilities of english pretrained models",
            "venue": "arXiv preprint arXiv:2204.08110.",
            "year": 2022
        },
        {
            "authors": [
                "Samuel Cahyawijaya",
                "Genta Indra Winata",
                "Bryan Wilie",
                "Karissa Vincentio",
                "Xiaohong Li",
                "Adhiguna Kuncoro",
                "Sebastian Ruder",
                "Zhi Yuan Lim",
                "Syafri Bahar",
                "Masayu Khodra",
                "Ayu Purwarianti",
                "Pascale Fung"
            ],
            "title": "IndoNLG: Benchmark and resources",
            "year": 2021
        },
        {
            "authors": [
                "Hongshen Chen",
                "Xiaorui Liu",
                "Dawei Yin",
                "Jiliang Tang."
            ],
            "title": "A survey on dialogue systems: Recent advances and new frontiers",
            "venue": "Acm Sigkdd Explorations Newsletter, 19(2):25\u201335.",
            "year": 2017
        },
        {
            "authors": [
                "Jonathan H. Clark",
                "Eunsol Choi",
                "Michael Collins",
                "Dan Garrette",
                "Tom Kwiatkowski",
                "Vitaly Nikolaev",
                "Jennimaria Palomaki."
            ],
            "title": "TyDi QA: A benchmark for information-seeking question answering in typologically diverse languages",
            "venue": "Transactions of the As-",
            "year": 2020
        },
        {
            "authors": [
                "Raj Dabre",
                "Himani Shrotriya",
                "Anoop Kunchukuttan",
                "Ratish Puduppully",
                "Mitesh Khapra",
                "Pratyush Kumar."
            ],
            "title": "IndicBART: A pre-trained model for indic natural language generation",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Nobel Dhar",
                "Gaurob Saha",
                "Prithwiraj Bhattacharjee",
                "Avi Mallick",
                "Md Saiful Islam."
            ],
            "title": "Pointer over attention: An improved bangla text summarization approach using hybrid pointer generator network",
            "venue": "2021 24th International Conference on Computer",
            "year": 2021
        },
        {
            "authors": [
                "Angela Fan",
                "David Grangier",
                "Michael Auli."
            ],
            "title": "Controllable abstractive summarization",
            "venue": "Proceedings of the 2nd Workshop on Neural Machine Translation and Generation, pages 45\u201354, Melbourne, Australia. Association for Computational Linguistics.",
            "year": 2018
        },
        {
            "authors": [
                "Strobelt",
                "Nishant Subramani",
                "Wei Xu",
                "Diyi Yang",
                "Akhila Yerukola",
                "Jiawei Zhou."
            ],
            "title": "The GEM benchmark: Natural language generation, its evaluation and metrics",
            "venue": "Proceedings of the 1st Workshop on Natural Language Generation,",
            "year": 2021
        },
        {
            "authors": [
                "Naman Goyal",
                "Cynthia Gao",
                "Vishrav Chaudhary",
                "PengJen Chen",
                "Guillaume Wenzek",
                "Da Ju",
                "Sanjana Krishnan",
                "Marc\u2019Aurelio Ranzato",
                "Francisco Guzm\u00e1n",
                "Angela Fan"
            ],
            "title": "The Flores-101 evaluation",
            "year": 2022
        },
        {
            "authors": [
                "Tahmid Hasan",
                "Abhik Bhattacharjee",
                "Md. Saiful Islam",
                "Kazi Mubasshir",
                "Yuan-Fang Li",
                "Yong-Bin Kang",
                "M. Sohel Rahman",
                "Rifat Shahriyar."
            ],
            "title": "XLsum: Large-scale multilingual abstractive summarization for 44 languages",
            "venue": "Findings of the Association",
            "year": 2021
        },
        {
            "authors": [
                "Tahmid Hasan",
                "Abhik Bhattacharjee",
                "Kazi Samin",
                "Masum Hasan",
                "Madhusudan Basak",
                "M. Sohel Rahman",
                "Rifat Shahriyar."
            ],
            "title": "Not low-resource anymore: Aligner ensembling, batch filtering, and new datasets for Bengali-English machine translation",
            "venue": "In",
            "year": 2020
        },
        {
            "authors": [
                "P Hayes-Roth",
                "M Fox",
                "G Gill",
                "DJ Mostow",
                "R Reddy."
            ],
            "title": "Speech understanding systems: Summary of results of the five-year research effort",
            "venue": "CarnegieMellon University, Computer Science Department Interim Report.",
            "year": 1976
        },
        {
            "authors": [
                "Jeremy Howard",
                "Sebastian Ruder."
            ],
            "title": "Universal language model fine-tuning for text classification",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 328\u2013339, Melbourne, Australia.",
            "year": 2018
        },
        {
            "authors": [
                "Junjie Hu",
                "Sebastian Ruder",
                "Aditya Siddhant",
                "Graham Neubig",
                "Orhan Firat",
                "Melvin Johnson."
            ],
            "title": "XTREME: A massively multilingual multitask benchmark for evaluating cross-lingual generalisation",
            "venue": "Proceedings of the 37th International",
            "year": 2020
        },
        {
            "authors": [
                "Pratik Joshi",
                "Sebastin Santy",
                "Amar Budhiraja",
                "Kalika Bali",
                "Monojit Choudhury."
            ],
            "title": "The state and fate of linguistic diversity and inclusion in the NLP world",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages",
            "year": 2020
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.",
            "year": 2015
        },
        {
            "authors": [
                "Philipp Koehn."
            ],
            "title": "Statistical significance tests for machine translation evaluation",
            "venue": "Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 388\u2013395, Barcelona, Spain. Association for Computational Linguistics.",
            "year": 2004
        },
        {
            "authors": [
                "Taku Kudo",
                "John Richardson."
            ],
            "title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System",
            "year": 2018
        },
        {
            "authors": [
                "Aman Kumar",
                "Himani Shrotriya",
                "Prachi Sahu",
                "Raj Dabre",
                "Ratish Puduppully",
                "Anoop Kunchukuttan",
                "Amogh Mishra",
                "Mitesh M. Khapra",
                "Pratyush Kumar"
            ],
            "title": "Indicnlg suite: Multilingual datasets for diverse nlg tasks in indic languages",
            "year": 2022
        },
        {
            "authors": [
                "Mike Lewis",
                "Marjan Ghazvininejad",
                "Gargi Ghosh",
                "Armen Aghajanyan",
                "Sida Wang",
                "Luke Zettlemoyer."
            ],
            "title": "Pre-training via paraphrasing",
            "venue": "Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS\u201920, Red Hook,",
            "year": 2020
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "2020b. BART: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Yanran Li",
                "Hui Su",
                "Xiaoyu Shen",
                "Wenjie Li",
                "Ziqiang Cao",
                "Shuzi Niu."
            ],
            "title": "DailyDialog: A manually labelled multi-turn dialogue dataset",
            "venue": "Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers),",
            "year": 2017
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "ROUGE: A package for automatic evaluation of summaries",
            "venue": "Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain. Association for Computational Linguistics.",
            "year": 2004
        },
        {
            "authors": [
                "Dayiheng Liu",
                "Yu Yan",
                "Yeyun Gong",
                "Weizhen Qi",
                "Hang Zhang",
                "Jian Jiao",
                "Weizhu Chen",
                "Jie Fu",
                "Linjun Shou",
                "Ming Gong",
                "Pengcheng Wang",
                "Jiusheng Chen",
                "Daxin Jiang",
                "Jiancheng Lv",
                "Ruofei Zhang",
                "Winnie Wu",
                "Ming Zhou",
                "Nan Duan"
            ],
            "title": "GLGE: A new",
            "year": 2021
        },
        {
            "authors": [
                "Yinhan Liu",
                "Jiatao Gu",
                "Naman Goyal",
                "Xian Li",
                "Sergey Edunov",
                "Marjan Ghazvininejad",
                "Mike Lewis",
                "Luke Zettlemoyer."
            ],
            "title": "Multilingual denoising pretraining for neural machine translation",
            "venue": "Transactions of the Association for Computational Linguis-",
            "year": 2020
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Alexandra Luccioni",
                "Joseph Viviano."
            ],
            "title": "What\u2019s in the box? an analysis of undesirable content in the Common Crawl corpus",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Confer-",
            "year": 2021
        },
        {
            "authors": [
                "Shuming Ma",
                "Li Dong",
                "Shaohan Huang",
                "Dongdong Zhang",
                "Alexandre Muzio",
                "Saksham Singhal",
                "Hany Hassan Awadalla",
                "Xia Song",
                "Furu Wei"
            ],
            "title": "Deltalm: Encoder-decoder pre-training for language generation and translation",
            "year": 2021
        },
        {
            "authors": [
                "Joshua Maynez",
                "Shashi Narayan",
                "Bernd Bohnet",
                "Ryan McDonald."
            ],
            "title": "On faithfulness and factuality in abstractive summarization",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1906\u20131919, On-",
            "year": 2020
        },
        {
            "authors": [
                "Tomas Mikolov",
                "Martin Karafi\u00e1t",
                "Lukas Burget",
                "Jan Cernock\u1ef3",
                "Sanjeev Khudanpur."
            ],
            "title": "Recurrent neural network based language model",
            "venue": "Interspeech, 2(3):1045\u20131048.",
            "year": 2010
        },
        {
            "authors": [
                "Diego Moussallem",
                "Paramjot Kaur",
                "Thiago Ferreira",
                "Chris van der Lee",
                "Anastasia Shimorina",
                "Felix Conrads",
                "Michael R\u00f6der",
                "Ren\u00e9 Speck",
                "Claire Gardent",
                "Simon Mille",
                "Nikolai Ilinykh",
                "Axel-Cyrille Ngonga Ngomo"
            ],
            "title": "A general benchmarking",
            "year": 2020
        },
        {
            "authors": [
                "Mohammad Abdullah Al Mumin",
                "Md Hanif Seddiqui",
                "Muhammed Zafar Iqbal",
                "Mohammed Jahirul Islam."
            ],
            "title": "Neural machine translation for lowresource english-bangla",
            "venue": "Journal of Computer Science, 15(11):1627\u20131637.",
            "year": 2019
        },
        {
            "authors": [
                "Mohammad Abdullah Al Mumin",
                "Md Hanif Seddiqui",
                "Muhammed Zafar Iqbal",
                "Mohammed Jahirul Islam."
            ],
            "title": "shu-torjoma: An english-bangla statistical machine translation system",
            "venue": "Journal of Computer Science, 15(7):1022\u20131039.",
            "year": 2019
        },
        {
            "authors": [
                "Yasin Tarabar",
                "Ankit Gupta",
                "Tao Yu",
                "Yi Chern Tan",
                "Xi Victoria Lin",
                "Caiming Xiong",
                "Richard Socher",
                "Nazneen Fatema Rajani."
            ],
            "title": "DART: Opendomain structured data record to text generation",
            "venue": "Proceedings of the 2021 Conference of the North",
            "year": 2021
        },
        {
            "authors": [
                "Linguistics. Matt Post"
            ],
            "title": "A call for clarity in reporting",
            "year": 2018
        },
        {
            "authors": [
                "Wei Li",
                "Peter J Liu"
            ],
            "title": "Exploring the limits",
            "year": 2020
        },
        {
            "authors": [
                "Siva Reddy",
                "Danqi Chen",
                "Christopher D. Manning."
            ],
            "title": "CoQA: A conversational question answering challenge",
            "venue": "Transactions of the Association for Computational Linguistics, 7:249\u2013266.",
            "year": 2019
        },
        {
            "authors": [
                "Sascha Rothe",
                "Shashi Narayan",
                "Aliaksei Severyn."
            ],
            "title": "Leveraging pre-trained checkpoints for sequence generation tasks",
            "venue": "Transactions of the Association for Computational Linguistics, 8:264\u2013280.",
            "year": 2020
        },
        {
            "authors": [
                "Noam Shazeer"
            ],
            "title": "Glu variants improve transformer",
            "year": 2020
        },
        {
            "authors": [
                "Ilya Sutskever",
                "Oriol Vinyals",
                "Quoc V Le."
            ],
            "title": "Sequence to sequence learning with neural networks",
            "venue": "Proceedings of the 27th International Conference on Neural Information Processing Systems (NIPS 2014), pages 3104\u20133112, Montreal, Canada.",
            "year": 2014
        },
        {
            "authors": [
                "Christian Szegedy",
                "Vincent Vanhoucke",
                "Sergey Ioffe",
                "Jon Shlens",
                "Zbigniew Wojna."
            ],
            "title": "Rethinking the inception architecture for computer vision",
            "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2818\u20132826.",
            "year": 2016
        },
        {
            "authors": [
                "Yuqing Tang",
                "Chau Tran",
                "Xian Li",
                "Peng-Jen Chen",
                "Naman Goyal",
                "Vishrav Chaudhary",
                "Jiatao Gu",
                "Angela Fan."
            ],
            "title": "Multilingual translation with extensible multilingual pretraining and finetuning",
            "venue": "arXiv preprint arXiv:2008.00401.",
            "year": 2020
        },
        {
            "authors": [
                "Erik F. Tjong Kim Sang",
                "Fien De Meulder."
            ],
            "title": "Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition",
            "venue": "Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003, pages 142\u2013",
            "year": 2003
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Proceedings of the 31st International Conference on Neural Information Processing Sys-",
            "year": 2017
        },
        {
            "authors": [
                "Alex Wang",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel Bowman."
            ],
            "title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
            "venue": "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing",
            "year": 2018
        },
        {
            "authors": [
                "Guillaume Wenzek",
                "Marie-Anne Lachaux",
                "Alexis Conneau",
                "Vishrav Chaudhary",
                "Francisco Guzm\u00e1n",
                "Armand Joulin",
                "Edouard Grave."
            ],
            "title": "CCNet: Extracting high quality monolingual datasets from web crawl data",
            "venue": "Proceedings of the 12th Lan-",
            "year": 2020
        },
        {
            "authors": [
                "Linting Xue",
                "Noah Constant",
                "Adam Roberts",
                "Mihir Kale",
                "Rami Al-Rfou",
                "Aditya Siddhant",
                "Aditya Barua",
                "Colin Raffel."
            ],
            "title": "mT5: A massively multilingual pre-trained text-to-text transformer",
            "venue": "Proceedings of the 2021 Conference of the North American Chap-",
            "year": 2021
        },
        {
            "authors": [
                "Saizheng Zhang",
                "Emily Dinan",
                "Jack Urbanek",
                "Arthur Szlam",
                "Douwe Kiela",
                "Jason Weston"
            ],
            "title": "Personalizing dialogue agents: I have a dog, do you have pets too",
            "venue": "In Proceedings of the 56th Annual Meeting of the Association for Computational Lin-",
            "year": 2018
        },
        {
            "authors": [
                "Yizhe Zhang",
                "Siqi Sun",
                "Michel Galley",
                "Yen-Chun Chen",
                "Chris Brockett",
                "Xiang Gao",
                "Jianfeng Gao",
                "Jingjing Liu",
                "Bill Dolan."
            ],
            "title": "DIALOGPT : Large-scale generative pre-training for conversational response generation",
            "venue": "Proceedings of the 58th Annual Meet-",
            "year": 2020
        },
        {
            "authors": [
                "Bhattacharjee"
            ],
            "title": "2022) curated the Bangla2B+ corpus by document-level language filtering, these documents preserve foreign text sequences occurring in the Bangla documents",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "Findings of the Association for Computational Linguistics: EACL 2023, pages 726\u2013735 May 2-6, 2023 \u00a92023 Association for Computational Linguistics"
        },
        {
            "heading": "1 Introduction",
            "text": "The emergence of pretrained language models (Devlin et al., 2019; Radford et al., 2019; Liu et al., 2019) has brought about a revolutionary change in natural language processing (NLP). With little taskspecific fine-tuning, these models have achieved state-of-the-art results on many NLP tasks (Wang et al., 2018; Rajpurkar et al., 2016; Tjong Kim Sang and De Meulder, 2003). However, the focus of these models has predominantly been on natural language understanding (NLU). Even models pretrained with generative objectives (Raffel et al., 2020) concern themselves with NLU tasks more than natural language generation (NLG) tasks. Although there have been recent efforts to uplift NLG (Gehrmann et al., 2021), they are primarily geared towards high- and mid-resource languages. For example, despite being the sixth most spoken language in the world with over 230 million native speakers comprising 3% of the world\u2019s total population,1 Bangla has remained an underrepresented\n1https://w.wiki/Psq\nlanguage in the NLP literature (Joshi et al., 2020). There have been only a handful of benchmark studies on Bangla NLG (Dabre et al., 2022; Kumar et al., 2022), and that too without Bangla being the main focus. This can be attributed to the lack of diverse NLG tasks under a single benchmark and strong pretrained Bangla NLG models.\nTo this end, we present \u2018BanglaNLG,\u2019 a comprehensive benchmark for Bangla language generation comprising six representative tasks on machine translation, text summarization, question answering, dialogue generation, headline generation, and cross-lingual summarization. To our knowledge, BanglaNLG is the first NLG benchmark exclusively for a low-resource language.\nTo establish a strong baseline for this benchmark, we pretrain BanglaT5 \u2013 a sequence-to-sequence Transformer model (Vaswani et al., 2017) pretrained on a 27.5 GB clean Bangla text corpus covering a broad range of domains. In summary:\n\u2022 We develop the BanglaNLG benchmark bringing together six NLG tasks.\n\u2022 We introduce a Multi-turn Dialogue dataset. \u2022 We pretrain BanglaT5 and evaluate it on the\nsix NLG tasks, showing strong results.\nBanglaT5 outperforms similar-sized multilingual models, achieving new state-of-the-art results on three tasks with a 4% gain on average. We are releasing the BanglaT5 model and a live leaderboard to promote future research on Bangla NLG."
        },
        {
            "heading": "2 The Bangla Natural Language Generation (BanglaNLG) Benchmark",
            "text": "There have been sporadic works on Bangla NLG, mostly catered to machine translation (Hasan et al., 2020; Mumin et al., 2019a,b) and text summarization (Bhattacharjee et al., 2021b; Dhar et al., 2021). However, Bangla NLG lacks a unified study comprising diverse and challenging tasks. Motivated by the popular benchmarks like GLUE (Wang\n726\net al., 2018), XTREME (Hu et al., 2020), GEM (Gehrmann et al., 2021), that have facilitated the training/evaluation of NLP models, we establish the first-ever Bangla Natural Language Generation (BanglaNLG) Benchmark."
        },
        {
            "heading": "2.1 Task Selection Criteria",
            "text": "We consider the following factors while choosing the evaluation tasks:\n1. Diversity: The tasks should focus on evaluating the model\u2019s generalization capabilities. Therefore, they should vary in task nature \u2013 the input and output length, the type of generated text, the target domain, and the dataset size.\n2. Practical Applicability: The choice of tasks should be driven by their practical implications. Rather than being used in abstract situations, NLG models trained on these tasks should be able to aid/reduce human effort in real-world scenarios.\n3. Difficulty: The tasks should be challenging while not being unsolvable. There should be clear room for improvement to foster future research.\n4. Accessibility: The selected datasets for these tasks should be openly accessible to encourage researchers to design better NLG models.\n5. Evaluation: The selected tasks should have reliable automated metrics for evaluating the focused abilities of an NLG model."
        },
        {
            "heading": "2.2 Selected Tasks",
            "text": "Considering the criteria mentioned above, we design BanglaNLG as an aggregation of six tasks:\n1. Machine Translation (MT): MT is perhaps the most studied NLG task in Bangla and the most commonly benchmarked NLG task in general. We use the BanglaNMT parallel corpus (Hasan et al., 2020), the largest Bangla-English MT dataset curated, with 2.75 million parallel pairs for training. The sentence pairs originate from various domains such as Wikipedia, news articles, religious and law\ndocuments, etc. We evaluate the NLG models using FLoRes-100 (Goyal et al., 2022) in both directions on this dataset, i.e., Bangla to English and English to Bangla. This task is particularly challenging since it assesses an NLG model\u2019s bilingual generation capabilities. Following standard practice, we use detokenized SacreBLEU (Post, 2018) as the evaluation metric for this task. 2. Text Summarization (TS): This task aims to generate a short and fluent summary given a long text document. We chose the Bangla portion of XLSum (Hasan et al., 2021) for this task. XL-Sum is a large comprehensive dataset for abstractive TS where the article and summaries are written by professional editors of BBC News. The articles cover various topics such as entertainment, politics, science, sports, etc. For this task, we use ROUGE22 (Lin, 2004) as the evaluation metric. 3. Question Answering (QA): This is a fundamental NLP task that can be modeled as both an NLU and NLG task. We use the BQA (Bhattacharjee et al., 2022) dataset for this task. The training data is machine translated from SQuAD 2.0 (Rajpurkar et al., 2018), while the evaluation data come from the human-annotated question-answer pairs of the TyDi-QA (Clark et al., 2020) secondary gold passage task. Although TyDi-QA only contains answerable questions, BQA introduced unanswerable questions to make the task more challenging. Following SQuAD 2.0, we use Exact Match (EM) and F1 as the evaluation metrics. 4. Multi-turn Dialogue (MTD): Conversational AI is a crucial task for NLG (Chen et al., 2017). However, there is no public dataset for dialogue generation in Bangla. As such, we curate a new multi-turn dialogue dataset by translating the DailyDialog (Li et al., 2017) dataset using the English to Bangla translation model introduced by Hasan\n2We use Bangla stemming supported ROUGE implementation from https://github.com/csebuetnlp/xl-sum/ tree/master/multilingual_rouge_scoring.\net al. (2020). Unlike standard QA-style conversation datasets, DailyDialog reflects real-life conversations in various social situations rich in emotion, making it a perfect candidate for our benchmark. We automatically translate the training data following the same procedure described in Bhattacharjee et al. (2022) and have the evaluation sets manually translated by expert human translators. We use BLEU-1 as the evaluation metric for this task to properly differentiate between models since averaged BLEU scores of up to 4-gram tend to be quite low in dialogue evaluation (Zhang et al., 2020).\n5. News Headline Generation (NHG): Automating headline generation can help news editors write compelling headlines to draw readers\u2019 attention. We consider NHG as a complementary task to TS. Given an article, the objective is to generate an appropriate headline that accurately depicts the article. We repurpose the XL-Sum (Hasan et al., 2021) dataset for this task since it also includes the titles of the articles. Like TS, we use ROUGE-2 as the evaluation metric."
        },
        {
            "heading": "6. Cross-lingual Summarization (XLS): As",
            "text": "another task for evaluating models\u2019 bilingual generation capabilities, we consider XLS. In this task, given a piece of text in a source language, we have to generate the corresponding summary in a target language. This is potentially harder than both MT and TS considering it combines both in a single task. We consider the English-Bengali portion of the CrossSum (Bhattacharjee et al., 2021a) dataset for this task. It is curated by aligning identical articles written in different languages from the XLSum dataset. For evaluation, we use ROUGE-2.\nWe present detailed statistics of the BanglaNLG benchmark in Table 1."
        },
        {
            "heading": "3 BanglaT5",
            "text": "We introduce BanglaT5, a sequence-to-sequence Transformer model (Vaswani et al., 2017), to establish a strong baseline for BanglaNLG benchmark. In this section, we describe the pretraining data, objectives, and model architecture of BanglaT5."
        },
        {
            "heading": "3.1 Pretraining Data",
            "text": "We chose Bangla2B+ (Bhattacharjee et al., 2022) as the pretraining corpus for BanglaT5. This is a 27.5 GB dataset containing 5.25 million documents collected from a meticulously selected list of web sources. While larger sources like CCNet (Wenzek et al., 2020) and mC4 (Xue et al., 2021) are\navailable, these contain a lot of noise and offensive texts that are difficult to remove. For a generative model, even small amounts of unwanted texts in pretraining could lead to potentially dangerous biases in generated text (Luccioni and Viviano, 2021). Therefore, we decided not to use them."
        },
        {
            "heading": "3.2 Data Pre-processing",
            "text": "Following Hasan et al. (2020), we preprocessed the texts using their normalization pipeline3. We trained a SentencePiece (Kudo and Richardson, 2018) vocabulary of 32k subword tokens on the normalized corpus with a character coverage of 0.99995. While creating a training sample, we limited the maximum sequence length to 512 tokens for both input and output and discarded documents with a token count below 7. After tokenization, we had 4.8 million data points with an average sequence length of 402.32 tokens."
        },
        {
            "heading": "3.3 Pretraining Objective",
            "text": "For generative language modeling, two standard choices are decoder-only models (Mikolov et al., 2010) and encoder-decoder models (Sutskever et al., 2014). Radford et al. (2019) trained a decoder-only Transformer (Vaswani et al., 2017) pretrained on the conditional continuation objective. However, to provide more flexibility on generation and possible usage on understanding tasks, we only consider encoder-decoder models following the original design of the Transformer. They are generally trained with different denoising objectives to increase the encoder\u2019s and decoder\u2019s capacity. For instance, BART (Lewis et al., 2020b), and mBART (Liu et al., 2020) use a text-infillingbased objective. In contrast, MARGE (Lewis et al., 2020a) is a multilingual encoder-decoder model trained to reconstruct a document in one language by retrieving documents in other languages. Following Raffel et al. (2020), we pretrained BanglaT5 using a \"span-correction\" objective, empirically shown to be an optimal choice for encoder-decoder models. In this objective, consecutive spans of input tokens are replaced with a mask token, and the model is trained to reconstruct them."
        },
        {
            "heading": "3.4 Model Architecture & Hyperparameters",
            "text": "We pretrained the base variant of the T5 model: 12 layers, 12 attention heads, 768 hidden size, 2048 feed-forward size with GeGLU activation (Shazeer,\n3https://github.com/csebuetnlp/normalizer\n2020) with a batch size of 65536 tokens for 3 million steps on a v3-8 TPU instance on GCP. We used the Adam (Kingma and Ba, 2015) optimizer with a 3e-4 learning rate, linear warmup of 10k steps, and \u2018inverse square root\u2019 learning rate decay."
        },
        {
            "heading": "4 Experiments & Results",
            "text": "We compared BanglaT5 it with four multilingual models: mT5 (base) (Xue et al., 2021), mBART50 (Tang et al., 2020), XLM-ProphetNet (Qi et al., 2021), and IndicBART (both unified and separate script variants) (Dabre et al., 2022).4 All pretrained models were fine-tuned for 3-15 epochs with batch size 32 (128 for MT). We used linear warmup with a ratio of 0.1, label smoothing of 0.1 (Szegedy et al., 2016), and weight decay of 1e-6 with the Adam optimizer (Kingma and Ba, 2015). The learning rate was tuned from the set {5e-5, 1e-4, 5e-4}. The best model was evaluated based on the validation performance after each epoch.\nDuring inference, we used beam-search (HayesRoth et al., 1976) with beam size 5 (on all tasks except QA), removed duplicated trigrams during beam search (Fan et al., 2018), and used a length penalty (Wu et al., 2016) of 0.6. For QA, we used greedy decoding, i.e., picking the most probable token during each decoding step.\nThe evaluation results are presented in Table 2. In all the tasks, BanglaT5 outperformed all multilingual models by a considerable margin, on average 4% over the second-best, mT5. In all monolingual tasks except MTD, BanglaT5 achieves a big performance gain over others (up to 9.54% in QA), which can be attributed to the quality of the pretraining data. In MD, BanglaT5 lags marginally behind XLM-ProphetNet. We hypothesize this is due to the lack of colloquial data in Bangla2B+ since Bhattacharjee et al. (2022) left out such sources to avoid\n4Due to computational budget limitations, we do not benchmark on billion-parameter models like large mT5 variants.\ntoxic and biased conversations. We find the MT results particularly interesting, where BanglaT5 outperforms larger multilingual models in both directions. This suggests that despite having very little English data in the pretraining corpus, BanglaT5 can generalize well to a new translation language, given high-quality fine-tuning data. We explore this more in the Appendix. Conspicuously, all the models achieve relatively poor scores on the XLS task. This can be attributed to the smaller amount of training data.\nBanglaT5 proves its superiority in compute and memory efficiency along with its performance due to its smaller size (less than half the parameters of all multilingual models except IndicBART). In practice, we observe 2-2.5x faster training and inference times with BanglaT5 than these larger multilingual models."
        },
        {
            "heading": "5 Related Works",
            "text": "Pretrained models NLP has witnessed a sea of change with the advent of pretrained language models like ULMfit (Howard and Ruder, 2018), ELMo (Peters et al., 2018), and most notably BERT (Devlin et al., 2019), achieving state-of-the-art results in many NLU benchmarks. Besides these NLU models, more and more pretrained models designed for NLG tasks have been proposed. Rothe et al. (2020) adopted pretrained NLU model checkpoints for generative tasks. GPT-2 (Radford et al., 2019), and later GPT-3 (Brown et al., 2020) showed that pretrained generative language models can perform remarkably well in zero-shot transfer tasks. More recently, Qi et al. (2020) proposed ProphetNet, which introduces the future n-gram prediction mechanism for language generation. Dabre et al. (2022) introduced IndicBART, which is pretrained on 11 Indic languages, including Bangla.\nNLG Benchmarks Recently, many multi-task benchmarks have been proposed to drive the\nprogress of NLG models. Moussallem et al. (2020) proposed the BENG benchmark for NLG and knowledge extraction. GLGE (Liu et al., 2021) is a similar benchmark with a different set of tasks and difficulty levels. However, these benchmarks are limited to English only. Gehrmann et al. (2021) introduced the GEM benchmark for various tasks such as summarization (Narayan et al., 2018), datato-text generation (Nan et al., 2021) across different languages. Cahyawijaya et al. (2021) introduced different tasks and baselines for 3 Indonesian languages. More recently, Kumar et al. (2022) introduced IndicNLG, a benchmark with five tasks in 11 Indic languages, including Bangla."
        },
        {
            "heading": "6 Conclusion & Future Works",
            "text": "NLP research in low-resource languages is lagging behind due to the lack of reliable benchmarks and datasets. To facilitate the development, evaluation, and comparison of new NLG models, we introduced a multi-task evaluation benchmark for Bangla NLG, a widely spoken yet low-resource language. We presented BanglaT5, a pretrained NLG model in Bangla, setting new state-of-the-art results with BanglaT5. We strongly believe that our contributions in this work will help the Bangla NLP community benchmark NLG tasks more easily under a unified setup.\nIn future work, we plan to introduce new tasks to BanglaNLG, such as personalized dialogue generation (Zhang et al., 2018), conversational questionanswering (Reddy et al., 2019). We will also add more recent multilingual models to our comparison to BanglaT5, e.g., DeltaLM (Ma et al., 2021).\nLimitations\nAlthough Bhattacharjee et al. (2022) claimed that Bangla2B+, the pretraining corpus for BanglaT5, had been carefully filtered for offensive or unwanted texts, they alerted that there might be small amounts of these contents may be present, which can result in bias or toxicity in the pretrained model. We, therefore, recommend using BanglaT5 with caution, especially for real-world deployment.\nEthics Statement\nLicense The TyDiQA dataset (Clark et al., 2020) is released under the Apache License 2.0, allowing modifications and distribution. All other pretraining and fine-tuning datasets are released under the\nCreative Commons Attribution-NonCommercialShareAlike 4.0 International License (CC BY-NCSA 4.0), which allows modifications and distributions for non-commercial research purposes. We strictly adhere to these licenses and will release BanglaT5 and BanglaNLG benchmark resources under CC BY-NC-SA 4.0. Annotation Expert translators who provide translation services for renowned Bangla newspapers were hired to translate the evaluation sets of the dialogue dataset. Each translated sentence was further assessed for quality by another expert. It was again translated by the original translator if found to be of low quality. If the re-translation was found to be of low quality, it was then translated by the other expert. The experts were paid hourly as per standard rates in local currency. Hallucinated Text It is well-known that text generation models can hallucinate outputs that may not necessarily be faithful to the original input (Maynez et al., 2020). Though the texts may be fluent and human-like, the hallucinations may be factually inconsistent and impact the outputs negatively. BanglaT5 may be susceptible to the same kinds of hallucinations. Carbon Footprint We avoided using large models for pretraining and fine-tuning, reducing their environmental impacts. BanglaT5 was trained for about 30 days on Google v3 TPUs. Google\u2019s TPUs are specifically designed for machine learning, which makes them up to five times more efficient than GPUs. Assuming 0.080kg carbon emission per kWh,5 the pretraining would emit fewer than 100kg carbon into the environment, far below most computationally demanding models. All fine-tuning experiments were done on a desktop machine with an 8-core Intel Core-i7 11700k CPU and NVIDIA RTX 3090 GPU, and no single run except machine translation took more than 12 hours, which amounts to fewer than 0.5kg carbon emission. On average, machine translation runs took three days each, emitting less than 3kg of carbon."
        },
        {
            "heading": "Acknowledgements",
            "text": "We would like to thank the Research and Innovation Centre for Science and Engineering (RISE), BUET, for funding the project and Google TPU Research Cloud (TRC) program for providing cloud support.\n5https://blog.google/technology/ai/ minimizing-carbon-footprint/"
        },
        {
            "heading": "A Multi-turn Dialogue Scores",
            "text": "In Table 3, we mention BLEU-1, BLEU-2, BLEU3, and BLEU-4 scores for different models in the multi-turn dialogue generation task."
        },
        {
            "heading": "B Cross-lingual Capabilities of BanglaT5",
            "text": "Despite being a monolingual model pretrained on heavily filtered Bangla data, BanglaT5 exhibits strong cross-lingual abilities, particularly in the machine translation (MT) task. In addition to the quality and size of the fine-tuning dataset, this performance can also be attributed to the presence of a significant amount of non-Bangla tokens (\u223c10.3%) in the BanglaT5 vocabulary.\nSince Bhattacharjee et al. (2022) curated the Bangla2B+ corpus by document-level language filtering, these documents preserve foreign text sequences occurring in the Bangla documents. We deliberately maintain these tokens while training the vocabulary of BanglaT5, using a relatively high character coverage. Our rationale behind doing this was to capture code-switching and allow better generalization across languages co-occurring with Bangla, as well as romanized forms of Bangla texts during fine-tuning, which is reflected in the MT results. However, it should be noted that the quality and size of fine-tuning data are essential for a strong cross-lingual performance since the mere existence of foreign tokens in the vocabulary is not enough to produce meaningful generation performance, as demonstrated by the poor performance in the cross-lingual summarization (XLS) task.\nThis phenomenon has been studied in-depth by Blevins and Zettlemoyer (2022) in the context of pretrained language models in English, where they showed that these models develop strong\ncross-lingual transfer capabilities due to the nonnegligible amount of foreign text present in the pretraining data and robustness to UNK tokens during fine-tuning."
        }
    ],
    "title": "BanglaNLG and BanglaT5: Benchmarks and Resources for Evaluating Low-Resource Natural Language Generation in Bangla",
    "year": 2023
}