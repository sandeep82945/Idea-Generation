{
    "abstractText": "Advanced Air Mobility (AAM) is the next generation of air transportation that includes new entrants such as electric vertical takeoff and landing (eVTOL) aircraft, increasingly autonomous flight operations, and small UAS package delivery. With these new vehicles and operational concepts comes a desire to increase densities far beyond what occurs today in and around urban areas, to utilize new battery technology, and to move toward more autonomously-piloted aircraft. To achieve these goals, it becomes essential to introduce new safety management system capabilities that can rapidly assess risk as it evolves across a span of complex hazards and, if necessary, mitigate risk by executing appropriate contingencies via supervised or automated decision-making during flights. Recently, reinforcement learning has shown promise for real-time decision making across a wide variety of applications including contingency management. In this work, we formulate the contingency management problem as a Markov Decision Process (MDP) and integrate the contingency management MDP into the AAM-Gym simulation framework. This enables rapid prototyping of reinforcement learning algorithms and evaluation of existing systems, thus providing a community benchmark for future algorithm development. We report baseline statistical information for the environment and provide example performance metrics.",
    "authors": [
        {
            "affiliations": [],
            "name": "Luis E. Alvarez"
        },
        {
            "affiliations": [],
            "name": "Marc Brittain"
        },
        {
            "affiliations": [],
            "name": "Kara Breeden"
        }
    ],
    "id": "SP:d377fdc74c44e71e35a32f9f683df80f929d0343",
    "references": [
        {
            "authors": [
                "Federal Aviation Administration"
            ],
            "title": "Urban Air Mobility Concept of Operations version 1.0",
            "venue": "2020. [Online]. Available: https://nari.arc.nasa. gov/sites/default/files/attachments/UAM ConOps v1.0.pdf",
            "year": 2020
        },
        {
            "authors": [
                "National Aeronautics",
                "Space Administration"
            ],
            "title": "National Aeronautics and Space Administration (NASA) UAM Vision Concept of Operations (ConOps) UAM Maturity Level (UML)4",
            "venue": "2020. [Online]. Available: https://ntrs.nasa.gov/api/citations/20205011091/ downloads/UAMVisionConceptofOperationsUML-4v1.0.pdf",
            "year": 2020
        },
        {
            "authors": [
                "Federal Aviation Administration"
            ],
            "title": "Urban Air Mobility Concept of Operations version 2.0",
            "venue": "2022. [Online]. Available: https://www.faa.gov/sites/ faa.gov/files/UrbanAirMobilityUAMConceptofOperations2.0 0.pdf",
            "year": 2022
        },
        {
            "authors": [
                "K.K. Ellis",
                "P. Krois",
                "J. Koelling",
                "L.J. Prinzel",
                "M. Davies",
                "R. Mah"
            ],
            "title": "A Concept of Operations (ConOps) of an in-time aviation safety management system (IASMS) for Advanced Air Mobility (AAM)",
            "venue": "AIAA Scitech 2021 Forum, 2021, p. 1978.",
            "year": 2021
        },
        {
            "authors": [
                "J. Kim",
                "P. Sharma",
                "E. Atkins",
                "N. Neogi",
                "E. Dill",
                "S. Young"
            ],
            "title": "Assured contingency landing management for advanced air mobility",
            "venue": "2021 IEEE/AIAA 40th Digital Avionics Systems Conference (DASC), 2021, pp. 1\u201312.",
            "year": 2021
        },
        {
            "authors": [
                "H. Haghighi",
                "D. Delahaye",
                "D. Asadi"
            ],
            "title": "Performance-based emergency landing trajectory planning applying meta-heuristic and dubins paths",
            "venue": "Applied Soft Computing, vol. 117, p. 108453, 2022. [Online]. Available: https://www.sciencedirect.com/science/article/pii/ S1568494622000242",
            "year": 2022
        },
        {
            "authors": [
                "P. V\u00e1\u0148a",
                "J. Sl\u00e1ma",
                "J. Faigl",
                "P. Pa\u010des"
            ],
            "title": "Any-time trajectory planning for safe emergency landing",
            "venue": "2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2018, pp. 5691\u20135696.",
            "year": 2018
        },
        {
            "authors": [
                "E. Ancel",
                "S.D. Young",
                "C.C. Quach",
                "R.F. Haq",
                "K. Darafsheh",
                "K.M. Smalling",
                "S.L. Vazquez",
                "E.T. Dill",
                "R.C. Condotta",
                "B.E. Ethridge",
                "L.R. Teska",
                "T.A. Johnson"
            ],
            "title": "Design and Testing of an Approach to Automated In-Flight Safety Risk Management for sUAS Operations",
            "year": 2022
        },
        {
            "authors": [
                "S. Choudhury",
                "S. Scherer",
                "S. Singh"
            ],
            "title": "Rrt*-ar: Sampling-based alternate routes planning with applications to autonomous emergency landing of a helicopter",
            "venue": "2013 IEEE International Conference on Robotics and Automation, 2013, pp. 3947\u20133952.",
            "year": 2013
        },
        {
            "authors": [
                "M. Warren",
                "L. Mejias",
                "J. Kok",
                "X. Yang",
                "F. Gonzalez",
                "B. Upcroft"
            ],
            "title": "An automated emergency landing system for fixedwing aircraft: Planning and control",
            "venue": "Journal of Field Robotics, vol. 32, no. 8, pp. 1114\u20131140, 2015. [Online]. Available: https: //onlinelibrary.wiley.com/doi/abs/10.1002/rob.21641",
            "year": 2015
        },
        {
            "authors": [
                "D. Silver",
                "A. Huang",
                "C.J. Maddison",
                "A. Guez",
                "L. Sifre",
                "G. Van Den Driessche",
                "J. Schrittwieser",
                "I. Antonoglou",
                "V. Panneershelvam",
                "M. Lanctot"
            ],
            "title": "Mastering the game of Go with deep neural networks and tree search",
            "venue": "nature, vol. 529, no. 7587, pp. 484\u2013489, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "V. Mnih",
                "K. Kavukcuoglu",
                "D. Silver",
                "A.A. Rusu",
                "J. Veness",
                "M.G. Bellemare",
                "A. Graves",
                "M. Riedmiller",
                "A.K. Fidjeland",
                "G. Ostrovski"
            ],
            "title": "Human-level control through deep reinforcement learning",
            "venue": "nature, vol. 518, no. 7540, pp. 529\u2013533, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "O. Vinyals",
                "I. Babuschkin",
                "W.M. Czarnecki",
                "M. Mathieu",
                "A. Dudzik",
                "J. Chung",
                "D.H. Choi",
                "R. Powell",
                "T. Ewalds",
                "P. Georgiev"
            ],
            "title": "Grandmaster level in StarCraft II using multi-agent reinforcement learning",
            "venue": "Nature, vol. 575, no. 7782, pp. 350\u2013354, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "M.W. Brittain",
                "P. Wei"
            ],
            "title": "One to Any: Distributed Conflict Resolution with Deep Multi-Agent Reinforcement Learning and Long Short-Term Memory",
            "venue": "[Online]. Available:",
            "year": 1952
        },
        {
            "authors": [
                "D.-T. Pham",
                "P.N. Tran",
                "S. Alam",
                "V. Duong",
                "D. Delahaye"
            ],
            "title": "Deep reinforcement learning based path stretch vector resolution in dense traffic with uncertainties",
            "venue": "Transportation Research Part C: Emerging Technologies, vol. 135, p. 103463, 2022. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S0968090X21004514",
            "year": 2022
        },
        {
            "authors": [
                "P.N. Tran",
                "D.-T. Pham",
                "S.K. Goh",
                "S. Alam",
                "V. Duong"
            ],
            "title": "An Interactive Conflict Solver for Learning Air Traffic Conflict Resolutions",
            "venue": "Journal of Aerospace Information Systems, vol. 17, no. 6, pp. 271\u2013277, 2020. [Online]. Available: https://doi.org/10.2514/ 1.I010807",
            "year": 2020
        },
        {
            "authors": [
                "M.W. Brittain",
                "X. Yang",
                "P. Wei"
            ],
            "title": "Autonomous Separation Assurance with Deep Multi-Agent Reinforcement Learning",
            "venue": "Journal of Aerospace Information Systems, vol. 18, no. 12, pp. 890\u2013905, 2021. [Online]. Available: https://doi.org/10.2514/1.I010973",
            "year": 2021
        },
        {
            "authors": [
                "W. Guo",
                "M. Brittain",
                "P. Wei"
            ],
            "title": "Safety Enhancement for Deep Reinforcement Learning in Autonomous Separation Assurance",
            "venue": "2021 IEEE International Intelligent Transportation Systems Conference (ITSC), 2021, pp. 348\u2013354.",
            "year": 2021
        },
        {
            "authors": [
                "M. Brittain",
                "P. Wei"
            ],
            "title": "scalable autonomous separation assurance with heterogeneous multi-agent reinforcement learning",
            "venue": "IEEE Transactions on Automation Science and Engineering."
        },
        {
            "authors": [
                "S. Li",
                "M. Egorov",
                "M. Kochenderfer"
            ],
            "title": "Optimizing collision avoidance in dense airspace using deep reinforcement learning",
            "venue": "arXiv preprint arXiv:1912.10146, 2019.",
            "year": 1912
        },
        {
            "authors": [
                "M.J. Kochenderfer",
                "J.P. Chryssanthacopoulos"
            ],
            "title": "Collision avoidance using partially controlled Markov decision processes",
            "venue": "Agents and Artificial Intelligence, J. Filipe and A. Fred, Eds. Springer, 2013, vol. 271, pp. 86\u2013100.",
            "year": 2013
        },
        {
            "authors": [
                "M.P. Owen",
                "A. Panken",
                "R. Moss",
                "L. Alvarez",
                "C. Leeper"
            ],
            "title": "ACAS Xu: Integrated collision avoidance and detect and avoid capability for UAS",
            "venue": "Digital Avionics Systems Conference (DASC). IEEE, 2019, pp. 1\u201310.",
            "year": 2019
        },
        {
            "authors": [
                "L.E. Alvarez",
                "I. Jessen",
                "M.P. Owen",
                "J. Silbermann",
                "P. Wood"
            ],
            "title": "ACAS sXu: Robust decentralized detect and avoid for small unmanned aircraft systems",
            "venue": "Digital Avionics Systems Conference (DASC). IEEE, 2019, pp. 1\u20139.",
            "year": 2019
        },
        {
            "authors": [
                "C.J. Watkins",
                "P. Dayan"
            ],
            "title": "Q-learning",
            "venue": "Machine learning, vol. 8, pp. 279\u2013292, 1992.",
            "year": 1992
        },
        {
            "authors": [
                "W. Dabney",
                "M. Rowland",
                "M. Bellemare",
                "R. Munos"
            ],
            "title": "Distributional reinforcement learning with quantile regression",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, vol. 32, no. 1, Apr. 2018. [Online]. Available: https://ojs.aaai.org/index.php/AAAI/article/ view/11791",
            "year": 2018
        },
        {
            "authors": [
                "M. Andrychowicz",
                "F. Wolski",
                "A. Ray",
                "J. Schneider",
                "R. Fong",
                "P. Welinder",
                "B. McGrew",
                "J. Tobin",
                "O. Pieter Abbeel",
                "W. Zaremba"
            ],
            "title": "Hindsight experience replay",
            "venue": "Advances in Neural Information Processing Systems, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, Eds., vol. 30. Curran Associates, Inc., 2017. [Online]. Available: https://proceedings.neurips.cc/paper files/paper/2017/file/453fadbd8a1a3af50a9df4df899537b5-Paper.pdf",
            "year": 2017
        },
        {
            "authors": [
                "J. Schulman",
                "S. Levine",
                "P. Abbeel",
                "M. Jordan",
                "P. Moritz"
            ],
            "title": "Trust region policy optimization",
            "venue": "Proceedings of the 32nd International Conference on Machine Learning, ser. Proceedings of Machine Learning Research, F. Bach and D. Blei, Eds., vol. 37. Lille, France: PMLR, 07\u201309 Jul 2015, pp. 1889\u20131897. [Online]. Available: https://proceedings.mlr.press/v37/schulman15.html",
            "year": 2015
        },
        {
            "authors": [
                "J. Schulman",
                "F. Wolski",
                "P. Dhariwal",
                "A. Radford",
                "O. Klimov"
            ],
            "title": "Proximal policy optimization algorithms",
            "venue": "arXiv preprint arXiv:1707.06347, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "V. Mnih",
                "A.P. Badia",
                "M. Mirza",
                "A. Graves",
                "T. Lillicrap",
                "T. Harley",
                "D. Silver",
                "K. Kavukcuoglu"
            ],
            "title": "Asynchronous methods for deep reinforcement learning",
            "venue": "Proceedings of The 33rd International Conference on Machine Learning, ser. Proceedings of Machine Learning Research, M. F. Balcan and K. Q. Weinberger, Eds., vol. 48. New York, New York, USA: PMLR, 20\u201322 Jun 2016, pp. 1928\u20131937. [Online]. Available: https://proceedings.mlr.press/v48/mniha16.html",
            "year": 2016
        },
        {
            "authors": [
                "M. Brittain",
                "L.E. Alvarez",
                "K. Breeden",
                "I. Jessen"
            ],
            "title": "Aam-gym: Artificial intelligence testbed for advanced air mobility",
            "venue": "2022 IEEE/AIAA 41st Digital Avionics Systems Conference (DASC), 2022, pp. 1\u201310.",
            "year": 2022
        },
        {
            "authors": [
                "J. Hoekstra",
                "J. Ellerbroek"
            ],
            "title": "BlueSky ATC Simulator Project: an Open Data and Open Source Approach",
            "venue": "Proceedings of the seventh International Conference for Research on Air Transport (ICRAT), 06 2016.",
            "year": 2016
        },
        {
            "authors": [
                "L.E. Alvarez",
                "J.C. Jones",
                "A. Bryan",
                "A.J. Weinert"
            ],
            "title": "Demand and Capacity Modeling for Advanced Air Mobility",
            "year": 2021
        },
        {
            "authors": [
                "G. Brockman",
                "V. Cheung",
                "L. Pettersson",
                "J. Schneider",
                "J. Schulman",
                "J. Tang",
                "W. Zaremba"
            ],
            "title": "OpenAI Gym",
            "venue": "2016. [Online]. Available: https://arxiv.org/abs/1606.01540",
            "year": 2016
        },
        {
            "authors": [
                "E. Liang",
                "R. Liaw",
                "R. Nishihara",
                "P. Moritz",
                "R. Fox",
                "K. Goldberg",
                "J. Gonzalez",
                "M. Jordan",
                "I. Stoica"
            ],
            "title": "RLlib: Abstractions for distributed reinforcement learning",
            "venue": "International Conference on Machine Learning. PMLR, 2018, pp. 3053\u20133062.",
            "year": 2018
        },
        {
            "authors": [
                "A. Nuic",
                "D. Poles",
                "V. Mouillet"
            ],
            "title": "Bada: An advanced aircraft performance model for present and future atm systems",
            "venue": "International journal of adaptive control and signal processing, vol. 24, no. 10, pp. 850\u2013866, 2010.",
            "year": 2010
        },
        {
            "authors": [
                "V. Mouillet",
                "D. Phu"
            ],
            "title": "Bada family h-a simple helicopter performance model for atm applications",
            "venue": "2018 IEEE/AIAA 37th Digital Avionics Systems Conference (DASC). IEEE, 2018, pp. 1\u20138.",
            "year": 2018
        },
        {
            "authors": [
                "J.C. Jones",
                "T. Bonin",
                "E. Mitchell"
            ],
            "title": "Evaluating Wind Hazards for Advanced Air Mobility Operations",
            "year": 2023
        },
        {
            "authors": [
                "O. Yadan"
            ],
            "title": "Hydra - a framework for elegantly configuring complex applications",
            "venue": "Github, 2019. [Online]. Available: https://github.com/ facebookresearch/hydra",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "I. INTRODUCTION\nAdvanced Air Mobility (AAM) is a rapidly evolving field of aviation that has the potential to revolutionize air transportation. AAM encompasses transportation of people and cargo (e.g., passenger shuttling and package delivery) between local, regional, and urban locations with a diverse set of vehicle types which includes small uncrewed aerial systems (sUAS) and larger occupied vehicles such as autonomous conventional takeoff and landing (CTOL) and vertical takeoff and landing (VTOL) aircraft. Due in part to the envisioned high density of\nDISTRIBUTION STATEMENT A. Approved for public release. Distribution is unlimited. This material is based upon work supported by the National Aeronautics and Space Administration under Air Force Contract No. FA8702-15-D-0001. Any opinions, findings, conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Aeronautics and Space Administration. \u00a9 2023 Massachusetts Institute of Technology. Delivered to the U.S. Government with Unlimited Rights, as defined in DFARS Part 252.227-7013 or 7014 (Feb 2014). Notwithstanding any copyright notice, U.S. Government rights in this work are defined by DFARS 252.227-7013 or DFARS 252.227-7014 as detailed above. Use of this work other than as specifically authorized by the U.S. Government may violate any copyrights that exist in this work.\noperations, AAM requires increasingly autonomous operations to enable operational efficiency and reduce risk in the airspace.\nPhases of AAM operations are described by the FAA and NASA as evolving in UAM Maturity Levels (UML) that range from 1 to 6 with increasing levels of expected densities and complexities of roles and responsibilities [1], [2], [3]. For example, UML 1 envisions a low-density human piloted airspace environment, whereas UML 4+ envisions 100s of simultaneous operations over a local region with a mix of human and autonomously piloted aircraft. UML 4+ expects high density, dynamic, and autonomous operations enabled by artificial intelligence (AI) based decision-making algorithms. Therefore, it is important to understand the trade-offs of various AI algorithms by comparing their performance across a range of safety-critical situations.\nContingency management (CM) is a capability within a larger umbrella referred to by NASA and the National Academies as In-Time Aviation Safety Management Systems (IASMS) [4], [5]. The scope of CM includes (a) planning a set contingencies prior and/or during flight, (b) ability to decide which contingency to execute based on available information (e.g., aircraft or system states, hazard level or likelihood) and (c) execution of contingency. In particular, IASMS CM capability has focused on path planning techniques that have an online system monitor various hazards, namely collisions (airborne or ground), loss of power, loss of control, and loss of flight capability [4].\nTo reduce the probability of entering an unsafe state, the majority of these online systems use path planning heuristic algorithms. Predominant algorithms explored are Monte Carlo approaches of path planning where the algorithm finds an optimal path within a time horizon or explores a set of alternate trajectories given the aircraft\u2019s dynamics constraints [6], [7], [8]. Example path planning approaches utilize variations of Rapidly-Exploring Random Tree (RRT) and A* to generate an optimized path based on distance or other environmental metrics (e.g., surrounding air traffic). Parameter training for these approaches leverage techniques from machine learning such as bagging methods (e.g., bootstrap sampling) with uncertainties represented by random distributions or pre-trained Bayesian belief networks [9], [10], [11]. A drawback of these techniques is the requirement for subject matter experts to\nar X\niv :2\n31 1.\n10 80\n5v 1\n[ cs\n.L G\n] 1\n7 N\nov 2\n02 3\ncreate and bound the heuristics used by the model. Reinforcement Learning (RL), a subset of machine learning, alleviates the drawbacks of the above approaches by learning a policy (akin to heuristics) through repeated interaction with an environment to achieve a specified objective. Recently, RL has shown promise in many challenging games such as Go, Atari, Warcraft, and, most recently, Starcraft II with beyond humanlevel performance [12], [13], [14]. In addition, RL has been applied to many real-world air transportation problems such as conflict detection and resolution [15], [16], [17], autonomous separation assurance [18], [19], [20], and collision avoidance [21]. Furthermore, the Airborne Collision Avoidance System X (ACAS X) family of systems proved RL agents can improve safety and reduce nuisance alerts in real-world operation compared to the heuristic-based systems, in particular the Traffic Alert and Collision Avoidance System (TCAS) that is currently mandated on all commercial aircraft [22], [23], [24], [25]. These notable advancements demonstrate the capability of computational learning algorithms to potentially augment and facilitate human tasks in real-world environments.\nIn this work, a framework is proposed to enable the development and evaluation of RL-based contingency management designs, particularly those that constrain contingency actions to a limited and deterministic set of highly assured maneuvers. This is a principle of the run-time assurance concept used for safety-critical systems. A set of metrics are introduced to evaluate the nominal (unequipped) performance of the framework, providing a baseline for RL research."
        },
        {
            "heading": "II. BACKGROUND",
            "text": "RL is one type of sequential decision making where the objective is to learn a policy in a given environment. RL requires the environment to be formulated as a Markov Decision Process (MDP), a mathematical framework for modeling decision making processes with stochastic transitions. An MDP can be defined by the five-tuple (S,A,R, T, \u03b3), where S is the state space, A is the action set, R is the reward function R(s, a) providing the reward for taking action a from state s, and T is the probability transition function, T (s, a, s\u2032), that produces the probability of transition from state s to the next state s\u2032 when taking action a. In RL, the transition matrix T is often unknown. The discount factor \u03b3 determines how far in the future to look for rewards. As \u03b3 \u2192 0, immediate rewards are emphasized, whereas, when \u03b3 \u2192 1, future rewards are prioritized.\nFrom this formulation, the RL agent is able to derive an optimal policy in the environment by maximizing a cumulative reward function. Let \u03c0 represent some policy and \u03c4 represent the total time for a given environment, then the optimal policy can be defined as follows:\n\u03c0\u2217 = argmax \u03c0 E[ \u03c4\u2211 t=0 (r(st, at)|\u03c0)]. (1)\nBy designing the reward function to reflect the objective in the environment, the optimal solution can be obtained by maximizing the total reward.\nIn environments with a low dimensional state-action space, or environments with discrete state-actions values, the policy \u03c0 can be represented as a look-up table and solved using dynamic programming approaches such as Q-learning [26]. However, in many real-world environments the state-action space can not be represented by discrete values, or the dimensionality of the state-action space is too large to construct a look-up table. Deep reinforcement learning (DRL) alleviates the aforementioned issues by introducing a neural network to represent the policy \u03c0. Various approaches exist for training DRL agents including value-based methods [13], [27], [28], and policy-based optimization [29], [30], [31].\nLarge scale simulation capabilities are essential for RL algorithms as they explore the transitions between states as a consequence of their actions within the modeled environment. Recently, MIT Lincoln Laboratory introduced the AI testbed for advanced air mobility, or AAM-Gym to provide a representative fast-time simulation environment for research and development of decision making tools for AAM concepts of operation [32]. By leveraging AAM-Gym, new RL use-cases, such as contingency management, can be developed to explore the trade-offs of AI/RL-based decision-making functions with existing approaches. The proposed RL contingency management framework utilizes AAM-Gym [32] with BlueSky [33] as the back-end simulation capability to explore the impact of RL algorithm choice, hazards (e.g., wind fields, rate of loss of navigation, battery discharge prediction failures), and aircraft parameters on the overall training and effectiveness of agents. By utilizing AAM-Gym, the state space of each agent can make use of all the aircraft and environment information if desired, as seen in Tables I and II.\nIn order to provide representative real world scenarios to AAM-Gym, we leverage the Lincoln Laboratory UAMToolKit framework; an event-driven simulation environment that can generate spatio-temporal traffic operations with a diverse set of flight routes [34]. When modeling Urban Air Mobility (UAM) operations, time variant cycles of demand across the New York City metropolitan area are provided by estimating the demand as a percentage of the expected replacement by AAM of forhire-vehicle (FHV) operations (e.g. Uber, Lyft, taxi, limousine services). The event driven model outputs a complete history of aircraft movements and metadata, which includes departure times, cruising speeds, arrival times, flight route (i.e., flight level and waypoints), origin-destination pair, and aircraft type. UAMToolKit can be queried by AAM-Gym allowing for vast amounts of unique and diverse traffic patterns for the RL agents to train and evaluate against."
        },
        {
            "heading": "III. APPROACH",
            "text": "In this section, the expansions of AAM-Gym for the RL contingency management use-case are described as well as the MDP formulation."
        },
        {
            "heading": "A. Traffic Generation",
            "text": "When modeling UAM operations, UAMToolKit requires information about the fleet size, aircraft capacity of each\nvertiport, vehicle passenger capacity, cruising speeds, the corridor network for operation, distributions of turn-around time, and wind data. The focus of this research is on contingency management during flight and not scheduling impacts due to turn-around and wind data. Therefore, operations are modeled with zero wind field and a one minute of turn-around time, the minimum simulation step. Operations occur between eight altitude lanes evenly spaced from 1,000\u20135,000 ft utilizing the existing helicopter network as depicted in [34]. Passenger capacity is maintained at four passengers, and 50\u2013500 aircraft are attempting to meet 5% of the FHV demand, with landing pad spaces in each vertiport maintained so all aircraft have a landing pad at the beginning of the simulation and an additional landing pad for emergency. For example, in the case of the 500 fleet size scenario, each of the 29 vertiports selected in [34] would have 18 landing pads."
        },
        {
            "heading": "B. Algorithms",
            "text": "AAM-Gym allows researchers to develop, train, and evaluate RL algorithms that adhere to the OpenAI Gym [35] protocol, allowing for rapid prototyping of new and existing algorithms. AAM-Gym has support for both general RL based frameworks such as Ray\u2019s RL-Lib [36] and air transportation based RL algorithms such as the Deep Distributed Multi-Agent Variable with Attention (D2MAV-A) [15]."
        },
        {
            "heading": "C. Hazard Modeling",
            "text": "The hazards initially modeled by the RL framework include: uncertainty in battery consumption, max charge capacity due to number of charge cycles, wind information, and navigation capability. Other hazards will be modeled in the future.\nAs part of the RL contingency management use-case, two energy consumption rate models were developed. The energy consumption rate is modeled within the RL framework as either a linear consumption model or a mixture of a BADA 3 and Bada H [37], [38] based energy consumption model during cruising phases with a linear model at speeds below 20 knots.\nThe linear energy consumption model is represented by (2) where \u03b1 is the constant energy consumption rate per time step, \u2206t is the time step between simulation iteration, C is the total number of charge cycles an aircraft has experienced, and \u03b2 is the number of charge cycles when uncertainty in consumption begins to occur. The uncertainty in consumption rate after \u03b2 charge cycles is modeled as a truncated Gaussian distribution, N , of range [0, 1] for values above \u03d5.\n\u2206E = { \u03b1 \u00b7\u2206t if C \u2264 \u03b2 \u03b1 \u00b7\u2206t+N if C > \u03b2 . (2)\nWhen each aircraft is created, it is randomly assign a value for the number of charge cycles, C, by a uniform distribution of a given range. The model then uses the defined minimum energy level (Emin), maximum energy level (Emax), minimum charge cycles (Cmin), and maximum charge cycles (Cmax) to create a linear fit and assign the maximum energy level possible given the number of cycles the aircraft has experienced (example shown in Fig. 1).\nThe wind fields can be provided as constant wind fields, input data from Weather Research Forecast (WRF) model [39], or as multivariate distributions. When providing discrete locations for wind field information, the BlueSky simulation environment uses multi-linear interpolation to provide the wind vector at specified locations.\nThe probability of navigation loss, Pnav , can also be modeled as a multivariate distribution providing the probability of losing navigation at specified latitude and longitudes or as a discrete event occurring with a set probability."
        },
        {
            "heading": "D. Reward Model and Terminal States",
            "text": "As previously presented, the RL framework uses an MDP to construct the contingency management problem where the researchers define the state space, actions, transitions, and rewards for each agent. In this framework the transition model is unknown as its dictated by the simulation environment and the hazard parameter chosen. Therefore, it is not explicitly\ndefined by the researchers. The action space of the MDP evaluated is shown in Table III. The available actions are defined as follows:\n1) Heading change - The aircraft turns left or right by a pre-defined magnitude (e.g., \u00b15\u25e6, 0\u25e6) and continues on this heading until a new action is chosen. 2) Land now - The aircraft chooses to execute a controlled vertical decent from present position. 3) No alert - The aircraft takes no action and continues on its current heading or active flight plan. 4) Use assigned flight route - The aircraft continues on its planned flight route or returns to the nearest waypoint, if prior actions caused a deviation, and continues along flight route.\nThe reward function utilized by the MDP is defined by\nR(st, ht, at) = R(st) +R(ht) +R(at)\u2212 \u2126, (3)\nwhere R(st), R(ht), and R(at) are defined by (4), (5), (7) and \u2126 is a step penalty. R(st) is a state dependant reward function based on the terminal states of the aircraft as defined by (4). The RL framework allows for three types of terminal states to be defined as total loss of energy, loss of navigation, and when an aircraft reaches an altitude of zero feet, each respectively adding a penalty \u03b4energy, \u03b4navigation, and \u03b4range to destination.\nR(st) = \u03b4energy + \u03b4navigation + \u03b4range to destination (4)\nA multivariate Gaussian distribution is used to define the rewards of landing at destination and alternative vertiports. It is defined as\nR(ht) = n\u2211 i=0 \u03b4vertiport \u00b7 e (X\u2212\u00b5xi) 2+(Y \u2212\u00b5yi) 2 \u22122\u03c32 (5)\nwhere X and Y are the latitude and longitude of the aircraft under control. The latitude and longitude of each vertiport are defined by \u00b5x and \u00b5y respectively. The standard deviation \u03c3 is set to 0.0005\u25e6. The variable \u03b4i is defined by\n\u03b4vertiport ={ \u03b4vertiport destination if vertiport = destination \u03b4vertiport other if vertiport \u0338= destination . (6)\nLast, R(at) is defined by\nR(at) ={ \u03b4land + \u03b4action penalty if at = \u201cLand now\u201d \u03b4action penalty if at \u0338= \u201cNo alert\u201d , (7)\nwhere \u03b4action penalty is a penalty applied when any action is chosen other than \u201cNo alert\u201d and \u03b4land is the penalty for choosing to \u201cLand now\u201d."
        },
        {
            "heading": "E. Metrics",
            "text": "The RL framework extends the capabilities of AAM-Gym and allows users to define performance metrics to help assess the CM algorithm within a Hydra configuration file [40] leveraging the aircraft and environment information, as defined in Table I and II. In addition, the following information was recorded: spatial distance from corridor, number of aircraft that reached their destination, energy level of each aircraft, total reward experienced by each aircraft, average wind in scenario, time spent in flight route, and actions taken."
        },
        {
            "heading": "IV. NUMERICAL EXPERIMENT AND RESULTS",
            "text": "Preliminary experiments conducted to date are described in this section with the parameters explored and their resulting impact on the operations. Additional experiments are planned."
        },
        {
            "heading": "A. Parameter Sweep",
            "text": "The Hydra configuration interface of AAM-Gym allows for multiple experiments to be launched simultaneously to explore the impact of environment decisions and rewards. This experiment explores the impact of modeling an increased consumption rate on the battery model, differences in maximum charge capacity, and activating probability of loss of navigation, Pnav , per aircraft per simulation step. The parameters that were explored and the range of values chosen are shown in Table IV. For this paper we explore the impact of each of these parameters on the likelihood that an operation will reach its destination or other assigned vertiports as a function of training steps."
        },
        {
            "heading": "B. Results",
            "text": "When no RL-based CM agent is in control of the aircraft (i.e., unequipped) the likelihood of reaching their destination is strongly correlated to the maximum initial energy level, Emax. This trend is evident by the higher fraction of aircraft reaching the destination, P\u0304dest, as seen in Fig. 2. Due to all AAM-Gym parallel workers initializing their respective simulations at the same time with a low number of aircraft, we see a spike in fraction of aircraft reaching their destination in Fig. 2. As the AAM-Gym parallel workers continue the simulation, the number of aircraft stabilizes, thus creating a steady number of\naircraft reaching their destination as a function of Emax and Pnav .\nThis is expected as operations with lower energy levels are at risk of quickly depleting their energy reserves during flight without reaching any vertiport if an agent does not dictate an action to undertake. The unequipped results also show the number of aircraft reaching their final destination decreases when the probability of navigation loss, Pnav , is above zero, as seen in Table V. A change in Pnav from 0 to 1 \u00d7 10\u22125 results in an decrease of aircraft arrivals by 0.12% to 0.25% as it increases the likelihood of entering a terminal state."
        },
        {
            "heading": "V. CONCLUSION",
            "text": "In this work, a RL framework was developed to enable standardized research and development in the area of contingency management for AAM. It is shown how the problem is formulated as a Markov Decision Process and through using AAM-Gym and UAMToolkit, a representative simulation environment is established for training and validation of RL agents. Preliminary results are presented to demonstrate the impact of various simulation parameters that induce a contingency event.\nIn future work, further analysis of the reward model, MDP environment structure, and algorithm design is required to assess the viability of RL algorithms in contingency manage-\nment. While baseline results were presented in this study, future work will explore various state-of-the-art RL approaches to understand the viability of learning-based algorithms. After a broad analysis of the performance of these algorithms, a selected set will be tuned through the use of surrogate optimization. In addition, the simulation environment will be further refined with the introduction of crowd source population databases for causality penalties, and use of eVTOL vehicle performance models with improved battery models."
        },
        {
            "heading": "VI. ACKNOWLEDGMENT",
            "text": "The authors would like to thank Steve Young of NASA for his guidance and support of this effort. The work was funded by NASA\u2019s System-Wide Safety Project via an Interagency Agreement with the US Air Force (IA 80LARC23TA002, Project 10384)."
        }
    ],
    "title": "Towards a Standardized Reinforcement Learning Framework for AAM Contingency Management",
    "year": 2023
}