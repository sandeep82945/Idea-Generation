{
    "abstractText": "In this work, we study the performance of wide-used keypoints detection and description algorithms: Scale-Invariant Feature Transform (SIFT), Speeded Up Robust Features (SURF), Oriented FAST and Rotated BRIEF (ORB), Binary Robust Invariant Scalable Keypoints(BRISK), Accelerated KAZE(AKAZE), which were originally developed for images taken in visible light but widely applied in the fields where images are taken in a different spectrum.We compare the quality of algorithms and their robustness to various image transformations. The algorithms\u2019 performance is tested on two image sets in the different spectra: digital X-Ray images and images taken in the visible spectrum. Each dataset captures complex scenes with many objects and partial occlusions. Geometrical transformations (rotation, shearing, scaling), linear color transformations, Gaussian blur are applied to the images. Then the detection and description algorithms are tested on the original and transformed images. The repeatability and number of corresponding points are calculated to assess detection algorithms. The ratio of correctly matched descriptors together with the ratio of the distances between the query descriptor, the nearest descriptor, and the second matched descriptor is computed to evaluate descriptors\u2019 quality. The algorithms showed different behavior on different spectra. SURF demonstrated to be the best X-ray keypoint detector and for the visible spectrum, it shares first place with AKAZE detector. SIFT is the best descriptor in both spectra. The strong and weak points of each algorithm are discussed in the paper. INDEX TERMS Keypoints, repeatability, robustness, digital X-ray images, computed tomography, CT, detectors, descriptors.",
    "authors": [
        {
            "affiliations": [],
            "name": "MIKHAIL CHEKANOV"
        },
        {
            "affiliations": [],
            "name": "OLEG SHIPITKO"
        },
        {
            "affiliations": [],
            "name": "NATALIA SKORYUKINA"
        }
    ],
    "id": "SP:2fc7bffcef8c6ce230915dfe6e6861f6cb150a24",
    "references": [
        {
            "authors": [
                "R.M. Haralick",
                "L.G. Shapiro"
            ],
            "title": "Connected components labeling",
            "venue": "Comput. Robot. Vis., vol. 1, pp. 28\u201348, Oct. 1992.",
            "year": 1992
        },
        {
            "authors": [
                "V. Lepetit",
                "P. Fua"
            ],
            "title": "Keypoint recognition using randomized trees",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 28, no. 9, pp. 1465\u20131479, Sep. 2006.",
            "year": 2006
        },
        {
            "authors": [
                "N.S. Skoryukina",
                "A.N. Milovzorov",
                "D.V. Polevoy",
                "V.V. Arlazarov"
            ],
            "title": "Paintings recognition in uncontrolled conditions using one-shot learning",
            "venue": "Trudy ISA RAN, vol. 68, pp. 5\u201314, May 2018, doi: 10.14357/20790279180501.",
            "year": 2018
        },
        {
            "authors": [
                "S. Agarwal"
            ],
            "title": "Building Rome in a day",
            "venue": "Commun. ACM, vol. 54, no. 10, pp. 105\u2013112, Oct. 2011.",
            "year": 2011
        },
        {
            "authors": [
                "D. Scaramuzza"
            ],
            "title": "Performance evaluation of 1-point-RANSAC visual odometry",
            "venue": "J. Field Robot., vol. 28, no. 5, pp. 792\u2013811, Sep. 2011.",
            "year": 2011
        },
        {
            "authors": [
                "D.G. Lowe"
            ],
            "title": "Distinctive image features from scale-invariant keypoints",
            "venue": "Int. J. Comput. Vis., vol. 60, no. 2, pp. 91\u2013110, 2004.",
            "year": 2004
        },
        {
            "authors": [
                "E. Rublee",
                "V. Rabaud",
                "K. Konolige",
                "G. Bradski"
            ],
            "title": "ORB: An efficient alternative to SIFT or SURF",
            "venue": "Proc. Int. Conf. Comput. Vis., Nov. 2011, pp. 2564\u20132571.",
            "year": 2011
        },
        {
            "authors": [
                "P.F. Alcantarilla",
                "A. Bartoli",
                "A.J. Davison"
            ],
            "title": "Kaze features",
            "venue": "Proc. Eur. Conf. Comput. Vis. Berlin, Germany: Springer, 2012, pp. 214\u2013227.",
            "year": 2012
        },
        {
            "authors": [
                "H. Bay",
                "T. Tuytelaars",
                "L.V. Gool"
            ],
            "title": "Surf: Speeded up robust features",
            "venue": "Proc. Eur. Conf. Comput. Vis. Berlin, Germany: Springer, 2006, pp. 404\u2013417.",
            "year": 2006
        },
        {
            "authors": [
                "M. Rodriguez",
                "G. Facciolo",
                "R.G. Von Gioi",
                "P. Muse",
                "J.-M. Morel",
                "J. Delon"
            ],
            "title": "SIFT-AID: Boosting sift with an affine invariant descriptor based on convolutional neural networks",
            "venue": "Proc. IEEE Int. Conf. Image Process. (ICIP), Sep. 2019, pp. 4225\u20134229.",
            "year": 2019
        },
        {
            "authors": [
                "Z. Song andR.Klette"
            ],
            "title": "Robustness of point feature detection",
            "venue": "inProc. Int. Conf. Comput. Anal. Images Patterns. Berlin, Germany: Springer, 2013, pp. 91\u201399.",
            "year": 2013
        },
        {
            "authors": [
                "K. Mikolajczyk",
                "T. Tuytelaars",
                "C. Schmid",
                "A. Zisserman",
                "J. Matas",
                "F. Schaffalitzky",
                "T. Kadir",
                "L. Van Gool"
            ],
            "title": "A comparison of affine region detectors",
            "venue": "Int. J. Comput. Vis., vol. 65, no. 1, pp. 43\u201372, 2005.",
            "year": 2005
        },
        {
            "authors": [
                "J. Hu",
                "X. Peng",
                "C. Fu"
            ],
            "title": "A comparison of feature description algorithms",
            "venue": "Optik, vol. 126, no. 2, pp. 274\u2013278, Jan. 2015.",
            "year": 2015
        },
        {
            "authors": [
                "S. Akhmedkhan",
                "S. Gladilin",
                "E.A. Shvets"
            ],
            "title": "Optical-to-SAR image registration using a combination of CNN descriptors and cross-correlation coefficient",
            "venue": "Proc. 12th Int. Conf. Mach. Vis. (ICMV), Jan. 2020, Art. no. 114331.",
            "year": 2020
        },
        {
            "authors": [
                "M. Moradi",
                "P. Abolmaesoumi",
                "P. Mousavi"
            ],
            "title": "Deformable registration using scale space keypoints",
            "venue": "Proc. SPIE Med. Imag. Process., vol. 6144, Aug. 2006, Art. no. 61442G.",
            "year": 2006
        },
        {
            "authors": [
                "V.V. Volkov",
                "E.A. Shvets"
            ],
            "title": "Dataset and method for evaluating optical+to+sar image registration algorithms based on keypoints",
            "venue": "Inf. Tekhnol. Syst., vol. 2, pp. 44\u201357, Oct. 2021, doi: 10.14357/20718632210205.",
            "year": 2021
        },
        {
            "authors": [
                "F. Lecron",
                "M. Benjelloun",
                "S. Mahmoudi"
            ],
            "title": "Descriptive image feature for object detection in medical images",
            "venue": "Proc. Int. Conf. Image Anal. Recognit. Berlin, Germany: Springer, 2012, pp. 331\u2013338.",
            "year": 2012
        },
        {
            "authors": [
                "M.O. Chekanov",
                "O.C. Shipitko",
                "E.I. Ershov"
            ],
            "title": "1-point ransac for axial rotation angle estimation by tomographic projections",
            "venue": "Sensory Syst., vol. 34, no. 1, pp. 72\u201386, 2020, doi: 10.31857/S0235009220010060.",
            "year": 2020
        },
        {
            "authors": [
                "D. Mery",
                "E. Svec",
                "M. Arias",
                "V. Riffo",
                "J.M. Saavedra",
                "S. Banerjee"
            ],
            "title": "Modern computer vision techniques for X-ray testing in baggage inspection",
            "venue": "IEEE Trans. Syst., Man, Cybern. Syst., vol. 47, no. 4, pp. 682\u2013692, Apr. 2017.",
            "year": 2017
        },
        {
            "authors": [
                "V. Balntas",
                "K. Lenc",
                "A. Vedaldi",
                "K. Mikolajczyk"
            ],
            "title": "HPatches: A benchmark and evaluation of handcrafted and learned local descriptors",
            "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jul. 2017, pp. 5173\u20135182.",
            "year": 2017
        },
        {
            "authors": [
                "D. Mery",
                "V. Riffo",
                "U. Zscherpel",
                "G. Mondrag\u00f3n",
                "I. Lillo",
                "I. Zuccar",
                "H. Lobel",
                "M. Carrasco"
            ],
            "title": "GDXRay: The database of X-ray images for nondestructive testing",
            "venue": "J. Nondestruct. Eval., vol. 34, no. 4, pp. 1\u201312, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "S. Leutenegger",
                "M. Chli",
                "R.Y. Siegwart"
            ],
            "title": "BRISK: Binary robust invariant scalable keypoints",
            "venue": "Proc. Int. Conf. Comput. Vis., Nov. 2011, pp. 2548\u20132555.",
            "year": 2011
        },
        {
            "authors": [
                "M. Chekanov",
                "O. Shipitko"
            ],
            "title": "X-ray and visible spectra circular motion images dataset",
            "venue": "2019, arXiv:1909.13730.",
            "year": 2019
        },
        {
            "authors": [
                "G. Bradski"
            ],
            "title": "The OpenCV Library",
            "venue": "Softw. Tools Prof. Programmer, vol. 25, no. 11, pp. 120\u2013123, 2000.",
            "year": 2000
        }
    ],
    "sections": [
        {
            "text": "INDEX TERMS Keypoints, repeatability, robustness, digital X-ray images, computed tomography, CT, detectors, descriptors.\nI. INTRODUCTION In computer vision systems a keypoint is a projection of a point of a three-dimensional scene onto the image plane, which meets the following requirements [1]:\n1) distinctness - a specific point must clearly stand out from the background and be distinguishable (unique) in its vicinity; 2) invariance - the definition of a singular point must be resistant to affine transformations; 3) stability - The definition of a special point must be robust to noise and errors;\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Hengyong Yu .\n4) uniqueness - In addition to being locally distinct, a feature must be globally unique to improve the discernibility of repeating patterns; 5) interpretability - Feature points should be defined so that they can be used to analyze correspondences and identify interpretable information from the image.\nKeypoints are used to solve problems such as the body pose estimation [2], object classification [3], three-dimensional scenes reconstruction [4], visual odometry and navigation [5], image registration, radiometric correction and many others. In addition to the task of keypoints detection, there is the task of keypoint description which is performed based on the detected point\u2019s vicinity to compare it with keypoints in other images. The algorithm that solves the first problem is called a detector, and the second one \u2013 a descriptor.\n38964 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ VOLUME 10, 2022\nDue to the practical importance of keypoints, a variety of detection and description algorithms have been proposed [6]\u2013[9]. These algorithms are based on classical image processing techniques. In addition, detectors and descriptors based on neural networks are actively developed [10]. In this paper, only classical algorithms for the detection and description of keypoints are considered. They all use different approaches to define informative image areas. For this reason, these algorithms show different behavior and quality depending on the image structure.Manyworks are devoted to the keypoints comparison. Among them, the works of Song and Klette [11] and Mikolajczyk et al. [12], which compare different detection algorithms on geometrically and photometrically transformed images. In the work of Hu et al. [13] the description algorithms are compared on a set of images obtained from different viewpoints. All mentioned studies investigate the quality of detectors and descriptors only on images in the visible spectrum. At the same time, keypoints are applied in many problems where images have a different nature [14]\u2013[16] and, in particular, images obtained in the X-ray spectrum. Thus, in [17], keypoints are used to detect vertebrae on X-ray images. In the article [18], keypoints are used to restore the trajectory of the circular motion of the tomograph from the tomographic projections made by it. The work [19] considers the problem of prohibited items detection on X-ray images of baggage and uses keypoints to detect an object.\nWhen X-rays pass through the material, they are attenuated. Digital X-ray images are generated by recording this attenuation along the beam corresponding to each pixel of the detector matrix. Thus, the principle of formation of X-ray images differs significantly from the formation of images in the optical spectrum, which records information about the intensity of electromagnetic signal reflected from an object. Unlike images made in the visible spectrum, X-ray images are characterized by the translucency of the recorded object, the absence of textures, and low image sharpness. Therefore the applicability and robustness of detectors and descriptors designed to work with images of other properties on digital X-ray images need to be studied. The authors are not aware of the works in which this issue was investigated. The purpose of this work is to study the difference in performance and robustness of wide-used keypoints detection and description algorithms for digital X-ray and the visible spectra images and for various photometric.\nFor the study, we use two datasets: in the visible (HPatches [20]) and X-ray spectra (GDXray [21]). Various affine transformations (rotation, scaling, bevel), color correction (changes in brightness and contrast), and Gaussian blur were applied to the images. On pairs of original and transformed images, we tested five detection and description algorithms: SIFT, SURF, ORB, BRISK, AKAZE, and compared quality metrics separately for X-ray and visible spectrum images.\nThe main contributions of this paper are:\n\u2022 The article for the first time carefully studies the effect which different nature of X-ray images compared to the visible spectra images takes on the quality and robustness of keypoints detection algorithms. \u2022 The robustness of the algorithms to affine transformations, as well as to changes in lighting conditions, was investigated in both spectra. \u2022 Best detector and descriptor algorithms were choosed: AKAZE detector, SIFT descriptor for the visible spectrum, SURF detector, SIFT descriptor for the digital X-ray images.\nThe findings of the article might be used to choose the propper algorithm for applications in which the X-ray images are analyzed.\nII. COMPARED ALGORITHMS We start with the description of the keypoints detection and description algorithms compared in this study.\nSIFT (Scale Invariant Feature Transform) was proposed in [6]. At the detection stage, the algorithm builds an image pyramid \u2013 an ordered sequence containing the original image and its reduced copies, calculates the difference of Gaussians of its layers, and then finds local extrema among the calculated values. The SIFT descriptor is a histogram of the orientations of the image gradients in the area around the detected feature point. The area around the keypoint is divided into 4 \u00d7 4 sub-areas, in each, a histogram of the gradient directions is built (each histogram has 8 intervals). The histograms are combined and normalized. The resulting vector is a keypoint descriptor.\nSURF (Speeded-Up Robust Features), described in [9], was designed based on SIFT. The main goal of the authors was to offer a less computationally expensive algorithm and, at the same time, to obtain the quality comparable to SIFT. Like SIFT, SURF calculates the difference of Gaussians of the image pyramid layers, but unlike SIFT, instead of calculating the difference of Gaussians, it approximates the Laplacian of Gaussian using a set of wavelets. During the description, the area around the keypoint is divided into 4\u00d7 4 sub-areas. The horizontal and vertical wavelets values are then calculated and weighted for the grids of pixels 5 \u00d7 5 within each subregion. After that, the sums of the horizontal and vertical wavelet values, as well as the sums of their modules, are calculated. The values of these sums of each sub-region are combined into a vector that describes a keypoint.\nORB (Oriented FAST and Rotated BRIEF) was proposed in [7]. The ORB detector considers a point to be a keypoint if, at a certain image scale, on a circle of fixed radius, namely 9 pixels, centered at the point, there is a sequence of pixels that are darker or lighter than the central pixel. At the stage of description, the ORB compares 256 pairs of pixels of the smoothed image patch rotated according to keypoint\u2019s orientation. Pairs are obtained from a fixed sample of the Gaussian distribution around the keypoint\u2019s center. If the first pixel of the i-th pair is darker than the second, then 1 is written to the i-th bit of the descriptor, otherwise 0.\nVOLUME 10, 2022 38965\nBRISK (Binary Robust Invariant Scalable Keypoints) [22] was created as an algorithm comparable in quality to SURF, but computationally more efficient. The BRISK detection stage is similar to ORB, but it discards the points at which the FAST-score (the sum of the absolute values of the difference between the pixels of the circle with the central pixel) is not a local maximum in the image pyramid. The BRISK descriptor is similar to ORB but uses a different pattern of matched pixel pairs.\nAKAZE (Accelerated-KAZE) is presented in [8].\nL iHessian = \u03c3 2 i,norm(L i xxL i yy \u2212 L 2 xy),\nwhere L ixx , L i yy, L 2 xy are the second-order derivatives of the i-th image, \u03c3 2i,norm is the coefficient determined by the scale of the image on which the function value is calculated. The description stage of AKAZE algorithm is similar to ORB and BRISK, with the difference that the intensities are not compared for individual pixels, but the average value of some image areas.\nIII. MATERIALS AND METHODS This section describes data used for experiments and algorithms evaluation methodology.\nIdeally, to compare keypoints algorithms in different spectra we would need a set of images of the same scenes taken in both spectrums. Unfortunately, there is no known to authors public dataset meeting this requirement. In our previous work [23] we presented such a dataset, however, it contains images of a single object and therefore is not extensive enough to make a reasonable conclusion. Thus, in this work algorithms were tested on two common datasets with a large number of scenes in each spectrum (see Fig. 1). For visible spectrum Homography-patches dataset (HPatches [20]) was used. It is widely used to study keypoints detection and description algorithms. For X-ray spectrum GDXray dataset [21] was used. To perform fair comparison of spectrum influence on algorithms quality all images were scaled with saving ratio so the large side is 407 pixels. Note that we do not specify whether the height or width of the image is the larger side, since the datasets we use have both vertically and horizontally oriented images. Also, only part of GDXray dataset was used \u2013 sequences 46-48 with baggage, as they are rich for features, so all the algorithms can detect enough keypoints for a fair comparison.\nA. DETECTORS EVALUATION Let there are two images: I1 and its transformed version I2.H \u2013 transformation converting I1 to I2. Fig. 2 shows examples of images transformations and corresponding transformation expressions. To compare the performance of detectors on a pair of images I1, I2, the so-called repeatability was calculated [12]:\nr = Ncorrespond min(K1,K2) , (1)\nwhere Ncorrespond is the number of keypoints matched between two images,K1,K2 are the number of points detected in the first and second images, respectively.\nLet R\u00b5a be a circle area around a keypoint on I1. The diameter of the region is equal to the size of the keypoint\u2019s region used to compute it\u2019s descriptor. Let alsoRHT\u00b5bH be the elliptic area around the keypoint on I2 projected onto I1 by the inverse mapping H\u22121. Points are considered matched if the ratio of the intersection area of two eleptic areas to their union is greater than a certain threshold 1\u2212 \u03b5 (see Fig. 3):\n1\u2212 R\u00b5a \u2229 RHT\u00b5bH R\u00b5a \u222a RHT\u00b5bH < \u03b5. (2)\nIn all experiments \u03b5 was equal to 0.4 \u2013 the default value used in OpenCV implementation.\n38966 VOLUME 10, 2022\nNote that when performing turn and shearing transforms some image points go beyond the fixed image boundaries. Points that are lost in the transformed image are not taken into account when calculating the repeatability.\nThe experiments to compare detectors were carried out as follows:\n1) Image I1 was randomly selected from the dataset; For optical images, the original image I1 was converted to single-channel grayscale image; 2) To obtain I2, an H transforms (one of the list: rotation, shift, scaling, additive brightness changes, contrast changes, or Gaussian blur) was applied to I1; 3) The repeatability r was calculated for the pair I1, I2. For each fixed transform H , a described experiment was performed 100 times, after which the average repeatability was calculated. The experiments were carried out separately for the images of the visible and X-ray spectra.\nTransformations H were determined by the following parameters: \u2022 image rotation angle \u03b1 for rotation (from \u221215 to 15 degrees with step 0.5 degree);\n\u2022 shearing sh alongX and Y image axes (from 0 to 0.5with step 0.02); \u2022 image scaling factor s (from 0.25 to 2 with step 0.05); \u2022 intensity b added to the intensity of all pixels to convert brightness (from\u2212255 to 255 with a minimum intensity value of 0 and a maximum value of 255 and with step 5); \u2022 image intensity multiplier c for contrasting (from 0 to 4 with step 0.1) \u2022 standard deviation \u03c3 for Gaussian blur (from 0.04 to 8 pixels with step 0.04);\nThe dependencies of the repeatability r and the number of matched keypoints Ncorrespond on the transformation are shown in Fig. 4. In all experiments, software implementations of detectors and descriptors from the OpenCV library were used [24]. The parameters proposed by the authors of the original detectors and descriptors were used [6]\u2013[9], [22].\nB. DESCRIPTORS EVALUATION While comparing descriptors one has to mitigate the influence of the detetors quality, since detection stage precedes the description. To evaluate descriptors and to completeley exclude the influence of the detectors on the quality of the description, the identical points were chosen for description in all images instead of automatically detected keypoints. Description was performed in the nodes of the regular grid superimposed on the image and transformed with it (see Fig. 5). The grid size was 200 \u00d7 200 nodes. The nodes for which any algorithm could not compute a descriptor (here the descriptor is understood as the mathematical description that is produced by a description algorithm, and not an algorithm itself) did not participate in the calculation of the quality metrics described below. The scale and orientation of the grid points were also fixed. In contrast to the experiment with detectors, the transformed images were not cropped (see Fig. 5).\nAKAZE was excluded from consideration in this experiment, since the software implementation of its descriptor in OpenCV requires using the points detected by its detector as input.\nVOLUME 10, 2022 38967\nLet l1 be the shortest distance among all distances from some fixed keypoint\u2019s descriptor of the original image to all descriptors of the transformed one, and l2 be the second shortest distance as well. Here by distance we mean L2 norm for SURF and SIFT descriptors and Hamming distance for BRISK,ORB. To compare descriptors wemeasured l1l2 value. It was shown in [6] that for the SIFT descriptor for correctly matched points, this ratio most likely does not exceed\u2248 0.75. However, this value is sometimes used when choosing the correct matches for other descriptors [25].\nIt is possible that a pair of the nearest descriptors for the original and transformed images are not descriptors of the same point. To take this into account, we additionally measured the proportion of correctly matched points, that is, points for which the descriptor of the transformed point is closest to the descriptor of the original point. When this fraction becomes less than 0.5 \u2013 the algorithm becomes unreliable \u2013 obtaining an incorrect match is more probable than a correct one.\nDescriptors comparison experiments were carried out as follows:\n1) Image I1 was randomly selected from the dataset; For optical images, the original image I1 was converted to single-channel grayscale image; 2) To obtain I2, an H transforms (one of the list: rotation, shift, scaling, additive brightness changes, contrast changes, or Gaussian blur) was applied to I1; 3) The ratio of the distances from descriptor of each point of the original grid to the first and second nearest descriptor among the points of the transformed gridwas calculated. Then the calculated values for all grid nodes were averaged.\nAs in the study of descriptors for each transformation H , measurements were made on 100 randomly selected images and then averaged. The experimental results are shown in Fig. 6. Based on the results obtained, it can be concluded that often the ratio of correctly matched points falls below 50% earlier, or, on the contrary, much later than the ratio of the two nearest descriptors reaches 0.75. This, however, is not an indicator of the inconsistency of the selected quality metrics, since the position of the points for which the\ndescriptors were calculated was initially set without taking into account the informativeness of their surrounding areas. The obtained result should be considered as a demonstration of the fact that this metric should be used with caution when analyzing different pairs of detectors and descriptors, since the detector can detect keypoints that are not informative for the descriptor.\nIV. RESULTS A. DETECTORS Table 1 lists the best and worst detection algorithms for each type of image and applied transformation. The algorithms were ranked based on the repeatability value (the first two columns of the graphs in Fig. 4).We found it difficult to choose the worst algorithm in experiments to change the contrast in the X-ray spectra, as different detectors exhibited the lowest repeatability for different values of the contrast parameter. We chose SIFT as the worst detector under contrasting, because when the contrast is increased, it is definitely inferior to the others, and when decreased it is comparable to them.\nA sharp drop in the repeatability of all detectors is observed when the image is rotated. All algorithms are robust to\n38968 VOLUME 10, 2022\nbrightness changes. All algorithms find significantly fewer points in the X-ray spectrum than in the visible one, which is most likely due to the absence of textures. Table 2 lists the types of transformations in which the algorithm showed the best or worst quality of work, as well as the average number of detected points. And table 3 contains ratios of area under the actual detector\u2019s plot to the \u2018\u2018perfect\u2019\u2019 one\u2019s, i.e. constant equal to the actual plot\u2019s max value. On X-ray data, SURF is often themost repeatable. AKAZE, in turn, outperforms other algorithms on images in the visible spectrum. SIFT showed itself worst of all.\nB. DESCRIPTORS On Fig. 6) the proportion of correct matches, a sharp jump is observed with changes in brightness and contrast. This is due to the fact that the images become almost uniformly black or white during conversion, which leads to almost identical descriptors for different image patches. Table 4 provides an analysis of the best and worst descriptors for each type of transformation. The main criterion for choosing algorithms was the proportion of correct matches (the third and fourth columns of the graphs in Fig. 6). In disputable situations, an algorithm was chosen with a smaller ratio of the distance to the two nearest descriptors. Table 5 contains ratios of area under the descriptor\u2019s plot for all algotithms, transforms and spectra (the x-axis is normalized, i.e. the maximal possible area under the graph is 1). Table 6 shows the types of transformations in which the description algorithm showed the best or worst quality of work. The SIFT descriptor showed definitely the best quality, SURF showed itself the worst. We found it difficult to choose the best algorithm in experiments on changing the brightness of X-ray images, since the three algorithms demonstrated similar performance. The quality of the description algorithms, as in the case of detectors, is most sensitive to image rotation. In general, the quality indicators for the description algorithms differ less in different spectra than the quality indicators of the detection algorithms. Let\u2019s analyze in more detail all detection and description algorithms.\nC. SIFT"
        },
        {
            "heading": "1) DETECTOR",
            "text": "The algorithm demonstrates the worst performance in the X-ray spectrum and comparatively poor in the visible one. Differences in the behavior of the algorithm on images in different spectra are observed for rotation and shearing: the\nrepeatability for the X-ray spectrum decreases significantly faster. The algorithm turned out to be the most stable to a change in scale; on both spectra, after a certain drop, the repeatability remained generally constant. The other advantages of the algorithm also include a fairly large number of detected points (about 750 for the X-ray spectrum and about 5000 for the visible one)."
        },
        {
            "heading": "2) DESCRIPTOR",
            "text": "In terms of the description quality, SIFT demonstrates the best quality for many transformations. Specifically, for shearing and rotation, the proportion of correct matches decreases much slower than for other algorithms. For scaling, brightness, and contrast changes, the algorithm also shows the best quality, albeit with a smaller advantage over other descriptors. The only case where the algorithm does not show superior quality is blurring. It is interesting to note the similarity of the behavior of correct matches for SIFT and BRISK for blurring.\nIn general, both in the visible and X-ray spectra, the algorithm shows the same behavior. The only exception is the behavior when changing brightness and contrast. With an increase in the brightness in the visible spectrum, the quality of the algorithm decreases somewhat slowly (although comparable) than with a decrease. In the X-ray spectrum, the opposite is observed: the quality decreases noticeably slowly with decreasing brightness than with increasing. It can be assumed that this is due to the nature of the images, for example, that the areas in the X-ray image well described by the algorithm contain mainly black or light pixels, therefore, with increasing brightness, light pixels of the area become simply white. As the brightness decreases, the pixels remain distinguishable. Similarly, a sharp drop in quality can be explained when the contrast changes.\nD. SURF 1) DETECTOR SURF demonstrates consistently good quality at all transformations in both spectra. The algorithm performs best of all under shearing and rotating. Inder blurring, the SURF quality is sometimes inferior comparable to AKAZE and superior to other algorithms. For scaling, brightness changing, and contrasting, the quality of the algorithm is not much different from most others. The difference in behavior on different spectra is observed for rotation and shearing \u2013 as for most algorithms, the quality in the X-ray spectrum drops faster. This is true for all of the detectors except ORB, which repeatability does not change. As an advantage, we can note the fact that SURF detects the largest average number of points in the X-ray spectrum \u2013 1600, which is 1.5 times higher than the next best algorithm for this parameter - BRISK. For the visible spectrum, the number of points is inferior to BRISK but superior to other algorithms."
        },
        {
            "heading": "2) DESCRIPTOR",
            "text": "Unlike the detection algorithm, the SURF descriptor performed poorly. When rotating, changing contrast, blurring,\nVOLUME 10, 2022 38969\nand scaling (in the visible spectrum), the algorithm works worst of all, because, before the rest of the algorithms, it experiences the quality drop when it correctly matches less than half of the described points. With shearing in the X-ray spectrum, the algorithm does not work much better than the worst descriptor \u2013 BRISK, but in the visible spectrum, they show similar performance. The algorithm works relatively well when changing the brightness, at some values it reaches the quality of the SIFT that showed itself best in this experiment. A significant change in behavior in different spectra can be noted only for the change in brightness. We assume that the explanation of this fact coincides with the explanation given for the SIFT.\nE. ORB 1) DETECTOR In most experiments, ORB performed worse than other algorithms and became the worst in the visible spectrum.\nEspecially poorly, the algorithm performs under scaling. When decreasing the scale, the repeatability of the algorithm quickly decreases to 0, while for other algorithms it remains above 0.5. The repeatability plot for this transformation has a stepped-like appearance which indicates poor ability to detect keypoints on intermediate scales between image pyramid layers. However, it can be noted that under blurring the algorithm shows moderate results comparable to BRISK and outperforming SIFT up to \u03c3 \u2248 7. The ORB also detects the smallest number of keypoints. Changes in the algorithm behavior in different spectra can be noted in brightness, contrast, and blurring. For these transformations, repeatability decreases slightly more slowly in the X-ray spectrum. A possible explanation for this is that changing brightness and contrast leads to clipping pixel values and that changes ORB descriptors as it uses the pairwise comparison of pixel intensities. At the same time, X-ray images contain mostly middle range (grey) pixel values, so the clippings occur later for X-ray images than for visible ones. Also, visible-spectrum images are characterized by a lot of small details (e.g. in textures) that disappear when blurred. Thus blurring changes detector comparing pixels\u2019 intensity more strongly for such images."
        },
        {
            "heading": "2) DESCRIPTOR",
            "text": "The ORB description algorithm is of ambiguous quality. In such transformations as changing brightness, rotation, scaling, its quality decreases rapidly. At the same time, when increasing the contrast for the visible spectrum,\n38970 VOLUME 10, 2022\nthe proportion of correct matches of the algorithm is comparable to the best algorithms for this transformation (SIFT, BRISK), and for blurring, the algorithm is superior to the rest. The behavior when changing the brightness is noteworthy, since there is a temporary decrease in the ratio of two nearest descriptors.\nAs with other algorithms, a change in behavior for different spectra is observed when changing the brightness and contrast. In addition, upon rotation in the X-ray spectrum, a small plateau is observed in the vicinity of 0 for the fraction of correct matches (if we exclude the identical transformation). This plateau is absent in the visible spectrum.\nF. BRISK"
        },
        {
            "heading": "1) DETECTOR",
            "text": "BRISK outperforms other algorithms under increasing contrast. As well as AKAZE it also demonstrates the best repeatability in experiments with changing brightness and the best one after SURF in visible spectrum under rotation. For other transformations, quality varies but never becomes the worst. It should be noted that the algorithm detects the largest average number of detected keypoints in the visible spectrum \u2013 7700. There are no significant changes in the behavior of the detector when changing the spectrum, except for the previously mentioned shearing and rotation."
        },
        {
            "heading": "2) DESCRIPTOR",
            "text": "The algorithm demonstrates moderate quality for all transformations. Under shearing BRISK works worse than other\nalgorithms (comparable to ORB), however, under the shearing, the quality of all algorithms, except SIFT, is poor. Most significant behavior changes in different spectra can be seen under changes in brightness, contrast, and blurring. A possible explanation for this was given in the discussion of the results for the ORB detector.\nG. AKAZE"
        },
        {
            "heading": "1) DETECTOR",
            "text": "The algorithm has demonstrated good quality of work in most experiments. For scaling and blurring experiments AKAZE has become the leader among the compared algorithms. When changing the contrast, the algorithm lags a little behind BRISK in some places, but still performs well. For the rest of the transformations, it shows moderate results. When changing spectrum the behavior changes for brightness transformation: repeatability drops for X-ray images faster."
        },
        {
            "heading": "2) DESCRIPTOR",
            "text": "As mentioned before, AKAZE was not involved in the study of descriptors. In the experiments, we used OpenCV implementations of the algorithms and AKAZE implementation requires descriptors computation only for points detected by the same detector. This is not consistent with the design of the experiment.\nV. CONCLUSION In this work, we studied the performance of wide-used keypoints detection and description algorithms (SIFT, SURF, ORB, BRISK, AKAZE) applied to images taken in visible and X-ray spectra. We also studied the robustness of the algorithms to various transformations (rotation, shearing, scaling, brightness and contrast changes, Gaussian blur).\nWe confirmed the assumption about the difference in the behavior of algorithms when working with images in X-ray and visible spectra. First of all, in the images of the X-ray spectrum, all algorithms detect a significantly smaller number of keypoints, which can adversely affect the quality of the computer vision systems that use keypoints. SURF showed the best quality among detection algorithms on X-ray images, and on visible ones, it shares the primacy with AKAZE. The worst detection quality was observed for SIFT and ORB for X-ray and visible spectra respectively. It is shown that in the X-ray spectrum all detection algorithms are less robust for rotation and shearing. For the rest of the studied transformations, in general, there is no significant change in the quality of both keypoints detection and description.\nSIFT became the best descriptor in our experiments, and SURF became the worst one. Note that the AKAZE descriptor, due to the limitations of its software implementation and the specifics of the experiment, did not participate in the comparison. Taking into account the fact that SURF uses similar principles of keypoints detections as SIFT, one should expect that a SURF detector paired with a SIFT descriptor will give the best performance in the majority of applications. We plan to investigate this issue in more detail in future work.\nVOLUME 10, 2022 38971\nNote that the performed experiment for comparing descriptors has a drawback: it calculates descriptors, at the nodes of a regular grid (see Fig.5), which makes the comparison fair, but does not guarantee that selected pixels and their vicinity are informative. It can lead to poor quality of keypoints matching. In the future, we plan to conduct additional experiments to take described drawback into account. In addition, we plan to add neural network detectors and descriptors into the study.\nIn addition, the speed of the algorithms was not investigated, which is certainly important for real-time application. Although it is beyond the scope of this paper, we are planning to include relevant studies in further works.\nREFERENCES [1] R. M. Haralick and L. G. Shapiro, \u2018\u2018Connected components labeling,\u2019\u2019\nComput. Robot. Vis., vol. 1, pp. 28\u201348, Oct. 1992. [2] V. Lepetit and P. Fua, \u2018\u2018Keypoint recognition using randomized trees,\u2019\u2019\nIEEE Trans. Pattern Anal. Mach. Intell., vol. 28, no. 9, pp. 1465\u20131479, Sep. 2006. [3] N. S. Skoryukina, A. N. Milovzorov, D. V. Polevoy, and V. V. Arlazarov, \u2018\u2018Paintings recognition in uncontrolled conditions using one-shot learning,\u2019\u2019 Trudy ISA RAN, vol. 68, pp. 5\u201314, May 2018, doi: 10.14357/20790279180501. [4] S. Agarwal, \u2018\u2018Building Rome in a day,\u2019\u2019 Commun. ACM, vol. 54, no. 10, pp. 105\u2013112, Oct. 2011. [5] D. Scaramuzza, \u2018\u2018Performance evaluation of 1-point-RANSAC visual odometry,\u2019\u2019 J. Field Robot., vol. 28, no. 5, pp. 792\u2013811, Sep. 2011. [6] D. G. Lowe, \u2018\u2018Distinctive image features from scale-invariant keypoints,\u2019\u2019 Int. J. Comput. Vis., vol. 60, no. 2, pp. 91\u2013110, 2004. [7] E. Rublee, V. Rabaud, K. Konolige, and G. Bradski, \u2018\u2018ORB: An efficient alternative to SIFT or SURF,\u2019\u2019 in Proc. Int. Conf. Comput. Vis., Nov. 2011, pp. 2564\u20132571. [8] P. F. Alcantarilla, A. Bartoli, and A. J. Davison, \u2018\u2018Kaze features,\u2019\u2019 in Proc. Eur. Conf. Comput. Vis. Berlin, Germany: Springer, 2012, pp. 214\u2013227. [9] H. Bay, T. Tuytelaars, and L. V. Gool, \u2018\u2018Surf: Speeded up robust features,\u2019\u2019 in Proc. Eur. Conf. Comput. Vis. Berlin, Germany: Springer, 2006, pp. 404\u2013417. [10] M. Rodriguez, G. Facciolo, R. G. Von Gioi, P. Muse, J.-M. Morel, and J. Delon, \u2018\u2018SIFT-AID: Boosting sift with an affine invariant descriptor based on convolutional neural networks,\u2019\u2019 in Proc. IEEE Int. Conf. Image Process. (ICIP), Sep. 2019, pp. 4225\u20134229. [11] Z. Song andR.Klette, \u2018\u2018Robustness of point feature detection,\u2019\u2019 inProc. Int. Conf. Comput. Anal. Images Patterns. Berlin, Germany: Springer, 2013, pp. 91\u201399. [12] K. Mikolajczyk, T. Tuytelaars, C. Schmid, A. Zisserman, J. Matas, F. Schaffalitzky, T. Kadir, and L. Van Gool, \u2018\u2018A comparison of affine region detectors,\u2019\u2019 Int. J. Comput. Vis., vol. 65, no. 1, pp. 43\u201372, 2005. [13] J. Hu, X. Peng, and C. Fu, \u2018\u2018A comparison of feature description algorithms,\u2019\u2019 Optik, vol. 126, no. 2, pp. 274\u2013278, Jan. 2015. [14] S. Akhmedkhan, S. Gladilin, and E. A. Shvets, \u2018\u2018Optical-to-SAR image registration using a combination of CNN descriptors and cross-correlation coefficient,\u2019\u2019 in Proc. 12th Int. Conf. Mach. Vis. (ICMV), Jan. 2020, Art. no. 114331. [15] M. Moradi, P. Abolmaesoumi, and P. Mousavi, \u2018\u2018Deformable registration using scale space keypoints,\u2019\u2019 Proc. SPIE Med. Imag. Process., vol. 6144, Aug. 2006, Art. no. 61442G. [16] V. V. Volkov and E. A. Shvets, \u2018\u2018Dataset and method for evaluating optical+to+sar image registration algorithms based on keypoints,\u2019\u2019 Inf. Tekhnol. Syst., vol. 2, pp. 44\u201357, Oct. 2021, doi: 10.14357/20718632210205. [17] F. Lecron, M. Benjelloun, and S. Mahmoudi, \u2018\u2018Descriptive image feature for object detection in medical images,\u2019\u2019 in Proc. Int. Conf. Image Anal. Recognit. Berlin, Germany: Springer, 2012, pp. 331\u2013338. [18] M. O. Chekanov, O. C. Shipitko, and E. I. Ershov, \u2018\u20181-point ransac for axial rotation angle estimation by tomographic projections,\u2019\u2019 Sensory Syst., vol. 34, no. 1, pp. 72\u201386, 2020, doi: 10.31857/S0235009220010060. [19] D. Mery, E. Svec, M. Arias, V. Riffo, J. M. Saavedra, and S. Banerjee, \u2018\u2018Modern computer vision techniques for X-ray testing in baggage inspection,\u2019\u2019 IEEE Trans. Syst., Man, Cybern. Syst., vol. 47, no. 4, pp. 682\u2013692, Apr. 2017.\n[20] V. Balntas, K. Lenc, A. Vedaldi, and K. Mikolajczyk, \u2018\u2018HPatches: A benchmark and evaluation of handcrafted and learned local descriptors,\u2019\u2019 in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jul. 2017, pp. 5173\u20135182. [21] D. Mery, V. Riffo, U. Zscherpel, G. Mondrag\u00f3n, I. Lillo, I. Zuccar, H. Lobel, and M. Carrasco, \u2018\u2018GDXRay: The database of X-ray images for nondestructive testing,\u2019\u2019 J. Nondestruct. Eval., vol. 34, no. 4, pp. 1\u201312, 2015. [22] S. Leutenegger, M. Chli, and R. Y. Siegwart, \u2018\u2018BRISK: Binary robust invariant scalable keypoints,\u2019\u2019 in Proc. Int. Conf. Comput. Vis., Nov. 2011, pp. 2548\u20132555. [23] M. Chekanov and O. Shipitko, \u2018\u2018X-ray and visible spectra circular motion images dataset,\u2019\u2019 2019, arXiv:1909.13730. [24] G. Bradski, \u2018\u2018The OpenCV Library,\u2019\u2019 Softw. Tools Prof. Programmer, vol. 25, no. 11, pp. 120\u2013123, 2000. [25] O. Andersson and S. R. Marquez, \u2018\u2018A comparison of object detection algorithms using unmanipulated testing images: Comparing SIFT, KAZE, AKAZE and ORB,\u2019\u2019 B.S. Thesis, Dept. Comput. Sci., KTH Roy. Inst. Technol., Stockholm, Sweden, 2016.\nMIKHAIL CHEKANOV was born in Russia, in 1998. He received the B.E. degree (Hons.) in applied mathematics and physics from the Moscow Institute of Physics and Technology (MIPT), National Research University, Moscow, Russia, in 2020. He is currently pursuing the master\u2019s degree with MIPT. He joined as a Research Intern with the 11th Visual Systems Laboratory, IITP, RAS, Moscow, in 2019. Since 2020, he has been a Software Engineer with Evocargo LLC,\nMoscow. His main research interests include robotics and classical computer vision.\nOLEG SHIPITKO was born in Yeisk, Russia. He received the bachelor\u2019s degree in electrical engineering from Bauman Moscow State Technical University, in 2015, and the master\u2019s degree in computer science from the Skolkovo Institute of Science and Technology, in 2017. Since 2017, he has beenwith the Laboratory ofVision Systems, Institute for Information Transmission Problems, Russian Academy of Sciences (Kharkevich Institute). His research interest includes vision-based localization algorithms for robotics.\nNATALIA SKORYUKINA received the graduate degree from the National University of Science and Technology MISiS , in 2013, majoring in applied mathematics. She is a Computer Programmer (1st category) with the Federal Research Center of Computer Science and Control, Russian Academy of Sciences. Her research interests include image analysis and computer vision.\n38972 VOLUME 10, 2022"
        }
    ],
    "title": "Study of Keypoints Detectors and Descriptors Performance on X-Ray Images Compared to the Visible Light Spectrum Images",
    "year": 2022
}