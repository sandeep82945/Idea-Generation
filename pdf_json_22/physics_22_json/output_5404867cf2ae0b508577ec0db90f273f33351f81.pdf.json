{
    "abstractText": "Artificial neural networks have been widely adopted as ansatzes to study classical and quantum systems. However, for some notably hard systems, such as those exhibiting glassiness and frustration, they have mainly achieved unsatisfactory results, despite their representational power and entanglement content, thus suggesting a potential conservation of computational complexity in the learning process. We explore this possibility by implementing the neural annealing method with autoregressive neural networks on a model that exhibits glassy and fractal dynamics: the two-dimensional Newman\u2013Moore model on a triangular lattice. We find that the annealing dynamics is globally unstable because of highly chaotic loss landscapes. Furthermore, even when the correct ground-state energy is found, the neural network generally cannot find degenerate ground-state configurations due to mode collapse. These findings indicate that the glassy dynamics exhibited by the Newman\u2013Moore model caused by the presence of fracton excitations in the configurational space likely manifests itself through trainability issues and mode collapse in the optimization landscape.",
    "authors": [
        {
            "affiliations": [],
            "name": "Estelle M. Inack"
        },
        {
            "affiliations": [],
            "name": "Roger G. Melko"
        }
    ],
    "id": "SP:e2d63894a5adbde9391b7bc3e195a1d5fadb6c61",
    "references": [
        {
            "authors": [
                "G. Carleo",
                "I. Cirac",
                "K. Cranmer",
                "L. Daudet",
                "M. Schuld",
                "N. Tishby",
                "L. Vogt-Maranto",
                "L. Zdeborov\u00e1"
            ],
            "title": "Machine learning and the physical sciences",
            "venue": "Rev. Mod. Phys",
            "year": 2019
        },
        {
            "authors": [
                "G. Torlai",
                "R.G. Melko"
            ],
            "title": "Learning thermodynamics with Boltzmann machines",
            "venue": "Phys. Rev. B",
            "year": 2016
        },
        {
            "authors": [
                "L. Wang"
            ],
            "title": "Discovering phase transitions with unsupervised learning",
            "venue": "Phys. Rev. B 2016,",
            "year": 1951
        },
        {
            "authors": [
                "J. Carrasquilla",
                "R. Melko"
            ],
            "title": "Machine learning phases of matter",
            "venue": "Nat. Phys. 2017,",
            "year": 2017
        },
        {
            "authors": [
                "E. Van Nieuwenburg",
                "Y. Liu",
                "S. Huber"
            ],
            "title": "Learning phase transitions by confusion",
            "venue": "Nat. Phys. 2017,",
            "year": 2017
        },
        {
            "authors": [
                "D.L. Deng",
                "X. Li",
                "S. Das Sarma"
            ],
            "title": "Machine learning topological states",
            "venue": "Phys. Rev. B 2017,",
            "year": 1951
        },
        {
            "authors": [
                "J. Liu",
                "Y. Qi",
                "Z.Y. Meng",
                "L. Fu"
            ],
            "title": "Self-learning Monte Carlo method",
            "venue": "Phys. Rev. B",
            "year": 2017
        },
        {
            "authors": [
                "E.M. Inack",
                "G.E. Santoro",
                "L. Dell\u2019Anna",
                "S. Pilati"
            ],
            "title": "Projective quantum Monte Carlo simulations guided by unrestricted neural network states",
            "venue": "Phys. Rev. B",
            "year": 2018
        },
        {
            "authors": [
                "T. Parolini",
                "E.M. Inack",
                "G. Giudici",
                "S. Pilati"
            ],
            "title": "Tunneling in projective quantum Monte Carlo simulations with guiding wave functions",
            "venue": "Phys. Rev. B",
            "year": 2019
        },
        {
            "authors": [
                "S. Pilati",
                "E.M. Inack",
                "P. Pieri"
            ],
            "title": "Self-learning projective quantum Monte Carlo simulations guided by restricted Boltzmann machines",
            "venue": "Phys. Rev. E 2019,",
            "year": 2019
        },
        {
            "authors": [
                "M.S. Albergo",
                "G. Kanwar",
                "P.E. Shanahan"
            ],
            "title": "Flow-based generative models for Markov chain Monte Carlo in lattice field theory",
            "venue": "Phys. Rev. D",
            "year": 2019
        },
        {
            "authors": [
                "D. Wu",
                "R. Rossi",
                "G. Carleo"
            ],
            "title": "Unbiased Monte Carlo cluster updates with autoregressive neural networks",
            "venue": "Phys. Rev. Res. 2021,",
            "year": 2024
        },
        {
            "authors": [
                "S. Czischek",
                "M.S. Moss",
                "M. Radzihovsky",
                "E. Merali",
                "R.G. Melko"
            ],
            "title": "Data-Enhanced Variational Monte Carlo for Rydberg Atom Arrays",
            "venue": "arXiv 2022,",
            "year": 2022
        },
        {
            "authors": [
                "G. Carleo",
                "M. Troyer"
            ],
            "title": "Solving the quantum many-body problem with artificial neural networks",
            "venue": "Science",
            "year": 2017
        },
        {
            "authors": [
                "G. Torlai",
                "G. Mazzola",
                "J. Carrasquilla",
                "M. Troyer",
                "R. Melko",
                "G. Carleo"
            ],
            "title": "Neural-network quantum state tomography",
            "venue": "Nat. Phys. 2018,",
            "year": 2018
        },
        {
            "authors": [
                "Z. Cai",
                "J. Liu"
            ],
            "title": "Approximating quantum many-body wave functions using artificial neural networks",
            "venue": "Phys. Rev. B",
            "year": 2018
        },
        {
            "authors": [
                "M. Hibat-Allah",
                "M. Ganahl",
                "L.E. Hayward",
                "R.G. Melko",
                "J. Carrasquilla"
            ],
            "title": "Recurrent neural network wave functions",
            "venue": "Phys. Rev. Res. 2020,",
            "year": 2020
        },
        {
            "authors": [
                "J. Carrasquilla"
            ],
            "title": "Machine learning for quantum matter",
            "venue": "Adv. Phys. X 2020,",
            "year": 2020
        },
        {
            "authors": [
                "A. Szab\u00f3",
                "C. Castelnovo"
            ],
            "title": "Neural network wave functions and the sign problem",
            "venue": "Phys. Rev. Res. 2020,",
            "year": 2020
        },
        {
            "authors": [
                "T. Westerhout",
                "N. Astrakhantsev",
                "K.S. Tikhonov",
                "M.I. Katsnelson",
                "A.A. Bagrov"
            ],
            "title": "Generalization properties of neural network approximations to frustrated magnet ground states",
            "venue": "Nat. Commun",
            "year": 2020
        },
        {
            "authors": [
                "C.Y. Park",
                "M.J. Kastoryano"
            ],
            "title": "Expressive power of complex-valued restricted Boltzmann machines for solving non-stoquastic Hamiltonians",
            "venue": "arXiv 2021,",
            "year": 2021
        },
        {
            "authors": [
                "H. Li",
                "Z. Xu",
                "G. Taylor",
                "C. Studer",
                "T. Goldstein"
            ],
            "title": "Visualizing the Loss Landscape of Neural Nets",
            "venue": "arXiv 2018,",
            "year": 2018
        },
        {
            "authors": [
                "P. Huembeli",
                "A. Dauphin"
            ],
            "title": "Characterizing the loss landscape of variational quantum circuits",
            "venue": "Quantum Sci. Technol. 2021,",
            "year": 2021
        },
        {
            "authors": [
                "M.S. Rudolph",
                "S. Sim",
                "A. Raza",
                "M. Stechly",
                "J.R. McClean",
                "E.R. Anschuetz",
                "L. Serrano",
                "A. Perdomo-Ortiz"
            ],
            "title": "ORQVIZ: Visualizing High-Dimensional Landscapes in Variational Quantum Algorithms",
            "venue": "arXiv 2021,",
            "year": 2021
        },
        {
            "authors": [
                "S. Kirkpatrick",
                "C.D. Gelatt",
                "M.P. Vecchi"
            ],
            "title": "Optimization by Simulated Annealing",
            "venue": "Science",
            "year": 1983
        },
        {
            "authors": [
                "M.E.J. Newman",
                "C. Moore"
            ],
            "title": "Glassy dynamics and aging in an exactly solvable spin model",
            "venue": "Phys. Rev. E",
            "year": 1999
        },
        {
            "authors": [
                "J.P. Garrahan",
                "M.E.J. Newman"
            ],
            "title": "Glassiness and constrained dynamics of a short-range nondisordered spin model",
            "venue": "Phys. Rev. E",
            "year": 2000
        },
        {
            "authors": [
                "L.M. Vasiloiu",
                "T.H.E. Oakes",
                "F. Carollo",
                "J.P. Garrahan"
            ],
            "title": "Trajectory phase transitions in noninteracting spin systems",
            "venue": "Phys. Rev. E",
            "year": 2020
        },
        {
            "authors": [
                "T. Devakul",
                "Y. You",
                "F.J. Burnell",
                "S.L. Sondhi"
            ],
            "title": "Fractal Symmetric Phases of Matter",
            "venue": "SciPost Phys. 2019,",
            "year": 2019
        },
        {
            "authors": [
                "T. Devakul",
                "D.J. Williamson"
            ],
            "title": "Fractalizing quantum codes",
            "venue": "Quantum 2021,",
            "year": 2021
        },
        {
            "authors": [
                "Z. Zhou",
                "X.F. Zhang",
                "F. Pollmann",
                "Y. You"
            ],
            "title": "Fractal Quantum Phase Transitions: Critical Phenomena Beyond Renormalization",
            "venue": "arXiv 2021,",
            "year": 2021
        },
        {
            "authors": [
                "S. Hochreiter",
                "J. Schmidhuber"
            ],
            "title": "Long Short-Term Memory",
            "venue": "Neural Comput",
            "year": 1997
        },
        {
            "authors": [
                "A. Graves"
            ],
            "title": "Supervised sequence labelling",
            "year": 2012
        },
        {
            "authors": [
                "H. Siegelmann",
                "E. Sontag"
            ],
            "title": "On the Computational Power of Neural Nets",
            "venue": "J. Comput. Syst. Sci",
            "year": 1995
        },
        {
            "authors": [
                "A.M. Sch\u00e4fer",
                "H.G. Zimmermann"
            ],
            "title": "Recurrent Neural Networks Are Universal Approximators",
            "venue": "In Proceedings of the Artificial Neural Networks\u2014ICANN",
            "year": 2006
        },
        {
            "authors": [
                "D.P. Kingma",
                "J. Ba"
            ],
            "title": "Adam: A Method for Stochastic Optimization",
            "venue": "arXiv 2014,",
            "year": 2014
        },
        {
            "authors": [
                "D.J. Im",
                "M. Tao",
                "K. Branson"
            ],
            "title": "An empirical analysis of the optimization of deep network loss surfaces. arXiv 2017, arXiv:1612.04010",
            "venue": "Condens. Matter 2022,",
            "year": 2022
        },
        {
            "authors": [
                "F. Becca",
                "S. Sorella"
            ],
            "title": "Quantum Monte Carlo Approaches for Correlated Systems",
            "year": 2017
        },
        {
            "authors": [
                "D.L. Deng",
                "X. Li",
                "S. Das Sarma"
            ],
            "title": "Quantum Entanglement in Neural Network States",
            "venue": "Phys. Rev. X 2017,",
            "year": 2017
        },
        {
            "authors": [
                "O. Sharir",
                "A. Shashua",
                "G. Carleo"
            ],
            "title": "Neural tensor contractions and the expressive power of deep neural quantum states",
            "venue": "arXiv 2021,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "Citation: Inack, E.M.; Morawetz, S.;\nMelko, R.G. Neural Annealing and\nVisualization of Autoregressive\nNeural Networks in the\nNewman\u2013Moore Model. Condens.\nMatter 2022, 7, 38. https://doi.org/\n10.3390/condmat7020038\nAcademic Editors: Sebastiano Pilati,\nBruno Julia-Diaz and Stefano\nGiorgini\nReceived: 6 April 2022\nAccepted: 23 May 2022\nPublished: 27 May 2022\nPublisher\u2019s Note: MDPI stays neutral\nwith regard to jurisdictional claims in\npublished maps and institutional affil-\niations.\nCopyright: \u00a9 2022 by the authors.\nLicensee MDPI, Basel, Switzerland.\nThis article is an open access article\ndistributed under the terms and\nconditions of the Creative Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n4.0/).\nKeywords: artificial neural networks; neural network trainability; variational neural annealing; computational complexity; glassy dynamics"
        },
        {
            "heading": "1. Introduction",
            "text": "In recent years, the performance of machine learning models has increased considerably in areas such as computer vision or natural language processing. Deep learning [1], in particular, has surpassed previously known algorithms, improving the performance of tasks such as object recognition and machine translation, to name a few. Undoubtedly, this improvement mainly came from artificial neural networks (ANNs), powerful models that capture intricate correlations present in data. Thanks to their representational power, they act as very efficient feature extraction machines whose output is meaningful information obtained from a nonlinear transformation of an input. Inspired by the successes of ANNs in computer science, physicists have also started to use them to study problems in various branches of physics [2], such as optics, cosmology, quantum information, and condensed matter. In the latter, they are used to identify phases of matter [3\u20137], increase the performance of Monte Carlo simulations [8\u201314], and find precise representations of the ground state of quantum systems [15\u201318]. A particular aspect of their capacity to characterize quantum matter [19] is their ability to be used as parameterized functions to represent the underlying probability distribution of a physical system. However, it was observed that, for certain hard problems, such as those exhibiting frustration [20\u201323] (e.g., due to the infamous sign problem), it was difficult for ANNs to learn the correct distribution despite their expressiveness, entanglement content, or symmetries. This observation hints at a possible conservation of computational complexity\nCondens. Matter 2022, 7, 38. https://doi.org/10.3390/condmat7020038 https://www.mdpi.com/journal/condensedmatter\nhardness, which manifests itself in the trainability of ANNs. This work investigates this issue by visualizing the ANNs\u2019 loss landscapes. Loss landscape visualization is an approach that aims at representing the high dimensionality of ANNs in the more intuitive two- or three-dimensional spaces. It has been successfully used in the machine learning community to study the trainability and generalization of deep neural networks [24]. In particular, it was used to understand why skip connections in residual neural networks are generalizing better than vanilla convolutional neural networks. Nowadays, in the quantum computing community, it is more and more employed to benchmark different quantum circuit architectures over a variety of tasks, such as quantum optimization or quantum machine learning [25,26]. This paper uses it to study trainability in the neural annealing paradigm. Neural annealing is a recent technique that aims at solving hard optimization problems through the variational simulation of the annealing procedure using ANNs [27]. It was shown to be more efficient than simulated annealing [28] and simulated quantum annealing [29] in finding the ground state of various spin glasses. In this work, we use it in both its classical and quantum formulations with autoregressive neural networks, to find the ground state of the Newman\u2013Moore model on a triangular lattice [30\u201332]. We choose the Newman\u2013Moore model as a benchmark because it is exactly solvable and displays several exotic features, such as glassiness without disorder, extensive ground-state degeneracy, and fractal symmetry; the latter proved to have application in quantum memories [33,34]. Furthermore, the fractal symmetry of the Newman\u2013Moore generates immobile excitations under local Hamiltonian dynamics, which hamper both classical and quantum Monte Carlo simulations [30,35]. In this work, we show that the neural annealing dynamics of the Newman\u2013Moore display instabilities due to the highly rugged nature of the loss landscape geometry, despite the use of ancestral sampling, which was shown to be superior compared to Metropolis Monte Carlo sampling. Ancestral sampling, also known as autoregressive sampling, is implemented via recurrent neural networks (RNNs) [36,37], which are known to be Turing complete [38], and universal function approximators [39]. However, we find that when the annealing dynamics results in the correct ground-state energy, the sampling is generally unable to capture the multi-modal distribution of degenerate ground states, hinting at a possible mode collapse in the training procedure. The remainder of the article is organized as follows. In Section 2, we describe the classical and quantum Newman\u2013Moore models. In Section 3, we describe the RNN ansatz, the neural annealing protocols, and the visualization method used in this work. Section 4 reports classical and quantum variational annealing results on the Newman\u2013 Moore model, along with the corresponding visualization of the loss landscape during annealing dynamics. Conclusions are reported in Section 5."
        },
        {
            "heading": "2. The Newman\u2013Moore Model",
            "text": "In order to investigate trainability in the neural annealing method, we use as a test-bed the two-dimensional (2D) Newman\u2013Moore model on a triangular lattice. Its Hamiltonian is given by:\nH = J 2 \u2211i,j,k in \u2207 \u03c3i\u03c3j\u03c3k, (1)\nwhere \u03c3i = \u00b11 are Ising spins located at the lattice sites i, j, k of a downward-facing triangle (see Figure 1a). The parameter J > 0 fixes the strength of the interactions of each spin triplet, and is used as the energy scale of the system. In this work, we set J = 1. We consider as a computational basis the state \u03c3 = (\u03c31, . . . , \u03c3N) of N = L\u00d7 L spins, where L is the length of the lattice.\nOne peculiarity of the Newman\u2013Moore model is that its macroscopic behavior strongly depends on the characteristics of its finite-sized lattice. If one of the sides has a dimension that can be expressed as L = 2k for some integer k, then the model is exactly solvable and has a single ground-state configuration, which is the trivial one with all spins pointing down. For different values of L, the ground state may be degenerate, with a degeneracy that is a non-analytic function of the lattice size. This is due to the Newman\u2013Moore model exhibiting a fractal symmetry that acts on some subextensive d-dimensional (d, the fractal dimension is generally not an integer) subsystem of the whole system [33]. Furthermore, the Newman\u2013Moore model exhibits glassy behavior at low temperatures and under single-spin-flip dynamics despite having neither randomness nor frustration. The glassiness causes a loss of ergodicity due to the presence of fracton excitations, thus hampering sampling in traditional Monte Carlo methods, even when thermal annealing is implemented [30]. Contrary to the classical Newman\u2013Moore model, which does not have a thermodynamic phase transition at finite temperature, the quantum Newman\u2013Moore model, in the presence of the transverse field \u2212\u0393 \u2211Ni=1 \u03c3xi , exhibits a fractal quantum-phase transition at \u0393 = 1 [35], with first-order-like fluctuations [32]. However, fracton excitations on the\nSierpinski triangle still induce restricted mobility, hence impeding the sampling procedure of quantum Monte Carlo methods in the glassy phase [35]. In the next section, we describe our implementation of the variational classical and quantum annealing methods with recurrent neural networks."
        },
        {
            "heading": "3. Methods",
            "text": ""
        },
        {
            "heading": "3.1. The Recurrent Neural Network Ansatz",
            "text": "We use recurrent neural networks (RNNs) to parameterize the Boltzmann probability distribution of the Newman\u2013Moore Hamiltonian in Equation (1). The joint probability distribution of a spin configuration \u03c3 is modeled with \u03b8 parameters as follows:\np\u03b8(\u03c3) = p\u03b8(\u03c31)p\u03b8(\u03c32|\u03c31) \u00b7 \u00b7 \u00b7 p\u03b8(\u03c3N |\u03c3<N), (2)\nwhere p\u03b8(\u03c3i|\u03c3<i) is a conditional probability given by:\np\u03b8(\u03c3i|\u03c3<i) = Softmax(Uhn,m + b) \u00b7 \u03c3i. (3)\nThe Softmax activation function guarantees that the probability distribution p\u03b8(\u03c3) is normalized to unity. \u03c3i is the one-hot representation of the spin \u03c3i, and \u00b7 denotes the dot product operation. Note that the index i covers 1, . . . , N. Indices n, m cover 1, . . . , L, respectively, along the horizontal and vertical sides of the triangular lattice. hn,m is the RNN hidden state, which encodes information about the previous spins \u03c3i\u2032<i. It obeys the following recurrent relation (see Figure 1b):\nhn,m = f (W(h)[hn\u22121,m; \u03c3n\u22121,m] + W(v)[hn,m\u2212L; \u03c3n,m\u2212L] + c), (4)\nwhere f is a non-linear activation function of the RNN cell (green circle in Figure 1b). In this work, we use the exponential linear unit or ELU activation function. The brackets [. . . ; . . . ] represent a vector concatenation operation. The parameters U, W(h), W(v) and b, c in Equations (3) and (4) are, respectively, the weights and biases of the RNN. They are encompassed in the trainable parameters \u03b8 of the RNN ansatz. The sampling of new spin configurations is performed autoregressively in a zigzag fashion (along the horizontal blue lines in Figure 1b) to capture the 2D structure of the lattice [18]. It is employed with the hope of sampling the degenerate ground-state configurations of the Newman\u2013Moore model, provided that the RNN ansatz can capture its multi-modal distribution. It was shown that this approach was superior to the Markov chain Monte Carlo sampling in disordered spin glasses [27]. We used the Vanilla RNN cell in this work as we did not observe substantial improvements in using more powerful representations such as the Gated Recurrent Unit or GRU cells. We equally maintained a weight-sharing approach across all the lattice sites for the same reasons. We note, however, that more powerful representations of RNN cells, such as Tensorized RNN cells [27], could enhance the representation power of our RNN ansatz. For the quantum case, we use the RNN ansatz in Equation (2) to model the groundstate wavefunction amplitude of the Hamiltonian as:\n\u03a8\u03b8(\u03c3) = \u221a p\u03b8(\u03c3). (5)\nThe stochastic nature of the quantum Newman\u2013Moore model allows for representing the wavefunction amplitudes with real and positive numbers instead of complex ones. For more details on RNN ansatzes, the interested reader is referred to [18,27]."
        },
        {
            "heading": "3.2. Variational Neural Annealing",
            "text": "Variational neural annealing is a recently introduced method used to solve optimization problems by representing the instantaneous probability distribution of the system with neural networks during the annealing procedure [27]. Its classical formulation\u2014\ndubbed Variational Classical Annealing (VCA)\u2014involves training a neural network ansatz to minimize the variational free energy at temperature T:\nF = \u3008H\u3009\u03b8\u2212 TS(p\u03b8). (6)\n\u3008H\u3009\u03b8 stands for the ensemble averages of the Newman\u2013Moore Hamiltonian in Equation (1). It is computed by taking Monte Carlo averages over a finite number of Ns samples drawn from the RNN probability distribution p\u03b8(\u03c3). Given that p\u03b8(\u03c3) is normalized by construction, the von Neumann entropy S(p\u03b8) = \u2212\u3008log p\u03b8(\u03c3)\u3009\u03b8 is equally estimated at a moderate computational cost. The parameters \u03b8 are trained till F converges over a number of Nwarmup steps using gradient-based methods (see warmup step in Algorithm 1). Note that the variational free energy is regarded as a loss function L to mimic machine learning jargon. In this work, the Adam method [40] is utilized with the batch gradient descent method. The gradients of the free energy are computed efficiently using automatic differentiation, and their noise is mitigated using control-variate methods. The temperature is slowly reduced during the annealing process according to a userdefined schedule. As is typical in simulated annealing calculations, we employ a linear annealing schedule T(t) = T0(1\u2212 t) with t \u2208 [0, 1]. Transfer learning of parameters \u03b8 between subsequent annealing steps is implemented, as it was shown to help maintain the stability of the annealing protocol. During the annealing process, the system shifts from maximizing entropy to minimizing energy, thus finding the ground state of the Newman\u2013Moore model provided that the rate of annealing is sufficiently slow, the ansatz is expressive enough, and the loss landscape of the free energy allows for efficient training of the RNN. When the neural annealing algorithm is driven by quantum fluctuations instead of thermal ones, it is referred to as Variational Quantum Annealing or VQA. VQA is based on the Variational Monte Carlo method, where the RNN wavefunction \u03a8\u03b8(\u03c3) in Equation (5) is used to approximate the ground-state wavefunction of the quantum Newman\u2013Moore model. This is achieved by minimizing the so-called variational energy of the Hamiltonian:\nE = \u3008\u03a8\u03b8|H\u0302|\u03a8\u03b8\u3009 \u2212 \u0393\u3008\u03a8\u03b8| N\n\u2211 i=1 \u03c3xi |\u03a8\u03b8\u3009, (7)\nwhere H\u0302 is the Hamiltonian operator for the Newman\u2013Moore spin model in Equation (1). E turns out to be an appropriate loss function given that it serves as an upper bound to the Hamiltonian ground energy. In VQA, the training and annealing procedures are implemented similarly to VCA, albeit at the difference of using the variational energy E as a loss function with a linear annealing of the transverse field \u0393. Note, however, that the computational complexity of implementing a gradient descent step is O(N2) for VQA while being O(N) for VCA. This is due to the off-diagonal term of the transverse field. Thus, to account for larger computation costs, smaller system sizes will be used when presenting the VQA results in the next section. A generic description of the variational neural annealing protocol (including both VCA and VQA) is displayed in Algorithm 1. More in-depth details of the neural annealing procedure can be found in Reference [27].\nAlgorithm 1 Variational neural annealing\nInitialization Step: Randomly initialize RNN parameters \u03b8 Set T = T0 for VCA ( \u0393 = \u03930 for VQA) Set loss function L as Equation (6) for VCA ( Equation (7) for VQA)\nWarmup Step: for t = 1, . . . , Nwarmup do\nGenerate Ns samples \u03c3 from Equation (2) Update \u03b8 by minimizing L\nend for\nAnnealing Step: do\nT \u2190 T \u2212 \u03b4T (\u0393\u2190 \u0393\u2212 \u03b4\u0393 ) for t = 1, . . . , Ntrain do\nGenerate Ns samples \u03c3 from Equation (2) Update \u03b8 by minimizing L\nend for while T 6= 0 (\u0393 6= 0) Generate desired samples \u03c3 of Hamiltonian in Equation (1)"
        },
        {
            "heading": "3.3. Loss Landscape Visualization",
            "text": "To study trainability in the neural annealing protocol, we employ a technique used to obtain a qualitative description of the loss landscape geometry in a neighborhood of the current network parameters \u03b8\u2217 [24,41]. This involves plotting the loss function L on a randomly chosen plane given by:\nf (\u03b1, \u03b2) = L(\u03b8\u2217 + \u03b1\u03b4 + \u03b2\u03b7), (8)\nwhere \u03b4 and \u03b7 are vectors of length ||\u03b8\u2217|| whose entries are N (0, 1). \u03b1 and \u03b2 are arbitrary real numbers defining the 2D scan of the loss landscape. Although this is an arbitrary slice of the loss function high-dimensional space, it gave insight into deep neural networks\u2019 trainability and generalization properties. As shown in Algorithm 2, for each point in the vicinity of the current optimal parameters \u03b8\u2217, new Ns samples are generated to compute the value of the loss function at that point. This makes the landscape topography dependent on the number of samples that are used. Ns is therefore fixed to the finite value used in the neural annealing simulations so that the landscape geometry reflects what the simulations experience. Recall that the loss function in Equation (8) is, respectively, the variational free energy F when VCA is implemented or the variational energy E when it is VQA that is used. Thus, according to the variational adiabatic theorem introduced in [27], the variational annealing protocol is guaranteed to be adiabatic, provided that L remains convex in the vicinity of \u03b8\u2217 throughout the annealing procedure. Note that the filter-wise normalization technique of [24] is not implemented here, given that the RNN ansatz used is not scale-invariant due to the presence of ELU activation functions. A principal component analysis approach, similar to the one used in the visualization of variational quantum circuit landscapes, could also be implemented [26]. However, we found the previously described visualization method sufficient to interpret our results.\nAlgorithm 2 Loss landscape visualization procedure\nInitialization: Set a 2D grid of Npoints \u00d7 Npoints Set the magnitude of each point (\u03b1, \u03b2) in the 2D grid Set the Gaussian random directions \u03b4 and \u03b7\nfor each point (\u03b1, \u03b2) in the 2D grid do \u03b8 \u2217 = \u03b8\u2217 + \u03b1\u03b4 + \u03b2\u03b7\nGenerate Ns samples \u03c3 \u223c p\u03b8\u2217(\u03c3) Compute L as Equation (6) for VCA (Equation (7) for VQA)\nend for Plot the function f (\u03b1, \u03b2) in Equation (8)"
        },
        {
            "heading": "4. Results and Discussion",
            "text": ""
        },
        {
            "heading": "4.1. Variational Classical Annealing",
            "text": "In this section, we present the results of implementing the variational classical annealing method to find the ground state of the Newman\u2013Moore model. The system is firstly equilibrated at a high temperature T0 = 10 to provide enough thermal energy for exploration in the spirit of simulated annealing. This is done by minimizing the variational free energy F by training the RNN ansatz over Nwarmup = 1000 warmup steps. Then, annealing is performed by slowly reducing the temperature linearly over Nannealing = 10,000 annealing steps. These parameters were used to find the ground-state energy of the 2D Edwards\u2013Anderson spin glass with 99.999% accuracy on a 40\u00d7 40 lattice [27]; thus, we adopt them here. Details on all the hyper-parameters used in this work can be found in Table A1 in the Appendix B. Figure 2a shows the instantaneous free energy per spin during the VCA protocol for lattices of size L = 5, 16 whose ground states are non-degenerate. The red curve in Figure 2a corresponds to the analytical results for the free energy density, which are exact for L = 2k [30]. As expected, the variational free energy (dark curve) values are consistently above the exact result since it acts as an upper bound to the true free energy. However, it is interesting to observe that the annealing process is not smooth, with F occasionally deviating from its true value. This phenomenon is even more pronounced for the system size with 16\u00d7 16 spins. This may be caused by the chaotic geometry of the instantaneous loss landscape, as shown in the next section. Even though, at the end of annealing, i.e., at T = 0, VCA recovers the correct ground-state energy of the Newman\u2013Moore model (despite the simulation\u2019s instabilities in the training procedure), we have noticed that this is not always the case for different initial conditions, even, at times, for small system sizes. Next, we perform simulations with an increased difficulty level by considering lattice sizes L for which the ground state has many degenerate solutions. The results of the free energy for L = 3, 6 are displayed in Figure 2b. They seem to follow the same pattern as their non-degenerate counterparts, displaying a successful finding of the ground-state energy despite the somewhat chaotic annealing dynamics. Unfortunately, we have also noticed that different initializations may end up in excited states. Furthermore, even for runs where the Newman\u2013Moore ground-state energy is found, we have observed that the solution is almost always the trivial ground-state configuration. At the end of annealing, even after sampling a large number of configurations autoregressively, only the trivial ground-state configuration is found. Appendix A shows the only case where we noticed that the RNN was able to capture the multi-modal distribution of the ground state. In general, it seems that the model is never able to capture the multi-modal distribution of the ground-state configurations. In every instance in which VCA was tested, annealing always concluded with the unique configuration being sampled\u2014regardless of seeding, system size, or (large) initial temperature. It is difficult to ascertain the exact cause of this mode collapse. Nonetheless, this preference for sampling a single configuration may\nparallel the frozen dynamics of conventional Monte Carlo impaired by fracton excitations, potentially translating into trainability issues in VCA."
        },
        {
            "heading": "4.2. Variational Quantum Annealing",
            "text": "We now investigate the behavior of variational quantum annealing in finding the ground state of the Newman\u2013Moore model. For comparison purposes with VCA, we perform simulations on degenerate (L = 3) and non-degenerate lattice (L = 5) sizes for which exact results are available. The exact results are obtained using the Lanczos algorithm. We equally use the same rate of annealing as in VCA. This is done by equilibrating the system at a large value of the transverse field \u03930 = 10 before its linear decrease over Nannealing = 10,000 annealing steps. Figure 3 displays the instantaneous variational energy per spin during the annealing process. It is interesting to note that, in contrast to VCA (see Figure 2), the annealing dynamics is in general much smoother. As shown in the insets, a significant jump is observed after the quantum phase transition (\u0393 < 1); then, the variational energy E falls back to its exact value a few steps before annealing ends. However, this does not seem to affect the solution of the optimization problem as we have noticed that, for the same number of runs, VQA and VCA in general find the ground-state energy the same number of times. A possible reason for this is could be the fast convergence of directly optimizing the Newman\u2013Moore model in Equation (1) for the system sizes used in VQA (data not shown). Note that, despite finding the correct ground-state energy, VQA suffers from the same mode collapse issues as VCA. It often displays a strong preference for sampling only the trivial configuration once annealing is completed, regardless of system size, initial seed, or large initial transverse field (except for the example discussed in Appendix A). We equally notice that penalizing the trivial ground state does not solve the mode collapse issue, resulting in another spin configuration\u2019s mode collapse. Again, it seems that the presence of topological quasi-particle excitations in the configurational space of the quantum Newman\u2013Moore model, which hinders practical QMC simulations, somehow translates into the inability of the variational energy loss function to capture the multi-modal distribution of the ground-state configurations."
        },
        {
            "heading": "4.3. Loss Landscapes",
            "text": "To further understand the issues encountered during VCA and VQA simulations, we probe their loss landscapes during their neural annealing process. In Figure 4a, we show the variational free energy density landscape at the beginning, middle, and end of annealing of the simulation previously shown in Figure 2a. For each annealing snapshot, the loss landscape is plotted in the vicinity of the RNN current parameters \u03b8\u2217, in two random directions, as elaborated in Section 3.3. A system size of 5\u00d7 5 spins is considered. Note that we have not observed significant qualitative differences between landscapes of degenerate and non-degenerate lattice sizes. The free energy is rescaled between [\u22121, 1], as indicated by the color bar. A couple of interesting qualitative phenomena are observable here. Before annealing at T = 10, there is a characteristic bright spot at the center of the panel, indicating that the current RNN parameters likely represent the global minimum of F. Thus, it points to a successful implementation of the warmup step. Mid-annealing seems to retain the same feature, although with a slightly reduced radius of the bright spot. This is somewhat expected given that, for p\u03b8\u2217(\u03c3) to represent the Boltzmann distribution at each temperature, the curvature around the minimum should retain its convexity. Note that some regions of low free energy separated by barriers of higher free energy are visible. This might explain the sharp deviation sometimes observed during the annealing dynamics in Figure 2a. After annealing has been completed, the landscape displays a broad plateau of constant free energy with a distinctive delimitation with a higher energy plateau and rapidly oscillating barriers. This corresponds to parameter regimes in which the same configuration is sampled exclusively and is likely the signature of the strong preference that the network has for sampling a single configuration at T = 0. In Figure 4b, loss landscapes corresponding to VQA simulations in Figure 3 (for L = 5) are displayed. The first two panels\u2019 qualitative behaviors are similar to the ones of VCA. At mid-annealing, though, we observe a large region of constant low energy close to \u03b8\u2217, which may evolve to local minima as the transverse field is reduced. This may explain why the variational energy sometimes gets excited out of its exact course. The snapshot at \u0393 = 0 shows that around \u03b8\u2217, the variational energy has a local minimum separated by high-energetic barriers. This feature should be expected for a successful VQA run. We equally note the presence of another region of low energy, which might point to a regime of parameters for which low-lying excited states are sampled. However, we note\nthat, in general, we did not observe such a trend in other VQA runs; thus, we can only remain speculative in our observation. Furthermore, it is important to mention that, from the plots in Figure 4, it is difficult to ascertain why the VCA simulations seem more chaotic than VQA simulations. Analyzing the low-dimensional optimization trajectories during annealing might shed more light on this issue and is worth further investigation.\na\nNext, we look at the loss landscape geometry for larger lattices for which trainability issues were more pronounced. Figure 5 shows 3D snapshots of the loss landscape during the VCA simulations of Figure 2a. In the first panel, before the warmup step, we observe that the free energy landscape has a local minimum around the parameters \u03b8\u2217. This is likely a random event, given that the RNN parameters were randomly initialized. After the warmup, the landscape maintains its shape, except that the variational free energy minimum value has now converged close to the exact one at T0 = 10. As the temperature is reduced, the landscape shape becomes more rugged, with the appearance of sizeable high-energy plateaus and rapidly changing barriers, eventually leading to the disappear-\nance of the local minimum into an utterly chaotic landscape at the end of annealing. Thus, from this standpoint, it is evident that trainability issues are at work here, hindering a successful application of variational neural annealing.\n\u00c6\n\u00b01.0 \u00b00.5\n0.0 0.5 1.0 \u00d8 \u00b01.0 \u00b00.5\n0.0\n0.5\n1.0\nL os\ns fu\nn ct\nio n\n\u00b03\n\u00b02\n\u00b01\n0\nFree energy before warmup: T0 = 10\n\u00c6\n\u00b01.0 \u00b00.5\n0.0 0.5\n1.0\n\u00d8\n\u00b01.0 \u00b00.5\n0.0\n0.5\n1.0\nL os\ns fu\nn ct\nio n\n\u00b06\n\u00b04\n\u00b02\n0\nFree energy after warmup: T0 = 10\n\u00c6\n\u00b01.0 \u00b00.5\n0.0 0.5\n1.0\n\u00d8\n\u00b01.0 \u00b00.5\n0.0\n0.5\n1.0\nL os\ns fu\nn ct\nio n \u00b05 \u00b04 \u00b03 \u00b02 \u00b01 0\nFree energy during annealing T = 8.33: T0 = 10\n\u00c6\n\u00b01.0 \u00b00.5\n0.0 0.5\n1.0\n\u00d8\n\u00b01.0 \u00b00.5\n0.0\n0.5\n1.0\nL os\ns fu\nn ct\nio n \u00b04 \u00b03 \u00b02 \u00b01 0\nFree energy during annealing T = 6.67: T0 = 10\nL o ss\nfu n ct\nio n"
        },
        {
            "heading": "5. Conclusions",
            "text": "We have implemented the variational neural annealing method using RNN ansatzes to find the ground state of the 2D Newman\u2013Moore model on a triangular lattice. We have observed that, even when the ground-state energy was found, the neural annealing dynamics often displayed strong deviations from the instantaneous free energy for VCA, and ground-state energy for VQA, the effect being more pronounced in the former case. Furthermore, we noticed that even when VCA and VQA succeeded in finding the exact ground-state energy at the end of annealing, they consistently failed to identify the other ground-state configurations of degenerate lattices, only finding the trivial one (except for very small system sizes). These results indicate that the glassy dynamics exhibited by the Newman\u2013Moore model due to the presence of fracton excitations likely manifests as training issues and mode collapse in neural annealing protocols. To shed more light on our findings, we analyzed the loss landscape topologies of the VCA and VQA cost functions using a visualization technique [24] borrowed from the machine learning community. We noticed that the instabilities during the annealing protocol were caused by the chaotic geometry of the loss function, thus impeding the effective training of the RNN parameters. This result points to a potential link between glassiness in the configurational landscape of the Hamiltonian with trainability issues in the parameters space of the loss landscape. A more in-depth investigation of this phenomenon is needed. Potential directions could be to study the optimization paths during annealing or the effect of different optimizers (e.g., stochastic reconfiguration [42]), especially those incorporating knowledge of the loss function curvature (e.g., the Hessian). Using more representative neural network architectures is also an option, even though we argue that the expressivity of the RNN ansatz used in this work is enough to represent probability amplitudes (being Turing complete) and that the simulation issues observed mainly come\nfrom the complex loss landscapes. However, as skip connections were shown to provide a smoother landscape in convolutional neural networks [24], this is a possibility that cannot be completely ruled out. Encoding the symmetries of the Newman\u2013Moore model in the RNN ansatz could also help to improve the simulations. With artificial neural networks becoming standard tools to probe condensed matter systems, it is important to understand their features that contribute to efficient simulations. Symmetries, entanglement, and expressivity [43,44] are some features that have already been shown to be important. In this work, we showed that neural network learnability is also essential. Understanding when, how, and where conservation of computational complexity occurs is primordial in employing neural networks to tackle complex systems. It will provide a solid framework for which they might (or not) outperform traditional methods, which fail on significant condensed matter problems such as non-stochastic systems [22,23].\nAuthor Contributions: Conceptualization, E.M.I., S.M. and R.G.M.; Data curation, E.M.I. and S.M.; Formal analysis, E.M.I. and S.M.; Funding acquisition, E.M.I. and R.G.M.; Software, E.M.I. and S.M.; Writing\u2014original draft, E.M.I.; Writing\u2014review and editing, E.M.I., S.M. and R.G.M. All authors have read and agreed to the published version of the manuscript.\nFunding: Resources used in preparing this research were provided, in part, by the Province of Ontario, the Government of Canada through CIFAR, and companies sponsoring the Vector Institute www.vectorinstitute.ai/#partners. This work was made possible by the facilities of the Shared Hierarchical Academic Research Computing Network (SHARCNET) and Compute Canada. This work was supported by NSERC, the Canada Research Chair program, and the Perimeter Institute for Theoretical Physics. Research at the Perimeter Institute is supported in part by the Government of Canada through the Department of Innovation, Science and Economic Development Canada and by the Province of Ontario through the Ministry of Economic Development, Job Creation and Trade.\nInstitutional Review Board Statement: Not applicable.\nInformed Consent Statement: Not applicable.\nData Availability Statement: The data are contained within the article.\nAcknowledgments: We thank Jeremy C\u00f4t\u00e9, Mohamed Hibat-Allah, Juan Carrasquilla, Sebastian Wetzel, Zheng Zhou and Andrey Gromov for the fruitful discussions. We thank Sebastiano Pilati for his valuable comments on the manuscript.\nConflicts of Interest: The authors declare no conflict of interest."
        },
        {
            "heading": "Appendix A",
            "text": "This section provides additional neural annealing results on a degenerate system size of the Newman\u2013Moore model. In Figure A1, we show results of VCA on a lattice of size L = 3 (smallest size to have degenerate ground states) corresponding to the end of annealing in Figure 2b. At the end of annealing, we observed that the 100 number of training samples are all in one of the four ground-state configurations. For statistics purposes, in Figure A1a, we show principal component analysis results on 13,222 new spin configurations sampled autoregressively from the RNN ansatz at the end of annealing. The color bar represents the Hamming distance between a new spin configuration \u03c3 and the configuration \u03c3\u2217 where all spins point down. It is given by:\nd(\u03c3, \u03c3\u2217) = \u2016\u03c3 \u2212 \u03c3\u2217\u20161, (A1)\nwhere \u2016 . . . \u20161 stands for the L1 norm. We indeed observe that the RNN samples the four distinct ground-state configurations. The blue dot with Hamming distance zero represents the trivial ground state. The three other red dots represent the three other ground states, each one separated from the trivial one by six spin flips. Thus, we show that, for this case, the RNN model is able to capture the multi-modal distribution of the ground state. Note\nthat in Figure A1b, the distribution of the ground-state configurations is almost uniform, with a slight advantage for the first ground state (the trivial one).\na\nGS 1 GS 2 GS 3 GS 4 Degenerate ground states\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nP ro\nb ab\nil it y\nb\n\u00b01.5 \u00b01.0 \u00b00.5 0.0 0.5 1st principal component\n\u00b01.0\n\u00b00.5\n0.0\n0.5\n2n d\np ri n ci\np al\nco m\np on\nen t\n0\n1\n2\n3\n4\n5\n6\nFigure A1. (a) Principal component analysis of 13,222 degenerate ground-state configurations obtained at the end of annealing of Figure 2b for L = 3. The color bar represents the Hamming distance between the solutions obtained from the RNN ansatz and the trivial ground-state configuration. (b) Probability distribution of the ground-state configurations.\nVQA simulations in Figure 3 also capture the multi-modal ground-state distribution (data not shown). However, we have noticed that for both VCA and VQA, not all runs of neural annealing were able to capture all the ground states, with some finding only the trivial one. We have also observed that penalizing the loss function in VCA and VQA with the trivial ground-state magnetization often resulted in mode collapse into another ground-state configuration. It is, however, possible that a more elaborate regularization function recovers the other configurations. Furthermore, for the larger degenerate system sizes, such as L = 6 shown in Figure 2b, for successful runs, the simulations always resulted in the trivial ground-state configuration, even for runs with a more considerable annealing time, number of training samples, or number of hidden state variables."
        },
        {
            "heading": "Appendix B",
            "text": "Table A1. Hyper-parameters used to perform neural annealing and loss landscape visualization.\nHyper-Parameters Values\nInitial temperature T0 = 10 Initial transverse field \u03930 = 10\nNumber of warmup steps Nwarmup = 1000 Number of training steps Ntrain = 5\nNumber of annealing steps Nannealing = 10,000 Number of samples Ns = 100\nBatch size Nb = 10 Hidden state dimension dh = 40\nLearning rate \u03b7 = 10\u22124\nNumber of grid points Npoints = 200"
        }
    ],
    "title": "Neural Annealing and Visualization of Autoregressive Neural Networks in the Newman\u2013Moore Model",
    "year": 2022
}