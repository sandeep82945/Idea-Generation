{
    "abstractText": "We present a domain-independent unsupervised topic segmentation approach based on hybrid document indexing. Lexical chains have been successfully employed to evaluate lexical cohesion of text segments and to predict topic boundaries. Our approach is based in the notion of semantic cohesion. It uses spectral embedding to estimate semantic association between content nouns over a span of multiple text segments. Our method significantly outperforms the baseline on the topic segmentation task and achieves performance comparable to state-of-the-art methods that incorporate domain specific information.",
    "authors": [
        {
            "affiliations": [],
            "name": "Irina Matveeva"
        }
    ],
    "id": "SP:f08a5c469e3ce7566a27831f8e0a3c999aa3351b",
    "references": [
        {
            "authors": [
                "J. Allan",
                "editor"
            ],
            "title": "2002.Topic Detection and Tracking: Event-based Information Organization",
            "year": 2002
        },
        {
            "authors": [
                "Doug Beeferman",
                "Adam Berger",
                "John D. Lafferty"
            ],
            "title": "Statistical models for text segmentation",
            "year": 1999
        },
        {
            "authors": [
                "Paolo Bientinesi",
                "Inderjit S. Dhilon",
                "Robert A. van de Geijn"
            ],
            "title": "A parallel eigensolver for dense symmetric matrices based on multiple relatively robust",
            "venue": "Machine Learning,",
            "year": 2003
        },
        {
            "authors": [
                "Freddy Choi",
                "Peter Wiemer-Hastings",
                "Johanna Moore."
            ],
            "title": "Latent Semantic Analysis for text segmentation",
            "venue": "InProceedings of EMNLP, pages 109\u2013 117.",
            "year": 2001
        },
        {
            "authors": [
                "J.G. Fiscus",
                "George Doddington",
                "John S. Garofolo",
                "Alvin Martin."
            ],
            "title": "NIST\u2019s 1998 topic detection and tracking evaluation (tdt2)",
            "venue": "InProceedings of NIST\u2019s 1998 Topic Detection and Tracking Evaluation.",
            "year": 1998
        },
        {
            "authors": [
                "M. Galley",
                "K. McKeown",
                "E. Fosler-Lussier",
                "H. Jing."
            ],
            "title": "Discourse segmentation of multi-party conversation",
            "venue": "InProceedings of ACL.",
            "year": 2003
        },
        {
            "authors": [
                "V. Hatzivassiloglou",
                "Luis Gravano",
                "Ankineedu Maganti."
            ],
            "title": "An investigation of linguistic features and clustering algorithms for topical document clustering",
            "venue": "Proceedings of SIGIR, pages 224\u2013231.",
            "year": 2000
        },
        {
            "authors": [
                "V. Hatzivassiloglou",
                "Melissa L. Holcombe",
                "Kathleen R. McKeown"
            ],
            "title": "Simfinder: A flexible",
            "year": 2001
        },
        {
            "authors": [
                "Marti A. Hearst."
            ],
            "title": "Multi-paragraph segmentation of expository text",
            "venue": "InProceedings of ACL, pages 9\u201316.",
            "year": 1994
        },
        {
            "authors": [
                "Hideki Kozima."
            ],
            "title": "Text segmentation based on similarity between words",
            "venue": "InProceedings of ACL, pages 286\u2013288.",
            "year": 1993
        },
        {
            "authors": [
                "Gina-Anne Levow",
                "Douglas W. Oard",
                "Philip Resnik."
            ],
            "title": "Dictionary-based techniques for cross-language information retrieval",
            "venue": "Information Processing and Management: Special Issue on Cross-language Information Retrieval.",
            "year": 2005
        },
        {
            "authors": [
                "Irina Matveeva",
                "Gina-Anne Levow."
            ],
            "title": "Graphbased Generalized Latent Semantic Analysis for document representation",
            "venue": "In",
            "year": 2006
        },
        {
            "authors": [
                "Irina Matveeva",
                "Gina-Anne Levow",
                "Ayman Farahat",
                "Christian Royer"
            ],
            "title": "Generalized Latent Semantic Analysis for term representation",
            "venue": "Proc. of the TextGraphs Workshop",
            "year": 2005
        },
        {
            "authors": [
                "Lev Pevzner",
                "Marti A. Hearst."
            ],
            "title": "A critique and improvement of an evaluation metric for text segmentation",
            "venue": "Comput. Linguist., 28(1):19\u201336.",
            "year": 2002
        },
        {
            "authors": [
                "Noam Slonim",
                "Naftali Tishby."
            ],
            "title": "Document clustering using word clusters via the information bottleneck method",
            "venue": "InResearch and Development in Information Retrieval, pages 208\u2013215.",
            "year": 2000
        },
        {
            "authors": [
                "Peter D. Turney."
            ],
            "title": "Mining the web for synonyms: PMI\u2013IR versus LSA on TOEFL",
            "venue": "Lecture Notes in Computer Science, 2167:491\u2013502.",
            "year": 2001
        },
        {
            "authors": [
                "C. Wayne."
            ],
            "title": "Multilingual topic detection and tracking: Successful research enabled by corpora and evaluation",
            "venue": "Proceedings of Language Resources and Evaluation Conference (LREC), pages 1487\u20131494.",
            "year": 2000
        }
    ],
    "sections": [
        {
            "text": "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp. 351\u2013359, Prague, June 2007. c\u00a92007 Association for Computational Linguistics\nWe present a domain-independent unsupervised topic segmentation approach based on hybrid document indexing. Lexical chains have been successfully employed to evaluate lexical cohesion of text segments and to predict topic boundaries. Our approach is based in the notion of semantic cohesion. It uses spectral embedding to estimate semantic association between content nouns over a span of multiple text segments. Our method significantly outperforms the baseline on the topic segmentation task and achieves performance comparable to state-of-the-art methods that incorporate domain specific information."
        },
        {
            "heading": "1 Introduction",
            "text": "The goal of topic segmentation is to discover story boundaries in the stream of text or audio recordings. Story is broadly defined as segment of text containing topically related sentences. In particular, the task may require segmenting a stream of broadcast news, addressed by the Topic Detection and Tracking (TDT) evaluation project (Wayne, 2000; Allan, 2002). In this case topically related sentences belong to the same news story. While we are considering TDT data sets in this paper, we would like to pose the problem more broadly and consider a domainindependent approach to topic segmentation.\nPrevious research on topic segmentation has shown that lexical coherence is a reliable indicator of topical relatedness. Therefore, many approaches\nhave concentrated on different ways of estimating lexical coherence of text segments, such as semantic similarity between words (Kozima, 1993), similarity between blocks of text (Hearst, 1994), and adaptive language models (Beeferman et al., 1999). These approaches use word repetitions to evaluate coherence. Since the sentences covering the same story represent a coherent discourse segment, they typically contain the same or related words. Repeated words build lexical chains that are consequently used to estimate lexical coherence. This can be done either by analyzing the number of overlapping lexical chains (Hearst, 1994) or by building a short-range and long-range language model (Beeferman et al., 1999). More recently, topic segmentation with lexical chains has been successfully applied to segmentation of news stories, multi-party conversation and audio recordings (Galley et al., 2003).\nWhen the task is to segment long streams of text containing stories which may continue at a later point in time, for example developing news stories, building of lexical chains becomes intricate. In addition, the word repetitions do not account for synonymy and semantic relatedness between words and therefore may not be able to discover coherence of segments with little word overlap.\nOur approach aims at discovering semantic relatedness beyond word repetition. It is based on the notion of semantic cohesion rather than lexical cohesion. We propose to use a similarity metric between segments of text that takes into account semantic associations between words spanning a number of segments. This method approximates lexical chains by averaging the similarity to a number of previous text\n351\nsegments and accounts for synonymy by using a hybrid document indexing scheme. Our text segmentation experiments show a significant performance improvement over the baseline.\nThe rest of the paper is organized as follows. Section 2 discusses hybrid indexing. Section 3 describes our segmentation algorithm. Section 5 reports the experimental results. We conclude in section 6."
        },
        {
            "heading": "2 Hybrid Document Indexing",
            "text": "For the topic segmentation task we would like to define a similarity measure that accounts for synonymy and semantic association between words. This similarity measure will be used to evaluate semantic cohesion between text units and the decrease in semantic cohesion will be used as an indicator of a story boundary. First, we develop a document representation which supports this similarity measure.\nCapturing semantic relations between words in a document representation is difficult. Different approaches tried to overcome the term independence assumption of the bag-of-words representation (Salton and McGill, 1983) by using distributional term clusters (Slonim and Tishby, 2000) and expanding the document vectors with synonyms, see (Levow et al., 2005). Since content words can be combined into semantic classes there has been a considerable interest in low-dimensional representations. Latent Semantic Analysis (LSA) (Deerwester et al., 1990) is one of the best known dimensionality reduction algorithms. In the LSA space documents are indexed with latent semantic concepts. LSA maps all words to low dimensional vectors. However, the notion of semantic relatedness is defined differently for subsets of the vocabulary. In addition, the numerical information, abbreviations and the documents\u2019 style may be very good indicators of their topic. However, this information is no longer available after the dimensionality reduction.\nWe use a hybrid approach to document indexing to address these issues. We keep the notion of latent semantic concepts and also try to preserve the specifics of the document collection. Therefore, we divide the vocabulary into two sets: nouns and the rest of the vocabulary. The set of nouns does not include proper nouns. We use a method of spectral embedding, as described below and compute a\nlow-dimensional representation for documents using only the nouns. We also compute atf-idf representation for documents using the other set of words. Since we can treat each latent semantic concept in the low-dimensional representation as part of the vocabulary, we combine the two vector representations for each document by concatenating them."
        },
        {
            "heading": "2.1 Spectral Embedding",
            "text": "A vector space representation for documents and sentences is convenient and makes the similarity metrics such as cosine and distance readily available. However, those metrics will not work if they don\u2019t have a meaningful linguistic interpretation.\nSpectral methods comprise a family of algorithms that embed terms and documents in a lowdimensional vector space. These methods use pairwise relations between the data points encoded in a similarity matrix. The main step is to find an embedding for the data that preserves the original similarities.\nGLSA We use Generalized Latent Semantic Analysis (GLSA) (Matveeva et al., 2005) to compute spectral embedding for nouns. GLSA computes term vectors and since we would like to use spectral embedding for nouns, it is well-suited for our approach. GLSA extends the ideas of LSA by defining different ways to obtain the similarities matrix and has been shown to outperform LSA on a number of applications (Matveeva and Levow, 2006).\nGLSA begins with a matrix of pair-wise term similaritiesS, computes its eigenvectorsU and uses the first k of them to represent terms and documents, for details see (Matveeva et al., 2005). The justification for this approach is the theorem by Eckart and Young (Golub and Reinsch, 1971) stating that inner product similarities between the term vectors based on the eigenvectors ofS represent the best elementwise approximation to the entries inS. In other words, the inner product similarity in the GLSA space preserves the semantic similarities inS.\nSince our representation will try to preserve semantic similarities inS it is important to have a matrix of similarities which is linguistically motivated."
        },
        {
            "heading": "2.2 Distributional Term Similarity",
            "text": "PMI Following (Turney, 2001; Matveeva et al., 2005), we use point-wise mutual information (PMI) to compute the matrixS. PMI between random variables representing the wordswi andwj is computed as\nPMI(wi, wj) = log P (Wi = 1,Wj = 1)\nP (Wi = 1)P (Wj = 1) . (1)\nThus, for GLSA,S(wi, wj) = PMI(wi, wj).\nCo-occurrence Proximity An advantage of PMI is the notion of proximity. The co-occurrence statistics for PMI are typically computed using a sliding window. Thus, PMI will be large only for words that co-occur within a small context of fixed size.\nSemantic Association vs. Synonymy Although GLSA was successfully applied to synonymy induction (Matveeva et al., 2005), we would like to point out that the GLSA discovers semantic association in a broad sense. Table 1 shows a few words from the TDT2 corpus and their nearest neighbors in the GLSA space. We can see that for \u201cwitness\u201d, \u201cfinance\u201d and \u201cbroadcast\u201d words are grouped into corresponding semantic classes. The nearest neighbors for \u201chearing\u201d and \u201cstay\u201d represent their different senses. Interestingly, even for the abstract noun \u201csurprise\u201d the nearest neighbors are meaningful."
        },
        {
            "heading": "2.3 Document Indexing",
            "text": "We have two sets of the vocabulary terms: a set of nouns,N , and the other words,T . We computetf-idf document vectors indexed with the words inT :\n~di = (\u03b1i(w1), \u03b1i(w2), ..., \u03b1i(w|T |)), (2)\nwhere\u03b1i(wt) = tf(wt, di) \u2217 idf(wt).\nWe also compute ak-dimensional representation with latent conceptsci as a weighted linear combination of GLSA term vectors~wt:\n~di = (c1, ..., ck) = \u2211\nt=1:|N |\n\u03b1i(wt) \u2217 ~wt, (3)\nWe concatenate these two representations to generate a hybrid indexing of documents:\n~di = (\u03b1i(w1), ..., \u03b1i(w|T |), c1, ...ck) (4)\nIn our experiments, we compute document and sentence representation using three indexing schemes: thetf-idf baseline, the GLSA representation and the hybrid indexing. The GLSA indexing computes term vectors for all vocabulary words; document and sentence vectors are generated as linear combinations of term vectors, as shown above."
        },
        {
            "heading": "2.4 Document similarity",
            "text": "One can define document similarity at different levels of semantic content. Documents can be similar because they discuss the same people or events and because they discuss related subjects and contain semantically related words. Hybrid Indexing allows us to combine both definitions of similarity. Each representation supports a different similarity measure.tf-idf uses term-matching, the GLSA representation uses semantic association in the latent semantic space computed for all words, and hybrid indexing uses a combination of both: term-matching for named entities and content words other than nouns combined with semantic association for nouns.\nIn the GLSA space, the inner product between document vectors contains all pair-wise inner product between their words, which allows one to detect semantic similarity beyond term matching:\n\u3008~di, ~dj\u3009 = \u2211\nw\u2208di\n\u2211\nv\u2208dj\n\u03b1i(w)\u03b1j(v)\u3008~w,~v\u3009 (5)\nIf documents contain words which are different but semantically related, the inner product between the term vectors will contribute to the document similarity, as illustrated with an example in section 5.\nWhen we compare two documents indexed with the hybrid indexing scheme, we compute a combination of similarity measures:\n\u3008~di, ~dj\u3009 = \u2211\nnk\u2208di\n\u2211\nnm\u2208dj\n\u03b1i(nk)\u03b1j(nm)\u3008 ~nk, ~nm\u3009+\n\u2211\nt\u2208T\n\u03b1i(t) \u2217 \u03b1j(t).\n(6)\nDocument similarity contains semantic association between all pairs of nouns and uses term-matching for the rest of the vocabulary."
        },
        {
            "heading": "3 Topic Segmentation with Semantic Cohesion",
            "text": "Our approach to topic segmentation is based on semantic cohesion supported by the hybrid indexing. Topic segmentation approaches use either sentences (Galley et al., 2003) or blocks of words as text units (Hearst, 1994). We used both variants in our experiments. When using blocks, we computed blocks of a fixed size (typically 20 words) sliding over the documents in a fixed step size (10 or 5 words). The algorithm predicts a story boundary when the semantic cohesion between two consecutive units drops. Blocks can cross story boundaries, thus many predicted boundaries will be displaced with respect to the actual boundary.\nAveraged similarity In our preliminary experiments we used the largest difference in score to predict story boundary, following the TextTiling approach (Hearst, 1994). We found, however, that in our document collection the word overlap between sentences was often not large and pair-wise similarity could drop to zero even for sentences within the same story, as will be illustrated below. We could not obtain satisfactory results with this approach.\nTherefore, we used the average similarity by using a history of fixed sizen. The semantic cohesion score was computed for the position between two\next units,ti andtj as follows:\nscore(ti, tj) = 1\nn\nn\u22121\u2211\nk=0\n\u3008ti\u2212k, tj\u3009 (7)\nOur approach predicts story boundaries at the minima of the semantic cohesion score.\nApproximating Lexical Chains One of the motivations for our cohesion score is that it approximates lexical chains, as for example in (Galley et al., 2003). Galley et al. (Galley et al., 2003) define lexical chainsR1, .., RN by considering repetitions of termst1, .., tN and assigning larger weights to short and compact chains. Then the lexical cohesion score between two text unitsti andtj is based on the number of chains that overlap both of them:\nscore(ti, tj) = N\u2211\nk=1\nwk(ti)wk(tj), (8)\nwherewk(ti) = score(Rj) if the chainRj overlapsti and zero otherwise. Our cohesion score takes i to account only the chains for words that occur in tj and have another occurrence withinn previous sentences. Due to this simplification, we compute the score based on inner products. Once we make the transition to inner products, we can use hybrid indexing and compute semantic cohesion score beyond term repetition."
        },
        {
            "heading": "4 Related Approaches",
            "text": "We compare our approach to the LCseg algorithm which uses lexical chains to estimate topic boundaries (Galley et al., 2003). Hybrid indexing allows us to compute semantic cohesion score rather than the lexical cohesion score based on word repetitions.\nChoi at al. used LSA for segmentation (Choi et al., 2001). LSA (Deerwester et al., 1990) is a special case of spectral embedding and Choi at al. (Choi et al., 2001) used all vocabulary words to compute low-dimensional document vectors. We use GLSA (Matveeva et al., 2005) because it computes term vectors as opposed to the dual document-term representation with LSA and uses a different matrix of pair-wise similarities. Furthermore, Choi at al. (Choi et al., 2001) used clustering to predict boundaries whereas we used the average similarity scores.\nExisting approaches to hybrid indexing used different weights for proper nouns, nouns phrase heads and use WordNet synonyms to expand the documents, for example (Hatzivassiloglou et al., 2000; Hatzivassiloglou et al., 2001). Our approach does not require linguistic resources and learning the weights. The semantic associations between nouns are estimated using spectral embedding."
        },
        {
            "heading": "5 Experiments",
            "text": ""
        },
        {
            "heading": "5.1 Data",
            "text": "The first TDT collection is part of the LCseg toolkit1 (Galley et al., 2003) and we used it to compare our approach to LCseg. We used the part of this collection with 50 files with 22 documents each.\nWe also used the TDT2 collection2 of news articles from six news agencies in 1998. We used only 9,738 documents that are assigned to one topic and have length more than 50 words. We used the Lemur toolkit3 with stemming and stop words list for the tf-idf indexing; we used Bikel\u2019s parser4 to obtain the POS-tags and select nouns; we used the PLAPACK package (Bientinesi et al., 2003) to compute the eigenvalue decomposition.\n1http://www1.cs.columbia.edu/ galley/tools.html 2http://nist.gov/speech/tests/tdt/tdt98/ 3http://www.lemurproject.org/ 4http://www.cis.upenn.edu/ dbikel/software.html\nEvaluation For the TDT data we use the error metric pk (Beeferman et al., 1999) and WindowDiff (Pevzner and Hearst, 2002) which are implemented in the LCseg toolkit. We also used the TDT cost metric Cseg5, with the default parameters P(seg)=0.3, Cmiss=1, Cfa=0.3 and distance of 50 words. All these measures look at two units (words or sentences)N units apart and evaluate how well the algorithm can predict whether there is a boundary between them or not. Lower values mean better performance for all measures.\nGlobal vs. Local GLSA Similarity To obtain the PMI values we used the TDT2 collection, denoted as GLSAlocal. Since co-occurrence statistics based on larger collections give a better approximation to linguistic similarities, we also used 700,000 documents from the English GigaWord collection, denoted as GLSA. We used a window of size 8."
        },
        {
            "heading": "5.2 Topic Segmentation",
            "text": "The first set of experiments was designed to evaluate the advantage of the GLSA representation over the baseline. We compare our approach to the LCseg algorithm (Galley et al., 2003) and use sentences as segmentation unit. To avoid the issue of parameters setting when the number of boundaries is not known, we provide each algorithm with the actual numbers\n5www.nist.gov/speech/tests/tdt/tdt98/doc/ tdt2.eval.plan.98.v3.7.ps\nof boundaries.\nTDT We use the LCseg approach and our approach with the baselinetf-idf representation and the GLSA representation to segment this corpus. Table 2 shows a few sentences. Many content words are repeated, so the lexical chains is definitely a sound approach. As shown in Table 2, in the first story the word \u201cCuba\u201d or \u201cCuban\u201d is repeated in every sentence thus generating a lexical chain. On the topic boundary, the word overlap between sentences is very small. At the same time, the repetition of words may also be interrupted within a story: sentence 5, 6 and sentences 14, 15, 16 have little word overlap. LCseg deals with this by defining several parameters to control chain length and gaps. This simple example illustrates the potential benefit of semantic cohesion. Table 2 shows that \u201cGeneral Motors\u201d or \u201cGM\u201d are not repeated in every sentence of the second story. However, \u201cGM\u201d, \u201ccarmaker\u201d and\n\u201ccompany\u201d are semantically related. Making this information available to the segmentation algorithm allows it to establish a connection between each sentence of the second story.\nWe computed pair-wise sentence similarities between pairs of consecutive sentences in thetf-idf and GLSA representations. Figure 1 shows the similarity values plotted for each sentence break. The pairwise similarities based on term-matching are very spiky and there are many zeros within the story. The GLSA-based similarity makes the dips in the similarities at the boundaries more prominent. The last plot gives the details for the sentences in table 2. In the tf-idf representation sentences without word overlap receive zero similarity but the GLSA representation is able to use the semantic association between between \u201cemigrants\u201d and \u201crefugees\u201d for sentences 5 and 6, and also the semantic association between \u201ccarmaker\u201d and \u201ccompany\u201d for sentences 14\nand 15. This effect increases as we use the semantic cohesion score as in equation 7. Figure 2 shows the similarity values fortf-idf and GLSA and also the lexical cohesion scores computed by LCseg. The GLSAbased similarities are not quite as smooth as the LCseg scores, but they correctly discover the bound-\naries. LCseg parameters are fine-tuned for this doc-\nument collection. We used a general TDT2 GLSA\nrepresentation for this collection, and the only seg-\nmentation parameter we used is to avoid placing next boundary withinn=3 sentences of the previous one. For this reason the predicted boundary may be one sentence off the actual boundary. These results are summarized in Table 3. The GLSA representation performs significantly better than thef-idf baseline. Itspk and WindowDiff scores with default parameters for LCseg are worse than for LCseg. We attribute it to the fact that we did not fine-tuned our method to this collection and that boundaries are often placed one position off the actual boundary.\nTDT2 For this collection we used three different indexing schemes: thetf-idf baseline, the GLSA representation and the hybrid indexing. Each representation supports a different similarity measure. Our TDT experiments showed that the semantic cohesion score based on the GLSA representation improves the segmentation results. The variant of the TDT corpus we used is rather small and wellbalanced, see (Galley et al., 2003) for details. In the second phase of experiments we evaluate our approach on the larger TDT2 corpus. The experiments were designed to address the following issues:\n\u2022 performance comparison between GLSA and Hybrid indexing representations. As mentioned before, GLSA embeds all words in a low-dimensional space. Whereas semantic\n#b known Method Pmiss Pfa Cseg tf-idf 0.52 0.14 0.19 GLSA 0.4 0.1 0.14 GLSA local 0.44 0.12 0.16 Hybrid 0.34 0.10 0.12 Hybrid local 0.38 0.09 0.13 LCseg 0.80 0.19 0.28 #b unknown Method Pmiss Pfa Cseg tf-idf 0.42 0.2 0.17 GLSA 0.37 0.13 0.14 GLSA local 0.35 0.19 0.14 Hybrid 0.26 0.16 0.11 Hybrid local 0.27 0.18 0.12\nclasses for nouns have theoretical linguistic justification, it is harder to motivate a latent space representation for example for proper nouns. Therefore, we want to evaluate the advantage of using spectral embedding only for nouns.\n\u2022 collection dependence of similarities. The similarity matrix S is computed using the TDT2 corpus (GLSAlocal) and using the larger GigaWord corpus. The larger corpus provides more reliable co-occurrence statistics. On the other hand, word distribution is different from that in the TDT2 corpus. We wanted to evaluate whether semantic similarities are collection independent.\nTable 4 shows the performance evaluation. We show the results computed using blocks containing 20 words (after preprocessing) with step size 10. We tried other parameter values but did not achieve better performance, which is consistent with other research (Hearst, 1994; Galley et al., 2003). We show the results for two settings: predict a known number of boundaries, and predict boundaries using a threshold. In our experiments we used the average of the smallestN scores as threshold,N = 4000 showing best results.\nThe spectral embedding based representations (GLSA, Hybrid) significantly outperform the baseline. This confirms the advantage of the semantic cohesion score vs. term-matching. Hybrid indexing outperforms the GLSA representation supporting our intuition that semantic association is best defined for nouns.\nWe used the GigaWord corpus to obtain the pairwise word associations for the GLSA and Hybrid representations. We also computed GLSAlocal and Hybridlocal using the TDT2 corpus to obtain the pair-wise word associations. The co-occurrence statistics based on the GigaWord corpus provide more reliable estimations of semantic association despite the difference in term distribution. The difference is larger for the GLSA case when we compute the embedding for all words, GLSA performs better than GLSAlocal. Hybridlocal performs only slightly worse than Hybrid. This seems to support the claim that semantic associations between nouns are largely collection independent. On the other hand, semantic associations for proper names are collection dependent at least because the collections are static but the semantic relations of proper names may change over time. The semantic space for a name of a president, for example, is different for the period of time of his presidency and for the time before and after that.\nDisappointingly, we could not achieve good results with LCseg. It tends to split stories into short paragraphs. Hybrid indexing could achieve results comparable to state-of-the art approaches, see (Fiscus et al., 1998) for an overview."
        },
        {
            "heading": "6 Conclusion and Future Work",
            "text": "We presented a topic segmentation approach based on semantic cohesion scores. Our approach is domain independent, does not require training or use of lexical resources. The scores are computed based on the hybrid document indexing which uses spectral embedding in the space of latent concepts for nouns and keeps proper nouns and other specifics of the documents collections unchanged. We approximate the lexical chains approach by simplifying the definition of a chain which allows us to use inner products as basis for the similarity score. The similarity score takes into account semantic relations be-\ntween nouns beyond term matching. This semantic cohesion approach showed good results on the topic segmentation task.\nWe intend to extend the hybrid indexing approach by considering more vocabulary subsets. Syntactic similarity is more appropriate for verbs, for example, than co-occurrence. As a next step, we intend to embed verbs using syntactic similarity. It would also be interesting to use lexical chains for proper names and learn the weights for different similarity scores."
        }
    ],
    "title": "Topic Segmentation with Hybrid Document Indexing",
    "year": 2007
}