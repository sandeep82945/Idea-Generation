{
    "abstractText": "The identification of material parameters occurring in constitutive models has a wide range of applications in practice. One of these applications is the monitoring and assessment of the actual condition of infrastructure buildings, as the material parameters directly reflect the resistance of the structures to external impacts. Physics-informed neural networks (PINNs) have recently emerged as a suitable method for solving inverse problems. The advantages of this method are a straightforward inclusion of observation data. Unlike grid-based methods, such as the least square finite element method (LS-FEM) approach, no computational grid and no interpolation of the data is required. In the current work, we propose PINNs for the calibration of constitutive models from full-field displacement and global force data in a realistic regime on the example of linear elasticity. We show that conditioning and reformulation of the optimization problem play a crucial role in real-world applications. Therefore, among others, we identify the material parameters from initial estimates and balance the individual terms in the loss function. In order to reduce the dependency of the identified material parameters on local errors in the displacement approximation, we base the identification not on the stress boundary conditions but instead on the global balance of internal and external work. We demonstrate that the enhanced PINNs are capable of identifying material parameters from both experimental onedimensional data and synthetic full-field displacement data in a realistic regime. Since displacement data measured by, e.g., a digital image correlation (DIC) system is noisy, we additionally investigate the robustness of the method to different levels of noise.",
    "authors": [
        {
            "affiliations": [],
            "name": "David Antona"
        },
        {
            "affiliations": [],
            "name": "Henning Wesselsa"
        }
    ],
    "id": "SP:436f5978f2fbedf3a4f962a6b922457dd6c3e769",
    "references": [
        {
            "authors": [
                "F.-K. Chang"
            ],
            "title": "Structural Health Monitoring: A Summary Report on the First Stanford Workshop on Structural Health Monitoring",
            "venue": "Tech. rep., Stanford University ",
            "year": 1998
        },
        {
            "authors": [
                "A. Entezami"
            ],
            "title": "Structural Health Monitoring by Time Series Analysis and Statistical Distance Measures",
            "venue": "SpringerBriefs in Applied Sciences and Technology, Springer",
            "year": 2021
        },
        {
            "authors": [
                "M.A. Sutton",
                "J.-J. Orteu",
                "H. Schreier"
            ],
            "title": "Image Correlation for Shape",
            "venue": "Motion and Deformation Measurements, 1st Edition, Springer",
            "year": 2009
        },
        {
            "authors": [
                "S. Avril",
                "M. Bonnet",
                "A.-S. Bretelle",
                "M. Gr\u00e9diac",
                "F. Hild",
                "P. Ienny",
                "D. Lemosse",
                "S. Pagano",
                "E. Pagnacco",
                "F. Pierron"
            ],
            "title": "Overview of identification methods of mechanical parameters based on full-field measurements",
            "venue": "Experimental Mechanics 48 (4) ",
            "year": 2008
        },
        {
            "authors": [
                "M. Raissi",
                "P. Perdikaris",
                "G.E. Karniadakis"
            ],
            "title": "Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations",
            "venue": "Journal of Computational Physics 378 ",
            "year": 2019
        },
        {
            "authors": [
                "G.E. Karniadakis",
                "I.G. Kevrekidis",
                "L. Lu",
                "P. Perdikaris",
                "S. Wang",
                "L. Yang"
            ],
            "title": "Physics-informed machine learning",
            "venue": "Nature Reviews Physics 3 (6) ",
            "year": 2021
        },
        {
            "authors": [
                "D.C. Psichogios",
                "L.H. Ungar"
            ],
            "title": "A hybrid neural network-first principles approach to process modeling",
            "venue": "American Institute of Chemical Engineers Journal 38 (10) ",
            "year": 1992
        },
        {
            "authors": [
                "I.E. Lagaris",
                "A. Likas",
                "D.I. Fotiadis"
            ],
            "title": "Artificial neural networks for solving ordinary and partial differential equations",
            "venue": "IEEE Transactions on Neural Networks 9 (5) ",
            "year": 1998
        },
        {
            "authors": [
                "A.G. Baydin",
                "B.A. Pearlmutter",
                "A.A. Radul",
                "J.M. Siskind"
            ],
            "title": "Automatic differentiation in machine learning: a survey",
            "venue": "Journal of Machine Learning Research 18 (1) ",
            "year": 2018
        },
        {
            "authors": [
                "M. Abadi",
                "A. Agarwal",
                "P. Barham",
                "E. Brevdo",
                "Z. Chen",
                "C. Citro",
                "G.S. Corrado",
                "A. Davis",
                "J. Dean",
                "M. Devin",
                "S. Ghemawat",
                "I. Goodfellow",
                "A. Harp",
                "G. Irving",
                "M. Isard",
                "Y. Jia",
                "R. Jozefowicz",
                "L. Kaiser",
                "M. Kudlur",
                "J. Levenberg",
                "D. Man\u00e9",
                "R. Monga",
                "S. Moore",
                "D. Murray",
                "C. Olah",
                "M. Schuster",
                "J. Shlens",
                "B. Steiner",
                "I. Sutskever",
                "K. Talwar",
                "P. Tucker",
                "V. Vanhoucke",
                "V. Vasudevan",
                "F. Vi\u00e9gas",
                "O. Vinyals",
                "P. Warden",
                "M. Wattenberg",
                "M. Wicke",
                "Y. Yu",
                "X. Zheng"
            ],
            "title": "TensorFlow: Large-scale machine learning on heterogeneous distributed systems",
            "venue": "software available from https://www.tensorflow.org ",
            "year": 2015
        },
        {
            "authors": [
                "J. Bradbury",
                "R. Frostig",
                "P. Hawkins",
                "M.J. Johnson",
                "C. Leary",
                "D. Maclaurin",
                "G. Necula",
                "A. Paszke",
                "J. VanderPlas",
                "S. Wanderman-Milne",
                "Q. Zhang"
            ],
            "title": "JAX: Composable transformations of Python+NumPy programs",
            "venue": "version 0.3.13. Software available from http://github.com/google/jax ",
            "year": 2018
        },
        {
            "authors": [
                "I. Depina",
                "S. Jain",
                "S.M. Valsson",
                "H. Gotovac"
            ],
            "title": "Application of physicsinformed neural networks to inverse problems in unsaturated groundwater 29 flow",
            "venue": "Georisk: Assessment and Management of Risk for Engineered Systems and Geohazards 16 (1) ",
            "year": 2021
        },
        {
            "authors": [
                "A.D. Jagtap",
                "Z. Mao",
                "N. Adams",
                "G.E. Karniadakis"
            ],
            "title": "Physics-informed neural networks for inverse problems in supersonic flows",
            "venue": "Journal of Computational Physics 466 ",
            "year": 2022
        },
        {
            "authors": [
                "C. Xu",
                "B.T. Cao",
                "Y. Yuan",
                "G. Meschke"
            ],
            "title": "Transfer learning based physics-informed neural networks for solving inverse problems in engineering structures under different loading scenarios",
            "venue": "arXiv Preprint ",
            "year": 2022
        },
        {
            "authors": [
                "W. Li",
                "K.-M. Lee"
            ],
            "title": "Physics informed neural network for parameter identification and boundary force estimation of compliant and biomechanical systems",
            "venue": "International Journal of Intelligent Robotics and Applications 5 (3) ",
            "year": 2021
        },
        {
            "authors": [
                "Y. Chen",
                "L. Lu",
                "G.E. Karniadakis",
                "L. Dal Negro"
            ],
            "title": "Physics-informed neural networks for inverse problems in nano-optics and metamaterials",
            "venue": "Optics Express 28 (8) ",
            "year": 2020
        },
        {
            "authors": [
                "E. Kharazmi",
                "M. Cai",
                "X. Zheng",
                "Z. Zhang",
                "G. Lin",
                "G.E. Karniadakis"
            ],
            "title": "Identifiability and predictability of integer- and fractional-order epidemiological models using physics-informed neural networks",
            "venue": "Nature Computational Science 1 (11) ",
            "year": 2021
        },
        {
            "authors": [
                "K. Shukla",
                "A.D. Jagtap",
                "J.L. Blackshire",
                "D. Sparkman",
                "G.E. Karniadakis"
            ],
            "title": "A physics-informed neural network for quantifying the microstructural properties of polycrystalline nickel using ultrasound data: A promising approach for solving inverse problems",
            "venue": "IEEE Signal Processing Magazine 39 (1) ",
            "year": 2022
        },
        {
            "authors": [
                "C.J.G. Rojas",
                "M.L. Bitterncourt",
                "J.L. Boldrini"
            ],
            "title": "Parameter identification for a damage model using a physics-informed neural network",
            "venue": "arXiv Preprint ",
            "year": 2021
        },
        {
            "authors": [
                "E. Zhang",
                "M. Dao",
                "G.E. Karniadakis",
                "S. Suresh"
            ],
            "title": "Analyses of internal structures and defects in materials using physics-informed neural networks",
            "venue": "Science Advances 8 (7) ",
            "year": 2022
        },
        {
            "authors": [
                "E. Haghighat",
                "M. Raissi",
                "A. Moure",
                "H. Gomez",
                "R. Juanes"
            ],
            "title": "A physicsinformed deep learning framework for inversion and surrogate modeling in solid mechanics",
            "venue": "Computer Methods in Applied Mechanics and Engineering 379 ",
            "year": 2021
        },
        {
            "authors": [
                "C.M. Hamel",
                "K.N. Long",
                "S.L.B. Kramer"
            ],
            "title": "Calibrating constitutive models with full-field data via physics-informed neural networks",
            "venue": "Strain ",
            "year": 2022
        },
        {
            "authors": [
                "E. Zhang",
                "M. Yin",
                "G.E. Karniadakis"
            ],
            "title": "Physics-informed neural networks for nonhomogeneous material identification in elasticity imaging",
            "venue": "arXiv Preprint ",
            "year": 2020
        },
        {
            "authors": [
                "D. Anton",
                "H. Wessels"
            ],
            "title": "Identification of material parameters from full-field displacement data using physics-informed neural networks",
            "venue": "in: M. Beer, E. Zio, K.-K. Phoon, B. M. Ayyub (Eds.), Proceedings of the 8th International Symposium on Reliability Engineering and Risk Management, Research Publishing",
            "year": 2022
        },
        {
            "authors": [
                "S. Wang",
                "Y. Teng",
                "P. Perdikaris"
            ],
            "title": "Understanding and mitigating gradient flow pathologies in physics-informed neural networks",
            "venue": "SIAM Journal on Scientific Computing 43 (5) ",
            "year": 2021
        },
        {
            "authors": [
                "S. Wang",
                "X. Yu",
                "P. Perdikaris"
            ],
            "title": "When and why PINNs fail to train: A neural tangent kernel perspective",
            "venue": "Journal of Computational Physics 449 ",
            "year": 2022
        },
        {
            "authors": [
                "A. Henkes",
                "H. Wessels",
                "R. Mahnken"
            ],
            "title": "Physics informed neural networks for continuum micromechanics",
            "venue": "Computer Methods in Applied Mechanics and Engineering 393 ",
            "year": 2022
        },
        {
            "authors": [
                "A.D. Jagtap",
                "K. Kawaguchi",
                "G.E. Karniadakis"
            ],
            "title": "Adaptive activation functions accelerate convergence in deep and physics-informed neural networks",
            "venue": "Journal of Computational Physics 404 ",
            "year": 2020
        },
        {
            "authors": [
                "A.D. Jagtap",
                "Y. Shin",
                "K. Kawaguchi",
                "G.E. Karniadakis"
            ],
            "title": "Deep Kronecker neural networks: A general framework for neural networks with adaptive activation functions",
            "venue": "Neurocomputing 468 ",
            "year": 2022
        },
        {
            "authors": [
                "M.A. Nabian",
                "R.J. Gladstone",
                "H. Meidani"
            ],
            "title": "Efficient training of physicsinformed neural networks via importance sampling",
            "venue": "Computer-Aided Civil and Infrastructure Engineering 36 (8) ",
            "year": 2021
        },
        {
            "authors": [
                "L. McClenny",
                "U. Braga-Neto"
            ],
            "title": "Self-adaptive physics-informed neural networks using a soft attention mechanism",
            "venue": "in: J. Lee, E. F. Darve, P. K. Kitanidis, M. W. Mahoney, A. Karpatne, M. W. Farthing, T. Hesser (Eds.), Proceedings of the AAAI 2021 Spring Symposium on Combining Artificial Intelligence and Machine Learning with Physical Sciences, Stanford, CA, USA, March 22nd - to - 24th, 2021, Vol. 2964 of CEUR Workshop Proceedings, CEUR-WS.org",
            "year": 2021
        },
        {
            "authors": [
                "J. Berg",
                "K. Nystr\u00f6m"
            ],
            "title": "A unified deep artificial neural network approach to partial differential equations in complex geometries",
            "venue": "Neurocomputing 317 ",
            "year": 2018
        },
        {
            "authors": [
                "G. Cybenko"
            ],
            "title": "Approximation by superpositions of a sigmoidal function",
            "venue": "Mathematics of Control, Signals, and Systems 2 (4) ",
            "year": 1989
        },
        {
            "authors": [
                "K. Hornik",
                "M. Stinchcombe",
                "H. White"
            ],
            "title": "Multilayer feedforward networks are universal approximators",
            "venue": "Neural Networks 2 (5) ",
            "year": 1016
        },
        {
            "authors": [
                "X. Li"
            ],
            "title": "Simultaneous approximations of multivariate functions and their derivatives by neural networks with one hidden layer",
            "venue": "Neurocomputing 12 (4) ",
            "year": 2312
        },
        {
            "authors": [
                "I. Goodfellow",
                "Y. Bengio",
                "A. Courville"
            ],
            "title": "Deep Learning",
            "venue": "MIT Press",
            "year": 2016
        },
        {
            "authors": [
                "B. Moseley",
                "A. Markham",
                "T. Nissen-Meyer"
            ],
            "title": "Finite basis physics-informed neural networks (FBPINNs): A scalable domain decomposition approach for solving differential equations",
            "venue": "arXiv Preprint ",
            "year": 2021
        },
        {
            "authors": [
                "E. Haber",
                "U.M. Ascher"
            ],
            "title": "Preconditioned all-at-once methods for large",
            "venue": "sparse parameter estimation problems, Inverse Problems 17 (6) ",
            "year": 2001
        },
        {
            "authors": [
                "A. Schlintl",
                "B. Kaltenbacher"
            ],
            "title": "All-at-once formulation meets the Bayesian approach: A study of two prototypical linear inverse problems",
            "venue": "in: B. Jadamba, A. A. Khan, S. Mig \u0301orski, M. Sama (Eds.), Deterministic and Stochastic Optimal Control and Inverse Problems, 1st Edition, CRC Press",
            "year": 2021
        },
        {
            "authors": [
                "P.A. Guth",
                "C. Schillings",
                "S. Weissmann"
            ],
            "title": "Ensemble kalman filter for neural network based one-shot inversion",
            "venue": "in: R. Herzog, M. Heinkenschloss, D. Kalise, G. Stadler, E. Tr\u00e9lat (Eds.), Optimization and Control for Partial Differential Equations: Uncertainty quantification, open and closedloop control, and shape optimization, De Gruyter",
            "year": 2022
        },
        {
            "authors": [
                "A. Buljac",
                "C. Jailin",
                "A. Mendoza",
                "J. Neggers",
                "T. Taillandier-Thomas",
                "A. Bouterf",
                "B. Smaniotto",
                "F. Hild",
                "S. Roux"
            ],
            "title": "Digital Volume Correlation: Review of Progress and Challenges",
            "venue": "Experimental Mechanics 58 ",
            "year": 2018
        },
        {
            "authors": [
                "Y.A. LeCun",
                "L. Bottou",
                "G.B. Orr",
                "K.-R. M\u00fcller"
            ],
            "title": "Efficient BackProp",
            "venue": "in: G. Montavon, G. B. Orr, K.-R. a. M\u00fcller (Eds.), Neural Networks: Tricks of the Trade, 2nd Edition, Lecture Notes in Computer Science, Springer",
            "year": 2012
        },
        {
            "authors": [
                "J. Nocedal",
                "S.J. Wright"
            ],
            "title": "Numerical Optimization",
            "venue": "2nd Edition, Springer Series in Operations Research and Financial Engineering, Springer",
            "year": 2006
        },
        {
            "authors": [
                "E. Samaniego",
                "C. Anitescu",
                "S. Goswami",
                "V.M. Nguyen-Thanh",
                "H. Guo",
                "K. Hamdia",
                "X. Zhuang",
                "T. Rabczuk"
            ],
            "title": "An energy approach to the solution of partial differential equations in computational mechanics via machine learning: Concepts",
            "venue": "implementation and applications, Computer Methods in Applied Mechanics and Engineering 362 ",
            "year": 2020
        },
        {
            "authors": [
                "C.G. Broyden"
            ],
            "title": "The convergence of a class of double-rank minimization algorithms 1",
            "venue": "General considerations, IMA Journal of Applied Mathematics 6 (1) ",
            "year": 1970
        },
        {
            "authors": [
                "R. Fletcher"
            ],
            "title": "A new approach to variable metric algorithms",
            "venue": "The Computer Journal 13 (3) ",
            "year": 1970
        },
        {
            "authors": [
                "D.F. Shanno"
            ],
            "title": "Conditioning of Quasi-Newton methods for function minimization",
            "venue": "Mathematics of Computation 24 (111) ",
            "year": 1970
        },
        {
            "authors": [
                "D. Goldfarb"
            ],
            "title": "A family of variable-metric methods derived by variational means",
            "venue": "Mathematics of Computation 24 (109) ",
            "year": 1970
        },
        {
            "authors": [
                "X. Glorot",
                "Y. Bengio"
            ],
            "title": "Understanding the difficulty of training deep feedforward neural networks",
            "venue": "in: Y. W. Teh, M. Titterington (Eds.), Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, Vol. 9 of Proceedings of Machine Learning Research, PMLR",
            "year": 2010
        },
        {
            "authors": [
                "S. Hartmann",
                "R.R. Gilbert",
                "A.K. Marghzar",
                "C. Leistner",
                "P.K. Dileep"
            ],
            "title": "Material parameter identification of unidirectional fiber-reinforced composites",
            "venue": "Archive of Applied Mechanics 91 (2) ",
            "year": 2021
        },
        {
            "authors": [
                "S. Hartmann",
                "R.R. Gilbert"
            ],
            "title": "Material parameter identification using finite elements with time-adaptive higher-order time integration and experimental full-field strain information",
            "venue": "Computational Mechanics 68 (3) ",
            "year": 2021
        },
        {
            "authors": [
                "M.S. Alnaes",
                "J. Blechta",
                "J. Hake",
                "A. Johansson",
                "B. Kehlet",
                "A. Logg",
                "C. Richardson",
                "J. Ring",
                "M.E. Rognes"
            ],
            "title": "G",
            "venue": "N. Wells, The FEniCS Project Version 1.5, Archive of Numerical Software 3 (100) ",
            "year": 2015
        },
        {
            "authors": [
                "F. Pierron",
                "S. Avril",
                "V.T. Tran"
            ],
            "title": "Extension of the virtual fields method to elasto-plastic material identification with cyclic loads and kinematic hardening",
            "venue": "International Journal of Solids and Structures 47 (22) ",
            "year": 2010
        },
        {
            "authors": [
                "D. Zhang",
                "L. Lu",
                "L. Guo",
                "G.E. Karniadakis"
            ],
            "title": "Quantifying total uncertainty in physics-informed neural networks for solving forward and inverse stochastic problems",
            "venue": "Journal of Computational Physics 397 ",
            "year": 2019
        },
        {
            "authors": [
                "L. Yang",
                "X. Meng",
                "G.E. Karniadakis"
            ],
            "title": "B-PINNs: Bayesian physics-informed neural networks for forward and inverse PDE problems with noisy data",
            "venue": "Journal of Computational Physics 425 ",
            "year": 2021
        },
        {
            "authors": [
                "S. Hartmann",
                "R.R. Gilbert"
            ],
            "title": "Identifiability of material parameters in solid mechanics",
            "venue": "Archive of Applied Mechanics 88 (1) ",
            "year": 2018
        },
        {
            "authors": [
                "A. Beltr\u00e1n-Pulido",
                "I. Bilionis",
                "D. Aliprantis"
            ],
            "title": "Physics-Informed Neural Networks for Solving Parametric Magnetostatic Problems",
            "venue": "arXiv Preprint ",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "The identification of material parameters occurring in constitutive models has a wide range of applications in practice. One of these applications is the monitoring and assessment of the actual condition of infrastructure buildings, as the material parameters directly reflect the resistance of the structures to external impacts. Physics-informed neural networks (PINNs) have recently emerged as a suitable method for solving inverse problems. The advantages of this method are a straightforward inclusion of observation data. Unlike grid-based methods, such as the least square finite element method (LS-FEM) approach, no computational grid and no interpolation of the data is required. In the current work, we propose PINNs for the calibration of constitutive models from full-field displacement and global force data in a realistic regime on the example of linear elasticity. We show that conditioning and reformulation of the optimization problem play a crucial role in real-world applications. Therefore, among others, we identify the material parameters from initial estimates and balance the individual terms in the loss function. In order to reduce the dependency of the identified material parameters on local errors in the displacement approximation, we base the identification not on the stress boundary conditions but instead on the global balance of internal and external work. We demonstrate that the enhanced PINNs are capable of identifying material parameters from both experimental onedimensional data and synthetic full-field displacement data in a realistic regime. Since displacement data measured by, e.g., a digital image correlation (DIC) system is noisy, we additionally investigate the robustness of the method to different levels of noise. Keywords: Model Calibration, Inverse Problems, Physics-Informed Neural Networks, Realistic Data Regime, Structural Health Monitoring\n\u2217Corresponding author Email address: d.anton@tu-braunschweig.de (David Anton ) 1https://orcid.org/0000-0002-0888-0220\nPreprint submitted to Elsevier June 14, 2023\nar X\niv :2\n21 2.\n07 72\n3v 2\n[ cs\n.L G\n] 1\n3 Ju\nn 20"
        },
        {
            "heading": "1. Introduction",
            "text": "The identification of material parameters occurring in constitutive models is a major research subject in the field of solid mechanics and has a wide range of applications in practice. Probably the most obvious application is the characterization of unknown materials from experimental data. Another application that motivates this paper is continuous structural health monitoring (SHM). Building structures and materials age during service life due to chemical and physical processes. This in turn leads to a deterioration in both reliability and quality of the structure. At the same time, external impacts to the structure, e.g., traffic loads, often increase more than assumed at design. Therefore, continuous SHM is the prerequisite of a reliable prediction of the remaining service life of an infrastructure building [1, 2]. For an assessment of the actual building condition, it is crucial to consider the actual parameters of the building material. These material parameters indicate damage or material degradation, since they directly reflect the resistance of the structure to external impacts. Since we focus on steel structures in this paper, we assume linear-elastic, isotropic material behavior. The material condition of steel is thus directly reflected by Young\u2019s modulus and Poisson\u2019s ratio. Once the material parameters have been identified, they can be fed into a forward simulation, e.g., a finite element (FE) simulation, to calculate the actual resistance of the infrastructure building. Based on the simulation results, maintenance intervals and reinforcement measures can be derived.\nThe material parameters of interest can be identified from displacement data, e.g., measured by digital image correlation (DIC) [3], by solving an inverse problem. The underlying equation of the inverse problem is the balance of linear momentum. Traditionally, this inverse problem is solved by numerical methods, such as the least square finite element method (LS-FEM) approach or the virtual fields method (VFM) [4]. Recently, it has also been shown that physics-informed neural networks (PINNs) [5] are particularly suitable for solving inverse problems. PINNs are a framework for solving forward and inverse problems involving nonlinear partial differential equations (PDEs) from the field of physics-informed machine learning [6]. Although the idea behind this method goes back to the 1990s [7, 8], it became applicable only recently due to developments in automatic differentiation [9], software frameworks, such as TensorFlow [10] and JAX [11], and more powerful hardware as well as the ability to record and store large amounts of data. In addition to the data, PINNs also exploit the physical laws behind the data. This can improve generalization capability and reduce the amount of required training data. Recently, PINNs have been applied to inverse problems from versatile fields in engineering and science, including the simulation of unsaturated groundwater flow [12], super sonic flows [13], tunneling [14], biomechanics [15], nano-optics [16] and epidemiology [17], to name only a few. The main advantages of PINNs over traditional numerical methods are a straightforward inclusion of observation data and that PINNs can directly approximate the strong form of the PDE. No computational grid is required when using PINNs.\nIn the context of solid mechanics, Shukla et al. [18] applied PINNs to quan-\ntify the microstructural properties of polycristalline nickel using ultrasound data. Rojas et al. [19] used PINNs in combination with some classical estimation methods to identify parameters of a damage model. For the analyses of internal structures and defects, Zhang et al. [20] presented a general framework for identifying unknown geometric and material parameters. Haghighat et al. [21] proposed a multi-network model for the identification of material parameters from displacement and stress data. They applied their approach to linear elasticity and further extended it to von Mises plasticity. Hamel et al. [22] developed a framework for the calibration of constitutive models from full-field displacements and global force-displacement data and demonstrated it by means of hyperelastic material models. Contrary to the conventional PINN approach, they imposed the physical constraints by using the weak form of the PDE. This method, however, does not realize the full potential of PINNs as it relies on a computational grid for integrating the weak form of the PDE. Furthermore, in [23], Zhang et al. considered the identification of heterogeneous, incompressible, hyper-elastic material from full-field displacement data using two independent artificial neural networks (ANNs). One ANN was used for the approximation of the displacement field and another ANN for approximating the spatially dependent material parameter.\nIn summary, the previous contributions have demonstrated that PINNs can in principle be successfully applied to inverse problems in solid mechanics and particular for material parameter identification from displacement data. However, a severe restriction is that the assumptions made in the aforementioned contributions do not match the conditions of real-world inverse problems. This mainly concerns the magnitude and quality of the measured displacements as well as the magnitude of the material parameters to be identified. In the literature, normalized domains and material parameters have been predominantly considered so far. Apart from the simplifying assumptions mentioned above, in real-world applications, we cannot assume that we know the stress state within the domain of interest. As soon as the method is applied to realistic data, problems arise that have so far been neglected in the literature.\nTo the best of the authors knowledge, in this paper, the linear-elastic constitutive model is the first time calibrated from full-field displacement data in a realistic regime using PINNs. The enhanced PINN we propose in this contribution works with the strong form of the PDE and does not rely on a computational grid and thus realizes the full potential of PINNs. In contrast, when applying the standard PINN as proposed in [5] without further extensions to the same displacement data, we observed that it fails in solving the inverse problem [24]. Some of the possible failure modes when training PINNs have already been pointed out in the literature. Training of PINNs is a multi-objective optimization problem where a minimum in a composite loss function with respect to data and physics is searched. Hence, a trade-off between all loss terms has to be found. The convergence issues caused by the multi-objective optimization problem are investigated in [25, 26]. In order to improve the convergence, loss term weighting [25] and the adaption of the neural tangent kernel [26] were proposed. However, as reported in [27], we found that adaptive loss term weighting\nmay not improve the training dynamics for problems with many different loss terms, such as those encountered in solid mechanics. The neural tangent kernel is not of much interest in practice, since it involves considerably high computational effort. Other proposed approaches to improve convergence and stability of PINN training include adaptive activation functions [28, 29], adaptive sampling strategies [30] and self-adaptive PINNs using a soft attention mechanism [31].\nIn this contribution, we further develop PINN towards the calibration of linear-elastic constitutive models from full-field displacement and global force data in a realistic regime. We demonstrate that the standard PINN fails in identifying material parameters from displacement data in a realistic regime due to ill-posedness of the optimization problem. Based on our observations, we therefore introduce the following extensions:\n1. We scale the parameters to be optimized by providing initial estimates for the material parameters and normalize both the inputs and outputs of the PINN. We also condition the loss function by giving more weight to the data loss term. Note that the normalization and scaling are introduced in a way that does not affect the physics.\n2. We found that when using stress boundary conditions to account for the force information, the error in the identified material parameters is strongly dependent on the local error of the displacement approximation or the data quality in the boundary region. To mitigate this dependency, we base the material parameter identification on global information and consider the balance law of internal and external work to account for the force situation.\n3. Formulating the constitutive model in terms of bulk and shear modulus leads to more robust results than trying to identify Young\u2019s modulus and Poisson\u2019s ratio directly.\nWe demonstrate that the enhanced PINNs are able to identify the material parameters from both one-dimensional analytical as well as experimental and two-dimensional synthetic displacement data in a realistic regime. In addition, we conduct sensitivity analyses to show that the relative errors of the identified parameters are independent of the initial estimates in a range reasonable for real-world applications. As we approximate the integrals when calculating the internal and external work, we also perform a convergence study with respect to the number of collocation points used. Finally, we investigate the robustness of the enhanced PINNs to different levels of artificial noise imposed to the synthetic displacement data of a plate with a hole. The novelty of our contribution is thus the conditioning of the ill-posed optimization problem arising from a realistic data regime that has not been previously considered in the literature.\nThe remainder of this paper is structured as follows. In Section 2 the governing equations of solid mechanics are reviewed and a brief introduction to ANNs and particularly PINNs is provided. The further developments of the PINNs towards realistic data are presented in Section 3. In Section 4, the proposed PINN is first validated using one-dimensional analytical and experimental displacement data. Subsequently, in Section 5, the enhanced PINN is applied to clean as well as artificially noisy, synthetic displacement data of a plate with a\nhole. Finally, we conclude our investigations and point out possible directions of further research in Section 6."
        },
        {
            "heading": "2. Preliminaries",
            "text": "In this section, we provide an overview of the governing equations and introduce the general notation of ANNs and PINNs which are essential for the following sections."
        },
        {
            "heading": "2.1. Governing equations",
            "text": "In computational mechanics, modelling usually starts with the balance of linear momentum. Assuming a static and stationary setting, the balance of linear momentum at the material point x in the domain \u2126 can be derived as\ndiv \u03c3(x) + \u03c1(x) b(x) = 0, x \u2208 \u2126, (1)\nwhere \u03c3(x) denotes the Cauchy stress tensor, \u03c1(x) the density and b(x) contains accelerations related to external body forces, such as gravity. In order to close the above equation, the Cauchy stress tensor \u03c3(x) is expressed as a function of strain by a constitutive model via the kinematics.\nFor an isotropic, linear-elastic material, the constitutive model states\n\u03c3(x) = E1 + \u03bd ( \u03b5(x) + \u03bd1 \u2212 2\u03bd (tr \u03b5(x))I ) . (2)\nThe strain tensor \u03b5(x) can be expressed by the kinematic law in terms of the primary variables, which in solid mechanics are the displacements u(x), as\n\u03b5(x) = 12\n( grad u(x) + gradT u(x) ) . (3)\nConstitutive modeling comes along with the introduction of material parameters \u03ba. According to Eq. (2), the material is characterized by the Young\u2019s modulus E and Poisson\u2019s ratio \u03bd, so that \u03ba = [E, \u03bd]. Under the same assumptions, in one dimension, the constitutive model in Eq. (2) simplifies to\n\u03c3(x) = E \u03b5(x). (4)\nwith \u03ba = E. In analogy to Eq. (2), the constitutive model can also be formulated in terms of bulk modulus K and shear modulus G with \u03ba = [K, G], such that\n\u03c3(x) = Ktr \u03b5(x)I + 2G\u03b5D(x), (5)\nwhere \u03b5D(x) = \u03b5(x) \u2212 tr \u03b5(x)/3I is the deviatoric part of the strain tensor. The relation between these two sets of material parameters is given by\nK = E3(1 \u2212 2\u03bd) , G = E 2(1 + \u03bd) . (6)\nSubstituting the stress in Eq. (1) by the constitutive model in Eq. (2) or Eq. (5), we obtain a parameterized system of PDEs\ndiv \u03c3(x, \u03ba) + \u03c1(x) b(x) = 0, x \u2208 \u2126, \u03c3(xN ) \u00b7 n(xN ) = t\u0304(xN ) xN \u2208 \u0393N \u2282 \u2202\u2126,\nu(xD) = u\u0304(xD) xD \u2208 \u0393D \u2282 \u2202\u2126, (7)\nwhere \u2126 is the domain, \u2202\u2126 = \u0393N \u222a \u0393D its boundary, n(x) the normal vector and t\u0304(x) and u\u0304(x) are the boundary functions for the Neumann and Dirichlet boundary condition, respectively. Finally, Eq. (7) demonstrates that the relation between the displacements and the material parameters can be established by the balance of linear momentum from Eq. (1).\nIn addition to the balance of linear momentum, the balance of internal and external work applies according to the principle of conservation of mechanical energy. The internal work corresponds to the strain energy and is defined as\nWint = \u222b\n\u2126 \u222b \u03b5 0 \u03c3(x) : d\u03b5(x)d\u2126, (8)\nand the external work is composed of a surface and a volume part and can be calculated from\nWext = \u222b\n\u2202\u2126 \u222b u 0 t\u0304(x) \u00b7 du(x)d\u2202\u2126 + \u222b \u2126 \u222b u 0 b(x) \u00b7 du(x)d\u2126. (9)\nAssuming that the loads are static and no energy is dissipated in the form of heat, the balance of internal and external work thus states\nWint = Wext,\u222b \u2126 \u222b \u03b5 0 \u03c3(x) : d\u03b5(x)d\u2126 = \u222b \u2202\u2126 \u222b u 0 t\u0304(x) \u00b7 du(x)d\u2202\u2126 + \u222b \u2126 \u222b u 0 b(x) \u00b7 du(x)d\u2126.\n(10)"
        },
        {
            "heading": "2.2. Artificial Neural Networks",
            "text": "Artificial neural networks (ANNs) are global, smooth function approximators defining a mapping Rdin \u2192 Rdout from an input space to an output space. The computational units of an ANN are called neurons and are typically arranged in an input, an output and any number of hidden layers. In the following, we consider fully connected feed-forward neural networks (FFNNs) with L+1 layers. These FFNNs each have in total L \u2212 1 hidden layers, where layer 0 is the input layer and layer L the output layer. In a fully connected FFNN, the neurons of each two successive layers are connected. The weight of the connection between neuron k in layer l \u2212 1 and neuron j in layer l is denoted by wljk. All weights between layer l \u2212 1 and l can then be combined in the weight matrix W l, whose entries are wljk. In addition, the neurons in the hidden layers and the output layer each have a bias, where blj denotes the bias of neuron j in layer l. Similarly,\nthe biases of all neurons in layer l can be combined in vector bl with entries blj . Thus, the weights and biases of all neurons are the trainable parameters of the ANN \u03b8 = {W l, bl}1\u2264l\u2264L. The schematic structure of an ANN is illustrated in Fig. 1. In the forward pass, the output of the neurons in the hidden layers and the output layer are computed from the sum of their weighted inputs and their bias as an argument of an activation function.\nLayer 0 Layer l \u2212 1 Layer l Layer L\nThe mapping from an input to an output vector by a FFNN can be formulated recursively according to [32]. The weighted input zlj of neuron j in the hidden layer or output layer l with an upstream layer consisting of Nl\u22121 neurons is defined as\nzlj = Nl\u22121\u2211 k=1 wljky l\u22121 k + b l j , \u2200 l \u2208 [1, L]. (11)\nIn Eq. (11), yl\u22121k is the output of neuron k in the upstream layer l \u2212 1 given by\nyl\u22121k = \u03d5 l\u22121 k (z l\u22121 k ). (12)\nHere, \u03d5l\u22121k is the activation function of neuron k in layer l \u2212 1. Throughout this paper, for all neurons in a layer the same activation function is used, denoted by \u03d5l for layer l. Inserting Eq. (12) in Eq. (11) we obtain in symbolic notation\nzl = W l\u03d5l\u22121(zl\u22121) + bl, \u2200 l \u2208 [1, L], (13)\nwhere zl\u22121 contains the weighted inputs of all neurons in layer l \u2212 1 and \u03d5l\u22121 is applied elementwise. In the hidden layers, nonlinear functions, such as hyperbolic tangent, are usually used as activation functions. The activation of the output neurons is typically computed by the identity function. Since we do not use any activation function in the input layer, for the input layer l = 0 applies\n\u03d50(z0) = x, (14)\nwhere x is the input vector to the ANN. Given Eqs. (11)\u2013(14), the output vector yL of the FFNN can be defined recursively as a function of x as follows:\nyL = \u03d5L(zL) zL = W L\u03d5L\u22121(zL\u22121) + bL\nzL\u22121 = W L\u22121\u03d5L\u22122(zL\u22122) + bL\u22121\n... z2 = W 2\u03d51(z1) + b2\nz1 = W 1x + b1.\n(15)\nThe definition in Eq. (15) demonstrates that ANNs are highly parameterized, nonlinear, composed functions.\nThe training of ANNs is an optimization problem. The objective of the optimization problem is the loss function L(\u03b8; T ) which depends both on the trainable ANN parameters \u03b8 and the training data T . For the optimization of the ANN parameters \u03b8, usually gradient based optimization algorithms are used. The required gradient of the loss function with respect to the ANN parameters \u03b8 can be calculated using automatic differentiation [9].\nIt is well known that ANNs are universal function approximators [33, 34, 35]. Provided an ANN has a sufficient number of parameters, then according to the universal approximation theorem the ANN can theoretically approximate any continuous function and its derivatives to an arbitrarily small error with mild assumptions on the activation function only. It should be noted, however, that the issue of optimal training of ANNs has not yet been solved. For a more in-depth introduction to deep learning, the reader is referred to standard text books, e.g., [36]."
        },
        {
            "heading": "2.3. Physics-Informed Neural Networks",
            "text": "Physics-informed neural networks (PINNs) are a deep learning framework for solving forward and inverse problems involving PDEs recently emerged in the field of scientific machine learning [5]. The key characteristic of PINNs is the design of the loss function that enables leveraging physical knowledge during the training process. This succeeds by directly including the governing PDEs into the loss function as regularizing terms. The PINN acts as a global function approximator of the hidden solution of the PDE. Due to the regularizing terms in the loss function, the solution space is constrained and the ANN is enforced to satisfy the governing PDEs. For a more detailed overview of scientific machine learning, we refer to [6].\nAlthough PINNs can be applied to more general problems [6], in the course of this paper we consider stationary elliptic, nonlinear PDEs. Following the notation from [37], these PDEs can be formulated as\nD[u(x); \u03ba] = f(x), x \u2208 \u2126, Bk[u(x); \u03ba] = gk(x), x \u2208 \u0393k \u2282 \u2202\u2126,\n(16)\nwhere D is a differential operator acting on u(x) and parameterized by \u03ba, u(x) \u2208 Rdout is the hidden solution of the PDE with x representing spatial points in the domain \u2126 \u2208 Rdin and f(x) is a forcing term. The system of PDEs is further defined by a set of suitable boundary condition operators Bk acting on u(x) and a set of boundary functions gk(x) which are imposed on the corresponding part \u0393k \u2282 \u2202\u2126 on the boundary with k = 1, . . . , nbc. With Eq. (16), a general formulation of a variety of physical problems is provided.\nWhen solving PDEs using PINNs, the PDEs are directly embedded into the loss function. Defining the loss function starts with approximating the hidden solution u(x) by an ANN N (x; \u03b8), such that\nu(x) \u2248 N (x; \u03b8). (17)\nAdditionally, in the inverse setting, the PDE parameters \u03ba become trainable parameters and are optimized during the PINN training alongside the ANN parameters \u03b8 [5]. Thus, the loss function introduced in Section 2.2 depends on both \u03b8 and \u03ba as well as the training data T and is defined as\nL(\u03b8, \u03ba; T ) := \u03bbpdeLpde(\u03b8, \u03ba; T pde) + Lbc(\u03b8, \u03ba; T bc) + \u03bboLo(\u03b8; T o). (18)\nThe loss terms Lpde, Lbc and Lo penalize the residual of the approximation N (x; \u03b8) with respect to the PDE, the boundary condition data and the observation data, respectively, and are defined as\nLpde(\u03b8, \u03ba; T pde) = 1\nNpde Npde\u2211 i=1 \u2225D[N (xi; \u03b8); \u03ba] \u2212 f(xi)\u222522 ,\nLbc(\u03b8, \u03ba; T bc) = \u03bbkbc \u2211\nk\n1 Nkbc Nkbc\u2211 j=1 \u2225\u2225Bk[N (xkj ; \u03b8); \u03ba] \u2212 u\u0302kj \u2225\u222522 , Lo(\u03b8; T o) =\n1 No No\u2211 l=1 \u2225N (xl; \u03b8) \u2212 u\u0302l\u222522 .\n(19)\nHere, the training data T is comprised of three sets, T pde, T bc and T o. We refer to T pde as the Npde collocation points {xi}Npdei=1 sampled from the domain \u2126. Furthermore, T bc is comprised of k subsets, each with Nkbc training points {xkj , u\u0302kj } Nkbc j=1 with u\u0302kj = gk(xkj ) sampled from the boundary \u0393k associated with the boundary condition. T o is set of No data points {xl, u\u0302l}Nol=1 located in the domain \u2126 for which observations of the hidden function u\u0302l = u(xl) are available. The different loss terms Lpde, Lbc and Lo can additionally be weighted by \u03bbpde, \u03bbkbc and \u03bbo, where \u03bbkbc can in principle be different for each k. For calculating Lpde(\u03b8, \u03ba; T pde) in Eq. (19), the partial derivatives of the ANN outputs with respect to the inputs are calculated using automatic differentiation [9].\nFor inverse problems, the optimization problem is to find the optimal PINN parameters \u03b8\u2217 and material parameters \u03ba\u2217:\n[\u03b8\u2217, \u03ba\u2217] = arg min \u03b8,\u03ba L(\u03b8, \u03ba; T ). (20)\nSolving the problem defined in Eq. (20) is called training. From Eq. (20), it can be seen that the PDE parameters \u03ba are estimated while simultaneously the forward solution is approximated by the ANN. Thus, for inverse problems, PINNs follow the all-at-once approach. All-at-once approaches are outlined, e.g., in [38], [39] and in [40]."
        },
        {
            "heading": "3. Further development of PINNs towards a realistic data regime",
            "text": "It is shown that standard PINNs, as presented in Section 2.3, fail in identifying material parameters from displacement data in a realistic regime without further modifications. Previous work, including [21] and [23], among others, have already proven PINNs to be generally capable of identifying material parameters from full-field displacement data. However, for this purpose, simplified assumptions were made which often do not match real-world conditions. These assumptions concern the domain size, the availability of stress data, the boundary conditions and the magnitude of both displacements and material parameters as well as the amount of noise in the data. In the following, we elaborate the necessary developments of the PINN approach, which essentially aim at conditioning the optimization problem and presenting it in an appropriate formulation with respect to the goal of material model calibration. Some of the extensions described below were first introduced in a similar form in [24]. In this paper, we present further improvements and demonstrate the applicability in the two-dimensional regime. We expect that an extension to the three-dimensional regime is straightforward. Three-dimensional displacement data can be measured, e.g., by digital volume correlation [41]."
        },
        {
            "heading": "3.1. Normalization of inputs and outputs",
            "text": "For PINNs to be able to approximate the displacement field from given experimental displacement data, both the inputs and outputs of the PINN must be normalized. It is well known that the convergence of ANN training can be accelerated by normalizing its inputs, where the mean values of all input features should each be close to zero according to [42]. In addition, we found that in our use case, the output of the ANN also needs to be normalized. Therefore, we approximate the hidden solution in Eq. (7) by the normalized ANN N\u0304 , defined as N\u0304 (x) := T out(N (T in(x); \u03b8)). (21) Since we use the hyperbolic tangent as activation function in the hidden layers, the output of these neurons is always in the range [\u22121, 1]. Therefore, we also map the input into the range [\u22121, 1] before we forward it to the ANN. We then retransform the ANN outputs back to the real output data range and thus force the ANN to predict the outputs in the range [\u22121, 1]. The elementwise linear transformation T in and re-transformation T out are defined as\nT ini (xi) = 2 (\nxi \u2212 xmini xmaxi \u2212 xmini\n) \u2212 1, \u2200 i \u2208 [1, din],\nT outj (u\u0302j) = ( u\u0302maxj \u2212 u\u0302minj ) u\u0302j + 1 2 + u\u0302 min j , \u2200 j \u2208 [1, dout], .\n(22)\nThe transformation and re-transformation is based on xmini , xmaxi , u\u0302minj and u\u0302maxj which are the minimum and maximum values of the PINN inputs and outputs for dimension i, respectively, and can be taken from the training data. As a consequence, the PINN learns the mapping between the normalized inputs and the normalized outputs instead of learning the mapping between the real inputs and the real outputs. Applying the chain and product rule, we obtain the first and second derivatives of the output of the normalized ANN with respect to the inputs in index notation:\ndN\u0304i dxj = \u2202T out i \u2202Ni \u2202Ni \u2202T inj dT inj dxj ,\nd2N\u0304i dxjxk = \u2202T out i \u2202Ni \u22022Ni \u2202T inj T in k dT inj dxj dT ink dxk .\n(23)\nIt is important to emphasize that at any time the real inputs are fed into the normalized ANN. Likewise, the normalized ANN always outputs the real outputs. Thus, the physics are not violated by the normalization when the outputs of the PINN are derived according to the inputs during training."
        },
        {
            "heading": "3.2. Conditioning of loss function",
            "text": "We also found that the loss function is ill-conditioned for small displacements, which prevents the PINN to accurately approximate the displacement field. In real-world problems, the measured displacements may be in the order of magnitude of 10\u22123 mm or less. With displacements of this magnitude, even a relative error RE = 100% of the approximation by the PINN results in a data loss in the order of magnitude of 10\u22126 according to Eq. (19). The small losses result in small gradients of the loss with respect to the ANN parameters to be optimized. This in turn makes gradient based optimization difficult. Therefore, we first introduce the relative mean squared error. Applying this error metric in the data loss term in Eq. (19), we obtain\nLo(\u03b8; T o) = 1\nNo No\u2211 l=1 \u2225\u2225(N\u0304 (xl; \u03b8) \u2212 u\u0302l) \u2298 uchar\u2225\u222522 , (24) where uchar is a vector of characteristic displacements and \u2298 the Hadamard division operator. We propose to choose the characteristic displacements as the mean displacements from the training data set:\nuchar = 1 No No\u2211 i u\u0302i. (25)\nThe displacement may have different orders of magnitudes in the different dimensions. As a result, the contributions of the different dimensions to the data loss would vary depending on their magnitude. The relative mean squared error takes this into account and adaptively equalizes the contributions to the same\nlevel. Second, in order to increase the impact of the total data loss term Lo and to balance the loss terms, it is necessary to increase the weight \u03bbo in Eq. (18). While the relative mean squared error used in Eq. (24) is adaptive, the loss term weight is manually selected. Care must be taken to ensure that, on the one hand, the data loss term is given sufficient priority, but on the other hand, overfitting to the data is prevented. In principle, there are also approaches to adaptively weight the loss terms, such as [25, 26]. We have found, however, that adaptive loss term weighting may not improve the training dynamics for problems with many different loss terms, such as those encountered in solid mechanics. As can be seen in Section 4 and Section 5, we also succeed in manually balancing the loss terms. A proper conditioning of the data loss term in combination with normalization allows PINNs to approximate even displacement fields of small magnitude. Note, however, that an accurate approximation of the displacement field is a necessary, but not a sufficient, criterion for identifying material parameters from realistic displacement data."
        },
        {
            "heading": "3.3. Scaling of the parameters to be optimized",
            "text": "Due to different scales of the material and ANN parameters, the calibration with the standard PINN is not feasible for the optimizer. We examined the range of the optimized ANN parameters \u03b8 for the standard PINN and found that for the analytical example in Section 4 these are in the range [\u22121457.0, 1881.38] , also see [24]. In contrast, the material parameters to be identified for construction steel are approximately \u03bd = 0.3 and E = 210,000 Nmm2 or K \u2248 175,000.00 N mm2 and G \u2248 80,769.23 Nmm2 . Since both the ANN and the material parameters are optimized simultaneously, this leads to a poorly scaled optimization problem [43]. We also assume that the optimizer encounters many local minima, which complicates the optimization. In order to obtain a similar scale for all parameters to be optimized, the real material parameters in the constitutive model will be replaced by\n\u03ba = (1.0 + \u03b1\u03ba)\u03baest. (26)\nHere \u03b1\u03ba and \u03baest are a correction factor and an initial estimate for the material parameter to be identified, respectively. From now on, the correction factor is identified instead of the material parameter. If we provide the exact Young\u2019s modulus, then the optimized parameters of the enhanced PINN \u03b8 are now in the range [\u22125.34, 4.40] and the correction factor \u03b1\u03ba very close to 1, as we will see in Section 4. Thus, the parameters to be optimized have a similar scale and the optimization problem is well posed. Unless otherwise specified, the correction factors are initialized with zeros throughout this paper."
        },
        {
            "heading": "3.4. Problem reformulation",
            "text": "The accuracy as well as the robustness of the identified material parameters strongly depend on how the optimization problem is formulated. The advantage of Young\u2019s modulus and Poisson\u2019s ratio over alternative elasticity parameters is\nthat they are often easier to interpret. However, in attempting to identify them directly, we observed that the results are less accurate and, more importantly, less robust than when the constitutive model is formulated with respect to bulk and shear modulus. We made this observation for the case where we provide an estimate of \u03bdest = 0.45 for Poisson\u2019s ratio. For some random initializations of the PINN parameters, this estimate results in a failure of the calibration. Therefore, we reformulated the optimization problem in terms of the material parameters to be identified. We first converted the estimated Young\u2019s modulus and Poisson\u2019s ratio into the corresponding estimates for bulk and shear modulus according to Eq. (6). With the scaling of the parameters proposed in Section 3.3, the modified material parameters in Eq. (7) then become \u03ba\u0304 = [\u03b1K , \u03b1G]. In onedimensional problems, the modified material parameter remains \u03ba\u0304 = \u03b1E . Finally, the identified bulk and shear modulus are converted back to Young\u2019s modulus and Poisson\u2019s ratio. We assume that this formulation of the optimization problem is better posed because it introduces a constraint on the Poisson\u2019s ratio. As soon as the Poisson\u2019s ratio approaches the incompressibility limit of \u03bd = 0.5, the bulk modulus would approach infinity. Critical values \u03bd \u2265 0.5 can thus be prevented and the optimization becomes more robust as a result."
        },
        {
            "heading": "3.5. Incorporating balance of internal and external work",
            "text": "In order to be able to identify material parameters, we need to account either for the global or at least some local force information. As soon as no volume force is present, at least some information on the force boundary conditions must be taken into account so that the material parameter is uniquely identifiable. Otherwise, the inverse problem would be ill-posed. There would exist an infinite number of combinations of material parameters and force situations, which lead to the same displacement field. In the standard PINN formulation, the accuracy of the material parameters depends strongly on the accuracy of the displacement field approximation in regions where the stress boundary conditions are applied. A large error in the approximation near the boundaries then inevitably leads to a large error in the identified material parameters. This can cause problems especially when dealing with erroneous or noisy data. In order to remove this local dependency, instead of the stress boundary conditions, we consider the internal and external energy of the mechanical system. Contrary to [44], however, we do not minimize the total potential energy of the system but use the balance of internal and external work equivalent to the principle of conservation of mechanical energy. According to Eq. (10), we enforce the balance of internal and external work by the loss term\nLW (\u03b8, \u03ba\u0304; T W ) = Wint(\u03b8, \u03ba\u0304; T Wint) \u2212 Wext(\u03b8; T Wext), (27)\nwhere the training data T W is comprised of T Wint and T Wext . For the approximation of the domain and boundary integrals in the calculation of the internal and external work, we adopt the Monte-Carlo integration. The internal work\ncan thus be calculated approximately by\nWint(\u03b8, \u03ba\u0304; T Wint) \u2248 1 2 V\u2126 NWint NWint\u2211 m=1 \u03c3(N\u0304 (xm; \u03b8); \u03ba) : \u03b5(N\u0304 (xm; \u03b8)), (28)\nwhere T Wint is a set of NWint collocation points {xm} NWint m=1 sampled from the domain \u2126. In one and two dimensions, the volume V\u2126 corresponds to the length and the area of the domain \u2126, respectively. The first derivative of the PINN output with respect to its inputs, which is needed to calculate the strains \u03b5(u(x)) from Eq. (3), can again be calculated using automatic differentiation. In the absence of body forces, the external work can be approximated by\nWext(\u03b8; T Wext) \u2248 1 2 V\u2202\u2126 NWext NWext\u2211 n=1 t(xn) \u00b7 N\u0304 (xn; \u03b8) (29)\nHere, T Wext is a set of NWext collocation points {xn} NWext n=1 sampled from the boundary \u2202\u2126. We assume that the traction t(x) is known at least for the collocation points in T Wext and that the collocation points are distributed as evenly as possible in the domain. Thus, we shift the dependence of the material parameter identification from the local accuracy of the displacement approximation in the boundary regions to the accuracy of the whole domain."
        },
        {
            "heading": "3.6. Summary of the method developments",
            "text": "Based on the general PINN formulation reviewed in Section 2.3 and the extensions introduced above, the PINN and the associated optimization problem for calibrating the linear-elastic material model are defined as follows: In two dimensions, the displacement field is approximated by two independent ANNs, such that\nu(x) \u2248 N\u0304 (x; \u03b8\u0304) \u2248 [ N\u0304x(x; \u03b8x) N\u0304y(x; \u03b8y) ] , (30)\nwith \u03b8\u0304 = [\u03b8x, \u03b8y]. According to [21], this leads to a more accurate approximation then using one ANN with two outputs, since the cross-dependency between the outputs may not be accurately represented by a single network. This crossdependency results from the kinematic law in Eq. (3) and the constitutive model. The loss function of the inverse problem is given by\nL(\u03b8\u0304, \u03ba\u0304; T ) := Lpde(\u03b8\u0304, \u03ba\u0304; T pde) + LW (\u03b8\u0304, \u03ba\u0304; T W ) + \u03bboLox(\u03b8x; T o) + \u03bboLoy (\u03b8y; T o),\n(31)\nwith the loss terms defined as\nLpde(\u03b8\u0304, \u03ba\u0304; T pde) = 1\nNpde Npde\u2211 i=1 \u2225\u2225div \u03c3(N\u0304 (xi; \u03b8\u0304); \u03ba\u0304)\u2225\u222522 , LW (\u03b8\u0304, \u03ba\u0304; T W ) =\nV\u2126 NWint NWint\u2211 m=1 \u03c3(N\u0304 (xm; \u03b8\u0304); \u03ba\u0304) : \u03b5(N\u0304 (xm; \u03b8\u0304))\n\u2212 V\u2202\u2126 NWext NWext\u2211 n=1 t(xn) \u00b7 N\u0304 (xn; \u03b8\u0304),\nLox(\u03b8x; T o) = 1\nNo No\u2211 l=1 \u2225\u2225\u2225\u2225Nx(xl; \u03b8x) \u2212 u\u0302lxucharx \u2225\u2225\u2225\u22252 2 ,\nLoy (\u03b8y; T o) = 1\nNo No\u2211 l=1 \u2225\u2225\u2225\u2225\u2225Ny(xl; \u03b8y) \u2212 u\u0302lyuchary \u2225\u2225\u2225\u2225\u2225 2\n2\n,\n(32)\nwhere [u\u0302lx, u\u0302ly]T = u(xl) is known from the observation data. In one dimension, the displacement is approximated by only one ANN, such that u(x) \u2248 N\u0304 (x; \u03b8), (33)\nand the loss function simplifies to\nL(\u03b8, \u03ba\u0304; T ) := Lpde(\u03b8, \u03ba\u0304; T pde) + LW (\u03b8, \u03ba\u0304; T W ) + \u03bboLo(\u03b8; T o) (34)\nCorrespondingly, the loss terms are then defined as\nLpde(\u03b8, \u03ba\u0304; T pde) = 1\nNpde Npde\u2211 i=1 ( E \u2202 \u22022x N\u0304 (xi; \u03b8) )2 ,\nLW (\u03b8, \u03ba\u0304; T W ) = V\u2126\nNWint NWint\u2211 m=1 \u03c3(N\u0304 (xm; \u03b8); \u03ba\u0304) \u00b7 \u03b5(N\u0304 (xm; \u03b8))\n\u2212 V\u2202\u2126 \u00b7 t(xn) \u00b7 N\u0304 (xn; \u03b8),\nLo(\u03b8; T o) = 1\nNo No\u2211 l=1 ( N (xl; \u03b8) \u2212 u\u0302l uchar )2 .\n(35)\nHere, xn is the coordinate of the boundary where traction is applied. Unless otherwise specified, the weight for the data loss terms \u03bbo and the characteristic displacements uchar are set to \u03bbo = 105 and according to Eq. (25). In addition, we use the same number and location of collocation points Ncol to determine the PDE residual \u03bbpde and to determine the internal work Wint, such that Npde = NWint = Ncol."
        },
        {
            "heading": "4. Parameter identification from one-dimensional displacement data",
            "text": "We first demonstrate the enhanced method for one-dimensional displacement data. Using analytical data, the method is validated for a realistic data regime and a sensitivity analysis is conducted with respect to the initial estimate of the material parameter to be identified. We then show that PINNs are, in principle, also able to identify the Young\u2019s modulus from one-dimensional experimental displacement data."
        },
        {
            "heading": "4.1. Analytical displacement data",
            "text": "As a first test case, we consider analytical displacement data for a stretched rod. The stretched rod has a length of L = 100 mm and its upper end is clamped. A traction of t\u0304 = 100 Nmm2 is applied at the free end, causing a deformation. External body forces, such as gravity, are neglected. We assume linear-elastic material with Young\u2019s modulus Etrue = 210,000 Nmm2 . From the analytical solution, we calculate the displacements at No = 128 equidistantly sampled points to train the PINN and at another Nval = 1,024 points to validate it. The collocation points for calculating the PDE residual as well as the internal work during training have the same number and location as the data points, such that Ncol = No = 128. The geometry and the boundary conditions are schematically illustrated in Fig. 2.\nFor the approximation of the displacement, we use a fully connected FFNN as defined in Section 2.2 and set the architecture and training hyperparameters as follows: The ANN has 2 hidden layers, each with 8 neurons. The hyperbolic tangent is used as activation function in the hidden layers. For optimization of the ANN parameters and the correction factor of Young\u2019s modulus we use the BFGS [45, 46, 47, 48] optimizer. The weights are initialized using Glorot normal initialization [49]. The biases and the correction factor introduced in Section 3 are initialized with zeros. Optimization is performed in full batch mode. Since\nthe chosen hyperparameters have proven to be suitable for this use case, no hyperparameter optimization is conducted.\nIn order to demonstrate the effectiveness of the enhanced method, we try to identify the Young\u2019s modulus from the analytical displacement data with both the standard PINN with Neumann boundary condition and the enhanced PINN. For the enhanced PINN, we compute the characteristic displacement uchar = 0.0238 mm and set the data loss weight to \u03bbo = 105. The initial estimate is first set to the exact Young\u2019s modulus Etrue = 210,000 Nmm2 which we used for data generation. In Fig. 3, the displacement approximation for both the standard and the enhanced PINNs are shown. The comparison underlines that the standard PINN is not capable to accurately approximate the displacement of the stretched rod. As a consequence, the identified Young\u2019s modulus is with Eident = 1,442.35 Nmm2 far from the correct value. In contrast, the enhanced PINN succeeds in identifying a Young\u2019s modulus of Eident = 209,999.99 Nmm2 with a relative error (RE) of REE = \u22124.45 \u00b7 10\u22126%. In addition, the relative L2-norm (rL2-norm) of the displacement approximation is rL2 = 1.5577 \u00b7 10\u22127.\nThe results presented above, and the method in general, are only useful as long as the results are satisfactory even if the exact material parameters are not known. Hence, we investigate the sensitivity of the relative error of the identified Young\u2019s modulus with respect to the initial estimate. We perform a sensitivity analysis in which we solve the inverse problem for several initial estimates in the range Eest = [10%, 1,000%] \u00b7 Etrue of the Young\u2019s modulus used for data generation. The ANN parameters are initialized identically for each simulation of one analysis. Since we found that the optimization result depends on the initialization of the PINN parameters, we repeated the sensitivity analysis for a total of 10 different random initializations. From the 10 individual results for each initial estimate, we calculate the mean error and the standard error of the\nmean (SEM). The latter is defined as\nSEM = \u03c3\u221a ns . (36)\nHere, \u03c3 is the standard deviation of the results and ns is the number of samples, where in our case ns = 10. The results in Fig. 4 suggest that the error of the identified material parameter is not sensitive to the initial estimate within a reasonable range."
        },
        {
            "heading": "4.2. Experimental displacement data",
            "text": "We test the enhanced PINN using one-dimensional experimental displacement data. For the material model calibration, we use the same PINN architecture and hyperparameters as in Section 4.1 and set the data loss weight to \u03bbo = 104. We choose the initial estimate as Eest = 210,000 Nmm2 . The characteristic displacement is uchar = 0.0579 mm. As a reference solution, we use the prediction of a LS-FEM simulation. For more information on this approach, we refer to [50, 51].\nThe experimental displacement data were measured in an uniaxial tensile test with the following setting: A specimen of TS275 steel was used, the geometry of which is shown in Fig. 5. The test was performed displacement-controlled using the tensile testing machine Z100 from ZwickRoell GmbH & Co. KG. While one end of the specimen was clamped, the testing machine pulled on the other end up to an averaged axial strain of \u03b5mean = 4.69 \u00b7 10\u22122%. Thus, the strain is still in the assumed linear-elastic range of the material under consideration.\nIn addition, the displacements were measured by the DIC system Aramis 12M from Carl Zeiss GOM Metrology GmbH for an area of L = 80 mm length and W = 19.25 mm width in the center of the specimen. The displacements measured in two dimensions were then interpolated to a total of No = 161 equidistant locations on the central axis of the specimen, as illustrated in Fig. 5. As in the analytical test case, the data points are also used as collocation points in the PDE loss term and for calculating the internal work when training the PINN, such that Ncol = No = 161. Apart from the displacements, the force applied to initiate the displacements was measured by a force gauge. The force and the cross-sectional area of the specimen result in a traction t = 212.55 Nmm2 . The external work is then calculated from the reaction forces on the left and right boundaries of the free body of the considered area. The rigid body displacements are subtracted from the measured displacements.\nThe approximated displacement, which is shown in Fig. 6, is visually in good agreement with the measured displacements. The PINN identified a Young\u2019s modulus of Eident = 239,195.53 Nmm2 , which differs by REE = 7.41% from the value determined by the LS-FEM approach. We manually selected the data loss weight so that the data and PDE loss terms are reasonably balanced, similar to what can be seen in Fig. 9 for the full-field displacement data. We assume that the noise in the data is the main reason for the observed discrepancy between the Young\u2019s modulus determined using the enhanced PINN and LS-FEM approach. Therefore, we examine the influence of different noise levels on the identification in more detail in Section 5."
        },
        {
            "heading": "5. Parameter identification from full-field displacement data",
            "text": "In the next step, we apply the enhanced PINN formulation to synthetically generated displacement data of a plate with a hole, considering both clean and noisy data. As in the previous section, we first conduct an error sensitivity analysis for the identified Young\u2019s modulus and Poisson\u2019s ratio with respect to the initial estimate. Furthermore, we show how the relative errors of the identified material parameters evolve with an increasing number of collocation points. Finally, we investigate the error sensitivity of the identified material parameters with respect to the noise level of the displacement data."
        },
        {
            "heading": "5.1. Clean synthetic displacement data",
            "text": "For the two-dimensional test case, we generate synthetic DIC data for a plate with a hole using the finite element method (FEM). Since the plate with a hole is two-fold symmetric, we consider only the quadrant at the top left of the plate and define symmetry boundary conditions on the bottom and right boundaries. The quadrant has an edge length of L = 100 mm and the radius of the hole is R = 10 mm. Furthermore, the thickness of the plate is set to T = 1 mm and the plate is assumed to be in plane stress condition. We simulate uniaxial tension and load the left edge with t\u0304 = [\u2212100 Nmm2 , 0]T . No force is applied to the upper and the hole boundaries. External body forces are neglected. The geometry, dimensions and boundary conditions of the considered test case are illustrated in Fig. 7. We again assume isotropic, linear-elastic material. In two-dimensions, linear-elastic materials can be characterized by the Young\u2019s modulus and Poisson\u2019s ratio which are set to Etrue = 210,000 Nmm2 and \u03bdtrue = 0.3, respectively, emulating the behaviour of steel. For the FE simulation, the domain is meshed using linear triangular elements and the FE solution is evaluated and recorded at a total of 101,496 nodes. Discretization errors are neglected in the following due to the high resolution of the computational\nmesh. Under the given boundary conditions, the maximum calculated strain is \u03b5max = 0.1505% and thus in the assumed linear-elastic range of the considered material. The FEM code for data generation is implemented in Python and based on the FEniCS project [52].\nTo solve the inverse problem, we only use a small subset of the synthetically generated displacement data consisting of No = 4,096 data points within the domain of interest. We select the data points directly from the FE solution to avoid interpolation errors. In addition, the external work is approximated from NWext = 64 equidistantly distributed data points on the left Neumann boundary. It should be noted that the displacement data set also contains points on the boundary. Therefore, we do not explicitly consider the Dirichlet boundary condition by a separate data set and loss term. The architecture of the PINN and the training hyperparameters are defined as follows: We use two fully connected FFNNs to approximate the displacement fields ux(x) and uy(x). Both ANN consist of 2 input neurons, 2 hidden layers with 16 neurons each, and 1 output neuron. The hyperbolic tangent is used as activation function in the hidden layers. The weights and biases of the ANN are initialized using Glorot normal initialization and zeros, respectively. As in the previous test cases, we initialize the correction factors for bulk and shear modulus with zeros. We solve the resulting optimization problem using the BFGS optimizer.\nIn order to investigate the error sensitivities of the identified Young\u2019s modulus and Poisson\u2019s ratio with respect to their initial estimates, we again perform a sensitivity analysis. For this purpose, we consider initial estimates for both Young\u2019s modulus and Poisson\u2019s ratio of 66, 6\u0304%, 100% and 133, 3\u0304% of the exact\nvalues used for data generation, respectively. Here we use the same number and locations of data and collocation points, so that No = Ncol = 4,096. We then calibrate the linear-elastic material model from Eq. (5) for all of the resulting 9 combinations of initial estimates using a PINN. The ANN parameters are initialized identically for each of the 9 simulations of one analysis. To obtain more meaningful results, we again repeat the analysis a total of 10 times with different random initializations of the PINN parameters and locations of the data and collocation points.\nThe results of the sensitivity analysis, summarized in Fig. 8, show that the maximum mean absolute relative error (MARE) of the identified material parameters averaged over all runs are MAREmaxE = 1.91% and MAREmax\u03bd = 0.24% for Young\u2019s modulus and Poisson\u2019s ratio, respectively. If we consider all 90 simulations separately, in the worst case, the maximum absolute relative errors (AREs) are AREmaxE = 17.62% and AREmax\u03bd = 1.46% for Young\u2019s modulus and Poisson\u2019s ratio, respectively. It should be noted that the above mentioned high AREmaxE also leads to the relatively high MAREE for Eest = 140,000 Nmm2 and \u03bdest = 0.4. For the remaining 9 runs with this combination of initial estimates, the relative errors of the identified Young\u2019s modulus are less than AREE = 0.9%. Overall, the sensitivity analysis shows that the relative error has just a low sensitivity to the initial material parameter estimates within a reasonable range of estimates for real-world applications. Since the relative error for the identified Young\u2019s modulus was high in only 1 out of 10 runs, it is not a systematic error. Instead, the random initialization of the neural network parameters and the random distribution of the data and collocation points seem to have an impact on the accuracy.\nIn Fig. 9, we show an example of how the loss terms and the correction factors for bulk and shear modulus for initial estimates of Eest = 280,000 Nmm2 and \u03bdest = 0.2 evolve during training. The identified bulk and shear modulus for this example result in relative errors (REs) of REE = 0.15% and RE\u03bd = \u22120.11% for Young\u2019s modulus and Poisson\u2019s ratio, respectively. In figure Fig. 10, we show the PINN approximation of the displacement field after training in comparison with the FE solution. To validate the accuracy of the displacement field, we use another Nval = 4,096 data points randomly sampled from the FE solution and different from the training data. The displacement field is approximated with a relative L2-norm (rL2-norm) of rL2x = 1.1193 \u00b710\u22125 in x- and rL2y = 2.1224 \u00b710\u22125 in y-direction.\nNote that the approximation of the integrals for calculating the internal and external work in Eqs. (28) and (29) for heterogeneous strain and stress fields is erroneous for a finite number of collocation points. However, we assume that the accuracy of the approximation depends directly on the number of collocation points. To verify this assumption, we examine the convergence of the relative errors with respect to the number of collocation points within the range Ncol = [512, 32,768]. The number of data points No = 4,096 remains unchanged. We again repeat the convergence study a total of 10 times with different randomly initialized ANN parameters and locations for the data and collocation points to obtain more meaningful and robust results. For this analysis, we use the\nsame PINN architecture and hyperparameters as before and provide the material parameters we used for data generation as initial estimates. The results shown in Fig. 11 demonstrate that both the relative error of the identified Young\u2019s modulus and Poisson\u2019s ratio decrease with increasing number of collocation points. In direct comparison, the relative error for the identified Young\u2019s modulus decreases much faster."
        },
        {
            "heading": "5.2. Noisy synthetic displacement data",
            "text": "Finally, we investigate how sensitive the relative errors of the identified Young\u2019s modulus and Poisson\u2019s ratio are with respect to the level of noise. So far, we have not yet taken into account the fact that displacement data measured by a DIC system is inevitably affected by noise. In order to emulate real DIC data, we apply Gaussian noise N (0, \u03c32) with zero mean to the synthetic data used in Section 5.1. The noise in the DIC measurements and thus also the\nstandard deviation depend on the pixel accuracy of the imaging device and is not proportional to the displacement magnitude. For this reason, we have determined the standard deviation relative to the order of magnitude of the mean displacement, which is 10\u22122 mm. We applied the same absolute noise level to all nodal values of the FEM solution independently of the individual magnitude. For the sensitivity analysis, we apply varying levels of Gaussian noise N (0, \u03c32) to the clean data we used in the previous section in a range of [0.1%, 10%] relative to the order of magnitude of the mean displacement. This results in absolute standard deviations in the range of \u03c3 = [10\u22125, 10\u22123] mm. Just as with the sensitivity analysis in Section 5.1, we use No = Ncol = 4,096 training and collocation points. As initial estimates, we provide the same material parameters that we used to generate the clean displacement data. To avoid overfitting of the ANNs to the noisy data, we reduce the data loss weight in Eq. (31) to \u03bbo = 103. Apart from that, we use the same PINN architecture and hyperparameters as\nin Section 5.1. The results of the sensitivity analysis, shown in Fig. 12, indicate that PINNs can deal with moderate levels of noise when calibrating the linear-elastic material model. From the results, it can be seen that especially the relative error of the identified Young\u2019s modulus increases only slightly in its absolute values up to a standard deviation of \u03c3 = 10\u22124 mm. The mean absolute relative errors (MAREs) at this level of noise are MAREE = 0.43% and MARE\u03bd = 3.34% for Young\u2019s modulus and Poisson\u2019s ratio, respectively. For larger standard deviations, the relative errors increase significantly. According to [53], a representative value for the standard deviation of Gaussian random noise in DIC measurements is \u03c3 = 4 \u00b7 10\u22124 mm. However, this requires optimal conditions of the experiment, which includes, among others, the optical setup as well as the imaging device. To account for the fact that these are not always met in practice, the authors test the PINN for a standard deviation of \u03c3 = 5 \u00b710\u22124 mm. Under these assumptions, the mean absolute relative errors for a realistic standard deviation of the noise in DIC measurements are MAREE = 2.67% and MARE\u03bd = 34.68% for Young\u2019s modulus and Poisson\u2019s ratio, respectively. In particular, for the Poisson\u2019s ratio, the relative error is no longer in an acceptable range. In future work, we therefore aim to further enhance the method to increase the methods performance in the presence of noisy data as we discuss in the next section."
        },
        {
            "heading": "6. Conclusion and outlook",
            "text": "PINNs have emerged as a suitable alternative approach to traditional numerical methods, such as LS-FEM or VFM, to solve inverse problems in the field of solid mechanics. It has recently been shown that PINNs are in principal capable of identifying the material parameters of constitutive models from full-field displacement data [20, 21, 22, 23]. However, the assumptions made in the literature often do not match real-world conditions. Indeed, as soon as the standard PINN is applied to realistic data, problems arise that have so far been neglected in the literature. In addition, not all approaches in the literature realize the full potential of PINNs. This potential lies in the fact that the inclusion of observation data is straightforward and no computational grid is required.\nIn this contribution, we have done the first steps to further enhance the standard PINN, as proposed in [5], towards full-field displacement data in a realistic regime. The realistic data regime refers to a realistic order of magnitude of the displacement data and material parameters as well as the fact that the data is inevitably affected by noise. The focus of our contribution was on the conditioning and reformulation of the resulting optimization problem. In this process, we first normalized the input and output of the PINN and scaled the parameters to be optimized by providing initial estimates for the material parameters. A sensitivity analysis then demonstrated that the relative errors of the identified material parameters have only a low sensitivity to the initial\nestimates in a reasonable range for real-world applications. Since we found that the cost function was ill-conditioned for realistic displacement data, we balanced the latter by weighting the data loss term. In order to reduce the dependence of the identified material parameters on local approximation errors in the boundary region, we based the identification not on the stress boundary condition but instead on the global balance of internal and external work. In a convergence study, we were able to show that the errors of the identified material parameters converge with the number of collocation points used to approximate the integral for calculating the internal work and the PDE residual. Furthermore, we found that the resulting optimization problem is better posed when it is formulated in terms of bulk and shear modulus. Finally, we investigated the sensitivity of the identified Young\u2019s modulus and Poisson\u2019s ratio with respect to various levels of noise.\nAlthough the enhanced PINN shows satisfactory results for clean as well as moderately noisy data, future work is still needed for a real-world application of PINNs, e.g., as part of a monitoring system as motivated in the introduction. Future work includes, but is not limited to, increasing the robustness of the method to noise in the measurement data. It needs to be investigated whether this can be achieved by quantifying and accounting for the uncertainties in the identified material parameters. For this purpose, we will consider extended variants of PINNs [54, 55]. Since the data loss term is currently weighted manually, we also plan to investigate methods for adaptively balance the cost function, see e.g. [25, 26]. A drawback of the proposed method is that the PINN must be re-trained for each full-field displacement measurement. To accelerate the convergence of the identification process, future work should investigate and validate different approaches, such as adaptive activation functions [28]. Furthermore, the results show that the accuracy of the calibration depends on the initialization of the ANN parameters and the location of the data and collocation points. To improve the robustness of the method, this random factor must be resolved. In order to be able to evaluate the quality of the identified material parameters in real-world applications, qualitative metrics, such as derived in [56] in the context of the LS-FEM approach, are required.\nAnother challenge in real-world applications is to take into account material inhomogeneities and to detect fracture at an early stage. Therefore, we intend to further develop the method towards heterogeneous materials and approximate the material field by another ANN, as proposed in [23]. The determined parameter field could then also be the basis for identifying fracture in the material. In this paper, using the linear-elastic material model, we have pointed out some issues that arise for a realistic data regime and have conditioned the optimization problem. In principle, the first four extensions presented in Section 3 are not restricted to linear problems. The method presented here can therefore also be applied to more complex material models in combination with more sophisticated mechanical test cases. Basically, the method presented in this contribution, which follows the all-at-once approach, is not the only option for material model calibration using PINNs. A concurrent approach is based on parametric PINNs [57], where the PINN acts as a surrogate and learns the parameterized solution\nof the underlying parametric PDE. The pre-trained surrogate can then be used for model calibration. From the authors\u2019 perspective, therefore, conceptual work is prioritized before applications to more complex materials and geometries will be considered.\nDeclaration of competing interests\nThe authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper."
        },
        {
            "heading": "Acknowledgement",
            "text": "The support of the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) is gratefully acknowledged in the following projects:\n\u2022 DFG 255042459/GRK2075-2: Modelling the constitutional evolution of building materials and structures with respect to aging.\n\u2022 DFG 501798687: Monitoring data driven life cycle management with AR based on adaptive, AI-supported corrosion prediction for reinforced concrete structures under combined impacts. Subproject of SPP 2388: Hundred plus - Extending the Lifetime of Complex Engineering Structures through Intelligent Digitalization.\nIn both projects, the solution of inverse problems is a key enabler to link measurement data and physical models. We further thank the Institute of Applied Mechanics at Technische Universit\u00e4t Clausthal for providing us the experimental data and the reference results of the LS-FEM approach. The first author wants also to thank Alexander Henkes, Jendrik-Alexander Tr\u00f6ger, Knut Andreas Meyer and Ralf J\u00e4nicke for the fruitful discussions.\nData availability\nThe code will be published on zenodo.org upon acceptance of this manuscript."
        }
    ],
    "title": "Physics-Informed Neural Networks for Material Model Calibration from Full-Field Displacement Data",
    "year": 2023
}