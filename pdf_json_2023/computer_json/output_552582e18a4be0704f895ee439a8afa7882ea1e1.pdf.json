{
    "abstractText": "Deep learning methods have recently exhibited impressive performance in object detection. However, such methods needed much training data to achieve high recognition accuracy, which was time-consuming and required considerable manual work like labeling images. In this paper, we automatically prepare training data using robots. Considering the low efficiency and high energy consumption in robot motion, we proposed combining robotic in-hand observation and data synthesis to enlarge the limited data set collected by the robot. We first used a robot with a depth sensor to collect images of objects held in the robot\u2019s hands and segment the object pictures. Then, we used a copy-paste method to synthesize the segmented objects with rack backgrounds. The collected and synthetic images are combined to train a deep detection neural network. We conducted experiments to compare YOLOv5x detectors trained with images collected using the proposed method and several other methods. The results showed that combined observation and synthetic images led to comparable performance to manual data preparation. They provided a good guide on optimizing data configurations and parameter settings for training detectors. The proposed method required only a single process and was a low-cost way to produce the combined data. Interested readers may find the data sets and trained models from the following GitHub repository: github.com/wrslab/tubedet Note to Practitioners\u2014The background of this study is a requirement in lab automation \u2013 Using robots to arrange randomly placed tubes automatically. Before sending test tubes to an examination machine for gradient tests, humans need to categorize and organize the tubes into specific patterns to fit the machine\u2019s internal design. Employing humans is difficult as the tube arrangement requirements are time-varying. A preferred solution is using robots to replace humans. The robots should have a vision system to detect the tubes and a manipulation system to perform physical arranging actions. They will be used in busy seasons while deployed for other tasks in leisure time. Deep neural networks like YOLO are effective for the tube detection task. However, preparing the training data is challenging and unsuitable for lab end users. Pre-trained neural networks are options but have limited tube detection ability and cannot deal with newly included tube types. The method developed in this work helps solve the training data preparation problem. With its support, the robot can automatically prepare training data that has comparable quality to manually labeled ones in a single-process and low-cost way.",
    "authors": [
        {
            "affiliations": [],
            "name": "Hao Chen"
        },
        {
            "affiliations": [],
            "name": "Weiwei Wan"
        },
        {
            "affiliations": [],
            "name": "Masaki Matsushita"
        },
        {
            "affiliations": [],
            "name": "Takeyuki Kotaka"
        },
        {
            "affiliations": [],
            "name": "Kensuke Harada"
        }
    ],
    "id": "SP:2b2af9276820cef77acc188eee82d5b9496a58ce",
    "references": [
        {
            "authors": [
                "C. Shorten",
                "T.M. Khoshgoftaar"
            ],
            "title": "A survey on image data augmentation for deep learning",
            "venue": "Journal of big data, vol. 6, no. 1, pp. 1\u201348, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "A. Gupta",
                "A. Vedaldi",
                "A. Zisserman"
            ],
            "title": "Synthetic data for text localisation in natural images",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 2315\u20132324.",
            "year": 2016
        },
        {
            "authors": [
                "X. Yi",
                "E. Walia",
                "P. Babyn"
            ],
            "title": "Generative adversarial network in medical imaging: A review",
            "venue": "Medical image analysis, vol. 58, p. 101552, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "S. Hinterstoisser",
                "O. Pauly",
                "H. Heibel",
                "M. Martina",
                "M. Bokeloh"
            ],
            "title": "An annotation saved is an annotation earned: Using fully synthetic training for object detection",
            "venue": "IEEE/CVF international conference on computer vision workshops, 2019, pp. 0\u20130.",
            "year": 2019
        },
        {
            "authors": [
                "K. Karsch",
                "V. Hedau",
                "D. Forsyth",
                "D. Hoiem"
            ],
            "title": "Rendering synthetic objects into legacy photographs",
            "venue": "ACM Transactions on Graphics, vol. 30, no. 6, pp. 1\u201312, 2011.",
            "year": 2011
        },
        {
            "authors": [
                "P. P\u00e9rez",
                "M. Gangnet",
                "A. Blake"
            ],
            "title": "Poisson image editing",
            "venue": "ACM Transactions on Graphics, vol. 22, no. 3, p. 313\u2013318, 2003.",
            "year": 2003
        },
        {
            "authors": [
                "D. Dwibedi",
                "I. Misra",
                "M. Hebert"
            ],
            "title": "Cut, paste and learn: Surprisingly easy synthesis for instance detection",
            "venue": "IEEE International Conference on Computer Vision, 2017, pp. 1301\u20131310.",
            "year": 2017
        },
        {
            "authors": [
                "G. Georgakis",
                "A. Mousavian",
                "A. Berg",
                "J. Ko\u0161eck\u00e1"
            ],
            "title": "Synthesizing training data for object detection in indoor scenes",
            "venue": "Robotics: Science and Systems, 07 2017.",
            "year": 2017
        },
        {
            "authors": [
                "G. Ghiasi",
                "Y. Cui",
                "A. Srinivas",
                "R. Qian",
                "T.-Y. Lin",
                "E.D. Cubuk",
                "Q.V. Le",
                "B. Zoph"
            ],
            "title": "Simple copy-paste is a strong data augmentation method for instance segmentation",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 2918\u20132928.",
            "year": 2021
        },
        {
            "authors": [
                "V. Florence",
                "J.J. Corso",
                "B. Griffin"
            ],
            "title": "Robot-supervised learning for object segmentation",
            "venue": "IEEE International Conference on Robotics and Automation, 2020, pp. 1343\u20131349.",
            "year": 2020
        },
        {
            "authors": [
                "W. Boerdijk",
                "M. Sundermeyer",
                "M. Durner",
                "R. Triebel"
            ],
            "title": "what\u2019s this?\u201d \u2013 learning to segment unknown objects from manipulation sequences",
            "venue": "IEEE International Conference on Robotics and Automation, 2021, pp. 10 160\u201310 167.",
            "year": 2021
        },
        {
            "authors": [
                "D. Pathak",
                "Y. Shentu",
                "D. Chen",
                "P. Agrawal",
                "T. Darrell",
                "S. Levine",
                "J. Malik"
            ],
            "title": "Learning instance segmentation by interaction",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition Workshops, 2018, pp. 2042\u20132045.",
            "year": 2018
        },
        {
            "authors": [
                "W. Boerdijk",
                "M. Sundermeyer",
                "M. Durner",
                "R. Triebel"
            ],
            "title": "Selfsupervised object-in-gripper segmentation from robotic motions",
            "venue": "Conference on Robot Learning, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "B. Sapp",
                "A. Saxena",
                "A.Y. Ng"
            ],
            "title": "A fast data collection and augmentation procedure for object recognition",
            "venue": "AAAI, 2008, pp. 1402\u20131408.",
            "year": 2008
        },
        {
            "authors": [
                "M. Suchi",
                "T. Patten",
                "D. Fischinger",
                "M. Vincze"
            ],
            "title": "Easylabel: A semi-automatic pixel-wise object annotation tool for creating robotic rgb-d datasets",
            "venue": "IEEE International Conference on Robotics and Automation, 2019, pp. 6678\u20136684.",
            "year": 2019
        },
        {
            "authors": [
                "T. Kiyokawa",
                "K. Tomochika",
                "J. Takamatsu",
                "T. Ogasawara"
            ],
            "title": "Fully automated annotation with noise-masked visual markers for deep-learningbased object detection",
            "venue": "IEEE Robotics and Automation Letters, vol. 4, no. 2, pp. 1972\u20131977, 2019.",
            "year": 1972
        },
        {
            "authors": [
                "D. De Gregorio",
                "A. Tonioni",
                "G. Palli",
                "L. Di Stefano"
            ],
            "title": "Semiautomatic labeling for deep learning in robotics",
            "venue": "IEEE Transactions on Automation Science and Engineering, vol. 17, no. 2, pp. 611\u2013620, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "P. Fitzpatrick",
                "G. Metta"
            ],
            "title": "Grounding vision through experimental manipulation",
            "venue": "Philosophical Transactions of the Royal Society of London. Series A: Mathematical, Physical and Engineering Sciences, vol. 361, no. 1811, pp. 2165\u20132185, 2003.",
            "year": 1811
        },
        {
            "authors": [
                "K. Welke",
                "J. Issac",
                "D. Schiebener",
                "T. Asfour",
                "R. Dillmann"
            ],
            "title": "Autonomous acquisition of visual multi-view object representations for object recognition on a humanoid robot",
            "venue": "IEEE International Conference on Robotics and Automation. IEEE, 2010, pp. 2012\u20132019.",
            "year": 2010
        },
        {
            "authors": [
                "B. Browatzki",
                "V. Tikhanoff",
                "G. Metta",
                "H.H. B\u00fclthoff",
                "C. Wallraven"
            ],
            "title": "Active object recognition on a humanoid robot",
            "venue": "IEEE International Conference on Robotics and Automation, 2012, pp. 2021\u20132028.",
            "year": 2012
        },
        {
            "authors": [
                "M. Krainin",
                "P. Henry",
                "X. Ren",
                "D. Fox"
            ],
            "title": "Manipulator and object tracking for in-hand 3d object modeling",
            "venue": "International Journal of Robotics Research, vol. 30, no. 11, pp. 1311\u20131327, 2011.",
            "year": 2011
        },
        {
            "authors": [
                "M. Bj\u00f6rkman",
                "D. Kragic"
            ],
            "title": "Active 3d scene segmentation and detection of unknown objects",
            "venue": "IEEE International Conference on Robotics and Automation, 2010, pp. 3114\u20133120.",
            "year": 2010
        },
        {
            "authors": [
                "T. F\u00e4ulhammer",
                "R. Ambru\u015f",
                "C. Burbridge",
                "M. Zillich",
                "J. Folkesson",
                "N. Hawes",
                "P. Jensfelt",
                "M. Vincze"
            ],
            "title": "Autonomous learning of object models on a mobile robot",
            "venue": "IEEE Robotics and Automation Letters, vol. 2, no. 1, pp. 26\u201333, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "C.D. Singh",
                "N.J. Sanket",
                "C.M. Parameshwara",
                "C. Ferm\u00fcller",
                "Y. Aloimonos"
            ],
            "title": "Nudgeseg: Zero-shot object segmentation by repeated physical interaction",
            "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems, 2021, pp. 2714\u20132712.",
            "year": 2021
        },
        {
            "authors": [
                "D. Schiebener",
                "A. Ude",
                "T. Asfour"
            ],
            "title": "Physical interaction for segmentation of unknown textured and non-textured rigid objects",
            "venue": "IEEE International Conference on Robotics and Automation, 2014, pp. 4959\u2013 4966.",
            "year": 2014
        },
        {
            "authors": [
                "A. Eitel",
                "N. Hauff",
                "W. Burgard"
            ],
            "title": "Self-supervised transfer learning for instance segmentation through physical interaction",
            "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems, 2019, pp. 4020\u20134026.",
            "year": 2019
        },
        {
            "authors": [
                "S.K. Divvala",
                "D. Hoiem",
                "J.H. Hays",
                "A.A. Efros",
                "M. Hebert"
            ],
            "title": "An empirical study of context in object detection",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition, 2009, pp. 1271\u20131278.",
            "year": 2009
        },
        {
            "authors": [
                "E. Barnea",
                "O. Ben-Shahar"
            ],
            "title": "Exploring the bounds of the utility of context for object detection",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 7412\u20137420.",
            "year": 2019
        },
        {
            "authors": [
                "S.K. Divvala",
                "D. Hoiem",
                "J.H. Hays",
                "A.A. Efros",
                "M. Hebert"
            ],
            "title": "An empirical study of context in object detection",
            "venue": "IEEE Conference on computer vision and Pattern Recognition, 2009, pp. 1271\u20131278.",
            "year": 2009
        },
        {
            "authors": [
                "N. Dvornik",
                "J. Mairal",
                "C. Schmid"
            ],
            "title": "Modeling visual context is key to augmenting object detection datasets",
            "venue": "European Conference on Computer Vision, 2018, pp. 364\u2013380.",
            "year": 2018
        },
        {
            "authors": [
                "H. Wang",
                "Q. Wang",
                "H. Zhang",
                "J. Yang",
                "W. Zuo"
            ],
            "title": "Constrained online cut-paste for object detection",
            "venue": "IEEE Transactions on Circuits and Systems for Video Technology, vol. 31, no. 10, pp. 4071\u20134083, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "T. Hoda\u0148",
                "V. Vineet",
                "R. Gal",
                "E. Shalev",
                "J. Hanzelka",
                "T. Connell",
                "P. Urbina",
                "S.N. Sinha",
                "B. Guenter"
            ],
            "title": "Photorealistic image synthesis for object instance detection",
            "venue": "IEEE International Conference on Image Processing, 2019, pp. 66\u201370.",
            "year": 2019
        },
        {
            "authors": [
                "S.R. Richter",
                "V. Vineet",
                "S. Roth",
                "V. Koltun"
            ],
            "title": "Playing for data: Ground truth from computer games",
            "venue": "European Conference on Computer Vision, 2016, pp. 102\u2013118.",
            "year": 2016
        },
        {
            "authors": [
                "J. Tobin",
                "R. Fong",
                "A. Ray",
                "J. Schneider",
                "W. Zaremba",
                "P. Abbeel"
            ],
            "title": "Domain randomization for transferring deep neural networks from simulation to the real world",
            "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems, 2017, pp. 23\u201330.",
            "year": 2017
        },
        {
            "authors": [
                "A. Carlson",
                "K.A. Skinner",
                "R. Vasudevan",
                "M. Johnson-Roberson"
            ],
            "title": "Modeling camera effects to improve visual learning from synthetic data",
            "venue": "European Conference on Computer Vision Workshops, 2018, pp. 0\u20130.",
            "year": 2018
        },
        {
            "authors": [
                "A. Prakash",
                "S. Boochoon",
                "M. Brophy",
                "D. Acuna",
                "E. Cameracci",
                "G. State",
                "O. Shapira",
                "S. Birchfield"
            ],
            "title": "Structured domain randomization: Bridging the reality gap by context-aware synthetic data",
            "venue": "IEEE International Conference on Robotics and Automation, 2019, pp. 7249\u20137255.",
            "year": 2019
        },
        {
            "authors": [
                "J. Tremblay",
                "A. Prakash",
                "D. Acuna",
                "M. Brophy",
                "V. Jampani",
                "C. Anil",
                "T. To",
                "E. Cameracci",
                "S. Boochoon",
                "S. Birchfield"
            ],
            "title": "Training deep networks with synthetic data: Bridging the reality gap by domain randomization",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition workshops, 2018, pp. 969\u2013977.",
            "year": 2018
        },
        {
            "authors": [
                "A. Tsirikoglou",
                "G. Eilertsen",
                "J. Unger"
            ],
            "title": "A survey of image synthesis methods for visual machine learning",
            "venue": "Computer Graphics Forum, vol. 39, no. 6, 2020, pp. 426\u2013451. ARXIV VERSION, 2022 12",
            "year": 2020
        },
        {
            "authors": [
                "X. Yang",
                "X. Fan",
                "J. Wang",
                "K. Lee"
            ],
            "title": "Image translation based synthetic data generation for industrial object detection and pose estimation",
            "venue": "IEEE Robotics and Automation Letters, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "M. Sundermeyer",
                "Z.-C. Marton",
                "M. Durner",
                "M. Brucker",
                "R. Triebel"
            ],
            "title": "Implicit 3D orientation learning for 6D object detection from RGB images",
            "venue": "European Conference on Computer Vision, 2018, pp. 699\u2013 715.",
            "year": 2018
        },
        {
            "authors": [
                "S. Chatterjee",
                "D. Hazra",
                "Y.-C. Byun",
                "Y.-W. Kim"
            ],
            "title": "Enhancement of image classification using transfer learning and gan-based synthetic data augmentation",
            "venue": "Mathematics, vol. 10, no. 9, p. 1541, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "I. Lundberg",
                "M. Bj\u00f6rkman",
                "P. \u00d6gren"
            ],
            "title": "Intrinsic camera and hand-eye calibration for a robot vision system using a point marker",
            "venue": "IEEE-RAS International Conference on Humanoid Robots, 2014, pp. 59\u201366.",
            "year": 2014
        },
        {
            "authors": [
                "W. Wan",
                "K. Harada"
            ],
            "title": "Developing and comparing single-arm and dual-arm regrasp",
            "venue": "IEEE Robotics and Automation Letters, vol. 1, no. 1, pp. 243\u2013250, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "J. Li",
                "J. Zhang",
                "S.J. Maybank",
                "D. Tao"
            ],
            "title": "Bridging composite and real: towards end-to-end deep image matting",
            "venue": "International Journal of Computer Vision, vol. 130, no. 2, pp. 246\u2013266, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "G. Jocher"
            ],
            "title": "ultralytics/yolov5: v3.1 - Bug Fixes and Performance Improvements",
            "venue": "Oct. 2020. [Online]. Available: https: //doi.org/10.5281/zenodo.4154370",
            "year": 2020
        },
        {
            "authors": [
                "Z. Zou",
                "Z. Shi",
                "Y. Guo",
                "J. Ye"
            ],
            "title": "Object detection in 20 years: A survey",
            "venue": "arXiv preprint arXiv:1905.05055, 2019.",
            "year": 1905
        }
    ],
    "sections": [
        {
            "text": "Note to Practitioners\u2014The background of this study is a requirement in lab automation \u2013 Using robots to arrange randomly placed tubes automatically. Before sending test tubes to an examination machine for gradient tests, humans need to categorize and organize the tubes into specific patterns to fit the machine\u2019s internal design. Employing humans is difficult as the tube arrangement requirements are time-varying. A preferred solution is using robots to replace humans. The robots should have a vision system to detect the tubes and a manipulation system to perform physical arranging actions. They will be used in busy seasons while deployed for other tasks in leisure time. Deep neural networks like YOLO are effective for the tube detection task. However, preparing the training data is challenging and unsuitable for lab end users. Pre-trained neural networks are options but have limited tube detection ability and cannot deal with newly included tube types. The method developed in this work helps solve the training data preparation problem. With its support, the robot can automatically prepare training data that has comparable quality to manually labeled ones in a single-process and low-cost way.\nIndex Terms\u2014Robotic data preparation, data synthesis, test tube detection\nI. INTRODUCTION Recent advances in deep learning have led to a revolution in object detection. Deep learning-based methods use deep 1Department of System Innovation, Graduate School of Engineering Science, Osaka University, Toyonaka, Osaka, Japan. 2H.U. Group Research Inst. G. K., Japan. 3National Inst. of AIST, Japan. \u2217Contact: Weiwei Wan, wan@sys.es.osaka-u.ac.jp\nneural networks to learn features from training data. They outperform traditional hand-crafted features with impressive results. Despite these advantages, deep learning-based object detection requires collecting a large amount of labeled data for training, which is time-consuming and labor-intensive, and has significantly hindered the scalability and flexibility of deep learning-based applications.\nPreviously, researchers have developed several methods to reduce data collection costs. For example, data augmentation [1] enriched existing training data sets by applying random transformations like image rotation or scaling. Data synthesis [2][3] generated previously unseen data using simulation or adversarial neural networks. The main challenge of the augmentation or synthesis methods was the \u201cdomain gap\u201d [4][5]:\nar X\niv :2\n30 1.\n01 44\n1v 1\n[ cs\n.C V\n] 4\nJ an\n2 02\n3\nAugmented data had less varied visual contexts. Synthesized data was prone to discrepancies with the real world. Recently, researchers have revisited using the copy-paste method [6] to increase data. The method was effective in compensating for the \u201cdomain gap\u201d problem, exhibiting impressive performance. There is no clear boundary between augmentation and synthesis when using the copy-paste method to generate data. It was mainly classified as an synthesis method [7][8], although some studies considered it to be augmentation [9]. This paper calls it a synthesis method to avoid confusion with transformation and scale-based data generation.\nThe most tiring aspect of the copy-and-paste method is how to neatly cut a large variety of target regions and paste them onto a new background. Previously, researchers working on robotic manipulation have developed robotic methods to segment novel objects from backgrounds. For example, Florence et al. [10], Boerdijk et al. [11], and Pathak et al. [12] respectively used robotic in-hand or non-prehensile manipulation to change objects\u2019 observation viewpoints and segmented the objects based on the robot motion. Such systems could replace humans to segment goal object regions under various conditions. Very recent studies [11][13] has noticed the advantage, and increased data size and contextual variety by pasting the objects segmented by robotic systems onto random backgrounds. Despite their seminal proposals, the need for copy-and-paste synthesis and the impact of data volume and ratios remain undiscussed.\nBased on the current research status, this paper further delves into using robots to collect training data automatically. Considering the low efficiency and high energy consumption in robotic data collection, we propose combining robotic observation and copy-paste synthesis to reduce costs. We assume a test tube detection task shown in Fig. 1 and use a robot with a depth sensor to move and observe tubes. The robot collects observation images and, at the same time, segments tubes from the images for copy-paste synthesis. The observation and synthetic images are used as training data for deep detection neural networks. Especially for the synthesis routine, we value the co-occurrence of tubes and racks, and paste tubes inside a rack area to obtain contextual consistency. Also, we take into account factors like tube-to-tube occlusions and foreground changes caused by environment or visual difference to reduce unrealistic synthetic results. The proposed method helps enrich the data set and resolve the \u201cdomain gap\u201d. It does not need heavy robotic effort.\nIn experiments, we trained several YOLOv5x networks to understand the performance of the proposed method. The training data was collected using the proposed and several other methods. The results confirmed data collected using the proposed method do have claimed advantages. We also conducted multiple ablation studies to look into the impact of data volumes and ratios when training detection neural networks using data collected with the proposed method. The results provided a good guide on optimizing data configurations and parameter settings for training detectors.\nThe contributions of this work are as follows. (1) We develop an automatic data-collection method in which a robot holds target objects and observes them. The method yields\nobservation images and target regions segmented from the images. (2) We develop a copy-paste image synthesis method to enrich the training data. The method pastes object regions on various rack backgrounds to balance \u201cdomain randomization\u201d and \u201cdomain gap\u201d. The rack backgrounds are also automatically collected by the robot. (3) We examined combinations of the observation and synthetic images and compared them with other data sets to understand the impact of data volume and ratios.\nThe remaining part of this paper is organized as follows: Section II reviews related work. Section III presents the hardware system and the proposed method\u2019s workflow. Section IV delivers technical details. Section V shows experiments and analysis. Section VI draws conclusions."
        },
        {
            "heading": "II. RELATED WORK",
            "text": "We review the related work considering robotic data collection and data synthesis, respectively."
        },
        {
            "heading": "A. Automatic Data Collection Using Robots",
            "text": "Segmenting the object regions from a picture is the basis of automatic data collection. Conventional methods used simple backgrounds [14], known environments [15], or designed easily identifiable gadgets [16][17] to simplify object extraction. The methods required careful preparation about scenes and objects.\nRobot-based methods leverage actuated robots to simplify object segmentation. They can be traced back to early studies in object recognition and 3D object modeling [18][19][20][21]. These work took advantages of robotic manipulation sequence to perceive objects from different viewpoints and segment the objects from the background. From the robotic manipulation perspective, such segmentation can be divided into two categories: In-hand object segmentation and Interaction-based segmentation.\n1) In-hand object segmentation: Previous work on in-hand object segmentation used known robot models and handcrafted visual features to isolate in-hand objects from background environments and robot hands. For example, Krainin et al. [21] isolated in-hand objects\u2019 point clouds by examining the Euclidean distance to the robot model. Welke et al. [19] segmented in-hand objects from images based on Eigen background subtraction, disparity map, and hand localization. These methods required manually preparing detectors for various targets considering their visual features.\nMore recent studies used deep learning to reduce the reliance on hand-crafted visual features for in-hand object segmentation. For instance, Florence et al. [10] proposed a self-supervised framework to segment in-hand objects. The framework involved two steps that used the same training and learning routine. In the first step, the authors generated masks for the robot by considering combined depth and RGB information, and trained a neural network model based on the masks to differentiate the robot from the background. In the second step, the authors masked the grasped object and train neural network models to isolate the object from the robot hand. Boerdijk et al. [13] used optical flow to\nrespectively segment manipulators that were holding and not holding objects. The segmented data set were used to train a neural network for isolating manipulators and grasped objects.\n2) Robot-object interaction: On the other hand, some researchers took advantages of non-prehensile robot manipulation like push to change object perspectives and segment them based on robot motion cues [12][22][23]. For example, Pathak et al. [12] designed a framework to continuously refine a neural network model that generates object segmentation masks through robot interaction. The model initially generated hypothesis segmentation masks for objects. The masks were refined based on the pixel differences of the images captured before and after robotic interactions. The generating model was updated along with the refined masks. Singh et al. [24] proposed to segment unknown objects in a cluttered scene while repeatedly using robotic nudge motions to interact with objects and induce geometric constraints. Robotic interactive segmentation often requires a static scene or surface to permit interaction between robots and objects [25][26]. It is more complicated compared with the in-hand object segmentation as the object poses needs to be controlled and changed through robotic manipulation.\nA critical problem of the robotic methods is that they are unsuitable for preparing a large amount of training data as robots consume much time and energy to perform the physical motion. Conducting thousands of robotic motion trajectories to collect data is impractical. Also, the robots in the systems are fixed, have limited views, and can only collect data in a narrow range of scenarios. Neural networks trained using the collected data may suffer from contextual (background) bias and have bad generalizability [27][28].\nThis study focuses on robotic data collection while considering leveraging data synthesis to reduce robotic usage. We first ask the robot to hold a single tube and annotate the tube\u2019s bounding polyhedron by extracting in-hand point cloud according to the robot\u2019s tool center point (TCP) and hand model. Then, we map the annotated bounding polyhedron to 2D image regions in the robot\u2019s camera view for extracting the tube region. The robot moves the tube to different positions and rotations to obtain many varieties of 2D images and tube regions. The images and tube regions are respectively used for training and synthesizing new data in a later stage."
        },
        {
            "heading": "B. Data Augmentation and Synthesis",
            "text": "Data augmentation and synthesis are the two most wellused methods to enrich training data. Data augmentation generates new data by transforming the existing training data with specific rules or learning-based methods. Data synthesis generates synthetic data by merging existing data with others or using computer simulations. Concurrent publications tend to mix these nomenclatures. Therefore we conduct a uniform literature review of them below without differentiation.\nThe copy-paste method is widely used for generating synthetic data. It segments foreground objects from existing images, possibly modifies them, and pastes them onto new backgrounds [8][7][9]. The copy-paste method is easy to implement and shows notable performance over using pure\nreal data. Previous studies showed that it was important to carefully select the backgrounds when pasting objects. For example, Divvala et al. [29] experimentally showed visual context benefited object detection performance and reduced detection errors. Dvornik et al. [30] showed that the correct visual context when pasting object can improve prediction performance while inappropriate visual context led to negative results. Wang et al. [31] swapped objects of the same class in different images to ensure contextual consistency between objects and backgrounds and showed using the exsiting backgrounds had better performance than random ones. Also, the copy-paste method requires a data set containing many possible views of the object that are easy to be cut out. It is burdensome for humans to prepare them.\nGraphical simulation is another popular method for synthesizing training data. The benefits of simulation is that it allows freely changing light conditions and materials to increase variation. It also allows capturing many views of objects by simply transforming virtual camera poses. For example, Hodan\u030c et al. [32] and Richter et al. [33] respectively used photorealistic rendering to synthesize images of 3D object models and scenes. The methods required a lot of computational resources to narrow down the domain gap between synthetic and realistic data. Tobin et al. [34] proposed the concept of domain randomization (DR). They randomized a simulator to expose models to a wide range of environments and obtain varied training data. Instead of photo-realistic rendering, the method only required low-fidelity rendering results to reach satisfying accuracy for medium-size objects. Carlson et al. [35], Hinterstoisser et al. [4], Prakash et al. [36], and Tremblay et al. [37] respectively used DR to narrow down the domain gap. The authors randomly changed the context in simulation so that \u201cthe real data was made to be just like another simulation\u201d [38]. Yang et al. [39] and Sundermeyer et al. [40] respectively sampled viewpoints of 3D object models using simulation and mixed the samples with real backgrounds to reduce the human effort for preparing scenes with rich domain randomness. Besides DR, Generative Adversarial Networks (GANs) were also promising to reduce domain gap. For example, Chatterjee et al. [41] designed a lightweight-GAN to synthesize data for training plastic bottle detectors.\nIn this study, we leverage data synthesis to enrich the training data. We develop a copy-paste based method to attach tube cap regions separated from robotic observation images to rack backgrounds and thus synthesize new images. Various constraints like rack dimensions and tube occlusions can be considered during the synthesis to reduce the domain gap. The synthetic data is mixed with real-world data to promote the performance of YOLO-based tube recognition neural networks. It is also compared with other data collection methods to understand the influence of data volume and data combination ratio."
        },
        {
            "heading": "III. ROBOT SYSTEM AND WORKFLOW",
            "text": ""
        },
        {
            "heading": "A. Configurations of the Robot System",
            "text": "Fig. 2(a) shows our robot system used for preparing the training data. A Photoneo Phoxi M 3D Scanner is used for\ncapturing objects on the flat table. An ABB Yumi dual-arm robot with a two-finger gripper is used to manipulate objects in the system. A flat table is set up in the front of the Yumi robot. The in-rack test tubes to be recognized are placed on the surface. The Phoxi scanner is a structured-light based depth sensor. It can capture gray images and point clouds simultaneously. Each data point of a point cloud captured by the Phoxi scanner have a one-to-one correspondence to a pixel in a gray image. We can segment an object in the gray image by considering its point cloud.\nEspecially, we install the Phoxi scanner on top of the robot to obtain a top view of the racks and tubes. When recognizing tubes in the rack, we select the tube caps as the primary identifiers. There are two reasons why we prefer using the tube caps for identification. The first one is that obtaining the point cloud of a translucent or crystal test tube fails easily due to limitations of the structured-light based depth sensors. The second one is that the tube bodies are blocked by the caps and also occluded by surrounding tubes when placed in the rack and viewed from a top position. They are less visible. However, despite the reasons and their merits, there is a problem that different types of tubes may share a same cap type. In this work, we assume the test tubes with the same caps can be identified by their heights in the rack and analyze the point cloud to differentiate them."
        },
        {
            "heading": "B. Workflow for Data Preparation",
            "text": "We prepare the training data using the robot system following the workflow shown in Fig. 3. There are four dashed boxes in the chart, where (a.1) and (a.2) have a blue background color and represent the data collection component, (b) has an orange background color and represents the data synthesis component, (c) has a gray background and represents the resulted data.\nThe first blue dashed box (Fig. 3(a.1)) comprises three steps. First, a human hands over an unknown test tube to the robot. The tube is assumed to be grasped vertically by the robot after handover, with the tube cap left above the robotic fingertips. Second, the robot moves the test tube to the observation poses prepared offline while considering avoiding self-occlusions. The Phoxi sensor will capture the test tube\u2019s gray image and point cloud at each observation pose. Third, the system segments the cap region out of the captured image based on a mapping from its counterpart point cloud. The segmentation result only includes the cap. The background will be removed thanks to the point cloud mapping. The output of this dashed\nbox includes many cap region pictures. They are observed from different views and thus have different illumination and visual conditions.\nThe second blue dashed box (Fig. 3(a.2)) is similar to the first one and also comprises three steps. First, a person places a rack in the environment. Then, the robot pushes the rack to random poses, capturing the rack\u2019s gray image and point cloud at each pose. Third, the system segments the rack region out of the captured image based on the mapping from the rack\u2019s counterpart point cloud. The result of this dashed box includes many rack region pictures. Like the caps, the rack region pictures also have different illumination and visual conditions since the data is captured from different view positions.\nThe orange dashed box shows the data synthesis process, where the cap region pictures obtained in the first \u201cData Collection\u201d dashed box are pasted onto the rack region pictures obtained in the second \u201cData Collection\u201d dashed box for synthesizing new images. Constraints like rack boundaries and overlapping caused by perspective projection are considered during the synthesis. The output of the dashed box will be racks filled with many tube caps. The \u201cCopy-paste data synthesis\u201d sub-block illustrates several examples of the output.\nThe final data preparation results include the images obtained during collecting the tube cap data (observation images) and the synthetic images. They are illustrated in the gray dashed box (Fig. 3(c)).\nNote that the above workflow is not completely automatic. The sub-blocks with texts highlighted in a green color involve human intervention. Also, before data collection, we need to prepare the camera calibration matrix and test tube observation poses. The camera calibration matrix transforms the point cloud captured in the camera\u2019s local coordinate system into the robot coordinate system. Many existing methods exist for obtaining the calibration matrix [42]. To avoid repetition, we don\u2019t discuss the details in this manuscript. The test tube observation poses are a set of tube positions and rotations for the robot to hold and capture observation images. The developed method will generate robot joint configurations considering the robot grasping and tube observation poses. Section IV will present detailed algorithms on the generation.\nIV. IMPLEMENTATION DETAILS"
        },
        {
            "heading": "A. Observation Poses for Collecting Tube Caps",
            "text": "When collecting the tube cap data, the robot moves the tube held in its hand to different poses for observation. The observation poses are generated considering two constraints: (1) Diversity of the captured cap data; (2) Occlusions by robot links. Taking into account these two constraints allow us to include the tube caps from many viewpoints and thus cover lots of illumination and visual conditions. Meanwhile, they help to prevent the robot links from occluding the grasped test tubes and make sure the tubes are visible to the vision sensor.\nFig. 4 illustrates the observation pose generation process and how the two constraints are taken into account in it. First, we sample the positions and rotations of a tube held by the robot hand uniformly in the Phoxi depth sensor\u2019s visible range. Tube\ndata captured under the sampled poses will have rich light conditions and a large variety of visible tube edges for training a recognition neural network. Especially, the tube rotations are sampled according to the vertices of a level-four icosphere [43]. An icosphere is a spherical polyhedron with regularly distributed vertices. The vectors pointing to the vertices of an icosphere help to define the rotations of a tube1. A level-four icosphere has 642 vertices and thus leads to 642 vectors and test tube rotation poses. Thanks to the visibility constraints, we do not move a test tube to all of the rotation poses for capturing data as the tube caps facing downward will not be seen by the Phoxi sensor. We filter the 642 vectors by considering their angles with the normal of the table surface for placing a rack. The vectors with large angles from the surface normal cannot be seen and will not be considered. The spherical polyhedron in Fig. 4(b.1) illustrates the level-four icosphere. Vectors pointing to the red vertices have more than \u03b8 angles from the surface normal and are removed. The green vertices are the remaining candidates. The purple tube bouquet on the right side of Fig. 4(b.1) illustrate the tube poses implied by vectors pointing to the remaining candidate vertices.\nNext, we plan the robot motion to move the test tube held in a robot hand to the sampled tube positions and rotations. We assume a test tube is vertically grasped at the finger center of a robot hand. Since a tube is central symmetric, many grasping poses meet the assumption. The grasping hand may rotate freely around the symmetry axis of the test tube, as shown in Fig. 4(b.2). The rotation is compact and forms a SO(2) group. For numerical analysis, we sample the rotation in the SO(2) group with a rotation interval hyperparameter named \u03c9 to obtain a series of discretized grasping poses. The hand\n1A tube is centeral symmetric. We do not need to consider its rotation around the central axis. The vectors pointing to the vertices of an icosphere can thus define a tube pose.\nillustrations in Fig. 4(b.2) are the grasping poses obtained with \u03c9 = 60\u25e6. The sampled grasping poses provide many candidate goals for robot motion planning and thus increase the chances of successfully moving and observing the tube.\nWhen determining which exact candidate goal to move to, we examine the occlusions from the robot arm links and avoid choosing the grasping poses that lead to invisible tubes. In detail, examining the occlusion is done by checking the collision between a visual polyhedron and the robot arm links. The visual polyhedron is computed using the camera origin and vertices of the robot hand model, as illustrated in Fig. 4(c.1). The robot arm may occlude the tube and the vision sensor fails to capture it when there is collision between the visual polyhedron and the robot arm links. Fig. 4(c.2) exemplifies such a case."
        },
        {
            "heading": "B. Using Annotation Masks to Segment Cap Pictures",
            "text": "Since the tube is handed over from a human and the Phoxi sensor captures the cap data from many different views, the captured tube point clouds change dynamically and have noises. It is unstable to extract cap point clouds by autonomously detecting them. Thus, instead of autonomous detection, we prepare an annotation mask in the robot hand\u2019s local coordinate system to help extract the test tube cap\u2019s point clouds. The extracted point clouds will be back-projected to the corresponding 2D grey image for segmenting a picture of the cap region. Fig. 5 shows the details of this mask and how it helps to segment the cap regions. The mask and back projection enable us to precisely segment the cap regions while avoiding including backgrounds.\nTo prepare an annotation mask, we move the robot hand that holds a test tube to a fixed position under the Phoxi sensor and trigger the sensor to capture a point cloud. We can\neasily get the cap\u2019s point cloud data by examining the area on top of the holding fingers and obtain an annotation mask by considering a bounding polyhedron of the data. However, a single bounding polyhedron may not be general for others since the captured point cloud is susceptible to light reflection or perspective projection (self-occlusion). Thus, instead of a single point cloud and polyhedron, we collect point clouds from multiple views, merge them under the robot hand\u2019s local coordinate system, and compute a bounding box of the merged result as an annotation mask. Fig. 6 shows an example. The multiple views are sampled the same way as the observation poses mentioned in the previous subsection. However, we do not need to change the observation positions since we aim to\nobtain a bounding box mask in the hand\u2019s coordinate system. The views under various rotations could provide enough superficial point cloud data to meet the requirements. Note that the merged result may include noise point data induced by reflections from the transparent tube body and lead to a mask larger than the cap. We provide an interactive user interface for manually adjusting the bounding box sizes and minimizing the negative influences caused by the noises. The adjustment is optional and may be performed when precisely segmenting the cap region is demanded."
        },
        {
            "heading": "C. Copy-Paste Synthesis",
            "text": "We apply random scaling, blurring, brightness, and contrast to the segmented tube caps and then paste them onto the segmented rack background for data synthesis. During pasting, we permit the overlap among the cap regions to approximate tube-to-tube occlusion. After pasting, we randomize the environmental background (background of the rack) to narrow further the domain gap between synthetic images and images captured in the real world.\nA critical maneuver here is that we consider the cooccurrence of the test tubes and the rack and paste the tube cap pictures onto a rack instead of random backgrounds like [11]. We randomly sample positions inside rack pictures for pasting tube caps and use a pasting number T to control the clutter. Note that there is no need to exactly paste a tube cap near the hole centers of a rack as the tubes tilt randomly inside the rack holes. The visible cap regions may reasonably overlap with a hole boundary or other holes.\nFor tube-to-tube occlusion, we consider the perspective projection of a vision sensor and define an occlusion threshold t to permit overlap among the visible cap regions. A vision sensor\u2019s perspective projection leads to mutual occlusions in the rack at certain viewpoints. The occlusion threshold helps to simulate the occlusion and defines the maximum percentage that segmented cap pictures can overlap or occlude. Fig. 7 shows how the t threshold works. It adds a constraint to pasting, where a previously pasted cap picture \u201cA\u201d must have less than t percentage overlap with the union of caps pasted later. The B \u222a C \u222a ... component in the nominator of Fig. 7 implies the union of caps pasted after \u201cA\u201d. When a new cap is randomized, it must be unioned with this component\nto ensure the t constraint on all previous \u201cA\u201d is not violated. There are two noticeable points for t. First, its value could be devised respectively considering the heights of specific tube types. Second, its value is correlated with the pasting number T . The maximum number of pasted tube caps in a rack that meet the t threshold may be less than a given T . In that case, we constrain the maximum number of pasted tube caps to the smaller value to ensure t is not invalidated.\nFor the environmental background, we use the BG-20k data set [44] to obtain high-resolution random background images and change the background of a synthetic image with a 0.5 probability."
        },
        {
            "heading": "V. EXPERIMENTS AND ANALYSIS",
            "text": "We carried out experiments to compare YOLOv5x [45] detectors trained using data sets collected with the proposed method and several other methods to understand the performance. Table II shows the methods. The SR (Synthesis by pasting to Racks) method pastes randomly selected cap pictures onto rack backgrounds to synthesize training data. It represents the synthesizing method used in this work. The SB (Synthesis by pasting to BG-20k) method is an alternative synthesis method. Instead of being pasted onto a rack, randomly selected cap pictures are pasted to random backgrounds selected from the BG-20k data set. The RO (Robotic Observation) method is a byproduct of robotic cap segmentation, where the robot holds test tubes for data collection. We considered RO an independent method because we wondered if the hand-held observation was enough for training. We also combined RO, SR, and SB methods (the ** row) to see if they help achieve a satisfying performance. The RO+SR combination is exactly our proposed method in this work. We especially proposed it since RO is a pre-process of robotic cap segmentation. Using combined RO+SR does not increase effort. Combining RO+SB or RO+SR+SB are also candidate choices. They have the same cost as using independent SR or SB data2. Finally, the CL (Crowd-source Labeling) method is a conventional one that requires humans to place racks with tubes under the robot and label the captured images manually. Fig. 8 shows exemplary images collected using the different methods.\n2Synthesizing data is considered to be free as it only require computational work. Thus, the costs of SR and SB depend on the RO process."
        },
        {
            "heading": "A. Performance of Various Data sets",
            "text": "We collected various data sets with the methods and their combinations, used the data sets to train YOLOv5x detectors, and examined the performance of the trained detectors using a testing data set for comparison.\nThe first data set is CL200. It is considered a baseline for comparison. In collecting the data set, we collected 200 images with random tube and rack states and labeled the tube regions manually using LabelImg3. There are, in total, 5916 labeled instances in the 200 images.\nThe second data set is SR1600. In order to collect it, we first prepared many cap pictures using robotic observation. As shown in Fig. 2(b), we assumed four different test tubes and took advantage of the Yumi robot\u2019s both arms to collect cap data quickly. For each tube type, we handed over two same ones to the two robotic arms for observation. Each arm moved its held tube to 400 observation poses for data collection. See Fig. 9(a) for example. Here, we set the hyperparameter \u03b8 and \u03c9 to 30\u25e6 and 360\u25e6 (single grasping pose) and set the positions to be evenly sampled on the table with a granularity of 0.1m for generating the observation poses. In total, more than 400 observation poses were obtained under the parameter setting for each arm, and we used the first 400 for collecting images. As a result, we obtained 400 observation images (800 cap pictures since there are two tubes in each image, see Fig. 9(b) for example) for a single tube type and 1600 observation images for all tube types. We segmented 3200 pictures of cap regions from the observation images considering point cloud mapping. Fig. 9(c) shows the collected point clouds with highlighted caps (green). Fig. 9(d) shows the segmented cap regions. Besides the cap regions, we collected 15 images with racks (a single rack in each image) and segmented 15 pictures of racks. We synthesized a data set of 1600 images by pasting caps randomly selected from the 3200 cap pictures to racks randomly selected from the 15 rack pictures (SR method). During synthesis, we set the pasting number to be T = 30, and set the occlusion threshold for the \u201cBlue Tube\u201d to be tblue = 0.4 and other tubes to be tothers = 0.15. We chose these parameter settings because the \u201cBlue Tube\u201d was shorter and susceptible to occlusion. We increased its occlusion threshold\n3https://github.com/heartexlabs/labelImg\nto mimic frequent visual blockage from other tubes. Also, we increased the variety of the segmented cap pictures by applying random scaling (0.9 \u223c 1.1 of original picture size, 0.5 probability), random blur (3 \u00d7 3 kernel, 0.5 probability), random brightness (0.9 \u223c 1.1 of original brightness, 0.5 probability), and random contrast (0.9 \u223c 1.1 of the original contrast value, 0.5 probability) using the Albumentations4 library. The background of the rack was randomly chosen from the BG-20k data set with a 0.5 changing probability.\nThe third data set is RO1600. It is a semi product of robotic cap segmentation and comprises the 1600 observation images obtained during robotic observation.\nThe fourth data set is SB1600. In contrast with the SR1600 data set, we pasted randomly selected caps directly to images from the BG-20k data set for obtaining data. The pasted caps might freely distribute on the image background. The segmented racks were not used. The pasting number T and occlusion threshold t are 35 and 0.15 respectively. There was no difference on t for different tubes. The randomization were performed in the same way as obtaining SR1600.\nWe also used combined methods to collect data sets and study if the combination led to better results. The combined data sets include RO1600+SR800, RO1600+SB800, RO1600+SR400+SB400, SR800+SB800. Here, the superscript number on the upper-right of a method name means the number of images collected using the method. The \u201c+\u201d symbol indicates that the data sets comprise data collected using different methods. The RO1600+SR800 data set represents the data collected using the proposed method.\nThe left part of Table II summarizes the various data sets. They are used to train YOLOv5x detectors for comparison. Before training, the YOLOv5x detectors for all data sets were initialized with weights pre-trained using the COCO data set. The images in all data sets were regulated into a resolution of 1376 \u00d7 1376. Each data set is divided into a training subset and a validation subset according to a 4 to 1 data ratio. During learning, the training subset was fed to the training program with a batch size of 2, and the training program performed validation per episode. The training process was stopped when the mAPs (mean Average Precision) [46] for all objects reached higher than 99.0% under a 0.5 IoU (Intersection over Union). Here, we defined a detected bounding box to be correct when its IoU with a ground truth cap bounding box was larger than 0.5.\n4https://albumentations.ai/\nFor evaluating the performance of YOLOv5x detectors trained using the various data sets, we collected a testing data set with 100 images and labeled their ground truth using the same method as CL. We used the trained detectors to detect tubes in the testing data set. Like validation, we defined a detected bounding box as correct when its IoU with a ground truth cap bounding box is larger than 0.5. We used the AP (Average Precision) metric to measure the detection performance of a single object class and used the mAP for all objects. Since the detector that met a single satisfying validation was not necessarily the best, we trained each detector twice and took the higher precision value on the testing data set as the final evaluation result.\nTable II shows the evaluation results. We obtained the following observations and speculations from them.\ni) Using the data set collected by robotic observation for training exhibited the worst performance, as shown by the 2nd row (RO1600). Speculation: All images in the data set had a similar robotic background. They suffered from a domain shift.\nii) The synthetic data sets do not necessarily lead to a good AP, as shown by the 3rd (SR1600) and 4th (SB1600) rows. The SR1600 data set exhibited higher performance than the SB1600 data set. Speculation: The copy-paste synthesis failed to cover certain visual contexts; Pasting onto racks (SR) provided more effective visual contexts and benefited the neural network more than pasting onto random backgrounds (SB).\niii) Combining the synthetic data sets with robotic observations is effective. It can be concluded by comparing the 5th, 6th, and 7th rows (RO1600+SR800, RO1600+SB800, RO1600+SR400+SB400) with the 2nd, 3rd, and 4th rows (RO1600, SR1600, and SB1600). The former rows had higher mAP than the latter. Speculation: The robotic observation data set additionally provided helpful visual contexts. iv) The 5th row (RO1600+SR800) had a 2.4% higher mAP than the 6th row (RO1600+SB800). Especially, the AP of the \u201cBlue Tube\u201d on the 5th row was 8.7% higher than that on the 6th row. The AP of other tubes also had 0.1% \u223c 0.7% performance increase. Speculation: Considering the rack as a local context helped improve domain-specific performance; The short \u201cBlue Tube\u201d could be easily blocked. The data set collected using the SR method had more simulated occlusions. They were important for recognizing the short \u201cBlue Tube\u201d.\nv) The 7th row (RO1600+SR400+SB400) exhibited slightly higher mAP (0.3%) than the 5th row (RO1600+SR800). Speculation: Pasting onto racks (SR) provided better domain-specific features. Random backgrounds for the tubes slightly benefited the neural network and were less necessary if the goal context was limited.\nvi) The 5th row (RO1600+SR800) is competitive compared with the 1st row (CL200). The mAP was 0.7% lower. The AP of the \u201cBlue Tube\u201d and \u201cWhite Tube\u201d were 2.5% and 1.1% lower, respectively. The AP of the \u201cPurple Tube\u201d\nwas the same. The AP of the \u201cPurple Ring\u201d tube was 0.8% higher. Speculation: The robotic observation and paste-to-rack synthesis compensated for each other\u2019s shortcomings; There remained extreme cases that could be labeled manually but failed to be covered by robotic observation or synthesis, especially for the \u201cBlue Tube\u201d. Several failure cases are visualized in Fig. 10 to provide the readers an insight into our observations and speculations. Fig. 10(a) and (b) exemplify the recognition results of detectors trained using the 5th (RO1600+SR800) and 6th data sets (RO1600+SR800). The latter one failed to recognize occluded tubes as the training data set had fewer simulated occlusions. The example is consistent with the observation and speculation in iv). Fig. 10(c) and (d) exemplify cases that the detectors trained using the 5th (RO1600+SB800) data set failed. In the first case, shadows from other test tubes were cast on a blue test tube cap. The detector failed to recognize the tube. In the second case, the detector misrecognized a crystal tube body as the \u201cBlue Tube\u201d cap due to the illusion caused by bodyand-rack overlap. The two failure examples are consistent with the observation and speculation in vi). The synthetic data sets do not involve shadows or tube bodies. The detectors trained using them had worse performance in these cases than the one trained using the crowd-sourced real-world data.\nIn summary, the results of the various training data sets showed that combining data collected using the RO and SR\nmethods was effective. The conclusion was satisfying as the RO method is a subset of the SR method. The workflow for collecting them is simple and clean. However, we wonder if the number of images in the RO data set could be reduced, as it needs much manual handover to collect them. This query prompted us to carry out the studies in the following subsection."
        },
        {
            "heading": "B. Ablation Study",
            "text": "In this subsection, we conduct multiple ablation studies on the combined RO+SR data set to further understand 1) the influence of the data combination ratio and 2) the influence of pasting number T and occlusion threshold t used for generating synthetic data.\n1) Influence of data combination ratio: The experiments for studying the influence of data combination ratio are divided into two parts. In the first part, we set the number of images collected using the RO method to 800 and varied the number of images collected using the SR method from 200 to 1600 in a 2-fold ratio to understand the importance of the SR data. The upper section of Table III shows the precision of detectors trained using the varied data. The results indicate that the mAP improved when the SR image numbers increased from 200 to 1600. The second part is similar to the first one. In this part, we fixed the number of images collected using the SR method to 800 and varied the number of images collected using the RO method from 200 to 1600 in a two-fold ratio to understand the importance of the RO data. The lower section of Table III shows the precision of detectors trained using the varied data. The result indicates that the mAP improved when the RO image numbers increased from 200 to 1600.\n2) Influence of hyperparameters: Besides the data combination ratio, we also studied the influence of pasting number T and occlusion threshold t used in the SR method. We set both the RO and SR image numbers to 800 and observed the performance of detectors trained with data sets collected using different T and t values. Although we previously used a different t value for the \u201cBlue Tube\u201d, we did not differentiate the tubes here. Like the study on different data combination ratios, this study also comprised two parts. In the first part, we fixed t to be 0.1 and increased T from 10 to 40 with a step length of 10. The upper section of Table IV shows the precision changes under the parameter variations. The\nresults exhibited a significant increase from 10 to 30. However, an even larger T had little influence on the recognition performance. In the second part, we set T to be 30 and varied t from 0.20 to 0.80 with a step length of 0.2. The lower section of Table IV shows the precision changes under the parameter variations. The results exhibited a clear precision increase on the \u201cBlue Tube\u201d. We speculate that the reason was that the \u201cBlue Tube\u201d was shorter and vulnerable to occlusions. A larger t helped provide more occlusion cases in the training data set, leading to a higher detection rate. The results also indicated that the precision of the \u201dWhite Tube\u201d and \u201dPurple Ring Tube\u201d irregularly changed as the t increased. They were taller and did not suffer from occlusions. Adding occlusions for them caused unexpected errors. For a complete observation, we recommend interested readers to compare with the third row of the table\u2019s upper section to catch the changes starting from t = 0.1. The T value of the upper section\u2019s third row was the same as the rows in the lower section."
        },
        {
            "heading": "C. Further Analysis on Synthetic Data",
            "text": "We also studied the influence of cap variation and combination ratio on synthetic data sets (the data sets collected using the SR, SB, or SR+SB methods). The goal was to understand the best performance we could reach with synthesis.\nFirst, we fixed the number of images collected by the SR and SB methods to 800, respectively. We changed the number of cap region pictures (equals to the number of observation images multiplied by two) used for synthesis from 400 to 3200\nin a 2-fold ratio to study the influence of cap variation. The previsions YOLOv5x detectors using the changing data sets are shown in Table V. The results showed that the 400 row had competitive precision compared to the 1600 or 3200 rows. The number was enough to support a satisfying detector. The cap variations were thus considered to have a low influence on learning.\nSecond, we fix the number of cap region pictures to 3200 and change the number of images collected using the SR and SB methods, respectively, to study the influence of the combination ratio. Like the ablation study in Section V-B1, we divided the experiment here into two parts. In the first part, we set the number of images collected by the SR method to 800 and varied the number of images collected by the SB method from 200 to 1600 in a 2-fold ratio to understand the importance of the SB data. The upper section of Table VI shows the precision of detectors trained using the varied data. The number of SB images did not appear to be positively correlated with the final detector\u2019s precision, although the largest mAP was observed when the number of SB images was 800. In the second part, we fixed the number of images collected by the SB method to 800 and varied the number of images collected by the SR method to understand the importance of the SR data. The lower section of Table VI shows the precision of detectors trained using the varied data. The result indicated that the mAP improved as the SR image number increased to 800. There was no significant difference when the image number increased from 800 to 1600."
        },
        {
            "heading": "VI. CONCLUSIONS",
            "text": "In this paper, we proposed an integrated robot observation and data synthesis framework for data preparation. The proposed framework can significantly reduce the human effort in data preparation. It required only a single process and was a low-cost way to produce the combined data. The experimental result showed that combined observation and synthetic images led to comparable performance to manual data preparation. The ablation studies provided a good guide on optimizing data configurations and parameter settings for training detectors using the combined data."
        }
    ],
    "title": "Automatically Prepare Training Data for YOLO Using Robotic In-Hand Observation and Synthesis",
    "year": 2023
}