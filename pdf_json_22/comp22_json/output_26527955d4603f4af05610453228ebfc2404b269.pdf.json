{
    "abstractText": "Few-Shot Text Classification (FSTC) imitates humans to learn a new text classifier efficiently with only few examples, by leveraging prior knowledge from historical tasks. However, most prior works assume that all the tasks are sampled from a single data source, which cannot adapt to real-world scenarios where tasks are heterogeneous and lie in different distributions. As such, existing methods may suffer from their globally knowledge-shared mechanisms to handle the task heterogeneity. On the other hand, inherent task relation are not explicitly captured, making task knowledge unorganized and hard to transfer to new tasks. Thus, we explore a new FSTC setting where tasks can come from a diverse range of data sources. To address the task heterogeneity, we propose a self-supervised hierarchical task clustering (SS-HTC) method. SS-HTC not only customizes cluster-specific knowledge by dynamically organizing heterogeneous tasks into different clusters in hierarchical levels but also disentangles underlying relations between tasks to improve the interpretability. Extensive experiments on five public FSTC benchmark datasets demonstrate the effectiveness of SSHTC.",
    "authors": [
        {
            "affiliations": [],
            "name": "Juan Zha"
        },
        {
            "affiliations": [],
            "name": "Zheng Li"
        },
        {
            "affiliations": [],
            "name": "Ying Wei"
        },
        {
            "affiliations": [],
            "name": "Yu Zhang"
        }
    ],
    "id": "SP:e29fe0e42a7d5b4ef17c7bd408d99717ae2d87be",
    "references": [
        {
            "authors": [
                "Trapit Bansal",
                "Rishikesh Jha",
                "Andrew McCallum."
            ],
            "title": "Learning to few-shot learn across diverse natural language classification tasks",
            "venue": "COLING, pages 5108\u20135123.",
            "year": 2020
        },
        {
            "authors": [
                "Trapit Bansal",
                "Rishikesh Jha",
                "Tsendsuren Munkhdalai",
                "Andrew McCallum."
            ],
            "title": "Self-supervised meta-learning for few-shot natural language classification tasks",
            "venue": "EMNLP, pages 522\u2013534.",
            "year": 2020
        },
        {
            "authors": [
                "Yujia Bao",
                "Menghua Wu",
                "Shiyu Chang",
                "Regina Barzilay."
            ],
            "title": "Few-shot text classification with distributional signatures",
            "venue": "ICLR.",
            "year": 2020
        },
        {
            "authors": [
                "Wei-Yu Chen",
                "Yen-Cheng Liu",
                "Zsolt Kira",
                "Yu-Chiang Wang",
                "Jia-Bin Huang."
            ],
            "title": "A closer look at few-shot classification",
            "venue": "ICLR.",
            "year": 2019
        },
        {
            "authors": [
                "Shumin Deng",
                "Ningyu Zhang",
                "Jiaojian Kang",
                "Yichi Zhang",
                "Wei Zhang",
                "Huajun Chen."
            ],
            "title": "Metalearning with dynamic-memory-based prototypical network for few-shot event detection",
            "venue": "WSDM, pages 151\u2013159.",
            "year": 2020
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "NAACL-HLT, pages 4171\u20134186.",
            "year": 2019
        },
        {
            "authors": [
                "Zi-Yi Dou",
                "Keyi Yu",
                "Antonios Anastasopoulos."
            ],
            "title": "Investigating meta-learning algorithms for low-resource natural language understanding tasks",
            "venue": "EMNLP-IJCNLP, pages 1192\u20131197.",
            "year": 2019
        },
        {
            "authors": [
                "Chelsea Finn",
                "Pieter Abbeel",
                "Sergey Levine."
            ],
            "title": "Model-agnostic meta-learning for fast adaptation of deep networks",
            "venue": "ICML, pages 1126\u20131135.",
            "year": 2017
        },
        {
            "authors": [
                "Tianyu Gao",
                "Xu Han",
                "Zhiyuan Liu",
                "Maosong Sun."
            ],
            "title": "Hybrid attention-based prototypical networks for noisy few-shot relation classification",
            "venue": "AAAI, volume 33, pages 6407\u20136414.",
            "year": 2019
        },
        {
            "authors": [
                "Tianyu Gao",
                "Xu Han",
                "Hao Zhu",
                "Zhiyuan Liu",
                "Peng Li",
                "Maosong Sun",
                "Jie Zhou"
            ],
            "title": "2019b. FewRel 2.0: Towards more challenging few-shot relation classification",
            "venue": "In EMNLP-IJCNLP,",
            "year": 2019
        },
        {
            "authors": [
                "Ruiying Geng",
                "Binhua Li",
                "Yongbin Li",
                "Jian Sun",
                "Xiaodan Zhu."
            ],
            "title": "Dynamic memory induction networks for few-shot text classification",
            "venue": "ACL, pages 1087\u20131094.",
            "year": 2020
        },
        {
            "authors": [
                "Ruiying Geng",
                "Binhua Li",
                "Yongbin Li",
                "Xiaodan Zhu",
                "Ping Jian",
                "Jian Sun."
            ],
            "title": "Induction networks for few-shot text classification",
            "venue": "EMNLP-IJCNLP, pages 3904\u20133913.",
            "year": 2019
        },
        {
            "authors": [
                "Jiatao Gu",
                "Yong Wang",
                "Yun Chen",
                "Victor O.K. Li",
                "Kyunghyun Cho."
            ],
            "title": "Meta-learning for lowresource neural machine translation",
            "venue": "EMNLP, pages 3622\u20133631.",
            "year": 2018
        },
        {
            "authors": [
                "Kishaloy Halder",
                "Alan Akbik",
                "Josip Krapac",
                "Roland Vollgraf."
            ],
            "title": "Task-aware representation of sentences for generic text classification",
            "venue": "COLING, pages 3202\u20133213.",
            "year": 2020
        },
        {
            "authors": [
                "Xu Han",
                "Hao Zhu",
                "Pengfei Yu",
                "Ziyun Wang",
                "Yuan Yao",
                "Zhiyuan Liu",
                "Maosong Sun."
            ],
            "title": "FewRel: A large-scale supervised few-shot relation classification dataset with state-of-the-art evaluation",
            "venue": "EMNLP, pages 4803\u20134809.",
            "year": 2018
        },
        {
            "authors": [
                "Ruining He",
                "Julian McAuley."
            ],
            "title": "Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering",
            "venue": "WWW, pages 507\u2013517.",
            "year": 2016
        },
        {
            "authors": [
                "Junjie Hu",
                "Sebastian Ruder",
                "Aditya Siddhant",
                "Graham Neubig",
                "Orhan Firat",
                "Melvin Johnson."
            ],
            "title": "XTREME: A massively multilingual multitask benchmark for evaluating cross-lingual generalisation",
            "venue": "ICML, volume 119, pages 4411\u20134421.",
            "year": 2020
        },
        {
            "authors": [
                "Zijie Huang",
                "Zheng Li",
                "Haoming Jiang",
                "Tianyu Cao",
                "Hanqing Lu",
                "Bing Yin",
                "Karthik Subbian",
                "Yizhou Sun",
                "Wei Wang."
            ],
            "title": "Multilingual knowledge graph completion with self-supervised adaptive graph alignment",
            "venue": "ACL.",
            "year": 2022
        },
        {
            "authors": [
                "Seyoung Kim",
                "Eric P Xing."
            ],
            "title": "Tree-guided group lasso for multi-task regression with structured sparsity",
            "venue": "ICML.",
            "year": 2010
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980.",
            "year": 2014
        },
        {
            "authors": [
                "Ken Lang."
            ],
            "title": "Newsweeder: Learning to filter netnews",
            "venue": "Machine Learning Proceedings 1995, pages 331\u2013339. Elsevier.",
            "year": 1995
        },
        {
            "authors": [
                "David Lewis"
            ],
            "title": "Reuters-21578 text categorization test collection, distribution 1.0. http://www.research/.att.com",
            "year": 1997
        },
        {
            "authors": [
                "David D Lewis",
                "Yiming Yang",
                "Tony G Rose",
                "Fan Li."
            ],
            "title": "Rcv1: A new benchmark collection for text categorization research",
            "venue": "JMLR, 5(Apr):361\u2013 397.",
            "year": 2004
        },
        {
            "authors": [
                "Zheng Li",
                "Xin Li",
                "Ying Wei",
                "Lidong Bing",
                "Yu Zhang",
                "Qiang Yang."
            ],
            "title": "Transferable end-to-end aspect-based sentiment analysis with selective adversarial learning",
            "venue": "EMNLP, pages 4590\u20134600, Hong Kong, China.",
            "year": 2019
        },
        {
            "authors": [
                "Zheng Li",
                "Ying Wei",
                "Yu Zhang",
                "Qiang Yang."
            ],
            "title": "Hierarchical attention transfer network for crossdomain sentiment classification",
            "venue": "AAAI, pages 5852\u20135859.",
            "year": 2018
        },
        {
            "authors": [
                "Zheng Li",
                "Ying Wei",
                "Yu Zhang",
                "Xiang Zhang",
                "Xin Li."
            ],
            "title": "Exploiting coarse-to-fine task transfer for aspect-level sentiment classification",
            "venue": "AAAI, pages 4253\u20134260.",
            "year": 2019
        },
        {
            "authors": [
                "Zheng Li",
                "Danqing Zhang",
                "Tianyu Cao",
                "Ying Wei",
                "Yiwei Song",
                "Bing Yin."
            ],
            "title": "Metats: Meta teacher-student network for multilingual sequence labeling with minimal supervision",
            "venue": "EMNLP, pages 3183\u20133196.",
            "year": 2021
        },
        {
            "authors": [
                "Zheng Li",
                "Yu Zhang",
                "Ying Wei",
                "Yuxiang Wu",
                "Qiang Yang."
            ],
            "title": "End-to-end adversarial memory network for cross-domain sentiment classification",
            "venue": "IJCAI, pages 2237\u20132243.",
            "year": 2017
        },
        {
            "authors": [
                "Yanbin Liu",
                "Juho Lee",
                "Minseop Park",
                "Saehoon Kim",
                "Eunho Yang",
                "Sungju Hwang",
                "Yi Yang."
            ],
            "title": "Learning to propagate labels: Transductive propagation network for few-shot learning",
            "venue": "ICLR.",
            "year": 2019
        },
        {
            "authors": [
                "Yu Meng",
                "Yunyi Zhang",
                "Jiaxin Huang",
                "Chenyan Xiong",
                "Heng Ji",
                "Chao Zhang",
                "Jiawei Han."
            ],
            "title": "Text classification using label names only: A language model self-training approach",
            "venue": "EMNLP, pages 9006\u20139017.",
            "year": 2020
        },
        {
            "authors": [
                "Alex Nichol",
                "Joshua Achiam",
                "John Schulman."
            ],
            "title": "On first-order meta-learning algorithms",
            "venue": "arXiv preprint arXiv:1803.02999.",
            "year": 2018
        },
        {
            "authors": [
                "Abiola Obamuyide",
                "Andreas Vlachos."
            ],
            "title": "Model-agnostic meta-learning for relation classification with limited supervision",
            "venue": "ACL, pages 5873\u2013 5879.",
            "year": 2019
        },
        {
            "authors": [
                "Sora Ohashi",
                "Junya Takayama",
                "Tomoyuki Kajiwara",
                "Yuki Arase."
            ],
            "title": "Distinct label representations for few-shot text classification",
            "venue": "ACL-IJCNLP, pages 831\u2013836.",
            "year": 2021
        },
        {
            "authors": [
                "Boris Oreshkin",
                "Pau Rodr\u00edguez L\u00f3pez",
                "Alexandre Lacoste."
            ],
            "title": "Tadam: Task dependent adaptive metric for improved few-shot learning",
            "venue": "NeurIPS, pages 721\u2013731.",
            "year": 2018
        },
        {
            "authors": [
                "Sinno Jialin Pan",
                "Qiang Yang."
            ],
            "title": "A survey on transfer learning",
            "venue": "TKDE, 22(10):1345\u20131359.",
            "year": 2009
        },
        {
            "authors": [
                "Raul Puri",
                "Bryan Catanzaro."
            ],
            "title": "Zero-shot text classification with generative language models",
            "venue": "arXiv preprint arXiv:1912.10165.",
            "year": 2019
        },
        {
            "authors": [
                "Jake Snell",
                "Kevin Swersky",
                "Richard Zemel."
            ],
            "title": "Prototypical networks for few-shot learning",
            "venue": "NeurIPS, pages 4077\u20134087.",
            "year": 2017
        },
        {
            "authors": [
                "Shengli Sun",
                "Qingfeng Sun",
                "Kevin Zhou",
                "Tengchao Lv."
            ],
            "title": "Hierarchical attention prototypical networks for few-shot text classification",
            "venue": "EMNLPIJCNLP, pages 476\u2013485.",
            "year": 2019
        },
        {
            "authors": [
                "Flood Sung",
                "Yongxin Yang",
                "Li Zhang",
                "Tao Xiang",
                "Philip HS Torr",
                "Timothy M Hospedales."
            ],
            "title": "Learning to compare: Relation network for few-shot learning",
            "venue": "CVPR, pages 1199\u20131208.",
            "year": 2018
        },
        {
            "authors": [
                "Oriol Vinyals",
                "Charles Blundell",
                "Timothy Lillicrap",
                "Koray Kavukcuoglu",
                "Daan Wierstra."
            ],
            "title": "Matching networks for one shot learning",
            "venue": "NeurIPS, pages 3637\u20133645.",
            "year": 2016
        },
        {
            "authors": [
                "Risto Vuorio",
                "Shao-Hua Sun",
                "Hexiang Hu",
                "Joseph J Lim."
            ],
            "title": "Toward multimodal model-agnostic meta-learning",
            "venue": "arXiv preprint arXiv:1812.07172.",
            "year": 2018
        },
        {
            "authors": [
                "Jixuan Wang",
                "Kuan-Chieh Wang",
                "Frank Rudzicz",
                "Michael Brudno."
            ],
            "title": "Grad2task: Improved fewshot text classification using gradients for task representation",
            "venue": "NeurIPS.",
            "year": 2021
        },
        {
            "authors": [
                "Ruijie Wang",
                "Zheng Li",
                "Dachun Sun",
                "Shengzhong Liu",
                "Jinning Li",
                "Bing Yin",
                "Tarek Abdelzaher"
            ],
            "title": "Learning to sample and aggregate: Few-shot reasoning over temporal knowledge graphs",
            "year": 2022
        },
        {
            "authors": [
                "Thomas Wolf",
                "Lysandre Debut",
                "Victor Sanh",
                "Julien Chaumond",
                "Clement Delangue",
                "Anthony Moi",
                "Pierric Cistac",
                "Tim Rault",
                "R\u00e9mi Louf",
                "Morgan Funtowicz"
            ],
            "title": "Huggingface\u2019s transformers: State-of-the-art natural language processing",
            "year": 2019
        },
        {
            "authors": [
                "Jiawei Wu",
                "Wenhan Xiong",
                "William Yang Wang."
            ],
            "title": "Learning to learn and predict: A metalearning approach for multi-label classification",
            "venue": "EMNLP-IJCNLP, pages 4354\u20134364.",
            "year": 2019
        },
        {
            "authors": [
                "Wenpeng Yin",
                "Jamaal Hay",
                "Dan Roth."
            ],
            "title": "Benchmarking zero-shot text classification: Datasets, evaluation and entailment approach",
            "venue": "EMNLP, pages 3914\u20133923.",
            "year": 2019
        },
        {
            "authors": [
                "Dani Yogatama",
                "Cyprien de Masson d\u2019Autume",
                "Jerome Connor",
                "Tomas Kocisky",
                "Mike Chrzanowski",
                "Lingpeng Kong",
                "Angeliki Lazaridou",
                "Wang Ling",
                "Lei Yu",
                "Chris Dyer"
            ],
            "title": "Learning and evaluating general linguistic intelligence",
            "year": 2019
        },
        {
            "authors": [
                "Mo Yu",
                "Xiaoxiao Guo",
                "Jinfeng Yi",
                "Shiyu Chang",
                "Saloni Potdar",
                "Yu Cheng",
                "Gerald Tesauro",
                "Haoyu Wang",
                "Bowen Zhou."
            ],
            "title": "Diverse few-shot text classification with multiple metrics",
            "venue": "NAACL, pages 1206\u20131215.",
            "year": 2018
        },
        {
            "authors": [
                "Amir R Zamir",
                "Alexander Sax",
                "William Shen",
                "Leonidas J Guibas",
                "Jitendra Malik",
                "Silvio Savarese."
            ],
            "title": "Taskonomy: Disentangling task transfer learning",
            "venue": "CVPR, pages 3712\u20133722.",
            "year": 2018
        },
        {
            "authors": [
                "Hanlei Zhang",
                "Hua Xu",
                "Ting-En Lin."
            ],
            "title": "Deep open intent classification with adaptive decision boundary",
            "venue": "AAAI.",
            "year": 2021
        },
        {
            "authors": [
                "N. Zhang",
                "Z. Sun",
                "S. Deng",
                "J. Chen",
                "H. Chen"
            ],
            "title": "Improving few-shot text classification via pretrained language representations",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Recent advances in deep learning highly rely on massive human annotations. This reliance increases the burden of data collection and meanwhile hinders its potentials to the low-data regime, where the labeled data is scarce and difficult to obtain. Inspired by human beings\u2019 capabilities that can quickly learn with a few examples, Few-Shot Learning (FSL) (Vinyals et al., 2016; Finn et al., 2017), which aims to learn a classifier that generalizes well even with a few training instances per class, has recently attracted much attention.\n\u2217Most of the work was done when the first author was a research assistant at Southern University of Science and Technology; * Equal contribution; \u2020Corresponding author.\nSingle source Multiple sources\nSeen tasks\nUnseen tasks\nFew-shot learning\n\ud835\udc37!\"#$%& \ud835\udc37!\"#$% ' \ud835\udc37!\"#$%( \ud835\udc37!\"#$% )\ud835\udc37!\"#$%\n\ud835\udc37!*+!\nOur setting\n\ud835\udc37!*+!& \ud835\udc37!*+!' \ud835\udc37!*+!( \ud835\udc37!*+!)\nFigure 1: Comparison of existing FSTC formulation and our proposed practical problem setting.\nIn the NLP domain, Few-Shot Text Classification (FSTC) (Han et al., 2018) has been actively investigated in data-sparsity scenarios, e.g., relation classification (Han et al., 2018), event classification (Deng et al., 2020), and intent classification (Zhang et al., 2021), where new categories such as relations, events, or intent types tend to emerge frequently and lack sufficient annotations. Meta-learning (a.k.a. learning to learn) approaches (Finn et al., 2017), which transfer prior knowledge from previous tasks to improve the effectiveness in learning new tasks, have achieved superior performance for FSTC (Gao et al., 2019a; Sun et al., 2019; Bao et al., 2020). The prior knowledge can be instantiated as a transferable metric space for retrieving nearest prototypes in (Gao et al., 2019a; Sun et al., 2019), dynamic capsules in (Geng et al., 2019), and distributional signatures in (Bao et al., 2020), etc.\nDespite their early success, such approaches have two main drawbacks: (1) they assume that all previous tasks are sampled from a single data source or domain, leading to tasks with low intertask variance. As a consequence, these methods globally share the prior knowledge across all tasks but fail to handle real-world applications where the historical tasks that potentially contribute may come from diverse data sources in different distri-\nar X\niv :2\n21 1.\n08 58\n8v 1\n[ cs\n.C L\n] 1\n6 N\nov 2\n02 2\nbutions (a.k.a. task heterogeneity (Vuorio et al., 2018)). For example, the knowledge learned from categorizing different types of products or services may be hardly transferred to classify public comments or opinions among different topics, or to determine users\u2019 intent in dialogues with chatbot services, with only limited labeled data; (2) handling heterogeneous tasks for the better generalization ability exactly requires reliable knowledge organization, which highly relies on disentangling underlying task relations that are ignored by prior works. Motivated by those, we study a new FSTC setting where tasks come from a diverse range of data sources with possibly different data distributions as shown in Figure 1. To embrace the skills learned from multiple task sources to improve the generalization ability, we propose a novel metalearning framework named Self-Supervised Hierarchical Task Clustering (SS-HTC), which groups tasks into different clusters based on inherent task relations in multiple levels. When a new task arrives, it can quickly take advantage of the historical knowledge learned within the cluster it belongs to.\nSpecifically, learning a superior task embedding is the cornerstone to disentangle underlying relations among tasks and group them into different clusters. However, a FSL task is hard to represent, as labeled training data in each FSL task are insufficient. To tackle this issue, we propose a labeloriented masked language modeling to recover the corresponding label texts using each training sample itself from the task. Such a self-supervised manner, considering informative label text semantics, encourages the model to generate more discriminative task embedding to discover reasonable task relationships even with limited label information.\nAfter that, each task embedding is passed to a hierarchical task tree to dynamically perform soft task clustering in multiple levels, so that the knowledge is shared among highly related tasks in the same cluster but differentiated between different clusters of tasks. Then, the updated task embedding outputted by the task tree encodes the representation of the cluster it belongs to. This cluster representation is finally passed to modulate the prior knowledge, a metric space for finding nearest prototypes following (Snell et al., 2017), to be cluster-specific. In a nutshell, SS-HTC not only quickly accesses the most relevant cluster and tailors the prior knowledge to address the challenge of task heterogeneity, but also increases the model\ninterpretability by disentangling task correlations. Empirically, extensive experiments on five public FSTC benchmark datasets demonstrate that SSHTC significantly and consistently outperforms state-of-the-art FSTC methods by a large margin.\nOur contributions can be summarized as follows. (1) A more realistic FSTC setting that allows diverse tasks with different distributions is investigated; (2) A novel SS-HTC framework is proposed to both tackle task heterogeneity and improve the interpretability by hierarchical task clustering; (3) Extensive experiments verify the effectiveness of the proposed SS-HTC method."
        },
        {
            "heading": "2 Preliminaries",
            "text": "Few-shot learning (FSL) Considering a task T = {S,Q} that contains the training set S and the testing setQ, the objective of FSL is to learn a modelG for this task given only a few labeled samples in S . Typically, FSL is characterized as a N -way K-shot problem with S containingK labeled examples per class for N classes, i.e., S={(xji , yi)} N,K i,j=1, where xji is the j-th sample for the i-th class yi. Then xQ denotes an unlabeled sample ofQ belonging to one of the N classes and y\u0302Q denotes the estimated label of a model, i.e., G(S,xQ)\u2192 y\u0302Q. In many existing works on FSL, S and Q are also known as the support set and query set, respectively.\nTraditional deep learning models would severely overfit on FSL tasks since only a few labeled samples cannot accurately represent the true data distribution, which will result in learning classifiers with high variance and generalizing poorly to new data. In order to solve the overfitting problem in FSL, Vinyals et al. (2016) proposed an effective episodic meta-learning strategy that learns a generic classifier from diverse few-shot classification tasks and then employs the classifier to a new task. The purpose of episodic training is to mimic the real testing environment where tasks contain insufficient support sets and unlabeled query sets. The consistency between training and testing environments alleviates the shift gap and boosts the generalization. Specifically, using the episodic strategy, the whole process of meta-learning can be divided into three parts: meta-training with training tasks {T ktrain} Ntrain k=1 = {S\nk,Qk}Ntraink=1 , meta-validation with validation tasks {T kval} Nval k=1 = {S\nk,Qk}Nvalk=1, and meta-testing with testing tasks {T ktest} Ntest k=1 = {Sk,Qk}Ntestk=1. Note that for meta-training and meta-validation tasks, the label for the query set\nis available to train the model G and to select best hyper-parameters, respectively. In this way, metalearning algorithms are capable of adapting to new tasks effectively even with a shortage of training data for each new task."
        },
        {
            "heading": "2.1 Problem Formulation",
            "text": "Multi-source FSTC Prior works assume that all the tasks are sampled from a single dataset D, making the tasks lying in the same distribution. In this way, we usually split D into three parts: Dtrain, Ddev, and Dtest in terms of class splits. Each part has a specific label space and disjoints with other parts. For each training episode, we first sample a label setC withN classes fromDtrain, and then use C to sample a task T ktrain containing the support set S and the query set Q. Finally, we feed S and Q to the model and minimize the loss.\nThis assumption restricts the task diversity and degrades the model\u2019s out-of-distribution generalization. To resolve it, we assume that the tasks can be sampled from M diverse datasets {D1, . . . , DM} with possibly different distributions. For each dataset Dm, we use the same strategy to split Dm into the training, validation, and testing parts. And we sample the meta-training, meta-validation, and meta-testing tasks based on the corresponding parts of all the datasets. That is, we sample {Ttrain} from {D1train\u222aD2train\u222a...DMtrain}, while each task is sampled to consist of only classes from a single dataset."
        },
        {
            "heading": "3 Method",
            "text": "SS-HTC aims to handle the task heterogeneity by automatically organizing tasks into a hierarchical task structure that explicitly tailors the transferable knowledge to different task clusters.The overall framework of SS-HTC is illustrated in Figure 2.\nSS-HTC mainly consists of three components:\n\u2022 Prototypical network (ProtoNet) (Snell et al., 2017) is an advanced metric-based model, which learns to predict by comparing the distance between the labeled support and unlabeled query sets. We choose it as the building block (Base Model) since it is computationally efficient and simple. More importantly, our framework is general and can be easily compatible with any other metric-based models, e.g., Matching Network (Vinyals et al., 2016) and Signature (Bao et al., 2020).\n\u2022 Label-Oriented Mask Language Modeling\n(LOMLM) is a self-supervised learning objective to automatically learn the task embedding of each few-shot task T by considering informative label text semantics. LOMLM encourages the model to generate discriminative task embeddings, which are the prerequisite to identify underlying task relationships for knowledge organization and reuse.\n\u2022 Hierarchical Task Clustering (HTC) can automatically group task knowledge into a hierarchical clustering tree, by softly assigning highly correlated tasks into the same cluster, while keeping irrelevant tasks apart. When a new task arrives, it can leverage the historical knowledge within the clusters it belongs to customize a cluster-specific metric for the prototypical network."
        },
        {
            "heading": "3.1 Prototypical Network",
            "text": "The prototypical network (Snell et al., 2017) is a simple yet effective metric-based method that learns to predict the label of a query sample xQ by comparing its distance with each class prototype vector. Specifically, given a N -way K-shot task T defined in Section 2, we use a prototype vector pi as the representative feature of each class yi, where pi is the average of all the embedded support samples {xji}Kj=1 that belong to class yi, i.e., pi= 1 K \u2211K j=1 f\u03b8(x j i ), where f\u03b8(x) denotes the embedding of a sample. Here, we use the pre-trained language model BERT (Devlin et al., 2019) as the powerful encoder f\u03b8. Then the probability distribution over the N classes for the query sample xQ can be calculated via a softmax function over distances between all the prototype vectors and the embedding for xQ as\ny\u0302Q= exp(\u2212d(f\u03b8(xQ),pi))\u2211N i\u2032=1 exp(\u2212d(f\u03b8(xQ),pi\u2032)) , (1)\nwhere d(\u00b7, \u00b7) denotes the Euclidean distance. The training objective is to minimize the N -way crossentropy loss ` for all the query samples in the query set Q for each meta-training task as\nLcls= \u2211 Q `(yQ, y\u0302Q).\nHowever, the prototypical network relies on a globally shared metric (d, f\u03b8), which may lack the ability to handle heterogeneous tasks lying in different distributions. Thus, the proposed SS-HTC method uses it as the base model and aims to improve it with the cluster-specific metric to tackle the task heterogeneity problem."
        },
        {
            "heading": "3.2 Label-Oriented Mask Language Modeling",
            "text": "Learning a superior task embedding is the prerequisite to capture underlying correlations between tasks. On the one hand, prior works rely on learning intermediate hidden representations and then aggregate hidden representations of all the training samples as the task embedding (Zamir et al., 2018). This may be infeasible in the few-shot regime since the labeled training data S (i.e., N \u00d7K examples) of each few-shot task T are insufficient. On the other hand, existing methods for FSTC (Gao et al., 2019a; Geng et al., 2019; Sun et al., 2019; Bao et al., 2020) only treat each task as a simple N -way classification by mapping informative class label names of each task into indices {0, 1, ..., N\u22121}. As such, the model can only focus on discriminating among classes instead of realizing what categories to be classified. Thus, each task is actually underrepresented due to ignoring label semantics.\nInspired by those, we propose a Label-Oriented Mask Language Modeling (LOMLM) that exploits underused label semantics to enhance the task representation learning. The LOMLM uses the same denoising auto-encoding (Devlin et al., 2019) from BERT as the self-supervised learning objective. Specifically, we augment each support text sample xji with the label name tokens x\u0303i of its corresponding class yi (e.g., x\u0303i is \u201cmusical instruments\u201d for the class yi=0). We denote the augmented support\nsample as [xji ; x\u0303i]. Then we mask each token in the label name with a special symbol [MASK], and use the remaining tokens to recover them.1\nLet the masked tokens {[MASK]t} T=|x\u0303i| t=1 be ui. Then, the training objective of LOMLM is to reconstruct [xji ; x\u0303i] from [x j i ;ui] over all support samples by minimizing Llomlm, which is formulated as\nLlomlm=\u2212 N\u2211 i=1 K\u2211 j=1 logP ([xji ; x\u0303i]|[x j i ;ui]).\nUnder the LOMLM supervision, we simply use an average pooling to aggregate the embeddings of all augmented support samples [xji ; x\u0303i] as the representation of the task T by\ngin = Pool N,K i,j=1(f\u03b8([x j i ; x\u0303i])). (2)"
        },
        {
            "heading": "3.3 Hierarchical Task Clustering",
            "text": "To cluster tasks into different groups, where the knowledge from similar historical tasks can be accumulated together and transferred to newly related tasks, we propose a hierarchical task clustering (HTC) to dynamically locate which cluster the task belongs to. The hierarchical structure adopts the top-down hierarchy design to imitate the product taxonomy from coarse to fine granularity (e.g., the \u201celectronics\u201d category has more specific subcategories such as \u201claptop\u201d, \u201cphone\u201d, and \u201cTV\u201d). Given complex dependencies among tasks, hierarchical levels of task clusters are more sufficient to\n1We explain no information leakage in Appendix A.1.\ncapture real-world task relations than the flat clustering (Kim and Xing, 2010). This allows the task organization and reuse in a coarse-to-fine manner, which can better disentangle inherent task relations such that transferable knowledge among tasks can be maximally leveraged.\nIn the hierarchical cluster tree, each task T is soft-assigned into the clusters in each level to encourage less information loss compared with the hard assignment and allow SS-HTC to be trained in an end-to-end manner. Specifically, the assignment score for the next level is a function of the task embedding at the current level. For example, we assign the task embedding g(l)o in the o-th cluster of the (l)-th level to the o\u2032-th cluster of the (l+1)-th level with the probability po \u2032(l+1)\no(l) , which is com-\nputed by applying the softmax function over Euclidean distances between g(l)o and all the (l+1)-th level cluster centers {c(l+1)o\u2032 } O(l+1) o\u2032=1\npo \u2032(l+1)\no(l) =\nexp(\u2212||g(l)o \u2212 c(l+1)o\u2032 || 2 2/2\u03c3 2)\u2211O(l+1) o\u2032=1 exp(\u2212||g (l) o \u2212 c(l+1)o\u2032 ||22/2\u03c32) ,\nwhere \u03c32 is a scaling factor to control the distance between tasks and clusters and O(l+1) denotes the number of clusters in the (l+1)-th level. Then, the task embedding g(l+1)o\u2032 of the o\n\u2032-th cluster in the (l+1)-th level can be calculated by the weighted sum of all the task embeddings in the previous l-th level as\ng (l+1) o\u2032 = O(l)\u2211 o=1 po \u2032(l+1) o(l) tanh(W(l+1)o\u2032 g (l) o + b (l+1) o\u2032 ),\nwhere W(l+1)o\u2032 and b (l+1) o\u2032 are learnable parameters. The full pipeline of HTC starts from l=0 and O(l) = 1, where the initialization for g(0)1 is the input task embedding gin defined in Eq. (2), and ends at O(L) = 1. The output embedding gout = g (L) 1 from the tree encrypts the cluster-specific historical knowledge that can be transferred to the input task. Note that we provide more details for the working mechanism of HTC in our Appendix A.2. Cluster-specific feature transformation After obtaining the cluster-specific knowledge gout from the tree that is highly correlative and transferable to the task, we concatenate the input and output task embeddings for the tree as the final task embedding, i.e., gT = gin \u2295 gout. The task embedding gT is used to learn the cluster-specific feature transformation v(\u00b7|\u03b3,\u03b2) for the augmented support samples and query samples, which consists of two factors\n\u03b3 and \u03b2 both derived from gT as\n\u03b3 = \u03c1(W\u03b3gT + b\u03b3), \u03b2 = \u03c1(W\u03b2gT + b\u03b2),\nwhere \u03c1 denote the ReLU function. \u03b3 and \u03b2 are learnable scaling and shift parameters of the feature-wise transformation, which can dynamically adjust feature representations to be more discriminative based on the cluster-specific task embeddings such that it can well adapt to diverse task distributions. Recall that f\u03b8(x) is the BERT representation of a sample x, where x can be an augmented support sample [xji ; x\u0303i] or a query sample xQ. For simplicity, let h = f\u03b8(x), then the two factors will make a residual affine transformation v(\u00b7|\u03b3,\u03b2) on h as\nv(h|\u03b3,\u03b2) = \u03c1((1 + \u03b3) h + \u03b2) + h,\nwhere is the element-wise multiplication. With the aid of the proposed SS-HTC, we will use the cluster-specific transformed embeddings v(f\u03b8(x)|\u03b3,\u03b2) instead of the BERT feature embedding f\u03b8(x) for the inference of the prototypical network as defined in Eq. (1)."
        },
        {
            "heading": "3.4 Joint Training",
            "text": "We combine each component loss into an overall object function as\nL = Lcls + \u03bbLlomlm, where \u03bb is a hyper-parameter to balance the classification loss and the LOMLM loss. The goal of joint learning is to learn superior task embeddings to guide the cluster-specific discriminative learning for the ultimate few-shot text classification."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Setup",
            "text": "Datasets We evaluate SS-HTC on five FSTC benchmark datasets: Amazon Product Review (He and McAuley, 2016), 20 Newsgroups (Lang, 1995), HuffPost (Misra, 2018), Reuters (Lewis, 1997), and RCV1 (Lewis et al., 2004). Following (Bao et al., 2020), we use the same class splits to divide each dataset into meta-training, meta-validation and meta-testing parts, from which N -way K-shot tasks are randomly sampled. Setting In experiments, tasks can be sampled from multiple diverse datasets. Thus, all models are trained and evaluated on the combination of the five aforementioned benchmark datasets instead of each\ndataset separately. Following prior works on FSTC, the classification accuracy is used as the evaluation metric as each task is under the few-shot learning setting and has no data imbalance issue. Moreover, we use the average accuracy on randomly sampled 1, 000 meta-testing tasks for each dataset as the final results to avoid the problem of randomness. All the experiments repeat 3 times and average results over 3 runs are reported. Baselines For a fair comparison, we use the BERT as the base encoder for all baselines. \u2022 Supervised learning. BERT (FT) (Chen et al., 2019) trains a BERT with a generic N -way classifier on all meta-training tasks and finetunes it on the support set and evaluate it on the query set of each meta-testing task independently. \u2022 Gradient-based meta-learning methods aim to learn a well-generalized model initialization that can be adapted to new tasks within a few optimization steps. (i) Reptile (Nichol et al., 2018) is a fast first-order gradient approximation of MAML which could be hardly optimized based on BERT (Finn et al., 2017). (ii) PMAML (Zhang et al., 2019) employs the masked language model pretraining before using the first-order MAML. \u2022 Metric-based meta-learning methods are to learn an invariant metric space where classes can be differentiated between each other. (i) MatchNet (Vinyals et al., 2016) uses an attention-based scheme where the cosine distance is used as the metric. (ii) ProtoNet (Snell et al., 2017) learns a metric space by minimizing the Euclidean distance between class prototype and query samples. (iii) InductionNet (Geng et al., 2019) encapsulates different classes by a dynamic routing induction method. (iv) HybridAPN (Gao et al., 2019a) is a hybrid attention prototypical network that exploits a hybrid attention mechanism. (v) HierAPN (Sun et al., 2019) is a hierarchical attention prototypical network that designs a hierarchical attention mechanism. (vi) Signature (Bao et al., 2020) utilizes the distributional statistics to implement the attention transfer between tasks. (vii) DEM (Ohashi et al., 2021) introduces a difference extractor to derive distinctive label representations with multitask learning based on ProtoNet."
        },
        {
            "heading": "4.2 Implementation details",
            "text": "Environment Our proposed SS-HTC model and baseline methods are implemented in TensorFlow 2.4.0 with CUDA 10.1, using Python 3.7.0 from\nAnaconda 4.9.2. All the models are trained/tested on a single TESLA V100-PCIE 32GB GPU with Linux system. Encoder We use the BERT-base model: bertbase-uncased (Wolf et al., 2019) model as the encoder, which has 12 layers, 768-dimensional hidden representations, 12 heads, and 110M parameters in total. We use the pooled representation (i.e., averaged token embeddings) as the sentence embedding since we have found that [CLS] embedding performs very poorly, even worse than CNN encoders under the few-shot setting. The BERT is jointly optimized with other parameters during the training stage. Initialization & Training For all the experiments, SS-HTC is optimized by the Adam algorithm (Kingma and Ba, 2014) for training. The maximal sentence length is 450. The weight matrices are initialized with a uniform distribution U(\u22120.01, 0.01). Gradients with the l2 norm larger than 40 are normalized to be 40. To alleviate overfitting, we perform early stopping on the metavalidation tasks. Hyperparameter The hyper-parameters are manually tuned on the average accuracy of the 10% randomly held-out meta-training sets. The initial learning rate is 10\u22125, which is tuned amongst {10\u22126, 5\u00d7 10\u22126, 1\u00d710\u22125, 5\u00d7 10\u22125}. The weight \u03bb for Llomlm is 0.1, which is tuned amongst {0.01, 0.03, 0.1, 0.3}. The scaling factor \u03c32 is 2.0. Due to the limited GPU memory, we only feed one task to SS-HTC for each step."
        },
        {
            "heading": "4.3 Main Results",
            "text": "K-Shot Evaluation. We present in Table 1 experimental results in terms of different shots under the setting of five ways/classes. Based on the results, we can observe: SS-HTC: SS-HTC significantly and consistently outperforms all the baseline methods on five datasets by a large margin (i.e., 1-shot:+9.81%, 5-shot:+5.19% average accuracy) over the best baselines (i.e., Signature and DEM). \u2022 Supervised method: Even with powerful pretrained language models (PLMs) like BERT, the supervised method BERT (FT) still performs very poorly in the few-shot regime. This circumstance has also been shown in prior studies (Yogatama et al., 2019), which shows that PLMs highly rely on sufficient fine-tuning data for downstream tasks. \u2022 Gradient-based methods: As gradient-based methods, Reptile and PMAML show inferior per-\nformance to metric-based baselines in FSTC. This phenomenon has also been observed in recent FSTC works (Gao et al., 2019a; Bao et al., 2020). The gradient-based methods mainly focus on lownoise vision tasks, which makes them hard to directly deal with diverse and noisy text data in FSTC tasks, especially for our setting that tasks are coming from multiple resources with large diversity. \u2022 Metric-based methods: (i) Compared with gradient-based methods, most metric-based baselines can generally obtain better results for FSTC. (ii) Recent proposed text-specific metric-based methods like HybridAPN and HierAPN have better performance than their base model - ProtoNet when tasks are all sampled from a single dataset (Gao et al., 2019a; Sun et al., 2019). However, when tasks are heterogeneous from diverse datasets in our setting, they do not outperform ProtoNet. This indicates that their sophisticated metric designs may not be able to handle the task heterogeneity due to the global knowledge-sharing strategies used. (iii) SS-HTC can outperform those metric-based baselines. This is because that SS-HTC can customize the transferable knowledge to be clusterspecific and preserve knowledge generalization among highly related tasks by taking advantage of the dynamic task clustering. N -way Evaluation. We present in Figure 3 the results in terms of different ways with a fixed number of shots. We report the average accuracy across all the five datasets with N=2, 3, ..., 7. Generally, as the number of ways increases, the performance degrades as the FSTC tasks become more difficult. We can observe that SS-HTC performs better than other baselines and that the gap among them becomes larger as the number of ways increases. This indicates the proposed SS-HTC method is less\nsensitive to the difficulty of the FSTC task by leveraging the knowledge from the most similar tasks based on hierarchical task clustering."
        },
        {
            "heading": "4.4 Ablation Study",
            "text": "To verify the efficacy of each component, we progressively incorporate the hierarchical task clustering (HTC) and label-oriented masked language modeling (LOMLM) into the base model (i.e., ProtoNet). We present the ablation results in Table 2. \u2022 w/ HTC v.s. w/o HTC: For ProtoNet+HTC, we use the proposed HTC method to dynamically organize tasks into hierarchical clusters, where the knowledge from similar tasks can be accumulated together. As such, each new incoming task can leverage the transferable knowledge within the cluster it belongs to and customize the cluster-specific metric for few-shot learning. We observe HTC can bring a significant gain (i.e., 2.45%) over ProtoNet in terms of the average accuracy. This shows the effectiveness of HTC to handle heterogeneous tasks lying in different distributions. \u2022 w/ LOMLM v.s. w/o LOMLM: For ProtoNet+HTC, we simply average embeddings of all support samples and their corresponding label texts as the embedding of a task without any supervision. Thus, this task embedding could be underrepresented. By incorporating the LOMLM, the task embedding is enhanced to be more label-aware to discriminate among classes. According to Table 2, we observe that adding LOMLM can achieve an additional 6.88% gain in terms of the average accuracy, which is a very large improvement over ProtoNet+HTC. This implies that a superior task embedding is critical to better disentangling task relations and customize the cluster-specific metric."
        },
        {
            "heading": "4.5 The Effect of Tree Structure",
            "text": "We further study the effect of tree structure to the performance. We vary the tree structure and record the results in Table 3. From the results, we can observe that the proposed hierarchical clustering shows the superiority over the flat task clustering. For the hierarchical clustering, we can see that too few clusters may be insufficient to learn the task clustering characteristic (e.g., the case (2,2,1)). When we increase the number of clusters, SS-HTC can achieve better results (e.g., case (5,3,1)) until reaching a stable status (e.g., case (5,5,1)). This indicates that more clusters introduce more parameters and may result in the overfitting problem."
        },
        {
            "heading": "4.6 Visualization of Hierarchical Task Tree",
            "text": "To demonstrate that the proposed SS-HTC method can automatically disentangle the underlying task relationship, we visualize the SS-HTC with clus-\nter structure (5, 3, 1) for tasks from each dataset. Specifically, we first select 1,000 5-way 1-shot tasks randomly from each dataset and show their averaged soft-assignments of clusters (C1, C2, C3, C4, C5) in the first layer. As illustrated in the left subfigure in Figure 4 where a darker color means a higher probability, we can see that different datasets mainly activate different clusters: Reuters\u2192C2, Amazon\u2192C3, RCV1\u2192C4, and 20News\u2192C1. Particularly, Huffpost activates both C2 and C4, which indicates that the Huffpost and Reuters datasets may have a large overlap. By checking the classes sets of both datasets, we have found that several classes in the two datasets are highly-related (e.g., Huffpost: \u201ctaste\u201d and \u201cword news\u201d, Reuters: \u201csugar\u201d and \u201cwholesale price Iindex\u201d).\nBesides, we also explore the activated task clusters (A, B, C) in the second layer which further accumulates the transferable knowledge among tasks from different datasets. We observe tasks from different datasets that have similar classes are highly aggregated into the same cluster. Meanwhile, tasks from the same dataset that contains different classes can activate different clusters. For example, in the #1 case of Figure 4 Right, a 5- way task from 20News with the class set {\u201calt atheism\u201d, \u201csoc religion christian\u201d, \u201ctalk politics guns\u201d, \u201ctalk politics misc\u201d, \u201ctalk religion misc\u201d} and a 5-way task from RCV1 with the class set \"{\u201creligion\u201d, \u201cequity markets\u201d, \u201cdomestic politics\u201d, \u201cinterbank markets\u201d, \u201cmoney markets\u201d}\" both activate the first cluster A, since the two tasks are all related to religion and politics. Similarly, in the #2 case, a 5-way task from 20News with the class set {\u201cbooks\u201d, \u201cclothing shoes jewelry\u201d, \u201celectronics\u201d, \u201cmusical instruments\u201d, \u201ctools home improvement\u201d} and a 5-way task from Huffpost with the\nclass set {\u201carts culture\u201d, \u201cgood news\u201d, \u201cenvironment\u201d, \u201ctech\u201d, \u201cstyle\u201d} both activate the second cluster B because they are all about culture and life-related stuffs. In the #3 case, a 5-way task from Amazon with the class set {\u201cBooks\u201d, \u201ckindle store\u201d, \u201cmovies tv\u201d, \u201coffice products\u201d, \u201ctools home improvement\u201d\"} and a 5-way task from RCV1 with the class set {\u201ceconomics\u201d, \u201cgovernment finance\u201d, \u201cmanagement\u201d, \u201cperformance\u201d, \u201cshare listings\u201d} are both assigned to the third cluster C as they both concern the economy and education. Those qualitative results indicate that the proposed SS-HTC model can capture the latent relations between diverse tasks to improve the model interpretability."
        },
        {
            "heading": "5 Related Work",
            "text": "Meta-Learning Inspired by human beings\u2019 ability to transfer knowledge from previous experiences (Pan and Yang, 2009; Li et al., 2017, 2018, 2019b,a), meta-learning (Vinyals et al., 2016; Finn et al., 2017) has become the mainstream paradigm to resolve few-shot learning problems. Prior studies mainly focus on low-noise vision tasks (Snell et al., 2017; Sung et al., 2018; Nichol et al., 2018; Oreshkin et al., 2018; Liu et al., 2019). Recently, those techniques have been initiated to lowresource NLP problems such as few-shot text classification (Yu et al., 2018; Wu et al., 2019; Geng et al., 2019; Sun et al., 2019; Geng et al., 2020; Bao et al., 2020; Wang et al., 2021; Ohashi et al., 2021), relation classification (Han et al., 2018; Gao et al., 2019a; Obamuyide and Vlachos, 2019; Gao et al., 2019b), machine translation (Gu et al., 2018), knowledge graph completion (Huang et al., 2022; Wang et al., 2022), and natural language understanding (Dou et al., 2019; Bansal et al., 2020a,b; Li et al., 2021) with minimal supervision. Different from them that globally share the prior knowledge\nacross homogeneous tasks within a single source, SS-HTC can embrace the skills learned from multiple heterogeneous sources to improve the outof-distribution robustness More importantly, they neglect underlying task relations in the low-data regime, which is imperative to automatically organize knowledge from heterogeneous tasks. Label-aware Modeling To alleviate data scarcity, label-aware methods are recently investigated (Yin et al., 2019; Puri and Catanzaro, 2019; Meng et al., 2020; Halder et al., 2020) to incorporate label semantics into text representation learning. Yin et al. (Yin et al., 2019) incorporate label texts into text samples and convert the text classification into a text entailment task. Yu et al. (Meng et al., 2020) propose the category vocabulary, which can be good label supplements for our LOMLM to enrich the label semantics in the future work. However, those methods are less investigated in task adaptation for FSTC. More importantly, our ultimate goal aims to leverage label information to enhance few-shot task representations for discovering and disentangling inherent and complicated task correlations. This can facilitate the knowledge organization and handle heterogeneous new tasks as well as improving the model interpretability."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this paper, we propose the self-supervised hierarchical task clustering (SS-HTC) method to tackle the task heterogeneity for FSTC by dynamically organizing the tasks into hierarchical clusters and customize the cluster-specific knowledge. Extensive experiments on various FSTC benchmark datasets quantitatively and qualitatively demonstrate the effectiveness of SS-HTC. In the future, the proposed SS-HTC can be potentially generalized to the multilingual few-shot setting (Hu et al., 2020)."
        },
        {
            "heading": "7 Limitations",
            "text": "Although we introduce a more realistic and practical problem setting for few-shot learning and verify the proposed SS-HTC method on extensive experiments, there are still some future directions that need further investigation and exploration. Firstly, our proposed setting is supposed to generalize to more heterogeneous NLP tasks under the few-shot regime instead of restricting to text classification. Secondly, how to dynamically adapt the task organization structures like humans in terms of input tasks is still underexploited. We view our works as the start point and will further explore those interesting problems in the future work."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work is supported by NSFC general grant under grant no. 62076118 and Shenzhen fundamental research program JCYJ20210324105000003."
        },
        {
            "heading": "A Clarification",
            "text": "In this section, we give more clarification regarding the details of problem setting, framework as well as hierarchical task clustering.\nThe proposed SS-HTC framework exactly comes from the collective power of self-supervised LOMLM and HTC. By this means, SS-HTC balances between globally shared meta-knowledge and cluster-specific meta-knowledge, where the transferable knowledge can be adapted to different clusters of tasks, while it is still shared among highly correlated tasks within the same cluster.\nA.1 No Information leakage claim There is no information leakage for the proposed Label-Oriented Mask Language Modeling (LOMLM) module. We only utilize masked label tokens for support samples (training set) instead of query samples (testing set) in each task. For metalearning, the final performance evaluation is based on query samples of each meta-testing task that has disjoint classes with all meta-training tasks.\nA.2 Hierarchical Task Clustering The characteristics of HTC can be summarized as two aspects: (1) the hierarchical task clusters {c(l)o }O (l)\no=1 in each (l)-th level of the tree are learnable and randomly initialized, which are shared by all tasks. We only need to feed each task embedding into the tree, automatically obtain the soft assignment to each cluster, and output the clusterspecific historical knowledge used for the prototypical network. The structure of the hierarchical tree is predefined since we found that jointly learning with additional structures can bring more challenges into the optimization. Despite that, the cluster representations and their connection weights are\njointly learned with other parameters in an online manner to model complex task relationships; (2) hierarchical clustering tree is optimized on the task level instead of the class level, which can capture more enriched task-specific information beyond the class itself. We found this information is particularly useful to handle the diversity of few-shot tasks, as our practical setting allows tasks to be sampled from a diverse range of data sources with possibly different data distributions."
        },
        {
            "heading": "B Baselines",
            "text": "We provide the available open source code for the baseline methods we compare with, including:\n\u2022 Supervised learning.\n\u2013 BERT (FT) (Chen et al., 2019)2\n\u2022 Gradient-based meta-learning\n\u2013 Reptile (Nichol et al., 2018)3 \u2013 PMAML (Zhang et al., 2019)4\n\u2022 Metric-based meta-learning\n\u2013 MatchNet (Vinyals et al., 2016)5 \u2013 ProtoNet (Snell et al., 2017)6 \u2013 InductionNet (Geng et al., 2019)7 \u2013 HybridAPN (Gao et al., 2019a)8 \u2013 Signature (Bao et al., 2020)9 \u2013 DEM (Ohashi et al., 2021)10\nFor HierAPN (Sun et al., 2019), we reimplement it according to the original paper since the source code is not publicly available.\n2https://github.com/wyharveychen/ CloserLookFewShot\n3https://github.com/openai/supervised-reptile 4https://github.com/zxlzr/FewShotNLP 5https://github.com/gitabcworld/\nMatchingNetworks 6https://github.com/jakesnell/ prototypical-networks 7https://github.com/YujiaBao/ Distributional-Signatures 8https://github.com/thunlp/HATT-Proto 9https://github.com/YujiaBao/ Distributional-Signatures 10https://github.com/21335732529sky/difference_ extractor"
        }
    ],
    "title": "Disentangling Task Relations for Few-shot Text Classification via Self-Supervised Hierarchical Task Clustering",
    "year": 2022
}