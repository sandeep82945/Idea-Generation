{
    "abstractText": "Data-Free Knowledge Distillation (DFKD) is a novel task that aims to train high-performance student models using only the teacher network without original training data. Despite encouraging results, existing DFKD methods rely heavily on generation modules with high computational costs. Meanwhile, they ignore the fact that the generated and original data exist domain shifts due to the lack of supervision information. Moreover, knowledge is transferred through each example, ignoring the implicit relationship among multiple examples. To this end, we propose a novel Open-world Data Sampling Distillation (ODSD) method without a redundant generation process. First, we try to sample open-world data close to the original data\u2019s distribution by an adaptive sampling module. Then, we introduce a low-noise representation to alleviate the domain shifts and build a structured relationship of multiple data examples to exploit data knowledge. Extensive experiments on CIFAR-10, CIFAR-100, NYUv2, and ImageNet show that our ODSD method achieves state-of-the-art performance. Especially, we improve 1.50%-9.59% accuracy on the ImageNet dataset compared with the existing results.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yuzheng Wang"
        },
        {
            "affiliations": [],
            "name": "Zhaoyu Chen"
        },
        {
            "affiliations": [],
            "name": "Jie Zhang"
        },
        {
            "affiliations": [],
            "name": "Dingkang Yang"
        },
        {
            "affiliations": [],
            "name": "Zuhao Ge"
        },
        {
            "affiliations": [],
            "name": "Yang Liu"
        },
        {
            "affiliations": [],
            "name": "Siao Liu"
        },
        {
            "affiliations": [],
            "name": "Yunquan Sun"
        },
        {
            "affiliations": [],
            "name": "Wenqiang Zhang"
        },
        {
            "affiliations": [],
            "name": "Lizhe Qi"
        }
    ],
    "id": "SP:791fc9f4ee38fa97f5c40243203d74033476bd15",
    "references": [
        {
            "authors": [
                "G\u00f6rkem Algan",
                "Ilkay Ulusoy"
            ],
            "title": "Image classification with deep learning in the presence of noisy labels: A survey",
            "venue": "Knowledge-Based Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Mathilde Caron",
                "Hugo Touvron",
                "Ishan Misra",
                "Herv\u00e9 J\u00e9gou",
                "Julien Mairal",
                "Piotr Bojanowski",
                "Armand Joulin"
            ],
            "title": "Emerging properties in self-supervised vision transformers",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Hanting Chen",
                "Tianyu Guo",
                "Chang Xu",
                "Wenshuo Li",
                "Chunjing Xu",
                "Chao Xu",
                "Yunhe Wang"
            ],
            "title": "Learning student networks in the wild",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Hanting Chen",
                "Yunhe Wang",
                "Chang Xu",
                "Zhaohui Yang",
                "Chuanjian Liu",
                "Boxin Shi",
                "Chunjing Xu",
                "Chao Xu",
                "Qi Tian"
            ],
            "title": "Data-free learning of student networks",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Ting Chen",
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Geoffrey Hinton"
            ],
            "title": "A simple framework for contrastive learning of visual representations",
            "venue": "In International conference on machine learning,",
            "year": 2020
        },
        {
            "authors": [
                "Zhaoyu Chen",
                "Bo Li",
                "Shuang Wu",
                "Shouhong Ding",
                "Wenqiang Zhang"
            ],
            "title": "Query-efficient decision-based black-box patch attack",
            "venue": "arXiv preprint arXiv:2307.00477,",
            "year": 2023
        },
        {
            "authors": [
                "Zhaoyu Chen",
                "Bo Li",
                "Shuang Wu",
                "Kaixun Jiang",
                "Shouhong Ding",
                "Wenqiang Zhang"
            ],
            "title": "Content-based unrestricted adversarial attack",
            "venue": "arXiv preprint arXiv:2305.10665,",
            "year": 2023
        },
        {
            "authors": [
                "Zhaoyu Chen",
                "Bo Li",
                "Shuang Wu",
                "Jianghe Xu",
                "Shouhong Ding",
                "Wenqiang Zhang"
            ],
            "title": "Shape matters: deformable patch attack",
            "venue": "In European conference on computer vision,",
            "year": 2022
        },
        {
            "authors": [
                "Zhaoyu Chen",
                "Bo Li",
                "Jianghe Xu",
                "Shuang Wu",
                "Shouhong Ding",
                "Wenqiang Zhang"
            ],
            "title": "Towards practical certifiable patch defense with vision transformer",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Yoojin Choi",
                "Jihwan Choi",
                "Mostafa El-Khamy",
                "Jungwon Lee"
            ],
            "title": "Data-free network quantization with adversarial knowledge distillation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops,",
            "year": 2020
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "In 2009 IEEE conference on computer vision and pattern recognition,",
            "year": 2009
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805,",
            "year": 2018
        },
        {
            "authors": [
                "Kien Do",
                "Hung Le",
                "Dung Nguyen",
                "Dang Nguyen",
                "Haripriya Harikumar",
                "Truyen Tran",
                "Santu Rana",
                "Svetha Venkatesh"
            ],
            "title": "Momentum adversarial distillation: Handling large distribution shifts in data-free knowledge distillation",
            "venue": "Advances in neural information processing systems,",
            "year": 2022
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "arXiv preprint arXiv:2010.11929,",
            "year": 2020
        },
        {
            "authors": [
                "Gongfan Fang",
                "Kanya Mo",
                "Xinchao Wang",
                "Jie Song",
                "Shitao Bei",
                "Haofei Zhang",
                "Mingli Song"
            ],
            "title": "Up to 100x faster datafree knowledge distillation",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Gongfan Fang",
                "Jie Song",
                "Chengchao Shen",
                "Xinchao Wang",
                "Da Chen",
                "Mingli Song"
            ],
            "title": "Data-free adversarial distillation",
            "venue": "arXiv preprint arXiv:1912.11006,",
            "year": 2019
        },
        {
            "authors": [
                "Gongfan Fang",
                "Jie Song",
                "Xinchao Wang",
                "Chengchao Shen",
                "Xingen Wang",
                "Mingli Song"
            ],
            "title": "Contrastive model inversion for data-free knowledge distillation",
            "venue": "arXiv preprint arXiv:2105.08584,",
            "year": 2021
        },
        {
            "authors": [
                "Jean-Bastien Grill",
                "Florian Strub",
                "Florent Altch\u00e9",
                "Corentin Tallec",
                "Pierre Richemond",
                "Elena Buchatskaya",
                "Carl Doersch",
                "Bernardo Avila Pires",
                "Zhaohan Guo",
                "Mohammad Gheshlaghi Azar"
            ],
            "title": "Bootstrap your own latent-a new approach to self-supervised learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Zhiwei Hao",
                "Yong Luo",
                "Han Hu",
                "Jianping An",
                "Yonggang Wen"
            ],
            "title": "Data-free ensemble knowledge distillation for privacyconscious multimedia model compression",
            "venue": "In Proceedings of the 29th ACM International Conference on Multimedia,",
            "year": 2021
        },
        {
            "authors": [
                "Kaiming He",
                "Haoqi Fan",
                "Yuxin Wu",
                "Saining Xie",
                "Ross Girshick"
            ],
            "title": "Momentum contrast for unsupervised visual representation learning",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Geoffrey Hinton",
                "Oriol Vinyals",
                "Jeff Dean"
            ],
            "title": "Distilling the knowledge in a neural network",
            "venue": "arXiv preprint arXiv:1503.02531,",
            "year": 2015
        },
        {
            "authors": [
                "Jeff Johnson",
                "Matthijs Douze",
                "Herv\u00e9 J\u00e9gou"
            ],
            "title": "Billionscale similarity search with GPUs",
            "venue": "IEEE Transactions on Big Data,",
            "year": 2019
        },
        {
            "authors": [
                "Tero Karras",
                "Samuli Laine",
                "Timo Aila"
            ],
            "title": "A style-based generator architecture for generative adversarial networks",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Geoffrey Hinton"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "year": 2009
        },
        {
            "authors": [
                "Siao Liu",
                "Zhaoyu Chen",
                "Wei Li",
                "Jiwei Zhu",
                "Jiafeng Wang",
                "Wenqiang Zhang",
                "Zhongxue Gan"
            ],
            "title": "Efficient universal shuffle attack for visual object tracking",
            "venue": "In ICASSP 2022-",
            "year": 2022
        },
        {
            "authors": [
                "Yang Liu",
                "Jing Liu",
                "Jieyu Lin",
                "Mengyang Zhao",
                "Liang Song"
            ],
            "title": "Appearance-motion united auto-encoder framework for video anomaly detection",
            "venue": "IEEE Transactions on Circuits and Systems II: Express Briefs,",
            "year": 2022
        },
        {
            "authors": [
                "Yang Liu",
                "Jing Liu",
                "Mengyang Zhao",
                "Shuang Li",
                "Liang Song"
            ],
            "title": "Collaborative normality learning framework for weakly supervised video anomaly detection",
            "venue": "IEEE Transactions on Circuits and Systems II: Express Briefs,",
            "year": 2022
        },
        {
            "authors": [
                "Yang Liu",
                "Dingkang Yang",
                "Yan Wang",
                "Jing Liu",
                "Liang Song"
            ],
            "title": "Generalized video anomaly event detection: Systematic taxonomy and comparison of deep models",
            "venue": "arXiv preprint arXiv:2302.05087,",
            "year": 2023
        },
        {
            "authors": [
                "Ze Liu",
                "Yutong Lin",
                "Yue Cao",
                "Han Hu",
                "Yixuan Wei",
                "Zheng Zhang",
                "Stephen Lin",
                "Baining Guo"
            ],
            "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Raphael Gontijo Lopes",
                "Stefano Fenu",
                "Thad Starner"
            ],
            "title": "Data-free knowledge distillation for deep neural networks",
            "venue": "arXiv preprint arXiv:1710.07535,",
            "year": 2017
        },
        {
            "authors": [
                "Liangchen Luo",
                "Mark Sandler",
                "Zi Lin",
                "Andrey Zhmoginov",
                "Andrew Howard"
            ],
            "title": "Large-scale generative data-free distillation",
            "venue": "arXiv preprint arXiv:2012.05578,",
            "year": 2020
        },
        {
            "authors": [
                "Paul Micaelli",
                "Amos J Storkey"
            ],
            "title": "Zero-shot knowledge transfer via adversarial belief matching",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Wonpyo Park",
                "Dongju Kim",
                "Yan Lu",
                "Minsu Cho"
            ],
            "title": "Relational knowledge distillation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Nathan Silberman",
                "Derek Hoiem",
                "Pushmeet Kohli",
                "Rob Fergus"
            ],
            "title": "Indoor segmentation and support inference from rgbd images",
            "venue": "In European conference on computer vision,",
            "year": 2012
        },
        {
            "authors": [
                "Chen Sun",
                "Abhinav Shrivastava",
                "Saurabh Singh",
                "Abhinav Gupta"
            ],
            "title": "Revisiting unreasonable effectiveness of data in deep learning era",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2017
        },
        {
            "authors": [
                "Yonglong Tian",
                "Dilip Krishnan",
                "Phillip Isola"
            ],
            "title": "Contrastive representation distillation",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Laurens Van der Maaten",
                "Geoffrey Hinton"
            ],
            "title": "Visualizing data using t-sne",
            "venue": "Journal of machine learning research,",
            "year": 2008
        },
        {
            "authors": [
                "Yuzheng Wang",
                "Zhaoyu Chen",
                "Dingkang Yang",
                "Pinxue Guo",
                "Kaixun Jiang",
                "Wenqiang Zhang",
                "Lizhe Qi"
            ],
            "title": "Model robustness meets data privacy: Adversarial robustness distillation without original data",
            "venue": "arXiv preprint arXiv:2303.11611,",
            "year": 2023
        },
        {
            "authors": [
                "Yuzheng Wang",
                "Zhaoyu Chen",
                "Dingkang Yang",
                "Yang Liu",
                "Siao Liu",
                "Wenqiang Zhang",
                "Lizhe Qi"
            ],
            "title": "Adversarial contrastive distillation with adaptive denoising",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2023
        },
        {
            "authors": [
                "Yuzheng Wang",
                "Zuhao Ge",
                "Zhaoyu Chen",
                "Xian Liu",
                "Chuangjia Ma",
                "Yunquan Sun",
                "Lizhe Qi"
            ],
            "title": "Explicit and implicit knowledge distillation via unlabeled data",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2023
        },
        {
            "authors": [
                "Zhirong Wu",
                "Yuanjun Xiong",
                "Stella X Yu",
                "Dahua Lin"
            ],
            "title": "Unsupervised feature learning via non-parametric instance discrimination",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Dingkang Yang",
                "Zhaoyu Chen",
                "Yuzheng Wang",
                "Shunli Wang",
                "Mingcheng Li",
                "Siao Liu",
                "Xiao Zhao",
                "Shuai Huang",
                "Zhiyan Dong",
                "Peng Zhai",
                "Lihua Zhang"
            ],
            "title": "Context deconfounded emotion recognition",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2023
        },
        {
            "authors": [
                "Dingkang Yang",
                "Shuai Huang",
                "Shunli Wang",
                "Yang Liu",
                "Peng Zhai",
                "Liuzhen Su",
                "Mingcheng Li",
                "Lihua Zhang"
            ],
            "title": "Emotion recognition for multiple context awareness",
            "venue": "In Proceedings of the European Conference on Computer Vision (ECCV),",
            "year": 2022
        },
        {
            "authors": [
                "Dingkang Yang",
                "Shuai Huang",
                "Zhi Xu",
                "Zhenpeng Li",
                "Shunli Wang",
                "Mingcheng Li",
                "Yuzheng Wang",
                "Yang Liu",
                "Kun Yang",
                "Zhaoyu Chen"
            ],
            "title": "Aide: A vision-driven multi-view, multimodal, multi-tasking dataset for assistive driving perception",
            "venue": "arXiv preprint arXiv:2307.13933,",
            "year": 2023
        },
        {
            "authors": [
                "Dingkang Yang",
                "Yang Liu",
                "Can Huang",
                "Mingcheng Li",
                "Xiao Zhao",
                "Yuzheng Wang",
                "Kun Yang",
                "Yan Wang",
                "Peng Zhai",
                "Lihua Zhang"
            ],
            "title": "Target and source modality co-reinforcement for emotion understanding from asynchronous multimodal sequences",
            "venue": "Knowledge-Based Systems,",
            "year": 2023
        },
        {
            "authors": [
                "Jingwen Ye",
                "Yixin Ji",
                "Xinchao Wang",
                "Xin Gao",
                "Mingli Song"
            ],
            "title": "Data-free knowledge amalgamation via groupstack dual-gan",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Mang Ye",
                "Xu Zhang",
                "Pong C Yuen",
                "Shih-Fu Chang"
            ],
            "title": "Unsupervised embedding learning via invariant and spreading instance feature",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Junho Yim",
                "Donggyu Joo",
                "Jihoon Bae",
                "Junmo Kim"
            ],
            "title": "A gift from knowledge distillation: Fast optimization, network minimization and transfer learning",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Hongxu Yin",
                "Pavlo Molchanov",
                "Jose M Alvarez",
                "Zhizhong Li",
                "Arun Mallya",
                "Derek Hoiem",
                "Niraj K Jha",
                "Jan Kautz"
            ],
            "title": "Dreaming to distill: Data-free knowledge transfer via deepinversion",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Jie Zhang",
                "Chen Chen",
                "Bo Li",
                "Lingjuan Lyu",
                "Shuang Wu",
                "Shouhong Ding",
                "Chunhua Shen",
                "Chao Wu"
            ],
            "title": "Dense: Datafree one-shot federated learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Deep learning has made refreshing progress in various computer vision fields [15, 43, 29, 44, 10, 26, 40, 9, 7]. Despite success, large-scale models [13, 24, 30, 28, 46, 8, 27] and unavailable privacy data [2, 36, 45, 39] often impede the application of advanced technology on mobile devices. Therefore, model compression and data-free technology have become the key to breaking the bottleneck. To this end, Lopes et al. [31] propose Data-Free Knowledge Distillation (DFKD). In this process, knowledge is transferred from the cumbersome model to a small model that is more suitable for deployment without using the original training dataset. As a result, this widely applicable technology has gained much attention.\nTo replace unavailable private data and effectively train small models, most existing data-free knowledge distilla-\ntion methods relied on alternately training of the generator and the student, called the generation-based method. Despite not using the original training data, these generationbased methods have many issues. First, their trained generators are abandoned after the students\u2019 training [5, 17, 33, 20, 14, 51]. The training of generators brings additional computational costs, especially for large datasets. For instance, a thousand generators are trained for the ImageNet dataset [12], which introduces more computational waste [32, 16]. Then, large domain shifts exist between the generated data and the original data. The substitute data are composed of random noise transformation without supervision information. Hence, the substitute domain usually does not match the unavailable original data domain and includes extensive label noise predicted by the teacher [47].\nRather than relying on generation-based methods, Chen et al. [4] propose a sampling-based method for training the student network via open-world unlabeled data without the generation calculations. Compared with generation-based methods, sampling-based methods can avoid the training cost of generators. The comparison of the two methods is shown in Figure 1. Meanwhile, they try to reduce label noise by updating the learnable noise matrix, but the noise matrix\u2019s computational costs are expensive. More importantly, their sampling method only relies on strict confi-\nar X\niv :2\n30 7.\n16 60\n1v 1\n[ cs\n.C V\n] 3\n1 Ju\nl 2 02\n3\ndence ranking and does not consider the data domain similarity problem, so the domain shift problem is still severe. In addition, the existing generation-based and sampling-based methods can be summarized as the distillation methods of the student to mimic the outputs of a particular data example represented by the teacher [49, 34, 41]. Therefore, these methods do not adequately utilize the implicit relationship among multiple data examples, which leads to the lack of effective knowledge expression in the distillation process.\nBased on the above observations, we construct a novel sampling-based method to avoid unnecessary computational costs. The difference is that we hope to mitigate the domain shifts issue better and exploit the relationship among multiple samples. To cope with the domain shifts issue between the open-world and source data, we propose a comprehensive solution to it from two aspects. Firstly, we preferentially try to sample data with similar distribution to the original data domain to reduce the shifts. Secondly, low-noise knowledge representation learning is introduced to suppress the interference of label noise. To explore the data knowledge adequately, we set up a structured representation of unlabeled data to enable the student to learn the implicit knowledge among multiple data examples. As a result, the student can learn from carefully sampled unlabeled data instead of absolutely relying on the teacher. At the same time, to explore an effective distillation process, we introduce a contrastive structured relationship between the teacher and student. The student can make better progress through the structured prediction of the teacher network.\nIn this paper, we consider a solution of DFKD that does not require additional generation costs. On the one hand, we hope to find a solution to data domain shifts from both data source and distillation methods. On the other hand, we try to explore an effective structured knowledge representation method to deal with the issues of lack of supervision information and the training difficulties in DFKD scenes. Therefore, we propose an Open-world Data Sampling Distillation (ODSD) method, which includes Adaptive Prototype Sampling (APS) and Denoising Contrastive Relational Distillation (DCRD) modules. Specifically, the primary contributions and experiments are summarized as follows:\n\u2022 We propose an Open-world Data Sampling Distillation (ODSD) method. The method does not require additional training of one or more generation modules, thus avoiding unnecessary computational costs.\n\u2022 Considering the domain shifts between the open-world and source data, we introduce an Adaptive Prototype Sampling (APS) mechanism to obtain data closer to the original data distribution.\n\u2022 We propose a Denoising Contrastive Relational Distillation (DCRD) module, which utilizes a low-noise representation to suppress label noise and builds contrast\nstructured relationships to exploit knowledge from data and the teacher adequately.\n\u2022 Experiments show that the proposed ODSD method improves the current state-of-the-art (SOTA) in various benchmarks. In particular, our method improves 1.50%-9.59% accuracy on the ImageNet dataset."
        },
        {
            "heading": "2. Related Work",
            "text": ""
        },
        {
            "heading": "2.1. Data-Free Knowledge Distillation",
            "text": "Data-free knowledge distillation is proposed to deal with the problem of a lightweight model when the original data are unavailable. Therefore, substitute data are indispensable to help transfer knowledge from the cumbersome teacher to the flexible student. According to the source of these data, existing methods are divided into generation-based and sampling-based methods. Generation-based Methods. The generation-based methods depend on the generation module to synthesize the substitute data. Lopes et al. [31] propose the first generationbased DFKD method, which uses the data means to fit the training data. Due to the weak generation ability, it can only be used on a simple dataset such as the MNIST dataset. The following methods combine the Generative Adversarial Networks (GANs) to generate more authentic and reliable data. Chen et al. [5] firstly put the idea into practice and define an information entropy loss to increase the diversity of data. However, this method relies on a long training time and a large batch size. Fang et al. [17] suggest forcing the generator to synthesize images that do not match between the two networks to enhance the training effect. Hao et al. [20] suggest using multiple pre-trained teachers to help the student, which leads to additional computational costs. Do et al. [14] propose a momentum adversarial distillation method to help the student recall past knowledge and prevent the student from adapting too quickly to new generator updates. The same domain typically shares some reusable patterns, so Fang et al. [16] introduce the sharing of local features of the generated graph, which speeds up the generation process. Since the generation quality is still not guaranteed, some methods spend extra computational costs on gradient inversion to synthesize more realistic data [50, 18]. In addition, Choi et al. [11] combine DFKD with other compression technologies and achieve encouraging performance. However, generation-based DFKD methods generate a large number of additional calculation costs in generation modules, while these modules will be discarded after students\u2019 training [4]. Sampling-based Methods. To train the student more exclusively, Chen et al. [4] propose to sample unlabeled data to replace the unavailable data without the generation module. Firstly, they use a strict confidence ranking to sample unlabeled data. Then, they propose a simple distil-\nlation method with a learnable adaptive matrix. Despite no additional training costs and promoting encouraging results, their method ignores the intra-class relationships of multiple unlabeled data. Simultaneously, the simple strict confidence causes more data to be sampled for simple classes, leading to imbalanced data classes. In addition, their proposed distillation method is relatively simple and lacks structured relationship expression, which limits the student\u2019s performance."
        },
        {
            "heading": "2.2. Contrastive Learning",
            "text": "Contrastive learning makes the model\u2019s training efficient by learning the data differences [48]. The unsupervised training usually requires to store negative data by a memory bank [42], large dictionaries [21], or a large batch size [6]. Even it requires a lot of computation, additional normalization [19], and network update operations [3]. The high storage and computing costs seriously reduce knowledge distillation efficiency. But at the same time, this idea of mining knowledge in unlabeled data may be helpful for the student\u2019s learning. Due to such technical conflicts, there are few methods to perfectly combine knowledge distillation and contrastive learning in the past. As a rare attempt, Tian et al. [37] propose a contrastive data-based distillation method by an update a large memory bank. But for datafree knowledge distillation, the data quality cannot be guaranteed, and data domain shifts are intractable, which makes the above process challenging to carry out.\nIn this work, we attempt to explore additional knowledge from both data and the teacher. Therefore, we further stimulate students\u2019 learning ability by using the internal relation-\nship of unlabeled data and constructing a structured contrastive relationship. To our knowledge, this is the first combination of data-free knowledge distillation and contrastive learning at a low cost, which achieves an unexpected effect."
        },
        {
            "heading": "3. Methodology",
            "text": ""
        },
        {
            "heading": "3.1. Overview",
            "text": "Considering the existing issues, our pipeline includes two stages: 1) unlabeled data sampling and 2) distillation training, as shown in Figure 2. For the first stage, we sample unlabeled data by an adaptive sampling mechanism to obtain data closer to the original distribution. For the second stage, the student learns the knowledge representation after denoising through a spatial mapping denoise module. Further, we mine more profound knowledge of the unlabeled data and build the structured relational distillation to help the student gain better performance. The complete algorithm is shown in Supplementary Sec.3."
        },
        {
            "heading": "3.2. Adaptive Prototype Sampling",
            "text": "The class and scale of the unavailable source dataset and the unlabeled dataset are different in many cases, so there is a severe issue of data domain shifts, which will be discussed in Figure 3. To this end, we aim to find unlabeled data closer to the distribution of source domain data. Therefore, we propose an Adaptive Prototype Sampling (APS) method that considers the teacher\u2019s familiarity, the intra-class outliers, and the class balance of the unlabeled data. Based on these, we design three score indicators to evaluate the effectiveness of the unlabeled data for student training: the\ndata confidence score, the data outlier score, and the class density score.\n(a) Data confidence score. To sample data with similar distribution to original training data, we try to keep consistent prediction logits of the teacher. Firstly the teacher provides the prediction logits for the unlabeled dataset as P = [p1, p2, ..., ps] \u2208 Rs\u00d7C , where pi is the prediction for a single sample satisfying R1\u00d7C . i is the i-th data, s is the number of data for the unlabeled dataset, and C is the number of classes in the teacher\u2019s prediction. Then the prediction is converted into the probability of the unified scale as: pi \u2032 = softmax(pi), p\u0303i = argmax(pi \u2032), and argmax(p\u2032) denotes the confidence probability corresponding to the predicted result class. Therefore, p\u0303 = [p\u03031, p\u03032, ..., p\u0303s] represents the confidence of each data in the whole dataset. We choose the largest one max {p\u0303} for normalization. The confidence score can be calculated as: sci= p\u0303i|max{p\u0303}| .\n(b) Data outlier score. The label space of the two data domains is different, so there are edge data, which may affect the student\u2019s learning and need to be excluded. For example, we try to exclude data like tigers from the real class of cats, as shown in the orange part of Stage 1 in Figure 2. Firstly we separate the data according to the classes predicted by the teacher. Each class is clustered to explore the intra-class relationships through prototype learning. Then we refer to a group of CK data sampling prototypes, i.e.,{ \u00b5c,k \u2208 R1\u00d7C }C,K c,k=1\n, which are based solely on the subcenter of target classes. c is the c-th class, and K is a hyperparameter that represents the number of prototypes defined for each class. After clustering [23], the prediction results of the c-th class can be expressed as K prototypes as {\u00b5c,k}Kk=1, which reflect the intra-class relationship of the data predicted as class c. To calculate the outlier of data in class c, the predictions of these data are expressed as \u03c1i,c = pi, which couples the predictions and class information. According to the predictions and the prototype centers of the class c, the intra-class outliers of i-th data can be calculated as o\u0303i = \u2211K k=1 cos(\u03c1i,c, \u00b5c,k), where cos denotes the cosine similarity. Similar to the above, we select the maximum value for normalization. As a result, the outlier score can be calculated as: soi= o\u0303i|max{o\u0303}| .\n(c) Class density score. To better help the student learn various classes, we calculate the class density to better meet the sampled data\u2019s balance. As shown in Stage 1 of Figure 2, we increase the sampling range for classes with sparse data (the blue part) while we reduce the sampling range for classes with redundant data (the orange part). Based on this, we firstly separate the above intra-class outliers o\u0303i of all data by their classes. The outliers mean value of each class can be calculated as: uc = 1nc \u2211 pi\u2208co\u0303i, where nc is the number of the data predicted as c-th class. Therefore, the Dcluster parameter Dc can be calculated as: Dc = \u221a uc\nloge (nc+C) .\nEach data\u2019s density value equals the density value of its class as: di = Dc(pi \u2208 c). After selecting the maximum value for normalization, the density score of each data can be calculated as: sdi= di|max{d}| .\nFinally, we define the total score with Stotal, which is calculated as: Stotal = sci\u2212 soi+ sdi. According to the total score, the data closer to the distribution of the original data domain are sampled, which can help the student learn better. The quantitative analysis is shown in Table 7."
        },
        {
            "heading": "3.3. Denoising Contrastive Relational Distillation",
            "text": "After obtaining the high score data, the distillation process can be carried out. We denote fT and fS as the output of the teacher and student networks and denote x as the sampled data. According to [22], the knowledge distillation loss is calculated as:\nLKD = \u2211 x\u2208X DKL(fT (x)/\u03c4, fS(x)/\u03c4), (1)\nwhere DKL is the Kullback-Leibler divergence, and \u03c4 is the distillation temperature. Although LKD allows the student to imitate the teacher\u2019s output, only its use leads to poor learning results. The main challenge is the distribution differences between the substitute and original data domains, leading to label noise interference. Simultaneously, the ground-truth labels are unavailable, so correct information supervision is missing. Therefore, we propose a Denoising Contrastive Relational Distillation (DCRD) module, which includes a spatial mapping denoise component and a contrastive relationship representation component to help the student get better performance."
        },
        {
            "heading": "3.3.1 Spatial Mapping Denoise",
            "text": "The data distribution in the unlabeled data is different from the unavailable source data, which indicates the label noise is inevitable. Low dimensional information contains purer knowledge, which is subject to less noise interference [1]. Here, we use a low dimensional spatial mapping denoise component to help the student learn low-noise knowledge representation. Zt, Zs are the low dimensional representation of teacher and student prediction. In order to obtain a distance invariant spatial projection transformation \u03a6, the autocorrelation matrix d2ij is defined as: d\n2 ij =\u2225\u2225\u2225\u2212\u2192fT (xi)\u2212\u2212\u2192fT (xj)\u2225\u2225\u2225 = \u2225\u2212\u2192zi \u2212\u2212\u2192zj\u2225 = bii+bjj\u22122bij , where bij = \u2212\u2192zi \u00b7 \u2212\u2192zj . We sum d2ij in a mini-batch as:\nN\u2211 i N\u2211 j d2ij = 2N \u00b7 tr(ZtZTt ), (2)\nwhere N denotes the batch size, and tr(\u00b7) denotes the trace of a matrix. Then Zt can be calculated as Zt = Vt\u039b 1/2 t ,\nwhere Vt is the eigenvalue after eigendecomposition, and \u039bt is the eigenmatrix. Similarly, we can get the student predictions of low dimensional representation as Zs. Then, we set up a distillation loss to correct the impact of label noise by the spatial mapping of the two networks. The spatial mapping denoise distillation loss is calculated as:\nLn = \u2113h(\u03a6(fT \u00b7 fTT ),\u03a6(fS \u00b7 fST )) = \u2113h(Zt, Zs), (3)\nwhere \u2113h(\u00b7, \u00b7) denotes the Huber loss. We can match the teacher-student relationship in a low dimensional space to learn a low-noise knowledge representation by Ln."
        },
        {
            "heading": "3.3.2 Contrastive Relational Distillation",
            "text": "The missing supervision information limits the student\u2019s performance. It is indispensable to adequately mine the knowledge in unlabeled data to compensate for lack of information. To avoid single imitation of a particular data example, we build two kinds of structured relationship to mine knowledge from data and the teacher.\nFirstly, the student can adequately explore the structured relation among data by learning the instance invariant. xi, xj are the different data in a mini-batch. We calculate the prediction difference between data as:\n\u2113xixjs = cos(fS(xi), fS(xj))/\u03c41\u22112N\nk=1,k \u0338=i cos(fS(xi), fS(xk))/\u03c41 , (4)\nwhere \u03c41 denotes contrastive temperature. Next, we can calculate the consistency instance discrimination loss as:\nLc1 = \u2212 1\nN N\u2211 j=1 log \u2113 xjxj\u0304 s , (5)\nwhere xj\u0304 denotes the data augmentation transform of data xj . The student can find knowledge directly from the multiple unlabeled data through data consistency learning. This unsupervised method is especially effective when the teacher makes wrong results.\nSecondly, we construct a structured contrastive relationship between the teacher and student, which promotes consistent learning between the teacher and student. The structured knowledge learning process is calculated as:\n\u2113 x\u2032i ts =\ncos(fT (x \u2032 i), fS(x \u2032 i))/\u03c42\u22114N\nk=1,k \u0338=i cos(fT (x \u2032 i), fS(x \u2032 k))/\u03c42\n, (6)\nwhere x\u2032 = x \u222a x\u0304. Then, we can calculate the teacherstudent consistency loss as:\nLc2 = \u2212 1\n2N 2N\u2211 j=1 log \u2113 x\u2032j ts . (7)\nThe student can obtain better learning performance through the mixed structured and consistent relationship learning between the two networks. Then, the contrastive relational distillation loss is Lc =Lc1 + Lc2. Finally, we can get the total denoising contrastive relational distillation loss as:\nLtotal = LKD + \u03bb1 \u00b7Ln + \u03bb2 \u00b7Lc, (8)\nwhere \u03bb1, \u03bb2 are the trade-off parameters for training losses."
        },
        {
            "heading": "4. Experiments",
            "text": ""
        },
        {
            "heading": "4.1. Experimental Settings",
            "text": "Datasets and Models. We evaluate the proposed ODSD method for the classification and semantic segmentation tasks. For classification, we evaluate it on widely used datasets: 32 \u00d7 32 CIFAR-10, CIFAR-100 [25], and 224 \u00d7 224 ImageNet [12]. In addition, we use the pre-trained models from CMI [18] and unify the teacher models among all baseline methods. The number of sampled data is 150k or 600k for CIFAR, and 600k for ImageNet following DFND [4]. More detailed classification settings are shown in Supplementary Sec.1. For semantic segmentation, we evaluate the proposed method on 128\u00d7128 NYUv2 dataset [35]. 200k data are sampled. More detailed segmentation settings are shown in Supplementary Sec.4. Besides, the corresponding open-world datasets are shown in Table 1, which is the same as DFND [4] for a fair comparison.\nBaselines. We compare two kinds of data-free knowledge distillation methods. One is to have to spend extra computing costs to obtain generation data by generation module, including: DeepInv [50], CMI [18], DAFL [5], ZSKT [33], DFED [20], DFQ [11], Fast [16], MAD [14], DFD [32], and DFAD [17]. Another is to use unlabeled data as the substitute data from easily accessible open source datasets based on sampling, i.e., DFND [4]."
        },
        {
            "heading": "4.2. Performance Comparison",
            "text": "To evaluate the effectiveness of our ODSD, we comprehensively compare it with current SOTA DFKD methods regarding the student\u2019s performance, the effectiveness of the sampling method and training costs. In addition, some other methods only apply to small datasets and models (e.g., MNIST and AlexNet) for some reason. The test baselines mentioned in the article may be difficult for these methods, causing serious performance decline and non-competitive experimental results. To compare these methods fairly, we conduct experiments on the MNIST dataset and maintain\nthe same experimental settings. The experimental results are shown in Supplementary Sec.2.\nExperiments on CIFAR-10 and CIFAR-100. We first verify the proposed method on the CIFAR-10 and CIFAR100 [25]. We collate the performance of two kinds of SOTA methods based on data generation and data sampling. The baseline \u201cTeacher\u201d and \u201cStudent\u201d means to use the corresponding backbones of the teacher or student for direct training with the original training data, and \u201cKD\u201d represents distilling the student network with the original training data. Generation-based methods include training additional generators and calculating model gradient inversion. Sampling-\nbased methods use the unlabeled ImageNet dataset. We reproduce the DFND using the unified teacher models, and the result is slightly higher than the original paper.\nAs shown in Table 2, our ODSD has achieved the best results on each baseline. Under most baseline settings, ODSD brings gains of 1% or even higher than the SOTA methods, even though students\u2019 accuracy is very close to their teachers. In particular, the students of our ODSD outperform the teachers on some baselines. As far as we know, it is the first DFKD method to achieve such performance. The main reasons for its breakthrough in analyzing the algorithm\u2019s performance come from three aspects. First, our\ndata sampling method comprehensively analyzes the intraclass relationships in the unlabeled data, excluding the difficult edge data and significant distribution differences data. At the same time, the number of data in each class is relatively more balanced, which is conducive to all kinds of balanced learning compared with other sampling methods. Second, our knowledge distillation method considers the representation of low-dimensional and low-noise information and expands the representation of knowledge through data augmentation. The structured relationship distillation method helps the student effectively learn knowledge from both multiple data and its teacher. Finally, the knowledge of our ODSD does not entirely come from the teacher but also the consistency and differentiated representation learning of unlabeled data, which is helpful when the teacher makes mistakes. The previous methods ignore the in-depth mining of data knowledge, which affect students\u2019 performance.\nExperiments on ImageNet. We conduct experiments on a large-scale ImageNet dataset to further verify the effectiveness. Due to the larger image size, it is challenging to synthesize training data for most generation-based methods effectively. Most of them failed. A small number of successful methods have to train 1,000 generators (one generator for one class), resulting in a large amount of additional computational costs. We set up three baselines to compare the performance of our method with the SOTA methods. Table 3 reports the experimental results. Our ODSD still achieves several percentage points increase compared with other SOTA methods, especially in the cross-backbones situation (9.59%). Due to the lack of structured knowledge representation, the DNFD algorithm performs poorly on the large-scale dataset. Comparing the performance of DFND and ODSD, our structured knowledge framework improves the overall understanding ability of the student.\nComparison of training costs. In order to verify that the generation-based methods add extra costs that we mentioned in the introduction section, we further calculate the total floating point operations (FLOPs) and parameters (params) required by various DFKD algorithms, as shown\n\u2217 For fair comparisons, we select the original version of DeepInv without the mixup data augmentation, which is the same as other methods.\nin Table 4. Because without additional generation modules, our method only needs training costs and params of the student network. Other methods list the required calculation cost and params of both the generation module and the student. These generation modules will be discarded after student training, which causes a waste of computing power.\nComparison of data sampling efficiency. To verify the effectiveness of the sampling mechanism, we compare the performance of our APS method compared with the current SOTA unlabeled data sampling method DFND [4]. Three data sampling methods (random sampling, DFND sampling, and our proposed APS) are set on three different distillation algorithms, including: KD [22], DFND [4], and our proposed ODSD. Table 5 reports the results. For KD, we use the sampled data instead of the original generated data with LKD distillation loss. From the result, this setting is competitive, even better than the distillation loss of DFND. For DFND, we reproduce it with open-source codes and keep the original training strategy unchanged. We find the performance of the DFND sampling method is unstable, which causes it to be lower than random sometimes. For ODSD, we use the distillation loss in Equation (8). Our proposed sampling method achieves the best performance in all three benchmarks and significantly improves performance. By comprehensively considering the data confidence, the data outliers, and the class density, our ODSD can more fully mine intra-class relationships of the unlabeled data. As a result, the sampled data are more helpful for subsequent student learning.\nExperiments about semantic segmentation. We also conduct experiments on segmentation tasks. Mean Intersection over Union (mIoU) is set as the evaluation metric. Table 6 shows segmentation results on the NYUv2 dataset. Our ODSD also achieves the best performance. The visualization results and more detailed analysis are shown in Supplementary Sec.4."
        },
        {
            "heading": "4.3. Diagnostic Experiment",
            "text": "To verify the effectiveness of our method, we conduct diagnostic studies on the CIFAR-100 dataset. We use ResNet34 as the teacher\u2019s backbone and ResNet-18 as the student\u2019s backbone. 150k data are sampled, and the student trains 200 epochs. The optimal values obtained by diagnostic experiments are also the default setting of 4.2 comparison experiments. In addition to what is shown in this section, more diagnostic experiments are shown in Supplementary Sec.2.\nDistillation training objective. We first investigate our overall training objective (cf. Equation (8)). Two different data sampling numbers are set in this experiment. As shown in the experiments (1-4) of Table 7, the model with LKD alone achieves accuracy scores of 74.39% and 77.27% on 50k and 150k data sampling settings. Adding Ln or Lc individually brings gains (i.e., 0.32%, 0.31%/ 0.43%, 0.44%), indicating the effectiveness of our proposed distillation method. By combining all the training objectives, our method achieves better performance with 75.26% and 77.90%. Therefore, the proposed training objectives are effective and can help students gain good performance.\nData sampling scores. To obtain more efficient data, we define three scores for our sampling method in section 3.2. To verify their effectiveness, we further carry out ablation experiments. When using the complete three efficient evaluation criteria, the model can achieve the best performance with 75.26% and 77.90% accuracy shown in experiments (5-8) of Table 7. When the confidence score sci is abandoned, the familiarity of the teacher network with the sampled data decreases, reducing the amount of adequate information contained in the data. Without the outlier score soi, the lack of modelling of the intra-class relationship of the data to be sampled leads to increased data distribution difference between the substitute data domain and the original data domain. Further, the class density score sdi can measure the number of data in each class and maintain the balance of the sampled data. In summary, all three score indicators can help students perform better."
        },
        {
            "heading": "4.4. Visualization",
            "text": "To verify the distribution difference between sampled data and the original data of each sampling-based method, we use t-SNE [38] to visualize the feature distribution. The pre-trained ResNet-34 network on the CIFAR-100 dataset and ResNet-50 network on the ImageNet dataset is used as the teacher network. For both datasets, we reserve 100 classes of validation data. We compare random sampling, the DFND sampling method, and our Adaptive Prototype Sampling (APS) method. Figure 3 shows the data distribution differentiation results. Our clustering results are closer to the extracted features of the original data. Reducing the distribution difference between sampled and original data\nhelps reduce data label noise, which is the key for the student to perform well."
        },
        {
            "heading": "5. Conclusion",
            "text": "Since the original training data may not be available due to privacy concerns, most existing data-free knowledge distillation methods rely heavily on additional generation modules. However, these generation modules bring additional computational costs. Meanwhile, existing DFKD methods disregard the domain shifts issue between the substitute and original data, and only consider the teacher\u2019s knowledge ignoring the data knowledge. In this paper, we propose an Open-world Data Sampling Distillation (ODSD) method without unnecessary generation costs. We sample unlabeled data with similar distribution to original data and introduce low-noise knowledge representation learning to cope with domain shifts. To explore the data knowledge adequately, we design a structured knowledge representation. Comprehensive experiments have illustrated the effectiveness of our proposed data sampling and distillation method. As a result, the proposed method achieves significant improvement and state-of-the-art performance on various benchmarks."
        }
    ],
    "title": "Sampling to Distill: Knowledge Transfer from Open-World Data",
    "year": 2023
}