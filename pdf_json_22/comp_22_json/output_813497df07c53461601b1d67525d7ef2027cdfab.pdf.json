{
    "abstractText": "Many complex systems in the real world can be characterized by attributed networks. To mine the potential information in these networks, deep embedded clustering, which obtains node representations and clusters simultaneously, has been paid much attention in recent years. Under the assumption of consistency for data in different views, the cluster structure of network topology and that of node attributes should be consistent for an attributed network. However, many existing methods ignore this property, even though they separately encode node representations from network topology and node attributes meanwhile clustering nodes on representation vectors learnt from one of the views. Therefore, in this study, we propose an endto-end deep embedded clustering model for attributed networks. It utilizes graph autoencoder and node attribute autoencoder to respectively learn node representations and cluster assignments. In addition, a distribution consistency constraint is introduced to maintain the latent consistency of cluster distributions of two views. Extensive experiments on several datasets demonstrate that the proposed model achieves significantly better or competitive performance compared with the state-of-the-art methods. The source code can be found at https://github.com/Zhengymm/DCP.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yimei Zheng"
        },
        {
            "affiliations": [],
            "name": "Caiyan Jiaa"
        },
        {
            "affiliations": [],
            "name": "Jian Yu"
        },
        {
            "affiliations": [],
            "name": "Xuanya Li"
        }
    ],
    "id": "SP:5db35cbdf1e66ecbceae7036289d910deac9136a",
    "references": [
        {
            "authors": [
                "T.N. Kipf",
                "M. Welling"
            ],
            "title": "Semi-supervised classification with graph convolutional networks",
            "venue": "in: Proc. of the 5th Int. Conf. on Learning Representations (ICLR)",
            "year": 2017
        },
        {
            "authors": [
                "C. Wang",
                "S. Pan",
                "R. Hu",
                "G. Long",
                "J. Jiang",
                "C. Zhang"
            ],
            "title": "Attributed graph clustering: A deep attentional embedding approach",
            "venue": "in: Proc. of the 28th Int. Joint Conf. on Artificial Intelligence (IJCAI)",
            "year": 2019
        },
        {
            "authors": [
                "X. He",
                "K. Deng",
                "X. Wang",
                "Y. Li",
                "Y. Zhang",
                "M. Wang"
            ],
            "title": "Lightgcn: Simplifying and powering graph convolution network for recommendation",
            "venue": "in: Proc. of the 43rd Int. ACM SIGIR conf. on Research and Development in Information Retrieval(SIGIR)",
            "year": 2020
        },
        {
            "authors": [
                "J. Xie",
                "R.B. Girshick",
                "A. Farhadi"
            ],
            "title": "Unsupervised deep embedding for clustering analysis",
            "venue": "in: Proc. of the 33rd Int. Conf. on Machine Learning (ICML), Vol. 48",
            "year": 2016
        },
        {
            "authors": [
                "X. Guo",
                "L. Gao",
                "X. Liu",
                "J. Yin"
            ],
            "title": "Improved deep embedded clustering with local structure preservation",
            "venue": "in: Proc. of the 26th Int. Joint Conf. on Artificial Intelligence (IJCAI)",
            "year": 2017
        },
        {
            "authors": [
                "J. Zhang",
                "C. Li",
                "C. You",
                "X. Qi",
                "H. Zhang",
                "J. Guo",
                "Z. Lin"
            ],
            "title": "Self-supervised convolutional subspace clustering network",
            "venue": "in: IEEE Conf. on Computer Vision and Pattern Recognition, (CVPR)",
            "year": 2019
        },
        {
            "authors": [
                "C. Wang",
                "S. Pan",
                "G. Long",
                "X. Zhu",
                "J. Jiang"
            ],
            "title": "MGAE: marginalized graph autoencoder for graph clustering",
            "venue": "in: Proc. of the 2017 ACM Conf. on Information and Knowledge Management, CIKM",
            "year": 2017
        },
        {
            "authors": [
                "H. Gao",
                "H. Huang"
            ],
            "title": "Deep attributed network embedding",
            "venue": "in: Proc. of the 27th Int. Joint Conf. on Artificial Intelligence (IJCAI)",
            "year": 2018
        },
        {
            "authors": [
                "S. Pan",
                "R. Hu",
                "G. Long",
                "J. Jiang",
                "L. Yao",
                "C. Zhang"
            ],
            "title": "Adversarially regularized graph autoencoder for graph embedding",
            "venue": "in: Proc. of the 27th Int. Joint Conf. on Artificial Intelligence (IJCAI)",
            "year": 2018
        },
        {
            "authors": [
                "G. Cui",
                "J. Zhou",
                "C. Yang",
                "Z. Liu"
            ],
            "title": "Adaptive graph encoder for attributed graph embedding",
            "venue": "in: Proc. of the 26th ACM SIGKDD Conf. on Knowledge Discovery and Data Mining (KDD)",
            "year": 2020
        },
        {
            "authors": [
                "J.B. MacQueen"
            ],
            "title": "Some methods for classification and analysis of multivariate observations",
            "venue": "in: Proc. of the 5th Berkeley symposium on mathematical statistics and probability, Vol. 1",
            "year": 1967
        },
        {
            "authors": [
                "J. Shi",
                "J. Malik"
            ],
            "title": "Normalized cuts and image segmentation",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell. 22 (8) ",
            "year": 2000
        },
        {
            "authors": [
                "D. Bo",
                "X. Wang",
                "C. Shi",
                "M. Zhu",
                "E. Lu",
                "P. Cui"
            ],
            "title": "Structural deep clustering network",
            "venue": "in: The Web Conference (WWW)",
            "year": 2020
        },
        {
            "authors": [
                "M. Luo",
                "H. Yan"
            ],
            "title": "Adaptive attributed network embedding for community detection",
            "venue": "in: Pattern Recognition and Computer Vision, PRCV, Vol. 12307",
            "year": 2020
        },
        {
            "authors": [
                "F. Li",
                "H. Qiao",
                "B. Zhang"
            ],
            "title": "Discriminatively boosted image clustering with fully convolutional auto-encoders",
            "venue": "Pattern Recognit. 83 ",
            "year": 2018
        },
        {
            "authors": [
                "H. Sun",
                "F. He",
                "J. Huang",
                "Y. Sun",
                "Y. Li",
                "C. Wang",
                "L. He",
                "Z. Sun",
                "X. Jia"
            ],
            "title": "Network embedding for community detection in attributed networks",
            "venue": "ACM Trans. Knowl. Discov. Data 14 (3) ",
            "year": 2020
        },
        {
            "authors": [
                "P. Veli\u010dkovi\u0107",
                "G. Cucurull",
                "A. Casanova",
                "A. Romero",
                "P. Li\u00f2",
                "Y. Bengio"
            ],
            "title": "Graph attention networks",
            "venue": "in: Proc. of the 6th Int. Conf. on Learning Representations (ICLR)",
            "year": 2018
        },
        {
            "authors": [
                "P. Cui",
                "X. Wang",
                "J. Pei",
                "W. Zhu"
            ],
            "title": "A survey on network embedding",
            "venue": "IEEE Trans. Knowl. Data Eng. 31 (5) ",
            "year": 2019
        },
        {
            "authors": [
                "D. Zhang",
                "J. Yin",
                "X. Zhu",
                "C. Zhang"
            ],
            "title": "Network representation learning: A survey",
            "venue": "IEEE Trans. Big Data 6 (1) ",
            "year": 2020
        },
        {
            "authors": [
                "H. Cai",
                "V.W. Zheng",
                "K.C. Chang"
            ],
            "title": "A comprehensive survey of graph embedding: Problems",
            "venue": "techniques, and applications, IEEE Trans. Knowl. Data Eng. 30 (9) ",
            "year": 2018
        },
        {
            "authors": [
                "S. Pan",
                "J. Wu",
                "X. Zhu",
                "C. Zhang",
                "Y. Wang"
            ],
            "title": "Tri-party deep network representation",
            "venue": "in: Proc. of the 25th Int. Joint Conf. on Artificial Intelligence (IJCAI)",
            "year": 2016
        },
        {
            "authors": [
                "B. Perozzi",
                "R. Al-Rfou",
                "S. Skiena"
            ],
            "title": "Deepwalk: online learning of social representations",
            "venue": "in: Proc. of the 20th ACM SIGKDD Int. Conf. on Knowledge Discovery and Data Mining (KDD)",
            "year": 2014
        },
        {
            "authors": [
                "A. Grover",
                "J. Leskovec"
            ],
            "title": "node2vec: Scalable feature learning for networks",
            "venue": "in: Proc. of the 22nd ACM SIGKDD Int. Conf. on Knowledge Discovery and Data Mining (KDD)",
            "year": 2016
        },
        {
            "authors": [
                "H. Wei",
                "Z. Pan",
                "G. Hu",
                "H. Yang",
                "X. Li",
                "X. Zhou"
            ],
            "title": "Attributed network representation learning via deepwalk",
            "venue": "Intell. Data Anal. 23 (4) ",
            "year": 2019
        },
        {
            "authors": [
                "X. Liu",
                "F. Zhang",
                "Z. Hou",
                "L. Mian",
                "Z. Wang",
                "J. Zhang",
                "J. Tang"
            ],
            "title": "Self-supervised learning: Generative or contrastive",
            "venue": "IEEE Trans. Knowl. Data Eng. ",
            "year": 2021
        },
        {
            "authors": [
                "D.P. Kingma",
                "M. Welling"
            ],
            "title": "Auto-encoding variational bayes",
            "venue": "in: Proc. of the 2nd Int. Conf. on Learning Representations (ICLR)",
            "year": 2014
        },
        {
            "authors": [
                "H. Hafidi",
                "M. Ghogho",
                "P. Ciblat",
                "A. Swami"
            ],
            "title": "Negative sampling strategies for contrastive self-supervised learning of graph representations",
            "venue": "Signal Processing 190 ",
            "year": 2022
        },
        {
            "authors": [
                "W.L. Hamilton",
                "Z. Ying",
                "J. Leskovec"
            ],
            "title": "Inductive representation learning on large graphs",
            "venue": "in: Advances in Neural Information Processing Systems 30 (NeurIPS)",
            "year": 2017
        },
        {
            "authors": [
                "X. Yang",
                "C. Deng",
                "F. Zheng",
                "J. Yan",
                "W. Liu"
            ],
            "title": "Deep spectral clustering using dual autoencoder network",
            "venue": "in: IEEE Conf. on Computer Vision and Pattern Recognition, (CVPR)",
            "year": 2019
        },
        {
            "authors": [
                "Q. Huang",
                "Y. Zhang",
                "H. Peng",
                "T. Dan",
                "W. Weng",
                "H. Cai"
            ],
            "title": "Deep subspace clustering to achieve jointly latent feature extraction and discriminative learning",
            "venue": "Neurocomputing 404 ",
            "year": 2020
        },
        {
            "authors": [
                "C. Wang",
                "S. Pan",
                "C.P. Yu",
                "R. Hu",
                "G. Long",
                "C. Zhang"
            ],
            "title": "Deep neighbor-aware embedding for node clustering in attributed graphs",
            "venue": "Pattern Recognit. 122 ",
            "year": 2022
        },
        {
            "authors": [
                "H. Zhang",
                "P. Li",
                "R. Zhang",
                "X. Li"
            ],
            "title": "Embedding graph auto-encoder for graph clustering",
            "venue": "IEEE Trans. Neural Networks Learn. Syst. ",
            "year": 2022
        },
        {
            "authors": [
                "W. Tu",
                "S. Zhou",
                "X. Liu",
                "X. Guo",
                "Z. Cai",
                "E. Zhu",
                "J. Cheng"
            ],
            "title": "Deep fusion clustering network",
            "venue": "in: Proc. of the 35th AAAI Conf. on Artificial Intelligence, (AAAI)",
            "year": 2021
        },
        {
            "authors": [
                "L. Guo",
                "Q. Dai"
            ],
            "title": "Graph clustering via variational graph embedding",
            "venue": "Pattern Recognit. 122 ",
            "year": 2022
        },
        {
            "authors": [
                "S. Fan",
                "X. Wang",
                "C. Shi",
                "E. Lu",
                "K. Lin",
                "B. Wang"
            ],
            "title": "One2multi graph autoencoder for multi-view graph clustering",
            "venue": "in: The Web Conference (WWW)",
            "year": 2020
        },
        {
            "authors": [
                "P. Vincent",
                "H. Larochelle",
                "Y. Bengio",
                "P. Manzagol"
            ],
            "title": "Extracting and composing robust features with denoising autoencoders",
            "venue": "in: Proc. of the 25th Int. Conf. on Machine learning (ICML), Vol. 307",
            "year": 2008
        },
        {
            "authors": [
                "S. Rifai",
                "P. Vincent",
                "X. Muller",
                "X. Glorot",
                "Y. Bengio"
            ],
            "title": "Contractive auto-encoders: Explicit invariance during feature extraction",
            "venue": "in: Proc. of the 28th Int. Conf. on Machine Learning (ICML)",
            "year": 2011
        },
        {
            "authors": [
                "R. Hong",
                "Y. He",
                "L. Wu",
                "Y. Ge",
                "X. Wu"
            ],
            "title": "Deep attributed network embedding by preserving structure and attribute information",
            "venue": "IEEE Trans. Syst. Man Cybern. Syst. 51 (3) ",
            "year": 2021
        },
        {
            "authors": [
                "X. Wang",
                "M. Zhu",
                "D. Bo",
                "P. Cui",
                "C. Shi",
                "J. Pei"
            ],
            "title": "AM-GCN: adaptive multi-channel graph convolutional networks",
            "venue": "in: Proc. of the 26th ACM SIGKDD Conf. on Knowledge Discovery and Data Mining (KDD)",
            "year": 2020
        },
        {
            "authors": [
                "Q. Li",
                "Z. Han",
                "X. Wu"
            ],
            "title": "Deeper insights into graph convolutional networks for semi-supervised learning",
            "venue": "in: Proc. of the 32nd AAAI Conf. on Artificial Intelligence (AAAI)",
            "year": 2018
        },
        {
            "authors": [
                "J. Park",
                "M. Lee",
                "H.J. Chang",
                "K. Lee",
                "J.Y. Choi"
            ],
            "title": "Symmetric graph convolutional autoencoder for unsupervised graph representation learning",
            "venue": "in: IEEE Int. Conf. on Computer Vision (ICCV)",
            "year": 2019
        },
        {
            "authors": [
                "D. He",
                "Y. Song",
                "D. Jin",
                "Z. Feng",
                "B. Zhang",
                "Z. Yu",
                "W. Zhang"
            ],
            "title": "Community-centric graph convolutional network for unsupervised community detection",
            "venue": "in: Proc. of the 29th Int. Joint Conf. on Artificial Intelligence (IJCAI)",
            "year": 2020
        },
        {
            "authors": [
                "S. Lerique",
                "J.L. Abitbol",
                "M. Karsai"
            ],
            "title": "Joint embedding of structure and features via graph convolutional networks",
            "venue": "Appl. Netw. Sci. 5 (1) ",
            "year": 2020
        },
        {
            "authors": [
                "Y. LeCun",
                "O. Matan",
                "B.E. Boser",
                "J.S. Denker",
                "D. Henderson",
                "R.E. Howard",
                "W.E. Hubbard",
                "L.D. Jacket",
                "H.S. Baird"
            ],
            "title": "Handwritten zip code recognition with multilayer networks",
            "venue": "in: Proc. of the 10th IAPR Int. Conf. on Pattern Recognition (ICPR)",
            "year": 1990
        },
        {
            "authors": [
                "A. Stisen",
                "H. Blunck",
                "S. Bhattacharya",
                "T.S. Prentow",
                "M.B. Kj\u00e6rgaard",
                "A.K. Dey",
                "T. Sonne",
                "M.M. Jensen"
            ],
            "title": "Smart devices are different: Assessing and mitigating mobile sensing heterogeneities for activity recognition",
            "venue": "in: Proc. of the 13th ACM Conf. on Embedded Networked Sensor Systems",
            "year": 2015
        },
        {
            "authors": [
                "D.D. Lewis",
                "Y. Yang",
                "T.G. Rose",
                "F. Li"
            ],
            "title": "RCV1: A new benchmark collection for text categorization research",
            "venue": "J. Mach. Learn. Res. 5 ",
            "year": 2004
        },
        {
            "authors": [
                "A. Lancichinetti",
                "S. Fortunato",
                "F. Radicchi"
            ],
            "title": "Benchmark graphs for testing community detection algorithms",
            "venue": "Phys. Rev. E. 78 ",
            "year": 2008
        }
    ],
    "sections": [
        {
            "text": "Many complex systems in the real world can be characterized by attributed networks. To mine the potential information in these networks, deep embedded clustering, which obtains node representations and clusters simultaneously, has been paid much attention in recent years. Under the assumption of consistency for data in different views, the cluster structure of network topology and that of node attributes should be consistent for an attributed network. However, many existing methods ignore this property, even though they separately encode node representations from network topology and node attributes meanwhile clustering nodes on representation vectors learnt from one of the views. Therefore, in this study, we propose an endto-end deep embedded clustering model for attributed networks. It utilizes graph autoencoder and node attribute autoencoder to respectively learn node representations and cluster assignments. In addition, a distribution consistency constraint is introduced to maintain the latent consistency of cluster distributions of two views. Extensive experiments on several datasets demonstrate that the proposed model achieves significantly better or competitive performance compared with the state-of-the-art methods. The source code can be found at https://github.com/Zhengymm/DCP.\nKeywords: Deep embedded clustering, Autoencoder, Graph autoencoder, Node representation learning, Cluster distribution consistency"
        },
        {
            "heading": "1. Introduction",
            "text": "Many real-world systems can be modeled as networks, such as social networks, academic citation networks, and protein-protein interaction networks, where nodes denote objects and links denote pairs of relations between nodes. In addition to node connectivity information, nodes are often associated with attributes characterizing their own properties. Both kinds of information can implicitly reflect and affect\n\u2217Corresponding author. E-mail addresses: ymmzheng@bjtu.edu.cn (Yimei Zheng), cyjia@bjtu.edu.cn (Caiyan Jia), jianyu@bjtu.edu.cn (Jian Yu), lixuanya@baidu.com (Xuanya Li).\nPreprint submitted to Elsevier May 31, 2022\nar X\niv :2\n20 5.\n14 30\n3v 1\n[ cs\n.L G\n] 2\n8 M\nay 2\n02 2\nthe structures of networks. We generally call this kind of networks \u201cattributed networks\u201d. For example, an academic citation network can be viewed as a representative attributed network. Each paper is a node of the network, and the citation relationship between a pair of papers constitutes edges in the network. The attributes of each paper are usually a bag-of-words characterizing the paper. In recent years, there are more and more researches on attributed networks analysis, such as node classification, node clustering, and even recommendation [1, 2, 3].\nAs the saying goes, \u2018Birds of a feather flock together\u2019, therefore, clusters are important components in these attributed networks. In general, clustering aims to partition nodes in networks into disjoint clusters. In this way, nodes in the same cluster are not only more densely connected than nodes outside the cluster, but also the attribute similarity of a pair of nodes in the cluster is greater than that of a node pair in different clusters. Therefore, identifying these clusters is an effective means to understand the underlying functions of systems that attributed networks represent.\nClustering has been studied extensively in data mining and machine learning fields over the past decades. Furthermore, with the development of deep learning in recent years, many deep models [4, 5, 6] have been developed to promote the capacities of traditional clustering methods via neural networks. Since deep learning models [2, 7] have the advantage of leveraging high-dimensional nonlinear features and highrelational features from nodes, they are becoming more and more resilient to high sparsity networks and attract considerable attention for node clustering, especially for large-scale networks.\nThe existing deep models for attributed network clustering can be roughly divided into two categories. The first one utilizes a network embedding method to obtain low-dimensional representation vectors with deep learning techniques [8, 9, 10], then adopts a typical clustering method, such as K-means [11] or spectral clustering [12], to obtain node clusters. We call this kind of methods \u201ctwo-stage methods\u201d. However, they highly depend on the representation learning ability of deep models, and the learned representation may not be good for the task of node clustering, although they can also be directly used for other downstream tasks such as node classification and link prediction [8, 9]. In contrast, the second class of deep embedded clustering (DEC) methods directly integrate the objectives of clustering and representation learning into a unified framework [2, 13, 14]. We call this kind of methods \u201cone-stage methods\u201d. Through joint optimization, DEC models are able to simultaneously learn node representations and the corresponding cluster structure, thereby gaining benefits and promoting each other. The existing studies indicate that one-stage end-to-end methods for node clustering are better than two-stage methods [2, 13].\nRecently, various DEC models [4, 5, 15] have been developed and arouse a lot of interest among researchers. However, most of the existing DEC models are designed to process image data, which can not be used to process attributed networks directly. Moreover, a given image dataset can be transformed into an attributed network by constructing a kNN (k-nearest neighbor) graph for obtaining global structure information, while keeping the image features as node attributes. Thus, it is necessary to further develop\nDEC models to capture cluster structure implied in network topology and node attributes while obtaining node representations for an attributed network.\nSince graph convolutional networks (GCN) provide a way to combine link structures and node features smoothly [1], some recent DEC models employ GCN as the backbone to make up for the deficiency of the previous methods that ignore topology information and achieve better clustering performance for attributed networks [2, 13, 14, 16]. DAEGC [2] utilizes graph attention network (GAT) [17] to form an autoencoder deep clustering framework, but lacks the reconstruction of node attributes which is a useful technique in self-supervised learning. SDCN [13] fuses autoencoder and GCN to learn node representations and hidden cluster distributions in a self-supervised way. However, it ignores to reconstruct the network topology. We believe that reconstructing the original network (including both topology and node attributes) will be good for learning node representations. Meanwhile, it is worth investigating how to project the hidden vectors learned by the network into cluster distributions and maintain the actual cluster structure. Corresponding to the consistency of multi-view clustering, the cluster structure of network topology and that of node attributes should be consistent for an attributed network. We suggest that the consistency constraint of cluster distributions obtained in the DEC process from different views is more robust than (or can be the substitute for) fusing two representation vectors before clustering.\nBased on these observations, in this study, we propose an end-to-end deep embedded clustering model named DCP-DEC (Distribution Consistency Preserving Deep Embedded Clustering) for attributed networks. This model aims at utilizing network structure and node attributes to learn more comprehensive node representations and simultaneously achieve better clustering performance. In the DCP-DEC model, we reconstruct a given network with a graph autoencoder (GAE) model where the backbone is still GCN. We keep DNN autoencoder in [13] to encode and decode node attributes. Once we obtain the node representation vectors from the above two autoencoders, we use them to obtain soft cluster assignments. Meanwhile, the KL-based clustering losses are then formed from two views in the means of self-supervised deep embedded clustering mechanism for network topology and node attributes. For maintaining the latent distribution consistency of the obtained two cluster distributions, we further introduce a consistency constraint to the proposed model. The experimental studies demonstrate that the proposed model DCP-DEC is able to achieve our goal.\nThe main contributions of this study are summarized as follows.\n\u2022 We propose an end-to-end deep embedded clustering model for attributed networks, which exploits\nGAE and AE to learn node representations and cluster assignments simultaneously from network topology and node attributes.\n\u2022 We further introduce a distribution consistency constraint to maintain the latent consistency of the\ntwo cluster assignments.\n\u2022 Extensive experiments are carefully designed and conducted on various datasets, and the results demon-\nstrate that our DCP-DEC model is highly competitive and even better than the SOTA methods.\nThe rest of this study is organized as follows. In section 2, we introduce the related works. The proposed deep embedded clustering model is presented in Section 3. In Section 4, we carefully analyze the performance of our model on several datasets compared with the state-of-the-art methods. Finally, the conclusion is drawn in section 5."
        },
        {
            "heading": "2. Related works",
            "text": ""
        },
        {
            "heading": "2.1. Attributed network embedding",
            "text": "Since attributed networks contain richer available information than pure networks, many attributed network embedding methods have been proposed [18, 19, 20]. They utilize both information from topological structure and node attributes to learn more discriminative representations while capturing properties hidden in attributed networks.\nOne type of the representative attributed network embedding method is based on random walk. TriDNR [21] exploits the inter-node relationship of a given node in a random walk sequence, and also captures the node-word correlation and label-word correspondence. Since [22] introduces the attributed random walk framework, it serves as a basis for generalizing many existing methods, such as DeepWalk [23] and node2vec [24], to attributed network embedding. The model proposed in [25] proposes a biased random walk between topology neighbors and attribute neighbors to get node representations.\nIn recent years, self-supervised deep models such as autoencoders and contrastive learning models [26] have played an important role in attributed network embedding tasks. For instance, DANE [8] employs an autoencoder to capture the non-linearity and proximity of link structures and node attributes. Furthermore, GAE/VGAE (Variational GAE) [27] extends AE/VAE (Variational AE) [28] to attributed networks to learn node representations using GCN as the encoder, and reconstructs the adjacency relations of nodes. Moreover, recent breakthroughs in contrastive learning shed light on the potential of discriminative models for representation learning. GraphCL [29] learns node embeddings by maximizing the similarity between the representations of two randomly perturbed versions of the intrinsic features and link structures of the same node\u2019s local sub-graph. GRACE [30] generates two views by jointly corrupting both topological relations and node attributes, and learns node representations by maximizing the agreement of these two views at the node level.\nExcept for the self-supervised learning framework mentioned above, GCN-based encoding models, which take both network topology and node attributes into consideration, are conveniently used in attributed network embedding. GraphSAGE [31] leverages a set of aggregator functions that learn to aggregate node attribute information from a node\u2019s local neighborhood to efficiently generate the embedding of the node.\nAGE [10] first applies a carefully-designed Laplacian smoothing filter, then employs an adaptive graph encoder that iteratively strengthens the filtered features to get better node embeddings in attributed networks.\nAll these attributed network embedding methods devote to getting powerful node representations, then other downstream tasks such as node clustering, node classification, and link prediction can be carried out directly on the learned representation vectors. However, the learned node vectors may not be the best for the task of node clustering. It is the concern of this study."
        },
        {
            "heading": "2.2. Deep embedded clustering",
            "text": "Deep embedded clustering models gradually come into the field because they can jointly optimize node clustering and representations. As a pioneer, the DEC method [4] trains an autoencoder to learn representation vectors and defines a clustering loss based on them to obtain clusters. It provides a way to learn node representations and cluster assignments simultaneously with a shared encoder. IDEC [5] further extends the DEC method [4] to integrate the reconstruction loss of AE and the clustering loss of DEC into a unified objective, training the model in an end-to-end framework. Subsequently, a number of approaches for deep spectral clustering and deep subspace clustering inspired by them also begin to emerge [6, 32, 33].\nHowever, the above methods cannot be used to process attributed networks directly since they ignore the link structures of data. To solve this limitation, a few methods which inherit the advantages of GCNbased models have been designed and used for DEC tasks for attributed networks. For example, DAEGC [2] employs GAT network [17] as the encoder to rank the importance of attributed nodes within a neighborhood and learn node representations. It is able to supervise the process of clustering to get clusters in the means of self-training for attributed networks. [34] extends the encoder of DAEGC to GCN to further aggregate the neighbor information of nodes in the network. NEC [16] introduces a relaxed soft modularity that can be optimized with the reconstruction loss of GAE. Furthermore, it combines them with the KL divergence based clustering loss, thus improving cluster assignments and feature representations in a self-learning manner. The model proposed in [35] incorporates a joint clustering model into the graph encoder. It fuses relaxed Kmeans and spectral clustering to get clusters, and the adjacency is shared by both GAE and joint clustering. Besides, SDCN [13] first combines GCN and DNN autoencoder with a delivery operator and then designs a dual self-supervised mechanism for clustering to get the final cluster assignments through joint training. However, it directly projects the representation vectors of GCN into cluster distributions by a subsequent GCN layer without the real clustering process, thus may lead to poor clustering performance. Based on SDCN, DFCN in [36] fuses two representation vectors learned from IGAE on an attributed graph and AE on node attributes, then applies triplet self-supervised clustering on the learned vectors to preserve the cluster structures in the embedding space. What\u2019s more, after the learning process of DFCN, K-means clustering is used again on the final fused representation vectors to get the cluster assignments. [37] combines a variational graph autoencoder for network embedding with a self-training mechanism into a unified framework, enabling\nthe better clustering of networks. Recently, O2MAC [38] extends DEC models to multi-view attributed networks. It consists of one encoder and multiple decoders to learn the shared representations. Furthermore, a self-training clustering objective is also designed to optimize node embeddings and clusters simultaneously. In this study, we concentrate on this branch of research and intend to design a more powerful one-stage end-to-end model to learn node representations effectively and efficiently for the task of node clustering."
        },
        {
            "heading": "3. Proposed methodology",
            "text": ""
        },
        {
            "heading": "3.1. Notations and problem definition",
            "text": "In this study, an undirected attributed network is represented as a graph G = (V ;E;X), where |V | = n consists a set of n nodes V = {v1, v2, . . . , vn}, and E represents a set of edges. The topological structure of G is specified by an adjacency matrix A \u2208 Rn\u00d7n. If there is an edge between node vi and vj (i.e., eij = (vi, vj) \u2208 E), then aij = 1, otherwise aij = 0. Furthermore, X \u2208 Rn\u00d7m represents the attribute matrix, where m is the dimension of attributes. Each row of X describes the attributes of node vi, which can be discrete binary values or continuous real-values.\nGiven an attributed network G, deep embedded clustering aims to learn low-dimensional node representations while clustering the network into groups. The two tasks are expected to be optimized jointly in an end-to-end unified framework, and promote each other during the training process. Specifically, our purpose is to learn a map function f : (A,X) 7\u2192 Z \u2208 Rn\u00d7d, zi is the i-th row of Z, which represents the latent representations of node vi, and the dimension of zi is d, generally d < m & d < n. Meanwhile, we intent to partition the attributed network into K disjoint groups (G1, G2, . . . , GK) such that nodes in the same group are not only more densely connected than nodes outside the group, but also the attribute similarity of a pair of nodes in the group is greater than that of a node pair with only one in the group."
        },
        {
            "heading": "3.2. Overview of the framework",
            "text": "In this section, we will introduce our end-to-end deep embedded clustering model DCP-DEC (Distribution Consistency Preserving Deep Embedded Clustering) for attributed networks. Fig. 1 illustrates the framework of our proposed model. It mainly consists of three parts: autoencoder (AE) based DEC module for node attributes, graph autoencoder (GAE) based DEC module mainly for network topology, and distribution consistency constraint to balance and fuse the above two modules.\n\u2022 AE-based DEC module. This module adopts DNN to encode and decode node attributes to learn\nrepresentations of nodes. With the obtained vectors, we get the soft cluster assignment for each node through Student\u2019s t-distribution and then correspondingly construct a target distribution. During the training, we minimize the reconstruction loss of the decoder and the clustering loss in a unified way, where the clustering loss is characterized by KL divergence between the target distribution and the\nsoft assignment. Therefore, in this step, we only use attribute information of nodes to get clustering results and node representations.\n\u2022 GAE-based DEC module. In this module, we fuse network structure and node attributes together\nto learn latent representations using the GCN encoder, then reconstruct link connections via an inner product decoder to guide the learning process. As is well known, network topology is usually very sparse, so nodes attributes are very helpful to guide the reconstruction of network structures. The clustering process is conducted in the same way as the above AE-based DEC module to get cluster assignments in this view. Obviously, in this module, we use network topology as the main information source and attribute information as the complementary.\n\u2022 Distribution consistency constraint. From the above two modules, we have obtained two cluster\nassignments using different information and different learning processes. Based on the assumption of consistency for data in different views, we introduce a distribution consistency constraint to maintain the consistency of the cluster structure of node attributes and that of network topology (enhanced by the node attributes). By minimizing the KL divergence between two cluster distributions from the\nabove two modules, we can obtain a more robust and smooth cluster structure, thus better fusing two different views for the task of attributed network clustering."
        },
        {
            "heading": "3.3. DCP-DEC model",
            "text": ""
        },
        {
            "heading": "3.3.1. AE-based deep embedded clustering",
            "text": "AE-based module only encodes node attributes to learn node representations, and uses a decoder to reconstruct the input. In this way, various autoencoders and their variants can be used, including variational autoencoder [28], denoising autoencoder [39], and stacked autoencoder [40], etc. In this study, we simply use a fully connected DNN as an encoder to map the attributes of each node to a nonlinear low-dimension latent representation vector. The encoder can be formulated in the following.\nZ(l)a = \u03c6(W (l) e Z (l\u22121) a + b (l) e ) (1)\nwhere l \u2208 {1, 2, . . . , L} is the number of layers for DNN, Z(0)a = X is the initial input of node attributes, W (l) e and b (l) e are weights and bias of the l-th layer in the encoder, and \u03c6 is the activation function, such as ReLU or Sigmoid.\nThen a decoder follows after the encoder. It employs the representation of the last layer in the encoder\nto reconstruct the input attributes. Also, there are L fully connected layers, which are formulated below.\nZ\u0302(l)a = \u03c6(W (l) d Z\u0302 (l\u22121) a + b (l) d ) (2)\nwhere l \u2208 {1, 2, . . . , L}, W(l)d and b (l) d are weights and bias of the l-th layer in the decoder, respectively, and Z\u0302 (0) a = Z (L) a . The output of the decoder is the reconstructed attributes X\u0302 = Z\u0302 (L) a . Thus, the reconstruction loss of the autoencoder is defined as:\nLare = 1\n2N \u2016X\u2212 X\u0302\u2016\n2 2 (3)\nOnce we obtain the latent representation Z (L) a \u2208 Rn\u00d7d from the attributes of nodes, we can utilize it to generate the soft cluster assignment Qa \u2208 Rn\u00d7K , where qaik obtained by Eq. 4 represents the probability that node vi is assigned into the cluster k. Since q a ik measures the similarity between the node representation zai and clustering center \u00b5 a k through Student\u2019s t-distribution as a kernel, once nodes are closer to the cluster center, the corresponding soft assignments get high probability and are more likely to be trustworthy. Then the target distribution Pa of nodes is successfully constructed through Eq. 5, which intends to put more emphasis on a data point that is assigned with high confidence to improve cluster purity. It is worth noticing that to get the initial centers \u00b5ak (k = 1, 2, \u00b7 \u00b7 \u00b7 ,K), we first pre-train the autoencoder through only minimizing the reconstruction loss to obtain the meaningful representation Za. After that, we run K-means on the learned representations for initializing, and in the following iterative training, the K-means clustering is never used again.\nqaik = (1 + \u2016zai \u2212 \u00b5ak\u20162)\u22121\u2211 j(1 + \u2016zai \u2212 \u00b5aj \u20162)\u22121\n(4)\npaik = qaik\n2/ \u2211\ni q a ik\u2211\nj(q a ij\n2/ \u2211\ni q a ij)\n(5)\nFollowing the previous studies [2, 4, 5, 13], we aim to force the current assignment Qa to approach the target distribution Pa, such that the final clusters can be obtained through jointly training autoencoder and cluster assignments. The clustering loss is defined according to the KL divergence between two distributions as below.\nLaKL = KL(Pa\u2016Qa) = \u2211 i \u2211 k paik log paik qaik\n(6)\nIn order to get reliable clusters, we minimize Eq. 6, when Qa reaches to idempotent, it closes to 0. At this condition, the cluster structure is clear and the entropy of the cluster distribution is minimal. This strategy is also called \u201cself-supervised\u201d, which is the main idea of deep embedded clustering [4].\nIn brief, AE-based deep embedded clustering module jointly optimizes node representations and the\ncluster structure using only node attributes. The whole objective can be formulated as:\nLAE = Lare + \u03b1 \u2217 LaKL (7)\nwhere \u03b1 \u2265 0 is a control coefficient that balances the reconstruction loss and the clustering loss."
        },
        {
            "heading": "3.3.2. GAE-based deep embedded clustering",
            "text": "As for attributed networks, not only node attributes can reflect the properties of clusters, but also the network topology indicates the relationships between nodes to form clusters. Moreover, previous study [41] has shown that if only use link connections to partition networks, the obtained cluster structure may not be good since the sparseness of network topology in the real world. Hence, the GAE-based module exploits both kinds of information to achieve deep embedded clustering. It complements the AE-based module from another view and is likely to help us obtain a more robust and reliable final cluster distribution, meanwhile getting powerful node representations.\nSpecifically, we employ two-layer GCN as an encoder to learn node representations since it has a powerful ability to retain the information in an attributed network [42, 43]. In general, the layer-wise transformation of the encoder can be expressed as below.\nZ(l)g = \u03c6(D\u0303 \u2212 12 A\u0303D\u0303\u2212 1 2Z(l\u22121)g W) (8)\nwhere A\u0303 = A + I, represents the topology of a network with a self-loop on each node, I and D\u0303 are the identity diagonal matrix and degree matrix of nodes respectively, where D\u0303ii = \u2211 j a\u0303ij . In addition, Z (0) g = X means that node attributes are used as the initial representation vector for each node, W is the trainable weight matrix, \u03c6 is the ReLu activation function.\nIn order to preserve the original network structure and make the learned representations more discriminative, we also introduce a decoder to guide the learning process. In the literature, there are various kinds\nof decoders, which reconstruct either the graph structure [9, 27], node attributes [7, 44], or both of them [45, 46]. As GCN views network topology as the main information and nodes with similar attributes tend to link together, we use the inner product decoder to reconstruct the relationships between nodes. This works as a counterpart to the former AE-based module and is formulated in the following.\nA\u0302 = Sigmoid(Z(L)g Z (L) g\nT ) (9)\nSimilarly, by minimizing the reconstruction error of the GAE in Eq. 10, we can get another node repre-\nsentation vector Z (L) g .\nLgre = 1\n2N \u2016A\u2212 A\u0302\u2016\n2 2 (10)\nAfter getting node representations from GAE, we can perform the same clustering operation as the AE-based DEC module of node attributes. Similar to Eqs. 4 and 5, we utilize Z (L) g to generate the soft cluster assignment Qg and target distribution Pg, where the initial cluster centers \u00b5 g k (k = 1, 2, \u00b7 \u00b7 \u00b7 ,K) are obtained by running K-means on the representation of the pre-trained GAE. The pre-training process is the same for AE and GAE models, we will describe it in detail in the experiments section. Then, we iteratively update node representations and nodes\u2019 clusters by learning from the high confidence assignments through optimizing the following clustering loss.\nLgKL = KL(Pg\u2016Qg) = \u2211 i \u2211 k pgik log pgik qgik\n(11)\nThe same as before, in order to learn better node representations for the task of clustering, we combine the reconstruction loss and the clustering loss together for this GAE-based module. The objective function in this part is defined as below.\nLGAE = Lgre + \u03b2 \u2217 LgKL (12)\nwhere \u03b2 \u2265 0 is a factor that controls the balance between the two losses. Therefore, we can refine cluster assignments and simultaneously obtain node representations by jointly training.\nAlthough the AE-based and GAE-based DEC modules apply the same idea and look similar, the information they have contained is different. Obviously, the major difference between these two modules is that AE-based module takes node attributes as its information sources, while GAE-based module takes network topology as the major information and node attributes as auxiliary. Note that if a GCN model has many convolutional layers, the output features may be over-smoothed and vertices from different clusters may become indistinguishable, thus the clustering performance will be affected [13, 43]. Besides, multiple layers of GCN will increase the computational complexity. However, shallow GCNs with only two or three layers may not have the ability to fully mine the information hidden in an attributed network. Fortunately, it has been proved that fusing other information such as node attributes to GCN is able to strengthen GCN\u2019s learning ability. Therefore, in this study, we use shallow GCN and take node attributes as complementary, such that our model is able to efficiently learn effective node representations and cluster assignments."
        },
        {
            "heading": "3.3.3. Distribution consistency constraint",
            "text": "As mentioned above, we can obtain two node representations and cluster distributions from two different deep embedded clustering modules, AE-based DEC module and GAE-based DEC module. Under the assumption of cluster consistency for data objects in different views, the cluster structure of network topology and that of node attributes should be consistent for an attributed network. Therefore, we propose a clustering distribution consistency constraint to make the assignments of clusters learned from the above two DEC modules as consistent as possible. We believe that this constraint is more robust to get good clusters or can be the substitute for fusing two representation vectors before clustering.\nWe employ KL divergence to measure the difference between two soft assignments Qg and Qa in Eq. 13. Since Qg is obtained from the GAE module, which exploits both topology and node attributes, we believe that Qg contains richer information than Qa that only considers attributes. We utilize Qg as the guidance to form the KL constraint. That is to say, we encourage the cluster distribution learned from the AE-based module to match that learned from the GAE-based module.\nLcon = KL(Qg\u2016Qa) = \u2211 i \u2211 k qgik log qgik qaik\n(13)\nDuring minimizing the KL divergence between Qg and Qa, the consistency constraint leads the two cluster distributions to be similar and makes the model satisfy the cluster consistency assumption. In this way, the representations of nodes learned from two DEC modules are updated iteratively under the guidance of this constraint, forming a unified end-to-end framework."
        },
        {
            "heading": "3.3.4. Overall objective function and optimization",
            "text": "To sum up, the above three parts cooperate with each other to jointly optimize the process of learning node representations and clustering nodes in networks. The overall loss function of the DCP-DEC model is defined as:\nL = LAE + LGAE + \u03b3 \u2217 Lcon (14)\nThese three items correspond to the three parts mentioned above. The first is the loss of AE-based module, the second is that of GAE-based module and the last is the distribution consistency constraint. In addition, \u03b3 \u2265 0 is the coefficient controlling the weight of consistency constraint to balance the whole loss during training. We expect to minimize Eq. 14 of a given attributed network via stochastic gradient descent in back-propagation, and obtain better cluster assignments meanwhile learning discriminative node representations. The whole learning process of the proposed DCP-DEC model is shown in Algorithm 1.\nIt is worthy of mentioning that the newly proposed model DCP-DEC outputs the final clustering result directly from the cluster assignment Qg, which matches our one-stage strategy. Namely, the cluster ci for node vi can be estimated as follows.\nci = arg max k\nqgik (15)\nAlgorithm 1 Distribution Consistency Preserving Deep Embedded Clustering Input: An attributed network G with adjacent matrix A and attribute matrix X, the number of clusters\nK, and the maximum number of iterations MaxIter.\nOutput: Clustering results C.\n1: Train AE and GAE by minimizing Eqs. 3 and 10 to get the initial parameters of AE and GAE and\nobtain their initial representations Za and Zg; {Pre-training}\n2: Run K-means on Za and Zg to get initial cluster centers \u00b5 a and \u00b5g, respectively. {Pre-training} 3: for iter \u2208 {0, 1, \u00b7 \u00b7 \u00b7 ,MaxIter} do {Training} 4: Learn node representations Za and Zg via Eqs. 2 and 8; 5: Compute soft assignments Qa and Qg of two modules; 6: Compute target distributions Pa and Pg, correspondingly; 7: Calculate loss of AE-based and GAE-based modules LAE and LGAE via Eqs. 7 and 12; 8: Calculate distribution consistency constraint Lcon via Eq. 13; 9: Update the whole model by minimizing Eq. 14;\n10: end for 11: return C, where ci = arg max k qgik(i = 1, \u00b7 \u00b7 \u00b7 , n; k = 1, \u00b7 \u00b7 \u00b7 ,K)."
        },
        {
            "heading": "4. EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "4.1. Experiment settings",
            "text": ""
        },
        {
            "heading": "4.1.1. Datasets",
            "text": "We validate the proposed model on eight real-world datasets and several artificial attributed networks,\nthe detail information of these networks are summarized in Table 1 and Table 2, respectively.\nThe real-world datasets have five attributed networks, including three popular citation networks1 (i.e., Citeseer, Cora, and PubMed, where nodes represent documents and each document is described by a 0/1- value word vector for Citeseer and Cora and a TF/IDF weighted word vector for PubMed), one paper network (i.e., ACM2, where there is an edge between two papers if they are written by the same author and the node features are represented by the bag-of-keywords in each paper), and one author cooperation network (i.e., DBLP3, where the features of each author are characterized by the bag-of-keywords related to this author). Besides, the real-world datasets contain three non-graph datasets, including USPS [47], HHAR [48], and Reuters [49]. In detail, USPS is a gray-scale handwritten digit image dataset, where the features\n1https://linqs.soe.ucsc.edu/data 2https://dl.acm.org/ 3https://dblp.uni-trier.de/\nof each image are described by the gray value of pixel points in the image. HHAR contains sensor records from smart phones and smart watches, where the features of each record are composed of commonly used time and frequency features. Reuters is a text dataset containing around 810,000 English news stories. As usual [4], for the task of text clustering, we select 4 root categories: corporate/industrial, government/social, markets and economics, and sample a random subset with 10,000 stories where each story is characterized by 2,000 most frequent words with their TF/IDF values. For each of these three non-graph datasets, we construct a kNN graph, and combine its original features to form the corresponding attributed network. Following the previous studies [13, 36], we choose k with the best performance for each dataset. The selected number of edges for the constructed kNN graph of each dataset is also shown in Table 1.\nThe artificial attributed networks are mainly generated by LFR benchmark [50] with the settings showed in Table 2, where N is the number of nodes, \u00b5 is the mixing parameter, \u3008k\u3009 is the average degree of nodes, kmax is the maximum degree of nodes, Cmin and Cmax are the minimum and maximum cluster sizes, \u03b3 and \u03b2 are exponents of the power-law distribution of node degree and cluster size, respectively. Among these, \u00b5 is designed to control the clearness of cluster structure in a network. Each node shares a fraction 1 \u2212 \u00b5 of its links with other nodes in its cluster and a fraction \u00b5 of its links with the other nodes in the network. Thus, the smaller \u00b5, the clearer cluster structure will be. Since previous studies have shown that almost all methods can achieve very good performance when network structure is clear, we only focus on the scenarios where the cluster structure is unclear, i.e., \u00b5 = {0.6, 0.7, 0.8}. In addition, under each parameter setting, in order to avoid randomness, we randomly generate 10 networks and report their average performance in subsequent experiments.\nFor constructing synthetic attributed networks, we generate a m-dimensional binary attribute vector for each node according to its ground-truth label generated by the LFR benchmark, where the same D(D < m)\nattributes are assigned to nodes in the same cluster. In this way, nodes can be perfectly grouped into ideal clusters in terms of attributes. In our experiments, we set m = 100 and D = 10 for testing high dimensional attributes. On the other hand, in order to verify the importance of node attributes, we add noise to blur the attribute cluster structure, which can be mimicked by randomly flipping a portion of attributes of each node (we call it noise ratio). With the increase of the noise ratio, the clearness of cluster structure decreases. Here, we set the noise ratio from 10% to 40% with a step length of 10%. These artificial attributed networks can clearly show the clearness of the cluster structures of network topology and node attributes. The experimental results on these networks are conducive to explaining the performance of different models and are able to avoid the influence of human bias on labelling real-world data."
        },
        {
            "heading": "4.1.2. Evaluation metrics",
            "text": "The clustering performance is evaluated by four popular metrics [51]: Accuracy (ACC), Normalized Mutual Information (NMI), Average Rand Index (ARI), and F1-score (F1). As for these metrics, the larger value indicates the better clustering results. They are defined as follows.\nAccuracy (ACC) measures the correctly partitioned clusters C according to the ground-truth ones C\u2217.\nACC(C,C\u2217) =\n\u2211|C| i=1 \u03b4(Ci, C \u2217 i )\n|C| (16)\nwhere \u03b4(a, b) denotes the Kronecker function which equals to one if a = b and zero otherwise.\nNormalized Mutual Information (NMI) quantifies the similarity between the ground-truth clusters\nC\u2217 and the inferred ones C.\nNMI(C,C\u2217) = 2MI(C,C\u2217)\nH(C) +H(C\u2217) (17)\nwhere MI(C,C\u2217) measures the mutual information between clusters C and C\u2217, while H(C) and H(C\u2217) are the entropy of these two clusters, respectively.\nAverage Rand Index (ARI) is the enhancement clustering index of the rand index (RI), it is defined\nas follows.\nARI =\n\u2211 ij ( nij 2 ) \u2212 [ \u2211 i ( ni\u00b7 2 )\u2211 j ( n\u00b7j 2 ) ]/ ( N 2 ) 1 2 [ \u2211 i ( ni\u00b7 2 ) + \u2211 j ( n\u00b7j 2 ) ]\u2212 [ \u2211 i ( ni\u00b7 2 )\u2211 j ( n\u00b7j 2 ) ]/ ( N 2 ) (18)\nwhere N is the number of nodes, nij represents the number of nodes belonging to both cluster i and cluster j, ni\u00b7 and n\u00b7j are the numbers of nodes in cluster i and cluster j, respectively.\nF1-score (F1) is the harmonic mean of Precision and Recall. Let T denotes the set of nodes in the ground-truth clusters and S denotes the set of nodes assigned by a given algorithm in the corresponding clusters. | \u00b7 | denotes the cardinality of a set. Then F1 can be formulated as below.\nF1 = 2\u00d7 precision\u00d7 recall precision+ recall\n(19)\nwhere precision = |S \u2229 T |/|S|, recall = |S \u2229 T |/|T |."
        },
        {
            "heading": "4.1.3. Baseline methods",
            "text": "We compare our model with several state-of-the-art network embedding methods and deep embedded clustering models for clustering benchmark attributed networks to give a comprehensive study. For network embedding approaches, we run K-means to get the final clustering results on the learned representation vectors. All the compared models are listed below.\n\u2022 AE: It is the basis of our AE-based module, it reconstructs the node attributes to get node represen-\ntations.\n\u2022 GAE [27]: It employs GCN as the encoder and reconstructs the link structure using the inner product\ndecoder, which is the basis of our GAE-based module.\n\u2022 DFCN [36]: It fuses the representations learned from two different views and combines triplet self-\nsupervised clustering loss with reconstruction loss, guiding the whole learning process.\n\u2022 DEC [4]: It uses a stacked autoencoder to learn node representations using only node attributes, and\nthen defines a clustering loss on the learned vectors to cluster the nodes.\n\u2022 IDEC [5]: It adds the reconstruction loss on the basis of the clustering loss of DEC, and optimizes\nthe unified objective function jointly.\n\u2022 DAEGC [2]: It utilizes a graph attention network to capture the importance of nodes and carries out\ndeep embedded clustering for attributed networks.\n\u2022 SDCN [13]: It introduces a delivery operator to combine the GCN module for attributed graphs and\nthe AE module for node attributes, then jointly optimizes node representations and clusters using a dual self-supervised module."
        },
        {
            "heading": "4.1.4. Implementation details",
            "text": "For the newly proposed DCP-DEC model, in order to jointly optimize the cluster assignments and node representations, we first pre-train AE and GAE models 50 epochs without the clustering part. That is to say, we simply minimize the reconstruction losses of Eqs. 3 and 10 in AE and GAE modules, respectively, as shown in step 1 of Algorithm 1. Consequently, we obtain the initial parameters and the initial representations of two parts. Subsequently, the K-means algorithm is then performed on two groups of generated representation vectors 20 times, respectively. For each representation vector Za and Zg, we minimize the standard Kmeans error, and then select the cluster centers \u00b5a and \u00b5g corresponding to the best clustering result in 20 runs for initialization. After that, the entire model is jointly trained with three parts for representation learning and clustering. We set the final dimension of representation vectors in two DEC modules as 64. As for the encoder of GAE-based module, we set the hidden layer dimension to 512 for ACM and DBLP, and 256 for other real-world datasets and artificial attributed networks. Furthermore, we tune the number of latent layers and the dimension of each layer for AE-based module on real datasets. Consequently, each real dataset has different settings for the AE-based module. For artificial networks, in order to fully extract attribute representations of nodes while keeping the simplicity of our model, we set the dimension of hidden layers to 256-64 or 512-256-64, which respectively correspond to the networks with clear attribute clusters (noise ratio is 10%) or unclear attribute clusters (noise ratio is 20%, 30%, or 40%). Besides, we use the grid search strategy to select the optimal combination of balance coefficients \u03b1, \u03b2, \u03b3, where each value ranges from 0 to 1. We use the Adam optimizer to train the whole model with the learning rate 10\u22123.\nFor a fair comparison, with the exception of DAEGC, we replicate the other methods according to the optimal settings reported in the original paper. Since DAEGC has no open source code, we directly report the results listed in the original paper for Citeseer, Cora, and PubMed networks, while for other real-world datasets, we use the results shown in SDCN. We run our model and all other methods 10 times and report the average results to avoid extreme cases."
        },
        {
            "heading": "4.2. Results and analysis",
            "text": ""
        },
        {
            "heading": "4.2.1. Performance of real-world datasets",
            "text": "In this section, we validate the clustering performance of our proposed model compared with other state-of-the-art methods listed above on eight real-world datasets. We also report the performance of the other three DCP-DEC variants. Instead of using assignments Qg to obtain final clusters, we utilize DCP Qa to represent the clustering results generated through cluster distribution Qa. What\u2019s more, we carry out K-means on two representations Z (L) g and Z (L) a learned from GAE and AE modules to obtain the other two groups of results, denoted as Zg clu and Za clu, respectively. All the results are shown in Table 3. Except for the three DCP-DEC variant results represented in the last three columns, the best results are highlighted\nin bold, and the sub-optimal ones are underlined. For the four versions of our DCP-DEC model, the best results are marked in italics.\non most benchmarks across the evaluation metrics. More specifically, we have the following observations.\n\u2022 The one-stage DEC models consistently outperform the compared network embedding methods for\nclustering. Although DFCN [36] looks like a two-stage method, it employs a triplet self-supervised strategy as the guidance for node clustering during the process of representation learning, which uses the same idea as one-stage end-to-end models. This verifies that with the explicit clustering constraint, the learned representation vectors can produce better clustering results.\n\u2022 For deep embedded clustering models, DEC [4] and IDEC [5] only utilize node features, so their\nperformance is worse than other methods that fuse topological and attribute information. This also proves the effectiveness of considering two kinds of information in attributed networks comprehensively.\n\u2022 Compared with the recently proposed methods, our model shows significant improvements over SDCN\n[13]. This strongly verifies our two intuitions. 1) Reconstructing network topology is helpful. 2) The cluster distribution constraint is a good guidance to learn the representations of nodes and their cluster structure. Though our DCP-DEC model is inferior to DAEGC [2] on Cora and PubMed networks, and worse than DFCN [36] on HHAR, the performance is still competitive. Moreover, the DCP-DEC model is simple compared to DFCN.\n\u2022 As for the four variants of the DCP-DEC model, the results are satisfactory if we directly use the\ncluster assignments as the final clustering. In most cases, the cluster distributions of Qg and that of Qa tend to be consistent, which proves the effectiveness of the distribution consistency constraint. Meanwhile, the results of Qg are stronger than Qa, indicating that it is reasonable to select Qg as a guiding target. In addition, if we perform K-means on the learned representations, the model achieves even better results on some datasets such as PubMed and USPS, which shows that during our joint optimization, representation learning and clustering can promote each other."
        },
        {
            "heading": "4.2.2. Performance of artificial attributed networks",
            "text": "Since the newly proposed SDCN [13] and DFCN [36] have better clustering results than other methods, in this section, we further compare them with our newly proposed model DCP-DEC on artificial attributed networks to avoid the human bias on labelling datasets and give a more fair comparison. We report the clustering performance in Table 4, where we highlight the best results in bold, and LFR* represents attributed networks generated by the LFR benchmark with the noise ratio \u2217%. As mentioned before, DFCN further conducts K-means on the obtained node representation vectors to achieve clustering. We use \u2018Z\u2019 to represent these DFCN\u2019s results in Table 4. Meanwhile, \u2018Q\u2019 indicates that the soft cluster assignments updated during training are directly used as the clustering results of DFCN, which corresponds to the joint training mechanism in DEC models. \u2018DCP\u2019 represents our DCP-DEC model in Table 4 for space limitation.\nFrom Table 4, we can see that our proposed DCP-DEC model gets the best performance in most cases. Note that the cluster structure of a network is unclear when \u00b5 = 0.8, and the number of clusters in an artificial network is much larger than that in real networks. This increases the difficulty of clustering. However, our model still achieves significantly better performance compared with DFCN or SDCN.\nSpecifically, though SDCN introduces a delivery operator to integrate the autoencoder-specific representation into structure-aware representation of GCN, from our results, it may not effectively combine network topology and node attributes, thus its results are much worse than those of DPC-DEC even when the cluster structure of an attributed network is clear (for example, LFR10 with \u00b5 = 0.6). In addition, DCP-DEC is\ncomparable or even better than DFCN in most cases. We can draw the conclusion that our distribution consistency constraint between the two cluster assignments of DEC modules can well maintain the consistency of cluster structures obtained from topology and attributes. In other words, the cluster distribution consistency constraint is more robust or can be a substitute for fusing two representation vectors before clustering as DFCN does. However, once we use the soft cluster assignments of DFCN as final clustering results (the \u2018Q\u2019 column in Table 4), the performance decreases significantly. This means that DFCN cannot directly well reveal the cluster structure of a given attributed network like DCP-DEC. All the results in synthetic networks further demonstrate the effectiveness of our DCP-DEC model for clustering tasks."
        },
        {
            "heading": "4.3. Ablation study",
            "text": "In this section, we aim to prove the effectiveness of the distribution consistency constraint in the whole DCP-DEC model. Once we remove this constraint (i.e., \u03b3 = 0), there are two cluster distributions from two different deep embedded clustering modules. Therefore, we compare these two clustering results with the final results of the whole DCP-DEC model (showed in Fig. 2) on eight real-world datasets, where DCP-W/G and DCP-W/A respectively represent the clustering results of GAE-based module and AE-based module without the consistency constraint.\nAccording to Fig. 2, we have the following results. 1) The results of DCP-DEC model are significantly\nbetter than DCP-W/G and DCP-W/A. This indicates the effectiveness of the distribution consistency constraint used in DCP-DEC. 2) The results of DCP-W/A are usually better than DCP-W/G with the exception of DBLP network, verifying the importance of a deep autoencoder on node attributes. 3) The overall performance of DCP-DEC demonstrates that our framework takes advantage of network topology and node attributes, and provides an effective way to combine two cluster assignments obtained from two views. All in all, the distribution consistency constraint plays an important role in our whole model."
        },
        {
            "heading": "4.4. Visualization of training",
            "text": "In order to have a more intuitive understanding of the training process and the convergence of the model, we visualize the loss of the whole objective function and that of each part of the function in Fig. 3 (a)-(c). What\u2019s more, the curves of the evaluation results for each measurement based on the cluster assignments Qg and Qa over the training epochs are also shown accordingly in Fig. 3 (d)-(f). Since all the datasets have similar tendencies, we randomly take three real-world datasets, including ACM, DBLP, and USPS, as examples to illustrate and analyze the results.\nFrom Fig. 3, we can see that in the first few epochs, the total loss value and accuracy change rapidly. However, after several rounds of iterations, the loss decreases steadily and can quickly reach convergence. At the same time, the performance of the DCP-DEC model gradually increases and reaches the optimum when the training converges. With the help of distribution consistency constraint, the cluster assignments of GAE and AE modules gradually tend to be consistent, illustrating the importance of end-to-end joint optimization in deep embedding clustering."
        },
        {
            "heading": "4.5. Parameter sensitivity analysis",
            "text": "To show the clustering performance of the proposed model, we also study the sensitivity of the parameters\nused in the experiments."
        },
        {
            "heading": "4.5.1. Effect of the final dimension",
            "text": "First of all, we examine the effect of the final dimension d of representation vectors in two DEC modules. In detail, we set d = {16, 32, 64, 128, 256} and carry out the experiments from three aspects. 1) Fix the final dimension d of GAE-based module as 64, then change the dimension of the AE-based module; 2) Inversely, fix d of the AE-based module, and change that of the GAE-based module; 3) Change the dimensions of both modules simultaneously, but keep the two dimensions be identical. Fig. 4 shows the results of Citeseer and ACM as examples, where \u2018AE-c & GAE-f\u2019, \u2018GAE-c & AE-f\u2019, and \u2018AE-c & GAE-c\u2019 correspond to the above three cases. For simplicity, we only show the performance of ACC and NMI, and represent them with solid and dashed lines respectively.\nFrom Fig. 4, we can see that only changing the dimension of the GAE-based module has little impact on the clustering performance in most cases, indicating the GAE-based module is robust to d. Besides, once we change the final dimension d of representation vectors of AE-based module or both of the two modules, the clustering performance rises when d enlarges from 16 to 64. However, when d > 64, the accuracy begins\nto drop significantly. This is because a high dimensionality may introduce redundant and noisy information that can harm the clustering performance. Therefore, the dimension of representation vectors in the AEbased module for node attributes has a greater impact on the final clustering performance. Thus, we choose 64 as the final representation dimension to extract useful information hidden in attributed networks."
        },
        {
            "heading": "4.5.2. The number and the dimensions of hidden layers",
            "text": "In this section, we study the impact of the number and the dimensions of hidden layers. Since the GAE-based module only utilizes two hidden layers, we focus on testing the performance of different settings for the AE-based module. Here we take USPS as an example. The validation process for other datasets is similar. Table 5 presents the clustering results when the number of hidden layers changes from two to five. Once the number of hidden layers is given, we also compare the impact of the different dimensions of the layers on clustering performance. In Table 5, the best performance is highlighted in bold, and the underlined results indicate the sub-optimal results for other settings.\nAccording to Table 5, we can draw the conclusion that with the increase of the number of layers, the clustering performance steadily improves. Among the results in Table 5, cases {1), 3), 6)} and {1), 4), 7), 9), 10)} are obvious examples. In addition, when we set the same number of layers, the larger the dimension of the hidden layer, the more significant the clustering performance improves. This means that higher dimensionality can encode more useful information, thus benefiting the joint optimization. As a result, we use the setting of case 10) for USPS network.\nIn our experiments, we carefully tune the number of hidden layers and the dimension of each layer for\neach real-world dataset. We report the result with the best performance among all testing cases."
        },
        {
            "heading": "4.5.3. Influence of balance coefficients",
            "text": "In this section, we verify the tendency of the final clustering performance as the balance coefficient changes (including the weights of clustering loss of two modules \u03b1 and \u03b2, the coefficient of distribution consistency constraint \u03b3). We also take three real networks, i.e., ACM, DBLP, and USPS as examples, and present the grid search results on these networks in Fig. 5 to illustrate the influence of the coefficients, where the figures in the first and the second row correspond to ACC and NMI metrics, respectively.\nFrom Fig. 5, we can see that for different networks, different balance coefficients of each part of the loss have different effects on the overall clustering performance, but in most cases, a variety of coefficient combinations can obtain better clustering structures. More specifically, once we fix \u03b1 and \u03b2, as \u03b3 increases, the clustering performance improves gradually. This well verifies the importance of the distribution consistency constraint for combining two DEC modules in DCP-DEC. Meanwhile, although it seems that the choice of \u03b1 and \u03b2 has relative sensitivity, except for the case of \u03b1 = \u03b2 = 0, a suitable combination will further improve the quality of clusters. This shows again that the introduction of clustering losses in the two DEC modules is useful. At last, the clustering results change little when the value of \u03b2 changes from 0 to 1, regardless of the other two coefficient values (\u03b3 > 0). The performance on ACM network is a clear example. In our experiments, we balance the influence of each part and choose the combinations of coefficients with the best cluster distribution for different networks. In summary, both the KL-based clustering losses formed from network topology and node attributes and the constraint of distribution consistency play an indispensable role during the joint optimization of the whole model."
        },
        {
            "heading": "4.5.4. Complexity analysis",
            "text": "In this section, we analyze and compare the efficiency of the proposed model with all baseline methods in terms of their time complexities and computational costs. Following the settings of the original paper, except that DEC and IDEC models adopt the early stopping strategy, we run all other models for 200 epochs and report the training time in Table 6.\nIn addition, we list the time complexity of each model for a clearer comparison in Table 6. Where n and |E| denote the number of nodes and edges in a graph, d is the maximum dimension of hidden layers, k represents the number of clusters, and t denotes the number of iterations of K-means clustering. Taking the DCP-DEC model as an example, the time complexity of the AE-based module is O(nd2). GAE-based module costs O(|E|d + dn2), which includes the GCN encoding and inner product decoding. Meanwhile, computing the cluster distribution takes O(nk). Therefore, the overall time complexity of the proposed DCP-DEC is O(nd2 + |E|d+ dn2 + nk). From Table 6, we can conclude that although our model does not explicitly pursue computational efficiency during the training, it is also competitive in computational time compared to other algorithms while maintaining the accuracy for clustering tasks."
        },
        {
            "heading": "5. Conclusion",
            "text": "In this study, we propose an end-to-end deep embedded clustering model DCP-DEC for attributed networks. Our model consists of three parts: AE-based DEC module, GAE-based DEC module, and distribution consistency constraint. The former two modules make full use of network topology and node attributes to learn node representations and cluster assignments via a self-supervised strategy. After that, the distribution consistency constraint characterized by KL divergence is applied to the above two assignments to maintain the latent consistency of two cluster distributions in two DEC modules. By minimizing the overall objective function, we can obtain node clusters and simultaneously learn discriminative node representations. Extensive experiments on various datasets demonstrate the effectiveness of the proposed model. In future work, we will explore more effective and efficient training strategies to jointly optimize the network embedding and node clustering. Meanwhile, it is also worth investigating the different importance of network topology and node attributes for graph clustering, and developing other fusion and distribution measurement strategies."
        },
        {
            "heading": "Acknowledgment",
            "text": "This work was supported by the National Key R&D Program of China (grant numbers 2017YFC1703506, 2018AAA0100302); and National Natural Science Foundation of China (grant numbers 61876016, 61632004)."
        }
    ],
    "title": "Deep Embedded Clustering with Distribution Consistency Preservation for Attributed Networks",
    "year": 2022
}