{
    "abstractText": "We consider the problem of contextual bandits and imitation learning, where the learner lacks direct knowledge of the executed action\u2019s reward. Instead, the learner can actively query an expert at each round to compare two actions and receive noisy preference feedback. The learner\u2019s objective is twofold: to minimize the regret associated with the executed actions, while simultaneously, minimizing the number of comparison queries made to the expert. In this paper, we assume that the learner has access to a function class that can represent the expert\u2019s preference model under appropriate link functions, and provide an algorithm that leverages an online regression oracle with respect to this function class for choosing its actions and deciding when to query. For the contextual bandit setting, our algorithm achieves a regret bound that combines the best of both worlds, scaling as O(min{ \u221a T , d/\u2206}), where T represents the number of interactions, d represents the eluder dimension of the function class, and \u2206 represents the minimum preference of the optimal action over any suboptimal action under all contexts. Our algorithm does not require the knowledge of \u2206, and the obtained regret bound is comparable to what can be achieved in the standard contextual bandits setting where the learner observes reward signals at each round. Additionally, our algorithm makes only O(min{T, d2/\u22062}) queries to the expert. We then extend our algorithm to the imitation learning setting, where the learning agent engages with an unknown environment in episodes of length H each, and provide similar guarantees for regret and query complexity. The regret bound for our imitation learning algorithm, which relies on preferencebased feedback, matches the prior results in interactive imitation learning (Ross and Bagnell, 2014) that require access to the expert\u2019s actions as well as reward signals. Furthermore, we show that our algorithm enjoys improved query complexity bounds. Interestingly, in some cases, our algorithm for imitation learning via preference-feedback can even learn to outperform the underlying expert thus highlighting a practical benefit of considering preference-based feedback in imitation learning.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ayush Sekhari"
        },
        {
            "affiliations": [],
            "name": "Karthik Sridharan"
        },
        {
            "affiliations": [],
            "name": "Wen Sun"
        },
        {
            "affiliations": [],
            "name": "Runzhe Wu"
        }
    ],
    "id": "SP:6d298ddf6100bfd9e262ae768e4272421f4f39a4",
    "references": [
        {
            "authors": [
                "Yasin Abbasi-Yadkori",
                "D\u00e1vid P\u00e1l",
                "Csaba Szepesv\u00e1ri"
            ],
            "title": "Improved algorithms for linear stochastic bandits",
            "venue": "Advances in neural information processing systems,",
            "year": 2011
        },
        {
            "authors": [
                "Pieter Abbeel",
                "Andrew Y Ng"
            ],
            "title": "Apprenticeship learning via inverse reinforcement learning",
            "venue": "In Proceedings of the twenty-first international conference on Machine learning,",
            "year": 2004
        },
        {
            "authors": [
                "Alekh Agarwal"
            ],
            "title": "Selective sampling algorithms for cost-sensitive multiclass prediction",
            "venue": "In International Conference on Machine Learning,",
            "year": 2013
        },
        {
            "authors": [
                "Alekh Agarwal",
                "Nan Jiang",
                "Sham M Kakade",
                "Wen Sun"
            ],
            "title": "Reinforcement learning: Theory and algorithms",
            "venue": "CS Dept., UW Seattle,",
            "year": 2019
        },
        {
            "authors": [
                "Nir Ailon",
                "Zohar Karnin",
                "Thorsten Joachims"
            ],
            "title": "Reducing dueling bandits to cardinal bandits",
            "venue": "In International Conference on Machine Learning,",
            "year": 2014
        },
        {
            "authors": [
                "Jean-Yves Audibert",
                "S\u00e9bastien Bubeck",
                "R\u00e9mi Munos"
            ],
            "title": "Best arm identification in multi-armed bandits",
            "venue": "In COLT, pages",
            "year": 2010
        },
        {
            "authors": [
                "Peter Auer",
                "Nicolo Cesa-Bianchi",
                "Yoav Freund",
                "Robert E Schapire"
            ],
            "title": "The nonstochastic multiarmed bandit problem",
            "venue": "SIAM journal on computing,",
            "year": 2002
        },
        {
            "authors": [
                "Alex Ayoub",
                "Zeyu Jia",
                "Csaba Szepesvari",
                "Mengdi Wang",
                "Lin Yang"
            ],
            "title": "Model-based reinforcement learning with value-targeted regression",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Viktor Bengs",
                "R\u00f3bert Busa-Fekete",
                "Adil El Mesaoudi-Paul",
                "Eyke H\u00fcllermeier"
            ],
            "title": "Preference-based online learning with dueling bandits: A survey",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2021
        },
        {
            "authors": [
                "Erdem Biyik",
                "Dorsa Sadigh"
            ],
            "title": "Batch active preference-based learning of reward functions",
            "venue": "In Conference on robot learning,",
            "year": 2018
        },
        {
            "authors": [
                "Ralph Allan Bradley",
                "Milton E Terry"
            ],
            "title": "Rank analysis of incomplete block designs: I. the method of paired comparisons",
            "year": 1952
        },
        {
            "authors": [
                "Daniel Brown",
                "Wonjoon Goo",
                "Prabhat Nagarajan",
                "Scott Niekum"
            ],
            "title": "Extrapolating beyond suboptimal demonstrations via inverse reinforcement learning from observations",
            "venue": "In International conference on machine learning,",
            "year": 2019
        },
        {
            "authors": [
                "Daniel S Brown",
                "Wonjoon Goo",
                "Scott Niekum"
            ],
            "title": "Better-than-demonstrator imitation learning via automatically-ranked demonstrations",
            "venue": "In Conference on robot learning,",
            "year": 2020
        },
        {
            "authors": [
                "R\u00f3bert Busa-Fekete",
                "Bal\u00e1zs Sz\u00f6r\u00e9nyi",
                "Paul Weng",
                "Weiwei Cheng",
                "Eyke H\u00fcllermeier"
            ],
            "title": "Preference-based reinforcement learning: evolutionary direct policy search using a preference-based racing algorithm",
            "venue": "Machine learning,",
            "year": 2014
        },
        {
            "authors": [
                "Nicolo Cesa-Bianchi",
                "G\u00e1bor Lugosi"
            ],
            "title": "Prediction, learning, and games",
            "venue": "Cambridge university press,",
            "year": 2006
        },
        {
            "authors": [
                "Nicolo Cesa-Bianchi",
                "G\u00e1bor Lugosi",
                "Gilles Stoltz"
            ],
            "title": "Minimizing regret with label efficient prediction",
            "venue": "IEEE Transactions on Information Theory,",
            "year": 2005
        },
        {
            "authors": [
                "Kai-Wei Chang",
                "Akshay Krishnamurthy",
                "Alekh Agarwal",
                "Hal Daum\u00e9 III",
                "John Langford"
            ],
            "title": "Learning to search better than your teacher",
            "venue": "In International Conference on Machine Learning,",
            "year": 2015
        },
        {
            "authors": [
                "Xiaoyu Chen",
                "Han Zhong",
                "Zhuoran Yang",
                "Zhaoran Wang",
                "Liwei Wang"
            ],
            "title": "Human-in-the-loop: Provably efficient preference-based reinforcement learning with general function approximation",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Ching-An Cheng",
                "Byron Boots"
            ],
            "title": "Convergence of value aggregation for imitation learning",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2018
        },
        {
            "authors": [
                "Paul F Christiano",
                "Jan Leike",
                "Tom Brown",
                "Miljan Martic",
                "Shane Legg",
                "Dario Amodei"
            ],
            "title": "Deep reinforcement learning from human preferences",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Wei Chu",
                "Zoubin Ghahramani"
            ],
            "title": "Preference learning with gaussian processes",
            "venue": "In Proceedings of the 22nd international conference on Machine learning,",
            "year": 2005
        },
        {
            "authors": [
                "Robert Cohn",
                "Edmund Durfee",
                "Satinder Singh"
            ],
            "title": "Comparing action-query strategies in semi-autonomous agents",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2011
        },
        {
            "authors": [
                "Varsha Dani",
                "Thomas P Hayes",
                "Sham M Kakade"
            ],
            "title": "Stochastic linear optimization under bandit feedback",
            "venue": "In 21st Annual Conference on Learning Theory,",
            "year": 2008
        },
        {
            "authors": [
                "Hal Daum\u00e9",
                "John Langford",
                "Daniel Marcu"
            ],
            "title": "Search-based structured prediction",
            "venue": "Machine learning,",
            "year": 2009
        },
        {
            "authors": [
                "Ofer Dekel",
                "Claudio Gentile",
                "Karthik Sridharan"
            ],
            "title": "Selective sampling and active learning from single and multiple experts",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2012
        },
        {
            "authors": [
                "Simon S Du",
                "Yuping Luo",
                "Ruosong Wang",
                "Hanrui Zhang"
            ],
            "title": "Provably efficient q-learning with function approximation via distribution shift error checking oracle",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Miroslav Dud\u0131\u0301k",
                "Katja Hofmann",
                "Robert E Schapire",
                "Aleksandrs Slivkins",
                "Masrour Zoghi"
            ],
            "title": "Contextual dueling bandits",
            "venue": "In Conference on Learning Theory,",
            "year": 2015
        },
        {
            "authors": [
                "Dylan Foster",
                "Alexander Rakhlin"
            ],
            "title": "Beyond ucb: Optimal and efficient contextual bandits with regression oracles",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Dylan Foster",
                "Alekh Agarwal",
                "Miroslav Dud\u0131\u0301k",
                "Haipeng Luo",
                "Robert Schapire"
            ],
            "title": "Practical contextual bandits with regression oracles",
            "venue": "In International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Dylan Foster",
                "Alexander Rakhlin",
                "David Simchi-Levi",
                "Yunzong Xu"
            ],
            "title": "Instance-dependent complexity of contextual bandits and reinforcement learning: A disagreement-based perspective",
            "venue": "In Conference on Learning Theory,",
            "year": 2021
        },
        {
            "authors": [
                "Dylan J Foster",
                "Satyen Kale",
                "Haipeng Luo",
                "Mehryar Mohri",
                "Karthik Sridharan"
            ],
            "title": "Logistic regression: The importance of being improper",
            "venue": "In Conference On Learning Theory,",
            "year": 2018
        },
        {
            "authors": [
                "Johannes F\u00fcrnkranz",
                "Eyke H\u00fcllermeier",
                "Weiwei Cheng",
                "Sang-Hyeun Park"
            ],
            "title": "Preference-based reinforcement learning: a formal framework and a policy iteration algorithm",
            "venue": "Machine learning,",
            "year": 2012
        },
        {
            "authors": [
                "Aur\u00e9lien Garivier",
                "Pierre M\u00e9nard",
                "Gilles Stoltz"
            ],
            "title": "Explore first, exploit next: The true shape of regret in bandit problems",
            "venue": "Mathematics of Operations Research,",
            "year": 2019
        },
        {
            "authors": [
                "Steve Hanneke",
                "Liu Yang"
            ],
            "title": "Minimax analysis of active learning",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2015
        },
        {
            "authors": [
                "Steve Hanneke",
                "Liu Yang"
            ],
            "title": "Toward a general theory of online selective sampling: Trading off mistakes and queries",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2021
        },
        {
            "authors": [
                "Jiafan He",
                "Dongruo Zhou",
                "Quanquan Gu"
            ],
            "title": "Logarithmic regret for reinforcement learning with linear function approximation",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Baihe Huang",
                "Jason D Lee",
                "Zhaoran Wang",
                "Zhuoran Yang"
            ],
            "title": "Towards general function approximation in zero-sum markov games",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Haque Ishfaq",
                "Qiwen Cui",
                "Viet Nguyen",
                "Alex Ayoub",
                "Zhuoran Yang",
                "Zhaoran Wang",
                "Doina Precup",
                "Lin Yang"
            ],
            "title": "Randomized exploration in reinforcement learning with general value function approximation",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Ashesh Jain",
                "Shikhar Sharma",
                "Thorsten Joachims",
                "Ashutosh Saxena"
            ],
            "title": "Learning preferences for manipulation tasks from online coactive feedback",
            "venue": "The International Journal of Robotics Research,",
            "year": 2015
        },
        {
            "authors": [
                "Tiancheng Jin",
                "Haipeng Luo"
            ],
            "title": "Simultaneously learning stochastic and adversarial episodic mdps with known transition",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Sham M Kakade",
                "Ambuj Tewari"
            ],
            "title": "On the generalization ability of online strongly convex programming algorithms",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2008
        },
        {
            "authors": [
                "Junpei Komiyama",
                "Junya Honda",
                "Hisashi Kashima",
                "Hiroshi Nakagawa"
            ],
            "title": "Regret lower bound and optimal algorithm in dueling bandit problem",
            "venue": "In Conference on learning theory,",
            "year": 2015
        },
        {
            "authors": [
                "Akshay Krishnamurthy",
                "Alekh Agarwal",
                "Tzu-Kuo Huang",
                "Hal Daum\u00e9 III",
                "John Langford"
            ],
            "title": "Active learning for cost-sensitive classification",
            "venue": "In International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "John Langford",
                "Tong Zhang"
            ],
            "title": "The epoch-greedy algorithm for multi-armed bandits with side information",
            "venue": "Advances in neural information processing systems,",
            "year": 2007
        },
        {
            "authors": [
                "Michael Laskey",
                "Sam Staszak",
                "Wesley Yu-Shu Hsieh",
                "Jeffrey Mahler",
                "Florian T Pokorny",
                "Anca D Dragan",
                "Ken Goldberg. Shiv"
            ],
            "title": "Reducing supervisor burden in dagger using support vectors for efficient learning from demonstrations in high dimensional state spaces",
            "venue": "IEEE International Conference on Robotics and Automation (ICRA),",
            "year": 2016
        },
        {
            "authors": [
                "Tor Lattimore",
                "Csaba Szepesv\u00e1ri"
            ],
            "title": "Bandit algorithms",
            "year": 2020
        },
        {
            "authors": [
                "Kimin Lee",
                "Laura Smith",
                "Anca Dragan",
                "Pieter Abbeel"
            ],
            "title": "B-pref: Benchmarking preference-based reinforcement learning",
            "venue": "In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round",
            "year": 2021
        },
        {
            "authors": [
                "Kimin Lee",
                "Laura M Smith",
                "Pieter Abbeel"
            ],
            "title": "Pebble: Feedback-efficient interactive reinforcement learning via relabeling experience and unsupervised pre-training",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Zihao Li",
                "Zhuoran Yang",
                "Mengdi Wang"
            ],
            "title": "Reinforcement learning with human feedback: Learning dynamic choices via pessimism",
            "venue": "arXiv preprint arXiv:2305.18438,",
            "year": 2023
        },
        {
            "authors": [
                "Thodoris Lykouris",
                "Max Simchowitz",
                "Alex Slivkins",
                "Wen Sun"
            ],
            "title": "Corruption-robust exploration in episodic reinforcement learning",
            "venue": "In Conference on Learning Theory,",
            "year": 2021
        },
        {
            "authors": [
                "Vivek Myers",
                "Erdem B\u0131y\u0131k",
                "Dorsa Sadigh"
            ],
            "title": "Active reward learning from online preferences",
            "venue": "arXiv preprint arXiv:2302.13507,",
            "year": 2023
        },
        {
            "authors": [
                "Ellen Novoseller",
                "Yibing Wei",
                "Yanan Sui",
                "Yisong Yue",
                "Joel Burdick"
            ],
            "title": "Dueling posterior sampling for preference-based reinforcement learning",
            "venue": "In Conference on Uncertainty in Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Takayuki Osa",
                "Joni Pajarinen",
                "Gerhard Neumann",
                "J Andrew Bagnell",
                "Pieter Abbeel",
                "Jan Peters"
            ],
            "title": "An algorithmic perspective on imitation learning",
            "venue": "Foundations and Trends\u00ae in Robotics,",
            "year": 2018
        },
        {
            "authors": [
                "Ian Osband",
                "Benjamin Van Roy"
            ],
            "title": "Model-based reinforcement learning and the eluder dimension",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2014
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeffrey Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Aldo Pacchiano",
                "Aadirupa Saha",
                "Jonathan Lee"
            ],
            "title": "Dueling rl: reinforcement learning with trajectory preferences",
            "venue": "arXiv preprint arXiv:2111.04850,",
            "year": 2021
        },
        {
            "authors": [
                "Alexander Rakhlin",
                "Karthik Sridharan"
            ],
            "title": "Online non-parametric regression",
            "venue": "In Conference on Learning Theory,",
            "year": 2014
        },
        {
            "authors": [
                "Stephane Ross",
                "J Andrew Bagnell"
            ],
            "title": "Reinforcement and imitation learning via interactive no-regret learning",
            "venue": "arXiv preprint arXiv:1406.5979,",
            "year": 2014
        },
        {
            "authors": [
                "St\u00e9phane Ross",
                "Geoffrey Gordon",
                "Drew Bagnell"
            ],
            "title": "A reduction of imitation learning and structured prediction to no-regret online learning",
            "venue": "In Proceedings of the fourteenth international conference on artificial intelligence and statistics,",
            "year": 2011
        },
        {
            "authors": [
                "St\u00e9phane Ross",
                "Narek Melik-Barkhudarov",
                "Kumar Shaurya Shankar",
                "Andreas Wendel",
                "Debadeepta Dey",
                "J Andrew Bagnell",
                "Martial Hebert"
            ],
            "title": "Learning monocular reactive uav control in cluttered natural environments",
            "venue": "IEEE international conference on robotics and automation,",
            "year": 2013
        },
        {
            "authors": [
                "Daniel Russo",
                "Benjamin Van Roy"
            ],
            "title": "Eluder dimension and the sample complexity of optimistic exploration",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2013
        },
        {
            "authors": [
                "Dorsa Sadigh",
                "Anca D Dragan",
                "Shankar Sastry",
                "Sanjit A Seshia"
            ],
            "title": "Active preference-based learning of reward functions",
            "year": 2017
        },
        {
            "authors": [
                "Aadirupa Saha",
                "Pierre Gaillard"
            ],
            "title": "Dueling bandits with adversarial sleeping",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Aadirupa Saha",
                "Pierre Gaillard"
            ],
            "title": "Versatile dueling bandits: Best-of-both world analyses for learning from relative preferences",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Aadirupa Saha",
                "Akshay Krishnamurthy"
            ],
            "title": "Efficient and optimal algorithms for contextual dueling bandits under realizability",
            "venue": "In International Conference on Algorithmic Learning Theory,",
            "year": 2022
        },
        {
            "authors": [
                "Aadirupa Saha",
                "Aldo Pacchiano",
                "Jonathan Lee"
            ],
            "title": "Dueling rl: Reinforcement learning with trajectory preferences",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2023
        },
        {
            "authors": [
                "Ayush Sekhari",
                "Karthik Sridharan",
                "Wen Sun",
                "Runzhe Wu"
            ],
            "title": "Selective sampling and imitation learning via online regression",
            "venue": "arXiv preprint arXiv:2307.04998,",
            "year": 2023
        },
        {
            "authors": [
                "David Simchi-Levi",
                "Yunzong Xu"
            ],
            "title": "Bypassing the monster: A faster and simpler optimal algorithm for contextual bandits under realizability",
            "venue": "Mathematics of Operations Research,",
            "year": 2022
        },
        {
            "authors": [
                "Max Simchowitz",
                "Kevin G Jamieson"
            ],
            "title": "Non-asymptotic gap-dependent regret bounds for tabular mdps",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Nisan Stiennon",
                "Long Ouyang",
                "Jeffrey Wu",
                "Daniel Ziegler",
                "Ryan Lowe",
                "Chelsea Voss",
                "Alec Radford",
                "Dario Amodei",
                "Paul F Christiano"
            ],
            "title": "Learning to summarize with human feedback",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Wen Sun",
                "Arun Venkatraman",
                "Geoffrey J Gordon",
                "Byron Boots",
                "J Andrew Bagnell"
            ],
            "title": "Deeply aggrevated: Differentiable imitation learning for sequential prediction",
            "venue": "In International conference on machine learning,",
            "year": 2017
        },
        {
            "authors": [
                "Aleksandar Taranovic",
                "Andras Gabor Kupcsik",
                "Niklas Freymuth",
                "Gerhard Neumann"
            ],
            "title": "Adversarial imitation learning with preferences",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Alexander B Tsybakov"
            ],
            "title": "Optimal aggregation of classifiers in statistical learning",
            "venue": "The Annals of Statistics,",
            "year": 2004
        },
        {
            "authors": [
                "Ruosong Wang",
                "Russ R Salakhutdinov",
                "Lin Yang"
            ],
            "title": "Reinforcement learning with general value function approximation: Provably efficient approach via bounded eluder dimension",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Zheng Wen",
                "Benjamin Van Roy"
            ],
            "title": "Efficient exploration and value function generalization in deterministic systems",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2013
        },
        {
            "authors": [
                "Christian Wirth",
                "Johannes F\u00fcrnkranz"
            ],
            "title": "On learning from game annotations",
            "venue": "IEEE Transactions on Computational Intelligence and AI in Games,",
            "year": 2014
        },
        {
            "authors": [
                "Christian Wirth",
                "Riad Akrour",
                "Gerhard Neumann",
                "Johannes F\u00fcrnkranz"
            ],
            "title": "A survey of preference-based reinforcement learning methods",
            "venue": "Journal of Machine Learning Research,",
            "year": 2017
        },
        {
            "authors": [
                "Huasen Wu",
                "Xin Liu"
            ],
            "title": "Double thompson sampling for dueling bandits",
            "venue": "Advances in neural information processing systems,",
            "year": 2016
        },
        {
            "authors": [
                "Yue Wu",
                "Tao Jin",
                "Hao Lou",
                "Farzad Farnoud",
                "Quanquan Gu"
            ],
            "title": "Borda regret minimization for generalized linear dueling bandits",
            "venue": "arXiv preprint arXiv:2303.08816,",
            "year": 2023
        },
        {
            "authors": [
                "Yichong Xu",
                "Ruosong Wang",
                "Lin Yang",
                "Aarti Singh",
                "Artur Dubrawski"
            ],
            "title": "Preference-based reinforcement learning with finite-time guarantees",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Yisong Yue",
                "Thorsten Joachims"
            ],
            "title": "Beat the mean bandit",
            "venue": "In Proceedings of the 28th international conference on machine learning",
            "year": 2011
        },
        {
            "authors": [
                "Yisong Yue",
                "Josef Broder",
                "Robert Kleinberg",
                "Thorsten Joachims"
            ],
            "title": "The k-armed dueling bandits problem",
            "venue": "Journal of Computer and System Sciences,",
            "year": 2012
        },
        {
            "authors": [
                "Wenhao Zhan",
                "Masatoshi Uehara",
                "Wen Sun",
                "Jason D Lee"
            ],
            "title": "How to query human feedback efficiently in rl",
            "venue": "arXiv preprint arXiv:2305.18505,",
            "year": 2023
        },
        {
            "authors": [
                "David Zhang",
                "Micah Carroll",
                "Andreea Bobu",
                "Anca Dragan"
            ],
            "title": "Time-efficient reward learning via visually assisted cluster ranking",
            "venue": "arXiv preprint arXiv:2212.00169,",
            "year": 2022
        },
        {
            "authors": [
                "Banghua Zhu",
                "Jiantao Jiao",
                "Michael I Jordan"
            ],
            "title": "Principled reinforcement learning with human feedback from pairwise or k-wise comparisons",
            "venue": "In International Conference on Machine Learning. PMLR,",
            "year": 2023
        },
        {
            "authors": [
                "Yinglun Zhu",
                "Robert Nowak"
            ],
            "title": "Efficient active learning with abstention",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Brian D Ziebart",
                "Andrew Maas",
                "J Andrew Bagnell",
                "Anind K Dey"
            ],
            "title": "Maximum entropy inverse reinforcement learning",
            "venue": "In Proceedings of the 23rd national conference on Artificial intelligence-Volume",
            "year": 2008
        },
        {
            "authors": [
                "Masrour Zoghi",
                "Shimon Whiteson",
                "Remi Munos",
                "Maarten Rijke"
            ],
            "title": "Relative upper confidence bound for the k-armed dueling bandit problem",
            "venue": "In International conference on machine learning,",
            "year": 2014
        }
    ],
    "sections": [
        {
            "text": "ar X\niv :2\n30 7.\n12 92\n6v 1\n[ cs\n.L G\n] 2\n4 Ju\nWe consider the problem of contextual bandits and imitation learning, where the learner lacks direct knowledge of the executed action\u2019s reward. Instead, the learner can actively query an expert at each round to compare two actions and receive noisy preference feedback. The learner\u2019s objective is twofold: to minimize the regret associated with the executed actions, while simultaneously, minimizing the number of comparison queries made to the expert. In this paper, we assume that the learner has access to a function class that can represent the expert\u2019s preference model under appropriate link functions, and provide an algorithm that leverages an online regression oracle with respect to this function class for choosing its actions and deciding when to query. For the contextual bandit setting, our algorithm achieves a regret bound that combines the best of both worlds, scaling as O(min{ \u221a T , d/\u2206}), where T represents the number of interactions, d represents the eluder dimension of the function class, and \u2206 represents the minimum preference of the optimal action over any suboptimal action under all contexts. Our algorithm does not require the knowledge of \u2206, and the obtained regret bound is comparable to what can be achieved in the standard contextual bandits setting where the learner observes reward signals at each round. Additionally, our algorithm makes only O(min{T, d2/\u22062}) queries to the expert. We then extend our algorithm to the imitation learning setting, where the learning agent engages with an unknown environment in episodes of length H each, and provide similar guarantees for regret and query complexity. The regret bound for our imitation learning algorithm, which relies on preferencebased feedback, matches the prior results in interactive imitation learning (Ross and Bagnell, 2014) that require access to the expert\u2019s actions as well as reward signals. Furthermore, we show that our algorithm enjoys improved query complexity bounds. Interestingly, in some cases, our algorithm for imitation learning via preference-feedback can even learn to outperform the underlying expert thus highlighting a practical benefit of considering preference-based feedback in imitation learning."
        },
        {
            "heading": "1 Introduction",
            "text": "Human feedback for training machine learning models has been widely used in many scenarios, including robotics (Ross et al., 2011, 2013; Jain et al., 2015; Laskey et al., 2016; Christiano et al., 2017) and natural language processing (Stiennon et al., 2020; Ouyang et al., 2022). By integrating human feedback into the training process, these prior works provide techniques to align machine-learning models with human intention and enable high-quality human-machine interaction (e.g., ChatGPT).\n*Authors are listed in alphabetical order of their last names.\nExisting methods generally leverage two types of human feedback. The first is the action from human experts, which is the dominant feedback mode used in the literature of imitation learning or learning from demonstrations (Abbeel and Ng, 2004; Ziebart et al., 2008; Daume\u0301 et al., 2009; Ross et al., 2011; Ross and Bagnell, 2014; Sun et al., 2017; Osa et al., 2018; Li et al., 2023). The second type of feedback is preference-based feedback, which involves comparing pairs of actions. In this approach, the expert provides feedback by indicating their preference between two options selected by the learner. While both types of feedback have their applications, our focus in this work is on preference-based feedback, which is particularly suitable for scenarios where it is challenging for human experts to recommend the exact optimal action while making pairwise comparisons is much easier.\nLearning via preference-based feedback has been extensively studied, particularly in the field of dueling bandits (Yue and Joachims, 2011; Yue et al., 2012; Zoghi et al., 2014; Ailon et al., 2014; Komiyama et al., 2015; Wu and Liu, 2016; Saha and Gaillard, 2021; Bengs et al., 2021; Saha and Gaillard, 2022) and contextual dueling bandits (Dud\u0131\u0301k et al., 2015; Saha and Krishnamurthy, 2022; Wu et al., 2023). Different from the standard bandit setting, the learner proposes two actions in dueling bandits and only gets noisy preference feedback from the human expert. Follow-up works extend the preference-based learning model from the onestep bandit setting to the multi-step decision-making (e.g., IL and RL) setting (Chu and Ghahramani, 2005; Sadigh et al., 2017; Christiano et al., 2017; Lee et al., 2021b; Chen et al., 2022; Saha et al., 2023). These studies mainly focus on how to learn a high-quality policy from human feedback, without concerning the question of active query in order to minimize the query complexity.\nHowever, query complexity is an important metric to optimize when learning from human feedback, as human feedback is expensive to collect (Lightman et al., 2023). For instance, InstructGPT (Ouyang et al., 2022) is trained only on around 30K pieces of human feedback, which is significantly fewer than the internetscale dataset used for pre-training the base model GPT3, indicating the challenge of scaling up the size of human feedback datasets. In other areas, such as robotics, learning from human feedback is also not easy, and prior studies (e.g., Cohn et al. (2011); Zhang et al. (2022); Myers et al. (2023)) have explored this issue from various perspectives. Ross et al. (2013); Laskey et al. (2016) pointed out that querying human feedback in the learning loop is challenging, and extensively querying for feedback puts too much burden on the human experts.\nIn this work, we design principled algorithms that can learn from preference-based feedback while at the same time minimizing query complexity, under the settings of contextual bandits (Auer et al., 2002; Langford and Zhang, 2007) and imitation learning (Ross et al., 2011). Our main contributions can be summarized as follows.\n\u2022 In the contextual dueling bandits setting, the stochastic preference feedback is generated based on some\npreference matrix (Saha and Krishnamurthy, 2022). We propose an algorithm (named AURORA \u2013 in short of Active preference qUeRy fOR contextual bAndits) that can achieve a best-of-both-worlds regret bound (i.e., achieves the minimum of the worst-case regret and an instance dependent regret), while at the same providing an instance-dependent query complexity bound. For benign instances with small eluder dimension and large gap, our regret and query complexity bounds both scale with ln(T ) where T is the total number of interactions in contextual bandits.\n\u2022 In imitation learning, the stochastic preference feedback is generated based on the underlying reward-to-\ngo of the expert\u2019s policy (e.g., the expert prefers actions that lead to higher reward-to-go). We propose an algorithm named AURORAE, in short of Active preference qUeRy fOR imitAtion lEarning, which instantiates H instances of AURORA, one per each time step for the finite horizon Markov Decision Process\n(MDP), where H is the horizon. By leveraging preference-based feedback, we show that, interestingly, our algorithm can learn to outperform the expert when the expert is suboptimal. Such a result is beyond the scope of the classic imitation learning algorithm DAGGER, and previously can only be achieved by algorithms like AGGREVATE(D) (Ross and Bagnell, 2014; Sun et al., 2017; Cheng and Boots, 2018) and LOLS (Chang et al., 2015) which require direct access to expert\u2019s actions and also reward signal \u2013 a much stronger feedback mode than ours.\nTo the best of our knowledge, for both contextual bandit and imitation learning with preference-based feedback, our algorithms are the first to achieve best-of-both-worlds regret bounds via active querying."
        },
        {
            "heading": "1.1 Related works",
            "text": "Selective Sampling. Numerous studies have been conducted on selective sampling across various settings (Cesa-Bianchi et al., 2005; Dekel et al., 2012; Agarwal, 2013; Hanneke and Yang, 2015, 2021; Zhu and Nowak, 2022), with the work of Sekhari et al. (2023) being closest to ours. Sekhari et al. (2023) presented a suite of provably efficient algorithms that are applicable to settings including contextual bandits and imitation learning. The primary distinction between our setting and the prior works lies in the feedback modality\u2013we assume preference-based feedback, whereas they assume direct label feedback or reward signals.\nContextual bandits with preference feedback. Dud\u0131\u0301k et al. (2015) is the first to consider contextual dueling bandits, and one of their algorithms achieves the optimal regret rate. Saha and Krishnamurthy (2022) studied contextual dueling bandits using a value function class and proposed an algorithm based on a reduction to online regression, which also achieves an optimal worst-case regret bound. In this paper, we mainly follow the setting of the latter and make notable improvements in two aspects: (1) in addition to the O( \u221a AT ) optimal regret rate where A is the number of actions and T is the number of interaction rounds, we established an instance-dependent regret upper bound that can be significantly smaller when the bandit exhibits a favorable structure; (2) our algorithm has an instance-dependent upper bound on the number of queries, and thus when the underlying instance is well behaved (has small eluder dimension and large gap), we will make significantly fewer queries.\nAnother related work is Saha and Gaillard (2022) which achieves the best-of-both-worlds regret for noncontextual dueling bandits. We note that our setting is more general due to the existence of context and general function approximation, enabling us to leverage function class beyond linear and tabular cases.\nRL with preference feedback. RL with preference feedback has been widely employed in recent advancements in AI (Ouyang et al., 2022; OpenAI, 2023). According to Wirth et al. (2017), there are generally three types of preference feedback: action preferences (Fu\u0308rnkranz et al., 2012), state preferences (Wirth and Fu\u0308rnkranz, 2014), and trajectory preferences (Busa-Fekete et al., 2014; Novoseller et al., 2020; Xu et al., 2020; Lee et al., 2021a; Chen et al., 2022; Saha et al., 2023; Pacchiano et al., 2021; Biyik and Sadigh, 2018; Taranovic et al., 2022; Sadigh et al., 2017). We focus on the action preference modality with the goal of achieving tight regret bounds and query complexities.\nThe concurrent work from Zhan et al. (2023) investigates the experimental design in both the trajectoriesbased and action-based preference settings, for which they decouple the process of collecting trajectories from querying for human feedback. Their action-based setting is the same as ours, but they mainly focus\non linear parameterization, while our approach is a reduction to online regression and can leverage general function approximation beyond linear function classes.\nImitation learning. In imitation learning, two common feedback modalities are typically considered: demonstrations that contain experts\u2019 actions, and preferences. The former involves directly acquiring expert actions (e.g., Ross et al. (2011); Ross and Bagnell (2014); Sun et al. (2017); Chang et al. (2015); Sekhari et al. (2023)), while the latter focuses on obtaining preferences between selected options (Chu and Ghahramani, 2005; Lee et al., 2021b; Zhu et al., 2023). Brown et al. (2019, 2020) leveraged both demonstrations and preference-based information and empirically showed that their algorithm can learn to outperform experts. Our imitation learning setting belongs to the second category, and we established bounds on the regret and the query complexity for our algorithm. We show that our algorithm can learn a policy that can provably outperform the expert (when it is suboptimal for the underlying environment)."
        },
        {
            "heading": "2 Preliminaries",
            "text": "In this section, we introduce the setup for contextual bandits and imitation learning with preference-based feedback. We denote [N ] as the set of integers {1, . . . , N}. The set of all distributions over a set S is denoted by \u2206(S)."
        },
        {
            "heading": "2.1 Contextual Bandits with Preference-Based Feedback",
            "text": "In this section, we introduce the contextual dueling bandits setting. We assume a context set X and an action space A = [A]. At each round t \u2208 [T ], a context xt is drawn adversarially, and the learner\u2019s task is to decide whether to make a query to the expert. If the learner makes a query, it needs to select a pair of actions (at, bt) \u2208 A \u00d7 A, upon which a noisy feedback yt \u2208 {\u22121, 1} is revealed to the learner regarding whether at or bt is better. Specifically, we assume that the expert relies on a preference function f\u22c6 : X \u00d7A\u00d7A \u2192 [\u22121, 1] based on which, it samples its feedback yt as\nPr(at is preferred to bt |xt) := Pr(yt = 1 |xt, at, bt) = \u03c6 ( f\u22c6(xt, at, bt) )\nwhere \u03c6(d) : [\u22121, 1] \u2192 [0, 1] is the link function, which satisfies \u03c6(d) + \u03c6(\u2212d) = 1 for any d. If the learner does not make a query, it should still select a pair of actions (at, bt) \u2208 A \u00d7 A but will not receive any feedback. Let Zt \u2208 {0, 1} indicate whether the learner makes a query at round t. We assume that the learner has access to a function class F \u2286 X \u00d7 A \u00d7 A \u2192 [\u22121, 1] that realizes f\u22c6. Furthermore, we assume that f\u22c6, as well as the functions in F , is transitive and anti-symmetric. Assumption 1. We assume f\u22c6 \u2208 F and any functions f \u2208 F satisfies the following two properties: (1) transitivity: for any x \u2208 X and a, b, c \u2208 A, if f(x, a, b) > 0 and f(x, b, c) > 0, then we must have f(x, a, c) > 0; (2) anti-symmetry: f(x, a, b) = \u2212f(x, b, a) for any x \u2208 X and any a, b \u2208 A. We provide an example below for which Assumption 1 is satisfied.\nExample 1. Assume there exists a function r\u22c6 : X\u00d7A \u2192 [0, 1] such that f\u22c6(x, a, b) = r\u22c6(x, a)\u2212r\u22c6(x, b) for any x \u2208 X and a, b \u2208 A. Typically, such a function r\u22c6 represents the \u201creward function\u201d of the contextual bandit. In such a scenario, we can first parameterize a reward class R \u2286 X \u00d7 A \u2192 [0, 1] and define F = {f : f(x, a, b) = r(x, a)\u2212 r(x, b), r \u2208 R}. Moreover, it is common to have \u03c6(d) := 1/(1+ exp(\u2212d))\nin this setting, which recovers the Bradley-Terry-Luce (BTL) model (Bradley and Terry, 1952)\u2014a commonly used model in practice for learning reward models (Christiano et al., 2017).\nAssumption 1 ensures the existence of an optimal arm, as stated below.\nLemma 1. Under Assumption 1, for any function f \u2208 F and any context x \u2208 X , there exists an arm a \u2208 A such that f(x, a, b) \u2265 0 for any arm b \u2208 A. We denote this best arm by \u03c0f (x) := a.1\nThe learner\u2019s goal is to minimize the regret while minimizing the number of queries, which are defined as:\nRegretCBT :=\nT\u2211\nt=1\n( f\u22c6(xt, \u03c0f\u22c6(xt), at) + f \u22c6(xt, \u03c0f\u22c6(xt), bt) ) , QueriesCBT := T\u2211\nt=1\nZt.\nIt is worth noting that when f\u22c6 is the difference in rewards (as in Example 1), the regret defined above reduces to the standard regret of a contextual bandit. We also remark that our feedback generation generalizes that of Saha and Krishnamurthy (2022) in that we assume an additional link function \u03c6, while they assume the feedback is sampled from Pr(y = 1 |x, a, b) = (Pt[at, bt] + 1)/2, which is captured in our setting (see Example 2). However, Saha and Krishnamurthy (2022) do not assume transitivity."
        },
        {
            "heading": "2.2 Imitation Learning with Preference-Based Feedback",
            "text": "In our imitation learning setup, we consider that the learner operates in a finite-horizon Markov decision process (MDP), which is a tuple M(X ,A, r, P,H) where X is the state space, A is the action space, P is the transition kernel, r : X \u00d7 A \u2192 [0, 1] is the reward function, and H is the length of each episode. The interaction between the learner and the environment proceeds as follows: at each episode t \u2208 [T ], the learner receives an initial state xt,0 which could be chosen adversarially. Then, the learner interacts with the environment for H steps. At each step h, the learner first decides whether to make a query. If making a query, the learner needs to select a pair of actions (at,h, bt,h) \u2208 A \u00d7 A, upon which a feedback yt,h \u2208 {\u22121, 1} is revealed to the learner regarding which action is preferred from the expert\u2019s perspective. Here the feedback is sampled according to\nPr(at,h is preferred to bt,h |xt,h, h) := Pr(yt,h = 1 |xt,h, at,h, bt,h, h) = \u03c6 ( f\u22c6h(x, at,h, bt,h) ) .\nIrrespective of whether the learner made a query, it then picks a single action from at,h, bt,h and transit to the next step (our algorithm will just pick an action uniformly at random from at,h, bt,h). After H steps, the next episode starts. Let Zt,h \u2208 {0, 1} indicate whether the learner decided to query at step h in episode t. We assume that the function class F is a product of H classes, i.e., F = F0 \u00d7 \u00b7 \u00b7 \u00b7 FH\u22121 where, for each h, we use Fh = {f : X \u00d7A\u00d7A \u2192 [\u22121, 1]} to model f\u22c6h and assume that Fh satisfies Assumption 1. A policy is a mapping \u03c0 : X \u2192 \u2206(A). For a policy \u03c0, the state value function for a state x at step h is defined as V \u03c0h (x) := E[ \u2211H\u22121 i=h ri |xh = x] and the state-action value function for a state-action pair (x, a) is\nQ\u03c0h(x, a) := E[ \u2211H\u22121\ni=h ri |xh = x, ah = a], where the expectations are taken w.r.t. the trajectories sampled by \u03c0 in the underlying MDP.\nIn the imitation learning setting, we assume that the expert (who gives the preference-based feedback) is equipped with a markovian policy \u03c0e, and that the preference of the expert is dependent on the rewardto-go under \u03c0e (i.e. on a state x, actions with higher values of Q \u03c0e(s, a) will be preferred by the expert).\n1When the best arms is not unique, the ties are broken arbitrarily but consistently.\nFormalizing this intuition, we assume that f\u22c6h is defined such that as f \u22c6 h(x, a, b) := Q \u03c0e h (x, a) \u2212 Q \u03c0e h (x, b). The goal of the learner is still to minimize the regret and number of queries:\nRegretILT := T\u2211\nt=1\n( V \u03c0e0 (xt,0)\u2212 V \u03c0t0 (xt,0) ) , QueriesILT := T\u2211\nt=1\nH\u22121\u2211\nh=0\nZt,h.\nHere \u03c0t is the strategy the learner uses to select actions at episode t."
        },
        {
            "heading": "2.3 Link Function and Online Regression Oracle",
            "text": "Following the standard practice in the literature (Agarwal, 2013), we assume \u03c6 is the derivative of some \u03b1-strongly convex function (see Definition 3) \u03a6 : [\u22121, 1] \u2192 R and define the associated loss function as \u2113\u03c6(d, y) = \u03a6(d) \u2212 d(y + 1)/2. Additionally, in line with prior works in the literature (Foster et al., 2021; Foster and Rakhlin, 2020; Simchi-Levi and Xu, 2022; Foster et al., 2018a; Sekhari et al., 2023), our algorithm utilizes an online regression oracle, which is assumed to have a sublinear regret guarantee w.r.t. F on arbitrary data sequences.\nAssumption 2. We assume the learner has access to an online regression oracle pertaining to the loss \u2113\u03c6 such that for any sequence {(x1, a1, b1, y1), . . . , (xT , aT , bT , yT )} where the label yt is generated by yt \u223c \u03c6(f\u22c6(xt, at, bt)), we have\nT\u2211\nt=1\n\u2113\u03c6 ( ft(xt, at, bt), yt ) \u2212 inf f\u2208F \u2113\u03c6 ( f(xt, at, bt), yt ) \u2264 \u03a5(F , T )\nfor some \u03a5(F , T ) that grows sublinearly with respect to T .2 For notational simplicity, whenever clear from the context, we define \u03a5 := \u03a5(F , T ). Here \u03a5 represents the regret upper bound and is typically of logarithmic order in T or the cardinality of the function class F in many cases (here we drop the dependence on T in notation for simplicity). We provide a few examples below:\nExample 2 (Squared loss). If we consider \u03a6(d) = d2/4 + d/2 + 1/4, which is 1/4-strongly convex, then we obtain \u03c6(d) = (d + 1)/2 and \u2113\u03c6(d, y) = (d \u2212 y)2/4, thereby recovering the squared loss, which has been widely studied in prior works. For example, Rakhlin and Sridharan (2014) characterized the minimax rates for online square loss regression in terms of the offset sequential Rademacher complexity, resulting in favorable bounds for the regret. Specifically, we have \u03a5 = O(log |F|) assuming the function class F is finite, and \u03a5 = O(d log(T )) assuming F is a d-dimensional linear class. We also kindly refer the readers to Krishnamurthy et al. (2017); Foster et al. (2018a) for efficient implementations.\nExample 3 (Logistic loss). When \u03a6(d) = log(1 + exp(d)) which is strongly convex at [\u22121, 1], we have \u03c6(d) = 1/(1 + exp(\u2212d)) and \u2113\u03c6(d, y) = log(1 + exp(\u2212yd)). Thus, we recover the logistic regression loss, which allows us to use online logistic regression and achieve \u03a5 = O(log |F|) assuming finite F . There have been numerous endeavors in minimizing the log loss, such as Foster et al. (2018b) and Cesa-Bianchi and Lugosi (2006, Chapter 9).\n2The online regression oracle updates as follows: in each iteration, after seeing xt, at, bt, it proposes a decision ft, then yt is\nrevealed and the online regression oracle incurs loss \u2113\u03c6(ft(xt, at, bt), yt).\nAlgorithm 1 Active preference qUeRy fOR contextual bAndits (AURORA)\nRequire: Function class F , confidence parameter \u03b2 = 4\u03a5 \u03b1 + 16+24\u03b1 \u03b12\nlog ( 4\u03b4\u22121 log(T ) ) .\n1: Online regression oracle produces f1. 2: for t = 1, 2, . . . , T do 3: Learner receives context xt, and computes the version space\nFt \u2190 { f \u2208 F : t\u22121\u2211\ns=1\nZs ( f(xs, as, bs)\u2212 fs(xs, as, bs) )2 \u2264 \u03b2 } .\nand the candidate arm set At \u2190 {\u03c0f (xt) : \u2200f \u2208 Ft}. 4: Learner decides whether to query Zt \u2190 1{|At| > 1}. 5: if Zt = 1 then 6: Set wt \u2190 supa,b\u2208At supf,f \u2032\u2208Ft f(xt, a, b) \u2212 f \u2032(xt, a, b), and \u03bbt \u2190 1{ \u2211t\u22121 s=1 Zsws \u2265 \u221a AT/\u03b2}. 7: if \u03bbt = 0 then 8: pt \u2190 Uniform(At). 9: else\n10: \u03b3t \u2190 \u221a\nAT/\u03b2. 11: Let pt be a solution of maxa\u2208At \u2211 b ft(xt, a, b)pt(b) + 2 \u03b3tpt(a) \u2264 5A \u03b3t . 12: end if 13: Learner samples at, bt \u223c pt independently and receives the feedback yt. 14: Learner feeds ((xt, at, bt), yt) to the online regression oracle which returns ft+1. 15: else 16: Learner sets at and bt to be the only action in At, and plays them. 17: ft+1 \u2190 ft. 18: end if 19: end for"
        },
        {
            "heading": "3 Contextual Bandits with Preference-Based Active Queries",
            "text": "We first present our algorithm, named AURORA, for contextual dueling bandits, as shown in Algorithm 1. At each round t \u2208 [T ], the online regression oracle outputs a predictor ft, using which the learner constructs a version space Ft containing all functions close to past predictors on observed data. Here, the threshold \u03b2 set to 4\u03a5/\u03b1 + (16 + 24\u03b1) log ( 4\u03b4\u22121 log(T ) ) /\u03b12 ensures that f\u22c6 \u2208 Ft for any t \u2208 [T ] with probability at least 1 \u2212 \u03b4 (Lemma 9). Thus, At is non-empty for all t \u2208 [T ] and correspondingly Line 16 is well defined. The learner then forms a candidate arm set At consisting of greedy arms induced by all functions in the version space. When |At| = 1, the only arm in the set is the optimal arm since f\u22c6 \u2208 Ft, and thus no query is needed (Zt = 0). However, when |At| > 1, any arm in At could potentially be the optimal arm, and thus the learner needs to make a comparison query to obtain more information.\nNext, we explain the strategy used by the learner for making a query. Firstly, the learner computes wt, which represents the \u201cwidth\u201d of the version space. Specifically, wt overestimates the instantaneous regret for playing any arm inAt (Lemma 8). Then, the learner defines \u03bbt that indicates if the estimated cumulative regret \u2211t\u22121 s=1 Zwws has exceeded \u221a AT/\u03b2. Note that Zt is multiplied to wt since no regret is incurred when Zt = 0. The strategy to choose the actions (to be queried) for different values of \u03bbt are as follows:\n\u2022 If \u03bbt = 0, the cumulative reward has not yet exceeded \u221a AT/\u03b2 = O( \u221a T ), so the learner will explore as\nmuch as possible by uniform sampling from At. \u2022 If \u03bbt = 1, the regret may have reached O( \u221a T ), and therefore the learner uses a technique similar to\ninverse gap weighting (IGW), as inspired by Saha and Krishnamurthy (2022), to achieve a better balance between exploration and exploitation. Specifically, the learner solves the convex program3 in Line 11, which is feasible and whose solution pt satisfies (see Lemma 11)\nE a\u223cpt\n[ f\u22c6(xt, \u03c0f\u22c6(x), a) ] = O ( \u03b3t E\na,b\u223cpt\n[( ft(xt, a, b) \u2212 f\u22c6(xt, a, b) )2] + A\n\u03b3t\n) . (1)\nAs a result of the above relation, we note that one can convert the instantaneous regret to the pointwise error between the predictor ft and the truth f \u22c6 plus an additive A/\u03b3t. This allows us to bound the cumulative point-wise error by the regret of the online regression oracle. In the special case, when there exists a \u201creward function\u201d r : X \u00d7 A \u2192 [0, 1] for each f \u2208 F such that f(x, a, b) = r(x, a) \u2212 r(x, b) (Example 1), the solution pt can be directly written as\npt(a) =   \n1 A+\u03b3t ( rt(xt,\u03c0ft(xt))\u2212rt(xt,a) ) a 6= \u03c0ft(xt) 1\u2212\u2211a\u2032 6=\u03c0ft(xt) pt(a \u2032) a = \u03c0ft(xt) ,\nwhere rt is the reward function associated with ft, i.e., ft(x, a, b) = rt(x, a) \u2212 rt(x, b). This is the standard IGW exploration strategy (Foster and Rakhlin, 2020) and leads to the same guarantee as (1) (see Lemma 12)."
        },
        {
            "heading": "3.1 Theoretical Analysis",
            "text": "Towards the theoretical guarantees of Algorithm 1, we employ two quantities to characterize a contextual bandit instance: the uniform gap and the eluder dimension, which are introduced below.\nAssumption 3 (Uniform gap). We assume the optimal arm \u03c0f\u22c6(x) induced by f \u22c6 under any context x \u2208 X is unique. Further, we assume a uniform gap \u2206 := infx infa6=\u03c0f\u22c6(x) f \u22c6(x, \u03c0f\u22c6(x), a) > 0.\nWe note that the existence of a uniform gap is a standard assumption in the literature of contextual bandits (Dani et al., 2008; Abbasi-Yadkori et al., 2011; Audibert et al., 2010; Garivier et al., 2019; Foster and Rakhlin, 2020; Foster et al., 2021). Next, we introduce the eluder dimension (Russo and Van Roy, 2013) and begin by defining \u201c\u01eb-dependence\u201d.\nDefinition 1 (\u01eb-dependence). Let G \u2286 X \u2192 R be any function class. We say an element x \u2208 X is \u01eb-dependent on {x1, x2, . . . , xn} \u2286 X with respect to G if any pair of functions g, g\u2032 \u2208 G satisfying\u2211n\ni=1(g(xi) \u2212 g\u2032(xi)) \u2264 \u01eb2 also satisfies g(x) \u2212 g\u2032(x) \u2264 \u01eb. Otherwise, we say x is \u01eb-independent of {x1, x2, . . . , xn}. Definition 2 (Eluder dimension). The \u01eb-eluder dimension of a function class G \u2286 X \u2192 R, denoted by dimE(G, \u01eb), is the length d of the longest sequence of elements in X satisfying that there exists some \u01eb\u2032 \u2265 \u01eb such that every element in the sequence is \u01eb\u2032-independent of its predecessors.\nEluder dimension is a standard complexity measure for function classes and has been used in the literature of bandits and RL extensively (Chen et al., 2022; Osband and Van Roy, 2014; Wang et al., 2020; Foster et al.,\n3It is convex as it can be written as |At| convex constraints: \u2211 b ft(xt, a, b)pt(b) + 2 \u03b3tpt(a) \u2264 5A \u03b3t , \u2200a \u2208 At.\n2021; Wen and Van Roy, 2013; Jain et al., 2015; Ayoub et al., 2020; Ishfaq et al., 2021; Huang et al., 2022). Examples where the eluder dimension is small include linear functions, generalized linear models, and functions in Reproducing Kernel Hilbert Space (RKHS).\nGiven these quantities, we are ready to state our main results. The proofs are provided in Appendix B.\nTheorem 1. Under Assumptions 1 to 3, Algorithm 1 guarantees the following upper bounds of the regret and the number of queries:\nRegretCBT = O\u0303\n( min {\u221a AT\u03b2,\nA2\u03b22dimE (F ,\u2206) \u2206\n}) ,\nQueriesCBT = O\u0303\n( min { T,\nA3\u03b23dim2E (F ,\u2206) \u22062\n})\nwith probability at least 1 \u2212 \u03b4. We recall that \u03b2 = O(\u03b1\u22121\u03a5 + \u03b1\u22122 log(\u03b4\u22121 log(T ))), and \u03b1 denotes the coefficient of strong convexity of \u03a6. We have hidden logarithmic terms in the upper bounds for brevity.\nWhen the loss \u2113\u03c6 is either square loss or logistic loss (Examples 2 and 3), the parameter \u03b2 is logarithmic in T . In such cases, the regret is O\u0303(min{ \u221a T ,dimE (F ,\u2206) /\u2206}) and the number of queries is O\u0303(min{T,dim2E(F ,\u2206)/\u22062}), ignoring A and logarithmic terms. Both consist of two components: the worst-case and the instance-dependent upper bounds. The worst-case bound provides a guarantee under all circumstances, while the instance-dependent one may significantly improve the upper bound when the underlying problem is well-behaved (i.e., has a small eluder dimension and a large gap).\nIntuition of proofs. We next provide intuition for why our algorithm has the aforementioned theoretical guarantees. First, we observe that from the definition of \u03bbt, the left term inside the indicator is nondecreasing, which allows us to divide rounds into two phases. In the first phase, \u03bbt is always 0, and then at some point, it changes to 1 and remains 1 for the rest rounds. After realizing this, we first explain the intuition of the worst-case regret. In the first phase, as wt is an overestimate of the instantaneous regret (see Lemma 8), the accumulated regret in this phase cannot exceed O( \u221a T ). In the second phase, we adapt the analysis of IGW to this scenario to obtain an O( \u221a T ) upper bound. A similar technique has been used in Saha and Krishnamurthy (2022); Foster et al. (2021). As the regret in both phases is at most O( \u221a T ), the total regret cannot exceed O( \u221a T ). Next, we explain the intuition of instance-dependent regret. Due to the existence of a uniform gap \u2206, we can first prove that as long as |At| > 1, we must have wt \u2265 \u2206 (see Lemma 7). This means that for all rounds that may incur regret, the corresponding width is at least \u2206. However, this cannot happen too many times as this frequency is bounded by the eluder dimension, which leads to an instance-dependent regret upper bound. Leveraging a similar technique, we can also obtain an upper bound on the number of queries.\nComparion to MINMAXDB (Saha and Krishnamurthy, 2022). In this prior work, the authors assume that Pr(y = 1 |x, a, b) = (f\u22c6(x, a, b) + 1)/2, which is a specification of our feedback model (Example 2). While our worst-case regret bound matches their regret bound, our paper improves upon their results by having an additional instance-dependent regret bound that depends on the eluder dimension and gap. Furthermore, we also provide bounds on the query complexity which could be small for benign instances while MINMAXDB simply queries on every round.\nComparion to ADACB (Foster et al., 2021). Our method shares some similarities with Foster et al. (2021), especially in terms of theoretical results, but differs in two aspects: (1) they assume regular contextual bandits where the learner observes the reward directly, while we assume preference feedback, and (2) they assume a stochastic setting where contexts are drawn i.i.d., but we assume that the context is adversarially chosen. While these two settings may not be directly comparable, it should be noted that (Foster et al., 2021) do not aim to minimize query complexity.\nLower bounds. To understand whether our algorithm attains tight upper bounds, we provide the following lower bound which follows from a reduction from regular multi-armed bandits to contextual dueling bandits.\nTheorem 2 (Lower bounds). The following two claims hold:\n(1) For any algorithm, there exists an instance that leads to RegretCBT = \u2126( \u221a AT );\n(2) For any algorithm achieving a worse-case expected regret upper bound in the form of E[RegretCBT ] = O( \u221a AT ), there exists an instance with gap \u2206 = \u221a A/T that results in E[RegretCBT ] = \u2126(A/\u2206) and\nE[QueriesCBT ] = \u2126(A/\u2206 2) = \u2126(T ).\nBy relating these lower bounds to Theorem 1, we conclude that our algorithm achieves a tight dependence on the gap \u2206 and T , up to logarithmic factors, in both the regret and query complexity upper bounds. Furthermore, as an additional contribution, we establish an alternative lower bound in Section B.4.1 by conditioning on the limit of regret, rather than the worst-case regret as assumed in Theorem 2.\nResults without the uniform gap assumption. We highlight that Theorem 1 can naturally extend to scenarios where a uniform gap does not exist (i.e., when Assumption 3 is not satisfied) without any modifications to the algorithm. The result is stated below, which is analogous to Theorem 1.\nTheorem 3. Under Assumptions 1 and 2, Algorithm 1 guarantees the following upper bounds of the regret and the number of queries:\nRegretCBT = O\u0303\n( min {\u221a AT\u03b2, min\n\u01eb>0\n{ T\u01eb\u03b2 +\nA2\u03b22dimE (F , \u01eb) \u01eb\n}}) ,\nQueriesCBT = O\u0303\n( min { T, min\n\u01eb>0\n{ T 2\u01eb \u03b2/A+\nA3\u03b23dim2E (F , \u01eb) \u01eb2\n}})\nwith probability at least 1 \u2212 \u03b4. Here we define the gap of context x as Gap(x) := mina6=\u03c0f\u22c6(x) f\n\u22c6(x, \u03c0f\u22c6(x), a) and the number of rounds where contexts have small gap as T\u01eb :=\u2211T t=1 1{Gap(xt) \u2264 \u01eb}. We also recall that \u03b2 = O(\u03b1\u22121\u03a5 + \u03b1\u22122 log(\u03b4\u22121 log(T ))), and \u03b1 denotes the coefficient of strong convexity of \u03a6.\nCompared to Theorem 1, the above result has an extra gap-dependent term defined as T\u01eb. Here \u01eb denotes a gap threshold, and T\u01eb measures how many times the context falls into a small-gap region. We highlight that T\u01eb is small under certain conditions such as the Tsybakov noise condition (Tsybakov, 2004). It is also worth mentioning that our algorithm is agnostic to \u01eb, thus allowing us to take the minimum over all \u01eb > 0.\nComparion to SAGE-BANDIT (Sekhari et al., 2023). Theorem 3 bears similarity to Theorem 4 in Sekhari et al. (2023), which examines active queries in contextual bandits with standard reward signal (0\u22121 reward). It is worth noting that although our result looks slightly worse in terms of the factor A (the number\nAlgorithm 2 Active preference qUeRy fOR imitAtion lEarning (AURORAE) Require: Function class F0,F1, . . . ,FH\u22121, confidence parameter \u03b2. 1: Learner creates H instances of Algorithm 1: AURORAh(Fh, \u03b2) for h = 0, 1, . . . ,H \u2212 1. 2: for t = 1, 2, . . . , T do 3: Learner receive initial state xt,0. 4: for h = 0, 1, . . . ,H \u2212 1 do 5: Learner feeds xt,h to AURORAh(Fh, \u03b2), and receives back at,h, bt,h, Zt,h. 6: if Zt,h = 1 then 7: Learner receives feedback yt,h. 8: Learner feeds ((xt,h, at,h, bt,h), yt,h) to AURORAh(Fh, \u03b2) to update its online regression oracle\nand local variables.\n9: end if\n10: Learner executes a \u223c Uniform({at,h, bt,h}) and transits to xt,h+1. 11: end for 12: end for\nof actions), we believe that this inferiority is reasonable since our approach requires two actions to form a query, thus analytically expanding the action space to A2. Whether this dependency can be improved remains a question for future investigation."
        },
        {
            "heading": "4 Imitation Learning with Preference-Based Active Queries",
            "text": "In this section, we introduce our second algorithm, which is presented in Algorithm 2 for imitation learning. In essence, the learner treats the MDP as a concatenation of H contextual bandits and runs an instance of AURORA (Algorithm 1) for each time step. Specifically, the learner first creates H instances of AURORA, denoted by AURORAh (for h = 0, . . . ,H \u2212 1). Here, AURORAh should be thought of as an interactive program that takes the context x as input and outputs a, b, and Z . At each episode t, and each step h therein, the learner first feeds the current state xt,h to AURORAh as the context; then, AURORAh decides whether to query (i.e. Zt,h) and returns the actions at,h and bt,h. If it decides to make a query, the learner will ask for the feedback yt,h on the proposed actions at,h, bt,h, and provide the information ((xt,h, at,h, bt,h), yt,h) back to AURORAh to update its online regression oracle (and other local variables). We recall that the noisy binary feedback yt,h is sampled as yt,h \u223c \u03c6(Q\u03c0eh (xt,h, at,h) \u2212 Q \u03c0e h (xt,h, bt,h)), and also emphasize that the learner neither has access to a \u223c \u03c0e(xt,h) like in DAGGER (Ross et al., 2011) nor reward-to-go like in AGGREVATE(D) (Ross and Bagnell, 2014; Sun et al., 2017). Finally, the learner chooses one of the two actions uniformly at random, executes it in the underlying MDP, and transits to the next state xt,h+1 in the episode. The above process is then repeated with AURORAh+1 till the episode ends. We name this algorithm AURORAE, the plural form of AURORA, which signifies that the algorithm is essentially a stack of multiple AURORA instances."
        },
        {
            "heading": "4.1 Theoretical Analysis",
            "text": "As Algorithm 2 is essentially a stack of Algorithm 1, we can inherit many of the theoretical guarantees from the previous section. To state the results, we first extend Assumption 3 into imitation learning.\nAssumption 4 (Uniform Gap). Let f\u22c6h be defined such that for any x \u2208 X , a, b \u2208 A2, f\u22c6h(x, a, b) =\nQ\u03c0eh (x, a) \u2212 Q \u03c0e h (x, b). For all h, we assume the optimal action for f \u22c6 h under any state x \u2208 X is unique. Further, we assume a uniform gap \u2206 := infh infx infa6=\u03c0f\u22c6 h (x) f \u22c6 h(x, \u03c0f\u22c6h (x), a) > 0. This assumption essentially says that Q\u03c0eh has a gap in actions. We remark that, just as Assumption 3 is a common condition in the bandit literature, Assumption 4 is also common in MDPs (Du et al., 2019; Foster et al., 2021; Simchowitz and Jamieson, 2019; Jin and Luo, 2020; Lykouris et al., 2021; He et al., 2021). The theoretical guarantee for Algorithm 2 is presented in Theorem 4. We note a technical difference between this result and Theorem 1: although we treat the MDP as a concatenation of H contextual bandits, the instantaneous regret of imitation learning is defined as the performance gap between the combined policy \u03c0t derived from the H instances as a cohesive unit and the expert policy. This necessitates the use of performance difference lemma (Lemma 5) to get a unified result.\nTheorem 4. Under Assumptions 1, 2 and 4, Algorithm 2 guarantees the following upper bounds of the regret and the number of queries:\nRegretILT \u2264 O\u0303 ( H \u00b7min {\u221a AT\u03b2,\nA2\u03b22dimE (F ,\u2206) \u2206\n}) \u2212AdvT ,\nQueriesILT \u2264 O\u0303 ( H \u00b7min { T,\nA3\u03b23dim2E (F ,\u2206) \u22062\n})\nwith probability at least 1\u2212\u03b4. Here AdvT := \u2211T\nt=1 \u2211H\u22121 h=0 Ext,h\u223cd\n\u03c0t xt,0,h\n[maxaA \u03c0e h (xt,h, a)] is non-negative,\nand d\u03c0txt,0,h(x) denotes the probability of \u03c0t 4 reaching the state x at time step h starting from inital state xt,0. In the above, \u03b2 = O(\u03b1\u22121\u03a5 + \u03b1\u22122 log(H\u03b4\u22121 log(T ))) and \u03b1 denotes the coefficient of strong convexity of \u03a6.\nCompared to Theorem 1, the main terms of the upper bounds for imitation learning are precisely the bounds in Theorem 1 multiplied by H . In the proof presented in Appendix B.6, we use the performance difference lemma to reduce the regret of imitation learning to the sum of the regret of H contextual dueling bandits, which explains this additional factor of H .\nAnother interesting point is that the main term of the regret upper bound is subtracted by a non-negative term AdvT , which measures the degree to which we can outperform the expert policy. This means that our algorithm not only competes with the expert policy but can also surpass it to some extent. This guarantee is stronger than that of DAGGER (Ross et al., 2011) in that DAGGER cannot ensure the learned policy is better than the expert policy regardless of how suboptimal the expert may be. While this may look surprising at first glance since we are operating under a somewhat weaker query mode than that of DAGGER, we note that by querying experts for comparisons on pairs of actions with feedback sampling as y \u223c \u03c6(Q\u03c0e(x, a) \u2212 Q\u03c0e(x, b)), it is possible to identify the action that maximizes Q\u03c0e(x, a) (even if we cannot identify the value Q\u03c0e(x, a)). Finally, we remark that our worst-case regret bound is similar to that of Ross and Bagnell (2014); Sun et al. (2017), which can also outperform a suboptimal expert but require access to both expert\u2019s actions and reward signals\u2014a much stronger query model than ours.\n4Policy \u03c0t consists of H time-dependent policies \u03c0t,1, . . . , \u03c0t,H , where each \u03c0t,h is defined implicitly via AURORAh, i.e., \u03c0t,h generates action as follows: given xt,h, AURORAh recommends at,h, bt,h, followed by uniformly sampling an action from {at,h, bt,h}."
        },
        {
            "heading": "5 Discussion and Future Work",
            "text": "We presented interactive decision-making algorithms that learn from preference-based feedback while minimizing query complexity. Our algorithms for contextual bandits and imitation learning share worst-case regret bounds similar to the bounds of the state-of-art algorithms in standard settings while maintaining instance-dependent regret bounds and query complexity bounds. Notably, our imitation learning algorithm can outperform suboptimal experts, matching the result of (Ross and Bagnell, 2014; Sun et al., 2017), which operates under much stronger feedback.\nIn terms of future work, we believe our result on contextual dueling bandits can be extended to the stochastic setting where we may replace the eluder dimension with the value function disagreement coefficient (Foster et al., 2021), which is typically smaller than the eluder dimension, and replace the online regression oracle by a supervised-learning batch regression oracle. We also conjecture that the dependence on the eluder dimension in the query complexity bound can be improved. Finally, another interesting direction is to develop practical implementations of our proposed algorithms."
        },
        {
            "heading": "Acknowledgements",
            "text": "AS acknowledges support from the Simons Foundation and NSF through award DMS-2031883, as well as from the DOE through award DE-SC0022199. KS acknowledges support from NSF CAREER Award 1750575, and LinkedIn-Cornell grant."
        },
        {
            "heading": "A Preliminaries",
            "text": "Lemma 2 (Kakade and Tewari (2008, Lemma 3)). Suppose X1, . . . ,XT is a martingale difference sequence with |Xt| \u2264 b. Let VartXt = Var (Xt | X1, . . . ,Xt\u22121) Let V = \u2211T t=1 VartXt be the sum of conditional variances of Xt \u2019s. Further, let \u03c3 = \u221a V . Then we have, for any \u03b4 < 1/e and T \u2265 3,\nPr\n( T\u2211\nt=1\nXt > max{2\u03c3, 3b \u221a ln(1/\u03b4)} \u221a ln(1/\u03b4) ) \u2264 4 ln(T )\u03b4.\nLemma 3 (Foster and Rakhlin (2020, Lemma 3)). For any vector y\u0302 \u2208 [0, 1]A, if we define p to be\np(a) =   \n1 A+\u03b3 ( y\u0302(a\u0302)\u2212y\u0302(a) ) if a 6= a\u0302, 1\u2212\u2211a6=a\u0302 p(a) if a = a\u0302\nwhere a\u0302 = argmaxa y\u0302(a), then for any y \u22c6 \u2208 [0, 1]A and \u03b3 > 0, we have\nE a\u223cp\n[( y\u22c6(a\u22c6)\u2212 y\u22c6(a) ) \u2212 \u03b3 ( y\u0302(a)\u2212 y\u22c6(a) )2] \u2264 A\n\u03b3 .\nLemma 4 (Zhu and Nowak (2022, Lemma 2)). Let (Zt)t\u2264T to be real-valued sequence of positive random variables adapted to a filtration Ft. If |Zt| \u2264 B almost surely, then with probability at least 1\u2212 \u03b4,\nT\u2211\nt=1\nZt \u2264 3\n2\nT\u2211\nt=1\nEt [Zt] + 4B log ( 2\u03b4\u22121 ) ,\nand T\u2211\nt=1\nEt [Zt] \u2264 2 T\u2211\nt=1\nZt + 8B log ( 2\u03b4\u22121 ) .\nLemma 5 (Performance difference lemma (Agarwal et al., 2019)). For any two policies \u03c0 and \u03c0\u2032 and any state x0 \u2208 X , we have\nV \u03c00 (x0)\u2212 V \u03c0 \u2032 0 (x0) = H\u22121\u2211\nh=0\nE xh,ah\u223cd\n\u03c0 x0,h\n[ A\u03c0 \u2032 h (xh, ah) ]\nwhere A\u03c0h(x, a) = Q \u03c0 h(x, a)\u2212V \u03c0h (x, a) and d\u03c0x0,h(x, a) is the probability of \u03c0 reaching the state-action pair (x, a) at time step h starting from initial state x0.\nLemma 6. For any two Bernoulli distributions Bern(x) and Bern(y) with x, y \u2208 [b, 1 \u2212 b] for some 0 < b \u2264 1/2, the KL divergence is bounded as\nKL ( Bern(x),Bern(y) ) \u2264 2(x\u2212 y) 2\nb .\nProof of Lemma 6. Denote \u2206 = x\u2212 y. Then, by definition, we have\nKL ( Bern(x),Bern(y) ) =x ln x\ny + (1\u2212 x) ln 1\u2212 x 1\u2212 y\n=x ln x x\u2212\u2206 + (1\u2212 x) ln 1\u2212 x 1\u2212 x+\u2206\n=x ln ( 1 + \u2206\nx\u2212\u2206\n) + (1\u2212 x) ln ( 1\u2212 \u2206\n1\u2212 x+\u2206\n)\nSince ln(1 + x) \u2264 x for all x > \u22121, we have\nKL ( Bern(x),Bern(y) ) \u2264x \u00b7 \u2206\nx\u2212\u2206 \u2212 (1\u2212 x) \u00b7 \u2206 1\u2212 x+\u2206\n=\u2206 \u00b7 ( x\nx\u2212\u2206 \u2212 1\u2212 x 1\u2212 x+\u2206\n)\n=\u2206 \u00b7 ( \u2206\nx\u2212\u2206 + \u2206 1\u2212 x+\u2206\n)\n\u2264\u22062 \u00b7 ( 1\ny +\n1\n1\u2212 y\n) \u2264 2\u2206 2\nb ."
        },
        {
            "heading": "B Missing Proofs",
            "text": ""
        },
        {
            "heading": "B.1 Supporting Lemmas",
            "text": "Definition 3 (Strong convexity). A function \u03a6 : [\u22121, 1] \u2192 R is \u03b1-strongly-convex if for all u, u\u2032 \u2208 R, we have\n\u03b1 2 (u\u2032 \u2212 u)2 \u2264 \u03a6(u\u2032)\u2212\u03a6(u)\u2212\u2207\u03a6(u)(u\u2032 \u2212 u).\nwhere \u2207\u03a6 means the derivative of \u03a6. Lemma 7. For any t \u2208 [T ], if f\u22c6 \u2208 Ft, then we have wt \u2265 \u2206 whenever |At| > 1.\nProof of Lemma 7. When |At| > 1, we know there exists a function f \u2032 \u2208 Ft satisfying\na\u2032 := \u03c0f \u2032(xt) 6= \u03c0f\u22c6(xt) =: a\u22c6t .\nThen we have \u2206 \u2264 f\u22c6(xt, a\u22c6t , a\u2032) \u2264 f\u22c6(xt, a\u22c6t , a\u2032)\u2212 f \u2032(xt, a\u22c6t , a\u2032) \u2264 wt where the second inequality holds since f \u2032(xt, a \u22c6 t , a \u2032) \u2264 0.\nLemma 8. For any t \u2208 [T ] and any arm a \u2208 At, we have f\u22c6(xt, \u03c0f\u22c6(xt), a) \u2264 wt.\nProof of Lemma 8. For any a \u2208 At, by the definition of At, there must exists a function f for which a = \u03c0f (xt). Hence,\nf\u22c6(xt, \u03c0f\u22c6(xt), a) \u2264 f\u22c6(xt, \u03c0f\u22c6(xt), a) \u2212 f(xt, \u03c0f\u22c6(xt), a) \u2264 wt,\nwhere the first inequality holds since f(xt, \u03c0f\u22c6(xt), a) \u2264 0.\nThe following lemma is adapted from Agarwal (2013, Lemma 2).\nLemma 9. The following holds with probability at least 1\u2212 \u03b4 for any T > 3, T\u2211\nt=1\nZt ( f\u22c6(xt, at, bt)\u2212 ft(xt, at, bt) )2 \u2264 4\u03a5 \u03b1 + 16 + 24\u03b1 \u03b12 log ( 4\u03b4\u22121 log(T ) ) .\nProof of Lemma 9. Throughout the proof, we denote zt := (xt, at, bt) for notational simplicity. We define D\u03a6 as the Bregman divergence of the function \u03a6:\nD\u03a6(u, v) = \u03a6(u)\u2212 \u03a6(v)\u2212 \u03c6(v)(u\u2212 v)\nwhere we recall that \u03c6 = \u03a6\u2032 is the derivative of \u03a6. Since \u03a6 is \u03b1-strong convex, we have \u03b1(u \u2212 v)2/2 \u2264 D\u03a6(u, v), and hence,\nT\u2211\nt=1\nZt ( f\u22c6(zt)\u2212 ft(zt) )2 \u2264 2 \u03b1 T\u2211\nt=1\nZtD\u03a6(ft(zt), f \u22c6(zt)). (2)\nHence, it suffice to derive an upper bound for the Bregman divergence in the right hand side above. Define \u03bdt as below:\n\u03bdt :=Zt [ D\u03a6 (ft(zt), f \u22c6(zt))\u2212 (\u2113\u03c6 (ft(zt), yt)\u2212 \u2113\u03c6 (f\u22c6(zt), yt)) ]\n=Zt [ D\u03a6 (ft(zt), f \u22c6(zt))\u2212 (\u03a6 (ft(zt))\u2212 (yt + 1)ft(zt)/2 \u2212 \u03a6 (f\u22c6(zt)) + (yt + 1)f\u22c6(zt)/2) ]\n=Zt [ \u03a6 (ft(zt))\u2212 \u03a6 (f\u22c6(zt))\u2212 \u03c6 (f\u22c6(zt)) (ft(zt)\u2212 f\u22c6(zt))\n\u2212 (\u03a6 (ft(zt))\u2212 (yt + 1)ft(zt)/2\u2212 \u03a6 (f\u22c6(zt)) + (yt + 1)f\u22c6(zt)/2) ]\n=Zt ( ft(zt)\u2212 f\u22c6(zt) )( (yt + 1)/2 \u2212 \u03c6(f\u22c6(zt)) )\nWe note that Et[(yt + 1)/2] = \u03c6(f \u22c6(zt)), and thus Et[\u03bdt] = 0, which means \u03bdt is a martingale difference sequence. Now we bound the value and the conditional variance of \u03bdt in order to derive concentration results.\n1. Bound the value of \u03bdt:\n|\u03bdt| \u2264 |(yt + 1)/2 \u2212 \u03c6 (f\u22c6(zt)) | \u00b7 |ft(zt)\u2212 f\u22c6(zt)| \u2264 1 \u00b7 2 = 2.\n2. Bound the conditional variance of \u03bdt:\nE t [\u03bd2t ] =Zt E t\n[ ((yt + 1)/2 \u2212 \u03c6 (f\u22c6(zt)))2 (ft(zt)\u2212 f\u22c6(zt))2 ]\n\u2264Zt E t\n[ (ft(zt)\u2212 f\u22c6(zt))2 ]\n\u2264Zt E t\n[ 2\n\u03b1 \u00b7D\u03a6(ft(zt), f\u22c6(zt))\n]\n\u22642Zt \u03b1 D\u03a6(ft(zt), f \u22c6(zt))\nwhere for the last line we note that xt, gt are measurable at t.\nNow we apply Lemma 2, which yields for any \u03b4 < 1/e and T > 3, with probability at least 1\u2212 4\u03b4 log(T ),\nT\u2211\nt=1\n\u03bdt \u2264max   2 \u221a\u221a\u221a\u221a T\u2211\nt=1\n2Zt \u03b1\nD\u03a6(ft(zt), f\u22c6(zt)), 6 \u221a log(1/\u03b4)    \u221a log(1/\u03b4)\n\u22642\n\u221a\u221a\u221a\u221a T\u2211\nt=1\n2Zt \u03b1 D\u03a6(ft(zt), f\u22c6(zt))log(1/\u03b4) + 6log(1/\u03b4) (since max(a, b) \u2264 a+ b)\n\u2264 T\u2211\nt=1\n1 2 ZtD\u03a6(ft(zt), f \u22c6(zt)) + 4 log(1/\u03b4) \u03b1 + 6log(1/\u03b4) (AM-GM)\nRecall the definition of \u03bdt, and we conclude that\nT\u2211\nt=1\nZtD\u03a6 (ft(zt), f \u22c6(zt))\u2212\nT\u2211\nt=1\nZt ( \u2113\u03c6 ( ft(zt), yt ) \u2212 \u2113\u03c6 ( f\u22c6(zt), yt )) \u2264\nT\u2211\nt=1\n1 2 ZtD\u03a6(ft(zt), f \u22c6(zt)) + 4 log(1/\u03b4) \u03b1 + 6log(1/\u03b4),\nwhich implies\n1\n2\nT\u2211\nt=1\nZtD\u03a6 (ft(zt), f \u22c6(zt)) \u2264\nT\u2211\nt=1\nZt ( \u2113\u03c6 ( ft(zt), yt ) \u2212 \u2113\u03c6 ( f\u22c6(zt), yt )) + 4 log(1/\u03b4)\n\u03b1 + 6log(1/\u03b4).\nPlugging this upper bound of Bregman divergence into (2), we obtain that, with probability at least 1 \u2212 4\u03b4 log(T ), for any \u03b4 < 1/e and T > 3, we have\nT\u2211\nt=1\nZt ( f\u22c6(zt)\u2212 ft(zt) )2 \u2264 4 \u03b1 \u03a5+\n( 16\n\u03b12 +\n24\n\u03b1\n) log(\u03b4\u22121) =: \u03b2\nFinally, we finish the proof by adjusting the coefficient \u03b4 and taking a union bound to obtain the desired result.\nThe following lemma is a variant of Russo and Van Roy (2013, Proposition 3), with the main difference being that (1) the version space is established using the function produced by the oracle instead of the least squares estimator, and (2) the extra multiplicative factor Zt.\nLemma 10. For Algorithm 1, it holds that\nT\u2211\nt=1\nZt1\n{ sup\nf,f \u2032\u2208Ft\nf(xt, at, bt)\u2212 f \u2032(xt, at, bt) > \u01eb } \u2264 ( 4\u03b2\n\u01eb2 + 1\n) dimE(F , \u01eb) (3)\nfor any constant \u01eb > 0,\nProof of Lemma 10. We first define a subsequence consisting only of the elements for which we made a query in that round. Specifically, we define ((xi1 , ai1 , bi1), (xi2 , ai2 , bi2), . . . , (xik , aik , bik)) where 1 \u2264\ni1 < i2 < \u00b7 \u00b7 \u00b7 < ik \u2264 T and (xt, at, bt) belongs to the subsequence if and only if Zt = 1. We further simplify the notation by defining zj := (xij , aij , bij ) and f(zj) := f(xij , aij , bij ). Then we note that the left-hand side of (3) is equivalent to\nk\u2211\nj=1\n1 { sup\nf,f \u2032\u2208Fj\nf(zj)\u2212 f \u2032(zj) > \u01eb } , (4)\nand the version space in Algorithm 1 is equal to\nFj = { f \u2208 F : j\u22121\u2211\ns=1\n( f(zs)\u2212 ft(zs) )2 \u2264 \u03b2 } . (5)\nHence, it suffice to establish the lower bound for (4) under the version space of (5). To that end, we make one more simplicication in notation: we denote\nw\u2032j := sup f,f \u2032\u2208Fj f(zj)\u2212 f \u2032(zj)\nWe begin by showing that if w\u2032j > \u01eb for some j \u2208 [k], then zj is \u01eb-dependent on at most 4\u03b2/\u01eb2 disjoint subsequence of its predecessors. To see this, we note that when w\u2032j > \u01eb, there must exist two function f, f \u2032 \u2208 Fj such that f(zj) \u2212 f \u2032(zj) > \u01eb. If zj is \u01eb-dependent on a subsequence (zi1 , zi2 , . . . , zin) of its predecessors, we must have\nn\u2211\ns=1\n( f(zis)\u2212 f \u2032(zis) )2 > \u01eb2.\nHence, if zj is \u01eb-dependent on l disjoint subsequences, we have\nj\u22121\u2211\ns=1\n( f(zs)\u2212 f \u2032(zs) )2 > l\u01eb2. (6)\nFor the left-hand side above, we also have\nj\u22121\u2211\ns=1\n( f(zs)\u2212 f \u2032(zs) )2 \u2264 2 j\u22121\u2211\ns=1\n( f(zs)\u2212 ft(zs) )2 + 2 j\u22121\u2211\ns=1\n( ft(zs)\u2212 f \u2032(zs) )2 \u2264 4\u03b2 (7)\nwhere the first inequality holds since (a+ b)2 \u2264 2(a2 + b2) for any a, b, and the second inequality holds by (5). Combining (6) and (7), we get that l \u2264 4\u03b2/\u01eb2. Next, we show that for any sequence (z\u20321, . . . , z \u2032 \u03c4 ), there is at least one element that is \u01eb-dependent on at least \u03c4/d \u2212 1 disjoint subsequence of its predecessors, where d := dimE(F , \u01eb). To show this, let m be the integer satisfying md + 1 \u2264 \u03c4 \u2264 md + d. We will construct m disjoint subsequences, B1, . . . , Bm. At the beginning, let Bi = (z \u2032 i) for i \u2208 [m]. If z\u2032m+1 is \u01eb-dependent on each subsequence B1, . . . , Bm, then we are done. Otherwise, we select a subsequence Bi which z \u2032 m+1 is \u01eb-independent of and append z \u2032 m+1 to Bi. We repeat this process for all elements with indices j > m + 1 until either z \u2032 j is \u01eb-dependent on each\nsubsequence or j = \u03c4 . For the latter, we have \u2211m\ni=1 |Bi| \u2265 md, and since each element of a subsequence\nBi is \u01eb-independent of its predecesors, we must have |Bi| = d for all i. Then, z\u03c4 must be \u01eb-dependent on each subsequence by the definition of eluder dimension.\nFinally, let\u2019s take the sequence (z\u20321, . . . z \u2032 \u03c4 ) to be the subsequence of (z1, . . . , zk) consisting of elements zj for which w\u2032j > \u01eb. As we have established, we have (1) each z \u2032 j is \u01eb-dependent on at most 4\u03b2/\u01eb 2 disjoint subsequences, and (2) some z\u2032j is \u01eb-dependent on at least \u03c4/d\u22121 disjoint subsequences. Therefore, we must have \u03c4/d\u2212 1 \u2264 4\u03b2/\u01eb2, implying that \u03c4 \u2264 (4\u03b2/\u01eb2 + 1)d.\nThe following lemma is adopted from Saha and Krishnamurthy (2022, Lemma 3).\nLemma 11. For any function f \u2208 F and any context x \u2208 X , the following convex program of p \u2208 \u2206(A) is always feasible:\n\u2200a \u2208 A : \u2211\nb\nf(x, a, b)p(b) + 2 \u03b3p(a) \u2264 5A \u03b3 .\nFurthermore, any solution p satisfies:\nE a\u223cp\n[ f\u22c6(x, \u03c0f\u22c6(x), a) ] \u2264 \u03b3\n4 E\na,b\u223cp\n[( f(x, a, b)\u2212 f\u22c6(x, a, b) )2] + 5A\n\u03b3\nwhenever \u03b3 \u2265 2A. Lemma 12. Assume that for each f \u2208 F , there exists an associated function r : X \u00d7A \u2192 [0, 1] such that f(x, a, b) = r(x, a)\u2212 r(x, b) for any x \u2208 X and a, b \u2208 A. In this case, for any context x \u2208 X , if we define p as\np(a) =   \n1 A+\u03b3 ( r(x,\u03c0f (x))\u2212r(x,a) ) a 6= \u03c0f (x) 1\u2212\u2211a6=\u03c0f (x) p(a) a = \u03c0f (x) ,\nthen we have\nE a\u223cp\n[ f\u22c6(x, \u03c0f\u22c6(x), a) ] \u2264 \u03b3 E\na,b\u223cp\n[( f(x, a, b)\u2212 f\u22c6(x, a, b) )2] + A\n\u03b3\nProof of Lemma 12. Fix any b \u2208 A. Then, the distribution p can be rewritten as\np(a) =    ( A+ 2\u03b3 ( r(x,\u03c0f (x))\u2212r(x,b)+1 2 \u2212 r(x,a)\u2212r(x,b)+1 2 ))\u22121 a 6= \u03c0f (x)\n1\u2212\u2211a6=\u03c0f (x) p(a) a = \u03c0f (x) .\nTherefore, denoting f\u22c6(x, a, b) = r\u22c6(x, a) \u2212 r\u22c6(x, b) for some function r\u22c6, we have\nE a\u223cp\n[ f\u22c6(x, \u03c0f\u22c6(x), a) ] = E\na\u223cp\n[ r\u22c6(x, \u03c0f\u22c6(x)) \u2212 r\u22c6(x, a) ]\n=2 E a\u223cp\n[ r\u22c6(x, \u03c0f\u22c6(x))\u2212 r\u22c6(x, b) + 1\n2 \u2212 r \u22c6(x, a)\u2212 r\u22c6(x, b) + 1 2\n]\n\u22642 \u00b7 2\u03b3 E a\u223cp\n[( r(x, a)\u2212 r(x, b) + 1\n2 \u2212 r \u22c6(x, a)\u2212 r\u22c6(x, b) + 1 2\n)2] + A\n\u03b3\n=\u03b3 E a\u223cp\n[( f(x, a, b)\u2212 f\u22c6(x, a, b) )2] + A\n\u03b3\nwhere for the inequality above we invoked Lemma 3 with y\u0302(a) = (r(x, a) \u2212 r(x, b) + 1)/2 and y\u22c6(a) = (r\u22c6(x, a) \u2212 r\u22c6(x, b) + 1)/2. We note that the above holds for any b \u2208 A. Hence, we complete the proof by sampling b \u223c p.\nLemma 13. Assume f\u22c6 \u2208 Ft for all t \u2208 [T ]. Suppose there exists some t\u2032 \u2208 [T ] such that \u03bbt = 0 for all t \u2264 t\u2032. Then we have\nt\u2032\u2211\nt=1\nZtwt \u2264 56A2\u03b2 \u00b7 dimE (F ,\u2206)\n\u2206 \u00b7 log(2/(\u03b4\u2206))\nwith probability at least 1\u2212 \u03b4.\nProof. Since f\u22c6 \u2208 Ft, we always have \u03c0f\u22c6(xt) \u2208 At for all t \u2208 [T ]. Hence, whenever Zt is zero, we have At = {\u03c0f\u22c6(xt)} and thus we do not incur any regret. Hence, we know Ztwt is either 0 or at least \u2206 by Lemma 7. Let us fix an integer m > 1/\u2206, whose value will be specified later. We divide the interval [\u2206, 1] into bins of width 1/m and conduct a refined study of the sum of Ztwt:\nt\u2032\u2211\nt=1\nZtwt \u2264 t\u2032\u2211\nt=1\n(1\u2212\u2206)m\u22121\u2211\nj=0\nZtwt \u00b7 1 { Ztwt \u2208 [ \u2206+ j\nm , \u2206+\nj + 1\nm\n]}\n\u2264 (1\u2212\u2206)m\u22121\u2211\nj=0\n( \u2206+ j + 1\nm\n) t\u2032\u2211\nt=1\nZt1 { wt \u2265 \u2206+ j\nm\n}\n=\n(1\u2212\u2206)m\u22121\u2211\nj=0\n( \u2206+ j + 1\nm\n) t\u2032\u2211\nt=1\nZt1\n{ sup\na,b\u2208At sup f,f \u2032\u2208Ft\nf(xt, a, b) \u2212 f \u2032(xt, a, b) \u2265 \u2206+ j\nm\n}\n=\n(1\u2212\u2206)m\u22121\u2211\nj=0\n( \u2206+ j + 1\nm\n) t\u2032\u2211\nt=1\nZt sup a,b\u2208At 1\n{ sup\nf,f \u2032\u2208Ft\nf(xt, a, b)\u2212 f \u2032(xt, a, b) \u2265 \u2206+ j\nm\n}\n\u2264 (1\u2212\u2206)m\u22121\u2211\nj=0\n( \u2206+ j + 1\nm\n) t\u2032\u2211\nt=1\nZt \u2211\na,b\n1 { sup\nf,f \u2032\u2208Ft\nf(xt, a, b) \u2212 f \u2032(xt, a, b) \u2265 ( \u2206+ j\nm\n)}\n\u2264 (1\u2212\u2206)m\u22121\u2211\nj=0\n( \u2206+ j + 1\nm\n) A2 t\u2032\u2211\nt=1\nZt E a,b\u223cpt 1\n{ sup\nf,f \u2032\u2208Ft\nf(xt, a, b) \u2212 f \u2032(xt, a, b) \u2265 ( \u2206+ j\nm\n)}\n\ufe38 \ufe37\ufe37 \ufe38 (\u2217)\nwhere in the third inequality we replace the supremum over a, b by the summation over a, b, and in the last inequality we further replace it by the expectation. Here recall that pt(a) is uniform when \u03bbt = 0, leading to the extra A2 factor. To deal with (\u2217), we first apply Lemma 4 to recover the empirical at and bt, and then\napply Lemma 10 to get an upper bound via the eluder dimension:\n(\u2217) \u22642 t\u2032\u2211\nt=1\nZt1\n{ sup\nf,f \u2032\u2208Ft\nf(xt, at, bt)\u2212 f \u2032(xt, at, bt) \u2265 ( \u2206+ j\nm\n)} + 8 log(\u03b4\u22121)\n\u22642  \n4\u03b2 ( \u2206+ j\nm\n)2 + 1\n  dimE (F ;\u2206) + 8 log(\u03b4\u22121)\n\u2264 10\u03b2( \u2206+ j\nm\n)2 \u00b7 dimE (F ;\u2206) + 8 log(\u03b4 \u22121)\nwith probability at least 1\u2212 \u03b4. Plugging (\u2217) back, we obtain t\u2032\u2211\nt=1\nZtwt \u2264 (1\u2212\u2206)m\u22121\u2211\nj=0\n( \u2206+ j + 1\nm\n) \u00b7 10A\n2\u03b2 ( \u2206+ j\nm\n)2 \u00b7 dimE (F ;\u2206) + 8mA 2 log(\u03b4\u22121)\n=10A2\u03b2 \u00b7 dimE (F ,\u2206) (1\u2212\u2206)m\u22121\u2211\nj=0\n\u2206+ j+1 m(\n\u2206+ j m\n)2 + 8mA 2 log(\u03b4\u22121)\n\u226410A2\u03b2 \u00b7 dimE (F ,\u2206)\n \u2206+ 1/m\n\u22062 +\n(1\u2212\u2206)m\u22121\u2211\nj=1\n2\n\u2206 + j m\n + 8mA2 log(\u03b4\u22121)\n\u226410A2\u03b2 \u00b7 dimE (F ,\u2206) (1\u2212\u2206)m\u22121\u2211\nj=0\n2\n\u2206 + j m\n+ 8mA2 log(\u03b4\u22121)\n\u226420A2\u03b2 \u00b7 dimE (F ,\u2206) (1\u2212\u2206)m\u22121\u2211\nj=0\n\u222b j\nj\u22121\n1\n\u2206 + x m\ndx+ 8mA2 log(\u03b4\u22121)\n=20A2\u03b2 \u00b7 dimE (F ,\u2206) \u222b (1\u2212\u2206)m\u22121\n\u22121\n1\n\u2206 + x m\ndx+ 8mA2 log(\u03b4\u22121)\n=20A2\u03b2 \u00b7 dimE (F ,\u2206) \u00b7m log (\n1 \u2206\u2212m\u22121 ) + 8mA2 log(\u03b4\u22121)\nwhere for the second inequality, we use the fact that (j + 1)/m \u2264 2j/m for any j \u2265 1; for the third inequality, we assume m > 1/\u2206. Setting m = 2/\u2206, we arrive at\nt\u2032\u2211\nt=1\nZtwt \u226440A2\u03b2 \u00b7 dimE (F ,\u2206)\n\u2206 \u00b7 log(2/\u2206) + 16A2 log(\u03b4\u22121)/\u2206\n\u226456A2\u03b2 \u00b7 dimE (F ,\u2206) \u2206 \u00b7 log(2/(\u03b4\u2206)),\nwhich completes the proof.\nLemma 14. Whenever\n56A2\u03b2 \u00b7 dimE (F ,\u2206) \u00b7 log(2/(\u03b4\u2206))/\u2206 < \u221a AT/\u03b2,\nwe have \u03bb1 = \u03bb2 = \u00b7 \u00b7 \u00b7 = \u03bbT = 0 with probability at least 1\u2212 \u03b4.\nProof of Lemma 14. We prove it via contradiction. Assume the inequality holds but there exists t\u2032 for which \u03bbt\u2032 = 1. Without loss of generality, we assume that \u03bbt = 0 for all t < t\n\u2032, namely that t\u2032 is the first time that \u03bbt is 1. Then by definition of \u03bbt\u2032 , we have\nt\u2032\u22121\u2211\ns=1\nZsws \u2265 \u221a AT/\u03b2.\nOn the other hand, by Lemma 13, we have\nt\u2032\u22121\u2211\ns=1\nZsws \u2264 56A2\u03b2 \u00b7 dimE (F ,\u2206)\n\u2206 \u00b7 log(2/(\u03b4\u2206)).\nThe combination of the above two inequalities contradicts with the conditions."
        },
        {
            "heading": "B.2 Proof of Lemma 1",
            "text": "Proof of Lemma 1. We prove it via contradiction. If no such arm exists, meaning that for any arm a, there exists an arm b such that f\u22c6(x, a, b) < 0. Then we can find a sequence of arms (a1, a2, . . . , ak) such that f\u22c6(x, ai, ai+1) < 0 for any i = 1, . . . , k \u2212 1 and f\u22c6(x, ak, a1) < 0, which contradicts with the transitivity (Assumption 1)."
        },
        {
            "heading": "B.3 Proof of Theorem 1",
            "text": "We begin by showing the worst-case regret upper bound.\nLemma 15 (Worst-case regret upper bound). For Algorithm 1, assume f\u22c6 \u2208 Ft for all t \u2208 [T ]. Then, we have\nRegretCBT \u2264 68 \u221a AT\u03b2 \u00b7 log(4\u03b4\u22121)\nwith probability at least 1\u2212 \u03b4.\nProof of Lemma 15. We recall that the regret is defined as\nRegretCBT =\nT\u2211\nt=1\n( f\u22c6(xt, \u03c0f\u22c6(xt), at) + f \u22c6(xt, \u03c0f\u22c6(xt), bt) ) .\nSince at and bt are always drawn independently from the same distribution in Algorithm 1, we only need to consider the regret of the at part in the following proof for brevity \u2014 multiplying the result by two would yield the overall regret. We first observe the definition of \u03bbt in Algorithm 1: the left term \u2211t\u22121\ns=1 Zsws in the indicator is nondecreasing in t while the right term remains constant. This means that there exists a particular time step t\u2032 \u2208 [T ] dividing the time horizon into two phases: \u03bbt = 0 for all t \u2264 t\u2032 and \u03bbt = 1 for all t > t\u2032. Now, we proceed to examine these two phases individually.\nFor all rounds before or on t\u2032, we can compute the expected partial regret as\nt\u2032\u2211\nt=1\nE a\u223cpt\n[ f\u22c6(xt, \u03c0f\u22c6(xt), a) ] =\nt\u2032\u2211\nt=1\nZt E a\u223cpt\n[ f\u22c6(xt, \u03c0f\u22c6(xt), a) ] \u2264\nt\u2032\u2211\nt=1\nZtwt \u2264 \u221a AT\u03b2, (8)\nwhere the equality holds since we have At = {\u03c0f\u22c6(xt)} whenever Zt = 0 under the condition that f\u22c6 \u2208 Ft, and thus we don\u2019t incur regret in this case. The first inequality is Lemma 8, and the second inequality holds by the definition of \u03bbt and the condition that \u03bbt = 0.\nOn the other hand, for all rounds after t\u2032, we have\nT\u2211\nt=t\u2032+1\nE a\u223cpt\n[ f\u22c6(xt, \u03c0f\u22c6(xt), a) ]\n= T\u2211\nt=t\u2032+1\nZt E a\u223cpt\n[ f\u22c6(xt, \u03c0f\u22c6(xt), a) ]\n\u2264 T\u2211\nt=t\u2032+1\nZt\n( 5A\n\u03b3t + \u03b3t 4 E a,b\u223cpt\n[( f\u22c6(xt, a, b)\u2212 ft(xt, a, b) )2] )\n= T\u2211\nt=t\u2032+1\nZt ( 5A\u221a AT/\u03b2 + \u221a AT/\u03b2 4 E a,b\u223cpt [( f\u22c6(xt, a, b)\u2212 ft(xt, a, b) )2] )\n\u22645 \u221a AT\u03b2 +\n\u221a AT/\u03b2\n4\nT\u2211\nt=t\u2032+1\nZt E a,b\u223cpt\n[( f\u22c6(xt, a, b) \u2212 ft(xt, a, b) )2]\n\u22645 \u221a AT\u03b2 +\n\u221a AT/\u03b2\n2\nT\u2211\nt=t\u2032+1\nZt ( f\u22c6(xt, at, bt)\u2212 ft(xt, at, bt) )2 + 8 \u221a AT/\u03b2 \u00b7 log(4\u03b4\u22121)\n\u22645 \u221a AT\u03b2 +\n\u221a AT\u03b2\n2 + 8 \u221a AT/\u03b2 \u00b7 log(4\u03b4\u22121). (9)\nwhere the first inequality holds by Lemma 11 (or Lemma 12 for specific function classes), the second equality is by the definition of \u03b3t, the third inequality is by Lemma 4, and the fourth inequality holds by Lemma 9.\nPutting the two parts, (8) and (9), together, we arrive at\nT\u2211\nt=1\nE a\u223cpt\n[ f\u22c6(xt, \u03c0f\u22c6(xt), a) ] \u2264 7 \u221a AT\u03b2 + 8 \u221a AT/\u03b2 \u00b7 log(4\u03b4\u22121) \u2264 15 \u221a AT\u03b2 \u00b7 log(4\u03b4\u22121).\nNow we apply Lemma 4 again. The following holds with probability at least 1\u2212 \u03b4/2, T\u2211\nt=1\nf\u22c6(xt, \u03c0f\u22c6(xt), at) \u2264 2 T\u2211\nt=1\nE a\u223cpt\n[ f\u22c6(xt, \u03c0f\u22c6(xt), a) ] + 4 log(4\u03b4\u22121) \u2264 34 \u221a AT\u03b2 \u00b7 log(4\u03b4\u22121).\nThe above concludes the regret of the at part. The regret of the bt can be shown in the same way. Adding them together, we conclude that\nRegretCBT =\nT\u2211\nt=1\n( f\u22c6(xt, \u03c0f\u22c6(xt), at) + f \u22c6(xt, \u03c0f\u22c6(xt), bt) ) \u2264 68 \u221a AT\u03b2 \u00b7 log(4\u03b4\u22121).\nLemma 16 (Instance-dependent regret upper bound). For Algorithm 1, assume f\u22c6 \u2208 Ft for all t \u2208 [T ]. Then, we have\nRegretCBT \u2264 3808A2\u03b22 \u00b7 dimE (F ,\u2206)\n\u2206 \u00b7 log2(4/(\u03b4\u2206))\nwith probability at least 1\u2212 \u03b4.\nProof of Lemma 16. We consider two cases. First, when\n56A2\u03b2 \u00b7 dimE (F ,\u2206) \u2206 \u00b7 log(2/(\u03b4\u2206)) < \u221a AT/\u03b2, (10)\nwe invoke Lemma 14 and get that \u03bbt = 0 for all t \u2208 [T ]. Hence, we have\nRegretCBT =\nT\u2211\nt=1\n( f\u22c6(xt, \u03c0f\u22c6(xt), at) + f \u22c6(xt, \u03c0f\u22c6(xt), bt) )\n\u22642 T\u2211\nt=1\nZtwt\n\u2264112A2\u03b2 \u00b7 dimE (F ,\u2206) \u2206 \u00b7 log(2/(\u03b4\u2206)) \u22643808A2\u03b22 \u00b7 dimE (F ,\u2206) \u2206 \u00b7 log2(4/(\u03b4\u2206))\nwhere the first inequality is by Lemma 8 and the fact that we incur no regret when Zt = 0 since f \u22c6 \u2208 Ft. The second inequality is by Lemma 13.\nOn the other hand, when the contrary of (10) holds, i.e.,\n56A2\u03b2 \u00b7 dimE (F ,\u2206) \u2206 \u00b7 log(2/(\u03b4\u2206)) \u2265 \u221a AT/\u03b2, (11)\napplying Lemma 15, we have\nRegretCBT \u226468 \u221a\nAT\u03b2 \u00b7 log(4\u03b4\u22121) =68\u03b2 \u00b7 log(4\u03b4\u22121) \u00b7 \u221a AT/\u03b2 \u226468\u03b2 \u00b7 log(4\u03b4\u22121) \u00b7 56A2\u03b2 \u00b7 dimE (F ,\u2206) \u2206 \u00b7 log(2/(\u03b4\u2206)) \u22643808A2\u03b22 \u00b7 dimE (F ,\u2206) \u2206 \u00b7 log2(4/(\u03b4\u2206))\nwhere we apply the condition (11) in the second inequality.\nLemma 17 (Query complexity). For Algorithm 1, assume f\u22c6 \u2208 Ft for all t \u2208 [T ]. Then, we have\nQueriesCBT \u2264 min { T, 3136A3\u03b23\ndim2E (F ,\u2206) \u22062\n\u00b7 log2(2/(\u03b4\u2206)) }\nwith probability at least 1\u2212 \u03b4.\nProof of Lemma 17. We consider two cases. First, when\n56A2\u03b2 \u00b7 dimE (F ,\u2206) \u2206 \u00b7 log(2/(\u03b4\u2206)) < \u221a AT/\u03b2 (12)\nwe can invoke Lemma 14 and get that \u03bbt = 0 for all t \u2208 [T ]. Hence,\nQueriesCBT =\nT\u2211\nt=1\nZt\n=\nT\u2211\nt=1\nZt1{wt \u2265 \u2206}\n= T\u2211\nt=1\nZt sup a,b\u2208At 1\n{ sup\nf,f \u2032\u2208Ft\nf(xt, a, b)\u2212 f \u2032(xt, a, b) \u2265 \u2206 }\n\u2264 T\u2211\nt=1\nZt \u2211\na,b\n1 { sup\nf,f \u2032\u2208Ft\nf(xt, a, b) \u2212 f \u2032(xt, a, b) \u2265 \u2206 }\n\u2264A2 T\u2211\nt=1\nZt E a,b\u223cpt 1\n{ sup\nf,f \u2032\u2208Ft\nf(xt, a, b) \u2212 f \u2032(xt, a, b) \u2265 \u2206 }\n\ufe38 \ufe37\ufe37 \ufe38 (\u2217)\nwhere the second equality is by Lemma 7, the second inequality holds as pt(a) is uniform for any a, b when \u03bbt = 0. We apply Lemma 4 and Lemma 10 to (\u2217) and obtain\n(\u2217) \u22642 T\u2211\nt=1\nZt1\n{ sup\nf,f \u2032\u2208Ft\nf(xt, at, bt)\u2212 f \u2032(xt, at, bt) \u2265 \u2206 } + 8 log(\u03b4\u22121)\n\u22642 ( 4\u03b2\n\u22062 + 1\n) dimE(F ;\u2206) + 8 log(\u03b4\u22121)\n\u226410\u03b2 \u22062 \u00b7 dimE(F ;\u2206) + 8 log(\u03b4\u22121).\nPlugging this back, we obtain\nQueriesCBT \u2264 10A2\u03b2\n\u22062 \u00b7 dimE(F ;\u2206) + 8A2 log(\u03b4\u22121)\n\u22643136A3\u03b23dim 2 E (F ,\u2206) \u22062 \u00b7 log2(2/(\u03b4\u2206)).\nOn the other hand, when the contrary of (12) holds, i.e.,\n56A2\u03b2 \u00b7 dimE (F ,\u2206) \u2206 \u00b7 log(2/(\u03b4\u2206)) \u2265 \u221a AT/\u03b2.\nSquaring both sides, we obtain\n3136A4\u03b22 dim2E (F ,\u2206)\n\u22062 \u00b7 log2(2/(\u03b4\u2206)) \u2265 AT/\u03b2\nwhich leads to\nT \u2264 3136A3\u03b23dim 2 E (F ,\u2206) \u22062 \u00b7 log2(2/(\u03b4\u2206)).\nWe note that we always have QueriesCBT \u2264 T , and thus,\nQueriesCBT \u2264 T \u2264 3136A3\u03b23 dim2E (F ,\u2206)\n\u22062 \u00b7 log2(2/(\u03b4\u2206)).\nHence, we complete the proof.\nHaving established the aforementioned lemmas, we are now able to advance towards the proof of Theorem 1.\nProof of Theorem 1. By Lemma 9 and the construction of version spaces Ft in Algorithm 1, we have f\u22c6 \u2208 Ft for all t \u2208 [T ] with probability at least 1 \u2212 \u03b4. Then, the rest of the proof follows from Lemmas 15 to 17."
        },
        {
            "heading": "B.4 Proof of Theorem 2",
            "text": "In this section, we will prove the following theorem, which is stronger than Theorem 2.\nTheorem 5 (Lower bounds). The following two claims hold:\n(1) for any algorithm, there exists an instance that leads to RegretCBT = \u2126( \u221a AT );\n(2) for any algorithm achieving a worse-case expected regret upper bound in the form of E[RegretCBT ] = O( \u221a A \u00b7 T 1\u2212\u03b2) for some \u03b2 > 0, there exists an instance with gap \u2206 = \u221a A \u00b7 T\u2212\u03b2 that results in\nE[RegretCBT ] = \u2126(A/\u2206) = \u2126( \u221a A \u00b7 T \u03b2) and E[QueriesCBT ] = \u2126(A/\u22062) = \u2126(T 2\u03b2).\nWe observe that Theorem 2 can be considered as a corollary of the above theorem when setting \u03b2 = 1/2.\nIn what follows, we will first demonstrate lower bounds in the setting of multi-armed bandits (MAB) with active queries and subsequently establish a reduction from it to contextual dueling bandits in order to achieve these lower bounds. We start by formally defining the setting of MAB with active queries below.\nMulti-armed bandits with active queries. We consider a scenario where there exist A arms. Each arm a is assumed to yield a binary reward (0 or 1), which is sampled from a Bernoulli distribution Bern(r\u0304a), where r\u0304a denotes the mean reward associated with arm a.The arm with the highest mean reward is denoted by a\u22c6 := argmaxa r\u0304a. Let \u2206a := r\u0304a\u22c6 \u2212 r\u0304a denote the gap of arm a \u2208 [A]. The interaction proceeds as follows: at each round t \u2208 [T ], we need to pull an arm but can choose whether to receive the reward signal (denote this choice by Zt). The objective is to minimize two quantities: the regret and the number of queries,\nRegretT =\nT\u2211\nt=1\n\u2206at , QueriesT =\nT\u2211\nt=1\nZt. (13)\nTowards the lower bounds, we will start with a bound on the KL divergence over distributions of runs under two different bandits. This result is a variant of standard results which can be found in many bandit literature (e.g., Lattimore and Szepesva\u0301ri (2020)).\nLemma 18. Let I1 and I2 be two instances of MAB. We define p1 and p2 as their respective distributions over the outcomes of all pulled arms and reward signals when a query is made. Concretely, p1 and p2 are measuring the probability of outcomes (denoted by O) in the following form:\nO = ( Z1, a1, (r1), . . . , ZT , aT , (rT ) )\nwhere the reward rt is included only when Zt = 1, and we added parentheses above to indicate this point. We denote Pr1 (resp. Pr2) as the reward distribution of I1 (resp. I2). We define n\u0304a = \u2211T t=1 Zt1{at = a} as the number of times arm a is pulled when making a query. Then, given any algorithm A, the Kullback\u2013Leibler divergence between p1 and p2 can be decomposed in the following way\nKL(p1, p2) =\nA\u2211\na=1\nE p1 [n\u0304a] \u00b7KL\n( Pr1(r | a),Pr2(r | a) ) .\nProof of Lemma 18. We define the conditional distribution\nPr1(rt |Zt, at) { Pr1(rt | at) if Zt = 1 1 if Zt = 0 ,\nand similarly for Pr2. Additionally, we denote PrA as the probability associated with algorithm A. Then, for any outcome O, we have\np1(O) =\nT\u220f\nt=1\nPrA ( Zt, at |Z1, a1, (r1), . . . , Zt\u22121, at\u22121, (rt\u22121) ) Pr1(rt |Zt, at),\nand we can write p2(O) in a similar manner. Hence,\nKL(p1, p2) = E O\u223cp1\n[ log (\u220fT t=1 PrA ( Zt, at |Z1, a1, (r1), . . . , Zt\u22121, at\u22121, (rt\u22121) ) Pr1(rt |Zt, at)\u220fT\nt=1 PrA ( Zt, at |Z1, a1, (r1), . . . , Zt\u22121, at\u22121, (rt\u22121) ) Pr2(rt |Zt, at)\n)]\n= E O\u223cp1\n[ T\u2211\nt=1\nlog ( Pr1(rt |Zt, at) Pr2(rt |Zt, at)\n)]\n= E O\u223cp1\n[ T\u2211\nt=1\nZt log ( Pr1(rt | at) Pr2(rt | at)\n)]\n= E O\u223cp1\n[ T\u2211\nt=1\nZt E rt\u223cPr1(\u00b7 | at)\n[ log ( Pr1(rt | at) Pr2(rt | at)\n)]]\n= E O\u223cp1\n[ T\u2211\nt=1\nZt \u00b7KL ( Pr1(\u00b7 | at),Pr2(\u00b7 | at)\n) ]\n=\nA\u2211\na=1\nE O\u223cp1\n[n\u0304a] \u00b7KL ( Pr1(\u00b7 | at),Pr2(\u00b7 | at) )\nwhere the third equality holds by the definition of Pr1 and Pr2.\nThe following lemma establishes lower bounds for MAB with active queries. It presents a trade-off between the regret and the number of queries.\nLemma 19. Let I denote the set of all MAB instances. Assume ALG is an algorithm that achieves the following worst-case regret upper bound for some C and \u03b2:\nE [ RegretT ] \u2264 CT 1\u2212\u03b2,\nfor all I \u2208 I . Then, for any MAB instance I \u2208 I , the regret and the number of queries made by algorithm ALG are lower bounded:\nE [ RegretT ] \u2265 \u2211\na6=a\u22c6\n\u03b6\n\u2206a log\n( \u2206a\n4CT\u2212\u03b2\n) , E [ QueriesT ] \u2265 \u2211\na6=a\u22c6\n\u03b6\n\u22062a log\n( \u2206a\n4CT\u2212\u03b2\n)\nwhere the coefficient \u03b6 = minamin{r\u0304a, 1\u2212 r\u0304a} depends on the instance I .\nProof of Lemma 19. For any MAB instance I and any arm a\u2020, we define a corresponding MAB instance I \u2032 as follows. Denote r\u0304 and r\u0304\u2032 as the mean reward of I and I \u2032, respectively. For I \u2032, we set the mean reward r\u0304\u2032a = r\u0304a for any a 6= a\u2020 and r\u0304\u2032a\u2020 = r\u0304a\u2020 + 2\u2206a\u2020 . Consequently, the optimal arm of I \u2032 is a\u2020 with margin \u2206a\u2020 . Let na denote the number of times that arm a is pulled. We define the event\nE = {na\u2020 > T/2}.\nThen, we have\nE p\n[ RegretT ] \u2265 T\u2206a\u2020\n2 \u00b7 p(E), E p\u2032\n[ RegretT ] \u2265 T\u2206a\u2020\n2 \u00b7 p\u2032(E\u2201).\nHence,\n2CT 1\u2212\u03b2 \u2265E p\n[ RegretT ] + E\np\u2032\n[ RegretT ]\n\u2265T\u2206a\u2020 2\n( p(E) + p\u2032(E\u2201) )\n= T\u2206a\u2020\n2\n( 1\u2212 ( p\u2032(E)\u2212 p(E) ))\n\u2265T\u2206a\u2020 2\n( 1\u2212TV ( p, p\u2032 ))\n\u2265T\u2206a\u2020 2\n( 1\u2212 \u221a 1\u2212 exp ( \u2212KL(p, p\u2032) ))\n\u2265T\u2206a\u2020 2 exp ( \u22121 2 \u00b7KL(p, p\u2032) ) .\nBy Lemma 18, we have\nKL(p, p\u2032) = A\u2211\na=1\nE p [n\u0304a] \u00b7KL\n( Pr(r | a),Pr\u2032(r | a) )\n=E p [n\u0304a\u2020 ] \u00b7KL\n( Pr(r | a\u2020),Pr\u2032(r | a\u2020) )\n\u2264E p [n\u0304a\u2020 ] \u00b7\u22062a\u2020 \u00b7 2/\u03b6\nwhere the last inequality is by Lemma 6. Putting the above two inequality together, we arrive at\nE p [n\u0304a\u2020 ] \u2265\n\u03b6\n\u22062 a\u2020\nlog\n( \u2206a\u2020\n4CT\u2212\u03b2\n) .\nThis establishes a query lower bound for arm a\u2020. Consequently, we have\nE[RegretT ] \u2265 \u2211\na6=a\u22c6 E p [n\u0304a] \u00b7\u2206a \u2265\n\u2211\na6=a\u22c6\n\u03b6\n\u2206a log\n( \u2206a\n4CT\u2212\u03b2\n) ,\nand similarly,\nE[QueriesT ] \u2265 \u2211\na6=a\u22c6\nE p [n\u0304a] \u2265\n\u2211\na6=a\u22c6\n\u03b6\n\u22062a log\n( \u2206a\n4CT\u2212\u03b2\n) .\nNow we can proceed with the proof of Theorem 5.\nProof of Theorem 5. We provide a reduction from the multi-armed bandits with active queries to the contextual dueling bandits. Our desired lower bound for the contextual dueling bandit setting thus follows from the above lower bound for Multi-Armed Bandits (MABs). Let ALG denote any algorithm for contextual dueling bandits.\nReduction. Since we focus on the multi-armed bandit where no context is involved, we just ignore the notation of context everywhere for brevity. We will start from an MAB instance, and then simulate a binary feedback and feed it to a dueling bandit algorithm ALG which is used to solve the original MAB instance. Particularly, consider the MAB instance with A-many actions each with an expected reward denoted as r\u0304a.\nAt the beginning of iteration t in the MAB instance, the learner calls the dueling algorithm ALG to generate two actions at and bt. The learner plays at at iteration t to receive a reward yat ; the learner then moves to iteration t + 1 to play bt, and receives reward ybt . At the end of iteration t + 1, the learner simulates a binary feedback by setting o = 1 if yat > ybt ; o = \u22121 if yat < ybt ; o being 1 or \u22121 uniform randomly if yat = ybt . Then, the learner sends (at, bt, o) to the dueling algorithm ALG to query for two actions which will be played at iterations t+ 2 and t+ 3, respectively.\nFrom the dueling algorithm ALG\u2019s perspective, given two actions a and b, we can verify that the probability of seeing label 1 is (r\u0304a\u2212 r\u0304b +1)/2. So we can just specify the link function to be \u03c6(d) = (d+1)/2. As we verified earlier, the corresponding \u03a6 is strongly convex (Example 2). Moreover, since f\u22c6(a, b) = r\u0304a \u2212 r\u0304b, if we define the gap of the MAB instance as \u2206\u0304 := mina6=a\u22c6(r\u0304a\u22c6 \u2212 r\u0304a) where a\u22c6 := argmaxi r\u0304i, then we have \u2206\u0304 = \u2206 in this reduction where \u2206 is the definition of the gap in the dueling setting. We further note that the regret of the MAB instance is\nT\u2211\nt=1\n(r\u0304a\u22c6 \u2212 r\u0304at) + T\u2211\nt=1\n(r\u0304a\u22c6 \u2212 r\u0304bt),\nwhich, by our definition of f\u22c6, is equivalent to the preference-based regret that occurred to the dueling algorithm ALG. The number of queries is clearly equivalent as well. Thus, the regret and the query complexity of the dueling algorithm ALG can be directly translated to the regret and the query complexity of the MAB instance.\nNow, we are ready to prove the two claims in our statement.\nProof of the first claim. We refer the reader to Lattimore and Szepesva\u0301ri (2020, Theorem 15.2) for a proof of the minimax regret lower bound of \u2126( \u221a AT ) for the MAB. Through the reduction outlined above, that lower bound naturally extends to the dueling bandits setting, yielding RegretCBT \u2265 \u2126( \u221a AT ) (otherwise, via the above reduction, we would have achieved an approach that breaks the lower bound of MAB).\nProof of the second claim. We choose an arbitrary MAB for which \u03b6 = minamin{r\u0304a, 1 \u2212 r\u0304a} > 0.2 and the gaps of all arms are equal to \u2206. Invoking Lemma 19, we have\nE [ RegretT ] \u2265 0.2(A \u2212 1)\n\u2206 log\n( \u2206\n4CT\u2212\u03b2\n) \u2265 \u2126 ( A\n\u2206\n) ,\nE [ QueriesT ] \u2265 0.2(A \u2212 1)\n\u22062 log\n( \u2206\n4CT\u2212\u03b2\n) \u2265 \u2126 ( A\n\u22062\n) .\nWe further choose \u2206 = 40CT\u2212\u03b2 and C = \u221a A, leading to\nE [ RegretT ] \u2265 0.2(A \u2212 1)\n40 \u221a A\n\u00b7 T \u03b2 = \u2126 (\u221a A \u00b7 T \u03b2 ) ,\nE [ QueriesT ] \u2265 0.2(A \u2212 1)\n1600A \u00b7 T 2\u03b2 = \u2126\n( T 2\u03b2 ) .\nVia the reduction we have shown above, these lower bounds naturally extend to the contextual dueling bandit setting, thereby completing the proof."
        },
        {
            "heading": "B.4.1 Alternative Lower Bounds Conditioning on the Limit of Regret",
            "text": "In this section, we establish an analogue of Theorem 5 but under a different condition. We first introduce the concept of diminishing regret.\nDefinition 4. We say that an algorithm guarantees a diminishing regret if for all contextual dueling bandit instances and p > 0, it holds that\nlim T\u2192\u221e\nE[RegretCBT ]\nT p = 0.\nThe lower bounds under the assumption of diminishing regret guarantees are stated as follows.\nTheorem 6 (Lower bounds). The following two claims hold:\n(1) for any algorithm, there exists an instance that leads to RegretCBT \u2265 \u2126( \u221a AT );\n(2) for any gap \u2206 and any algorithm achieving diminishing regret, there exists an instance with gap \u2206 that results in E[RegretCBT ] \u2265 \u2126(A/\u2206) and E[QueriesCBT ] \u2265 \u2126(A/\u22062) for sufficiently large T .\nWe should highlight that the condition of diminishing regret (Theorem 6) and the worst-case regret upper bounds (Theorems 2 and 5) are not comparable in general. However, Theorem 6 is also applicable to our algorithm (Algorithm 1) since our algorithm possesses an instance-dependent regret upper bound that is clearly diminishing.\nTo prove Theorem 6, we first show the following lemma, which is a variant of Lemma 19.\nLemma 20. Let I denote the set of all MAB instances. Assume ALG is an algorithm that achieves diminishing regret for all MAB instances in I , i.e., for any I \u2208 I and p > 0, it holds that\nlim T\u2192\u221e\nE[RegretT ]\nT p = 0.\nThen, for any MAB instance I \u2208 I , the regret and the number of queries made by algorithm ALG are lower bounded in the following manner:\nlim inf T\u2192\u221e\nE [ RegretT ]\nlog T \u2265 \u2211\na6=a\u22c6\n\u03b6\n\u2206a , lim inf T\u2192\u221e\nE [ QueriesT ]\nlog T \u2265 \u2211\na6=a\u22c6\n\u03b6\n\u22062a\nwhere the coefficient \u03b6 := minamin{r\u0304a, 1 \u2212 r\u0304a} depends on the instance I . Recall that RegretT and QueriesT are defined in (13).\nProof of Lemma 20. The proof is similar to Lemma 19. For any MAB instance I \u2208 I and any arm a\u2020, we define a corresponding MAB instance I \u2032 as follows. Denote r\u0304 and r\u0304\u2032 as the mean reward of I and I \u2032, respectively. For I \u2032, we set the mean reward r\u0304\u2032a = r\u0304a for any a 6= a\u2020 and r\u0304\u2032a\u2020 = r\u0304a\u2020 + 2\u2206a\u2020 . Consequently, the optimal arm of I \u2032 is a\u2020 with margin \u2206a\u2020 . Let na denote the number of times that arm a is pulled. We define the event\nE = {na\u2020 > T/2}.\nLet p and p\u2032 denote the probability of I and I \u2032, respectively. Then, we have\nE p\n[ RegretT ] \u2265 T\u2206a\u2020\n2 \u00b7 p(E), E p\u2032\n[ RegretT ] \u2265 T\u2206a\u2020\n2 \u00b7 p\u2032(E\u2201)\nwhere E\u2201 means the complement of event E. Hence,\nE p\n[ RegretT ] + E\np\u2032\n[ RegretT ] \u2265T\u2206a\u2020\n2\n( p(E) + p\u2032(E\u2201) )\n= T\u2206a\u2020\n2\n( 1\u2212 ( p\u2032(E) \u2212 p(E) ))\n\u2265T\u2206a\u2020 2\n( 1\u2212 TV ( p, p\u2032 ))\n\u2265T\u2206a\u2020 2\n( 1\u2212 \u221a 1\u2212 exp ( \u2212KL(p, p\u2032) ))\n\u2265T\u2206a\u2020 2 exp ( \u22121 2 \u00b7KL(p, p\u2032) ) .\nHere TV denotes the total variation distance. By Lemma 18, we have\nKL(p, p\u2032) = A\u2211\na=1\nE p [n\u0304a] \u00b7KL\n( Pr(r | a),Pr\u2032(r | a) )\n=E p [n\u0304a\u2020 ] \u00b7KL\n( Pr(r | a\u2020),Pr\u2032(r | a\u2020) )\n\u2264E p [n\u0304a\u2020 ] \u00b7\u22062a\u2020 \u00b7 2/\u03b6\nwhere the last inequality is by Lemma 6. Putting it all together, we arrive at\nE p [n\u0304a\u2020 ] \u2265\n\u03b6\n\u22062 a\u2020\nlog   T\u2206a\u2020 2 ( Ep [ RegretT ] + Ep\u2032 [ RegretT ])   .\nTaking the limit on both sides yields\nlim inf T\u2192\u221e\nEp[n\u0304a\u2020 ]\nlog T \u2265 lim inf T\u2192\u221e\n\u03b6\n\u22062 a\u2020\n\u00b7\nlog\n  T\u2206a\u2020\n2 ( Ep [ RegretT ] +Ep\u2032 [ RegretT ])\n \nlog T\n= lim inf T\u2192\u221e\n\u03b6\n\u22062 a\u2020\n\u00b7  1 + log(\u2206a\u2020/2)\nlog T\ufe38 \ufe37\ufe37 \ufe38 (i)\n\u2212 log ( Ep [ RegretT ] + Ep\u2032 [ RegretT ])\nlog T\ufe38 \ufe37\ufe37 \ufe38 (ii)\n  .\nHere the limit of (i) is clearly 0. For the limit of (ii), we note that by the definition of diminishing regret, for any C > 0, there exists a T \u2032 such that E[RegretT ]/T p \u2264 C for any T > T \u2032. This implies\nlog ( Ep [ RegretT ] + Ep\u2032 [ RegretT ])\nlog T \u2264\nlog ( 2CT p )\nlog T =\nlog(2C)\nlog T + p\nfor any p > 0. Therefore, the limit of (ii) is also 0. Plugging these back, we obtain\nlim inf T\u2192\u221e\nEp[n\u0304a\u2020 ] log T \u2265 \u03b6\n\u22062 a\u2020\n.\nThis establishes a query lower bound for arm a\u2020. Consequently, we have\nlim inf T\u2192\u221e\nE[RegretT ]\nlog T \u2265 lim inf T\u2192\u221e\n\u2211\na6=a\u22c6\nEp[n\u0304a] \u00b7\u2206a log T \u2265 \u2211\na6=a\u22c6\n\u03b6\n\u2206a ,\nand similarly,\nlim inf T\u2192\u221e\nE[QueriesT ]\nlog T \u2265 lim inf T\u2192\u221e\n\u2211\na6=a\u22c6\nEp[n\u0304a] log T \u2265 \u2211\na6=a\u22c6\n\u03b6\n\u22062a .\nNow, we proceed with the proof of Theorem 6.\nProof of Theorem 6. The proof of the first claim is the same as Theorem 5, so we will omit it here. Let us now focus on the proof of the second claim. By Lemma 20, for any algorithm achieving diminishing regret, the following is true for any MAB instance:\nlim inf T\u2192\u221e\nE [ RegretT ]\nlog T \u2265 \u2211\na6=a\u22c6\n\u03b6\n\u2206a , lim inf T\u2192\u221e\nE [ QueriesT ]\nlog T \u2265 \u2211\na6=a\u22c6\n\u03b6\n\u22062a .\nWe choose an arbitrary MAB for which \u03b6 \u2265 0.2 and the gaps of all suboptimal arms are equal to \u2206. Then, for this instance, we have\nlim inf T\u2192\u221e\nE [ RegretT ]\nlog T \u2265 0.2(A \u2212 1) \u2206 , lim inf T\u2192\u221e E\n[ QueriesT ]\nlog T \u2265 0.2(A \u2212 1) \u22062 .\nBy the definition of limit, when T is large enough (exceeding a certain threshold), we have\nE [ RegretT ]\nlog T \u2265 0.1(A \u2212 1) \u2206 , E\n[ QueriesT ]\nlog T \u2265 0.1(A \u2212 1) \u22062 .\nVia the reduction we have shown in the proof of Theorem 5, these lower bounds naturally extend to the contextual dueling bandit setting, thereby completing the proof."
        },
        {
            "heading": "B.5 Proof of Theorem 3",
            "text": "Proof of Theorem 3. We establish the bounds for regret and the number of queries, consecutively. First, we set an arbitrary gap threshold \u01eb > 0. Since our algorithm is independent of \u01eb, we can later choose any \u01eb that minimizes the upper bounds.\nProof of regret. We start with the regret upper bound. By definition, we have\nRegretCBT = T\u2211\nt=1\n( f\u22c6(xt, \u03c0f\u22c6(xt), at) + f \u22c6(xt, \u03c0f\u22c6(xt), bt) ) .\nSince at and bt are always drawn independently from the same distribution in Algorithm 1, we only need to consider the regret of the at part in the following proof for brevity \u2014 multiplying the result by two would yield the overall regret.\nThe worst-case regret upper bound presented in Lemma 15 doesn\u2019t reply on the gap assumption and thus remains applicable in this setting. Hence, we only need to prove the instance-dependent regret upper bound. To that end, we first need an analogue of Lemma 14.\nLemma 21. Fix any \u01eb > 0. Whenever\n2T\u01eb + 56A 2\u03b2 \u00b7 dimE (F , \u01eb)\n\u01eb \u00b7 log(2/(\u03b4\u01eb)) <\n\u221a AT/\u03b2,\nwe have \u03bb1 = \u03bb2 = \u00b7 \u00b7 \u00b7 = \u03bbT = 0 with probability at least 1\u2212 \u03b4.\nProof of Lemma 21. The proof is similar to Lemma 14 and is via contradiction. Assume the inequality holds but there exists t\u2032 for which \u03bbt\u2032 = 1. Without loss of generality, we assume that \u03bbt = 0 for all t < t \u2032, namely that t\u2032 is the first time that \u03bbt is 1. Then by definition of \u03bbt\u2032 , we have\nt\u2032\u22121\u2211\ns=1\nZsws \u2265 \u221a AT/\u03b2.\nOn the other hand, we have\nt\u2032\u22121\u2211\ns=1\nZsws =\nt\u2032\u22121\u2211\ns=1\n1{Gap(xt) \u2264 \u01eb}Zsws + t\u2032\u22121\u2211\ns=1\n1{Gap(xt) > \u01eb}Zsws\n\u22642T\u01eb + 56A2\u03b2 \u00b7 dimE (F , \u01eb)\n\u01eb \u00b7 log(2/(\u03b4\u01eb))\nwhere the inequality is by Lemma 13. The above two inequalities contradicts with the conditions.\nTowards an instance-dependent regret upper bound, we adapt the proof of Lemma 16 to this setting. We consider two cases. First, when\n2T\u01eb + 56A 2\u03b2 \u00b7 dimE (F , \u01eb)\n\u01eb \u00b7 log(2/(\u03b4\u01eb)) <\n\u221a AT/\u03b2, (14)\nwe invoke Lemma 21 and get that \u03bbt = 0 for all t \u2208 [T ]. Hence, we have\nRegretCBT =\nT\u2211\nt=1\n( f\u22c6(xt, \u03c0f\u22c6(xt), at) + f \u22c6(xt, \u03c0f\u22c6(xt), bt) )\n\u22642 T\u2211\nt=1\n1{Gap(xt) \u2264 \u01eb}Ztwt + 2 T\u2211\nt=1\n1{Gap(xt) > \u01eb}Ztwt\n\u22644T\u01eb + 112A2\u03b2 \u00b7 dimE (F , \u01eb)\n\u01eb \u00b7 log(2/(\u03b4\u01eb))\n\u2264136\u03b2 \u00b7 log(4\u03b4\u22121) \u00b7 T\u01eb + 3808A2\u03b22 \u00b7 dimE (F , \u01eb)\n\u01eb \u00b7 log2(4/(\u03b4\u01eb))\nwhere the first inequality is by Lemma 8 and the fact that we incur no regret when Zt = 0 since f \u22c6 \u2208 Ft. The second inequality is by Lemma 13.\nOn the other hand, when the contrary of (14) holds, i.e.,\n2T\u01eb + 56A 2\u03b2 \u00b7 dimE (F , \u01eb)\n\u01eb \u00b7 log(2/(\u03b4\u01eb)) \u2265\n\u221a AT/\u03b2, (15)\napplying Lemma 15, we have\nRegretCBT \u226468 \u221a\nAT\u03b2 \u00b7 log(4\u03b4\u22121) =68\u03b2 \u00b7 log(4\u03b4\u22121) \u00b7 \u221a AT/\u03b2 \u226468\u03b2 \u00b7 log(4\u03b4\u22121) \u00b7 ( 2T\u01eb + 56A\n2\u03b2 \u00b7 dimE (F , \u01eb) \u01eb\n\u00b7 log(2/(\u03b4\u01eb)) )\n\u2264136\u03b2 \u00b7 log(4\u03b4\u22121) \u00b7 T\u01eb + 3808A2\u03b22 \u00b7 dimE (F , \u01eb)\n\u01eb \u00b7 log2(4/(\u03b4\u01eb))\nwhere we apply the condition (15) in the second inequality.\nProof of the number of queries. To show an upper bound for the number of queries, we also consider two cases. First, when\n2T\u01eb + 56A 2\u03b2 \u00b7 dimE (F , \u01eb)\n\u01eb \u00b7 log(2/(\u03b4\u01eb)) <\n\u221a AT/\u03b2, (16)\nwe can invoke Lemma 21 and get that \u03bbt = 0 for all t \u2208 [T ]. Hence, similar to the proof of Lemma 17, we have\nQueriesCBT =\nT\u2211\nt=1\nZt\n= T\u2211\nt=1\nZt1{Gap(xt) < \u01eb}+ T\u2211\nt=1\nZt1{Gap(xt) \u2265 \u01eb}\n=T\u01eb +\nT\u2211\nt=1\nZt sup a,b\u2208At 1\n{ sup\nf,f \u2032\u2208Ft\nf(xt, a, b) \u2212 f \u2032(xt, a, b) \u2265 \u01eb }\n\u2264T\u01eb + T\u2211\nt=1\nZt \u2211\na,b\n1 { sup\nf,f \u2032\u2208Ft\nf(xt, a, b)\u2212 f \u2032(xt, a, b) \u2265 \u01eb }\n\u2264T\u01eb +A2 T\u2211\nt=1\nZt E a,b\u223cpt 1\n{ sup\nf,f \u2032\u2208Ft\nf(xt, a, b) \u2212 f \u2032(xt, a, b) \u2265 \u01eb }\n\ufe38 \ufe37\ufe37 \ufe38 (\u2217)\nwhere the second inequality holds as pt(a) is uniform for any a, b when \u03bbt = 0. We apply Lemma 4 and Lemma 10 to (\u2217) and obtain\n(\u2217) \u22642 T\u2211\nt=1\nZt1\n{ sup\nf,f \u2032\u2208Ft\nf(xt, at, bt)\u2212 f \u2032(xt, at, bt) \u2265 \u01eb } + 8 log(\u03b4\u22121)\n\u22642 ( 4\u03b2\n\u01eb2 + 1\n) dimE(F ; \u01eb) + 8 log(\u03b4\u22121)\n\u226410\u03b2 \u01eb2 \u00b7 dimE(F ; \u01eb) + 8 log(\u03b4\u22121).\nPlugging this back, we obtain\nQueriesCBT \u2264T\u01eb + 10A2\u03b2\n\u01eb2 \u00b7 dimE(F ; \u01eb) + 8A2 log(\u03b4\u22121)\n\u22648T 2\u01eb \u03b2/A+ 6272A3\u03b23 dim2E (F , \u01eb)\n\u01eb2 \u00b7 log2(2/(\u03b4\u01eb))\nwhere the second line corresponds to the upper bound derived from the alternative case, which is shown below.\nWhen the contrary of (16) holds, i.e.,\n2T\u01eb + 56A 2\u03b2 \u00b7 dimE (F , \u01eb)\n\u01eb \u00b7 log(2/(\u03b4\u01eb)) \u2265\n\u221a AT/\u03b2.\nSquaring both sides and leveraging the inequality (a+ b)2 \u2264 2a2 + 2b2, we obtain\n8T 2\u01eb + 6272A 4\u03b22 dim2E (F , \u01eb) \u01eb2 \u00b7 log2(2/(\u03b4\u01eb)) \u2265 AT/\u03b2\nwhich leads to\nT \u2264 8T 2\u01eb \u03b2/A+ 6272A3\u03b23 dim2E (F , \u01eb)\n\u01eb2 \u00b7 log2(2/(\u03b4\u01eb)).\nWe note that we always have QueriesCBT \u2264 T and thus\nQueriesCBT \u2264 T \u2264 8T 2\u01eb \u03b2/A+ 6272A3\u03b23 dim2E (F , \u01eb)\n\u01eb2 \u00b7 log2(2/(\u03b4\u01eb)).\nMinimizing on \u01eb. Given that the aforementioned proofs hold for any threshold \u01eb, we can select the specific value of \u01eb that minimizes the upper bounds. Hence, we deduce the desired result."
        },
        {
            "heading": "B.6 Proof of Theorem 4",
            "text": "Proof of Theorem 4. The upper bound of the number of queries is straightforward: Algorithm 2 is simply running H instances of Algorithm 1, so the total number of queries is simply the sum of these H instances. For bounding the regret, we have\nRegretILT = T\u2211\nt=1\nV \u03c0e0 (xt,0)\u2212 V \u03c0t0 (xt,0)\n\u2264 H\u22121\u2211\nh=0\nT\u2211\nt=1\nE xt,h,at,h\u223cd\n\u03c0t xt,0,h\n[ Q\u03c0eh (xt,h, \u03c0 \u03c0e h (xt,h))\u2212Q \u03c0e h (xt,h, at,h) ]\n\u2264 H\u22121\u2211\nh=0\nT\u2211\nt=1\nE xt,h,at,h\u223cd\n\u03c0t xt,0,h\n[ Q\u03c0eh (xt,h, \u03c0 + h (xt,h))\u2212Q \u03c0e h (xt,h, at,h) ]\n\u2212 H\u22121\u2211\nh=0\nT\u2211\nt=1\nE xt,h\u223cd\n\u03c0t xt,0,h\n[ A\u03c0eh (xt,h, \u03c0 + h (xt,h)) ]\n\u2264H \u00b7 E [ RegretCBT ] \u2212AdvT .\nwhere the first inequality holds by Lemma 5, and we denote \u03c0+h (xt,h) = argmaxaQ \u03c0e h (xt,h, a) in the second inequality. Then, we can plug the upper bound of RegretCBT (Theorem 1). Moreover, we need to take a union bound over all h \u2208 [H]."
        }
    ],
    "title": "Contextual Bandits and Imitation Learning via Preference-Based Active Queries*",
    "year": 2023
}