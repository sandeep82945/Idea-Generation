{
    "abstractText": "Large language models (LLMs) are increasingly being used for tasks beyond text generation, including complex tasks such as data labeling, information extraction, etc. With the recent surge in research efforts to comprehend the full extent of LLM capabilities, in this work, we investigate the role of LLMs as counterfactual explanation modules, to explain decisions of black-box text classifiers. Inspired by causal thinking, we propose a pipeline for using LLMs to generate post-hoc, model-agnostic counterfactual explanations in a principled way via (i) leveraging the textual understanding capabilities of the LLM to identify and extract latent features, and (ii) leveraging the perturbation and generation capabilities of the same LLM to generate a counterfactual explanation by perturbing input features derived from the extracted latent features. We evaluate three variants of our framework, with varying degrees of specificity, on a suite of state-of-theart LLMs, including ChatGPT and LLaMA 2. We evaluate the effectiveness and quality of the generated counterfactual explanations, over a variety of text classification benchmarks. Our results show varied performance of these models in different settings, with a full two-step feature extraction based variant outperforming others in most cases. Our pipeline can be used in automated explanation systems, potentially reducing human effort.",
    "authors": [
        {
            "affiliations": [],
            "name": "Amrita Bhattacharjee"
        },
        {
            "affiliations": [],
            "name": "Raha Moraffah"
        },
        {
            "affiliations": [],
            "name": "Joshua Garland"
        },
        {
            "affiliations": [],
            "name": "Huan Liu"
        }
    ],
    "id": "SP:a12236d60b4ba8a92341db369ed5d06fc3a8d210",
    "references": [
        {
            "authors": [
                "Amina Adadi",
                "Mohammed Berrada."
            ],
            "title": "Peeking inside the black-box: a survey on explainable artificial intelligence (xai)",
            "venue": "IEEE access, 6:52138\u201352160.",
            "year": 2018
        },
        {
            "authors": [
                "Pepa Atanasova",
                "Jakob Grue Simonsen",
                "Christina Lioma",
                "Isabelle Augenstein."
            ],
            "title": "A diagnostic study of explainability techniques for text classification",
            "venue": "arXiv preprint arXiv:2009.13295.",
            "year": 2020
        },
        {
            "authors": [
                "Parikshit Bansal",
                "Amit Sharma."
            ],
            "title": "Large language models as annotators: Enhancing generalization of nlp models at minimal cost",
            "venue": "arXiv preprint arXiv:2306.15766.",
            "year": 2023
        },
        {
            "authors": [
                "Samuel R Bowman",
                "Gabor Angeli",
                "Christopher Potts",
                "Christopher D Manning."
            ],
            "title": "A large annotated corpus for learning natural language inference",
            "venue": "arXiv preprint arXiv:1508.05326.",
            "year": 2015
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "S\u00e9bastien Bubeck",
                "Varun Chandrasekaran",
                "Ronen Eldan",
                "Johannes Gehrke",
                "Eric Horvitz",
                "Ece Kamar",
                "Peter Lee",
                "Yin Tat Lee",
                "Yuanzhi Li",
                "Scott Lundberg"
            ],
            "title": "Sparks of artificial general intelligence: Early experiments with gpt-4",
            "year": 2023
        },
        {
            "authors": [
                "Oana-Maria Camburu",
                "Tim Rockt\u00e4schel",
                "Thomas Lukasiewicz",
                "Phil Blunsom."
            ],
            "title": "e-snli: Natural language inference with natural language explanations",
            "venue": "Advances in Neural Information Processing Systems, 31.",
            "year": 2018
        },
        {
            "authors": [
                "Paul F Christiano",
                "Jan Leike",
                "Tom Brown",
                "Miljan Martic",
                "Shane Legg",
                "Dario Amodei."
            ],
            "title": "Deep reinforcement learning from human preferences",
            "venue": "Advances in neural information processing systems, 30.",
            "year": 2017
        },
        {
            "authors": [
                "Tim Dettmers",
                "Artidoro Pagnoni",
                "Ari Holtzman",
                "Luke Zettlemoyer."
            ],
            "title": "Qlora: Efficient finetuning of quantized llms",
            "venue": "arXiv preprint arXiv:2305.14314.",
            "year": 2023
        },
        {
            "authors": [
                "Qingxiu Dong",
                "Lei Li",
                "Damai Dai",
                "Ce Zheng",
                "Zhiyong Wu",
                "Baobao Chang",
                "Xu Sun",
                "Jingjing Xu",
                "Zhifang Sui."
            ],
            "title": "A survey for in-context learning",
            "venue": "arXiv preprint arXiv:2301.00234.",
            "year": 2022
        },
        {
            "authors": [
                "Alexander Dunn",
                "John Dagdelen",
                "Nicholas Walker",
                "Sanghoon Lee",
                "Andrew S Rosen",
                "Gerbrand Ceder",
                "Kristin Persson",
                "Anubhav Jain."
            ],
            "title": "Structured information extraction from complex scientific text with fine-tuned large language models",
            "venue": "arXiv",
            "year": 2022
        },
        {
            "authors": [
                "Leo Gao",
                "Stella Biderman",
                "Sid Black",
                "Laurence Golding",
                "Travis Hoppe",
                "Charles Foster",
                "Jason Phang",
                "Horace He",
                "Anish Thite",
                "Noa Nabeshima"
            ],
            "title": "The pile: An 800gb dataset of diverse text for language modeling",
            "venue": "arXiv preprint arXiv:2101.00027",
            "year": 2020
        },
        {
            "authors": [
                "Riccardo Guidotti",
                "Anna Monreale",
                "Salvatore Ruggieri",
                "Franco Turini",
                "Fosca Giannotti",
                "Dino Pedreschi."
            ],
            "title": "A survey of methods for explaining black box models",
            "venue": "ACM computing surveys (CSUR), 51(5):1\u2013",
            "year": 2018
        },
        {
            "authors": [
                "Xingwei He",
                "Zhenghao Lin",
                "Yeyun Gong",
                "Alex Jin",
                "Hang Zhang",
                "Chen Lin",
                "Jian Jiao",
                "Siu Ming Yiu",
                "Nan Duan",
                "Weizhu Chen"
            ],
            "title": "Annollm: Making large language models to be better crowdsourced annotators",
            "venue": "arXiv preprint arXiv:2303.16854",
            "year": 2023
        },
        {
            "authors": [
                "Stefan Hegselmann",
                "Alejandro Buendia",
                "Hunter Lang",
                "Monica Agrawal",
                "Xiaoyi Jiang",
                "David Sontag."
            ],
            "title": "Tabllm: Few-shot classification of tabular data with large language models",
            "venue": "International Conference on Artificial Intelligence and Statistics,",
            "year": 2023
        },
        {
            "authors": [
                "Marc H\u00f6fler."
            ],
            "title": "Causal inference based on counterfactuals",
            "venue": "BMC medical research methodology, 5(1):1\u201312.",
            "year": 2005
        },
        {
            "authors": [
                "Fan Huang",
                "Haewoon Kwak",
                "Jisun An."
            ],
            "title": "Chain of explanation: New prompting method to generate higher quality natural language explanation for implicit hate speech",
            "venue": "arXiv preprint arXiv:2209.04889.",
            "year": 2022
        },
        {
            "authors": [
                "Sarthak Jain",
                "Byron C Wallace."
            ],
            "title": "Attention is not explanation",
            "venue": "arXiv preprint arXiv:1902.10186.",
            "year": 2019
        },
        {
            "authors": [
                "Nayeon Lee",
                "Belinda Z Li",
                "Sinong Wang",
                "Wen-tau Yih",
                "Hao Ma",
                "Madian Khabsa"
            ],
            "title": "Language models as fact checkers? arXiv preprint arXiv:2006.04102",
            "year": 2020
        },
        {
            "authors": [
                "Vladimir I Levenshtein"
            ],
            "title": "Binary codes capable of correcting deletions, insertions, and reversals",
            "venue": "Soviet physics doklady, volume 10, pages 707\u2013710. Soviet Union.",
            "year": 1966
        },
        {
            "authors": [
                "Miaoran Li",
                "Baolin Peng",
                "Zhu Zhang."
            ],
            "title": "Selfchecker: Plug-and-play modules for fact-checking with large language models",
            "venue": "arXiv preprint arXiv:2305.14623.",
            "year": 2023
        },
        {
            "authors": [
                "Shiyang Li",
                "Jianshu Chen",
                "Yelong Shen",
                "Zhiyu Chen",
                "Xinlu Zhang",
                "Zekun Li",
                "Hong Wang",
                "Jing Qian",
                "Baolin Peng",
                "Yi Mao"
            ],
            "title": "Explanations from large language models make small reasoners better. arXiv preprint arXiv:2210.06726",
            "year": 2022
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Andrew L. Maas",
                "Raymond E. Daly",
                "Peter T. Pham",
                "Dan Huang",
                "Andrew Y. Ng",
                "Christopher Potts."
            ],
            "title": "Learning word vectors for sentiment analysis",
            "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human",
            "year": 2011
        },
        {
            "authors": [
                "Bill MacCartney",
                "Christopher D Manning."
            ],
            "title": "Modeling semantic containment and exclusion in natural language inference",
            "venue": "Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 521\u2013528.",
            "year": 2008
        },
        {
            "authors": [
                "Nishtha Madaan",
                "Srikanta Bedathur",
                "Diptikalyan Saha."
            ],
            "title": "Plug and play counterfactual text generation for model robustness",
            "venue": "arXiv preprint arXiv:2206.10429.",
            "year": 2022
        },
        {
            "authors": [
                "Nishtha Madaan",
                "Inkit Padhi",
                "Naveen Panwar",
                "Diptikalyan Saha."
            ],
            "title": "Generate your counterfactuals: Towards controlled counterfactual generation for text",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 13516\u201313524.",
            "year": 2021
        },
        {
            "authors": [
                "Christoph Molnar."
            ],
            "title": "Interpretable machine learning",
            "venue": "Lulu. com.",
            "year": 2020
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "Gpt-4 technical report",
            "venue": "arXiv, pages 2303\u201308774.",
            "year": 2023
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeffrey Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human",
            "year": 2022
        },
        {
            "authors": [
                "Guilherme Penedo",
                "Quentin Malartic",
                "Daniel Hesslow",
                "Ruxandra Cojocaru",
                "Alessandro Cappelli",
                "Hamza Alobeidli",
                "Baptiste Pannier",
                "Ebtesam Almazrouei",
                "Julien Launay"
            ],
            "title": "The refinedweb dataset for falcon llm: outperforming curated corpora",
            "year": 2023
        },
        {
            "authors": [
                "Fabio Petroni",
                "Tim Rockt\u00e4schel",
                "Patrick Lewis",
                "Anton Bakhtin",
                "Yuxiang Wu",
                "Alexander H Miller",
                "Sebastian Riedel"
            ],
            "title": "Language models as knowledge bases? arXiv preprint arXiv:1909.01066",
            "year": 2019
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Nazneen Fatema Rajani",
                "Bryan McCann",
                "Caiming Xiong",
                "Richard Socher."
            ],
            "title": "Explain yourself! leveraging language models for commonsense reasoning",
            "venue": "arXiv preprint arXiv:1906.02361.",
            "year": 2019
        },
        {
            "authors": [
                "Yanou Ramon",
                "David Martens",
                "Foster Provost",
                "Theodoros Evgeniou."
            ],
            "title": "A comparison of instance-level counterfactual explanation algorithms for behavioral and textual data: Sedc, lime-c and shap-c",
            "venue": "Advances in Data Analysis and Classifica-",
            "year": 2020
        },
        {
            "authors": [
                "Marco Tulio Ribeiro",
                "Sameer Singh",
                "Carlos Guestrin."
            ],
            "title": " why should i trust you?\" explaining the predictions of any classifier",
            "venue": "Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pages 1135\u2013",
            "year": 2016
        },
        {
            "authors": [
                "Marcel Robeer",
                "Floris Bex",
                "Ad Feelders."
            ],
            "title": "Generating realistic natural language counterfactuals",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3611\u20133625.",
            "year": 2021
        },
        {
            "authors": [
                "Adam Roberts",
                "Colin Raffel",
                "Noam Shazeer"
            ],
            "title": "How much knowledge can you pack into the parameters of a language model? arXiv preprint arXiv:2002.08910",
            "year": 2020
        },
        {
            "authors": [
                "Victor Sanh",
                "Lysandre Debut",
                "Julien Chaumond",
                "Thomas Wolf."
            ],
            "title": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
            "venue": "arXiv preprint arXiv:1910.01108.",
            "year": 2019
        },
        {
            "authors": [
                "Yucheng Shi",
                "Hehuan Ma",
                "Wenliang Zhong",
                "Gengchen Mai",
                "Xiang Li",
                "Tianming Liu",
                "Junzhou Huang."
            ],
            "title": "Chatgraph: Interpretable text classification by converting chatgpt knowledge to graphs",
            "venue": "arXiv preprint arXiv:2305.03513.",
            "year": 2023
        },
        {
            "authors": [
                "Karan Singhal",
                "Shekoofeh Azizi",
                "Tao Tu",
                "S Sara Mahdavi",
                "Jason Wei",
                "Hyung Won Chung",
                "Nathan Scales",
                "Ajay Tanwani",
                "Heather Cole-Lewis",
                "Stephen Pfohl"
            ],
            "title": "Large language models encode clinical knowledge",
            "venue": "arXiv preprint arXiv:2212.13138",
            "year": 2022
        },
        {
            "authors": [
                "Chi Sun",
                "Xipeng Qiu",
                "Yige Xu",
                "Xuanjing Huang"
            ],
            "title": "How to fine-tune bert for text classification",
            "venue": "In Chinese Computational Linguistics: 18th China National Conference,",
            "year": 2019
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "2023a. Llama: Open and efficient foundation language models",
            "year": 2023
        },
        {
            "authors": [
                "Hugo Touvron",
                "Louis Martin",
                "Kevin Stone",
                "Peter Albert",
                "Amjad Almahairi",
                "Yasmine Babaei",
                "Nikolay Bashlykov",
                "Soumya Batra",
                "Prajjwal Bhargava",
                "Shruti Bhosale"
            ],
            "title": "2023b. Llama 2: Open foundation and fine-tuned chat models",
            "year": 2023
        },
        {
            "authors": [
                "Miles Turpin",
                "Julian Michael",
                "Ethan Perez",
                "Samuel R Bowman."
            ],
            "title": "Language models don\u2019t always say what they think: Unfaithful explanations in chain-of-thought prompting",
            "venue": "arXiv preprint arXiv:2305.04388.",
            "year": 2023
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems, 30.",
            "year": 2017
        },
        {
            "authors": [
                "Sandra Wachter",
                "Brent Mittelstadt",
                "Chris Russell."
            ],
            "title": "Counterfactual explanations without opening the black box: Automated decisions and the gdpr",
            "venue": "Harv. JL & Tech., 31:841.",
            "year": 2017
        },
        {
            "authors": [
                "Alex Wang",
                "Yada Pruksachatkun",
                "Nikita Nangia",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel Bowman."
            ],
            "title": "Superglue: A stickier benchmark for general-purpose language understanding systems",
            "venue": "Advances in neural information",
            "year": 2019
        },
        {
            "authors": [
                "Alex Wang",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel R Bowman."
            ],
            "title": "Glue: A multi-task benchmark and analysis platform for natural language understanding",
            "venue": "arXiv preprint arXiv:1804.07461.",
            "year": 2018
        },
        {
            "authors": [
                "Yizhong Wang",
                "Yeganeh Kordi",
                "Swaroop Mishra",
                "Alisa Liu",
                "Noah A Smith",
                "Daniel Khashabi",
                "Hannaneh Hajishirzi."
            ],
            "title": "Self-instruct: Aligning language model with self generated instructions",
            "venue": "arXiv preprint arXiv:2212.10560.",
            "year": 2022
        },
        {
            "authors": [
                "Xiang Wei",
                "Xingyu Cui",
                "Ning Cheng",
                "Xiaobin Wang",
                "Xin Zhang",
                "Shen Huang",
                "Pengjun Xie",
                "Jinan Xu",
                "Yufeng Chen",
                "Meishan Zhang"
            ],
            "title": "Zeroshot information extraction via chatting with chatgpt",
            "venue": "arXiv preprint arXiv:2302.10205",
            "year": 2023
        },
        {
            "authors": [
                "Sarah Wiegreffe",
                "Jack Hessel",
                "Swabha Swayamdipta",
                "Mark Riedl",
                "Yejin Choi."
            ],
            "title": "Reframing human-ai collaboration for generating free-text explanations",
            "venue": "arXiv preprint arXiv:2112.08674.",
            "year": 2021
        },
        {
            "authors": [
                "Sarah Wiegreffe",
                "Yuval Pinter."
            ],
            "title": "Attention is not not explanation",
            "venue": "arXiv preprint arXiv:1908.04626.",
            "year": 2019
        },
        {
            "authors": [
                "Tongshuang Wu",
                "Marco Tulio Ribeiro",
                "Jeffrey Heer",
                "Daniel S Weld."
            ],
            "title": "Polyjuice: Generating counterfactuals for explaining, evaluating, and improving models",
            "venue": "arXiv preprint arXiv:2101.00288.",
            "year": 2021
        },
        {
            "authors": [
                "Can Xu",
                "Qingfeng Sun",
                "Kai Zheng",
                "Xiubo Geng",
                "Pu Zhao",
                "Jiazhan Feng",
                "Chongyang Tao",
                "Daxin Jiang."
            ],
            "title": "Wizardlm: Empowering large language models to follow complex instructions",
            "venue": "arXiv preprint arXiv:2304.12244.",
            "year": 2023
        },
        {
            "authors": [
                "Linyi Yang",
                "Eoin M Kenny",
                "Tin Lok James Ng",
                "Yi Yang",
                "Barry Smyth",
                "Ruihai Dong."
            ],
            "title": "Generating plausible counterfactual explanations for deep transformers in financial text classification",
            "venue": "arXiv preprint arXiv:2010.12512.",
            "year": 2020
        },
        {
            "authors": [
                "Zhilin Yang",
                "Zihang Dai",
                "Yiming Yang",
                "Jaime Carbonell",
                "Russ R Salakhutdinov",
                "Quoc V Le."
            ],
            "title": "Xlnet: Generalized autoregressive pretraining for language understanding",
            "venue": "Advances in neural information processing systems, 32.",
            "year": 2019
        },
        {
            "authors": [
                "Xi Ye",
                "Greg Durrett."
            ],
            "title": "The unreliability of explanations in few-shot prompting for textual reasoning",
            "venue": "Advances in neural information processing systems, 35:30378\u201330392.",
            "year": 2022
        },
        {
            "authors": [
                "Andrea Zaninello",
                "Bernardo Magnini."
            ],
            "title": "A smashed glass cannot be full: Generation of commonsense explanations through prompt-based fewshot learning",
            "venue": "Proceedings of the 1st Workshop on Natural Language Reasoning and Structured Expla-",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Explainability and interpretability are major challenges in many NLP applications such as text classification (Ribeiro et al., 2016; Atanasova et al., 2020; Camburu et al., 2018). The expressiveness of complex models involving deep neural networks and pre-trained transformer based language models (Vaswani et al., 2017) has enabled huge performance improvements on a variety of text classification benchmarks, often exceeding human-level performance as well (Wang et al., 2018, 2019).\nHowever, with these complex models, comes the challenge of interpretability. Such deep neural network based classification models are essentially black-boxes and do not offer transparency on why it predicted a certain label, or even what features in the input resulted in the prediction. The ubiquity of these black-box classifiers necessitates the development of explanation frameworks and techniques that provide some degree of understanding into the decision-making function of the model. Broadly there exist two kinds of explainability: intrinsic explainability wherein the model is explainable by design such as decision trees or logistic regression, and post-hoc explainability, where the explanation module is not directly tied to the classification module (Adadi and Berrada, 2018). Explanations can also be classified into model-agnostic or model specific (Adadi and Berrada, 2018; Guidotti et al., 2018). Many works in NLP have used attention scores as a proxy for explanation (Wiegreffe and Pinter, 2019), in order to identify which parts of the input resulted in the model predicted output, although there are works disputing the use of attention as explanation (Jain and Wallace, 2019). ar X iv :2 30 9. 13 34\n0v 1\n[ cs\n.C L\n] 2\n3 Se\np 20\n23\nWhile there is no universal agreed-upon type of explainability in different NLP tasks, (Atanasova et al., 2020) put forward a set of diagnostic properties to evaluate different categories of explanation methods. Some of these are (i) agreement with human rationales, (ii) confidence indication, (iii) faithfulness, etc.\nOne of the more human-readable types of explanation in NLP are free-text explanations, which are not necessarily grounded in the input, but provide some implicit or explicit reason as to why the input was assigned a specific label. A lot of work has been done in the realm of generating free-text explanations, and more recently generating such explanations using language models (Zaninello and Magnini, 2023; Wiegreffe et al., 2021; Huang et al., 2022). However, free-text explanations are often unreliable and do not offer good understanding of what exactly the model is doing, and are often not factually grounded in the input (Ye and Durrett, 2022). Furthermore, such explanation is also susceptible to spurious correlations in the input prompt used to generate such explanations (Turpin et al., 2023). Therefore, in this work, we investigate a more challenging type of explanations: counterfactual explanations. In order to generate plausible, human-interpretable explanations, we use the latest instruction-tuned conversational large language models to generate our counterfactual explanations. To evaluate faithfulness of these explanations, we incorporate a feedback loop in our framework and probe the black-box model with generated counterfactuals to verify the correctness of the explanation. To the best of our knowledge, this is the first piece of work to propose using LLMs as Counterfactual Explanation Modules. Alongside the novelty of the task, we also propose a novel pipeline to facilitate this task, and experiment on text classification and natural language inference benchmark datasets. Overall our contributions in this paper are as follows:\n1. We propose a Framework for Language Models in Automated CounteRfactual Explanation Generation called FLARE, along with two additional variants FLAREwords and FLAREvanilla.\n2. We design and structure effective taskagnostic prompts for all three variants and demonstrate how to effectively use the proposed pipeline for explaining any black-box text classifier.\n3. Through experiments on three benchmark datasets and tasks we investigate the effectiveness of the proposed pipeline and discuss implications for future work in this direction.\nThe rest of the paper is organized as follows: Section 2 introduces some preliminary concepts, Section 3 takes a deep dive into our proposed framework. We describe our methodology and experimental settings in Section 4 and elaborate our results in Section 5. We discuss some related research directions in Section 6 and finally conclude with a discussion of future works in Section 7."
        },
        {
            "heading": "2 Background & Preliminaries",
            "text": "In this section, we go over some relevant background and preliminary concepts."
        },
        {
            "heading": "2.1 Black-box Text Classifiers",
            "text": "In this work, we look at text classification tasks1. In general, the form of such a task is that you input some text sequences(s) into a black-box classifier, and it outputs a label out of a pre-defined finite set of labels. Formally, we can define this black-box classifier as a function f : X 7\u2192 Y where X is the set of input sequences in a test set, X = {xi}Ni=1 where N is the number of data samples in the test set. Y is the set of finite labels Y = {yj}kj=1 where k is the number of labels for the classification problem. Each data instance xi \u2208 X is a sequence of tokens (t1, t2, ..., tl) where l is the length of the sequence. Typically during evaluation of such black-box classifiers, ground truth labels from the test set are compared with the labels predicted by the classifier. For the rest of the paper we use y to refer to predicted label and y\u0302 to denote the ground truth label. In practice, many state-ofthe-art models for standard benchmark datasets in NLP use transformer-based architectures, thus making these models essentially black-box by design. These transformer-based architectures include pretrained language models such as BERT (Sun et al., 2019), RoBERTa (Liu et al., 2019), XLNet (Yang et al., 2019), etc. Although such transformer-based models have achieved very good performance on these tasks, explainability of the classifiers remain an issue due to the inherent black-box nature of these models.\n1For purposes of this paper, we consider NLI as a text classification task, since it involves classifying the hypothesis into one of three label classes."
        },
        {
            "heading": "2.2 Large Language Models",
            "text": "Large Language Models (LLMs) are usually transformer-based models that are capable of generating human-like text. Recent examples of large language models include the GPT family of models (Radford et al., 2019; Brown et al., 2020; OpenAI, 2023), LlaMA (Touvron et al., 2023a), LlaMA-2 (Touvron et al., 2023b), Falcon2, etc. A general training recipe for training LLMs include an unsupervised pre-training step, where the model is trained on huge corpora of text, mostly comprising of text from the internet (Penedo et al., 2023; Gao et al., 2020), followed by one or more supervised fine-tuning steps (often referred to as instruction tuning (Ouyang et al., 2022)). More recent state-of-the-art LLMs such as ChatGPT 3.5 or GPT4 are further instruction tuned via reinforcement learning with human feedback (RLHF) (Christiano et al., 2017), in order to \u2018align\u2019 such models more with human preferences and values. During the pre-training stage for autoregressive language models (such as ChatGPT 3.5 and GPT-4), the model learns to perform next-token prediction, whereby it learns the distribution of next tokens, given the previous tokens, i.e., the context: xt \u223c P (\u00b7|x<t). In the subsequent fine-tuning and RLHF stages, these LLMs learn to follow instructions for specific tasks and respond in a helpful manner. The vast amount of training, both via the pre-training and the instruction-tuning stages, enables the LLMs to perform complex tasks (Bubeck et al., 2023), perform in-context learning (Dong et al., 2022), etc. Modern LLMs are highly expressive natural language generators, and the instruction-tuned versions of such LLMs are often used as helpful assistants to perform several tasks simply by effective prompting and providing few-shot examples."
        },
        {
            "heading": "2.3 Counterfactual Explanations",
            "text": "Counterfactual explanations are a category of human-understandable explanations that provide information to the user of a machine learning model regarding what could have changed the decision of the model: such as \u2018if event A1 did not occur, then the label would have changed from y1 to y2\u2019 (Molnar, 2020). Counterfactuals are typically similar to the input instance, and vary from it in a small number of features. Due to its simplicity, this type of explanation is often preferred over attempts to explain the inner workings of complex black-box\n2https://huggingface.co/tiiuae/falcon-40b\ndeep neural network models with hundred of thousands or millions of parameters (Wachter et al., 2017). From a causal perspective, counterfactuals may provide insights on what the causal features of the learned model are and distinguish them from the spurious ones (H\u00f6fler, 2005). In the context of text classification, which is the scope of this paper, counterfactual examples can be generated via token-based substitution methods, maskedlanguage modeling, controlled text generation via control codes (Madaan et al., 2022; Wu et al., 2021; Madaan et al., 2021) etc.\n3 FLARE: Our Counterfactual Explanation Generation Framework\nIn this section, we describe our Framework for Language Models in Automated CounteRfactual Explanation Generation, which we call FLARE throughout the paper. We develop and experiment with three variants of FLARE: (1) FLARE: the full framework involving extraction of latent features and then words in the input text, followed by the counterfactual explanation generation. (2) FLAREwords: that identifies the words in the input text directly, without identifying the latent features first, and then generates the counterfactual explanation. Finally we have, (3) FLAREvanilla: which directly generates the counterfactual explanation without performing a prior feature extraction step. We show the full framework in Figure 1. In the following part of this section, we go over the components in the full framework, additionally going over the differences across these three variants wherever applicable.\nBlack-box Classifier For our purposes, we assume black-box access to a text classifier that has already been trained for the specific task/dataset we are using. For uniformity across all our experiments, we use a DistilBERT (Sanh et al., 2019) model as the black-box classifier, which is fine-tuned on the task dataset. Note that this can be replaced by any similar blackbox model, including other transformer-based language models such as BERT (Sun et al., 2019), RoBERTa (Liu et al., 2019), etc. As explained in Section 2.1, we denote this black-box classifier as a function f : X 7\u2192 Y, where X is the set of input texts, and Y is a finite set of labels for the specific task. Note that this module could be replaced by any black-box classifier. During inference, we query this classifier with an input text from the test\nset of the task dataset: f(xi) = yi,\u2200xi \u2208 Xtest. We store these predictions as triples of the form (xi, yi, y\u0302), where y\u0302 is the ground truth, for use in the next step.\nLLM-Guided Feature Extractor\nIn this step, we aim to identify the features in xi that resulted in the predicted label yi. For the correctly classified test samples, we design prompts to extract features from the input text xi in two phases: first by extracting the latent features, i.e., higher level features such as \u2018tone\u2019, \u2018ambiguity\u2019, etc. Then, in the second stage, we prompt the LLM to extract the input features i.e., words in the input that are related to that particular high level latent feature. In the FLAREwords variant of our model, we skip the first step of finding the latent features, and simply prompt the LLM to identify the words in the text that led to the predicted label. In the FLAREvanilla variant, we completely skip this feature extraction step, and immediately proceed to the counterfactual explanation generation step, described in the next subsection.\nCounterfactual Explanation Generation\nFinally, we use the LLM to change a minimal set of words in the input to produce a counterfactual explanation, i.e., an updated version of the input that would change the label when fed into the same black-box classifier. In the FLAREvanilla variant of our model, this is the only step that is executed, and we prompt the LLM to generate the explanation without hinting at how to generate it.\nDetailed prompts enabling all these steps are provided in Table 1 along with an accompanying discussion in Section 4.3. This is a novel style of generating counterfactual explanations for blackbox text classifiers. Compared to other counterfactual generation methods that often use random perturbations, slot-filling with masked language models, or control-code based generation methods (Madaan et al., 2021; Wu et al., 2021), our framework FLARE selects important input features (i.e., tokens) in a two-step manner by leveraging the understanding capabilities of large language models, and then using the same LLM to generate the counterfactual. This enables an informed and well-guided counterfactual generation strategy, that is appropriate for the text classification task at\nhand."
        },
        {
            "heading": "4 Methodology & Experimental Settings",
            "text": "In this section, we go over the datasets and the LLMs we use, along with the evaluation metrics and a discussion on the nuances of how we performed the prompt designing."
        },
        {
            "heading": "4.1 Datasets Used",
            "text": "In this work, we focus on two broad categories of language tasks: text classification and reasoning (in the context of natural language inference). For text classification we use two different datasets: IMDB (Maas et al., 2011) for sentiment classification and AG News3 for news topic classification. For reasoning we use the SNLI dataset (MacCartney and Manning, 2008; Bowman et al., 2015). This variety of datasets allows us to evaluate the LLMs and our Counterfactual Explanation Generation framework over a variety of label situations from binary to multi-class.\nThe IMDB dataset4 consists of a total of 50k highly polar movie reviews from IMDB (Internet Movie Database). Each data instance consists of a text string comprising the review text, and a label, either \u2018negative\u2019 or \u2018positive\u2019. The AG News dataset consists of over 120k news articles, belonging to one of four news topics: \u2018world\u2019, \u2018sports\u2019, \u2018business\u2019 and \u2018science/technology\u2019. The Stanford Natural Language Inference (SNLI) dataset5 consists of 570k sentence pairs consisting of a premise and a hypothesis. Each premise-hypothesis pair is labeled with one of \u2018entailment\u2019, \u2018contradiction\u2019 or \u2018neutral\u2019 labels. For all of these datasets, due to resource constraints, we use 500 samples from the test set to evaluate our proposed pipeline. For GPT-4 we use 250 samples."
        },
        {
            "heading": "4.2 LLMs Used",
            "text": "For our experiments, we use two categories of large language models: (1) proprietary LLMs that are accessed via an API, and (2) opensource LLMs. Under the first category, we use text-davinci-003, ChatGPT 3.56 and GPT4 (OpenAI, 2023) (version as of August 2023) via the OpenAI API. Under the second category, we use the state-of-the-art open-source model Llama 2 (Touvron et al., 2023b), which is a large language\n3https://huggingface.co/datasets/ag_news 4https://huggingface.co/datasets/imdb 5https://huggingface.co/datasets/snli 6https://platform.openai.com/docs/models/gpt-3-5\nmodel developed by Meta AI. Due to resource constraints we use the 7B and 13B versions of Llama 2. For both sizes, we use the chat versions of the model, which are variants that have been fine-tuned for chat-style use cases. This is especially important for our application, since we use instructionstyle prompts to prompt the LLM to perform tasks."
        },
        {
            "heading": "4.3 Prompt Design Considerations",
            "text": "Since we use the LLMs off-the-shelf, without any additional fine-tuning for the explanation generation task, the choice of prompt is of utmost importance. Upon careful initial experimentation, we aim to build prompts that are clear without any ambiguity, sufficiently descriptive, and as short as possible. In order to prime the LLM for better feature extraction, for all three variants of the framework, we use the following system prompt:\n\u2018You are an oracle explanation module in a machine learning pipeline.\u2019\nThe rest of the prompts follow the structure as shown in Table 1. Interestingly, in our initial experiments, we notice that most LLMs (except GPT-4) struggle to identify \u2018latent features\u2019. This could be because the concept of latent features is a machine learning concept, and perhaps the LLMs have not encountered too many occurrences of the term \u2018latent features\u2019 to learn what it means in this context. However, we see that providing just two examples of latent features (i.e., \u2018tone\u2019 and \u2018ambiguity\u2019) in the prompt worked well, and improved the quality of the features extracted. Hence, for uniformity, we add the latent feature examples to prompts for all the LLMs. Furthermore, from our initial experiments, we identify that most LLMs do not perform well when asked to generate a \u2018counterfactual explanation\u2019, possibly because of similar reasons as above. In order to guide the LLM into generating actual counterfactual explanations, we provide a definition of \u2018counterfactual explanation\u2019 in the generation prompt for all LLMs. We see that this significantly improved the generation quality and the LLMs (albeit the larger models) can effectively generate counterfactual explanations. Finally, we also instructed the LLMs to generate the counterfactual explanation in a specified output format, to make it easier to parse out the counterfactual explanations once generated. This is a crucial functionality in order to use the FLARE framework in a large-scale automated manner."
        },
        {
            "heading": "4.4 Evaluation Metrics",
            "text": "Following prior work in counterfactual generation (Madaan et al., 2021; Wu et al., 2021), we evaluate the generated counterfactual explanations on two criteria:\nLabel Flip Score: We use Label Flip Score to measure the effectiveness of the generated counterfactual explanations. For each input text xi in the test split of the dataset, with predicted label f(xi) = yk, we evaluate the corresponding LLM-generated counterfactual xcfi using the same black-box classifier f(\u00b7) and obtain a label for the counterfactual. For an effective counterfactual, the obtained label should be different from the original label yk. Then Label Flip Score (LFS) is computed as:\nLFS = 1\nn n\u2211 i=1 1[f(xi) \u0338= f(xcfi )]\u00d7 100 (1)\nwhere n is the number of samples in the test set and 1 is the identity function.\nTextual Similarity: Counterfactual explanations generated by the LLMs should ideally be as \u2018similar\u2019 to the original input text as possible. To evaluate this similarity, we use two metrics: similarity\nof the text embeddings using the Universal Sentence Encoder (USE) (Cer et al., 2018) in the latent space, and a normalized Levenshtein distance (Levenshtein et al., 1966) to measure word edits in the token space. The semantic similarity using the embeddings of the original input and the generated counterfactual is computed as the inner product of the original and the counterfactual embeddings, averaged over the test dataset:\nsimsemantic = 1\nn n\u2211 i=1 Enc(xi) \u00b7 Enc(xcfi ) (2)\nwhere Enc(\u00b7) refers to the Universal Sentence Encoder, n is the number of samples in the test set.\nLevenshtein distance between two strings is defined as the minimum number of single character edits that are required to convert one string to another. To measure the distance between the original input text and the generated counterfactual in the token space we use a normalized Levenshtein distance, further averaged over the test dataset. This is computed as:\nedit_dist = 1\nn n\u2211 i=1 lev(xi, x cf i ) max(|xi|, |xcfi |) (3)\nwhere |xi| and |xcfi | refer to the length of xi and xcfi respectively, lev(\u00b7, \u00b7) refers to the Levenshtein distance, and n is the number of samples in the test set."
        },
        {
            "heading": "4.5 Experimental Setup",
            "text": "All experiments on open-source models were performed on two A100 GPUs with a total of 80G memory. For 13B Llama 2 models, we use 4-bit quantization using the optimal \u2018nf4\u2019 datatype (Dettmers et al., 2023). For all LLMs, we use top_p sampling with p = 1, temperature t = 0.4 and a repetition penalty of 1.1."
        },
        {
            "heading": "5 Results",
            "text": "In this section, we go over the quantitative and qualitative results of using FLARE and its variants to generate counterfactual explanations for the three chosen tasks."
        },
        {
            "heading": "5.1 Effectiveness of Generated Counterfactual Explanations",
            "text": "We report the main quantitative results of our framework in Table 2. For each LLM, we evaluate all three variants of our framework: the full FLARE involving the two-step feature extraction, the wordsonly FLAREwords and the naive FLAREvanilla that does not perform any feature extraction. Overall, we see varied performance of the LLMs and the three variants across the different tasks. Similar to other counterfactual generation works (Madaan et al., 2021), we see an obvious tradeoff between the Label Flip Score and the semantic similarity. This is intuitive since the more the generated counterfactual deviates from the original input text, higher the chances are for it to be a successful counterfactual for the original input (i.e, it would flip the label of the classifier). However, ideally we want counterfactuals that are semantically as close to the input as possible, that is, higher values of semantic similarity are desirable.\nFor the IMDB dataset, we see that counterfactual explanations generated by text-davinci-003 are very effective, with the FLAREwords variant being marginally better than the other two. However, the best performance is achieved by GPT-4 in the FLAREvanilla variant. In fact, the FLAREvanilla variant of GPT-4 has better performance than the other two variants in both the IMDB and SNLI tasks. This might be due to the fact that GPT-4 is extremely good at understanding complex tasks\nand instructions, and the two-step feature extraction step in the full FLARE variant does not provide additional useful guidance. We see a similar trend with ChatGPT 3.5 where the FLAREvanilla variant outperforms the other two for the AG News and SNLI tasks, while retaining high semantic similarity values.\nFor the open-source models Llama 2 7B and 13B, we see an interesting trend where the best performing variants are FLARE for the IMDB dataset, FLAREwords for AG News and FLAREvanilla for SNLI datasets. However, the Llama 2 models struggle to keep the generated counterfactuals semantically similar to the original input, implying they either make too many edits to the input text, or output some unrelated, low-quality text that does not conform to the instructions provided in the prompt. We show a few examples of this in Table 4 along with an accompanying discussion in Section 5.2.\nNot only should generated counterfactuals be semantically similar to the original input, but also be similar in the token space. In other words, the edit distance between the original and the generated counterfactual text should be minimal. We measure this using the normalized Levenshtein distance and report these results in Table 3. Interestingly, we see that the GPT-family of models, i.e., text-davinci003, ChatGPT 3.5 and GPT-4 have the lowest edit distances. This could imply that these models are superior when it comes to instruction following abilities and the counterfactual explanations generated by these models are cleaner. Another interesting observation is that the full FLARE variant is superior than the other two and results in lower edit distance. This might be due to the LLM being guided to extract features and subsequently editing only those features in the input, instead of modifying the input text entirely to change the label (which is the case for the FLAREvanilla variant)."
        },
        {
            "heading": "5.2 Qualitative Analysis of Generated Explanations",
            "text": "We show some examples of counterfactual explanations generated by our framework for the SNLI task in Table 4 for a small qualitative analysis. We choose the SNLI dataset for this comparative analysis since the reasoning component of the task adds an extra layer of nuance, and is therefore a more challenging setting than the text classification tasks. For Llama 2 7B as the LLM, we see that the FLARE variant successfully generate a plausi-\nble counterfactual explanation, with few number of edits, while the FLAREvanilla completely fails. This variant generates some kind of nonsensical rationale that is not a counterfactual explanation. This implies that for the smaller, less-expressive LLMs, the two-step feature extraction method in FLARE is useful in generating good, effective and semantically similar counterfactuals. For the ChatGPT 3.5 one, we see the FLAREvanilla variant does generate a successful counterfactual explanation albeit by simply negating the hypothesis, while the FLARE variant generates one with more diversity and plausible edits. Similarly for text-davinci-003, the FLAREwords variant does generate a successful counterfactual but mostly by adding simple negations in the original text. Finally, for GPT-4, all three variants of our framework generate plausible, successful and good quality counterfactual explanations. The FLARE variant goes one step further and generates a hypothesis that has new and plausible information: \u2018the man is ordering a drink\u2019 which GPT-4 possibly inferred would be a probable activity if the man is at the \u2018bar\u2019. This, again, demonstrates the effectiveness of GPT-4 in following complex instructions and understanding complex tasks. Overall, we see that in most cases the full FLARE variant of our counterfactual explanation generation framework is capable of generating high quality and diverse counterfactual explanations."
        },
        {
            "heading": "6 Related Work",
            "text": "Free-text and Counterfactual Explanations: Recently there has been some effort in using large language models to generate explanations for different tasks (Rajani et al., 2019; Huang et al., 2022), most of which deal with free text explanations. (Zaninello and Magnini, 2023) used pre-trained language models BART, PEGASUS and T5 to generate explanations on the commonsense reasoning task on the e-SNLI dataset (Camburu et al., 2018). Human in the loop approaches for generating freetext explanations with LLMs have been explored as well (Wiegreffe et al., 2021). Free text explanations from LLMs have also been used to improve downstream task performance such as in commonsense reasoning (Li et al., 2022). While free-text explanations may be sufficient for end-user satisfaction in certain use-cases, counterfactual explanations are stronger and provide insights into what features might have actually caused the prediction. While some work exists on automatically generating counterfactual examples (Wu et al., 2021; Madaan et al., 2021; Robeer et al., 2021), much less work has been done in the space of counterfactual explanations. Among these (Ramon et al., 2020) investigates counterfactual explanations in the context of behavioral and textual data, and (Yang et al., 2020) for financial text data, where words are considered as features.\nLLMs as Pseudo-oracles: Several recent efforts have explored the potential of LLMs to act as data annotators or oracles (He et al., 2023). This is particularly useful in scenarios where labeling might be costly. Authors in (Bansal and Sharma, 2023) propose a method for using LLM for annotations and also a sampling strategy to select good candidates for labeling, as is done in standard active learning use cases. Several recent works (Xu et al., 2023; Wang et al., 2022) have also investigated how to create large-scale instruction data (such as ones used by InstructGPT) by using LLMs to then further be used for LLM training. The potential for using LLMs as knowledge bases or fact-checking agents has also been explored (Petroni et al., 2019; Roberts et al., 2020; Lee et al., 2020; Li et al., 2023). One piece of recent work (Hegselmann et al., 2023) tries to use LLMs to perform zero-shot and few-shot classification on tabular data. LLMs have also been used extensively in information extraction tasks (Wei et al., 2023; Singhal et al., 2022;\nLLM FrameworkVariant Input Text & Generated Counterfactual\nLlama 2 7B\nOriginal: \u2018Four adults eat while sitting on a tile floor. Two couples eat food on a kitchen floor.\u2019 Label: Neutral\nFLAREvanilla Counterfactual: \u2018If the four adults had eaten while standing on a carpeted floor instead of a tile one,\nthen they would have been more comfortable and less likely to get hungry again soon.\u2019 Label: Neutral\nFLARE Counterfactual: \u2018Four adults eat while standing on a carpeted floor. Two couples eat food on a dining table.\u2019\nLabel: Contradiction\nChatGPT 3.5\nOriginal: \u2018A goofy looking woman is singing on stage. A woman is singing.\u2019 Label: Entailment\nFLAREvanilla Counterfactual: \u2018A goofy looking woman is singing on stage. A woman is not singing.\u2019\nLabel: Contradiction\nFLARE Counterfactual: \u2018A serious looking man is dancing on stage. A woman is singing.\u2019\nLabel: Contradiction\ntext-davinci-003\nOriginal: \u2018A couple is eating outside at a table and he is pointing at something. People are eating at a table outside.\u2019\nLabel: Entailment\nDunn et al., 2022; Shi et al., 2023). Our work leverages the language understanding capabilities of LLMs, first to perform feature extraction and then generation guided by the extracted features."
        },
        {
            "heading": "7 Conclusion & Future Work",
            "text": "In this paper, we developed a pipeline to generate counterfactual explanations using large language models (LLMs) to explain the predictions of blackbox classifiers for text classification tasks. We experiment and evaluate three variants of the model: (i) FLARE: which first utilizes a two-step feature extraction step to guide the subsequent generation, (ii) FLAREwords: which involves a singlestep feature extraction step, wherein only the input words responsible for the prediction are identified and then used to guide the generation, and finally (iii) FLAREvanilla: which is a naive variant involving only the generation step. We design effective prompts alongside our pipeline to perform this task, and our experiments show varied performance across different tasks and framework variants. Overall, we see benefits to using the full FLARE framework in most cases, although the choice of the variant would likely depend on the dataset being used.\nFuture work may investigate modifications to this proposed framework and investigate with more fine-grained methods of prompting. Since one of the challenges in our method was to ensure that the generated text is actually a counterfactual, devising ways to ensure more conformity of the generated counterfactual explanations to the definition of \u2018counterfactual explanation\u2019 is also be an area that can be improved."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work is funded in part by DARPA SemaFor (HR001120C0123), Office of Naval Research (N00014-21-1-4002), Army Research Office (W911NF2110030) and Army Research Lab (W911NF2020124). The views, opinions and/or findings expressed are those of the authors and should not be interpreted as representing the official views or policies of the Department of Defense or the U.S. Government."
        }
    ],
    "title": "LLMs as Counterfactual Explanation Modules: Can ChatGPT Explain Black-box Text Classifiers?",
    "year": 2023
}