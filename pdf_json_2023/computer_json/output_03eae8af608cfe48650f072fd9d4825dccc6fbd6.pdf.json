{
    "abstractText": "Deep hashing methods utilize an end-to-end framework to mutually learn feature representations and hash codes, thereby achieving a better retrieval performance. Traditional supervised hashing methods adopt handcrafted features for hashing function learning and then generate hash codes through classification and quantization. The lack of adaptability and independence of the quantization procedure leads to low retrieval accuracy of supervised hashing methods with handcrafted features in image retrieval. In this study, a non-relaxation deep hashing method for fast image retrieval is proposed. In this method, a differentiable host thresholding function is used to encourage hash-like codes to approach -1 or 1 non-linearly at the output of the convolutional neural, instead of the symbol function for quantization used in the traditional method. The output of the host thresholding function is directly used to compute the network training error, and a loss function is elaborately designed with the norm to constrain each bit of the hash-like code to be as binary as possible. Finally, a symbol function is added outside the trained network model to generate binary hash codes for image storage and retrieval in a low-dimensional binary space. Extensive experiments on two large-scale public datasets show that our method can effectively learn image features, generate accurate binary hash codes, and outperform state-of-the-art methods in terms of the mean average precision. INDEX TERMS Image retrieval, deep hash, convolutional neural network",
    "authors": [
        {
            "affiliations": [],
            "name": "Xiaofei Li"
        }
    ],
    "id": "SP:40da0d0bdacab58c8b9f8546f685a5c5654597ef",
    "references": [
        {
            "authors": [
                "L. Zheng",
                "Y. Yang",
                "Q. Tian"
            ],
            "title": "SIFT meets CNN: A decade survey of instance retrieval,\u2019",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,",
            "year": 2018
        },
        {
            "authors": [
                "C. Ma",
                "I.W. Tsang",
                "F. Shen",
                "C. Liu"
            ],
            "title": "Error correcting input and output hashing",
            "venue": "IEEE Trans. Cybern., vol. 49, no. 3, pp. 781\u2013791, Mar. 2019.",
            "year": 2019
        },
        {
            "authors": [
                "T. Li",
                "Z.zhang",
                "L.Pei",
                "Y. Gan"
            ],
            "title": "HashFormer:Vision transformer based deep hashing for image retrieval",
            "venue": "IEEE signal processiong letters, vol. 29, 2022,pp. 827\u2013831.",
            "year": 2022
        },
        {
            "authors": [
                "C. Qin",
                "L.Wu",
                "X.Zhang",
                "G. Feng"
            ],
            "title": "Efficient non-targeted attack for deep hashing based image retrieval",
            "venue": "IEEE signal processiong letters, vol. 28, 2021,pp. 1893\u20131897.",
            "year": 2021
        },
        {
            "authors": [
                "Zhenyu Wang",
                "Yuesheng Zhu"
            ],
            "title": "Online Supervised Sketching Hashing for Large-Scale Image Retrieval",
            "venue": "IEEE Access,2019,4:88369-88379",
            "year": 2019
        },
        {
            "authors": [
                "M. Datar",
                "N. Immorlica",
                "P. Indyk",
                "V.S. Mirrokni"
            ],
            "title": "Localitysensitive hashing scheme based on P-stable distributions",
            "venue": "Proc. 20th Annu. Symp. Comput. Geometry (SCG), New York, NY, USA, 2004, pp. 253\u2013262.",
            "year": 2004
        },
        {
            "authors": [
                "B. Kulis",
                "K. Grauman"
            ],
            "title": "Kernelized locality-sensitive hashing for scalable image search",
            "venue": "Proc. IEEE 12th Int. Conf. Comput. Vis., Kyoto,Japan, Sep. 2009, pp. 2130\u20132137.",
            "year": 2009
        },
        {
            "authors": [
                "A. Andoni",
                "P. Indyk"
            ],
            "title": "Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions",
            "venue": "Proc. 47th Annu. IEEE Symp. Found. Comput. Sci. (FOCS), Berkeley, CA, USA, Oct. 2006,pp. 459\u2013468.",
            "year": 2006
        },
        {
            "authors": [
                "Y. Weiss",
                "A. Torralba",
                "R. Fergus"
            ],
            "title": "Spectral hashing",
            "venue": "Proc. Adv.Neural Inf. Process. Syst., 2008, pp. 1753\u20131760.",
            "year": 2008
        },
        {
            "authors": [
                "Y. Gong",
                "S. Lazebnik",
                "A. Gordo",
                "F. Perronnin"
            ],
            "title": "Iterative quantization: A procrustean approach to learning binary codes for largescale image retrieval",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 35, no. 12,pp. 2916\u20132929, Dec. 2013.",
            "year": 2013
        },
        {
            "authors": [
                "J. Heo",
                "Y. Lee",
                "J. He",
                "S. Chang",
                "S. Yoon"
            ],
            "title": "Spherical hashing",
            "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit., Providence, RI, USA, Jun. 2012, pp. 2957\u20132964.",
            "year": 2012
        },
        {
            "authors": [
                "W. Liu",
                "J. Wang",
                "R. Ji",
                "Y. Jiang",
                "S. Chang"
            ],
            "title": "Supervised hashing with kernels",
            "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit., Providence, RI, USA, Jun. 2012, pp. 2074\u20132081.",
            "year": 2012
        },
        {
            "authors": [
                "M. Norouzi",
                "D.M. Blei"
            ],
            "title": "Minimal loss hashing for compact binary codes",
            "venue": "Proc. Int. Conf. Mach. Learn., 2011, pp. 353\u2013360.",
            "year": 2011
        },
        {
            "authors": [
                "F. Shen",
                "C. Shen",
                "W. Liu",
                "H.T. Shen"
            ],
            "title": "Supervised discrete hashing,\u2019\u2019in",
            "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR),",
            "year": 2015
        },
        {
            "authors": [
                "Y. Yang",
                "L.Geng",
                "H. Lai",
                "Y. Pan",
                "J. Yin"
            ],
            "title": "Feature pyramid hashing",
            "venue": "Proc. ICMR, Jul. 2019, pp. 114\u2013122. This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3244813 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ VOLUME XX, 2017 9",
            "year": 2019
        },
        {
            "authors": [
                "Q. Li",
                "Z. Sun",
                "R. He",
                "T. Tan"
            ],
            "title": "Deep supervised discrete hashing",
            "venue": "Proc. NIPS, 2017, pp. 2482\u20132491.",
            "year": 2017
        },
        {
            "authors": [
                "Z. Cao",
                "M. Long",
                "J. Wang",
                "P.S. Yu"
            ],
            "title": "HashNet: Deep learning to hash by continuation",
            "venue": "Proc. IEEE Int. Conf. Comput. Vis. (ICCV), Venice,Italy, Oct. 2017, pp. 5609\u20135618.",
            "year": 2017
        },
        {
            "authors": [
                "H. Zhu",
                "M. Long",
                "J. Wang",
                "Y. Cao"
            ],
            "title": "Deep hashing network for efficient similarity retrieval",
            "venue": "Proc. AAAI, 2016, pp. 2415\u20132421.",
            "year": 2016
        },
        {
            "authors": [
                "Y. Cao",
                "M. Long",
                "B. Liu",
                "J. Wang"
            ],
            "title": "Deep cauchy hashing for Hamming space retrieval",
            "venue": "Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., Salt Lake City, UT, USA, Jun. 2018, pp. 1229\u20131237.",
            "year": 2018
        },
        {
            "authors": [
                "Y. Cao",
                "M. Long",
                "J. Wang",
                "H. Zhu",
                "Q. Wen"
            ],
            "title": "Deep quantization network for efficient image retrieval",
            "venue": "Proc. AAAI, 2016, pp. 3457\u20133463.",
            "year": 2016
        },
        {
            "authors": [
                "J. Donahue",
                "P. Krahenb \u0308 uhl",
                "T. Darrell"
            ],
            "title": "Adversarial feature learn- \u0308ing,",
            "year": 2017
        },
        {
            "authors": [
                "X. Zhou",
                "F. Shen",
                "W. Liu",
                "L. Nie",
                "Y. Yang"
            ],
            "title": "Graph convolutional network hashing",
            "venue": "IEEE Trans Cybern. 2018, pp. 1460\u20131472.",
            "year": 2018
        },
        {
            "authors": [
                "J. Li",
                "WWY. Ng",
                "X. Tian",
                "S. Kwong",
                "H. Wang"
            ],
            "title": "Weighted multi-deep ranking supervised hashing for efficient image retrieval",
            "venue": "Int J Mach Learn Cybern,",
            "year": 2020
        },
        {
            "authors": [
                "Han Sun",
                "Yejia Fan",
                "Jiaquan Shen"
            ],
            "title": "A Novel Semantics-Preserving Hashing for Fine-Grained Image Retrieval",
            "venue": "IEEE Access,2020,8:26199- 26209.",
            "year": 2020
        },
        {
            "authors": [
                "YANG LI",
                "YAPENG WANG",
                "ZHUANG MIAO",
                "JIABAO WANG",
                "RUI ZHANG"
            ],
            "title": "Contrastive Self-Supervised Hashing With Dual Pseudo Agreement",
            "venue": "IEEE Access,2020,8:165034-165043",
            "year": 2020
        },
        {
            "authors": [
                "L Jin",
                "XB Shu",
                "Li K",
                "Li ZC"
            ],
            "title": "Deep Ordinal Hashing With Spatial Attention",
            "venue": "IEEE Transactions on Image Processing, 2019,28(5):2173-2186.",
            "year": 2019
        },
        {
            "authors": [
                "X. Lu",
                "Y. Chen",
                "X. Li"
            ],
            "title": "Discrete Deep Hashing With Ranking Optimization for Image Retrieval.",
            "venue": "IEEE transactions on neural networks and learning systems",
            "year": 2019
        },
        {
            "authors": [
                "Lai HJ",
                "Yan P",
                "XB Shu",
                "YC Wei",
                "SC Yan"
            ],
            "title": "Instance-aware hashing for multi-label image retrieval",
            "venue": "IEEE Transactions on Image Processing, 2016,25(6):2469-2479.",
            "year": 2016
        },
        {
            "authors": [
                "W Li",
                "S Wang",
                "W. Kang"
            ],
            "title": "Feature learning based deep supervised hashing with pairwise labels",
            "venue": "Proceedings of the 25th International Joint Conference on Artificial Intelligence. New York, USA: AAAI Press,",
            "year": 2016
        }
    ],
    "sections": [
        {
            "text": "VOLUME XX, 2017 1\nINDEX TERMS Image retrieval, deep hash, convolutional neural network\nI. INTRODUCTION In a machine-learning algorithm, the hashing algorithm can map similar information from a high-dimensional space to a low-dimensional space with good similarity. The main idea of the hash algorithm is to map information of any dimension into a low-dimensional space with semantic similarity and fixed dimensions. Owing to its low computational cost and high storage efficiency, it is one of the most commonly used techniques for content-based image retrieval(CBIR)[1]. Considering the computational loss and storage capacity of the algorithm, a deep hash was used for large-scale information searches[2]-[6].\nIn the early days, LSH [7] mapped the original data to binary codes using a random hash function. Methods [8]\u2013 [10] such as LSH are data-independent hash methods that learn hash functions without training data. To generate a hash function related to training data, several data-dependent hash methods [11]\u2013[15] that achieve better retrieval performance have been proposed over the past decades. Furthermore, data-dependent hashing can be categorized into\nunsupervised [11]\u2013[12] and supervised [13]\u2013[15] methods, based on whether supervised information is used. Traditional data-dependent hashing methods comprise hand-crafted feature representations and hash coding.\nIn recent years, inspired by the advanced achievements of convolutional neural networks (CNN), deep hashing methods [16]\u2013[21] have attracted increasing interest. To make better use of more widely available unlabeled data, unsupervised deep hashing methods have also been proposed. However, natural images usually contain considerable variability in trivial factors such as location, color, and pose. Pixel-wise reconstruction may degrade the learned hash codes by focusing on these trivial variations. Other recent deep hashing methods learn hash codes by maximizing their representation capacity [22] or enforcing the similarity between rotated images and their corresponding original images [23]. HashNet[18] presented a novel method that solved the original non-smooth optimization problem by iteratively optimizing a similar smooth loss function. In addition, many graph-hashing\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nVOLUME XX, 2017 9\nmethods have recently been proposed[24]. Deep hashing methods exhibit promising performance in terms of image retrieval and classification with a binary representation of data [25]. NSPH[26] proposes a simple yet effective method for fine-grained image retrieval. The model adds quantization and bit-balance losses to the hash layer. Quantization loss reduces the error caused by binarizing realvalued feature representations to hash codes, and bit balance loss weakens hash code bias. CSH[27] proposed a simple but effective approach to self-supervised hash learning based on dual pseudo-agreement. By adding a consistency constraint, this method can prevent corrupted labels and encourage generalization for effective knowledge distillation. A novel deep ordinal hashing method(DOH)[28] learns ordinal representations to generate ranking-based hash codes by leveraging the ranking structure of the feature space from both local and global views. In the hashing framework, the local spatial and global semantic nature of the images are captured in an end-to-end ranking-to-hashing manner. RODH[29] directly generates discrete hash codes from raw images by balancing the effective category-level information of discretization and discrimination of ranking information. A deep architecture that learns instance-aware image representations for multi-label image data[30] was proposed. It is organized into multiple groups, with each group containing the features for one category. Li et al. proposed a feature learning based deep supervised hashing with pairwise labels (DPSH) algorithm based on tag pairs[31], which constructed an image tag pair matrix through image category labels, and then constructed a cross-entropy loss function based on the image tag pairs. The algorithm relaxed the constraint conditions, removed the constraint conditions of the symbolic function, and solved the discrete constraint problem using the relaxation optimization method based on the Lagrange multiplier method. However, some hash bits would be excessively relaxed, resulting in incomplete semantic information between similar point pairs because the algorithm uses Lagrange multipliers.\nTo avoid the influence of the relaxation solution on the accuracy of the model and the influence of the inaccuracy of the similarity matrix decomposition on the subsequent quantization process, this study proposes a non-relaxation deep hashing method for fast image retrieval(NRDH). Fig.1 provides an overview of the proposed method. The main contributions of this study are summarized as follows:\n(1)A deep hashing learning framework is proposed, which is an end-to-end structure. The framework generates discrete hash codes directly from raw images, and integrates image feature extraction and discrete hash learning modules into a unified framework. An improved network structure and suitable hash generation function are designed to solve the problem of non-derivable discrete space in deep hashing.\n(2)A loss function is elaborately designed with the norm to constrain each bit of the hash-like code to be as binary as possible.\n(3) To reduce the quantization error, a host threshold function is used at the network output to make the output quasi-hash code non-linearly close to -1 or 1. The symbolic function sgn() is used outside the model to quantize the quasi-hash code into binary value code."
        },
        {
            "heading": "II. PROPOSED METHOD",
            "text": ""
        },
        {
            "heading": "A. PROBLEM DESCRIPTION AND DEFINITION",
            "text": ""
        },
        {
            "heading": "1) PROBLEM DESCRIPTION",
            "text": "We define a dataset of images, where 1{ } n d n i iX x R    n\nis each input image, and is the size of the image. d ix R d The hash code corresponding to the output image is , where is the i-th column of output { 1,1}l nB   { 1,1}lib   data B, which represents the binary hash code of the i-th sample data , and the length is . The purpose of ix l perceptual hash learning is to obtain an automatically learned hash function, , by training from the training set data. ()H\nSuppose that an image is represented as using a hash function. In the\n1( ) [ ( ), ( )]i i i l ib H x h x h x  \u2026 \u2026 linear hash coding function, the hash function maps an ()ih image to a hash bit, and the hash functions map an image l to a string of -bit binary hash codes. In the supervised l hashing algorithm, each image sample data point has a label, and the label matrix is , where denotes 1{ } n c n i iY y R    c\nthe number of categories. The sample data and are ix jx related by similarity matrix . For any two data samples (\nijS\nand ), if and are similar, then , otherwise ix jx ix jx 1ijS  . 0ijS "
        },
        {
            "heading": "2) CROSS ENTROPY LOSS FUNCTION",
            "text": "For any two hash codes of equal length, and , of ib jb\nequal length, the similarity of the two hash codes is ij defined by their inner product:\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nVOLUME XX, 2017 9\n(1) 1( , ) 2 T ij i j i jb b b b  \nThe larger the inner product, the greater the similarity. The similarity is thresholded non-linearly using the sigmoid ij function, and its range is normalized to the interval (0,1). ij can be obtained:\n(2) 1( ) 1 ijij e \n    The cross-entropy loss function is used to maintain the similarity between image point pairs based on the similarity measure of hash codes. The likelihood between the ( | )ijQ s B hash code and similarity of the image point pairs is defined as\n(3) ( ), 1( | ) 1 ( ), 0 ij ij ij\nij ij\ns Q s B s        \nrepresents the similarity between sample pairs in ijs Eq.(3), where B represents the hash code corresponding to sample data. The likelihood function shows that when the hash codes and are more similar, that is, the larger theib jb\n, the larger the corresponding likelihood function ( )ij  . The negative logarithm of the likelihood is the ( | )ijQ s B\ncross-entropy loss function, which can be expressed as\n(4) 1( ) log ( | ) [ log(1 )]ij ij ij ij ij ij s s s s Loss B Q s B s e         \nThe maximum likelihood estimation was converted to minimize the cross-entropy loss function, and the constrained optimization problem was established as follows:\nmin [ log(1 )]ij ij ij ijB s s s e     (5) sgn[ ( ; ) ], 1, 2,Ti ib W x v i n     \nrepresents the neuron parameters of the fully connected W layer in Eq.(5), represents the offset; represents the v  parameter set of the network convolution layer; represents ib the binary hash code; and represents the image features ( )  extracted by the network. In ,each bit is quantized to a ib discrete value of -1 or 1."
        },
        {
            "heading": "B. NETWORK ARCHITECHTURE",
            "text": "In this study, adjustments are made based on the CNN-F network structure. The network structure is shown in Fig.2, and consists of five convolutional layers, three pooling layers, and four fully connected layers. In this network structure, a fully connected layer, FUL7, is added before the binary hash code is generated to learn the parameters before the saturated\nactivation function is input. The data are processed using local response normalization (LRN) in each convolutional layer, oversampled to automatically extract the feature representation of the image, and then the result is output through a fully connected layer.\nThe process of feature extraction and hash coding and quantization in this network is:\n(1) Through the two convolutional layers of Conv1 and Conv2, a series of underlying visual features, such as the color, brightness, layout, and texture of the image, are extracted.\n(2) The maximum pooling layer reduces the dimension of the feature value passed in from the previous layer to increase the rotational invariance of the feature and reduce the computational load of the network.\n(3) The extracted image features are combined through the last three layers of Conv to generate a higher-level feature semantic representation.\n(4) The high-level semantic features of the generated images are embedded in FUL6. FUL7 is the semantic layer that provides adjustable parameter learning generated by the perceptual hash function, and FUL8 is the output layer of the quasi-hash code."
        },
        {
            "heading": "C. LOSS FUNCTION",
            "text": "Cross-entropy was used to maintain the semantic similarity between sample pairs in the proposed method. To reduce the quantization error of the quasi-hash code output by the network, the norm is used to constrain the distribution of 1 the quasi-hash code output by the network as follows:\n(6) 1 1 2( ) | | 1\nn\ni i Loss B b   \nThe purpose of this regular term is to make each hash bit of the quasi-hash code approximately equal to two discrete ib values -1 or 1; that is, the closer the absolute value of each bit in is to 1, the smaller the loss. In the iterative process, ib the output of the network is directly used as quasi-hash code with a high probability of excessive deviation from -1 or 1, which increases the loss value. Although the sign L 2( )oss B function can quantify this well, it is not differentiable. Compared with the discrete encoding of the sign function, the hyperbolic tangent function can make each hash bit of the quasi-hash code non-linearly close to -1 or 1 and has the good property of continuous infinite derivation, and its form is\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nVOLUME XX, 2017 9\n(7) 2 2 1tanh( ) 1 x x ex e     \nBased on the hyperbolic tangent function, the parameter is added to control the slope of the threshold function, and \nthe optimized function is ( )host x\n(8) ) 1 1 ( x x ehost x e       \nUnlike the HashNet algorithm, the HashNet algorithm gradually increased the scale coefficient of the threshold function during the training process. With an increase in the number of iterations, the threshold function continues to approach and finally converges to the sign function. In the proposed algorithm, the new threshold coefficient is the  model parameter. After obtaining the optimal parameters through experiments, remains unchanged in each  iteration, which simplifies the model and causes it to converge quickly. The threshold function is used for ( )host x the network output in the proposed method. The discrete constraint problem is transformed into a problem of deriving the model parameters using a differentiable loss function. In the iterative process, the threshold function mapped ( )host x the output of the network smoothly and nonlinearly to (-1,1). Most hash bits are thresholded at approximately -1 and 1.\nBy combining the cross-entropy loss function in Eq.(4)and the regular term in Eq.(7), and using the threshold function at the output of the network structure, the objective ( )host x function is established as\n1 1\nmin [ log(1 )]\n| | 1\nij\nij\nij ijB s s\nn\ni i\nLS s e\nb\n\n\n\n\n   \n \n\n\n\uff089\uff09 [ ( ; ) ], 1, 2,\n1 , , 1, 2 2\nT i i\nT ij i j\nb host W x v i n\nb b i j n\n \n\n   \n  \n\n\nIn Eq.(9), represents the number of samples, n {0,1}ijS  represents whether the sample and sample are similar, i j\nrepresents the regular term coefficient,  ( )host x represents the threshold function, represents the control u parameter of the threshold function, and represents the ib quasi-hash code output by the forward network, ij represents the similarity between two hash codes."
        },
        {
            "heading": "D. PARAMETER LEARING",
            "text": "The variable parameters and are solved using back v W propagation(BP), and in each iteration, and are updated v W using stochastic gradient descent(SGD). During the training process, the two terms of the objective function are represented by and respectively: 1LS 2LS\n(10) 1\n2 1 1\n[ log(1 )]\n| | 1\nij\nij\nij ij s s\nn\ni i\nLS s e\nLS b\n\n\n\n\n   \n \n\n"
        },
        {
            "heading": "1) PARTIAL DERIVATIVES FOR QUASI HASH CODE",
            "text": "Find the partial derivatives of the first item with 1LS respect to ,get: ib\n(11) 1 : : 1 1( ) ( ) 2 2\nij ij\nij ij j ji ji j j s S j s Si LS m s b m s b b          \nthere, 1( ) 2 T ij i jm b b\nFind the partial derivatives of the second item with 2LS respect to ,get: ib\n(12) 2 ( )i i LS b b   \nthere, 1, -1 0 1\n( ) 1,\nx or x x\notherwise      \nCombining Eq.(11) and Eq.(12), the partial derivative of the loss function with respect to can be obtained as LS ib follows:\n(13) :\n:\n1 ( ) 2\n1 ( ) ( ) 2\nij\nij\nij ij j j s Si\nji ji j i j s S\nLS m s b b\nm s b b\n\n\n   \n\n \n\n"
        },
        {
            "heading": "2) PARTIAL DERIVATIVES FOR PARAMETER W",
            "text": "Find the partial derivatives of the loss function with LS\nrespect to ,get: W\n(14) T i\ni\nbLS LS W W b          \nThere, was got by ib W  \n' 2 2( ) (1 )\nux\nx ehost x e     \n(15) 2 2( ; ) (1 ) u i i u b uex W e          \n(16) ( ; )T iW x v   \nThe partial derivatives of the loss function with LS respect to can be obtained: W\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nVOLUME XX, 2017 9\n(17) 2 2( ; ) (1 )\nTu\ni u i\nLS ue LSx W e b\n               "
        },
        {
            "heading": "3) PARTIAL DERIVATIVES FOR PARAMETER v",
            "text": "Similar to 2), the partial derivatives of the loss function with respect to are obtained as follows: LS v\n(18) 2 2\n(1 )\nu\nu i\nLS LS ue v b e\n\n\n\n\n   \n  \nIt can be seen from the above solution process that the loss function is derivable, that is, the partial derivative of the backpropagation process exists, and the network model can converge after a certain iteration."
        },
        {
            "heading": "E. ALGORITHM DESCRIPTION",
            "text": "The pseudocode for the algorithm used in this study is shown in Algorithm 1. The input data are an image in a certain format, and the feature representation of the image is extracted through convolutional and pooling layers. According to the objective function, the parameters and v are updated using the back-propagation algorithm, and the W model is completed using a specified number of iterations. Finally, a symbolic function is used outside the trained model to quantize the quasi-hash code of the image and output a binary hash code.\nAlgorithm 1: NRDH Input: A dataset of images X n Output: Hash code corresponding to images, network B n parameter. Initialization: Gaussian distribution to initialize weights W and offsets . v Iteration process:\n1)Read the data from the training set and preprocess it, and input the samples into the network; 2)Calculate the corresponding through the BP B algorithm; 3) Calculate the loss by using the loss function; 4)Calculate the gradient layer by layer by using the SGD algorithm, adjust the network parameters, and update the parameters and ; W v\nStop: The maximum number of iterations is reached. Return: network parameters and , sgn( ) , and the hash W v code by forward propagation. B"
        },
        {
            "heading": "III. EXPERIMENTS AND ANALYSIS",
            "text": ""
        },
        {
            "heading": "A. DATASETS",
            "text": "We compare our proposed method with other state-of-theart methods by using two widely used benchmark datasets.\n1) CIFAR-10\uff1aThis dataset consists of 60,000 32 \u00d7 32 color images divided into 10 classes (6000 images per class). Among these, 50,000 are the training set and 10,000 are the test set. It is a single-label dataset in which each image\nbelongs to one of ten classes. The images are resized to 224 \u00d7 224 pixels before being inputted into the CNN-based models.\n2) NUS-WIDE: This dataset contain 269,648 images gathered from Flickr. It is a multi-label dataset, where each image belongs to one or multiple class labels from 81 classes. In this experiment, 21 commonly used categories are selected, with each category containing at least 5000 images.\nIn the experimental stage, in the CIFAR-10 data-set, 500 images are randomly selected from each category as the training data and 100 images as the test data. A total of 5000 images are obtained from the training set and 1000 images are obtained from the test set. In the NUS-WIDE data-set, 500 images are randomly selected from each category as training data and 100 images as test data. A total of 10500 images from the training set and 2100 images from the test set are used, and each image is adjusted to 224 \u00d7224. The size of 224 is suitable for the input of the network model. To verify the robustness of the algorithm, the norm regular term coefficients are both set to 0.05 on the CIFAR-10 and NUSWIDE datasets, and the threshold function control parameters are all set to 24. "
        },
        {
            "heading": "B. EXPERIMENTAL RESULTS",
            "text": "Because a fixed size 224\u00d7224 image is used as input in the network, the images in CIFAR-10 and NUS-WIDE are scaled to 224\u00d7224 before training. To eliminate the commonality of the images and facilitate computer understanding, the mean value of the entire image dataset is subtracted from each image in the experiment, which can also construct the central data distribution so that the gradient descent algorithm can operate quickly and efficiently.\nAfter selecting a certain length of hash code during the experiment, in the test set, we select a part of the images as the samples to be retrieved, calculate the Hamming distance between the images to be retrieved and other images in the data-set, and sort and calculate the images according to the Hamming distance. The ratio of the number of images in the same category as the images to be retrieved in the sorted list to the total number of retrieved images is used as the accuracy rate.\nThe NRDH method is compared with several popular hash learning algorithms: FPH[16], HashNet[18], NSPH[26], CSH[27], DOH[28], and RODH[29]. Table 1 compares the MAP of our DNRH algorithm and existing hash learning algorithms on the CIFAR-10 dataset. As shown in Table 1,the average accuracy of our DHFR algorithm for the four hash code lengths is significantly higher than that of all other algorithms. By comparing the MAPs of the six deep hashing algorithms and other 6 non-deep hashing algorithms, it can be seen that the deep hashing algorithm has a higher average accuracy than the non-deep hashing algorithms, which shows that the deep hashing learning algorithm using the CNN model automatically extracts image feature representations with better performance than the traditional manual extraction of image feature representations. For a hash code of 16 bits, the retrieval accuracy of all algorithms\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nVOLUME XX, 2017 9\nis relatively low. As the length of the hash code increases, the retrieval accuracy of all algorithms gradually increases. When the hash code length is 64, the retrieval accuracy of all algorithms reaches compared with the hash code below 64bits, using the hash code of 48bits can store more image features, and can use more image features during retrieval to achieve higher accuracy.\nCompared with the images of the CIFAR-10 dataset, the images of the NUS-WIDE dataset have higher pixels, more complete image details, and are closer to the images in practical applications. In the NUS-WIDE dataset, an image may contain multiple images during the retrieval process; as long as the retrieved image and the image to be retrieved contain the same label, it is judged as correct retrieval.\nTable 2. compares the average accuracy of our NRDH algorithm with existing hash learning algorithms for hash codes of different lengths on the NUSWIDE dataset. Due to the large number of images in the NUS-WIDE dataset, on this dataset, this paper uses the first 5000 samples retrieved from each test sample to calculate the MAP. For hash codes of the same length, the average accuracy rates of the DHFR algorithm in this paper for 16bits, 32bits, 64bits, and 128bits are 0.801, 0.818, 0.829, and 0.833,respectively, which are higher than those of other hash learning algorithms. This proves the universality of the algorithm used in this study. Among them, for the RODH algorithm, we still used the Lagrange multiplier experiment on the CIFAR-10 10u  dataset and re-ran the NUS-WIDE dataset with various algorithms using the same training set and test set. We ran the code provided by the authors and calculated their average accuracy. As the length of the hash code increases, the average retrieval accuracy of almost all algorithms increases to a certain extent, particularly for the FPH algorithm, and the average accuracy of the 48-bit hash code is higher than that of the 12-bit hash code. This is nearly 7%, indicating that\nmore hash bits can represent more image features and improve retrieval accuracy.\nOwing to the large number of images in the NUS-WIDE dataset, the MAP is obtained using the first 5000 samples returned. For hash codes of the same length, the average accuracy rates of the NRDH algorithm in this study are higher than those of the other hash learning algorithms, which proves the universality of the algorithm in this study.\nIn addition to mAP, we evaluate our method using precision curves of 64-bit hash codes with different recall rates, as shown in Fig. 3,which not only reflects the precision of the search results but also reflects the recall rate. The larger the area enclosed by the PR curve and axes, the better the retrieval performance. As shown in Fig.3(a), our work is outstanding among all methods. Fig.3(b) shows the precision rates of 64-bit hash codes for different numbers of topreturned images. According to the precision curves, as the number of returned samples increased, the precision rate of the deep hashing method decreased slightly. By contrast, our approach always returns positive samples with satisfaction."
        },
        {
            "heading": "C. ABLATION STUDY ON LOSS FUNCTION",
            "text": "In the NRDH algorithm proposed in this study, the role of the host threshold function is to directly threshold the results of the network output in the forward calculation of the model, and the \u21131 norm is used as the regular term of the objective function to constrain the quasi-hash in the backpropagation of the model. The function of these two modules is to constrain quasi-hash code. In order to verify the constraint performance of the combined use of the \u21131 norm and the host threshold function, this paper uses the CIFAR10 data On the set, experiments are carried out on the \u21131 norm regular term independent constraint, the host threshold\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nVOLUME XX, 2017 9\nfunction independent constraint, and the \u21131 norm and host threshold function joint constraints.\nTable 3 lists the average accuracy rates corresponding to different models on hash codes of four lengths, where \"cross entropy + host threshold\" means using the loss function of Eq.(4), and using host at the output of the network, \"cross entropy + \u21131 norm\" means using the loss function of formula (9), and omitting the constraints, that is, the model that does not use the host threshold function at the output of the network, \"cross entropy + \u21131 norm\" + host threshold\" represents the NRDH algorithm model in this paper, that is, using the \u21131 norm and the host threshold function jointly. Observing Table 3, it can be seen that the average accuracy of the two models \"cross entropy + \u21131 norm\" and \"cross entropy + host threshold\" is lower, indicating that the effect of using the \u21131 norm and the host threshold function alone is not as good as the algorithm for Lagrange multiplier relaxation solution. The combined use of the \u21131 norm and the host threshold function (cross entropy + \u21131 norm + host threshold) in the length of the 4-length hash code, its MAP is compared to using one of the modules alone. Both improved by nearly 10% and are higher. Therefore, it can be concluded that the combined use of the \u21131 norm and the host threshold function can better constrain the hash code and improve the performance of the algorithm."
        },
        {
            "heading": "D. PARAMETER IMPACT ANALYSIS",
            "text": ""
        },
        {
            "heading": "1) INFLUENCE OF REGULAR TERM COEFFICIENT ON QUASI-HASH CODE DISTRIBUTION",
            "text": "To test the constraining ability of the \u21131 norm regular term on the quasi-hash code output by the fully connected layer, this study presents statistics on the distribution of the output quasi-hash code in the CIFAR-10 dataset. The distance of the absolute value of one bit relative to one is distributed in the\nintervals [0, 0.1), [0.1, 0.2), [0.2, 0.3), and [0.3, 0.4). Fig. 4 shows the distribution of the quasi-hash code. In different cases, different colors represent different distribution intervals; the horizontal axis represents the regular term coefficient and the vertical axis represents the percentage of hash bits that fall in different intervals.\nIt can be seen from the distribution of each hash bit of the quasi-hash code in Fig.4, that with the increase of , the  absolute value of each hash bit of the quasi-hash code is closer to 1, especially when the \u21131 norm is not used ( In the case of =0) constraint, the hash bits of the quasi-hash code  are relatively evenly distributed between 0 and 0.4, so that the loss will increase in the final quantization process, resulting in inaccurate results. In the objective function, the true term is used to maintain the similarity between point pairs, and the \u21131 norm regular term is used to constrain the quasi-hash code distribution. If the regular-term coefficient is too large, the proportion of the \u21131 norm regular term  increases excessively, thereby reducing semantic preservation. The function of the true term affects classification. It can be seen that the appropriate \u2113 1 norm regular term has a good constraining effect on the distribution of the hash code."
        },
        {
            "heading": "2) INFLUENCE OF REGULAR TERM COEFFICIENT ",
            "text": ""
        },
        {
            "heading": "ON EXPERIMENTAL ACCURACY",
            "text": "The value of the regular-term coefficient not only  affects the distribution of each hash bit of the quasi-hash code output by the model but also affects the accuracy of the model trained by the NRDH algorithm in this study. Table 4 shows that when the length of the hash code is 64, the average accuracy of different values of on the CIFAR-10  and NUS-WIDE datasets.\nIt can be observed from Table 4 that the value of has  the same distribution of influence on MAP in the two datasets. When =0.05, the retrieval effect on the test set is  the best, and if the value of is too small or too large, the  retrieval will be affected. This is because the value is too  small, the constraint of the objective function aligned with the hash code distribution becomes weaker, and some of the hash bits of the quasi-hash code output by the model deviate significantly from -1 or 1, resulting in the final result. The loss increases when it is quantized into hash code. If the value of is too large, the proportion of semantic fidelity  items in the objective function will decrease. This proportion causes the distance between the same categories to increase or the distance between different categories to decrease, that is, the similarity constraints between images become weaker, which worsens the retrieval effect.\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nVOLUME XX, 2017 9"
        },
        {
            "heading": "E. RESULTS VISUALIZATION",
            "text": "A visualization of the results is shown in Fig.5. When the number of coding bits is 64, all methods in the experiment used the Hamming distance directly for the retrieval instances of trucks in the CIFAR-10 image database. In the experiment, the first 36 images with the smallest Hamming distance from the query image are used as returned results. The images marked in the red box are not related to the query image. It can be observed that the method in this study achieved better results than the other image-hashing algorithms.\nIn addition, we visualize the T-SNE of hash codes generated by NSPH, HashNet, and our method on the CIFAR-10 image database in Fig.6. For simplicity, we sample 10 categories. We observe that the hash codes generated by our method in different classes are well separated, and those in the same class are more compact. This suggests that the hash codes generated by the proposed\nmethod is more discriminative than those generated by the other two methods."
        },
        {
            "heading": "IV. CONCLUSION",
            "text": "To avoid the influence of the relaxation solution on the accuracy of the model and the influence of the inaccuracy of the similarity matrix decomposition on the subsequent quantization process, a non-relaxation deep hashing method was proposed to achieve effective and efficient large-scale image retrieval. To demonstrate the advantages of the proposed method, extensive experimental studies are conducted, and the results show that the proposed method significantly outperforms other hashing methods on benchmark datasets. In future work, it will be interesting and promising to develop a theoretical framework to further optimize the performance and apply the framework to other types of data (e.g., audio, video, and text)."
        }
    ],
    "title": "Non-relaxation Deep Hashing Method for Fast Image Retrieval",
    "year": 2023
}