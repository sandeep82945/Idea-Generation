{
    "abstractText": "In this paper, we devise a mechanism for the addition of multimodal information with an existing pipeline for continuous sign language recognition and translation. In our procedure, we have incorporated optical flow information with RGB images to enrich the features with movement-related information. This work studies the feasibility of such modality inclusion using a cross-modal encoder. The plugin we have used is very lightweight and doesn\u2019t need to include a separate feature extractor for the new modality in an end-to-end manner. We have applied the changes in both sign language recognition and translation, improving the result in each case. We have evaluated the performance on the RWTH-PHOENIX2014 dataset for sign language recognition and the RWTHPHOENIX-2014T dataset for translation. On the recognition task, our approach reduced the WER by 0.9, and on the translation task, our approach increased most of the BLEU scores by \u223c0.6 on the test set.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zaber Ibn Abdul Hakim"
        },
        {
            "affiliations": [],
            "name": "Rasman Mubtasim Swargo"
        },
        {
            "affiliations": [],
            "name": "Muhammad Abdullah Adnan"
        }
    ],
    "id": "SP:16e7e85e5e61a04d9ffe59ecd5bc46985ee93e23",
    "references": [
        {
            "authors": [
                "Necati Cihan Camg\u00f6z",
                "Oscar Koller",
                "Simon Hadfield",
                "R. Bowden"
            ],
            "title": "Sign language transformers: Joint end-to-end sign language recognition and translation",
            "venue": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 10020\u201310030, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Zhe Niu",
                "Brian Kan-Wing Mak"
            ],
            "title": "Stochastic finegrained labeling of multi-state sign glosses for continuous sign language recognition",
            "venue": "European Conference on Computer Vision, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Aiming Hao",
                "Yuecong Min",
                "Xilin Chen"
            ],
            "title": "Selfmutual distillation learning for continuous sign language recognition",
            "venue": "2021 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 11283\u201311292, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Junfu Pu",
                "Wengang Zhou",
                "Houqiang Li"
            ],
            "title": "Iterative alignment network for continuous sign language recognition",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp. 4165\u20134174.",
            "year": 2019
        },
        {
            "authors": [
                "Runpeng Cui",
                "Hu Liu",
                "Changshui Zhang"
            ],
            "title": "A deep neural framework for continuous sign language recognition by iterative training",
            "venue": "IEEE Transactions on Multimedia, vol. 21, pp. 1880\u20131891, 2019.",
            "year": 1880
        },
        {
            "authors": [
                "Necati Cihan Camg\u00f6z",
                "Simon Hadfield",
                "Oscar Koller",
                "Hermann Ney",
                "R. Bowden"
            ],
            "title": "Neural sign language translation",
            "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7784\u20137793, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "Andreas Voskou",
                "Konstantinos P Panousis",
                "Dimitrios Kosmopoulos",
                "Dimitris N Metaxas",
                "Sotirios Chatzis"
            ],
            "title": "Stochastic transformer networks with linear competing units: Application to end-to-end sl translation",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 11946\u2013 11955.",
            "year": 2021
        },
        {
            "authors": [
                "Songyao Jiang",
                "Bin Sun",
                "Lichen Wang",
                "Yue Bai",
                "Kunpeng Li",
                "Yun Fu"
            ],
            "title": "Skeleton aware multi-modal sign language recognition",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 3413\u20133423.",
            "year": 2021
        },
        {
            "authors": [
                "Yutong Chen",
                "Ronglai Zuo",
                "Fangyun Wei",
                "Yu Wu",
                "Shujie Liu",
                "Brian Mak"
            ],
            "title": "Two-stream network for sign language recognition and translation",
            "venue": "Advances in Neural Information Processing Systems, vol. 35, pp. 17043\u2013 17056, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Taivanbat Badamdorj",
                "Mrigank Rochan",
                "Yang Wang",
                "Li Cheng"
            ],
            "title": "Joint visual and audio learning for video highlight detection",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 8127\u20138137.",
            "year": 2021
        },
        {
            "authors": [
                "Mathieu De Coster",
                "Karel D\u2019Oosterlinck",
                "Marija Pizurica",
                "Paloma Rabaey",
                "Severine Verlinden",
                "Mieke Van Herreweghe",
                "Joni Dambre"
            ],
            "title": "Frozen pretrained transformers for neural sign language translation",
            "venue": "18th Biennial Machine Translation Summit (MT Summit 2021). Association for Machine Translation in the Americas, 2021, pp. 88\u201397.",
            "year": 2021
        },
        {
            "authors": [
                "Jens Forster",
                "Christoph Schmidt",
                "Thomas Hoyoux",
                "Oscar Koller",
                "Uwe Zelle",
                "Justus Piater",
                "Hermann Ney"
            ],
            "title": "Rwth-phoenix-weather: A large vocabulary sign language recognition and translation corpus",
            "venue": "05 2012.",
            "year": 2012
        },
        {
            "authors": [
                "Ilias Papastratis",
                "Kosmas Dimitropoulos",
                "Dimitrios Konstantinidis",
                "Petros Daras"
            ],
            "title": "Continuous sign language recognition through cross-modal alignment of video and text embeddings in a joint-latent space",
            "venue": "IEEE Access, vol. 8, pp. 91170\u201391180, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Yuecong Min",
                "Aiming Hao",
                "Xiujuan Chai",
                "Xilin Chen"
            ],
            "title": "Visual alignment constraint for continuous sign language recognition",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 11542\u201311551.",
            "year": 2021
        },
        {
            "authors": [
                "Hao Zhou",
                "Wengang Zhou",
                "Yun Zhou",
                "Houqiang Li"
            ],
            "title": "Spatial-temporal multi-cue network for continuous sign language recognition",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, 2020, vol. 34, pp. 13009\u201313016.",
            "year": 2020
        },
        {
            "authors": [
                "Ronglai Zuo",
                "Brian Mak"
            ],
            "title": "C2slr: Consistencyenhanced continuous sign language recognition",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 5131\u20135140.",
            "year": 2022
        },
        {
            "authors": [
                "Yuecong Min",
                "Peiqi Jiao",
                "Yanan Li",
                "Xiaotao Wang",
                "Lei Lei",
                "Xiujuan Chai",
                "Xilin Chen"
            ],
            "title": "Deep radial embedding for visual sequence learning",
            "venue": "European Conference on Computer Vision. Springer, 2022, pp. 240\u2013256.",
            "year": 2022
        },
        {
            "authors": [
                "Jens Forster",
                "Christoph Schmidt",
                "Oscar Koller",
                "Martin Bellgardt",
                "Hermann Ney"
            ],
            "title": "Extensions of the sign language recognition and translation corpus rwthphoenix-weather",
            "venue": "05 2014, vol. 1.",
            "year": 2014
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014 Sign Language, Multi-modal Learning, Cross-Attention, Video Representation"
        },
        {
            "heading": "1. INTRODUCTION",
            "text": "1.5 billion people on this earth have some form of hearing impairment, and statistically, eight out of 100,000 people are born mute [1] and the most important medium to communicate with these deaf-mute people is through Sign Language (SL). But, sign language involves complex gestures and quick body movements, making them hard for many to understand. Therefore, an automated system to translate them into readable format is essential.\nContinuous Sign Language Recognition (CSLR) involves detecting a sequence of glosses from the input video stream. Sign glosses can be said to be equivalent to spoken language words (with different grammar and lexicon) that match the meaning of signs [2]. On the other hand, Sign Language Translation (SLT) deals with generating a whole sentence from the input video stream, making it a more comprehensive and understandable version of CSLR.\nThe recent advancements in CNNs and Transformers have revolutionized the procedures for CSLR and SLT. CSLR models using deep learning typically have three parts [3]. The first\none is a visual feature extractor model. It can be either 2D CNN [4, 3] or 3D CNNs [5]. Afterward, comes a sequential module that helps incorporate some extent of global information into the features. Popular component for this module includes RNNs [3, 5], 1D-CNNs, or Transformers [2, 3]. And finally, an alignment module that uses CTC [4] in most of the recent cases. Some studies [6, 4] have explored iterative training where they discovered deep learning pipelines were inadequate in properly training the feature extractor.\nIn the case of SLT, Camgo\u0308z et al. proposed the NeuralSLT model, treating the problem as a neural machine translation problem [7]. After the introduction of Transformer networks, Camgo\u0308z et al. applied it for sign language tasks for the first time [2], proposing an end-to-end SLT and CSLR pipeline that does not require any real temporal information. Voskou et al. extended it by applying local-winner-takes-all instead of ReLU activation in the dense layers [8].\nSign language identification depends on understanding the temporal dynamics of intricate movements of multiple body parts, including hands and fingers. Methods like LSTM and self-attention as a temporal module after the feature extraction, or using 3D CNNs are generally used to address this issue. However integrating optical flow can improve the performance, as it highlights movement-involved regions that are vital for sign detection.\nSeveral studies have already tried to incorporate multimodal information such as optical flow and body key points along with RGB images. Jiang et al. trained separate networks for each modality [9]. As they were ensembled, it could potentially miss the joint co-relation among the modalities. Cui et al. used simple summation to combine RGB and optical flow features [6]. It might fail to capture the inter-frame relationship of different modalities, as summation works component-wise. Most recently, Chen et al. proposed a two-stream network for RGB and body point data [10]. However, it trained two feature extractors(S3D model) simultaneously with the network, raising concerns about high computational complexity.\nIn our work, we use cross-modal attention [11] between RGB and optical flow features as a plugin module, that can be added to any existing network to improve its performance. Also, to mimic both RGB and optical flow representation, we added soft distillation loss between the multi-modal features\nar X\niv :2\n30 9.\n01 86\n0v 3\n[ cs\n.C V\n] 6\nD ec\n2 02\n3\nand two original (RGB and flow) features. Besides, we have used only one feature extractor to be trained in an end-to-end fashion in SLR. In SLT, no feature extractor was used in an end-to-end manner. Though this reduces the computational complexity in several folds, it also affects the performance slightly. However, we were able to improve the performance from the baseline method (SMKD [4] for SLR and Stochastic Transformer [8] for SLT) in both cases. Also, we were able to achieve competitive performance against existing approaches."
        },
        {
            "heading": "2. METHODOLOGY",
            "text": ""
        },
        {
            "heading": "2.1. Cross-Modal Attention Module",
            "text": "This paper uses a generic plug-in cross-modal attention module that can be integrated into any existing network that deals with unimodal continuous sign language recognition or translation. The module has been illustrated in Figure 1b.\nWe characterize each video V having n frames by RGB images (img i) and optical flows (flow i), where i \u2208 1, 2, 3, ..., n. By using different feature extractors, we get RGB features (ri) and flow features (fi). These two features are passed into the cross-modal attention module. For generating flow attended RGB features, fr i, the following steps are followed.\nFirstly, the query (Q) is generated from optical flow features (fi), and key (K) and value (V ) are generated from RGB features (ri).\nQi = Wf\u2192Q \u2217 fi Ki = Wr\u2192K \u2217 ri Vi = Wr\u2192V \u2217 ri\n(1)\nHere W represents the weight vector of a simple linear neural network.\nThe attention weights of RGB features when optical flow features are used as the query is defined as, \u03c9f\u2192r\n\u03c9f\u2192r = softmax\n( QTK\u221a\ndk\n) (2)\nwhere dk is the length of the features vectors. Finally, the dot product between the value (V ) and attention wights (\u03c9f\u2192r) are passed through a dense layer to generate the final output, fr .\nm = V \u2217 \u03c9f\u2192r fr = Wm\u2192fr \u2217m\n(3)\nRGB features attended optical flow features rf i, can be generated in the same way by using the RGB features as query (Q) and flow features as key (K), value (V ).\nFinally, the multi-modal feature, mmi, is generated as following,\nmmi = w1 \u2217 ri + w2 \u2217 fr i + w3 \u2217 rf i (4)\n, where the weights ( w1, w2, w3 ) are trainable. The optical flow features (fi) are excluded from the final multi-modal feature because it was used only to aid the representational capability of the RGB features."
        },
        {
            "heading": "2.2. Sign Language Recognition",
            "text": "For our approach regarding SLR, we have used SMKD [4] as the baseline and added the optical flow information on top of it using the plugin explained earlier. The detailed methodology has been shown in Figure 1a.\nWe will need the RGB features (ri) and optical flow features (fi) to proceed in this case. We use different feature extractors for different input modalities, correspondingly FE r,\nFE f .\nri = FE r(img i) fi = FE f (flow i) (5)\nBetween these feature extractors, FE r is trained along with the network in an end-to-end fashion and FE f is pre-trained using SMKD procedure.\nThese features then undergo temporal dimension reduction with two different networks (temp reducer, temp reducef ) that use a combination of 1D-CNN and 1D-Maxpooling to reduce the number of frames to a factor of 1/4 .\nr reduced j = temp reducer(ri) f reduced j = temp reducef (fi) (6)\n, where j \u2208 1, 2, 3, ..., n4 . These two features then enter the cross-modal attention module to create multi-modal features, which are then passed through a Bi-LSTM layer for global context. A classifier then generates a vector for gloss prediction.\nmmj = CrossAttention(r reduced j , f reduced j)\nglobal j = BiLSTM (mmj)\nglossj = clsfinal(global j)\n(7)\nA classifier is also used for both of the r reduced and f reduced to generate gloss vector prediction for RGB and optical flow separately.\ngloss rgbj = clsrgb(r reduced j)\ngloss flow j = clsflow(f reduced j) (8)\nThe original loss of the network is Connectionist Temporal Classification (CTC) loss between one-hot ground truth gloss vector (gt gloss) and gloss . Along with gloss , gloss rgb and gloss flow are also supervised to be as close as the gt gloss .\nLCTC = CTC (gloss, gt gloss) L1 = CTC (gloss rgb, gt gloss) L2 = CTC (gloss flow , gt gloss)\n(9)\nTo enable the final gloss vector to have features similar to both RGB and optical flow features, we used KL-Divergence Loss for distillation for both modalities.\nL3 = KL Divergence(gloss rgb, gloss) L4 = KL Divergence(gloss flow , gloss)\n(10)\nThe final loss is defined as,\nL = LCTC + L1 + L2 + \u03b1L3 + \u03b2L4 (11)"
        },
        {
            "heading": "2.3. Sign Language Translation",
            "text": "For our approach regarding SLT, we have used Stochastic Transformer [8] as the baseline and added optical flow data with it. In this case, the network is trained with features of both RGB and optical flow modalities which have been extracted by pre-trained networks. The pre-trained networks in each case was trained according to SMKD [4] procedure.\nNo feature extractor was trained alongside the model for this case. RGB features and optical flow features are passed into the cross-modal attention module to generate multimodal features. These multi-modal features are then passed onto the Stochastic Transformer network instead of the RGB features. An overview of the method has been shown in Figure 2.\nri = FE r(img i) fi = FE f (flow i) (12)\nmmi = CrossAttention(ri, fi) (13)\npred tokens = Stochastic Transformer(mm) (14)\nIn this scenario, the model is trained with cross-entropy loss between predicted tokens and ground truth tokens.\nL = CrossEntropy(pred tokens, gt token) (15)"
        },
        {
            "heading": "3. EXPERIMENTAL RESULTS",
            "text": ""
        },
        {
            "heading": "3.1. Sign Language Recognition",
            "text": "We have used RWTH-PHOENIX-Weather [13] dataset to report the results of the following experiments.\nExperiments with different feature extractors: Feature extraction is vital to our solution. We tested with different\nbackbones and the results in Table 2 show, complex backbone leads to overfitting and reduced performance.\nComparison with other multi-modal approaches: Earlier multi-modal feature inclusion practices used ensembling [9] and summation [6] as discussed in section 1. Table 3 compares them by using their method in our approach in the place of cross-modal attention.\nComparison with existing Methods: In Table 4, we compared existing SLR approaches against our cross-attention approach and it shows our approach achieved better test score and dev-test performance stability."
        },
        {
            "heading": "3.2. Sign Language Translation",
            "text": "We have used RWTH-PHOENIX-Weather with translation corpus [19] dataset for evaluation of the approaches in SLT.\nIn transformer networks, normally the encoders and decoders are stacked with multiple layers. Through an exhaustive search, we found that 1 layer of cross-modal attention\nmodule, 2 layers of encoders, and 2 layers of decoders provided the optimal result.\nFinally, we compared the results with existing approaches and our approaches. The baseline result of the Stochastic Transformer was achieved by running the code on the default setting. In our work, we haven\u2019t experimented with the ensembled version.\nThe results have been summarized in Table 1."
        },
        {
            "heading": "4. CONCLUSION",
            "text": "In this work, we use cross-modal attention between RGB features and optical flow features to generate a unified multimodal feature representation that contains the movementrelated information from optical flow along with usual RGB features. The additional plugin cross-modal module is very lightweight (only two attention modules) and doesn\u2019t cause a notable inference time increase. Besides, we have used the same module on two different works reinforcing our claim that it can be integrated into any uni-modal procedure. Furthermore, in both cases, we witnessed improvement in performance on the benchmark dataset."
        },
        {
            "heading": "5. REFERENCES",
            "text": "[1] \u201cSign language in schools?,\u201d https: //www.voicesofyouth.org/blog/ sign-language-schools, 2021, Accessed: 2023-03-06.\n[2] Necati Cihan Camgo\u0308z, Oscar Koller, Simon Hadfield, and R. Bowden, \u201cSign language transformers: Joint end-to-end sign language recognition and translation,\u201d 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 10020\u201310030, 2020.\n[3] Zhe Niu and Brian Kan-Wing Mak, \u201cStochastic finegrained labeling of multi-state sign glosses for continuous sign language recognition,\u201d in European Conference on Computer Vision, 2020.\n[4] Aiming Hao, Yuecong Min, and Xilin Chen, \u201cSelfmutual distillation learning for continuous sign language recognition,\u201d 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 11283\u201311292, 2021.\n[5] Junfu Pu, Wengang Zhou, and Houqiang Li, \u201cIterative alignment network for continuous sign language recognition,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp. 4165\u20134174.\n[6] Runpeng Cui, Hu Liu, and Changshui Zhang, \u201cA deep neural framework for continuous sign language recognition by iterative training,\u201d IEEE Transactions on Multimedia, vol. 21, pp. 1880\u20131891, 2019.\n[7] Necati Cihan Camgo\u0308z, Simon Hadfield, Oscar Koller, Hermann Ney, and R. Bowden, \u201cNeural sign language translation,\u201d 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7784\u20137793, 2018.\n[8] Andreas Voskou, Konstantinos P Panousis, Dimitrios Kosmopoulos, Dimitris N Metaxas, and Sotirios Chatzis, \u201cStochastic transformer networks with linear competing units: Application to end-to-end sl translation,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 11946\u2013 11955.\n[9] Songyao Jiang, Bin Sun, Lichen Wang, Yue Bai, Kunpeng Li, and Yun Fu, \u201cSkeleton aware multi-modal sign language recognition,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 3413\u20133423.\n[10] Yutong Chen, Ronglai Zuo, Fangyun Wei, Yu Wu, Shujie Liu, and Brian Mak, \u201cTwo-stream network for sign\nlanguage recognition and translation,\u201d Advances in Neural Information Processing Systems, vol. 35, pp. 17043\u2013 17056, 2022.\n[11] Taivanbat Badamdorj, Mrigank Rochan, Yang Wang, and Li Cheng, \u201cJoint visual and audio learning for video highlight detection,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 8127\u20138137.\n[12] Mathieu De Coster, Karel D\u2019Oosterlinck, Marija Pizurica, Paloma Rabaey, Severine Verlinden, Mieke Van Herreweghe, and Joni Dambre, \u201cFrozen pretrained transformers for neural sign language translation,\u201d in 18th Biennial Machine Translation Summit (MT Summit 2021). Association for Machine Translation in the Americas, 2021, pp. 88\u201397.\n[13] Jens Forster, Christoph Schmidt, Thomas Hoyoux, Oscar Koller, Uwe Zelle, Justus Piater, and Hermann Ney, \u201cRwth-phoenix-weather: A large vocabulary sign language recognition and translation corpus,\u201d 05 2012.\n[14] Ilias Papastratis, Kosmas Dimitropoulos, Dimitrios Konstantinidis, and Petros Daras, \u201cContinuous sign language recognition through cross-modal alignment of video and text embeddings in a joint-latent space,\u201d IEEE Access, vol. 8, pp. 91170\u201391180, 2020.\n[15] Yuecong Min, Aiming Hao, Xiujuan Chai, and Xilin Chen, \u201cVisual alignment constraint for continuous sign language recognition,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 11542\u201311551.\n[16] Hao Zhou, Wengang Zhou, Yun Zhou, and Houqiang Li, \u201cSpatial-temporal multi-cue network for continuous sign language recognition,\u201d in Proceedings of the AAAI Conference on Artificial Intelligence, 2020, vol. 34, pp. 13009\u201313016.\n[17] Ronglai Zuo and Brian Mak, \u201cC2slr: Consistencyenhanced continuous sign language recognition,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 5131\u20135140.\n[18] Yuecong Min, Peiqi Jiao, Yanan Li, Xiaotao Wang, Lei Lei, Xiujuan Chai, and Xilin Chen, \u201cDeep radial embedding for visual sequence learning,\u201d in European Conference on Computer Vision. Springer, 2022, pp. 240\u2013256.\n[19] Jens Forster, Christoph Schmidt, Oscar Koller, Martin Bellgardt, and Hermann Ney, \u201cExtensions of the sign language recognition and translation corpus rwthphoenix-weather,\u201d 05 2014, vol. 1."
        }
    ],
    "title": "ATTENTION-DRIVEN MULTI-MODAL FUSION: ENHANCING SIGN LANGUAGE RECOGNITION AND TRANSLATION",
    "year": 2023
}