{
    "abstractText": "Accurate and robust abdominal multi-organ segmentation from CT imaging of different modalities is a challenging task due to complex interand intra-organ shape and appearance variations among abdominal organs. In this paper, we propose a probabilistic multi-organ segmentation network with hierarchical spatial-wise feature modulation to capture flexible organ semantic variants and inject the learnt variants into different scales of feature maps for guiding segmentation. More specifically, we design an input decomposition module via a conditional variational auto-encoder to learn organ-specific distributions on the low dimensional latent space and model richer organ semantic variations that is conditioned on input images. Then by integrating these learned variations into the V-Net decoder hierarchically via spatial feature transformation, which has the ability to convert the variations into conditional Affine transformation parameters for spatial-wise feature maps modulating and guiding the fine-scale segmentation. The proposed method is trained on the publicly available AbdomenCT-1K dataset and evaluated on two other open datasets, i.e., 100 challenging/pathological testing patient cases from AbdomenCT-1K fully-supervised abdominal organ segmentation benchmark and 90 cases from TCIA+&BTCV dataset. Highly competitive or superior quantitative segmentation results have been achieved using these datasets for four abdominal organs of liver, kidney, spleen and pancreas with reported Dice scores improved by 7.3% for kidneys and 9.7% for pancreas, while being \u223c7 times faster than two strong baseline segmentation methods (nnUNet and CoTr).",
    "authors": [
        {
            "affiliations": [],
            "name": "Minfeng Xu"
        },
        {
            "affiliations": [],
            "name": "Heng Guo"
        },
        {
            "affiliations": [],
            "name": "Jianfeng Zhang"
        },
        {
            "affiliations": [],
            "name": "Ke Yan"
        }
    ],
    "id": "SP:aec349a99ce9b48b0b5863b51e036e73fa8be535",
    "references": [
        {
            "authors": [
                "J.J. Cerrolaza",
                "M. Reyes",
                "R.M. Summers",
                "M.\u00c1. Gonz\u00e1lez-Ballester",
                "M.G. Linguraru"
            ],
            "title": "Automatic multi-resolution shape modeling of multi-organ structures",
            "venue": "Medical image analysis 25(1), 11\u201321",
            "year": 2015
        },
        {
            "authors": [
                "J. Chen",
                "Y. Lu",
                "Q. Yu",
                "X. Luo",
                "E. Adeli",
                "Y. Wang",
                "L. Lu",
                "A.L. Yuille",
                "Y. Zhou"
            ],
            "title": "Transunet: Transformers make strong encoders for medical image segmentation",
            "venue": "arXiv preprint arXiv:2102.04306",
            "year": 2021
        },
        {
            "authors": [
                "K. Clark",
                "B. Vendt",
                "K. Smith",
                "J. Freymann",
                "J. Kirby",
                "P. Koppel",
                "S. Moore",
                "S. Phillips",
                "D. Maffitt",
                "M Pringle"
            ],
            "title": "The cancer imaging archive (tcia): maintaining and operating a public information repository",
            "venue": "Journal of digital imaging 26(6), 1045\u20131057",
            "year": 2013
        },
        {
            "authors": [
                "H. De Vries",
                "F. Strub",
                "J. Mary",
                "H. Larochelle",
                "O. Pietquin",
                "A.C. Courville"
            ],
            "title": "Modulating early visual processing by language",
            "venue": "Advances in Neural Information Processing Systems 30",
            "year": 2017
        },
        {
            "authors": [
                "E. Gibson",
                "F. Giganti",
                "Y. Hu",
                "E. Bonmati",
                "S. Bandula",
                "K. Gurusamy",
                "B. Davidson",
                "S.P. Pereira",
                "M.J. Clarkson",
                "D.C. Barratt"
            ],
            "title": "Automatic multi-organ segmentation on abdominal ct with dense v-networks",
            "venue": "IEEE transactions on medical imaging 37(8), 1822\u20131834",
            "year": 2018
        },
        {
            "authors": [
                "I. Higgins",
                "L. Matthey",
                "A. Pal",
                "C. Burgess",
                "X. Glorot",
                "M. Botvinick",
                "S. Mohamed",
                "A. Lerchner"
            ],
            "title": "beta-vae: Learning basic visual concepts with a constrained variational framework (2016",
            "year": 2016
        },
        {
            "authors": [
                "X. Huang",
                "S. Belongie"
            ],
            "title": "Arbitrary style transfer in real-time with adaptive instance normalization",
            "venue": "Proceedings of the IEEE international conference on computer vision. pp. 1501\u20131510",
            "year": 2017
        },
        {
            "authors": [
                "F. Isensee",
                "P.F. Jaeger",
                "S.A. Kohl",
                "J. Petersen",
                "K.H. Maier-Hein"
            ],
            "title": "nnu-net: a self-configuring method for deep learning-based biomedical image segmentation",
            "venue": "Nature methods 18(2), 203\u2013211",
            "year": 2021
        },
        {
            "authors": [
                "H. Kim",
                "A. Mnih"
            ],
            "title": "Disentangling by factorising",
            "venue": "International Conference on Machine Learning. pp. 2649\u20132658. PMLR",
            "year": 2018
        },
        {
            "authors": [
                "S. Kohl",
                "B. Romera-Paredes",
                "C. Meyer",
                "J. De Fauw",
                "J.R. Ledsam",
                "K. Maier-Hein",
                "S. Eslami",
                "D. Jimenez Rezende",
                "O. Ronneberger"
            ],
            "title": "A probabilistic u-net for segmentation of ambiguous images",
            "venue": "Advances in neural information processing systems 31",
            "year": 2018
        },
        {
            "authors": [
                "B. Landman",
                "Z. Xu",
                "J. Igelsias",
                "M. Styner",
                "T. Langerak",
                "A. Klein"
            ],
            "title": "Miccai multiatlas labeling beyond the cranial vault\u2013workshop and challenge",
            "venue": "Proc. MICCAI Multi-Atlas Labeling Beyond Cranial Vault\u2014Workshop Challenge. vol. 5, p. 12",
            "year": 2015
        },
        {
            "authors": [
                "J. Ma",
                "Y. Zhang",
                "S. Gu",
                "C. Zhu",
                "C. Ge",
                "Y. Zhang",
                "X. An",
                "C. Wang",
                "Q. Wang",
                "X Liu"
            ],
            "title": "Abdomenct-1k: Is abdominal organ segmentation a solved problem",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "year": 2021
        },
        {
            "authors": [
                "F. Milletari",
                "N. Navab",
                "S.A. Ahmadi"
            ],
            "title": "V-net: Fully convolutional neural networks for volumetric medical image segmentation",
            "venue": "2016 fourth international conference on 3D vision (3DV). pp. 565\u2013571. IEEE",
            "year": 2016
        },
        {
            "authors": [
                "T. Miyato",
                "T. Kataoka",
                "M. Koyama",
                "Y. Yoshida"
            ],
            "title": "Spectral normalization for generative adversarial networks",
            "venue": "arXiv preprint arXiv:1802.05957",
            "year": 2018
        },
        {
            "authors": [
                "T. Okada",
                "M.G. Linguraru",
                "M. Hori",
                "R.M. Summers",
                "N. Tomiyama",
                "Y. Sato"
            ],
            "title": "Abdominal multi-organ segmentation from ct images using conditional shape\u2013 location and unsupervised intensity priors",
            "venue": "Medical image analysis 26(1), 1\u201318",
            "year": 2015
        },
        {
            "authors": [
                "T. Park",
                "M.Y. Liu",
                "T.C. Wang",
                "J.Y. Zhu"
            ],
            "title": "Semantic image synthesis with spatially-adaptive normalization",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 2337\u20132346",
            "year": 2019
        },
        {
            "authors": [
                "E. Perez",
                "H. De Vries",
                "F. Strub",
                "V. Dumoulin",
                "A. Courville"
            ],
            "title": "Learning visual reasoning without strong priors",
            "venue": "arXiv preprint arXiv:1707.03017",
            "year": 2017
        },
        {
            "authors": [
                "E. Perez",
                "F. Strub",
                "H. De Vries",
                "V. Dumoulin",
                "A. Courville"
            ],
            "title": "Film: Visual reasoning with a general conditioning layer",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence. vol. 32",
            "year": 2018
        },
        {
            "authors": [
                "F. P \u0301erez-Garc \u0301\u0131a",
                "R. Sparks",
                "S. Ourselin"
            ],
            "title": "Torchio: a python library for efficient loading, preprocessing, augmentation and patch-based sampling of medical images in deep learning",
            "venue": "Computer Methods and Programs in Biomedicine 208, 106236",
            "year": 2021
        },
        {
            "authors": [
                "H.R. Roth",
                "A. Farag",
                "E. Turkbey",
                "L. Lu",
                "J. Liu",
                "R.M. Summers"
            ],
            "title": "Data from pancreas-ct",
            "venue": "the cancer imaging archive. IEEE Transactions on Image Processing",
            "year": 2016
        },
        {
            "authors": [
                "H.R. Roth",
                "L. Lu",
                "A. Farag",
                "H.C. Shin",
                "J. Liu",
                "E.B. Turkbey",
                "R.M. Summers"
            ],
            "title": "Deeporgan: Multi-level deep convolutional networks for automated pancreas segmentation",
            "venue": "International conference on medical image computing and computerassisted intervention. pp. 556\u2013564. Springer",
            "year": 2015
        },
        {
            "authors": [
                "H.R. Roth",
                "H. Oda",
                "X. Zhou",
                "N. Shimizu",
                "Y. Yang",
                "Y. Hayashi",
                "M. Oda",
                "M. Fujiwara",
                "K. Misawa",
                "K. Mori"
            ],
            "title": "An application of cascaded 3d fully convolutional networks for medical image segmentation",
            "venue": "Computerized Medical Imaging and Graphics 66, 90\u201399",
            "year": 2018
        },
        {
            "authors": [
                "A. Shimizu",
                "R. Ohno",
                "T. Ikegami",
                "H. Kobatake",
                "S. Nawano",
                "D. Smutek"
            ],
            "title": "Segmentation of multiple organs in non-contrast 3d abdominal ct images",
            "venue": "International journal of computer assisted radiology and surgery 2(3), 135\u2013142",
            "year": 2007
        },
        {
            "authors": [
                "K. Sohn",
                "H. Lee",
                "X. Yan"
            ],
            "title": "Learning structured output representation using deep conditional generative models",
            "venue": "Advances in neural information processing systems 28",
            "year": 2015
        },
        {
            "authors": [
                "C.H. Sudre",
                "W. Li",
                "T. Vercauteren",
                "S. Ourselin",
                "M. Jorge Cardoso"
            ],
            "title": "Generalised dice overlap as a deep learning loss function for highly unbalanced segmentations",
            "venue": "Deep learning in medical image analysis and multimodal learning for clinical decision support, pp. 240\u2013248. Springer",
            "year": 2017
        },
        {
            "authors": [
                "J. Sykes"
            ],
            "title": "Reflections on the current status of commercial automated segmentation systems in clinical practice",
            "year": 2014
        },
        {
            "authors": [
                "T. Tong",
                "R. Wolz",
                "Z. Wang",
                "Q. Gao",
                "K. Misawa",
                "M. Fujiwara",
                "K. Mori",
                "J.V. Hajnal",
                "D. Rueckert"
            ],
            "title": "Discriminative dictionary learning for abdominal multi-organ segmentation",
            "venue": "Medical image analysis 23(1), 92\u2013104",
            "year": 2015
        },
        {
            "authors": [
                "B. Van Ginneken",
                "C.M. Schaefer-Prokop",
                "M. Prokop"
            ],
            "title": "Computer-aided diagnosis: how to move from the laboratory to the clinic",
            "venue": "Radiology 261(3), 719\u2013732",
            "year": 2011
        },
        {
            "authors": [
                "X. Wang",
                "K. Yu",
                "C. Dong",
                "C.C. Loy"
            ],
            "title": "Recovering realistic texture in image superresolution by deep spatial feature transform",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 606\u2013615",
            "year": 2018
        },
        {
            "authors": [
                "Y. Wang",
                "Y. Zhou",
                "W. Shen",
                "S. Park",
                "E.K. Fishman",
                "A.L. Yuille"
            ],
            "title": "Abdominal multi-organ segmentation with organ-attention networks and statistical fusion",
            "venue": "Medical image analysis 55, 88\u2013102",
            "year": 2019
        },
        {
            "authors": [
                "Y. Xie",
                "J. Zhang",
                "C. Shen",
                "Y. Xia"
            ],
            "title": "Cotr: Efficiently bridging cnn and transformer for 3d medical image segmentation",
            "venue": "International conference on medical image computing and computer-assisted intervention. pp. 171\u2013180. Springer",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "Keywords: Abdominal multi-organ segmentation \u00b7 Conditional variational auto-encoder \u00b7 Hierarchical spatial feature transform."
        },
        {
            "heading": "1 Introduction",
            "text": "Automatic abdominal multi-organ segmentation is crucial for the down-streamed clinical tasks of computer-aided diagnosis, imaging biomarker measurement and treatment respond [28,26]. In the abdomen, multi-organ segmentation is still a challenge task due to the nature of large variations of patient data. First, the CT intensity contrast level between abdominal organs is relatively low [12].\nSecond, the heterogeneous lesions and complex spatial structures of the organs cause the diversity of organ morphology. Finally, different CT scanners, different scanning protocols and contrast agents further diversify the intensity distributions of the organs. In the abdominal region, a variety of classical model-based approaches have been proposed to exploit multi-organ segmentation in past few years. For example, statistical shape models [1,15] use the positional relationship between the organs and the shape of the organs in general statistical space as constraints to regularize/optimize the segmentation. However, when the shape distribution of the target organ has too large non-Gaussian deviations with the mean shape, these methods often fail to segment well. Multi-atlas label fusion related work [23,27] first reconstruct a few numbers of atlases with segmentation annotations, then attempt to fit the target image to the original CT image in each of the atlases via nonrigid registration, and finally propagate and fuse the annotation labels for segmentation. Nevertheless, the nonrigid registration of 3D images is time-consuming and requires similar texture patterns between the target image and original images in the atlases to work.\nRecently, fully convolutional neural networks (CNN) based methods and CNN-Transformer based techniques have been proposed and applied to addressing this important task [5,30,22]. For example, two stages segmentation methods are reported [30,22] which usually utilize the localization information from the first stage to assist the second stage of multi-organ segmentation tasks. Isensee et al. [8] propose a general CNN-based segmentation framework obtaining the stateof-the-art performances in many medical image segmentation benchmarks and often serving as a strong segmentation baseline. Due to its ability of long-range contextual modelling, transformer is used to medical image segmentation. Chen et al. [2] present a novel network architecture that uses transformer to further encode the CNN encoder for building a stronger encoder and report competitive multiorgan segmentation results. Xie et al. [31] present a CNN-Transformer based technique that incorporates the deformable self-attention mechanism into transformer for reducing the computational and spatial complexities. Although these methods have shown competitive results, they may fail to segment the data which is rarely seen in the training set or the heavily pathological organs.\nIn this paper, we propose a novel fully-supervised deep learning image segmentation model that provides robust and accurate segmentation results for multiple abdominal organs (Fig. 1a). It is a modified V-Net model [13] combined with an input decomposition module via a conditional variational auto-encoder (cVAE) [6,9,24] and a hierarchical spatial feature transform module for boosting segmentation accuracy and feature representation. 1) The input decomposition module can represent complex distributions and probabilistically encode possible organ-specific segmentation variants to a low dimensional latent space [10]. 2) The hierarchical spatial feature transform (HSFT) module generates conditional Affine transformation parameter pairs for each spatial scale based on the sample from the learned latent space, and it can better preserve organ semantic variations against common normalization layers [16]. 3) Different from [29,16], HSFT module does not need segmentation probability maps or segmentation\nmasks as the external data for generating the Affine transformation parameters. By integrating the predicted organ-specific semantic information as prior and modulating the feature maps for each scale, it generalizes well to the data which is rarely representative as seen from the training set or the organs are heavily affected by lesions. We have evaluated our approach on two publicly available datasets. The quantitative experimental results demonstrate that our method is highly competitive or superior than previous state-of-the-art techniques [8,31,2]."
        },
        {
            "heading": "2 Methods",
            "text": "Our proposed network architecture is a new V-Net model modified with a cVAE as input decomposition module and a HSFT generator module. Fig. 1a shows the overall structures of the proposed network architecture: the prior encoder and posterior encoder have the same architecture that decomposes the inputs into Ndimensional axis-aligned Gaussian latent space (N is set to 64 in our experiments) and probabilistically learns organ-specific semantic variants [10]. During training, the random sample from the posterior is set to the hierarchical spatial feature transform generator, then to each spatial feature transform (SFT) block by convolution, in order to produce the modulation parameter pairs (\u03b1i, \u03b2i) with spatial dimensions at each scale. Finally, this learned transformation effectively propagates the semantic information throughout the decoder of V-Net.\nWe use multi-class cross entropy and generalized dice loss functions [25] as the segmentation loss LSeg. We employ Kullback-Leiber divergence to make the prior encoder and conditional posterior encoder approximating each other. When in testing without the ground truth segmentation, the prior encoder still can predict the sample zp approximately towards the posterior zq as much as possible, LKL = DKL(q (z |y , x )||p(z |x )). Both losses are combined as a weighted sum with weighting factors \u03bb1 = 1.0 and \u03bb2 = 10.0 in our experiments,\nL = \u03bb1LSeg + \u03bb2LKL (1)\nInput decomposition. In the abdominal multi-organ segmentation task, the huge diversity or variations of inter-subject and intra-subject is the main challenge. We attempt to construct a low dimensional latent space to encode the possible or all anatomically feasible variants that can be observed in the training dataset to facilitate the downstream tasks. As shown in [10], a cVAE model can capture complex distributions of organ appearance and shape variants. This input decomposition module consists of a prior encoder and a posterior encoder. These two encoders have the same network architecture as composed of four blocks. Each block contains four Conv-ReLU layers, and each layer includes a 3D convolutional layer followed by Rectified Linear Unit (ReLU) activation. Average pooling is used between blocks to increase the receptive field. The organ prior distribution is modeled as axis-aligned Gaussian functions with estimated means and variances, generated by a 1 x 1 x 1 convolution layer. [10] fuses an additional channel obtained from the predicted latent space to the last decoder block of UNet which suits well for single organ segmentation task. However, it cannot extend to multiple organs segmentation because the sample z from latent space without encoding the multi-organ spatial relations. Directly upsampling and tiling the sample z will lead to semantic (infeasible) segmentation errors.\nHierarchical spatial feature transform (HSFT). Conditional batch normalization has proven highly effective in many vision tasks [7,4,17,18]. Inspired by previous studies on feature normalization [29,16], HSFT module is designed to apply Affine spatial transformation hierarchically on whose parameters from the predicted organ-specific sample z, which enables each V-Net decoder block to incorporate semantic information for guiding segmentation in a coarse-to-fine manner. Different from [29,16], our HSFT module does not need segmentation probability maps or segmentation masks as the external data for generating the Affine transformation parameters (consisting of 4 SFT blocks). More formally, each SFT block learns three functions fi, hi and gi based on the predicted prior zi\u22121, where function gi is used to upsample the prior zi\u22121 as the input of fi, hi and the next SFT block; and fi and hi can be arbitrary functions (such as neural networks). They generate the Affine transformation parameter pairs \u03b1i and \u03b2i, respectively. zi = gi (zi\u22121 ), \u03b1i = fi (zi ), \u03b2i = hi (zi ) (2) As shown in Fig. 1b, function gi consists of two gated transposed convolutional layers. Function fi contains two Conv-SN layers, and each layer includes a 3D covolutional layer followed by spectral normalization (SN) [14], and Leaky Rectified Linear Unit (LeakyReLU) is used between these two layers. The architecture of function hi is the same with function fi, except that the sigmoid layer as the output layer is applied.\nAfter obtaining the Affine transformation parameters from the organ-specific priors, the operator of modulation is performed. More specifically, the Affine\n\u2297\ntransformation parameters are multiplied and added to each decoder block in the V-Net architecture, in both feature-wise and spatial-wise manipulations. It restricts the prior information stored in z to only refine the specific organs of the input images.\nFi = \u03b1i \u2297 Fi + \u03b2i (3)\nwhere Fi is the output of V-Net ith decoder block, is the element-wise multiplication."
        },
        {
            "heading": "3 Experiments and Results",
            "text": "Training dataset: AbdomenCT-1K contains 1112 abdominal CT scans by incorporating and extending several existing benchmark datasets of LiTS1, KiTS2, MSD3 and NIH Pancreas4. These CT scans are from 12 medical centers with multi-phase, multi-vendor and multi-disease patient cases [12]. All cases are provided with the annotated liver, left and right kidney, spleen and pancreas masks and the re-annotated existing benchmark datasets termed as LiTS+, KiTS+, MSD+ and NIH+. The fully supervised abdominal organ segmentation benchmark5 builds two subtasks based on different collections of 361 training cases from AbdomenCT-1K. Training set of subtask 1 is composed of MSD+(281 cases) and NIH+(80 cases) with all portal phase CT scans. Training set of subtask 2 is composed of MSD+(281 cases), KiTS+(40 cases) and LiTS+(40 cases).\nTesting dataset: Our proposed approach is evaluated on two publicly available testing datasets. The first set contains 100 challenging (mostly pathological) patient cases from the AbdomenCT-1K fully supervised abdominal organ segmentation benchmark. The other testing dataset has 90 abdominal CT scans in total: 43 scans are re-annotated by [5] from the Cancer Imaging Archive Pancrease-CT(termed as TCIA+) [21,20,3] and 47 scans are from Beyond the Cranial Vault(BTCV) [11]. Different from the first testing dataset, the TCIA+& BTCV6 set evaluates only liver, left kidney, spleen and pancreas."
        },
        {
            "heading": "3.1 Implementation Details",
            "text": "For pre-processing, we first crop the CT scans by thresholding to get a rough abdominal 3D region of interest (ROI), then clip the voxel intensity value to the soft tissue CT window range [-175, 275] Hounsfield Units (HU) to remove the irrelevant details and scale to [0, 1] linearly. The target ROI is resized to 128 x 128 x 128 voxels. In the training stage, we employ RandomBlur, RandomGamma and Ran- domNoise from TorchIO Library [19] to do online intensity augmentation\nwith 1 https://competitions.codalab.org/competitions/15595 2 https://kits19.grand-challenge.org/ 3 http://medicaldecathlon.com/ 4 https://wiki.cancerimagingarchive.net/display/Public/Pancreas-CT 5 https://abdomenct-1k-fully-supervised-learning.grand-challenge.org/ 6 http://doi.org/10.5281/zenodo.1169361\ndefault parameters. According to the preset probability of each transform, we apply only one of the given transforms at a time. We adopt Adam optimizer with the weight decay of 1e 8 and an initial learning rate of 1e 4 decayed by 10% every 20 epochs. The training batch size is set as 4 and we trained 200 epochs in total. A NVIDIA V100 GPU with 32 GB memory is used for training. For inference, we process the testing cases following the above pre-processing steps. Then we resize the segmentation prediction from 128 x 128 x 128 to the size of the cropped abdominal ROI and fill it back to the original volume size."
        },
        {
            "heading": "3.2 Results",
            "text": "Our proposed abdomen multi-organ segmentation method is evaluated on Abdomen CT-1K fully supervised abdominal organ segmentation benchmark (Liver, left and right Kidneys, Spleen and Pancreas) and TCIA+&BTCV dataset (Liver, left Kidney, Spleen and Pancreas) of four organs in each subtask respectively. We employ the Dice coefficient scores (in percentages %) as the quantitative evaluation metric with standard deviations reported. To further evaluate the performance of our proposed approach, we compare it to the commonly representative CNN based architecture of [8] which leading many benchmarks on multi-organ segmentation, and compare to two other CNN-Transformer architectures of [2] and [31].\nPerformance on AbdomenCT-1K: Table 1 gives the quantitative segmentation evaluation results on each organ for each subtask. It can be observed that our method overall achieves very competitive segmentation accuracy performance against three strong baselines [8,2,31] in subtask 1. In particular, our model produces average Dice gains of 7.3% and 9.7% than [8] for kidneys and pancreas respectively from subtask 1. In subtask 2, the performance of four methods on liver, kidney and kidneys are almost the same or very comparable. Our method\u2019s performance on pancreas segmentation is 3.5% lower than [8] in Dice coefficient, but with the lowest standard deviation 14.9% reported. Fig. 2 shows qualitative segmentation results of two challenging patient cases (listed\nin [12] from each subtask). The first row in Fig. 2 presents a case with fatty liver where [8] and [31] cause severe under-segmentation; our model and [2] generate evidently more complete liver in subtask 1. The second row shows another case\u2019s segmentation result in subtask 2. Our model can segment spleen completely, whereas spleen is under-segmented using [8,2] and [31] exists incorrect segmentation (kidney mask on Spleen).\nPerformance on TCIA+&BTCV: Table 2 provides the numerical segmentation results of our models and three comparison methods of each organ in each subtask. Except for pancreas, our approach achieves quantitatively the best accuracy (averaged Dice) and most stable (standard deviation) segmentation results than other methods [8,2,31]. Fig. 3 illustrates the segmentation results from each subtask on TCIA+&BTCV. In subtask 1 (first row), it can be seen that our model generates the correct and more complete segmentation result than all other three methods. The second row shows a case\u2019s segmentation result in subtask 2, only our model is able to segment right kidney completely. Discussion In contrast from [8] and [31], our approach is designed based on the learned semantic variants via a probabilistic cVAE model and learned organ-\nspecific semantic variations being integrated into each V-Net decoder block by hierarchical spatial feature transformation for segmentation regularization. So even if some testing sets have severe diseases and demonstrate low image quality (where the training sets have very few similar cases), our model achieves quantitatively competitive segmentation results for each organ in each evaluation subtask. Especially in subtask 1, the training set is all portal phase CT, which is a pancreas related subset from AbdomenCT-1K, our method can do well by adequately generalizing to the lesion-affected liver, kidney and spleen organs, it also handles well multi-phase CT imaging data in testing sets (shown as the first row in Fig. 2). From Table 1 and Table 2, we observe that our approach obtains more statistically stable segmentation performance with different training sets. However, [8] and [31] have shown better performance in pancreas segmentation (in Table 2). In order to achieve very high inference efficiency, we downsample the input to 128 128 128, which likely affecting the spatially narrow type of organ segmentation such as pancreas. In unseen testing cases where lesions could have severely affected the shape and appearance of the organs, both our approach and several state-of-the-art methods [8,31,2] have challenges on handling multi-organ segmentation accurately and reliably (Appendix A).\nSubtask 2: 361 Ours 95.9\u00b10.94 91.8\u00b19.98 95.4\u00b13.51 78.4\u00b19.09 \u2217The comparison results of Subtask 1 are from only BTCV dataset and Subtask 2 are from TCIA+&BTCV."
        },
        {
            "heading": "4 Conclusion",
            "text": "In this paper, we present a new probabilistic V-Net model with hierarchical spatial feature transform for abdominal multi-organ segmentation. Our key idea is that we use a cVAE module conditioned on input images to learn the organspecific distributions on low dimensional latent space and capture richer semantic organ appearance variations. The learned semantic information is injected into each V-Net decoder block hierarchically by each spatial feature transform block for boosting the multi-organ segmentation performance in both accuracy and inference efficiency. The proposed approach has been quantitatively evaluated on two public datasets, demonstrating robust generality and high accuracy to handle lesion-affected/pathological organs and cope with low image quality cases."
        },
        {
            "heading": "Appendix Challenging segmentation cases",
            "text": "Challenging Segmentation Cases with Failures\nChallenging but Successful Segmentation Cases"
        }
    ],
    "title": "A New Probabilistic V-Net Model with Hierarchical Spatial Feature Transform for Efficient Abdominal Multi-Organ Segmentation",
    "year": 2022
}