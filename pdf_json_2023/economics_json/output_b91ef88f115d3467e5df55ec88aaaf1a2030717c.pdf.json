{
    "abstractText": "This paper proposes a statistical framework with which artificial intelligence can improve human decision making. The performance of each human decision maker is first benchmarked against machine predictions; we then replace the decisions made by a subset of the decision makers with the recommendation from the proposed artificial intelligence algorithm. Using a large nationwide dataset of pregnancy outcomes and doctor diagnoses from prepregnancy checkups of reproductive age couples, we experimented with both a heuristic frequentist approach and a Bayesian posterior loss function approach with an application to abnormal birth detection. We find that our algorithm on a test dataset results in a higher overall true positive rate and a lower false positive rate than the diagnoses made by doctors only. We also find that the diagnoses of doctors from rural areas are more frequently replaceable, suggesting that artificial intelligence assisted decision making tends to improve precision more in less developed regions.",
    "authors": [
        {
            "affiliations": [],
            "name": "Kai Feng"
        },
        {
            "affiliations": [],
            "name": "Han Hong"
        },
        {
            "affiliations": [],
            "name": "Ke Tang"
        },
        {
            "affiliations": [],
            "name": "Jingyuan Wang"
        }
    ],
    "id": "SP:6a6cb7d9ccaeecc1cd89cf34b7a560039962b5a6",
    "references": [
        {
            "authors": [
                "Athey",
                "Susan",
                "Stefan Wager"
            ],
            "title": "Policy learning with observational data,",
            "venue": "Econometrica, 2021,",
            "year": 2021
        },
        {
            "authors": [
                "Bansal",
                "Gagan",
                "Tongshuang Wu",
                "Joyce Zhou",
                "Raymond Fok",
                "Besmira Nushi",
                "Ece Kamar",
                "Marco Tulio Ribeiro",
                "Daniel Weld"
            ],
            "title": "Does the whole exceed its parts? the effect of ai explanations on complementary team performance,",
            "venue": "in \u201cProceedings of the 2021 CHI Conference on Human Factors in Computing Systems\u201d",
            "year": 2021
        },
        {
            "authors": [
                "Berk",
                "Richard"
            ],
            "title": "An impact assessment of machine learning risk forecasts on parole board decisions and recidivism,",
            "venue": "Journal of Experimental Criminology, 2017,",
            "year": 2017
        },
        {
            "authors": [
                "Breiman",
                "Leo"
            ],
            "title": "Random forests,",
            "venue": "Machine learning,",
            "year": 2001
        },
        {
            "authors": [
                "Castro",
                "Victor M",
                "Dmitriy Dligach",
                "Sean Finan",
                "Sheng Yu",
                "Anil Can",
                "Muhammad Abd-El-Barr",
                "Vivian Gainer",
                "Nancy A Shadick",
                "Shawn Murphy",
                "Tianxi Cai"
            ],
            "title": "Large-scale identification of patients with cerebral aneurysms using natural language processing,",
            "venue": "Neurology, 2017,",
            "year": 2017
        },
        {
            "authors": [
                "Currie",
                "Janet",
                "WB MacLeod"
            ],
            "title": "Diagnosing expertise: Human capital, decision making, and performance among physicians,",
            "venue": "Journal of labor economics, 2017,",
            "year": 2017
        },
        {
            "authors": [
                "Elliott",
                "Graham",
                "Robert P"
            ],
            "title": "Lieli, \u201cPredicting binary outcomes,",
            "venue": "Journal of Econometrics,",
            "year": 2013
        },
        {
            "authors": [
                "Erickson",
                "Bradley J",
                "Panagiotis Korfiatis",
                "Zeynettin Akkus",
                "Timothy L Kline"
            ],
            "title": "Machine learning for medical imaging,",
            "venue": "Radiographics, 2017,",
            "year": 2017
        },
        {
            "authors": [
                "Esteva",
                "Andre",
                "Brett Kuprel",
                "Roberto A Novoa",
                "Justin Ko",
                "Susan M Swetter",
                "Helen M Blau",
                "Sebastian Thrun"
            ],
            "title": "Dermatologist-level classification of skin cancer with deep neural networks,",
            "year": 2017
        },
        {
            "authors": [
                "Fauw",
                "Jeffrey De",
                "Joseph R Ledsam",
                "Bernardino Romera-Paredes",
                "Stanislav Nikolov",
                "Nenad Tomasev",
                "Sam Blackwell",
                "Harry Askham",
                "Xavier Glorot",
                "Brendan O\u2019Donoghue",
                "Daniel Visentin"
            ],
            "title": "Clinically applicable deep learning for diagnosis and referral in retinal disease,",
            "venue": "Nature medicine,",
            "year": 2018
        },
        {
            "authors": [
                "Feng",
                "Kai",
                "Han Hong",
                "Ke Tang"
            ],
            "title": "Statistical Inference of Optimal Allocations,",
            "venue": "Working paper,",
            "year": 2023
        },
        {
            "authors": [
                "Fuster",
                "Andreas",
                "Paul Goldsmith-Pinkham",
                "Tarun Ramadorai",
                "Ansgar Walther"
            ],
            "title": "Predictably unequal? The effects of machine learning on credit markets,",
            "venue": "The Journal of Finance, 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Grove",
                "William M",
                "David H Zald",
                "Boyd S Lebow",
                "Beth E Snitz",
                "Chad Nelson"
            ],
            "title": "Clinical versus mechanical prediction: a meta-analysis.,",
            "venue": "Psychological assessment,",
            "year": 2000
        },
        {
            "authors": [
                "Gruber",
                "Jonathan",
                "Maria Owings"
            ],
            "title": "Physician financial incentives and cesarean section delivery,",
            "venue": "The Rand Journal of Economics,",
            "year": 1996
        },
        {
            "authors": [
                "Gulshan",
                "Varun",
                "Lily Peng",
                "Marc Coram",
                "Martin C Stumpe",
                "Derek Wu",
                "Arunachalam Narayanaswamy",
                "Subhashini Venugopalan",
                "Kasumi Widner",
                "Tom Madams",
                "Jorge Cuadros"
            ],
            "title": "Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs,",
            "venue": "Jama, 2016,",
            "year": 2016
        },
        {
            "authors": [
                "Guo",
                "Jonathan",
                "Bin Li"
            ],
            "title": "The application of medical artificial intelligence technology in rural areas of developing countries,",
            "venue": "Health equity, 2018,",
            "year": 2018
        },
        {
            "authors": [
                "Kahneman",
                "Daniel",
                "Gary Klein"
            ],
            "title": "Conditions for intuitive expertise: a failure to disagree.,",
            "venue": "American psychologist,",
            "year": 2009
        },
        {
            "authors": [
                "Kermany",
                "Daniel S",
                "Michael Goldbaum",
                "Wenjia Cai",
                "Carolina CS Valentim",
                "Huiying Liang",
                "Sally L Baxter",
                "Alex McKeown",
                "Ge Yang",
                "Xiaokang Wu",
                "Fangbing Yan"
            ],
            "title": "Identifying Medical Diagnoses and Treatable Diseases by Image-Based Deep Learning,",
            "venue": "Cell, 2018,",
            "year": 2018
        },
        {
            "authors": [
                "Kitagawa",
                "Toru",
                "Aleksey"
            ],
            "title": "Tetenov, \u201cWho should be treated? empirical welfare maximization methods for treatment choice,",
            "venue": "Econometrica, 2018,",
            "year": 2018
        },
        {
            "authors": [
                "Kleinberg",
                "Jon",
                "Himabindu Lakkaraju",
                "Jure Leskovec",
                "Jens Ludwig",
                "Sendhil"
            ],
            "title": "Mullainathan, \u201cHuman decisions and machine predictions,",
            "venue": "The Quarterly Journal of Economics, 2018,",
            "year": 2018
        },
        {
            "authors": [
                "Liang",
                "Huiying",
                "Brian Y. Tsui",
                "Hao Ni",
                "Carolina C.S. Valentim",
                "Sally L. Baxter",
                "Guangjian Liu",
                "Wenjia Cai",
                "Daniel S. Kermany",
                "Xin Sun",
                "Jiancong Chen"
            ],
            "title": "Evaluation and accurate diagnoses of pediatric diseases using artificial intelligence,",
            "venue": "Nature Medicine, 2019,",
            "year": 2019
        },
        {
            "authors": [
                "Litjens",
                "Geert",
                "Thijs Kooi",
                "Babak Ehteshami Bejnordi",
                "Arnaud Arindra Adiyoso Setio",
                "Francesco Ciompi",
                "Mohsen Ghafoorian",
                "Jeroen Awm Van Der Laak",
                "Bram Van Ginneken",
                "Clara I S\u00e1nchez"
            ],
            "title": "A survey on deep learning in medical image analysis,",
            "venue": "Medical image analysis,",
            "year": 2017
        },
        {
            "authors": [
                "Long",
                "Erping",
                "Haotian Lin",
                "Zhenzhen Liu",
                "Xiaohang Wu",
                "Liming Wang",
                "Jiewei Jiang",
                "Yingying An",
                "Zhuoling Lin",
                "Xiaoyan Li",
                "Jingjing Chen"
            ],
            "title": "An artificial intelligence platform for the multihospital collaborative management of congenital cataracts,",
            "venue": "Nature biomedical engineering,",
            "year": 2017
        },
        {
            "authors": [
                "Manski",
                "Charles F"
            ],
            "title": "Credible ecological inference for medical decisions with personalized risk assessment,",
            "venue": "Quantitative Economics, 2018,",
            "year": 2018
        },
        {
            "authors": [
                "Meehl",
                "Paul E"
            ],
            "title": "Clinical versus statistical prediction: A theoretical analysis and a review of the evidence.,",
            "year": 1954
        },
        {
            "authors": [
                "Merenstein",
                "Daniel"
            ],
            "title": "Winners and losers,",
            "venue": "Jama, 2004,",
            "year": 2004
        },
        {
            "authors": [
                "Mondal",
                "Himel",
                "Shaikat Mondal",
                "Rajeev K Singla"
            ],
            "title": "Artificial Intelligence in Rural Health in Developing Countries,",
            "venue": "in \u201cArtificial Intelligence in Medical Virology,\u201d Springer,",
            "year": 2023
        },
        {
            "authors": [
                "Peng",
                "Sui",
                "Yihao Liu",
                "Weiming Lv",
                "Longzhong Liu",
                "Qian Zhou",
                "Hong Yang",
                "Jie Ren",
                "Guangjian Liu",
                "Xiaodong Wang",
                "Xuehua Zhang"
            ],
            "title": "Deep learning-based artificial intelligence model to assist thyroid nodule diagnosis and management: a multicentre diagnostic study,",
            "venue": "The Lancet Digital Health, 2021,",
            "year": 2021
        },
        {
            "authors": [
                "Raghu",
                "Maithra",
                "Katy Blumer",
                "Rory Sayres",
                "Ziad Obermeyer",
                "Bobby Kleinberg",
                "Sendhil Mullainathan",
                "Jon Kleinberg"
            ],
            "title": "Direct uncertainty prediction for medical second opinions,",
            "venue": "in \u201cInternational Conference on Machine Learning\u201d",
            "year": 2019
        },
        {
            "authors": [
                "Rajpurkar",
                "Pranav",
                "Awni Y Hannun",
                "Masoumeh Haghpanahi",
                "Codie Bourn",
                "Andrew Y Ng"
            ],
            "title": "Cardiologist-level arrhythmia detection with convolutional neural networks,",
            "venue": "arXiv preprint arXiv:1707.01836,",
            "year": 2017
        },
        {
            "authors": [
                "Strasser",
                "Roger",
                "Sophia M Kam",
                "Sophie M Regalado"
            ],
            "title": "Rural health care access and policy in developing countries,",
            "venue": "Annual review of public health,",
            "year": 2016
        },
        {
            "authors": [
                "Studdert",
                "David M",
                "Michelle M Mello",
                "William M Sage",
                "Catherine M DesRoches",
                "Jordon Peugh",
                "Kinga Zapert",
                "Troyen A Brennan"
            ],
            "title": "Defensive medicine among high-risk specialist physicians in a volatile malpractice environment,",
            "year": 2005
        },
        {
            "authors": [
                "Wang",
                "Jingyuan",
                "Edoardo Gallo",
                "Wei Zhang",
                "Ke Tang",
                "Han Hong"
            ],
            "title": "Diagnosing with the help of artificial intelligence,",
            "year": 2023
        },
        {
            "authors": [
                "Wilson",
                "Nathan W",
                "Ian D Couper",
                "Elma De Vries",
                "Steve Reid",
                "Therese Fish",
                "Ben J Marais"
            ],
            "title": "A critical review of interventions to redress the inequitable distribution of healthcare professionals to rural and remote areas,",
            "venue": "Rural and remote health,",
            "year": 2009
        },
        {
            "authors": [
                "Yang",
                "Jiehua",
                "Mingfei Xie",
                "Canpei Hu",
                "Osamah Alwalid",
                "Yongchao Xu",
                "Jia Liu",
                "Teng Jin",
                "Changde Li",
                "Dandan Tu",
                "Xiaowu Liu"
            ],
            "title": "Deep learning for detecting cerebral aneurysms with CT angiography,",
            "venue": "Radiology, 2021,",
            "year": 2021
        },
        {
            "authors": [
                "Zeng",
                "Jiaming",
                "Berk Ustun",
                "Cynthia Rudin"
            ],
            "title": "Interpretable classification models for recidivism prediction,",
            "venue": "Journal of the Royal Statistical Society. Series A (Statistics in Society),",
            "year": 2017
        }
    ],
    "sections": [
        {
            "text": "Keywords: Artificial Intelligence, Machine Learning, Decision Making, ROC Curve JEL Classification: C44, C11, C12\nar X\niv :2\n30 6.\n11 68\n9v 1\n[ ec\non .E"
        },
        {
            "heading": "1 Introduction",
            "text": "In the current era of machine learning, novel artificial intelligence (AI) algorithms have emerged as valuable tools that can learn significant features from data, thereby showing potential for facilitating decision making in various disciplines. To mention a few, Gulshan et al. (2016) used a deep learning algorithm to detect diabetic retinopathy in retinal fundus photographs; Castro et al. (2017) applied natural language processing techniques to electronic medical records to identify patients with cerebral aneurysms; Fuster et al. (2022) leveraged machine learning algorithms to predict default probability for lending decision. Berk (2017) applied machine learning predictions to parole release decisions.\nA considerable number of papers have compared the performance of algorithms with that of the representative human decision makers. For instance, Rajpurkar et al. (2017) trained a deep convolutional neural network (CNN) to process ECG sequences and compared its performance to the aggregate performance of cardiologists; Esteva et al. (2017) proposed a deep CNN for skin cancer classification and claimed that the algorithm outperformed the aggregate performance of dermatologists; Kermany et al. (2018) used transfer learned CNN to classify macular degeneration and concluded that the machine algorithm outperforms some retinologists; Kleinberg et al. (2018) trained a gradient boosted decision tree to predict crime risk and suggested that substituting human judges with the algorithm could result in large welfare gains.\nThe findings of the aforementioned literature rely mostly on the observation that the pair of false positive rate (FPR) and true positive rate (TPR) point lies strictly below the receiver operating characteristic (ROC) curve generated by the machine learning algorithm, implying that algorithms can achieve a higher TPR for a given FPR or a lower FPR for a given TPR1. Typically, an algorithm first learns a propensity score function p pxq from data, then applies the decision rule 1 pp pxq \u0105 cq to compute the FPR/TPR pair, where c is a cutoff threshold. The machine ROC curve is generated by varying the cutoff thresholds c. Additionally, using historical data, the aggregate sample FPR/TPR point for human decision makers can also be calculated.\nWe first caution against such interpretations without a deeper understanding of human decision making processes. The literature findings can be rationalized not only by the superior information quality of algorithms but also by the incentive heterogeneity of human decision makers, who can be no less accurate than algorithms in processing statistical information from observational data. Second, the FPR/TPR pairs of human decision makers are typically imprecisely measured, especially when the number of cases for each the decision makers is not large. The data in our empirical analysis clearly shows substantial heterogeneity between\n1In binary classification / decision making, FPR is the number of wrongly classified negative events divided by the number of actual negative events, and TPR is the number of correctly classified positive events divided by the number of total positive events. A pair of FPR/TPR is similar to the tradeoff between size and power for classical hypothesis tests.\nindividual human decision makers; the data also shows that the randomness in measuring the quality of decision makers plays a key role when the performance of human decision makers is compared to that of machine algorithms.\nTo illustrate the first issue of concern, consider Figure 1, in which a collection of FPR/TPR pairs for human decision makers, denoted as j \u201c 1, . . . , J , all lie approximately on a ROC curve generated by their employed decision rules pYi,j \u201c 1 pp pXi,j , Ui,jq \u0105 cjq with different individual cutoff threshold cj for decision maker j. In the decision rule, Xi,j are observed features used in the machine learning algorithm; Ui,j are private information only observable to the human decision makers, and p px, uq \u201c P pY \u201c 1|x, uq is a correctly specified propensity score function. Also drawn is the machine ROC curve based on classification rules pYi \u201c 1 pp pXiq \u0105 cq, where p pxq \u201c P pY \u201c 1|xq is the true propensity score function using observable features x discovered by the machine learning algorithm. However, after averaging over all decision makers, the aggregate human FPR/TPR point lies visibly below the machine ROC curve.\n[Figure 1 about here.]\nThe visual illusion that hampers the comparison between machine learning algorithms and human decision makers is an immediate artifact of Jensen\u2019s inequality. It is well known that an optimal ROC generated by a correctly specified propensity score function is necessarily concave, since it is the primal function of a concave program. See for example Feng et al. (2023). Lemma A.1 in the supplementary appendix provides a set of primitive conditions under which the aggregate human decision maker FPR/TPR lies strictly below the machine ROC curve. More precisely, denote the FPR as \u03b1j and the TPR as \u03b2j , and suppose the variables are related by \u03b2 \u201c g p\u03b1q, where g p\u00a8q is strictly concave. Then, by Jensen\u2019s inequality:\n\u03b2\u0304 \u201c 1 J\nJ \u00ff j\u201c1 \u03b2j \u201c 1 J J \u00ff j\u201c1 g p\u03b1jq \u0103 g\n\u02dc\n1 J\nJ \u00ff j\u201c1 \u03b1i\n\u00b8\n\u201c g p\u03b1\u0304q , (1)\nwhen \u03b1j are not all equal. This important observation appears to have gone largely unnoticed by the literature. In the NFPC dataset of this paper, doctors have highly variable tendencies to diagnose risky pregnancy. Some doctors are highly conservative and diagnose almost all patients with a high-risk pregnancy. Meanwhile, there are also doctors that rarely diagnose any patient as highly risky. Similarly, suppose the cutoff thresholds ci,j of doctor j differ across their patients i \u201c 1, . . . , nj . Then even if each patient i is diagnosed using a point on the human ROC curve p\u03b1i,j , \u03b2i,jq, it is also the case that\n\u03b2j \u201c 1\nnj\nnj \u00ff i\u201c1 \u03b2i,j \u201c 1 nj nj \u00ff i\u201c1 g p\u03b1i,jq \u0103 g\n\u02dc\n1\nnj\nnj \u00ff i\u201c1 \u03b1i,j\n\u00b8\n\u201c g p\u03b1jq .\nWe do not observe the latent human ROC curve corresponding to p px, uq. By itself, an aggregate human decision maker FPR/TPR pair that lies below a machine ROC curve does not\ndifferentiate between a model of incentive heterogeneity, as depicted in Figure 1, where humans are better informed than machines, and a model of inferior human information processing capacity without incentive heterogeneity, where the human ROC curve corresponding to a mis-specified p px, uq lies below the machine ROC and contains the aggregate FPR/TPR pair.\nIn this paper, in order to compare machines with human decision makers, we need to assume away incentive heterogeneity, and focus on information processing capacity. We interpret a human FPR/TPR pair below the machine ROC curve as evidence of the superiority of the machine algorithms. Under these assumptions, if the individual FPR/TPR pairs are precisely known without sampling errors, they can be directly compared to the machine ROC curve. As shown in Figure 3, a precisely known human FPR/TPR pair below the machine ROC curve can be dominated by any point on the line segment of the ROC curve between A and B. Replacing the human FPR/TPR pair by any point on the A-B segment of the machine ROC curve segment results in higher TPR without increasing FPR, or lower FPR without sacrificing TPR.\nIn reality, only an estimate of the FPR/TPR pair can be obtained from the empirical data. A statistical framework to compare the performance of algorithms with that of human decision makers requires accounting for estimation sampling errors. We focus on two main issues. First, we seek statistical evidence supporting the information superiority of an algorithm that favors the replacement of a human decision maker. Second, we aim to identify the most appropriate point on the machine ROC curve for this purpose.\nTo address these two issues, we experiment with both a heuristic frequentist confidence set approach and a subsequent Bayesian inference approach. In both scenarios, confidence and credible regions are formed to inform the decision making process. The Bayesian approach results in more replacement and additional improvement in the aggregate TPR/FPR pair. We suggest the Bayesian analysis as an applicable decision framework.\nIn practice, it is likely that humans have either less information processing capacity or superior information processing capacity than an algorithm has. For instance, Liang et al. (2019) proposes a machine learning algorithm to diagnose childhood diseases. A comparison of the algorithm with physicians shows that the authors\u2019 model outperforms the junior physician groups but marginally underperforms senior physician groups. Similar findings are reported in Peng et al. (2021). In Esteva et al. (2017) and Kermany et al. (2018), the human experts\u2019 performance also exhibit large inter-individual variability. Our goal is to provide and formalize a rigorous statistical framework for comparing the performance of individual decision makers with that of an algorithm. Our framework determines a winner between each human decision maker and the algorithm for making future decisions. It is implemented through a human-algorithm mixed decision rule under which the less capable human decision makers are replaced by the algorithm, whereas skillful human decision makers who are comparable to or better than the algorithm are retained. The eventual future decision making is done through a combination of\ncapable humans and the algorithm. We apply our statistical framework to data from a unique National Free Prepregnancy Checkups (NFPC) project. The NFPC is a free health checkup service for conceiving couples and is conducted across 31 provinces in China. The data set includes the doctors\u2019 IDs and diagnoses of adverse pregnancy outcomes. We first split the overall dataset into two parts. The first part is used to compare doctors with algorithms. Specifically, we employ a random forest (RF) method for diagnosing risky pregnancy, which achieves an area under the curve (AUC) above 0.68. Using 95% credible level, our Bayesian statistical framework suggests that the random forest algorithm outperforms 46.1% of doctors. Less capable doctors are replaced with the algorithm. The second part of the data set is used to validate a decision making procedure that combines the decisions by capable doctors and the decisions of the algorithm. In the second part of Bayesian empirical analysis, the combined decision making procedure achieves an increase of 46.6% in the TPR and a reduction of 10.1% in the FPR. The potential for machine algorithms to improve decision making performance is substantial. We also find that doctors from rural areas under-perform relative to their urban colleagues and are more likely to be replaced by the algorithm. In the context of our Bayesian approach, 53.3% of doctors in rural clinics are replaced compared to 37.1% of doctors in urban clinics.\nOur paper relates to several strands of literature. An extensive literature studies the utilization of AI to enhance medical diagnostic capabilities. To name a few more, Long et al. (2017), De Fauw et al. (2018), Raghu et al. (2019), and Yang et al. (2021) proposed image-based methods while Razzaki et al. (2018), and Brennan et al. (2019) used graphic and structured data. See also the literature reviews in Litjens et al. (2017), Erickson et al. (2017), and Huang et al. (2020). Our method is not limited to medical diagnoses, and is generally applicable to human decision making problems. Beyond medical diagnoses, Berk (2017), Zeng et al. (2017), Kleinberg et al. (2018) and Fuster et al. (2022), among others, investigate the use of AI in social-economic decision making, including loan approval, bail, and parole.\nNumerous papers have benchmarked algorithms against human decision makers. See for example Rajpurkar et al. (2017), Esteva et al. (2017), Kermany et al. (2018), Kleinberg et al. (2018), Kermany et al. (2018), Long et al. (2017), De Fauw et al. (2018), Razzaki et al. (2018), Liang et al. (2019), Brennan et al. (2019), Raghu et al. (2019), Peng et al. (2021) and Yang et al. (2021). Reviews by Meehl (1954) and Grove et al. (2000) argue that algorithms tend to either outperform or match human experts. The health economics literature suggests that doctors may be swayed by multiple non-medical incentives including lawsuit avoidance (Merenstein, 2004; Studdert et al., 2005), financial gain (Gruber and Owings, 1996; Gruber et al., 1999; Johnson and Rehavi, 2016) and procedural skills (Currie and MacLeod, 2017). Our paper differs from this literature by ruling out unspecified sources of incentive and assuming that doctors intend to treat their respective patients consistently. We also abstract away from the interaction between human decision makers and algorithms. In the literature, Brennan\net al. (2019) and Yang et al. (2021) report salient performance improvement when doctors can refer to the prediction results of algorithms. Wang et al. (2023) found that doctors are more willing to follow the recommendation of the algorithm when provided with algorithm\u2019s predictions and accompanying explanations.\nOptimal decision making using machine learning techniques is also drawing increasing attention from econometrics. Kitagawa and Tetenov (2018) and Athey and Wager (2021) explored the asymptotic properties of empirical optimal decision making when algorithms are involved. Manski (2018) considered the identification problem of prediction conditional on the complete set of covariates when only the prediction conditional on a subset of covariates and the conditional distribution of the \u201comitted\u201d covariates are known. In this paper, we assume, as in Kitagawa and Tetenov (2018), that the machine algorithm is known and deterministic, in view of the big data nature of the training procedure and the comprehensive list of covariates available to algorithms in the NFPC dataset.\nThe inequality in health service provision quality between rural and urban areas, especially in developing countries, can be attributed to the uneven distribution of resources, insufficient medical training, excessive workload, and barriers to latest information and continuous education for rural physicians. See for example Strasser et al. (2016); Zhu et al. (2016); Wilson et al. (2009). The potential of artificial intelligence to empower medical diagnose in rural areas has been advocated by researchers including Adepoju et al. (2017); Guo and Li (2018); Mondal et al. (2023) and others. Our finding using the NFPC dataset documents quantitative evidence of the urban-rural medicare quality disparity in China.\nThe rest of this paper is organized as follows. Section 2 presents the statistical model of human-algorithm comparison. Section 3 describes the data and the algorithm. Section 4 reports results from the empirical analysis of the machine algorithm and human decision makers. Section 5 presents a set of synthetic data analysis for additional insights, and Section 6 concludes."
        },
        {
            "heading": "2 Replacing Human Decision Makers with Algorithms",
            "text": "Consider a sample dataset with labels Yi P t0, 1u and features Xi, i \u201c 1, . . . , n. Define\nTPR \u201c 1 n \u0159n i\u201c1 Yi pYi\np\u0302 , FPR \u201c\n1 n \u0159n i\u201c1p1 \u00b4 YiqpYi 1 \u00b4 p\u0302 , p\u0302 \u201c 1 n n \u00ff\ni\u201c1 Yi, (2)\nwhere Y\u0302i P t0, 1u is a predictor for Yi based on a general decision rule, such that \u00b4 Xi, Yi, Y\u0302i \u00af are i.i.d draws from an underlying population. The Law of Large Number implies that the\nTPR/FPR converge to their population analogs (denoted as \u03b2 and \u03b1):\n\u03b2 \u201c 1 p EYi pYi, \u03b1 \u201c 1 1 \u00b4 pE \u201d p1 \u00b4 Yiq pYi \u0131 , where p \u201c EYi \u201c P pYi \u201c 1q . (3)\nTo compare decision makers and machines, we make the following two assumptions:\nAssumption 1. The machine ROC curve represents a propensity score model of prediction:\ny\u0302i,M \u201c 1 pm pxiq \u0105 cq .\nThe decision maker j\u2019s decision is also based on such a perceived model:\ny\u0302i,j \u201c 1 pqj pxi,j , ui,jq \u0105 cjq ."
        },
        {
            "heading": "Both m p\u00a8q and qj p\u00a8q can be correctly specified or misspecified.",
            "text": "Assumption 2. The machine propensity score model and the decision maker\u2019s propensity score model are known without sampling errors by the algorithm and the decision maker respectively.\nA key concept in this paper is incentive heterogeneity. In general terms, incentive refers to the motivation of certain actions, and incentive heterogeneity refers to different payoffs across multiple decision makers for taking the same actions. For human decision making, incentive heterogeneity can typically be classified intuitively as either intra-individual or interindividual. In this paper, intra-individual incentive heterogeneities refer to situations where a human decision maker will change the cost assessment based on publicly observed information features x that are accessible by the machine algorithm and private information features u that are exclusively available only to human decision makers. In other words, the decision maker j uses a model y\u0302 \u201c 1 pqj px, uq \u0105 cj px, uqq. Consequently, for px1, u1q, px2, u2q sharing the same propensity score qj px1, u1q \u201c qj px2, u2q, the decision of the decision maker can be different due to cost differences: cj px1, u1q \u2030 cj px2, u2q. Such a decision rule is considered in Elliott and Lieli (2013). Even if we assume that all doctors have full information processing capacity, so that qj px, uq \u201c p px, uq for a correctly specified propensity function p px, uq such that p px, uq \u201c P pY \u201c 1|x, uq, an arbitrary FPR/TPR pair below the machine ROC curve in any empirical data set of doctor diagnoses can still be rationalized by some decision rule y\u0302 \u201c 1 pp px, uq \u0105 cj px, uqq corresponding to a suitable choice of the cost function cj px, uq.\nTo illustrate using a synthetic example, suppose u is not present; x is uniformly distributed\non p0, 1q and p pxq \u201c x. Let the optimal decision rule be y\u0302 \u201c 1 pp pxq \u011b c pxqq, where\nc pxq \u201c\n$ \u2019 \u2019 \u2019 &\n\u2019 \u2019 \u2019 %\n0, x \u0103 1{4;\n2 px \u00b4 1{4q , 1{4 \u0103 x \u0103 3{4;\n1, x \u0105 3{4.\nThen y\u0302 \u201c 1 ` x \u0103 12 \u02d8 and pTPR, FPRq \u201c p1{8, 3{8q, and this pair lies strictly below the 45 degree line. For instance, x might represent age. The probability of abnormal birth outcome is typically increasing in x. However, doctors might place a high weight on diagnosing older couples as normal if they are sympathetic to older couples\u2019 preference of not forgoing chances of having a child. Then c pxq can be increasing in x. As a result, it is plausible for a doctor to be more inclined to diagnose high risk pregnancy in younger couples. Another example is financial loan application and approval, where y P t0, 1u typically denotes whether a loan is paid back, and y\u0302 P t0, 1u denotes whether a loan application is approved. The amount of principal and interests are features x that are likely to affect the probability p pxq of not defaulting by the borrower, but that are also likely to affect the cost perception c pxq by the lender when the lender decides whether to approval the loan application given knowledge of the probability of not defaulting p pxq. Assumption 1 essentially excludes such difficulty of interpreting a FPR/TPR pair arising from incentive heterogeneity models.\nIn contrast, inter-individual incentive heterogeneity refers to a situation where decision makers differ in their perception of the relative costs between false positives and false negatives, but each of them applies the same threshold to their respective patients. More precisely, decision maker j and k may have cj \u2030 ck. However, the same cj is applied to all the patients of doctor j, and likewise for doctor k.\nAssumption 1 rules out intra-individual incentive heterogeneity and allows us to focus on inter-individual incentive heterogeneity. Figure 2 shows that doctors in our dataset exhibit highly variable degrees of risk aversion and conservativeness in their diagnoses. There exist both doctors who tend to diagnose almost all patients as highly risky, and doctors who rarely make a positive diagnosis.\n[Figure 2 about here.]\nThe average risk level of patients for each doctor is likely to be different across doctors. See for example Kleinberg et al. (2018). To understand whether the cost perception of doctors might vary systematically with the risk level of their patients, we first calculated the FPR rank for each doctor and then used the rank of FPR as a conservativeness measurement for them. We then tested the correlation between these conservativeness ranks and a pregnancy risk level indicator. In particular, we regress the predicted abnormal birth rate for patient i who is diagnosed by doctor j from a random forest algorithm against the doctor\u2019s FPR rank,\ni.e.,\nri,j \u201c \u03b20 ` \u03b21qj ` \u03f5i,j ,\nwhere ri,j denote the patient\u2019s predicted risk by the random forest algorithm described in section 3, and qj their doctor\u2019s FPR rank. The doctors\u2019 FPR rank are normalized to r0, 1s. In other words, if the FPR of doctor j is higher than 80% of the doctors, then qj \u201c 0.8. The regression results for doctors with more than 300 diagnoses and for doctors with more than 500 diagnoses are summarized in Table 12. The regression results suggest that there is no strong evidence that the conservativeness of doctors is associated with the risk level of their patients.\n[Table 1 about here.]\nIt is also important for Assumption 1 to allow for private information ui,j that is only observed by the doctors and not used in the machine learning algorithm. In the absence of the private information variable ui,j , the doctors\u2019 decisions would be perfectly predictable by the machine learning algorithm, which is very unlikely.\nAssumption 2 can be justified by the fact that the sample size used to estimate the machine ROC curve is typically some orders of magnitude larger than the number of cases for an individual decision maker. For instance, around 150 thousand cases are used to estimate the machine ROC curve in our abnormal birth outcome example, whereas the median number of diagnoses per doctor in the classification set is only approximately 400. The estimation error in the machine ROC curve can be considered negligible relative to the error in estimating doctors\u2019 FPR/TPR pairs. Accounting for the sampling error in estimating the machine ROC curve is studied in a subsequent paper by Feng et al. (2023). Section 3 provides more details about the data processing procedure. It is important to note that Assumption 2 also rules out decision makers learning from the sample cases. This is a reasonable assumption in our empirical data set because the eventual pregnancy outcomes are typically not available to doctors when they diagnose consecutive patient cases. Sampling errors arise mainly from the relatively fewer number of observations for each individual decision maker. We are interested in the parameter of the population pair of true positive and false positive rates for the individual decision maker: \u03b8H \u201c p\u03b1H , \u03b2Hq, where \u03b1 and \u03b2 are defined as in (3) and the subscript H refers to \u201chuman\u201d."
        },
        {
            "heading": "2.1 Human FPR/TPR pairs and ROC curves in the population",
            "text": "If \u03b8H is above an machine ROC curve, as in Figure 1, then given the false positive rate level \u03b1H , the algorithm has a lower TPR \u03b2M than humans have. Similarly, given \u03b2H , the machine has a larger FPR \u03b1M than humans have. In this sense, humans outperform the algorithm. In\n2Regression results including quadratic and cubic terms are reported in columns (3) to (6) in Table 1. We use clustered (by doctor) robust standard error.\nparticular, under Assumption 1, if humans correctly use at least as much information as the algorithm does, in the sense that the propensity model p px, uq is correctly specified, then \u03b8H lies above the machine ROC. The formal argument is given in Lemma A.2 in the supplementary appendix.\nFigure 3 shows a \u03b8H pair for a human decision maker that is below the ROC curve of an algorithm. When \u03b8H is below the machine ROC curve, along the machine ROC curve, a point A can be found that matches the \u03b1H but that has a higher \u03b2M . Similarly, another point B can be found that matches \u03b2H but that has a lower \u03b1M . The \u03b1M and \u03b2M of any point between points A and B on the machine ROC curve are smaller and larger, respectively, than the human decision maker\u2019s pair p\u03b1H , \u03b2Hq. A machine decision rule corresponding to a point on the machine ROC between A and B performs better than the human decision maker. The segment of the curve to the left of B (to the right of A) has a smaller \u03b1M but a smaller \u03b2M (a larger \u03b2M but a larger \u03b1M ) than the human decision maker. Points on these segments of the machine ROC curve are not directly comparable to the human decision maker.\n[Figure 3 about here.]\nThe classical Neyman-Pearson lemma states that when p pxq \u201c P pY \u201c 1|xq is correctly specified, for any pair of positive numbers p\u03b7, \u03d5q, there exists a cutoff value c, corresponding to a point on the ROC curve, such that y\u0302 \u201c 1 pp pxq \u0105 cq maximizes a linear combination \u03d5\u03b2 \u00b4 \u03b7\u03b1 of the TPR and FPR. Conversely, each point on the ROC curve optimizes some linear combination of TPR and FPR. Neyman-Pearson Lemma is also motivated by a Bayesian decision maker who is equipped with the true propensity function p pxq, which can be calculatd using the prior odds p and the conditional distribution of the features f px|y \u201c 1q and f px|y \u201c 0q, and who minimizes posterior expected loss of misclassification upon observing the features x, based on a loss matrix of the following form:\nLoss matrix y\u0302 \u201c 0 y\u0302 \u201c 1 y \u201c 0 0 c01 y \u201c 1 c10 0,\nwhere c10 represents the disutility of misclassifying 1 as 0 and c01 represents the cost of misclassifying 0 as 1. Each point on the ROC curve, corresponding to a decision rule y\u0302 \u201c 1 pp pxq \u0105 cq, minimizes Bayesian posterior expected loss for some choice of c10 and c01 through the relation c \u201c c01c10`c01 . Importantly, the cost matrix c01, c10 and linear combination coefficients \u03b7, \u03d5 in the Neyman-Pearson Lemma are independent of the features x in order to provide optimality justification of the points on the ROC curve. Assumption 1 is then interpreted as each doctor behaving as a Bayesian decision maker adopting the same constant cost matrix c10 and c01 among their patients. The ex ante expected minimized disutility for the Bayesian decision\nmaker is also a linear combination of FPR and TPR:\nc10E \u201d p pXq \u00b4 1 \u00b4 Y\u0302 \u00af\u0131 ` c01E \u201d p1 \u00b4 p pXqq Y\u0302 \u0131 \u201c p1 \u00b4 pq c01FPR \u00b4 pc10TPR ` pc10.\nIf we denote point A and B in Figure 3 as p\u03b1H , \u03b2Rq and p\u03b1R, \u03b2Hq, then any point p\u03b1M , \u03b2M q between p\u03b1R, \u03b2Hq and p\u03b1H , \u03b2Rq on the machine ROC curve satisfies \u03b1M \u0103 \u03b1H and \u03b2M \u0105 \u03b2H . For arbitrary \u03d5, \u03b7 \u0105 0 and c10, c01,\n\u03b7\u03b1M \u00b4 \u03d5\u03b2M \u0103 \u03b7\u03b1H \u00b4 \u03d5\u03b2H , and\np1 \u00b4 pq c01\u03b1M \u00b4 pc10\u03b2M \u0103 p1 \u00b4 pq c01\u03b1H \u00b4 pc10\u03b2H .\nTherefore, compared with human decisions, any point between p\u03b1R, \u03b2Hq and p\u03b1H , \u03b2Rq on the ROC curve achieves a better linear combination for all possible values of \u03d5, \u03b7, and lower expected Bayesian disutility for all loss matrixes of c01, c10, representing an improved decision rule. In this sense, we consider p\u03b1M , \u03b2M q to dominate p\u03b1H , \u03b2Hq.\nIn order to provide a more precise interpretation of the sense in which the machine algorithm performs better than the human decision maker when \u03b8H lies below the machine ROC curve, recall that \u03b8H is a population parameter that measures the average performance of the human decision maker after the decision maker sees an infinite number of cases. Therefore, on average, given \u03b1H or \u03b2H , the machine algorithm can outperform the human decision maker by increasing the population TPR or reducing the population FPR, respectively. However, unless the algorithm can make predictions with perfect accuracy, for each specific patient case, the human decision maker can either underperform or outperform the algorithm. From this point on, we will agree that our goal is to infer whether \u03b8H is above or below the machine ROC curve or whether the machine algorithm outperforms the individual human decision maker on average.\nIn light of the previous interpretation, our framework is likely to be applicable when people make many repeated decisions to allow for a sufficiently precise estimate of their error rates, but is less applicable in situations when people only ever make a few decisions of a type. Such settings may arise even when decisions are frequent if some action rarely is or should be taken. The lack of information renders the task of distinguishing humans from algorithms difficult, even if humans might be detectably worse in aggregate3. Our application features a comprehensive set of labels, a large data set, reliable and measurable outcomes, and a stable prediction environment that is consistent with the scenarios in the important papers by Currie and MacLeod (2017) and Kahneman and Klein (2009).\n3We are grateful to the associate editor for pointing out this important clarification."
        },
        {
            "heading": "2.2 Human FPR/TPR pair and ROC curve comparison in the sample",
            "text": "Typically, the population value of \u03b8H is not directly observed, and instead needs to be estimated from a data set. Denote the sample estimate as \u03b8\u0302H \u201c \u00b4 \u03b1\u0302H , \u03b2\u0302H \u00af\n, also denoted as pFPR,TPRq as in (2). In the presence of sampling uncertainty, the inference problem pertains to how we can make a probabilistic statement regarding whether \u03b8H is above or below the machine ROC curve. From a frequentist analysis point of view, because the sample estimate \u03b8\u0302H is a vector function of a multinomial distribution, the finite sample distribution function of \u03b8\u0302H can be analytically intractable. Large sample frequentist analysis and simulation methods, however, are facilitated by the joint asymptotic normal distribution of \u03b8\u0302H .\nLemma 2.1. For an i.i.d. sample !\nYi, pYi\n)n\ni , the joint asymptotic distribution of a human\nFPR/TPR pair \u03b8\u0302H \u201c \u00b4 \u03b1\u0302H , \u03b2\u0302H \u00af is multivariate normal. In particular\n? n \u00b4 \u03b8\u0302H \u00b4 \u03b8H \u00af d\u00dd\u00d1 N p0,\u03a3q , \u03a3 \u201c \u02dc \u03b1Hp1\u00b4\u03b1Hq 1\u00b4p 0\n0 \u03b2Hp1\u00b4\u03b2Hqp\n\u00b8\n.\nThe proof of Lemma 2.1 is given in the appendix and follows from standard arguments for approximating multinomial distributions with normal limits. The asymptotic covariance \u03a3 can be consistently estimated by a sample analog \u03a3\u0302 where the parameters \u03b1H , \u03b2H and p are replaced by \u03b1\u0302H , \u03b2\u0302H and p\u0302. As a consequence of the diagonal asymptotic covariance matrix, the resulting asymptotic confidence set of the FPR/TPR pairs is elliptically shaped. As illustrated in Figure 4, the blue ellipse indicates a 95% level confidence set for a decision maker in our dataset, as described in the next section. The yellow points are the bootstrapped FPR/TPR pairs.\n[Figure 4 about here.]\nAn immediate consequence of Lemma 2.1 is that a direct comparison of the sample estimate \u00b4\n\u03b1\u0302H , \u03b2\u0302H\n\u00af\nwith the machine ROC curve is likely to be insufficient. Consider a situation where the true population \u03b1H , \u03b2H lies strictly below the machine ROC curve: \u03b2H \u0103 g p\u03b1Hq where g p\u00a8q denotes the machine ROC curve. Then with probability converging to 1, \u03b1\u0302H , \u03b2\u0302H lies strictly below the machine ROC. Suppose we replace \u00b4\n\u03b1\u0302H , \u03b2\u0302H\n\u00af\nby p\u03b1\u0302H , g p\u03b1\u0302Hqq. By Lemma 2.1, P p\u03b1\u0302H \u011b \u03b1Hq \u00d1 0.5. In other words, the replacement point on the ROC curve p\u03b1\u0302H , g p\u03b1\u0302Hqq does not dominate the true population pair of p\u03b1H , \u03b2Hq approximately half of the time. Similarly, suppose we replace \u00b4\n\u03b1\u0302H , \u03b2\u0302H\n\u00af by \u00b4 g\u00b41 \u00b4\n\u03b2\u0302H\n\u00af\n, \u03b2\u0302H\n\u00af . Then by Lemma 2.1, P \u00b4 \u03b2\u0302H \u010f \u03b2H \u00af \u00d1 0.5.\nIt is also the case that the replacement point on the ROC curve \u00b4 g\u00b41 \u00b4\n\u03b2\u0302H\n\u00af , g p\u03b1\u0302Hq \u00af\ndoes not dominate the true population p\u03b1H , \u03b2Hq approximately half of the time. A more formal statistical framework is necessary in order to provide a probabilistic statement regarding the\nproperty of the replacement procedure. We consider both a heuristic frequentist approach and a Bayesian approach."
        },
        {
            "heading": "2.3 A heuristic frequentist approach",
            "text": "We initially experiment with a heuristic procedure based on the frequentist principle, the predominant school of thought in statistical inference which typically begins with confidence set construction. The procedure is described as follows.\n\u2022 Form a confidence set S\u0302 for \u03b8H . We will base the confidence set on the asymptotic normal distribution of \u03b8\u0302H .\n\u2022 For each s \u201c p\u03b1s, \u03b2sq P S\u0302, define As as the set of points that dominate s:\nAs \u201c t\u03b8 \u201c p\u03b1, \u03b2q : \u03b1 \u010f \u03b1s, \u03b2 \u011b \u03b2su . (4)\n\u2022 Define A \u201c \u015e sPS\u0302 As: A is the set of points that dominate all the points in S\u0302.\n\u2022 Next, define A\u0304 \u201c A X ROC: A\u0304 is the set of points on the machine ROC curve that simultaneously dominate all the points in S\u0302.\nThe convention of choosing confidence levels in hypothesis testing suggests that the status quo of human decision making is to be replaced by an algorithm only when there is overwhelming evidence from the data indicating the superiority of the algorithm. In our application, we will only consider the machine decision rule to outperform the human decision when A\u0304 \u2030 H, i.e., when there exists a point on the machine ROC curve that dominates an entire confidence set of a conventional level of the human decision maker\u2019s \u03b8H parameter. In other words, we use estimates of the sampling uncertainty of the FPR/TPR pair of \u03b8\u0302H given the level of confidence to provide guidance on how far \u03b8\u0302H needs to be below the machine ROC curve in order to justify replacing the human decision maker with the machine algorithm.\nTo illustrate, consider an elliptical confidence set around the estimated human FPR/TPR pair \u03b8\u0302H . Denote by ` \u03b1\u03b2high , \u03b2high \u02d8\nthe point that achieves the highest TPR on the confidence set, and denote by p\u03b1low, \u03b2\u03b1lowq the point that achieves the smallest FPR on the confidence set. The point p\u03b1low, \u03b2highq corresponds to the point P in Figure 4. Regarding the relation between the position of the confidence set for the human FPR/TPR pair \u03b8H and the machine ROC curve, there are three possibilities.\n1. Case 1: The human elliptical confidence set and point p\u03b1low, \u03b2highq are all below the machine ROC curve. In this case, corresponding to Figure 4, we consider the human decision maker to perform \u201cworse\u201d than the algorithm and hence can be replaced by the algorithm. On the machine ROC curve, we can find two points, namely, p\u03b1R, \u03b2highq and\np\u03b1low, \u03b2Rq, labeled as points B and A in Figure 4, such that any point on the ROC curve between A and B can provide a machine decision rule to replace the human decision maker.\n2. Case 2: The entire human elliptical confidence area is below the ROC curve, but the point p\u03b1low, \u03b2highq is above the ROC curve. In this case, even though each point in the confidence region of \u03b8H is dominated by some point on the machine ROC curve, it is not possible to find a nonempty fraction of the machine ROC curve that simultaneously dominates all the points in the human\u2019s confidence set. In this case, which corresponds to Figure 5(a), the data evidence is not sufficiently convincing to account for the randomness of estimating \u03b8H by \u03b8\u0302H . Consequently, the human decision maker is not replaced by the algorithm.\n3. Case 3: The human elliptical confidence set has a certain area above the machine ROC curve. This case corresponds to Figure 5(b), where the human decision maker is not replaced by the algorithm either.\nIn summary, if the P point p\u03b1low, \u03b2highq is below the ROC curve, the human decision maker is replaced; otherwise, they are not. In case 2 and case 3, it is possible that the human decision maker is sufficiently capable compared to the machine algorithm. An alternative interpretation of these two cases is when the human \u03b8H is not estimated precisely enough, for example, due to a lack of historical data that results in a large oval confidence area.\n[Figure 5 about here.]\nIn order to provide a frequentist justification for the heuristic procedure we just described, recall that in classical statistical inference, \u03b8H is a fixed number and not a random variable. It is either above or below the machine ROC curve, without an associated probability. The confidence set itself S\u0302 is a random set, such that P \u00b4 \u03b8H P S\u0302 \u00af\n\u00dd\u00d1 1\u00b4\u03b1. For a conventional size of \u03b1 \u201c 0.05, if the confidence set is to be constructed 100 times from different samples, S\u0302 will contain \u03b8H approximately 95 times. Consequently, by definition of the confidence set, A\u0304 is a random set such that asymptotically,\nP \u00b4 \u03b8H P S\u0302 \u00af \u201c P \u2423 A\u0304 \u201c H and P dominates \u03b8H ( ` P \u2423 A\u0304 \u2030 H and A\u0304 dominates \u03b8H ( \u011b 1 \u00b4 \u03b1.\nTherefore, we have asymptotically\nP \u2423 A\u0304 dominates \u03b8H |A\u0304 \u2030 H ( \u011b 1 \u00b4 \u03b1 \u00b4 P\n\u2423 A\u0304 \u201c H and P dominates \u03b8H (\nP \u2423 A\u0304 \u2030 H (\nIf \u03b8H lies below the ROC curve, P \u2423 A\u0304 \u201c H ( \u00d1 0 as the sample of cases for the human decision maker increases to infinity. Furthermore, P \u2423 A\u0304 \u201c H and P dominates \u03b8H ( \u0103 P \u2423 A\u0304 \u201c H ( \u00d1 0,\nimplying that asymptotically, P \u2423 A\u0304 dominates \u03b8H |A\u0304 \u2030 H ( \u011b 1 \u00b4 \u03b1. If \u03b8H lies above the ROC, we can control P \u2423 A\u0304 \u201c H ( \u011b 1 \u00b4 \u03b1 by reasoning that\nP \u2423 A\u0304 \u201c H ( \u201c P tP is above ROCu \u011b P tP dominates \u03b8Hu \u011b 1 \u00b4 \u03b1.\nHowever, the finite sample size of the test can be severely distorted. By insisting on finding a segment of the machine ROC curve that simultaneously dominates the entire confidence region of \u03b8H , the frequentist procedure is also conservative by construction.\nWhile conventional confidence sets are mostly symmetric, it is also well known in statistics that by inverting a hypothesis test, alternative confidence sets can be constructed as the collection of parameter values for which the null hypothesis is not rejected. For example, if we let the null hypothesis be H0 : \u03b8H \u201c \u03b80H and the alternative hypothesis be H0 : \u03b1H \u0105 \u03b10H or \u03b2H \u0103 \u03b20H , a test can be formed to reject the null hypothesis H0 when either \u03b1\u0302H \u011b \u03b1H ` c or when \u03b2\u0302H \u010f \u03b2H \u00b4d, where c and d are chosen by the size of the test. The set of \u03b80H where the test does not reject takes the form of t\u03b80H : \u03b10H \u010f \u03b1\u0302H \u00b4 c, and \u03b20H \u010f \u03b2\u0302H ` du, which is a lower rectangular confidence set instead of an elliptical shape. While forming confidence sets based on inverting inequality tests is theoretically intriguing, it is rarely used empirically.\nAnother alternative is to test whether the population value of \u03b8H is above or below the machine ROC curve. Conventionally, the status quo of human decision making is typically chosen as the null hypothesis: H0 : \u03b8H lies above ROC against H1 : \u03b8H lies below ROC. This implies that a priori, we maintain confidence in human beings unless there is overwhelming evidence suggesting otherwise. Because we represent the ROC curve by \u03b2 \u201c g p\u03b1q, where g p\u00a8q is known, increasing, and concave, we can rewrite the null and alternative hypotheses as"
        },
        {
            "heading": "H0 : \u03b2H \u011b g p\u03b1Hq against H1 : \u03b2H \u0103 g p\u03b1Hq .",
            "text": "Under the least favorable null hypothesis, the asymptotic distribution of a test statistic t\u0302 \u201c \u03b2\u0302H \u00b4 g p\u03b1\u0302Hq follows from combining Lemma 2.1 with the Delta method. More precisely, if we define B\u0302 \u201c p\u00b4g1 p\u03b1\u0302Hq , 1q, then the following central limit theorem holds under the least favorable null hypothesis:\nP\n\u00a8\n\u02dd\n? n \u00b4 \u03b2\u0302H \u00b4 g p\u03b1\u0302Hq \u00af\na B\u0302\u03a3\u0302B\u03021 \u010f \u03a6\u00b41 p\u03b1q\n\u02db\n\u201a\u00d1 \u03b1.\nIn the above, we use \u03a6\u00b41 p\u03b1q to denote the \u03b1-th percentile of the standard normal distribution. For example, \u03a6\u00b41 p0.05q \u201c \u00b41.645. A test that rejects the null hypothesis when ? np\u03b2\u0302H\u00b4gp\u03b1\u0302Hqq?\nB\u0302\u03a3\u0302B\u03021\nis less than \u03a6\u00b41 p\u03b1q will have asymptotic size \u03b1. A drawback of the hypothesis test is that even if we reject the null hypothesis of the status quo of human decision making, it does not inform\nus how to choose segments of the machine ROC curve to replace human decisions. Because of this limitation we do not use hypothesis tests."
        },
        {
            "heading": "2.4 The Bayesian Approach",
            "text": "In the following we focus on the alternative Bayesian method, which often allows for a more transparent interpretation than frequentist inference. In particular, a confidence set is often related to a Bayesian posterior credible region with a diffuse prior. In the Bayesian setting, we can interpret the event that A\u0304 \u2030 H as the existence of a FPR/TPR pair along the machine ROC curve that simultaneously dominates 95% of the posterior distribution of \u03b8H . A Bayesian credible set is then constructed to replace approximate frequentist confidence regions based on the asymptotic normal distribution.\nComputing a Bayesian posterior distribution requires two key inputs: the likelihood of the data given the population value of \u03b8H , and a prior distribution for the underlying population parameters. Each observation in the data set consists of both the doctor diagnoses and the ground truth of pregnancy outcomes, and follows a multinomial distribution with four categories: diagnosed as risky and abnormal birth outcome; not diagnosed as risky and abnormal birth outcome; diagnosed as risky and normal birth outcome; not diagnosed as risky and normal birth outcome. The multinomial probabilities have three free parameters:\nt1 \u201c EY Y\u0302 , t2 \u201c E \u201d p1 \u00b4 Y q Y\u0302 \u0131 , t3 \u201c E \u201d Y \u00b4 1 \u00b4 Y\u0302 \u00af\u0131 , t4 \u201c E \u201d p1 \u00b4 Y q \u00b4 1 \u00b4 Y\u0302 \u00af\u0131 ,\nwhere t1 ` t2 ` t3 ` t4 \u201c 1. The multinomial distribution is a completely specified parametric model and allows for exact likelihood Bayesian posterior distribution computation. The likelihood of the data given the parameters t \u201c pt1, t2, t3q depends only on a set of sufficient statistics t\u0302 \u201c ` t\u03021, t\u03022, t\u03023 \u02d8 summarizing information from the data, where\nt\u03021 \u201c 1\nn\nn \u00ff i\u201c1 YiY\u0302i, t\u03022 \u201c 1 n n \u00ff i\u201c1 \u201d p1 \u00b4 Yiq Y\u0302i \u0131 , t\u03023 \u201c 1 n n \u00ff i\u201c1 \u201d Yi \u00b4 1 \u00b4 Y\u0302i \u00af\u0131 .\nIn particular, we can write the data likelihood as (where we use D to refer to Data):\nLpD|tq \u201c \u02c6 n\nnt\u03021\n\u02d9\ntnt\u030211\n\u02c6\nn \u00b4 nt\u03021 nt\u03022\n\u02d9\ntnt\u030222\n\u02c6\nn \u00b4 nt\u03021 \u00b4 nt\u03022 nt\u03023\n\u02d9\ntnt\u030233 t nt\u03024 4 . (5)\nGiven a prior distribution of t, denoted as \u03c0 ptq, the posterior distribution of the parameter t, denoted as p pt|Dq can usually be simulated or calculated analytically:\np pt|Dq \u201c \u03c0 ptqL pD|tq\u015f \u03c0 pt1qL pD|t1q dt1 .\nWe are interested in the posterior distribution of \u03b8H \u201c p\u03b1H , \u03b2Hq, which are functions of the\nunderlying multinomial parameters: \u03b8H \u201c h ptq \u201c p\u03b1H ptq , \u03b2H ptqq. If we can simulate from the posterior distribution p pt|Dq and denote the simulated draws as tr, r \u201c 1, . . . , R, then the posterior distribution of \u03b8H , denoted as p p\u03b8H |Dq, can be estimated using the empirical distribution of h ptrq , r \u201c 1, . . . , R.\nComputing the posterior distribution using a multinomial likelihood is facilitated by the Dirichlet conjugate prior. A Dirichlet prior on the K-dimensional simplex of tk, k \u201c 1, . . . ,K, where\n\u0159K k\u201c1 tk \u201c 1, is specified by hyper-parameters \u03b3 \u201c p\u03b3k, k \u201c 1, . . . ,Kq. Its density is given\nby \u03c0 pt|\u03b3q \u201c 1Bp\u03b3q \u015bK k\u201c1 t \u03b3k\u00b41 k , where B p\u03b3q is the multivariate Beta function (see for example https://en.wikipedia.org/wiki/Dirichlet_distribution). The case with \u03b3k \u201c \u03b3,@k \u201c 1, . . . ,K is called a symmetric Dirichlet distribution. When t \u201c 1 is one dimensional, the Dirichlet distribution specializes to a Beta distribution, which in turn includes the uniform distribution as a further special case when \u03b3 \u201c 1.\nBecause the Dirichlet distribution is conjugate to the multinomial likelihood, the posterior distribution is also Dirichlet with parameters \u03b3\u0302 \u201c p\u03b3\u0302k, k \u201c 1, . . . ,Kq, where \u03b3\u0302k \u201c \u03b3k ` nt\u0302k for each k \u201c 1, . . . ,K. Simulating from the posterior distribution of \u03b8H \u201c p\u03b1H , \u03b2Hq can be accomplished by recomputing \u03b8Hr \u201c h ptrq after drawing for tr from the posterior Dirichlet distribution with parameters \u03b3\u0302. Specifically, for each tr \u201c ptrkqKk\u201c1, we calculate\n\u03b8H,r \u201c h ptrq \u201c p\u03b1H ptrq , \u03b2H ptrqq , where \u03b1H ptrq \u201c tr2\ntr2 ` tr4 , \u03b2H ptrq \u201c tr1 tr1 ` tr3\n. (6)\nThe simulated draws \u03b8Hr, r \u201c 1, . . . , R can be used to estimate different posterior probabilities. For example, the posterior probability that \u03b8H lies below the ROC curve, denoted as\n\u017c\n\u03b8H below ROC p p\u03b8H |Dq d\u03b8H , (7)\ncan be estimated by the fraction of times where \u03b8H,r lies below the ROC curve:\n1 R\nR \u00ff r\u201c1 1 p\u03b8H,r lies below the machine ROC curveq \u201c 1 R R \u00ff r\u201c1 1 p\u03b2H ptrq \u010f g p\u03b1H ptrqqq .\nWe are mainly interested in the maximum posterior probability of \u03b8H that are dominated simultaneously by a single point on the ROC curve, denoted as\nqmax \u201d max \u03b1Pr0,1s\n\u017c\n\u03b1H\u011b\u03b1,\u03b2H\u010fgp\u03b1q p p\u03b8H |Dq d\u03b8H , (8)\nand the corresponding maximizing point, denoted as \u03b8d \u201c p\u03b1d, \u03b2d \u201c g p\u03b1dqq, where\n\u03b1d \u201d argmax\u03b1Pr0,1s\n#\n\u017c\n\u03b1H\u011b\u03b1,\u03b2H\u010fgp\u03b1q p p\u03b8H |Dq d\u03b8H\n+\n. (9)\nBoth (8) and (9) can be estimated by the simulated posterior draws of \u03b8H,r, r \u201c 1, . . . , R by\nq\u0302max \u201c max \u03b1Pr0,1s\n1 R\nR \u00ff r\u201c1 1 p\u03b1H ptrq \u011b \u03b1, \u03b2H ptrq \u010f g p\u03b1qq ,\n\u03b1\u0302d \u201cargmax\u03b1Pr0,1s R \u00ff\nr\u201c1 1 p\u03b1H ptrq \u011b \u03b1, \u03b2H ptrq \u010f g p\u03b1qq .\n(10)\nMore generally, decision-theoretic Bayesian analysis features the minimization of posterior expected losses. Let \u03c1 p\u03b8M , \u03b8Hq denote a loss function that represents the disutility of replacing a human FPR/TPR pair \u03b8H by a machine FPR/TPR pair \u03b8M . Consistent with Assumptions 1 and 2, we specify \u03c1 p\u03b8M , \u03b8Hq \u201c 0 when \u03b1M \u010f \u03b1H and \u03b2M \u011b \u03b2H , or when the machine FPR/TPR pair strictly dominates the human FPR/TPR pair. Otherwise, \u03c1 p\u03b8M , \u03b8Hq may be strictly positive. Given the choice of the functional form of \u03c1 p\u03b8M , \u03b8Hq, a point on the ROC curve can then be chosen to minimize the posterior estimated loss\n\u03b80M \u201c argmin\u03b8M :\u03b2M\u201cgp\u03b1M q \u017c \u03c1 p\u03b8M , \u03b8Hq p p\u03b8H |Dq d\u03b8H , (11)\nwhich is also estimated using the simulated posterior draws of \u03b8H,r, r \u201c 1, . . . , R as\n\u03b8\u03020M \u201c argmin\u03b8M :\u03b2M\u201cgp\u03b1M q 1\nR\nR \u00ff r\u201c1 \u03c1 p\u03b8M , \u03b8H,rq .\nIn particular, (8) is a special case of (11) when \u03c1 p\u03b8M , \u03b8Hq \u201c 1 \u00b4 1 p\u03b1H \u011b \u03b1M , \u03b2H \u010f \u03b2M q. It is also possible to explore more general loss functions. For example, we may weight the loss of replacing \u03b8H by \u03b8M by the distance between \u03b8H and the machine ROC curve:\n\u03c1 p\u03b8M , \u03b8Hq \u201c 1 \u00b4 1 p\u03b1H \u011b \u03b1M , \u03b2H \u010f \u03b2M q \u00a8 inf \u03b8 t}\u03b8H \u00b4 \u03b8} : \u03b8 on ROCu , (12)\nwhere } \u00a8 } is the Euclidean norm. Intuitively, the farther a human FPR/TPR pair \u03b8H is form the ROC curve, the smaller the loss (or the larger the benefit) of replacing it with a dominating point on the machine ROC curve. Alternatively, we can replace the distance from \u03b8H to the ROC curve by the distance of each \u03b8M to the complement of the set of FPR/TPR points that dominate \u03b8H , and use instead the loss function:\n\u03c1 p\u03b8M , \u03b8Hq \u201c 1 \u00b4 1 p\u03b1H \u011b \u03b1M , \u03b2H \u010f \u03b2M q \u00a8 min p\u03b1H \u00b4 \u03b1M , \u03b2M \u00b4 \u03b2Hq . (13)\nThe Euclidean distance from the machine ROC curve may also not fully capture the disutility of not replacing a human FPR/TPR pair \u03b8H that lies under the machine ROC curve. For example, a diagonal ROC curve corresponds to a completely random decision making process without using any feature information. Similarly, a ROC curve below the diagonal can be generated by a decision rule that counteracts against the propensity score, i.e, one that awards rather than penalizes mis-classification errors. Taking these considerations into account, the loss of replacing a human FPR/TPR pair \u03b8H below the diagonal line with a dominating \u03b8M along the machine ROC curve can be specified to zero, or an a priori chosen negative number. We can also adopt a convention of normalizing the loss to be 1 when \u03b8M does not dominate \u03b8H , i.e., when \u03b1M \u011b \u03b1H or \u03b2M \u010f \u03b2H .\nWe are then left to specify the loss \u03c1 p\u03b8M , \u03b8Hq when \u03b8M dominates \u03b8H and when \u03b8H lies between the machine ROC curve and the diagonal line. Each of such \u03b8H can be reproduced by a convex combination of a point on the machine ROC curve and a point on the diagonal line, using a decision rule that randomizes between the point on the machine ROC curve and the point on the diagonal. For example, p\u03b1H , g p\u03b1Hqq is generated by a machine based decision rule Y\u0302i \u201c 1 pm pXiq \u0105 c0q for some threshold c0, while p\u03b1H , \u03b1Hq is generated by a fully randomized decision Y\u0302i \u201c 1 pUi \u011b 1 \u00b4 \u03b1q where Ui is independently uniformly distributed on p0, 1q. Then p\u03b1H , \u03b2Hq can be generated by a lottery that places weight p\u03b2H \u00b4 \u03b1Hq { pg p\u03b1Hq \u00b4 \u03b1Hq on Y\u0302i \u201c 1 pm pXiq \u0105 c0q, and the remaining weight on Y\u0302i \u201c 1 pUi \u011b 1 \u00b4 \u03b1Hq. More precisely, if we let Vi denote the realization of an independent random variable uniformly distributed on p0, 1q, the decision rule\nY\u0302i \u201c 1 \u02c6 Vi \u010f \u03b2H \u00b4 \u03b1H\ng p\u03b1Hq \u00b4 \u03b1H\n\u02d9 1 pm pXiq \u0105 c0q ` 1 \u02c6 Vi \u0105 \u03b2H \u00b4 \u03b1H\ng p\u03b1Hq \u00b4 \u03b1H\n\u02d9\n1 pUi \u0105 1 \u00b4 \u03b1Hq\ngenerates the p\u03b1H , \u03b2Hq FPR/TPR pair. The larger the weight \u03bb\u03b8H \u201d \u03b2H\u00b4\u03b1H\ngp\u03b1Hq\u00b4\u03b1H placed on the machine ROC curve that is used to in the convex combination, the smaller the benefit, or the larger the loss, of replacing \u03b8H with a dominating pair \u03b8M on the machine ROC curve. The randomization scheme motivates the use of the weight \u03bb\u03b8H as a component of the loss function\n\u03c1 p\u03b8M , \u03b8Hq \u201c 1 \u00b4 1 p\u03b1H \u011b \u03b1M , \u03b2H \u010f \u03b2M q p1 \u00b4 max t\u03bb\u03b8H , 0uq . (14)\nOther randomization schemes can also be used. We will refer to the above construction as a vertical decomposition. Similarly, a horizontal decomposition can be used where the weights are replaced by \u03bb\u03b8H \u201d \u03b2H\u00b4\u03b1H \u03b2H\u00b4g\u00b41p\u03b2Hq . In addition, we also experimented with decomposing the distance between \u03b8M \u201c p\u03b1M , \u03b2M q on the machine ROC curve and the diagonal line into two parts. The first part is the distance from \u03b8M to the boundary of the set of FPR/TPR points that do not dominate \u03b8H . The second part is the distance from this boundary to the diagonal\nline. The corresponding loss function that mirrors (14) is then\n\u03c1 p\u03b8M , \u03b8Hq \u201c 1 \u00b4 1 p\u03b1H \u011b \u03b1M , \u03b2H \u010f \u03b2M q p1 \u00b4 max t\u03bb\u03b8H ,\u03b8M , 0uq . (15)\nWe measure distance both horizontally, implying that \u03bb\u03b8H ,\u03b8M \u201c \u03b2M\u00b4\u03b1H \u03b2M\u00b4\u03b1M , and vertically, implying that \u03bb\u03b8H ,\u03b8M \u201c \u03b2H\u00b4\u03b1M \u03b2M\u00b4\u03b1M .\nIn general, replacing \u03b8H by \u03b8M is considered beneficial when \u03b1M \u0103 \u03b1H and \u03b2M \u0105 \u03b2H . If we denote the benefit by b p\u03b8M , \u03b8Hq, then the magnitude of b p\u03b8M , \u03b8Hq is likely to be larger when \u03b8H is further below the machine ROC curve. In contrast, we expect a cost \u2113 p\u03b8M , \u03b8Hq when either \u03b1M \u0105 \u03b1H or \u03b2M \u0103 \u03b2H . These considerations suggest that a general loss function when \u03b8H consists of both a cost component and a benefit component, in the form of\n\u03c1 p\u03b8M , \u03b8Hq \u201c 1 p\u03b1H \u0103 \u03b1M or \u03b2H \u0105 \u03b2M q \u2113 p\u03b8M , \u03b8Hq \u00b4 1 p\u03b1H \u011b \u03b1M , \u03b2H \u010f \u03b2M q b p\u03b8M , \u03b2Hq . (16)\nFor each form of the loss function \u03c1 p\u03b8M , \u03b8Hq, a guardrail threshold is needed to account for the status quo of human decision making, i.e., a human decision maker is only replaced by the machine algorithm when the posterior expected loss of the replacement is below a given prespecified level, or when the posterior expected profit exceeds a certain level.\nIn addition to the pure strategy decision rules of relying exclusively on either an individual doctor or the machine algorithm, mixed strategy decision rules can be generated by randomizing between a given point on the machine ROC curve and an individual doctor. Let \u03c9 denote the randomizing probability of using the machine algorithm, and 1\u00b4\u03c9 the corresponding probability of relying on the human decision makers. The minimized expected posterior loss of the mixing strategy can then be written as\nmin \u03c9Pr0,1s,\u03b8M :\u03b2M\u201cgp\u03b1M q\n\u017c\n\u03c1 p\u03c9\u03b8M ` p1 \u00b4 \u03c9q \u03b8H , \u03b8Hq p p\u03b8H |Dq d\u03b8H . (17)\nIn practice, without expert domain knowledge of medical diagnoses, it is difficult to design (16) and implement (17). Investigating expert domain knowledge is beyond the scope of the current paper and is left for future research.\nIt is also worth pointing out that our framework differs conceptually from the minimization of posterior expected loss function in Bayesian point estimation. A Bayesian point estimator of \u03b8H , denoted as \u03b8\u0302H,B, is often defined as the minimizer of an expected posterior loss function:\n\u03b8\u0302H,B \u201c arg min \u03b8Pr0,1s2\n\u017c\n\u2113 p\u03b8 \u00b4 \u03b8Hq p p\u03b8H |Dq d\u03b8H ,\nwhere the estimation loss function \u2113 p\u03b8 \u00b4 \u03b8Hq typically depends symmetrically only on |\u03b8 \u00b4 \u03b8H | and is increasing in |\u03b8 \u00b4 \u03b8H |, which is different from our decision loss function \u03c1 p\u03b8M , \u03b8M q.\nFinally, our current paper focuses on choosing between machine algorithms and doctors as\nfinal decision makers. A broader scope experimental project may involve investigating whether the decision making process of doctors can be enhanced by providing doctors with information from the results of the machine algorithm analysis. This is beyond the scope of the current paper and will be left for future studies."
        },
        {
            "heading": "3 Data and Algorithm Description",
            "text": "The data of our work came from the National Free Prepregnancy Checkups (NFPC) project. Starting from 2010, this project offers free health checkup for couples planning to conceive and is conducted across 31 provinces in China. The dataset contains more than 300 features for each observation. These features include age, demographic characteristics, results from medical examinations and clinical tests, disease and medication history, pregnancy history, lifestyle and environmental information of both wives and husbands. The data also include the true birth outcome, which is denoted as normal (y \u201c 0) or abnormal (y \u201c 1). In addition, the dataset records doctors\u2019 IDs and diagnoses of pregnancy risk. The doctors\u2019 diagnoses are graded according to 4 levels: 0 for normal, 1 for high-risk wife, 2 for high-risk husband, and 3 for high-risk wife and husband. In this paper, we regroup the doctor\u2019s diagnoses into 2 levels, where a grade of 0 corresponds to normal pregnancy and any higher grade corresponds to a diagnosis of risky pregnancy.\nThe original dataset we accessed includes information about 3,330,304 couples that have pregnancy outcomes between January 1, 2014, and December 31, 2015. We excluded redundant features and the samples with missing information on doctors\u2019 diagnoses and those for which more than 50% of feature values are missing. Then we were left with 168 dimensional features of 1,137,010 couples who were diagnosed by 28,716 doctors for analysis. Of these observations, 61,184 couples (5.38%) had abnormal birth outcomes.\nThe basic statistical measurement of the quality of a binary classifier is accuracy, which is the proportion of correct predictions among the total number of cases examined. However, accuracy itself is inadequate for measuring prediction quality in many applications. As the adverse birth rate is about 5%, a naive classifier that categorizes all cases as low risk would achieve an accuracy of nearly 95%. This result is clearly controversial. The doctors\u2019 overall accuracy is 73.63%, and 24.04% of couples are diagnosed as having a high-risk pregnancy. The false positive rate (the rate of incorrectly misjudging a normal birth as a high-risk pregnancy) of aggregate doctors is 0.2379. The true positive rate (the rate of judgment of a high-risk pregnancy as a high-risk pregnancy) of the aggregate doctors is 0.2843. In contrast, the FPR and TPR of the naive predictor are both zero. In other words, doctors are willing to tolerate a higher FPR to achieve a higher TPR.\nTo improve the precision of statistical inference, we focus on doctors who diagnosed more than 300 patients, and we had 584 such doctors corresponding to 584,181 patient cases. We\nfirst randomly split our sample into two sets. The first set is the classification set. We use this set to train a selected machine learning algorithm, and then we classified a doctor as \u201cless capable\u201d or \u201ccapable (other)\u201d according to their performance relative to the algorithm. The second set is the \u201cperformance\u201d set. More specifically, we split the cases of each doctor into the classification set and the performance set, with a respective ratio of 7:3, by using stratified splitting methods. All cases in the data are grouped into the following four classes based on the actual birth outcomes and the doctors\u2019 diagnoses: true positive cases, true negative cases, false negative cases and false positive cases. The random splitting process then is performed through each class of data with the same ratio of 7:3. Subsequently, the data in each class are merged to form both the classification and performance sets. There are, in total, 408,661 cases in the classification set and 175,520 cases in the performance set.\nThe first objective in the classification exercise is to train an algorithm and evaluate its performance. We further split our classification set into two subsets: The first subset is the training set used to estimate the parameters in an machine learning algorithm. The second subset is the validation set that is used to evaluate the performance of the algorithm and obtain the machine ROC curve. We split each doctor\u2019s cases in the classification set into training and validation subsets with a ratio of 4:3 by using the same stratified splitting methods as described above. There are 233,435 cases in the training set and 175,226 cases in the validation set.\nTo check the robustness of our approaches we also conduct the experiments based on doctors who diagnosed more than 500 patients. We have 367 such doctors and 495,320 cases in total. For the classification set, we have 197,978 patient cases in the training sample and 148,579 patient cases in the validation sample. Altogether, 346,557 cases are used as historical records for finding less capable doctors. For the performance set, we have 148,763 cases for evaluating the performance of doctors and the algorithm.\nWe experimented with several algorithms. We began first with decision trees, which are widely used in many machine learning applications. However, a single tree method usually has high variance despite its low bias. The results from a single tree can vary substantially even when a small amount of noise is present in the data, and training the tree tends to overfit the data. As an alternative, the random forest (RF) is a commonly used ensemble learning algorithm proposed by Breiman (2001). RFs overcome these problems by constructing a collection of decision trees that are trained using different feature subspaces with bootstrapped samples. The predictions of each tree are aggregated to make predictions. Appendix A.2 provides a more detailed description of the random forest algorithm.4\n4We choose the following parameter settings. The number of estimators N is 100. The number of max features per node M is 50, and the minimum number of samples required to split is set to 50."
        },
        {
            "heading": "4 Empirical Results for AI and doctor decision making",
            "text": "In this section, we present the empirical results from two different approaches discussed in section 2, including the heuristic frequentist approach described in section 2.3 and the Bayesian approach described in section 2.4."
        },
        {
            "heading": "4.1 Results of the heuristic frequentist approach",
            "text": "We first implement the frequentist approach for identifying a dominating segment of the machine ROC, which is combined with random sampling of points on this dominating segment. For each doctor, we obtain 100 bootstrap samples from the set of patient cases that they classified, and obtain the 95% elliptical confidence area of \u03b8H using the bootstrapped asymptotic covariance matrix of FPR and TPR that is centered around the sample estimates. Then, we find the largest TPR point of the oval shape `\n\u03b1\u03b2high , \u03b2high \u02d8 and the smallest FPR point of the oval shape p\u03b1low, \u03b2lowq. Next, we obtain the point P \u201c p\u03b1low, \u03b2highq, as shown in Figure 4, as a reference point of this doctor. Then, we can classify doctors into two groups.\n1. Less capable doctors. The reference point p\u03b1low, \u03b2highq is below the machine ROC curve, thus corresponding to case 1 in section 2.3. These doctors are replaced in future decision making.\n2. Capable (other) doctors. The doctor\u2019s reference point p\u03b1low, \u03b2highq is above the machine ROC curve, thus corresponding to cases 2 or 3 in section 2.3. Either these doctors are more capable than the algorithm or these doctors\u2019 capability cannot be precisely measured, likely because there are fewer observations (thus resulting in a larger size confidence set area). These doctors are not replaced by the algorithm in future decision making.\nAfter searching for less capable doctors by applying the frequentist approach to the classification data set, we found that among the 584 doctors, 175 less capable doctors were replaced by the algorithm. Therefore, we have 175 machine doctors (30.0%) and 409 (70.0%) human doctors ready to diagnose patient cases in the performance set.\nFor each of the less capable doctors, we need to determine a threshold value c for the algorithm to make classification decisions. The threshold choice procedure is summarized in Appendix A.3. In this experiment, we pick N \u201c 100 points on the dominating segment of the machine ROC curve. Figure 6(a) shows the results of this experiment. The aggregate FPR/TPR pair of all doctors on the performance set is 0.2065 for FPR and 0.2264 for TPR, represented by the blue point in Figure 6(a). The ROC curve of the random forest model on the performance set is the green curve in this figure, and the area under the curve (AUC) of the model is 0.6851.\nThe yellow point corresponds to using the endpoint of the dominating interval of the machine ROC curve that has the highest FPR and the highest TPR. The yellow point has 0.2017 for\nFPR and 0.3326 for TPR. Compared to the case where the diagnoses are all performed by the doctors, replacing less capable doctors with the random forest algorithm improves the TPR by 46.9% and reduces the FPR by 2.3%. The green point corresponds to the other endpoint of the cyan interval, i.e., the point of lowest FPR and lowest TPR on the dominating interval on the machine ROC curve. It achieves a FPR of 0.1836 and a TPR of 0.2974. Compared to aggregate doctors\u2019 diagnoses, using the threshold corresponding to this point as the machine algorithm\u2019s decision threshold improves the TPR by 31.4% and the FPR by 11.1%.\nEssentially, the cyan interval in Figure 6(a) reflects a trade-off between improving the TPR and reducing the FPR, such that any point along the interval represents an enhancement over the aggregated doctors\u2019 FPR/TPR pair. Furthermore, the red point is the point that achieves the maximum F1 score5 along the cyan interval corresponding to the dominating segment of the machine ROC curve. It achieves an F1 score of 0.1309.\n[Figure 6 about here.]\nFor a robustness check, we conduct the experiment again based on the subsample of doctors who diagnosed more than 500 patients. Other settings of the algorithm implementation are kept the same. After classifying the performance of the doctors in the sample we find 135 less capable doctors among 367 doctors. In other words, 36.8% of doctors would be replaced by the algorithm and 63.2% of human doctors would be retained in the performance data set.\nFigure 6(b) shows the results of this experiment. Similar to the results shown in Figure 6(a), the aggregate FPR/TPR pair of doctors on the performance set (the blue point) is 0.1942 for the FPR and 0.2135 for the TPR. The AUC of the random forest model is 0.6892. The highest right (yellow) point on the cyan interval has an FPR of 0.1947 and a TPR of 0.3419. These results represent an improvement of 60.1% in the TPR for the experiment, and a corresponding slight worsening of 0.3% in the FPR. The small deterioration of the FPR is somewhat surprising, but is numerically possible because the FPR/TPR pairs are calculated out of sample using the performance data set. It may be due to isolated sample points of the doctors whose FPR/TPR pairs are close to the machine ROC curve. Figure 7 shows a scatter plot of the FPR/TPR pairs of both replaced and retained doctors against the ROC curve in the performance data set.\n[Figure 7 about here.]\nThe lower left (green) point of the cyan interval achieves 0.1723 for the FPR and 0.2966 for the TPR. Compared to all doctor diagnoses, the mixture of the algorithm\u2019s decision rule and human decisions corresponding to this point obtains an improvement of 38.9% in the TPR and a reduction of 11.3% in the FPR. The point that achieves the maximum F1 score\n5The F1 score is the harmonic average of precision and recall, and is often used as a single measurement that conveys the balance between precision and recall. It is widely used in evaluating the performance of algorithms (see for example Baeza-Yates et al. (1999)).\namong the cyan curves is the red point in Figure 6(b), which registered an F1 score of 0.1351. The overall performance of the combination of the algorithm with capable doctors does not differ substantially from the performance obtained in the previous experiments conducted with doctors diagnosing more than 300 patients, providing empirical evidence assuring the robustness of the experiment results."
        },
        {
            "heading": "4.2 Results of the Bayesian Approach",
            "text": "To implement the Bayesian approach, we first set a prior \u03c0ptq, which is a symmetry Dirichlet distribution with four hyper-parameters \u03b31, \u03b32, \u03b33, \u03b34 \u201c \u03b3, all of which are initialized to \u03b3 \u201c 0.1. For each doctor j, we denote the doctor\u2019s set of patient cases as !\nY\u0302j,i, Yj,i\n)\n, where Y\u0302j,i is the doctor\u2019s diagnosis and Yj,i is the ground-truth label of pregnancy outcome for doctor j\u2019s ith patient case. We group each doctor\u2019s cases into four multinomial classes according to the values of Y\u0302 and Y of each case and calculate the frequency of each class; i.e.,\n1. nt\u03021 equals the number of cases in which Y \u201c 1, Y\u0302 \u201c 1;\n2. nt\u03022 equals the number of cases in which Y \u201c 0, Y\u0302 \u201c 1;\n3. nt\u03023 equals the number of cases in which Y \u201c 1, Y\u0302 \u201c 0;\n4. nt\u03024 equals the number of cases in which Y \u201c 0, Y\u0302 \u201c 0.\nThe likelihood for data is given in (5). We combine the prior distribution and the data likelihood to analytically compute the posterior distribution, which is also a Dirichlet distribution with parameters p\u03b3\u03021, \u03b3\u03022, \u03b3\u03023, \u03b3\u03024q, where \u03b3\u0302k \u201c \u03b3 ` nt\u0302k \u201c 0.01 ` nt\u0302k, for k \u201c 1, 2, 3, 4. The posterior distribution of \u03b8H \u201c p\u03b1H , \u03b2Hq can then be simulated. First, R samples of ttr \u201c pt1r, t2r, t3r, t4rquRr\u201c1 are drawn from the posterior Dirichlet distribution with parameters p\u03b3\u03021, \u03b3\u03022, \u03b3\u03023, \u03b3\u03024q. Then, we compute t\u03b8H,r \u201c p\u03b1H ptrq , \u03b2H ptrqquRr\u201c1 with the simulated R samples of tr, r \u201c 1, . . . , R using (6).\nWe then apply the simulation method described in (10). If q\u0302max \u011b 0.95, we mark the doctor as a less capable doctor who will be replaced by the algorithm in future decision making, and the threshold c that corresponds to the point p\u03b1\u0302d, g p\u03b1\u0302dqq given by (10) on the machine ROC curve is used as the decision threshold for the algorithms. Designating a doctor as being \u201cless capable\u201d is to be understood in the Bayesian sense, i.e., that the posterior distribution of \u03b8H is dominated significantly by the machine ROC curve. Otherwise, the doctor will not be replaced by the algorithm. For example, Figure 8(a) shows a case where a doctor will be replaced by the algorithm. The maximum coverage rate q achieves 0.9657 at the black point on the machine ROC curve. However, in Figure 8(b), the maximum coverage rate achieves only 0.3179, suggesting that there is not sufficient evidence to replace the doctor\u2019s decisions with the algorithm\u2019s decisions.\n[Figure 8 about here.]\nFigure 9 (a) shows the experiment results for doctors who had diagnosed more than 300 cases. A total of 269 out of 584 doctors were classified as less capable doctors. Therefore, we have 315 (53.9%) human doctors and 269 (46.1%) machine doctors. By replacing less capable doctors\u2019 decisions with the algorithm\u2019s decisions, the combined decisions, indicated in the figure by the yellow point, result in an FPR of 0.1856 and a TPR of 0.3319. These results imply an increase of 46.6% in the TPR and a reduction of 10.1% in the FPR.\nTo evaluate the more general Bayesian posterior expected loss functions in (11), we experimented with several alternative loss function definitions described in equations (12) to (15). Their results are shown in Figure 11. Ranking the doctors by their expected posterior losses also allows us to replace only a fraction of the doctors whose expected loss of being substituted by the machine algorithm is relatively low. In Figure 11 we trace the dynamics of the aggregate FPR/TPR pairs of combining the doctor decisions with the machine decisions, when the replacement ratio increases from 0% of decisions entirely by human to 100% of decisions entirely by machine algorithms.\n[Figure 11 about here.]\nIn Figure 11 legends, Bayesian Test refers to the baseline construction in (8); Euclidean Distance to the ROC curve refers to the loss function in (12); Complementary Set Boundary Euclidean Distance refers to the loss function in (13); Diagonal Line Horizontal and Vertical Decompositions refer to the loss function in (14); Complement Set Boundary Vertical and Horizontal Decompositions refer to the loss function in (15). It can be seen that no individual loss function demonstrates saliently stronger discriminatory power over the entire replacement path. Furthermore, there appears to be little incremental benefit with going beyond a 50% replacement rate. The only visible change beyond the 50% replacement rate is a marginal\nreduction of FPR of modest magnitudes. In addition, the baseline 95% credible level Bayesian test implemented using (8) fairs very well in the FPR/TPR comparison and is close to the optimal tradeoff along the replacement paths.\nBoth the Bayesian analysis in this section and the frequentist analysis in the previous section are grounded in the premise of a null hypothesis of the status quo of human decision making, where a human decision maker is replaced by the machine algorithm only when there is strong evidence from the data favoring the machine algorithm. An alternative conceptual paradigm is to treat the machine algorithm as the status quo null hypothesis, so that human decision makers are by default replaced by the machine algorithm unless there is strong data evidence validating the superiority of human decision making.\nHowever, when we experiment with the alternative specification of the status quo, we find that all except a few doctors are determined to be replaceable by the machine algorithm. In the cross-point labeled as the \u201cdominate method\u201d in Figure 12, a doctor is only retained if there exists a point on the machine ROC curve that is dominated by at least 95% of the posterior distribution of p p\u03b8H |Dq. A mere 6 doctors are retained in this method. In the cross-point labeled as the \u201cabove method\u201d in Figure 12, a doctor is only retained if the posterior probability of p p\u03b8H |Dq above the machine ROC curve is more than 95%. Only 7 doctors are retained using this method. For doctors who are replaced by the machine algorithm, the replacement point on the ROC curve is determined by the baseline Bayesian tests in (10).\n[Figure 12 about here.]\nAcross all the loss functions that we considered, at very high replacement ratios, the FPR and TPR levels decrease simultaneously. It is worth noting that even when all the doctors are substituted by the machine algorithm, the resulting FPR/TPR pair still lies below the machine ROC curve. This is not surprising because different threshold cutoff values on the machine ROC curve are employed to replace different doctors and because of the concavity arguments formalized in Lemma A.1.\nUsing the current empirical data set, in which only a small fraction of doctors have FPR/TPR pairs above the machine ROC curve, we have not been able to use a statistical combination of doctors with the machine algorithm to generate a FPR/TPR pair above the machine ROC curve. In a synthetic data analysis, reported in section 5, we are able to combine the doctors and the machine algorithm to generate a FPR/TPR pair that outperforms even the machine ROC curve. This is accomplished by designing a data generating process that allows for a substantial portion of doctors with FPR/TPR pairs above machine ROC curve, or by an artificial dilution of the discriminatory power of the machine algorithm.\nInstead of replacing the less capable doctors deterministically by the machine algorithm, Appendix A.4 reports the results of a randomized replacement experiment where each less capable doctor accepts the machine decision probabilistically according to a pre-specified ac-\nceptance rate \u03bb P r0, 1s. As expected, the overall FPR/TPR pairs vary incrementally from when \u03bb \u201c 0, corresponding to human decision making, to when \u03bb \u201c 1, corresponding to deterministic replacement of the less capable doctors by the machine algorithm."
        },
        {
            "heading": "4.3 Characteristics of Replaced Doctors",
            "text": "The NFPC dataset records the urban/rural location of doctors in a dummy variable named \u201ctownship\u201d. If a doctor works in a rural clinic, their township dummy variable will be recorded as 1, and otherwise 0. Our paper provides a method to categorize doctors who have less adequate diagnostic capabilities than the machine algorithms have according to FPR/TPR indicators. As a byproduct of the identification procedure, we are also able to investigate whether the locations of the doctors have statistically significant associations with their likelihood of being replaced by the machine algorithm.\nWe create a dummy variable \u201cless capable\u201d that is set to 1 when a doctor is replaced by the algorithm and is set to 0 otherwise, and examine the partial correlation coefficient between the \u201cless capable\u201d dummy variable with the rural/urban location dummy variable in a regression analysis. In the regression we control for four confounding factors. The first factor is city-level GDP per capita (the data unit is ten thousand Chinese yuan per capita). The second factor is the number of city-level medical staff per capita (the data unit is number of medical staff per ten thousand people). The third factor is city-level birth rates. The GDP and the birth rate data are collected from the China City Statistical Yearbook of 2014, which coincides with the starting year of our data6. The medical staff data comes from each city\u2019s statistical yearbook of 2014. These data are all obtained from https://data.cnki.net/. The last factor is the logarithm of the number of patient cases of doctors.\nTable 2 reports the results of a logistic regression of the \u201cless capable\u201d dummy variable on the \"township\u201d dummy variable and the confounding factors that are being controlled for, using the classification from the baseline Bayesian test approach for all doctors who diagnosed more than 300 patient cases. Doctors with missing data are excluded from the regression7. Relative to doctors from urban clinics, doctors from rural clinics have a saliently higher (by 16.2%) probability of being replaced by the algorithm. In particular, in the context of the baseline Bayesian test results, 53.3% of 319 doctors from rural clinics are replaced by the algorithm, while only 37.1% of 244 urban clinic doctors are replaced. In the logistic regressions, the estimated coefficient of the township dummy is statistically significant and positive, see Table 2.\n[Table 2 about here.] 6It should be noted that the statistical yearbook of year A records the data of year A \u00b4 1 or the latest available data before year A \u00b4 1. 7In order to correct for spatial autocorrelation that may occur in the regressions, we use the cluster (by city where the doctors are located) robust standard deviation.\nThe city-level GDP per capita variable has surprising positive regression coefficients, which appears to suggest that doctors from higher GDP areas are more likely to be replaced by the machine algorithm. However, the GDP variable is less statistically significant compared to the township dummy variable. When we replace municipal GDP per capita by provincial GDP per capita, the regression coefficient becomes negative but is still statistically insignificant. For brevity we do not include these additional regressions. The sign of the regression coefficient of birth rate is as expected, although it is not statistically significant. The lower the birth rate, the less likely it is that the doctor will be replaced. A plausible conjecture is that lower birth rates are positively correlated with how parents value the health of newborns and with stronger parental incentive to obtain better prenatal medical care service. The negative coefficients of the medical staff per capita are not anticipated either. A possible explanation is that the prepregnancy diagnostic capability of a region is more associated with the quality instead of the quantity of the medical staff. The coefficients of the log number of patients are positive and statistically significant. We also note that on average, each doctor in rural areas diagnoses more patient cases than doctors in urban areas do8. This finding is consistent with our assumption that doctors do not accumulate experience from additional patient cases. Instead, doctors might be overburdened by a higher workload of a larger number of patient cases, resulting in lower average service quality. Besides, the positive coefficient of the log number of patients can also be attributed to the higher distinguishing power of the Bayesian test when a doctor sees more patients. We expect that the FPR/TPR pairs of doctors with more patient cases can be estimated more precisely. As a consequence, it is more likely for a doctor with a FPR/TPR pair below the machine ROC curve to be identified as replaceable by our statistical procedure. The estimated FPR/TPR pairs of the majority of doctors in our sample lie below the machine ROC curve.\nOverall, these regression results indicate that artificial intelligence technology assisted decision making has a tendency to improve precision in rural areas more than in developed cities. The statistical association revealed by the empirical data analysis has an intuitive appeal: doctors in economically disadvantaged areas are likely to have received less advanced training in sophisticated medical skills, and to be equipped with insufficient frontier knowledge in making quality diagnostic decisions. An important implication of our empirical analysis is the potential of employing artificial intelligence technology in designing poverty-related economic policies to improve the welfare of impoverished areas. These empirical findings are also reinforced by the observation that the coefficients on the township dummy and their statistical significance are stable across different columns of Table 2 and robust against the inclusion of additional confounders. Our empirical results provide yet another example to strengthen the conjecture that AI might help poor areas more, adding to a variety of conferences and papers studying\n8For urban doctors, the mean number is 840.40 and the standard deviation is 707.92; for rural doctors, the mean is 1144.41 and the standard deviation is 1048.79.\nAI for poorer areas that show similar results. See for example https://www.md4sg.com and https://sites.google.com/view/ml4d9. Ultimately, a broad AI-based system needs to be deployed and evaluated in order to validate these scientific conjectures and speculations."
        },
        {
            "heading": "5 Synthetic Data Analysis",
            "text": "In our empirical data set, only a small fraction of doctors lie above the machine ROC curve. The statistical combination of the machine algorithm and the doctor decisions results in aggregate FPR/TPR pairs that are still below the machine ROC curve. The implication of the Jensen\u2019s inequality discussed in sections 1 and 2 is a compounding factor that likely also contributes to the combined aggregate FPR/TPR pair lying below the machine ROC curve. In particular, even if we replace every doctor by the machine algorithm, unless the replacement threshold is identical for all doctors, i.e., unless the same point on the machine ROC curve is used to replace all doctors, the combined aggregate FPR/TPR pair will still necessarily be below the machine ROC curve because of the Jensen\u2019s inequality arguments in sections 1 and 2. The end points of the replacement paths in Figure 11 correspond to replacing all doctors by the machine algorithm. As can be seen in these diverging endpoints, the algorithms that we implemented typically result in using different points on the machine ROC curve to replace different doctors. In order for the combined aggregate FPR/TPR pair to be above the machine ROC curve, it is necessary that a sizable portion of the original doctor diagnoses lie above the machine ROC curve.\nAside from the context of the current empirical data set, it is conceivable that other data sets might exist where a significant portion of human decision makers correctly use private information not available to the machine algorithm. Figure 13 reports the results from a synthetic data analysis where the combined aggregate FPR/TPR pair between doctors and the machine algorithm lies above the machine ROC curve.\n[Figure 13 about here.]\nThe data generating process for producing the synthetic simulation in Figure 13 is as follows. The input feature data is three dimensional: xi1, xi2 and ui. On the one hand, the first two features, denoted xi1 and xi2, are assumed to be observable both by the doctors and the machine algorithm. On the other hand, the last feature, denoted u, is only observable to the doctors and is not used by the machine algorithm. The three features are generated from independent normal distributions with pre-specified means and variances: Xi1 \u201e N p0, 1.95q,\n9We are grateful to both anonymous referees for referring us to the literature and for interpreting the empirical findings.\nXi2 \u201e N p0, 0.25q, and Ui \u201e N p0, 2q. The ground truth variable Y P t0, 1u is generated by\nYi \u201c 1 pp pXi1, Xi2, Uiq \u0105 \u03b4iq where p pXi1, Xi2, Uiq \u201c eXi1`Xi2`Ui\n1 ` eXi1`Xi2`Ui ,\nwhere \u03b4i \u201e U p0, 1q is uniformly distributed between 0 and 1. Using this specification of the data generating process, we simulated 600,000 patient cases and allocate them evenly to 2000 doctors. Each doctor is assigned 300 simulated patient cases.\nDoctors are partitioned into two groups according to their information processing capacities. The first group, consisting of 750 more capable doctors, use the correct full information propensity score p pxi1, xi2, uiq with different incentives to diagnose patient cases. Their decision rule is given by, using Ci \u201e U p0.4, 1q that is uniformly distributed between 0.4 and 1:\nY\u0302i \u201c 1 pp pXi1, Xi2, Uiq \u0105 Ciq .\nThe second group of doctors, consisting of 1250 less capable doctors, use a misspecified full information propensity score to diagnose patients. For Ci \u201e U p0.4, 1q, their decision rule is\nY\u0302i \u201c 1 pq pXi1, Xi2, Uiq \u0105 Ciq , where q pXi1, Xi2, Uiq \u201c e\u00b4Xi1`Xi2`Ui\n1 ` e\u00b4Xi1`Xi2`Ui\nis a misspecified full information propensity score function. After the synthetic data is generated, we divide the 600,000 observation data set into a training subset that consists of 240,000 observations, a validation subset consisting of 180,000 observations, and a test subset consisting of another 180,000 observations. We then estimate a random forest model using only the training subset of the data and only the first two features x1i and x2i, and use the baseline Bayesian test method in equation (10) to combine doctors with the machine algorithm. The red point in Figure 13 represents the combination of doctors and the machine algorithm. It lies above the ROC curve of the random forest model, and also strictly dominates the blue point of the aggregate FPR/TPR pair of all doctors. In summary, the synthetic data analysis represented by Figure 13 illustrates the recent works on the scientific principle of complementarity in human algorithm collaborations. See for example Bansal et al. (2021).10\nOur data set is unique in that the ground-truth of the eventual pregnancy outcomes are observed and can be used as labels to train machine learning algorithms. Ground-truth label data can be difficult to come by in many empirical applications. Instead, doctor diagnose information is likely to be more readily available. In the remaining of this section we discuss whether estimating a model that predicts the doctors\u2019 decisions, e.g. applying a machine learning algorithm to the label of doctors\u2019 diagnoses, can assist in improving the FPR/TPR\n10We are grateful to an anonymous referee for pointing us to the science literature on human-algorithm collaboration complementary.\ntradeoff of the diagnoses. A model of predicted doctors can be intuitively interpreted as doctors\u2019 collective wisdom. The collective wisdom of more experienced doctors can also provide guidance to newly practicing doctors with limited clinical exposures.\nIn our particular data set, a model of predicted doctors does not appear to improve on the aggregate FPR/TPR pair. However, in the following we present several synthetic data examples where a predicted doctor model can provide additional information beyond the diagnose of an individual doctor. To begin with, we first note that as discussed in the introduction, the doctors\u2019 diagnoses may encode information heterogeneity, or incentive heterogeneity, or both. For example, consider a model of incentive heterogeneity, where doctors\u2019 decision rules are given by y\u0302i \u201c 1 pp pxiq \u0105 uiq such that p pxiq is the correctly specified propensity score, and ui is uniformly distributed between 0 and 1. The predicted doctor model, which is given by E r1 pp pXiq \u0105 Uiq |Xis \u201c P pUi \u010f p pXiqq \u201c p pXiq, recovers the ground-truth machine ROC curve, even though the aggregate doctor FPR/TPR lies below the ROC curve due to Jensen\u2019s inequality. It is worth noting that in order for the ROC curve of the predicted doctor model to coincide with the ground-truth machine ROC curve, it suffices for the propensity score of the predicted doctor model to be a monotonic transformation of the propensity score of the ground-truth machine learning model.\nWe next focus on several scenarios of information heterogeneity, where we rule out incentive heterogeneity as in Assumption 1. Consider a baseline example where doctors have access to private information ui in addition to the publicly observed features xi and are aware of the correct full information propensity score model p pxi, uiq. If we observe the prediction of the model p pxi, uiq employed by the doctors and use it to train a machine learning model, then the resulting predicted doctor model will necessarily coincide with the ground-truth machine learned propensity score model:\nE rp pXi, Uiq |Xis \u201c E rE rYi|Xi, Uis |Xis \u201c E rYi|Xis \u201c p pXiq .\nHowever, in reality we rarely solicit doctors\u2019 probabilistic assessment of the likelihood of an illness. Instead, we only observe the binary diagnosis decisions by the doctors, which corresponds to observing y\u0302i \u201c 1 pp pxi, uiq \u0105 c0q for some threshold value c0. When we use y\u0302i as the label to train a machine learning model, the propensity score of the resulting predicted doctor model is\nq pXiq \u201c E r1 pp pXi, Uiq \u0105 c0q |Xis .\nIn general, q pXiq is different from p pXiq. The ROC curve implied by q pXiq coincides with p pXiq only when q pXiq is an increasing transformation of p pXiq: q pXiq \u201c \u039b pp pXiqq, where \u039b p\u00a8q is an increasing function of a scalar variable. There is no guarantee that this is necessarily the case, especially when Xi and Ui can be multidimensional.\nConsider now three scenarios in which doctors employ misspecified models to process pri-\nvate information. In the first scenario, doctors process the full information pXi, Uiq using an incorrect propensity score model, but the ROC curve implied by the predicted model still coincides with the ground-truth machine ROC curve. Figure 14 illustrates this scenario using the following data generating process. The ground-truth propensity score model is p pXiq \u201c exp pXiq { p1 ` exp pXiqq, but the doctors use a misspecified propensity model of q pXi, Uiq \u201c exp pXi ` Uiq { p1 ` exp pXi ` Uiqq, where Xi \u201e U p\u00b41, 1q and Ui \u201e U p\u00b42, 2q. The \u201cXi only\u201d ROC curve is generated by p pXiq; the \u201cXi and Ui\u201d ROC curve is generated by q pXi, Uiq, whereas the \u201cPredicted Doctors on X\u201d ROC curve is generated by\nq pXiq \u201d E r1 pq pXi, Uiq \u0105 c0q |Xis ,\nwhere we now change the notation from p pXi, Uiq to q pXi, Uiq to emphasize that doctors might use a mis-specified information model. The same legends apply to the following scenarios.\n[Figure 14 about here.]\nFor any homogeneous cutoff threshold c0 used by the doctors to make diagnosing decisions, the propensity score of the predicted doctor model is an increasing function of Xi, as is p pXiq. Therefore the predicted doctor ROC curve and the ground-truth machine ROC curve coincide, and both lie above the ROC curve of the misspecified private information ROC curve corresponding to doctors\u2019 decisions and also above the aggregate doctor FPR/TPR pair (c0 \u201c 0.7). Collective wisdom improves on the diagnosing quality of the misspecified model employed by individual doctors as represented by the ROC curves. It is important to emphasize, nevertheless, that the aggregate FPR/TPR pair is only dominated by a segment of the ROC curves.\nThe second scenario, pictured in Figure 15, is generated by a two dimensional observable feature model. The ground-truth propensity score model learned by the machine algorithm is p pXi1, Xi2q \u201c exp pXi1 ` Xi2q { p1 ` exp pXi1 ` Xi2qq, where Xi1 and Xi2 are independently distributed standard normal random variables. The doctors\u2019 (misspecified) information model is q pXi1, Xi2, Uiq \u201c exp pXi1 \u00b4 Xi2 ` Uiq { p1 ` exp pXi1 \u00b4 Xi2 ` Uiqq where Xi1 \u201e N p0, 1q, Xi2 \u201e N p0, 0.5q, and Ui \u201e N p0, 4q.\n[Figure 15 about here.]\nIn this scenario, doctors also process the full information Xi, Ui using an incorrect propensity score model. While the predicted doctor ROC curve lies below the ground-truth machine ROC curve, it still lies above the ROC curve corresponding to the individual doctor misspecified information model, and has a dominating segment over the aggregate doctor FPR/TPR pair. Collective wisdom also improves on the diagnosing quality of misspecified information models.\nFinally, the third scenario is pictured in Figure 16. In this figure, the ground-truth propensity score model learned by the machine algorithm is p pXiq \u201c exp pXiq { p1 ` exp pXiqq but the\ndoctors\u2019 incorrect information model is\nq pXi, Uiq \u201c exp p\u00b4Xi ` Uiq { p1 ` exp p\u00b4Xi ` Uiqq ,\nwhere Xi and Ui are distributed as in the first scenario. In this model, where doctors\u2019 information processing capacity is very limited, the predicted doctor model exacerbates the bias in the misspecified private information propensity score model, and leads to a ROC curve that lies strictly below even the ROC curve of the individual doctor misspecified private information propensity score model.\n[Figure 16 about here.]\nThe analysis in Figures 14, 15, 16 shows that depending on the extent of misspecification of the doctors\u2019 information model, the predicted doctor model that aggregates the heterogeneous information among doctors using the subset of observable features available to the machine learning algorithm might either reduce or exacerbate the bias in the propensity score model."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this paper, we propose principles for utilizing artificial intelligence to improve human decision making. By making the assumption that the preferences of individual decision makers are stable, we can identify and replace less capable decision makers. Specifically, decision rules that robustly dominate decisions made by less capable humans can be determined based on the machine ROC curves. By replacing these less capable decision makers with algorithms, we can achieve higher quality decision making outcome. Experiments on a dataset of prepregnancy checkups demonstrate that the replacement of less capable doctors significantly improves the overall performance of risky pregnancy detection. Furthermore, our results indicate that less capable doctors are more frequently found in rural clinics than in urban ones. This finding suggests that artificial intelligence may have greater potential in underdeveloped regions."
        },
        {
            "heading": "Acknowledgments",
            "text": "We thank the editors and two anonymous referees, Xiaohong Chen, Ron Gallant, Peter Hansen, Adam Rosen, Andres Santos, George Tauchen, Valentin Verdier, and participants at various seminars and conferences for insightful comments. We also thank Xin Lin and Yichuan Zhang for excellent research assistance. This study was approved by the National Health Commission. Informed consents were obtained from all the NFPC participants. We acknowledge funding support from the National Science Foundation (SES 1658950 to Han Hong), the National Natural Science Foundation of China (72192802 to Ke Tang), Guoqiang Institute, Tsinghua\nUniversity (2020GQG1018 to Ke Tang), and the National Natural Science Foundation of China (72222022, 72171013, 72242101 to Jingyuan Wang)."
        },
        {
            "heading": "Bansal, Gagan, Tongshuang Wu, Joyce Zhou, Raymond Fok, Besmira Nushi, Ece",
            "text": "Kamar, Marco Tulio Ribeiro, and Daniel Weld, \u201cDoes the whole exceed its parts? the effect of ai explanations on complementary team performance,\u201d in \u201cProceedings of the 2021 CHI Conference on Human Factors in Computing Systems\u201d 2021, pp. 1\u201316.\nBerk, Richard, \u201cAn impact assessment of machine learning risk forecasts on parole board decisions and recidivism,\u201d Journal of Experimental Criminology, 2017, 13 (2), 193\u2013216.\nBreiman, Leo, \u201cRandom forests,\u201d Machine learning, 2001, 45 (1), 5\u201332."
        },
        {
            "heading": "Brennan, Meghan, Sahil Puri, Tezcan Ozrazgat-Baslanti, Zheng Feng, Matthew",
            "text": "Ruppert, Haleh Hashemighouchani, Petar Momcilovic, Xiaolin Li, Daisy Zhe Wang, and Azra Bihorac, \u201cComparing clinical judgment with the MySurgeryRisk algorithm for preoperative risk assessment: a pilot usability study,\u201d Surgery, 2019, 165 (5), 1035\u20131045.\nCastro, Victor M, Dmitriy Dligach, Sean Finan, Sheng Yu, Anil Can, Muhammad Abd-El-Barr, Vivian Gainer, Nancy A Shadick, Shawn Murphy, Tianxi Cai et al., \u201cLarge-scale identification of patients with cerebral aneurysms using natural language processing,\u201d Neurology, 2017, 88 (2), 164\u2013168.\nCurrie, Janet and WB MacLeod, \u201cDiagnosing expertise: Human capital, decision making, and performance among physicians,\u201d Journal of labor economics, 2017, 35 (1), 1\u201343.\nElliott, Graham and Robert P Lieli, \u201cPredicting binary outcomes,\u201d Journal of Econometrics, 2013, 174 (1), 15\u201326.\nErickson, Bradley J, Panagiotis Korfiatis, Zeynettin Akkus, and Timothy L Kline, \u201cMachine learning for medical imaging,\u201d Radiographics, 2017, 37 (2), 505\u2013515."
        },
        {
            "heading": "Esteva, Andre, Brett Kuprel, Roberto A Novoa, Justin Ko, Susan M Swetter,",
            "text": "Helen M Blau, and Sebastian Thrun, \u201cDermatologist-level classification of skin cancer with deep neural networks,\u201d Nature, 2017, 542 (7639), 115."
        },
        {
            "heading": "Fauw, Jeffrey De, Joseph R Ledsam, Bernardino Romera-Paredes, Stanislav",
            "text": "Nikolov, Nenad Tomasev, Sam Blackwell, Harry Askham, Xavier Glorot, Brendan O\u2019Donoghue, Daniel Visentin et al., \u201cClinically applicable deep learning for diagnosis and referral in retinal disease,\u201d Nature medicine, 2018, 24 (9), 1342\u20131350.\nFeng, Kai, Han Hong, and Ke Tang, \u201cStatistical Inference of Optimal Allocations,\u201d Working paper, 2023."
        },
        {
            "heading": "Fuster, Andreas, Paul Goldsmith-Pinkham, Tarun Ramadorai, and Ansgar",
            "text": "Walther, \u201cPredictably unequal? The effects of machine learning on credit markets,\u201d The Journal of Finance, 2022, 77 (1), 5\u201347.\nGrove, William M, David H Zald, Boyd S Lebow, Beth E Snitz, and Chad Nelson, \u201cClinical versus mechanical prediction: a meta-analysis.,\u201d Psychological assessment, 2000, 12 (1), 19.\nGruber, Jon, John Kim, and Dina Mayzlin, \u201cPhysician fees and procedure intensity: the case of cesarean delivery,\u201d Journal of health economics, 1999, 18 (4), 473\u2013490.\nGruber, Jonathan and Maria Owings, \u201cPhysician financial incentives and cesarean section delivery,\u201d The Rand Journal of Economics, 1996, 27 (1), 99.\nGulshan, Varun, Lily Peng, Marc Coram, Martin C Stumpe, Derek Wu, Arunachalam Narayanaswamy, Subhashini Venugopalan, Kasumi Widner, Tom Madams, Jorge Cuadros et al., \u201cDevelopment and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs,\u201d Jama, 2016, 316 (22), 2402\u20132410.\nGuo, Jonathan and Bin Li, \u201cThe application of medical artificial intelligence technology in rural areas of developing countries,\u201d Health equity, 2018, 2 (1), 174\u2013181."
        },
        {
            "heading": "Huang, Shih-Cheng, Anuj Pareek, Saeed Seyyedi, Imon Banerjee, and Matthew P",
            "text": "Lungren, \u201cFusion of medical imaging and electronic health records using deep learning: a systematic review and implementation guidelines,\u201d NPJ digital medicine, 2020, 3 (1), 136.\nJohnson, Erin M and M Marit Rehavi, \u201cPhysicians treating physicians: information and incentives in childbirth,\u201d American Economic Journal: Economic Policy, 2016, 8 (1), 115\u2013 141.\nKahneman, Daniel and Gary Klein, \u201cConditions for intuitive expertise: a failure to disagree.,\u201d American psychologist, 2009, 64 (6), 515."
        },
        {
            "heading": "Kermany, Daniel S, Michael Goldbaum, Wenjia Cai, Carolina CS Valentim, Huiying Liang, Sally L Baxter, Alex McKeown, Ge Yang, Xiaokang Wu, Fangbing",
            "text": "Yan et al., \u201cIdentifying Medical Diagnoses and Treatable Diseases by Image-Based Deep Learning,\u201d Cell, 2018, 172 (5), 1122\u20131131.\nKitagawa, Toru and Aleksey Tetenov, \u201cWho should be treated? empirical welfare maximization methods for treatment choice,\u201d Econometrica, 2018, 86 (2), 591\u2013616.\nKleinberg, Jon, Himabindu Lakkaraju, Jure Leskovec, Jens Ludwig, and Sendhil Mullainathan, \u201cHuman decisions and machine predictions,\u201d The Quarterly Journal of Economics, 2018, 133 (1), 237\u2013293."
        },
        {
            "heading": "Liang, Huiying, Brian Y. Tsui, Hao Ni, Carolina C. S. Valentim, Sally L. Baxter,",
            "text": "Guangjian Liu, Wenjia Cai, Daniel S. Kermany, Xin Sun, Jiancong Chen et al., \u201cEvaluation and accurate diagnoses of pediatric diseases using artificial intelligence,\u201d Nature Medicine, 2019, 25 (3), 433\u2013438."
        },
        {
            "heading": "Litjens, Geert, Thijs Kooi, Babak Ehteshami Bejnordi, Arnaud Arindra Adiyoso",
            "text": "Setio, Francesco Ciompi, Mohsen Ghafoorian, Jeroen Awm Van Der Laak, Bram Van Ginneken, and Clara I S\u00e1nchez, \u201cA survey on deep learning in medical image analysis,\u201d Medical image analysis, 2017, 42, 60\u201388."
        },
        {
            "heading": "Long, Erping, Haotian Lin, Zhenzhen Liu, Xiaohang Wu, Liming Wang, Jiewei",
            "text": "Jiang, Yingying An, Zhuoling Lin, Xiaoyan Li, Jingjing Chen et al., \u201cAn artificial intelligence platform for the multihospital collaborative management of congenital cataracts,\u201d Nature biomedical engineering, 2017, 1 (2), 0024.\nManski, Charles F, \u201cCredible ecological inference for medical decisions with personalized risk assessment,\u201d Quantitative Economics, 2018, 9 (2), 541\u2013569.\nMeehl, Paul E, \u201cClinical versus statistical prediction: A theoretical analysis and a review of the evidence.,\u201d 1954.\nMerenstein, Daniel, \u201cWinners and losers,\u201d Jama, 2004, 291 (1), 15\u201316.\nMondal, Himel, Shaikat Mondal, and Rajeev K Singla, \u201cArtificial Intelligence in Rural Health in Developing Countries,\u201d in \u201cArtificial Intelligence in Medical Virology,\u201d Springer, 2023, pp. 37\u201348."
        },
        {
            "heading": "Peng, Sui, Yihao Liu, Weiming Lv, Longzhong Liu, Qian Zhou, Hong Yang, Jie",
            "text": "Ren, Guangjian Liu, Xiaodong Wang, Xuehua Zhang et al., \u201cDeep learning-based artificial intelligence model to assist thyroid nodule diagnosis and management: a multicentre diagnostic study,\u201d The Lancet Digital Health, 2021, 3 (4), e250\u2013e259."
        },
        {
            "heading": "Raghu, Maithra, Katy Blumer, Rory Sayres, Ziad Obermeyer, Bobby Kleinberg,",
            "text": "Sendhil Mullainathan, and Jon Kleinberg, \u201cDirect uncertainty prediction for medical second opinions,\u201d in \u201cInternational Conference on Machine Learning\u201d PMLR 2019, pp. 5281\u2013 5290."
        },
        {
            "heading": "Rajpurkar, Pranav, Awni Y Hannun, Masoumeh Haghpanahi, Codie Bourn, and",
            "text": "Andrew Y Ng, \u201cCardiologist-level arrhythmia detection with convolutional neural networks,\u201d arXiv preprint arXiv:1707.01836, 2017."
        },
        {
            "heading": "Razzaki, Salman, Adam Baker, Yura Perov, Katherine Middleton, Janie Baxter,",
            "text": "Daniel Mullarkey, Davinder Sangar, Michael Taliercio, Mobasher Butt, Azeem Majeed, Arnold DoRosario, Megan Mahoney, and Saurabh Johri, \u201cA comparative study of artificial intelligence and human doctors for the purpose of triage and diagnosis,\u201d arXiv:1806.10698 [cs, stat], 2018. arXiv: 1806.10698.\nStrasser, Roger, Sophia M Kam, and Sophie M Regalado, \u201cRural health care access and policy in developing countries,\u201d Annual review of public health, 2016, 37, 395\u2013412."
        },
        {
            "heading": "Studdert, David M, Michelle M Mello, William M Sage, Catherine M DesRoches,",
            "text": "Jordon Peugh, Kinga Zapert, and Troyen A Brennan, \u201cDefensive medicine among high-risk specialist physicians in a volatile malpractice environment,\u201d Jama, 2005, 293 (21), 2609\u20132617.\nWang, Jingyuan, Edoardo Gallo, Wei Zhang, Ke Tang, and Han Hong, \u201cDiagnosing with the help of artificial intelligence,\u201d 2023. Working paper."
        },
        {
            "heading": "Wilson, Nathan W, Ian D Couper, Elma De Vries, Steve Reid, Therese Fish, and",
            "text": "Ben J Marais, \u201cA critical review of interventions to redress the inequitable distribution of healthcare professionals to rural and remote areas,\u201d Rural and remote health, 2009, 9 (2), 1\u201321."
        },
        {
            "heading": "Yang, Jiehua, Mingfei Xie, Canpei Hu, Osamah Alwalid, Yongchao Xu, Jia Liu,",
            "text": "Teng Jin, Changde Li, Dandan Tu, Xiaowu Liu et al., \u201cDeep learning for detecting cerebral aneurysms with CT angiography,\u201d Radiology, 2021, 298 (1), 155\u2013163.\nZeng, Jiaming, Berk Ustun, and Cynthia Rudin, \u201cInterpretable classification models for recidivism prediction,\u201d Journal of the Royal Statistical Society. Series A (Statistics in Society), 2017, pp. 689\u2013722.\nZhu, Jiming, Wenkai Li, and Lincoln Chen, \u201cDoctors in China: improving quality through modernisation of residency education,\u201d The Lancet, 2016, 388 (10054), 1922\u20131929.\nT ab\nle 2:\nR ep\nla ce\nd D\noc to\nrs an\nd G\neo gr\nap hi\nc C\nha ra\nct er\nis ti\ncs\n(1 )\n(2 )\n(3 )\n(4 )\n(5 )\n(6 )\n(7 )\n(8 )\n(9 )\nG D\nP pe\nr ca\npi ta\n0. 00\n87 2\n0. 02\n49 0.\n02 42\n0. 02\n57 0.\n00 62 2 (0 .5 9) (1 .4 5) (1 .4 1) (1 .6 9) (0 .4 2)\nM ed\nic al\nst aff\npe r\nca pi\nta -0\n.0 07\n04 -0\n.0 07\n07 -0\n.0 05\n98 -0\n.0 06 40 (- 2. 19 ) (- 2. 37 ) (- 2. 37 ) (- 2. 03 ) B ir th ra te 37 .9 17 31 .6 79\n31 .2 15 (1 .4 7) (1 .1 1) (1 .2 1)\nLo g\npa ti\nen t\nnu m\nbe r\n0. 76\n7 0.\n84 1\n(5 .4\n3) (6\n.0 1)\nT ow\nns hi\np in\ndi ca\nto r\n0. 66\n2 0.\n66 6\n0. 63\n6 0.\n64 6\n0. 48 3 (4 .1 2) (4 .1 3) (3 .8 6) (3 .8 7) (2 .8 6)\nC on\nst an\nt -0\n.5 28\n-0 .5\n70 -0\n.2 30\n-0 .7\n31 -5\n.7 22\n-0 .1\n73 0.\n21 7\n-0 .5\n58 -5\n.7 36\n(- 4.\n52 )\n(- 4.\n03 )\n(- 1.\n10 )\n(- 1.\n83 )\n(- 5.\n44 )\n(- 1.\n61 )\n(1 .1\n9) (-\n1. 57\n) (-\n6. 25\n)\nO bs\ner va\nti on\ns 49\n8 49\n8 49\n8 49\n8 49\n8 49\n8 49\n8 49\n8 49 8 z st at is ti cs in pa re nt he se s\n0 .0\n0 .2\n0 .4\n0 .6\n0 .8\n1 .0\nF a ls\ne P\nos it\niv e\nR at\ne\n0. 0 0. 2 0. 4 0. 6 0. 8 1. 0\nTruePositiveRate\nR O\nC of\nth e\nR F\nM o d\nel ,\nA U\nC =\n0. 68\n51\nA ll\nd o ct\no rs\n, F\nP R\n= 0.\n20 65\n, T\nP R\n= 0.\n2 26\n4\nR F\nM o d\nel w\nit h\nF P\nR R\nan ge\nR F\nM o d\nel w\nit h\nm ax\nF 1-\nsc or\ne 0 .1\n30 9,\nF P\nR =\n0. 20\n09 ,\nT P\nR =\n0. 33\n18\nR F\nM o d\nel ,\nF P\nR =\n0 .2\n01 7,\nT P\nR =\n0. 33\n26\nR F\nM o d\nel ,\nF P\nR =\n0 .1\n83 6,\nT P\nR =\n0. 29\n74\n0. 18\n0. 19\n0 .2\n0 0.\n2 1\n0. 22\n0 .2\n3\nF a ls\ne P\nos it\niv e\nR at\ne\n0 .2\n2\n0 .2\n4\n0 .2\n6\n0 .2\n8\n0 .3\n0\n0 .3\n2\n0 .3\n4\nTruePositiveRate\n(a )\nP er\nfo rm\nan ce\nof R\nan do\nm Fo\nre st\n(R F )\nM od\nel an\nd D\noc to\nrs (D\noc to\nr\u2019 s\nD ia\ngn os\nes >\n= 30\n0)\n0. 0\n0. 2\n0 .4\n0. 6\n0. 8\n1 .0\nF a ls\ne P\no si\nti ve\nR at\ne\n0. 0 0. 2 0. 4 0. 6 0. 8 1. 0\nTruePositiveRate\nR O\nC o f\nth e\nR F\nM o d\nel ,\nA U\nC =\n0. 68\n9 2\nA ll\nd o ct\no rs\n, F\nP R\n= 0.\n19 4 2,\nT P\nR =\n0 .2\n13 5\nR F\nM o d\nel w\nit h\nF P\nR R\nan g e\nR F\nM o d\nel w\nit h\nm a x\nF 1-\nsc o re\n0 .1\n3 51 , F P R = 0 .1 94 7, T P R = 0. 34 19 R F M o d el , F P R = 0 .1 7 23 , T P R = 0 .2\n9 6 6\n0. 17\n0. 18\n0. 1 9\n0. 2 0\n0. 21\n0 .2\n2\nF a ls\ne P\no si\nti ve\nR at\ne\n0 .2\n0\n0 .2\n2\n0 .2\n4\n0 .2\n6\n0 .2\n8\n0 .3\n0\n0 .3\n2\n0 .3\n4\nTruePositiveRate\n(b )\nP er\nfo rm\nan ce\nof th\ne R\nan do\nm Fo\nre st\n(R F )\nM od\nel an\nd D\noc to\nrs (D\noc -\nto rs\n\u2019D ia\ngn os\nes >\n= 50\n0)\nF ig\nur e\n6: E\nm pi\nri ca\nlR es\nul ts\nof C\nom bi\nni ng\nD oc\nto rs\n\u2019a nd\nM ac\nhi ne\nD ec\nis io\nns :\nth e\nhe ur\nis ti\nc fr\neq ue\nnt is\nt ap\npr oa\nch\n0 .0\n0 .2\n0. 4\n0. 6\n0 .8\n1 .0\nF al\nse P\nos it\niv e\nR a te\n0. 0 0. 2 0. 4 0. 6 0. 8 1. 0\nTruePositiveRate\nR ep\nla ce\nP a th\nof D\nia gn\na l\nL in\ne, V\ner ti\nca l\nD ec\nom p\nos it\nio n\nR ep\nla ce\nP a th\nof D\nia gn\na l\nL in\ne, H\nor iz\non ta\nl D\nec om\np os\nit io\nn\nR ep\nla ce\nP a th\nof E\nu cl\nid ea\nn D\nis ta\nn ce\nto th\ne R\nO C\nC u\nrv e\nR ep\nla ce\nP a th\nof C\no m\np le\nm en\nt S\net B\nou n\nd ar\ny, E\nu cl\nid ea\nn D\nis ta\nn ce\nR ep\nla ce\nP a th\nof C\no m\np le\nm en\nt S\net B\nou n\nd ar y, V er ti ca l D ec o m p os it io n R ep la ce P a th of C o m p le m en t S et B ou n d ar y, H or iz o n ta l D ec om p o si ti o n R ep la ce P a th of B ay es ia n T es t R O C of th e R F M o d el , A U C = 0. 68 51 R ep la ce m en t P ro p or ti on 25 % - 1 00 % (I n cr\nem en\nt 25\n% )\nR ep\nla ce\nm en\nt P\nro p\nor ti\non 46\n.1 %\nA ll\nD o ct\nor s,\nT P\nR =\n0. 22\n64 ,\nF P\nR =\n0. 20\n65\n0 .1\n2 0.\n1 4\n0. 1 6\n0. 1 8\n0. 20\nF al\nse P\nos it\niv e\nR a te\n0 .2\n2\n0 .2\n4\n0 .2\n6\n0 .2\n8\n0 .3\n0\n0 .3\n2\n0 .3\n4\n0 .3\n6\n0 .3\n8\nTruePositiveRate\n(a )\nD oc\nto rs\n\u2019D ia\ngn os\nes >\n= 30\n0\n0. 0\n0. 2\n0 .4\n0. 6\n0 .8\n1 .0\nF a ls\ne P\nos it\niv e\nR at\ne\n0. 0 0. 2 0. 4 0. 6 0. 8 1. 0\nTruePositiveRate\nR ep\nla ce\nP at\nh o f\nD ia\ng n\na l\nL in\ne, V\ner ti\nca l\nD ec\no m\np os\nit io\nn\nR ep\nla ce\nP at\nh o f\nD ia\ng n\na l\nL in\ne, H\nor iz\no n ta\nl D\nec om\np o si\nti o n\nR ep\nla ce\nP at\nh o f\nE u\ncl id\nea n\nD is\nta n\nce to\nth e\nR O\nC C\nu rv\ne\nR ep\nla ce\nP at\nh o f\nC om\np le\nm en\nt S\net B\nou n\nd a ry\n, E\nu cl\nid ea\nn D\nis ta\nn ce\nR ep\nla ce\nP at\nh o f\nC om\np le\nm en\nt S\net B\nou n\nd a ry , V er ti ca l D ec om p o si ti o n R ep la ce P at h o f C om p le m en t S et B ou n d a ry , H or iz o n ta l D ec o m p o si ti o n R ep la ce P at h o f B ay es ia n T es t R O C o f th e R F M o d el , A U C = 0 .6 89 2 R ep la ce m en t P ro p o rt io n 2 5% - 1 00 % (I n cr\nem en\nt 2 5%\n)\nR ep\nla ce\nm en\nt P\nro p\no rt\nio n\n5 4.\n8 %\nA ll\nD o ct\no rs\n, T\nP R\n= 0.\n2 13\n5, F\nP R\n= 0 .1\n94 2\n0. 12\n0 .1\n4 0 .1\n6 0.\n1 8\n0 .2\n0\nF a ls\ne P\nos it\niv e\nR at\ne\n0. 22 0. 24 0. 26 0. 28 0. 30 0. 32 0. 34 0. 36\nTruePositiveRate\n(b )\nD oc\nto rs\n\u2019D ia\ngn os\nes >\n= 50\n0\nF ig\nur e\n11 :\nB ay\nes ia\nn A\npp ro\nac h\nw it\nh Lo\nss es\nFu nc\nti on\nC on\nst ru\nct ed\nby E\nuc lid\nea n\nD is\nta nc\ne an\nd R\nan do\nm iz\ned D\nec om\npo si\nti on"
        },
        {
            "heading": "A Supplementary appendix",
            "text": ""
        },
        {
            "heading": "A.1 Additional Lemmas and Proofs",
            "text": "Lemma A.1. Consider a model where the human decision maker follows the classification rule Y\u0302i \u201c 1 pp pXiq \u0105 c pXi, Uiqq for the ith patient case, where p pXiq is a correctly specified propensity score function p pXiq \u201c P pYi \u201c 1|Xiq, and Ui is a random variable that does not contain information about Yi beyond those in Xi: P pYi \u201c 1|Xi, Uiq \u201c P pYi \u201c 1|Xiq. Then the aggregate human decision maker FPR/TPR pair lies strictly below the ROC curve traced out by 1 pp pXiq \u0105 cq when c varies between 0 and 1, under the following conditions:\n1. The p-score p pxq is uniformly bounded away from 0 and 1 on x P X , the support of X.\n2. p pXq is continuously distributed with a bounded and strictly positive density.\n3. There is no c P r0, 1s such that with probability 1, p pXq \u0105 c if and only if p pXq \u0105 c pX,Uq.\nProof. First, we write human FPR/TPR pair as \u03b8H \u201c p\u03b1H , \u03b2Hq, where\n\u03b1H \u201c 1\n1 \u00b4 p\n\u017c\n\u03bb pxq p1 \u00b4 p pxqq fX pxq dx, \u03b2H \u201c 1\np\n\u017c\n\u03bb pxq p pxq fX pxq dx.\nIn the above we have used p \u201c P pYi \u201c 1q and\n\u03bb pxq \u201c \u017c\n1 pp pxq \u0105 c px, uqq fU |X pu|xqdu.\nWe can then always find a threshold c\u02da P r0, 1s that satisfies\n\u03b1O pc\u02daq \u201c \u015f 1 pp pxq \u0105 c\u02daq p1 \u00b4 p pxqq fX pxqdx 1 \u00b4 p \u201c \u015f \u03bb pxq p1 \u00b4 p pxqq fX pxq dx 1 \u00b4 p \u201d \u03b1H\nGiven c\u02da, we then next define, on the ROC curve:\n\u03b2O pc\u02daq \u201c \u015f 1 pp pxq \u0105 c\u02daq p pxq fX pxq dx p .\nBy Neyman-Pearson arguments, \u03d5\u02da pxq \u201d 1 pp pxq \u0105 c\u02daq solves\nmax \u03d5p\u00a8q\np1 \u00b4 c\u02daq \u017c \u03d5 pxq p pxq fX pxq dx \u00b4 c\u02da \u017c \u03d5 pxq p1 \u00b4 p pxqq fX pxqdx\n\u201c p p1 \u00b4 c\u02daq\u03b2O pc\u02daq \u00b4 p1 \u00b4 pq c\u02da\u03b1O pc\u02daq \u011b p p1 \u00b4 c\u02daq\u03b2H \u00b4 p1 \u00b4 pq c\u02da\u03b1H .\nThe difference between the two sides can be further written as\np p1 \u00b4 c\u02daq\u03b2O pc\u02daq \u00b4 p1 \u00b4 pq c\u02da\u03b1O pc\u02daq \u00b4 rp p1 \u00b4 c\u02daq\u03b2H \u00b4 p1 \u00b4 pq c\u02da\u03b1Hs\n\u201c \u017c p1 pp pxq \u0105 c\u02daq \u00b4 \u03bb pxqq pp pxq \u00b4 c\u02daq fX pxq dx,\nwhich is strictly positive unless P p\u03bb pXq \u201c 1 pp pXq \u0105 c\u02daqq \u201c 1. This is ruled out by the stated assumptions. In particular, by condition 3, with positive probability, either one of the following two events (A and B) holds. In event A, p pXq \u0105 c\u02da but p pXq \u010f c pX,Uq, in which case 1 pp pXq \u0105 c\u02daq \u201c 1 but \u03bb pXq \u0103 1. In event B, p pXq \u010f c\u02da but p pXq \u0105 c pX,Uq, in which case 1 pp pXq \u0105 c\u02daq \u201c 0 but \u03bb pXq \u0105 0. Together P p\u03bb pXq \u2030 1 pp pXq \u0105 c\u02daqq \u011b P pA Y Bq \u0105 0.\nLemma A.2. Let p pX,Uq \u201c P pY \u201c 1|X,Uq and p pXq \u201c P pY \u201c 1|Xq be two correctly specified propensity score functions. The ROC curve generated p pX,Uq lies above the ROC curve generated by p pXq. Specifically, assuming that P pp pX,Uq \u201c cq \u201c 0 for all c, then if the decision rules 1 pp pxq \u0105 cq and 1 pp px, uq \u0105 c1q both achieve FPR level \u03b1 and\nP \u2423 1 pp pXq \u0105 cq \u2030 1 ` p pX,Uq \u0105 c1 \u02d8( \u0105 0,\nthen 1 pp px, uq \u0105 c1q achieves higher TPR than 1 pp pxq \u0105 cq does.\nProof. If we denote the ROC curve generated by p pxq as\n\u03b2x p\u03b1q \u201c fx p\u03b1q \u201c E rY 1 pp pXq \u0105 cqs\np , where c satisfies E rp1 \u00b4 Y q1 pp pXq \u0105 cqs 1 \u00b4 p \u201c \u03b1,\nthen it can also be written as the solution of a constrained maximum of (Feng et al. (2023))\n\u03b2x p\u03b1q \u201c fx p\u03b1q \u201c max \u03d5p\u00a8q EY \u03d5 pXq p such that E rp1 \u00b4 Y q\u03d5 pXqs 1 \u00b4 p \u201c \u03b1 and 0 \u010f \u03d5 pxq \u010f 1.\nSimilarly we can also denote the ROC curve generated by p px, uq as\n\u03b2x,u p\u03b1q \u201c fx,u p\u03b1q \u201c max \u03d5p\u00a8,\u00a8q EY \u03d5 pX,Uq p such that E rp1 \u00b4 Y q\u03d5 pX,Uqs 1 \u00b4 p \u201c \u03b1 and 0 \u010f \u03d5 px, uq \u010f 1.\nSince the class of functions \u03d5 px, uq includes the class \u03d5 px, uq \u201d \u03d5 pxq as a subset, it follows immediately that \u03b2x,u p\u03b1q \u011b \u03b2x p\u03b1q.\nFor the second part of our statement, consider the sets\nS` \u201c \u2423 px, uq : 1 ` p px, uq \u0105 c1 \u02d8 \u0105 1 pp pxq \u0105 cq ( , S\u00b4 \u201c \u2423 px, uq : 1 ` p px, uq \u0105 c1 \u02d8 \u0103 1 pp pxq \u0105 cq ( .\nThen by assumption, P pS` Y S\u00b4q \u0105 0. If px, uq P S`, then p px, uq \u0105 c1; if px, uq P S\u00b4, then p px, uq \u010f c1. By assumption, P pp pX,Uq \u201c c1q \u201c 0, and thus,\n\u017c \u017c\n\u201c 1 ` p px, uq \u0105 c1 \u02d8 \u00b4 1 pp pxq \u0105 cq \u2030 ` p px, uq \u00b4 c1 \u02d8 fX,U px, uq dxdu \u0105 0.\nThe difference in power (TPR multiplied by p) then satisfies \u017c \u017c\n\u201c 1 ` p px, uq \u0105 c1 \u02d8 \u00b4 1 pp pxq \u0105 cq \u2030 p px, uq fX,U px, uq dxdu\n\u0105 c1 \u017c \u017c \u201c 1 ` p px, uq \u0105 c1 \u02d8 \u00b4 1 pp pxq \u0105 cq \u2030 fX,U px, uq dxdu\n\u201c c1 \u017c \u017c \u201c 1 ` p px, uq \u0105 c1 \u02d8 \u00b4 1 pp pxq \u0105 cq \u2030 p px, uq fX,U px, uq dxdu\nThe equation comes from the assumption that both decision rules achieve the same FPR: \u017c \u017c\n\u201c 1 ` p px, uq \u0105 c1 \u02d8 \u00b4 1 pp pxq \u0105 cq \u2030 p1 \u00b4 p px, uqq fX,U px, uq dxdu \u201c 0.\nWe then have \u017c \u017c\n\u201c 1 ` p px, uq \u0105 c1 \u02d8 \u00b4 1 pp pxq \u0105 cq \u2030 p px, uq f px, uq dxdu \u0105 0,\ni.e., 1 pp px, uq \u0105 c1q achieves higher TPR.\nProof of Lemma 2.1. Recall the definitions of sample and population FPR/TPR pairs:\n\u03b1\u0302 \u201c \u0159n i\u201c1 p1 \u00b4 Yiq pYi \u0159n\ni\u201c1 p1 \u00b4 Yiq , \u03b2\u0302 \u201c\n\u0159n i\u201c1 Yi\npYi \u0159n\ni\u201c1 Yi , \u03b1 \u201c\nE \u201d p1 \u00b4 Yiq pYi \u0131\nEYi , \u03b2 \u201c EYi pYi EYi .\nIt then follows from these definitions that for \u03b8 \u201c p\u03b1, \u03b2q1 and \u03b8\u0302 \u201c \u00b4 \u03b1\u0302, \u03b2\u0302 \u00af1 ,\n? n \u00b4 \u03b8\u0302 \u00b4 \u03b8 \u00af \u201c H\u0302\u00b41 1? n n \u00ff\ni\u201c1\n\u00a8\n\u02dd\np1 \u00b4 Yiq \u00b4 Y\u0302i \u00b4 \u03b1 \u00af\nYi\n\u00b4 Y\u0302i \u00b4 \u03b2 \u00af\n\u02db \u201a where H\u0302 \u201c \u02c6\n1 \u00b4 p\u0302 0 0 p\u0302\n\u02d9\n.\nBy the Law of Large Numbers (LLN), H\u0302 converges to H in probability:\nH\u0302 \u201c H ` oP p1q where H \u201c \u02c6 1 \u00b4 p 0 0 p \u02d9 .\nThe multivariate central limit theorem (CLT) also states that\n1? n\nn \u00ff\ni\u201c1\n\u00a8\n\u02dd\np1 \u00b4 Yiq \u00b4 Y\u0302i \u00b4 \u03b1 \u00af\nYi\n\u00b4 Y\u0302i \u00b4 \u03b2 \u00af\n\u02db\n\u201a d\u00dd\u00d1 N p0,\u2126q where \u2126 \u201c \u02dc \u03c32\u03b1\n0 \u03c32\u03b2\n\u00b8\n.\nWe then use the relations Y 2i \u201c Yi, p1 \u00b4 Yiq 2 \u201c 1 \u00b4 Yi, and Y\u0302 2i \u201c Y\u0302i to calculate that\n\u03c32\u03b1 \u201cE \u201e p1 \u00b4 Yiq2 \u00b4 Y\u0302i \u00b4 \u03b1 \u00af2 \u201c p1 \u00b4 pq\u03b1 p1 \u00b4 \u03b1q \u0237 ,\n\u03c32\u03b2 \u201cE \u201e Y 2i \u00b4 Y\u0302i \u00b4 \u03b2 \u00af2 \u201c p\u03b2 p1 \u00b4 \u03b2q \u0237 .\nIt then follows from the multivariate Slutsky\u2019s Lemma that\n? n \u00b4 \u03b8\u0302 \u00b4 \u03b8 \u00af d\u00dd\u00d1 N p0,\u03a3q , where \u03a3 \u201c \u02c6 \u03b1p1\u00b4\u03b1q 1\u00b4p 0\n0 \u03b2p1\u00b4\u03b2qp\n\u02d9\n.\nA typical concern with the application of central limit theorems is the quality of approximation in the finite sample when each of p, \u03b1 and \u03b2 is close to either 0 or 1. In particular, when the true population proportion of p is very close to the boundary of r0, 1s relative to the sample size, both the numerator and the denominator in either \u03b1\u0302 or \u03b2\u0302 may converge to a Poisson type limit random variable. The ratio of either \u03b1\u0302 or \u03b2\u0302 then remains random even in large samples and may fail to converge consistently to its population limit. In our application, the population parameter p is about 5%. Given that the sample size for each doctor is at least a few hundred, we believe that the asymptotic normal limit distribution still offers a reasonable approximation.\nThe cases when only either \u03b1 or \u03b2 is close to the boundary of r0, 1s does not by itself invalidate consistency, but may lead to a different convergence rate and asymptotic distribution. As such the approximation in the finite sample is likely to be more accurate for doctors whose FPR and TPR are further away from the boundary of r0, 1s. While the frequentist and Bayesian approaches are not directly comparable, employing the Bayesian methodology does allow us to sidestep issues related to the asymptotic distribution approximation.\nA.2 The implemented random forest algorithm\nWe implemented a random forest algorithm with N estimators and M max features per node. For each node in a tree, no more than M features will be considered to obtain the best split. The algorithm works as follows:\n1. For i \u201c 1 to N :\n(a) Draw a bootstrap sample D\u0303i from the training data D that has the same sample size\nas D and that is drawn with replacements.\n(b) Grow an unpruned tree Ti using D\u0303i by repeating the following steps for each node of the tree until the nodes are pure or until the number of leaves for a node falls below the minimum number of samples required to split:\ni. Randomly choose M features from the d dimensional input feature vector.\nii. Select the best of the M features to split using the Gini impurity criterion.\niii. Split the node into two subnodes using the best feature.\n2. Obtain the random forest output tTiuNi\u201c1.\nGiven a new input feature vector x, the random forest predicts the propensity score function by aggregating the results of N trees, where the class probability m pxq is computed as the mean of the class probability of each tree in the forest.\nA.3 Frequentist approach machine decision thresholds\nFor each of the less capable doctors j, we generate the machine decision thresholds with a specific cutoff value cj using the following algorithm. First, we identify the lowest and highest points (for the FPR/TPR pair) on the dominating segment of the machine ROC curve, corresponding to points B ad A in Figure 4. We label these two points as cj,1 and cj,N . Second, along this dominating interval of the machine ROC curve between points B and A, we uniformly sample N cutoff thresholds for computing FPR/TPR pairs, using the following procedure:\n1. The stepsize of threshold spacing is set to cj,step \u201c cj,N\u00b4cj,1 N\u00b41 ;\n2. For each l \u201c 0, . . . , N \u00b4 1, set the lth threshold between cj,1 and cj,N to\ncj,l \u201c cj,1 ` l\nN \u00b4 1 .\nCompute the corresponding FPR/TPR pair and denote as \u03b8j,l`1 \u201c p\u03b1j,l`1, \u03b2j,l`1q.\n3. Collect the uniformly sampled FPR/TPR pairs \u03b8j,l, l \u201c 1, . . . , N .\nA.4 Randomized replacement by the machine algorithm\nThe experiments in section 4 identify less capable doctors to be replaced by the machine algorithm. Here we consider an extension where the machine decision is randomly accepted by these doctors according to a pre-specified probability. In particular, let \u03bb P r0, 1s denote the human acceptance probability for the machine decision. For each less capable doctor j,\nwe generate decisions for their patient cases in the performance data set using the following randomized rule that combines y\u0302i,M and y\u0302i,j in Assumption 1:\nY\u0304i,j \u201c 1pNi,j \u010f \u03bbqY\u0302i,M ` 1pNi,j \u0105 \u03bbqY\u0302i,j ,\nwhere Ni,j is independent and uniformly distributed on r0, 1s. In summary, for each patient case, each less capable doctor will accept the machine decision with a probability of \u03bb. The larger the \u03bb, the more likely the doctor will accept the machine algorithm to make the decision.\nWe apply the randomized decision rule with different acceptance rates to the basic Bayesian test approach. Figure 17(a) shows the results of the Bayesian test described in Section 2.4.\n[Figure 17 about here.]\nThe results in the Figure 17(a) are based on doctors who had diagnosed more than 300 patient cases. The black crosses on the figures correspond to when \u03bb P t0.2, 0.4, 0.6, 0.8u. Figures 17(a) show that as the acceptance rate \u03bb grows, the black cross gradually moves from the blue cross (where all doctors make their own decisions; \u03bb \u201c 0) to the purple cross (where less capable doctors are completely replaced by machine decisions; \u03bb \u201c 1). Additional details of the performance of FPR/TPR under various levels of acceptance rates are tabulated in panel A of Table 3, from which it is evident that the overall FPT/TPR performance improves monotonically when the probability of replacing less capable doctors increases from 0 to 1.\n[Table 3 about here.]\nIt is likely that all doctors, both more capable and less capable doctors, may benefit from being referred to the machine decision. It is natural to consider a setting where the more capable doctors are also randomized into being replaced by artificial intelligence. Figure 17(b) shows the results when different acceptance rates \u03bb are used to randomize all doctors. When a doctor is randomized into being replaced by the machine learning, we continue to apply the baseline Bayesian test in (10) to determine the replacement point on the machine ROC curve.\nFigure 17(b) shows that replacing less capable doctors (45.2% of the total headcount) tends to produce a much better FPR/TPR pair than randomly replacing the same proportion of doctors\u2019 decisions. In addition, full replacement of doctors (the yellow cross) is not saliently better than the replacement of only less capable doctors only (the purple cross).\nInstead of relying on a single constant acceptance rate \u03bb to randomize all doctors involved, doctors can also be randomized by individually personalized acceptance rates \u03bb\u2019s that are designed to vary with the capability of the doctors. Using the baseline Bayesian test in (10), as the Bayesian loss (credible level) goes up, meaning that the doctor becomes less capable, we increase the acceptance probability \u03bb correspondingly. Figure 18 displays two scenarios. In scenario 1, we first rank the loss of all doctors using the baseline Bayesian test in (10) before\nassigning them an acceptance rate \u03bb that depends linearly on their ranks. The acceptance rate \u03bb of the most capable doctor, with rank 1, is set to 0, while \u03bb for the least capable doctor, with rank n, is set to 1. Each doctor whose rank r is between 1 and n is given an acceptance probability of \u03bb \u201c r\u00b41n\u00b41 . Scenario 2 is similar to scenario 1, but instead of randomizing all doctors, we only randomize those \"less capable\" doctors who have been identified by the baseline Bayesian test.\n[Figure 18 about here.]\nFigure 19 illustrates the consequence of an alternative assignment procedure for the acceptance rates \u03bb based on the reverse capability rank. Scenario 3 and scenario 4 are analogous to scenarios 1 and 2. The main difference is that here \u03bb is calculated as n\u00b4rn\u00b41 . We interpret the reverse rank ordering as capable doctors being more willing to accept machine decisions in these two scenarios.\n[Figure 19 about here.]\nCompared with scenarios 1 and 2, the FPR/TPR pair of aggregating the machine algorithm and doctors in scenario 3 and 4 are inferior. These results illustrate that if less capable doctors are not willing to adopt the decision from a machine algorithm, the average diagnose quality will not improve significantly. More specifically, the aggregate FPR/TPR point in scenario 1 is better than that in scenario 2, while the aggregate FPR/TPR points in scenarios 3 and 4 are not comparable to each other. These results accord with our intuition: improving the overall FPR/TPR performance calls for higher acceptance rate for less capable doctors. Meanwhile, using the machine algorithm to replace the more capable doctors\u2019 decisions with high probability is unlikely to be beneficial."
        }
    ],
    "title": "Statistical tests for replacing human decision makers with algorithms",
    "year": 2023
}