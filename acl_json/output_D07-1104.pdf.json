{
    "abstractText": "A major engineering challenge in statistical machine translation systems is the efficient representation of extremely large translation rulesets. In phrase-based models, this problem can be addressed by storing the training data in memory and using a suffix array as an efficient index to quickly lookup and extract rules on the fly. Hierarchical phrasebased translation introduces the added wrinkle of source phrases with gaps. Lookup algorithms used for contiguous phrases no longer apply and the best approximate pattern matching algorithms are much too slow, taking several minutes per sentence. We describe new lookup algorithms for hierarchical phrase-based translation that reduce the empirical computation time by nearly two orders of magnitude, making on-the-fly lookup feasible for source phrases with gaps.",
    "authors": [
        {
            "affiliations": [],
            "name": "Adam Lopez"
        }
    ],
    "id": "SP:5c8142ac7d4ba198a47e44d1fb9cb052f76f815d",
    "references": [
        {
            "authors": [
                "Mohamed Ibrahim Abouelhoda",
                "Stefan Kurtz",
                "Enno Ohlebusch."
            ],
            "title": "Replacing suffix trees with enhanced suffix arrays",
            "venue": "Journal of Discrete Algorithms, 2(1):53\u201386, Mar.",
            "year": 2004
        },
        {
            "authors": [
                "Ricardo Baeza-Yates",
                "Alejandro Salinger."
            ],
            "title": "Experimental analysis of a fast intersection algorithm for sorted sequences",
            "venue": "M. Consens and G. Navarro, editors, Proc. of SPIRE, number 3772 in LNCS, pages 13\u201324, Berlin. Springer-Verlag.",
            "year": 2005
        },
        {
            "authors": [
                "Ricardo Baeza-Yates."
            ],
            "title": "A fast intersection algorithm for sorted sequences",
            "venue": "Proc. of Combinatorial Pattern Matching, number 3109 in LNCS, pages 400\u2013 408, Berlin. Springer-Verlag.",
            "year": 2004
        },
        {
            "authors": [
                "Chris Callison-Burch",
                "Colin Bannard",
                "Josh Shroeder."
            ],
            "title": "Scaling phrase-based statistical machine translation to larger corpora and longer phrases",
            "venue": "Proc. of ACL, pages 255\u2013262, Jun.",
            "year": 2005
        },
        {
            "authors": [
                "David Chiang",
                "Adam Lopez",
                "Nitin Madnani",
                "Christof Monz",
                "Philip Resnik",
                "Michael Subotin."
            ],
            "title": "The Hiero machine translation system: Extensions, evaluation, and analysis",
            "venue": "Proc. of HLT-EMLP, pages 779\u2013 786, Oct.",
            "year": 2005
        },
        {
            "authors": [
                "David Chiang."
            ],
            "title": "Hierarchical phrase-based translation",
            "venue": "Computational Linguistics, 33(2). In press.",
            "year": 2007
        },
        {
            "authors": [
                "Marcello Federico",
                "Nicola Bertoldi."
            ],
            "title": "How many bits are needed to store probabilities for phrasebased translation? In Proc",
            "venue": "of NAACL Workshop on Statistical Machine Translation, pages 94\u2013101, Jun.",
            "year": 2006
        },
        {
            "authors": [
                "Dan Gusfield."
            ],
            "title": "Algorithms on Strings, Trees, and Sequences",
            "venue": "Cambridge University Press.",
            "year": 1997
        },
        {
            "authors": [
                "Dan Klein",
                "Christopher D. Manning."
            ],
            "title": "Parsing with treebank grammars: Empirical bounds, theoretical models, and the structure of the Penn Treebank",
            "venue": "Proc. of ACL-EACL, pages 330\u2013337, Jul.",
            "year": 2001
        },
        {
            "authors": [
                "Philipp Koehn",
                "Franz Josef Och",
                "Daniel Marcu."
            ],
            "title": "Statistical phrase-based translation",
            "venue": "Proc. of HLT-NAACL, pages 127\u2013133, May.",
            "year": 2003
        },
        {
            "authors": [
                "Adam Lopez",
                "Philip Resnik."
            ],
            "title": "Word-based alignment, phrase-based translation: What\u2019s the link? In Proc",
            "venue": "of AMTA, pages 90\u201399, Aug.",
            "year": 2006
        },
        {
            "authors": [
                "Adam Lopez."
            ],
            "title": "Hierarchical phrase-based translation with suffix arrays",
            "venue": "Technical Report 2007-26, University of Maryland Institute for Advanced Computer Studies, May.",
            "year": 2007
        },
        {
            "authors": [
                "Adam Lopez."
            ],
            "title": "A survey of statistical machine translation",
            "venue": "Technical Report 2006-47, University of Maryland Institute for Advanced Computer Studies, Apr.",
            "year": 2007
        },
        {
            "authors": [
                "Udi Manber",
                "Gene Myers."
            ],
            "title": "Suffix arrays: A new method for on-line string searches",
            "venue": "SIAM Journal of Computing, 22(5):935\u2013948.",
            "year": 1993
        },
        {
            "authors": [
                "Daniel Marcu."
            ],
            "title": "Towards a unified approach to memory- and statistical-based machine translation",
            "venue": "Proc. of ACL-EACL, pages 378\u2013385, Jul.",
            "year": 2001
        },
        {
            "authors": [
                "Gonzalo Navarro."
            ],
            "title": "A guided tour to approximate string matching",
            "venue": "ACM Computing Surveys, 33(1):31\u2013 88, Mar.",
            "year": 2001
        },
        {
            "authors": [
                "Franz Josef Och",
                "Hermann Ney."
            ],
            "title": "A systematic comparison of various statistical alignment models",
            "venue": "Computational Linguistics, 29(1):19\u201351, Mar.",
            "year": 2003
        },
        {
            "authors": [
                "Franz Josef Och",
                "Hermann Ney."
            ],
            "title": "The alignment template approach to machine translation",
            "venue": "Computational Linguistics, 30(4):417\u2013449, Jun.",
            "year": 2004
        },
        {
            "authors": [
                "Franz Josef Och."
            ],
            "title": "Statistical Machine Translation: From Single-Word Models to Alignment Templates",
            "venue": "Ph.D. thesis, RWTH Aachen, Oct.",
            "year": 2002
        },
        {
            "authors": [
                "Franz Josef Och."
            ],
            "title": "Statistical machine translation: The fabulous present and future",
            "venue": "Proc. of ACL Workshop on Building and Using Parallel Texts, Jun. Invited talk.",
            "year": 2005
        },
        {
            "authors": [
                "Chris Quirk",
                "Arul Menezes",
                "Colin Cherry."
            ],
            "title": "Dependency treelet translation: Syntactically informed phrasal SMT",
            "venue": "Proc. of ACL, pages 271\u2013279, Jun.",
            "year": 2005
        },
        {
            "authors": [
                "Mohammad Sohel Rahman",
                "Costas S. Iliopoulos",
                "Inbok Lee",
                "Manal Mohamed",
                "William F. Smyth."
            ],
            "title": "Finding patterns with variable length gaps or don\u2019t cares",
            "venue": "Proc. of COCOON, Aug.",
            "year": 2006
        },
        {
            "authors": [
                "Michel Simard",
                "Nicola Cancedda",
                "Bruno Cavestro",
                "Marc Dymetman",
                "Eric Gaussier",
                "Cyril Goutte",
                "Kenji Yamada",
                "Philippe Langlais",
                "Arne Mauser."
            ],
            "title": "Translating with non-contiguous phrases",
            "venue": "Proc. of HLT-EMNLP, pages 755\u2013762, Oct.",
            "year": 2005
        },
        {
            "authors": [
                "Peter van Emde Boas",
                "R. Kaas",
                "E. Zijlstra."
            ],
            "title": "Design and implementation of an efficient priority queue",
            "venue": "Mathematical Systems Theory, 10(2):99\u2013127.",
            "year": 1977
        },
        {
            "authors": [
                "Richard Zens",
                "Hermann Ney."
            ],
            "title": "Efficient phrasetable representation for machine translation with applications to online MT and speech translation",
            "venue": "Proc. of HLT-NAACL. To appear.",
            "year": 2007
        },
        {
            "authors": [
                "Ying Zhang",
                "Stephan Vogel."
            ],
            "title": "An efficient phrase-to-phrase alignment model for arbitrarily long phrase and large corpora",
            "venue": "Proc. of EAMT, May.",
            "year": 2005
        }
    ],
    "sections": [
        {
            "text": "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp. 976\u2013985, Prague, June 2007. c\u00a92007 Association for Computational Linguistics"
        },
        {
            "heading": "1 Introduction",
            "text": "Current statistical machine translation systems rely on very large rule sets. In phrase-based systems, rules are extracted from parallel corpora containing tens or hundreds of millions of words. This can result in millions of rules using even the most conservative extraction heuristics. Efficient algorithms for rule storage and access are necessary for practical decoding algorithms. They are crucial to keeping up with the ever-increasing size of parallel corpora, as well as the introduction of new data sources such as web-mined and comparable corpora.\nUntil recently, most approaches to this problem involved substantial tradeoffs. The common practice of test set filtering renders systems impractical for all but batch processing. Tight restrictions on phrase length curtail the power of phrase-based models. However, some promising engineering solutions are emerging. Zens and Ney (2007) use a disk-based prefix tree, enabling efficient access to phrase tables much too large to fit in main memory. An alternative approach introduced independently by both Callison-Burch et al. (2005) and Zhang and Vogel (2005) is to store the training data itself in memory, and use a suffix array as an efficient index to look up, extract, and score phrase pairs on the fly. We believe that the latter approach has several important applications (\u00a77).\nSo far, these techniques have focused on phrasebased models using contiguous phrases (Koehn et al., 2003; Och and Ney, 2004). Some recent models permit discontiguous phrases (Chiang, 2007; Quirk et al., 2005; Simard et al., 2005). Of particular interest to us is the hierarchical phrase-based model of Chiang (2007), which has been shown to be superior to phrase-based models. The ruleset extracted by this model is a superset of the ruleset in an equivalent phrase-based model, and it is an order of magnitude larger. This makes efficient rule representation even more critical. We tackle the problem using the online rule extraction method of Callison-Burch et al. (2005) and Zhang and Vogel (2005).\nThe problem statement for our work is: Given an input sentence, efficiently find all hierarchical phrase-based translation rules for that sentence in the training corpus.\n976\nWe first review suffix arrays (\u00a72) and hierarchical phrase-based translation (\u00a73). We show that the obvious approach using state-of-the-art pattern matching algorithms is hopelessly inefficient (\u00a74). We then describe a series of algorithms to address this inefficiency (\u00a75). Our algorithms reduce computation time by two orders of magnitude, making the approach feasible (\u00a76). We close with a discussion that describes several applications of our work (\u00a77)."
        },
        {
            "heading": "2 Suffix Arrays",
            "text": "A suffix array is a data structure representing all suffixes of a corpus in lexicographical order (Manber and Myers, 1993). Formally, for a text T , the ith suffix of T is the substring of the text beginning at position i and continuing to the end of T . This suffix can be uniquely identified by the index i of its first word. The suffix array SAT of T is a permutation of [1, |T |] arranged by the lexicographical order of the corresponding suffixes. This representation enables fast lookup of any contiguous substring using binary search. Specifically, all occurrences of a length-m substring can be found in O(m + log |T |) time (Manber and Myers, 1993). 1\nCallison-Burch et al. (2005) and Zhang and Vogel (2005) use suffix arrays as follows.\n1. Load the source training text F , the suffix array SAF , the target training text E, and the alignment A into memory.\n2. For each input sentence, look up each substring (phrase) f\u0304 of the sentence in the suffix array.\n3. For each instance of f\u0304 found in F , find its aligned phrase e\u0304 using the phrase extraction method of Koehn et al. (2003).\n4. Compute the relative frequency score p(e\u0304|f\u0304) of each pair using the count of the extracted pair and the marginal count of f\u0304 .\n5. Compute the lexical weighting score of the phrase pair using the alignment that gives the best score.\n1Abouelhoda et al. (2004) show that lookup can be done in optimal O(m) time using some auxiliaray data structures. For our purposes O(m + log |T |) is practical, since for the 27Mword corpus used to carry out our experiments, log |T | \u223c 25.\n6. Use the scored rules to translate the input sentence with a standard decoding algorithm.\nA difficulty with this approach is step 3, which can be quite slow. Its complexity is linear in the number of occurrences of the source phrase f\u0304 . Both Callison-Burch et al. (2005) and Zhang and Vogel (2005) solve this with sampling. If a source phrase appears more than k times, they sample only k occurrences for rule extraction. Both papers report that translation performance is nearly identical to extracting all possible phrases when k = 100. 2"
        },
        {
            "heading": "3 Hierarchical Phrase-Based Translation",
            "text": "We consider the hierarchical translation model of Chiang (2007). Formally, this model is a synchronous context-free grammar. The lexicalized translation rules of the grammar may contain a single nonterminal symbol, denoted X . We will use a, b, c and d to denote terminal symbols, and u, v, and w to denote (possibly empty) sequences of these terminals. We will additionally use \u03b1 and \u03b2 to denote (possibly empty) sequences containing both terminals and nonterminals.\nA translation rule is written X \u2192 \u03b1/\u03b2. This rule states that a span of the input matching \u03b1 is replaced by \u03b2 in translation. We require that \u03b1 and \u03b2 contain an equal number (possibly zero) of coindexed nonterminals. An example rule with coindexes is X \u2192 uX 1 vX 2 w/u\u2032X 2 v\u2032X 1 w\u2032. When discussing only the source side of such rules, we will leave out the coindexes. For instance, the source side of the above rule will be written uXvXw. 3\nFor the purposes of this paper, we adhere to the restrictions described by Chiang (2007) for rules extracted from the training data.\n\u2022 Rules can contain at most two nonterminals.\n\u2022 Rules can contain at most five terminals.\n\u2022 Rules can span at most ten words. 2A sample size of 100 is actually quite small for many phrases, some of which occur tens or hundreds of thousands of times. It is perhaps surprising that such a small sample size works as well as the full data. However, recent work by Och (2005) and Federico and Bertoldi (2006) has shown that the statistics used by phrase-based systems are not very precise.\n3In the canonical representation of the grammar, source-side coindexes are always in sorted order, making them unambiguous.\n\u2022 Nonterminals must span at least two words.\n\u2022 Adjacent nonterminals are disallowed in the source side of a rule.\nExpressed more economically, we say that our goal is to search for source phrases in the form u, uXv, or uXvXw, where 1 \u2264 |uvw| \u2264 5, and |v| > 0 in the final case. Note that the model also allows rules in the form Xu, uX , XuX , XuXv, and uXvX . However, these rules are lexically identical to other rules, and thus will match the same locations in the source text."
        },
        {
            "heading": "4 The Collocation Problem",
            "text": "On-the-fly lookup using suffix arrays involves an added complication when the rules are in form uXv or uXvXw. Binary search enables fast lookup of contiguous substrings. However, it cannot be used for discontiguous substrings. Consider the rule aXbXc. If we search for this rule in the following logical suffix array fragment, we will find the boldfaced matches. ... a c a c b a d c a d ... a c a d b a a d b d ... a d d b a a d a b c ... a d d b d a a b b a ... a d d b d d c a a a ... ...\nEven though these suffixes are in lexicographical order, matching suffixes are interspersed with nonmatching suffixes. We will need another algorithm to find the source rules containing at least one X surrounded by nonempty sequences of terminal symbols."
        },
        {
            "heading": "4.1 Baseline Approach",
            "text": "In the pattern-matching literature, words spanned by the nonterminal symbols of Chiang\u2019s grammar are called don\u2019t cares and a nonterminal symbol in a query pattern that matches a sequence of don\u2019t cares is called a variable length gap. The search problem for patterns containing these gaps is a variant of approximate pattern matching, which has received substantial attention (Navarro, 2001). The best algorithm for pattern matching with variable-length gaps in a suffix array is a recent algorithm by Rahman\net al. (2006). It works on a pattern w1Xw2X...wI consisting of I contiguous substrings w1, w2, ...wI , each separated by a gap. The algorithm is straightforward. After identifying all ni occurrences of each wi in O(|wi| + log |T |) time, collocations that meet the gap constraints are computed using an efficient data structure called a stratified tree (van Emde Boas et al., 1977). 4 Although we refer the reader to the source text for a full description of this data structure, its salient characteristic is that it implements priority queue operations insert and next-element in O(log log |T |) time. Therefore, the total running time for an algorithm to find all contiguous subpatterns and compute their collocations is O( \u2211I i=1 [|wi|+ log|T |+ ni log log |T |]).\nWe can improve on the algorithm of Rahman et al. (2006) using a variation on the idea of hashing. We exploit the fact that our large text is actually a collection of relatively short sentences, and that collocated patterns must occur in the same sentence in order to be considered a rule. Therefore, we can use the sentence id of each subpattern occurrence as a kind of hash key. We create a hash table whose size is exactly the number of sentences in our training corpus. Each location of the partially matched pattern w1X...Xwi is inserted into the hash bucket with the matching sentence id. To find collocated patterns wi+1, we probe the hash table with each of the ni+1 locations for that subpattern. When a match is found, we compare the element with all elements in the bucket to see if it is within the window imposed by the phrase length constraints. Theoretically, the worst case for this algorithm occurs when all elements of both sets resolve to the same hash bucket, and we must compare all elements of one set with all elements of the other set. This leads to a worst case complexity of O(\n\u2211I i=1 [|wi|+ log|T |] +\u220fI\ni=1 ni). However, for real language data the performance for sets of any significant size will be O( \u2211I i=1 [|wi|+ log|T |+ ni]), since most patterns will occur once in any given sentence."
        },
        {
            "heading": "4.2 Analysis",
            "text": "It is instructive to compare this with the complexity for contiguous phrases. In that case, total lookup time is O(|w| + log|T |) for a contiguous pattern w.\n4Often known in the literature as a van Emde Boas tree or van Emde Boas priority queue.\nThe crucial difference between the contiguous and discontiguous case is the added term \u2211I i=1 ni. For even moderately frequent subpatterns this term dominates complexity.\nTo make matters concrete, consider the training corpus used in our experiments (\u00a76), which contains 27M source words. The three most frequent unigrams occur 1.48M, 1.16M and 688K times \u2013 the first two occur on average more than once per sentence. In the worst case, looking up a contiguous phrase containing any number and combination of these unigrams requires no more than 25 comparison operations. In contrast, the worst case scenario for a pattern with a single gap, bookended on either side by the most frequent word, requires over two million operations using our baseline algorithm and over thirteen million using the algorithm of Rahman et al. (2006). A single frequent word in an input sentence is enough to cause noticeable slowdowns, since it can appear in up to 530 hierarchical rules.\nTo analyze the cost empirically, we ran our baseline algorithm on the first 50 sentences of the NIST Chinese-English 2003 test set and measured the CPU time taken to compute collocations. We found that, on average, it took 2241.25 seconds (\u223c37 minutes) per sentence just to compute all of the needed collocations. By comparison, decoding time per sentence is roughly 10 seconds with moderately aggressive pruning, using the Python implementation of Chiang (2007)."
        },
        {
            "heading": "5 Solving the Collocation Problem",
            "text": "Clearly, looking up patterns in this way is not practical. To analyze the problem, we measured the amount of CPU time per computation. Cumulative lookup time was dominated by a very small fraction of the computations (Fig. 1). As expected, further analysis showed that these expensive computations all involved one or more very frequent subpatterns. In the worst cases a single collocation took several seconds to compute. However, there is a silver lining. Patterns follow a Zipf distribution, so the number of pattern types that cause the problem is actually quite small. The vast majority of patterns are rare. Therefore, our solution focuses on computations where one or more of the component patterns is frequent. Assume that we are computing a collo-\ncation of pattern w1X...Xwi and pattern wi+1, and we know all locations of each. There are three cases.\n\u2022 If both patterns are frequent, we resort to a precomputed intersection (\u00a75.1). We were not aware of any algorithms to substantially improve the efficiency of this computation when it is requested on the fly, but precomputation can be done in a single pass over the text at decoder startup.\n\u2022 If one pattern is frequent and the other is rare, we use an algorithm whose complexity is dependent mainly on the frequency of the rare pattern (\u00a75.2). It can also be used for pairs of rare patterns when one pattern is much rarer than the other.\n\u2022 If both patterns are rare, no special algorithms are needed. Any linear algorithm will suffice. However, for reasons described in \u00a75.3, our other collocation algorithms depend on sorted sets, so we use a merge algorithm.\nFinally, in order to cut down on the number of unnecessary computations, we use an efficient method to enumerate the phrases to lookup (\u00a75.4). This method also forms the basis of various caching strategies for additional speedups. We analyze the memory use of our algorithms in \u00a75.5."
        },
        {
            "heading": "5.1 Precomputation",
            "text": "Precomputation of the most expensive collocations can be done in a single pass over the text. As input, our algorithm requires the identities of the k\nmost frequent contiguous patterns. 5 It then iterates over the corpus. Whenever a pattern from the list is seen, we push a tuple consisting of its identity and current location onto a queue. Whenever the oldest item on the queue falls outside the maximum phrase length window with respect to the current position, we compute that item\u2019s collocation with all succeeding patterns (subject to pattern length constraints) and pop it from the queue. We repeat this step for every item that falls outside the window. At the end of each sentence, we compute collocations for any remaining items in the queue and then empty it.\nOur precomputation includes the most frequent n-gram subpatterns. Most of these are unigrams, but in our experiments we found 5-grams among the 1000 most frequent patterns. We precompute the locations of source phrase uXv for any pair u and v that both appear on this list. There is also a small number of patterns uXv that are very frequent. We cannot easily obtain a list of these in advance, but we observe that they always consist of a pair u and v of patterns from near the top of the frequency list. Therefore we also precompute the locations uXvXw of patterns in which both u and v are among these super-frequent patterns (all unigrams), treating this as the collocation of the frequent pattern uXv and frequent pattern w. We also compute the analagous case for u and vXw."
        },
        {
            "heading": "5.2 Fast Intersection",
            "text": "For collocations of frequent and rare patterns, we use a fast set intersection method for sorted sets called double binary search (Baeza-Yates, 2004). 6 It is based on the intuition that if one set in a pair of sorted sets is much smaller than the other, then we can compute their intersection efficiently by performing a binary search in the larger data set D for each element of the smaller query set Q.\nDouble binary search takes this idea a step further. It performs a binary search in D for the median element of Q. Whether or not the element is found, the\n5These can be identified using a single traversal over a longest common prefix (LCP) array, an auxiliary data structure of the suffix array, described by Manber and Myers (1993). Since we don\u2019t need the LCP array at runtime, we chose to do this computation once offline.\n6Minor modifications are required since we are computing collocation rather than intersection. Due to space constraints, details and proof of correctness are available in Lopez (2007a).\nsearch divides both sets into two pairs of smaller sets that can be processed recursively. Detailed analysis and empirical results on an information retrieval task are reported in Baeza-Yates (2004) and Baeza-Yates and Salinger (2005). If |Q| log |D| < |D| then the performance is guaranteed to be sublinear. In practice it is often sublinear even if |Q| log |D| is somewhat larger than |D|. In our implementation we simply check for the condition \u03bb|Q| log |D| < |D| to decide whether we should use double binary search or the merge algorithm. This check is applied in the recursive cases as well as for the initial inputs. The variable \u03bb can be adjusted for performance. We determined experimentally that a good value for this parameter is 0.3."
        },
        {
            "heading": "5.3 Obtaining Sorted Sets",
            "text": "Double binary search requires that its input sets be in sorted order. However, the suffix array returns matchings in lexicographical order, not numeric order. The algorithm of Rahman et al. (2006) deals with this problem by inserting the unordered items into a stratified tree. This requires O(n log log |T |) time for n items. If we used the same strategy, our algorithm would no longer be sublinear.\nAn alternative is to precompute all n-gram occurrences in order and store them in an inverted index. This can be done in one pass over the data. 7 This approach requires a separate inverted index for each n, up to the maximum n used by the model. The memory cost is one length-|T | array per index.\nIn order to avoid the full n|T | cost in memory, our implementation uses a mixed strategy. We keep a precomputed inverted index only for unigrams. For bigrams and larger n-grams, we generate the index on the fly using stratified trees. This results in a superlinear algorithm for intersection. However, we can exploit the fact that we must compute collocations multiple times for each input n-gram by caching the sorted set after we create it (The caching strategy is described in \u00a75.4). Subsequent computations involving this n-gram can then be done in linear or sublinear time. Therefore, the cost of building the inverted index on the fly is amortized over a large number of computations.\n7We combine this step with the other precomputations that require a pass over the data, thereby removing a redundant O(|T |) term from the startup cost."
        },
        {
            "heading": "5.4 Efficient Enumeration",
            "text": "A major difference between contiguous phrasebased models and hierarchical phrase-based models is the number of rules that potentially apply to an input sentence. To make this concrete, on our data, with an average of 29 words per sentence, there were on average 133 contiguous phrases of length 5 or less that applied. By comparison, there were on average 7557 hierarchical phrases containing up to 5 words. These patterns are obviously highly overlapping and we employ an algorithm to exploit this fact. We first describe a baseline algorithm used for contiguous phrases (\u00a75.4.1). We then introduce some improvements (\u00a75.4.2) and describe a data structure used by the algorithm (\u00a75.4.3). Finally, we discuss some special cases for discontiguous phrases (\u00a75.4.4)."
        },
        {
            "heading": "5.4.1 The Zhang-Vogel Algorithm",
            "text": "Zhang and Vogel (2005) present a clever algorithm for contiguous phrase searches in a suffix array. It exploits the fact that for each m-length source phrase that we want to look up, we will also want to look up its (m\u2212 1)-length prefix. They observe that the region of the suffix array containing all suffixes prefixed by ua is a subset of the region containing the suffixes prefixed by u. Therefore, if we enumerate the phrases of our sentence in such a way that we always search for u before searching for ua, we can restrict the binary search for ua to the range containing the suffixes prefixed by u. If the search for u fails, we do not need to search for ua at all. They show that this approach leads to some time savings for phrase search, although the gains are relatively modest since the search for contiguous phrases is not very expensive to begin with. However, the potential savings in the discontiguous case are much greater."
        },
        {
            "heading": "5.4.2 Improvements and Extensions",
            "text": "We can improve on the Zhang-Vogel algorithm. An m-length contiguous phrase aub depends not only on the existence of its prefix au, but also on the existence of its suffix ub. In the contiguous case, we cannot use this information to restrict the starting range of the binary search, but we can check for the existence of ub to decide whether we even need to search for aub at all. This can help us avoid searches that are guaranteed to be fruitless.\nNow consider the discontiguous case. As in the analogous contiguous case, a phrase a\u03b1b will only exist in the text if its maximal prefix a\u03b1 and maximal suffix \u03b1b both exist in the corpus and overlap at specific positions. 8 Searching for a\u03b1b is potentially very expensive, so we put all available information to work. Before searching, we require that both a\u03b1 and \u03b1b exist. Additionally, we compute the location of a\u03b1b using the locations of both maximal subphrases. To see why the latter optimization is useful, consider a phrase abXcd. In our baseline algorithm, we would search for ab and cd, and then perform a computation to see whether these subphrases were collocated within an elastic window. However, if we instead use abXc and bXcd as the basis of the computation, we gain two advantages. First, the number elements of each set is likely to be smaller then in the former case. Second, the computation becomes simpler, because we now only need to check to see whether the patterns exactly overlap with a starting offset of one, rather than checking within a window of locations.\nWe can improve efficiency even further if we consider cases where the same substring occurs more than once within the same sentence, or even in multiple sentences. If the computation required to look up a phrase is expensive, we would like to perform the lookup only once. This requires some mechanism for caching. Depending on the situation, we might want to cache only certain subsets of phrases, based on their frequency or difficulty to compute. We would also like the flexibility to combine onthe-fly lookups with a partially precomputed phrase table, as in the online/offline mixture of Zhang and Vogel (2005).\nWe need a data structure that provides this flexibility, in addition to providing fast access to both the maximal prefix and maximal suffix of any phrase that we might consider."
        },
        {
            "heading": "5.4.3 Prefix Trees and Suffix Links",
            "text": "Our search optimizations are easily captured in a prefix tree data structure augmented with suffix links. Formally, a prefix tree is an unminimized deterministic finite-state automaton that recognizes all of the patterns in some set. Each node in the tree repre-\n8Except when \u03b1 = X , in which case a and b must be collocated within a window defined by the phrase length constraints.\nsents the prefix of a unique pattern from the set that is specified by the concatenation of the edge labels along the path from the root to that node. A suffix link is a pointer from a node representing path a\u03b1 to the node representing path \u03b1. We will use this data structure to record the set of patterns that we have searched for and to cache information for those that were found successfully.\nOur algorithm generates the tree breadth-search along a frontier. In the mth iteration we only search for patterns containing m terminal symbols. Regardless of whether we find a particular pattern, we create a node for it in the tree. If the pattern was found in the corpus, its node is marked active. Otherwise, it is marked inactive. For found patterns, we store either the endpoints of the suffix array range containing the phrase (if it is contiguous), or the list of locations at which the phrase is found (if it is discontiguous). We can also store the extracted rules. 9 Whenever a pattern is successfully found, we add all patterns with m + 1 terminals that are prefixed by it\n9Conveniently, the implementation of Chiang (2007) uses a prefix tree grammar encoding, as described in Klein and Manning (2001). Our implementation decorates this tree with additional information required by our algorithms.\nto the frontier for processing in the next iteration. To search for a pattern, we use location information from its parent node, which represents its maximal prefix. Assuming that the node represents phrase \u03b1b, we find the node representing its maximal suffix by following the b-edge from the node pointed to by its parent node\u2019s suffix link. If the node pointed to by this suffix link is inactive, we can mark the node inactive without running a search. When a node is marked inactive, we discontinue search for phrases that are prefixed by the path it represents. The algorithm is illustrated in Figure 2."
        },
        {
            "heading": "5.4.4 Special Cases for Phrases with Gaps",
            "text": "A few subtleties arise in the extraction of hierarchical patterns. Gaps are allowed to occur at the beginning or end of a phrase. For instance, we may have a source phrase Xu or uX or even XuX . Although each of these phrases requires its own path in the prefix tree, they are lexically identical to phrase u. An analogous situation occurs with the patterns XuXv, uXvX , and uXv. There are two cases that we are concerned with.\nThe first case consists of all patterns prefixed with X . The paths to nodes representing these patterns\nwill all contain the X-edge originating at the root node. All of these paths form the shadow subtree. Path construction in this subtree proceeds differently. Because they are lexically identical to their suffixes, they are automatically extended if their suffix paths are active, and they inherit location information of their suffixes.\nThe second case consists of all patterns suffixed with X . Whenever we successfully find a new pattern \u03b1, we automatically extend it with an X edge, provided that \u03b1X is allowed by the model constraints. The node pointed to by this edge inherits its location information from its parent node (representing the maximal prefix \u03b1).\nNote that both special cases occur for patterns in the form XuX ."
        },
        {
            "heading": "5.5 Memory Requirements",
            "text": "As shown in Callison-Burch et al. (2005), we must keep an array for the source text F , its suffix array, the target text E, and alignment A in memory. Assuming that A and E are roughly the size of F , the cost is 4|T |. If we assume that all data use vocabularies that can be represented using 32-bit integers, then our 27M word corpus can easily be represented in around 500MB of memory. Adding the inverted index for unigrams increases this by 20%. The main additional cost in memory comes from the storage of the precomputed collocations. This is dependent both on the corpus size and the number of collocations that we choose to precompute. Using detailed timing data from our experiments we were able to simulate the memory-speed tradeoff (Fig. 3). If we include a trigram model trained on our bitext and the Chinese Gigaword corpus, the overall storage costs for our system are approximately 2GB."
        },
        {
            "heading": "6 Experiments",
            "text": "All of our experiments were performed on ChineseEnglish in the news domain. We used a large training set consisting of over 1 million sentences from various newswire corpora. This corpus is roughly the same as the one used for large-scale experiments by Chiang et al. (2005). To generate alignments, we used GIZA++ (Och and Ney, 2003). We symmetrized bidirectional alignments using the growdiag-final heuristic (Koehn et al., 2003).\n0\n0\n0\n1000\n0\nNumber of frequent subpatterns\nInsert text here\n41 sec/sent\n41 seconds\n405 sec/sent\n0 MB\n725MB\nFigure 3: Effect of precomputation on memory use and processing time. Here we show only the memory requirements of the precomputed collocations.\nWe used the first 50 sentences of the NIST 2003 test set to compute timing results. All of our algorithms were implemented in Python 2.4. 10 Timing results are reported for machines with 8GB of memory and 4 3GHz Xeon processors running Red Hat linux 2.6.9. In order to understand the contributions of various improvements, we also ran the system with with various ablations. In the default setting, the prefix tree is constructed for each sentence to guide phrase lookup, and then discarded. To show the effect of caching we also ran the algorithm without discarding the prefix tree between sentences, resulting in full inter-sentence caching. The results are shown in Table 1. 11\nIt is clear from the results that each of the optimizations is needed to sufficiently reduce lookup time to practical levels. Although this is still relatively slow, it is much closer to the decoding time of 10 seconds per sentence than the baseline.\n10Python is an interpreted language and our implementations do not use any optimization features. It is therefore reasonable to think that a more efficient reimplementation would result in across-the-board speedups.\n11The results shown here do not include the startup time required to load the data structures into memory. In our Python implementation this takes several minutes, which in principle should be amortized over the cost for each sentence. However, just as Zens and Ney (2007) do for phrase tables, we could compile our data structures into binary memory-mapped files, which can be read into memory in a matter of seconds. We are currently investigating this option in a C reimplementation."
        },
        {
            "heading": "7 Conclusions and Future Work",
            "text": "Our work solves a seemingly intractable problem and opens up a number of intriguing potential applications. Both Callison-Burch et al. (2005) and Zhang and Vogel (2005) use suffix arrays to relax the length constraints on phrase-based models. Our work enables this in hierarchical phrase-based models. However, we are interested in additional applications.\nRecent work in discriminative learning for many natural language tasks, such as part-of-speech tagging and information extraction, has shown that feature engineering plays a critical role in these approaches. However, in machine translation most features can still be traced back to the IBM Models of 15 years ago (Lopez, 2007b). Recently, Lopez and Resnik (2006) showed that most of the features used in standard phrase-based models do not help very much. Our algorithms enable us to look up phrase pairs in context, which will allow us to compute interesting contextual features that can be used in discriminative learning algorithms to improve translation accuracy. Essentially, we can use the training data itself as an indirect representation of whatever features we might want to compute. This is not possible with table-based architectures.\nMost of the data structures and algorithms discussed in this paper are widely used in bioinformatics, including suffix arrays, prefix trees, and suffix links (Gusfield, 1997). As discussed in \u00a74.1, our problem is a variant of the approximate pattern matching problem. A major application of approximate pattern matching in bioinformatics is query processing in protein databases for purposes of sequencing, phylogeny, and motif identification.\nCurrent MT models, including hierarchical mod-\nels, translate by breaking the input sentence into small pieces and translating them largely independently. Using approximate pattern matching algorithms, we imagine that machine translation could be treated very much like search in a protein database. In this scenario, the goal is to select training sentences that match the input sentence as closely as possible, under some evaluation function that accounts for both matching and mismatched sequences, as well as possibly other data features. Once we have found the closest sentences we can translate the matched portions in their entirety, replacing mismatches with appropriate word, phrase, or hierarchical phrase translations as needed. This model would bring statistical machine translation closer to convergence with so-called example-based translation, following current trends (Marcu, 2001; Och, 2002). We intend to explore these ideas in future work."
        },
        {
            "heading": "Acknowledgements",
            "text": "I would like to thank Philip Resnik for encouragement, thoughtful discussions and wise counsel; David Chiang for providing the source code for his translation system; and Nitin Madnani, Smaranda Muresan and the anonymous reviewers for very helpful comments on earlier drafts of this paper. Any errors are my own. This research was supported in part by ONR MURI Contract FCPO.810548265 and the GALE program of the Defense Advanced Research Projects Agency, Contract No. HR001106-2-001. Any opinions, findings, conclusions or recommendations expressed in this paper are those of the author and do not necessarily reflect the view of DARPA."
        }
    ],
    "title": "Hierarchical Phrase-Based Translation with Suffix Arrays",
    "year": 2007
}