{
    "abstractText": "Programmable networks have received tremendous attention recently. Apart from exciting network innovations, in-network computing has been explored as a means to accelerate a variety of distributed systems concerns, by leveraging programmable network devices. In this paper, we extend in-network computing to an important class of applications called deep neural network (DNN) serving. In particular, we propose to run DNN inferences in the network data plane in a distributed fashion and make our programmable network a powerful accelerator for DNN serving. We demonstrate the feasibility of this idea through a case study with a real-world DNN on a typical data center network architecture.",
    "authors": [
        {
            "affiliations": [],
            "name": "Kamran Razavi"
        },
        {
            "affiliations": [],
            "name": "George Karlos"
        },
        {
            "affiliations": [],
            "name": "Vinod Nigade"
        },
        {
            "affiliations": [],
            "name": "Max M\u00fchlh\u00e4user"
        },
        {
            "affiliations": [],
            "name": "Lin Wang"
        }
    ],
    "id": "SP:c46be472a71abe0ce0d15a9baf82babb4d4ea53f",
    "references": [
        {
            "authors": [
                "Anurag Agrawal",
                "Changhoon Kim"
            ],
            "title": "Intel tofino2\u2013a 12.9 tbps p4programmable ethernet switch",
            "venue": "IEEE Hot Chips 32 Symposium (HCS). IEEE Computer Society,",
            "year": 2020
        },
        {
            "authors": [
                "Marcel Bl\u00f6cher",
                "Lin Wang",
                "Patrick Eugster",
                "Max Schmidt"
            ],
            "title": "Switches for HIRE: resource scheduling for data center in-network computing",
            "venue": "ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Virtual Event,",
            "year": 2021
        },
        {
            "authors": [
                "Xin Jin",
                "Xiaozhou Li",
                "Haoyu Zhang",
                "Nate Foster",
                "Jeongkeun Lee",
                "Robert Soul\u00e9",
                "Changhoon Kim",
                "Ion Stoica"
            ],
            "title": "NetChain: Scale-Free Sub-RTT Coordination",
            "venue": "In 15th USENIX Symposium onNetworked Systems Design and Implementation,",
            "year": 2018
        },
        {
            "authors": [
                "Xin Jin",
                "Xiaozhou Li",
                "Haoyu Zhang",
                "Robert Soul\u00e9",
                "Jeongkeun Lee",
                "Nate Foster",
                "Changhoon Kim",
                "Ion Stoica"
            ],
            "title": "NetCache: Balancing Key-Value Stores with Fast In-Network Caching",
            "venue": "In Proceedings of the 26th Symposium on Operating Systems Principles,",
            "year": 2017
        },
        {
            "authors": [
                "George Karlos",
                "Henri E. Bal",
                "Lin Wang"
            ],
            "title": "Don\u2019t You Worry \u2019Bout a Packet: Unified Programming for In-Network Computing",
            "venue": "In HotNets \u201921: The 20th ACM Workshop on Hot Topics in Networks,",
            "year": 2021
        },
        {
            "authors": [
                "Daehyeok Kim",
                "Jacob Nelson",
                "Dan R.K. Ports",
                "Vyas Sekar",
                "Srinivasan Seshan"
            ],
            "title": "RedPlane: enabling fault-tolerant stateful in-switch applications",
            "venue": "In ACM SIGCOMM 2021 Conference, Virtual Event,",
            "year": 2021
        },
        {
            "authors": [
                "Alex Krizhevsky"
            ],
            "title": "Learning Multiple Layers of Features from Tiny Images",
            "year": 2009
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Ilya Sutskever",
                "Geoffrey E Hinton"
            ],
            "title": "Imagenet classification with deep convolutional neural networks",
            "venue": "Commun. ACM 60,",
            "year": 2017
        },
        {
            "authors": [
                "ChonLam Lao",
                "Yanfang Le",
                "Kshiteej Mahajan",
                "Yixi Chen",
                "Wenfei Wu",
                "Aditya Akella",
                "Michael M. Swift"
            ],
            "title": "ATP: In-network Aggregation for Multitenant Learning",
            "venue": "In 18th USENIX Symposium on Networked Systems Design and Implementation,",
            "year": 2021
        },
        {
            "authors": [
                "Seulki Lee",
                "Shahriar Nirjon"
            ],
            "title": "SubFlow: A Dynamic Induced-Subgraph Strategy Toward Real-Time DNN Inference and Training",
            "venue": "IEEE Real-Time and Embedded Technology and Applications Symposium (RTAS). 15\u201329",
            "year": 2020
        },
        {
            "authors": [
                "Dan R.K. Ports",
                "Jacob Nelson"
            ],
            "title": "When Should The Network Be The Computer",
            "venue": "In Proceedings of the Workshop on Hot Topics in Operating Systems, HotOS 2019, Bertinoro,",
            "year": 2019
        },
        {
            "authors": [
                "Amedeo Sapio",
                "Marco Canini",
                "Chen-Yu Ho",
                "Jacob Nelson",
                "Panos Kalnis",
                "Changhoon Kim",
                "Arvind Krishnamurthy",
                "Masoud Moshref",
                "Dan R.K. Ports",
                "Peter Richt\u00e1rik"
            ],
            "title": "Scaling Distributed Machine Learning with In-Network Aggregation",
            "venue": "In 18th USENIX Symposium on Networked Systems Design and Implementation,",
            "year": 2021
        },
        {
            "authors": [
                "Haichen Shen",
                "Lequn Chen",
                "Yuchen Jin",
                "Liangyu Zhao",
                "Bingyu Kong",
                "Matthai Philipose",
                "Arvind Krishnamurthy",
                "Ravi Sundaram"
            ],
            "title": "Nexus: a GPU cluster engine for accelerating DNN-based video analysis",
            "venue": "In Proceedings of the 27th ACM Symposium on Operating Systems Principles,",
            "year": 2019
        },
        {
            "authors": [
                "Arjun Singh",
                "Joon Ong",
                "Amit Agarwal",
                "Glen Anderson",
                "Ashby Armistead",
                "Roy Bannon",
                "Seb Boving",
                "Gaurav Desai",
                "Bob Felderman",
                "Paulie Germano"
            ],
            "title": "Jupiter rising: A decade of clos topologies and centralized control in google\u2019s datacenter network",
            "venue": "ACM SIGCOMM computer communication review 45,",
            "year": 2015
        },
        {
            "authors": [
                "Giuseppe Siracusano",
                "Salvator Galea",
                "Davide Sanvito",
                "Mohammad Malekzadeh",
                "Gianni Antichi",
                "Paolo Costa",
                "Hamed Haddadi",
                "Roberto Bifulco"
            ],
            "title": "Rearchitecting Traffic Analysis with Neural Network Interface Cards",
            "venue": "In 19th USENIX Symposium on Networked Systems Design and Implementation,",
            "year": 2022
        },
        {
            "authors": [
                "Tushar Swamy",
                "Alexander Rucker",
                "Muhammad Shahbaz",
                "Ishan Gaur",
                "Kunle Olukotun"
            ],
            "title": "Taurus: a data plane architecture for per-packet ML",
            "venue": "ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Lausanne, Switzerland,",
            "year": 2022
        },
        {
            "authors": [
                "Muhammad Tirmazi",
                "Ran Ben Basat",
                "Jiaqi Gao",
                "Minlan Yu"
            ],
            "title": "Cheetah: Accelerating Database Queries with Switch Pruning",
            "venue": "In Proceedings of the 2020 International Conference on Management of Data, SIGMOD Conference 2020, online conference [Portland, OR,",
            "year": 2020
        },
        {
            "authors": [
                "Zhaoqi Xiong",
                "Noa Zilberman"
            ],
            "title": "Do Switches Dream of Machine Learning?: Toward In-Network Classification",
            "venue": "In Proceedings of the 18th ACMWorkshop on Hot Topics in Networks,",
            "year": 2019
        },
        {
            "authors": [
                "Yifan Yuan",
                "Omar Alama",
                "Amedeo Sapio",
                "Jiawei Fei",
                "Jacob Nelson",
                "Dan R.K. Ports",
                "Marco Canini",
                "Nam Sung Kim"
            ],
            "title": "Unlocking the Power of Inline Floating- Point Operations on Programmable Switches",
            "venue": "In 19th USENIX Symposium on Networked Systems Design and Implementation,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": ""
        },
        {
            "heading": "CCS CONCEPTS",
            "text": "\u2022 Networks \u2192 Programmable networks; In-network processing; Data center networks;"
        },
        {
            "heading": "KEYWORDS",
            "text": "programmable networks, in-network computing, DNN serving"
        },
        {
            "heading": "ACM Reference Format:",
            "text": "Kamran Razavi, George Karlos, Vinod Nigade, Max M\u00fchlh\u00e4user, and Lin Wang. 2022. Distributed DNN Serving in the Network Data Plane. In P4 Workshop in Europe (EuroP4 \u201922), December 9, 2022, Roma, Italy. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3565475.3569079"
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "The emergence of programmable, high-performance network switches and SmartNICs, has not only enabled exciting innovations in networking but also inspired a new computing paradigm called innetwork computing [6, 12].With in-network computing, programmable network devices are instructed to accelerate application components by leveraging the high-throughput, low-latency processing capabilities, and convenient on-path placement of these devices [2]. Example applications that have been proven to benefit from innetwork computing are caching [5], aggregation [10, 13], agreement [4], and database query processing [18].\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. EuroP4 \u201922, December 9, 2022, Roma, Italy \u00a9 2022 Copyright held by the owner/author(s). Publication rights licensed to the Association for Computing Machinery. ACM ISBN 978-1-4503-9935-7/22/12. . . $15.00 https://doi.org/10.1145/3565475.3569079\nDeep learning has become the de facto approach for various inference tasks such as object detection and speech recognition. With the widespread adoption of deep learning, it becomes critical that we can serve DNNs with high performance in terms of both throughput and latency. DNN serving is usually done preferably with high-end accelerators like GPUs/TPUs over CPUs due to higher efficiency and lower costs [14]. GPUs/TPUs typically employ batching to improve throughput, at the cost of increased inference latency [14].\nConsidering both the fast development of programmable network devices and the high demand for DNN serving, we ask a bold question: Can we leverage a programmable network to perform DNN serving? Given that a modern Tofino2 switch can process packets with nanosecond latency, and at the rate of billions of packets per second [1], DNN serving would achieve an unprecedented level of performance if the DNN can be executed entirely in the data plane of the programmable network. Note that with this design the inference can be performed \u201con the fly\u201d while transferring DNN input data on the network, eliminating the need of accelerators.\nPrior work has explored the intersection of programmable networks and machine learning. For example, in-network aggregation has been used to accelerate the gradient synchronization in dataparallel DNN training [10, 13]. Other work has explored data-plane packet classification by running per-packet inference tasks, like decision trees, SVMs and small (binary) neural networks, on programmable switches and SmartNICs [16, 19]. Confined to a single device, such approaches limit the size of the supported ML models, and work towards addressing this issue only involves new hardware architectures [17]. So far, and to the best of our knowledge, none of these efforts support the serving of large DNNs (models with millions of weights) across a network of programmable network devices targeting user applications.\nIn this paper, we propose an in-network system for fast, endto-end DNN serving by distributing a DNN across a network of programmable switches, as depicted in Figure 1. Our inspiration stems from the observation that DNNs are dataflow computations similar to how packets flow through a network. Based on that, we 1 map the neurons in the DNN to the physical network switches, 2 craft and route packets carrying the input/intermediate data to go through the switches containing the corresponding neurons, and 3 instruct each switch to perform the computations specified by the neurons assigned to the switch. In the rest of this paper, we demonstrate the feasibility of this idea through a case study with a real-world DNN and a typical data center network architecture. We also discuss further challenges."
        },
        {
            "heading": "2 A CASE STUDY WITH MINI-ALEXNET",
            "text": "We use a simplified version of AlexNet [9] (mini-AlexNet) used in [11] and trained on CIFAR-10 [8] with three dimensions (height, width, and channel). We map the mini-AlexNet network to a set of programmable switches using the data center network architecture (spine/leaf) adapted from the Google infrastructure [15].\nThe mini-AlexNet neural network is described in Table 1. It consists of an input layer and three convolutional layers followed by three fully connected layers. The convolutional layers are composed of convolutional filters, ReLU activation functions, and 2D Max Pooling (2x2) layers. Due to the fact that the state-of-the-art programmable switches (Barefoot Tofino2 [3]) are not equipped with Floating Point Units (FPUs), we store the inputs and weights in 8-bit integers. To avoid multiplication and aggregation overflow, we can increase the input values to 32-bit integers as it does not affect the number of operations and the cost of storing results is either temporary, or minuscule compared to weights. Previous work has shown that fixed-point arithmetic is faster than floating point equivalent accuracy, and 8-bit precision is sufficient for DNN inference [17].\nDNN-switch mapping. The data center network architecture we use is depicted in Figure 2. It consists of four programmable switches as spines and eight programmable switches as leaves. Each leaf switch is connected to all spine switches with 400GbE links. The spine/leaf architecture advantage is that any leaf (spine) switch is connected to any other leaf (spine) switch in exactly two hops. We use this property to map the layers of the DNN to just leaf switches (one layer is mapped to one switch) to avoid stragglers in the synchronization.\nThe Tofino2 switchwe consider here has four processing pipelines, each equipped with eight 400GbE ports supporting up to 12.8Tb/s of aggregate throughput. To leverage the processing power of all four pipelines of each switch, we distribute the neurons of the same layer to all the pipelines on the same switch (as the neurons in the same layer do not need to communicate with each other) evenly based on the layer type: For the convolutional layer we divide the number of filters by the number of switch pipelines and allocate the convolutional filters to the respective pipelines. For the dense layer, we divide the number of dense neurons by the number of switch\npipelines and store the dense neurons\u2019 weights in the respective pipelines.\nIn the mini-AlexNet scenario, after we get the input from the client (on S1), we allocate and store 16 convolutional filters to each pipeline, as the first convolutional layer (on S2) has 64 filters. After the on-switch execution (detailed later), the packets for the next layer are generated (detailed later) and are sent to the next convolutional layer (on S3). We follow the same procedure until we reach the first dense layer (on S5) with 4096 neurons. We divide the dense layer into four parts and allocate each part containing 1024 neurons to each of the pipelines and perform the neuron computations until we reach the output layer (on S7), where we obtain the prediction result and send it back to the client.\nPacket generation and routing. Once the DNN has been partitioned and the neurons deployed on the switches, the next step is to generate packets and route them on the network following the DNN dataflow. For the input layer, the input data is encapsulated into as many packets as are required (based on the input and the packet header size) and is multicast to the switch\u2019s pipelines (as the neurons are distributed among all pipelines) hosting neurons that are directly connected to that layer. Upon completing its computations, a layer will emit packets encapsulating the input for the next layer to the spine switches, and these packets will be multicast to the next associated switch hosting the next layer. Each switch maintains a forwarding table applying the above logic to route the packets inside the network. Each packet carries a label through which its target layer can be identified. The switch multicasts the packet based on the label to all the pipelines of the next switch. Upon a packet\u2019s arrival, the switch knows the computations to apply to the data carried by that particular packet.\nIn the mini-AlexNet scenario, the input switch (S1) multicasts the input to all the spine switches (with recirculation), and the switches in the spine layer redirect the packets to the first convolutional layer (on S2). After the processing of the first convolutional layer is done, each pipeline has 1/4 of the input for the next layer. With the spine/leaf design, we transfer each input segment to the spine switches, and there we multicast the input segments to all four pipelines of the next switch (on S3). Each pipeline in S3 receives packets from a switch in the spine layer, shaping the current layer\u2019s input. We follow the same procedure until we reach the output layer, where we send back the final prediction.\nOn-switch execution. Switches will perform computations for the neurons they host upon packet arrivals. More specifically, as each pipeline of the switch gets the entire input, we perform the computations based on the layer type. If the layer is a convolutional layer, we apply the filter to a subset of the input and store the result\nof the dot product. If it is a dense layer, we multiply all the input values by all the weights and aggregate their results.\nWe decompose each multiplication into a number of shifts. The input is shifted left by \ud835\udc56 if the \ud835\udc56-th bit of the 8-bit weight is set, otherwise by 0. All shifts are performed in parallel, in a single stage, and the intermediate results are stored in temporaries. This step takes one stage, assuming predicate instructions (to check if the \ud835\udc56-th bit is set), otherwise two. Then, a reduction step aggregates the intermediate results. Since we use 8-bit weights, this step takes three stages. Given \ud835\udc41 free ALUs in the first stage, we can perform \ud835\udc41 /8 multiplications in parallel, each of which has a maximum depth of five stages. This allows us to replicate the process a number of times without recirculation. The (intermediate) result of each multiplication is accumulated to a register, with a cost, in terms of stages, logarithmic to the number of multiplications performed. If the dot product requires more multiplications, the packet is recirculated.\nThen we apply the ReLU activation function to the result of the dot product by checking whether the most significant bit (MSB) is 1 (sign bit) and replacing the value with 0. For the max pooling part, we check whether the current value is the last piece of the pooling window. A major challenge here is to find the pooling elements due to the fact that there is no mod operation in the switches available. To avoid this issue, we process the inputs in the order of the max pooling window. For a simple 2 \u00d7 2 pooling window, after we processed all four window\u2019s values, we get the maximum of them in four cycles (three comparisons in total; two cycles for the first two comparisons in parallel and storing the result and two cycles for the second stage comparison). To meet the promised 12.8Tb/s throughput, Tofino2 allows a limited number of operations per packet traversal (few 10\u2019s of multiplications like the one described above). Therefore, we need to recirculate to process all the inputs for all the filters in the same pipeline. For each filter/neuron on a switch, the switch accumulates the multiplication results and maintains a counter to ensure that packets from all weights have been processed before emitting the result as a packet to the downstream layers.\nIn Table 1 we calculate the number of operations and memory requirements for each layer of mini-AlexNet. Even the most memory hungry of the layers (Layer 6) requires less than 10% of the available memory on the Tofino2 (a couple of hundred of MBs). However, the number of operations greatly exceeds the 10\u2019s of operations we can perform in one traversal. To solve this issue, we recirculate the input in the switch with a new set of operations until all the required operations are done. In the most computation-intensive layer (Layer 3), each pipeline needs to compute roughly 10, 000, 000 operations,\nwe need to recirculate the same input less than 1, 000, 000 times. The packet recirculating comes with a latency cost similar to parsing another packet. Tofino2 can potentially process six billion packets (1.5 billion packets per pipeline), resulting in less than 1ms to process even the most computation-intensive layer in our scenario. In total, a set of available switches in the data centers require less than 2\ud835\udc5a\ud835\udc60 to perform inference in the mini-AlexNet scenario. Compared to the evaluations reported in [11], our in-network DNN serving system not only reduces the inference latency by over 2\u00d7 and 2.5\u00d7 compared to CPU and GPU, respectively but also eliminates the necessity of having inference servers."
        },
        {
            "heading": "3 CHALLENGES AND DISCUSSION",
            "text": "Accuracy improvement. Floating point addition on programmable switches has been carefully explored in [20] while other more complex arithmetic operations are still not feasible with the current switch design. Currently, we use 8-bit fixed point weights as we are still not able to implement the floating point multiplication operations in the data plane respecting the memory access limitation of Tofino2 switches. We plan to explore how packet re-circulation can help work around this limitation. Support for more complex layer types. So far, we have only discussed how to handle DNNs with convolutional and dense layers. However, popular DNNs typically involve a variety of layer types with more complex structures and activation functions, calling for a careful design of the data structure. There are also layers with nonlinear activation functions like tanh and sigmoid, which are currently hard to perform on programmable switches. Fault tolerance. Switches and links can fail, and ensuring that the final prediction is generated without being affected by such failures is essential. Also, a packet loss between switches could render a DNN execution stagnation due to the use of the per-neuron counter. We acknowledge that achieving reliability for stateful in-network computing like DNN serving is a big challenge, which has not been extensively studied yet [7]. We leave this for future work."
        },
        {
            "heading": "ACKNOWLEDGEMENTS",
            "text": "This work has been partially funded by the German Research Foundation (DFG) within the Collaborative Research Center (CRC) 1053 MAKI, the Dutch Research Council (NWO) Open Competition Domain Science XS Grant 12611, and Google Research."
        }
    ],
    "title": "Distributed DNN Serving in the Network Data Plane",
    "year": 2022
}