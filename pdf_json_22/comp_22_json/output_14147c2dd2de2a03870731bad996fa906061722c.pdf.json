{
    "abstractText": "Word translation or bilingual lexicon induction (BLI) is a key cross-lingual task, aiming to bridge the lexical gap between different languages. In this work, we propose a robust and effective two-stage contrastive learning framework for the BLI task. At Stage C1, we propose to refine standard cross-lingual linear maps between static word embeddings (WEs) via a contrastive learning objective; we also show how to integrate it into the self-learning procedure for even more refined cross-lingual maps. In Stage C2, we conduct BLI-oriented contrastive fine-tuning of mBERT, unlocking its word translation capability. We also show that static WEs induced from the \u2018C2-tuned\u2019 mBERT complement static WEs from Stage C1. Comprehensive experiments on standard BLI datasets for diverse languages and different experimental setups demonstrate substantial gains achieved by our framework. While the BLI method from Stage C1 already yields substantial gains over all state-of-the-art BLI methods in our comparison, even stronger improvements are met with the full two-stage framework: e.g., we report gains for 112/112 BLI setups, spanning 28 language pairs.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yaoyiran Li"
        },
        {
            "affiliations": [],
            "name": "Fangyu Liu"
        },
        {
            "affiliations": [],
            "name": "Nigel Collier"
        },
        {
            "affiliations": [],
            "name": "Anna Korhonen"
        }
    ],
    "id": "SP:1eee99339be28ae99317cb8f8eddd58503e0f910",
    "references": [
        {
            "authors": [
                "Ekin Akyurek",
                "Jacob Andreas."
            ],
            "title": "Lexicon learning for few shot sequence modeling",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Pro-",
            "year": 2021
        },
        {
            "authors": [
                "Alan Ansell",
                "Edoardo Maria Ponti",
                "Anna Korhonen",
                "Ivan Vuli\u0107."
            ],
            "title": "Composable sparse fine-tuning for cross-lingual transfer",
            "venue": "CoRR, abs/2110.07560.",
            "year": 2021
        },
        {
            "authors": [
                "Mikel Artetxe",
                "Gorka Labaka",
                "Eneko Agirre."
            ],
            "title": "A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL\u201918),",
            "year": 2018
        },
        {
            "authors": [
                "Mikel Artetxe",
                "Sebastian Ruder",
                "Dani Yogatama",
                "Gorka Labaka",
                "Eneko Agirre."
            ],
            "title": "A call for more rigor in unsupervised cross-lingual learning",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL\u201920),",
            "year": 2020
        },
        {
            "authors": [
                "Piotr Bojanowski",
                "Edouard Grave",
                "Armand Joulin",
                "Tomas Mikolov"
            ],
            "title": "Enriching word vectors with",
            "year": 2017
        },
        {
            "authors": [
                "Fredrik Carlsson",
                "Amaru Cuba Gyllensten",
                "Evangelia Gogoulou",
                "Erik Ylip\u00e4\u00e4 Hellqvist",
                "Magnus Sahlgren."
            ],
            "title": "Semantic re-tuning with contrastive tension",
            "venue": "Proceedings of the International Conference on Learning Representations (ICLR\u201921).",
            "year": 2021
        },
        {
            "authors": [
                "Ethan C. Chau",
                "Noah A. Smith."
            ],
            "title": "Specializing multilingual language models: An empirical study",
            "venue": "Proceedings of the 1st Workshop on Multilingual Representation Learning, pages 51\u201361, Punta Cana, Dominican Republic. Association for Compu-",
            "year": 2021
        },
        {
            "authors": [
                "Alexandra Chronopoulou",
                "Dario Stojanovski",
                "Alexander Fraser."
            ],
            "title": "Improving the lexical ability of pretrained language models for unsupervised neural machine translation",
            "venue": "Proceedings of the 2021 Conference of the North American Chap-",
            "year": 2021
        },
        {
            "authors": [
                "Sarkar Snigdha Sarathi Das",
                "Arzoo Katiyar",
                "Rebecca J Passonneau",
                "Rui Zhang."
            ],
            "title": "Container: Fewshot named entity recognition via contrastive learning",
            "venue": "arXiv preprint arXiv:2109.07589.",
            "year": 2021
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Sumanth Doddapaneni",
                "Gowtham Ramesh",
                "Anoop Kunchukuttan",
                "Pratyush Kumar",
                "Mitesh M. Khapra."
            ],
            "title": "A primer on pretrained multilingual language models",
            "venue": "CoRR, abs/2107.00676.",
            "year": 2021
        },
        {
            "authors": [
                "Xiangyu Duan",
                "Baijun Ji",
                "Hao Jia",
                "Min Tan",
                "Min Zhang",
                "Boxing Chen",
                "Weihua Luo",
                "Yue Zhang."
            ],
            "title": "Bilingual dictionary based neural machine translation without using parallel sentences",
            "venue": "Proceedings of the 58th Annual Meeting of the Associa-",
            "year": 2020
        },
        {
            "authors": [
                "Haim Dubossarsky",
                "Ivan Vuli\u0107",
                "Roi Reichart",
                "Anna Korhonen."
            ],
            "title": "The secret is in the spectra: Predicting cross-lingual task performance with spectral similarity measures",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Lan-",
            "year": 2020
        },
        {
            "authors": [
                "Philipp Dufter",
                "Hinrich Sch\u00fctze."
            ],
            "title": "Identifying elements essential for BERT\u2019s multilinguality",
            "venue": "Proceedings of the 2020 Conference on",
            "year": 2020
        },
        {
            "authors": [
                "Ashwinkumar Ganesan",
                "Francis Ferraro",
                "Tim Oates."
            ],
            "title": "Learning a reversible embedding mapping using bi-directional manifold alignment",
            "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3132\u20133139, On-",
            "year": 2021
        },
        {
            "authors": [
                "Tianyu Gao",
                "Xingcheng Yao",
                "Danqi Chen."
            ],
            "title": "SimCSE: Simple contrastive learning of sentence embeddings",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP\u201921), pages 6894\u20136910, Online",
            "year": 2021
        },
        {
            "authors": [
                "Eric Gaussier",
                "J.M. Renders",
                "I. Matveeva",
                "C. Goutte",
                "H. Dejean."
            ],
            "title": "A geometric view on bilingual lexicon extraction from comparable corpora",
            "venue": "Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL\u201904), pages",
            "year": 2004
        },
        {
            "authors": [
                "John Giorgi",
                "Osvald Nitski",
                "Bo Wang",
                "Gary Bader."
            ],
            "title": "DeCLUTR: Deep contrastive learning for unsupervised textual representations",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th Interna-",
            "year": 2021
        },
        {
            "authors": [
                "Goran Glava\u0161",
                "Robert Litschko",
                "Sebastian Ruder",
                "Ivan Vuli\u0107."
            ],
            "title": "How to (properly) evaluate crosslingual word embeddings: On strong baselines, comparative analyses, and some misconceptions",
            "venue": "Proceedings of the 57th Annual Meeting of the Associa-",
            "year": 2019
        },
        {
            "authors": [
                "Goran Glava\u0161",
                "Ivan Vuli\u0107."
            ],
            "title": "Non-linear instance-based cross-lingual mapping for nonisomorphic embedding spaces",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL\u201920), pages 7548\u20137555,",
            "year": 2020
        },
        {
            "authors": [
                "Hila Gonen",
                "Shauli Ravfogel",
                "Yanai Elazar",
                "Yoav Goldberg."
            ],
            "title": "It\u2019s not Greek to mBERT: Inducing word-level translations from multilingual BERT",
            "venue": "Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for",
            "year": 2020
        },
        {
            "authors": [
                "Geert Heyman",
                "Ivan Vuli\u0107",
                "Marie-Francine Moens."
            ],
            "title": "Bilingual lexicon induction by learning to combine word-level and character-level representations",
            "venue": "Proceedings of the 15th Conference of the",
            "year": 2017
        },
        {
            "authors": [
                "Geert Heyman",
                "Ivan Vuli\u0107",
                "Marie-Francine Moens."
            ],
            "title": "A deep learning approach to bilingual lexicon induction in the biomedical domain",
            "venue": "BMC Bioinformatics, 19(1):259:1\u2013259:15.",
            "year": 2018
        },
        {
            "authors": [
                "Yedid Hoshen",
                "Lior Wolf."
            ],
            "title": "Non-adversarial unsupervised word translation",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP\u201918), pages 469\u2013 478, Brussels, Belgium. Association for Computa-",
            "year": 2018
        },
        {
            "authors": [
                "Ann Irvine",
                "Chris Callison-Burch."
            ],
            "title": "A comprehensive analysis of bilingual lexicon induction",
            "venue": "Computational Linguistics, 43(2):273\u2013310.",
            "year": 2017
        },
        {
            "authors": [
                "Pratik Jawanpuria",
                "Arjun Balgovind",
                "Anoop Kunchukuttan",
                "Bamdev Mishra."
            ],
            "title": "Learning multilingual word embeddings in latent metric space: A geometric approach",
            "venue": "Transactions of the Association for Computational Linguistics,",
            "year": 2019
        },
        {
            "authors": [
                "Armand Joulin",
                "Piotr Bojanowski",
                "Tomas Mikolov",
                "Herv\u00e9 J\u00e9gou",
                "Edouard Grave."
            ],
            "title": "Loss in translation: Learning bilingual word mapping with a retrieval criterion",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Lan-",
            "year": 2018
        },
        {
            "authors": [
                "David Kamholz",
                "Jonathan Pool",
                "Susan Colowick."
            ],
            "title": "PanLex: Building a resource for panlingual lexical translation",
            "venue": "Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC\u201914), pages 3145\u2013",
            "year": 2014
        },
        {
            "authors": [
                "Mladen Karan",
                "Ivan Vuli\u0107",
                "Anna Korhonen",
                "Goran Glava\u0161."
            ],
            "title": "Classification-based self-learning for weakly supervised bilingual lexicon induction",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL\u201920),",
            "year": 2020
        },
        {
            "authors": [
                "Guillaume Lample",
                "Alexis Conneau."
            ],
            "title": "Cross-lingual language model pretraining",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS\u201919).",
            "year": 2019
        },
        {
            "authors": [
                "Guillaume Lample",
                "Alexis Conneau",
                "Marc\u2019Aurelio Ranzato",
                "Ludovic Denoyer",
                "Herv\u00e9 J\u00e9gou"
            ],
            "title": "Word translation without parallel data",
            "venue": "In Proceedings of the International Conference on Learning Representations (ICLR\u201918)",
            "year": 2018
        },
        {
            "authors": [
                "Jind\u0159ich Libovick\u00fd",
                "Rudolf Rosa",
                "Alexander Fraser."
            ],
            "title": "On the language neutrality of pre-trained multilingual representations",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1663\u20131674, Online. Association for Computa-",
            "year": 2020
        },
        {
            "authors": [
                "Fangyu Liu",
                "Ehsan Shareghi",
                "Zaiqiao Meng",
                "Marco Basaldella",
                "Nigel Collier."
            ],
            "title": "Selfalignment pretraining for biomedical entity representations",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association",
            "year": 2021
        },
        {
            "authors": [
                "Fangyu Liu",
                "Ivan Vuli\u0107",
                "Anna Korhonen",
                "Nigel Collier."
            ],
            "title": "Fast, effective, and self-supervised: Transforming masked language models into universal lexical and sentence encoders",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natu-",
            "year": 2021
        },
        {
            "authors": [
                "Qianchu Liu",
                "Fangyu Liu",
                "Nigel Collier",
                "Anna Korhonen",
                "Ivan Vuli\u0107."
            ],
            "title": "MirrorWiC: On eliciting word-in-context representations from pretrained language models",
            "venue": "Proceedings of the 25th Conference on Computational Natural Language Learn-",
            "year": 2021
        },
        {
            "authors": [
                "Yixin Liu",
                "Pengfei Liu."
            ],
            "title": "SimCLS: A simple framework for contrastive learning of abstractive summarization",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference",
            "year": 2021
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "Proceedings of the International Conference on Learning Representations (ICLR\u201919).",
            "year": 2019
        },
        {
            "authors": [
                "Tom\u00e1s Mikolov",
                "Quoc V. Le",
                "Ilya Sutskever."
            ],
            "title": "Exploiting similarities among languages for machine translation",
            "venue": "ArXiv preprint, abs/1309.4168.",
            "year": 2013
        },
        {
            "authors": [
                "Tasnim Mohiuddin",
                "M Saiful Bari",
                "Shafiq Joty."
            ],
            "title": "LNMap: Departures from isomorphic assumption in bilingual lexicon induction through nonlinear mapping in latent space",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natu-",
            "year": 2020
        },
        {
            "authors": [
                "Tasnim Mohiuddin",
                "Shafiq Joty."
            ],
            "title": "Revisiting adversarial autoencoder for unsupervised word translation with cycle consistency and improved training",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computa-",
            "year": 2019
        },
        {
            "authors": [
                "Kevin Musgrave",
                "Serge J. Belongie",
                "Ser-Nam Lim."
            ],
            "title": "A metric learning reality check",
            "venue": "Proceedings of the European Conference on Computer Vision (ECCV\u201920), pages 681\u2013699.",
            "year": 2020
        },
        {
            "authors": [
                "Arbi Haza Nasution",
                "Yohei Murakami",
                "Toru Ishida."
            ],
            "title": "Plan optimization to bilingual dictionary induction for low-resource language families",
            "venue": "ACM Transactions on Asian and Low-Resource Language Information Processing, 20(2):29:1\u201329:28.",
            "year": 2021
        },
        {
            "authors": [
                "Aaron van den Oord",
                "Yazhe Li",
                "Oriol Vinyals."
            ],
            "title": "Representation learning with contrastive predictive coding",
            "venue": "ArXiv preprint, abs/1807.03748.",
            "year": 2018
        },
        {
            "authors": [
                "Barun Patra",
                "Joel Ruben Antony Moniz",
                "Sarthak Garg",
                "Matthew R. Gormley",
                "Graham Neubig."
            ],
            "title": "Bilingual lexicon induction with semi-supervision in non-isometric embedding spaces",
            "venue": "Proceedings of the 57th Annual Meeting of the Association",
            "year": 2019
        },
        {
            "authors": [
                "Ye Qi",
                "Devendra Sachan",
                "Matthieu Felix",
                "Sarguna Padmanabhan",
                "Graham Neubig"
            ],
            "title": "When and why are pre-trained word embeddings useful for neural machine translation",
            "venue": "In Proceedings of the 2018 Conference of the North American Chapter",
            "year": 2018
        },
        {
            "authors": [
                "Reinhard Rapp."
            ],
            "title": "Identifying word translations in non-parallel texts",
            "venue": "33rd Annual Meeting of the Association for Computational Linguistics, pages 320\u2013 322, Cambridge, Massachusetts, USA. Association for Computational Linguistics.",
            "year": 1995
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "SentenceBERT: Sentence embeddings using Siamese BERTnetworks",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Nat-",
            "year": 2019
        },
        {
            "authors": [
                "Shuo Ren",
                "Shujie Liu",
                "Ming Zhou",
                "Shuai Ma."
            ],
            "title": "A graph-based coarse-to-fine method for unsupervised bilingual lexicon induction",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL\u201920), pages 3476\u20133485,",
            "year": 2020
        },
        {
            "authors": [
                "Sebastian Ruder",
                "Ivan Vuli\u0107",
                "Anders S\u00f8gaard."
            ],
            "title": "A survey of cross-lingual word embedding models",
            "venue": "Journal of Artificial Inteligence Research, 65:569\u2013631.",
            "year": 2019
        },
        {
            "authors": [
                "Vin Sachidananda",
                "Ziyi Yang",
                "Chenguang Zhu."
            ],
            "title": "Filtered inner product projection for crosslingual embedding alignment",
            "venue": "Proceedings of the International Conference on Learning Representations (ICLR\u201921).",
            "year": 2021
        },
        {
            "authors": [
                "Peter H Sch\u00f6nemann."
            ],
            "title": "A generalized solution of the orthogonal Procrustes problem",
            "venue": "Psychometrika, 31(1):1\u201310.",
            "year": 1966
        },
        {
            "authors": [
                "Haoyue Shi",
                "Luke Zettlemoyer",
                "Sida I. Wang."
            ],
            "title": "Bilingual lexicon induction via unsupervised bitext construction and word alignment",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International",
            "year": 2021
        },
        {
            "authors": [
                "Anders S\u00f8gaard",
                "Sebastian Ruder",
                "Ivan Vuli\u0107."
            ],
            "title": "On the limitations of unsupervised bilingual dictionary induction",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL\u201918), pages 778\u2013788, Melbourne,",
            "year": 2018
        },
        {
            "authors": [
                "Laurens van der Maaten",
                "Geoffrey E. Hinton."
            ],
            "title": "Visualizing non-metric similarities in multiple maps",
            "venue": "Machine Learning, 87(1):33\u201355.",
            "year": 2012
        },
        {
            "authors": [
                "Thomas Viklands."
            ],
            "title": "Algorithms for the weighted orthogonal Procrustes problem and other least squares problems",
            "venue": "Ph.D. thesis, Datavetenskap.",
            "year": 2006
        },
        {
            "authors": [
                "Ivan Vuli\u0107",
                "Simon Baker",
                "Edoardo Maria Ponti",
                "Ulla Petti",
                "Ira Leviant",
                "Kelly Wing",
                "Olga Majewska",
                "Eden Bar",
                "Matt Malone",
                "Thierry Poibeau",
                "Roi Reichart",
                "Anna Korhonen"
            ],
            "title": "2020a. Multi-SimLex: A largescale evaluation of multilingual and crosslingual lex",
            "year": 2020
        },
        {
            "authors": [
                "Ivan Vuli\u0107",
                "Goran Glava\u0161",
                "Roi Reichart",
                "Anna Korhonen"
            ],
            "title": "Do we really need fully unsupervised cross-lingual embeddings",
            "venue": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Interna-",
            "year": 2019
        },
        {
            "authors": [
                "Ivan Vuli\u0107",
                "Edoardo Maria Ponti",
                "Anna Korhonen",
                "Goran Glava\u0161."
            ],
            "title": "LexFit: Lexical fine-tuning of pretrained language models",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint",
            "year": 2021
        },
        {
            "authors": [
                "Ivan Vuli\u0107",
                "Edoardo Maria Ponti",
                "Robert Litschko",
                "Goran Glava\u0161",
                "Anna Korhonen."
            ],
            "title": "Probing pretrained language models for lexical semantics",
            "venue": "Proceedings of the 2020 Conference on",
            "year": 2020
        },
        {
            "authors": [
                "Shufan Wang",
                "Laure Thompson",
                "Mohit Iyyer."
            ],
            "title": "Phrase-BERT: Improved phrase embeddings from BERT with an application to corpus exploration",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Process-",
            "year": 2021
        },
        {
            "authors": [
                "Linting Xue",
                "Noah Constant",
                "Adam Roberts",
                "Mihir Kale",
                "Rami Al-Rfou",
                "Aditya Siddhant",
                "Aditya Barua",
                "Colin Raffel."
            ],
            "title": "mT5: A massively multilingual pre-trained text-to-text transformer",
            "venue": "Proceedings of the 2021 Conference of the North",
            "year": 2021
        },
        {
            "authors": [
                "Michelle Yuan",
                "Mozhi Zhang",
                "Benjamin Van Durme",
                "Leah Findlater",
                "Jordan Boyd-Graber."
            ],
            "title": "Interactive refinement of cross-lingual word embeddings",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Process-",
            "year": 2020
        },
        {
            "authors": [
                "Jinpeng Zhang",
                "Baijun Ji",
                "Nini Xiao",
                "Xiangyu Duan",
                "Min Zhang",
                "Yangbin Shi",
                "Weihua Luo."
            ],
            "title": "Combining static word embeddings and contextual representations for bilingual lexicon induction",
            "venue": "Findings of the Association for Computational Lin-",
            "year": 2021
        },
        {
            "authors": [
                "Glava\u0161"
            ],
            "title": "2019), are available in Table 9, and can be seen as additional evidence which supports the claims from the main paper (see \u00a74.1) G Appendix: Translation Examples",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction and Motivation",
            "text": "Bilingual lexicon induction (BLI) or word translation is one of the seminal and long-standing tasks in multilingual NLP (Rapp, 1995; Gaussier et al., 2004; Heyman et al., 2017; Shi et al., 2021, inter alia). Its main goal is learning translation correspondences across languages, with applications of BLI ranging from language learning and acquisition (Yuan et al., 2020; Akyurek and Andreas, 2021) to machine translation (Qi et al., 2018; Duan et al., 2020; Chronopoulou et al., 2021) and the development of language technology in low-resource languages and domains (Irvine and Callison-Burch, 2017; Heyman et al., 2018). A large body of recent BLI work has focused on the so-called mappingbased methods (Mikolov et al., 2013; Artetxe et al.,\n2018; Ruder et al., 2019).1 Such methods are particularly suitable for low-resource languages and weakly supervised learning setups: they support BLI with only as much as few thousand word translation pairs (e.g., 1k or at most 5k) as the only bilingual supervision (Ruder et al., 2019).2\nUnlike for many other tasks in multilingual NLP (Doddapaneni et al., 2021; Chau and Smith, 2021; Ansell et al., 2021), state-of-the-art (SotA) BLI results are still achieved via static word embeddings (WEs) (Vulic\u0301 et al., 2020b; Liu et al., 2021b). A typical modus operandi of mapping-based approaches is to first train monolingual WEs independently on monolingual corpora and then map them to a shared cross-lingual space via linear (Mikolov et al., 2013;\n1They are also referred to as projection-based or alignmentbased methods (Glava\u0161 et al., 2019; Ruder et al., 2019).\n2In the extreme, fully unsupervised mapping-based BLI methods can leverage monolingual data only without any bilingual supervision (Lample et al., 2018; Artetxe et al., 2018; Hoshen and Wolf, 2018; Mohiuddin and Joty, 2019; Ren et al., 2020, inter alia). However, comparative empirical analyses (Vulic\u0301 et al., 2019) show that, with all other components equal, using seed sets of only 500-1,000 translation pairs, always outperforms fully unsupervised BLI methods. Therefore, in this work we focus on this more pragmatic (weakly) supervised BLI setup (Artetxe et al., 2020); we assume the existence of at least 1,000 seed translations per each language pair.\nar X\niv :2\n20 3.\n08 30\n7v 3\n[ cs\n.C L\n] 2\n6 M\nar 2\n02 2\nGlava\u0161 et al., 2019) or non-linear mapping functions (Mohiuddin et al., 2020). In order to achieve even better results, many BLI methods also apply a self-learning loop where training dictionaries are iteratively (and gradually) refined, and improved mappings are then learned in each iteration (Artetxe et al., 2018; Karan et al., 2020). However, there is still ample room for improvement, especially for lower-resource languages and dissimilar language pairs (Vulic\u0301 et al., 2019; Nasution et al., 2021).\nOn the other hand, another line of recent research has demonstrated that a wealth of lexical semantic information is encoded in large multilingual pretrained language models (LMs) such as mBERT (Devlin et al., 2019), but 1) it is not straightforward to transform the LMs into multilingual lexical encoders (Liu et al., 2021b), 2) extract word-level information from them (Vulic\u0301 et al., 2020b, 2021), and 3) word representations extracted from these LMs still cannot surpass static WEs in the BLI task (Vulic\u0301 et al., 2020b; Zhang et al., 2021). Motivated by these insights, in this work we investigate the following research questions:\n(RQ1) Can we further improve (weakly supervised) mapping-based BLI methods based on static WEs? (RQ2) How can we extract more useful crosslingual word representations from pretrained multilingual LMs such as mBERT or mT5? (RQ3) Is it possible to boost BLI by combining cross-lingual representations based on static WEs and the ones extracted from multilingual LMs?\nInspired by the wide success of contrastive learning techniques in sentence-level representation learning (Reimers and Gurevych, 2019; Carlsson et al., 2021; Gao et al., 2021), we propose a twostage contrastive learning framework for effective word translation in (weakly) supervised setups; it leverages and combines multilingual knowledge from static WEs and pretrained multilingual LMs. Stage C1 operates solely on static WEs: in short, it is a mapping-based approach with self-learning, where in each step we additionally fine-tune linear maps with contrastive learning that operates on gradually refined positive examples (i.e., true translation pairs), and hard negative samples. Stage C2 fine-tunes a pretrained multilingual LM (e.g., mBERT), again with a contrastive learning objective, using positive examples as well as negative examples extracted from the output of C1. Finally, we extract word representations from the multilingual LM fine-tuned in Stage C2, and combine them\nwith static cross-lingual WEs from Stage C1; the combined representations are then used for BLI.\nWe run a comprehensive set of BLI experiments on the standard BLI benchmark (Glava\u0161 et al., 2019), comprising 8 diverse languages, in several setups. Our results indicate large gains over state-of-the-art BLI models: e.g.,\u2248+8 Precision@1 points on average, +10 points for many language pairs, gains for 107/112 BLI setups already after Stage C1 (cf., RQ1), and for all 112/112 BLI setups after Stage C2 (cf., RQ2 and RQ3). Moreover, our findings also extend to BLI for lowerresource languages from another BLI benchmark (Vulic\u0301 et al., 2019). Finally, as hinted in recent work (Zhang et al., 2021), our findings validate that multilingual lexical knowledge in LMs, when exposed and extracted as in our contrastive learning framework, can complement the knowledge in static cross-lingual WEs (RQ3), and benefit BLI. We release the code and share the data at: https: //github.com/cambridgeltl/ContrastiveBLI."
        },
        {
            "heading": "2 Methodology",
            "text": "Preliminaries and Task Formulation. In BLI, we assume two vocabularies X={wx1 , . . . , wx|X |} andY={wy1 , . . . , w y |Y|} associated with two respective languages Lx and Ly. We also assume that each vocabulary word is assigned its (static) typelevel word embedding (WE); that is, the respective WE matrices for each vocabulary are X\u2208R|X |\u00d7d, Y \u2208R|Y|\u00d7d. Each WE is a d-dim row vector, with typical values d=300 for static WEs (e.g., fastText) (Bojanowski et al., 2017), and d=768 for mBERT.3 We also assume a set of seed translation pairs D0={(wxm1 , w y n1), ..., (w x m|D0|\n, wyn|D0|)} for training (Mikolov et al., 2013; Glava\u0161 et al., 2019), where 1 \u2264 mi \u2264 |X |, 1 \u2264 ni \u2264 |Y|. Typical values for the seed dictionary size |D0| are 5k pairs and 1k pairs (Vulic\u0301 et al., 2019), often referred to as supervised (5k) and semisupervised or weakly supervised settings (1k) (Artetxe et al., 2018). Given another test lexicon DT={(wxt1 , w y g1), ..., (w x t|DT |\n, wyg|DT |)}, where D0 \u2229DT = \u2205, for each Lx test word wxti in DT the goal is to retrieve its correct translation from Ly\u2019s vocabulary Y , and evaluate it against the gold Ly translation wygi from the pair.\nMethod in a Nutshell. We propose a novel 3We also tried XLM (d=1, 280) and mT5small (d=512); mBERT is the best-performing pretrained LM in our preliminary investigation.\ntwo-stage contrastive learning (CL) method, with both stages C1 and C2 realised via contrastive learning objectives (see Figure 1). Stage C1 (\u00a72.1) operates solely on static WEs, and can be seen as a contrastive extension of mapping-based BLI approaches with static WEs. In practice, we blend contrastive learning with the standard SotA mapping-based framework with self-learning: VecMap (Artetxe et al., 2018), with some modifications. Stage C1 operates solely on static WEs in exactly the same BLI setup as prior work, and thus it can be evaluated independently. In Stage C2 (\u00a72.2), we propose to leverage pretrained multilingual LMs for BLI: we contrastively fine-tune them for BLI and extract static \u2018decontextualised\u2019 WEs from the tuned LMs. These LM-based WEs can be combined with WEs obtained in Stage C1 (\u00a72.3)."
        },
        {
            "heading": "2.1 Stage C1",
            "text": "Stage C1 is based on the VecMap framework (Artetxe et al., 2018) which features 1) dual linear mapping, where two separate linear transformation matrices map respective source and target WEs to a shared cross-lingual space; and 2) a self-learning procedure that, in each iteration i refines the training dictionary and iteratively improves the mapping. We extend and refine VecMap\u2019s self-learning for supervised and semi-supervised settings via CL.\nInitial Advanced Mapping. After `2-normalising word embeddings,4 the two mapping matrices, denoted as Wx for the source language Lx and Wy for Ly, are computed via the Advanced Mapping (AM) procedure based on the training dictionary, as fully described in Appendix A.1; while VecMap leverages whitening, orthogonal mapping, re-weighting and de-whitening operations to derive mapped WEs, we compute Wx and Wy such that a one-off matrix multiplication produces the same result (see Appendix A.1 for the details).\nContrastive Fine-Tuning. At each iteration i, after the initial AM step, the two mapping matrices Wx and Wy are then further contrastively finetuned via the InfoNCE loss (Oord et al., 2018), a standard and robust choice of a loss function in CL research (Musgrave et al., 2020; Liu et al., 2021c,b). The core idea is to \u2018attract\u2019 aligned WEs of positive examples (i.e., true translation pairs) coming from the dictionary Di\u22121, and \u2018repel\u2019 hard negative samples, that is, words which are semantically similar\n4Unlike VecMap, we do not mean-center WEs as this yielded slightly better results in our preliminary experiments.\nAlgorithm 1 Stage C1: Self-Learning 1: Require: X ,Y ,D0,Dadd \u2190 \u2205 2: for i\u2190 1 to Niter do 3: Wx,Wy \u2190 Initial AM using Di\u22121; 4: DCL \u2190D0 (supervised) or Di\u22121 (semi-super); 5: for j\u2190 1 to NCL do 6: Retrieve D\u0304 for the pairs from DCL; 7: Wx,Wy \u2190 Optimise Contrastive Loss; 8: Compute new Dadd; 9: Update Di\u2190D0 \u222a Dadd; 10: return Wx,Wy;\nbut do not constitute a word translation pair. These hard negative samples are extracted as follows. Let us suppose that (wxmi , w y ni) is a translation pair in the current dictionary Di\u22121, with its constituent words associated with static WEs xmi ,yni\u2208R1\u00d7d. We then retrieve the nearest neighbours of yniWy from XWx and derive w\u0304 x mi \u2282 X (wxmi excluded) , a set of hard negative samples of size Nneg. In a similar (symmetric) manner, we also derive the set of negatives w\u0304yni \u2282 Y (w y ni excluded). We use D\u0304 to denote a collection of all hard negative set pairs over all training pairs in the current iteration i. We then fine-tune Wx and Wy by optimising the following contrastive objective:\nsi,j = exp(cos(xiWx , yjWy)/\u03c4), (1) pi = smi,ni\u2211\nw y j \u2208{w y ni } \u22c3 w\u0304 y ni\nsmi,j + \u2211\nwxj \u2208w\u0304 x mi\nsj,ni , (2)\nmin Wx,Wy \u2212 E(wxmi ,w y ni )\u2208DCL log(pi). (3)\n\u03c4 denotes a standard temperature parameter. The objective, formulated here for a single positive example, spans all positive examples from the current dictionary, along with the respective sets of negative examples computed as described above.\nSelf-Learning. The application of (a) initial mapping via AM and (b) contrastive fine-tuning can be repeated iteratively. Such self-learning loops typically yield more robust and better-performing BLI methods (Artetxe et al., 2018; Vulic\u0301 et al., 2019). At each iteration i, a set of automatically extracted high-confidence translation pairs Dadd are added to the seed dictionary D0, and this dictionary Di= D0 \u222a Dadd is then used in the next iteration i+ 1.\nOur dictionary augmentation method slightly deviates from the one used by VecMap. We leverage the most frequent Nfreq source and target vocabulary words, and conduct forward and backward dictionary induction (Artetxe et al., 2018). Unlike VecMap, we do not add stochasticity to the process, and simply select the top Naug high-confidence\nword pairs from forward (i.e., source-to-target) induction and another Naug pairs from the backward induction. In practice, we retrieve the 2\u00d7Naug pairs with the highest Cross-domain Similarity Local Scaling (CSLS) scores (Lample et al., 2018),5 remove duplicate pairs and those that contradict with ground truth in D0, and then add the rest into Dadd.\nFor the initial AM step, we always use the augmented dictionary D0 \u222aDadd; the same augmented dictionary is used for contrastive fine-tuning in weakly supervised setups.6 We repeat the selflearning loop for Niter times: in each iteration, we optimise the contrastive loss NCL times; that is, we go NCL times over all the positive pairs from the training dictionary (at this iteration). Niter and NCL are tunable hyper-parameters. Self-learning in Stage C1 is summarised in Algorithm 1."
        },
        {
            "heading": "2.2 Stage C2",
            "text": "Previous work tried to prompt off-the-shelf multilingual LMs for word translation knowledge via masked natural language templates (Gonen et al., 2020), averaging over their contextual encodings in a large corpus (Vulic\u0301 et al., 2020b; Zhang et al., 2021), or extracting type-level WEs from the LMs directly without context (Vulic\u0301 et al., 2020a, 2021). However, even sophisticated templates and WE extraction strategies still typically result in BLI performance inferior to fastText (Vulic\u0301 et al., 2021).\n(BLI-Oriented) Contrastive Fine-Tuning. Here, we propose to fine-tune off-the-shelf multilingual LMs relying on the supervised BLI signal: the aim is to expose type-level word translation knowledge directly from the LM, without any external corpora. In practice, we first prepare a dictionary of positive examples for contrastive fine-tuning: (a) DCL=D0 when |D0| spans 5k pairs, or (b) when |D0|=1k, we add the Naug=4k automatically extracted highest-confidence pairs from Stage C1 (based on their CSLS scores, not present in D0) to D0 (i.e., DCL spans 1k + 4k word pairs). We then extractNneg hard negatives in the same way as in \u00a72.1, relying on the shared cross-lingual space derived as the output of Stage C1. Our hypothesis is that a difficult task of discerning between true translation pairs and highly similar non-translations as hard negatives, formulated within a contrastive\n5Further details on the CSLS similarity and its relationship to cosine similarity are available in Appendix A.2.\n6When starting with 5k pairs, we leverage only D0 for contrastive fine-tuning, as Dadd might deteriorate the quality of the 5k-pairs seed dictionary due to potentially noisy input.\nlearning objective, will enable mBERT to expose its word translation knowledge, and complement the knowledge already available after Stage C1.\nThroughout this work, we assume the use of pretrained mBERTbase model with 12 Transformer layers and 768-dim embeddings. Each raw word input w is tokenised, via mBERT\u2019s dedicated tokeniser, into the following sequence: [CLS][sw1] . . . [swM ][SEP ], M \u2265 1, where [sw1] . . . [swM ] refers to the sequence of M constituent subwords/WordPieces of w, and [CLS] and [SEP ] are special tokens (Vulic\u0301 et al., 2020b).\nThe sequence is then passed through mBERT as the encoder, its encoding function denoted as f\u03b8(\u00b7): it extracts the representation of the [CLS] token in the last Transformer layer as the representation of the input word w. The full set of mBERT\u2019s parameters \u03b8 then gets contrastively fine-tuned in Stage C2, again relying on the InfoNCE CL loss:\ns\u2032i,j = exp(cos(f\u03b8(w x i ), f\u03b8(w y j ))/\u03c4), (4) p\u2032i = s\u2032mi,ni\u2211\nw y j \u2208{w y ni } \u22c3 w\u0304 y ni\ns\u2032mi,j + \u2211\nwxj \u2208w\u0304 x mi\ns\u2032j,ni , (5)\nmin \u03b8 \u2212 E(wxmi ,w y ni )\u2208DCL log(p \u2032 i). (6)\nType-level WE for each input word w is then obtained simply as f\u03b8\u2032(w), where \u03b8\u2032 refers to the parameters of the \u2018BLI-tuned\u2019 mBERT model."
        },
        {
            "heading": "2.3 Combining the Output of C1 and C2",
            "text": "In order to combine the output WEs from Stage C1 and the mBERT-based WEs from Stage C2, we also need to map them into a \u2018shared\u2019 space: in other words, for each word w, its C1 WE and its C2 WE can be seen as two different views of the same data point. We thus learn an additional linear orthogonal mapping from the C1-induced cross-lingual WE space into the C2induced cross-lingual WE space. It transforms `2- normed 300-dim C1-induced cross-lingual WEs into 768-dim cross-lingual WEs. Learning of the linear map W\u2208Rd1\u00d7d2 , where in our case d1=300 and d2=768, is formulated as a Generalised Procrustes problem (Sch\u00f6nemann, 1966; Viklands, 2006) operating on all (i.e., both Lx and Ly) words from the seed translation dictionary D0.7\n7Technical details of the learning procedure are described in Appendix A.3. It is important to note that in this case we do not use word translation pairs (wxmi , w y ni) directly to learn the mapping, but rather each word wxmi and w y ni is duplicated to create training pairs (wxmi , w x mi) and (w y ni , w y ni), where the left word/item in each pair is assigned its WE from C1, and the right word/item is assigned its WE after C2.\nUnless noted otherwise, a final representation of an input word w is then a linear combination of (a) its C1-based vector vw mapped to a 768- dim representation via W , and (b) its 768-dim encoding f\u03b8\u2032(w) from BLI-tuned mBERT:\n(1\u2212 \u03bb) vwW\u2016vwW \u20162 + \u03bb f\u03b8\u2032(w) \u2016f\u03b8\u2032(w)\u20162 , (7)\nwhere \u03bb is a tunable interpolation hyper-parameter."
        },
        {
            "heading": "3 Experimental Setup",
            "text": "Monolingual WEs and BLI Setup. We largely follow the standard BLI setup from prior work (Artetxe et al., 2018; Joulin et al., 2018; Glava\u0161 et al., 2019; Karan et al., 2020, inter alia). The main evaluation is based on the standard BLI dataset from Glava\u0161 et al. (2019): it comprises 28 language pairs with a good balance of typologically similar and distant languages (Croatian: HR, English: EN, Finnish: FI, French: FR, German: DE, Italian: IT, Russian: RU, Turkish: TR). Again following prior work, we rely on monolingual fastText vectors trained on full Wikipedias for each language (Bojanowski et al., 2017), where vocabularies in each language are trimmed to the 200K most frequent words (i.e., |X |=200k and |Y|=200k). The same fastText WEs are used for our Stage C1 and in all baseline BLI models. mBERT in Stage C2 operates over the same vocabularies spanning 200k word types in each language.\nWe use 1k translation pairs (semi-supervised BLI mode) or 5k pairs (supervised) as seed dictionary D0; test sets span 2k pairs (Glava\u0161 et al., 2019). With 56 BLI directions in total,8 this yields a total of 112 BLI setups for each model in our comparison. The standard Precision@1 (P@1) BLI measure is reported, and we rely on CSLS (k=10) to score word similarity (Lample et al., 2018).9\nTraining Setup and Hyperparameters. Since standard BLI datasets typically lack a validation set (Ruder et al., 2019), following prior work (Glava\u0161 et al., 2019; Karan et al., 2020) we conduct hyperparameter tuning on a single, randomly selected language pair EN\u2192TR, and apply those hyperparameter values in all other BLI runs.\n8For any two languages Li and Lj , we run experiments both for Li \u2192 Lj and Lj \u2192 Li directions.\n9The same trends in results are observed with Mean Reciprocal Rank (MRR) as another BLI evaluation measure (Glava\u0161 et al., 2019); we omit MRR scores for clarity. Moreover, similar relative trends, but with slightly lower absolute BLI scores, are observed when replacing CSLS with the simpler cosine similarity measure: the results are available in the Appendix.\nIn Stage C1, when |D0|=5k, the hyperparameter values are Niter=2, NCL=200, Nneg=150, Nfreq=60k, Naug=10k. SGD optimiser is used, with a learning rate of 1.5 and \u03b3=0.99. When |D0|=1k, the values are Niter=3, NCL=50, Nneg=60, Nfreq=20k, and Naug=6k; SGD with a learning rate of 2.0, \u03b3=1.0. \u03c4=1.0 and dropout is 0 in both cases, and the batch size for contrastive learning is always equal to the size of the current dictionary |DCL| (i.e., |D0| (5k case), or |D0\u222aDadd| which varies over iterations (1k case); see \u00a72.1). In Stage C2, Nneg=28 and the maximum sequence length is 6. We use AdamW (Loshchilov and Hutter, 2019) with learning rate of 2e\u2212 5 and weight decay of 0.01. We fine-tune mBERT for 5 epochs, with a batch size of 100; dropout rate is 0.1 and \u03c4=0.1. Unless noted otherwise, \u03bb is fixed to 0.2.\nBaseline Models. Our BLI method is evaluated against four strong SotA BLI models from recent literature, all of them with publicly available implementations. Here, we provide brief summaries:10\nRCSLS (Joulin et al., 2018) optimises a relaxed CSLS loss, learns a non-orthogonal mapping, and has been established as a strong BLI model in empirical comparative analyses as its objective function is directly \u2018BLI-oriented\u2019 (Glava\u0161 et al., 2019). VecMap\u2019s core components (Artetxe et al., 2018) have been outlined in \u00a72.1. LNMap (Mohiuddin et al., 2020) non-linearly maps the original static WEs into two latent semantic spaces learned via non-linear autoencoders,11\nand then learns another non-linear mapping between the latent autoencoder-based spaces. FIPP (Sachidananda et al., 2021), in brief, first finds common (i.e., isomorphic) geometric structures in monolingual WE spaces of both languages, and then aligns the Gram matrices of the WEs found in those common structures.\nFor all baselines, we have verified that the hyperparameter values suggested in their respective repositories yield (near-)optimal BLI performance. Unless noted otherwise, we run VecMap, LNMap, and FIPP with their own self-learning procedures.12\n10For further technical details and descriptions of each BLI model, we refer to their respective publications. We used publicly available implementations of all the baseline models.\n11This step is directed towards mitigating anisomorphism (S\u00f8gaard et al., 2018; Dubossarsky et al., 2020) between the original WE spaces, which should facilitate their alignment.\n12RCSLS is packaged without self-learning; extending it to support self-learning is non-trivial and goes beyond the scope of this work.\nModel Variants. We denote the full two-stage BLI model as C2 (Mod), where Mod refers to the actual model/method used to derive the shared crosslingual space used by Stage C2. For instance, C2 (C1) refers to the model variant which relies on our Stage C1, while C2 (RCSLS) relies on RCSLS as the base method. We also evaluate BLI performance of our Stage C1 BLI method alone.\nMultilingual LMs. We adopt mBERT as the default pretrained multilingual LM in Stage C2. Our supplementary experiments also cover the 1280- dim XLM model13 (Lample and Conneau, 2019) and 512-dim mT5small (Xue et al., 2021).14 For clarity, we use C2 [LM] to denote C2 (C1) obtained from different LMs; when [LM] is not specified, mBERT is used. We adopt a smaller batch size of 50 for C2 [XLM] considering the limit of GPU memory, and train C2 [mT5] with a larger learning rate of 6e\u22124 for 6 epochs, since we found it much harder to train than C2 [mBERT]."
        },
        {
            "heading": "4 Results and Discussion",
            "text": "The main results are provided in Table 1, while the full results per each individual language pair, and also with cosine similarity as the word retrieval function, are provided in Appendix E. The main findings are discussed in what follows.\nStage C1 versus Baselines. First, we note that there is not a single strongest baseline among the four SotA BLI methods. For instance, RCSLS and VecMap are slightly better than LNMap and FIPP with 5k supervision pairs, while FIPP and VecMap come forth as the stronger baselines with 1k supervision. There are some score fluctuations over individual language pairs, but the average performance of all baseline models is within a relatively narrow interval: the average performance of all four baselines is within 3 P@1 points with 5k pairs (i.e., ranging from 38.22 to 41.22), and VecMap, FIPP, and LNMap are within 2 points with 1k pairs.\nStrikingly, contrastive learning in Stage C1 already yields substantial gains over all four SotA BLI models, which is typically much higher than the detected variations between the baselines. We mark that C1 improves over all baselines in 51/56 BLI setups (in the 5k case), and in all 56/56 BLI setups when D0 spans 1k pairs. The average gains\n13We pick the XLM large model pretrained on 100 languages with masked language modeling (MLM) objective.\n14We also tested XLM-Rbase, but in our preliminary experiments it shows inferior BLI performance.\nwith the C1 variant are \u22485 P@1 points over the SotA baselines with 5k pairs, and \u22486 P@1 points with 1k pairs (ignoring RCSLS in the 1k scenario). Note that all the models in comparison, each currently considered SotA in the BLI task, use exactly the same monolingual WEs and leverage exactly the same amount of bilingual supervision. The gains achieved with our Stage C1 thus strongly indicate the potential and usefulness of word-level contrastive fine-tuning when learning linear crosslingual maps with static WEs (see RQ1 from \u00a71).\nStage C1 + Stage C2. The scores improve further with the full two-stage procedure. The C2 (C1) BLI variant increases average P@1 by another 3.3\n(5k) and 3 P@1 points (1k), and we observe gains for all language pairs in both translation directions, rendering Stage C2 universally useful. These gains indicate that mBERT does contain word translation knowledge in its parameters. However, the model must be fine-tuned (i.e., transformed) to \u2018unlock\u2019 the knowledge from its parameters: this is done through a BLI-guided contrastive fine-tuning procedure (see \u00a72.2). Our findings thus further confirm the \u2018rewiring hypothesis\u2019 from prior work (Vulic\u0301 et al., 2021; Liu et al., 2021b; Gao et al., 2021), here validated for the BLI task (see RQ2 from \u00a71), which states that task-relevant knowledge at sentence- and word-level can be \u2018rewired\u2019/exposed from the off-the-shelf LMs, even when leveraging very limited task supervision, e.g., with only 1k or 5k word translation pairs as in our experiments.\nPerformance over Languages. The absolute BLI scores naturally depend on the actual source and target languages: e.g., the lowest absolute performance is observed for morphologically rich (HR, RU, FI, TR) and non-Indo-European languages (FI, TR). However, both C1 and C2 (C1) mode variants offer wide and substantial gains in performance for all language pairs, irrespective of the starting absolute score. This result further suggests wide applicability and robustness of our BLI method."
        },
        {
            "heading": "4.1 Further Discussion",
            "text": "Evaluation on Lower-Resource Languages. The robustness of our BLI method is further tested on another BLI evaluation set: PanLex-BLI (Vulic\u0301 et al., 2019), which focuses on BLI evaluation for lower-resource language; 1k training pairs and 2k test pairs are derived from PanLex (Kamholz et al., 2014). The results for a subset of six languages (Basque: EU, Bulgarian: BG, Catalan: CA, Estonian: ET, Hebrew: HE, Hungarian: HU) are\npresented in Table 2. Overall, the results further confirm the efficacy of the C2 (C1), with gains observed even with typologically distant language pairs (e.g., HE\u2192BG and EU\u2192ET).\nUsefulness of Stage C2? The results in Table 1 have confirmed the effectiveness of our two-stage C2 (C1) BLI method (see RQ3 in \u00a71). However, Stage C2 is in fact independent of our Stage C1, and thus can also be combined with other standard BLI methods. Therefore, we seek to validate whether combining exposed mBERT-based translation knowledge can also aid other BLI methods. In other words, instead of drawing positive and negative samples from Stage C1 (\u00a72.2) and combining C2 WEs with WEs from C1 (\u00a72.3), we replace C1 with our baseline models. The results of these C2 (RCSLS) and C2 (VecMap) BLI variants for a selection of language pairs are provided in Table 3.\nThe gains achieved with all C2 (\u00b7) variants clearly indicate that Stage C2 produces WEs which aid all BLI methods. In fact, combining it with RC-\nSLS and VecMap yields even larger relative gains over the base models than combining it with our Stage C1. However, since Stage C1 (as the base model) performs better than RCSLS and VecMap, the final absolute scores with C2 (C1) still outperform C2 (RCSLS) and C2 (VecMap).\nDifferent Multilingual LMs? Results on eight language pairs, shown in Table 4, indicate that C2 (C1) is also compatible with different LMs. The overall trend is that all three C2 [LM] variants derive some gains when compared to C1. C2 [mBERT] is the best-performing model and derives gains in all 112/112 BLI setups (also see Appendix E); C2 [mT5] outperforms C1 in all 16/16 cases, and the gains are observed for 14/16 cases with C2 [XLM]. It is also worth noticing that C2 [XLM] can surpass C2 [mBERT] on several pairs.\nCombining C1 and C2? The usefulness of combining the representations from two stages is measured through varying the value of \u03bb for several BLI setups. The plots are shown in Figure 2, and indicate that Stage C1 is more beneficial to the performance, with slight gains achieved when allowing the \u2018influx\u2019 of mBERT knowledge (e.g., \u03bb in the [0.0 \u2212 0.3] interval). While mBERT-based WEs are not sufficient as standalone representations for BLI, they seem to be even more useful in the combined model for lower-resource languages on PanLex-BLI, with steeper increase in performance, and peak scores achieved with larger \u03bb-s.\nAblation Study, with results summarised in Table 5, displays several interesting trends. First, both CL and self-learning are key components in the 1k-setups: removing any of them yields substantial drops. In 5k-setups, self-learning becomes less important, and removing it yields only negligible drops, while CL remains a crucial component (see also Appendix F). Further, Table 5 complements the results from Figure 2 and again indicates that, while Stage C2 indeed boosts word translation capacity of mBERT, using mBERT features alone is still not sufficient to achieve competitive\nBLI scores. After all, pretrained LMs are contextualised encoders designed for (long) sequences rather than individual words or tokens. Finally, Table 5 shows the importance of fine-tuning mBERT before combining it with C1-based WEs (\u00a72.3): directly adding WEs extracted from the off-the-shelf mBERT does not yield any benefits (see the scores for the C1+mBERT variant, where \u03bb is also 0.2).\nThe Impact of Contrastive Fine-Tuning on mBERT\u2019s representation space for two language\npairs is illustrated by a t-SNE plot in Figure 3. The semantic space of off-the-shelf mBERT displays a clear separation of language-specific subspaces (Libovick\u00fd et al., 2020; Dufter and Sch\u00fctze, 2020), which makes it unsuitable for the BLI task. On the other hand, contrastive fine-tuning reshapes the subspaces towards a shared (cross-lingual) space, the effects of which are then also reflected in mBERT\u2019s improved BLI capability (see Table 5 again).\nTo understand the role of CL in Stage C1, we visualise static WEs mapped by C1 without CL (i.e., AM+SL, see \u00a72.1) and also from the complete Stage C1, respectively. Figure 4 shows that C1 without CL already learns a sensible cross-lingual space. However, we note that advanced mapping (AM) in C1 without CL learns a (near-)orthogonal map, which might result in mismatches, especially with dissimilar language pairs. With TR-HR, the plot reveals that there exists a gap between C1aligned WE spaces although the final BLI performance still gets improved: this might be due to \u2018repelling\u2019 negatives from each other during CL.\nFinally, we direct interested readers to Appendix G where we present some qualitative translation examples."
        },
        {
            "heading": "5 Related Work",
            "text": "This work is related to three topics, each with a large body of work; we can thus provide only a condensed summary of the most relevant research.\nMapping-Based BLI. These BLI methods are highly popular due to reduced bilingual supervision requirements; consequently, they are applicable to low-resource languages and domains, learning linear (Lample et al., 2018; Artetxe et al., 2018; Joulin et al., 2018; Patra et al., 2019; Jawanpuria et al., 2019; Sachidananda et al., 2021) and non-linear maps (Mohiuddin et al., 2020; Glava\u0161 and Vulic\u0301, 2020; Ganesan et al., 2021), typically using self-\nlearning in weakly supervised setups.\nContrastive Learning in NLP aims to learn a semantic space such that embeddings of similar text inputs are close to each other, while \u2018repelling\u2019 dissimilar ones. It has shown promising performance on training generic sentence encoders (Giorgi et al., 2021; Carlsson et al., 2021; Liu et al., 2021a; Gao et al., 2021) and downstream tasks like summarisation (Liu and Liu, 2021) or NER (Das et al., 2021).\nExposing Lexical Knowledge from Pretrained LMs. Extracting lexical features from off-the-shelf multilingual LMs typically yields subpar performance in lexical tasks (Vulic\u0301 et al., 2020b). To unlock the lexical knowledge encoded in PLMs, Liu et al. (2021a) and Vulic\u0301 et al. (2021) fine-tune LMs via contrastive learning with manually curated or automatically extracted phrase/word pairs to transform it into effective text encoders. Wang et al. (2021) and Liu et al. (2021c) apply similar techniques for phrase and word-in-context representation learning, respectively. The success of these methods suggests that LMs store a wealth of lexical knowledge: yet, as we confirm here for BLI, fine-tuning is typically needed to expose it."
        },
        {
            "heading": "6 Conclusion",
            "text": "We have proposed a simple yet extremely effective and robust two-stage contrastive learning framework for improving bilingual lexicon induction (BLI). In Stage C1, we tune cross-lingual linear mappings between static word embeddings with a contrastive objective and achieve substantial gains in 107 out of 112 BLI setups on the standard BLI benchmark. In Stage C2, we further propose a contrastive fine-tuning procedure to harvest crosslingual lexical knowledge from multilingual pretrained language models. The representations from this process, when combined with Stage C1 embeddings, have resulted in further boosts in BLI performance, with large gains in all 112 setups. We have also conducted a series of finer-grained evaluations, analyses and ablation studies."
        },
        {
            "heading": "Acknowledgements",
            "text": "F F FFF F F\nF F F F F We thank the anonymous reviewers for their valuable feedback. This work is supported by the ERC PoC Grant MultiConvAI (no. 957356) and a research donation from Huawei. YL and FL are supported by Grace & Thomas C. H. Chan Cambridge International Scholarship.\nEthics Statement\nOur research aims to benefit the efforts in delivering truly multilingual language technology also to under-resourced languages and cultures via bridging the lexical gap between languages, groups and cultures. As a key task in cross-lingual NLP, bilingual lexicon induction or word translation has broad applications in, e.g., machine translation, language acquisition and potentially protecting endangered languages. Furthermore, compared with many previous studies, we stress the importance of diversity in the sense that our experiments cover various language families and include six lowerresource languages from the PanLex-BLI dataset. Hoping that our work can contribute to extending modern NLP techniques to lower-resource and under-represented languages, we focus on semisupervised settings and achieve significant improvements with self-learning techniques.\nThe two BLI datasets we use are both publicly available. To our best knowledge, the data (i.e., word translation pairs) do not contain any sensitive information and have no foreseeable risk."
        },
        {
            "heading": "A Technical Details and Further Clarifications",
            "text": "A.1 Advanced Mapping (AM) in Stage C1 Suppose XD,YD \u2208 R|D|\u00d7d are source and target embedding matrices corresponding to the training dictionary D. Then XTD and Y TD are whitened, and singular value decomposition (SVD) is conducted on the whitened embeddings:\nX \u2032 D = XD(X T DXD) \u2212 1 2 , (8)\nY \u2032 D = YD(Y T D YD) \u2212 1 2 , (9)\nUSV T = X \u2032T D Y \u2032 D. (10)\nWx and Wy are then derived after re-weighting and de-whitening as follows:\nWx=(X T DXD)\n\u2212 1 2US 1 2UT (XTDXD) 1 2U , (11)\nWy = (Y T D YD)\n\u2212 1 2V S 1 2V T (Y TD YD) 1 2V . (12)\nA.2 Word Similarity/Retrieval Measures Given two word embeddings x \u2208 X and y \u2208 Y , their similarity can be defined as their cosine similarity m(x,y) = cosine(x,y). In the FIPP model, we calculate dot product m(x,y) = xT \u00b7 y between x and y instead without normalisation, as with FIPP this produces better BLI scores in general.15\nFor the simple Nearest Neighbor (NN) BLI with cosine (or dot product), we retrieve the word from the entire target language vocabulary of size 200k with the highest similarity score and mark it as the translation of the input/query word in the source language.\nFor the Cross-domain Similarity Local Scaling (CSLS) measure, a CSLS score is defined as CSLS(x,y) = 2m(x,y)\u2212rX(y)\u2212rY (x). rX(y) is the average m(\u00b7, \u00b7) score of y and its k-NNs (k = 10) in X; rY (x) is the average m(\u00b7, \u00b7) scores of x and its k-NNs (k = 10) in Y . Note that when using CSLS scores to retrieve the translation of x in Y , the term rY (x) can be ignored, as it is a constant for all y, and we can similarly ignore rX(y) when doing BLI in the opposite direction.\n15https://github.com/vinsachi/FIPPCLE/blob/ main/xling-bli/code/eval.py\nA.3 Generalised Procrustes in Stage C2\nWe consider the following Procrustes problem:\nargmin W\n\u2016XW \u2212 Y \u20162F ,WW T = I, (13)\nwhere X \u2208 Rn\u00d7d1 is a C1-induced cross-lingual space spanning all source and target words in the training set D, Y \u2208 Rn\u00d7d2 is a C2-induced space representing all mBERT-encoded vectors corresponding to the same words from X , and W \u2208 Rd1\u00d7d2 , d1 \u2264 d2. A classical Orthogonal Procrustes Problem assumes that d1 = d2 and W is an orthogonal matrix (i.e., it should be a square matrix), where its optimal solution is given by UV T ; here, USV T is the full singular value decomposition (SVD) of XTY . In our experiments, we need to address the case d1 < d2 when mapping 300-dimensional static fastText WEs to the 768- dimensional space of mBERT-based WEs. It is easy to show that when d1 < d2, U [S,0]V T=XTY (again the full SVD decomposition), the optimal W is then U [I,0]V T (it degrades to the Orthogonal Procrustes Problem when d1 = d2). Below, we provide a simple proof.\nLet \u2126 = UTWV , then \u2126\u2126T = I . Therefore, each of its element \u22121 \u2264 \u2126i,j \u2264 1.\nargmin W\n\u2016XW \u2212 Y \u20162F\n=argmin W\n\u3008XW \u2212 Y ,XW \u2212 Y \u3009F\n=argmin W\n\u2016XW \u20162F + \u2016Y \u2016 2 F \u2212 2\u3008XW ,Y \u3009F\n=argmax W\n\u3008XW ,Y \u3009F\n=argmax W\n\u3008W ,XTY \u3009F\n=argmax W\n\u3008W ,XTY \u3009F\n=argmax W\n\u3008W ,U [S,0]V T \u3009F\n=argmax W\n\u3008[S,0],UTWV \u3009F\n=argmax W \u3008[S,0],\u2126\u3009F (14)\nIn the formula above, \u2016\u00b7\u2016F and \u3008\u00b7, \u00b7\u3009F are Frobenius norm and Frobenius inner product, and we leverage their properties throughout the proof. Note that S is a diagonal matrix with non-negative elements and thus the maximum is achieved when \u2126 = [I,0] and W = U [I,0]V T .\nNote that the Procrustes mapping over word embedding matrices keeps word similarities on both sides intact. Since WW T=I , cos(xiW ,xjW ) = cos(xi,xj).\nWe would also like to add an additional note, although irrelevant to our own experiments, that the above derivation cannot address d1 > d2 scenarios: in that case WW T cannot be a full-rank matrix and thus WW T 6= I .\nA.4 Languages in BLI Evaluation"
        },
        {
            "heading": "B Reproducibility Checklist",
            "text": "\u2022 BLI Data: The two BLI datasets are publicly available.16 17\n\u2022 Static WEs: We use the preprocessed fastText WEs provided by Glava\u0161 et al. (2019). For PanLex-BLI, we follow the original paper\u2019s setup (Vulic\u0301 et al., 2019) and adopt fastText WEs pretrained on both Common Crawl and Wikipedia (Bojanowski et al., 2017).18\nFollowing prior work, all static WEs are trimmed to contain vectors for the top 200k most frequent words in each language.\n\u2022 Pretrained LM: The used model variants are \u2018bert-base-multilingual-uncased\u2019 for\n16https://github.com/vinsachi/FIPPCLE/blob/ main/xling-bli/code/eval.py\n17https://github.com/cambridgeltl/panlex-bli 18https://fasttext.cc/docs/en/crawl-vectors.html\nmBERT, \u2018xlm-mlm-100-1280\u2019 for XLM and \u2018google/mt5-small\u2019 for mT5, all retrieved from the huggingface.co model repository.\n\u2022 Baseline BLI Models: All models are accessible online as publicly available github repositories.\n\u2022 Source Code: Our code is available online at: https://github.com/cambridgeltl/ ContrastiveBLI.\n\u2022 Computing Infrastructure: We run our main experiments on a machine with a 4.00GHz 4-core i7-6700K CPU, 64GB RAM and two 12GB NVIDIA TITAN X GPUs. We rely on Python 3.6.10, PyTorch 1.7.0 and huggingface.co Transformers 4.4.2. Automatic Mixed Precision (AMP)19 is leveraged during C2 training. For the experiments with XLM and mT5 only, we leverage a cluster where we have access to two 24GB RTX 3090 GPUs.\n\u2022 Runtime: The training process (excluding data loading and evaluation) typically takes 650 seconds for Stage C1 (seed dictionary of 5k pairs, 2 self-learning iterations) and 200 seconds for C1 (1k pairs, 3 self-learning iterations) on a single GPU. Stage C2 runs for \u2248 500 seconds on two GPUs (TITAN X).\n\u2022 Robustness and Randomness: Our improvement is robust since both C1 and C2 outperform existing SotA methods in 112 BLI setups by a considerable margin. We regard our C1 as a deterministic algorithm because we adopt 0 dropout and a batch size equal to the size of the whole training dictionary (no randomness from shuffling). In C2, considering its robustness, we fix the random seed to 33 over all runs and setups.\nC Visualisation of mBERT-Based Word Representations\nTo illustrate the impact of the proposed BLIoriented fine-tuning of mBERT in Stage C2 on its representation space, we visualise the 768- dimensional mBERT word representations (i.e., mBERT-encoded word features alone, without the infusion of C1-aligned static WEs). We encode BLI test sets (i.e., these sets include 2k sourcetarget word pairs unseen during C2 fine-tuning),\n19https://pytorch.org/docs/stable/amp.html\nbefore and after fine-tuning, relying on 1k training samples as the seed dictionary D0.\nHere, we provide comparative t-SNE visualisations between source and target word mBERTbased decontextualised word representations (see \u00a72.2) for six language pairs from the BLI dataset of Glava\u0161 et al. (2019): EN-IT, FI-RU, EN-HR, HR-RU, DE-TR, and IT-FR, while two additional visualisations are available in the main paper (for RU-IT and TR-HR, see Figure 3 in \u00a74.1). As visible in all the figures below, before BLI-oriented fine-tuning in Stage C2, there is an obvious separation between mBERT\u2019s representation subspaces in the two languages. This undesired property gets mitigated, to a considerable extent, by the finetuning procedure in Stage C2.\nD Visualisation of fastText Word Representations\nTo show the impact of contrastive tuning in Stage C1, we provide t-SNE plots of 300-dimensional C1-aligned fastText embeddings with and without contrastive tuning (see \u00a72.1) respectively for the same six language pairs as in Appendix C. The C1 w/o CL alignment consists of advanced mapping and self-learning loops, which has already been discussed in our ablation study (see \u00a74.1). Like in Appendix C, the linear maps are learned on 1k seed translation pairs and our plots only cover the BLI test sets."
        },
        {
            "heading": "E Appendix: Full BLI Results",
            "text": "Complete results on the BLI dataset of Glava\u0161 et al. (2019), per each language pair and also including NN-based BLI scores, are provided in Tables 7-8. It can be seen as an expanded variant of the main Table 1 presented in the main paper."
        },
        {
            "heading": "F Appendix: Full Ablation Study",
            "text": "Complete results of the ablation study, over all languages in the evaluation set of Glava\u0161 et al. (2019), are available in Table 9, and can be seen as additional evidence which supports the claims from the main paper (see \u00a74.1)"
        },
        {
            "heading": "G Appendix: Translation Examples",
            "text": "We showcase some translation examples of both C1 alignment (see \u00a72.1) and C2 alignment (see \u00a72.2) in HR\u2192EN and IT\u2192EN word translation scenarios. In order to gain insight into the effectiveness of\ncontrastive learning, we adopt C1 w/o CL as a baseline (also used in Table 5). All three models (i.e., C1 w/o CL, C1 and C2) are learned with 5k seed training word pairs, and we report top five predictions via Nearest Neighbor (NN) retrieval (for simplicity) on the BLI test sets.\nWe consider both SUCCESS and FAIL examples in terms of BLI-oriented contrastive fine-tuning, where \u2018SUCCESS\u2019 represents the cases where at least one of C1 and C2 predicts the correct answer when the baseline fails, and \u2018FAIL\u2019 denotes the scenarios where the baseline succeeds but both C1 and C2 make wrong predictions. Here, we show some statistics for each language pair: (1) HR-EN sees 284 SUCCESS samples and 79 FAIL ones; (2) IT-EN has 165 SUCCESS data points, but only 27 FAIL ones. Table 10 provides 5 SUCCESS examples and 5 FAIL ones for each of the two language pairs."
        }
    ],
    "title": "Improving Word Translation via Two-Stage Contrastive Learning",
    "year": 2022
}