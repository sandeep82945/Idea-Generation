{
    "abstractText": "Purpose Surgical scene understanding plays a critical role in the technology stack of tomorrow\u2019s intervention-assisting systems in endoscopic surgeries. For this, tracking the endoscope pose is a key component, but remains challenging due to illumination conditions, deforming tissues and the breathing motion of organs. Method We propose a solution for stereo endoscopes that estimates depth and optical flow to minimize two geometric losses for camera pose estimation. Most importantly, we introduce two learned adaptive per-pixel weight mappings that balance contributions according to the input image content. To do so, we train a Deep Declarative Network to take advantage of the expressiveness of deep learning and the robustness of a novel geometric-based optimization approach. We validate our approach on the publicly available SCARED dataset and introduce a new in vivo dataset, StereoMIS, which includes a wider spectrum of typically observed surgical settings. Results Ourmethod outperforms state-of-the-artmethods on average andmore importantly, in difficult scenarioswhere tissue deformations and breathing motion are visible. We observed that our proposed weight mappings attenuate the contribution of pixels on ambiguous regions of the images, such as deforming tissues. Conclusion We demonstrate the effectiveness of our solution to robustly estimate the camera pose in challenging endoscopic surgical scenes. Our contributions can be used to improve related tasks like simultaneous localization and mapping (SLAM) or 3D reconstruction, therefore advancing surgical scene understanding in minimally invasive surgery.",
    "authors": [
        {
            "affiliations": [],
            "name": "Michel Hayoz"
        },
        {
            "affiliations": [],
            "name": "Christopher Hahne"
        },
        {
            "affiliations": [],
            "name": "\u00b7Mathias Gallardo"
        },
        {
            "affiliations": [],
            "name": "Daniel Candinas"
        },
        {
            "affiliations": [],
            "name": "Thomas Kurmann"
        },
        {
            "affiliations": [],
            "name": "Maximilian Allan"
        },
        {
            "affiliations": [],
            "name": "Raphael Sznitman"
        }
    ],
    "id": "SP:d0aa131a641c00ea33e6ffb796046bbe038b06ff",
    "references": [
        {
            "authors": [
                "R Mur-Artal",
                "JD Tard\u00f3s"
            ],
            "title": "Orb-slam2: an open-source slam system for monocular",
            "venue": "stereo, and rgb-d cameras. IEEE Trans Rob 33(5):1255\u20131262",
            "year": 2017
        },
        {
            "authors": [
                "T Whelan",
                "RF Salas-Moreno",
                "B Glocker",
                "AJ Davison",
                "S Leutenegger"
            ],
            "title": "2016) Elasticfusion: real-time dense slam and light source estimation",
            "venue": "Int J Rob Res 35(14):1697\u20131716",
            "year": 2016
        },
        {
            "authors": [
                "J Lamarca",
                "JMM Montiel"
            ],
            "title": "Camera tracking for slam in deformable maps. In: Computer vision-ECCV 2018 workshops, pp 730\u2013737",
            "year": 2018
        },
        {
            "authors": [
                "JJ G\u00f3mez-Rodr\u00edguez",
                "J Lamarca",
                "J Morlana",
                "JD Tard\u00f3s",
                "JMM Montiel"
            ],
            "title": "Sd-defslam: semi-direct monocular slam for deformable and intracorporeal scenes",
            "venue": "IEEE international conference on robotics and automation (ICRA),",
            "year": 2021
        },
        {
            "authors": [
                "X Liu",
                "Z Li",
                "M Ishii",
                "GD Hager",
                "RH Taylor",
                "M Unberath"
            ],
            "title": "Sage: slam with appearance and geometry prior for endoscopy",
            "venue": "International conference on robotics and automation (ICRA), pp 5587\u20135593",
            "year": 2022
        },
        {
            "authors": [
                "J Song",
                "J Wang",
                "ZhaoL",
                "S Huang",
                "DissanayakeG"
            ],
            "title": "2018)Mis-slam: real-time large-scale dense deformable slam system in minimal invasive surgery based on heterogeneous computing",
            "venue": "IEEE Robot Autom Lett 3(4):4068\u20134075",
            "year": 2018
        },
        {
            "authors": [
                "H Zhou",
                "J Jayender"
            ],
            "title": "EMDQ-SLAM: real-time highresolution reconstruction of soft tissue surface from stereo laparoscopy videos. In: Medical image computing and computer assisted intervention\u2014MICCAI",
            "year": 2021
        },
        {
            "authors": [
                "R Wei",
                "B Li",
                "H Mo",
                "B Lu",
                "Y Long",
                "B Yang",
                "Q Dou",
                "Y Liu",
                "D Sun"
            ],
            "title": "Stereo dense scene reconstruction and accurate localization for learning-based navigation of laparoscope in minimally invasive surgery",
            "venue": "IEEE Trans Biomed Eng",
            "year": 2023
        },
        {
            "authors": [
                "S Gould",
                "HartleyR",
                "D Campbell"
            ],
            "title": "2022)Deep declarative networks. IEEETrans PatternAnalMach Intell 44(8):3988\u20134004",
            "venue": "https://doi",
            "year": 2021
        },
        {
            "authors": [
                "CM Parameshwara",
                "G Hari",
                "C Ferm\u00fcller",
                "NJ Sanket",
                "Y Aloimonos"
            ],
            "title": "Diffposenet: direct differentiable camera pose estimation",
            "year": 2022
        },
        {
            "authors": [
                "Z Teed",
                "J Deng"
            ],
            "title": "RAFT: recurrent all-pairs field transforms for optical flow",
            "venue": "vision-ECCV",
            "year": 2020
        },
        {
            "authors": [
                "DC Liu",
                "J Nocedal"
            ],
            "title": "On the limited memory BFGS method for large scale optimization",
            "venue": "Mathematical programming,",
            "year": 1989
        },
        {
            "authors": [
                "O Ronneberger",
                "P Fischer",
                "T Brox"
            ],
            "title": "U-net: convolutional networks for biomedical image segmentation. In: Medical image computing and computer-assisted intervention\u2013MICCAI",
            "year": 2015
        },
        {
            "authors": [
                "AllanM",
                "AJ McLeod",
                "CC Wang",
                "J Rosenthal",
                "Z Hu",
                "N Gard",
                "P Eisert",
                "KX Fu",
                "T Zeffiro",
                "W Xia",
                "Z Zhu",
                "H Luo",
                "F Jia",
                "X Zhang",
                "X Li",
                "L Sharan",
                "T Kurmann",
                "S Schmid",
                "R Sznitman",
                "D Psychogyios",
                "M Azizian",
                "D Stoyanov",
                "L Maier-Hein",
                "S Speidel"
            ],
            "title": "Stereo correspondence and reconstruction of endoscopic data challenge",
            "year": 2021
        },
        {
            "authors": [
                "L-C Chen",
                "Y Zhu",
                "G Papandreou",
                "F Schroff",
                "H Adam"
            ],
            "title": "Encoder\u2013decoder with atrous separable convolution for semantic image segmentation",
            "venue": "vision\u2014ECCV",
            "year": 2018
        },
        {
            "authors": [
                "KB Ozyoruk",
                "GI Gokceler",
                "TL Bobrow",
                "G Coskun",
                "K Incetan",
                "AlmaliogluY",
                "MahmoodF",
                "CurtoE",
                "PerdigotoL",
                "OliveiraM",
                "H Sahin",
                "H Araujo",
                "H Alexandrino",
                "NJ Durr",
                "HB Gilbert"
            ],
            "title": "Endoslam dataset and an unsupervised monocular visual odometry and depth estimation approach for endoscopic videos",
            "venue": "TuranM",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "Keywords Camera pose estimation \u00b7 Endoscopic surgery \u00b7 Deep declarative network"
        },
        {
            "heading": "Introduction",
            "text": "Camera pose estimation is a well-established computer vision problem at the core of numerous applications of medical robotic systems for minimally invasive surgery (MIS). With a variety of methods proposed in recent years, most approaches have focused on Simultaneous Localization and Mapping (SLAM) and Visual Odometry (VO) frameworks to solve the pose estimation problem. Well-established techniques such as ORB-SLAM2 [1] and ElasticFusion [2] have shown great promise in rigid scenes. More recently, non-rigid cases in MIS using monocular [3\u20135] and stereo-\nB Michel Hayoz michel.hayoz@unibe.ch\n1 ARTORG Center, University of Bern, Bern, Switzerland\n2 Department of Visceral Surgery and Medicine, Inselspital, Bern, Switzerland\n3 Applied Research, Intuitive Surgical, Sunnyvale, USA\nscopic cameras [6\u20138] have also been studied. Yet to this day, pose estimation in typical MIS settings remains particularly difficult due to deformations caused by instruments and breathing, self or instrument-based occlusions, textureless surfaces and tissue specularities.\nIn this work, we tackle the problem of pose estimation in such difficult cases when using a stereo endoscopic camera system.\nThis allows depth estimation to be computed from parallax, which has been shown to improve robustness of SLAM methods [1]. In contrast to [6, 7] which assume the tissue is smooth and locally rigid, respectively, we avoid making assumptions on the tissue deformation and topology. Instead, we propose a dense stereo VO framework that handles tissue deformations and the complexity of surgical scenes. To do this, our approach leverages geometric pose optimization by inferringwhere to look at in the scene. At its core, ourmethod uses a Deep Declarative Network (DDN) [9] to enable backpropagation of gradients through the pose optimization.\nMore specifically, we propose to integrate two adaptive weight maps with the role of balancing the contribution of two geometrical losses and the contribution of each pixel toward each of these losses. We learn these adaptive weight maps using a DDN with the goal of solving the pose estimation problem, inspired by the recent DiffPoseNet [10] approach. Similarly to theirs, ourmethod exploits the expressiveness of neural networkswhile leveraging robustness from the geometric optimization approach.This allowsourmethod to adapt the contribution of the image region depending on the image content, for each loss but also between the two losses. We thoroughly evaluate our approach by characterizing its performance in comparisonwith the state-of-the-art on various practical scenarios: rigid scenes, breathing, scanning and deforming sequences. This validation is performed on two different datasets, and we show that our method allows for more precise pose estimation in a wide range of cases."
        },
        {
            "heading": "Method",
            "text": "In the following, we present our depth-based pose estimation approach from an optimization perspective. We first derive ourmethod in terms of context-specific adaptiveweightmaps in the pose estimation optimization and then show how to learn these from data in an end-to-end way using DDNs to facilitate differentiation [9]. Our proposed method is illustrated inFig. 1, andweprovide a notation overview inTable 1.\nFor all images in an image sequence, we first employ RAFT [11] to establish correspondences between frames for both the stereo and temporal domain. This model allows disparity and optical flow estimation to be computed simultaneously, based on the observation that both share similar constraints on relative pixel displacements. From these estimates, we extract the horizontal component of the parallax flow F \u2032t at time t as the stereo disparity to compute depth maps Dt . As we would typically expect large vertical disparities in areas of low-texture or for de-calibrated stereo endoscopes, we use this parallax flow F \u2032t as input for the weight map estimation described in the following."
        },
        {
            "heading": "Pose estimation",
            "text": "In contrast to most existing VO methods, we estimate the camera motion exclusively based on a geometric loss function given that photometric consistency is entailed in optical flow estimation. We thus compute a 2D residual function based on a single-depth map by,\nr2D(pt , x) = \u221a 1\nXY\n\u2225\u2225\u2225\u03c02D ( exp(pt ) \u03c03D (Dt , x) )\n\u2212(x + Ft (x))\u2225\u22252 , (1)\nwhere\u03c02D(exp(pt ) \u03c03D(Dt , x)) is the pixel location based on depth projection and the relative camera pose pt that aligns views I(l)t to I(l)t\u22121. We scale these residuals by the image dimensions X andY tomake values independent of the image size. Note that we normalize depth maps by the maximum expected depth value, such that rotation and translation components of pt have the same order of magnitude and thus equally contribute to the residuals, which is important for a well-conditioned optimization. Ideally, a projected pixel position coincides with the optical flow when the observed scene is rigid and when flow and depth maps are correct.\nWhileminimizing r2D(pt , x) helps to reliably estimate the camera motion in rigid scenes, detection of deformations is most effective by looking at the displacement of points in 3D space. To address this need, we propose to leverage the depth map at t \u2212 1 and introduce a 3D residual function by,\nr3D(pt , x) = \u2225\u2225\u2225 exp(pt ) \u03c03D(Dt , x)\n\u2212 \u03c03D (Dt\u22121, x + Ft (x)) \u2225\u2225\u2225 2 , (2)\nwhich measures the point-to-point alignment of the reprojected depth maps. As opposed to [2], we avoid using a point-to-plane distance as it is less constrained on planar surfaces such as organs (e.g., liver). While a known weakness of the point-to-point distance is its sensitivity to noise in regions with large perspectives, we mitigate this effect by combining 2D and 3D residuals. Intuitively, we expect r2D(pt , x) to be most accurate when camera motion is large and r3D(pt , x) when deformations are significant. Similar to [11], we use bilinear sampling to warp point clouds from \u03c03D\n(Dt\u22121, x) to \u03c03D\n(Dt\u22121, x + Ft (x)), using our optical flow estimates. In contrast to conventional scalar-weighted sum of residuals, we propose to weigh each residual using a dedicated weight map that is inferred from the image data. The final residual is computed as,\nr(pt , x) = \u03c92D(x) r2D(pt , x) + \u03c93D(x) r3D(pt , x) , (3)\nwhere \u03c92D(x) and \u03c93D(x) are the per-pixel weight maps for the 2D and 3D residuals, respectively.\nAt its core, our hypothesis is that we can learn how the weight maps should combine the contributions of both \u03c92D(x) and \u03c93D(x) even in challenging situations where tissue deformations are taking place. That is, the role of (\u03c92D(x), \u03c93D(x)) is to (1) weigh relative focus based on the context of tissue deformations, (2) weigh reliable residual functions (2D vs 3D) given a motion pattern and (3) balance the scale of the loss. In \u201cLearning the weight maps\u201d section, we detail how we learn a model to infer these weight maps.\nOptimization: To compute the relative pose p t \u2208 se(3), we then optimize,\np t = argmin pt {\u2211 x\u2208 r(pt , x)2 } , (4)\nin a Nonlinear Least-Squares (NLLS) problem. Here, is a set containing all spatial image coordinates x. We choose to optimize the pose in the Lie algebra vector space se(3) because this is a unique representation of the pose and has the same number of parameters as degrees of freedom. NLLS problems are typically solved iteratively using a secondorder optimizer. To do this, we use the quasi-Newton method L-BFGS [12] due to its fast convergence properties and computational efficiency. Identical to [10], we simply chain relative camera poses to obtain the full trajectory."
        },
        {
            "heading": "Learning the weight maps",
            "text": "In Eq. (3), we propose to learn residual weight maps \u03c92D(x) and \u03c93D(x), as determining these otherwise is not trivial.\nTo this end, we train a separate encoder\u2013decoder network, denoted by g(\u00b7), for each weight map. The input to these networks is all the elements used to compute residuals, \u03c92D(x) =g ( x, I(l)t ,Dt ,Ft ,F \u2032t , \u03b82D ) , (5)\n\u03c93D(x) =g ( x, I(l)t ,Dt ,Ft ,F \u2032t , I(l)t\u22121,Dt\u22121,F \u2032t\u22121, \u03b83D ) ,\n(6)\nwhere \u03b82D and \u03b83D are the network parameters that are learned at training time. For g(\u00b7), we employ a 3-layer UNet [13] with Sigmoid activation function to ensure outputs in [0, 1].\nTo train g(\u00b7), we aim to learn weight maps that lead to improved pose estimation by minimizing the 1 supervised training loss,\nLtrain = \u2016p t \u2212 p(gt)t \u20161, (7)\nwhere p(gt)t is the ground-truth pose. Because the pose optimization in Eq. (4) is not directly differentiable, we leverage a DDN [9] to enable end-to-end learning. This approach uses\nimplicit differentiation of Eq. (4) to compute the gradients of the weight map parameters (\u03b82D, \u03b83D) with respect to Ltrain. Therefore, the only requirements are that (1) the function to be optimized \u2211 x\u2208 r(pt , x)2 is twice differentiable and (2) we find a local or global minimum in the forward pass."
        },
        {
            "heading": "Experiments",
            "text": ""
        },
        {
            "heading": "Datasets",
            "text": "We evaluate our method on two separate stereo video datasets: one containing rigid MIS scenes and another containing non-rigid scenes:\nSCARED dataset [14]: consists of 9 in vivo porcine subjects with 4 sequences for each subject. The dataset contains a video stream captured using a da Vinci Xi surgical robot and camera forward kinematics. All sequences show rigid scenes without breathing motion or surgical instruments. We split the dataset into training (d2, d3, d6, d7) and testing sequences (d1, d8, and d9) where we exclude d4 and d5 due to bad camera calibrations.\nStereoMIS: Additionally, we introduce a new in vivo dataset also recorded using a da Vinci Xi surgical robot. Similarly to [14], ground-truth camera poses are generated from the endoscope forward kinematics and synchronizedwith the video feed. While we expect errors in the absolute camera pose due to accumulated errors in the forward kinematics, relative camera motions are expected to be accurate. It consists of 3 porcine (P1, P2, and P3) and 3 human subjects (H1, H2, and H3) with a total of 16 recorded sequences. We denote sequences with Px_y where Px is the subject and y the sequence number. Sequence durations range from 50s to 30min. They contain challenging scenes with breathing motions, tissue deformations, resections, bleeding, and presence of smoke. We assign P1 and H1 to the training set and the rest is kept for testing.\nTo provide a finer grained performance characterization of methods with this data, we extract from each video a number of short sequences that visually depict one of several possible settings:\n\u2022 breathing: only depicts breathing deformations and contains no camera or tool motion, \u2022 scanning: includes camera motion in addition to breathing deformations, \u2022 deforming: comprises tissue deformations due to breathing and manipulation or resection of tissue, while the camera is static.\nIn practice, we select 88 different, non-overlapping, and 150- frames-long sequences from P2, P3, H2, H3 and assigned\neach to one of the above categories or surgical scenarios (see supplementary material for more information)."
        },
        {
            "heading": "Implementation details",
            "text": ""
        },
        {
            "heading": "Segmentation of surgical instruments",
            "text": "For all experiments, we mask out surgical instrument pixels by setting corresponding residuals to 0. To do this, we use the DeepLabv3+ architecture [15] trained on the EndoVis2018 segmentation dataset [16] to generate instrument masks for each frame. Additionally, we mask out specularities, by means of maximum intensity detection, as they cause optical flow estimations to be ill-defined."
        },
        {
            "heading": "Training and inference",
            "text": "First, we classify all training frames from the SCARED and StereoMIS training sequences into \"moving\" and \"static\" based on the camera forward kinematics. We then randomly sample 4000 frames from each sequence keeping a balance between moving and static frames. For each sampled frame, wegenerate a sample pairwith an interval of 1 to 5 frames.We use the forward kinematics of the camera as the ground truth poses change between two frames in a sample pair. Note that forward kinematics entail minor deviations that may propagate during training. We randomly assign 80% of the sample pairs to the training set and 20% to the validation set.\nFor all experiments, we resize images to half resolution (512x640 pixels).We use a batch size of 8 and theAdamoptimizer with learning rate 10\u22125. We train for 200 epochs and perform early stopping on the validation loss. We implement our method using PyTorch and train on a NVIDIA RTX3090 GPU. We reach 6.5 frames per second at test time. RAFT is trained on the FlyingThings3D dataset, and we do not perform any fine-tuning."
        },
        {
            "heading": "Metrics and baselinemethods",
            "text": "We use trajectory error metrics as defined in [17], namely the absolute trajectory error ATE-RMSE to evaluate the overall shape of the trajectory and the relative pose errors, RPE-trans and RPE-rot, to evaluate relative pose changes from frame to frame. The ATE-RMSE is sensitive to drift and depends on the length of the sequence, whereas the RPE measures the average performance for frame-to-frame pose estimation.\nAs no stereo SLAM method dedicated for MIS has opensource code or is evaluated on a public dataset with trajectory ground truth, we compare our method to two general stateof-the art rigid SLAMmethods that contain loop closure and are based on the rigid scene assumption:\n\u2022 ORB-SLAM2 [1], a sparse SLAM leveraging bundle adjustment to compensate drift, \u2022 ElasticFusion [2], a dense SLAM and as such closer to our proposed method.\nIn addition, we compare our method to [8] on the frames of the SCARED dataset for which they reported performances. For fair comparison, we use the same input depth maps for all methods."
        },
        {
            "heading": "Results",
            "text": "Surgical scenarios and ablation study: Table 2 reports the performance of our approach on the StereoMIS surgical scenarios. To show the importance of learning the weight maps we perform an ablation study where we evaluate the impact of (1) constant weights, denoted by ours (w/o weight), where \u03c92D(x) = \u03c93D(x) = 1 for each x; (2) our method with only 2D-residuals, denoted by ours (only 2D); and (3) using only 3D-residuals, denoted by ours (only 3D).\nOur proposedmethod outperforms the baselines in all scenarios. Improvements in breathing and scanning are partly due to correct identification of errors in the optical flow and depth estimation as well as optimal balancing of the 2D and 3D residuals. Indeed, exploiting the complementary properties of 2D and 3D residuals improves the average performance. The fact that ours (only 3D) outperforms ours (only 2D) in breathing and deforming supports our intuition that it is easier to learn tissue deformations from the 3D residuals. Contrarily, in scanning where the optical flow is dominated by the camera motion, the 2D residuals lead to a more accurate pose estimation.\nIn general, it is not possible to detect or completely compensate the breathing motion on a frame-to-frame basis in our proposed optimization scheme as we cannot completely disambiguate the camera and tissue motion. However, the method learns which regions are more affected by breathing deformations and consequently assigns a smaller weight to those regions.\nWenote that theweightmaps inFig. 2 (seebreathing rows) support our claims that theweightmapshave lowvalues in the dark regions (A) where we expect the optical flow to be inaccurate and where tissue is moving most (B). The scanning example also illustrates that the weight maps have a different response depending on the motion pattern and deformation. Note that the presence of surgical instruments has no influence on the weight maps in scanning, as no tissue interaction takes place. As expected, the largest improvements can be seen in the deforming scenario. Inspecting the two last rows in Fig. 2 reveals that regions where the instruments deform tissue (C) are correctly ignored in the pose estimation. Similarly, the region occluded by smoke (D) has low values in\nthe weight maps. Additionally, we observe that \u03c92D usually has 100 times larger magnitude than \u03c93D compensating for the different scale of the 2D and 3D residuals.\nResults on full test StereoMIS sequences: Table 3 shows the pose estimation performance on the complete sequences in the StereoMIS testset. As the sequences are much longer than in the scenario experiment, accumulation of drift results in a large ATE-RMSE for all methods. Even though our frame-to-frame approach does not include any bundle adjustment or regularization over time, it still has the lowest ATE-RMSE on average. The reason for this good performance is reflected in the relative metrics RPE-trans and RPE-rot, where our method outperforms all others by almost a factor of three and five, respectively. Our method robustly estimates the pose in challenging situations, whereas ORBSLAM2 fails in two sequences (H2_0, P2_5). Figure3 shows two example trajectories. P2_7 does not include any tool\u2013 tissue interactions and consists of smooth camera motions. Its trajectory illustrates the drift of our method which results in an ATE-RMSE of 9.28 versus 3.76mm for ORB-SLAM2. On the other hand, P3_0 consists of strong tissue deformations and abrupt cameramovements. Despite visible drift, we can see that our method is able to follow the abrupt movements. The small-scale oscillations in the trajectories are due to breathingmotion. The trajectories of all test sequences and evaluation results excluding frames where the SLAM methods fail can be found in the supplementary materials.\nResults on SCARED dataset: Wei et al. reported ATERMSE results for rigid surgical scenes of the SCARED dataset in a frame-to-model approach [8]. For the sake of fair comparison, we extend our method to SLAM by accommodating a surfel map model denoted by ours (frame2model), which is equivalent to that used in [8]. Specifically,we replace input images I(l)t\u22121, I(r)t\u22121 by rendered images from the surfel map. Note, we can only adopt this approach for the SCARED dataset, as the surfel map model assumes scene rigidity. Results are provided in Table 4."
        },
        {
            "heading": "Conclusion",
            "text": "We proposed a visual odometry method for robust pose estimation in the challenging context of endoscopic surgeries. To do so, we learn adaptive weight maps for two geometrical residuals to leverage pose estimation performance on common surgical scenes including breathing motion and tissue deformations. Thanks to a performance analysis in common scenarios, we observed the complementary action of the 2D/3D residuals and the strong contribution of their specific weighting at pixel level. This results in better performances compared to state-of-the-art methods, on average and in the most challenging cases.We believe that our contributions are\nTable 2 The ATE-RMSE (mean\u00b1std mm) for the different scenarios from the StereoMIS dataset with average over sequences (microavg.) and scenarios (macroavg.)\nScenario Breathing Scanning Deforming Microavg. Macroavg.\n# Sequences 17 60 9\nCamera motion Breathing Tool interactions ORB-SLAM2 [1] 2.35 \u00b1 1.81 3.26 \u00b1 1.65 4.29 \u00b1 2.30 3.19 \u00b1 1.81 3.30 \u00b1 0.97 ElasticFusion [2] 1.94 \u00b1 0.93 4.04 \u00b1 3.46 6.47 \u00b1 8.64 3.88 \u00b1 4.12 4.15 \u00b1 2.27 Ours (w/o weight) 1.65 \u00b1 0.97 3.01 \u00b1 1.60 4.67 \u00b1 2.13 2.91 \u00b1 1.74 3.11 \u00b1 1.51 Ours (only 2D) 1.15 \u00b1 0.72 3.01 \u00b1 1.66 2.83 \u00b1 1.41 2.62 \u00b1 1.66 2.33 \u00b1 1.03 Ours (only 3D) 0.78 \u00b1 2.03 7.02 \u00b1 5.86 2.72 \u00b1 1.90 5.34 \u00b1 5.64 3.51 \u00b1 3.20 Ours (2D & 3D) 1.01 \u00b1 0.59 2.89 \u00b1 2.33 2.23 \u00b1 1.07 2.45 \u00b1 2.12 2.04 \u00b1 0.95\nTable 3 Pose estimation results on full StereoMIS test sequences for ORB-SLAM2 [1], ElasticFusion [2], and ours. Metrics are reported in (mean\u00b1std) when applicable\nH2 H3 P2 P3 Macroavg"
        },
        {
            "heading": "ATE-RMSE (mm)",
            "text": "ORB-SLAM2 [1] 18.0 9.1 14.0 21.4 15.6 \u00b1 5.3 ElasticFusion [2] 30.8 72.1 33.6 37.7 43.6 \u00b1 19.3 Ours 10.9 21.2 13.8 8.8 13.7 \u00b1 5.4"
        },
        {
            "heading": "RPE-trans (mm)",
            "text": "ORB-SLAM2 [1] 0.20 \u00b1 0.43 0.24 \u00b1 0.25 0.35 \u00b1 0.46 0.54 \u00b1 0.47 0.33 \u00b1 0.13 ElasticFusion [2] 0.87 \u00b1 1.11 0.56 \u00b1 1.03 0.81 \u00b1 1.11 0.71 \u00b1 0.79 0.74 \u00b1 0.12 Ours 0.10 \u00b1 0.27 0.10 \u00b1 0.18 0.16 \u00b1 0.32 0.19 \u00b1 0.31 0.14 \u00b1 0.04"
        },
        {
            "heading": "RPE-rot (deg)",
            "text": "ORB-SLAM2 [1] 0.16 \u00b1 0.36 0.16 \u00b1 0.22 0.19 \u00b1 0.24 0.28 \u00b1 0.27 0.20 \u00b1 0.05 ElasticFusion [2] 0.73 \u00b1 1.06 0.41 \u00b1 0.96 0.50 \u00b1 1.11 0.38 \u00b1 0.40 0.51 \u00b1 0.14 Ours 0.04 \u00b1 0.20 0.04 \u00b1 0.13 0.07 \u00b1 0.14 0.05 \u00b1 0.10 0.05 \u00b1 0.01\nbeneficial for some SLAM components, e.g., map building, and therefore downstream applications in MIS. Future work will focus on drift and breathing compensation.\nSupplementary information Appendix A: details on StereoMIS. Appendix B: trajectories of StereoMIS test set. Appendix C: Results on full StereoMIS sequences. Appendix D: trajectories of SCARED test set.\nSupplementary Information The online version contains supplementary material available at https://doi.org/10.1007/s11548-023-02919w.\nFunding Open access funding provided by University of Bern."
        },
        {
            "heading": "Declarations",
            "text": "Funding This work was supported by InnoSuisse grant # 50204.1 IPLS.\nConflict of interest Authors declare that theyhave no conflict of interest.\nEthics approval All applicable international, national, and institutional guidelines for the care and use of animalswere followed. All procedures performed in studies involving animals were in accordance with the ethical standards of the institution at which the studies were conducted.\nConsent Not applicable.\nAvailability of data, materials, and code StereoMIS porcine datahttps://doi.org/10.5281/zenodo.7727692. Code and models - https:// github.com/aimi-lab/robust-pose-estimator.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article\u2019s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article\u2019s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, youwill need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecomm ons.org/licenses/by/4.0/."
        }
    ],
    "title": "Learning how to robustly estimate camera pose in endoscopic videos",
    "year": 2023
}