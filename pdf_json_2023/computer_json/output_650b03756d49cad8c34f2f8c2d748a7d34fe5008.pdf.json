{
    "abstractText": "Apple scab is a fungal disease caused by Venturia inaequalis. Disease is of particular concern for growers, as it causes significant damage to fruit and leaves, leading to loss of fruit and yield. This article examines the ability of deep learning and hyperspectral imaging to accurately identify an apple symptom infection in apple trees. In total, 168 image scenes were collected using conventional RGB and Visible to Near-infrared (VIS-NIR) spectral imaging (8 channels) in infected orchards. Spectral data were preprocessed with an Artificial Neural Network (ANN) trained in segmentation to detect scab pixels based on spectral information. Linear Discriminant Analysis (LDA) was used to find the most discriminating channels in spectral data based on the healthy leaf and scab infested leaf spectra. Five combinations of false-colour images were created from the spectral data and the segmentation net results. The images were trained and evaluated with a modified version of the YOLOv5 network. Despite the promising results of deep learning using RGB images (P=0.8, mAP@50=0.73), the detection of apple scab in apple trees using multispectral imaging proved to be a difficult task. The high-light environment of the open field made it difficult to collect a balanced spectrum from the multispectral camera, since the infrared channel and the visible channels needed to be constantly balanced so that they did not overexpose in the images.",
    "authors": [
        {
            "affiliations": [],
            "name": "Robert Rou\u0161a"
        },
        {
            "affiliations": [],
            "name": "Joseph Pellerb"
        },
        {
            "affiliations": [],
            "name": "Gerrit Polderb"
        },
        {
            "affiliations": [],
            "name": "Selwin Hageraatsb"
        },
        {
            "affiliations": [],
            "name": "Thijs Ruigrokc"
        },
        {
            "affiliations": [],
            "name": "Pieter M. Blokb"
        }
    ],
    "id": "SP:66bf30d23f7798586350d0edbf8064ea239dcd23",
    "references": [
        {
            "authors": [
                "M. Afonso",
                "P.M. Blok",
                "G. Polder",
                "J.M. van der Wolf",
                "J. Kamp"
            ],
            "title": "Blackleg detection in potato plants using convolutional neural networks",
            "venue": "IFAC-PapersOnLine 52,",
            "year": 2019
        },
        {
            "authors": [
                "C. Bi",
                "J. Wang",
                "Y. Duan",
                "B. Fu",
                "J.R. Kang",
                "Y. Shi"
            ],
            "title": "MobileNet based apple leaf diseases identification",
            "venue": "Mobile Networks and Applications 27,",
            "year": 2020
        },
        {
            "authors": [
                "A. Fazari",
                "O.J. Pellicer-Valero",
                "J. G\u00f3mez-Sanch\u0131s",
                "B. Bernardi",
                "S. Cubero",
                "S. Benalia",
                "G. Zimbalatti",
                "J. Blasco"
            ],
            "title": "Application of deep convolutional neural networks for the detection of anthracnose in olives using VIS/NIR hyperspectral images",
            "venue": "Computers and Electronics in Agriculture",
            "year": 2021
        },
        {
            "authors": [
                "G. G",
                "J.A.P"
            ],
            "title": "Identification of plant leaf diseases using a nine-layer deep convolutional neural network. Computers & Electrical Engineering",
            "year": 2019
        },
        {
            "authors": [
                "N. Gorretta",
                "M. Nouri",
                "A. Herrero",
                "A. Gowen",
                "J.M. Roger"
            ],
            "title": "Early detection of the fungal disease",
            "venue": "\u201dapple scab\u201d using swir hyperspectral imaging,",
            "year": 2019
        },
        {
            "authors": [
                "S. Guti\u00e9rrez",
                "J. Fern\u00e1ndez-Novales",
                "M.P. Diago",
                "J. Tardaguila"
            ],
            "title": "Onthe-go hyperspectral imaging under field conditions and machine learning for the classification of grapevine varieties",
            "venue": "Frontiers in Plant Science",
            "year": 2018
        },
        {
            "authors": [
                "D.P. Hughes",
                "M. Salathe"
            ],
            "title": "An open access repository of images on plant health to enable the development of mobile disease",
            "venue": "diagnostics. doi:10",
            "year": 2015
        },
        {
            "authors": [
                "A. Kamilaris",
                "F.X. Prenafeta-Bold\u00fa"
            ],
            "title": "Deep learning in agriculture: A survey. Computers and electronics in agriculture",
            "year": 2018
        },
        {
            "authors": [
                "B. Liu",
                "Y. Zhang",
                "D. He",
                "Y. Li"
            ],
            "title": "Identification of apple leaf diseases",
            "year": 2017
        },
        {
            "authors": [
                "S.P. 3390/sym10010011. Mohanty",
                "D.P. Hughes",
                "M. Salath\u00e9"
            ],
            "title": "Using deep learning for image",
            "year": 2016
        },
        {
            "authors": [
                "M.T. Kwasniewski"
            ],
            "title": "Early detection of plant viral disease",
            "year": 2021
        },
        {
            "authors": [
                "S. s21030742. Paulus",
                "A.K. Mahlein"
            ],
            "title": "Technical workflows for hyperspectral plant",
            "year": 2020
        },
        {
            "authors": [
                "J. Kamp"
            ],
            "title": "Potato virus y detection in seed potatoes",
            "year": 2019
        },
        {
            "authors": [
                "I. Smirnov",
                "D. Khort",
                "A. Aksenov",
                "A. Kuzin"
            ],
            "title": "Linking tissue damage",
            "year": 2021
        },
        {
            "authors": [
                "D. s10462-021-10018-y. Zhang",
                "F. Lin",
                "Y. Huang",
                "X. Wang",
                "L. Zhang"
            ],
            "title": "Detection of wheat",
            "year": 2016
        }
    ],
    "sections": [
        {
            "text": "Apple scab is a fungal disease caused by Venturia inaequalis. Disease is of particular concern for growers, as it causes significant damage to fruit and leaves, leading to loss of fruit and yield. This article examines the ability of deep learning and hyperspectral imaging to accurately identify an apple symptom infection in apple trees. In total, 168 image scenes were collected using conventional RGB and Visible to Near-infrared (VIS-NIR) spectral imaging (8 channels) in infected orchards. Spectral data were preprocessed with an Artificial Neural Network (ANN) trained in segmentation to detect scab pixels based on spectral information. Linear Discriminant Analysis (LDA) was used to find the most discriminating channels in spectral data based on the healthy leaf and scab infested leaf spectra. Five combinations of false-colour images were created from the spectral data and the segmentation net results. The images were trained and evaluated with a modified version of the YOLOv5 network. Despite the promising results of deep learning using RGB images (P=0.8, mAP@50=0.73), the detection of apple scab in apple trees using multispectral imaging proved to be a difficult task. The high-light environment of the open field made it difficult to collect a balanced spectrum from the multispectral camera, since the infrared channel and the visible channels needed to be constantly balanced so that they did not overexpose in the images.\nKeywords: Plant disease detection, Computer vision, Spectral imaging, Deep learning, Integrated pest management PACS: 0000, 1111 2000 MSC: 0000, 1111"
        },
        {
            "heading": "1. Introduction",
            "text": "According to FAOSTAT (FAO, 2022) worldwide production of apples in the last 10 years averages more than 80 million tons annually. In terms of production, apple ranks third. Therefore, apple is a significant commercial fruit crop, and any research and development in the apple growing industry has an economic potential worthy of attention.\nApple scab is a fungal disease caused by Venturia inaequalis that affects apple trees and fruits. The disease is of particular concern for growers as it causes significant damage to fruit and leaves, leading to reduced yield and marketability. Traditionally, the evaluation of diseases and stresses is based on visual inspection and destructive diagnostic methods. However, these methods are time-consuming and labour intensive and do not allow for early detection of the disease. Automatic detection could alleviate this problem, while simultaneously reducing the application of plant protection agents.\nIn the last decade, much work has been done on the spectral imaging of diseases in agriculture. Spectral imaging allows for direct detection of pests in the plant, as well as changes in the physiology of the plant as it is infected. Studies have shown that spectroscopy can be used to detect mildew formation in leaves as the mycelium penetrates the leaf. Therefore, it is possible to use spectral imaging to detect such diseases in wine grapes (Knauer et al., 2017) or wheat (Zhang et al., 2016). Spectral imaging can also be used to detect chlorosis caused by\nnecrotrophic diseases such as Alternaria (Vijver et al., 2020) or scab in short-wave infrared (SWIR) spectra (Gorretta et al., 2019) or VIS-NIR (Solovchenko et al., 2021). Stress-induced biophysical and biochemical modifications will directly affect leaf reflectance obtained by spectral imaging. However, simply detecting these changes in the spectrum of the plant is often not enough, as these symptoms can be caused by multiple factors. Therefore, it is necessary to combine these techniques with machine learning and deep learning.\nDeep learning has been extensively researched and developed in the past decade. In particular, convolutional neural networks (CNNs) are being used increasingly for disease and pest detection applications (Kamilaris and Prenafeta-Boldu\u0301, 2018). The advantage of CNNs over traditional machine learning methods is their ability to automatically discriminate and learn features that would be difficult to be hand-crafted by humans (LeCun et al., 2015). Succesful applications of CNNs for plant disease detection include fusarium head blight detection in wheat (Qiu et al., 2019), blackleg detection in potato (Afonso et al., 2019), anthracnose detection in olives (Fazari et al., 2021) and the detection of alternaria leaf blotch and rust disease in apple leaves (Bi et al., 2020).\nMany recent articles use publicly available image data, such as the PlantVillage dataset (Hughes and Salathe, 2015) to train CNNs on RGB images to detect various diseases. Mohanty et al. (2016) or G. and J. (2019) use these data to compare differ-\nPreprint submitted to Computers and Electronics in Agriculture February 20, 2023\nar X\niv :2\n30 2.\n08 81\n8v 1\n[ cs\n.C V\n] 1\n7 Fe\nb 20\n23\nent types of neural network for disease detection in general, Liu et al. (2017) uses them to detect apple leaf diseases. Their results show that this approach is valid for this kind of task with an overall accuracy greater than 95%, but real-world applications can bring about more challenging situations. Such as changing weather conditions and thus not-stable lighting conditions. This problem can be mitigated by using custom imaging systems with top-view camera perspective that also allow the camera to be shielded by a box (with artificial light) and are moved over imaged plants. However, this is applicable only for crops such as potato plants (Vijver et al., 2020; Polder et al., 2019). Spectral imaging under field conditions for crops such as apple trees can be performed from the top with an unmanned aerial vehicle (UAV) or by using side-view imaging as proposed by Karpyshev et al. (2021) or Nguyen et al. (2021). Field conditions with natural light that also require the use of white reference (Gutie\u0301rrez et al., 2018; Paulus and Mahlein, 2020) are challenging due to the setting of the exposition time. The application of object detection also depends on object occlusion. If a real-time application is needed, for example, implementation into mobile agricultural machinery, the inference speed is also an important factor, and the resulting application is usually a trade-off between speed and accuracy.\nRelying only on \u201dobject detection\u201d methods from conventional RGB cameras can have disadvantages in terms of misinterpreting naturally occurring shapes in healthy plants as a disease.\nAttempts have been made to incorporate spectral imaging and deep learning. More than 30 publications have dealt with the application of deep learning in spectral imaging in agriculture in recent years (Wang et al., 2021). However, the large number of image layers in a spectral data array makes direct network training difficult.\nA reliable, accurate, and nondestructive measure is essential to quickly detect the incidence of diseases in crops to allow for timely intervention to prevent disease from spreading across the field. This research article investigates the use of spectral images in neural networks to detect apple scab in orchards and assesses their potential for effective disease management. Specifically, the article examines the ability of deep learning and hyperspectral imaging to accurately identify apple scab on apple trees, as well as the potential of these technologies to improve orchard management strategies. The implications of the findings for orchard management are also discussed."
        },
        {
            "heading": "2. Materials and Methods",
            "text": ""
        },
        {
            "heading": "2.1. Imaging system and acquisition",
            "text": "An imaging platform was deployed in the field with several cameras. A camera plate was designed to fit on a standard tripod and was controlled via a laptop computer. A RGB camera, an IDS 10 Mpixel NXT camera, and a SILIOS CMS4-V eightband spectral camera were attached to the plate. The SILIOS acquires images in nine channels, eight for each specific wavelength (545, 579, 622, 658, 701, 737, 779, and 816 nm) and one PAN (panchromatic) channel measuring the light intensity\nacross all eight channels. The resolution of the spectral images is 682 by 682 pixels. Both cameras were equipped with the same Tamron 8 mm adjustable lens and recorded an overlapping field of view. The imaging system is shown in Figure 1."
        },
        {
            "heading": "2.2. Samples and experiment location",
            "text": "Images were collected in apple orchards outside of E\u0301pila, Spain, in May 2022. Scouting was performed with the assistance of crop experts who helped identify areas of scab for imaging. Once an area was identified, a set of images was collected so that each RGB image had a corresponding spectral image. To ensure that the apple scab could be easily identified, an additional image with the expert pointing to the visible scab was also collected. In total, 168 image scenes were acquired. Each of these scenes contained one or more areas of apple scab. The 168 image scenes cover a total of 325 areas of apple scab."
        },
        {
            "heading": "2.3. Spectral data (pre)processing",
            "text": "In this research, the purpose of using spectral imaging data was to enhance the contrast between healthy and symptomatic leaf tissue, compared to the contrast captured by a regular RGB camera. Two tailored machine-learning based image processing procedures were developed to convert the eight-channel spectral images into images of reduced dimensionality that specifically highlight areas that are spectrally similar to scab symptoms.\nPrior to any dimensionality reduction procedures, all channels in the spectral images were first normalised using linear coefficients obtained from the PAN channel intensity of a white colour checker board. The coefficients were calculated from Equation 1 for each channel for every dataset where the calibration image was taken.\nnorm. coefficientchannel = channel intensity\nPAN intensity (1)\nThen, in order to provide the two machine learning-based image processing methods with enough information on the spectral differences between healthy and symptomatic leaf tissue,\nten normalised eight-channel spectral images were annotated by drawing masks over the visible scab symptoms. This automatically provided class labels for more than four million pixels and the associated eight-channel spectra.\nIn the first approach, a linear discriminant analysis (LDA) model was trained on all available labelled spectra. The LDA model weights then provide an indication for the discriminative power of each of the eight spectral channels. By selecting the three channels with the highest weights and combining them into false-colour RGB images, a dataset was generated that could be used for subsequent deep learning-based symptom detection.\nIn the second approach, a simple classification artificial neural network (ANN) was trained on the labelled spectra to discriminate between pixels belonging to symptomatic leaf tissue and pixels belonging to anything else (healthy leaf tissue, branches, soil, etc.). The neural network consisted of four layers: one input layer (8 nodes), two hidden layers (16 & 8 nodes), and one output layer (2 nodes). Instead of using only the eight output channels of the camera, a series of four spatial convolutions were performed on each of the eight channel images to extend the feature vector to length 40. The four kernels\u2013 described by Equation 2, and shown in Figure 2\u2014are meant to include some contextual information at different scales, while also providing spectral information at a higher signal-to-noise ratio in the direct vicinity of the central pixel.\nvalue(x, y) = e \u22121 \u03c32 \u00b7 \u221a (x\u2212\u03b1)2+(y\u2212\u03b1)2\u2212\u03b22 (2)\nThe four kernels used in this study were calculated with parameters: \u03b1 = 50, \u03b2 = 4, 8, 16, 32, and \u03c3 = 0.781, 1.56, 3.13, 6.25, where \u03c3 is the radial standard deviation, \u03b1 is half the dimension of the entire kernel matrix, and \u03b2 is proportional to the radius of each kernel\u2019s doughnut-shaped intensity profile.\nThe ANN classification model was trained on a random subset of the 4+ million feature vectors, after which the resulting weights were used to do per-pixel inference on the remaining images. The outputs of the final layer\u2019s softmax function for all pixels were then compiled into scab-specific probability maps, which were used to train subsequent deep-learning based symptom detection models. The image data processing pipeline is shown in Figure 3."
        },
        {
            "heading": "2.4. False-colour Image Sets",
            "text": "To compare the efficacy of the neural network using multispectral data, five image sets were created. Each image set was composed of false-colour images comprised of the bands from the multispectral camera or from the mask created from\nthe segmentation network. The priority of the selected bands was based on the strength of each wavelength using the LDA weighting vector. It was used to select image channels in image sets. Different image sets that were compared are shown in Table 1."
        },
        {
            "heading": "2.5. Deep learning",
            "text": "The 168 images were divided into training, validation and test set. The training set consisted of 118 images (70%), and these images were used to optimise the neural network weights during training. The validation set consisted of 25 images (15%) and these images were used during training to check whether the trained model was overfitting. The test set consisted of 25 images (15%) and these images were independent of the training process and therefore suitable for evaluation.\nThe images were trained and evaluated with a modified version of the You Only Look Once (YOLO) network (version 5) (Jocher, 2020). Our modification involved the customisation of the standard data loader of the YOLOv5 network so that images with more than three image channels could be processed. Our modified data loader stacked the tensors of the input layer in sets of three channels, allowing us to also analyse six-channel and nine-channel images. The input layer of the detection model was changed accordingly to create an input layer that could process three, six, or nine input channels, respectively. The transfer-learnt weights of the first layer were also stacked accordingly from the first three channels of a YOLOv5 network that was pre-trained in the Microsoft Common Objects in Context (COCO) dataset (Lin et al., 2014). We used YOLOv5x as our network architecture. This architecture was considered to provide the highest accuracy according to the results of Jocher (2020).\nDuring training, two types of data augmentation were applied: geometric transformations (rotation, translation, scaling, and horizontal and vertical flips) and image assemblage (merging multiple images into one composite image). The YOLOv5x network was trained with a batch size of 8 images and the number of training epochs was 100. The software of Jocher (Jocher, 2020) automatically saved the network weights with the best performance in the validation set. The best performing weights were then used for independent testing. This testing was performed with a threshold of 0.1 on the network confidence level and a threshold of 0.2 on the non-maximum\nsuppression (NMS). This NMS threshold allowed for marginal overlapping bounding boxes, which was deemed appropriate given our image annotations."
        },
        {
            "heading": "3. Results and discussion",
            "text": ""
        },
        {
            "heading": "3.1. Segmentation network results",
            "text": "Figure 4 shows the spectral data for two selected areas of the images, the scab and the healthy leaves. These graphs were created from 170 000 pixels for each class from randomly sampled 82 labelled images throughout the entire data set. These images were expertly annotated and mean values plotted. Two shaded areas represent borders of two standard deviations, and it is apparent that there is overlap in the spectral signatures of healthy and unhealthy leaves. Because of this, a more elaborate algorithm such as multilayer perceptron was required to separate the classes in the multidimensional image.\nA subset of 45 000 pixels from previously plotted data for each class was used to train the LDA. LDA was used to find the most discriminating channels using leaf pixels against scab pixels. Using the LDA weight vector areas of the spectrum that contribute the most to discrimination, this weighting vector can\nbe seen in the Figure 5. The confusion matrix for LDA is given in Table 2. An ordered vector of wavelengths from most to least discriminating was created to be used in the generation of false-colour images.\nThe order of wavelengths from most to least discriminating was 779 nm, 622 nm, 545 nm, 701 nm, 579 nm, 737 nm, 816 nm, 658 nm.\nFive combinations of false-colour images were created from spectral data and the segmentation net result. The preview of them is shown in figure 6."
        },
        {
            "heading": "3.2. Deep learning results",
            "text": "Table 3 and Figure 7 show the results of the deep learningbased object detection. The best object detection performance was achieved with the RGB images (image set 1). The F1 score when using RGB images was 0.16 to 0.21 higher than when using multispectral image sets. Both mAP scores were also significantly better when using RGB images (Table 3). This\nresult was not expected given the better colour contrast of the multispectral images compared to the RGB images (Figure 6)."
        },
        {
            "heading": "3.3. Discussion",
            "text": "Despite the promising results of deep learning using RGB images, the detection of apple scab in apple trees using multispectral imaging proved to be a difficult task. While the deep learning model trained on the spectral images was able to detect some general patterns, the model was unable to detect disease with the same accuracy as the YOLO trained on the RGB images. This is an interesting result, as the spectral images showed a much higher visual contrast between the apple scab lesions and healthy leaves compared to the RGB images (Figure 6). This may be due to the use of pretrained networks in this study. Pretrained networks use RGB image sets, such as MS COCO, to initialise the weights of the network before training. If the false-colour images were not similar enough to these\nimages, then that could explain the outstanding performance of RGB images over false-colour. Furthermore, the results may be affected by the outdoor environment in which apple trees are grown. False positives in the images were shown to be caused by shadows on leaves and other diseases such as leaf miners. The high-light environment of the open field made it difficult to collect a balanced spectrum from the multispectral camera as the infrared channel and the visible channels needed to be constantly balanced so that they did not overexpose in the images. These results suggest that more research is needed to deploy deep learning networks to spectral datasets. Further research is needed to determine whether more detailed data may be needed to improve the accuracy of deep learning models or if more finely tuned models are needed to handle spectral image data."
        },
        {
            "heading": "4. Conclusion",
            "text": "In this experiment, the efficacy of deep learning detection of apple scab disease was compared on multispectral and traditional RGB images. It was found that using multispctral imaging can be used to increase the contrast drastically between healthy and infected tissue; however, counter-intuitively, this did not lead to better results from the neural network detector.\nSeveral hypotheses for this were considered, including the small size of the dataset, or the effects of transfering learning on multispectral images that had been based on RGB colour images, specifically the COCO dataset.\nThe higher contrast in multispectral images still suggests that they may be used in the future with neural networks with further\ngroundwork. We will continue to explore this topic in future research and encourage other researchers to do so as well.\nCRediT authorship contribution statement\nRobert Rous\u030c: Software, Investigation, Methodology, Writing - Original Draft, Visualization; Joseph Peller: Methodology, Software, Writing - Original Draft & Editing; Gerrit Polder: Conceptualisation, Supervision, Project administration, Writing \u2013 Review & Editing; Selwin Hageraats: Software, Writing - Original Draft; Thijs Ruigrok: Software, Writing - Review & Editing; Pieter M. Blok: Software, Data Curation, Writing - Review & Editing."
        },
        {
            "heading": "Acknowledgement",
            "text": "This paper is supported by European Union\u2019s Horizon 2020 research and innovation programme under grant agreement No 773718, project OPTIMA (Optimised Pest Integrated Management to precisely detect and control plant diseases in perennial crops and open-field vegetables). We would like to acknowledge the following people from the Universitat Polite\u0300cnica de Catalunya (UPC): Emilio Gil (UPC), Paula Ortega (UPC), Fran Garcia (UPC)."
        }
    ],
    "title": "Apple scab detection in orchards using deep learning on colour and multispectral images",
    "year": 2023
}