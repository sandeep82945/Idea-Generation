{
    "abstractText": "Diffusion (score-based) generative models have been widely used for modeling various types of complex data, including images, audios, and point clouds. Recently, the deep connection between forward-backward stochastic differential equations (SDEs) and diffusion-based models has been revealed, and several new variants of SDEs are proposed (e.g., sub-VP, critically-damped Langevin) along this line. Despite the empirical success of the hand-crafted fixed forward SDEs, a great quantity of proper forward SDEs remain unexplored. In this work, we propose a general framework for parameterizing the diffusion model, especially the spatial part of the forward SDE. An abstract formalism is introduced with theoretical guarantees, and its connection with previous diffusion models is leveraged. We demonstrate the theoretical advantage of our method from an optimization perspective. Numerical experiments on synthetic datasets, MINIST and CIFAR10 are also presented to validate the effectiveness of our framework.",
    "authors": [
        {
            "affiliations": [],
            "name": "Weitao Du"
        },
        {
            "affiliations": [],
            "name": "Tao Yang"
        }
    ],
    "id": "SP:fee74c947aacbd6b1a1c1e45d69121cf883a7159",
    "references": [
        {
            "authors": [
                "Luis Alvarez",
                "Pierre-Louis Lions",
                "Jean-Michel Morel"
            ],
            "title": "Image selective smoothing and edge detection by nonlinear diffusion",
            "venue": "ii. SIAM Journal on numerical analysis, 29:845\u2013866,",
            "year": 1992
        },
        {
            "authors": [
                "Brian DO Anderson"
            ],
            "title": "Reverse-time diffusion equation models",
            "venue": "Stochastic Processes and their Applications,",
            "year": 1982
        },
        {
            "authors": [
                "Fan Bao",
                "Chongxuan Li",
                "Jun Zhu",
                "Bo Zhang"
            ],
            "title": "Analytic-dpm: an analytic estimate of the optimal reverse variance in diffusion probabilistic models",
            "venue": "arXiv preprint arXiv:2201.06503,",
            "year": 2022
        },
        {
            "authors": [
                "Luc Rey Bellet"
            ],
            "title": "Ergodic properties of markov processes. In Open quantum systems II, pages 1\u201339",
            "year": 2006
        },
        {
            "authors": [
                "Michael Betancourt"
            ],
            "title": "A general metric for riemannian manifold hamiltonian monte carlo",
            "venue": "In International Conference on Geometric Science of Information,",
            "year": 2013
        },
        {
            "authors": [
                "Michael Betancourt"
            ],
            "title": "A conceptual introduction to hamiltonian monte carlo",
            "venue": "arXiv preprint arXiv:1701.02434,",
            "year": 2017
        },
        {
            "authors": [
                "Ricky TQ Chen",
                "Jens Behrmann",
                "David K Duvenaud",
                "J\u00f6rn-Henrik Jacobsen"
            ],
            "title": "Residual flows for invertible generative modeling",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Giuseppe Da Prato"
            ],
            "title": "Introduction to stochastic analysis and Malliavin calculus, volume",
            "year": 2014
        },
        {
            "authors": [
                "Sybren Ruurds De Groot",
                "Peter Mazur"
            ],
            "title": "Non-equilibrium thermodynamics",
            "venue": "Courier Corporation,",
            "year": 2013
        },
        {
            "authors": [
                "Prafulla Dhariwal",
                "Alexander Nichol"
            ],
            "title": "Diffusion models beat gans on image synthesis",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Laurent Dinh",
                "Jascha Sohl-Dickstein",
                "Samy Bengio"
            ],
            "title": "Density estimation using real nvp",
            "venue": "arXiv preprint arXiv:1605.08803,",
            "year": 2016
        },
        {
            "authors": [
                "Tim Dockhorn",
                "Arash Vahdat",
                "Karsten Kreis"
            ],
            "title": "Score-based generative modeling with critically-damped langevin diffusion",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2022
        },
        {
            "authors": [
                "Weitao Du",
                "He Zhang",
                "Yuanqi Du",
                "Qi Meng",
                "Wei Chen",
                "Bin Shao",
                "Tie-Yan Liu"
            ],
            "title": "Equivariant vector field network for many-body system modeling, 2021",
            "year": 2021
        },
        {
            "authors": [
                "Charles Fefferman",
                "Sanjoy Mitter",
                "Hariharan Narayanan"
            ],
            "title": "Testing the manifold hypothesis",
            "venue": "Journal of the American Mathematical Society,",
            "year": 2016
        },
        {
            "authors": [
                "Chris Finlay",
                "J\u00f6rn-Henrik Jacobsen",
                "Levon Nurbekyan",
                "Adam Oberman"
            ],
            "title": "How to train your neural ode: the world of jacobian and kinetic regularization",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Sacha Friedli",
                "Yvan Velenik"
            ],
            "title": "Statistical Mechanics of Lattice Systems: A Concrete Mathematical Introduction",
            "year": 2017
        },
        {
            "authors": [
                "Mark Girolami",
                "Ben Calderhead"
            ],
            "title": "Riemann manifold langevin and hamiltonian monte carlo methods",
            "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),",
            "year": 2011
        },
        {
            "authors": [
                "Ian Goodfellow",
                "Jean Pouget-Abadie",
                "Mehdi Mirza",
                "Bing Xu",
                "David Warde-Farley",
                "Sherjil Ozair",
                "Aaron Courville",
                "Yoshua Bengio"
            ],
            "title": "Generative adversarial nets",
            "venue": "Advances in neural information processing systems,",
            "year": 2014
        },
        {
            "authors": [
                "Will Grathwohl",
                "Ricky TQ Chen",
                "Jesse Bettencourt",
                "Ilya Sutskever",
                "David Duvenaud"
            ],
            "title": "Ffjord: Free-form continuous dynamics for scalable reversible generative models",
            "venue": "arXiv preprint arXiv:1810.01367,",
            "year": 2018
        },
        {
            "authors": [
                "Martin Heusel",
                "Hubert Ramsauer",
                "Thomas Unterthiner",
                "Bernhard Nessler",
                "Sepp Hochreiter"
            ],
            "title": "Gans trained by a two time-scale update rule converge to a local nash equilibrium",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Lars H\u00f6rmander"
            ],
            "title": "Hypoelliptic second order differential equations",
            "venue": "Acta Mathematica,",
            "year": 1967
        },
        {
            "authors": [
                "E.P. Hsu"
            ],
            "title": "Stochastic Analysis on Manifolds",
            "venue": "Stochastic Analysis on Manifolds,",
            "year": 2002
        },
        {
            "authors": [
                "Chin-Wei Huang",
                "Jae Hyun Lim",
                "Aaron Courville"
            ],
            "title": "A variational perspective on diffusionbased generative models and score matching",
            "venue": "arXiv preprint arXiv:2106.02808,",
            "year": 2021
        },
        {
            "authors": [
                "Aapo Hyv\u00e4rinen"
            ],
            "title": "Estimation of non-normalized statistical models by score matching",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2005
        },
        {
            "authors": [
                "Aapo Hyv\u00e4rinen",
                "Peter Dayan"
            ],
            "title": "Estimation of non-normalized statistical models by score matching",
            "venue": "Journal of Machine Learning Research,",
            "year": 2005
        },
        {
            "authors": [
                "Alexia Jolicoeur-Martineau",
                "Ke Li",
                "R\u00e9mi Pich\u00e9-Taillefer",
                "Tal Kachman",
                "Ioannis Mitliagkas"
            ],
            "title": "Gotta go fast when generating data with score-based models, 2021",
            "year": 2021
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Tim Salimans",
                "Ben Poole",
                "Jonathan Ho"
            ],
            "title": "Variational diffusion models",
            "venue": "arXiv preprint arXiv:2107.00630,",
            "year": 2021
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Max Welling"
            ],
            "title": "Auto-encoding variational bayes",
            "venue": "arXiv preprint arXiv:1312.6114,",
            "year": 2013
        },
        {
            "authors": [
                "Durk P Kingma",
                "Prafulla Dhariwal"
            ],
            "title": "Glow: Generative flow with invertible 1x1 convolutions",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Zhifeng Kong",
                "Wei Ping",
                "Jiaji Huang",
                "Kexin Zhao",
                "Bryan Catanzaro"
            ],
            "title": "Diffwave: A versatile diffusion model for audio synthesis",
            "venue": "arXiv preprint arXiv:2009.09761,",
            "year": 2020
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Geoffrey Hinton"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "year": 2009
        },
        {
            "authors": [
                "Yann LeCun"
            ],
            "title": "The mnist database of handwritten digits. http://yann",
            "venue": "lecun. com/exdb/mnist/,",
            "year": 1998
        },
        {
            "authors": [
                "Songxiang Liu",
                "Yuewen Cao",
                "Dan Su",
                "Helen Meng"
            ],
            "title": "Diffsvc: A diffusion probabilistic model for singing voice conversion",
            "venue": "arXiv preprint arXiv:2105.13871,",
            "year": 2021
        },
        {
            "authors": [
                "Shitong Luo",
                "Wei Hu"
            ],
            "title": "Diffusion probabilistic models for 3d point cloud generation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Anton Mallasto",
                "Aasa Feragen"
            ],
            "title": "Learning from uncertain curves: The 2-wasserstein metric for gaussian processes",
            "venue": "In Proceedings of the 31st International Conference on Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Hariharan Narayanan",
                "Sanjoy Mitter"
            ],
            "title": "Sample complexity of testing the manifold hypothesis",
            "venue": "Advances in neural information processing systems,",
            "year": 2010
        },
        {
            "authors": [
                "Alexander Quinn Nichol",
                "Prafulla Dhariwal"
            ],
            "title": "Improved denoising diffusion probabilistic models",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Derek Onken",
                "S Wu Fung",
                "Xingjian Li",
                "Lars Ruthotto"
            ],
            "title": "Ot-flow: Fast and accurate continuous normalizing flows via optimal transport",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Achraf Oussidi",
                "Azeddine Elhassouny"
            ],
            "title": "Deep generative models: Survey",
            "venue": "In 2018 International Conference on Intelligent Systems and Computer Vision (ISCV),",
            "year": 2018
        },
        {
            "authors": [
                "P Perona",
                "J Malik"
            ],
            "title": "Scale-space and edge detection using anisotropic diffusion",
            "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence,",
            "year": 1990
        },
        {
            "authors": [
                "Xuanchi Ren",
                "Tao Yang",
                "Yuwang Wang",
                "Wenjun Zeng"
            ],
            "title": "Learning disentangled representation by exploiting pretrained generative models: A contrastive learning view",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Christof Seiler",
                "Simon Rubinstein-Salzedo",
                "Susan Holmes"
            ],
            "title": "Positive curvature and hamiltonian monte carlo",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2014
        },
        {
            "authors": [
                "Nicholas Sharp",
                "Souhaib Attaiki",
                "Keenan Crane",
                "Maks Ovsjanikov"
            ],
            "title": "Diffusionnet: Discretization agnostic learning on surfaces",
            "venue": "ACM Trans. Graph.,",
            "year": 2022
        },
        {
            "authors": [
                "Jascha Sohl-Dickstein",
                "Eric Weiss",
                "Niru Maheswaranathan",
                "Surya Ganguli"
            ],
            "title": "Deep unsupervised learning using nonequilibrium thermodynamics",
            "venue": "In International Conference on Machine Learning,",
            "year": 2015
        },
        {
            "authors": [
                "Jiaming Song",
                "Chenlin Meng",
                "Stefano Ermon"
            ],
            "title": "Denoising diffusion implicit models",
            "year": 2020
        },
        {
            "authors": [
                "Yang Song",
                "Conor Durkan",
                "Iain Murray",
                "Stefano Ermon"
            ],
            "title": "Maximum likelihood training of score-based diffusion models",
            "venue": "In Thirty-Fifth Conference on Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Yang Song",
                "Stefano Ermon"
            ],
            "title": "Generative modeling by estimating gradients of the data distribution",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Yang Song",
                "Sahaj Garg",
                "Jiaxin Shi",
                "Stefano Ermon"
            ],
            "title": "Sliced score matching: A scalable approach to density and score estimation",
            "venue": "In Uncertainty in Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Yang Song",
                "Jascha Sohl-Dickstein",
                "Diederik P Kingma",
                "Abhishek Kumar",
                "Stefano Ermon",
                "Ben Poole"
            ],
            "title": "Score-based generative modeling through stochastic differential equations",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "S S\u00e4rkk\u00e4",
                "A. Solin"
            ],
            "title": "Applied Stochastic Differential Equations. 2019",
            "year": 2019
        },
        {
            "authors": [
                "Arash Vahdat",
                "Karsten Kreis",
                "Jan Kautz"
            ],
            "title": "Score-based generative modeling in latent space",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Pascal Vincent"
            ],
            "title": "A connection between score matching and denoising autoencoders",
            "venue": "Neural Computation,",
            "year": 2011
        },
        {
            "authors": [
                "F.Y. Wang"
            ],
            "title": "Analysis for diffusion processes on riemannian manifolds",
            "year": 2014
        },
        {
            "authors": [
                "Daniel Watson",
                "William Chan",
                "Jonathan Ho",
                "Mohammad Norouzi"
            ],
            "title": "Learning fast samplers for diffusion models by differentiating through sample quality",
            "venue": "ArXiv, abs/2202.05830,",
            "year": 2022
        },
        {
            "authors": [
                "Joachim Weickert"
            ],
            "title": "Anisotropic diffusion in image processing, volume 1",
            "venue": "Teubner Stuttgart,",
            "year": 1998
        },
        {
            "authors": [
                "Tao Yang",
                "Xuanchi Ren",
                "Yuwang Wang",
                "Wenjun Zeng",
                "Nanning Zheng"
            ],
            "title": "Towards building a group-based unsupervised representation disentanglement framework",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Diffusion (score-based) models, which are originated from non-equilibrium statistical physics, have recently shown impressive successes on sample generations of a wide range of types, including images [10, 21, 38, 50] , 3D point clouds [13, 35] and audio generation [31, 34], among others. In addition to concrete applications of various diffusion generative models, it is also desirable to analyze them in an appropriate and flexible framework, by which novel improvements can be further developed.\nCurrently, one of the promising formal frameworks for unifying different types of diffusion models is the stochastic differential equations (SDE), as proposed in [50]. Under this formalism, the generative (denoising) process can be viewed as reversing the forward (noising) process from real data manifold to white noise. Furthermore, with the help of the Feynman-Kac formula and Girsanov transform [8], the score-matching training has been proved to be equivalent to certain log-likelihood training in the infinite-dimensional path space [24].\nAlthough the ELBO cost function for diffusion models derived in [24] explicitly contains both the forward and backward ingredients, the forward (noising) process is hand-crafted and set to be fixed throughout training, which is clearly a mismatch when comparing diffusion models with other loglikelihood based models (e.g., VAE [40]) in practice. Moreover, since the reverse generating process is uniquely determined by the forward process, the total flexibility of the model lies in parameterizing the forward process. This would to another cascading effect of freezing the forward process, especially when knowing that different noising schedules do affect the empirical performances (e.g., the different forward processes including VE, VP, sub-VP [50] and damped Langevin diffusion [12]\nPreprint. Under review.\nar X\niv :2\n20 6.\n10 36\n5v 1\n[ cs\n.L G\n] 1\n7 Ju\ndisplayed distinct generating performances in terms of perceptual quality). On the other hand, even reparameterizing and training the noising-schedule of the forward process would improve the diffusion model, as it was shown in [28].\nTo answer these concerns, it is crucial to incorporate flexible parameterized forward processes under the general SDE framework of [50]. Though the idea of training the forward process is intuitively reasonable, the implementation is far from straightforward. The first challenge is to find the appropriate search sub-space within the grand function space consisting of the whole stochastic processes. A hard constraint is that the stationary distribution of the candidate stochastic processes is centered Gaussian, which will be set as the prior distribution of the generative process to sample from. In fact, [28] has demonstrated the power of optimizing the one-dimensional time component (reparameterized by the signal to noise ratio) of the forward process. However, efficiently parameterizing the space components remains to be explored, especially taking into account the complex structure of the data distribution [37].\nThis paper concentrates on the theoretical and the practical aspects of solving the flexibility challenges of the diffusion model, emphasizing the spatial components of the forward process. First of all, inspired by concepts from Riemannian geometry and Hamilton Monte-Carlo methods, we define a unified and flexible class of diffusion processes (FP-Diffusion) that rigorously satisfies the fixed Gaussian stationary distribution condition with theoretical guarantee. To highlight the advantages of flexible diffusion models, we discuss the theoretical properties and effects of parameterized forward processes from the perspective of optimization. Furthermore, by introducing the flexible diffusion model, all sorts of regularizers [15, 39] on selecting more flat paths for continuous normalizing flows can be referenced and implemented.\nWe summarize our major contributions here:\n\u2022 We introduce the symplectic structure and the anisotropic Riemannian structure to the diffusion model, through which a framework for parameterizing the forward process with theoretical guarantees is formulated. Completeness and convergence properties as t\u2192\u221e are proved along the same route.\n\u2022 To theoretically motivate optimizing the forward process, we analyze the implications of parameterizing the forward (noising) process from the optimization function point of view and demonstrate how our method unifies previous diffusion models. Since our extension is compatible with merging regularization terms into the training loss, We provide experimental results in simulated scenarios and demonstrate how it behaves under regularization.\n\u2022 Except considering the continuous diffusion framework, we also develop a corresponding light version of our parameterization with an explicit formula of general terms for efficient Monte-Carlo training. It enables us to perform comparative studies on large-scale datasets, e.g., MNIST and CIFAR10."
        },
        {
            "heading": "2 Preliminaries and Related Works",
            "text": "Given a data distribution p(x), we associate it with a Gaussian diffusion process (forward) that increasingly adds noise to the data, then the high-level idea of diffusion generative models is to model\nthe real data distribution as a multi-step denoising (backward) process. In a discrete step setting, the forward process can be formulated as an N-steps Markov chain from real data x to each noised xt:\np(xt|xt\u22121) = N (\u03b1tx, \u03b2tI), t \u2208 {1, . . . , N}.\nFor DDPM model [21], \u03b1t is set to be \u03b1t := \u221a\n1\u2212 \u03b2t. If we take the continuous limit of \u03b2t (when\u221a 1\u2212 \u03b2t \u2248 1\u2212 12\u03b2t), we find that Xt satisfies a time-change of the following Ornstein\u2013Uhlenbeck stochastic differential equation (SDE):\ndXt = \u2212 1\n2 \u03b2(t)Xtdt+\n\u221a \u03b2(t)dWt, (1)\nthe so-called variance-preserving diffusion process (VP) in [50]. Therefore, DDPM can be taken as a discretization of the Ornstein\u2013Uhlenbeck process. Following this line, [50] proposed to characterize different types of diffusion models by formulating the underlying SDE of the model:\ndXt = f(Xt, t)dt+ g(t)dWt, 0 \u2264 t \u2264 T (2)\nwhere {Wt}\u221et=0 denotes the standard Brownian motion, whose dimension is set to be the same as the data. Usually we choose a different time schedule (time-change) from t. Let \u03b2(t) be a continuous function of time t such that \u03b2(t) > \u03b2(s) > 0 for 0 < s < t, then \u03b2(t) is called a time schedule (time-change) of t. It can be further shown that when t\u2192\u221e, the stationary distribution of Eq. 1 is the standard multivariate Gaussian: N (0, I) [23]. On the other hand, SMLD diffusion models [48] can be seen as a discretization of the variance-exploring (VE) process ((9) of [50]) {Xt}t=Tt=0 , which satisfies the following SDE:\ndXt = \u221a 2\u03c3(t)\u03c3\u2032(t)dWt. (3)\nNote that although a random process doesn\u2019t have a pointwise inverse, it\u2019s still valid to define a reverse SDE Yt of a forward SDE Xt such that the marginal distributions at each time and its corresponding \u2018reverse\u2019 time match: pt(Xt) \u2261 qT\u2212t(YT\u2212t). Then Yt is exactly the denoising (backward) stochastic process we are looking for. In other words, real data can be generated by sampling from the Gaussian distribution and tracking the denoising process from time T to 0. Surprisingly, the solution of the reverse-time SDE with respect to the forward Xt is derived analytically in [2, 50]:\ndYt = [f(Yt, t)\u2212 g2(Yt, t)\u2207 log pt(Yt)]dt+ g(t)dWt, (4)\nwhere Wt is a Brownian motion running backwards in time from T to 0. The unknown score function st(x) := \u2207 log pt(x) depends on the data distribution p0 and the forward process Xt. The continuous diffusion model approaches the score function by various types of (weighted) scorematching procedures, we will briefly review the loss function in section 3.2.\nNow we summarize more related works:\nDiffusion Probabilistic Model (DPM) as a generative model [18, 29, 42, 57] was first introduced in [45], as a probabilistic model inspired by non-equilibrium thermodynamics. The high-level idea is to treat the data distribution as the Gibbs (Boltzmann) equilibrium distribution [16], then the generating process corresponds to transitioning from non-equilibrium to equilibrium states [9]. DDPM [21] and [3, 27, 38, 46, 55] further improve DPMs by introducing the Gaussian Markov chain and various inference and sampling methods, through which the generative model is equivalent to a denoising diffusion model. [52] then introduces the diffusion to a latent space and the denoising steps are also further increased with improved empirical performance. Furthermore, as we will show in this article, there are infinite processes (thermodynamical systems) that can connect non-equilibrium states to equilibrium.\nScore Matching: Score-based energy model [25, 53] is based on minimizing the difference between the derivatives of the data and the model\u2019s log-density functions, which avoids fitting the normalization constant of the intractable distribution. [48, 49] introduce sliced score matching that enabled scalable generative training by leveraging different levels of Gaussian noise and several empirical tricks. [47, 50] further studied perturbing the data with a continuous-time stochastic process.Under this framework, [28] proposes to reparameterize the time variable of the forward process by the signal-tonoise ratio (SNR) variable and train the noising-schedule. Indeed, the sub-VP SDE of [50] can also be seen as modifying the time scale of the diffusion part of the original VP. From this point of view, our model can be seen as a novel spatial parameterization of the forward process, which takes into account the spatial inhomogeneity of the data."
        },
        {
            "heading": "3 Methods",
            "text": ""
        },
        {
            "heading": "3.1 A general framework for parameterizing diffusion models",
            "text": "From the preliminary section, we realize that the stationary distribution of the forward process corresponds to the initial distribution of the denoising (generative) process. Therefore, it must be a simple distribution we know how to sample from, mainly set to be standard Gaussian. In this article, we parameterize the spatial components of the forward process by considering the following SDE:\ndXt = f(Xt)dt+ \u221a\n2R(Xt)dWt, (5) under the hard constraint that the stationary distribution of Xt is standard Gaussian (the scaled Gaussian case can be easily generalized). Introducing the time change \u03b2(t), then by Ito\u2019s formula, X\u03b2(t) satisfies a variant of Eq. 5:\ndX\u03b2(t) = f(Xt)\u03b2 \u2032(t)dt+ \u221a 2\u03b2\u2032(t)R(Xt)dWt. (6)\nTo satisfy the theoretical constraint, it\u2019s obvious that the function class of f(x) and R(x) should be properly restricted. Inspired by ideas from the Riemannian Manifold Hamiltonian Monte-Carlo algorithm [5, 6, 17, 43] and anisotropic diffusion technique of image processing, graph deep learning [1, 41, 44, 56], we propose a flexible framework for parameterizing the forward process by introducing two geometric quantities: the Riemannian metric and the symplectic form in Rn.\nIntuitively, an anisotropic Riemannian metric implies that the space was curved and in-homogeneous, and the corresponding Brownian motion will inject non-uniform noise along with different directions. On the other hand, the symplectic form is crucial for defining the dynamics of a given Hamiltonian. Both of them set the stage for performing diffusion on the data manifold, from the data distribution to the standard multivariate normal distribution, whose density under the canonical volume form dx1 . . . dxn is\n1\u221a (2\u03c0)n exp(\u22121 2 \u2016x\u20162)dx1 . . . dxn. (7)\nMoreover, in Appendix A, we will prove that our parameterization is complete under certain constraints.\nNow we introduce the two concepts in detail. In a coordinate system, a Riemannian metric can be identified as a symmetric positive-definite matrix ( the Euclidean metric is exactly the identity matrix). Given a Riemannian metric R(x) := {Rij(x)}1\u2264i,j\u2264n, recall that for a smooth function H(x), the Riemannian Langevin process satisfies the following SDE:\ndXt = \u2212\u2207\u0303H(Xt)dt+ \u221a 2dBt, (8)\nwhere \u2207\u0303H(x) := R\u22121(x)\u2207H(x) is the gradient vector field of H , and Bt denotes the Riemannian Brownian motion [23]. In local coordinates, we have (see (13) of [17]):\ndBit = |R(Xt)|\u22121/2 n\u2211 j=1 \u2202 \u2202xj (R\u22121ij (Xt)|R(Xt)| 1/2)dt+ \u221a R\u22121(Xt)dW i t ,\nfor i \u2208 {1, 2, . . . , n}. One crucial property of the Riemannian Langevin process [54] is that the stationary distribution p(x) has the following form:\np(x) \u221d e\u2212H(x)dV (x), where dV (x) := \u221a |R(x)|dx1 . . . dxn is the Riemannian volume form. Transforming back to the canonical volume form and take H(x) = 14\u2016x\u2016 2 \u00b7 log(|R(x)|), we have proved the following lemma: Lemma 3.1. The stationary distribution of the SDE (Eq. 9) below is the standard Gaussian of Rn:\ndXt = 1 2 [\u2212 \u2211 j R\u22121ij (Xt) \u00b7 (Xt)j + \u2211 j \u2202 \u2202xj R\u22121ij (Xt)]dt+ \u221a R\u22121(Xt)dWt. (9)\nRemark 3.2. It\u2019s worth mentioning that the infinitesimal generator of the Riemannian motion is the Laplacian operator \u2206. When acting on a smooth function f ,\n\u2206f := 1\u221a |R(x)|\n\u2202i( \u221a |R(x)|(R\u22121)ij\u2202jf).\nIt has the same form as the anisotropic diffusion of (1.27) in [56]. The effectiveness of anisotropic noise is explored in section 4.1.\nOn the other hand, introducing a symplectic form \u03c9 allows us to do Hamiltonian dynamics in an even-dimensional space Rn, when n = 2d. Since a symplectic form is a non-degenerate closed 2-form, it automatically becomes zero in odd-dimensional spaces. In this article, we will restrict ourselves to a special type of symplectic form, which consists of constant anti-symmetric matrices {\u03c9ij}1\u2264i,j\u22642d. Then the corresponding Hamiltonian dynamics of H(x) is:\ndXt = \u03c9\u2207H(Xt)dt. (10)\nWe mainly focus on two remarkable properties of Hamiltonian dynamics: 1. It preserves the canonical volume form (the determinant of the Jacobi matrix equals one); 2. The Hamiltonian function H(x) takes a constant value along the integral curves (see the remark in Appendix A). Using the change of variables formula, we conclude that the probabilistic density of Xt preserves the equilibrium Gibbs distribution: p(x) \u221d e\u2212H(x)dx1 . . . dxn, when X0 is sampled from the Gibbs distribution.\nLet H(x) = 12x 2, the potential energy of the Harmonic oscillator. Then by merging the Riemannian part (Eq. 9) and the symplectic part (Eq. 10) we obtain the following theorem:\nTheorem 3.3. Suppose \u03c9 is a anti-symmetric matrix, and R\u22121(x) is a positive-definite symmetric matrix-valued function of x \u2208 Rn. Then the (unique) stationary distribution of (Eq. 11) below is the standard Gaussian (Eq. 7) of Rn:\ndXt = 1 2 [\u2212 \u2211 j R\u22121ij (Xt)\u00b7(Xt)j\u22122 \u2211 j \u03c9ij \u00b7(Xt)j+ \u2211 j \u2202 \u2202xj R\u22121ij (Xt)]dt+ \u221a R\u22121(Xt)dWt (11)\nWe name Eq. 11 as our FP-Diffusion model. Note that theorem 3.3 can also be verified and extended to scaled Gaussian distribution by direct computation, and we leave the proof in Appendix A.For a graphical presentation, Figure 1 plots the VP stochastic trajectories (the green curves) connected with our FP-Diffusion forward trajectories (the white curves) under random initialization. We also provide an informal argument on how the anisotropic FP-Diffusions mix with the low-dimensional data distribution in Appendix A.\nIn fact, to unify the critical damped Langevin diffusion model [12], FP-Diffusion is straightforward to generalize to the case when the inverse Riemannian matrix R\u22121(x) degenerates (contains zero eigenvalues). However, Xt may not converge to this Gaussian stationary distribution from a deterministic starting point. Intuitively, the diffusion part \u221a R\u22121(x)dWt is the source of randomness (noise). Suppose R\u22121(x) degenerates along the i-th direction (i.e., corresponding to zero eigenvalue), then no randomness is imposed on this direction, then the i-th component Xit will be frozen at X i 0. To remedy the issue, we impose additional restrictions, which lead us to the following corollary:\nCorollary 3.4. Under additional conditions: (1) the symplectic form \u03c9 \u2208 R2d\u00d72d has the block form:\n\u03c9 = ( 0 A \u2212A 0 ) , with a positive-definite matrix A \u2208 Rd\u00d7d; (2) the inverse (semi-) Riemannian\nmatrix R\u22121(x) has the block form: R\u22121(x) = (\n0 0 0 B\n) , with a constant positive-definite symmetric\nmatrix B \u2208 Rd\u00d7d, it induces that the forward diffusion Xs converges to the standard Gaussian distribution:\nps(Xs) s\u2192\u221e\u2212\u2212\u2212\u2192 N (0, I).\nWe will demonstrate how the corollary derives the damped diffusion model in Appendix A."
        },
        {
            "heading": "3.2 Parameterizing diffusion models from the optimization perspective",
            "text": "In this section, we illustrate the benefits of parameterized diffusion models from the view of the training objective. Recall that the ground-truth reverse-time (generative) SDE of the forward process Xt is denoted by Yt, and we parameterize Yt by Y \u03b8t :\ndY \u03b8t = [f(Yt, t)\u2212 g2(Yt, t)\u2207s\u03b8(Yt, t)]dt+ g(t)dWt, (12)\nwhere s\u03b8 is the score neural network parameterized by \u03b8. Then the (explicit) score-matching loss function for optimization is\nLESM := \u222b T 0 EXs [ 1 2 \u2016s\u03b8(Xs, s)\u2212\u2207 log ps(Xs)\u20162\u039b(s)]ds, (13)\nwhere \u039b(s) is a positive definite matrix as a weighting function for the loss. Since Yt and Xt shares the same marginal distributions, suppose our parameterized generative process Y \u03b8t matches Yt perfectly, namely s\u03b8(x, t) \u2261 \u2207 log pt(x) (14) for all t \u2208 [0, T ], then the marginal distribution of Y \u03b8t at t = 0 is exactly the data distribution. The major obstacle of optimizing Eq. 13 directly is that we don\u2019t have access to the ground truth score function\u2207 log ps(x, s). Fortunately, LESM can be transformed to a loss based on the accessible conditional score function \u2207 log pXs|X0(Xs) plus a constant [49, 50] (for a fixed forward process Xs). More precisely, given two time slices 0 < s < t < T ,\nEXt\u2016s\u03b8(Xt, t)\u2212\u2207 log pt(Xt)\u20162 \u2261 EXs,Xt\u2016s\u03b8(Xt, t)\u2212\u2207 log pt(Xt|Xs)\u20162 (15) + EXt\u2016\u2207 log pt(Xt)\u20162 \u2212 EXs,Xt\u2016\u2207 log pt(Xt|Xs)\u20162\ufe38 \ufe37\ufe37 \ufe38\ngap terms\n. (16)\nSince the gap terms between the absolute and conditional score function loss do not depend on the backward generative process, one theoretical advantage of FP-Diffusion is that the gap terms are also parameterized. This formula is adapted from [24, 49] by modifying the initial time, and full derivations are given in Appendix A for completeness.\nOn the other hand, compared with log-likelihood generative models like normalizing flows and VAE, the connection between score matching and the log-likelihood of data distribution log p0(x) is not straightforward due to the additional forward noising process Xt. Hence, we turn to the variational view established in [24], where the ELBO (evidence lower bound) of data\u2019s log-likelihood log p0(x) is related with the score matching scheme. More precisely,\nlog p0(x) \u2265 E\u221e(x), and the ELBO E\u221e(x) of the infinite-dimensional path space is defined by\nE\u221e(x) := EXT [log pT (XT )|X0 = x]\u2212 \u222b T\n0\nEXs [ 1\n2 \u2016s\u03b8\u20162g2 +\u2207 \u00b7 (g2s\u03b8 \u2212 f)|X0 = x]ds.\nThe above implies that learning a diffusion (score) model is equivalent to maximizing the ELBO in the variational path space defined by the generative process Y \u03b8t . Thus, treating f(x, t) and g(x, t) as learnable functions results in enlarging the variational path space from pre-fixed f and g to flexible variational function classes, and the domain of ELBO (parameterized by f and g) is extended correspondingly.\nBy Eq. 11, in FP-Diffusion model, we set\nf(x, t) := \u03b2\u2032(t) 2 [\u2212 \u2211 j R\u22121ij (x)xj \u2212 2 \u2211 j \u03c9ijxj + \u2211 j \u2202 \u2202xj R\u22121ij (x)], g(x, t) := \u221a \u03b2\u2032(t)R\u22121(x).\n(17) Since our variational function class of the forward process defined in Eq. 11 is theoretically guaranteed to approach Gaussian when T is large, the first term of E\u221e(x) is close to a small constant under Eq. 17. Therefore, we only need to investigate the second term (equivalent to the implicit score matching [26]), which depends on both the parameterized f , g and the score function. Finally, learning f and g opens the opportunity of adding additional regularization penalties to filter out irregular paths in the extended variational path space. Similar techniques had been applied in continuous normalizing flows [15]. Preliminary exploration on applying regularization to FP-Diffusion models is clarified in the experimental section."
        },
        {
            "heading": "3.3 A simplified formula of FP-Diffusion",
            "text": "Although we can always numerically simulate the SDE to sample at a given time t, the empirical success of the Monte-Carlo training of (14) in [21] (see also (7) of [50]) indicates the importance of\nderiving explicit solutions for the SDE. In this section, we derive the solution formula for a simplified version of Xt defined in Eq. 11 and implement it on the image generation task.\nTo obtain the closed-form of the transition probabilistic density function of the forward process Xt, we assume that R\u22121(x) of Eq. 11 is a constant symmetric positive-definite matrix independent of the spatial variable x. Then in the linear SDE region [51], we have the following characterization of the marginal distributions (see Appendix A for a full derivation): Theorem 3.5. Suppose the forward diffusion process Xt starting at X0 satisfies the following linear stochastic differential equation:\ndXt = 1\n2 \u03b2\u2032(t)[\u2212R\u22121Xt \u2212 2\u03c9Xt]dt+\n\u221a \u03b2\u2032(t)R\u22121dWt, (18)\nfor positive-definite symmetric R and anti-symmetric \u03c9. Then the marginal distribution of Xt at arbitrary time t > 0 follows the Gaussian distribution:\nXt \u223c N (e(\u2212 1 2R \u22121\u2212\u03c9)\u03b2(t)X0, I\u2212 e\u2212\u03b2(t)R \u22121 ).\nTo treat the drift and diffusion terms in Eq. 11 separately, we name Eq. 9 as the FP-Drift, Eq. 10 as the FP-Noise. In practice, we parameterize the anti-symmetric matrix \u03c9 in the FP-Drift model and the anti-symmetric matrix R\u22121 in the FP-Noise model. Both the anti-symmetric and symmetric matrices are realized through the exponential map. We leave the implementation details in Appendix A."
        },
        {
            "heading": "4 Experiment",
            "text": "We first use a synthetic 3D dataset to illustrate the significance of parameterizing the forward process according to the data distribution, then validate the effectiveness of our FP-Diffusion model on the image generation task."
        },
        {
            "heading": "4.1 Flexible SDEs learned from Synthetic 3D examples",
            "text": "By the low-dimensional manifold hypothesis [14], the real data distribution concentrates on a lowdimensional submanifold. However, during the generation phase, the dimension of the ambient space we sample from is much higher. In this case, FP-Diffusion plays a nontrivial role. To be more precise, note that only the diffusion part of Eq. 11 can blur the data submanifold to fill in the high-dimensional ambient space, which causes a distinction between the directions tangent to the data manifold and the normal directions for (anisotropic) diffusing. Since it is impossible to detect the data manifold of a complex dataset, we design a simplified scenario to demonstrate how the parameterized diffusion process helps generation.\nAssume the data lies in R3, and its probabilistic distribution intrinsically follows the 2-dimensional Gaussian concentrated at a hyper-plane. The most efficient way to generate the 2-dimensional Gaussian is to project the random points sampled from 3-dimensional Gaussian to this plane. To quantify this fact, we consider the optimal transport problem from the 3-dimensional Gaussian distribution to the 2-dimensional Gaussian. Define the cost function as c(x, y) := \u2016x \u2212 y\u20162, then the the Wasserstein distance between N (0, I) and N (\u00b5,\u03a3) [36] equals W2(N (0, I,N (\u00b5,\u03a3)) = \u2016\u00b5\u20162 + Tr(I+ \u03a3\u2212 2\u03a31/2). It implies that corresponding optimal transport map is exactly the vertical projection map, denoted by\u2207\u03c6.\nTable 1: NLLs on MNIST\nModel NLL \u2193 RealNVP [11] 1.06\nGlow [30] 1.05 FFJORD [19] 0.99 ResFlow [7] 0.97 DiffFlow [58] 0.93\nFP-Drift (Mix) 1.01\nFigure 3: MNIST samples Figure 4: CIFAR10 samples\nFor FP-Diffusion, the probabilistic flow of the generating process depends on the learnable forward process, which allows us to add regularization penalty terms to the score-matching loss to choose preferable paths. From (13) of [50], the vector field of the probability flow ODE is\nvpc(x, t) := f(x, t)\u2212 1 2 \u2207 \u00b7 [g2(x, t)]\u2212 1 2 g2(x, t)\u2207 log pt(x), x \u2208 R3. (19)\nNow we check whether the direction of the learned vpc is aligned with the ground-truth projection vector field. For the projection map\u2207\u03c6 from the 3-dimensional Gaussian to 2-dimensional Gaussian supported at the plane :z = 2, the corresponding vector field at a spatial point x = (x, y, z) \u2208 R3 is:\nvproj(x, t) := (0, 0,\u22121) if z > 2, and vproj(x, t) := (0, 0, 1) if z < 2. (20) We perform the experiment under three circumstances: 1. a fixed VP forward process (Eq. 1) diffusion model; 2. our parameterized forward process with no regularization; 3. our parameterized forward process with regularization terms. The regularization penalties are imposed on the vector field (Eq. 19), which are adapted from section 4 of [15]: Lreg(f, g) = \u03bb1 \u222b \u2016vpc(s)\u20162ds + \u03bb2E \u223cN(0,1)\u2016 T vpc(s)\u20162ds. This term is only designed for regularizing the parameterized f and g of the forward process.\nThe 2D visualization results of our comparative experiments are summarized in Fig. 2. Notice that the \u2018ground-truth\u2019 vector field (Eq. 20) is strictly vertical, so we plot the x\u2212 z projection of the trained three vector fields at a given time for the three scenarios. From Fig. 2, although our flexible diffusion method (b) is visibly more vertical than the forward-fixed VP (ddpm) model (a), the flexible model with regularization (c) is more close to vertical lines. We also sample the integration trajectories of the trained vector fields for comparison in Appendix B (see Figure 5)."
        },
        {
            "heading": "4.2 Image Generation",
            "text": "In this section, we demonstrate the generative capacity of the score models driven by our FP-Diffusion on two common image datasets: MNIST [33] and CIFAR10 [32].\nTraining strategy. The flexible FP-Diffusion framework is designed to simultaneously learn a suitable forward diffusion process dependent on the data distribution as well as the corresponding reverse-time process. However, for some complex scenarios like image generation, it is challenging to balance the optimization of the FP-Diffusion model and the score model. To compromise these two parts, we propose a two-stage training strategy. Particularly, in the first stage, we jointly optimize the parameters of both the FP-Diffusion model and the score neural network; in the second stage, we fix all parameters in the FP-Diffusion and only tune the score neural network as the prevailing score-based MCMC approaches [21, 48, 50].\nImplementation Details. For the forward diffusion process, we choose a linearly increasing time scheduler \u03b2(t) (same as the VP-SDE setting in [50]), where t \u2208 [0, T ] is a continuous time variable. To estimate the gradient vector field in the reverse-time process, we train a time-dependent score network s\u03b8(x(t), t) as described in Eq. 15. We adopt the same U-net style architecture used in [21] and [50] as our score network. We train both the FP-Drift model and the FP-Noise model in two training paradigms: 1) Joint Training: the parameterized FP-Diffusion model and the score network are jointly optimized for 1.2M iterations; 2) Mix Training: following the proposed two-stage training strategy, we separately train the model for 600k iterations in both stages. The batch size is set to 96 on all datasets. We apply the Euler-Maruyama method in our reverse-time SDEs for sampling images, where the discretization steps are set to 1000 as in [50]. All the experiments are conducted on 4 Nvidia Tesla V100 16G GPUs. We provide further implementation details in Appendix B.\nResults. We show the sampled images generated by our FP-Noise (Mix) model in Fig. 3 and Fig. 4. According to Eq. 19, we calculate the negative log-likelihood (NLL) in bits per dimension for our models by the instantaneous change of variables formula [19]. Then we list the NLL metrics of our models in Tab. 1 and Tab. 2. On MNIST, our FP-Drift model achieves comparable performance in terms of NLL, compared to five flow-based models (including DiffFlow [58]). On CIFAR10, both the FP-Drift (Mix) and the FP-Noise (Mix) models achieve a competitive performance compared to the state-of-the-art (SOTA) diffusion models. These results illustrate the strong capacity of FP-Diffusion in density estimation tasks.\nTo quantitatively evaluate the quality of sampled images, we also report the Fenchel Inception Distance (FID) [20] on CIFAR10. As shown in Tab. 2, the two variants of our FP-Diffusion model, FP-Drift (Mix) and FP-Noise (Mix), outperform DDPM [21] and Improved-DDPM [38] in FID and have a comparable performance with DDPM++ cont. (deep, VP) and NCSN++ cont. (deep, VE) [50]. We notice that only LSGM and CLD-SGM have obviously better FID values than other models (including us). However, LSGM [52] adopts a more complicated framework and a large model with \u2248 475M parameters to achieve its high performance. With a comparable parameter size (\u2248 100M ), our models could achieve a significantly better FID score than LSGM (\u201cLSGM-100M\u201d). CLD-SGM builds its diffusion model upon a larger phase space with a special training objective (given the data point x \u2208 Rn, its phase space corresponding point (x, v) belongs to R2n), which leads to a more expressive optimization space but brings extra computational cost as well. We leave testing our FP-Diffusion model on phase space (defined in Corollary 3.4) in the future. It should also be noted that we use a smaller batch size (96) compared to other baseline diffusion models (128) to train our models due to limited computational resources, which may influence our performance to some extent. We also report the performance of our two model variants in two training paradigms in Tab. 2. The model variants with the joint training paradigm consistently achieve a better performance, demonstrating the necessity of the two-stage training strategy. A possible reason of this phenomenon is that it is difficult for score models to match the reverse process of a dynamical forward process, so we need to re-train the score model with extra training steps after learning a suitable forward process."
        },
        {
            "heading": "5 Conclusion",
            "text": "We propose FP-Diffusion model, a novel method that parameterizes the spatial components of the diffusion (score) model with theoretical guarantees. This approach combines insights from Riemannian geometry and Hamiltonian Monte-Carlo methods to obtain a flexible forward diffusion framework that plays a nontrivial role from the variational perspective. Empirical results on speciallydesigned datasets and standard benchmarks confirm the effectiveness of our method. However, efficiently optimizing FP-Diffusion remains a critical challenge, which opens the door for promising future research."
        },
        {
            "heading": "A Theory",
            "text": "A.1 Discussion of Section 3.1\nA.1.1 Remark on the theoretical properties of Hamiltonian dynamics\nSuppose Xt follows the Hamiltonian dynamics (10), then\ndH(Xt) = \u2207H(Xt)\u03c9\u2207H(Xt)dt \u2261 0,\nby the anti-symmetry of \u03c9. Therefore, the Hamiltonian dynamics without random perturbations is a deterministic motion that can explore within a constant Hamiltonian (energy) surface. It means that, only by adding a diffusion term, the Hamiltonian dynamical system is able to traverse different energy levels.\nA.1.2 Verification of Theorem 3.3\nWe will verify the theorem under the more general case, when H(x) = m2 x 2. The corresponding stationary distribution is the scaled Gaussian N (0,mI), where m > 0 is the scale constant. In this case, Eq. 11 is modified to:\ndXt = m 2 [\u2212 \u2211 j R\u22121ij (Xt) \u00b7 (Xt)j\u22122 \u2211 j \u03c9ij \u00b7 (Xt)j + \u2211 j \u2202 \u2202xj R\u22121ij (Xt)]dt\n+ \u221a R\u22121(Xt)dWt. (21)\nNote that only the drift term is scaled by m.\nProof. Since the covariance matrix of the diffusion part is positive-definite, the forward process Eq. 21 satisfies the Feller property and the existence and uniqueness of the stationary distribution are guaranteed (see [54]). By the Fokker-Plank-Kolmogorov equation, the stationary distribution ps(x) of Eq. 21 should satisfy\n0 = \u2212 \u2211 i \u2202 \u2202xi [fi(x, t)ps(x)] + 1 2 \u22022 \u2202xi\u2202xj [(ggT )ijps(x)], (22)\nwhere we set f(x, t) := m2 [\u2212 \u2211 j R \u22121 ij (x) \u00b7 xj \u2212 2 \u2211 j \u03c9ij \u00b7 xj + \u2211 j \u2202 \u2202xj\nR\u22121ij (x)] and g(x, t) :=\u221a R\u22121(x). To check whether e\u2212 m 2 x 2\nsatisfies condition (22), notice that by the anti-symmetry of \u03c9ij , we automatically have\n\u2211 i \u2211 j \u2202 \u2202xi (\u03c9ijxje \u2212m2 x 2 ) = \u2212 \u2211 i \u2211 j \u03c9ijxixje \u2212m2 x 2 = 0.\nOn the other hand,\u2211 i \u2211 j \u22022 \u2202xi\u2202xj [R\u22121ij (x)e \u2212m2 x 2 ] = \u2212mTr(R\u22121ij (x))e \u2212m2 x 2 +m2 \u2211 i \u2211 j R\u22121ij (x)xixje \u2212m2 x 2\n\u2212 \u2211 i \u2211 j \u2202 \u2202xj (R\u22121ij (x))( \u2202 \u2202xi e\u2212 m 2 x 2 )\n+ \u2211 i \u2211 j \u22022 \u2202xi\u2202xj (R\u22121ij (x))e \u2212m2 x 2\n\u2212m \u2211 i \u2211 j \u2202 \u2202xj (R\u22121ij (x))xie \u2212m2 x 2\n=\u2212mTr(R\u22121ij (x))e \u2212m2 x 2 +m2 \u2211 i \u2211 j R\u22121ij (x)xixje \u2212m2 x 2\n+ \u2211 i \u2202 \u2202xi [ \u2211 j \u2202 \u2202xj R\u22121ij (x)e \u2212m2 x 2 ]\n\u2212m \u2211 i \u2211 j \u2202 \u2202xj R\u22121ij (x)xie \u2212m2 x 2 .\nTherefore, the last thing to check is that\u2211 i \u2202 \u2202xi [ \u2211 j R\u22121ij (x)xje \u2212m2 x 2 ] = Tr(R\u22121ij (x))e \u2212m2 x 2 \u2212\n\u2211 i \u2211 j [mR\u22121ij (x)xixj + \u2202 \u2202xj R\u22121ij (x)xi]e \u2212m2 x 2 ,\nwhich is obviously true, since the diffusion matrix R\u22121ij is symmetric. Combining the above, we have proved that Eq. 22 holds if ps(x) \u221d e\u2212 m 2 x 2 .\nA.1.3 Completeness of FP-Diffusion parameterization\nFrom the last section\u2019s derivation, we can deduce the following corollary: Corollary A.1. Consider the following SDE:\ndXt = A(Xt)dt\u2212 1\n2 R\u22121(Xt) \u00b7Xtdt+ (\u2207 \u00b7R\u22121(Xt)) \u00b7Xtdt+\n\u221a R\u22121(Xt)dWt,\nand let the spatial function A(x) be a linear function. Suppose we know its stationary distribution is standard Gaussian, then A(x) = \u2212 \u2211 j \u03c9ij \u00b7 xj ,\nfor some anti-symmetric matrix \u03c9.\nProof. In fact, every linear operator A can be decomposed into a symmetric part plus an antisymmetric part:\nA = A+AT\n2\ufe38 \ufe37\ufe37 \ufe38 symmetric\n+ A\u2212AT\n2\ufe38 \ufe37\ufe37 \ufe38 anti-symmetric .\nLet \u03c9 = A\u2212A T\n2 . Then we only need to prove that A+A T equals zero, if Xt converges to Gaussian.\nFrom the proof of theorem 3.3, we extract the fact that if ps(x) \u221d e\u2212 1 2x 2\n,\u2211 i \u2202 \u2202xi [(A+AT )ij \u00b7 xje\u2212 1 2x 2 ] = 0,\nthen \u2211 i,j [(A+AT )ij \u00b7 \u2202 \u2202xi \u2202 \u2202xj (e\u2212 1 2x 2 )] = 0,\nfor all x = (x1, . . . , xn). Note that\n\u2202\n\u2202xi\n\u2202\n\u2202xj (e\u2212 1 2x 2 ) = (xixj \u2212 \u03b4ij)e\u2212 1 2x 2 .\nSince A+AT is symmetric (doesn\u2019t hold for arbitrary linear operator), it implies that A+AT \u2261 0.\nA.1.4 Anisotropic diffusion on low dimensional data manifold\nIn this section, we give an informal discussion on how an anisotropic diffusion starting at a lowdimensional data manifold mixes with its own stationary distribution (supported in the high dimension ambient space).\nAssume the marginal distribution of the diffusion process Xt concentrates on a low dimensional manifold M \u21aa\u2192 Rn at a given time. Moreover, suppose Xt already achieves the Gaussian stationary distribution on M (defined with respect to the Laplacian operator of M ). Now we want to informally investigate the most efficient way for Xt to diffuse out of the low dimensional sub-manifold to the ambient space. By localizing in the Riemannian normal coordinates and by arranging the coordinates indexes, we can further assume that M is isometric to the hyperplane of Rn defined by\nM = {x \u2208 Rn|x = (x1, . . . , xp, 0, . . . , 0)}.\nThen the coordinate components of each point x \u2208 Rn can be decomposed into the tangential directions and the normal directions with respect to M :\nx \u2208 (x1, . . . , xp\ufe38 \ufe37\ufe37 \ufe38 tangent to M , xp+1, . . . , xn\ufe38 \ufe37\ufe37 \ufe38 normal to M ).\nUnder the above conditions, we are ready to compare the convergence rate (to the high-dimensional stationary Gaussian distribution of Rn) of different forward diffusions defined in (9). For a fair comparison, we set the norm of the noise matrix to be one: \u2016R\u22121\u20162 \u2261 \u221a n. Otherwise, the convergence can always be accelerated by increasing the noising scale (\u2016R\u22121\u20162 \u2192\u221e). Under our normal coordinates, the forward diffusion can be decomposed into two parts :X(t) = Xtan(t) +Xnor(t). For simplicity, suppose R\u22121 is a diagonal matrix, then the tangential part and the normal part of X(t) is completely decoupled. In other word,\nXitan(t) = 1 2 [\u2212R\u22121ii \u00b7 (Xt) i]dt+\n\u221a R\u22121ii dW i t , 1 \u2264 i \u2264 p\nis a diffusion process on M . Therefore, (Xtan(t), X(t)) is indeed a Markov coupling. Suppose Xtan(t) at t = 0 already converges to its stationary distribution (low dimensional Gaussian), then by Ito\u2019s formula,\nd(Xtan(t)\u2212X(t))2 = dX2nor(t)\n= 2Xnor(t)( 1\n2 [\u2212R\u22121nor \u00b7Xnor(t)]dt+\n\u221a R\u22121nordWt) + Tr(R\u22121nor)dt.\nTaking the expectation of both sides, it implies that\ndEX2nor(t) dt = \u2212ER\u22121nor \u00b7X2nor(t) + Tr(R\u22121nor).\nLet rmin denote the minimal eigenvalue of the normal part of R\u22121, then\ndEX2nor(t) dt \u2264 \u2212rminEX2nor(t) + Tr(R\u22121nor).\nApplying Gr\u00f6nwall\u2019s inequality and note that Xnor(0) = 0, we have\nEX2nor(t) \u2264 e\u2212rmin\u00b7t \u00b7 Tr(R\u22121nor)t.\nThe above gives an upper bound on the convergence speed of the coupling (Xtan(t), X(t)) with respect to the W2 distance (see [54]). Since the stationary distribution of X(t) is exactly the high dimensional Gaussian distribution (the diffusion model\u2019s prior distribution), we hope the convergence rate to be as fast as possible (given a fixed noising scale). For the VP-Diffusion,\nR\u22121nor \u2261 diag{1, . . . , 1}. However, in FP-Diffusion model, the diagonal elements of R\u22121nor are allowed to be inhomogeneous and greater than one (under the condition that Tr(R\u22121nor) < n). This will lead to a smaller rmin, which will speed up the convergence rate by our analysis.\nA.1.5 Verification of Corollary 3.4\nThe intuition of Corollary 3.4 can be stated as follows: To guarantee the geometric ergodicity property of FP-Diffusion on the phase space, we need enough noise such that the diffusion process can transverse the whole space. Suppose R\u22121(x) degenerates along the i-th direction (corresponding to a zero eigenvalue), then no randomness (noise) is imposed on this direction.\nTo remedy the issue, we require the symplectic form \u03c9 to be non-zero along the i-th direction, which makes it possible to mix the noise originated along other directions (where R\u22121(x) is strictly positive-definite) with the i-th direction. Now we give the formal proof:\nProof. We only prove for the simplified case when A and B are both diagonal matrices with two sets of positive eigenvalues {ai}di=1, {bi}di=1. The general situation can be handled by trivial linear transformation. By proposition 8.1 of [4], the proof boils down to prove that the H\u00f6rmander\u2019s condition [22] holds for the forward process Xt. When R\u22121(x) is a constant matrix, the infinitesimal generator L of (11) is:\nL = \u2211 i \u2211 j 1 2 [\u2212R\u22121ij \u2212 2\u03c9ij ]xj \u2202 \u2202xj + 1 2 \u2211 ij R\u22121ij \u22022 \u2202xi\u2202xj .\nFor notation simplicity, denote x := (u, v) \u2208 R2d, where u, v \u2208 Rd. To put the second-order differential operator L in H\u00f6rmander\u2019s form, set\nYj(u, v) = \u2212 1\n2\n\u221a bj \u2202\n\u2202vj , 1 \u2264 j \u2264 n,\nand Y0(u.v) = \u2211 i (\u2212aivi \u2202 \u2202ui + aiui \u2202 \u2202vi ). Then it suffices to show that the vector fields {[Y0, Yj ], Yj}1\u2264j\u2264d span the whole R2d. By direct calculation,\n[Y0, Yj ] = 1 2 aj \u221a bj \u2202 \u2202uj ,\nfor \u2200j. Therefore, we conclude that the H\u00f6rmander\u2019s condition holds for Xt. Then the ergodic proposition 8.1 of [4] implies that the forward diffusion Xs converges to the standard Gaussian distribution.\nRemark A.2. A recent study [12] proposed to improve the diffusion model by enlarging the spatial space (where the generated samples lie in) to the \"phase\" space: x\u2192 (x, v). Then the corresponding joint forward diffusion (xt, vt) satisfies the Critically-Damped Langevin diffusion:(\ndxt dvt\n) = ( M\u22121vt \u2212xt ) dt+ ( 0d \u2212\u0393M\u22121vt ) dt+ ( 0\u221a 2\u0393 ) dWt. (23)\nIf the coupling mass M = 1, the drift part of Eq. 23 can be decomposed to a symmetric part R\u22121 and an non-trivial anti-symmetric part \u03c9 of (18) by setting:\nR\u22121 := ( 0, 0 0, 2\u0393I ) , \u03c9 := ( 0, \u2212I I, 0 ) .\nIt\u2019s straightforward to check that they rigorously fit the conditions of Corollary 3.4. Therefore, we conclude from Corollary 3.4 that the Damped Langevin diffusion converges to the standard Gaussian distribution of the enlarged phase space (x, v) \u2208 R2d, which coincides with the results of Appendix B.2 in [12].\nA.2 Discussion of Section 3.2\nIn this section, following the arguments from [24], we demonstrate how to estimate the score gradient vector field \u2207 log p(x) by the analytically tractable conditional score gradient vector field (conditioned on a previous time).\nTo prove (15), by adapting Eq. 31 of [24], it\u2019s enough to show that\nEXt [sT\u03b8 (Xt, t) \u00b7 \u2207 log pt(Xt)] = EXs,Xt [sT\u03b8 (Xt, t) \u00b7 \u2207 log pt(Xt|Xs)].\nTransforming the expectation to probabilistic integration, we have EXt [sT\u03b8 (Xt, t) \u00b7 \u2207 log pt(Xt)] = \u222b pt(x)s T \u03b8 (x, t) \u00b7 \u2207 log pt(x)dx (24)\n= \u222b sT\u03b8 (x, t) \u222b \u2207pt(x|xs)ps(xs)dxdxs (25)\n= \u222b \u222b ps(xs)pt(x|xs)\u2207pt(x|xs)dxdxs (26)\n= EXs,Xt [sT\u03b8 (Xt, t) \u00b7 \u2207 log pt(Xt|Xs)], (27)\nfor 0 \u2264 s < t. By quadratic expanding EXt\u2016s\u03b8(Xt, t) \u2212 \u2207 log pt(Xt)\u20162 and plugging in (24), equality (15) follows directly.\nTo implement our discretized FP-diffusion forward diffusion, we usually choose s = t \u2212 1, the immediate time step before t. Then from t \u2212 1 to t, the conditional score gradient vector field of pt(xt|xt\u22121) is the Gaussian score function, which is analytically tractable.\nA.3 Discussion of Section 3.3\nIn this section, we prove Theorem 3.5 by applying Ito\u2019s formula and martingale representation theorem.\nRecall that the time-change of Eq. 11 satisfies\ndXt = \u03b2 \u2032(t)(\u22121\n2 R\u22121 \u2212 \u03c9)Xtdt+\n\u221a \u03b2\u2032(t)R\u22121dWt, (28)\nwhere X0 is a fixed point. Let Yt := e( 1 2R \u22121+\u03c9)\u03b2(t)Xt, then by Ito\u2019s formula,\nYt = \u222b t 0 e( 1 2R \u22121+\u03c9)\u03b2(s) \u221a \u03b2\u2032(s)R\u22121dWs. (29)\nFrom the martingale representation theorem, Yt is a Gaussian random variable for each t. Therefore, to fully determine the distribution of Xt, we only need to calculate the expectation and variance formulas of Xt. By the definition of stochastic integration, we have\nE[X(t)] = e(\u2212 1 2R \u22121\u2212\u03c9)\u03b2(t)X0.\nUtilizing the Ito\u2019s isometry to (29), we get\nV ar[Yt] = \u222b t 0 e\u03b2(s)(R \u22121+2\u03c9)\u03b2\u2032(s)R\u22121ds.\nSuppose \u03c9 = 0, then\nV ar[Xt] = I\u2212 e\u2212\u03b2(t)R \u22121 ,\nwhere I denotes the identity matrix of Rd. SupposeR\u22121 = I, since the Lie bracket [I+2\u03c9, I\u22122\u03c9] = 0, we further obtain\nV ar[Xt] = I\u2212 e\u2212\u03b2(t)I.\nIn conclusion, we have proved Theorem 3.5.\nA.4 How to parameterize symmetric and anti-symmetric matrix\nTo implement FP-Drift and FP-Noise models practically, we need to find an efficient way to parameterize positive-definite symmetric and anti-symmetric matrix.\nGiven a full-rank anti-symmetric matrix B, there always exist an orthogonal matrix P such that\nB = Pdiag {[\n0 \u03bb1 \u2212\u03bb1 0\n] , \u00b7 \u00b7 \u00b7 [ 0 \u03bbn \u2212\u03bbn 0 ]} PT ,\nwhere {\u03bb1, . . . , \u03bbn} are nonzero numbers. Then, the inverse of I +B (appeared in subsection A.4) is:\n(B + I)\u22121 = Pdiag\n{[ 1\n1+\u03bb21 \u2212\u03bb1 1+\u03bb21\n\u03bb1 1+\u03bb21 1 1+\u03bb21\n] , \u00b7 \u00b7 \u00b7 [ 1 1+\u03bb2n \u2212\u03bbn 1+\u03bb2n\n\u03bbn 1+\u03bb2n 1 1+\u03bb2n\n]} PT .\nFor positive-definite symmetric matrices, there always exist an orthogonal matrix P such that\nR = Pdiag{\u03bb1, . . . , \u03bbn}PT ,\nwhere {\u03bb1, . . . , \u03bbn} are positive numbers. To apply the above method, we only need to parameterize orthogonal matrices in an efficient and expressive way. By treating orthogonal matrices as elements in SO(n) orthogonal group, we utilize the exponential map to parameterize orthogonal matrices P :\nP = expH.\nNote that H is an element that belongs to the lie algebra so(n), which can be generated by upper triangular matrices."
        },
        {
            "heading": "B Experiments",
            "text": "B.1 Learned FP SDEs from synthetic 3D examples\nFigure 5 plots four 3D integration trajectories of the probabilistic flows (with respect to the fixed VP and learned FP-Diffusion models) starting at random initial positions. It\u2019s obvious that the trajectories of our flexible model are more straight than the fixed VP model, which demonstrates the power of selecting more regular generating paths of our FP-Diffusion model.\nB.2 Image Generation\nImplementation Details. Following [21] and [50], we rescale the range of the images into [\u22121, 1] before inputting them into the model. In the FP-Diffusion model, \u03b2(t) is an linearly increasing function with respect to the time t, i.e., \u03b2(t) = \u03b2\u0304min + t(\u03b2\u0304max \u2212 \u03b2\u0304min) for t \u2208 [0, 1]. It\u2019s worth mentioning that DDPM adopts a discretization form of this time scheduler, where \u03b2i = \u03b2\u0304min N + i\u22121 N(N\u22121) (\u03b2\u0304max \u2212 \u03b2\u0304min). These two forms are actually equivalent when N \u2192 \u221e. For all experiments, we set \u03b2\u0304max as 20 and \u03b2\u0304min as 0.1, which are also used in [21] and [50]. As\ndiscussed in A.4, we only need to parameterize the upper triangular matrices H and the diagonal elements \u039b = diag{\u03bb1, \u00b7 \u00b7 \u00b7 , \u03bbn} in the FP-Drift and FP-Noise models. Particularly, both H and \u039b are initialized with a multivariate normal distribution, and we adopt a exponential operation on \u039b to keep it a positive vector. As described in Section 4.2, we leverage a U-net style neural network to fit the score function of the reverse-time diffusion process. We keep the model architecture and the parameters of the score networks consistent with previous SOTA diffusion models (e.g., [50]) for a fair comparison. All models are trained with the Adam optimizer with a learning rate 2\u00d7 10\u22124 and a batchsize 96.\nIn the MNIST experiment, we first train the whole model for 50k iterations and train the score model for another 250k iterations with our Mix training strategy. We report the NLL of the model based on the last checkpoint. In the CIFAR10 experiment, the training iterations of both stage 1 and stage 2 are 600k. We also report the FIDs and NLL of the model based on the last checkpoint.\nResults. We provide more random samples from our best FP-Drift model\u2019s checkpoint in Fig. 6. We also provide the learned forward process of FP-Noise model in Fig. 7."
        }
    ],
    "title": "A Flexible Diffusion Model",
    "year": 2022
}