{
    "abstractText": "Generative Adversarial Networks (GANs) have been widely used for generating synthetic data for cases where there is a limited size real-world data set or when data holders are unwilling to share their data samples. Recent works showed that GANs, due to overfitting and memorization, might leak information regarding their training data samples. This makes GANs vulnerable to Membership Inference Attacks (MIAs). Several defense strategies have been proposed in the literature to mitigate this privacy issue. Unfortunately, defense strategies based on differential privacy are proven to reduce extensively the quality of the synthetic data points. On the other hand, more recent frameworks such as PrivGAN and PAR-GAN are not suitable for small-size training data sets. In the present work, the overfitting in GANs is studied in terms of the discriminator, and a more general measure of overfitting based on the Bhattacharyya coefficient is defined. Then, inspired by Fano\u2019s inequality, our first defense mechanism against MIAs is proposed. This framework, which requires only a simple modification in the loss function of GANs, is referred to as the maximum entropy GAN or MEGAN and significantly improves the robustness of GANs to MIAs. As a second defense strategy, a more heuristic model based on minimizing the information leaked from the generated samples about the training data points is presented. This approach is referred to as mutual information minimization GAN (MIMGAN) and uses a variational representation of the mutual information to minimize the information that a synthetic sample might leak about the whole training data set. Applying the proposed frameworks to some commonly used data sets against state-of-the-art MIAs reveals that the proposed methods can reduce the accuracy of the adversaries to the level of random guessing accuracy with a small reduction in the quality of the synthetic data samples.",
    "authors": [
        {
            "affiliations": [],
            "name": "Mohammadhadi Shateri"
        },
        {
            "affiliations": [],
            "name": "Francisco Messina"
        },
        {
            "affiliations": [],
            "name": "Fabrice Labeau"
        }
    ],
    "id": "SP:af8b98aa373873429c6a9159a5b76704d9417564",
    "references": [
        {
            "authors": [
                "V. Primault",
                "A. Boutet",
                "S.B. Mokhtar"
            ],
            "title": "and L",
            "venue": "Brunie, \u201cThe long road to computational location privacy: A survey,\u201d IEEE Communications Surveys & Tutorials, vol. 21, no. 3, pp. 2772\u20132793",
            "year": 2018
        },
        {
            "authors": [
                "M. Shateri",
                "F. Messina",
                "P. Piantanida"
            ],
            "title": "and F",
            "venue": "Labeau, \u201cReal-time privacypreserving data release for smart meters,\u201d IEEE Transactions on Smart Grid, vol. 11, no. 6, pp. 5174\u20135183",
            "year": 2020
        },
        {
            "authors": [
                "J. Liu",
                "J. Ma",
                "Y. Xiang",
                "W. Zhou"
            ],
            "title": "and X",
            "venue": "Huang, \u201cAuthenticated medical documents releasing with privacy protection and release control,\u201d IEEE Transactions on Dependable and Secure Computing, vol. 18, no. 1, pp. 448\u2013459",
            "year": 2021
        },
        {
            "authors": [
                "S. Niu",
                "B. Li",
                "X. Wang"
            ],
            "title": "and H",
            "venue": "Lin, \u201cDefect image sample generation with gan for improving defect recognition,\u201d IEEE Transactions on Automation Science and Engineering, vol. 17, no. 3, pp. 1611\u20131622",
            "year": 2020
        },
        {
            "authors": [
                "J. Yoon",
                "L.N. Drumright"
            ],
            "title": "and M",
            "venue": "van der Schaar, \u201cAnonymization through data synthesis using generative adversarial networks (ads-gan),\u201d IEEE Journal of Biomedical and Health Informatics, vol. 24, no. 8, pp. 2378\u20132388",
            "year": 2020
        },
        {
            "authors": [
                "G. Del Grosso",
                "G. Pichler"
            ],
            "title": "and P",
            "venue": "Piantanida, \u201cPrivacy-preserving synthetic smart meters data,\u201d in 2021 IEEE Power Energy Society Innovative Smart Grid Technologies Conference (ISGT), pp. 1\u20135",
            "year": 2021
        },
        {
            "authors": [
                "I.J. Goodfellow",
                "J. Pouget-Abadie",
                "M. Mirza",
                "B. Xu",
                "D. Warde-Farley",
                "S. Ozair",
                "A.C. Courville"
            ],
            "title": "and Y",
            "venue": "Bengio, \u201cGenerative adversarial nets,\u201d in NIPS",
            "year": 2014
        },
        {
            "authors": [
                "A. Brock",
                "J. Donahue"
            ],
            "title": "and K",
            "venue": "Simonyan, \u201cLarge scale GAN training for high fidelity natural image synthesis,\u201d in 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019, OpenReview.net",
            "year": 2019
        },
        {
            "authors": [
                "S. Zhao",
                "Z. Liu",
                "J. Lin",
                "J.-Y. Zhu"
            ],
            "title": "and S",
            "venue": "Han, \u201cDifferentiable augmentation for data-efficient gan training,\u201d Advances in Neural Information Processing Systems, vol. 33",
            "year": 2020
        },
        {
            "authors": [
                "J. Hayes",
                "L. Melis",
                "G. Danezis"
            ],
            "title": "and E",
            "venue": "De Cristofaro, \u201cLogan: Membership inference attacks against generative models,\u201d in Proceedings on Privacy Enhancing Technologies (PoPETs), vol. 2019, pp. 133\u2013152, De Gruyter",
            "year": 2019
        },
        {
            "authors": [
                "B. Hilprecht",
                "M. H\u00e4rterich"
            ],
            "title": "and D",
            "venue": "Bernau, \u201cMonte carlo and reconstruction membership inference attacks against generative models.,\u201d Proc. Priv. Enhancing Technol., vol. 2019, no. 4, pp. 232\u2013249",
            "year": 2019
        },
        {
            "authors": [
                "K.S. Liu",
                "C. Xiao",
                "B. Li"
            ],
            "title": "and J",
            "venue": "Gao, \u201cPerforming co-membership attacks against deep generative models,\u201d in 2019 IEEE International Conference on Data Mining (ICDM), pp. 459\u2013467, IEEE",
            "year": 2019
        },
        {
            "authors": [
                "D. Chen",
                "N. Yu",
                "Y. Zhang"
            ],
            "title": "and M",
            "venue": "Fritz, \u201cGan-leaks: A taxonomy of membership inference attacks against gans,\u201d ArXiv, vol. abs/1909.03935",
            "year": 2019
        },
        {
            "authors": [
                "R. Shokri",
                "M. Stronati",
                "C. Song"
            ],
            "title": "and V",
            "venue": "Shmatikov, \u201cMembership inference attacks against machine learning models,\u201d in 2017 IEEE Symposium on Security and Privacy (SP), pp. 3\u201318, IEEE",
            "year": 2017
        },
        {
            "authors": [
                "S. Yeom",
                "I. Giacomelli",
                "M. Fredrikson"
            ],
            "title": "and S",
            "venue": "Jha, \u201cPrivacy risk in machine learning: Analyzing the connection to overfitting,\u201d in 2018 IEEE 31st Computer Security Foundations Symposium (CSF), pp. 268\u2013 282, IEEE",
            "year": 2018
        },
        {
            "authors": [
                "M. Nasr",
                "R. Shokri"
            ],
            "title": "and A",
            "venue": "Houmansadr, \u201cComprehensive privacy analysis of deep learning: Passive and active white-box inference attacks against centralized and federated learning,\u201d in 2019 IEEE symposium on security and privacy (SP), pp. 739\u2013753, IEEE",
            "year": 2019
        },
        {
            "authors": [
                "K. Leino",
                "M. Fredrikson"
            ],
            "title": "Stolen memories: Leveraging model memorization for calibrated white-box membership inference,",
            "venue": "Security Symposium ({USENIX} Security",
            "year": 2020
        },
        {
            "authors": [
                "L. Song",
                "P. Mittal"
            ],
            "title": "Systematic evaluation of privacy risks of machine learning models,",
            "venue": "Security Symposium ({USENIX} Security",
            "year": 2021
        },
        {
            "authors": [
                "C.A. Choquette-Choo",
                "F. Tramer",
                "N. Carlini"
            ],
            "title": "and N",
            "venue": "Papernot, \u201cLabelonly membership inference attacks,\u201d in International Conference on Machine Learning, pp. 1964\u20131974, PMLR",
            "year": 2021
        },
        {
            "authors": [
                "S. Mukherjee",
                "Y. Xu",
                "A. Trivedi",
                "N. Patowary"
            ],
            "title": "and J",
            "venue": "L. Ferres, \u201cprivgan: Protecting gans from membership inference attacks at low cost to utility.,\u201d Proc. Priv. Enhancing Technol., vol. 2021, no. 3, pp. 142\u2013163",
            "year": 2021
        },
        {
            "authors": [
                "M. Azadmanesh",
                "B.S. Ghahfarokhi"
            ],
            "title": "and M",
            "venue": "A. Talouki, \u201cA white-box generator membership inference attack against generative models,\u201d in 2021 18th International ISC Conference on Information Security and Cryptology (ISCISC), pp. 13\u201317",
            "year": 2021
        },
        {
            "authors": [
                "J. Jordon",
                "J. Yoon"
            ],
            "title": "and M",
            "venue": "Van Der Schaar, \u201cPate-gan: Generating synthetic data with differential privacy guarantees,\u201d in International conference on learning representations",
            "year": 2018
        },
        {
            "authors": [
                "L. Xie",
                "K. Lin",
                "S. Wang",
                "F. Wang"
            ],
            "title": "and J",
            "venue": "Zhou, \u201cDifferentially private generative adversarial network,\u201d arXiv preprint arXiv:1802.06739",
            "year": 2018
        },
        {
            "authors": [
                "A. Triastcyn",
                "B. Faltings"
            ],
            "title": "Generating artificial data for private deep learning,",
            "venue": "Proceedings of the PAL: Privacy-Enhancing Artificial Intelligence and Language Technologies, AAAI Spring Symposium Series,",
            "year": 2019
        },
        {
            "authors": [
                "B. Wu",
                "S. Zhao",
                "C. Chen",
                "H. Xu",
                "L. Wang",
                "X. Zhang",
                "G. Sun"
            ],
            "title": "and J",
            "venue": "Zhou, \u201cGeneralization in generative adversarial networks: A novel perspective from privacy protection,\u201d Advances in Neural Information Processing Systems, vol. 32, pp. 307\u2013317",
            "year": 2019
        },
        {
            "authors": [
                "C. Xu",
                "J. Ren",
                "D. Zhang",
                "Y. Zhang",
                "Z. Qin"
            ],
            "title": "and K",
            "venue": "Ren, \u201cGanobfuscator: Mitigating information leakage under gan via differential privacy,\u201d IEEE Transactions on Information Forensics and Security, vol. 14, no. 9, pp. 2358\u20132371",
            "year": 2019
        },
        {
            "authors": [
                "Q. Chen",
                "C. Xiang",
                "M. Xue",
                "B. Li",
                "N. Borisov",
                "D. Kaarfar"
            ],
            "title": "and H",
            "venue": "Zhu, \u201cDifferentially private data generative models,\u201d arXiv preprint arXiv:1812.02274",
            "year": 2018
        },
        {
            "authors": [
                "L. Fan"
            ],
            "title": "A survey of differentially private generative adversarial networks,",
            "venue": "The AAAI Workshop on Privacy-Preserving Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "J. Chen",
                "W.H. Wang",
                "H. Gao"
            ],
            "title": "and X",
            "venue": "Shi, \u201cPar-gan: Improving the generalization of generative adversarial networks against membership inference attacks,\u201d in Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pp. 127\u2013137",
            "year": 2021
        },
        {
            "authors": [
                "A. Brock",
                "J. Donahue"
            ],
            "title": "and K",
            "venue": "Simonyan, \u201cLarge scale gan training for high fidelity natural image synthesis,\u201d in International Conference on Learning Representations",
            "year": 2018
        },
        {
            "authors": [
                "Y. Yazici",
                "C.-S. Foo",
                "S. Winkler",
                "K.-H. Yap"
            ],
            "title": "and V",
            "venue": "Chandrasekhar, \u201cEmpirical analysis of overfitting and mode drop in gan training,\u201d in 2020 IEEE International Conference on Image Processing (ICIP), pp. 1651\u20131655, IEEE",
            "year": 2020
        },
        {
            "authors": [
                "C. Meehan",
                "K. Chaudhuri"
            ],
            "title": "and S",
            "venue": "Dasgupta, \u201cA non-parametric test to detect data-copying in generative models,\u201d in International Conference on Artificial Intelligence and Statistics",
            "year": 2020
        },
        {
            "authors": [
                "T. Karras",
                "T. Aila",
                "S. Laine",
                "J. Lehtinen"
            ],
            "title": "Progressive growing of gans for improved quality",
            "venue": "stability, and variation,\u201d in International Conference on Learning Representations",
            "year": 2018
        },
        {
            "authors": [
                "G. van den Burg",
                "C. Williams"
            ],
            "title": "On memorization in probabilistic deep generative models,",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "H. Sun",
                "T. Zhu",
                "Z. Zhang",
                "D. Jin",
                "P. Xiong"
            ],
            "title": "and W",
            "venue": "Zhou, \u201cAdversarial attacks against deep generative models on data: A survey,\u201d IEEE Transactions on Knowledge & Data Engineering, no. 01, pp. 1\u20131",
            "year": 2021
        },
        {
            "authors": [
                "J. Li",
                "N. Li"
            ],
            "title": "and B",
            "venue": "Ribeiro, \u201cMembership inference attacks and defenses in classification models,\u201d in Proceedings of the Eleventh ACM Conference on Data and Application Security and Privacy, pp. 5\u201316",
            "year": 2021
        },
        {
            "authors": [
                "S. Theodoridis",
                "K. Koutroumbas"
            ],
            "title": "Pattern recognition",
            "venue": "Elsevier",
            "year": 2006
        },
        {
            "authors": [
                "T. Kailath"
            ],
            "title": "The divergence and bhattacharyya distance measures in signal selection,",
            "venue": "IEEE Transactions on Communication Technology,",
            "year": 1967
        },
        {
            "authors": [
                "T.M. Cover",
                "J.A. Thomas"
            ],
            "title": "Elements of information theory",
            "venue": "2nd edition,\u201d Willey-Interscience: NJ",
            "year": 2006
        },
        {
            "authors": [
                "D.P. Kingma",
                "J. Ba"
            ],
            "title": "Adam: A method for stochastic optimization,",
            "venue": "arXiv preprint arXiv:1412.6980,",
            "year": 2014
        },
        {
            "authors": [
                "A. Tripathy",
                "Y. Wang"
            ],
            "title": "and P",
            "venue": "Ishwar, \u201cPrivacy-preserving adversarial networks,\u201d in 2019 57th Annual Allerton Conference on Communication, Control, and Computing (Allerton), pp. 495\u2013505, IEEE",
            "year": 2019
        },
        {
            "authors": [
                "K. Shmelkov",
                "C. Schmid"
            ],
            "title": "and K",
            "venue": "Alahari, \u201cHow good is my gan?,\u201d in Proceedings of the European Conference on Computer Vision (ECCV), pp. 213\u2013229",
            "year": 2018
        },
        {
            "authors": [
                "D.S. Kermany",
                "K. Zhang"
            ],
            "title": "and M",
            "venue": "H. Goldbaum, \u201cLabeled optical coherence tomography ",
            "year": 2018
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014Generative Adversarial Networks, GANs, Membership Inference Attacks, Mutual Information, Maximum Entropy\nI. INTRODUCTION"
        },
        {
            "heading": "A. Motivation",
            "text": "Recent advances in the development of novel algorithms in machine learning and data analysis are mostly due to the availability of publicly accessible data sets and the possibility of data sharing. At the same time, several concerns have been raised regarding the violation of users\u2019 privacy, since adversaries can infer sensitive information about individuals by\nM. Shateri is with the Department of Systems Engineering, E\u0301cole de technologie supe\u0301rieure, QC, Canada. (e-mail: mohammadhadi.shateri@etsmtl.ca). F. Messina is with the School of Engineering - Universidad de Buenos Aires, Buenos Aires, Argentina (e-mail: fmessina@fi.uba.ar). F. Labeau are with the Department of Electrical and Computer Engineering, McGill University, QC, Canada. (e-mail: fabrice.labeau@mcgill.ca). P. Piantanida is with ILLS - International Laboratory on Learning Systems and MILA - Quebec AI Institute; CNRS, CentraleSupe\u0301lec; Montreal, QC H3C 1K3, Canada. (e-mail: pablo.piantanida@cnrs.fr).\nanalyzing the open access data sets [1]\u2013[3]. This is one of the main reasons why in some fields (e.g., medical applications, power systems, finances) there is a shortage of public realworld data sets. A promising solution to this problem is the development of data generators/synthesizers for producing synthetic data samples from the same underlying distribution of a given sensitive real-world data set [4]\u2013[6] while avoiding sharing data directly.\nGenerative adversarial networks (GANs) introduced firstly by Goodfellow et. al [7] have been extensively used for generating synthetic data in applications where there is limited access to real-world data sets. Despite their undeniable benefits, GANs are prone to overfitting and memorization of their training data sets [8], [9]. This makes them vulnerable to several privacy attacks such as membership inference attacks (MIAs) [10]\u2013[13]. To be more specific, if a GAN experiences overfitting or memorizes the training data, it subsequently facilitates an adversary\u2019s ability to differentiate between generated samples that originate from the training dataset and those that do not. Furthermore, its discriminator assigns elevated scores to the data points that were included in the training set. This implies that the influence of overfitting and memorization on the accuracy of Membership Inference Attacks (MIA) is substantial. This point is discussed in more detail in subsections II-B& II-C. As a consequence, there is a high practical interest in developing privacy-aware training mechanisms for GANs."
        },
        {
            "heading": "B. Related work",
            "text": "MIAs on machine learning models were proposed for the first time by Shokri et. al in [14]. Recently, many different MIA strategies have been proposed that are effective on the classification/regression models [15]\u2013[19]. However, developing MIAs on generative models is more challenging and the strategies studied for classification/regression models typically do not perform well. For example, in [10] an MIA based on shadow training, inspired by the MIA in [14], was applied to a GAN trained with LFW face data sets, and the adversary performance was found to be similar to random guessing.\nMembership inference attacks on GAN models can be generally classified into two categories based on their target, either focusing on the generator or the discriminator, namely, Discriminator-based Membership Inference Attacks (DMIA) and Generator-based Membership Inference Attacks (GMIA).\nThe first DMIAs were introduced in [10] where the adversary\u2019s goal is to distinguish data points used in the training\nar X\niv :2\n31 1.\n03 17\n2v 1\n[ cs\n.L G\n] 6\nN ov\n2 02\n3\n2 dataset, accomplished by accessing the target GAN\u2019s discriminator. Empirical results on various datasets reveal the effectiveness of this strategy, achieving very high accuracy, even 100% in some cases. In addition, Mukherjee et al. [20] introduced an upper limit for membership inference accuracy in white-box MIAs on a GAN\u2019s discriminator, determined by their Total Variation Distance (TVD) attack, which estimates the total variation distance between the distribution of discriminator scores on the training and holdout datasets.\nShifting to GMIAs, a Monte Carlo (MC) attack was introduced in [11]. This MIA leverages Monte Carlo integration to exploit synthetic samples that are in the vicinity of the target sample, inferring the likelihood that the target sample belongs to the training dataset. This approach particularly excels in set membership inference, where the adversary determines whether a set of data points belongs to the training dataset or not. A co-membership inference attack strategy, akin to the MC approach but using the L2-distance metric, was proposed in [12]. However, this method is computationally more complex than existing approaches. The first taxonomy of MIAs on generative models was outlined in [13], introducing a generic attack model in which the adversary aims to reconstruct the closest synthetic data point to a target sample. This approach relies on the generator\u2019s ability to produce synthetic samples resembling the training set, with the distance between the reconstructed synthetic sample and the target sample used to calculate the probability that the target sample belongs to the training data set. Extensive empirical comparisons across different scenarios and datasets have demonstrated the efficiency of this MIA strategy compared to other MIAs against generative models [13]. Another recent MIA targeting the generator of GANs was developed in [21], where an auto-encoder, with the target generator as its decoder, is trained based on generated samples and their associated latent samples. During inference, the reconstruction error of a target sample, processed through the trained auto-encoder, is used to infer its membership label. This attack proved to be successful in cases when the number of training samples was small while not much better than a random attacker in other cases.\nTaking into account all the proposed MIAs on GANs, it is widely acknowledged that the white-box attacks targeting the GAN\u2019s discriminator (DMIAs), specifically the TVD-based attack [20] and the MIA introduced in reference [10], are among the most effective MIAs against GANs. In contrast, other attack methods do not exhibit significantly improved performance compared to random attacks. Consequently, in our study, we assess the effectiveness of the proposed defense mechanisms based on the MIA strategies outlined in [10], [20].\nTo address the vulnerability of generative models to MIAs, several defenses have been presented in the literature. The main idea of most of these frameworks is based on differential privacy (DP) [22]\u2013[26]. Although these frameworks were shown effective in preventing membership inference by adversaries, DP-based GANs degrade significantly the quality of synthetic samples [10], [27]. For more details on DPGANs, the reader is referred to [28]. On the other hand, a regularization technique known as dropout has been suggested in [10], [11] in order to improve generalization in generative\nmodels and mitigate the membership inference issue. Using dropout techniques in GANs raises two main issues: determining the optimal dropout rate and placement, often requiring trial and error, and the significant slowdown in training, which can be challenging due to GANs\u2019 inherent instability. More sophisticated defenses designed specifically against MIAs in GANs were presented recently, including PrivGan [20] and PAR-GAN [29]. In both methods, the training data set is split into N disjoint sub-sets. In PrivGan, a GAN (including a generator and a discriminator) is trained for each sub-set and the generators are trained to not only fool their associated discriminator but also prevent a classifier from distinguishing their generated samples from the other generators\u2019 samples. On the other hand, in the PAR-GAN, a single generator is trained to fight with N discriminators (associated with N disjoint data subsets). Considering the empirical results of these methods applied to several data sets, although both approaches showed to be effective (particularly for large values of N ) in improving the generalization and mitigating the information leakage exploited by MIAs, they increase the computational complexity of the GAN training procedure quite significantly without providing any mathematical guarantees for reducing the overfitting or memorization. In addition, due to the inherent requirement of these models to use data partitions, they are not appropriate for cases in which there is a limited-size training data set. It also should be noted that for the PrivGan which includes N generators, although a random selection strategy is suggested in [20], it is not clear what is the best approach for sharing the final synthetic data samples.\nIn this work, we adopt measures from information theory and statistics to modify the GAN framework for the sake of making it robust to MIA. To gain a deeper understanding of overfitting in GANs, we utilize the Bhattacharyya coefficient, calculated at the discriminator\u2019s output, as a metric for overfitting assessment. Subsequently, we introduce a defense mechanism that addresses this measure of overfitting, named the Maximum Entropy GAN (MEGAN). MEGAN is a straightforward modification of the conventional GAN that ensures robust learning of training data distributions while reducing MIA accuracy to a level akin to random attacks. Additionally, our study puts forth a second heuristic defense approach, the Mutual Information Minimization GAN (MIMGAN), which minimizes the mutual information between generated and training data through a variational representation, offering a practical strategy for guarding against MIAs. Experimental studies are done using four commonly used datasets including MNIST, fashion-MNIST, Chest X-ray images (Pneumonia), and Anime Faces datasets. The performance of the proposed defense mechanisms is evaluated compared with state-of-theart models such as PrivGAN and DP-GAN, in terms of their robustness against MIAs and the fidelity and diversity of generated samples."
        },
        {
            "heading": "C. Contributions",
            "text": "The main contributions of this work can be summarized as follows: \u2022 We study overfitting in GANs in terms of the discrimi-\nnator by using the Bhattacharyya coefficient and discuss\n3 its relation and advantage over the classical notion of overfitting (i.e., generalization gap).\n\u2022 Considering this new perspective of overfitting and the well-known Fano\u2019s inequality applied to the discriminator error, we propose the maximum entropy GAN (MEGAN) as a defense mechanism to state-of-the-art MIAs. MEGAN is a simple modification of the vanilla GAN that ensures learning the distribution of training data and at the same time reducing the MIAs accuracy to a level similar to that of a random attacker. \u2022 As a second defense framework, a heuristic model based on minimizing the mutual information between the generated data and training data is proposed. In order to provide a simple implementation of this idea, we consider a variational representation of the mutual information. This method is referred to as the mutual information minimization GAN (MIMGAN) in the following.\nOrganization of the paper: The rest of the paper is organized as follows. In Section II we present a background on GANs, the notions of overfitting and memorization in GANs, and MIAs on GANs. Then, in Section III, the state-of-the-art in MIAs on GANs is considered from a new perspective and two defense mechanisms, MEGAN and MIMGAN, are proposed and discussed in detail. Extensive experimental results are presented and discussed in Section IV. Finally, concluding remarks are given in Section V.\nNotation and mathematical conventions: Throughout this article, we use bold-face small letters, e.g. x to denote random variables, regular-face small letters, e.g. x to denote specific realizations, and the capital letters, e.g. X to refer to the set of values. P refers to the probability measure. p is either a probability density function or a probability mass function. We use p(x|y) to denote the conditional probability density function of x given y. Only when it is necessary to avoid confusion we include a subscript in p. E[\u00b7] is the expectation with respect to the joint distribution of all random variables involved; H(x) is the Shannon entropy of random variable x and I(x;y) is the mutual information between x and y."
        },
        {
            "heading": "II. BACKGROUND",
            "text": ""
        },
        {
            "heading": "A. Generative Adversarial Networks",
            "text": "The generative adversarial network (GAN) is comprised of two deep neural networks: a generator G(z; \u03b8g), with parameters \u03b8g , and a discriminator D(x; \u03b8d), with parameters \u03b8d, that are trained by competing in a minimax game. The generator receives random samples from the latent space (noise data z \u223c pz) as the input and aims to generate synthetic data by learning pxtr , the probability distribution of the training dataset Xtr. On the other hand, the discriminator aims to determine the authenticity of the training data points versus the synthetic data points, by producing a real-valued number in the range [0, 1] interpreted as the probability that its input comes from original data rather than from the generator. The\nGAN framework is trained by playing the following minimax game with the value function V (G,D) [7]:\nmin G max D\nV (G,D) = E [ logD(xtr) ] +\nE [ log(1\u2212D(G(z))) ] , (1)\nwhere the training data points are modeled as independent and identically distributed (i.i.d.) samples of the random variable xtr \u223c pxtr . It can be shown that minimizing the GAN loss in equation (1) results in pxg = pxtr , i.e. the generator distribution would be identical to the distribution of the training dataset [7]. After training the GAN framework, the generator can be used for generating synthetic samples resembling real training data samples."
        },
        {
            "heading": "B. Overfitting and Memorization in GANs",
            "text": "In discriminative models (classification and regression), overfitting is defined in terms of the difference between the performance of a model on the training data set and its performance on the test/hold-out data samples. In other words, a model overfits if it performs significantly better on the training data compared with the test data. However, for generative models, there is no generally accepted notion of overfitting. In the literature, overfitting in GANs is typically defined in terms of the discriminator output for the training and test data samples [7], [20], [25], [29]\u2013[31]. This sometimes is referred to as the generalization gap and is defined as follows [29]:\ngDGAN = E [ \u03d5 ( D(xtr) )] \u2212 E [ \u03d5 ( D(xte) )] , (2)\nwhere gDGAN is the generalization gap and \u03d5(x) is selected as log(x) or simply as x.\nRegarding memorization, the nearest neighbor test has been used extensively in the literature [7], [30]\u2013[34], especially for the cases where a global sense of memorization is of interest [35]. More specifically, considering the premise that memorization in GANs occurs when the generated data samples are closer to the training dataset than the actual samples from the test set, equation (3) is used:\nmGGAN =\nE [\nmin x\u2208XTR d (xte, x) ] E [\nmin x\u2208XTR\nd ( G(z), x )] , (3) where mGGAN is the measure of memorization in GANs, and d(\u00b7, \u00b7) is a distance function (e.g., the Euclidean distance). It should be noted that mGGAN > 1 can be interpreted as memorization."
        },
        {
            "heading": "C. Membership Inference Attack",
            "text": "In a membership inference attack (MIA) to a GAN, the goal of the attacker is to determine whether a specific data sample x \u2208 X was used in the training of the target GAN or not. In these attacks, an adversary aims to infer whether an instance or a set of target samples was used to train a specific model or not. Depending on the information available to the\n4 adversary, the MIAs are categorized into two main families: black-box attacks and white-box attacks. In the former, it is assumed that the adversary can only get (unlimited) query access to the target model, while in the latter the adversary has full knowledge of the model parameters. In most MIA frameworks on GANs, it is assumed that the attacker has access to a data pool X that includes the training data set (i.e., Xtr \u2282 X) without having knowledge about training data samples. Therefore, an attacker seeks to learn a mapping MGAN : X \u2212\u2192 [0, 1], where MGAN(x) can be interpreted as the likelihood of x \u2208 Xtr.\nFor discriminative models, the relation between the generalization gap and MIAs was studied in [15]. Regarding GAN frameworks, this relation was proposed by Wu et.al. [25] where they theoretically showed how this generalization gap can be bounded for a GAN trained with a differentially private learning algorithm. In addition, this notion is asserted by other studies such as [29], [36], [37]. Incorporating this generalization gap in the attack mechanism is the main intuition beyond the membership inference attack proposed by Hayes et al. [10] which assumes a white-box scenario where the adversary not only can get queries from the discriminator but also has access to a substantial data pool and is aware of the size of the training dataset used in training the target generative model. As discussed before, this MIA has shown to be effective on GANs and has been considered widely for evaluating defense mechanisms [20], [29]. More precisely, this attack is performed based on the discriminator response to the target data samples. When the generalization gap is large, it means the discriminator overfits to the training dataset and therefore it returns high values (probability values close to one) for the training data samples and small values for the hold-out and synthetic data samples. By knowing the size of the training dataset and having access to a pool of datasets, the attacker can incorporate the discriminator confidence on the pool data samples to infer the training data samples. Fig. 1 represents the details of this MIA strategy.\nDuring this study, following the literature, we opt to evaluate our defense frameworks based on the proposed MIA in Fig. 1. Additionally, in this work, we will utilize the TVD attack, as proposed in [20], to establish an upper limit for the attackers\u2019 performance in inferring the training samples."
        },
        {
            "heading": "III. PROPOSED FRAMEWORKS",
            "text": ""
        },
        {
            "heading": "A. Measure of overfitting",
            "text": "It was mentioned that the accuracy of the membership inference attacker is closely connected to the generalization gap. Thus, most of the defense mechanisms proposed in the literature, such as those based on differential privacy or more recent frameworks such as PrivGAN [20] and PARGAN [29], were developed based on the idea of decreasing the generalization gap. In our work, we want to explore the relation between the generalization gap and the accuracy of membership inference attackers from another, more general, point of view. In fact, we show that although reducing the generalization gap can reduce the accuracy of MIAs, there might be other cases in which we can reduce the inference performance of the attacker without reducing the generalization gap.\nIn a GAN framework, the discriminator is a binary classifier that aims to distinguish between the real data samples (training data samples) and the fake/synthetic sample points crafted by the generator. However, the notion of overfitting in the discriminator of GANs is different from that of a regular classifier. When the discriminator overfits on training samples, it usually returns significantly high values for the training data samples and small values for the other samples including fake and test samples. This point is discussed more empirically in\nFig. 2 where a GAN is trained on MNIST data and the density of the output of the discriminator is presented at different training epochs for the training, test, and fake data samples. From this figure, it can be seen that at the initial epochs, the discriminator returns high scores for the train and test data samples while very small values for the fake samples. Once the generator is able to generate real-looking samples, the discriminator responds the same for fake and real (train and test) data (see results of epoch 100). However, after several hundred epochs, the discriminator overfits on the training data and starts returning high values for train data samples and small values for the fake and test data points. It should be noted that the discriminator responses are almost the same for the fake and test data samples and the generator is able to generate real-looking samples. This can be observed in the generated samples in Fig. 2. It can clearly be seen from Fig. 2 that overfitting in the discriminator increases the generalization gap. During the training process, although test data are not available, the generalization gap can be approximated from the discriminator responses to fake and train data samples. This generalization gap can be exploited by the attacker to infer the membership of data samples. However, it is not clear how the MIA accuracy and the generalization gap are related mathematically. In the following, we introduce the socalled Bhattacharyya coefficient, defined at the output of the discriminator, as a measure of overfitting and we discuss its relationship with the MIA performance.\nConsider a GAN framework trained on a training data set XTR. Let the score set S be defined as follows:\nS = {s : s = D(x), x \u2208 X},\n5\nwhere X is a publicly available data pool that includes XTR. Consider an MIA that aims to infer the training set XTR using the score set S. Let \u03c91 and \u03c90 refer to the class of scores associated with the training and non-training data samples, respectively. The minimum error of any membership inference attacker modeled as a binary classifier MdGAN : S \u2212\u2192 {\u03c90, \u03c91} is equal to the Bayes error defined below [38]:\nPme = P(s \u2208 R1, \u03c90) + P(s \u2208 R0, \u03c91), (4)\nwhere Pme is the error of membership inference attacker, Ri , i = 0, 1 is the score space in which the attacker decides in favor of \u03c9i and P(., .) is the joint probability. As it is mentioned by Kailath [39], considering \u03c00 and \u03c01 as the a priori probabilities, this error can be bounded as follows:\n1 2 \u2212 1 2\n\u221a 1\u2212 4\u03c00\u03c01\u03c12 \u2264 Pme \u2264 \u221a \u03c00\u03c01\u03c1, (5)\nwhere, considering likelihood functions p(s|\u03c90) and p(s|\u03c90), the Bhattacharyya coefficient \u03c1 is defined below [39]:\n\u03c1 := \u222b \u221a p(s|\u03c90)p(s|\u03c91)ds. (6)\nFrom the error bound (5) it can be seen that maximizing \u03c1 can be used to limit the performance of the membership inference attacker. In statistics, the Bhattacharyya coefficient is used as a separability measure between two classes/populations [38]. Therefore, maximizing \u03c1 can be interpreted as maximizing the overlap (minimum separability) between score classes \u03c90 and \u03c91. This point is directly related to overfitting in the discriminator since \u03c91 (the class of score associated with the training samples) and \u03c90 (the class of score associated with the non-training samples) have maximum separability in the case of overfitting discriminator. This fact is illustrated in Fig. 2 as discussed previously. As mentioned before, in this paper, we consider the Bhattacharyya coefficient \u03c1, instead of generalization gap gDGAN in equation (2), as a measure of overfitting. More specifically, a value of \u03c1 close to one (large overlap) is interpreted as a case with smaller overfitting, while a value of \u03c1 close to zero is associated with high overfitting in the GAN.\nIn the following, we analyze in more detail the Bhattacharyya coefficient for the Gaussian scenario in order to gain\nmore insight and discuss its relationship with the generalization gap.\nProposition 1: [38] Consider the Gaussian case where the densities p(s|\u03c90) and p(s|\u03c91) are, respectively, N ( \u00b50, \u03c3 2 0 ) and N ( \u00b51, \u03c3 2 1 ) . In this case, the Bhattacharyya coefficient would be as follows:\n\u03c1 = exp \u22121 4 ln 1 4 ( \u03c321 \u03c320 + \u03c320 \u03c321 + 2 )\u2212 1 4 [ (\u00b50 \u2212 \u00b51)2 \u03c320 + \u03c3 2 1 ]. (7)\nFrom equation (7), it can be seen that for fixed variances, reducing the differences between the mean values can increase \u03c1 (and thus reduce overfitting). This corresponds to the definition of the generalization gap. Nevertheless, equation (7) also illustrates that increasing the variances (even for fixed mean values) can increase the \u03c1. In other words, the Bhattacharyya coefficient is a more general indicator of overfitting compared with the generalization gap. From a GAN point of view, the Bhattacharyya coefficient between the score densities associated with training samples and fake samples can be calculated and used as a measure of overfitting in the discriminator. It is worth noting again that once the generator learns to generate real-looking samples, the discriminator responds almost equally to synthetic and test data samples (see\nFig. 2). Thus, the Bhattacharyya coefficient associated with the training and synthetic samples can be used as an estimation of the Bhattacharyya coefficient associated with the training and test data samples.\nIn the next subsections, two new GAN frameworks will be presented as defense mechanisms against membership inference attacks and the Bhattacharyya coefficient defined in equation (6) will be used to quantify the overfitting."
        },
        {
            "heading": "B. Maximum Entropy GAN (MEGAN)",
            "text": "In this section, the first defense framework against membership inference attacks for GANs is presented. To this end, overfitting is studied in terms of the discriminator error (as a binary classifier) and we discuss how overfitting and discriminator error are inversely related.\nProposition 2: In a GAN framework, the discriminator is a binary classifier that aims to measure the likelihood of each\n6 sample being real. Particularly, considering the binary random variable re \u2208 {r, f}, we can say:\nD(x) = pre|x(r|x), \u2200x \u2208 X, (8)\nor equivalently, pre|x(f |x) = 1 \u2212 D(x) denotes the probability that x is a fake sample. Let the regions Rr and Rf , respectively, refer to the sample spaces in which we decide in favor of the sample being real and fake. Then, the Bayes error of the discriminator, as a binary classifier, can be written as follows:\nP de = P(x \u2208 Rr, re = f) + P(r \u2208 Rf , re = r) = P(x \u2208 Rr|re = f)p(f) + P(x \u2208 Rf |re = r)p(r) (9)\n= p(f) \u222b Rr p(x|f)dx+ p(r) \u222b Rf p(x|r)dx (i) =\n\u222b Rr p(f |x)p(x)dx+ \u222b Rf p(r|x)p(x)dx\n(ii) = \u222b Rr (1\u2212D(x))p(x)dx+ \u222b Rf D(x)p(x)dx (iii) = p(r)\u2212\n\u222b Rr (2D(x)\u2212 1)p(x)dx\n(iv) = p(f)\u2212 \u222b Rf (1\u2212 2D(x))p(x)dx.\nwhere (i) is due to the Bayes rule, (ii) is based on the equation (8), and (iii) and (iv) are based on the following:\np(r) = \u222b Rr D(x)p(x)dx+ \u222b Rf D(x)p(x)dx,\np(f) = \u222b Rf (1\u2212D(x))p(x)dx+ \u222b Rr (1\u2212D(x))p(x)dx.\nWhen the discriminator overfits on the training data samples, D(x) is generally larger for x \u2208 Xtr \u2282 Rr than for x /\u2208 XTR. Thus, from equation (9), both last and second last equalities, it can be seen that the discriminator error decreases as D(x) increases in the region x \u2208 Xtr. In other words, overfitting in the GAN discriminator is directly related to the error of the discriminator. More specifically, a smaller error in the discriminator usually gives a larger overfitting on the training data samples. Therefore, to limit the performance of membership inference attacks, we need to control the minimum error of the discriminator of the GAN in order to avoid overfitting. This can be done by considering the socalled Fano\u2019s inequality from the field of information theory to obtain a bound on P de .\nProposition 3: From Fano\u2019s inequality [40], we have\nP de \u2265 H(re|x)\u2212 1\nlog 2 \u2248\nE [ H(D(x)) ] \u2212 1\nlog 2 . (10)\nFrom (10), it can be seen that maximizing E[H(D(x))] can avoid the discriminator error from being very small and thus can be used to control overfitting.\nMotivated by Proposition 4, we propose a novel framework to learn the distribution of training data and simultaneously reduce the accuracy of the membership inference attackers by decreasing overfitting through maximization of the lower bound in equation (10). Similar to the GAN, this framework\nincludes a generator and a discriminator but, unlike the GAN, the generator is trained to maximize the entropy of the discriminator outputs. More specifically, this framework named Maximum Entropy GAN (MEGAN) is developed by solving the following multi-objective optimization problem:\nMEGAN : max D,G (h1, h2) , (11)\nwith h1 =E [ logD(xtr) ] + E [ log(1\u2212D(G(z))) ] , (12)\nh2 =E [ H ( D(G(z)) )] , (13)\nwhere H(p) = \u2212p log p \u2212 (1 \u2212 p) log(1 \u2212 p) is the entropy of a Bernoulli random variable with parameter p [40]. On the one hand, the first optimization problem in (11) is exactly the discriminator optimization in the original GAN framework [7]. Therefore, for a fixed generator, following the proof proposed in [7] the optimum discriminator would be as follows:\nD\u2217(x) = pxtr(x)\npxtr(x) + pxg(x) . (14)\nOn the other hand, since the binary entropy H(p) is maximized for p = 0.5, the solution for the second optimization problem in (11) is D(G(z)) = 0.5. Thus, considering (14), the MEGAN optimization problem (11) converges to its optimum when pxg = pxtr . This means that, as in classical GANs, MEGAN can learn the underlying distribution of the training data points. However, in vanilla GANs, the generator learns the distribution through a minimax optimization by trying to fool the discriminator into confusing real and fake data samples, while the generator in MEGAN learns the distribution by maximizing the uncertainty of the discriminator. The experimental results will show how this can reduce the accuracy of MIAs. The training algorithm for MEGAN is explained in detail in Algorithm 1. It should be noted that, at each training iteration of the discriminator, the generator is generally trained for several iterations since, unlike the discriminator which is a simple binary classifier, the generator has a more difficult task."
        },
        {
            "heading": "C. Mutual Information Minimization GAN (MIMGAN)",
            "text": "In this section, we propose a more general framework motivated by the fact that synthetic samples might leak information regarding the training data points which can be used by membership inference attackers. To prevent such an information leakage, besides learning the training distribution, we train the generator to produce samples that give minimum information about the training data set Xtr = {xtr,i}ni=1. By defining the joint random variable xntr = (xtr,1,xtr,2, . . . ,xtr,n), the mutual information between the synthetic data and the training data points, i.e. I(xntr;xg), is considered. Basically, minimizing this mutual information means that each generated sample should leak minimum information about the whole training dataset. However, minimizing this mutual information\n7 Algorithm 1: Maximum Entropy GAN (MEGAN): Batch size B and number of steps to apply to the Generator k are hyperparameters.\n1: for number of training iterations do 2: Sample minibatch of B noise samples {z1, . . . , zB} from noise\ndistribution pz. 3: Sample minibatch of B examples {x1, . . . , xB} from training data\nset distribution pxtr . 4: Compute the gradient of LD(\u03b8d), approximated empirically for\nminibatch, with respect to \u03b8d and update \u03b8d by applying the Adam optimizer [41].\nLD(\u03b8d) := \u2212 1n \u2211n i=1 [ logD(xi) + log(1\u2212D(G(zi))) ] .\n5: for k steps do 6: Sample minibatch of B noise samples {z1, . . . , zB} from noise distribution pz. 7: Compute the gradient of LG(\u03b8g), approximated empirically for\nminibatch, with respect to \u03b8g and update \u03b8g by applying the Adam optimizer. LG(\u03b8g) := 1n \u2211n i=1 [ D(G(zi)) logD(G(zi))+( 1\u2212D(G(zi)) ) log ( 1\u2212D(G(zi)) ) ] .\n8: end for\n9: end for\ndirectly is generally very cumbersome. Thus, we look for a surrogate upper bound to optimize instead:\nI(xntr;xg) = H(x n tr)\u2212H(xntr|xg)\n(i) = n\u2211 i=1 H(xtr,i)\u2212H(xntr|xg) (ii) \u2264 n\u00d7H(xtr,j)\u2212H(xtr,j|xg) (15) = (n\u2212 1)\u00d7H(xtr) + I(xtr,j;xg),\nwhere step (i) is due to the assumption that training data points are independent and identically distributed (i.i.d.) and (ii) is based on the fact that the joint entropy is larger than each marginal entropy, where j \u2208 [1, n] is selected randomly. Therefore, instead of working with the I(xntr;xg), its upper bound in the equation (15) can be minimized. Since for a fixed training data set the first term of the upper bound in the equation (15) is a constant, the I(xtr,j;xg) can be minimized. It should be emphasized that the index j \u2208 [1, n] here is selected randomly. This is along with the nature of GANs, where the outputs of the generator are not labeled and so generally there are no pre-defined pairs of samples in the format (xtr, xg). Therefore, in the process of training our GAN framework, for each generated sample, a random data point from the training dataset should be paired with it. From now on, for the sake of convenience, the term I(xtr;xg) is used instead of I(xtr,j;xg). This term can be added as a regularization term to the GAN loss in equation (1). However, to simplify this regularization term, we consider an arbitrary conditional distribution qxtr|xg and note that:\nI(xtr;xg) = H(xtr)\u2212H(xtr|xg) = H(xtr)+ E [ log qxtr|xg ] + KL ( pxtr|xg\u2225qxtr|xg ) \u2265\nH(xtr) + E [ log qxtr|xg ] , (16)\nwhere KL(.\u2225.) is the Kullback\u2013Leibler divergence, a measure of how a probability distribution (the first term in KL\nfunction) is different from another probability distribution (the second term in KL function) [40], and the last inequality is due to the fact that the KL is non-negative. Since KL ( pxtr|xg\u2225qxtr|xg ) = 0 when pxtr|xg = qxtr|xg , the mutual information I(xtr;xg), considering equation (16), can be written as follows:\nI(xtr;xg) = H(xtr)+ max qxtr|xg\nE [ log qxtr|xg ] , (17)\nwhere the expectation is with respect to the true distribution pxtr|xg . Since the first term in equation (17) is constant, the minimization of the mutual information I(xtr;xg) would end up with a minimax problem between the generator and another network (named as adversary network). Adding the minimax game (17) to the minimax formulation of the GAN in equation (1), the total formulation of our proposed framework is as follows:\nmin G max D, A\nV (G,D,A) = E [ logD(xtr) ] +\nE [ log(1\u2212D(G(z))) ] + (18)\n\u03bb\u00d7 E [ log qxtr|xg ] ,\nwhere \u03bb is the Lagrangian coefficient and A(z; \u03b8a) is the adversary network with parameter \u03b8a.\nIt should be noted that for the cases where qxtr|xg is the probability density function of a continuous random variable, it can be approximated by a conditional Gaussian distribution, i.e. qxtr|xg(xtr|xg) = N ( xtr;\u00b5A(xg),\u03a3A(xg) ) where\nN (x;\u00b5,\u03a3) = det(2\u03c0\u03a3)\u22121/2 exp [ \u2212 12 (x\u2212 \u00b5) T\u03a3\u22121(x\u2212 \u00b5) ] is the probability distribution function of the Gaussian distribution [42]. Therefore, the GAN framework is updated by adding an adversary network that receives synthetic data at its input and outputs the vector of means \u00b5A and covariance matrix \u03a3A of the Gaussian distribution. More precisely, this adversary network aims to estimate the vector of means and the covariance matrix through maximizing qxtr|xg (maximum likelihood estimation). In this way, the generator learns to minimize I(xtr;xg).\nThe complete training algorithm for the proposed framework is explained in Algorithm 2. It should be noted that in the loss function of the generator, to get rid of the saturation, instead of minimizing E [ log(1\u2212D(G(z))) ] the term\nE [ \u2212 log(D(G(z))) ] is minimized [7]."
        },
        {
            "heading": "IV. NUMERICAL RESULTS AND DISCUSSION",
            "text": "In this study, four data sets that are commonly used in the literature are considered where the results of the MNIST and fashion-MNIST are presented here while the results for the Chest X-Ray Images (Pneumonia) dataset and Anime Faces dataset are shown in the Appendix. For all the experiments (except those that are explained explicitly) the models are trained on 10% of the total data points and the other 90% is used as the test or hold-out data points. Therefore, the accuracy of a random guessing strategy in inferring the training data points is 10%. The MIA strategy based on the discriminator, proposed in [10], is used to assess the performance of the models. It is assumed that the attacker is aware of all the data\n8 Algorithm 2: MIMGAN: Privacy-preserving GAN based on minimum information leakage about the training data set. Batch size B and the number of steps k to apply to the adversary and discriminator networks are hyperparameters. The least expensive choice, i.e. k = 1, is used in this study.\n1: for number of training iterations do 2: for k steps do 3: Sample minibatch of B noise samples {z1, . . . , zB} from noise distribution pz. 4: Sample minibatch of B examples {x1, . . . , xB} from training data set distribution pxtr . 5: Compute the gradient of LD(\u03b8d), approximated empirically for\nminibatch, with respect to \u03b8d and update \u03b8d by applying the Adam optimizer [41].\nLD(\u03b8d) := \u2212 1B \u2211B i=1 [ logD(xi) + log(1\u2212D(G(zi))) ] 6: Compute the gradient of LA(\u03b8a), approximated empirically for\nminibatch, with respect to \u03b8a and update \u03b8a by applying the Adam optimizer.\nLA(\u03b8a) := \u2212 1B \u2211B i=1 logN ( xi;\u00b5A(G(zi)),\u03a3A(G(zi)) ) 7: end for 8: Sample minibatch of B noise samples {z1, . . . , zB} from noise\ndistribution pz. 9: Sample minibatch of B examples {x1, . . . , xB} from training data\nset distribution pxtr . 10: Compute the gradient of LG(\u03b8g , \u03b8d, \u03b8a, \u03bb), approximated\nempirically for minibatch, with respect to \u03b8g and update \u03b8g by applying the Adam optimizer.\nLG(\u03b8g , \u03b8d, \u03b8a, \u03bb) := 1B \u2211B i=1 [ \u2212 logD(G(zi))+\n\u03bb\u00d7 logN ( xi;\u00b5A(G(zi)),\u03a3A(G(zi)) ) ] 11: end for\npoints and knows that 10% of that data was used to train the target GAN."
        },
        {
            "heading": "A. Effect of overfitting on the performance of MIA",
            "text": "As discussed previously, the membership inference attack proposed by Hayes et al. [10] relies on overfitting of the discriminator to the training data samples, i.e., the discriminator returns higher values for the training data than for the test (real unseen) data samples. In the following, we examine overfitting in relation to MIAs based on the MNIST dataset. To this end, different DCGAN frameworks are trained on the 10% of the total MNIST data points. To check if the discriminator is overfitting to training data samples, the discriminator output for the train, test (hold-out), and the synthetic data points are monitored at each iteration and a gap between the discriminator output for the train and test data samples is considered as overfitting. The ideal case is when the discriminator returns 0.5 for both train and test data. In addition, assigning the same value to test and synthetic data points by the discriminator is of interest since it means the generator is able to generate synthetic samples that match the real data samples. Fig. 3 presents the results of discriminator overfitting for three cases where GAN with very strong discriminator, strong discriminator, and mild discriminator is used, respectively. In this study, the overfitting in the discriminator is quantified by the Bhattacharyya coefficient defined in equation (6) where the density of train and test data samples at the output of discriminator is used (see Fig. 3). In addition, the memorization is measured using the mGGAN\ndefined in equation (3). In Table I, the results of overfitting in discriminator (based on the generalization gap defined in equation (2) and the Bhattacharyya coefficient), memorization in generator (calculated based on 2000 synthetic samples), and accuracy of MIA are presented for different GAN structures of Fig. 3.\nConsidering Table I, from Fig. 3 (a) it can be seen that when the overfitting of the discriminator is significant, the distribution of the discriminator scores for training samples does not overlap perfectly with the one for test samples. This can be used by the attacker to take apart more confidently the training samples from the test samples. On the other hand, Figs. 3 (b) & (c) show that reducing overfitting in the discriminator can reduce the attacker\u2019s inference ability.\nIn addition to reducing the MIA accuracy, Fig. 4 compares the case (a) in Fig. 3, as an extreme case, with the case (c), as a mild case, in terms of the class distribution and samples quality. From this figure, it can be seen that although both cases can generate synthetic data samples of almost the same quality (same precision), the extreme case (in terms of the discriminator overfitting) tends to generate some classes more than others (small recall).\n9\nIt is worth noting that such a policy, i.e., simplifying the discriminator architecture, cannot completely prevent membership inference attackers from inferring membership labels. This can be seen from Table I where for the case with a mild discriminator, the white-box attack accuracy is still far from random guessing accuracy 10%."
        },
        {
            "heading": "B. Evaluation of the proposed frameworks against MIAs",
            "text": "In this subsection, the experimental results of the proposed frameworks are presented and compared with the standard GAN. The model architectures and hyperparameters of each framework are listed in Appendix 2 and Appendix 3.\nFirst, we present the results of the MEGAN model trained on the MNIST and fashion-MNIST data sets. Fig. 5 shows the convergence of the MEGAN in terms of the loss functions of the discriminator and generator. In addition, the discriminator output for the train, test, and synthetic data samples is represented. From Fig. 5 it is clear that the MEGAN can converge and reach stability and is able to generate reallooking synthetic data samples. In addition, it can be seen that the density of the output of the discriminator for training and test data samples overlaps perfectly. Therefore, it can significantly reduce the accuracy of the membership inference attacker.\nRegarding the MIMGAN, for different values of \u03bb (see equation (18)) the histogram of the discriminator output for the train and test data sets along with examples of the generated data samples is presented in Fig. 6. From this figure, it can be seen that by increasing the value of \u03bb the amount of overlapping between the train and test density at the output of the discriminator increases. Thus, similar to MEGAN, we can expect MIMGAN to reduce the accuracy of the membership inference attackers. However, it should be noted that there is a clear difference between the MEGAN and MIMGAN approaches in reducing the accuracy of MIA. In MEGAN the overlapping between the discriminator output densities is done by making them distributed mainly around 0.5 i.e., the optimum point of the discriminator, while the MIMGAN increases the variance of the discriminator output.\nTo quantify this view, the overfitting of the discriminator and generator for GAN, MEGAN, and MIMGAN are examined in Table II in terms of the accuracy of MIA and the overfitting parameters (i.e. generalization gap defined in equation (2) and the Bhattacharyya coefficient proposed in equation (6)).\nLooking at Table II, compared with GAN, it was expected that MEGAN would reduce the discriminator overfitting significantly. In addition, the generator in MEGAN has less overfitting than the one in GAN and so it is expected to reduce the accuracy of MIA extensively. The MIMGAN can also reduce the accuracy of MIA to discriminator where the overlapping parameter increases by increasing the value of \u03bb while the generalization gap is almost unchanged. The MIMGAN is a good example to show why Bhattacharyya coefficient \u03c1 should be used (instead of the generalization gap gDGAN) as the measure of overfitting in the discriminator. MIMGAN has also the best performance in terms of reducing memorization. Thus, we expect that for MIAs on the generator, MIMGAN would have the best performance."
        },
        {
            "heading": "C. Precision-recall comparison with non-private GANs",
            "text": "So far, our comparison has focused on assessing MEGAN and MIMGAN against the vanilla GAN in terms of their effectiveness in mitigating overfitting and their performance in countering MIAs. In terms of the quality (precision) and diversity (recall) of the generated data samples, although Figs.5 & 6 provide visual examples of the generated data points, these concepts can also be quantified through alternative metrics, such as GAN-test and GAN-train measures [43]. GAN-test represents the accuracy of a classifier trained on real data samples and evaluated on generated data points, akin to precision, where high accuracy signifies high-quality generated data samples. Conversely, GAN-train measures the accuracy of a classifier trained on generated samples and evaluated using real data points, similar to recall, where higher values indicate greater diversity among the generated samples. In this study, we employed the classifier outlined in Appendix 4 to compute GAN-test and GAN-train values for both MEGAN and MIMGAN, comparing them with the performance of the standard GAN. The results of this evaluation are illustrated in Fig.7, alongside MIA accuracy. Examining this figure reveals that while both MEGAN and MIMGAN exhibit a slight reduction in precision and recall compared to the standard GAN, their substantial improvement in reducing MIA accuracy is evident. Furthermore, the figure highlights that in scenarios approaching extreme privacy (where MIA accuracy approximates randomness), MEGAN marginally outperforms MIMGAN in terms of precision and recall. However, MIMGAN offers flexibility by allowing control over the parameter \u03bb, enabling users to reach their desired privacy-utility trade-off."
        },
        {
            "heading": "D. Comparison with other private GANs",
            "text": "In this section, our objective is to conduct a comparative analysis of our proposed models in comparison to other privacy-preserving GANs, specifically PrivGAN [20] and DPGAN [23], across several key metrics. These metrics encompass MIA accuracy, the number of training parameters utilized\n10\nTABLE II COMPARISON OF THE MEGAN AND MIMGAN MODELS WITH GAN IN TERMS OF OVERFITTING, MEMORIZATION, AND MIA ACCURACY.\nDataset Model gDGAN \u03c1 MIA* m G GAN TVD Att.\nMNIST\nGAN 0.03 0.86 31.91% 0.96 0.35\nMEGAN 0.00 0.99 11.68% 0.93 0.06\nMIMGAN \u03bb = 10 0.03 0.95 21.94% 0.94 0.25\n\u03bb = 20 0.04 0.97 16.73% 0.89 0.18\n\u03bb = 100 0.04 0.99 11.30% 0.81 0.09\nfashion-MNIST\nGAN 0.13 0.83 33.41% 0.93 0.39\nMEGAN 0.00 \u223c1.00 11.01% 0.86 0.02\nMIMGAN \u03bb = 10 0.020 0.94 21.71% 0.90 0.24\n\u03bb = 20 0.03 0.97 15.96% 0.87 0.16\n\u03bb = 100 0.03 0.99 11.47% 0.82 0.08 *Random attacker has MIA accuracy of 10%.\nFig. 7. Evaluation of the GAN, MEGAN, and MIMGAN based on the (a) GAN-test and (b) GAN-train approaches versus the MIA accuracy.\nwithin each framework, and GAN-test accuracy, as introduced in Section IV-C. It is important to emphasize that we employ a classifier with an identical structure, as detailed in Appendix 4, for all models. Additionally, the GAN structure remains consistent for each framework when applied to both datasets. Regarding DP-GAN, the same architecture as the vanilla GAN and MEGAN is used. Moreover, DP-GAN is implemented\n11\nwith differential privacy1 with a parameter of \u03b4 = 1e\u22124. Consequently, for each model, the initial step involves training the classifier using generated data samples, followed by its evaluation on a pre-determined test dataset. The results of this comprehensive comparison are presented in Table III. The table unequivocally illustrates that while DP-GAN and PrivGAN exhibit the capacity to reduce MIA accuracy to a level approaching that of a random attacker, their performance in downstream utility is significantly compromised in comparison to MEGAN and MIMGAN, which are introduced in our work. More precisely, when we take into account both robustness against membership inference attacks and utility, as measured by GAN-test accuracy, our proposed models, MEGAN and MIMGAN, outperform Priv-GAN and DP-GAN. Notably, our MEGAN achieves this superiority while maintaining the same number of learning parameters as the standard GAN. In contrast, Priv-GAN has almost twice the number of parameters, making it significantly more computationally expensive. It is worth noting that this comparison is specifically conducted for the MNIST and fashion-MNIST datasets, primarily due to computational constraints."
        },
        {
            "heading": "V. SUMMARY AND CONCLUDING REMARKS",
            "text": "We have considered the problem of MIAs in GANs. First, we have revised the notion of overfitting in GANs and showed the limitations of the generalization gap. In particular, the Bhattacharyya coefficient between the distribution of the discriminator scores for training and non-training data points was introduced as a more complete measure of overfitting. The advantage of this coefficient is that it considers the shape of the distributions instead of only mean values. This was clarified with a Gaussian example in detail. Second, we proposed a new optimization framework for the GAN to mitigate the risks of membership inferences by maximizing the entropy of the discriminator scores for fake samples during training. This approach termed MEGAN was found to be quite effective in reducing the effectiveness of MIAs. Third, we consider another approach to try to mitigate the risks of MIAs by considering the leakage of information in fake samples. In this case, the GAN framework also was modified to include\n1https://github.com/tensorflow/privacy\nan additional network and a regularization term in the loss function to control the amount of information being leaked. The advantage of this scheme is that it provides a direct control, through the weight of the regularization term, over the amount of information leakage that is allowed. Thus, it is more flexible than the former approach. In all cases, there are trade-offs between the diversity and fidelity (quality) of the generated samples and the robustness against MIAs, as shown in Fig. 7. This is a topic worth of further research that may be considered in the future."
        },
        {
            "heading": "ACKNOWLEDGMENT",
            "text": "This work was supported by Hydro-Quebec, the Natural Sciences and Engineering Research Council of Canada, and McGill University in the framework of the NSERC/HydroQuebec Industrial Research Chair in Interactive Information Infrastructure for the Power Grid (IRCPJ406021-14)."
        },
        {
            "heading": "DISCRIMINATOR OVERFITTING ON THE PERFORMANCE OF",
            "text": ""
        },
        {
            "heading": "MIA",
            "text": "The model architectures of the generators and discriminators used to produce Fig. 3 are presented along with the optimizer. It should be noted that the same generator was used for all three examples and an Adam optimizer with a learning rate of 0.0002 and \u03b2 = 0.5 was used for optimization."
        },
        {
            "heading": "Generator",
            "text": "- Dense (units = 7\u00d77\u00d7512, input size = 100) - LeakyReLU (\u03b1 = 0.2) - Reshape ( target shape = (7,7,512)) - Conv2DTranspose (filters = 128, kernel size = (5,5), strides = (2,2), padding = \u2019same\u2019) - LeakyReLU (\u03b1 = 0.2) - Conv2DTranspose(filters = 128, kernel size = (5,5), strides = (2,2), padding = \u2019same\u2019) - LeakyReLU (\u03b1 = 0.2) - Conv2D (filters = 1, kernel size =(5,5), activation = \u2019sigmoid\u2019,padding = \u2019same\u2019) discriminator (a) - Conv2D (filters = 32, kernel size = (5,5), strides = (2,2), padding = \u2019same\u2019) - LeakyReLU (\u03b1 = 0.2) - Conv2D (filters = 64, kernel size = (5,5), strides = (2,2), padding = \u2019same\u2019) - LeakyReLU (\u03b1 = 0.2) - Conv2D(filters = 128, kernel size = (5,5), strides = (2,2), padding = \u2019same\u2019) - LeakyReLU (\u03b1 = 0.2) - Flatten() - Dense(units = 1, activation = \u2019sigmoid\u2019) discriminator (b) - Conv2D (filters = 64, kernel size = (5,5), strides = (2,2), padding = \u2019same\u2019) - LeakyReLU (\u03b1 = 0.2) - Conv2D(filters = 128, kernel size = (5,5), strides = (2,2), padding = \u2019same\u2019) - LeakyReLU (\u03b1 = 0.2) - Flatten() - Dense (units = 1, activation = \u2019sigmoid\u2019) discriminator (c)\n13\n- Conv2D (filters = 64, kernel size = (5,5), strides = (2,2), padding = \u2019same\u2019) - LeakyReLU (\u03b1 = 0.2) - Conv2D(filters = 64, kernel size = (5,5), strides = (2,2),\npadding = \u2019same\u2019) - LeakyReLU (\u03b1 = 0.2) - Flatten() - Dense (units = 1, activation = \u2019sigmoid\u2019)\nAPPENDIX 2: MODEL ARCHITECTURES FOR THE MEGAN The model architectures of the generators and discriminators used for MEGAN are presented along with the optimizer. The same architecture is used for both MNIST and fashionMNIST datasets. The Adam optimizer with a learning rate of 0.0002 and \u03b2 = 0.5 was used for optimization in Algorithm 1."
        },
        {
            "heading": "Generator",
            "text": "- Dense (units = 7\u00d77\u00d7512, input size = 100) - LeakyReLU (\u03b1 = 0.2) - Reshape ( target shape = (7,7,512)) - Conv2DTranspose (filters = 128, kernel size = (5,5),\nstrides = (2,2), padding = \u2019same\u2019) - LeakyReLU (\u03b1 = 0.2) - Conv2DTranspose(filters = 128, kernel size = (5,5),\nstrides = (2,2), padding = \u2019same\u2019) - LeakyReLU (\u03b1 = 0.2) - Conv2D (filters = 1, kernel size =(5,5), activation =\n\u2019sigmoid\u2019,padding = \u2019same\u2019) discriminator\n- Conv2D (filters = 32, kernel size = (5,5), strides = (2,2), padding = \u2019same\u2019) - BatchNormalization() - LeakyReLU (\u03b1 = 0.2) - Conv2D(filters = 32, kernel size = (5,5), strides = (2,2),\npadding = \u2019same\u2019) - BatchNormalization() - LeakyReLU (\u03b1 = 0.2) - Flatten() - Dense (units = 1, activation = \u2019sigmoid\u2019)\nAPPENDIX 3: MODEL ARCHITECTURES FOR THE MIMGAN The model architectures of the generator, discriminator, and Adversary used in MIMGAN to produce the results for MNIST and fashion-MNIST datasets are outlined here. For all cases, the same generator and Adversary architectures were used. In addition, an Adam optimizer with a learning rate of 0.0002 and \u03b2 = 0.5 was used for optimization."
        },
        {
            "heading": "Generator",
            "text": "- Dense (units = 7\u00d77\u00d7512, input size = 100) - LeakyReLU (\u03b1 = 0.2) - Reshape ( target shape = (7,7,512)) - Conv2DTranspose (filters = 128, kernel size = (5,5),\nstrides = (2,2), padding = \u2019same\u2019) - LeakyReLU (\u03b1 = 0.2) - Conv2DTranspose (filters = 128, kernel size = (5,5),\nstrides = (2,2), padding = \u2019same\u2019)\n- LeakyReLU (\u03b1 = 0.2) - Conv2D (filters = 1, kernel size = (5,5), activation =\n\u2019sigmoid\u2019, padding = \u2019same\u2019)"
        },
        {
            "heading": "Adversary",
            "text": "- Conv2D (filters = 64, kernel size = (3,3), strides = (2,2), padding = \u2019same\u2019) - LeakyReLU (\u03b1 = 0.2) - Conv2D(filters = 64, kernel size = (3,3), strides = (2,2),\npadding = \u2019same\u2019) - LeakyReLU (\u03b1 = 0.2) - Flatten() - Dense(units = 28\u00d728\u00d72)\ndiscriminator (MNIST) - Conv2D (filters = 64, kernel size = (5,5), strides = (2,2),\npadding = \u2019same\u2019) - LeakyReLU (\u03b1 = 0.2) - Conv2D (filters = 64, kernel size = (5,5), strides = (2,2),\npadding = \u2019same\u2019) - LeakyReLU (\u03b1 = 0.2) - Flatten() - Dense (units = 1, activation = \u2019sigmoid\u2019)\ndiscriminator (fashion-MNIST) - Conv2D (filters = 64, kernel size = (5,5), strides = (2,2),\npadding = \u2019same\u2019) - LeakyReLU (\u03b1 = 0.2) - Conv2D(filters = 128, kernel size =(5,5), strides = (2,2),\npadding = \u2019same\u2019) - LeakyReLU (\u03b1 = 0.2) - Flatten() - Dense (units = 1, activation = \u2019sigmoid\u2019)\nAPPENDIX 4: CLASSIFIER ARCHITECTURE FOR GAN-TEST AND GAN-TRAIN MEASURES\n- Conv2D (filters = 32, kernel size =(3,3), activation=\u2019relu\u2019) - MaxPooling2D(pool size = (2, 2)) - Conv2D (filters = 64, kernel size =(3,3), activation=\u2019relu\u2019) - Conv2D (filters = 64, kernel size =(3,3), activation=\u2019relu\u2019) - MaxPooling2D (pool size = (2, 2)) - Flatten() - Dense (units = 100, activation = \u2019relu\u2019) - Dense (units = 10, activation = \u2019softmax\u2019)\nSGD optimizer with a learning rate of 0.01 and momentum of 0.9 was used for optimization.\nAPPENDIX 5: MORE EXPERIMENTAL RESULTS\nIn this section, the experimental results of the proposed defense models applied to more datasets are proposed.\nRegarding the Chest X-Ray Images (Pneumonia) dataset, the model architectures of the networks used in GAN, MIMGAN, and MEGAN are outlined here. For all cases, the kernels of each layer is initialized by a zero-mean random normal with \u03c3 = 0.02. Moreover, the Adam optimizer with a learning rate of 0.0002 and \u03b2 = 0.5 was used."
        },
        {
            "heading": "Generator",
            "text": "- Dense (units = 8\u00d78\u00d7128, input size = 100)\n14"
        },
        {
            "heading": "Adversary (MIMGAN)",
            "text": "- Conv2D (filters = 32, kernel size = (5,5), padding = \u2019same\u2019) - LeakyReLU (\u03b1 = 0.2) - Conv2D(filters = 32, kernel size = (5,5), strides = (2,2),\npadding = \u2019same\u2019) - LeakyReLU (\u03b1 = 0.2) - Conv2D(filters = 32, kernel size = (5,5), strides = (2,2),\npadding = \u2019same\u2019) - LeakyReLU (\u03b1 = 0.2) - Conv2D(filters = 32, kernel size = (5,5), strides = (2,2),\npadding = \u2019same\u2019) - LeakyReLU (\u03b1 = 0.2) - Conv2D(filters = 32, kernel size = (5,5), strides = (2,2),\npadding = \u2019same\u2019) - LeakyReLU (\u03b1 = 0.2) - Flatten() - Dense(units = 128\u00d7128\u00d73\u00d72)\ndiscriminator (MIMGAN, GAN)\n- Conv2D (filters = 64, kernel size = (5,5), padding = \u2019same\u2019) - LeakyReLU (\u03b1 = 0.2) - Conv2D (filters = 64, kernel size = (5,5), strides = (2,2),\npadding = \u2019same\u2019) - LeakyReLU (\u03b1 = 0.2) - Conv2D (filters = 64, kernel size = (5,5), strides = (2,2),\npadding = \u2019same\u2019) - LeakyReLU (\u03b1 = 0.2) - Conv2D (filters = 32, kernel size = (5,5), strides = (2,2),\npadding = \u2019same\u2019) - LeakyReLU (\u03b1 = 0.2) - Conv2D (filters = 32, kernel size = (5,5), strides = (2,2),\npadding = \u2019same\u2019) - LeakyReLU (\u03b1 = 0.2) - Conv2D (filters = 32, kernel size = (5,5), strides = (2,2),\npadding = \u2019same\u2019) - LeakyReLU (\u03b1 = 0.2) - Flatten() - Dropout(0.5) - Dense (units = 1, activation = \u2019sigmoid\u2019)\ndiscriminator (MEGAN)\n- Conv2D (filters = 32, kernel size = (5,5), padding = \u2019same\u2019) - LeakyReLU (\u03b1 = 0.2) - Conv2D (filters = 32, kernel size = (5,5), strides = (2,2),\npadding = \u2019same\u2019) - LeakyReLU (\u03b1 = 0.2) - Conv2D (filters = 32, kernel size = (5,5), strides = (2,2),\npadding = \u2019same\u2019) - LeakyReLU (\u03b1 = 0.2) - Conv2D (filters = 32, kernel size = (5,5), strides = (2,2),\npadding = \u2019same\u2019) - LeakyReLU (\u03b1 = 0.2) - Conv2D (filters = 32, kernel size = (5,5), strides = (2,2),\npadding = \u2019same\u2019) - LeakyReLU (\u03b1 = 0.2)\n15\n- Conv2D (filters = 32, kernel size = (5,5), strides = (2,2), padding = \u2019same\u2019) - LeakyReLU (\u03b1 = 0.2) - Flatten() - Dropout(0.5) - Dense (units = 1, activation = \u2019sigmoid\u2019)\nRegarding the Anime Faces dataset, the model architectures of the networks used in GAN, MIMGAN, and MEGAN are outlined here where in all the cases the Adam optimizer with a learning rate of 0.0002 and \u03b2 = 0.5 was used."
        },
        {
            "heading": "Generator",
            "text": "- Dense (units = 4\u00d74\u00d7512, input size = 100) - LeakyReLU (\u03b1 = 0.2) - Reshape ( target shape = (4,4,512)) - Conv2DTranspose (filters = 512, kernel size = (4,4),\nstrides = (2,2), padding = \u2019same\u2019) - BatchNormalization(momentum=0.9, epsilon=0.0001) - LeakyReLU (\u03b1 = 0.2)\n- Conv2DTranspose (filters = 256, kernel size = (4,4), strides = (2,2), padding = \u2019same\u2019) - BatchNormalization(momentum=0.9, epsilon=0.0001) - LeakyReLU (\u03b1 = 0.2) - Conv2DTranspose (filters = 128, kernel size = (4,4),\nstrides = (2,2), padding = \u2019same\u2019) - BatchNormalization(momentum=0.9, epsilon=0.0001) - LeakyReLU (\u03b1 = 0.2) - Conv2DTranspose (filters = 64, kernel size = (4,4), strides\n= (2,2), padding = \u2019same\u2019) - BatchNormalization(momentum=0.9, epsilon=0.0001) - LeakyReLU (\u03b1 = 0.2) - Conv2D (filters = 3, kernel size = (5,5), activation =\n\u2019tanh\u2019, padding = \u2019same\u2019)"
        },
        {
            "heading": "Adversary (MIMGAN)",
            "text": "- Conv2D (filters = 32, kernel size = (5,5), padding = \u2019same\u2019) - LeakyReLU (\u03b1 = 0.2) - Conv2D(filters = 32, kernel size = (5,5), strides = (2,2),\npadding = \u2019same\u2019) - LeakyReLU (\u03b1 = 0.2) - Conv2D(filters = 32, kernel size = (5,5), strides = (2,2),\npadding = \u2019same\u2019) - LeakyReLU (\u03b1 = 0.2) - Conv2D(filters = 32, kernel size = (5,5), strides = (2,2),\npadding = \u2019same\u2019) - LeakyReLU (\u03b1 = 0.2) - Conv2D(filters = 32, kernel size = (5,5), strides = (2,2),\npadding = \u2019same\u2019) - LeakyReLU (\u03b1 = 0.2) - Flatten() - Dense(units = 64\u00d764\u00d73\u00d72)\ndiscriminator (MIMGAN, GAN) - Conv2D (filters = 64, kernel size = (3,3), padding =\n\u2019same\u2019) - LeakyReLU (\u03b1 = 0.2) - BatchNormalization() - Conv2D (filters = 64, kernel size = (3,3), padding =\n\u2019same\u2019) - LeakyReLU (\u03b1 = 0.2) - BatchNormalization() - MaxPooling2D(pool size=(3,3)) - Dropout(0.2) - Conv2D (filters = 64, kernel size = (3,3), padding =\n\u2019same\u2019) - LeakyReLU (\u03b1 = 0.2) - BatchNormalization() - Conv2D (filters = 64, kernel size = (3,3), padding =\n\u2019same\u2019) - LeakyReLU (\u03b1 = 0.2) - BatchNormalization() - MaxPooling2D(pool size=(3,3)) - Dropout(0.3) - Flatten() - Dense(64) - LeakyReLU (\u03b1 = 0.2) - Dense(64)\n16\n- LeakyReLU (\u03b1 = 0.2) - Dense (units = 1, activation = \u2019sigmoid\u2019)\ndiscriminator (MEGAN) - Conv2D (filters = 64, kernel size = (3,3), padding =\n\u2019same\u2019) - LeakyReLU (\u03b1 = 0.2) - BatchNormalization() - Conv2D (filters = 64, kernel size = (3,3), padding =\n\u2019same\u2019) - LeakyReLU (\u03b1 = 0.2) - BatchNormalization() - MaxPooling2D(pool size=(3,3)) - Dropout(0.2) - Conv2D (filters = 64, kernel size = (3,3), padding =\n\u2019same\u2019) - LeakyReLU (\u03b1 = 0.2) - BatchNormalization() - Conv2D (filters = 64, kernel size = (3,3), padding =\n\u2019same\u2019) - LeakyReLU (\u03b1 = 0.2) - BatchNormalization() - MaxPooling2D(pool size=(3,3)) - Dropout(0.3) - Flatten() - Dense(32) - LeakyReLU (\u03b1 = 0.2) - Dense(32) - LeakyReLU (\u03b1 = 0.2) - Dense (units = 1, activation = \u2019sigmoid\u2019)"
        }
    ],
    "title": "Preserving Privacy in GANs Against Membership Inference Attack",
    "year": 2023
}