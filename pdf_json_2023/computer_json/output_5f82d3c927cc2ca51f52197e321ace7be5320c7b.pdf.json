{
    "abstractText": "We present XPhoneBERT, the first multilingual model pretrained to learn phoneme representations for the downstream text-to-speech (TTS) task. Our XPhoneBERT has the same model architecture as BERT-base, trained using the RoBERTa pre-training approach on 330M phoneme-level sentences from nearly 100 languages and locales. Experimental results show that employing XPhoneBERT as an input phoneme encoder significantly boosts the performance of a strong neural TTS model in terms of naturalness and prosody and also helps produce fairly high-quality speech with limited training data. We publicly release our pre-trained XPhoneBERT with the hope that it would facilitate future research and downstream TTS applications for multiple languages.",
    "authors": [
        {
            "affiliations": [],
            "name": "Linh The Nguyen"
        },
        {
            "affiliations": [],
            "name": "Thinh Pham"
        },
        {
            "affiliations": [],
            "name": "Dat Quoc Nguyen"
        }
    ],
    "id": "SP:e05f88f1a3e1f91f8e23fcaef119d7572d591397",
    "references": [
        {
            "authors": [
                "J. Kong",
                "J. Kim",
                "J. Bae"
            ],
            "title": "HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis",
            "venue": "Proceedings of NeurIPS, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Ren",
                "Y. Ruan",
                "X. Tan",
                "T. Qin",
                "S. Zhao",
                "Z. Zhao",
                "T.-Y. Liu"
            ],
            "title": "FastSpeech: Fast, Robust and Controllable Text to Speech",
            "venue": "Proceedings of NeurIPS, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "V. Popov",
                "I. Vovk",
                "V. Gogoryan",
                "T. Sadekova",
                "M. Kudinov"
            ],
            "title": "Grad-TTS: A Diffusion Probabilistic Model for Text-to-Speech",
            "venue": "Proceedings of ICML, 2021, pp. 8599\u20138608.",
            "year": 2021
        },
        {
            "authors": [
                "X. Tan",
                "J. Chen"
            ],
            "title": "NaturalSpeech: End-to-End Text to Speech Synthesis with Human-Level Quality",
            "venue": "arXiv preprint, vol. arXiv:2205.04421, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "J. Shen",
                "R. Pang"
            ],
            "title": "Natural TTS Synthesis by Conditioning Wavenet on MEL Spectrogram Predictions",
            "venue": "Proceedings of ICASSP, 2018, pp. 4779\u20134783.",
            "year": 2018
        },
        {
            "authors": [
                "N. Li",
                "S. Liu",
                "Y. Liu",
                "S. Zhao",
                "M. Liu"
            ],
            "title": "Neural Speech Synthesis with Transformer Network",
            "venue": "Proceedings of AAAI, 2019, pp. 6706\u20136713.",
            "year": 2019
        },
        {
            "authors": [
                "J. Kim",
                "S. Kim",
                "J. Kong",
                "S. Yoon"
            ],
            "title": "Glow-TTS: A Generative Flow for Text-to-Speech via Monotonic Alignment Search",
            "venue": "Proceedings of NeurIPS, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Ren",
                "C. Hu",
                "X. Tan",
                "T. Qin",
                "S. Zhao",
                "Z. Zhao",
                "T.-Y. Liu"
            ],
            "title": "FastSpeech 2: Fast and High-Quality End-to-End Text to Speech",
            "venue": "Proceedings of ICLR, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "J. Kim",
                "J. Kong",
                "J. Son"
            ],
            "title": "Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech",
            "venue": "Proceedings of ICML, 2021, pp. 5530\u20135540.",
            "year": 2021
        },
        {
            "authors": [
                "J. Devlin",
                "M.-W. Chang",
                "K. Lee",
                "K. Toutanova"
            ],
            "title": "BERT: Pretraining of Deep Bidirectional Transformers for Language Understanding",
            "venue": "Proceedings of NAACL, 2019, pp. 4171\u20134186.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Liu",
                "M. Ott",
                "N. Goyal",
                "J. Du",
                "M. Joshi",
                "D. Chen",
                "O. Levy",
                "M. Lewis",
                "L. Zettlemoyer",
                "V. Stoyanov"
            ],
            "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
            "venue": "arXiv preprint, vol. arXiv:1907.11692, 2019.",
            "year": 1907
        },
        {
            "authors": [
                "Z. Lan",
                "M. Chen",
                "S. Goodman",
                "K. Gimpel",
                "P. Sharma",
                "R. Soricut"
            ],
            "title": "ALBERT: A lite BERT for self-supervised learning of language representations",
            "venue": "Proceedings of ICLR, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "T. Hayashi",
                "S. Watanabe",
                "T. Toda",
                "K. Takeda",
                "S. Toshniwal",
                "K. Livescu"
            ],
            "title": "Pre-Trained Text Embeddings for Enhanced Text-to- Speech Synthesis",
            "venue": "Proceedings of INTERSPEECH, 2019, pp. 4430\u20134434.",
            "year": 2019
        },
        {
            "authors": [
                "T. Kenter",
                "M. Sharma",
                "R. Clark"
            ],
            "title": "Improving the Prosody of RNN-based English Text-To-Speech Synthesis by Incorporating a BERT model",
            "venue": "Proceedings of INTERSPEECH, 2020, pp. 4412\u20134416.",
            "year": 2020
        },
        {
            "authors": [
                "G. Xu",
                "W. Song",
                "Z. Zhang",
                "C. Zhang",
                "X. He",
                "B. Zhou"
            ],
            "title": "Improving Prosody Modelling with Cross-Utterance BERT Embeddings for End-to-end Speech Synthesis",
            "venue": "Proceedings of ICASSP, 2021, pp. 6079\u20136083.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Jia",
                "H. Zen",
                "J. Shen",
                "Y. Zhang",
                "Y. Wu"
            ],
            "title": "PnG BERT: Augmented BERT on Phonemes and Graphemes for Neural TTS",
            "venue": "Proceedings of INTERSPEECH, 2021, pp. 151\u2013155.",
            "year": 2021
        },
        {
            "authors": [
                "G. Zhang",
                "K. Song",
                "X. Tan",
                "D. Tan",
                "Y. Yan",
                "Y. Liu",
                "G. Wang",
                "W. Zhou",
                "T. Qin",
                "T. Lee",
                "S. Zhao"
            ],
            "title": "Mixed-Phoneme BERT: Improving BERT with Mixed Phoneme and Sup-Phoneme Representations for Text to Speech",
            "venue": "Proceedings of INTERSPEECH, 2022, pp. 456\u2013460.",
            "year": 2022
        },
        {
            "authors": [
                "Y.A. Li",
                "C. Han",
                "X. Jiang",
                "N. Mesgarani"
            ],
            "title": "Phoneme-Level BERT for Enhanced Prosody of Text-to-Speech with Grapheme Predictions",
            "venue": "arXiv preprint, vol. arXiv:2301.08810, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "S. Ruder"
            ],
            "title": "Why You Should Do NLP Beyond English",
            "venue": "https://ruder.io/nlp-beyond-english/, 2020. [Online]. Available: https://ruder.io/nlp-beyond-english/",
            "year": 2020
        },
        {
            "authors": [
                "A. Vaswani",
                "N. Shazeer",
                "N. Parmar",
                "J. Uszkoreit",
                "L. Jones",
                "A.N. Gomez",
                "\u0141. Kaiser",
                "I. Polosukhin"
            ],
            "title": "Attention is All you Need",
            "venue": "Proceedings of NIPS, 2017, pp. 5998\u20136008.",
            "year": 2017
        },
        {
            "authors": [
                "J. Zhu",
                "C. Zhang",
                "D. Jurgens"
            ],
            "title": "ByT5 model for massively multilingual grapheme-to-phoneme conversion",
            "venue": "Proceedings of INTERSPEECH, 2022, pp. 446\u2013450.",
            "year": 2022
        },
        {
            "authors": [
                "M. Guo",
                "Z. Dai",
                "D. Vrande\u010di\u0107",
                "R. Al-Rfou"
            ],
            "title": "Wiki-40B: Multilingual Language Model Dataset",
            "venue": "Proceedings of LREC, 2020, pp. 2440\u20132452.",
            "year": 2020
        },
        {
            "authors": [
                "Q. Lhoest",
                "A. Villanova del Moral",
                "Y. Jernite"
            ],
            "title": "Datasets: A Community Library for Natural Language Processing",
            "venue": "Proceedings of EMNLP: System Demonstrations, 2021, pp. 175\u2013184.",
            "year": 2021
        },
        {
            "authors": [
                "D.Q. Nguyen",
                "D.Q. Nguyen",
                "T. Vu",
                "M. Dras",
                "M. Johnson"
            ],
            "title": "A Fast and Accurate Vietnamese Word Segmenter",
            "venue": "Proceedings of LREC, 2018, pp. 2582\u20132587.",
            "year": 2018
        },
        {
            "authors": [
                "T. Vu",
                "D.Q. Nguyen",
                "D.Q. Nguyen",
                "M. Dras",
                "M. Johnson"
            ],
            "title": "VnCoreNLP: A Vietnamese Natural Language Processing Toolkit",
            "venue": "Proceedings of NAACL: Demonstrations, 2018, pp. 56\u201360.",
            "year": 2018
        },
        {
            "authors": [
                "O. Kuchaiev",
                "J. Li"
            ],
            "title": "NeMo: a toolkit for building AI applications using Neural Modules",
            "venue": "Proceedings of NeurIPS Workshop on Systems for ML, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "M. Bernard",
                "H. Titeux"
            ],
            "title": "Phonemizer: Text to Phones Transcription for Multiple Languages in Python",
            "venue": "Journal of Open Source Software, vol. 6, no. 68, p. 3958, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "M. Ott",
                "S. Edunov",
                "A. Baevski",
                "A. Fan",
                "S. Gross",
                "N. Ng",
                "D. Grangier",
                "M. Auli"
            ],
            "title": "fairseq: A Fast, Extensible Toolkit for Sequence Modeling",
            "venue": "Proceedings of NAACL-HLT 2019: Demonstrations, 2019, pp. 48\u201353.",
            "year": 2019
        },
        {
            "authors": [
                "D.P. Kingma",
                "J. Ba"
            ],
            "title": "Adam: A Method for Stochastic Optimization",
            "venue": "Proceedings of ICLR, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "T. Wolf",
                "L. Debut"
            ],
            "title": "Transformers: State-of-the-Art Natural Language Processing",
            "venue": "Proceedings of EMNLP 2020: System Demonstrations, 2020, pp. 38\u201345.",
            "year": 2020
        },
        {
            "authors": [
                "K. Ito",
                "L. Johnson"
            ],
            "title": "The LJ Speech Dataset",
            "venue": "https://keithito. com/LJ-Speech-Dataset/, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "D.Q. Nguyen",
                "A.T. Nguyen"
            ],
            "title": "PhoBERT: Pre-trained language models for Vietnamese",
            "venue": "Findings of EMNLP, 2020, pp. 1037\u2013 1042.",
            "year": 2020
        },
        {
            "authors": [
                "I. Loshchilov",
                "F. Hutter"
            ],
            "title": "Decoupled Weight Decay Regularization",
            "venue": "Proceedings of ICLR, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "A. Tamamori",
                "T. Hayashi",
                "K. Kobayashi",
                "K. Takeda",
                "T. Toda"
            ],
            "title": "Speaker-dependent WaveNet vocoder",
            "venue": "Proceedings of IN- TERSPEECH, 2017, pp. 1118\u20131122.",
            "year": 2017
        },
        {
            "authors": [
                "T. Saeki",
                "K. Tachibana",
                "R. Yamamoto"
            ],
            "title": "DRSpeech: Degradation-Robust Text-to-Speech Synthesis with Frame-Level and Utterance-Level Acoustic Representation Learning",
            "venue": "Proceedings of INTERSPEECH, 2022, pp. 793\u2013797.",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "Index Terms: XPhoneBERT, Multilingual model, Pre-trained model, Phoneme representation, Text-to-speech, Neural TTS, Speech synthesis."
        },
        {
            "heading": "1. Introduction",
            "text": "Advancements in neural TTS technology have led to significant improvements in producing natural-sounding speech [1, 2, 3, 4], increasingly closing the gap between artificial speech and human-recorded speech in terms of naturalness. Early work such as [5] employs an encoder to directly convert input raw texts to mel-spectrograms that are then fed into a decoder to generate output speech. Other works often take phoneme sequences as input for their encoder [6, 7, 8, 9]. Here, the encoder in these works might be extended by utilizing recent large-scale pre-trained language models that are learned from unlabeled textual or phonemic description data to enhance the naturalness of speech outputs.\nThe large-scale pre-trained language models, e.g. BERT [10], RoBERTa [11] and ALBERT [12], have proved their effectiveness, improving state-of-the-art performances of various natural language processing research and application tasks. For TTS, some works incorporate contextualized word embeddings generated by the pre-trained BERT [10] into their standard encoder [13, 14, 15]. In general, an input phoneme sequence is fed into the standard TTS encoder to produce phoneme representations, while its corresponding raw text is fed into BERT to obtain contextualized word embeddings. To construct the input vectors of the TTS decoder, the produced representations of the input phonemes are concatenated with the BERT-based contextualized embedding of the corresponding word that the phonemes belong to. As a result, BERT helps increase the quality of the output synthesized speech. Here, the pre-trained BERT is used to provide additional contextual information for phoneme representations indirectly. Therefore, it might be better if the contextualized phoneme representations are directly\nproduced by a pre-trained BERT-type model that is learned from unlabeled phoneme-level data.\nRecent works confirm that pre-trained models for phoneme representations, including PnG BERT [16], Mixed-Phoneme BERT [17] and Phoneme-level BERT [18], help improve advanced TTS systems. PnG BERT and Mixed-Phoneme BERT are trained based on the BERT pre-training approach [10], in which PnG BERT takes both phonemes and graphemes (i.e. subword tokens) as the input, while Mixed-Phoneme BERT takes both phonemes and sup-phoneme tokens as the input. Phoneme-level BERT is trained based on the ALBERT pretraining approach [12], only taking phonemes as the input. In addition to the standard masked token prediction task as used in PnG BERT and Mixed-Phoneme BERT, the Phoneme-level BERT also proposes an additional auxiliary task that predicts the corresponding grapheme for each phoneme. Here, PnG BERT, Mixed-Phoneme BERT and Phoneme-level BERT can be directly used as an input encoder in a typical neural TTS system. Note that the success of these pre-trained language models has been limited to the English language only. Taking into account a societal, linguistic, cultural, machine learning and cognitive perspective [19], it is worth exploring pre-trained models for phoneme representations in languages other than English.\nTo fill the gap, we train the first large-scale multilingual language model for phoneme representations, using a pre-training corpus of 330M phonemic description sentences from nearly 100 languages and locales. Our model is trained based on the RoBERTa pre-training approach [11], using the BERT-base model configuration [10]. We conduct experiments on the downstream TTS task, directly employing our model as an input phoneme encoder of the strong model VITS [9]. Experimental results show that our model helps boost the performance of VITS, obtaining more natural prosody than the original VITS without pre-training and also producing fairly high-quality synthesized speech with limited training data. We summarize our contribution as follows:\n\u2022 We present the first large-scale pre-trained multilingual model for phoneme representations, which we name XPhoneBERT.\n\u2022 On the downstream TTS task, XPhoneBERT helps significantly improve the performance of the strong baseline VITS, thus confirming its effectiveness.\n\u2022 We publicly release XPhoneBERT at https://github. com/VinAIResearch/XPhoneBERT. We hope that our XPhoneBERT model would help facilitate future research and downstream TTS applications for nearly 100 languages and locales.\nar X\niv :2\n30 5.\n19 70\n9v 1\n[ cs\n.C L\n] 3\n1 M\nay 2\n02 3"
        },
        {
            "heading": "2. Our XPhoneBERT",
            "text": "This section outlines the architecture and describes the multilingual pre-training corpus and optimization setup that we use for XPhoneBERT."
        },
        {
            "heading": "2.1. Model architecture",
            "text": "XPhoneBERT has the same model architecture as BERT-base [10]\u2014a multi-layer bidirectional Transformer encoder [20]\u2014in which the number of Transformer blocks, the hidden size and the number of self-attention heads are 12, 768 and 12, respectively. To pre-train XPhoneBERT, we use the masked language modeling objective [10] and follow the RoBERTa pre-training approach [11] which robustly optimizes BERT for better performance, i.e. using a dynamic masking strategy and without the next sentence prediction objective. Given the popularity of BERT and RoBERTa, we do not further detail about the architecture here. See [10, 11] for more information."
        },
        {
            "heading": "2.2. Multilingual pre-training data",
            "text": "Our multilingual pre-training dataset is constructed following three phases. The first phase is to collect text documents and then perform word and sentence segmentation as well as duplicate removal and text normalization. The second phase is to convert texts into phonemes, employing the CharsiuG2P toolkit [21] that supports 90+ languages and locales. Finally, the third phase is to perform phoneme segmentation."
        },
        {
            "heading": "2.2.1. First phase: Data collection and pre-processing",
            "text": "We collect texts for the languages supported by CharsiuG2P. Here, we employ the multilingual datasets wiki40b [22] and wikipedia [23], available to download from the Hugging Face datasets library [24]. In particular, we first download the wiki40b dataset consisting of text documents for 41 Wikipedia languages and locales.1 We then use wikipedia to extract texts from Wikipedia dumps for remaining languages other than those belonging to wiki40b.2\nWe perform word and sentence segmentation on all text documents in each language by using the spaCy toolkit,3 except for Vietnamese where we employ RDRSegmenter [25] from the VnCoreNLP toolkit [26]. We then lowercase all sentences and filter out duplicate sentences and single-word ones. We also apply text normalization to convert texts from their written form into their verbalized form for only English, German, Spanish, Vietnamese and Chinese (it is because we could not find an effective text normalization tool publicly available for other languages). Here, we use the text normalization component from the NVIDIA NeMo toolkit [27] for English, German, Spanish and Chinese, and the Vinorm text normalization package for Vietnamese.4"
        },
        {
            "heading": "2.2.2. Second phase: Text-to-phoneme conversion",
            "text": "For each language whose locales do not have their own Wikipedia data,5 we randomly divide the language\u2019s Wikipedia\n1https://huggingface.co/datasets/wiki40b 2https://huggingface.co/datasets/wikipedia 3https://spacy.io 4https://github.com/v-nhandt21/Vinorm 5Languages whose locales do not have their own Wikipedia data are: English (eng-uk & eng-us), French (fra & fra-qu), Greek (grc & gre), Latin (lat-clas & lat-eccl), Portuguese (por-po & por-bz), SerboCroatian (hbs-latn & hbs-cyrl), Spanish (spa & spa-latin & spa-me),\ndata into equal parts (each with the same number of sentences), with each part corresponding to a locale. For example, we divide 67 million English sentences into two equal parts that are then separately converted into phonemic descriptions in British English (eng-uk) and American English (eng-us).\nTo convert sentences into their phonemic description, we employ the grapheme-to-phoneme conversion toolkit CharsiuG2P [21]. The pre-trained CharsiuG2P is a strong multilingual Transformer-based model that generates the pronunciation of a word given its orthographic form and ISO 639-3 language code pair. Following the recommendation from [21], if the input word is in the CharsiuG2P toolkit\u2019s pronunciation dictionary of the target language/locale, we employ the pronunciation dictionary to generate the word\u2019s phonemic description. Otherwise, if the word is out of the vocabulary, we employ the pre-trained CharsiuG2P model to generate its phonemic description.\nFor example, given an input word \u201cmodel\u201d and the language code eng-us of American English, CharsiuG2P produces an output phoneme sequence of \u201c\"mAd@\u00eb\u201d. Such an American English sentence as \u201ca multilingual model\u201d is thus converted into a phoneme sequence of \u201c\"eI\n\" m@\u00ebti\"\u00ebINw@\u00eb \"mAd@\u00eb\u201d. Note that in\nthis conversion phase, we keep punctuations intact, as do the TTS systems [6, 7, 8, 9].\nThai (tha & tts), Vietnamese (vie-n, vie-c & vie-s) and Welsh (wel-nw & wel-sw). By contrast, Armenian and Chinese have the corresponding Wikipedia data for their locales (Armenian: arm-e & arm-w; Chinese: min, yue, zho-s & zho-t).\nfrom transformers import AutoModel, AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"vinai/xphonebert-base\") model = AutoModel.from_pretrained(\"vinai/xphonebert-base\") input_phonemes = \"\"e I _\n\" m @ \u00eb t i \"\u00eb I N w @ \u00eb _ \"m A d @ \u00eb\"\ninput_ids = tokenizer(input_phonemes, return_tensors=\"pt\") features = model(\u2217\u2217input_ids)\nFigure 1: An example code using XPhoneBERT for feature extraction with the Hugging Face transformers library in Python. Here, the input phonemes represent a phonemic description of the word-level sequence \u201ca multilingual model\u201d."
        },
        {
            "heading": "2.2.3. Third phase: Phoneme segmentation",
            "text": "CharsiuG2P converts each input word into a sequence of consecutive phonemes without a phoneme boundary indicator (e.g. white space). To better map between phonemes and speech [7, 9, 18], we would have to perform phoneme segmentation on the CharsiuG2P\u2019s output. Following [28], we employ the segments toolkit for phoneme segmentation.6 Thus an input word is now converted into a sequence of phonemes separated by white spaces, e.g. \u201cmodel\u201d is converted into \u201c\"m A d @ \u00eb\u201d in eng-us. Since we use the white space to separate phonemes, to distinguish phonemes belonging to different word tokens, we employ a meta symbol _ (U+2581) for marking word boundaries. For example, the American English sentence \u201ca multilingual model\u201d is now converted into the phoneme-segmented sequence \u201c\"e I _\n\" m @ \u00eb t i \"\u00eb I N w @ \u00eb _ \"m A d @ \u00eb\u201d.7"
        },
        {
            "heading": "2.2.4. Pre-training data statistics",
            "text": "Through the 3-phase construction process, we finally obtain a pre-training corpus of 330M phoneme-level sentences across 94 languages and locales. We present the data statistic for each language or locale in Table 1."
        },
        {
            "heading": "2.3. Optimization",
            "text": "We employ a white-space tokenizer, resulting in a vocabulary of 1960 phoneme types. Our XPhoneBERT thus has a total of 87.6M parameters. For training XPhoneBERT on our multilingual pre-training corpus, we employ the RoBERTa implementation [11] from the fairseq library [29]. We set a maximum sequence length of 512. We optimize the model using Adam [30] and use a batch size of 1024 sequence blocks across 8 A100 GPUs (40GB each) and a peak learning rate of 0.0001. We train for 20 epochs in about 18 days (here, the first 2 epochs are used for warming up the learning rate)."
        },
        {
            "heading": "2.4. Usage example",
            "text": "To show the potential use for downstream tasks, we present in Figure 1 a basic usage of our pre-trained model XPhoneBERT for feature extraction with the transformers library [31]. More usage examples of XPhoneBERT can be found at the XPhoneBERT\u2019s GitHub repository.\n6https://pypi.org/project/segments 7For convenience, we also create a Python package named text2phonemesequence, incorporating both CharsiuG2P and segments, to perform a direct conversion from an input word-level sentence (e.g. \u201ca multilingual model\u201d) to an output phoneme-segmented sequence (e.g. \u201c\"e I _\n\" m @ \u00eb t i \"\u00eb I N w @ \u00eb _ \"m A d @ \u00eb\u201d)."
        },
        {
            "heading": "3. Experimental setup",
            "text": "We evaluate the effectiveness of XPhoneBERT on the downstream text-to-speech (TTS) task. Due to a limited resource of human raters, we perform this TTS task for American English (eng-us) and Northern Vietnamese (vie-n).8"
        },
        {
            "heading": "3.1. TTS datasets",
            "text": "For English, we use the benchmark dataset LJSpeech [32] consisting of 13,100 audio clips of a single speaker with a total duration of about 24 hours (here, each clip is also provided with a gold-standard text transcription). Following [9], the dataset is split into training, validation and test sets of 12,500, 100 and 500 clip samples, respectively.\nFor Vietnamese, we randomly sample 12,300 different medium-length sentences from the PhoBERT pre-training news data [33]. We hire a professional speaker to read each sentence in a studio and record the corresponding audio, resulting in a total duration of about 18 hours for 12,300 high-quality audio clips. We split our Vietnamese TTS dataset into training, validation and test sets of 12,000, 100 and 200 clips, respectively."
        },
        {
            "heading": "3.2. TTS modeling and training",
            "text": "We employ the strong TTS model VITS [9].9 VITS is an end-to-end model that contains a Transformer encoder [20] to encode the input phoneme sequence. We extend VITS with XPhoneBERT by replacing the VITS\u2019s Transformer encoder with XPhoneBERT.\nFor the first setting of using the whole TTS training set, we train the original VITS model with optimal hyper-parameters used in its paper [9], e.g. using the AdamW optimizer [34] with \u03b21 = 0.8, \u03b22 = 0.99 and the weight decay \u03bb = 0.01, and an initial learning rate of 2 \u00d7 10\u22124 (here, the learning rate decay is scheduled by a 0.9991/8 factor in every epoch). We run for 300K training steps with a batch size of 64 (i.e. equivalent to about 1600 training epochs for both English and Vietnamese). For training the VITS variant extended with XPhoneBERT, we apply the same training protocol used for the original VITS. Here, XPhoneBERT is frozen in the first 25% of the training steps and then updated during the remaining training steps.\nWe also experiment with another setting where the TTS training data is limited. In particular, for each language, we\n8The model weights of PnG BERT (https://google. github.io/tacotron/publications/png_bert), Mixed-Phoneme BERT (https://speechresearch. github.io/mpbert) and Phoneme-level BERT (https: //github.com/yl4579/PL-BERT) are not published at the time of our empirical investigation (here, these pre-trained models are still not yet publicly available on 8th March 2023\u2014the INTERSPEECH 2023\u2019s paper update deadline). Therefore, we could not compare our multilingual XPhoneBERT with those monolingual models for English.\n9https://github.com/jaywalnut310/vits\nrandomly sample 5% of the training audio clips, and then only use those sampled audios for training (total duration of about 1.2 hours for English and about 0.9 hours for Vietnamese). We apply the same training protocol used for the first setting with an exception that we run for 100K training steps."
        },
        {
            "heading": "3.3. Evaluation protocol",
            "text": "We evaluate the performance of TTS models using subjective and objective metrics. For subjective evaluation of the naturalness, for each language, following [2, 7, 9], we randomly select 50 ground truth test audios and their text transcription to measure the Mean Opinion Score (MOS). Here, for each text transcription, we synthesize speeches using 4 different models (including the baseline VITS and the VITS variant extended with our XPhoneBERT, which are trained under the two different experimental settings detailed in the previous subsection). For each language, we hire 10 native speakers to rate each of the five speeches (i.e. the four synthesized speeches and the ground truth speech) on a naturalness scale from 1 to 5 with 1- point increments. Here, each rater does not know which model produces which speech.\nFor objective evaluations of the distortion and intonation difference between the ground truth speech and the synthesized speech, we compute two metrics of the mel-cesptrum distance (MCD; dB) and the F0 root mean square error (RMSEF0 ; cent), according to the implementation from [35]."
        },
        {
            "heading": "4. Main results",
            "text": "Tables 2 and 3 show obtained results for English and Vietnamese, respectively. We find that our XPhoneBERT helps improve the performance of VITS on all three evaluation metrics for both English and Vietnamese in both experimental settings. For example, for the first setting of using the whole TTS training set for training, the MOS score significantly increases from 4.00 to 4.14 (+0.14 absolute improvement) for English and from\n3.74 to 3.89 (+0.15) for Vietnamese. When it comes to the second setting of using limited training data, XPhoneBERT helps produce larger absolute MOS improvements than those for the first setting. That is, MOS increases from 2.88 to 3.22 (+0.34) for English and especially from 1.59 to 3.35 (+1.76) for Vietnamese, clearly showing the effectiveness of XPhoneBERT.\nSimilar to [36], we also find that the subjective evaluation metric MOS is not \u201calways\u201d correlated with the objective evaluation metrics MCD and RMSEF0 . That is, for Vietnamese in Table 3, the baseline VITS under the first setting obtains higher MOS but slightly poorer MCD and RMSEF0 than the VITS extended with XPhoneBERT under the second setting (MOS: 3.74 vs. 3.35; MCD: 5.41 vs. 5.39; RMSEF0 : 249 vs. 248).\nFrom obtained results for the baseline VITS under the first setting and the VITS extended with XPhoneBERT under the second setting in both Tables 2 and 3, we might consider that XPhoneBERT helps synthesize fairly high-quality speech with limited training data. We also visualize the spectrograms of synthesized and ground truth speeches for a Vietnamese text transcription in Figure 2, illustrating that XPhoneBERT helps improve the spectral details of the baseline VITS\u2019s output."
        },
        {
            "heading": "5. Conclusion",
            "text": "We have presented the first large-scale multilingual language model XPhoneBERT pre-trained for phoneme representations. We demonstrate the usefulness of XPhoneBERT by showing that using XPhoneBERT as an input phoneme encoder improves the quality of the speech synthesized by a strong neural TTS baseline. XPhoneBERT also helps produce fairly high-quality speech when the training data is limited. We publicly release XPhoneBERT and hope that it can foster future speech synthesis research and applications for nearly 100 languages and locales."
        },
        {
            "heading": "6. References",
            "text": "[1] J. Kong, J. Kim, and J. Bae, \u201cHiFi-GAN: Generative Adversarial\nNetworks for Efficient and High Fidelity Speech Synthesis,\u201d in Proceedings of NeurIPS, 2020.\n[2] Y. Ren, Y. Ruan, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T.-Y. Liu, \u201cFastSpeech: Fast, Robust and Controllable Text to Speech,\u201d in Proceedings of NeurIPS, 2019.\n[3] V. Popov, I. Vovk, V. Gogoryan, T. Sadekova, and M. Kudinov, \u201cGrad-TTS: A Diffusion Probabilistic Model for Text-to-Speech,\u201d in Proceedings of ICML, 2021, pp. 8599\u20138608.\n[4] X. Tan, J. Chen et al., \u201cNaturalSpeech: End-to-End Text to Speech Synthesis with Human-Level Quality,\u201d arXiv preprint, vol. arXiv:2205.04421, 2022.\n[5] J. Shen, R. Pang et al., \u201cNatural TTS Synthesis by Conditioning Wavenet on MEL Spectrogram Predictions,\u201d in Proceedings of ICASSP, 2018, pp. 4779\u20134783.\n[6] N. Li, S. Liu, Y. Liu, S. Zhao, and M. Liu, \u201cNeural Speech Synthesis with Transformer Network,\u201d in Proceedings of AAAI, 2019, pp. 6706\u20136713.\n[7] J. Kim, S. Kim, J. Kong, and S. Yoon, \u201cGlow-TTS: A Generative Flow for Text-to-Speech via Monotonic Alignment Search,\u201d in Proceedings of NeurIPS, 2020.\n[8] Y. Ren, C. Hu, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T.-Y. Liu, \u201cFastSpeech 2: Fast and High-Quality End-to-End Text to Speech,\u201d in Proceedings of ICLR, 2021.\n[9] J. Kim, J. Kong, and J. Son, \u201cConditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech,\u201d in Proceedings of ICML, 2021, pp. 5530\u20135540.\n[10] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u201cBERT: Pretraining of Deep Bidirectional Transformers for Language Understanding,\u201d in Proceedings of NAACL, 2019, pp. 4171\u20134186.\n[11] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov, \u201cRoBERTa: A Robustly Optimized BERT Pretraining Approach,\u201d arXiv preprint, vol. arXiv:1907.11692, 2019.\n[12] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut, \u201cALBERT: A lite BERT for self-supervised learning of language representations,\u201d in Proceedings of ICLR, 2020.\n[13] T. Hayashi, S. Watanabe, T. Toda, K. Takeda, S. Toshniwal, and K. Livescu, \u201cPre-Trained Text Embeddings for Enhanced Text-toSpeech Synthesis,\u201d in Proceedings of INTERSPEECH, 2019, pp. 4430\u20134434.\n[14] T. Kenter, M. Sharma, and R. Clark, \u201cImproving the Prosody of RNN-based English Text-To-Speech Synthesis by Incorporating a BERT model,\u201d in Proceedings of INTERSPEECH, 2020, pp. 4412\u20134416.\n[15] G. Xu, W. Song, Z. Zhang, C. Zhang, X. He, and B. Zhou, \u201cImproving Prosody Modelling with Cross-Utterance BERT Embeddings for End-to-end Speech Synthesis,\u201d in Proceedings of ICASSP, 2021, pp. 6079\u20136083.\n[16] Y. Jia, H. Zen, J. Shen, Y. Zhang, and Y. Wu, \u201cPnG BERT: Augmented BERT on Phonemes and Graphemes for Neural TTS,\u201d in Proceedings of INTERSPEECH, 2021, pp. 151\u2013155.\n[17] G. Zhang, K. Song, X. Tan, D. Tan, Y. Yan, Y. Liu, G. Wang, W. Zhou, T. Qin, T. Lee, and S. Zhao, \u201cMixed-Phoneme BERT: Improving BERT with Mixed Phoneme and Sup-Phoneme Representations for Text to Speech,\u201d in Proceedings of INTERSPEECH, 2022, pp. 456\u2013460.\n[18] Y. A. Li, C. Han, X. Jiang, and N. Mesgarani, \u201cPhoneme-Level BERT for Enhanced Prosody of Text-to-Speech with Grapheme Predictions,\u201d arXiv preprint, vol. arXiv:2301.08810, 2023.\n[19] S. Ruder, \u201cWhy You Should Do NLP Beyond English,\u201d https://ruder.io/nlp-beyond-english/, 2020. [Online]. Available: https://ruder.io/nlp-beyond-english/\n[20] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin, \u201cAttention is All you Need,\u201d in Proceedings of NIPS, 2017, pp. 5998\u20136008.\n[21] J. Zhu, C. Zhang, and D. Jurgens, \u201cByT5 model for massively multilingual grapheme-to-phoneme conversion,\u201d in Proceedings of INTERSPEECH, 2022, pp. 446\u2013450.\n[22] M. Guo, Z. Dai, D. Vrandec\u030cic\u0301, and R. Al-Rfou, \u201cWiki-40B: Multilingual Language Model Dataset,\u201d in Proceedings of LREC, 2020, pp. 2440\u20132452.\n[23] W. Foundation. Wikimedia downloads. [Online]. Available: https://dumps.wikimedia.org\n[24] Q. Lhoest, A. Villanova del Moral, Y. Jernite et al., \u201cDatasets: A Community Library for Natural Language Processing,\u201d in Proceedings of EMNLP: System Demonstrations, 2021, pp. 175\u2013184.\n[25] D. Q. Nguyen, D. Q. Nguyen, T. Vu, M. Dras, and M. Johnson, \u201cA Fast and Accurate Vietnamese Word Segmenter,\u201d in Proceedings of LREC, 2018, pp. 2582\u20132587.\n[26] T. Vu, D. Q. Nguyen, D. Q. Nguyen, M. Dras, and M. Johnson, \u201cVnCoreNLP: A Vietnamese Natural Language Processing Toolkit,\u201d in Proceedings of NAACL: Demonstrations, 2018, pp. 56\u201360.\n[27] O. Kuchaiev, J. Li et al., \u201cNeMo: a toolkit for building AI applications using Neural Modules,\u201d in Proceedings of NeurIPS Workshop on Systems for ML, 2019.\n[28] M. Bernard and H. Titeux, \u201cPhonemizer: Text to Phones Transcription for Multiple Languages in Python,\u201d Journal of Open Source Software, vol. 6, no. 68, p. 3958, 2021.\n[29] M. Ott, S. Edunov, A. Baevski, A. Fan, S. Gross, N. Ng, D. Grangier, and M. Auli, \u201cfairseq: A Fast, Extensible Toolkit for Sequence Modeling,\u201d in Proceedings of NAACL-HLT 2019: Demonstrations, 2019, pp. 48\u201353.\n[30] D. P. Kingma and J. Ba, \u201cAdam: A Method for Stochastic Optimization,\u201d in Proceedings of ICLR, 2015.\n[31] T. Wolf, L. Debut et al., \u201cTransformers: State-of-the-Art Natural Language Processing,\u201d in Proceedings of EMNLP 2020: System Demonstrations, 2020, pp. 38\u201345.\n[32] K. Ito and L. Johnson, \u201cThe LJ Speech Dataset,\u201d https://keithito. com/LJ-Speech-Dataset/, 2017.\n[33] D. Q. Nguyen and A. T. Nguyen, \u201cPhoBERT: Pre-trained language models for Vietnamese,\u201d in Findings of EMNLP, 2020, pp. 1037\u2013 1042.\n[34] I. Loshchilov and F. Hutter, \u201cDecoupled Weight Decay Regularization,\u201d in Proceedings of ICLR, 2019.\n[35] A. Tamamori, T. Hayashi, K. Kobayashi, K. Takeda, and T. Toda, \u201cSpeaker-dependent WaveNet vocoder,\u201d in Proceedings of INTERSPEECH, 2017, pp. 1118\u20131122.\n[36] T. Saeki, K. Tachibana, and R. Yamamoto, \u201cDRSpeech: Degradation-Robust Text-to-Speech Synthesis with Frame-Level and Utterance-Level Acoustic Representation Learning,\u201d in Proceedings of INTERSPEECH, 2022, pp. 793\u2013797."
        }
    ],
    "title": "XPhoneBERT: A Pre-trained Multilingual Model for Phoneme Representations for Text-to-Speech",
    "year": 2023
}