{
    "abstractText": "Multi-agent football poses an unsolved challenge in AI research. Existing work has focused on tackling simplified scenarios of the game, or else leveraging expert demonstrations. In this paper, we develop a multi-agent system to play the full 11 vs. 11 game mode, without demonstrations. This game mode contains aspects that present major challenges to modern reinforcement learning algorithms;multi-agent coordination, long-term planning, and non-transitivity. To address these challenges, we present TiZero; a self-evolving,multi-agent system that learns fromscratch. TiZero introduces several innovations, including adaptive curriculum learning, a novel self-play strategy, and an objective that optimizes the policies of multiple agents jointly. Experimentally, it outperforms previous systems by a large margin on the Google Research Football environment, increasing win rates by over 30%. To demonstrate the generality of TiZero\u2019s innovations, they are assessed on several environments beyond football; Overcooked, Multi-agent ParticleEnvironment, Tic-Tac-Toe and Connect-Four.",
    "authors": [
        {
            "affiliations": [],
            "name": "Fanqi Lin"
        },
        {
            "affiliations": [],
            "name": "Shiyu Huang"
        },
        {
            "affiliations": [],
            "name": "Tim Pearce"
        },
        {
            "affiliations": [],
            "name": "Wenze Chen"
        },
        {
            "affiliations": [],
            "name": "Wei-Wei Tu"
        }
    ],
    "id": "SP:d4d71837afa44b83cb924ad4fbf57d6de5373b3d",
    "references": [
        {
            "authors": [
                "Sepp Hochreiter",
                "J\u00fcrgen Schmidhuber"
            ],
            "title": "Long short-term memory",
            "venue": "Neural computation,",
            "year": 1997
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint 103 arXiv:1412.6980,",
            "year": 2014
        },
        {
            "authors": [
                "Andrew M Saxe",
                "James L McClelland",
                "Surya Ganguli"
            ],
            "title": "Exact solutions to the nonlinear 105 dynamics of learning in deep linear neural networks",
            "venue": "arXiv preprint arXiv:1312.6120,",
            "year": 2013
        },
        {
            "authors": [
                "Oriol Vinyals",
                "Igor Babuschkin",
                "Wojciech M Czarnecki",
                "Micha\u00ebl Mathieu",
                "Andrew Dudzik",
                "Jun- 107 young Chung",
                "David H Choi",
                "Richard Powell",
                "Timo Ewalds",
                "Petko Georgiev"
            ],
            "title": "Grandmaster 108 level in starcraft ii using multi-agent reinforcement learning",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "KEYWORDS Multi-agent Reinforcement Learning; Self-play; Google Research Football; Large-scale Training ACM Reference Format: Fanqi Lin, Shiyu Huang, Tim Pearce, Wenze Chen, and Wei-Wei Tu. 2023. TiZero: Mastering Multi-Agent Football with Curriculum Learning and SelfPlay. In Proc. of the 22nd International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2023), London, United Kingdom, May 29 \u2013 June 2, 2023, IFAAMAS, 29 pages."
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Deep reinforcement learning (DRL) has achieved great success in many games, including Atari classics [2, 23, 38], first-personshooters [20, 25, 43], real-time-strategy titles [4, 57, 65], board games [49, 51] and card games [15, 67]. Yet modern DRL systems still struggle in environments containing challenges such as multiagent coordination [44, 63, 66], long-term planning [9, 56, 68] and non-transitivity [3, 8]. The Google Research Football environment (GFootball) contains all these challenges [29] and more. This paper\nProc. of the 22nd International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2023), A. Ricci, W. Yeoh, N. Agmon, B. An (eds.), May 29 \u2013 June 2, 2023, London, United Kingdom. \u00a9 2023 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.\npresents the first system that successfully deals with all of them, learning to play the full 11 vs. 11 game mode from scratch. Experimentally, our method outperforms previous systems by a large margin with over 30% higher winning rates.\nGFootball has attracted the attention of many DRL researchers as it provides a test-bed for complex multi-agent control [12, 19, 31, 60, 63]. As in the popular real-world sport of football/soccer, to win at GFootball agents must combine short-term control techniques with coordinated, long-term global strategies. Challenges in GFootball include multi-agent cooperation, multi-agent competition, sparse rewards, large action space, large observation space and stochastic environments \u2013 a combination not present in other RL research environments. In GFootball, each agent needs both to cooperate with teammates, and compete against diverse and unknown opponents. Agents without the ball are hard to optimize as they do not obtain dense rewards, thus resulting in challenging multi-agent credit assignment problem [10, 11, 55, 70]. Moreover, GFootball transitions are stochastic, e.g. performing the same shooting action from the same state may sometimes result in a goal and sometimes a miss. Table 1 contrasts GFootball with other popoular RL environments, illustrating its complexity.\nPrior work on GFootball has mostly focused on \u2018Academy\u2019 scenarios [12, 31, 60, 63], which are drastically simplified versions of the full 11 vs. 11 game, for instance requiring agents to score in an empty goal or beat the goalkeeper in a 1-on-1. Any opponents are\nar X\niv :2\n30 2.\n07 51\n5v 2\n[ cs\n.A I]\n2 1\nFe b\n20 23\nrules-based bots. In contrast, our work introduces an AI system that plays on the full 11 vs. 11 game mode. This mode requires simultaneous control of ten players. (The goalkeeper is excluded since they have a unique action space and different purpose which would require training of a separate policy. Instead they are controlled via a simple rule-based strategy.) Each player performs 3, 000 steps per match, meaning long-term planning is required. Due to these challenges, previous work tackling the 11 vs. 11 game mode relied on an offline demonstration dataset [19]. The disadvantage of such methods is that the performance of trained agents is bounded by the skill-level of the demonstrators. In this paper, we do not use any demonstration dataset nor any pre-trained model, instead training tabula-rasa via curriculum-learning and self-play.\nThis paper proposes amulti-agent, curriculum, self-play framework for GFootball, which we name TiZero, that is trained at massscale. At its core, it is an actor-critic algorithm [28]. To facilitate multi-agent learning, a single centralized value network guides learning of all agent policies in a joint optimization step, while allowing for decentralised execution at test time. To tackle the challenge of long-term planning in a sparse-reward setting, we borrow ideas from curriculum learning [48, 64, 69]. Here, the agent initially learns basic behaviors in simpler tasks, strengthens these in increasingly difficult scenarios, eventually enabling high-quality long-term planning. To address the challenge of nontransitivity (that is, player A>player B, player B>player C =\u0338\u21d2 player A>player C) [3, 8], we develop a self-play learning strategy to manage the opponent pool, which ensures training is against a set of diverse and strong opponents. TiZero applies these algorithmic ideas in a large-scale distributed training infrastructure, training across hundreds of processors for about 40 days.\nIn addition to these core algorithmic ingredients, TiZero adopts a variety of modern DRL advances, including recurrent experience replay [24], actionmasking, reward shaping [39], agent-conditioned policy [12] and staggered resetting.\nTo evaluate the performance of TiZero, we compare against previous state-of-the-art methods [1, 19, 58, 71] on GFootball, outperforming them by a large margin. On a public evaluation platform of GFootball, TiZero also ranks first1. To understand the generality of TiZero, we assess our system on several public benchmarks not related to football, such as [61], Multi-agent Particle-world (MPE) [36],\n1JiDi AI Competition Platform: http://www.jidiai.cn/ranking_list?tab=34. The evaluation result was collected on October 28th, 2022.\nTic-Tac-Toe and Connect-Four, finding that it also shows promise in these new domains."
        },
        {
            "heading": "2 RELATEDWORK",
            "text": "In this section, we briefly review relevant work in the areas of multiagent reinforcement learning, competitive self-play and football game AI."
        },
        {
            "heading": "2.1 Multi-agent Reinforcement Learning",
            "text": "Multi-agent reinforcement learning (MARL) has received much research attention [44, 52, 55, 59, 66]. Recent algorithms usually focus on the centralized training with decentralized execution (CTDE) setup. In CTDE, the learning algorithm has access to the actionobservation histories of all agents during training. But at execution, each agent only has access to its local action-observation history. There are two main kinds of MARL algorithms: 1) value-based algorithms, such as QMIX [44], VDN [55], QPLEX [59]; 2) policy-based algorithms, such as MADDPG [36], MAPPO [66], MAT [63]. As an example of a value-based MARL algorithm, QMIX uses a centralized Q-value network over the joint action-value function. It is designed in a way that constrains the joint Q-value to be monotonic with respect to each individual agent\u2019s Q-value (these are mixed in a additive but possibly non-linear function). This allows decentralized execution as each agent can independently perform an argmax operation over their individual Q-values. TiZero builds upon MAPPO [66], which adapts the standard single-agent PPO algorithm [47] to the multi-agent setting by learning individual policies conditioned on local observations, and a centralized value function based on a global state. Several tricks have proven important to stabilize training, including Generalized Advantage Estimation (GAE) [46], observation normalization, value clipping, and orthogonal initialization. MAPPO is competitive with many MARL algorithms such as MADDPG [36], RODE [62], and QMIX, both in terms of sample efficiency and wall-clock time. In this paper, we propose a new variant of MAPPO, which can improve multi-agent coordination and reduce the video memory usage via multi-agent joint-policy optimization."
        },
        {
            "heading": "2.2 Competitive Self-play",
            "text": "Self-play has emerged as a powerful technique to obtain superhuman policies in competitive environments [4, 16, 30, 50, 57]. By\nplaying against recent copies of itself, an agent can continuously improve its performance, avoiding any limitation that might otherwise be imposed by the skill-level of a demonstration dataset. PolicySpace Response Oracle (PSRO) is a population learning framework for learning best-response agents to a mixture of previous agents with a meta-strategy solver [30]. Under this framework, Vinalys et al. [57] proposed a league training framework to train robust agents for StarCraft (\u2018prioritized fictitious self-play\u2019). Meanwhile, OpenAI Five [4] achieved super-human performance on Dota 2 via a simpler self-play strategy \u2013 the current set of agents plays against the most recent set of agents with 80% probability, and plays against past agents with 20% probability. However, such empirical successes require huge computing resources, and the training process may become stuck due to the non-transitivity dilemma when strategic cycles exist [3, 8]. Recent work solves this problem by increasing the diversity in the pool of opponent policies [6, 34, 35]. This paper also introduces a novel self-play training strategy that improves the diversity and performance of opponent policies. We design a two-step self-play improvement scheme, with the first step to challenge the prior agent and second step to generalise against the entire opponent pool."
        },
        {
            "heading": "2.3 Football Games and AI",
            "text": "Football environments are valuable for AI research, as they blend several challenges together; control, strategy, cooperation, and competition. Aside from GFootball, several popular simulators have been proposed. rSoccer [37] and JiDi Olympics Football [21] are simple environments \u2013 players are represented as rigid bodies with a limited action space, either moving or pushing the ball. In contrast, GFootball provides a rich action space, adding mechanics such as slide-tackling and sprinting. The RoboCup Soccer Simulator [22, 27, 53] and DeepMind MuJoCo Multi-Agent Soccer Environment [32, 33] environments emphasis low-level robotic control, requiring manipulation of the joints of a player. Meanwhile, GFootball abstracts this away, allowing agents to focus on the challenge\nof developing high-level behaviors and strategies. A competition in the GFootball environment was hosted on Kaggle [14] in 2020 that attracted over 1,000 teams. This required building an agent to control a single \u2018in-focus\u2019 player at any one time, while teammates were controlled by an in-built rules-based AI. The champion of the competition, named WeKick [71], utilized imitation learning and distributed league training. In contrast to this setup, our system tackles the far more challenging task of controlling all 10 outfield players on a team simultaneously, in a decentralized fashion. The only prior work directly tackling this objective first collected a demonstration dataset by rolling out WeKick, and then utilized offline RL techniques to train agents. This was named TiKick, [19]. By contrast, our work trains agents via self-play without requiring demonstration data. Through this methodology, TiZero exceeds the performance of all previous systems in GFootball, including the best entry among 1,000 (WeKick), TiKick, and others."
        },
        {
            "heading": "3 PRELIMINARIES",
            "text": "Multi-agent Reinforcement Learning: We formalize the multiagent reinforcement learning as a decentralized partially observable Markov decision process (Dec-POMDP) [5]. An \ud835\udc5b-agent DecPOMDP can be represented as a tuple (N ,S,A,T , \ud835\udc5f ,O,\ud835\udc3a,\ud835\udefe), where S is the state space, A is the action space, O is the observation space and \ud835\udefe \u2208 [0, 1) is a reward discount factor. N \u2261 {1, ..., \ud835\udc5b} is a set of \ud835\udc5b = |N | agents. At time step \ud835\udc61 , each agent \ud835\udc56 \u2208 N takes an action \ud835\udc4e\ud835\udc56 \u2208 A, forming a joint action a \u2208 A \u2261 A\ud835\udc5b . Agents receive an immediate reward \ud835\udc5f (\ud835\udc60, a) after taking action a in state \ud835\udc60 . The reward is shared by all agents. T (\ud835\udc60, a, \ud835\udc60 \u2032) : S \u00d7 A \u00d7 S \u21a6\u2192 [0, 1] is the dynamics function denoting the transition probability. In a DecPOMDP, an agent will receive its partially observable observation \ud835\udc5c\ud835\udc61 \u2208 O according to the observation function\ud835\udc3a (\ud835\udc60, \ud835\udc56) : S \u00d7A \u2192 O. Each agent has a policy \ud835\udf0b\ud835\udc56 (\ud835\udc4e\ud835\udc56\ud835\udc61 |\ud835\udc5c\ud835\udc561:\ud835\udc61 ) to produce action \ud835\udc4e \ud835\udc56 \ud835\udc61 from local historical observations \ud835\udc5c\ud835\udc561:\ud835\udc61 . We use \ud835\udc4e \u2212\ud835\udc56 \ud835\udc61 as the action of all complementary agents of agent \ud835\udc56 and use a similar convention for policies\n\ud835\udf0b\u2212\ud835\udc56 . The agents\u2019 objective is to learn a joint policy \ud835\udf0b that maximizes their expected return E(\ud835\udc60\ud835\udc61 ,a\ud835\udc61 ) [\u2211 \ud835\udc61 \ud835\udefe \ud835\udc61\ud835\udc5f (\ud835\udc60\ud835\udc61 , a\ud835\udc61 ) ] ."
        },
        {
            "heading": "4 METHODOLOGY",
            "text": "This section introduces TiZero in detail. Firstly, we describe the agent\u2019s architecture and observation space. We then introduce a new multi-agent learning algorithm and self-play strategy. We further describe several important implementation details found to be helpful. Finally, we introduce the distributed training framework used to scale up our training."
        },
        {
            "heading": "4.1 Agent Design",
            "text": "Observation space.GFootball provides the observations in several formats, such as an RGB image of the game or a rendered mini-map. Our agents learn from only state vectors \u2013 by default this is provided as a 115-dimensional vector, containing information such as player and ball coordinates. We follow previous work [19] which showed benefit in extending this vector with more auxiliary features (such as offside flags to mark potential offside teammates and relative positions among agents) to create a 268-dimensional vector. Instead of using this full vector directly as input, we split the agent observation into six parts, including the controlled player information, player ID, ball information, teammate information, opponent information and current match information. The value-network needs to approximate the value function for the whole team, thus we design a global state vector for the value-network with five parts, including ball information, information of ball holder, teammate information, opponent information and current match information. More details about our observation space can be found in the Appendix M.\nNetwork architecture. Six separate MLPs with two (one for the \"player ID\") fully-connected layers separately encode each part of the observation. These extracted hidden features are then concatenated together and processed by an LSTM layer [18], which provides the agent with memory. All hidden layers have layer normalization and ReLU non-linearities. We use the orthogonal matrix [45] for parameter initialization and the Adam optimizer [26]. To accelerate learning, we mask out any illegal actions by setting their probability of selection to zero. Figure 2 shows the overall policy network architecture. We also construct a similar architecture for the value network, which is trained with Mean Square Error (MSE) loss. Further hyperparameter details and also the value network structure are provided in the Appendix D."
        },
        {
            "heading": "4.2 Multi-agent Algorithm",
            "text": "We now introduce the Joint-ratio Policy Optimization (JRPO), a modified version of MAPPO [66]. MAPPO is a direct extension of PPO [47] to the multi-agent setting, with the value-network \ud835\udc49\ud835\udc61\ud835\udc5c\ud835\udc61\ud835\udc4e\ud835\udc59 (\ud835\udc60\ud835\udc61 ) centralized across all agents, learning from the global state. GAE can then be used to compute the global advantage function \ud835\udc34total (\ud835\udc60\ud835\udc61 , a\ud835\udc61 ) (abbreviated as \ud835\udc34\ud835\udc61 ). In MAPPO, this advantage function guides the improvement of each agent\u2019s policy independently \ud835\udf0b\ud835\udc56\n\ud835\udf03 (\ud835\udc62\ud835\udc56\ud835\udc61 |\ud835\udc5c\ud835\udc561:\ud835\udc61 ), \ud835\udc56 \u2208 N , where \ud835\udf03 are the parameters of the policy\nnetwork. Instead of this, we optimize the joint-policy using a decentralized factorization:\n\ud835\udf0b\ud835\udf03 (a\ud835\udc61 |o1:\ud835\udc61 ) \u2248 \ud835\udc5b\u220f \ud835\udc56=1 \ud835\udf0b\ud835\udc56 \ud835\udf03 (\ud835\udc4e\ud835\udc56\ud835\udc61 |\ud835\udc5c\ud835\udc561:\ud835\udc61 ). (1)\nThis allows us to write the joint-policy objective as: \ud835\udc3f\ud835\udc36\ud835\udc3f\ud835\udc3c\ud835\udc43 (\ud835\udf03 ) = E\u0302\ud835\udc61 [ min ( \ud835\udc5f\ud835\udc61 (\ud835\udf03 )\ud835\udc34\ud835\udc61 , clip(\ud835\udc5f\ud835\udc61 (\ud835\udf03 ), 1 \u2212 \ud835\udf16, 1 + \ud835\udf16)\ud835\udc34\ud835\udc61 )] , (2) where \ud835\udc5f\ud835\udc61 (\ud835\udf03 ) = \ud835\udf0b\ud835\udf03 (a\ud835\udc61 |o1:\ud835\udc61 )\ud835\udf0b\ud835\udf03old (a\ud835\udc61 |o1:\ud835\udc61 ) = \u220f\ud835\udc5b \ud835\udc56=1 \ud835\udf0b\ud835\udc56 \ud835\udf03 (\ud835\udc4e\ud835\udc56\ud835\udc61 |\ud835\udc5c\ud835\udc561:\ud835\udc61 )\n\ud835\udf0b\ud835\udc56 \ud835\udf03old\n(\ud835\udc4e\ud835\udc56\ud835\udc61 |\ud835\udc5c\ud835\udc561:\ud835\udc61 ) , and E\u0302\ud835\udc61 [\u00b7 \u00b7 \u00b7 ] is the\nexpectation using empirical samples. And clip(\u00b7 \u00b7 \u00b7 ) is a clipping function with clipping hyperparameter \ud835\udf16 . This is in contrast to vanilla MAPPO, where \ud835\udc5f\ud835\udc61 (\ud835\udf03 ) is computed on each agent\u2019s policy individually, \ud835\udc5f\ud835\udc61 (\ud835\udf03, \ud835\udc56) = \ud835\udf0b\ud835\udc56 \ud835\udf03 (\ud835\udc4e\ud835\udc56\ud835\udc61 |\ud835\udc5c\ud835\udc561:\ud835\udc61 )\n\ud835\udf0b\ud835\udc56 \ud835\udf03old\n(\ud835\udc4e\ud835\udc56\ud835\udc61 |\ud835\udc5c\ud835\udc561:\ud835\udc61 ) . By using our factorization, all\nagent policies are optimized jointly. Previous work has identified that this objective enjoys monotonic guarantees similar to PPO [54]. Our later experiments evidence the empirical advantage of this joint-policy objective, compared to vanilla MAPPO. We believe that framing the objective jointly may encourage the agents to achieve better coordination. Additionally, we found it reduces memory usage and improves training speed."
        },
        {
            "heading": "4.3 Curriculum & Self-play Training Strategy",
            "text": "TiZero\u2019s training strategy can be divided into two stages. In the first stage (\u2018Curriculum Self-play\u2019), the difficulty of the scenarios the agents are trained in gradually increases, guiding them to learn basic behaviors in an efficient manner. The opponents in this stage are versions of the agent trained in easier scenarios. In the second stage (\u2018Challenge & Generalise Self-play\u2019), the difficulty-level is fixed at maximum. Agents play against a diverse pool of increasingly strong opponents (previous copies of the agent), providing a route towards superhuman performance.\nCurriculum Self-play: The rewards provided in the GFootball 11 vs. 11 game mode are very sparse. This causes vanilla MARL algorithms to struggle. To address this issue, we design a curriculum self-play mechanism, in which agents are trained on a sequence of progressively more difficult scenarios, where the opponent is a copy of the agent from the previous difficulty-level scenario. We design ten difficulty levels by configuring the GFootball environment settings. The difficulty level is determined by two aspects; 1) The strength of opponent players, which can be varied from 0 to 1, with values closer to 1 meaning players are quicker and have better stamina. 2) The initial positions of players and the ball. For example, players can be set in positions closer to the opponent\u2019s goal and the ball is also set in position closer to the opponent\u2019s goal. At the beginning of training, agents are initialized with random weights, and learn on the lowest difficulty scenario (lowest opponent strength and positioned closest to the goal). This allows agents to receive denser rewards that encourage basic shooting and passing behaviors. When agent performance meets some threshold in the current scenario, the difficulty level automatically increases. Finally in the highest difficulty level, agents must compete with the whole opponent team that encourages more advanced tactics and team cooperations. The Appendix B provides further detail about the curriculum design.\nChallenge & Generalise Self-play: Through curriculum selfplay our agents achieve a basic performance level against a single opponent. To improve performance against a range of opponents, we design an algorithm that produces a monotonically-improving sequence of policies. This consists of two steps. 1) Challenge Selfplay. Current agents play against the most recently saved agents with probability of 80%, and play against older versions with probability of 20%. The main purpose of this step is ensure the current system can defeat the strongest agents seen so far. 2) Generalise Self-play. Current agents play against the whole opponent pool, sampling opponents according to their strength as follows. Denote the opponent pool M. Let \ud835\udc56 \u2209 M be the current training agent, \ud835\udc57 \u2208 M all other agents in pool, and \ud835\udc5d (\ud835\udc56, \ud835\udc57) be the probability that agent \ud835\udc56 defeats agent \ud835\udc57 . We sample model \ud835\udc57 to play against with probability:\n\ud835\udc5dsample ( \ud835\udc57) = \ud835\udc53hard (\ud835\udc5d (\ud835\udc56, \ud835\udc57))\u2211\n\ud835\udc5a\u2208M \ud835\udc53hard (\ud835\udc5d (\ud835\udc56,\ud835\udc5a)) , (3)\nwhere \ud835\udc53hard (\ud835\udc65) = (1\u2212\ud835\udc65)2. This sampling strategy focuses our agents training on opponents it is less likely to win against. Therefore, the training agent will maximize its performance over all existing opponents. Prioritized fictitious self-play addresses the non-transitivity dilemma and improves the robustness of agents [57]. Once agents perform well on this step, they are themselves added to the opponent pool for future versions to train against. The whole self-play algorithm and details of the our self-play strategy can be found in Appendix C & F."
        },
        {
            "heading": "4.4 Implementation Details",
            "text": "This section summarizes several details that were found to be important to achieving good performance.\nRecurrent Experience Replay: Allowing agents to learn from earlier observations in an episode helps agents with long-term planning and inferring an opponent\u2019s strategy. Hence, we utilize an LSTM [18]. Rather than initializing hidden states with zeros during training on each sequence, we reset to the hidden state that was actually generated by the agent at that timestep \u2013 these are stored in the replay buffer during roll outs. When collecting the training data, models are rolled out for sequences of 500 timesteps. During optimization, the LSTM backpropagates through timesteps with length of 25.\nAction Masking: Action masking reduces the effective action space by preventing selection of inappropriate actions. In GFootball, we mask out actions that are unavailable or nonsensical during certain situations, for instance slide-tackling is disabled when a teammate holds the ball.\nReward Shaping: The basic rewards provided by GFootball are 1 for scoring a goal, and \u22121 for conceding a goal. This is sparse and hard to optimize. To improve training efficiency, we design several more dense reward signals. Note all agents on a team receive rewards equally.\n- Holding-Ball reward: When the ball is controlled by an agents\u2019 team, a reward of 0.0001 per timestep is received. - Passing-Ball reward: When agents execute successful pass before a goal, they receive a reward of 0.05. This encourages agents to learn coordinated passing strategies. - Grouping penalty: If agents on the same team gather too closely together, a reward of \u22120.001 is received. This encourages agents to spread out across the pitch. - Out-of-bounds penalty: A reward of \u22120.001 is received when an agent is outside of the playing area.\nThe reward across the two teams is balanced to be zero-sum, which is a basic requirement for self-play to succeed. Hence, if one team receives a positive reward, the other team receives a negative reward of the same magnitude.\nPlayer-ID Conditioning: Instead of training a separate policy network per agent, we use a single shared policy network which receives the player ID as input. Hence, one set of parameters is used by all the agents on a team, while still allowing for decentralized execution. This makes the multi-agent training more sample efficient, since experience from one agent helps improve the network of other agents. It also improves inference speed \u2013 only one forward pass of the observation encoder and LSTM is required for the whole team. This produces the embedding that is used by all agents after appending their player-ID embedding to the output of the LSTM unit (Figure 2).\nStaggered Resetting: To encourage episodes to be initialized in interesting situations, we use the \u2018staggered resetting\u2019 trick when collecting the training data. After the first time resetting the environment, we apply random actions for all players for a variable number of timesteps. This avoids each episode beginning from the same state, which could introduce undesirable correlations in our data."
        },
        {
            "heading": "4.5 Distributed Training Framework",
            "text": "Training a deep neural network at the scale demanded by GFootball requires a large amount of computation. In this work, we built a scalable and loosely-decoupled distributed infrastructure for multiagent self-play training . There are two types of modules in our framework; Actors and Learners. These modules are decoupled, enabling researchers to develop their algorithm on a single local machine and then easily launch a large-scale training with zero-code change. The main function of the Actor is rolling out models in environments (or simulators), while storing observations, rewards, actions and LSTM hidden states. All the data collected by the Actors is sent to the Learners via a data server. Learners will train the value and policy networks using GPUs and the gradients are averaged through NCCL reduction [40]. The trained network parameters from the Learners are synchronized to the Actors through the data server. Last but not least, we maintain a dynamic opponent pool and continuously add new opponent to the pool when the training model meets certain performance indexes. Researchers can utilize the stored opponent information (such as winning rates and historical play information) and are flexible to design their own selfplay strategies to sample opponents for the self-play. Besides, our model definition and training is done using Pytorch [42]. Figure 3 summarises our distributed training framework."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "This section empirically evaluates the performance of TiZero. We first test the full system on our target domain, GFootball. We then explore the generality of several components of the system on environments unrelated to football. Specifically we show that JRPO improves over MAPPO, and that our Challenge & Generalise SelfPlay framework produces stronger and more diverse strategies than alternatives."
        },
        {
            "heading": "5.1 GFootball Evaluation",
            "text": "Experimental Settings: We train and evaluate TiZero on the full 11 vs. 11 game mode in GFootball. Each match lasts for five minutes, or 3, 000 timesteps (no injury time). The team with highest number of goals wins. Standard football rules are applied by the game, such as offside, penalty kicks and yellow/red cards. TiZero was trained over 45 days on a cluster with 800 CPUs and two NVIDIA A100 GPUs. The batch size for each GPU is set to 2, 150, 000, the hidden size of the LSTM layer is 256, and the discount factor \ud835\udefe is 0.999. We used the Adam optimizer with learning rate of 0.0001. Further hyperparameters can be found in the Appendix H.\nWe compare TiZero to several strong baselines: - WeKick [71]: An RL-based agent that placed first from over 1,000 entries in the 2020 Kaggle Football Competition [14] (see Section 2).\n- TiKick [19]: The current strongest agent for the GFootball full game, controls all players in the game (see Section 2). - JiDi_3rd: An agent initialized with a pre-trained model and improved via self-play. This was the second runner-up of the 2022 JiDi Football Competition2. - Built-in Hard: Agent provided by the GFootball environment, which directly controls the underlying game engine. - Rule-based: Two strong rules-based agents from the Kaggle Football Competition, which were hand designed. We use Rule-Based-1 [58] and Rule-Based-2 [1].\nWe use the TrueSkill rating system [17] to evaluate all systems. Ratings are computed by running a large amount of matches over 2http://www.jidiai.cn/compete_detail?compete=16\nTable 2: Comparison of TiZero with baseline systems on GFootball. Higher is better except for all metrics except \u2019Draw Rate\u2019 and \u2019Lose Rate\u2019. As well as a better win rate and goal difference, TiZero agents demonstrate a higher level of teamwork, passing the ball more and creating more assists.\nMetric TiZero (Ours) TiKick WeKick JiDi_3rd Built-in Hard Rule-Based-1 Rule-Based-2 Assist 1.30(1.02) 0.61(0.79) 0.20(0.47) 0.35(0.62) 0.20(0.55) 0.28(0.59) 0.22(0.53) Pass 19.2(3.44) 6.99(2.71) 5.33(2.44) 3.96(2.33) 11.5(4.63) 7.28(2.77) 7.50(3.12) Pass Rate 0.73(0.07) 0.65(0.17) 0.53(0.18) 0.44(0.19) 0.66(0.12) 0.64(0.17) 0.63(0.19) Goal 3.42(1.69) 1.79(1.41) 0.88(0.88) 1.43(1.34) 0.52(0.91) 0.73(0.69) 0.64(0.82) Goal Difference 2.27(1.93) 0.71(2.08) -0.47(1.68) -0.02(2.14) -1.06(1.93) -0.60(1.03) -0.71(1.45) Draw Rate (%) 8.50 22.2 29.0 23.2 24.8 28.7 27.8 Lose Rate (%) 6.50 23.5 44.2 33.8 59.6 48.2 49.5 Win Rate (%) 85.0 54.3 26.8 43.0 15.6 23.1 22.7 TrueSkill 45.2 37.2 30.9 35.0 24.9 28.2 27.1\n0 1 2 3 4 5 6 T (mil)\n0\n50\n100\n150\n200\n250\n300\nEp iso\nde R\new ar\nds\nOvercooked-Unident\n0 1 2 3 4 5 6 T (mil)\n0\n50\n100\n150\n200\nEp iso\nde R\new ar\nds\nOvercooked-Random0\n0 1 2 3 4 5 6 T (mil)\n0\n50\n100\n150\n200\nEp iso\nde R\new ar\nds\nOvercooked-Random1 JRPO MAPPO MAT QMIX\n0 1 2 3 4 5 6 T (mil)\n0\n50\n100\n150\n200\n250\nEp iso\nde R\new ar\nds\nOvercooked-Simple\n0 1 2 3 4 5 T (mil)\n220\n200\n180\n160\n140\n120\nEp iso\nde R\new ar\nds\nMPE-3-agent\n(a)\nOvercooked-Unident\nOvercooked-Random0\nOvercooked-Random1 JRPO MAPPO MAT QMIX\nOvercooked-Simple\nMPE-3-agent\na fixed pool of all systems \u2013 this pool comprises all the models checkpointed during TiZero\u2019s training process, as well as baseline systems. Figure 4 shows the evolution of the TrueSkill rating over 45 days of training. We also mark the TrueSkill rating of the baseline systems, all of which TiZero exceeds by a wide margin. One can observe that the curriculum self-play stage allows TiZero to rapidly improve early in training to a level just below the \u2018Builtin Hard\u2019 agent. Through challenge & generalize self-play, TiZero continues to gradually improve performance beyond the level of previous systems, eventually plateauing at a rating around 45. Table 2 presents TrueSkill ratings and win rates for all systems. To verify our system independently, we also submitted our best TiZero system to a public evaluation platform, which maintains a public leaderboard of GFootball systems. At present, TiZero ranks first with a score of 9.7 and win rate of 95.8%.\nWe conducted an analysis to understand how cooperative TiZero\u2019s decentralized agents are. It\u2019s possible that superior performance could be achieved through expert control of a single player, rather than through coordinated teamwork leveraging strategies such as \u2018tiki-taka\u2019, crosses and long balls. Table 2 reports statistics that reveal TiZero\u2019s agents indeed leverage cooperation more than other systems. TiZero has a higher number of passes or long balls that directly lead to scoring (\u2018Assists\u2019), more passes between teammates (\u2018Pass\u2019), and a higher chance of passes being successfully received by teammates (\u2018Pass Rate\u2019). The Appendix A provides detailed visualizations of TiZero\u2019s coordination behaviors."
        },
        {
            "heading": "5.2 Policy Optimization Ablation",
            "text": "This section compares the joint-policy version of PPO (JRPO) we introduced in Section 4.2, with established state-of-the-artMARL algorithmsMAPPO [66],MAT [63] andQMIX [44].We test across the environments Overcooked [61] andMulti-agent Particle-Environment (MPE) [36]. Overcooked is a grid-world game in which agents cooperate to complete a series of tasks, such as finding vegetables, making soup and delivering food. MPE is a continuous 2D world with multiple movable particles. More information about these environments can be found in Appendix G. To be comparable, all three MARL algorithms use the same desktop machine, neural network architecture and hyperparameters. Each method is run over 6 random seeds. More details about the setup of each environment can found in the Appendix I.\nFigure 5 shows the training curves of each methods w.r.t. environment steps and wall-clock time. JRPO\u2019s performance is equal or better than MAPPO, MAT and QMIX across all tasks. JRPO also achieves better results with faster wall-clock training time. Table 3 shows the GPU memory consumption of JRPO and MAPPO. Results show that JRPO consumes less GPU memory than MAPPO, especially when there are more agents (such as MPE with 20 agents and GFootball with 10 agents). More experimental results can be found in the Appendix K."
        },
        {
            "heading": "5.3 Self-Play Strategy Ablation",
            "text": "This section evaluates our \u2018Challenge & Generalize Self-Play\u2019 strategy on two classical 2-player board games, Tic-Tac-Toe and Connect-Four. In the Tic-Tac-Toe game, two players take turns placing marks on a three-by-three grid. Players win by placing three of their marks in a line. Our agent is a deep neural network with a 27-dimension state vector as input (O\u2019s, X\u2019s and blanks are one-hot encoded for the 9 cells). Connect-Four is an extended version of Tic-Tac-Toe with a four-by-four board and with placing four marks in a row to win. Our self-play strategy was described in Section 4.3. This ablation omits the curriculum self-play stage since the rewards are non-sparse. Thus, we focus our test on the challenge & generalize self-play stage. We construct several baselines as a comparison: (1) Challenge Self-Play: The training agent uses the Challenge Self-Play as described in Section 4.3; (2)Generalize SelfPlay: The training agent uses the Generalize Self-Play as described in Section 4.3; (3) Newest: The training agent only combats with itself. All methods share the same network architecture and learning paradigm. The only difference is how they sample opponents from the pool for self-play.\nFigure 6 shows the training curves of different self-play strategies over three random seeds. Results show that our method outperforms other baselines in both training efficiency and final performance. For Tic-Tac-Toe, we quantitatively evaluate the Diversity Index[41] of policies in the opponent pool of eachmethod, as shown in Table 4. We see that \u2019Challenge & Generalize Self-Play\u2019 achieves a larger Diversity Index than baselines, which indicates our self-play strategy can produce more diverse policies. Further experimental details and visualizations are given in Appendix J & L."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "This paper presented a distributed multi-agent reinforcement learning system for the complex Google Research Football environment. This environment encapsulates several challenges; multi-agent coordination, sparse rewards, and non-transitivity. Our work is the first to train strong agents for the GFootball 11 vs. 11 game mode from scratch, controlling all 10 outfield players in a decentralized fashion. Achieving this required combining existing techniques, with several innovations \u2013 a joint-policy optimization objective, curriculum self-play and a challenge & generalize self-play strategy. Our experiments in GFootball showed that TiZero outperforms previous systems by a wide margin in terms of win rate and goal difference. TiZero also utilizes complex coordination behaviors more often than prior systems. Experiments on other MARL and self-play benchmarks further evidence the effectiveness and generality of our algorithmic innovations."
        }
    ],
    "title": "TiZero: Mastering Multi-Agent Footballwith Curriculum Learning and Self-Play",
    "year": 2023
}