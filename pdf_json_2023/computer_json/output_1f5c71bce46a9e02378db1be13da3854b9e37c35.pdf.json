{
    "abstractText": "Context: Machine Learning (ML) is integrated into a growing number of systems for various applications. Because the performance of an ML model is highly dependent on the quality of the data it has been trained on, there is a growing interest in approaches to detect and repair data errors (i.e., data cleaning). Researchers are also exploring how ML can be used for data cleaning; hence creating a dual relationship between ML and data cleaning. To the best of our knowledge, there is no study that comprehensively reviews this relationship. Objective: This paper\u2019s objectives are twofold. First, it aims to summarize the latest approaches for data cleaning for ML and ML for data cleaning. Second, it provides future work recommendations. Method: We conduct a systematic literature review of the papers published between 2016 and 2022 inclusively. We identify different types of data cleaning activities with and for ML: feature cleaning, label cleaning, entity matching, outlier detection, imputation, and holistic data cleaning. Results: We summarize the content of 101 papers covering various data cleaning activities and provide 24 future work recommendations. Our review highlights many promising data cleaning techniques that can be further extended. Conclusion: We believe that our review of the literature will help the community develop better approaches to clean data.",
    "authors": [
        {
            "affiliations": [],
            "name": "Pierre-Olivier C\u00f4t\u00e9"
        },
        {
            "affiliations": [],
            "name": "Amin Nikanjam"
        },
        {
            "affiliations": [],
            "name": "Nafisa Ahmed"
        },
        {
            "affiliations": [],
            "name": "Dmytro Humeniuk"
        },
        {
            "affiliations": [],
            "name": "Foutse Khomh"
        }
    ],
    "id": "SP:2c242fe0cd88fe3469b8f7a37a9d5890477a31e4",
    "references": [
        {
            "authors": [
                "Z Abedjan",
                "X Chu",
                "D Deng",
                "RC Fernandez",
                "IF Ilyas",
                "M Ouzzani",
                "P Papotti",
                "M Stonebraker",
                "N Tang"
            ],
            "title": "Detecting data errors: Where are we and what needs to be done",
            "venue": "Proceedings of the VLDB Endowment",
            "year": 2016
        },
        {
            "authors": [
                "NZ Abidin",
                "AR Ismail",
                "NA Emran"
            ],
            "title": "Performance analysis of machine learning algorithms for missing value imputation",
            "venue": "International Journal of Advanced Computer Science and Applications",
            "year": 2018
        },
        {
            "authors": [
                "D Adhikari",
                "W Jiang",
                "J Zhan",
                "Z He",
                "DB Rawat",
                "U Aickelin",
                "HA Khorshidi"
            ],
            "title": "A comprehensive survey on imputation of missing data in internet of things",
            "year": 2022
        },
        {
            "authors": [
                "C Aggarwal Charu",
                "K Reddy Chandan"
            ],
            "title": "Data clustering: algorithms and applications",
            "year": 2013
        },
        {
            "authors": [
                "A Agrawal",
                "R Chatterjee",
                "C Curino",
                "A Floratou",
                "N Gowdal",
                "M Interlandi",
                "A Jindal",
                "K Karanasos",
                "S Krishnan",
                "B Kroth"
            ],
            "title": "Cloudy with high chance of dbms: A 10-year prediction for enterprise-grade ml",
            "venue": "arXiv preprint arXiv:190900084",
            "year": 2019
        },
        {
            "authors": [
                "HN Akouemo",
                "RJ Povinelli"
            ],
            "title": "Data improving in time series using arx and ann models",
            "venue": "IEEE Transactions on Power Systems",
            "year": 2017
        },
        {
            "authors": [
                "H Alimohammadi",
                "SN Chen"
            ],
            "title": "Performance evaluation of outlier detection techniques in production timeseries: A systematic review and meta-analysis",
            "venue": "Expert Systems with Applications",
            "year": 2022
        },
        {
            "authors": [
                "H Alsolai",
                "M Roper"
            ],
            "title": "A systematic literature review of machine learning techniques for software maintainability prediction",
            "venue": "Information and Software Technology 119:106214,",
            "year": 2020
        },
        {
            "authors": [
                "D AP"
            ],
            "title": "Upper and lower probabilities induced by a multivalued mapping",
            "venue": "The Annals of Mathematical Statistics",
            "year": 1967
        },
        {
            "authors": [
                "D Araci"
            ],
            "title": "Finbert: Financial sentiment analysis with pre-trained language models",
            "venue": "arXiv preprint arXiv:190810063",
            "year": 2019
        },
        {
            "authors": [
                "M Ataeyan",
                "N Daneshpour"
            ],
            "title": "A novel data repairing approach based on constraints and ensemble learning. Expert Systems with Applications",
            "year": 2020
        },
        {
            "authors": [
                "G Atkinson",
                "V Metsis"
            ],
            "title": "Identifying label noise in time-series datasets",
            "venue": "Adjunct Proceedings of the 2020 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2020 ACM International Symposium on Wearable Computers,",
            "year": 2020
        },
        {
            "authors": [
                "G Atkinson",
                "V Metsis"
            ],
            "title": "Tsar: a time series assisted relabeling tool for reducing label noise. In: The 14th PErvasive Technologies Related to Assistive Environments",
            "year": 2021
        },
        {
            "authors": [
                "MI Azeem",
                "F Palomba",
                "L Shi",
                "Q Wang"
            ],
            "title": "Machine learning techniques for code smell detection: A systematic literature review and metaanalysis. Information and Software Technology 108:115\u2013138, DOI https:// doi.org/10.1016/j.infsof.2018.12.009, URL https://www.sciencedirect",
            "year": 2019
        },
        {
            "authors": [
                "SH Bach",
                "M Broecheler",
                "B Huang",
                "L Getoor"
            ],
            "title": "Hinge-loss markov random fields and probabilistic soft logic",
            "venue": "Journal of Machine Learning Research",
            "year": 2017
        },
        {
            "authors": [
                "C Badue",
                "R Guidolini",
                "RV Carneiro",
                "P Azevedo",
                "VB Cardoso",
                "A Forechi",
                "L Jesus",
                "R Berriel",
                "TM Paixao",
                "F Mutz"
            ],
            "title": "Self-driving cars: A survey",
            "venue": "Expert Systems with Applications",
            "year": 2021
        },
        {
            "authors": [
                "P Bagherzadeh",
                "H Sadoghi Yazdi"
            ],
            "title": "Label denoising based on bayesian aggregation",
            "venue": "International Journal of Machine Learning and Cybernetics",
            "year": 2017
        },
        {
            "authors": [
                "N Barlaug",
                "JA Gulla"
            ],
            "title": "Neural networks for entity matching: A survey",
            "venue": "ACM Transactions on Knowledge Discovery from Data",
            "year": 2021
        },
        {
            "authors": [
                "I Beltagy",
                "K Lo",
                "Cohan"
            ],
            "title": "A (2019) Scibert: A pretrained language model for scientific text",
            "year": 1903
        },
        {
            "authors": [
                "I Ben-Gal"
            ],
            "title": "Outlier detection in: Data mining and knowledge discovery handbook: A complete guide for practitioners and researchers",
            "year": 2005
        },
        {
            "authors": [
                "J Bergstra",
                "D Yamins",
                "DD Cox"
            ],
            "title": "Hyperopt: A python library",
            "year": 2013
        },
        {
            "authors": [
                "P Bojanowski",
                "E Grave",
                "A Joulin",
                "T Mikolov"
            ],
            "title": "Enriching word vectors",
            "venue": "ference on Data Engineering (ICDE),",
            "year": 2017
        },
        {
            "authors": [
                "U Brunner",
                "K Stockinger"
            ],
            "title": "Entity matching with transformer",
            "year": 2020
        },
        {
            "authors": [
                "GA OpenProceedings Carpenter",
                "S Grossberg",
                "DB Rosen"
            ],
            "title": "Fuzzy art: Fast stable learning",
            "year": 1991
        },
        {
            "authors": [
                "M Cespedes",
                "S Yuan",
                "C Tar"
            ],
            "title": "Universal sentence encoder",
            "year": 2018
        },
        {
            "authors": [
                "C Chai",
                "J Wang",
                "Y Luo",
                "Z Niu",
                "G Li"
            ],
            "title": "Data management for machine",
            "year": 2022
        },
        {
            "authors": [
                "N Joseph",
                "G Brockman"
            ],
            "title": "Evaluating large language models",
            "year": 2021
        },
        {
            "authors": [
                "T Chen",
                "S Kornblith",
                "M Norouzi",
                "G Hinton"
            ],
            "title": "A simple framework",
            "year": 2020
        },
        {
            "authors": [
                "K Cheng",
                "X Li",
                "YE Xu",
                "XL Dong",
                "Y Sun"
            ],
            "title": "Pge: Robust product graph",
            "year": 2022
        },
        {
            "authors": [
                "V Christophides",
                "V Efthymiou",
                "T Palpanas",
                "G Papadakis",
                "K Stefanidis"
            ],
            "title": "An overview of end-to-end entity resolution for big data",
            "venue": "ACM Computing Surveys",
            "year": 2020
        },
        {
            "authors": [
                "X Chu",
                "IF Ilyas",
                "S Krishnan",
                "J Wang"
            ],
            "title": "2016a) Data cleaning: Overview and emerging challenges",
            "venue": "Proceedings of the 2016 International Conference on Management of Data, Association for Computing Machinery,",
            "year": 2016
        },
        {
            "authors": [
                "X Chu",
                "IF Ilyas",
                "S Krishnan",
                "J Wang"
            ],
            "title": "2016b) Data cleaning: Overview and emerging challenges",
            "venue": "Proceedings of the 2016 international conference on management of data,",
            "year": 2016
        },
        {
            "authors": [
                "PO C\u00f4t\u00e9",
                "A Nikanjam",
                "R Bouchoucha",
                "I Basta",
                "M Abidi",
                "F Khomh"
            ],
            "title": "Quality issues in machine learning software systems",
            "year": 2023
        },
        {
            "authors": [
                "S Das",
                "A Doan",
                "PS G C",
                "C Gokhale",
                "P Konda",
                "Y Govind",
                "D Paulsen"
            ],
            "title": "The magellan data repository. https://sites.google.com/site/ anhaidgroup/projects/data",
            "year": 2016
        },
        {
            "authors": [
                "AP Dempster"
            ],
            "title": "Upper and lower probabilities induced by a multivalued mapping. Classic works of the Dempster-Shafer theory of belief functions",
            "year": 2008
        },
        {
            "authors": [
                "D Deng",
                "RC Fernandez",
                "Z Abedjan",
                "S Wang",
                "M Stonebraker",
                "AK Elmagarmid",
                "IF Ilyas",
                "S Madden",
                "M Ouzzani",
                "N Tang"
            ],
            "title": "The data civilizer system",
            "venue": "Cidr",
            "year": 2017
        },
        {
            "authors": [
                "J Deng",
                "W Dong",
                "R Socher",
                "LJ Li",
                "K Li",
                "L Fei-Fei"
            ],
            "title": "Imagenet: A largescale hierarchical image database",
            "venue": "IEEE conference on computer vision and pattern recognition,",
            "year": 2009
        },
        {
            "authors": [
                "M Dolatshah",
                "M Teoh",
                "J Wang",
                "J Pei"
            ],
            "title": "Cleaning crowdsourced labels using oracles for supervised learning",
            "year": 2018
        },
        {
            "authors": [
                "R Domingues",
                "M Filippone",
                "P Michiardi",
                "J Zouaoui"
            ],
            "title": "A comparative evaluation of outlier detection algorithms: Experiments and analyses. Pattern recognition",
            "year": 2018
        },
        {
            "authors": [
                "XL Dong",
                "T Rekatsinas"
            ],
            "title": "Data integration and machine learning: A natural synergy",
            "venue": "Proceedings of the 2018 international conference on management of data,",
            "year": 2018
        },
        {
            "authors": [
                "A Dosovitskiy",
                "L Beyer",
                "A Kolesnikov",
                "D Weissenborn",
                "X Zhai",
                "T Unterthiner",
                "M Dehghani",
                "M Minderer",
                "G Heigold",
                "S Gelly"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition",
            "year": 2020
        },
        {
            "authors": [
                "M Ebraheem",
                "S Thirumuruganathan",
                "S Joty",
                "M Ouzzani",
                "N Tang"
            ],
            "title": "Distributed representations of tuples for entity resolution",
            "venue": "Proceedings of the VLDB Endowment",
            "year": 2018
        },
        {
            "authors": [
                "R Feldt",
                "A Magazinius"
            ],
            "title": "Validity threats in empirical software engineering research-an initial survey",
            "year": 2010
        },
        {
            "authors": [
                "W Feng",
                "Y Long",
                "S Wang",
                "Y Quan"
            ],
            "title": "A review of addressing class noise problems of remote sensing classification",
            "venue": "Journal of Systems Engineering and Electronics 34(1):36\u201346,",
            "year": 2023
        },
        {
            "authors": [
                "M Filippone",
                "G Sanguinetti"
            ],
            "title": "Information theoretic novelty detection. Pattern Recognition",
            "year": 2010
        },
        {
            "authors": [
                "L Flokas",
                "W Wu",
                "Y Liu",
                "J Wang",
                "N Verma",
                "E Wu"
            ],
            "title": "Complaint-driven training data debugging at interactive speeds",
            "venue": "Proceedings of the 2022 International Conference on Management of Data,",
            "year": 2022
        },
        {
            "authors": [
                "TL Fox",
                "CS Guynes",
                "VR Prybutok",
                "J Windsor"
            ],
            "title": "Maintaining quality in information systems",
            "venue": "Journal of Computer Information Systems",
            "year": 1999
        },
        {
            "authors": [
                "Y Freund",
                "HS Seung",
                "E Shamir",
                "N Tishby"
            ],
            "title": "Selective sampling using the query by committee algorithm. Machine learning",
            "year": 1997
        },
        {
            "authors": [
                "C Fu",
                "X Han",
                "J He",
                "L Sun"
            ],
            "title": "Hierarchical matching network for heterogeneous entity resolution",
            "venue": "Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Y Gal"
            ],
            "title": "Uncertainty in deep learning",
            "year": 2016
        },
        {
            "authors": [
                "Y Gal",
                "Z Ghahramani"
            ],
            "title": "Dropout as a bayesian approximation: Representing model uncertainty in deep learning",
            "venue": "DOI 10.48550/ARXIV.1506.02142,",
            "year": 2015
        },
        {
            "authors": [
                "K Gauen",
                "R Dailey",
                "J Laiman",
                "Y Zi",
                "N Asokan",
                "YH Lu",
                "GK Thiruvathukal",
                "ML Shyu",
                "SC Chen"
            ],
            "title": "Comparison of visual datasets for machine learning",
            "venue": "IEEE International Conference on Information Reuse and Integration (IRI),",
            "year": 2017
        },
        {
            "authors": [
                "C Ge",
                "Y Gao",
                "X Miao",
                "B Yao",
                "H Wang"
            ],
            "title": "A hybrid data cleaning framework using markov logic networks",
            "venue": "IEEE Transactions on Knowledge and Data Engineering",
            "year": 2020
        },
        {
            "authors": [
                "I Gemp",
                "G Theocharous",
                "M Ghavamzadeh"
            ],
            "title": "Automated data cleansing through meta-learning",
            "year": 2017
        },
        {
            "authors": [
                "I Goodfellow",
                "Y Bengio",
                "Courville"
            ],
            "title": "A (2016) Deep Learning",
            "year": 2016
        },
        {
            "authors": [
                "IJ Goodfellow",
                "J Pouget-Abadie",
                "M Mirza",
                "B Xu",
                "D Warde-Farley",
                "S Ozair",
                "A Courville",
                "Y Bengio"
            ],
            "title": "Generative adversarial networks",
            "year": 2014
        },
        {
            "authors": [
                "RD Gottapu",
                "C Dagli",
                "B Ali"
            ],
            "title": "Entity resolution using convolutional neural network",
            "venue": "Procedia Computer Science",
            "year": 2016
        },
        {
            "authors": [
                "JB Grill",
                "F Strub",
                "F Altch\u00e9",
                "C Tallec",
                "P Richemond",
                "E Buchatskaya",
                "C Doersch",
                "B Avila Pires",
                "Z Guo",
                "M Gheshlaghi Azar",
                "B Piot",
                "k kavukcuoglu",
                "R Munos",
                "M Valko"
            ],
            "title": "Bootstrap your own latent - a new approach",
            "year": 2020
        },
        {
            "authors": [
                "H Guan",
                "Y Zhang",
                "M Xian",
                "HD Cheng",
                "X Tang"
            ],
            "title": "Wenn for individualized cleaning in imbalanced data",
            "venue": "23rd International Conference on Pattern Recognition (ICPR),",
            "year": 2016
        },
        {
            "authors": [
                "G Guo",
                "D Adjeroh",
                "X Li"
            ],
            "title": "Automated cleaning of identity label noise in a large-scale face dataset using a face image quality control",
            "year": 2018
        },
        {
            "authors": [
                "Z Guo",
                "T Rekatsinas"
            ],
            "title": "Learning functional dependencies with sparse regression",
            "year": 2019
        },
        {
            "authors": [
                "B Han",
                "Q Yao",
                "X Yu",
                "G Niu",
                "M Xu",
                "W Hu",
                "I Tsang",
                "M Sugiyama"
            ],
            "title": "Co-teaching: Robust training of deep neural networks with extremely noisy labels",
            "venue": "DOI 10.48550/ARXIV.1804.06872,",
            "year": 2018
        },
        {
            "authors": [
                "S Hara",
                "A Nitanda",
                "T Maehara"
            ],
            "title": "Data cleansing for models trained with sgd",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2019
        },
        {
            "authors": [
                "X He",
                "K Zhao",
                "X Chu"
            ],
            "title": "2021a) Automl: A survey of the state-of-the-art. Knowledge-Based Systems",
            "year": 2021
        },
        {
            "authors": [
                "Y He"
            ],
            "title": "Automatic detection of grammatical errors in english verbs based on rnn algorithm: Auxiliary objectives for neural error detection models",
            "venue": "Computational Intelligence and Neuroscience",
            "year": 2021
        },
        {
            "authors": [
                "A Heidari",
                "J McGrath",
                "IF Ilyas",
                "T Rekatsinas"
            ],
            "title": "Holodetect: Few-shot learning for error detection",
            "venue": "Proceedings of the 2019 International Conference on Management of Data,",
            "year": 2019
        },
        {
            "authors": [
                "D Hendrycks",
                "K Gimpel"
            ],
            "title": "A baseline for detecting misclassified and outof-distribution examples in neural networks DOI 10.48550/ARXIV.1610",
            "year": 2016
        },
        {
            "authors": [
                "A Hern\u00e1ndez-Garc\u00eda",
                "P K\u00f6nig"
            ],
            "title": "Data augmentation instead of explicit regularization",
            "year": 2018
        },
        {
            "authors": [
                "S Hochreiter",
                "J Schmidhuber"
            ],
            "title": "Long short-term memory. Neural computation",
            "year": 1997
        },
        {
            "authors": [
                "J Huang",
                "L Qu",
                "R Jia",
                "B Zhao"
            ],
            "title": "O2u-net: A simple noisy label detection approach for deep neural networks",
            "venue": "Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2019
        },
        {
            "authors": [
                "J Huang",
                "W Hu",
                "Z Bao",
                "Q Chen",
                "Y Qu"
            ],
            "title": "Deep entity matching with adversarial active learning",
            "venue": "The VLDB Journal",
            "year": 2023
        },
        {
            "authors": [
                "Z Huang",
                "X Li",
                "L Deng",
                "K Wei",
                "Y Sui"
            ],
            "title": "Mislabeled samples adjustment based on self-paced learning framework",
            "venue": "7th International Conference on Computer and Communications (ICCC),",
            "year": 2021
        },
        {
            "authors": [
                "V Hurakadli",
                "S Kulkarni",
                "U Patil",
                "R Tabib",
                "U Mudengudi"
            ],
            "title": "Deep learning based radial blur estimation and image enhancement",
            "venue": "IEEE International Conference on Electronics, Computing and Communication Technologies (CONECCT),",
            "year": 2019
        },
        {
            "authors": [
                "P Hwang",
                "Y Kim"
            ],
            "title": "Data cleaning of sound data with label noise",
            "year": 2022
        },
        {
            "authors": [
                "X Chu"
            ],
            "title": "Data Cleaning. Association for Computing",
            "year": 2019
        },
        {
            "authors": [
                "IF books?id=RxieDwAAQBAJ Ilyas",
                "T Rekatsinas"
            ],
            "title": "Machine learning and data cleaning",
            "year": 2022
        },
        {
            "authors": [
                "J Johnson",
                "M Douze",
                "H J\u00e9gou"
            ],
            "title": "Billion-scale similarity search with GPUs",
            "year": 2019
        },
        {
            "authors": [
                "Johnson JM",
                "Khoshgoftaar"
            ],
            "title": "TM (2022) A survey on classifying big data",
            "venue": "IEEE Transactions on Big Data",
            "year": 2022
        },
        {
            "authors": [
                "Z Kang",
                "C Catal",
                "B Tekinerdogan"
            ],
            "title": "Machine learning applications",
            "venue": "Journal of Data and Information Quality",
            "year": 2020
        },
        {
            "authors": [
                "J Kasai",
                "K Qian",
                "S Gurajada",
                "Y Li",
                "L Popa"
            ],
            "title": "Low-resource deep entity",
            "year": 2019
        },
        {
            "authors": [
                "X Ke",
                "J Bai",
                "L Wen",
                "B Cao"
            ],
            "title": "Multi-index dialogue data cleaning model",
            "year": 2019
        },
        {
            "authors": [
                "J Kim",
                "CD Scott"
            ],
            "title": "Robust kernel density estimation",
            "venue": "Intelligence Conference (ITAIC),",
            "year": 2012
        },
        {
            "authors": [
                "JC Klie",
                "B Webber",
                "I Gurevych"
            ],
            "title": "Annotation error detection: Analyzing the past and present for a more coherent future",
            "year": 2022
        },
        {
            "authors": [
                "KM Knill",
                "MJ Gales",
                "P Manakul",
                "A Caines"
            ],
            "title": "Automatic grammatical error detection of non-native spoken learner english",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2019
        },
        {
            "authors": [
                "PW Koh",
                "P Liang"
            ],
            "title": "Understanding black-box predictions via influence functions",
            "venue": "In: International conference on machine learning,",
            "year": 2017
        },
        {
            "authors": [
                "JM K\u00f6hler",
                "M Autenrieth",
                "WH Beluch"
            ],
            "title": "Uncertainty based detection and relabeling of noisy image labels",
            "venue": "CVPR Workshops,",
            "year": 2019
        },
        {
            "authors": [
                "HP Kriegel",
                "P Kr\u00f6ger",
                "E Schubert",
                "A Zimek"
            ],
            "title": "Outlier detection in axisparallel subspaces of high dimensional data. In: Advances in Knowledge Discovery and Data Mining: 13th Pacific-Asia Conference, PAKDD 2009 Bangkok, Thailand",
            "venue": "Proceedings 13,",
            "year": 2009
        },
        {
            "authors": [
                "S Krishnan",
                "E Wu"
            ],
            "title": "Alphaclean: Automatic generation of data cleaning pipelines",
            "venue": "arXiv preprint arXiv:190411827",
            "year": 2019
        },
        {
            "authors": [
                "S Krishnan",
                "J Wang",
                "E Wu",
                "MJ Franklin",
                "K Goldberg"
            ],
            "title": "2016) Activeclean: Interactive data cleaning for statistical modeling",
            "venue": "Proceedings of the VLDB Endowment",
            "year": 2016
        },
        {
            "authors": [
                "S Krishnan",
                "MJ Franklin",
                "K Goldberg",
                "E Wu"
            ],
            "title": "Boostclean: Automated error detection and repair for machine learning",
            "year": 2017
        },
        {
            "authors": [
                "A Krizhevsky",
                "I Sutskever",
                "GE Hinton"
            ],
            "title": "Imagenet classification with deep convolutional neural networks. Communications of the ACM",
            "year": 2017
        },
        {
            "authors": [
                "K Lakshminarayan",
                "SA Harp",
                "T Samad"
            ],
            "title": "Imputation of missing data in industrial databases. Applied intelligence",
            "year": 1999
        },
        {
            "authors": [
                "B Lakshminarayanan",
                "A Pritzel",
                "C Blundell"
            ],
            "title": "Simple and scalable predictive uncertainty estimation using deep ensembles. DOI 10.48550/ARXIV",
            "year": 2016
        },
        {
            "authors": [
                "BE Laure",
                "B Angela",
                "M Tova"
            ],
            "title": "Machine learning to data management: A round trip",
            "venue": "IEEE 34th International Conference on Data Engineering (ICDE),",
            "year": 2018
        },
        {
            "authors": [
                "KH Lee",
                "X He",
                "L Zhang",
                "L Yang"
            ],
            "title": "Cleannet: Transfer learning for scalable image classifier training with label noise",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "A Lew",
                "M Agrawal",
                "D Sontag",
                "V Mansinghka"
            ],
            "title": "Pclean: Bayesian data cleaning at scale with domain-specific probabilistic programming",
            "venue": "In: International Conference on Artificial Intelligence and Statistics,",
            "year": 2021
        },
        {
            "authors": [
                "B Li",
                "W Wang",
                "Y Sun",
                "L Zhang",
                "MA Ali",
                "Y Wang"
            ],
            "title": "2020a) Grapher: tokencentric entity resolution with graph convolutional neural networks",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "P Li",
                "X Rao",
                "J Blase",
                "Y Zhang",
                "X Chu",
                "C Zhang"
            ],
            "title": "Cleanml: A benchmark for joint data cleaning and machine learning [experiments and analysis",
            "year": 2019
        },
        {
            "authors": [
                "Y Li",
                "J Li",
                "Y Suhara",
                "A Doan",
                "WC Tan"
            ],
            "title": "2020b) Deep entity matching with pre-trained language models",
            "year": 2004
        },
        {
            "authors": [
                "Z Li",
                "W Du",
                "N Rao"
            ],
            "title": "Research on error label screening method based on convolutional neural network",
            "venue": "IEEE 6th International Conference on Signal and Image Processing (ICSIP),",
            "year": 2021
        },
        {
            "authors": [
                "WC Lin",
                "CF Tsai"
            ],
            "title": "Missing value imputation: a review and analysis of the literature (2006\u20132017)",
            "venue": "Artificial Intelligence Review",
            "year": 2020
        },
        {
            "authors": [
                "D Liu",
                "Y Meng",
                "L Wang"
            ],
            "title": "Data cleaning of irrelevant images based on transfer learning",
            "venue": "International Conference on Intelligent Computing, Automation and Systems (ICICAS),",
            "year": 2020
        },
        {
            "authors": [
                "Y Liu",
                "Z Li",
                "C Zhou",
                "Y Jiang",
                "J Sun",
                "M Wang",
                "X He"
            ],
            "title": "Generative adversarial active learning for unsupervised outlier detection",
            "venue": "IEEE Transactions on Knowledge and Data Engineering",
            "year": 2019
        },
        {
            "authors": [
                "Z Liu",
                "Z Zhou",
                "T Rekatsinas"
            ],
            "title": "Picket: guarding against corrupted data in tabular data during learning and inference",
            "year": 2022
        },
        {
            "authors": [
                "M Mahdavi",
                "Z Abedjan"
            ],
            "title": "Baran: Effective error correction via a unified context representation and transfer learning",
            "venue": "Proceedings of the VLDB Endowment",
            "year": 2020
        },
        {
            "authors": [
                "M Mahdavi",
                "Z Abedjan"
            ],
            "title": "Semi-supervised data cleaning with raha and baran",
            "year": 2021
        },
        {
            "authors": [
                "M Mahdavi",
                "Z Abedjan",
                "R Castro Fernandez",
                "S Madden",
                "M Ouzzani",
                "M Stonebraker",
                "N Tang"
            ],
            "title": "Raha: A configuration-free error detection system",
            "venue": "Proceedings of the 2019 International Conference on Management of Data,",
            "year": 2019
        },
        {
            "authors": [
                "S Marsland",
                "J Shapiro",
                "U Nehmzow"
            ],
            "title": "A self-organising network that grows when required. Neural networks",
            "year": 2002
        },
        {
            "authors": [
                "R Mauritz",
                "F Nijweide",
                "J Goseling",
                "M van Keulen"
            ],
            "title": "A probabilistic database approach to autoencoder-based data cleaning",
            "year": 2021
        },
        {
            "authors": [
                "C Mayfield",
                "J Neville",
                "S Prabhakar"
            ],
            "title": "Eracer: a database approach for statistical inference and data cleaning",
            "venue": "Proceedings of the 2010 ACM SIGMOD International Conference on Management of data,",
            "year": 2010
        },
        {
            "authors": [
                "M Mazumder",
                "C Banbury",
                "X Yao",
                "B Karla\u0161",
                "WG Rojas",
                "S Diamos",
                "G Diamos",
                "L He",
                "A Parrish",
                "HR Kirk"
            ],
            "title": "Dataperf: Benchmarks for datacentric ai development",
            "year": 2022
        },
        {
            "authors": [
                "VV Meduri",
                "L Popa",
                "P Sen",
                "M Sarwat"
            ],
            "title": "A comprehensive benchmark framework for active learning methods in entity matching",
            "venue": "Proceedings of the 2020 ACM SIGMOD International Conference on Management",
            "year": 2020
        },
        {
            "authors": [
                "Z Miao",
                "Y Li",
                "X Wang"
            ],
            "title": "Rotom: A meta-learned data augmentation framework for entity matching, data cleaning, text classification, and beyond",
            "venue": "Proceedings of the 2021 International Conference on Management of Data,",
            "year": 2021
        },
        {
            "authors": [
                "HJ Motulsky",
                "RE Brown"
            ],
            "title": "Detecting outliers when fitting data with nonlinear regression\u2013a new method based on robust nonlinear regression and the false discovery rate. BMC bioinformatics",
            "year": 2006
        },
        {
            "authors": [
                "S Mudgal",
                "H Li",
                "T Rekatsinas",
                "A Doan",
                "Y Park",
                "G Krishnan",
                "R Deep",
                "E Arcaute",
                "V Raghavendra"
            ],
            "title": "Deep learning for entity matching: A design space exploration",
            "venue": "Proceedings of the 2018 International Conference on Management of Data,",
            "year": 2018
        },
        {
            "authors": [
                "H M\u00fcller",
                "S Castelo",
                "M Qazi",
                "J Freire"
            ],
            "title": "From papers to practice: the openclean open-source data cleaning library",
            "venue": "Proceedings of the VLDB Endowment",
            "year": 2021
        },
        {
            "authors": [
                "A Narayan",
                "I Chami",
                "L Orr",
                "C R\u00e9"
            ],
            "title": "Can foundation models wrangle your data",
            "year": 2022
        },
        {
            "authors": [
                "M Nashaat",
                "A Ghosh",
                "J Miller",
                "S Quader"
            ],
            "title": "Tabreformer: Unsupervised representation learning for erroneous data detection",
            "venue": "ACM/IMS Transactions on Data Science",
            "year": 2021
        },
        {
            "authors": [
                "AB Nassif",
                "MA Talib",
                "Q Nasir"
            ],
            "title": "Dakalbab FM (2021) Machine learning for anomaly detection: A systematic review",
            "venue": "Ieee Access",
            "year": 2021
        },
        {
            "authors": [
                "F Neutatz",
                "M Mahdavi",
                "Z Abedjan"
            ],
            "title": "Ed2: Two-stage active learning for error detection\u2013technical report",
            "year": 2019
        },
        {
            "authors": [
                "F Neutatz",
                "B Chen",
                "Z Abedjan",
                "E Wu"
            ],
            "title": "From cleaning before ml to cleaning for ml",
            "year": 2021
        },
        {
            "authors": [
                "A Ng"
            ],
            "title": "A chat with andrew on mlops: From model-centric to datacentric ai. URL https://www.youtube.com/watch?v=06-AZXmwHjo&amp; ab_channel=DeepLearningAI",
            "year": 2021
        },
        {
            "authors": [
                "A Ng",
                "L He",
                "D Laird"
            ],
            "title": "Data-centric ai competition",
            "venue": "URL https: //https-deeplearning-ai.github.io/data-centric-comp/",
            "year": 2021
        },
        {
            "authors": [
                "H Nie",
                "X Han",
                "B He",
                "L Sun",
                "B Chen",
                "W Zhang",
                "S Wu",
                "H Kong"
            ],
            "title": "Deep sequence-to-sequence entity matching for heterogeneous entity resolution",
            "venue": "Proceedings of the 28th ACM International Conference on Information and Knowledge Management,",
            "year": 2019
        },
        {
            "authors": [
                "CG Northcutt",
                "L Jiang",
                "IL Chuang"
            ],
            "title": "Confident learning: Estimating uncertainty in dataset labels DOI 10.48550/ARXIV.1911.00068, URL https://arxiv.org/abs/1911.00068",
            "year": 2019
        },
        {
            "authors": [
                "PH Oliveira",
                "DS Kaster",
                "IF Ilyas"
            ],
            "title": "Batchwise probabilistic incremental data cleaning",
            "year": 2020
        },
        {
            "authors": [
                "V Papastefanopoulos",
                "P Linardatos",
                "S Kotsiantis"
            ],
            "title": "Unsupervised outlier detection: a meta-learning algorithm based on feature selection",
            "year": 2021
        },
        {
            "authors": [
                "J Pennington",
                "R Socher",
                "CD Manning"
            ],
            "title": "Glove: Global vectors for word representation",
            "venue": "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP),",
            "year": 2014
        },
        {
            "authors": [
                "M Pham",
                "CA Knoblock",
                "M Chen",
                "B Vu",
                "J Pujara"
            ],
            "title": "Spade: A semisupervised probabilistic approach for detecting errors in tables",
            "year": 2021
        },
        {
            "authors": [
                "NN Pise",
                "P Kulkarni"
            ],
            "title": "A survey of semi-supervised learning methods",
            "venue": "International conference on computational intelligence and security, IEEE,",
            "year": 2008
        },
        {
            "authors": [
                "C Pit-Claudel",
                "Z Mariet",
                "R Harding",
                "S Madden"
            ],
            "title": "Outlier detection in heterogeneous datasets using automatic tuple expansion",
            "venue": "Tech. rep., MIT Computer Science and Artificial Intelligence Laboratory",
            "year": 2016
        },
        {
            "authors": [
                "F Ponzio",
                "E Macii",
                "E Ficarra",
                "S Di Cataldo"
            ],
            "title": "W2wnet: a two-module probabilistic convolutional neural network with embedded data cleansing functionality",
            "year": 2021
        },
        {
            "authors": [
                "S Pouyanfar",
                "S Sadiq",
                "Y Yan",
                "H Tian",
                "Y Tao",
                "MP Reyes",
                "ML Shyu",
                "SC Chen",
                "SS Iyengar"
            ],
            "title": "A survey on deep learning: Algorithms, techniques, and applications",
            "year": 2018
        },
        {
            "authors": [
                "G Press"
            ],
            "title": "Cleaning big data: Most time-consuming, least enjoyable data science task, survey says",
            "venue": "URL https://www",
            "year": 2022
        },
        {
            "authors": [
                "K Qian",
                "L Popa",
                "P Sen"
            ],
            "title": "Active learning for large-scale entity resolution",
            "venue": "Proceedings of the 2017 ACM on Conference on Information and Knowledge Management,",
            "year": 2017
        },
        {
            "authors": [
                "E Rahm",
                "HH Do"
            ],
            "title": "Data cleaning: Problems and current approaches",
            "year": 2000
        },
        {
            "authors": [
                "R Razavi-Far",
                "B Cheng",
                "M Saif",
                "M Ahmadi"
            ],
            "title": "Similarity-learning information-fusion schemes for missing data imputation. Knowledge-Based Systems",
            "year": 2020
        },
        {
            "authors": [
                "I Rehbein",
                "J Ruppenhofer"
            ],
            "title": "Detecting annotation noise in automatically labelled data",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2017
        },
        {
            "authors": [
                "M Rei",
                "H Yannakoudakis"
            ],
            "title": "Compositional sequence labeling models for error detection in learner writing. In: Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Association for Computational Linguistics, Berlin, Germany, pp",
            "year": 2016
        },
        {
            "authors": [
                "T Rekatsinas",
                "X Chu",
                "IF Ilyas",
                "C R\u00e9"
            ],
            "title": "Holoclean: Holistic data repairs with probabilistic inference",
            "year": 2017
        },
        {
            "authors": [
                "Y Roh",
                "G Heo",
                "SE Whang"
            ],
            "title": "A survey on data collection for machine learning: a big data-ai integration perspective",
            "venue": "IEEE Transactions on Knowledge and Data Engineering",
            "year": 2019
        },
        {
            "authors": [
                "B Rosner"
            ],
            "title": "Percentage points for a generalized esd many-outlier procedure",
            "year": 1983
        },
        {
            "authors": [
                "M Rottmann",
                "M Reese"
            ],
            "title": "Automated detection of label errors in semantic segmentation datasets via deep learning and uncertainty quantification",
            "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision,",
            "year": 2023
        },
        {
            "authors": [
                "O Russakovsky",
                "J Deng",
                "H Su",
                "J Krause",
                "S Satheesh",
                "S Ma",
                "Z Huang",
                "A Karpathy",
                "A Khosla",
                "M Bernstein"
            ],
            "title": "Imagenet large scale visual recognition challenge. International journal of computer vision",
            "year": 2015
        },
        {
            "authors": [
                "N Sambasivan",
                "S Kapania",
                "H Highfill",
                "D Akrong",
                "P Paritosh",
                "LM Aroyo"
            ],
            "title": "everyone wants to do the model work, not the data work\u201d: Data cascades in high-stakes ai",
            "venue": "In: proceedings of the 2021 CHI Conference on Human Factors in Computing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "EA Santos",
                "JC Campbell",
                "A Hindle",
                "JN Amaral"
            ],
            "title": "Finding and correcting syntax errors using recurrent neural networks. PeerJ PrePrints 5:e3123v1",
            "year": 2017
        },
        {
            "authors": [
                "IH Sarker"
            ],
            "title": "Deep learning: a comprehensive overview on techniques, taxonomy, applications and research directions",
            "venue": "SN Computer Science",
            "year": 2021
        },
        {
            "authors": [
                "B Sch\u00f6lkopf",
                "RC Williamson",
                "A Smola",
                "J Shawe-Taylor",
                "J Platt"
            ],
            "title": "Support vector method for novelty detection. Advances in neural information processing systems",
            "year": 1999
        },
        {
            "authors": [
                "EL Silva-Ram\u00edrez",
                "JF Cabrera-S\u00e1nchez"
            ],
            "title": "Co-active neuro-fuzzy inference system model as single imputation approach for non-monotone pattern of missing data",
            "venue": "Neural Computing and Applications",
            "year": 2021
        },
        {
            "authors": [
                "K Simonyan",
                "A Zisserman"
            ],
            "title": "Very deep convolutional networks for largescale image recognition",
            "year": 2014
        },
        {
            "authors": [
                "L Smyth"
            ],
            "title": "Training-valuenet: A new approach for label cleaning on weakly-supervised datasets",
            "year": 2020
        },
        {
            "authors": [
                "GP Spithourakis",
                "I Augenstein",
                "S Riedel"
            ],
            "title": "Numerically grounded language models for semantic error correction",
            "year": 2016
        },
        {
            "authors": [
                "J Su",
                "X Gao",
                "Y Qin",
                "S Guo"
            ],
            "title": "Correcting corrupted labels using mode dropping of acgan",
            "venue": "15th International Symposium on Medical Information and Communication Technology (ISMICT),",
            "year": 2021
        },
        {
            "authors": [
                "NMS Surameery",
                "MY Shakor"
            ],
            "title": "Use chat gpt to solve programming",
            "venue": "bugs. International Journal of Information Technology & Computer Engineering",
            "year": 2023
        },
        {
            "authors": [
                "K Suzuki",
                "Y Kobayashi",
                "T Narihira"
            ],
            "title": "Data cleansing for deep neural networks with storage-efficient approximation of influence functions",
            "year": 2021
        },
        {
            "authors": [
                "KH Tae",
                "Y Roh",
                "YH Oh",
                "H Kim",
                "SE Whang"
            ],
            "title": "Data cleaning for accurate, fair, and robust models: A big data-ai integration approach",
            "venue": "Proceedings of the 3rd International Workshop on Data Management for End-to-End Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "F Tambon",
                "G Laberge",
                "L An",
                "A Nikanjam",
                "PSN Mindom",
                "Y Pequignot",
                "F Khomh",
                "G Antoniol",
                "E Merlo",
                "F Laviolette"
            ],
            "title": "How to certify machine learning based safety-critical systems? a systematic literature review",
            "venue": "Automated Software Engineering",
            "year": 2022
        },
        {
            "authors": [
                "N Tang",
                "J Fan",
                "F Li",
                "J Tu",
                "X Du",
                "G Li",
                "S Madden",
                "M Ouzzani"
            ],
            "title": "Relational pretrained transformers towards democratizing data preparation [vision",
            "year": 2020
        },
        {
            "authors": [
                "NS Tawfik",
                "MR Spruit"
            ],
            "title": "Evaluating sentence representations for biomedical text: Methods and experimental results. Journal of biomedical informatics",
            "year": 2020
        },
        {
            "authors": [
                "OR Terrades",
                "A Berenguel",
                "D Gil"
            ],
            "title": "A flexible outlier detector based on a topology given by graph communities",
            "venue": "Big Data Research",
            "year": 2022
        },
        {
            "authors": [
                "S Teso",
                "A Bontempelli",
                "F Giunchiglia",
                "A Passerini"
            ],
            "title": "Interactive label cleaning with example-based explanations",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2021
        },
        {
            "authors": [
                "SS Tfwala",
                "YM Wang",
                "YC Lin"
            ],
            "title": "Prediction of missing flow records using multilayer perceptron and coactive neurofuzzy inference system",
            "venue": "The Scientific World Journal",
            "year": 2013
        },
        {
            "authors": [
                "S Thirumuruganathan",
                "N Tang",
                "M Ouzzani",
                "Doan"
            ],
            "title": "A (2020) Data curation with deep learning",
            "year": 2020
        },
        {
            "authors": [
                "F Tonolini",
                "PG Moreno",
                "A Damianou",
                "R Murray-Smith"
            ],
            "title": "Tomographic auto-encoder: Unsupervised bayesian recovery of corrupted data",
            "year": 2020
        },
        {
            "authors": [
                "A Vaswani",
                "N Shazeer",
                "N Parmar",
                "J Uszkoreit",
                "L Jones",
                "AN Gomez",
                "\u0141 Kaiser",
                "I Polosukhin"
            ],
            "title": "Attention is all you need. Advances in neural information processing systems",
            "year": 2017
        },
        {
            "authors": [
                "A Veit",
                "N Alldrin",
                "G Chechik",
                "I Krasin",
                "A Gupta",
                "S Belongie"
            ],
            "title": "Learning from noisy large-scale datasets with minimal supervision",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "L Visengeriyeva",
                "Z Abedjan"
            ],
            "title": "Metadata-driven error detection",
            "venue": "Proceedings of the 30th International Conference on Scientific and Statistical Database Management,",
            "year": 2018
        },
        {
            "authors": [
                "L Visengeriyeva",
                "A Akbik",
                "M Kaul",
                "T Rabl",
                "V Markl"
            ],
            "title": "Improving data quality by leveraging statistical relational learning",
            "year": 2016
        },
        {
            "authors": [
                "H Wang",
                "MJ Bah",
                "M Hammad"
            ],
            "title": "Progress in outlier detection techniques: A survey",
            "venue": "IEEE Access 7:107964\u2013108000,",
            "year": 2019
        },
        {
            "authors": [
                "Q Wang",
                "Y Tan"
            ],
            "title": "Grammatical error detection with self attention by pairwise training",
            "venue": "International Joint Conference on Neural Networks (IJCNN),",
            "year": 2020
        },
        {
            "authors": [
                "R Wang",
                "Y Li",
                "J Wang"
            ],
            "title": "Sudowoodo: Contrastive self-supervised learning for multi-purpose data integration and preparation",
            "year": 2022
        },
        {
            "authors": [
                "X Wang",
                "C Wang"
            ],
            "title": "Time series data cleaning: A survey",
            "venue": "Ieee Access",
            "year": 2019
        },
        {
            "authors": [
                "Z Wang",
                "B Sisman",
                "H Wei",
                "XL Dong",
                "S Ji"
            ],
            "title": "Cordel: a contrastive deep learning approach for entity linkage",
            "venue": "IEEE International Conference on Data Mining (ICDM),",
            "year": 2020
        },
        {
            "authors": [
                "J Wei",
                "K Zou"
            ],
            "title": "Eda: Easy data augmentation techniques for boosting performance on text classification",
            "year": 2019
        },
        {
            "authors": [
                "SE Whang",
                "Y Roh",
                "H Song",
                "JG Lee"
            ],
            "title": "Data collection and quality challenges in deep learning: A data-centric ai perspective",
            "year": 2021
        },
        {
            "authors": [
                "SE Whang",
                "Y Roh",
                "H Song",
                "JG Lee"
            ],
            "title": "Data collection and quality challenges in deep learning: A data-centric ai perspective",
            "venue": "The VLDB Journal",
            "year": 2023
        },
        {
            "authors": [
                "J White",
                "Q Fu",
                "S Hays",
                "M Sandborn",
                "C Olea",
                "H Gilbert",
                "A Elnashar",
                "J SpencerSmith",
                "Schmidt"
            ],
            "title": "DC (2023) A prompt pattern catalog to enhance prompt engineering with chatgpt",
            "year": 2023
        },
        {
            "authors": [
                "C Wohlin"
            ],
            "title": "Guidelines for snowballing in systematic literature studies and a replication in software engineering",
            "venue": "Proceedings of the 18th international conference on evaluation and assessment in software engineering,",
            "year": 2014
        },
        {
            "authors": [
                "R Wu",
                "S Chaba",
                "S Sawlani",
                "X Chu",
                "S Thirumuruganathan"
            ],
            "title": "Zeroer: Entity resolution using zero labeled examples",
            "venue": "Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data,",
            "year": 2020
        },
        {
            "authors": [
                "Y Wu",
                "J Weimer",
                "SB Davidson"
            ],
            "title": "Chef: a cheap and fast pipeline for iteratively cleaning label uncertainties (technical report)",
            "year": 2021
        },
        {
            "authors": [
                "S Xiang",
                "X Ye",
                "J Xia",
                "J Wu",
                "Y Chen",
                "S Liu"
            ],
            "title": "Interactive correction of mislabeled training data",
            "venue": "IEEE Conference on Visual Analytics Science and Technology (VAST),",
            "year": 2019
        },
        {
            "authors": [
                "Q Yu",
                "K Aizawa"
            ],
            "title": "Unknown class label cleaning for learning with open-set noisy labels",
            "venue": "IEEE International Conference on Image Processing (ICIP),",
            "year": 2020
        },
        {
            "authors": [
                "D Zha",
                "ZP Bhat",
                "KH Lai",
                "F Yang",
                "Z Jiang",
                "S Zhong",
                "X Hu"
            ],
            "title": "Data-centric artificial intelligence: A survey",
            "year": 2023
        },
        {
            "authors": [
                "A Zhang",
                "S Song",
                "J Wang",
                "PS Yu"
            ],
            "title": "2020a) Time series data cleaning: From anomaly detection to anomaly repairing (technical report)",
            "year": 2003
        },
        {
            "authors": [
                "H Zhang",
                "M Cisse",
                "YN Dauphin",
                "D Lopez-Paz"
            ],
            "title": "mixup: Beyond empirical risk minimization",
            "year": 2017
        },
        {
            "authors": [
                "W Zhang",
                "X Tan"
            ],
            "title": "Combining outlier detection and reconstruction error minimization for label noise reduction",
            "venue": "IEEE International Conference on Big Data and Smart Computing (BigComp),",
            "year": 2019
        },
        {
            "authors": [
                "W Zhang",
                "H Wei",
                "B Sisman",
                "XL Dong",
                "C Faloutsos",
                "D Page"
            ],
            "title": "2020b) Autoblock: A hands-off blocking framework for entity matching",
            "venue": "Proceedings of the 13th International Conference on Web Search and Data Mining,",
            "year": 2020
        },
        {
            "authors": [
                "X Zhang",
                "Y Ji",
                "C Nguyen",
                "T Wang"
            ],
            "title": "2018a) Deepclean: data cleaning via question asking",
            "venue": "IEEE 5th International Conference on Data Science and Advanced Analytics (DSAA),",
            "year": 2018
        },
        {
            "authors": [
                "X Zhang",
                "X Zhu",
                "S Wright"
            ],
            "title": "Training set debugging using trusted items",
            "venue": "Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "C Zhao",
                "Y He"
            ],
            "title": "Auto-em: End-to-end fuzzy entity-matching using pretrained deep models and transfer learning",
            "venue": "The World Wide Web Conference,",
            "year": 2019
        },
        {
            "authors": [
                "Z Zhao",
                "H Liu"
            ],
            "title": "Spectral feature selection for supervised and unsupervised learning",
            "venue": "Proceedings of the 24th international conference on Machine learning,",
            "year": 2007
        },
        {
            "authors": [
                "X Zhou",
                "Y Jin",
                "H Zhang",
                "S Li",
                "X Huang"
            ],
            "title": "2016) A map of threats to validity of systematic literature reviews in software engineering",
            "venue": "23rd AsiaPacific Software Engineering Conference (APSEC),",
            "year": 2016
        },
        {
            "authors": [
                "X Zhu",
                "Z Ghahramani"
            ],
            "title": "Learning from labeled and unlabeled data with label propagation",
            "venue": "ProQuest Number: INFORMATION TO ALL USERS",
            "year": 2002
        }
    ],
    "sections": [
        {
            "text": "Keywords Machine Learning, Data Cleaning, Systematic Literature Review, Survey, Taxonomy\nThis work is funded by the Fonds de Recherche du Quebec (FRQ), the Canadian Institute for Advanced Research (CIFAR), and the National Science and Engineering Research Council of Canada (NSERC).\n1 Polytechnique Montr\u00e9al, Qu\u00e9bec, Canada E-mail: {pierre-olivier.cote, amin.nikanjam, nafisa.abdelmutalab-ali-ahmed@polymtl.ca, dmytro.humeniuk@polymtl.ca, foutse.khomh}@polymtl.ca\nar X\niv :2\n31 0.\n01 76\n5v 1\n[ cs\n.L G\n] 3\nO ct\n2 02"
        },
        {
            "heading": "1 Introduction",
            "text": "Nowadays, Machine Learning (ML) integrates a growing number of industries, from transportation to healthcare and education. Recent applications of ML achieved performances similar to humans\u2019 performance on complex tasks, such as taking the bar exam (OpenAI, 2023) or driving a car (Badue et al., 2021; Gitnux, 2023). Behind much of the recent success of these applications are large amounts of training data and powerful computing infrastructure (Roh et al., 2019; Whang et al., 2021). While a major part of the research in ML is spent on developing better modeling techniques (Ng, 2021), data preparation often is the most arduous and time-consuming task for practitioners. Indeed, researchers have reported that data scientists sometimes spend over 80% of their time preparing data (Whang et al., 2021; Neutatz et al., 2021; Deng et al., 2017; Agrawal et al., 2019). As a consequence, there is an emerging trend, referred to as Data-Centric AI (DCAI) to steer research to focus on improving datasets instead of models for ML problems. In the last few years, various initiatives aimed at stimulating involvement in the DCAI trend have been proposed. For example, in August 2021 the first DCAI competition (Ng et al., 2021) was organized. During this competition, participants were challenged to improve a model\u2019s performance by only modifying the dataset it was trained on. Following this competition, a group of researchers released the DataPerf benchmark (Mazumder et al., 2022), a suite of data-centric benchmarks for different data tasks such as data cleaning and data debugging.\nAmongst the different steps forming data preparation is a central task that consists of detecting and removing data errors, namely data cleaning (Zha et al., 2023). Previous studies have often overlooked it assuming that the datasets used in their experiments are devoid of errors. However, as reported by a previous study (Northcutt et al., 2019), large public datasets considered for a long time to be error-free such as ImageNet (Russakovsky et al., 2015) contain erroneous labels. As a consequence, there is an increasing interest in developing data-cleaning approaches to improve machine learning performance. Data cleaning is a problem tackled by the database community for a long time (Rahm et al., 2000; Fox et al., 1999; Mayfield et al., 2010) and, recently, researchers started studying how ML can be leveraged to clean data.\nIn this study, we summarize the latest approaches in Data Cleaning for ML (DC4ML) and ML for Data Cleaning (ML4DC). For simplicity, we refer to the union of DC4ML and ML4DC as data cleaning and ML (DC&ML). To reach that goal, we adopt the Systematic Literature Review (SLR) approach, which differs from traditional literature reviews by its strict and rigorous methodology. The SLR methodology ensures that the literature review is exhaustive and devoid of biases when collecting and summarizing information (Kitchenham, 2004). With the increasing amount of work published on DC&ML (see Section 4), we believe that there is a need to summarize the latest approaches in the field. A replication package of our study is available on our public GitHub\nrepository1. In total, we summarize the content of 101 papers and provide 24 future work directions. Our review can serve as a basis for researchers and practitioners to understand state-of-the-art DC&ML approaches and to contribute to the field.\nThe rest of the paper is organized as follows. In Section 2, the methodology followed is described. Then, in Section 3, the taxonomy selected to structure our review of the literature is presented. Next, statistics about the papers included in our study are detailed in Section 4. The review of the literature is presented in Section 5 and future research directions are proposed in Section 6. Finally, we reflect on our work and discuss threats to validity in Section 8. The conclusion of the paper is presented in Section 9."
        },
        {
            "heading": "2 Methodology",
            "text": "In this section, we describe the methodology we followed to conduct our study. We provide an outline of it in Figure 1. Our methodology is divided into three phases: (1) paper collection, (2) paper selection, and (3) results analysis. We collected papers on 17 October 2022. For each step in the figure, we indicate (1) the section where this step is explained (in the lower left corner of the box) and (2) the number of papers left after the step (in the lower right corner of the box). To conduct our study, we followed the guidelines defined by\n1 https://github.com/poclecoqq/SLR-datacleaning\nKitchenham (2004) for systematic literature reviews. This document describes best practices for conducting systematic reviews in software engineering and is based on existing guidelines from medical research, where empirical research methods are mature and rigorous.\n2.1 Research Questions\nAs we mentioned earlier, the goal of this study is to conduct a systematic review of DC&ML. Thus, we define the following research questions.\nRQ1: What are the latest data cleaning techniques in DC&ML? As mentioned before, a significant portion of practitioners\u2019 time when building machine learning systems is spent on data. In parallel, ML has made significant progress in the last ten years and is applied to a growing number of applications. Applying ML to data cleaning has the potential of reducing the amount of time spent on data by practitioners, as well as improving the accuracy of data cleaning processes. Conversely, data cleaning can play an important role in improving a model\u2019s performance. By answering this RQ, we provide researchers with an understanding of the current data cleaning approaches in DC&ML. RQ2: What are future research opportunities in data cleaning and ML? In this RQ, our objective is to identify and highlight promising research directions in the field of data cleaning with machine learning.\n2.2 Scope\nFor a long time, data cleaning was mainly applied to tabular data (i.e., data stored in relational databases), and surveys on data cleaning focused primarily on this type of data (Chu et al., 2016a). Nowadays, data cleaning techniques for various data types such as images (Ponzio et al., 2021), text (Klie et al., 2022), graph data (Cheng et al., 2022), and sound (Hwang and Kim, 2022) are developed. The strategy used to clean data in these techniques may significantly vary from one data type to another because of the particularities of each data type. In our survey, we focused on the data types that are the most commonly processed by ML applications in practice. We used comprehensive surveys and books from the ML community to identify them (Sarker, 2021; Pouyanfar et al., 2018; Goodfellow et al., 2016; C\u00f4t\u00e9 et al., 2023). Hence, we limited ourselves to tabular, image, and text data.\n2.3 Search Terms Selection\nAs one of the first steps of the SLR, we conducted a preliminary search to help us identify the most relevant keywords to our study, as recommended by Kitchenham (2004). Hence, we read existing survey of the literature (Ilyas\nand Chu, 2019; Neutatz et al., 2021; Ilyas and Rekatsinas, 2022; Thirumuruganathan et al., 2020; Roh et al., 2019; Whang et al., 2023; Chai et al., 2022) and highly cited works (e.g., Rekatsinas et al. (2017); Krishnan et al. (2017)). Then, we defined a comprehensive search query to gather the relevant literature from academic databases. Our final search query is described in Figure 2. In the following, we describe how we selected keywords based on the main topics of our review: \u2018ML\u2019 and \u2018data cleaning\u2019. Then, we explain how we adapted our query for one of the academic databases we used, Google Scholar (GS). Contrary to other databases, GS had peculiarities that forced us to modify our search query.\n\u2013 Machine learning : ML is a field devoted to enabling machines to learn from data (Wikipedia, 2023a). We leveraged previous SLRs (Tambon et al., 2022; Kang et al., 2020; Azeem et al., 2019) on different topics in ML to build an initial list of keywords for ML. Our final list of terms is: machine learning, deep learning, neural network, reinforcement learning, supervised and unsupervised. \u2013 Data cleaning : Data cleaning refers to the activity of detecting and repairing errors in data (Ilyas and Chu, 2019). We used the term data cleaning and its synonyms data cleansing, data scrubbing to conduct our searches. Additionally, we added keywords for both processes of data cleaning: error detection and error repair. For the former one, we used the keywords: data repairing, data repair, error repairing, error repair. For the latter one, we only used one keyword: error detection. After running the search query with the aforementioned keywords, we initially obtained a lot of irrelevant papers in the results. The reason is that error detection is a very broad topic that is not only limited to the detection of errors in data2 Thus, in an attempt to filter out studies that do not propose an approach to detect data errors, we limited the results to papers that are also mentioning a data type. As mentioned in Section 2.2, we are focusing on three common data types: tabular, text, and image data. Hence, we used the following\n2 For example, \"A modular edge-/cloud-solution for automated error detection of industrial hairpin weldings using convolutional neural networks\" is a paper that was included in the results but not relevant to our study since it is not a data cleaning approach.\nterms to limit the papers on error detection: tab*, cell*, row*, image* and text*. Finally, because we are interested in data cleaning for ML (and ML datasets have labels), we added the following keywords: label cleaning and confident learning."
        },
        {
            "heading": "2.3.1 Adaptations for Google Scholar",
            "text": "As opposed to other academic databases, GS has many peculiarities that require us to adapt our search pattern. Our final search pattern can be visualized in Figure 3. Below, we describe GS\u2019s peculiarities and explain how we adapted the query.\n1. All queries are truncated to 256 characters. 2. Parentheses are not a priority operator. 3. OR operators have a higher precedence than AND. 4. AND operators are implicitly added between terms that are not opera-\ntors (e.g., terms that are not OR or AND). Hence, the search query data cleaning is equivalent to data AND cleaning.\nIn order to address the first constraint, we split the query in two and ran each part separately. For the same reason, we also removed any AND operators, since they are redundant and added automatically (constraint 4). As parenthesis do not have their usual effect (constraint 2), we also removed them from our query. This prevents any nested Boolean operator. Hence, we removed the conjunction we described in Section 2.3 to limit the number of results on error detection.\n2.4 Paper Collection\nIn the following, we describe how we collected papers from academic databases using the query described in Subsection 2.3. Then, we describe the separate paper collection process we followed for GS, because of the limitations of that database. Similarly to the other works (Tambon et al., 2022; Kang et al., 2020; Alsolai and Roper, 2020), we conducted the search in the following academic databases:\n\u2013 Google Scholar3 \u2013 Engineering Village4 \u2013 Web of Science5 \u2013 Science Direct6 \u2013 ACM Digital Library7\n3 https://scholar.google.com/ 4 https://www.engineeringvillage.com 5 https://webofknowledge.com 6 https://www.sciencedirect.com 7 https://dl.acm.org\n\u2013 IEEE Xplore8\nAs shown by the recent publications (Neutatz et al., 2021; Ilyas and Rekatsinas, 2022), DC&ML is a relatively new topic. Hence, when running our search query, we limited the results to papers published between 1 January 2016 and 17 October 2022 (the later date being when we ran the query). On Engineering Village, we used the duplicate removal feature, by keeping the results from Compendex over Inspec."
        },
        {
            "heading": "2.4.1 Adaptations for Google Scholar",
            "text": "Unlike other databases, GS does not provide a way to export records in bulk, which discourages mining techniques. Thus, to alleviate the task of collecting paper, we used the tool \u201cPublish or Perish\u201d9. Similarly to Tambon et al. (2022), we searched 1000 records for each year. As mentioned in Section 2.3.1, our initial query had to be divided into two parts because of GS\u2019s limitations. Thus, we separately ran both queries, for every year for a period of six years, and collected 1000 papers per year, for a total of 12 000 papers (2 queries \u00d7 6 years \u00d7 1000 papers = 12 000 papers).\n2.5 Paper Selection\nIn this section, we explain the steps we followed to filter out irrelevant results. Table 1 shows the number of papers obtained from each academic database prior to any kind of filtering. We provide the list of papers retained after each step of the paper selection process in our replication package10."
        },
        {
            "heading": "2.5.1 Duplicate Filtering",
            "text": "We uploaded all the titles of papers obtained from the various databases in Zotero11. We leveraged the duplicate removal functionality of the tool and 8 https://ieeexplore.ieee.org 9 Harzing, A.W. (2007) Publish or Perish, available from\nhttps://harzing.com/resources/publish-or-perish 10 https://github.com/poclecoqq/SLR-datacleaning 11 https://www.zotero.org/\nfound 3,742 duplicates in GS\u2019 papers and 769 in the results of the other databases. Thus, after this step, 8,258 papers from GS were retained, and 1,485 for the other databases."
        },
        {
            "heading": "2.5.2 Google Scholar Pruning",
            "text": "While GS returned a lot of results, a considerable amount of them are not relevant to our study, for two reasons. First, a lot of irrelevant papers are included because of the search engine\u2019s limitation which we describe in Section 2.3. Second, GS only searches keywords in the full text of the paper or in the title. Thus, papers irrelevant to our study that mentioned data cleaning and ML in the papers\u2019 body (e.g., as key/index works) matched our search query. While we could not address the first issue in any other way than manually inspecting and then filtering papers, we devised an approach for the second issue. We managed to only keep papers whose title, abstract, and author keywords matched the search query, similar to what was done by a similar study (Tambon et al., 2022). We did so using a Python script we implemented which is available in our replication package12. After this step, 1,862 results from GS remained. We merged this set of papers together with the 1,485 papers coming from the other databases. We removed duplicated papers from the merged set, resulting in a total of 2,968 papers."
        },
        {
            "heading": "2.5.3 Inclusion and Exclusion Criteria",
            "text": "From this step onward, papers were filtered manually. One author went through the whole set of papers and filtered out the ones irrelevant to our study. A second author verified the resulting set of papers. Any leftover duplicate was removed. For this step, the following inclusion and exclusion criteria were considered:\nInclusion criteria:\n\u2013 Papers proposing an approach to detect or fix errors in an ML dataset. \u2013 Papers presenting an approach to detect or fix errors in data while lever-\naging ML techniques.\n12 https://github.com/poclecoqq/SLR-datacleaning\nExclusion criteria:\n\u2013 Papers whose data cleaning approach is specific to their problem (i.e., the technique does not generalize). \u2013 Papers on data cleaning for other data types than tabular, image, or text data, such as sound data. \u2013 Papers on detecting or fixing any type of attacks on an ML model through data (i.e., data poisoning and adversarial examples). \u2013 Papers that can not be accessed free of charge through our institution\u2019s subscriptions. \u2013 Secondary works (e.g., surveys and review papers), research proposal, workshop, letters, undergrad theses, or position papers. \u2013 Papers that are not written in English. \u2013 Papers on text data cleaning for other languages than English.\nAfter this step, 124 papers remained."
        },
        {
            "heading": "2.5.4 Quality Control Assessment",
            "text": "While most papers are relevant to our study, not all of them are able to answer our RQs. Thus, based on a previous study (Tambon et al., 2022), we defined a set of questions to filter out papers with low quality. We also defined a second set of questions to filter out any remaining paper that is irrelevant to our study.\nQuality control questions:\nQ1.1 Is the objective of the research clearly defined? Q1.2 Is the context of research clearly defined? Q1.3 Does the study bring value to academia or industry? Q1.4 Are the findings clearly stated and supported by results? Q1.5 Are limitations explicitly mentioned and analyzed? Q1.6 Is the methodology clearly defined and justified? Q1.7 Is the experiment clearly defined and justified?\nRelevance control questions:\nQ2.1 Does the paper present an approach to detect or fix errors in an ML dataset? Q2.2 Does the paper present an approach to detect or fix errors in data using ML techniques?\nEach paper is considered by a reviewer who assigns it a score based on how well it complies with the questions. Papers under a certain threshold are considered by a second reviewer who repeats the process followed by the first reviewer. If the second reviewer\u2019s score is also under the threshold, the paper under consideration is excluded from the study. However, if the second reviewer assigns a score above the threshold, the reviewers have to discuss to\ndecide whether the paper should be included or not in our study. If they are unable to reach a consensus, a third reviewer makes the final decision.\nIn order to evaluate a paper, reviewers rated on a scale from 0 to 2 how much they agreed with the quality and relevance control questions. For the quality control questions (i.e., Q1.X), a paper is excluded by a reviewer if the sum of the 7 questions is lower than 7. For the relevance control question, a paper is excluded if it has a score of zero for both questions (i.e., Q2.1 and Q2.2).\nAs a result, we excluded 44 papers. Our ratings are available in our replication package13. After this step, 80 papers remained.\n2.6 Snowballing\nSnowballing refers to the act of discovering new papers through paper references (Wohlin, 2014). In this study, to read relevant studies we miss, we include any paper cited by at least two papers from the filtered list of papers from Section 2.5.4. We partially automated this process with a script available in our replication package14. In this script, we used the Python library Scholarly (Cholewiak et al., 2021) to fetch paper citations from GS. We evaluate the inclusion and exclusion criteria from Section 2.5.3 along with the control assessment questions from Section 2.5.4 before including a snowballed paper in our study. After this step, we found 21 new papers. Hence, in total, we collected 101 papers for examination in our study.\n2.7 Data extraction\nIn order to reason efficiently over the large amount of information contained in the set of papers we read, we extracted the following information from each paper:\nGeneral information\n\u2013 Title \u2013 URL to the paper \u2013 Authors \u2013 Year \u2013 Publication venue\nQuestions\n\u2013 What does the data cleaning approach try to fix (e.g. missing values, dirty data, inconsistent data, etc.)? \u2013 What are the datasets used in the experiment?\n13 https://github.com/poclecoqq/SLR-datacleaning 14 https://github.com/poclecoqq/SLR-datacleaning\n\u2013 What are the ML models used in the experiment? \u2013 What are the quality measurements (i.e., how is the data cleaning approach\nevaluated)? \u2013 Against what is the approach compared (e.g., against baselines, other ap-\nproaches, etc.)? \u2013 What is the performance of the approach, in terms of ML performance\n(e.g., accuracy, F1-score, etc.)? \u2013 What is the performance of the approach, in terms of engineering aspects\n(e.g., resource consumption, time to completion, etc.)? \u2013 What are the limitations of the approach?\nAdditionally, reviewers are asked to summarize the papers they have reviewed, in order to give a general picture of the study and to highlight any other interesting information. All extracted data is available in our replication package15."
        },
        {
            "heading": "3 Selected Taxonomy",
            "text": "Based on the papers included in our study and using existing studies (Ilyas and Chu, 2019; Ilyas and Rekatsinas, 2022), we built a taxonomy of DC&ML approaches, which we describe in this section. We use that taxonomy to structure our review of the literature in Section 5. In the taxonomy, illustrated in Figure 4, data cleaning approaches are first grouped based on the type of error they try to address (e.g., incorrect feature value, duplicates, etc.). Bold font is used in the figure to indicate the number of papers included in a category. The number of papers in a category may not be equal to the sum of the number of papers in its subcategories because a paper may be classified to zero or many subcategories of a category it belongs to.\nIn the following, we introduce the different types of data cleaning activities, which correspond to the categories on the first level of the taxonomy in Figure 4. The other categories will be introduced later in their respective subsections during the review (in Section 5).\n\u2013 Feature cleaning: Feature cleaning approaches try to detect or repair errors in the features of a record. It is a problem that has been addressed by the database community for a long time (Rahm et al., 2000; Fox et al., 1999; Mayfield et al., 2010) and is generally loosely referred to as data cleaning. It should be noted that data cleaning is not limited to feature cleaning and that other activities are also considered to be data cleaning activities (e.g., entity matching or outlier detection (Ilyas and Chu, 2019)). Thus, we use the term \u201cfeature cleaning\u201d instead of \u201cdata cleaning\u201d to refer to that data cleaning activity. Feature errors may happen for a large variety of reasons. For example, a noun may be misspelled by the person collecting data (C\u00f4t\u00e9 et al., 2023).\n15 https://github.com/poclecoqq/SLR-datacleaning\n\u2013 Label cleaning: Similar to feature cleaning, label cleaning approaches try to detect or repair label errors. Label errors may happen for a variety of reasons. For example, a record might be mislabeled because of a lack of domain knowledge (Sambasivan et al., 2021). \u2013 Entity matching: Entity matching refers to the process of finding tuples in a database that refer to the same real-world entity (Ilyas and Chu, 2019). Duplicates can be introduced in a dataset for many reasons. For example, a customer could be recorded multiple times in a database because the customer used different names at checkout (Ilyas and Chu, 2019). \u2013 Outlier detection: An outlier can be defined as an observation that deviates so much from the other records in a dataset that it arouses suspicions that it has been generated by a different process (Hawkins, 1980). Outliers may hinder an ML model\u2019s performance if they have been generated by a different process. Hence, outliers may be purged from a dataset during data cleaning. \u2013 Imputation: Imputation refers to assigning a value to a record\u2019s feature whose value is missing (Wikipedia, 2023b). Having missing values is a frequent problem (Alimohammadi and Chen, 2022) and is often experienced when data collection is manual (Lakshminarayan et al., 1999). An ML\nmodel can only ingest complete records; thus records with missing values have to be imputed otherwise they are removed from datasets. \u2013 Holistic data cleaning: We use the term \u201cholistic data cleaning\u201d to refer to the approaches that try to clean more than one type of error at a time. For example, Berti-Equille (2019) designed an RL agent that learns to clean a dataset using a set of data cleaning tools that can clean different types of errors."
        },
        {
            "heading": "4 Statistical results",
            "text": "In this section, we provide a statistical description of the papers included in this study. Figure 5 displays the number of papers included in our study per data cleaning activity (which we previously defined in Section 3). Studies proposing an approach to clean more than one error at a time are in the category\n\u201cMore than one\u201d. Generally, the papers that were included in that category proposed an approach that could be adapted for different cleaning goals (e.g., (Wang et al., 2022)). We can observe three main categories of papers, along with four minor ones. The main categories are \u201cFeature Cleaning\u201d (34%), \u201cLabel Cleaning\" (32%), and \u201cEntity Matching\u201d (18%). The minor categories are \u201cOutlier Detection\u201d (8%), \u201cHolistic\u201d (4%), \u201cImputation\u201d (3%), and \u201cMore than one\u201d (i.e., approaches that individually fix more than one type of error) (2%). Figure 6 shows the distribution of the papers\u2019 publication year. We can observe a generally growing trend. The number of papers in 2022 is less than in 2021 because we collected the papers during this year, in October 2022. In Figure 6, we display the authors\u2019 affiliation distribution. The majority of the papers\nincluded in our study have been written by researchers from academia (74%). We observe that 21% of the papers included in our study have been written by teams composed of a mix of researchers from academia and the industry. Finally, only 5% of the papers are attributed to teams from the industry. In Figure 8, we display the number of papers published per venue. We filtered out venues that counted only one paper for readability purposes. The most recurrent venues are the ACM SIGMOD conference, the Association for the Advancement of Artificial Intelligence conference (AAAI), the Expert Systems with Applications journal, and the VLDB Endowment conference."
        },
        {
            "heading": "5 Review",
            "text": "In this section, we summarize current data cleaning approaches for tabular, text, and image data. We use the taxonomy described in Section 3 to structure our review. Hence, the following subsections discuss about feature cleaning, label cleaning, entity matching, outlier detection, imputation, and holistic data cleaning. We provide a brief description of the existing approaches along with future work recommendations at the end of every subsection.\n5.1 Feature Cleaning\nIn this section, we cover data cleaning approaches to detect or repair feature errors. Because of the long-lasting efforts of the database community to clean tabular data (Rahm et al., 2000; Fox et al., 1999; Mayfield et al., 2010), the majority of the papers in this section process this data type. Thus, the following subsections present feature cleaning approaches for tabular data, except for the last one (Section 5.1.9). In the first one (Section 5.1.1), we present model-based approaches. Then, we follow with ensemble-based (Section 5.1.2), transformer-based (Section 5.1.3), and autoencoder-based approaches (Section 5.1.4), which all are subcategories of model-based approaches. We continue with error prioritization (Section 5.1.5) and data-cleaning rule generation approaches (Section 5.1.6). Then, we present other feature-cleaning approaches that do not belong to any of the aforementioned groups (Section 5.1.7). Finally, we describe some approaches commonly used to improve the data cleaning approaches performance (Section 5.1.8) and we finish with feature cleaning approaches for other data types than tabular (Section 5.1.9)."
        },
        {
            "heading": "5.1.1 Model-Based Approaches",
            "text": "The approaches presented in this category view data cleaning as a prediction problem. That is, a model must predict whether a value is clean or not (i.e., error detection) or what is the clean value of a dirty cell (i.e., error repair). Formally, for error detection, a model m must predict whether a record\u2019s feature value r[Ai] (where r refers to a record and Ai to the i-th attribute) is dirty\nor not (y \u2208 [0, 1]) given the full record r. Similarly, for error repair, a model m must predict the clean value y for a record\u2019s feature r[Ai] given the full record r, where y\u2019s domain is not necessarily restricted. Note that the following sections (i.e., ensemble-based, transformer-based, and autoencoder-based approaches) are also model-based approaches. In the following, we refer to the clean features of a record as records\u2019 labels. Indeed, the clean values are the labels used by the feature cleaning approaches.\nOne of the major differentiating factors between error detection approaches is their choice of feature engineering. We discerned three categories of features: (1) frequency-based features, (2) format-based features, and (3) metadata features. Similar to outlier detection approaches (see Section 5.4), the engineered features are generally designed to highlight the abnormal characteristics of data. A commonly used feature-engineering practice for error detection is to create features that measure how frequent a value is in a dataset (i.e., frequency-based features) (Visengeriyeva and Abedjan, 2018; Neutatz et al., 2019; Heidari et al., 2019; Pham et al., 2021). For example, the authors in (Neutatz et al., 2019) measured the TF-IDF score of n-grams inside a cell to encode how common a cell\u2019s value is. The less common it is, the higher the chances of it being an error. Frequency-based features can be measured using the information contained in other columns (Neutatz et al., 2019; Heidari et al., 2019). For example, Neutatz et al. (2019) used the co-occurrence of values among different attributes to facilitate the error detection process. Another category of engineered features focuses on the format of the values (i.e., format-based features) (Visengeriyeva and Abedjan, 2018; Neutatz et al., 2019; Heidari et al., 2019; Pham et al., 2021). Corrupted cells might not follow the syntactic format that is expected for a feature. For example, a value that represents a name should not have numbers. Pham et al. (2021) replaces all numbers and characters with symbols that are unique to them; thus focusing only on the shape of the data. For example, given the value \u201c400$\u201d, the encoded format could be \u201cnnns\u201d, where \u201cn\u201d represents numbers and \u201cs\u201d, symbols. Finally, the data cleaning approaches sometimes complement their feature set with metadata information (i.e., metadata features) (Visengeriyeva and Abedjan, 2018; Neutatz et al., 2019). For example, Neutatz et al. (2019) indicates the data type and the string length of a cell in the feature set. For error repair, ML can be used to predict values to repair corrupted cells, similar to data imputation (see Section 5.5), as it was done in Ataeyan and Daneshpour (2020)."
        },
        {
            "heading": "5.1.2 Ensemble-Based Approaches",
            "text": "Coined by Neutatz et al. (2021), ensemble-based approaches to data cleaning comprise every approach where a model uses the output of data cleaning tools to clean data. As it was pointed out in Abedjan et al. (2016), no single data cleaning tool is the best in every situation and no tool can detect all errors. Leveraging ensembles of data cleaning tools opens up the possibility of powerful approaches to clean errors since they could potentially detect and\nrepair a larger diversity of errors (Rekatsinas et al., 2017; Visengeriyeva et al., 2016; Mahdavi et al., 2019). Similarly to ensembling techniques with ML models, ensemble-based approaches for feature cleaning have an ensemble of data cleaning tools, which we refer to as the \u201cbase\u201d data cleaning tools. The outputs of the base data cleaning tools are ingested by one singular model making the final data cleaning prediction, which we refer to as the meta-model.\nUsually, the base data cleaning tools are simple cleaning tools developed without using ML techniques. The most commonly used ones are integrity constraints (Rekatsinas et al., 2017; Mahdavi et al., 2019; Visengeriyeva and Abedjan, 2018; Visengeriyeva et al., 2016), matching dependencies (Rekatsinas et al., 2017; Mahdavi et al., 2019), outlier detection (Rekatsinas et al., 2017; Mahdavi et al., 2019; Visengeriyeva and Abedjan, 2018), and pattern violation detection (Mahdavi et al., 2019; Visengeriyeva and Abedjan, 2018). For data repair, simple algorithms that memorize transformations between dirty and clean data are sometimes used (Mahdavi and Abedjan, 2020; Wang et al., 2022). These are designed specifically for ensemble-based approaches and can only suggest good repairs collectively. Hence, they are generally not viable data repair tools independently. Mahdavi et al. define three types of such tools: (1) value-based models, (2) vicinity-based models, and (3) domain-based models (Mahdavi and Abedjan, 2020). Models in the first category only consider the corrupted value when suggesting repairs (Mahdavi and Abedjan, 2020; Wang et al., 2022). For example, given a misspelled string such as \u201ca dg\u201d, the tool could suggest \u201ca dog\u201d. On the contrary, vicinity-based models leverage the context of the corrupted value to predict its repair. For example, given the \u201ccity\u201d attribute of a record is set to \u201cTokyo\u201d, then we know that the \u201ccountry\u201d attribute should be \u201cJapan\u201d. Finally, domain-based models repair a value based on the domain of the attribute. For example, a domain-based repair strategy could replace a corrupted value with the attribute\u2019s mode.\nOnce the base data cleaning tools have processed an example, one is left with a set of data cleaning predictions (error detection or repair) that are ingested by the meta-model for the final data cleaning prediction. We observed different ways to include the cleaning signal of the base data cleaning tools in the prediction process of the meta-model. When the meta-model of the cleaning approach is a Probabilistic Graphical Model (PGM), the base data cleaning tools are transformed into inference rules for the meta-model (Rekatsinas et al., 2017; Visengeriyeva et al., 2016). For error detection, Mahdavi et al. (2019) trains, for each feature, a model that ingests the base data cleaning tools\u2019 binary output (i.e., dirty or not) and predicts whether a record is dirty or not. Compared to error detection, error repair with ensemble-based approaches has the additional challenge of having base data cleaning tools that may output a wide variety of values. Thus, error repair tools adopt a different approach to combine the base data cleaning tool predictions. Instead of generating a repair, the meta-model selects the repair that is the most likely to be correct in a pool of repairs suggested by the base data cleaning tools (Mahdavi and Abedjan, 2020; Wang et al., 2022). In Mahdavi and Abedjan (2020), each base error repair tool suggests values to repair corrupted cells.\nThen, for each repair, a vector holding the confidence that the repair is correct according to each base error repairer is generated. This vector is then fed to a Neural Network (NN) that will predict how likely a repair is given the confidence of the base error repair tools. The repair with the highest score becomes the repaired version of a dirty record. Instead of evaluating repairs using the confidence score of the base tools, Wang et al. let the meta-model select the most probable repair by comparing the repaired versions of a record with its dirty version (Wang et al., 2022). To facilitate the comparison process, the records are transformed into embeddings. Then, an NN ingests the difference between the embeddings, along with the embedding resulting from the concatenation of both records. This strategy of encoding records captures both the differences and similarities between the compared records. For every considered pair, the NN will output a score, stating how likely is the proposed repair. The most likely repair for a record is then selected. This approach was originally designed for entity matching but showed to be apt for error repair as well.\nWhile ensemble-based approaches may seem appealing, they have a few challenges of their own. To begin with, the predictions of a meta-model can be skewed if the predictions of the base tools are correlated (Visengeriyeva and Abedjan, 2018). This is simply due to the fact that a larger feature space is allocated to what is essentially the same information. The authors in (Visengeriyeva and Abedjan, 2018) address this challenge by pruning out correlated tools and keeping the most effective ones. To achieve that, k-means clustering is run on the predictions of the base tools. Then, using a validation dataset, the best tool for each cluster is selected. In addition to the aforementioned issue, a large pool of base cleaning tools results in two additional problems. First, it may lead to a longer run time for the whole data cleaning process. Thus, Mahdavi and Abedjan (2021) prunes out tools that proved to be ineffective on similar datasets. The dataset similarity is measured by comparing the semantic and syntactic similarity between columns. Second, significant human labor can ensue if a large number of tools require manual parametrization. To address this problem, Mahdavi and Abedjan (2021) generates, for every tool, all configurations in a predefined set."
        },
        {
            "heading": "5.1.3 Transformer-Based Approaches",
            "text": "In this section, we present approaches that clean the features of a record using transformer models Vaswani et al. (2017). Because of their impressive performances in the last years, there has been an interest in the research community to apply this kind of model to a wide variety of tasks (Chen et al., 2021; Dosovitskiy et al., 2020; Chasmai, 2021). Two issues must be circumvented in order for these models to be used for feature cleaning. The first hurdle is in the very nature of the task: feature cleaning is different from text generation. Effectively, transformer models are designed to generate text, while feature cleaning is usually conceptualized as a classification problem. Three approaches have been suggested to address this problem. The first solution transforms feature\ncleaning into a text generation problem. More precisely, Narayan et al. (2022) prompts the foundation models with data-cleaning questions. For example, for error detection, the user may ask the model if there is an error in a given sample, to which the model will answer \u201cyes\u201d or \u201cno\u201d. The authors further show that very few training samples are required for the model to detect errors, from a few (few-shot learning) to none (zero-shot learning). For error repair, Tang et al. (2020) uses a transformer to predict the value of every cell of a record and replace it if it differs from the shown value. The transformer model is trained using the masked data model objective. That is, the model ingests a record with a masked value and outputs a record with the mask replaced with a value. If the value generated by the model differs from the actual value that was masked, then the example is considered to be corrupted. Instead of transforming data cleaning into a text generation task, Nashaat et al. (2021) adapts a transformer model to a classification task. A linear layer with softmax activation is added to the output of the transformer and the model is trained to minimize cross-entropy loss. Before being fine-tuned for this binary classification task, the transformer is trained using the masked data model objective, so it learns the structure of data. The third way to use transformers for data cleaning is by using them to generate embeddings that can be used by another model for data cleaning. Wang et al. (2022) pre-trained a transformer using SimCLR contrastive learning framework (Chen et al., 2020), so similar records have similar embeddings. Then, an NN ingests these embeddings and predicts how likely a repair is given a dirty sample. The most likely repair is selected.\nThe second issue that must be addressed before using transformers for data cleaning is the format of the data itself. While the data cleaning approaches in this section are applied to tabular data, transformers are trained with natural language. A naive approach would feed the record as it is to the model (i.e., transform the record into a sentence by concatenating its features using spaces). However, precious information about the features\u2019 semantics would be lost, since the model does not know what a value represents (e.g., a number does not make sense if we do not know what it counts). Hence, to address that problem, the tabular records must first be transformed into sentences, and then the model must learn to understand the structure of these sentences. To inject the tabular structure of data into strings, Narayan et al. (2022) prepends each attribute value with its name before concatenating it into a string. For a record that has a feature named \u201cCity\u201d and a value of \u201cMontreal\u201d, then the attribute would be represented by \u201cCity: Montreal\u201d. The latter would be concatenated into a sentence with the substrings generated for the other features. To inject the semantic difference between attributes\u2019 names and their values, Tang et al. (2020); Wang et al. (2022) further prepend attributes\u2019 names with the special token \u201c[A]\u201d and the values with \u201c[V]\u201d. Extending on the previous example, we would have \u201c[A]City [V]Montreal\u201d. Once tabular data is converted into strings, the model must learn to understand the format of the data. Similar to what is usually done with large language models (LLM), the masked data model objective is used in Nashaat et al. (2021); Tang et al. (2020) to\ntrain transformers in an unsupervised way. The model is trained to predict the missing value in a record\u2019s entry, which enables it to unlearn its assumption of processing natural language data and learn the structure of the dataset at hand. Wang et al. (2022) leveraged contrastive learning to train the encoder of a transformer to generate embeddings that are close in the latent space for similar records, and far for different records."
        },
        {
            "heading": "5.1.4 Autoencoder-Based Approaches",
            "text": "The data cleaning approaches in this section use autoencoders to detect or repair records. Autoencoders are a type of ML model trained to output the data they receive as input while being constrained to represent the data in a latent space with fewer dimensions than the input (Bank et al., 2020).\nMauritz et al. (2021) argues that autoencoders have denoising capabilities because of the compression process. Representing the input data into a lower dimension forces the autoencoder\u2019s intermediate representations to contain only the most important information necessary to recreate the input data. As a result, noise is removed from the data. Hence, they use the innate noise removal capability of autoencoders to repair records. To do so, they first transform records into probabilistic ones (i.e., records whose features have a probability distribution over potential values). Then, they feed the record to the autoencoder, which outputs another probabilistic record. The result can be transformed back to non-probabilistic data by picking the most likely value for each feature. Tonolini et al. (2020) proposes a variant of variational autoencoders (VAE) architecture for noise removal that does not suffer from the posterior collapse problem (when the model confidently suggests a small number of reconstructions). Authors employ a three-step process to clean samples using their autoencoder. First, an example to clean is ingested by the model and a latent distribution is generated by the model. Second, multiple latent vectors are sampled from that latent distribution. Third, examples are generated from each latent vector and averaged to generate the final clean value. Instead of using the autoencoder to generate values to repair errors, Liu et al. use the reconstruction loss of an autoencoder to detect corrupted records Liu et al. (2022). Records that have a high loss during the first epochs of training are flagged as erroneous. The study further shows that poisoned samples generally have the lowest loss; hence their method can also be used to detect such records."
        },
        {
            "heading": "5.1.5 Error Prioritization Approaches",
            "text": "Instead of directly cleaning data, the approaches covered in this section try to identify and prioritize the records that should be cleaned by a human expert.\nAuthors in Zhang et al. (2018a) leverage the knowledge stored in Wikipedia to clean data. More precisely, by generating questions for a QA engine that understands natural language and whose answers are Wikipedia pages, they are able to verify the values in a dataset by comparing them with what is\non Wikipedia. If a value conflicts with what the engine has returned, the records are sent to a human for review. Instead of identifying records that are potentially erroneous, authors in Karla\u0161 et al. (2020) search for records that would have the largest impact on a model\u2019s performance if cleaned. They argue that data cleaning efforts are wastefully spent on samples that do not alter a model\u2019s performance. Thus, the authors propose an approach that tries to find samples that, once cleaned, will render the next iterations of cleaning less useful (since the next iterations will not alter a model\u2019s performance). Similarly, Krishnan et al. (Krishnan et al., 2016) propose an approach to prioritize the samples sent to a human for review. A model is first trained on a dataset (for the prediction task of the dataset, not for data cleaning). Then, samples that would trigger the largest weight update if the model was retrained on them after being cleaned are prioritized. The weight updates are estimated using previous examples with similar feature values."
        },
        {
            "heading": "5.1.6 Data Cleaning Rule Generation Approaches",
            "text": "The approaches included in this category use ML to infer data cleaning rules that can be used to clean a dataset. Data cleaning rules are typically expressed using integrity constraints, a technique from database schema design (Abiteboul et al., 1995). Integrity constraints can express a wide variety of logical clauses, such as: \u201ctwo persons living in the same city must live in the same country\u201d or \u201ctwo persons can not have the same phone number\u201d (Ilyas and Chu, 2019).\nGe et al. (2020) proposes a hybrid data cleaning framework, named MLNClean, that leverages Markov Logic Networks (MLNs) to supplement integrity constraints and makes it possible to clean unknown data errors. MLNClean first uses MLN to generate a set of probable data cleaning rules with their corresponding probabilities. These rules are then leveraged to generate repair candidates. The repair candidate that is the most likely (based on the likeliness of the data cleaning rules that generated it) and that differs the least from the original value is then used to repair an observed value. In another work (Guo and Rekatsinas, 2019), authors propose AutoFD, a framework for Functional Dependency (FD) (a type of integrity constraint) discovery based on structure learning techniques in PGMs (Guo and Rekatsinas, 2019). A PGM is introduced to capture dependencies that FDs introduce among attributes in a dataset, revealing that learning the graph structure of this model is equivalent to discovering FDs."
        },
        {
            "heading": "5.1.7 Other Approaches",
            "text": "In this section, we cover other feature cleaning approaches for tabular data that are not included in the previous categories.\n\u2013 (Krishnan et al., 2017): Similar to ensemble-based feature cleaning, their work uses an ensemble of feature cleaning tools. However, contrary to the\naforementioned category, their goal is not to clean data but rather to directly generate a model that would have been trained on clean data. To do so, all feature cleaning tools are individually applied to a dataset, generating different versions of it. Then an ML algorithm trains a model for each processed dataset. The models\u2019 predictions are combined together using boosting (Wikipedia, 2023d). As a result, the predictions of the ensemble are influenced by all the data cleaning tools. \u2013 (Lew et al., 2021): Authors propose a new domain-specific Probabilistic Programming Language (PPL), namely PClean that is tailored for data cleaning. Similar to Rekatsinas et al. (2017), PClean leverages datasetspecific knowledge. Authors introduce a domain-general nonparametric prior on the number of latent objects and their link structure in the dataset. Code written by PClean customizes this prior probability via a relational schema (for relational dataset) and via generative models for objects\u2019 attributes. Inference in PClean is based on a novel Sequential Monte Carlo (SMC) algorithm, to initialize a latent object database with plausible guesses, and novel restoring updates to fix mistakes. \u2013 (Oliveira et al., 2020): Their paper proposes an improvement over HoloClean (Rekatsinas et al., 2017). Most of the data cleaning approaches consider that data cleaning is only applied once, before training a model. This does not reflect how data is cleaned with real ML software systems since new training data is continuously generated. Hence, they propose a few enhancements to HoloClean (Rekatsinas et al., 2017) to make it more efficient in that context. Notably, they avoid retraining the data cleaning model if the distribution of incoming data does not change."
        },
        {
            "heading": "5.1.8 Common Techniques to Improve Performance",
            "text": "In this section, we cover various techniques commonly used to improve a model\u2019s performance for model-based approaches, namely, data augmentation, semi-supervised techniques, and active learning.\nData Augmentation Data augmentation is a technique to generate synthetic new examples by applying transformations to existing data (Hern\u00e1ndez-Garc\u00eda and K\u00f6nig, 2018). Data augmentation is used in feature cleaning approaches to address the class imbalance problem; training datasets for feature cleaning have a larger amount of labeled clean records than labeled dirty records (Nashaat et al., 2021; Heidari et al., 2019). In other words, in labeled datasets, there is generally more clean data than dirty data and dirty data is needed for the ML approach to learn to detect dirty data (error detection) or to learn to clean it (error repair). Hence, data augmentation tries to address that problem by generating more labeled dirty data. To do so, data augmentation techniques transform clean data into dirty data by applying plausible corruption mechanisms (i.e., the error induced into the record is plausible given the dataset). In order to define a data augmentation strategy, two components must be defined: the process\nthat transforms clean data into dirty data and the process that selects data transformations (i.e., assigning a probability to each transformation). Instead of manually crafting data transformation rules, Nashaat et al. (2021); Pham et al. (2021); Heidari et al. (2019) learn them from the data. In Nashaat et al. (2021), Gestalt Pattern Matching (Tawfik and Spruit, 2020) is used to find the non-matching substrings between the clean and dirty versions of a text feature. These substrings are then used to form a new mapping, from the clean substring to the dirty one. Similarly, Pham et al. (2021); Heidari et al. (2019) adopt a hierarchical pattern matching approach to learn valid transformations. Once data transformation functions are defined, the data augmentation approach must build the function that selects the data transformation that will be applied to a record. In other research works Nashaat et al. (2021); Pham et al. (2021); Heidari et al. (2019), authors sample a valid transformation following the empirical distribution of transformations (i.e., how often each transformation was used empirically).\nSemi-Supervised Techniques\nSemi-supervised learning is a branch of ML that uses unlabeled data (in addition to labeled data) to build better models (Pise and Kulkarni, 2008). The semi-supervised techniques presented in this section use the labeled data to label the unlabeled data. In order to generate training data for its data augmentation approach, Heidari et al. (2019) fits a Naive-Bayes model that, similar to imputation, predicts a clean value for each feature of a record. The most confident predictions (i.e., with over 90% confidence) become the clean versions of the observed values. Pham et al. (2021); Mahdavi et al. (2019) use label propagation to increase a labeled dataset size for error detection. Label propagation consists of assigning the label of a record to a similar unlabeled record Zhu and Ghahramani (2002). The challenge with this approach is to devise an accurate measure of similarity between records to avoid mislabels. Pham et al. (2021) considered two feature vectors to be similar if all their attributes did not differ by more than a threshold. Similarly, Mahdavi et al. (2019) used hierarchical clustering (Aggarwal Charu and Reddy Chandan, 2013) to find groups of similar feature vectors. Records in the same cluster share the same label.\nActive Learning\nActive learning refers to a situation where a ML algorithm can iteratively query an oracle (e.g., a human) to label new data points (Wikipedia, 2023c). In the following, we show how some studies used active learning to achieve superior performance. Neutatz et al. (2019) relies on the confidence score of the data cleaning models to select the cells to send to a reviewer. Because they ask reviewers to clean cells, not tuples, their acquisition function (i.e., the function that selects records) starts by selecting a feature to clean (i.e., a column of a dataset), then picks a batch of cells inside that column to clean. The column selected in the first step is the one where the model is, on\naverage, less confident of its predictions. The cells are then selected using the query-by-committee algorithm (Freund et al., 1997). Pham et al. (2021) uses two different models for data cleaning and active learning. The model used for active learning is implemented using Probabilistic Soft Logic (Bach et al., 2017) and, similar to the data cleaning model, tries to predict whether cells are dirty or clean. Records that are more likely to be dirty are sent for review. Finally, for error repair, Mahdavi and Abedjan (2020) prioritizes tuples with a lot of errors that are common in the dataset. The goal is to create a training dataset that covers most types of errors in a dataset. Note that the approach assumes that an error-detection tool already marked the dirty cells."
        },
        {
            "heading": "5.1.9 Other Data Types",
            "text": "In this section, we cover feature cleaning approaches for other data types than tabular. We chose to present data cleaning approaches for each data type in a dedicated part because each data type has a peculiarity that makes the data cleaning task significantly different. Thus, in the following, we describe feature cleaning approaches for time series, text, and image data, respectively.\nTime Series Time series are a collection of observations made in chronological order (Wang and Wang, 2019). The feature cleaning approaches in this category have to consider the ordinal nature of time series data in order to clean it.\nZhang et al. (2020a) combined auto-regressive models with the minimum repairing principle of data cleaning (Ilyas and Chu, 2019) to iteratively clean data. Auto-regressive models are used to suggest values to repair anomalous observations. In their approach, an auto-regressive model suggests repairs for every observation. Only one repair is applied: the one that differs the less from the original observation. The auto-regressive model is retrained on the cleaned time series, and the procedure is repeated until the difference between the original and clean value is below a threshold or the maximum number of iterations is reached. Similarly, Akouemo and Povinelli (2017) presents an iterative approach for the detection and repair of anomalies in time series data. First, Auto-regressive with Exogenous inputs (ARX) and NN models are trained on a time series and used to predict repair values for every value of a time series. Then, hypothesis testing over the difference between observed values and predicted values is used to detect outliers. The most abnormal observed value is replaced by the predicted value, and the process is repeated.\nText In this section, we cover techniques for detecting errors in text data. The addressed errors include grammatical, syntactical, and semantic errors.\nNNs models are used for detecting and correcting grammatical errors in text. For example, in He et al. (2021b), an RNN-based model is used for detecting grammatical errors in English verbs. The model consists of two subRNNs such that one processes the sentence from the beginning to the target\nverb and the other processes the sentence from the end to the target verb. An additional layer is used to process the output of the RNNs and predict the verb which is then compared with the original verb to detect any potential error. Knill et al. approach error detection as a sequence labeling task where a Bi-directional LSTM is trained to classify each token in the input sentence as correct or incorrect within the sequence Knill et al. (2019). Wang and Tan (2020) utilize word contextual embeddings generated by BERT and concatenate them to character embeddings generated by an LSTM. They then use the BERT model, in particular, the first six layers of BERT to classify each word as correct or incorrect. Rei and Yannakoudakis (2016) investigates different NN architectures (CNN, Bi-RNN, Bi-LSTM, and their multi-layer variants). Provided with a sequence of tokens, the model calculates the probability for each token to be correct or incorrect. Among the compared models, Bi-LSTM outperforms all other models including the multi-layer Bi-LSTM variant. To detect syntactic errors, Santos et al. (2017) proposes to train two LSTM models, one is forward and the other is backward, to predict tokens. The disagreement between the two models is used as an indication of a possible syntax error. They also attempted to propose a correction by identifying an alternative token sequence derived from the models. The model introduced by Spithourakis et al. (2016) detects semantic errors in terms of inconsistencies between a numerical value (e.g., clinical test result) and its interpretation. When a document is received, sets of words with similar meanings are employed to generate all possible word substitutions within the document, creating a set of candidate repaired documents. Each of the candidate documents is fed to an LM to predict which one is the most likely to be correct. Likelihood ratios between candidate and original documents are then computed. Candidates with higher scores than the original indicate errors; the highest score is accepted as the correction.\nImage A common distortion that can be encountered in images, particularly in those captured by autonomous vehicle perception systems is a radial blur. This type of blur arises due to the motion of the imaging system during image acquisition. Hurakadli et al. (2019) proposes a DL-based pipeline to estimate and correct the radial blur in order to improve the detection of traffic signboards in images. The proposed pipeline consists of estimation and enhancement modules. The authors designed a convolutional neural network (CNN) named CuratorNet, to estimate the point spread function (PSF) of an image with a precision of up to a second decimal point. PSF is a mathematical description of how light gets spread out or blurred across the image due to various factors in the imaging system. By applying the inverse operation of the PSF to the observed image, it can be possible to recover the original scene. To correct the image, the authors propose a convolutional autoencoder-based enhancement module that eliminates the radial blur, based on the obtained PSF estimation.\nFeature Cleaning: Summary and Research Opportunities\nGeneral existing approaches: We presented approaches to clean tabular, text, and image data. We identified the following feature cleaning approaches for tabular data.\n\u2013 Model-based approaches: Approaches that train an ML model to directly clean data. One may observe the following three subcategories. \u2013 Ensemble-based approaches: An ML model uses the predictions of base data\ncleaning tools for cleaning. The base data cleaning tools are generally simple. \u2013 Transformer-based approaches: Approaches that use a transformer to clean the features of a record. They rely on the capabilities of pre-trained LLMs. \u2013 Autoencoder-based approaches: Approaches that use autoencoders to clean data. They rely on the denoising capability of auto-encoders. \u2013 Error prioritization approaches: Approaches that prioritize the records that should be reviewed by an expert. \u2013 Data cleaning rule generation approaches: Approaches that generate data cleaning rules and use them to clean data.\nResearch opportunities:\n\u2013 Develop feature cleaning approaches that consider the label of a record when cleaning its features. The approaches covered in our study do not leverage the special status of labels when cleaning a feature (they are not mentioned nor treated differently than other features). Potentially, labels could help clean the features of a record more efficiently than if treated as another feature. Thus, we encourage future works to explore how the records\u2019 labels can be used for feature cleaning. \u2013 Extend the advances done with ensemble-based approaches. Future works should experiment with a wider number of base data cleaning tools. For example, other feature cleaning approaches could themselves be included in the ensemble. \u2013 Continue exploring how LLMs can be adapted to data cleaning tasks. We extend on this topic in Section 6.4. \u2013 Explore how techniques from outlier detection can be transposed to error detection. Outlier detection and error detection are similar challenges since corrupted records often are outliers. Future studies could leverage the existing literature on outlier detection and ML (Pang et al., 2021; Nassif et al., 2021).\n5.2 Label Cleaning\nIn this section, we cover data cleaning approaches to detect or correct label errors. For simplicity, we refer to label errors as mislabels. We do not differentiate the label cleaning approaches based on type of the data being cleaned, since the main ideas are generic. That is, any approach can be adapted to other data types if an adequate representation learning technique is used.\nIn the following, we describe the different categories of label cleaning techniques seen in our review. The first category uses the confidence score of an ML model to detect corrupted samples. The second one adopts a similar approach but uses the loss of a model instead of its confidence scores. The third one detects mislabels by searching for instances that degrade a model\u2019s performance. The fourth one uses outlier techniques to find mislabels. We also\npresent label cleaning approaches that did not belong to any of the aforementioned categories at the end of this section."
        },
        {
            "heading": "5.2.1 Uncertainty-Based Approaches",
            "text": "A straightforward approach to label cleaning is to use the confidence score of an ML model to detect corrupted samples. After having been trained on a dataset, one can suppose that an ML model has learned the representative features of each class. Hence, if the model\u2019s predictions disagree with the assigned label for an instance (i.e., the model assigns a low probability for the labeled class), one can assume that the instance has a higher chance of being corrupted compared to other samples. However, the model may predict something different than a sample\u2019s label because of corrupted features, mislabels, or more simply because the model made a mistake. Thus, the predictions of the model can be used as a weak signal for detecting mislabels.\nOnce a sample is marked as being mislabeled, it must be cleaned. The cleaning strategies sometimes assume that someone will manually clean the flagged instances. When they do, either all the instances are sent for review (Rottmann and Reese, 2023; Teso et al., 2021) or only the most important ones. In the latter case, one must find criteria to prioritize samples over others. Rehbein and Ruppenhofer (2017) prioritizes records for which the model\u2019s predictions have the highest entropy. Similarly, Atkinson and Metsis (2021, 2020) prioritize the records where the model is highly uncertain about the labeled class. In order to avoid uselessly spending humans\u2019 time, Bernhardt et al. (2022) proposed to also consider the difficulty of labeling an instance for a human. Thus, the approach considers two things when selecting the next sample to clean: (1) how much the model disagrees with the label and (2) how difficult it is to clean for a human. To measure the latter, the authors considered the entropy of a model\u2019s predictions (the higher the entropy, the more difficult labeling). If different people cleaned a record differently, then we can suppose that it is difficult to clean, and we should avoid sending it again for cleaning. Other uncertainty-based approaches (Ponzio et al., 2021; Northcutt et al., 2019; Liu et al., 2020; Ke et al., 2019) do not assume the presence of a human expert for cleaning instances; hence, they must automatically handle the cleaned instances. The straightforward strategy is to drop all the flagged instances (Ponzio et al., 2021; Liu et al., 2020). As a result, the dataset is purged from every potentially corrupted instance. Instead of removing all potentially corrupted instances, Ke et al. (2019) iteratively removes the instances with the lowest confidence and retrains the model on the cleaned dataset until the model\u2019s confidence for all instances is above a threshold. Northcutt et al. (2019) builds a class-conditional noise matrix and suggests different methods for record pruning using that matrix. This approach uses the confidence of the model to estimate class-conditional noise. An instance might belong to another class if the model confidence for that other class is above the per-class threshold. The per-class threshold of a class j is the model\u2019s average confidence over the samples labeled j for the same class j. Having a threshold defined for every\nclass allows the approach to be more robust to heterogeneous class probability distributions and class imbalance. When an instance is above the threshold for more than one class, the approach selects the class the model is the most certain about. The authors propose five methods to prune out records while considering class-conditional noise. One of them prunes out the records that are predicted to belong to another class (when using the per-class thresholds).\nFor the uncertainty-based approaches to be successful, two things must be taken into consideration. First, the model should not overfit the dataset, or else the mislabeled instances will be correctly predicted (with respect to the labeled class, not the true class) and mislabeled instances will have a high confidence score. Second, an accurate measure of the model confidence must be used. Prior work (Gal and Ghahramani, 2015; Hendrycks and Gimpel, 2016) has shown that the softmax score should not be used to measure the model\u2019s confidence, since it only acts as a normalization factor.\nThe first problem can be addressed using techniques that are robust to label noise, for example, Bernhardt et al. (2022) used Co-Teaching (Han et al., 2018) and BYOL (Grill et al., 2020). Another solution to prevent the ML model from predicting the mislabeled class of an instance is to stop the training process before the model overfits the data. K\u00f6hler et al. (2019) made the observation that the mislabeled records are learned later in the model\u2019s learning process compared to the correctly labeled ones, and, as a result, stopping the training early would enable the model to have low confidence on mislabeled instances. Leveraging that observation, Ponzio et al. (2021) proposed an approach to automatically stop the training process. After each epoch, the model\u2019s confidence for each instance\u2019s labeled class is recorded and clustered using the k-means algorithm with a k value of 2. Hence, after each epoch, a cluster with the most certain records and another with the least certain records are generated. When less than 1% of the records change the cluster from one epoch to another, the model stops training.\nThe second problem can be addressed using modeling techniques able to provide uncertainty scores, such as Bayesian Neural Networks (BNN) (Ponzio et al., 2021). The output of these NNs is a posterior distribution over the classes. Therefore, BNNs give a good measure of confidence when adequately trained. They are, however, complicated to implement and hard to train due to the high number of hyperparameters and computational cost (Ponzio et al., 2021). Thus, uncertainty-based approaches use different approximations of BNNs when concerned with providing a good measure of confidence. Inspired by the work of (K\u00f6hler et al., 2019), which also leverages Deep Ensemble (Lakshminarayanan et al., 2016), Ponzio et al. (2021) approximates BNNs with Monte Carlo Dropout (MCD) (Gal, 2016). MCD provides confidence measures that are numerically tractable and easier to implement than BNNs."
        },
        {
            "heading": "5.2.2 Loss-Based Approaches",
            "text": "Similar to uncertainty-based approaches, loss-based approaches posit that a model trained on a dataset will have learned the representative features of\neach class and, as a result, will generate predictions that are not aligned with incorrect labels. However, instead of relying on a model\u2019s confidence, loss-based approaches will detect mislabels using the loss of a model. Records for which the model has a high loss are considered more likely to be mislabeled than other records.\nThe authors in Huang et al. (2021); Li et al. (2021) first train a model on a dataset, then flag any record with a high loss value as mislabeled. Flagged records are then relabeled with the model\u2019s predictions. A new model is then trained on the modified dataset, and the process is repeated using the new model and the modified dataset. The authors in Huang et al. (2019) made the observation that NNs tend to overfit on noisy examples later in the training process than clean ones. Thus, corrupted examples are identified by looking for examples with a higher loss than average during training. However, the loss values of noisy and clean samples might vary depending on how the NN is initialized, which makes the approach less reliable. To address that concern, model training is repeated several times and the loss values are averaged. The weights of the NN are periodically re-initialized by suddenly increasing the learning rate and taking large gradient steps, which makes the model unlearn. The authors in Yu and Aizawa (2020) address the challenge of detecting records not belonging to any of the classes of a dataset (noisy class). The approach adds a new class to the dataset for these samples. Then, a probability of 0.5 is assigned to the labeled class and the noisy class for every sample of the dataset. The approach iteratively trains a model and modifies the labels in such a way that the model\u2019s loss is decreased. Similarly to Huang et al. (2019), the authors use a high learning rate to avoid overfitting on noisy labels. Similarly to Picket (Liu et al., 2022) (see Section 5.1.4), the authors in Zhang and Tan (2019) measure the reconstruction error of autoencoders to detect corrupted samples. Examples with a high reconstruction error are flagged as mislabeled instances. Contrary to Picket (Liu et al., 2022), Zhang and Tan (2019) trains an autoencoder for each class. To repair a mislabeled instance, they measure the reconstruction error of all the autoencoders for that sample and assign the class associated with the autoencoder with the lowest error. Similarly, the authors in Salekshahrezaee et al. (2021) used and compared three ML techniques, namely principal component analysis (PCA), independent component analysis (ICA), and autoencoders for unsupervised label noise detection. With each technique, they constructed a binary classifier that identifies instances with a high reconstruction error as anomalies. Based on their results, the autoencoder-based classifier obtained the best label noise detection score."
        },
        {
            "heading": "5.2.3 Counterfactual Approaches",
            "text": "A reason to do data cleaning is to improve a model\u2019s performance (Hara et al., 2019). This implicitly assumes that data errors reduce a model\u2019s performance. Thus, by finding dataset repairs that improve a model\u2019s performance, it is possible to find corrupted instances. In this study, we observed two types of\ndataset repairs: instance removal (i.e., dropping a record from a dataset) and attribute modification (i.e., modifying a record\u2019s features values, or labels). Counterfactual approaches try to find or repair corrupted records by comparing the performance of a model trained on a dataset with the performance of a model trained on the counterfactual dataset (i.e., the same dataset with some features or labels modified). In the following, we describe how the counterfactual modifications (i.e., instance removal and attribute modification) are used to detect mislabels and how they are implemented. Then, we cover the metrics used to quantify the impact of a counterfactual modification on a model\u2019s performance.\nThe first category of counterfactual approaches detects mislabels by comparing the performance of a model when trained on a dataset including a sample with the performance of a model when trained on the same dataset without that sample. A naive approach would train the model twice, once with the sample and another time without it. However, this can quickly become intractable as the size of the dataset increases. To address that problem, the authors in Dolatshah et al. (2018) used parallel computing. Two other techniques have been proposed to address this problem in other works. The first technique is to use ML algorithms to predict the impact of including a record in the training dataset of a model (Smyth, 2020). In other words, an ML model is in charge of predicting the change in validation loss when a record is added to the training dataset of another ML model. To achieve that, Smyth (2020) records a model\u2019s validation loss at every step of the training process. Hence, each time a model learns from a sample, the model is evaluated on a validation dataset. The validation loss is averaged per sample and the average becomes each record\u2019s label. Using this new dataset, ML algorithms can be used to predict the change of validation loss for any sample added to a dataset. The number of samples used to train the model predicting deltas in validation loss is a subset of the whole dataset to clean. The second technique to approximate the impact of removing a sample from a model\u2019s training dataset is using influence functions (Suzuki et al., 2021; Wu et al., 2021; Koh and Liang, 2017). Influence functions are a technique from robust statistics that can be used to estimate how much a model\u2019s parameters change in reaction to an infinitesimal change in a training point\u2019s weight in a dataset. The influence function implementation of Koh and Liang (2017) can estimate the effect of removing an instance from a dataset needing only an oracle access to gradients and Hessian-vector products of the ML algorithms. However, it is theoretically sound only for convex-loss models. To address this limitation, Hara et al. (2019) designed a novel estimator that, similar to influence functions, can estimate the change in performance if an instance is removed from the training dataset for models trained with stochastic gradient descent. In an experiment where the instances with the worst influence on a model\u2019s performance are progressively dropped, Hara et al. (2019) have shown that the approach improves the model performance more effectively than Koh and Liang (2017). Similarly to Hara et al. (2019), (Suzuki et al., 2021) and (Wu et al., 2021) provide improvements in terms of efficiency to Koh and Liang\n(2017)\u2019s implementation of influence functions, so that it can be used more efficiently for label cleaning.\nThe second category of counterfactual approaches detects mislabels by comparing the performance of a model when trained on a dataset including a sample with the performance of a model when trained on the same dataset with a modified version of that sample. In addition to detecting potentially corrupted records, this counterfactual approach also provides repair suggestions (the counterfactual sample). Xiang et al. (2019); Zhang et al. (2018b) formulate the search for mislabeled instances as an optimization problem where the goal is to maximize the model accuracy on an error-free dataset, by correcting the training samples\u2019 labels. Both approaches can automatically correct mislabels given an error-free test dataset.\nExcept for one paper in our selected papers, the counterfactual approaches included in this review evaluated modifications to a dataset using a loss function. However, as pointed out by Flokas et al. (2022), loss functions are not always a good objective function to optimize for, since influence functions can not find records corrupted by systematic noise. Indeed, removing a record corrupted with systematic noise will not significantly impact the model\u2019s performance, since other samples are affected by the same noise. Hence, records corrupted with systematic noise are not influential per se. To address this issue, Flokas et al. (2022) designed an approach that uses complaints (filed by a human) to find corrupted instances. Complaints can be filed regarding tuples (e.g., \u201cthat record should not have this label\u201d) or aggregate values (e.g., \u201cthere should be a fewer number of records that have a label\u201d). The counterfactual approach then addresses that complaint using influence functions to find instances that, once repaired, will address the complaint."
        },
        {
            "heading": "5.2.4 Outlier-Based Approaches",
            "text": "Outliers are records that are significantly dissimilar from other records, to a point that one can believe that they have not been generated by the same process (as the rest of the data) (Wang et al., 2019). Thus, to detect mislabeled instances, outlier-based approaches for label cleaning search for records that differ significantly from the other records from a class. In Guo et al. (2018), a record is compared against every other clean record from its class using cosine distance. If the average distance is too far, then the record is marked as dirty. Because this approach does not scale to classes with many records, Lee et al. (2018) compares instances against class prototypes; a vector representing all the instances from a class. The class prototype is generated by encoding a subset of the most representative records from a class using self-attention. Self-attention allows the encoder to focus on the most important instances when generating the final embedding. Records whose cosine distance with the class prototype is above a selected threshold are flagged as dirty. The approach iteratively runs the cleaning technique and trains a model on the cleaned dataset. The first layers of the trained NN are used to generate embeddings for the data cleaning steps. Thus, every time the NN im-\nproves performance, the data cleaning technique uses improved embeddings. Bagherzadeh and Sadoghi Yazdi (2017) proposes a method for label denoising based on Bayesian aggregation. This approach addresses the limitations of kNN-based approaches, which are not robust to high levels of label noise rate since they only have a local view of instances. A local view refers to considering the immediate neighbors or similar instances of a data point within a certain vicinity. In contrast, a global view refers to comparing a record to others that are not in its neighbor. The proposed approach combines both views to detect mislabels. The global view of data is obtained by measuring the distance of an instance to its class distribution or by evaluating the value of the probability density function of each class at an instance. The aggregation leads to a robust detection of instances with noisy labels even in the presence of high levels of label noise."
        },
        {
            "heading": "5.2.5 Other Approaches",
            "text": "In this section, we cover other label cleaning approaches that are not included in the previous categories.\n\u2013 Ekambaram et al. (2016): The authors argue that records near the decision boundary are more likely to be mislabeled by a human because they are more difficult to label. Thus, the authors propose to review support vector examples of an SVM by a human. Because there might be a lot of samples to review, they prioritize the ones that are on the wrong side of the decision boundary of another model trained on the dataset devoid of the support vector records. \u2013 Su et al. (2021): Instead of detecting and repairing errors, they generate a synthetic dataset devoid of mislabels. To do so, the authors leverage the phenomenon of mode collapse (goo, 2022) in Generative Adversarial Networks (GANs). They train a GAN to generate realistic images for a class. Because GANs are subject to mode collapse, they expect the synthetic dataset to be free of mislabeled instances, since the GAN will not have memorized the less common instances of a class (i.e., mislabels). \u2013 Veit et al. (2017): Instead of detecting errors and repairing them in two different steps, the authors propose an approach that directly maps an example\u2019s label to the correct value. An NN is then trained to predict the correct label given an instance and its label. An identity skip-connection is between the input label and the output of the NN. Thus, its task is to map the input label to the correct value. Similar to Northcutt et al. (2019), the authors argue that label noise is often class-conditional. Thus, learning to map labels from one class to another is sensible. \u2013 Klie et al. (2022): This study evaluates existing methods for detecting annotation errors and inconsistencies in text. Inconsistencies in this context refer to instances that should be labeled in the same manner to indicate the same type but are assigned different labels instead. The authors reimplemented 18 such methods and assessed their performance on several\ndatasets for text classification, token labeling, and span labeling. The experiments indicate that inconsistencies are more challenging to detect than annotation errors.\nLabel Cleaning: Summary and Research Opportunities\nGeneral existing approaches:\n\u2013 Uncertainty-based approaches: Approaches relying on a model\u2019s confidence to detect mislabels. Records for which a trained model predicts a different class than the one that is labeled have a higher chance of being mislabeled than other records. \u2013 Loss-based approaches: Approaches relying on a model\u2019s loss to detect mislabels. Similar to uncertainty-based approaches, records that are mislabeled are more likely to be incorrectly predicted by the model than other records (and have a high loss). \u2013 Counterfactual approaches: Mislabels may hinder a model\u2019s performance. Thus, if the performance of a model improves after a record has been removed or relabeled, then the record is most likely mislabeled. \u2013 Outlier-based approaches: Records that are incorrectly labeled might differ significantly from the other records in the labeled class. These approaches rely on that observation to detect mislabeled.\nResearch opportunities:\n\u2013 Develop efficient and accurate methods to estimate the impact of removing a record from a dataset on ML performance. Current solutions may consume a lot of resources (Suzuki et al., 2021; Hara et al., 2019) or be limited to a specific type of model (Koh and Liang, 2017). Improving these techniques will allow for more performant counterfactual approaches (covered in Section 5.2.3) while enriching the knowledge of ML explainability. \u2013 Combine label cleaning methods using ensembling techniques for improved performance. In Section 5.1.2, we described approaches that combine feature cleaning tools using ensembling techniques. Reusing the same ideas but for label cleaning could be an interesting research direction. \u2013 Develop accurate model confidence measure. As we covered in Section 5.2.1, uncertainty-based approaches rely on a model\u2019s confidence to detect mislabels. However, using a model\u2019s softmax score is not a good measure of model confidence as previous research pointed out (Gal and Ghahramani, 2015; Hendrycks and Gimpel, 2016). Future works could develop techniques to better measure a model\u2019s confidence.\n5.3 Entity Matching\nIn this section, we cover approaches to detect records referring to the same real-world entity. For ML, entity matching can be framed as a binary problem, where the model receives two records and must predict if they are the same entity or not. Formally, given a pair of records (t1, t2), a model M predicts if the pair of records is a match or not (y \u2208 [0, 1]). For conciseness, we use the word \u201cpair\u201d to refer to a pair of records. Furthermore, for simplicity, we call pairs referring to the same real-world identity as positive pairs, and negative pairs if they do not.\nDeciding whether records refer to the same real-world entity essentially comes down to comparing records and looking for similarities and differences. Hence, when designing entity-matching approaches, researchers may bias the model into comparing records, which we refer to as the comparison bias. For example, Jin et al. (2021) compare records by looking for shared and unique words between two records. The sets of shared and unique words are used by the model to decide whether they refer to the same entity or not. Because the comparison bias plays a central role in the design of the approaches, we grouped the approaches based on the way this bias is injected. Before the record-matching step, pairs that are most certainly not a match are filtered out of the dataset for efficiency purposes. This step is referred to as blocking.\nIn the following, we explain entity matching and present the approaches included in our study in three parts. We will first cover the blocking strategies. Then we present the different entity-matching approaches using ML. Finally, we show common techniques used to improve a model\u2019s performance on entitymatching tasks (e.g., data augmentation). Note that all the entity-matching approaches in our review operate over tabular data."
        },
        {
            "heading": "5.3.1 Blocking",
            "text": "As mentioned earlier, blocking is applied before entity matching. The goal is to make the whole process of finding matching records more efficient. Instead of comparing each and every instance using the matching model, blocking places records into blocks so that entity matching is only applied to the tuples in the same blocks. This process effectively filters out pairs of records across different blocks, which have a low probability of being a match, making the whole process more efficient.\nIn the simplest case, hand-crafted rules can be used to filter out pairs of records that most likely are not a match. For example, for an entity matching task of companies\u2019 names, Li et al. (2020b) exclude pairs of records that do not share the same zip code or were not one of the top 20 nearest neighbors based on the TF-IDF cosine similarity on the name and address attributes. A downside of that approach is that designing blocking rules requires prior domain knowledge and can be time-consuming. Hence, Ebraheem et al. (2018) proposed an approach that is able to filter out pairs of records with minimal human involvement. To do so, Local Sensitive Hashing (LSH) (Indyk and Motwani, 1998) is applied to the distributed representations (i.e., embeddings) of the tuples. Initially designed to efficiently approximate the k-nearest neighbors problem in sublinear time, LSH is used to filter out improbable matches. It hashes the distributed representations and uses the hash function to place each record into blocks, where each block holds records that are spatially close. Only pairs inside the same block can be considered for entity matching. (Ebraheem et al., 2018) generate the records\u2019 distributed representations by transforming the record\u2019s tokens into embeddings using the Glove embedding model (Pennington et al., 2014) and merging them together with a bi-LSTM (Hochreiter and Schmidhuber, 1997). The token pre-trained to-\nken embeddings are fine-tuned on the domain of the task using vocabulary retrofitting. Similarly, Zhang et al. (2020b) applies LSH on records\u2019 embeddings. To the difference of Ebraheem et al. (2018), they generate more than one embedding per record. The records having at least one embedding in the same block as another record will be forwarded to the entity-matching model. The other pairs will be filtered out. The embeddings are generated by fusing fastText (Bojanowski et al., 2017) embeddings using attention mechanisms. Similarly to this work, Huang et al. (2023) uses the FAISS library (Johnson et al., 2019) to efficiently compute the k-nearest neighbors and then applies an algorithm to improve recall so that pairs of tuples that are a match are not filtered out. Essentially, the algorithm resizes the k-nearest neighbors set for each record (i.e., adjust \u201ck\u201d) so that the likeliness of the k neighbor being a match is significantly more probable than the k+1 neighbor."
        },
        {
            "heading": "5.3.2 Token Comparison",
            "text": "The first category of entity-matching approaches compares records at the feature level. These approaches first extract a set of shared and unique words from the compared records, transform the words into embedding, and feed them to the model. Jin et al. (2021) creates, for each attribute, a set of shared and different words. For example, for the values \"Montreal, Canada\" and \"Quebec, Canada\", the set of shared words would be \"{Canada}\" and the set of different words would be \"{Quebec, Montreal}\". Similarly, Wang et al. (2020) creates a set of shared words and two other sets containing the words unique to each record. The words in each set are then transformed into embeddings using a pre-trained language model (Jin et al., 2021) or word embeddings (Wang et al., 2020). Each set of word embeddings is then condensed down to an embedding using summation (Wang et al., 2020) or attention mechanisms (Jin et al., 2021; Wang et al., 2020). As a result, an embedding is created per set of words. These embeddings are then concatenated together and fed to the last layers of the model for prediction. In addition to the aforementioned strategies to combine embeddings, Wang et al. (2020) proposes to use the embedding resulting from the set of shared words to serve as the \"query\" key in the attention mechanisms applied to the sets of unique words. Their intuition is that information from the shared words is useful to know the important words in the set of unique words."
        },
        {
            "heading": "5.3.3 Latent Space Comparison",
            "text": "The second type of entity-matching approach compares records in a latent space using a distance measure such as cosine distance before predicting whether a pair of records is a match or not. The records can be compared at the token level, the attribute level, or at the record level. We will refer to the embeddings used for comparison at each of these levels as token embeddings, attribute embeddings, and record embeddings respectively. Effectively, these approaches use one of the aforementioned embeddings for each record,\ncompare the embeddings, aggregate the results of the comparison, and feed it to the entity-matching model.\nToken-Level Comparison\nFirst, embeddings must be generated for each token. They can be generated from pre-trained embedding models (Fu et al., 2021; Mudgal et al., 2018; Nie et al., 2019), transformers\u2019 encoders (Huang et al., 2023), or any other suitable technique. To generate token embedding, Li et al. (2020a) uses graph convolutional neural networks. The structured dataset is transformed into a graph by taking into consideration how frequent a word is for an attribute and the co-occurrence of words inside the same attribute. Once token embeddings are generated, they can be compared against the embeddings of the paired record. Huang et al. (2023) compares each token embedding against the most similar token embedding from the other record and generates similarity (with scalar product) and difference vectors (with element-wise difference). Similarly, Fu et al. (2021) uses attention mechanisms to find the most relevant token to compare against and performs element-wise difference for comparing. Instead of comparing word embeddings against each other, Li et al. (2020a) compares the word embeddings of an instance against a representation of the other record\u2019s attribute. Attention mechanisms are used to generate the attribute representations and comparison is done with an element-wise difference. For every approach, after the token embeddings have been compared, they are condensed together in a unique representation for the record using an NN. The representation of each record is concatenated together and fed to the entity-matching model.\nAttribute-Level Comparison\nWhen records are compared at the attribute level, embeddings must be generated for each attribute. Mudgal et al. (2018); Ebraheem et al. (2018); Kasai et al. (2019) generate word embeddings for each word inside an attribute, then create an attribute embedding by merging the word embeddings together using variants of recurrent neural networks (e.g., LSTM (Hochreiter and Schmidhuber, 1997)) or any other method (e.g., summation). Nie et al. (2019) use attention mechanisms to strategically combine the records\u2019 token embeddings. Bogatu et al. (2021); Lattar et al. (2020) use sentence encoders such as the universal sentence encoder (Cer et al., 2018) to directly output embeddings for attributes. Once the attribute embeddings are generated for both records of a pair, they can be compared using cosine distance (Lattar et al., 2020), or element-wise difference (Mudgal et al., 2018; Ebraheem et al., 2018; Kasai et al., 2019). For every approach, the result of the comparison for each attribute is concatenated into a vector and fed to the last layers of the model for conducting the prediction task.\nRecord-Level Comparison\nWang et al. (2022) generates record embeddings using a transformers\u2019 encoder. Tabular data is transformed into text using the serialization method of Ditto (Li et al., 2020b). The embeddings are compared using element-wise differences and fed to the model for the final prediction task. Because important information might be lost in the comparison vector (e.g., what the records have in common), they also give to the model the embedding of the concatenated records."
        },
        {
            "heading": "5.3.4 Learned Comparison",
            "text": "The third category groups all entity-matching approaches that do not have a comparison bias and lets the model learn how to compare instances by itself. These approaches generally adopt superior modeling techniques, such as attention. Mudgal et al. (2018) designed a NN with attention mechanisms and trained the model for entity-matching tasks. Zhao and He (2019) follows a three-step process to detect records referring to the same real-world entity. First, a model predicts the semantic type (e.g., company name, person name) for each attribute of a record. Second, an entity-matching model trained specifically for the semantic type predicted in the previous step is selected and predicts whether the attribute\u2019s value of both records refers to the same real-world entity (e.g., \"Bill Gates\" versus \"William Gates\"). The binary predictions (which are generated for each attribute) are fed to a onelayer NN to predict if the records are a match. As discussed in Section 5.1.3, the authors in Narayan et al. (2022) evaluate LLMs on data cleaning tasks, notably entity-matching. To perform entity-matching, they first transform the tabular records into text format (see Section 5.1.3 for more details on the serialization procedure), show the serialized record to the model, then prompt it to predict whether the records refer to the same real-world entity or not. Li et al. (2020b); Brunner and Stockinger (2020) added a classification layer on top of a pre-trained transformer model and fine-tuned it for the entity matching problem. Similarly to the feature cleaning approaches with transformers (see Section 5.1.3), both approaches transform tabular data into text format by concatenating the values of a record. Additionally, Li et al. (2020b) adds special tokens to differentiate the columns\u2019 names from the values of a record. For example, for a record of a person living in Montreal, Canada, a serialized entry could look like the following: [ATTRIBUTE NAME] city [ATTRIBUTE VALUE] Montreal [ATTRIBUTE NAME] country [ATTRIBUTE VALUE] Canada. In addition, authors added special tokens in the string for special types (e.g., phone number, zip code)."
        },
        {
            "heading": "5.3.5 Other Approaches",
            "text": "In this section, we cover other entity-matching approaches that are not included in the previous categories.\n\u2013 Wu et al. (2020): The authors propose an unsupervised approach for entity matching (i.e., no training data is required). Essentially, two Gaussian distributions are fit to the data, one to the positive pairs, and the other, to the negative pairs. The Gaussian distribution can then be used to estimate the likelihood of a pair of a record being a match or not. They propose optimizations to (1) reduce the number of parameters of the Gaussian distributions, (2) improve the prediction accuracy of their approach, and (3) address the performance issues caused by features with zero variance (i.e., the singularity problem in pattern recognition (Bishop and Nasrabadi, 2006)). \u2013 Gottapu et al. (2016): Instead of predicting if two records refer to the same entity, they predict to which entity each record refers. In other words, each class in their approach corresponds to a real-world entity. To classify records, they transformed each record into a sequence of word embeddings and then fed it to a convolutional NN for classification."
        },
        {
            "heading": "5.3.6 Common Techniques to Improve Performance",
            "text": "In this section, we cover various techniques commonly used to improve the entity-matching approaches\u2019 performance. Namely, these are data augmentation, semi-supervised techniques, active learning, and transfer learning.\nData Augmentation\nBecause entity matching is an unbalanced problem (i.e., there are more records that do not refer to the same real-world entity than do), generating negative pairs is fairly easy. Two records randomly selected from a dataset will most likely not refer to the same real-world entity. Wang et al. (2022) randomly select two records that are similar but are not a match and assign them the negative label. The authors argue that selecting similar instances is superior to random selection since it makes the prediction problem more difficult for the model. Generating positive pairs is, however, more difficult than generating negative pairs. Wang et al. (2022) applies task-specific data transformations that preserve the label of a pair of records. Wang et al. (2022) picks any two records whose embeddings have a cosine distance inferior to a fixed threshold. We describe in Section 5.3.4 how the embeddings are generated. Instead of selecting instances, Li et al. (2020b); Huang et al. (2023) apply transformations to a record that should preserve the record\u2019s identity (e.g., deleting an attribute\u2019s value). The modified record can replace the initial record in an already existing pair of records (Li et al., 2020b), or the modified record and the initial record can become a new pair (Huang et al., 2023). To minimize the risk that the pair is not truly a match, Li et al. (2020b) proposes MixDA, a technique inspired by MixUp (Zhang et al., 2017). The approach interpolates the initial record embedding with its transformed version and adds the interpolated version to the training dataset.\nSemi-Supervised Techniques\nTo create positive pairs, Kasai et al. (2019) assigns a positive label to any pair that is rated as very likely to be a positive pair according to the entitymatching model. The model is then re-trained on these newly labeled samples and the process can be repeated. Huang et al. (2023) used adversarial learning to generate additional training samples. The goal of the generator model is to fool the discriminator model (i.e., the entity-matching model) into believing that a generated record refers to the same real-world entity as a real record.\nActive Learning\nKasai et al. (2019) observed that entity matching is an imbalanced problem (i.e., there exist more negative pairs than positive pairs), and if it is not considered in the active learning process, more negative pairs than positive pairs might be labeled. Hence, the approach tries to send to the oracle an equal amount of positive and negative pairs by referring to the prediction of the entity-matching model, so only uncertain pairs are sent (i.e., when the probability that a pair is a match is close to 0.5). The oracle receives an equal amount of pairs slightly under and over the threshold (of 0.5). Li et al. (2020b)\u2019s approach prioritizes samples with the highest informativeness. They start by grouping unlabeled pairs into clusters based on a custom similarity measure. A pair\u2019s informativeness is based on the number of samples that will be labeled if the current pair is labeled positively or negatively. Meduri et al. (2020) benchmarked different active-learning-based approaches for entity matching. They tried different combinations of models (i.e., linear, non-convex, treebased, and rule-learning) with different acquisition functions to select pairs (i.e., query-by-committee (Freund et al., 1997), how close a sample is from the decision boundary, and likely false positives/negatives (Qian et al., 2017)). They adapt blocking (see Section 5.3.1) to sample selection in active learning to fasten the process.\nTransfer Learning\nTransfer learning refers to reusing a model trained on a different but related task hoping some of the knowledge will be transferable for the task at hand (Wikipedia, 2023e). To have better performances, some entity-matching approaches reuse the models trained for another entity-matching problem for the task at hand. For example, a model used to match citation records between two databases could be used for two other databases with different schema (Kasai et al., 2019). Here, we discuss the techniques used to make the transfer of knowledge learned from one task to another more efficient. As discussed in Section 5.3.3, Kasai et al. (2019) predicts if two records are a match by comparing the records\u2019 tuple embedding. In order to maximize the benefits of transfer learning, they trained the model generating tuple embeddings to be dataset-invariant. In other words, the goal is for the embeddings from the target domain to be similar to the ones of the source domain. The authors use adversarial learning to train the embedding model to generate dataset-\ninvariant embeddings. The role of the discriminator model in the adversarial setting is to predict the provenance of a record (i.e., to predict whether a record comes from the source or target domain). Similarly, Jin et al. (2021) tries to increase domain adaptation of their approach (i.e., transfer learning) by minimizing the KL divergence between embeddings from the source domain with the ones from the target domain.\nEntity Matching: Summary and Research Opportunities\nGeneral existing approaches: The approaches can be grouped based on how the records are compared (for the purpose of entity matching). We identified the following three categories.\n\u2013 Token comparison: A set of common and different words between two records are generated and fed to an ML model that will predict whether the records match or not. \u2013 Latent space comparison: The records are compared in a latent space using a distance measure such as cosine distance. The distance measure can then be used by an ML model to predict whether the records match or not. The embeddings used for comparison can be token, attribute, or record embeddings. \u2013 Learned comparison: A model ingests both records and predicts whether the records match or not. Contrary to other approaches, the model is entirely responsible for learning how to compare records.\nResearch opportunities:\n\u2013 Explore how comparing records using different latent representations at the same time can improve entity matching. In Section 5.3.3, we described three types of latent-space representation approaches (token-level, attribute-level, and record-level). Combining all three representation approaches could enable the detection of low-level differences between records as well as high-level ones, which could improve performances.\n5.4 Outlier Detection\nIn Section 5.2.4, we presented approaches to detect mislabels using ideas from outlier detection. Here, we cover approaches to detect outliers in a dataset (not for the purpose of detecting mislabels). Borrowed from Ilyas and Chu (2019), we identified three categories of outlier detection approaches: statisticbased, distance-based, and model-based approaches. Note that an approach may belong to more than one category. For example, the approach in Terrades et al. (2022) uses a model to detect outliers (model-based), but also preprocess data before feeding it to the model using distance-based approaches."
        },
        {
            "heading": "5.4.1 Statistic-Based Approaches",
            "text": "Statistic-based approaches detect outliers by looking for records that are in low-probability regions according to a stochastic model (Ilyas and Chu, 2019). To detect outliers, the authors in Pit-Claudel et al. (2016) model a dataset\nusing generative models and use them to detect outliers. Any record with a low probability according to the generative models is marked as an outlier. To improve the performance of their approach, the records are enriched with metadata (e.g., the string length of an attribute)."
        },
        {
            "heading": "5.4.2 Distance-Based Approaches",
            "text": "Distance-based approaches measure the distance between a record and other records to detect outliers Ilyas and Chu (2019). For example, a record that is far from any other data points could be declared an outlier. Guan et al. (2016) compares a record against its neighbors to determine if it is an outlier. If a significant portion of its neighbors come from another class, the record might be an outlier. The authors argue that previous works often incorrectly detect legitimate records from a class with a low number of records as outliers since the records are often in regions with a lot of records from other classes. To mitigate this problem, a weighted average of the labels in the neighborhood of a record is performed to detect outliers and give a larger weight to samples that are close and that are from the minority class. Similarly, Terrades et al. (2022) uses the neighbor of a sample to determine if it is an outlier. The authors make the observation that outliers often are in clusters of samples with high heterogeneity of classes, or in clusters where a large proportion of the samples have the same class. Thus, to detect outliers, both aspects are measured and fed to a model for prediction."
        },
        {
            "heading": "5.4.3 Model-Based Approaches",
            "text": "Model-based approaches primarily rely on a classifier to detect outliers Ilyas and Chu (2019). Detecting outliers is formulated as a binary prediction task. Liu et al. (2019) proposes an approach to detect outliers that is inspired by the GAN architecture (Goodfellow et al., 2014). Their approach consists of a discriminator model and k generator models whose goal is to generate synthetic samples. Each generator is trained on a different subset of similar data points to increase the diversity of generated samples. Once trained, the discriminator model can be used to detect outliers. The authors in Jiang et al. (2023) use the reconstruction error of an autoencoder (similar to autoencoderbased feature cleaning, see Section 5.1.4) and the distance between a record and the closest class distribution to detect outliers (similar to distance-based approaches, see Section 5.4.2). They conducted experiments with two distance measures, namely Mahalanobis and Euclidean distance. The more a record is poorly reconstructed and far from any class distribution, the more likely it is an outlier."
        },
        {
            "heading": "5.4.4 Other Approaches",
            "text": "In this section, we cover outlier detection approaches that were not included in the previous categories.\n\u2013 Papastefanopoulos et al. (2021): Several unsupervised detecting approaches are proposed to detect outliers; however, according to the authors they rely predominantly on a single metric (e.g., distance, density, etc.), and each has its shortcomings. In order to address this issue, a voting ensemble-based approach of multiple outlier detection methods relying on different metrics is introduced. To improve performances, non-informative methods are filtered from the ensemble using an unsupervised spectral feature selection algorithm (Zhao and Liu, 2007). \u2013 Domingues et al. (2018): The authors conduct a study in which they compare outlier detection methods. The compared methods are grouped into probabilistic-based (e.g., robust kernel density estimators (Kim and Scott, 2012)), neighbors-based (e.g., Subspace outlier detection (Kriegel et al., 2009)), distance-based (e.g., Mahalanobis distance (Ben-Gal, 2005)), information theory-based (e.g., Kullback-Leibler divergence (Filippone and Sanguinetti, 2010)), NN-based (e.g., Grow When Required (GWR) network (Marsland et al., 2002)), domain-based methods (e.g., One-class SVM (Sch\u00f6lkopf et al., 1999)) and isolation methods (e.g., Isolation forest (Liu et al., 2008)). The study shows that isolation forest (Liu et al., 2008) is an efficient method for detecting outliers in terms of average precision, training and prediction time, and scalability. \u2013 Alimohammadi and Chen (2022): Similar to Domingues et al. (2018), the work focuses on evaluating techniques for detecting outliers in time-series data. The evaluated techniques include ML-based techniques (e.g., oneclass SVM (Sch\u00f6lkopf et al., 1999), statistical-based techniques (e.g., zscore (Rosner, 1983) and regression-based techniques (e.g., polynomial fit (Motulsky and Brown, 2006). The experiments show that KNN is the optimal choice in terms of ML performance, simplicity of configuration, and computation time.\nOutlier Detection: Summary\nGeneral existing approaches:\n\u2013 Statistic-based approaches: Records are marked as outliers if they are in a low probability region according to a probability model. \u2013 Distance-based approaches: The distance between a record and others is measured to determine if the record is an outlier. \u2013 Model-based approaches: A model plays a central part in determining if a record is an outlier.\n5.5 Imputation\nIn this section, we cover approaches to impute values in datasets with missing values. Abidin et al. (2018) compares the performance of three ML algorithms (i.e., decision trees, k-nearest neighbor, and Bayesian networks) for imputation on 10 datasets. They found that Bayesian networks are the most\naccurate for most datasets, but they are expensive to train. (Razavi-Far et al., 2020) proposes two techniques, kEMI and kEMI+ to impute categorical and numerical missing data. Both techniques first utilize the K-Nearest Neighbors (KNN) algorithm as a local search algorithm to find the K-top similar records to a record with missing values. Then kEMI invokes the ExpectationMaximization Imputation (EMI) algorithm which uses the feature correlation among the K-top similar records to impute missing values. The idea is to use conditional probability distribution of a feature given the other features using only the K-top similar records. In contrast, kEMI+ repeatedly invokes EMI to gather a collection of estimates for the missing value and combines them using Dempster-Shafer fusion (AP, 1967; Dempster et al., 2008). Although kEMI+ can be more accurate than kEMI, it is less scalable. A novel technique proposed in (Silva-Ram\u00edrez and Cabrera-S\u00e1nchez, 2021) that combines Adaptive Resonance Theory (ART) algorithm (Carpenter et al., 1991) with Co-active Neuro-Fuzzy Inference System (CANFIS) (Tfwala et al., 2013) to impute missing data. CANFIS integrates the reasoning capabilities of fuzzy systems and the computational capabilities of NN such that the fuzzy logic provides a linguistic representation of variables and performs fuzzy inference, while the NN performs computations on the fuzzy rule outputs and maps them to crisp output values. ART complements CANFIS by providing unsupervised learning and clustering of the input data which helps the fuzzy rules construction and enhances the model representation capabilities.\nImputation: Summary\nGeneral existing approaches: The approaches covered in our study impute missing data by using feature correlation or by combining NNs with fuzzy systems.\n5.6 Holistic Data Cleaning\nIn this section, we present data cleaning approaches that try to clean more than one type of error at a time."
        },
        {
            "heading": "5.6.1 Data Cleaning Pipeline Generation",
            "text": "Instead of directly cleaning data, the approaches in this category try to find the optimal sequence and configuration of data cleaning tools. The process of building the optimal cleaning pipeline can be formulated as a search problem. Given a metric to optimize, the goal is to find the optimal ordering and configuration of cleaning tools. These kinds of approaches are sometimes referred to as a cleaning optimizer (Neutatz et al., 2021). The main factor that differentiates approaches from one another is the objective function and the optimization algorithm used, which we describe below.\nOptimally, one would want to decrease the total number of errors in the dataset. However, that information is not always known a priori. Thus, a common approach when cleaning for ML is to use the performance of the model trained on the cleaned dataset as a proxy of data quality (Berti-Equille, 2019; Krishnan and Wu, 2019). If the ultimate goal of cleaning the dataset is to improve the model\u2019s performance, this objective can be preferred to any other proxy of dataset quality. Li et al. (2019) showed that not all data cleaning tools improve a model\u2019s performance. Thus, cleaning a dataset using ML performance as a metric to maximize can avoid cleaning operations that decrease model performance. Other metrics can be used as well. For example, Krishnan and Wu (2019)\u2019s approach can minimize the number of outliers in a dataset or the number of integrity constraints violated.\nTechnically, ML hyperparameter optimizers, such as Python Hyperopt (Bergstra et al., 2013), could be used to configure the data pipelines. However, as Krishnan and Wu (2019) pointed out, these optimizers do not leverage the incremental nature of data cleaning. Effectively, instead of evaluating the impact of a data cleaning tool on a model\u2019s ML performance right after it has been applied to a dataset, ML optimizers evaluate complete solutions (i.e., a fully configured data cleaning pipeline). To address that problem, Krishnan and Wu (2019) uses beam search to find optimal pipeline configurations. The least promising pipelines are periodically pruned out based on the performance of the model trained on the current dataset. Another strategy proposed by Berti-Equille (2019) is to use Reinforcement Learning (RL) to find the optimal configuration for data-cleaning pipelines. In their work, an agent iteratively selects a data cleaning tool that will be applied to a dataset. The change in a model\u2019s performance after applying the data cleaning tool serves as the reward function of the agent, and the state is the last cleaning tool applied to the dataset. In addition to the data cleaning tools, pre-processing operations, such as normalization, can also be selected by the agent.\nInstead of searching for new pipeline configurations tailored to a dataset, Gemp et al. (2017) reuses data cleaning pipelines that were shown to be effective on similar datasets. For this to be effective, a good measure of similarity between datasets must be used. The authors chose to represent datasets by vectors of 22 dimensions composed of common meta-features from the literature, such as mean feature skew. The datasets\u2019 vectors are compared against one another using L1 distance. While the results were inconclusive, they argue that their approach could open the path to new ways of generating data-cleaning pipelines."
        },
        {
            "heading": "5.6.2 Other Approach",
            "text": "Li et al. (2019) benchmark different data cleaning techniques on real-world datasets and observe their impact on the performance of ML classification tasks. To perform their experiments, they used data cleaning tools from all the data cleaning categories discussed in this review (i.e., Section 5.1 to Section 5.5). Using different datasets, the authors measured the impact of using each\ntool individually on ML performance, then they repeated the experiments with different combinations of tools. Some of the findings of the study are that an improvement to a model\u2019s ML performance because of data cleaning will generalize to other models and that no single data cleaning tool is the best for all datasets.\nHolistic Data Cleaning: Summary and Research Opportunities\nGeneral existing approaches:\n\u2013 Data cleaning pipeline generation: Instead of directly cleaning data, these approaches try to find the best sequence and configuration of data cleaning tools to optimize a metric (e.g. ML accuracy of a model trained on the cleaned dataset).\nResearch opportunities: We elaborate on future research directions for holistic data cleaning in Section 6.5."
        },
        {
            "heading": "6 Future Directions",
            "text": "In Section 5, we proposed future work directions for each data cleaning activity. In this section, we suggest higher-level future work directions based on the full set of reviewed papers. We provide a simplified description of the research directions proposed in this section in Table 2.\n6.1 Data Augmentation\nAs mentioned in Section 5.1.8 and Section 5.3.6, datasets of data cleaning activities are often imbalanced. The datasets used for feature cleaning generally only have a few dirty (labeled) records (Nashaat et al., 2021; Heidari et al., 2019). Similarly, for entity matching, finding pairs of records referring to the same-world entity is more difficult than finding records not referring to the same entity (Kasai et al., 2019). Hence, it is common for data cleaning techniques to leverage data augmentation to generate more training samples for the class that is underrepresented. While it is true that data augmentation can improve a model\u2019s performance, it can also reduce it, if not properly tuned (Miao et al., 2021; Wei and Zou, 2019). For example, a data augmentation operator could transform a record in a way that its initial label does not apply anymore. Thus, when using data augmentation strategies, it is important to ensure they are correctly parameterized. Past studies in computer vision have tried to design search algorithms to automatically find the best parametrization of data augmentation techniques (Cubuk et al., 2019; Lim et al., 2019). Future works could continue in this direction and evaluate their approach for data cleaning tasks. While a good parametrization increases the chances that good samples are generated from the data augmentation tool, all samples are not guaranteed to be valid. Miao et al. (2021) proposes an approach to automatically filter out augmented samples that are invalid. Additionally, the approach automatically\nweights examples based on their likeliness to improve a model\u2019s performance. Thus, the model learns more from samples that are weighted more. While not a complete replacement for a good parametrization of data augmentation tools, this strategy makes configuring data augmentation tools less critical. We encourage researchers to develop approaches to automatically filter out augmented records that hinder a model\u2019s performance.\n6.2 Data Cleaning Datasets\nTo evaluate a data cleaning approach or train an ML model to be used in a data cleaning approach, one needs a labeled dataset. As building a labeled dataset is a laborious process (Gauen et al., 2017), researchers are interested in using the existing public datasets. Additionally, using public datasets provides a baseline for researchers to compare their approaches. The release of large public\ndatasets has been known to propel research in ML (Gauen et al., 2017). For instance, the release of ImageNet datasets (Deng et al., 2009) led to the birth of new neural architectures such as AlexNet (Krizhevsky et al., 2017) and VGG (Simonyan and Zisserman, 2014). For data cleaning tasks, there exist popular datasets used across many papers such as the Hospital dataset, the Flights dataset, and the Beers dataset (all of which are described in Mahdavi et al. (2019)). However, for researchers, finding and selecting a dataset to evaluate their data cleaning approaches can be a laborious process as it entails searching through many data cleaning papers for datasets. Furthermore, because data cleaning papers sometimes forget to provide the source of a dataset they used (and instead reference the paper they extracted the dataset from), obtaining the actual dataset can be arduous. To address this problem, researchers have published a dataset repository for entity matching tasks (Das et al., 2016). A similar work could be done for other data cleaning activities, such as feature cleaning.\nProviding a data repository for other data cleaning activities would open up other new research directions. For feature cleaning, one could build a taxonomy of errors in ML datasets using the dataset repository. This taxonomy would help researchers evaluate their feature-cleaning approach in a systematic way. Instead of solely selecting datasets based on previous works, the researchers would be able to select datasets that have the error types their approach tries to address.\nFinally, in the same effort to provide data-cleaning datasets to researchers, future works could target publishing labeled data-cleaning datasets. To address the lack of labeled datasets, previous works (Flokas et al., 2022; Liu et al., 2022; Neutatz et al., 2019) have used datasets with artificial errors (i.e., errors injected into clean data). Thus, in an attempt to enable researchers to evaluate their approach against real-world errors, we encourage future works to publish labeled data-cleaning datasets. Additionally, maintainers of any public ML dataset could facilitate the process of identifying data errors and repairs, so as to organically create labeled data-cleaning datasets.\n6.3 Tooling\nData cleaning is known to be tedious and time-consuming (Sambasivan et al., 2021). Researchers have reported that data cleaning can take up to 80% of data scientists\u2019 time (Press, 2022). However, as highlighted in Sambasivan et al. (2021), data cleaning is one of AI\u2019s most under-valued and de-glamorized aspects. To clean data efficiently, practitioners might look for data-cleaning tools. If the practitioner wants the latest advances in data cleaning, he may look for the studies\u2019 replication packages. However, not all papers provide a replication package. Out of the 101 papers included in our study, only 20 provided one. Additionally, using replication packages is not convenient for practitioners. They must familiarize themselves with the application programming interfaces (API), which might not be user-friendly and differ from one replication\npackage to another. Additionally, setting up the tool (e.g., installing dependencies) can be tedious and documentation may be lacking. Thus, to address this issue, M\u00fcller et al. (2021) proposes OpenClean, an open-source library where all data-cleaning approaches can be centralized and made available via a unique API. Similar initiatives have gained a lot of traction in ML, such as Scikit-learn (Pedregosa et al., 2011). On the contrary, OpenClean has not received any support apart from its creators two years ago. Thus, a practitioner looking for data-cleaning tools might be deterred from using this open-source library. We encourage future works on data cleaning to provide a ready-to-use implementation of their approach on open-source libraries such as OpenClean (M\u00fcller et al., 2021). Additionally, we suggest considering contributions to open-source libraries when evaluating data-cleaning approaches so as to incentivize researchers to contribute.\n6.4 Large Language Models\nWe presented in Section 5.1.3 and Section 5.3.4 data cleaning techniques that used LLMs to clean data. These approaches generally obtained state-of-the-art results on their respective tasks. Recently, OpenAI released ChatGPT 16, an LLM which has shown impressive performances on diverse tasks, from solving programming bugs (Surameery and Shakor, 2023) to passing the Bar exam (CNE, 2023). We believe that future work should continue proposing data cleaning approaches that leverage LLMs, so as to benefit from the improvements with these models and achieve state-of-the-art performances (Narayan et al., 2022).\nAs shown in Section 5.1.3 and Section 5.3.4, Narayan et al. (2022) ask questions in natural language to an LLM in order to clean data. For example, Narayan et al. (2022) uses the following sentence to clean data: \"Is there an error in attribute X\" (where X is the name of an attribute), to which the model answers \"yes\" or \"no\". Before asking the question, the tabular record is presented to the model in a special format that we describe in Section 5.1.3 and Section 5.3.4. Their approach achieved state-of-the-art performances. We believe that an interesting future research direction would be to explore and compare different ways tabular data can be formatted for data cleaning with LLMs. Similarly, we encourage researchers to explore and compare different ways to prompt an LLM for data-cleaning tasks. This could build upon the current knowledge in prompt engineering (White et al., 2023).\nFinally, we encourage future works to train and share LLMs trained on data cleaning tasks. Other researchers and practitioners could use these LLM models for their data-cleaning problems. A pre-trained model could leverage the knowledge it gained from previous data cleaning tasks for any other one. Similar contributions have been made in other fields (Beltagy et al., 2019; Araci, 2019). For example, in finance, Araci (2019) developed FinBert, a language model fine-tuned on finance texts. 16 https://openai.com/blog/chatgpt\n6.5 Holistic Data Cleaning\nExcept for the papers in Section 5.6, the papers included in our study considered each data cleaning task individually. However, in the real world, practitioners find themselves cleaning more than one type of error in a dataset. For example, a practitioner could have to remove outliers, and then clean the features in a dataset. Cleaning each data error individually could lead to inefficiencies. For example, Li et al. (2019) has shown that cleaning all data error types in a dataset does not necessarily improve a model\u2019s performance. Thus, a careful selection, configuration, and ordering of different data cleaning tools are important for better ML performance. In Section 5.6, we presented two approaches to perform holistic data cleaning (Berti-Equille, 2019; Krishnan and Wu, 2019). These approaches search for the best order and configuration of data cleaning tools. Because we have not found so many works on data cleaning pipeline generation (DCPG) approaches, we believe there is space in the literature for more research in that direction. Future works could build upon the work done in AutoML (He et al., 2021a), since the latter shares many similarities with DCPG. Indeed, DCPG could be seen as a subset of AutoML, where instead of considering the whole ML pipeline (as with AutoML), the search algorithm only focuses on a specific step in the pipeline (data cleaning).\nMore broadly, we encourage researchers to develop tools that try to clean more than one type of error in a dataset. A previous study (Wang et al., 2022) has demonstrated how two different types of errors (i.e., duplicates and feature errors) could be cleaned following a similar strategy. The authors (Wang et al., 2022) were able to adapt their entity-matching approach for feature cleaning with only minor modifications. Potentially, other data cleaning activities share similarities that are still uncovered. Thus, designing a learning approach that cleans more than one type of error at a time could clean a dataset more effectively than using each type of tool independently.\nOn an even broader scale, future work may concentrate on developing tools combining data cleaning with other data pre-processing activities. For example, in Section 5.6, we presented the work of Berti-Equille (2019), who combined other data preparation tasks (e.g., feature normalization and selection) with data cleaning techniques (e.g., outlier detection). Tae et al. (2019) proposed a framework that combines data cleaning with unfairness mitigation and poisoned sample removal.\nTo effectively design tools to holistically clean a dataset, one must understand how these tools interact with one another. However, we found little knowledge about that in our review of the literature. As we covered in Section 5.6, Li et al. (2019) studied how different types and combinations of data cleaning tools impacted the performance of a model trained on the cleaned dataset. Instead of only considering an ML model\u2019s performance, researchers could study how applying one type of data-cleaning activity affects the performance of another one. For example, it could be interesting to study how feature cleaning influences an entity-matching model. Maybe cleaning data errors would make entity matching easier. Additionally, future works could\nalso replicate Li et al. (2019)\u2019s study but with a larger set of tools, since Li et al. (2019) used only one tool for some of the data cleaning activities that were considered (e.g., label cleaning). This work could use some of the tools reviewed in this paper.\n6.6 Interactive Data Cleaning\nMost of the papers covered in this SLR (92/101 papers) provide standalone data-cleaning solutions that do not need someone\u2019s efforts to function (except for dataset labeling). In other words, the approach automatically cleans data once sufficient training data is provided. However, in practice, human experts are often involved in the cleaning process, since it is usually impossible for machines to clean datasets flawlessly (Ilyas and Chu, 2019). We refer to cleaning data using an expert\u2019s feedback as interactive data cleaning. These experts can provide valuable input to the data-cleaning process by solving various kinds of uncertainties. For example, for feature error repair (see Section 5.1), experts can validate the repairs proposed by the approach to ensure that no errors are introduced in the dataset. Considering the low number of interactive datacleaning approaches covered in this SLR and their relevance in practice, we encourage the community to develop such solutions."
        },
        {
            "heading": "7 Related Works",
            "text": "Previous works partially cover DC&ML. The authors in Ilyas and Chu (2019) explain data cleaning and describe different data cleaning activities (e.g., entity matching, outlier detection). The authors reserved a full chapter to MLbased techniques for data cleaning. However, their work is not as exhaustive (in terms of approaches covered) as the work presented in this paper, and has a broader scope: all of data cleaning (not limited to ML). The authors in Neutatz et al. (2021) present their vision of a holistic data-cleaning framework for ML applications. They also provide an overview of recent DC4ML or ML4DC approaches. Contrary to our work, they only provide an overview of the topic and do not examine the different techniques systematically and in detail as we do in this work. Similarly, Ilyas and Rekatsinas (2022) review the relationship between data cleaning and ML (i.e., DC4ML and ML4DC) and present current challenges in data cleaning for ML applications. In addition, they describe robust ML approaches. Their work is different from ours because their goal is not to systematically review the literature. The authors in Thirumuruganathan et al. (2020) explain how DL techniques can be leveraged for cleaning data and present other studies that leverage DL to clean data. As a vision paper, it proposes some research directions to the community. In Roh et al. (2019), authors conducted a survey of data collection for ML. They argue that data collection is composed of different processes, which feature cleaning, label cleaning, and entity matching are part of. Similar to ours, the\ndata cleaning approaches presented are part of ML4DC or DC4ML. Contrary to our work, their scope is broader and they are not performing a systematic literature review. In the extended version of their study (Whang et al., 2023), in addition to covering data collection for ML, the authors cover robust model training and fair model training. Similar to the two previous works, a survey on data management for ML is reported in Chai et al. (2022) and includes a section to describe data cleaning for ML. Dong and Rekatsinas (2018); Laure et al. (2018); Chu et al. (2016b) are tutorials that briefly discuss how ML can be used for data cleaning.\nOther related works focus on one of the data-cleaning activities covered in our review (e.g., feature cleaning, label cleaning, etc.). For example, Christophides et al. (2020) reviews the literature on entity matching including a section on entity matching using ML. Barlaug and Gulla (2021) performs a survey on neural networks for entity matching. Authors in Johnson and Khoshgoftaar (2022); Feng et al. (2023); Karimi et al. (2020) review techniques to handle labeling errors in ML datasets including label cleaning. Authors in Wang et al. (2019); Boukerche et al. (2020) perform a survey of outlier detection techniques and cover ML-based techniques. The works of Lin and Tsai (2020); Adhikari et al. (2022) survey techniques to impute missing data and discuss how ML can be used to impute missing data."
        },
        {
            "heading": "8 Threats to Validity",
            "text": "We use the threats to validity described in Feldt and Magazinius (2010); Zhou et al. (2016) to analyze the limitations of our works. We detected three types of threats to validity in our work: construct validity, internal validity, and conclusion validity.\nConstruct validity. First, there is a risk that our search query does not allow us to exhaustively cover the field of interest of this study, preventing us from effectively answering our RQs. As mentioned in Section 2.4, we did preliminary searches to build a list of keywords that most likely capture the majority of the papers we are interested in. However, there is a risk that we missed keywords that would have led us to papers relevant to our study. To mitigate that threat, we performed snowballing, as described in Section 2.6. The second threat relates to the existence of papers relevant to the study but published after our data collection date, and hence, not included in this paper. Future works should consider replicating our study. There is also a risk that the academic databases used in this study might be incomplete, i.e., missing some papers relevant to our study. To mitigate this threat, we searched on Google Scholar, which allowed us to find papers from other databases (which we had access to but were not on our list). Another threat to construct validity relates to the risk that our inclusion and exclusion criteria might have excluded works relevant to our study. However, similar criteria have been used successfully in previous studies (Tambon et al., 2022). Also, papers relevant to our study might have been excluded because they processed a different data type than\nimage, text, or tabular data (i.e., the data types considered in our study). However, as mentioned in Section 2.2, we selected three of the most common data types. Thus, we believe our scope allowed us to cover the most relevant papers for our study.\nInternal validity. There is a risk that papers relevant to our study have been incorrectly excluded from our study while (1) applying inclusion and exclusion criteria (Section 2.5.3) or while (2) applying quality control questions (Section 2.5.4). To avoid excluding relevant papers, in the first filtering step, we included papers in case of doubt. As mentioned in Section 2.5.3, a second researcher reviewed the list to exclude any paper that should not be included. To avoid excluding relevant papers in the second filtering step, a second researcher read every paper excluded because of the quality control questions and verified that the exclusion of a paper is justified, as mentioned in Section 2.5.4. There is also a risk that one of the authors misunderstood the approach proposed in a paper leading to an incorrect explanation of the paper in our review. To mitigate this threat, each reviewer asked other reviewers to read the paper and share their understanding whenever they felt they did not fully understand the approach. Furthermore, a researcher verified that the papers\u2019 descriptions in our review were coherent with the papers\u2019 abstracts.\nExternal validity. External limitations of our study are related to the generalization of the presented results. We attempted to cover different topics in DC4ML and ML4DC as much as possible. Nevertheless, given the scope and selection criteria used in our study, we believe that our findings can be generalized.\nConclusion validity. There is a risk that our study can not be reproduced. To mitigate it, we provide a replication package containing the list of papers retained after each step in the filtering process described in Section 2.5, along with their quality ratings (see Section 2.5.4) and extracted data (see Section 2.7) when applicable. Any script used to help us conduct the study is also provided in the replication package."
        },
        {
            "heading": "9 Conclusion",
            "text": "This study provides a comprehensive systematic review of the literature on DC4ML and ML4DC, i.e., data cleaning with and for ML. Our study includes 101 papers published between 2016 and 2022 inclusively, from various academic databases. We covered 6 different types of data cleaning activities: feature cleaning, label cleaning, entity matching, outlier detection, imputation, and holistic data cleaning. Based on our review, we provide 24 future research directions and we highlight some exciting data-cleaning approaches that can be further extended. Our future research directions revolve around 6 core ideas: (1) to improve data augmentation techniques for data cleaning, (2) to create more public data cleaning datasets, (3) to provide better tooling for cleaning datasets, (4) to study how LLMs can be used for data cleaning, (5) to explore holistic data cleaning approaches, and (6) to develop more interactive data\ncleaning approaches. We hope that this paper will serve as a solid foundation for future works on this important topic.\nAcknowledgements This work is funded by the Fonds de Recherche du Quebec (FRQ), the Canadian Institute for Advanced Research (CIFAR), and the National Science and Engineering Research Council of Canada (NSERC). We would like to thank Dr. Hyacinth Ali for contributing to improving this SLR with his valuable comments.\nDeclarations\nData availability\nAll data generated or analyzed during this study are available in the GitHub repository to help reproduce our results: https://github.com/poclecoqq/ SLR-datacleaning.\nCompeting interests\nThe authors declare that they have no conflict of interest."
        }
    ],
    "title": "Data Cleaning and Machine Learning: A Systematic Literature Review",
    "year": 2023
}