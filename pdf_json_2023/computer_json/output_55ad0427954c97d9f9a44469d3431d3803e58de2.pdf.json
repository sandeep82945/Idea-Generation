{
    "abstractText": "Factual Error Correction (FEC) aims to rectify false claims by making minimal revisions to align them more accurately with supporting evidence. However, the lack of datasets containing false claims and their corresponding corrections has impeded progress in this field. Existing distantly supervised models typically employ the mask-then-correct paradigm, where a masker identifies problematic spans in false claims, followed by a corrector to predict the masked portions. Unfortunately, accurately identifying errors in claims is challenging, leading to issues like over-erasure and incorrect masking. To overcome these challenges, we present PivotFEC, a method that enhances fewshot FEC with a pivot task approach using large language models (LLMs). Specifically, we introduce a pivot task called factual error injection, which leverages LLMs (e.g., ChatGPT) to intentionally generate text containing factual errors under few-shot settings; then, the generated text with factual errors can be used to train the FEC corrector. Our experiments on a public dataset demonstrate the effectiveness of PivotFEC in two significant ways: Firstly, it improves the widely-adopted SARI metrics by 11.3 compared to the best-performing distantly supervised methods. Secondly, it outperforms its few-shot counterpart (i.e., LLMs are directly used to solve FEC) by 7.9 points in SARI, validating the efficacy of our proposed pivot task.",
    "authors": [
        {
            "affiliations": [],
            "name": "Xingwei He"
        },
        {
            "affiliations": [],
            "name": "A-Long Jin"
        },
        {
            "affiliations": [],
            "name": "Jun Ma"
        },
        {
            "affiliations": [],
            "name": "Yuan Yuan"
        },
        {
            "affiliations": [],
            "name": "Siu Ming Yiu"
        }
    ],
    "id": "SP:4261ca9a8490b659dc92d7bb17e2e4664fd24c9f",
    "references": [
        {
            "authors": [
                "Abhijeet Awasthi",
                "Sunita Sarawagi",
                "Rasna Goyal",
                "Sabyasachi Ghosh",
                "Vihari Piratla."
            ],
            "title": "Parallel iterative edit models for local sequence transduction",
            "venue": "Proceedings of EMNLP, pages 4260\u20134270, Hong Kong, China. Association for Computational Linguis-",
            "year": 2019
        },
        {
            "authors": [
                "Christopher Bryant",
                "Mariano Felice",
                "Ted Briscoe."
            ],
            "title": "Automatic annotation and evaluation of error types for grammatical error correction",
            "venue": "Proceedings of ACL, pages 793\u2013805, Vancouver, Canada. Association for Computational Linguistics.",
            "year": 2017
        },
        {
            "authors": [
                "Meng Cao",
                "Yue Dong",
                "Jiapeng Wu",
                "Jackie Chi Kit Cheung."
            ],
            "title": "Factual error correction for abstractive summarization models",
            "venue": "Proceedings of EMNLP, pages 6251\u20136258, Online. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Nicola De Cao",
                "Gautier Izacard",
                "Sebastian Riedel",
                "Fabio Petroni."
            ],
            "title": "Autoregressive entity retrieval",
            "venue": "ICLR.",
            "year": 2021
        },
        {
            "authors": [
                "Shuyang Cao",
                "Lu Wang."
            ],
            "title": "CLIFF: Contrastive learning for improving faithfulness and factuality in abstractive summarization",
            "venue": "Proceedings of EMNLP, pages 6633\u20136649, Online and Punta Cana, Dominican Republic. Association for Computational",
            "year": 2021
        },
        {
            "authors": [
                "Jiangjie Chen",
                "Rui Xu",
                "Wenxuan Zeng",
                "Changzhi Sun",
                "Lei Li",
                "Yanghua Xiao."
            ],
            "title": "Converge to the truth: Factual error correction via iterative constrained editing",
            "venue": "Proceedings of AAAI.",
            "year": 2023
        },
        {
            "authors": [
                "Qian Chen",
                "Xiaodan Zhu",
                "Zhen-Hua Ling",
                "Si Wei",
                "Hui Jiang",
                "Diana Inkpen."
            ],
            "title": "Enhanced LSTM for natural language inference",
            "venue": "Proceedings of ACL, pages 1657\u20131668, Vancouver, Canada. Association for Computational Linguistics.",
            "year": 2017
        },
        {
            "authors": [
                "Wenhu Chen",
                "Hongmin Wang",
                "Jianshu Chen",
                "Yunkai Zhang",
                "Hong Wang",
                "Shiyang Li",
                "Xiyou Zhou",
                "William Yang Wang."
            ],
            "title": "Tabfact: A large-scale dataset for table-based fact verification",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "2019. BERT: Pre-training",
            "year": 2019
        },
        {
            "authors": [
                "Joseph L Fleiss."
            ],
            "title": "Measuring nominal scale agreement among many raters",
            "venue": "Psychological Bulletin, 76(5):378\u2013382.",
            "year": 1971
        },
        {
            "authors": [
                "Xingwei He."
            ],
            "title": "Parallel refinements for lexically constrained text generation with BART",
            "venue": "Proceedings of EMNLP, pages 8653\u20138666, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.",
            "year": 2021
        },
        {
            "authors": [
                "Xingwei He",
                "Yeyun Gong",
                "A-Long Jin",
                "Weizhen Qi",
                "Hang Zhang",
                "Jian Jiao",
                "Bartuer Zhou",
                "Biao Cheng",
                "Sm Yiu",
                "Nan Duan"
            ],
            "title": "Metric-guided distillation: Distilling knowledge from the metric to ranker and retriever for generative commonsense rea",
            "year": 2022
        },
        {
            "authors": [
                "Xingwei He",
                "Victor O.K. Li."
            ],
            "title": "Show me how to revise: Improving lexically constrained sentence generation with XLNet",
            "venue": "Proceedings of AAAI, volume 35, pages 12989\u201312997.",
            "year": 2021
        },
        {
            "authors": [
                "Xingwei He",
                "Zhenghao Lin",
                "Yeyun Gong",
                "A-Long Jin",
                "Hang Zhang",
                "Chen Lin",
                "Jian Jiao",
                "Siu Ming Yiu",
                "Nan Duan",
                "Weizhu Chen."
            ],
            "title": "Annollm: Making large language models to be better crowdsourced annotators",
            "venue": "arXiv preprint arXiv:2303.16854.",
            "year": 2023
        },
        {
            "authors": [
                "Hayate Iso",
                "Chao Qiao",
                "Hang Li."
            ],
            "title": "Fact-based Text Editing",
            "venue": "Proceedings of ACL, pages 171\u2013182, Online. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Vladimir Karpukhin",
                "Barlas Oguz",
                "Sewon Min",
                "Patrick Lewis",
                "Ledell Wu",
                "Sergey Edunov",
                "Danqi Chen",
                "Wen-tau Yih."
            ],
            "title": "Dense passage retrieval for open-domain question answering",
            "venue": "Proceedings of EMNLP, pages 6769\u20136781, Online. Association",
            "year": 2020
        },
        {
            "authors": [
                "J Richard Landis",
                "Gary G Koch."
            ],
            "title": "The measurement of observer agreement for categorical data",
            "venue": "Biometrics, 33(1):159\u2013174.",
            "year": 1977
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "2020a. BART: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Patrick Lewis",
                "Ethan Perez",
                "Aleksandra Piktus",
                "Fabio Petroni",
                "Vladimir Karpukhin",
                "Naman Goyal",
                "Heinrich K\u00fcttler",
                "Mike Lewis",
                "Wen-tau Yih",
                "Tim Rockt\u00e4schel",
                "Sebastian Riedel",
                "Douwe Kiela"
            ],
            "title": "Retrieval-augmented generation for knowledge",
            "year": 2020
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "ROUGE: A package for automatic evaluation of summaries",
            "venue": "Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain. Association for Computational Linguistics.",
            "year": 2004
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized BERT pretraining approach",
            "venue": "CoRR, abs/1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Zhenghao Liu",
                "Chenyan Xiong",
                "Maosong Sun",
                "Zhiyuan Liu."
            ],
            "title": "Fine-grained fact verification with kernel graph attention network",
            "venue": "Proceedings of ACL, pages 7342\u20137351, Online. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Zhenghao Liu",
                "Xiaoyuan Yi",
                "Maosong Sun",
                "Liner Yang",
                "Tat-Seng Chua."
            ],
            "title": "Neural quality estimation with multiple hypotheses for grammatical error correction",
            "venue": "Proceedings of NAACL, pages 5441\u20135452, Online. Association for Computational Linguistics.",
            "year": 2021
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "ICLR.",
            "year": 2019
        },
        {
            "authors": [
                "Joshua Maynez",
                "Shashi Narayan",
                "Bernd Bohnet",
                "Ryan McDonald."
            ],
            "title": "On faithfulness and factuality in abstractive summarization",
            "venue": "Proceedings of ACL, pages 1906\u20131919, Online. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Hwee Tou Ng",
                "Siew Mei Wu",
                "Ted Briscoe",
                "Christian Hadiwinoto",
                "Raymond Hendy Susanto",
                "Christopher Bryant."
            ],
            "title": "The CoNLL-2014 shared task on grammatical error correction",
            "venue": "Proceedings of the Eighteenth Conference on Computational Natu-",
            "year": 2014
        },
        {
            "authors": [
                "Paul F Christiano",
                "Jan Leike",
                "Ryan Lowe."
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "NIPS, volume 35, pages 27730\u2013 27744. Curran Associates, Inc.",
            "year": 2022
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "J. Mach. Learn. Res., 21(1).",
            "year": 2020
        },
        {
            "authors": [
                "Vikas Raunak",
                "Arul Menezes",
                "Marcin JunczysDowmunt."
            ],
            "title": "The curious case of hallucinations in neural machine translation",
            "venue": "Proceedings of NAACL, pages 1172\u20131183, Online. Association for Computational Linguistics.",
            "year": 2021
        },
        {
            "authors": [
                "Marco Ribeiro",
                "Sameer Singh",
                "Carlos Guestrin."
            ],
            "title": "why should I trust you?\u201d: Explaining the predictions of any classifier",
            "venue": "Proceedings of NAACL, pages 97\u2013101, San Diego, California. Association for Computational Linguistics.",
            "year": 2016
        },
        {
            "authors": [
                "Abigail See",
                "Peter J. Liu",
                "Christopher D. Manning."
            ],
            "title": "Get to the point: Summarization with pointer-generator networks",
            "venue": "Proceedings of ACL, pages 1073\u20131083, Vancouver, Canada. Association for Computational Linguistics.",
            "year": 2017
        },
        {
            "authors": [
                "Darsh Shah",
                "Tal Schuster",
                "Regina Barzilay."
            ],
            "title": "Automatic fact-guided sentence modification",
            "venue": "Proceedings of AAAI, volume 34, pages 8791\u20138798.",
            "year": 2020
        },
        {
            "authors": [
                "James Thorne",
                "Andreas Vlachos."
            ],
            "title": "Evidencebased factual error correction",
            "venue": "Proceedings of ACL, pages 3298\u20133309, Online. Association for Computational Linguistics.",
            "year": 2021
        },
        {
            "authors": [
                "James Thorne",
                "Andreas Vlachos",
                "Christos Christodoulopoulos",
                "Arpit Mittal."
            ],
            "title": "FEVER: a large-scale dataset for fact extraction and VERification",
            "venue": "Proceedings of NAACL, pages 809\u2013819, New Orleans, Louisiana. Association for",
            "year": 2018
        },
        {
            "authors": [
                "Andreas Vlachos",
                "Sebastian Riedel."
            ],
            "title": "Fact checking: Task definition and dataset construction",
            "venue": "Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science, pages 18\u201322, Baltimore, MD, USA. Associa-",
            "year": 2014
        },
        {
            "authors": [
                "David Wadden",
                "Shanchuan Lin",
                "Kyle Lo",
                "Lucy Lu Wang",
                "Madeleine van Zuylen",
                "Arman Cohan",
                "Hannaneh Hajishirzi."
            ],
            "title": "Fact or fiction: Verifying scientific claims",
            "venue": "Proceedings of EMNLP, pages 7534\u20137550, Online. Association for Computational",
            "year": 2020
        },
        {
            "authors": [
                "William Yang Wang."
            ],
            "title": "liar, liar pants on fire\u201d: A new benchmark dataset for fake news detection",
            "venue": "Proceedings of ACL, pages 422\u2013426, Vancouver, Canada. Association for Computational Linguistics.",
            "year": 2017
        },
        {
            "authors": [
                "Thomas Wolf",
                "Lysandre Debut",
                "Victor Sanh",
                "Julien Chaumond",
                "Clement Delangue",
                "Anthony Moi",
                "Pierric Cistac",
                "Tim Rault",
                "R\u2019emi Louf",
                "Morgan Funtowicz",
                "Jamie Brew"
            ],
            "title": "Huggingface\u2019s transformers: State-of-the-art natural language processing",
            "year": 2019
        },
        {
            "authors": [
                "Wei Xu",
                "Courtney Napoles",
                "Ellie Pavlick",
                "Quanze Chen",
                "Chris Callison-Burch."
            ],
            "title": "Optimizing statistical machine translation for text simplification",
            "venue": "Transactions of the Association for Computational Linguistics, 4:401\u2013415.",
            "year": 2016
        },
        {
            "authors": [
                "Zheng Yuan",
                "Ted Briscoe."
            ],
            "title": "Grammatical error correction using neural machine translation",
            "venue": "Proceedings of NAACL, pages 380\u2013386, San Diego, California. Association for Computational Linguistics.",
            "year": 2016
        },
        {
            "authors": [
                "Thorne",
                "Vlachos",
                "Chen"
            ],
            "title": "The retrieval module primarily consists of two steps: First, we employ a pre-trained seq2seq model called GENRE",
            "venue": "(Cao et al.,",
            "year": 2023
        },
        {
            "authors": [
                "Indiana Jones"
            ],
            "title": "George Lucas created the character in homage to the action heroes of 1930s film serials",
            "year": 1930
        },
        {
            "authors": [
                "Indiana Jones"
            ],
            "title": "George Lucas created the character in homage to the action heroes of 1930s film serials",
            "year": 1930
        },
        {
            "authors": [
                "Evidence: Nick Jonas"
            ],
            "title": "Island Records; Island Records is a British-American record label that operates as a division of Universal Music Group ( UMG",
            "venue": "Original claim: Nick Jonas was released by a British-American record label. Mutated claim: Nick Jonas",
            "year": 2014
        },
        {
            "authors": [
                "Kurt Angle"
            ],
            "title": "After graduating college , Angle won a gold medal in freestyle wrestling at the 1995 World Wrestling",
            "year": 1995
        },
        {
            "authors": [
                "Evidence: Malcolm Young",
                "Malcolm Mitchell"
            ],
            "title": "Young ( born 6 January 1953 ) is an Australian retired musician",
            "year": 1953
        },
        {
            "authors": [
                "Malcolm Young"
            ],
            "title": "Except for a brief absence",
            "year": 1988
        },
        {
            "authors": [
                "Morrissey",
                "Steven Patrick Morrissey"
            ],
            "title": "professionally known as Morrissey , is an English singer",
            "year": 1959
        }
    ],
    "sections": [
        {
            "text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 9960\u20139976 December 6-10, 2023 \u00a92023 Association for Computational Linguistics"
        },
        {
            "heading": "1 Introduction",
            "text": "ChatGPT is an artificial intelligence chatbot released by OpenAI on November 30, 2022 and built upon on the company\u2019s Generative Pretrained Transformer (GPT) series of large language (LLMs) (Ouyang et al., 2022; OpenAI, 2023). Since its launch, ChatGPT has garnered significant global attention due to its comprehensive and eloquent responses across various knowledge domains. Within just two months, by January 2023,\n\u2020Corresponding authors.\nit has amassed over 100 million users. However, one drawback of ChatGPT is its tendency to generate text that is nonsensical, or unfaithful to the provided source input, referred to as hallucination (Maynez et al., 2020; Raunak et al., 2021). To address this issue, the research community has dedicated efforts to the development of factual error correction (FEC), aiming to rectify false claims with minimal modifications to make them better supported by the given evidence. Consequently, research focused on this task plays a crucial role in mitigating the problem of hallucinations in LLMs.\nThe most straightforward way to develop FEC systems is by fine-tuning pre-trained models, such as BART (Lewis et al., 2020a) and T5 (Raffel et al., 2020), on parallel data, consisting of false claims along with their corresponding corrections. Nevertheless, the availability of such paired data is restricted due to the tremendous labor and time required for human annotations.\nTo overcome the data scarcity, researchers (Shah et al., 2020; Thorne and Vlachos, 2021; Chen et al., 2023) make use of distant supervision from the fact verification dataset, FEVER (Thorne et al., 2018). FEVER is a large resource consisting of claims paired with evidence from Wikipedia, where each instance is labeled as either SUPPORTED or REFUTED based on whether the claim is supported or refuted by the corresponding evidence. Existing distantly supervised models typically follow the mask-then-correct approach (Shah et al., 2020; Thorne and Vlachos, 2021). Concretely, the fact verification classifier (FVC), trained on FEVER, acts as the masker, designed to find problematic spans within false claims. The token-level explanations (Ribeiro et al., 2016; Chen et al., 2017) of FVC are usually exploited as masks. The corrector is trained on the SUPPORTED data from FEVER, with the objective of restoring/generating the original/correct claim based on the masked claim and evidence, during training/inference. Furthermore,\n9960\nChen et al. (2023) propose using the mask-thencorrect method to iteratively refine the false claims instead of relying on a single pass. However, accurately identifying factual errors using FVC is nontrivial. This limitation often leads to over-erasure and incorrect masking issues, becoming a bottleneck that restricts the performance of FEC models.\nTo bypass these issues, we propose to solve the FEC task by introducing a pivot task, factual error injection (FEI), aiming to generate false claims by injecting factual errors into correct claims. Our main motivation is that the FEI task is relatively easier than the FEC task. As shown in Figure 1, an FEC model is expected to precisely identify the factual error in the false claim \u201cThe 2013 NBA draft was held in Pennsylvania.\u201d and correct it to \u201cThe 2013 NBA draft was held in New York.\u201d based on the given evidence. In contrast, the FEI task allows for multiple ways to introduce factual errors into a correct claim. For example, one can replace \u201cNew York\u201d with \u201cLos Angeles\u201d, substitute \u201c2013\u201d with \u201c2012\u201d, or even insert a negative word such as \u201cnot\u201d into the correct claim. This distinction demonstrates that the FEI task encompasses a considerably larger solution space compared to FEC. By exploring this expanded solution space, we can leverage the relatively easier nature of FEI to enhance the overall performance of FEC systems.\nOur second motivation stems from the fact that LLMs, such as GPT-3.5, can serve as an excellent data annotator in few-shot settings, rivaling or even surpassing the performance of crowdsourced annotators (He et al., 2023). Inspired by this, we use LLMs to solve the FEI task, specifically generating false claims for correct claims. By doing so, we obtain a sufficient amount of paired data, which will be further used to train the FEC corrector.\nOur contributions are summarized as follows: (1) We propose PivotFEC1, a method that uses a Pivot task, factual error injection, to enhance FEC\n1Our code is available at: https://github.com/ NLPCode/PivotFEC.\nwith LLMs in few-shot settings. (2) Compared with distantly supervised baselines, PivotFEC only requires 8 labeled samples from FECDATA, eliminating the need for labeled data, FEVER, to train the FVC. (3) Extensive experiments conducted on the FECDATA dataset demonstrate that PivotFEC outperforms distantly supervised baselines by a large margin, achieving a new state-of-the-art (SOTA) result on the test set with scores of 66.3 on SARI and 66.68 on ROUGE-2. (4) PivotFEC exhibits much better performance than its few-shot counterpart (66.3 vs. 58.43 on SARI), where LLMs are directly used to solve FEC, confirming the effectiveness and necessity of our proposed pivot task."
        },
        {
            "heading": "2 Problem Statement",
            "text": "Factual error correction aims to revise the factual errors in claim C with minimal edits and generate a revised claim C \u2032 based on the provided evidence E. C \u2032 should be grammatical, supported by the evidence, and correct the factual errors in C."
        },
        {
            "heading": "3 Preliminary",
            "text": "In this section, we will introduce in-context learning and how to solve FEC using LLMs with incontext few-shot learning via prompting."
        },
        {
            "heading": "3.1 LLMs with In-context Learning",
            "text": "LLMs, especially ChatGPT, have demonstrated remarkable few-shot capability in various downstream tasks. Therefore, it is natural to employ LLMs for addressing the FEC task in a few-shot setting. Building upon the approach introduced by GPT-3 (Brown et al., 2020), we utilize LLMs with in-context few-shot learning through prompting to tackle FEC. Rather than fine-tuning LLMs specifically for individual tasks, we can efficiently prompt the model by providing a small set of input-output exemplars that demonstrate the task."
        },
        {
            "heading": "3.2 Factual Error Correction with LLMs",
            "text": "To address FEC using LLMs, we begin by choosing a set of demonstrated examples. Each example comprises three elements: a gold evidence, a mutated claim, and an original claim. The objective is for LLMs to learn how to modify the mutated claim based on the provided samples. Figure 3 illustrates a simplified prompt where the LLM (in this case, ChatGPT) accurately corrects the factual error in the mutated claim by replacing \u201cLos Angeles\u201d with \u201cNew York.\u201d For the full prompt of the FEC task, please refer to Table 6 in Appendix C."
        },
        {
            "heading": "4 Approach",
            "text": "In this section, we will first provide an overview of PivotFEC in \u00a74.1. As illustrated in Figure 2, our method comprises three main steps. We will begin by introducing the FEI task and demonstrating the utilization of LLMs to address the FEI task in \u00a74.2. Next, we will present the process of gathering synthetic paired data for the FEC task in \u00a74.3. Finally, we will demonstrate the training of the corrector using the synthetic data in \u00a74.4."
        },
        {
            "heading": "4.1 Overview",
            "text": "The main limitation in developing FEC systems is the scarcity of paired data comprising correct claims and their corresponding false claims. To mitigate this limitation, previous studies (Shah et al., 2020; Thorne and Vlachos, 2021; Chen et al., 2023) follow the mask-then-correct method, with the assumption that there are sufficient human annotated fact verification data (i.e., FEVER), which is used to train the FVC. They train the corrector by masking certain portions of correct claims and then recovering them. Therefore, during testing, it becomes necessary to identify factual errors within false claims and mask these errors with the FVC before using the corrector to revise them. Con-\nsequently, previous approaches suffer from issues such as over-erasure and incorrect masking.\nOur primary motivation is to generate false claims by injecting factual errors into correct claims. This allows us to obtain FEC data consisting of correct claims paired with their corresponding false claims, which can be directly used for training the FEC corrector. To achieve this goal, we introduce the pivot task, factual error injection (FEI), for FEC, and then employ LLMs to address the FEI task using a few-shot in-context learning approach. Compared to the previous mask-thencorrect method, our approach eliminates the need to mask factual errors before correction, thus avoiding the over-erasure and incorrect masking issues. Moreover, our approach does not depend on labeled fact verification data. Instead, we only require correct claims and a few labeled FEC samples."
        },
        {
            "heading": "4.2 Factual Error Injection with LLMs",
            "text": "To address FEC, LLMs are expected to identify factual errors and correct them. As previously analyzed, FEI requires LLMs to introduce factual errors into correct claims and has a significantly larger solution space than FEC (see Figure 1). Therefore, we assume that FEI is comparatively easier for LLMs than FEC. This is why we intro-\nduce this pivot task for FEC. Similar to FEC, we also employ LLMs to tackle FEI using the few-shot in-context learning approach. For fair comparisons, we utilize the same demonstrated exemplars as used in few-shot FEC, with the only difference being the order of the original claim and mutated claim. The left portion of Figure 2 illustrates a simplified prompt for FEI, where the LLM (specifically ChatGPT) injects a factual error by substituting\u201cNew York\u201d with \u201cLos Angeles.\u201d For the complete prompt of the FEI task, please refer to Table 7 in Appendix D."
        },
        {
            "heading": "4.3 Creating Synthetic Data for FEC",
            "text": "We assume that correct claims are readily available. For each correct claim Ct, we use LLMs to inject factual errors into Ct via in-context learning with the prompt in Table 7. The generated claim is referred to as Cf . By doing so, we collect the synthetic data D = {(Ct1, Cf1 ), . . . , (Cti , Cfi )} for FEC, where Cti and C f i denote the i-th correct claim and the corresponding false (i.e., generated) claim, respectively."
        },
        {
            "heading": "4.4 Training FEC Corrector",
            "text": "After obtaining the synthetic data D, we acquire the FEC corrector by fine-tuning pre-trained language models, such as BART or T5 on this data. To be concrete, we concatenate the false claim Cf and the gold evidence or retrieved evidence, and directly input them into the encoder (refer to the right part of Figure 2 for the input format). For more detailed information on obtaining evidence for the false claim, please refer to Appendix B.1. During training, we optimize the corrector by maximizing the conditional probability of Ct:\nP (Cti |Cfi , Ei; \u03b8) = N\u220f\nn=1\np(Cti,n|Cti,j<n, Cfi , Ei; \u03b8),\nwhere \u03b8 represents the parameters of the corrector, Ei denotes the corresponding evidence, and Cti,j<n refers to the sub-sequence preceding Cti,n."
        },
        {
            "heading": "5 Experiment",
            "text": ""
        },
        {
            "heading": "5.1 Experimental Setups",
            "text": "Dataset. Following previous work, we evaluate our model on the evidence-based FEC dataset (FECDATA) (Thorne and Vlachos, 2021), created based on the large fact verification dataset, FEVER (Thorne et al., 2018). The construction of the\nFEVER dataset involves two main steps: claim generation and claim labeling. In the claim generation phase, annotators extract the original claims (i.e., correct claims) from Wikipedia, and then use six types of mutation: paraphrasing, negation, substitution of an entity/relation with a similar/dissimilar one, and making the claim more general/specific to generate mutated claims for original claims. In the claim labeling phase, annotators classify claims as SUPPORTED, REFUTED or NOTENOUGHINFO based on whether the claim is supported, refuted or unverifiable by the given evidence.\nFECDATA extracts the SUPPORTED and REFUTED data instances from FEVER, and uses the original claims and mutated claims as the paired data. Table 1 shows the basic statistics of this dataset. To gain further insights, we provide Figure 6 in Appendix A, displaying the distribution of mutation types for the REFUTED claims.\nEvaluation Metrics. For automatic evaluation, we resort to SARI (Xu et al., 2016)2 and ROUGE-2 (Lin, 2004)3 metrics. The SARI metric explicitly assesses the goodness of words in the revised claim that are added, deleted and kept by FEC models from the source (mutated claim), compared with the referenced ground truth (original claim). We report the n-gram F1 score for \u201ckeep\u201d operations (Keep), the n-gram precision score for \u201cdelete\u201d operations (Delete), the n-gram F1 score for \u201cadd\u201d operations (Add), and the average of these three scores (Final). ROUGE-2 measures the surfacelevel similarities between revised claims and reference claims. The SARI Final score serves as the primary evaluation metric due to its strong positive correlation with manual evaluation, as indicated by Thorne and Vlachos (2021)\u2019s statistical findings.\nBaselines. We consider three types of baselines: Fully Supervised Baselines estimate the ceiling performance of FEC models, under the assump2The evaluation code for SARI is available at: https: //huggingface.co/spaces/evaluate-metric/sari. 3The evaluation code for ROUGE is available at: https: //huggingface.co/spaces/evaluate-metric/rouge.\ntion that a substantial amount of data is accessible. For this purpose, we fine-tune BART-base and T5base on FECDATA, where the encoder takes the false claim and corresponding evidence as inputs, while the decoder generates the revised claim.\nDistantly Supervised Baselines adopt the \u2018mask-then-correct\u2019 pipeline, consisting of a masker and a corrector. The masker can take various forms, such as the token-level explanations (Ribeiro et al., 2016; Chen et al., 2017) of a fact verification classifier (FVC), random masking, or heuristic masking. The FVC is initialized with BERT-base (Devlin et al., 2019) or RoBERTa-large (Liu et al., 2019), and trained on FEVER. On the other hand, the corrector is trained exclusively on the SUPPORTED data instances from FEVER. (1) Dual encoder pointer network (DEPN) (Shah et al., 2020) utilizes an FVC to predict masked words and subsequently generates a revised claim using the dual encoder pointer generator with the copy mechanism (See et al., 2017). (2) T5 Masker-Corrector (T5MC) (Thorne and Vlachos, 2021) differs from DEPN in two main ways: (a) The corrector (i.e., generator) is based on T5-base. (b) It randomly masks words in the input claim during training, but during inference, heuristic masking is employed, where words do not present in the evidence are masked. (3) T5MC-MLM differs from T5MC in that it uses the masked language model BERT as the masker during inference. (4) T5MC-V is a variant of T5MC, using FVC as the masker to predict the masked tokens. (5) VENCE (Chen et al., 2023) iteratively executes steps of mask-then-correct over the claim to make it supported by the evidence.\nRule-based Baselines first generate synthetic paired data, and then train factual error correctors on them. Rule-based methods are employed to construct the inconsistent summaries with the aim of improving the faithfulness in abstractive summarization (Cao et al., 2020; Cao and Wang, 2021). We begin by utilizing spaCy4, a free open-source library for NLP, to recognize name entities in correct claims, and then implement two rule-based baselines for factual error correction: (1) The first rule-based method creates false claims by swapping named entities from the correct claims with alternative entities of the same entity type randomly chosen from the training dataset. This method is referred to as SwapEntity. (2) The second rule-based baseline resorts to the \u2018mask-then-fill\u2019 pipeline to\n4https://spacy.io/\ncreate false claims. In this approach, named entities within the correct claims are substituted with [MASK] tokens. The masked claims are then processed through the BART-base model to generate new claims by filling in the [MASK] tokens. These newly generated claims are considered as false claims. This method is termed MaskEntity.\nFew-shot Baselines contains two models: 8- shot T5-base fine-tunes T5-base using 8 data examples. 8-shot ChatGPT revises false claims by prompting ChatGPT with 8 demonstrated examples. For fair comparisons, the few-shot baselines and our PivotFEC use the same set of examples.\nImplementation Details. Our implementation details are shown in Appendix B."
        },
        {
            "heading": "5.2 Experimental Results",
            "text": "The main experimental results on the FECDATA test set in Table 2 reveal the following key findings: LLMs exhibit a remarkable few-shot ability for FEC. Directly fine-tuning T5 on the 8 labeled data instances (i.e., 8-shot T5-base) does not bring any improvement over previous distantly supervised baselines, such as VENCE. However, the few-shot in-context learning baseline (i.e., 8-shot ChatGPT) achieves a noteworthy SARI Final score of 58.43, surpassing VENCE (RoBERTa) by approximately 3 points. These results highlight the impressive few-shot capability of ChatGPT. Our proposed pivot task is highly effective for few-shot FEC. To demonstrate the effectiveness of our proposed PivotFEC, we compare it with the 8-shot ChatGPT model. To ensure fair comparisons, both few-shot models are based on ChatGPT. As shown in Table 2, our proposed model, 8-shot PivotFEC, far exceeds its few-shot counterpart (8- shot ChatGPT) by a significant margin across all metrics. The SARI metric increased from 58.43 to 66.30 and the ROUGE-2 score increased from 49.43 to 66.68.\nOn the other hand, PivotFEC also notably outperforms the rule-based baselines. Additionally, PivotFEC achieves the peak performance with just 2,000 synthetic data instances for training the corrector, while rule-based methods require 10,000 synthetic data instances to reach their peak performance when training the correctors. These results can be attributed to the enhanced quality of false claims produced by ChatGPT. Rule-based methods, by contrast, often produce false claims with suboptimal quality in two key aspects: (1) Grammatical\nerrors might be present in generated false claims. (2) False claims generated by rule-based methods might deviate from the original topics present in correct claims. With ChatGPT\u2019s remarkable incontext learning capabilities, injecting factual errors into correct claims hardly introduces grammatical errors or deviates from the original topics.\nThese compelling improvements establish a new SOTA result and provide strong evidence for the effectiveness of the pivot task in enhancing the performance of FEC. PivotFEC lags behind supervised baselines. While PivotFEC outperforms distantly supervised and few-shot models, there still exists a significant performance gap compared to supervised methods. For example, the supervised T5-base achieves a score of 74.24 on SARI Final, whereas PivotFEC only scores 66.30, indicating that there is ample room for further improvement. The retrieved evidence is inadequate compared with gold evidence. To further explore the ceiling performance of our method, we conduct experiments using gold evidence as well. The results reveal that when using gold evidence, PivotFEC\nimproves the SARI Final score by approximately 4 points. This demonstrates the inadequacy of retrieved evidence, which aligns with previous findings (Thorne and Vlachos, 2021). As our work mainly focuses on improving FEC through the introduction of the pivot task, we defer the improvement of evidence retrieval to future work."
        },
        {
            "heading": "5.3 More Analysis and Discussion",
            "text": "The experiments conducted in this section utilize ChatGPT with 8-shot in-context learning and retrieved evidence, if there is no particular statement.\nEffect of the Number of Synthetic Data. To show the effect of synthetic data generated with FEI on PivotFEC, we first generate synthetic data for FEC with 8-shot PivotFEC (ChatGPT), and then fine-tune T5-base on varied numbers of synthetic data instances. For comparison, we also evaluate the performance of T5-base trained on the gold data (FECDATA). As shown in Figure 4 (a), when the data size does not exceed 1k, the performance of 8-shot PivotFEC increases linearly with the increase of data, even matching the performance of T5-base trained on the gold data. Nevertheless, our\nmodel\u2019s performance plateaus once the data size reaches 2k. In contrast, T5-base trained on gold data continues to improve. Even when using all FECDATA training data, its performance does not reach its peak. This observation suggests that the generated data contains noise compared to the gold data, limiting the upper performance of PivotFEC.\nEffect of the Number of In-Context Examples. Table 2 demonstrates the superiority of PivotFEC over the few-shot FEC with ChatGPT under the 8-shot setting. To further validate the effectiveness of PivotFEC, we compare its performance with the few-shot FEC using ChatGPT with varying numbers of in-context examples. As depicted in Figure 4 (b), PivotFEC exhibits a notable improvement of approximately 7 to 10 points on the SARI Final score compared to the few-shot FEC model at different shots. When utilizing 8 demonstrated examples, our model reaches a plateau.\nEffect of Different LLMs. To further emphasize the advantages of our method, we compare it with 8-shot FEC across different LLMs, including three InstructGPT models (text-ada-001, textbabbage-001, and text-curie-001) and ChatGPT (gpt-3.5-turbo-0301). Figure 4 (c) illustrates our method consistently outperforms the few-shot FEC baseline across different LLMs. Moreover, both our method and the baseline exhibit noticeable performance improvements as the model parameters increase. However, even with small models, our method still performs exceptionally well. For example, the smallest model falls only around 5 points behind the largest model. In contrast, the baseline\u2019s performance is heavily influenced by the choice of model, particularly evident in text-ada-001, which experiences a decrease of approximately 12 points compared to the larger model, text-curie-001.\nEffect of Mutation Types. As shown in Figure 6 in Appendix A, the refuted claims mainly stem from four mutation types. Therefore, we construct four prompts, each composed of examples from the corresponding mutation. For prompts consisting of examples from negate, substitute similar, substitute dissimilar, and specific mutations, please refer to Tables 7, 8, 9, and 10. Table 3 shows that PivotFEC with negate prompt yields the best results, possibly because this mutation constitutes the largest portion of the test set. Additionally, we present the performance of PivotFEC on separate test cases. Figure 5 illustrates that: (1) PivotFEC performs well on a test case of the specific mutation type when using a prompt tailored to that type, and (2) the variations in PivotFEC performance, when us-\ning different prompts, mainly arise from the negate test case."
        },
        {
            "heading": "5.4 Human Evaluation",
            "text": "We conduct a human evaluation to compare PivotFEC with the fully supervised T5-base, 8-shot T5-base and 8-shot ChatGPT models. The fully supervised T5-base model utilizes gold evidence to rectify false claims, while the others use retrieved evidence. For each model, we randomly sample 50 cases and ask three annotators5 to assess the revised claims based on the following Boolean questions: (1) Is the revised claim grammatically correct? (2) Is the revised claim supported by evidence? (3) Has the factual error in the false claim been corrected? The final question, measuring the correction of factual errors, is the most important metric in our human evaluation. As shown in Table 4, our proposed model outperforms the few-shot baselines on the corrected metric; however, there is still a gap to reach the ceiling performance of the supervised baseline. Inter-annotator agreement measured by Fleiss\u2019 kappa (Fleiss, 1971) is 0.75, 0.86, and 0.81 for grammatical, supported, and corrected scores, implying substantial agreement (> 0.6) (Landis and Koch, 1977)."
        },
        {
            "heading": "5.5 Samples and Analysis",
            "text": "Table 5 presents the revised claims generated by our approach and the baselines. From this table, we observe that the 8-shot T5-base method cannot identify errors in false claims. Similarly, 8-shot ChatGPT often struggles to precisely locate errors within false claims, and tends to simply copy content from the evidence into the modified claims rather than correct them. For example, in the first example, although 8-shot ChatGPT corrects the factual error in the original sentence, it does not\n5All annotators hold Ph.D. degrees and are independent from our research.\nmake the minimal edits. As for the second example, 8-shot ChatGPT fails to identify the error in the false claim, resulting in a text that exhibits a low correlation with the original claim. This also explains why this method achieves a relatively high supported value but demonstrates a low corrected score during the human evaluation. Most notably, our method can accurately identify errors and make modifications based on the retrieved evidence, similar to the performance of the supervised T5 model."
        },
        {
            "heading": "6 Related Work",
            "text": ""
        },
        {
            "heading": "6.1 Grammatical Error Correction",
            "text": "Grammatical error correction (GEC) (Ng et al., 2014; Yuan and Briscoe, 2016; Bryant et al., 2017; Awasthi et al., 2019; Liu et al., 2021) refers to the process of identifying and rectifying grammatical errors in written text. It has practical applications in several domains, such as helping nonnative speakers enhance their writing skills, aiding language learners in improving their grammatical accuracy, and assisting professional writers in producing error-free and polished content. GEC aims to improve the accuracy and fluency of the language by fixing various grammatical errors, including missing prepositions, mismatched subject-verb agreement, misspellings, and word choice errors. In comparison, factual error correction involves correcting the factual errors instead of the grammatical errors in the given content, such as incorrect dates, names, or historical events."
        },
        {
            "heading": "6.2 Retrieval-Augmented Generation",
            "text": "Retrieval-augmented generation (Lewis et al., 2020b) combines the power of information retrieval and language generation techniques to elevate the overall quality of the generated content. For example, (He et al., 2022) use dense retrievers to retrieve relevant sentences from an external corpus for the given keywords to improve lexically constrained text generation (He and Li, 2021; He, 2021). By incorporating external knowledge, retrieval-augmented generation effectively mitigates the risk of generating inaccurate or nonsensical content. Furthermore, factual error correction is another facet of retrieval-augmented generation. It rectifies factual inaccuracies based on the evidence retrieved, thereby fitting under the broader umbrella of retrieval-augmented generation.\nGold Claim: The SpongeBob SquarePants Movie is a film. False Claim: The SpongeBob SquarePants Movie is a dog. Retrieved Evidence: title: The SpongeBob SquarePants Movie; context: The SpongeBob SquarePants Movie is a 2004 American animated adventure comedy film based on the Nickelodeon television series SpongeBob SquarePants . . . Supervised T5-base: The SpongeBob SquarePants Movie is a film. ! 8-shot T5-base: The SpongeBob SquarePants Movie is a dog. % 8-shot ChatGPT: The SpongeBob SquarePants Movie is an animated film based on the Nickelodeon television series SpongeBob SquarePants. ! 8-shot PivotFEC (ChatGPT): The SpongeBob SquarePants Movie is a film. !\nGold Claim: The Hunger Games are based on a novel. False Claim: The Hunger Games are based on a play. Retrieved Evidence: title: The Hunger Games; context: . . . The Hunger Games is a trilogy of young adult dystopian novels written by American novelist Suzanne Collins . . . Supervised T5-base: The Hunger Games are based on a novel. ! 8-shot T5-base: The Hunger Games are based on a play. % 8-shot ChatGPT: The Hunger Games is a trilogy of young adult dystopian novels. % 8-shot PivotFEC (ChatGPT): The Hunger Games are based on a novel. !\nTable 5: Revised claims generated by our model and baselines based on the evidence for false claims extracted from the test set. The supervised T5-base revises false claims based on the gold evidence, while others utilize retrieved evidence. For simplicity, we do not show the gold evidence. Text in blue, red, and orange colors represents factual errors, correct modifications, and copied text, respectively."
        },
        {
            "heading": "6.3 Fact Verification",
            "text": "Fact verification, also known as fact-checking, aims to validate the accuracy of a given claim by examining the available evidence. This field of study has been extensively researched in recent years. Researchers assess the claim by analyzing both unstructured sources, such as political news (PolitiFact) (Vlachos and Riedel, 2014; Wang, 2017), Wikipedia (FEVER) (Thorne et al., 2018; Liu et al., 2020), and scientific literature (Wadden et al., 2020), as well as structured sources, including Wikipedia tables (TabFact) (Chen et al., 2020) and knowledge base (Iso et al., 2020). Fact verification seeks to determine whether a claim is supported or refuted by evidence, while factual error correction takes it a step further. Factual error correction not only involves identifying factual errors but also requires modifying them to obtain correct claims."
        },
        {
            "heading": "7 Conclusion",
            "text": "In this paper, we present PivotFEC, which introduces a pivot task, factual error injection, to improve factual error correction. Specifically, we first intentionally introduce factual errors into correct claims using LLMs under the few-shot setting. By doing so, we can obtain enough synthetic paired data for FEC, consisting of correct claims paired with their corresponding false claims, which will be used to train the FEC corrector. As a result, PivotFEC demonstrates a significant improvement over\nprevious distantly supervised baselines, establishing a new SOTA performance on FECDATA. Furthermore, our approach significantly outperforms its few-shot counterpart, providing strong evidence for the effectiveness of the pivot task."
        },
        {
            "heading": "8 Limitations",
            "text": "There are two potential limitations to this study. Firstly, due to limited computational resources, we only assess the effectiveness of our proposed method on GPT series models. Future work should include additional experiments with other language models, such as PaLM and LLaMA. The second limitation is that the retrieved evidence may not always be relevant to the input claim, which means it may not provide useful information for correcting factual errors within the claim. As our primary focus is on enhancing factual error correction through the introduction of the pivot task, we leave the task of improving evidence retrieval for future research."
        },
        {
            "heading": "Acknowledgements",
            "text": "This project is supported by National Natural Science Foundation of China (Grant No. 62202023), HKU-SCF FinTech Academy, Shenzhen-Hong Kong-Macao Science and Technology Plan Project (Category C Project: SGDX20210823103537030), and Theme-based Research Scheme of RGC, Hong Kong (T35-710/20-R). We would like to thank the anonymous reviewers for their constructive and informative feedback on this work."
        },
        {
            "heading": "A Distribution of Mutation Types",
            "text": "Figure 6 presents the distribution of mutation types for revised claims of the test set.\nB Implementation Details\nB.1 Evidence Retrieval Considering that our research does not focus on improving the retrieval model, we adopt the same retrieval process as previous studies (Thorne and Vlachos, 2021; Chen et al., 2023). The retrieval module primarily consists of two steps: First, we employ a pre-trained seq2seq model called GENRE (Cao et al., 2021) to predict the relevant Wikipedia pages for the input claim. Then, we utilize the dense passage retrieval model DPR (Karpukhin et al., 2020) to retrieve the most relevant passages from the pages predicted by GENRE.\nB.2 PivotFEC Synthetic data for FEC. We generate synthetic FEC data using REFUTED data instances from FECDATA by solving the FEI task. Since we assume that correct claims are readily available, we utilize only the correct claims from the REFUTED data instances and exclude their paired false claims.\nTo provide clarity, we construct a total of 1296 validation data instances and 2000 training data instances. It\u2019s worth noting that increasing the amount of training data does not yield substantial improvements, as discussed in Section 5.3.\nTraining and Inference. During training, we initialize the PivotFEC corrector with the T5-base model. Following previous work (Thorne and Vlachos, 2021), we input the top-2 retrieved evidence or gold evidence paired with the false\nclaim into the T5 encoder, as additional evidence does not yield significant improvements. The corrector is optimized using the AdamW optimizer (Loshchilov and Hutter, 2019) with a learning rate of 4e \u2212 5, a batch size of 64, and a linear learning rate schedule with 10% warm-up steps for 400 steps. The learning rate is selected from the set {5e \u2212 6, 1e \u2212 5, 2e \u2212 5, 3e \u2212 5, 4e \u2212 5, 5e \u2212 5}. We set the maximum source length to 512 and the maximum target length to 256. During training, we evaluate the model every 50 steps on the synthetic validation set and choose the checkpoint with the lowest negative log-likelihood (NLL) loss on the validation set.\nDuring inference, we employ beam search decoding with a beam width of 5 to generate revised claims for the test set.\nB.3 Supervised Models We fine-tune the pre-trained models, BART-base and T5-base, on the FECDATA training set for 4000 steps. We evaluate the model every 200 steps using the FECDATA validation set. Other parameters remain consistent with those of PivotFEC, as stated in Section B.2.\nTo implement all models, we utilize the HuggingFace Transformers library (Wolf et al., 2019). Additionally, all experiments are conducted on 2 NVIDIA Tesla V100 GPUs with 32 GB of memory."
        },
        {
            "heading": "C Full Few-shot Prompts for FEC",
            "text": "Table 6 shows the few-shot exemplars prompt of the negate mutation type for the FEC task. Table 6 and Table 7 use the same demonstrated exemplars with the only difference being the order of the original claim and mutated claim."
        },
        {
            "heading": "D Full Few-shot Prompts for FEI",
            "text": "Tables 7, 8, 9, and 10 show the few-shot exemplars prompt of the negate, substitute similar, substitute dissimilar and specific mutation types for the FEI task, respectively.\nEvidence: The Lion King; The story takes place in a kingdom of lions in Africa and was influenced by William Shakespeare \u2019s Hamlet . The Lion King; The Lion King tells the story of Simba , a young lion who is to succeed his father , Mufasa , as King of the Pride Lands ; however , after Simba \u2019s uncle Scar murders Mufasa , Simba is manipulated into thinking he was responsible and flees into exile . Mutated claim: The Lion King has nothing to do with lions. Original claim: The Lion King is about lions.\nEvidence: Indiana Jones; Henry Walton \u201c Indiana \u201d Jones Jr. ( also shortened to Indy ) is a fictional character and the protagonist of the Indiana Jones franchise . Indiana Jones; George Lucas created the character in homage to the action heroes of 1930s film serials . Mutated claim: Indiana Jones is real. Original claim: Indiana Jones is fictional.\nEvidence: Scott Eastwood; Scott Eastwood ( born Scott Clinton Reeves ; March 21 , 1986 ) is an American actor , model , and professional skydiver . Scott Eastwood; He has also been the model for the fragrance Cool Water by Davidoff . Mutated claim: Scott Eastwood was incapable of working as a model. Original claim: Scott Eastwood worked as a model.\nEvidence: Akshay Kumar; Kumar is also a producer and martial artist who has appeared in over a hundred Hindi films . Akshay Kumar; Having done so , he has established himself as a leading contemporary actor of Hindi cinema . Mutated claim: Akshay Kumar does not work in Hindi cinema. Original claim: Akshay Kumar works in Hindi cinema.\nEvidence: Gorillaz; Gorillaz are an English virtual band created in 1998 by musician Damon Albarn and artist Jamie Hewlett . Virtual band; In music , a virtual band ( also called a virtual group , cartoon group , or cartoon band ) is any group whose members are not corporeal musicians , but animated characters . Mutated claim: Gorillaz is a German live band. Original claim: Gorillaz is a British virtual band.\nEvidence: Grant Gustin; Thomas Grant Gustin ( born January 14 , 1990 ) is an American actor , singer , and dancer . Grant Gustin; He is known for his role as Barry Allen / the Flash ( based on the DC Comics character of the same name ) on the CW series The Flash and Arrow , both in the Arrowverse television franchise , and his role as Sebastian Smythe on the Fox series Glee . Mutated claim: Grant Gustin is only a writer. Original claim: Grant Gustin is a singer.\nEvidence: RB Leipzig; RasenBallsport Leipzig e.V. , commonly known as RB Leipzig , is a German association football club based in Leipzig , Saxony . Football in Germany; Football is the most popular sport in Germany . Mutated claim: RB Leipzig plays the least popular German sport. Original claim: RB Leipzig plays the most popular German sport.\nEvidence: One World Trade Center; One World Trade Center ( also known as 1 World Trade Center , 1 WTC or Freedom Tower ) is the main building of the rebuilt World Trade Center complex in Lower Manhattan , New York City . World Trade Center (2001\u2013present); The original World Trade Center featured the landmark Twin Towers , which opened in 1973 , and were the tallest buildings in the world at their completion . Mutated claim: One World Trade Center opened in 1876. Original claim: One World Trade Center opened in 2014.\nTable 6: Few-shot exemplars prompt of the negate mutation type for the FEC task.\nEvidence: The Lion King; The story takes place in a kingdom of lions in Africa and was influenced by William Shakespeare \u2019s Hamlet . The Lion King; The Lion King tells the story of Simba , a young lion who is to succeed his father , Mufasa , as King of the Pride Lands ; however , after Simba \u2019s uncle Scar murders Mufasa , Simba is manipulated into thinking he was responsible and flees into exile . Original claim: The Lion King is about lions. Mutated claim: The Lion King has nothing to do with lions.\nEvidence: Indiana Jones; Henry Walton \u201c Indiana \u201d Jones Jr. ( also shortened to Indy ) is a fictional character and the protagonist of the Indiana Jones franchise . Indiana Jones; George Lucas created the character in homage to the action heroes of 1930s film serials . Original claim: Indiana Jones is fictional. Mutated claim: Indiana Jones is real.\nEvidence: Scott Eastwood; Scott Eastwood ( born Scott Clinton Reeves ; March 21 , 1986 ) is an American actor , model , and professional skydiver . Scott Eastwood; He has also been the model for the fragrance Cool Water by Davidoff . Original claim: Scott Eastwood worked as a model. Mutated claim: Scott Eastwood was incapable of working as a model.\nEvidence: Akshay Kumar; Kumar is also a producer and martial artist who has appeared in over a hundred Hindi films . Akshay Kumar; Having done so , he has established himself as a leading contemporary actor of Hindi cinema . Original claim: Akshay Kumar works in Hindi cinema. Mutated claim: Akshay Kumar does not work in Hindi cinema.\nEvidence: Gorillaz; Gorillaz are an English virtual band created in 1998 by musician Damon Albarn and artist Jamie Hewlett . Virtual band; In music , a virtual band ( also called a virtual group , cartoon group , or cartoon band ) is any group whose members are not corporeal musicians , but animated characters . Original claim: Gorillaz is a British virtual band. Mutated claim: Gorillaz is a German live band.\nEvidence: Grant Gustin; Thomas Grant Gustin ( born January 14 , 1990 ) is an American actor , singer , and dancer . Grant Gustin; He is known for his role as Barry Allen / the Flash ( based on the DC Comics character of the same name ) on the CW series The Flash and Arrow , both in the Arrowverse television franchise , and his role as Sebastian Smythe on the Fox series Glee . Original claim: Grant Gustin is a singer. Mutated claim: Grant Gustin is only a writer.\nEvidence: RB Leipzig; RasenBallsport Leipzig e.V. , commonly known as RB Leipzig , is a German association football club based in Leipzig , Saxony . Football in Germany; Football is the most popular sport in Germany . Original claim: RB Leipzig plays the most popular German sport. Mutated claim: RB Leipzig plays the least popular German sport.\nEvidence: One World Trade Center; One World Trade Center ( also known as 1 World Trade Center , 1 WTC or Freedom Tower ) is the main building of the rebuilt World Trade Center complex in Lower Manhattan , New York City . World Trade Center (2001\u2013present); The original World Trade Center featured the landmark Twin Towers , which opened in 1973 , and were the tallest buildings in the world at their completion . Original claim: One World Trade Center opened in 2014. Mutated claim: One World Trade Center opened in 1876.\nTable 7: Few-shot exemplars prompt of the negate mutation type for the FEI task.\nEvidence: Notes on a Scandal (film); The soundtrack was composed by Philip Glass . Philip Glass; Philip Morris Glass ( born January 31 , 1937 ) is an American composer . Original claim: Notes on a Scandal has a soundtrack composed by an American. Mutated claim: Notes on a Scandal has a soundtrack composed by an Armenian.\nEvidence: The Lion King; The Lion King is a 1994 American animated epic musical film , produced by Walt Disney Feature Animation and released by Walt Disney Pictures . The Lion King; It is the 32nd Disney animated feature film . Original claim: The Lion King is a film. Mutated claim: The Lion King is a TV show.\nEvidence: Dead Man Down; Dead Man Down is an 2013 American neo-noir crime thriller film written by J.H. Wyman and directed by Danish director Niels Arden Oplev . Dead Man Down; The film stars Colin Farrell , Noomi Rapace , Dominic Cooper , and Terrence Howard , and was released on March 8 , 2013 . Original claim: Dead Man Down was released in 2013. Mutated claim: Dead Man Down was released in 2014.\nEvidence: Nick Jonas (album); It was released on November 10 , 2014 , by Island Records . Island Records; Island Records is a British-American record label that operates as a division of Universal Music Group ( UMG ) . Original claim: Nick Jonas was released by a British-American record label. Mutated claim: Nick Jonas was released by a Chinese-Mongolian record label.\nEvidence: Deadpool; Created by artist/writer Rob Liefeld and writer Fabian Nicieza , the character first appeared in The New Mutants # 98 ( cover-dated February 1991 ) . Deadpool; Initially Deadpool was depicted as a supervillain when he made his first appearance in The New Mutants and later in issues of X-Force , but later evolved into his more recognizable antiheroic persona . Original claim: Deadpool first appeared in The New Mutants. Mutated claim: Deadpool first appeared in X-Men.\nEvidence: Blue-ringed octopus; The blue-ringed octopodes ( genus Hapalochlaena ) are three octopus species that live in tide pools and coral reefs in the Pacific and Indian Oceans , from Japan to Australia . Blue-ringed octopus; They are recognized as one of the world \u2019s most venomous marine animals . Original claim: The blue-ringed octopus is a marine animal. Mutated claim: The blue-ringed octopus is a terrestrial animal.\nEvidence: Room (2015 film); Room is a 2015 independent drama film directed by Lenny Abrahamson and written by Emma Donoghue , based on her novel of the same name . Room (novel); Room is a 2010 novel by Irish-Canadian author Emma Donoghue . Original claim: Room is based on a novel of the same name. Mutated claim: Room is based on a short story of the same name.\nEvidence: Steve Buscemi; He made his directorial debut in 1996 , with Trees Lounge , in which he also starred . Trees Lounge; Trees Lounge is a 1996 feature film and the debut of Steve Buscemi as writer and director . Original claim: Steve Buscemi directed the film Trees Lounge. Mutated claim: Steve Buscemi directed the television show Trees Lounge.\nTable 8: Few-shot exemplars prompt of the substitute similar mutation type for the FEI task.\nEvidence: Kurt Angle; He then won a freestyle wrestling gold medal at the 1996 Summer Olympics . Kurt Angle; After graduating college , Angle won a gold medal in freestyle wrestling at the 1995 World Wrestling Championships . Original claim: Kurt Angle is a professional wrestler. Mutated claim: Kurt Angle is a fish.\nEvidence: Selena Gomez; Between 2009 and 2011 , Gomez starred in films such as Princess Protection Program , Ramona and Beezus , and Monte Carlo , and took on a more mature role in Spring Breakers ( 2013 ) . Princess Protection Program; Princess Protection Program is a 2009 Disney Channel Original Movie , directed by Allison Liddi-Brown and starring Demi Lovato and Selena Gomez . Original claim: Selena Gomez starred in Princess Protection Program. Mutated claim: Selena Gomez reviewed Princess Protection Program.\nEvidence: Sterling Archer; Sterling Malory Archer , known simply as Archer , is the titular character and the main protagonist of the American animated comedy series Archer . Archer (TV series); Archer is an American adult animated spy sitcom created by Adam Reed for the FX network . Original claim: Sterling Archer is the main character of a comedy series. Mutated claim: Sterling Archer directed a comedy series.\nEvidence: United States Congress; The House of Representatives has six non-voting members in addition to its 435 voting members . United States Congress; Congress has 535 voting members : 435 Representatives and 100 Senators . Original claim: The United States Congress has 435 Representatives. Mutated claim: The United States Congress has 435 minefields.\nEvidence: Carole King; Carole King ( born Carol Joan Klein , February 9 , 1942 ) is an American composer and singersongwriter . Carole King; She is the most successful female songwriter of the latter half of the 20th century , having written or co-written 118 pop hits on the Billboard Hot 100 between 1955 and 1999 . Original claim: Carole King is an American. Mutated claim: Carole King is an acrobat.\nEvidence: Manatee; Manatees ( family Trichechidae , genus Trichechus ) are large , fully aquatic , mostly herbivorous marine mammals sometimes known as sea cows . Herbivore; A herbivore is an animal anatomically and physiologically adapted to eating plant material , for example foliage , for the main component of its diet . Original claim: Manatees are similar to cows on land. Mutated claim: Manatees eat cows on land.\nEvidence: Lion (2016 film); Lion is a 2016 biographical film directed by Garth Davis ( in his feature debut ) and written by Luke Davies , based on the non-fiction book A Long Way Home by Saroo Brierley with Larry Buttrose . Garth Davis; Garth Davis is an Australian television , film and commercial director , best known for directing episodes of the series Top of the Lake ( 2013 ) , for which he received Emmy and BAFTA nominations . Original claim: Lion was directed by Garth Davis. Mutated claim: Lion was directed by plants.\nEvidence: Tropic Thunder; Tropic Thunder is a 2008 satirical action comedy film co-written , produced , and directed by Ben Stiller . Tropic Thunder; It was written by Stiller , Justin Theroux and Etan Cohen . Original claim: Tropic Thunder was written by Justin Theroux. Mutated claim: Tropic Thunder was awoken by Justin Theroux.\nTable 9: Few-shot exemplars prompt of the substitute dissimilar mutation type for the FEI task.\nEvidence: Bryan Adams; He has also won MTV , ASCAP , American Music awards , three Ivor Novello Awards for song composition and has been nominated five times for Golden Globe Awards and three times for Academy Awards for his songwriting for films . Ivor Novello Awards; The Ivor Novello Awards , named after the Cardiff-born entertainer Ivor Novello , are awards for songwriting and composing . Original claim: Bryan Adams has won Ivor Novello Awards. Mutated claim: Bryan Adams has won Ivor Novello Awards for his singing.\nEvidence: Malcolm Young; Malcolm Mitchell Young ( born 6 January 1953 ) is an Australian retired musician and songwriter , best known as a co-founder , rhythm guitarist , backing vocalist and songwriter for the hard rock band AC/DC . Malcolm Young; Except for a brief absence in 1988 , he was with the band from its November 1973 beginning until retiring permanently in 2014 , due to health reasons . Original claim: Malcolm Young co-founded AC/DC in 1973. Mutated claim: Malcolm Young co-founded AC/DC in July 1973.\nEvidence: Gillian Jacobs; Jacobs has also had a recurring role as Mimi-Rose Howard on the HBO series Girls and has appeared in films such as Gardens of the Night ( 2008 ) , The Lookalike ( 2014 ) , Life Partners ( 2014 ) , Hot Tub Time Machine 2 ( 2015 ) , Do n\u2019t Think Twice ( 2016 ) and Brother Nature ( 2016 ) . Hot Tub Time Machine 2; Hot Tub Time Machine 2 is a 2015 American comedy film directed by Steve Pink and written by Josh Heald . Original claim: Gillian Jacobs appeared in the film Hot Tub Time Machine 2. Mutated claim: Gillian Jacobs appeared in the horror film Hot Tub Time Machine 2.\nEvidence: Marc Maron; He has been host of The Marc Maron Show and co-host of both Morning Sedition and Breakroom Live , all politically oriented shows produced by Air America Media . The Marc Maron Show; It featured interviews ( both political and showbusiness ) , live comedy , and extensive banter between Maron and Jim Earl , Maron \u2019s co-host , who provides humorous introductions after each commercial break and plays several of the recurring characters in the show \u2019s skits . Original claim: Marc Maron was the host of The Marc Maron Show. Mutated claim: Marc Maron was the only host of The Marc Maron Show.\nEvidence: The Offspring; The band \u2019s third studio album , Smash ( 1994 ) , became their first commercial success , and has sold over eleven million copies worldwide , setting a record for most albums sold on an independent label and becoming the first album on Epitaph to obtain gold and platinum status . Smash (The Offspring album); Recording and production were finished a month later , and the album was released on April 8 , 1994 on Epitaph Records . Original claim: The Offspring released Smash in 1994. Mutated claim: The Offspring released Smash in May of 1994.\nEvidence: NASA; Since that time , most US space exploration efforts have been led by NASA , including the Apollo Moon landing missions , the Skylab space station , and later the Space Shuttle . Skylab; Skylab was the United States \u2019 first space station , orbiting Earth from 1973 to 1979 , when it fell back to Earth amid huge worldwide media attention . Original claim: NASA is responsible for the Skylab space station. Mutated claim: NASA is responsible for the Skylab space station, launched in 1980.\nEvidence: Golden State Warriors; The Warriors compete in the National Basketball Association ( NBA ) as a member of the league \u2019s Western Conference Pacific Division . National Basketball Association; It has 30 teams ( 29 in the United States and 1 in Canada ) , and is an active member of USA Basketball ( USAB ) , which is recognized by FIBA ( also known as the International Basketball Federation ) as the national governing body for basketball in the United States . Original claim: The Golden State Warriors are in the NBA. Mutated claim: The Golden State Warriors are one of 32 teams in the NBA.\nEvidence: Morrissey; Born in Davyhulme , Lancashire , to a working-class Irish migrant family , Morrissey grew up in Manchester . Morrissey; Steven Patrick Morrissey ( born 22 May 1959 ) , professionally known as Morrissey , is an English singer , songwriter and author . Original claim: Morrissey was born into a working-class family. Mutated claim: Morrissey was born into a working-class family in 1983.\nTable 10: Few-shot exemplars prompt of the specific mutation type for the FEI task."
        }
    ],
    "title": "PivotFEC: Enhancing Few-shot Factual Error Correction with a Pivot Task Approach using Large Language Models",
    "year": 2023
}