{
    "abstractText": "Large language models (LLMs) have demonstrated remarkable language abilities. GPT-4, based on advanced LLMs, exhibits extraordinary multimodal capabilities beyond previous visual language models. We attribute this to the use of more advanced LLMs compared with previous multimodal models. Unfortunately, the model architecture and training strategies of GPT-4 are unknown. To endow LLMs with multimodal capabilities, we propose X-LLM, which converts Multi-modalities (images, speech, videos) into foreign languages using X2L interfaces and inputs them into a large Language model (ChatGLM). Specifically, X-LLM aligns multiple frozen single-modal encoders and a frozen LLM using X2L interfaces, where \u201cX\u201d denotes multi-modalities such as image, speech, and videos, and \u201cL\u201d denotes languages. X-LLM\u2019s training consists of three stages: (1) Converting Multimodal Information: The first stage trains each X2L interface to align with its respective single-modal encoder separately to convert multimodal information into languages. (2) Aligning X2L representations with the LLM: single-modal encoders are aligned with the LLM through X2L interfaces independently. (3) Integrating multiple modalities: all single-modal encoders are aligned with the LLM through X2L interfaces to integrate multimodal capabilities into the LLM. Our experiments show that X-LLM demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 84.5% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. And we also conduct quantitative tests on using LLM for ASR and multimodal ASR, hoping to promote the era of LLM-based speech recognition.",
    "authors": [
        {
            "affiliations": [],
            "name": "Feilong Chen"
        },
        {
            "affiliations": [],
            "name": "Minglun Han"
        },
        {
            "affiliations": [],
            "name": "Haozhi Zhao"
        },
        {
            "affiliations": [],
            "name": "Qingyang Zhang"
        },
        {
            "affiliations": [],
            "name": "Jing Shi"
        },
        {
            "affiliations": [],
            "name": "Shuang Xu"
        },
        {
            "affiliations": [],
            "name": "Bo Xu"
        }
    ],
    "id": "SP:202ac3123df8a337e4ce511c69898243be06fd37",
    "references": [
        {
            "authors": [
                "Stanislaw Antol",
                "Aishwarya Agrawal",
                "Jiasen Lu",
                "Margaret Mitchell",
                "Dhruv Batra",
                "C Lawrence Zitnick",
                "Devi Parikh"
            ],
            "title": "Vqa: Visual question answering",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2015
        },
        {
            "authors": [
                "Bin Bi",
                "Chenliang Li",
                "Chen Wu",
                "Ming Yan",
                "Wei Wang",
                "Songfang Huang",
                "Fei Huang",
                "Luo Si"
            ],
            "title": "Palm: Pre-training an autoencoding&autoregressive language model for context-conditioned generation",
            "venue": "arXiv preprint arXiv:2004.07159,",
            "year": 2020
        },
        {
            "authors": [
                "Hui Bu",
                "Jiayu Du",
                "Xingyu Na",
                "Bengu Wu",
                "Hao Zheng"
            ],
            "title": "Aishell-1: An open-source mandarin speech corpus and a speech recognition baseline",
            "year": 2017
        },
        {
            "authors": [
                "Cheng Chen",
                "Zhenshan Tan",
                "Qingrong Cheng",
                "Xin Jiang",
                "Qun Liu",
                "Yudong Zhu",
                "Xiaodong Gu"
            ],
            "title": "Utc: a unified transformer with inter-task contrastive learning for visual dialog",
            "venue": "In Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Fei-Long Chen",
                "Du-Zhen Zhang",
                "Ming-Lun Han",
                "Xiu-Yi Chen",
                "Jing Shi",
                "Shuang Xu",
                "Bo Xu"
            ],
            "title": "Vlp: A survey on vision-language pre-training",
            "venue": "Machine Intelligence Research,",
            "year": 2023
        },
        {
            "authors": [
                "Feilong Chen",
                "Fandong Meng",
                "Jiaming Xu",
                "Peng Li",
                "Bo Xu",
                "Jie Zhou"
            ],
            "title": "Dmrm: A dualchannel multi-hop reasoning model for visual dialog",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Feilong Chen",
                "Duzhen Zhang",
                "Xiuyi Chen",
                "Jing Shi",
                "Shuang Xu",
                "Bo Xu"
            ],
            "title": "Unsupervised and pseudo-supervised vision-language alignment in visual dialog",
            "venue": "In Proceedings of the 30th ACM International Conference on Multimedia,",
            "year": 2022
        },
        {
            "authors": [
                "Wei-Lin Chiang",
                "Zhuohan Li",
                "Zi Lin",
                "Ying Sheng",
                "Zhanghao Wu",
                "Hao Zhang",
                "Lianmin Zheng",
                "Siyuan Zhuang",
                "Yonghao Zhuang",
                "Joseph E Gonzalez"
            ],
            "title": "Vicuna: An open-source chatbot impressing gpt-4 with 90 quality, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Abhishek Das",
                "Satwik Kottur",
                "Khushi Gupta",
                "Avi Singh",
                "Deshraj Yadav",
                "Jos\u00e9 MF Moura",
                "Devi Parikh",
                "Dhruv Batra"
            ],
            "title": "Visual dialog",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Mostafa Dehghani",
                "Josip Djolonga",
                "Basil Mustafa",
                "Piotr Padlewski",
                "Jonathan Heek",
                "Justin Gilmer",
                "Andreas Steiner",
                "Mathilde Caron",
                "Robert Geirhos",
                "Ibrahim Alabdulmohsin"
            ],
            "title": "Scaling vision transformers to 22 billion parameters",
            "venue": "arXiv preprint arXiv:2302.05442,",
            "year": 2023
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805,",
            "year": 2018
        },
        {
            "authors": [
                "Linhao Dong",
                "Bo Xu"
            ],
            "title": "CIF: continuous integrate-and-fire for end-to-end speech recognition",
            "venue": "In ICASSP,",
            "year": 2020
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "arXiv preprint arXiv:2010.11929,",
            "year": 2020
        },
        {
            "authors": [
                "Danny Driess",
                "Fei Xia",
                "Mehdi SM Sajjadi",
                "Corey Lynch",
                "Aakanksha Chowdhery",
                "Brian Ichter",
                "Ayzaan Wahid",
                "Jonathan Tompson",
                "Quan Vuong",
                "Tianhe Yu"
            ],
            "title": "Palm-e: An embodied multimodal language model",
            "venue": "arXiv preprint arXiv:2303.03378,",
            "year": 2023
        },
        {
            "authors": [
                "Jiayu Du",
                "Xingyu Na",
                "Xuechen Liu",
                "Hui Bu"
            ],
            "title": "Aishell-2: Transforming mandarin asr research into industrial scale",
            "venue": "arXiv preprint arXiv:1808.10583,",
            "year": 2018
        },
        {
            "authors": [
                "Yifan Du",
                "Zikang Liu",
                "Junyi Li",
                "Wayne Xin Zhao"
            ],
            "title": "A survey of vision-language pre-trained models",
            "venue": "arXiv preprint arXiv:2202.10936,",
            "year": 2022
        },
        {
            "authors": [
                "Zhengxiao Du",
                "Yujie Qian",
                "Xiao Liu",
                "Ming Ding",
                "Jiezhong Qiu",
                "Zhilin Yang",
                "Jie Tang"
            ],
            "title": "Glm: General language model pretraining with autoregressive blank infilling",
            "venue": "In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2022
        },
        {
            "authors": [
                "Lianli Gao",
                "Zhao Guo",
                "Hanwang Zhang",
                "Xing Xu",
                "Heng Tao Shen"
            ],
            "title": "Video captioning with attention-based lstm and semantic consistency",
            "venue": "IEEE Transactions on Multimedia,",
            "year": 2045
        },
        {
            "authors": [
                "Ross Girshick"
            ],
            "title": "Fast r-cnn",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2015
        },
        {
            "authors": [
                "Jiaxi Gu",
                "Xiaojun Meng",
                "Guansong Lu",
                "Lu Hou",
                "Niu Minzhe",
                "Xiaodan Liang",
                "Lewei Yao",
                "Runhui Huang",
                "Wei Zhang",
                "Xin Jiang"
            ],
            "title": "Wukong: A 100 million large-scale chinese cross-modal pre-training benchmark",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Anmol Gulati",
                "James Qin",
                "Chung-Cheng Chiu",
                "Niki Parmar",
                "Yu Zhang",
                "Jiahui Yu",
                "Wei Han",
                "Shibo Wang",
                "Zhengdong Zhang",
                "Yonghui Wu",
                "Ruoming Pang"
            ],
            "title": "Conformer: Convolutionaugmented transformer for speech recognition",
            "venue": "In INTERSPEECH,",
            "year": 2020
        },
        {
            "authors": [
                "Minglun Han",
                "Linhao Dong",
                "Zhenlin Liang",
                "Meng Cai",
                "Shiyu Zhou",
                "Zejun Ma",
                "Bo Xu"
            ],
            "title": "Improving end-to-end contextual speech recognition with fine-grained contextual knowledge selection",
            "venue": "In ICASSP,",
            "year": 2022
        },
        {
            "authors": [
                "Minglun Han",
                "Linhao Dong",
                "Shiyu Zhou",
                "Bo Xu"
            ],
            "title": "Cif-based collaborative decoding for end-to-end contextual speech recognition",
            "venue": "In ICASSP,",
            "year": 2021
        },
        {
            "authors": [
                "Shaohan Huang",
                "Li Dong",
                "Wenhui Wang",
                "Yaru Hao",
                "Saksham Singhal",
                "Shuming Ma",
                "Tengchao Lv",
                "Lei Cui",
                "Owais Khan Mohammed",
                "Qiang Liu"
            ],
            "title": "Language is not all you need: Aligning perception with language models",
            "venue": "arXiv preprint arXiv:2302.14045,",
            "year": 2023
        },
        {
            "authors": [
                "Salman Khan",
                "Muzammal Naseer",
                "Munawar Hayat",
                "Syed Waqas Zamir",
                "Fahad Shahbaz Khan",
                "Mubarak Shah"
            ],
            "title": "Transformers in vision: A survey",
            "venue": "ACM computing surveys (CSUR),",
            "year": 2022
        },
        {
            "authors": [
                "Wonjae Kim",
                "Bokyung Son",
                "Ildoo Kim"
            ],
            "title": "Vilt: Vision-and-language transformer without convolution or region supervision",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Ranjay Krishna",
                "Kenji Hata",
                "Frederic Ren",
                "Li Fei-Fei",
                "Juan Carlos Niebles"
            ],
            "title": "Densecaptioning events in videos",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2017
        },
        {
            "authors": [
                "Ranjay Krishna",
                "Yuke Zhu",
                "Oliver Groth",
                "Justin Johnson",
                "Kenji Hata",
                "Joshua Kravitz",
                "Stephanie Chen",
                "Yannis Kalantidis",
                "Li-Jia Li",
                "David A. Shamma",
                "Michael S. Bernstein",
                "Li Fei-Fei"
            ],
            "title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations",
            "venue": "International Journal of Computer Vision,",
            "year": 2017
        },
        {
            "authors": [
                "Junnan Li",
                "Dongxu Li",
                "Silvio Savarese",
                "Steven Hoi"
            ],
            "title": "Blip-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models",
            "venue": "arXiv preprint arXiv:2301.12597,",
            "year": 2023
        },
        {
            "authors": [
                "Junnan Li",
                "Dongxu Li",
                "Caiming Xiong",
                "Steven Hoi"
            ],
            "title": "Blip: Bootstrapping languageimage pre-training for unified vision-language understanding and generation",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Liunian Harold Li",
                "Mark Yatskar",
                "Da Yin",
                "Cho-Jui Hsieh",
                "Kai-Wei Chang"
            ],
            "title": "Visualbert: A simple and performant baseline for vision and language",
            "venue": "arXiv preprint arXiv:1908.03557,",
            "year": 1908
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Michael Maire",
                "Serge Belongie",
                "James Hays",
                "Pietro Perona",
                "Deva Ramanan",
                "Piotr Doll\u00e1r",
                "C Lawrence Zitnick"
            ],
            "title": "Microsoft coco: Common objects in context",
            "venue": "In Proceedings of the European Conference on Computer Vision,",
            "year": 2014
        },
        {
            "authors": [
                "Haotian Liu",
                "Chunyuan Li",
                "Qingyang Wu",
                "Yong Jae Lee"
            ],
            "title": "Visual instruction tuning",
            "venue": "arXiv preprint arXiv:2304.08485,",
            "year": 2023
        },
        {
            "authors": [
                "Siqu Long",
                "Feiqi Cao",
                "Soyeon Caren Han",
                "Haiqing Yang"
            ],
            "title": "Vision-and-language pretrained models: A survey",
            "venue": "arXiv preprint arXiv:2204.07356,",
            "year": 2022
        },
        {
            "authors": [
                "Jiasen Lu",
                "Dhruv Batra",
                "Devi Parikh",
                "Stefan Lee"
            ],
            "title": "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Vishvak Murahari",
                "Dhruv Batra",
                "Devi Parikh",
                "Abhishek Das"
            ],
            "title": "Large-scale pretraining for visual dialog: A simple state-of-the-art baseline",
            "venue": "In Computer Vision\u2013ECCV 2020: 16th European Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Vicente Ordonez",
                "Girish Kulkarni",
                "Tamara L Berg"
            ],
            "title": "Im2text: Describing images using 1 million captioned photographs",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2011
        },
        {
            "authors": [
                "Daniel S. Park",
                "William Chan",
                "Yu Zhang",
                "Chung-Cheng Chiu",
                "Barret Zoph",
                "Ekin Dogus Cubuk",
                "Quoc V. Le"
            ],
            "title": "Specaugment: A simple data augmentation method for automatic speech recognition",
            "venue": "In Interspeech, 2019",
            "year": 2019
        },
        {
            "authors": [
                "Bryan A Plummer",
                "Liwei Wang",
                "Chris M Cervantes",
                "Juan C Caicedo",
                "Julia Hockenmaier",
                "Svetlana Lazebnik"
            ],
            "title": "Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2015
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In International conference on machine learning,",
            "year": 2021
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu"
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Yi Ren",
                "Chenxu Hu"
            ],
            "title": "Fastspeech 2: Fast and high-quality end-to-end text to speech",
            "venue": "In Proc. ICLR,",
            "year": 2021
        },
        {
            "authors": [
                "Piyush Sharma",
                "Nan Ding",
                "Sebastian Goodman",
                "Radu Soricut"
            ],
            "title": "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning",
            "venue": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2018
        },
        {
            "authors": [
                "Yongliang Shen",
                "Kaitao Song",
                "Xu Tan",
                "Dongsheng Li",
                "Weiming Lu",
                "Yueting Zhuang"
            ],
            "title": "Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface",
            "venue": "arXiv preprint arXiv:2303.17580,",
            "year": 2023
        },
        {
            "authors": [
                "Yao Shi",
                "Hui Bu"
            ],
            "title": "Aishell-3: A multi-speaker mandarin tts corpus and the baselines",
            "venue": "arXiv preprint arXiv:2010.11567,",
            "year": 2020
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "venue": "arXiv preprint arXiv:2302.13971,",
            "year": 2023
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Oriol Vinyals",
                "Alexander Toshev",
                "Samy Bengio",
                "Dumitru Erhan"
            ],
            "title": "Show and tell: Lessons learned from the 2015 mscoco image captioning challenge",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2016
        },
        {
            "authors": [
                "Yue Wang",
                "Shafiq Joty",
                "Michael Lyu",
                "Irwin King",
                "Caiming Xiong",
                "Steven CH Hoi"
            ],
            "title": "Vd-bert: A unified vision and dialog transformer with bert",
            "venue": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "Tsung-Hsien Wen",
                "Milica Gasic",
                "Nikola Mrksic",
                "Pei-Hao Su",
                "David Vandyke",
                "Steve Young"
            ],
            "title": "Semantically conditioned lstm-based natural language generation for spoken dialogue systems",
            "venue": "arXiv preprint arXiv:1508.01745,",
            "year": 2015
        },
        {
            "authors": [
                "Chenfei Wu",
                "Shengming Yin",
                "Weizhen Qi",
                "Xiaodong Wang",
                "Zecheng Tang",
                "Nan Duan"
            ],
            "title": "Visual chatgpt: Talking, drawing and editing with visual foundation models",
            "venue": "arXiv preprint arXiv:2303.04671,",
            "year": 2023
        },
        {
            "authors": [
                "Jiahong Wu",
                "He Zheng",
                "Bo Zhao",
                "Yixin Li",
                "Baoming Yan",
                "Rui Liang",
                "Wenjia Wang",
                "Shipei Zhou",
                "Guosen Lin",
                "Yanwei Fu"
            ],
            "title": "Ai challenger: A large-scale dataset for going deeper in image understanding",
            "venue": "arXiv preprint arXiv:1711.06475,",
            "year": 2017
        },
        {
            "authors": [
                "Jun Xu",
                "Tao Mei",
                "Ting Yao",
                "Yong Rui"
            ],
            "title": "Msr-vtt: A large video description dataset for bridging video and language",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2016
        },
        {
            "authors": [
                "Aohan Zeng",
                "Xiao Liu",
                "Zhengxiao Du",
                "Zihan Wang",
                "Hanyu Lai",
                "Ming Ding",
                "Zhuoyi Yang",
                "Yifan Xu",
                "Wendi Zheng",
                "Xiao Xia",
                "Weng Lam Tam",
                "Zixuan Ma",
                "Yufei Xue",
                "Jidong Zhai",
                "Wenguang Chen",
                "Zhiyuan Liu",
                "Peng Zhang",
                "Yuxiao Dong",
                "Jie Tang"
            ],
            "title": "GLM-130b: An open bilingual pre-trained model",
            "venue": "In The Eleventh International Conference on Learning Representations (ICLR),",
            "year": 2023
        },
        {
            "authors": [
                "Xiaohua Zhai",
                "Alexander Kolesnikov",
                "Neil Houlsby",
                "Lucas Beyer"
            ],
            "title": "Scaling vision transformers",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Luowei Zhou",
                "Hamid Palangi",
                "Lei Zhang",
                "Houdong Hu",
                "Jason Corso",
                "Jianfeng Gao"
            ],
            "title": "Unified vision-language pre-training for image captioning and vqa",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Deyao Zhu",
                "Jun Chen",
                "Xiaoqian Shen",
                "Xiang Li",
                "Mohamed Elhoseiny"
            ],
            "title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
            "venue": "arXiv preprint arXiv:2304.10592,",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "In recent years, multimodal language models [31, 29, 24] have undergone rapid development. These models possess excellent abilities in multimodal understanding and response generation and can perform well in tasks such as image captioning [50], visual question answering [1], visual dialog [9], video captioning [18], and spoken dialogue [52]. It is worth noting that a large-scale multimodal model, GPT-4 [37], has recently been introduced, demonstrating many impressive capabilities. For example, GPT-4 can follow various instructions to complete language tasks, and can also answer various questions about images. For instance, GPT-4 can give detailed and accurate descriptions of images, understand and explain the humor in visual content, and even provide correct website-building code based on handwritten code images. Although GPT-4 demonstrates remarkable capabilities,\nPreprint. Work in progress\nar X\niv :2\n30 5.\n04 16\n0v 3\n[ cs\n.C L\n] 2\n2 M\nunfortunately, we do not know the details of its model structure and training methods. We believe that this is due to the fact that GPT-4 uses a more advanced and larger language model compared to previous multimodal models. With the support of powerful language abilities, GPT-4 can express understood visual content in the form of language.\nTo validate this hypothesis and endow LLM with multimodal capabilities, we propose X-LLM. It converts multimodal information, such as images, speech, and videos, into foreign languages using X2L interfaces, and then feeds converted multimodal information into a large language model (ChatGLM). Specifically, X-LLM aligns multiple frozen single-modal encoders and a frozen LLM using X2L interfaces. X2L interfaces consist of an image I2L interface, a video V2L interface, and a speech S2L interface, where \u201cX\u201d denotes the multi-modalities and \u201cL\u201d denotes languages. The image interface and video interface have the same structure, and we adopt the Q-Former from BLIP-2 [29] to convert visual information into foreign language representations. For efficiency, the video interface reuses the parameters of the image interface with image-text data but is further trained with video-text data to align the encoded video features with the LLM. The speech interface utilizes the continuous integrate-and-fire (CIF) mechanism [12, 23] and transformer structure to convert speech utterance into foreign language representations. The training of X-LLM consists of three stages. (1) Converting Multimodal Information: the first stage trains each X2L interface to align with its respective single-modal encoder separately to convert multimodal information into languages. (2) Aligning X2L representations with the LLM: single-modal encoders are aligned with the LLM through X2L interfaces. (3) Integrating multiple modalities: all single-modal encoders are aligned with the LLM through X2L interfaces to integrate multimodal capabilities into the LLM. In the first two stages, we use image caption data, video caption data and automatic speech recognition (ASR) data to train the X2L interfaces. To better equip LLM with multimodal capabilities, we construct a multimodal instruction dataset (\u223c10K) based on open-source datasets to further improve the proposed model. Although without the third training stage, X-LLM already has the ability to accomplish multimodal tasks such as visual spoken question answering, we find that with only rare additional multimodal instruction data, LLM can further unify the capabilities of multiple modalities.\nIn our experiments, we find that X-LLM has abilities similar to those of GPT-4. For example, X-LLM can generate complex image descriptions and explain unusual visual phenomena. In our research, when using input images, X-LLM can recognize the location in the image, such as identifying the Forbidden City and providing relevant information about it, observe the food in the image and provide detailed recipes; create stories for pictures, and come up with textual meanings for logos. We also find that X-LLM\u2019s image-related abilities can be extended to videos, such as introducing the content of a video, retrieving movie names, or art-related facts directly from the video. Moreover, X-LLM can answer questions based on spoken questions and can combine images or videos to answer spoken questions. These abilities are previously not present in previous multimodal models but are now made possible by the powerful language modeling capabilities of X-LLM.\nOur contributions are summarised as follows:\n\u2022 Multimodal LLM framework. We propose X-LLM, a Multimodal LLM which injects multiple modalities (such as images, speech, and videos) into LLM through X2L interfaces, giving LLM the ability to process multimodal data. This framework has good scalability and can be extended to more modalities. \u201cX\u201d in X2L interfaces can be any modality. We compare our X-LLM with LLaVA and MiniGPT-4 in terms of the ability to handle visual inputs with Chinese elements, and find that X-LLM outperformed them significantly. We also conduct quantitative tests on using LLM for ASR and multimodal ASR, hoping to promote the era of LLM-based speech recognition.\n\u2022 Transferability of parameters in English image-text alignment modules. We find that the Qformer module trained on English image-text data can be transferred to other languages. In our experiments, we have successfully transferred the model parameters from Indo-European English to Sino-Tibetan Chinese. The transferability of language greatly increases the possibility of using English image-text data and its trained model parameters, and improves the efficiency of training multimodal LLMs in other languages.\n\u2022 Open-source. We construct a concise and high-quality Chinese multimodal instruction dataset. By training X-LLM on this multimodal instruction data, X-LLM can better integrate the multimodal capabilities acquired through multiple encoders and corresponding X2L interfaces. And We release the following assets to the public: the generated multimodal instruction data, the codebase for model training, the model checkpoint, and a multimodal chat demo."
        },
        {
            "heading": "2 Related Work",
            "text": "Vision-Language Models. As summarized in many surveys [5, 16], visual language models [58, 30] have made great strides with the development of pre-training techniques [11, 2, 60]. In the early days, researchers used Faster-RCNN [19] to extract image features and concatenated them with language models such as BERT [11] to perform vision-language pre-training. VisualBERT [31], for example, combines image regions and language using a Transformer [49] to allow self-attention to discover implicit alignments between language and vision. It is pre-trained with masked language modeling [11] and a sentence-image prediction task [31]. With the introduction of ViLT [26], researchers use vision transformers [13, 25] to process images, textual transformers (such as BERT [11], GPT-2 [42], T5 [43]) to process text, and pre-training objectives such as masked language modeling, image-text matching, and image-text contrast to train visual language models. CLIP [41] uses a text encoder and an image encoder to encode text and images separately and then performs unsupervised contrastive learning to obtain good representations of vision-language alignment. BLIP [30] is a new VLP framework that transfers flexibly to both vision-language understanding and generation tasks.\nIn the field of visual dialogue [9, 6, 4], researchers design pre-training objectives related to visual dialogue based on vision-language pre-training models [11, 31] and finetune vison-language models on visual dialogue data [9] to achieve better dialogue performance. VisDial-BERT [36] and VDBERT [51], for example, use pre-trained ViLBERT [35] and BERT to finetune models on visual dialogue data using masked language modeling and image-text matching. AlignVD [7] proposes two methods for visual-language alignment based on pre-trained ViT [41] and BERT to achieve better performance in visual dialogue. Enhancing Vision-language Understanding with Advanced LLMs. Although the aforementioned vision-language models have achieved some success, there is still significant room for improvement in terms of language generation [5, 34, 29]. A recent method [29, 14, 37] for enhancing visual language understanding using advanced large-scale language models [48, 8] has been proposed. For example, BLIP2 [29] uses a Q-Former to connect a visual encoder with an LLM, aligning the learned queries of the Q-Former with language-related visual features extracted by the visual encoder. The Q-Former then connects the visual encoder with the language model, allowing the learned query representations to adapt to the LLM. PaLM-E [14] combines ViT-22B [10] with PaLM560B [2] to inject multimodal information into the embedding space of the pre-trained language model, establishing a connection between perception and language and greatly enhancing the model\u2019s visual language understanding ability. In addition, Visual ChatGPT [53] and HuggingGPT [46] use ChatGPT as the core logic controller, which understands user intent and then call upon specific domain visual language models. Finally, the recently proposed GPT-4 [37] demonstrates powerful multimodal capabilities: building on its strong language understanding abilities, it can generate complex image descriptions, create websites based on handwritten text instructions, and explain unusual visual phenomena. However, the model structure and training strategies of GPT-4 remain a mystery. MiniGPT-4 [59] and LLaVA [33] align text and image data to the large-scale language\nmodel Vicuna [8] and ViT [57] to complete image-based language tasks. In contrast, X-LLM is a universal framework for multimodal LLMs that bootstraps advanced large language models by treating multi-modalities as foreign languages. In this paper, we implement X-LLM that supports images, videos, and speech. Based on the X-LLM framework, we can extend the model to more modalities, such as injecting continuous space robot states, terminal information, or audio rather than speech into the LLM."
        },
        {
            "heading": "3 Approach",
            "text": "X-LLM aims to align multiple pre-trained single-modal encoders with advanced large-scale language models (LLMs), as shown in Figure 1. Specifically, we use ChatGLM 1 as the language decoder, which is built on top of GLM [17, 56] and can perform various complex language tasks. For visual perception, we adopt ViT-g [57], as the image encoder and video encoder. For speech perception, we use a speech encoder comprised of convolution layers and conformer structure [21]. We design a module that aligns multimodal information with LLM, collectively referred to as the X2L interfaces, which includes an image interface, a video interface, and a speech interface. The image interface and the video interface have the same structure which consists of Q-Formers [29] and Adapter modules. The speech interface includes the C-Former and an Adapter module. The C-Former could compress the frame-level speech feature sequence from the speech encoder into the token-level speech embedding sequence with continuous integrate-and-fire (CIF) mechanism [12, 23, 22]. As the token-level speech embedding sequence is strictly aligned with the token sequence of the transcription corresponding to the speech utterance, representing speech using token-level speech embeddings can effectively reduce the GPU memory usage when incorporating speech into LLMs."
        },
        {
            "heading": "3.1 X2L Interfaces",
            "text": "X2L interfaces aim to convert multimodal information into foreign languages, which includes an image interface, a video interface, and a speech interface.\nThe Image Interface. Inspired by [29], the image interface consists of a Q-Formers [29] and an I-Adapter module. The Q-Formers aims to convert images into languages, where image features obtained from the image encoder are converted into a sequence with Li quasi-linguistic embeddings. The I-Adapter module aims to align the dimensions of the quasi-linguistic embeddings and the embedding dimension of the LLM.\nThe Video Interface. The video interface has the same structure as the image interface, which also consists of Q-Formers [29] and a V-Adapter module. We use uniform sampling and represent each video with T frames. We then treat each frame as an image. The video interface converts each frame features into a sequence with Li quasi-linguistic embeddings. Then the video interface concatenates all the sequences to obtain the final quasi-linguistic embeddings, which have a length of T \u00d7 Li.\nThe Speech Interface. To transform the speech features from the speech encoder into more semantic representations, we introduce a speech-to-language interface called the speech interface. The speech interface consists of two parts, namely the C-Former and the S-Adaptor. The C-Former is the combination of a CIF module and a 12-layer transformer structure [11]. First, the CIF module compresses the speech feature sequence from the speech encoder into a token-level speech embedding sequence with the same length as the corresponding transcription via variable-length down-sampling. Assuming the length of the feature sequence emitted by the speech encoder for the input speech is U , and the length of the token sequence of the transcription of the speech utterance is Ls, the length of the token-level speech embedding sequence should be Ls (U is usually several times longer than Ls). Then, the transformer structure provides contextual modeling for the token-level speech embeddings from the CIF module. Finally, the S-Adaptor is used to project the outputs of the transformer structure to the input vector space of the LLM, further narrowing down the semantic gap between speech and language.\n1https://github.com/THUDM/ChatGLM-6B"
        },
        {
            "heading": "3.2 Training Strategy",
            "text": "To efficiently implement X-LLM, we propose a three-stage training strategy. (1) Converting Multimodal Information: we align the Image Encoder with the Q-Former of the image (green part), and the Speech Encoder with the CIF module. (2) Aligning X2L representations with the LLM: in the second stage, we align the Image Encoder with the LLM through the image interface, align the Video Encoder with the LLM through the video interface, and align the Speech Encoder with LLM through the speech interface. In the third stage, we integrate training of the image, video, and speech, and align the overall single-modal encoders with the LLM using a smaller but high-quality multimodal instruction dataset (such as instructions containing visual spoken dialogue, i.e., responding to spoken dialogue inputs based on images)."
        },
        {
            "heading": "3.2.1 First Training Stage: Converting Multimodal Information",
            "text": "In the first stage, the traditional approach is to align the Image Encoder with the image Q-Former using a large amount of image-text data, similar to the first stage of BLIP2 [29] which utilized around 500 million image-text pairs. However, we find that while BLIP2 used English data, we can still leverage the pretrained parameters of the Q-Former in BLIP2 to implement a Chinese Multimodal LLM. Therefore, in the first stage, to efficiently implement X-LLM, we only convert the representation of the speech encoder to a quasi-linguistic representation through the speech interface.\nFor the speech-related structures, we train a CIF-based ASR model with multiple ASR datasets containing to obtain the speech encoder and CIF module in the C-Former. The CIF-based ASR model consists of a speech encoder, a CIF module, and a decoder [12]. We employ the speech encoder of this ASR model as the speech encoder and employ the CIF module of this ASR model as that in the C-Former of the speech interface. Note that the parameters of the speech encoder and CIF module are kept frozen during all subsequent training stages. Please refer to the appendix for more details about the structure and training of the CIF-based ASR model."
        },
        {
            "heading": "3.2.2 Second Training Stage: Aligning X2L Representations with the LLM",
            "text": "As mentioned above, despite the difference in language, we are still able to reuse the parameters of the Q-Former in BLIP2. Specifically, we used the Q-Former trained in the second stage of BLIP2 to initialize the image interface\u2019s Q-Former in X-LLM. To adapt the Q-Former to Chinese LLM, we use a combined dataset, totaling approximately 14 million Chinese image-text pairs for training.\nNext, we use the trained image interface to initialize the video interface (the Q-Former and the V-Adapter) and train the video interface on the translated video-text data.\nFinally, we train the speech interface using ASR data to align the output of the speech interface with the LLM. It should be noted that throughout the entire second training stage, all the encoders and the LLM remain frozen, with only the interfaces being trained."
        },
        {
            "heading": "3.2.3 Third Training stage: Integrating Multiple Modalities",
            "text": "After the first two stages of training, our X-LLM has demonstrated a remarkable ability to provide reasonable answers to human queries based on multimodal information and has gained a vast amount of knowledge. We have observed that, even without the instruction for joint training on multiple modalities, such as \"answer questions based on images using voice input,\" X-LLM is capable of performing tasks that require multiple modalities, such as visual spoken dialogue, multimodal speech recognition, and multimodal machine translation. This remarkable ability is likely due to X-LLM\u2019s integration of LLM\u2019s excellent instruction generalization capability, which has been extended to the multimodal domain. This ability enables us to train more modalities independently in the first two stages and integrate them into the model without the need for joint training with existing modalities.\nTo explore the potential of multimodal joint instruction data in further enhancing X-LLM\u2019s ability to perform multimodal tasks, such as visual spoken question answering, we have constructed a concise but high-quality multimodal instruction dataset. Different from MiniGPT-4 [59] and LLaVA [33]\u2019s datasets, which only contain image-text instruction data and other textual instruction datasets for instruction finetuning and conversations, our dataset supports multimodal joint instructions and includes (1) image-text instruction data, (2) speech-text instruction data, (3) video-text instruction data, and (4) image-text-speech instruction data.\nConstructing a High-quality Alignment Dataset for Multimodal LLM. We use ChatGPT to translate 3.5K image-text instructions built by MiniGPT-4. Then, we manually select 2k data from AISHELL-2 [15] and write 5 different instructions for speech recognition tasks. We use ChatGPT to translate the ActivityNet dataset [27], followed by manually selecting 1k data and writing 5 different instructions for corresponding video-text tasks. We manually select and rewrite 1k data from self-constructed VSDial-CN data, aiming to enable the model to perform dialogue generation tasks based on images and speech. More details of the data can be found in the appendix, including the details of the training data for the first two stages and the multimodal instruction data.\nThe Third Training Stage. During this stage, we use the constructed compact yet high-quality data to finetune our model. During finetuning, we use the predefined prompts in the following template:\n<Image><ImageFeats></Image><Video><VideoFeats></Video><Speech><SpeechFeats> </Speech>Question: <Instruction>\\n Answer: In this prompt, <Instruction> represents a randomly sampled instruction from our predefined instruction set, including different forms such as \u201cdescribe this image in detail\u201d, \u201ccan you describe what you notice in the video\u201d, or \u201canswer the question in the speech based on the image\u201d. It should be noted that we do not calculate regression loss specifically for this particular instruction prompt. Therefore, X-LLM can integrate multiple modalities and generate more natural and reliable responses based on various combinations of instructions as needed."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Multimodal Chat",
            "text": "We have developed a Chatbot demo to show multimodal understanding and conversation abilities of X-LLM. For comparisons, query LLaVA [33]2 and MiniGPT-4 [59]3 from their online demos to get their response.\nAs shown in Table 2 and 3, although LLaVA and MiniGPT-4 also exhibit the characteristic of generating answers based on the given prompt, their answers regarding visual content with Chinese elements are not as satisfactory. In the first example about the Forbidden City shown in Table 2, X-LLM recognizes that the place is the Forbidden City and provides a detailed introduction to its history, architecture, and style. LLaVA describes Chinese palaces and flags, but it does not recognize that the famous palace is the Forbidden City and therefore cannot provide relevant information about it. MiniGPT-4 exhibits the same problem and tends to describe the image more. In the second example about the game \u201cHonor of Kings\u201d shown in Table 3, X-LLM identifies it as a multiplayer online battle arena game, \u201cHonor of Kings\u201d, developed by Tencent and provides accurate release time. LLaVA, on the other hand, gives multiple incorrect answers, as there are no elements of popular games such as snakes and pocket monsters in the image, and the game is not played with a mouse. MiniGPT-4 fails to recognize the game and provides a more generic description.\nFor video input and speech input, we provide some examples as shown in Appendix B.\nQuantitative Evaluation. In order to systematically evaluate the performance of the X-LLM model on visual input, we aim to use quantitative metrics to measure the model\u2019s ability to follow instructions. We adopt an evaluation method similar to that proposed by LLaVA [33] and use ChatGPT to measure the quality of the answers generated by our model. Specifically, we use the LLaVA-test dataset [33] provided by LLaVA, which contains 30 randomly selected images from the COCO validation set, each with three types of questions (conversation, detailed description, and complex reasoning). We first translate the questions into Chinese, and X-LLM predicts the answers based on the translated Chinese questions and visual input images. Then we translate the responses given by X-LLM into English for comparison with GPT-4. GPT-4 makes reference predictions based on the question, ground truth bounding boxes, and captions, marking the upper limit. After obtaining the responses from the two models, we provide the question, visual information (in the form of captions and bounding boxes), and generated responses from both assistants to ChatGPT. ChatGPT evaluates the usefulness,\n2https://llava-vl.github.io/ 3https://minigpt-4.github.io/\nrelevance, accuracy, and level of detail of the assistants\u2019 responses and gives an overall score from 1 to 10, with higher scores indicating better overall performance. ChatGPT is also required to provide a comprehensive evaluation explanation for a better understanding of the model. LLaVA used GPT-4 as a teacher to evaluate the quality of the responses generated by LLaVA and GPT-4, while we believe that using a non-GPT-4 evaluation model (i.e. using ChatGPT) will be more objective (Also because we do not have GPT-4 API.). Examples of test questions can be found in Appendix A.2.\nWe show the results in Table 1. Although different evaluation models are used (LLaVA uses GPT-4, X-LLM uses ChatGPT), we are able to make rough comparisons. The results show that X-LLM yields a performance of 84.5% nearly GPT-4. X-LLM outperforms LLaVA in terms of conversation and detail description but is inferior in complex reasoning. There are two reasons for this. One reason is that X-LLM do not use the 150k visual instruction dataset proposed by LLaVA, which has the same format as the test set. The second reason is that X-LLM has fewer language model parameters. It is based on ChatGLM with 6B parameters, while LLaVA is based on Vicuna with 13B parameters. And we do not finetune the LLM while LLaVA finetune the LLM Vicuna.\nFurthermore, comparing \u201cX-LLM w/ 4M\u201d and \u201cX-LLM w/ 4M no init\u201d, we can observe that using the BLIP2 pre-trained Q-Former parameters significantly improves the model\u2019s performance, which This verifies the transferability of parameters in the English image text alignment module. The transferability of language greatly increases the possibility of using English image-text data and its trained model parameters, and improves the efficiency of training multimodal LLMs in other languages. Comparing X-LLM and \u201cX-LLM w/ 4M\u201d, we can see that increasing the number of image-text pairs used during training can enhance the model\u2019s performance. However, we also notice that X-LLM performs worse than \u201cX-LLM w/ 4M\u201d in complex reasoning, which may be attributed to the additional use of the Wukong dataset [20], whose quality is inferior to that of the dataset comprising the 4M data. Note that all three variants of X-LLM achieve high performances on complex reasoning which is because of the powerful ability of language models and complex reasoning questions can also be answered without images to some extent."
        },
        {
            "heading": "4.2 ASR and Multimodal ASR",
            "text": "We evaluate the ASR and multmodal ASR (MASR) on AISHELL-2 and VSDial-CN datasets. Please refer to Appendix A for details.\nAs shown in Table 4, we compare X-LLM with the CIF-based model mentioned in Section 3.2.1. From the results in the table, overall, the speech recognition performance of X-LLM is weaker than that of the CIF-based Model. This is because the CIF-based model is trained with full parameters, while X-LLM only trains the BERT in the C-Former and the S-Adapter. In the results of multimodal speech recognition on the VSDial-CN dataset, the introduction of images caused a decrease in the speech recognition performance of X-LLM (from 3.4 to 4.7 on test-orig and from 4.7 to 6.8 on test-art). This may be because we provided X-LLM with a fixed single instruction \u201cPlease faithfully recognize the speech\u201d during the second training stage for speech recognition. However, when conducting multimodal speech recognition, we replace the instruction with \u201cPlease faithfully recognize the speech based on the image.\u201d. After finetuning X-LLM with 2k speech recognition-related instructions in the third stage, the error rate of X-LLM\u2019s multimodal speech recognition decreased from 4.7 to 4.3 on test-orig and from 6.8 to 6.3 on test-art. In the future, we will supplement the training process by using various speech recognition instructions in the second stage and increasing the amount of\ndata for finetuning instructions in the third stage to observe the changes in the multimodal speech recognition ability of X-LLM. Additionally, a more powerful LLM may have stronger instruction generalization, which could improve the performance of multimodal speech recognition.\nWe observe that although the addition of images to X-LLM\u2019s speech recognition task results in a slight decrease in performance, X-LLM is able to comprehend spoken questions in speech without finetuning, and provide appropriate responses. It can also incorporate images to provide suitable answers to spoken questions. After a small amount of data finetuning in the third phase, X-LLM\u2019s ability in this regard is further improved."
        },
        {
            "heading": "5 Discussions",
            "text": "This paper demonstrates the effectiveness of X-LLM, which injects multiple modalities as foreign languages into a large language model through the X2L interface, endowing LLM with powerful multimodal capabilities. We design a three-stage training method to train X-LLM, where each modality interface has high independence in the first two stages, facilitating simultaneous training. Through the first two stages of training, X-LLM can interact with each modality through language. Furthermore, X-LLM can complete tasks involving multiple modalities (such as visual spoken question answering) without further finetuning on joint instruction datasets, thanks to its integration of the instruction generalization ability of large language models and its adaptation to the multimodal domain. The integration of multiple modalities without training greatly facilitates the modality expansion of X-LLM. To further explore the impact of joint multimodal instruction data on X-LLM\u2019s ability to integrate multiple modalities, we construct a streamlined but high-quality multimodal instruction dataset, and X-LLM\u2019s performance is further improved after fine-tuning on this data.\nThis project is still ongoing and currently has several limitations: (1) Limitations of the language model. X-LLM is built on top of ChatGLM with only 6B parameters and inherits its limitations, including but not limited to unreliable reasoning ability and fabrication of non-existent facts. (2) Insufficient training for modal connections. X-LLM\u2019s multi-modal perception ability is somewhat limited. We only used a small amount of multi-modal data sets to connect the multi-modal encoder and a large language model. There are several directions for further exploration: (1) Data scale. Compared to BLIP2, we only used a small amount of Chinese multimodal data. We believe that using larger Chinese data for training can significantly improve the model\u2019s performance by increasing concept coverage. (2) Connecting more modalities. We can connect audio to enable LLM to understand and interact with non-verbal audio. We can also connect the status information of various terminals to LLM, so that LLM can control the terminals based on their status information. (3) Using better LLM. Due to the limitation of computing resources, we only used a 6B language model for experimentation. It can be expected that using a stronger language model, X-LLM will gain more powerful capabilities."
        },
        {
            "heading": "B Qualitative Examples",
            "text": "We provide more examples of video inputs, speech inputs and multimodal inputs. From the examples given, it can be seen that X-LLM has impressive video and speech understanding capabilities. As shown in Table 8 and 10, X-LLM is able to provide responses based on video content and perform multimodal machine translation, such as translating \u201cbank\u201d into Chinese as \u201csloping raised land\u201d rather than \u201ca financial organization\u201d according to the given image. Additionally, X-LLM can integrate multiple modalities without the need for further instruction finetuning, such as being able to answer spoken questions based on images without requiring ASR as shown in Table 9.\nIn addition, we propose two examples of ethical, moral, and legal design, and X-LLM is able to handle these situations well. In the example shown in Table 11, X-LLM refuses to answer the user\u2019s question because the image violated relevant Chinese laws, and reminded the user to comply with the relevant laws. In the example shown in Table 12, X-LLM refuses to make judgments or recommendations on ethical and moral issues and reminded the user to comply with traffic regulations and safety knowledge, drive carefully, and pay attention to safety.\n6https://huggingface.co/openai/clip-vit-base-patch16"
        }
    ],
    "title": "X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages",
    "year": 2023
}