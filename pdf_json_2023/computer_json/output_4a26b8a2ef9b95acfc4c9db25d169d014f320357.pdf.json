{
    "abstractText": "Non-asymptotic statistical analysis is often missing for modern geometry-aware machine learning algorithms due to the possibly intricate nonlinear manifold structure. This paper studies an intrinsic mean model on the manifold of restricted positive semi-definite matrices and provides a non-asymptotic statistical analysis of the Karcher mean. We also consider a general extrinsic signal-plus-noise model, under which a deterministic error bound of the Karcher mean is provided. As an application, we show that the distributed principal component analysis algorithm, LRC-dPCA, achieves the same performance as the full sample PCA algorithm. Numerical experiments lend strong support to our theories.",
    "authors": [
        {
            "affiliations": [],
            "name": "Hengchao Chen"
        },
        {
            "affiliations": [],
            "name": "Xiang Li"
        },
        {
            "affiliations": [],
            "name": "Qiang Sun"
        }
    ],
    "id": "SP:b9c7b6517c40529adab4697a32a675a79168ca51",
    "references": [
        {
            "authors": [
                "E. Abbe",
                "J. Fan",
                "K. Wang",
                "Y. Zhong"
            ],
            "title": "Entrywise eigenvector analysis of random matrices with low expected rank",
            "venue": "Annals of statistics,",
            "year": 2020
        },
        {
            "authors": [
                "V. Arsigny",
                "P. Fillard",
                "X. Pennec",
                "N. Ayache"
            ],
            "title": "Geometric means in a novel vector space structure on symmetric positive-definite matrices",
            "venue": "SIAM journal on matrix analysis and applications,",
            "year": 2007
        },
        {
            "authors": [
                "A. Bhaskara",
                "P.M. Wijewardena"
            ],
            "title": "On distributed averaging for stochastic k-pca",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "R. Bhattacharya",
                "V. Patrangenaru"
            ],
            "title": "Large sample theory of intrinsic and extrinsic sample means on manifolds",
            "year": 2003
        },
        {
            "authors": [
                "R. Bhattacharya",
                "V. Patrangenaru"
            ],
            "title": "Large sample theory of intrinsic and extrinsic sample means on manifolds\u2014ii",
            "venue": "The Annals of Statistics,",
            "year": 2005
        },
        {
            "authors": [
                "J. Bigot",
                "X. Gendre"
            ],
            "title": "Minimax properties of fr\u00e9chet means of discretely sampled curves",
            "venue": "The Annals of Statistics,",
            "year": 2013
        },
        {
            "authors": [
                "D.A. Bini",
                "B. Iannazzo"
            ],
            "title": "Computing the karcher mean of symmetric positive definite matrices. Linear Algebra and its Applications, 438(4):1700\u20131710",
            "year": 2013
        },
        {
            "authors": [
                "S. Bonnabel",
                "A. Collard",
                "R. Sepulchre"
            ],
            "title": "Rank-preserving geometric means of positive semidefinite matrices",
            "venue": "Linear Algebra and its Applications,",
            "year": 2013
        },
        {
            "authors": [
                "S. Bonnabel",
                "R. Sepulchre"
            ],
            "title": "Riemannian metric and geometric mean for positive semidefinite matrices of fixed rank",
            "venue": "SIAM Journal on Matrix Analysis and Applications,",
            "year": 2010
        },
        {
            "authors": [
                "J. Cape",
                "M. Tang",
                "C.E. Priebe"
            ],
            "title": "The two-toinfinity norm and singular subspace geometry with applications to high-dimensional statistics",
            "year": 2019
        },
        {
            "authors": [
                "Chang",
                "X.-W",
                "C.C. Paige",
                "G. Stewart"
            ],
            "title": "New perturbation analyses for the cholesky factorization",
            "venue": "IMA journal of numerical analysis,",
            "year": 1996
        },
        {
            "authors": [
                "Chang",
                "X.-W",
                "C.C. Paige",
                "G. Stewart"
            ],
            "title": "Perturbation analyses for the qr factorization",
            "venue": "SIAM Journal on Matrix Analysis and Applications,",
            "year": 1997
        },
        {
            "authors": [
                "V. Charisopoulos",
                "A.R. Benson",
                "A. Damle"
            ],
            "title": "Communication-efficient distributed eigenspace estimation",
            "venue": "SIAM Journal on Mathematics of Data Science,",
            "year": 2021
        },
        {
            "authors": [
                "X. Chen",
                "J.D. Lee",
                "H. Li",
                "Y. Yang"
            ],
            "title": "Distributed estimation for principal component analysis: An enlarged eigenspace analysis",
            "venue": "Journal of the American Statistical Association,",
            "year": 2021
        },
        {
            "authors": [
                "Y. Chen",
                "Y. Chi",
                "J. Fan",
                "C. Ma"
            ],
            "title": "Spectral methods for data science: A statistical perspective",
            "venue": "arXiv preprint arXiv:2012.08496",
            "year": 2020
        },
        {
            "authors": [
                "E. Cornea",
                "H. Zhu",
                "P. Kim",
                "J.G. Ibrahim",
                "A.D.N. Initiative"
            ],
            "title": "Regression models on riemannian symmetric spaces",
            "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),",
            "year": 2017
        },
        {
            "authors": [
                "A. Damle",
                "Y. Sun"
            ],
            "title": "Uniform bounds for invariant subspace perturbations",
            "venue": "SIAM Journal on Matrix Analysis and Applications,",
            "year": 2020
        },
        {
            "authors": [
                "I.L. Dryden",
                "A. Koloydenko",
                "D. Zhou"
            ],
            "title": "Noneuclidean statistics for covariance matrices, with applications to diffusion tensor imaging",
            "venue": "The Annals of Applied Statistics,",
            "year": 2009
        },
        {
            "authors": [
                "J. Fan",
                "D. Wang",
                "K. Wang",
                "Z. Zhu"
            ],
            "title": "Distributed estimation of principal eigenspaces",
            "venue": "Annals of statistics,",
            "year": 2019
        },
        {
            "authors": [
                "J. Fan",
                "W. Wang",
                "Y. Zhong"
            ],
            "title": "An `\u221e eigenvector perturbation bound and its application to robust covariance estimation",
            "venue": "Journal of Machine Learning Research,",
            "year": 2018
        },
        {
            "authors": [
                "M. Faraki",
                "M.T. Harandi",
                "F. Porikli"
            ],
            "title": "Image set classification by symmetric positive semi-definite matrices",
            "venue": "In 2016 IEEE Winter conference on applications of computer vision (WACV),",
            "year": 2016
        },
        {
            "authors": [
                "A. Gang",
                "H. Raja",
                "W.U. Bajwa"
            ],
            "title": "Fast and communication-efficient distributed pca",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2019
        },
        {
            "authors": [
                "D. Garber",
                "O. Shamir",
                "N. Srebro"
            ],
            "title": "Communication-efficient algorithms for distributed stochastic principal component analysis",
            "year": 2017
        },
        {
            "authors": [
                "A. Grammenos",
                "R. Mendoza Smith",
                "J. Crowcroft",
                "C. Mascolo"
            ],
            "title": "Federated principal component analysis",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "T. Hastie",
                "R. Tibshirani",
                "J.H. Friedman"
            ],
            "title": "The elements of statistical learning: data mining, inference, and prediction, volume",
            "year": 2009
        },
        {
            "authors": [
                "M. Journ\u00e9e",
                "F. Bach",
                "Absil",
                "P.-A",
                "R. Sepulchre"
            ],
            "title": "Low-rank optimization on the cone of positive semidefinite matrices",
            "venue": "SIAM Journal on Optimization,",
            "year": 2010
        },
        {
            "authors": [
                "H. Karcher"
            ],
            "title": "Riemannian center of mass and mollifier smoothing",
            "venue": "Communications on Pure and Applied Mathematics,",
            "year": 1977
        },
        {
            "authors": [
                "X. Li",
                "S. Wang",
                "K. Chen",
                "Z. Zhang"
            ],
            "title": "Communication-efficient distributed svd via local power iterations",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "L. Mackey",
                "A. Talwalkar",
                "M.I. Jordan"
            ],
            "title": "Divide-and-conquer matrix factorization",
            "venue": "In Proceedings of the 24th International Conference on Neural Information Processing Systems,",
            "year": 2011
        },
        {
            "authors": [
                "E. Massart",
                "Absil",
                "P.-A"
            ],
            "title": "Quotient geometry with simple geodesics for the manifold of fixed-rank positive-semidefinite matrices",
            "venue": "SIAM Journal on Matrix Analysis and Applications,",
            "year": 2020
        },
        {
            "authors": [
                "M. Moakher"
            ],
            "title": "A differential geometric approach to the geometric mean of symmetric positive-definite matrices",
            "venue": "SIAM Journal on Matrix Analysis and Applications,",
            "year": 2005
        },
        {
            "authors": [
                "M. Mohri",
                "A. Rostamizadeh",
                "A. Talwalkar"
            ],
            "title": "Foundations of machine learning",
            "year": 2018
        },
        {
            "authors": [
                "A.M. Neuman",
                "Y. Xie",
                "Q. Sun"
            ],
            "title": "Restricted riemannian geometry for positive semidefinite matrices. arXiv preprint arXiv:2105.14691",
            "year": 2021
        },
        {
            "authors": [
                "V. Patrangenaru",
                "L. Ellingson"
            ],
            "title": "Nonparametric statistics on manifolds and their applications to object data analysis",
            "year": 2016
        },
        {
            "authors": [
                "M. Pilanci",
                "M.J. Wainwright"
            ],
            "title": "Randomized sketches of convex programs with sharp guarantees",
            "venue": "IEEE Transactions on Information Theory,",
            "year": 2015
        },
        {
            "authors": [
                "G. Stewart"
            ],
            "title": "Perturbation bounds for the qr factorization of a matrix",
            "venue": "SIAM Journal on Numerical Analysis,",
            "year": 1977
        },
        {
            "authors": [
                "G. Stewart"
            ],
            "title": "On the perturbation of lu and cholesky factors",
            "venue": "IMA Journal of Numerical Analysis,",
            "year": 1997
        },
        {
            "authors": [
                "B. Vandereycken",
                "Absil",
                "P.-A",
                "S. Vandewalle"
            ],
            "title": "A riemannian geometry with complete geodesics for the set of positive semidefinite matrices of fixed rank",
            "venue": "IMA Journal of Numerical Analysis,",
            "year": 2013
        },
        {
            "authors": [
                "R. Vershynin"
            ],
            "title": "Introduction to the non-asymptotic analysis of random matrices, page 210\u2013268",
            "year": 2012
        },
        {
            "authors": [
                "M.J. Wainwright"
            ],
            "title": "High-dimensional statistics: A non-asymptotic viewpoint, volume 48",
            "year": 2019
        },
        {
            "authors": [
                "R. Wang",
                "H. Guo",
                "L.S. Davis",
                "Q. Dai"
            ],
            "title": "Covariance discriminative learning: A natural and efficient approach to image set classification",
            "venue": "IEEE conference on computer vision and pattern recognition,",
            "year": 2012
        },
        {
            "authors": [
                "Fan"
            ],
            "title": "Let \u03a3, \u03a3\u0302 \u2208 Rp\u00d7p be symmetric matrices with eigenvalues",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "Non-asymptotic statistical analysis is often missing for modern geometry-aware machine learning algorithms due to the possibly intricate nonlinear manifold structure. This paper studies an intrinsic mean model on the manifold of restricted positive semi-definite matrices and provides a non-asymptotic statistical analysis of the Karcher mean. We also consider a general extrinsic signal-plus-noise model, under which a deterministic error bound of the Karcher mean is provided. As an application, we show that the distributed principal component analysis algorithm, LRC-dPCA, achieves the same performance as the full sample PCA algorithm. Numerical experiments lend strong support to our theories."
        },
        {
            "heading": "1 Introduction",
            "text": "Positive semi-definite (PSD) matrices arise in a wide range of applications, such as covariance matrices in statistics (Wainwright, 2019), kernel matrices in machine learning (Hastie et al., 2009), diffusion tensor images in medical imaging (Dryden et al., 2009), semi-definite programming (Journe\u0301e et al., 2010), and covariance descriptors in image set classification (Wang et al., 2012), to name a few. From a geometric perspective, the cone of PSD matrices is not a vector space, since linear combinations of multiple PSD matrices are not necessarily PSD matrices. Instead, the set of (restricted) PSD matrices of fixed rank has been endowed with different metrics such that it forms a Riemannian manifold (Bonnabel and Sepulchre, 2010; Vandereycken et al., 2013; Massart and Absil, 2020; Neuman et al., 2021). By utilizing the geometric structures, researchers have developed many powerful statistical or computational methods (Faraki et al., 2016; Cornea et al., 2017; Patrangenaru and Ellingson, 2016).\nProceedings of the 26th International Conference on Artificial Intelligence and Statistics (AISTATS) 2023, Valencia, Spain. PMLR: Volume 206. Copyright 2023 by the author(s).\nOne important concept in Riemannian geometry or more generally metric spaces is the Karcher mean (Karcher, 1977). The Karcher mean is often referred to as the Fre\u0301chet mean or the barycenter of mass. GivenM points {zm}Mm=1 on a metric space (M, d) with distance function d(\u00b7, \u00b7), the Karcher mean z\u0303 of these points is given by\nz\u0303 = argmin z\u2208M \u2211 m d2(z, zm). (1.1)\nWhen the underlying space is Euclidean, the Karcher mean is reducesd to the arithmetic mean. In general, the existence and computation of the Karcher mean is already complicated due to the possibly intricate non-Euclidean structure (Karcher, 1977; Bini and Iannazzo, 2013). As a result, most works focus on the computation and applications of the Karcher mean, while few provide statistical guarantees. Statistically, Bhattacharya and Patrangenaru (2003, 2005) establish a large sample theory of the Karcher mean on manifolds with applications to spheres and projective spaces. Bigot and Gendre (2013) shows the minimax optimality of the Karcher mean of discretely sampled curves. In this paper, we consider the manifold of restricted PSD matrices by Neuman et al. (2021). In particular, we first study an intrinsic mean model, inspired by the geometric structure of the restricted PSD manifold. A non-asymptotic statistical analysis of the Karcher mean is provided under this intrinsic model. We further consider a general extrinsic signal-plus-noise model, which does not necessarily coincide with the manifold geometry by Neuman et al. (2021). For this general model, we give a deterministic error bound for the Karcher mean, which is then used to provide an error bound for a distributed principal component analysis algorithm.\nThe Karcher mean is closely related to distributed learning problems, especially the divide-and-conquer (DC) framework (Mackey et al., 2011). In distributed learning problems, massive datasets are scattered across distant servers and directly fusing these datasets is challenging due to concerns on communication cost, privacy, data security, and ownership, among others. A commonly used distributed framework is the DC framework which first computes local estimators locally and then aggregate them on the central server, where the last step is often equivalent to com-\nar X\niv :2\n30 2.\n12 42\n6v 3\n[ st\nat .M\nL ]\n2 1\nM ar\n2 02\nputing the Karcher mean on certain manifolds. For example, the divide-and-conquer principal component analysis (PCA) algorithms (Fan et al., 2019; Bhaskara and Wijewardena, 2019; Neuman et al., 2021) essentially compute the Karcher means on the Grasssmann manifold, Euclidean space, or the manifold of restricted PSD matrices, respectively. Motivated by this observation, we give theoretical guarantees of the DC PCA algorithm, LRC-dPCA, proposed in Neuman et al. (2021) by applying our nonasymptotic statistical analysis of the Karcher mean on the restricted PSD manifold. Specifically, we show that given sufficienly large local sample size, LRC-dPCA achieves the same performance as the full sample PCA algorithm, which outputs the top eigenvectors of the covariance matrix based on full data.\nOur contributions are three-fold. First, we provide a nonasymptotic statistical analysis of the Karcher mean on the restricted PSD manifold under an intrinsic model. Second, for a generic signal-plus-noise model, we give a deterministic characterization of the Karcher mean and then obtain a deterministic error bound. Third, as an application, we show that LRC-dPCA and full sample PCA share the same performance given sufficiently large local sample size. Numerical experiments are carried out to support our theories.\nThe rest of this paper proceeds as follows. We conclude this section with a discussion on related works. Section 2 reviews the geometry for restricted PSD matrices proposed in Neuman et al. (2021). Then in Section 3, we provide the theoretical analysis of the Karcher mean on the restricted PSD manifolds. Applications to distributed PCA algorithms are given in Section 4. Numerical experiments are carried out in Section 5 and we give concluding remarks in Section 6. Proofs are left to the Appendix."
        },
        {
            "heading": "1.1 Related work",
            "text": "Manifolds of PSD matrices The cone of symmetric positive definite (SPD) matrices is not a vector space. It can be viewed as different Riemannian manifolds when endowed with different metrics, such as the affine-invariant metric (Moakher, 2005) and the Log-Euclidean metric (Arsigny et al., 2007). It is, however, non-trivial to generalize these metrics to the rank-deficient (PSD) case. To this end, Bonnabel and Sepulchre (2010) treated a PSD matrix of rank K in a quotient space as a K-dimensional subspace coupled with a K-by-K SPD matrix and then endowed the manifold of PSD matrices with a weighted product metric. Using this geometry, Bonnabel et al. (2013) developed a rank-preserving geometric mean of PSD matrices. Later, Vandereycken et al. (2013) viewed a PSD manifold as a homogeneous space and Massart and Absil (2020) analyzed a quotient geometry on the manifold of PSD matrices. However, it is hard to give a statistical model on these manifolds. More recently, Neuman et al. (2021) proposed\na geometry for restricted PSD matrices which has closedform solutions for many geometric concepts including the Karcher mean. Our paper provides statistical analysis of the Karcher mean corresponding to this geometry.\nDistributed PCA To estimate the leading eigenvector, Garber et al. (2017) proposed a sign-fixing averaging approach. To estimate the top K eigenspace, Fan et al. (2019) proposed a projector averaging approach and Charisopoulos et al. (2021) proposed to average local eigenvector matrices after carefully rotating them. Disregarding the information of eigenvalues, both Fan et al. (2019)\u2019s and Charisopoulos et al. (2021)\u2019s methods require the knowledge of the precise location K of a large eigen gap. To alleviate this issue, Bhaskara and Wijewardena (2019) proposed to average the local rank-K approximation matrices and then conduct PCA on the aggregated matrix. Neuman et al. (2021) utilized the same methods as Bhaskara and Wijewardena (2019) except that the average of local rank-K approximation matrices is taken on the manifold of restricted PSD matrices. Neuman et al. (2021) did not provide statistical analysis for their proposed method, while our paper fixes this gap as an application of the main results. Another branch of research turns PCA into the problem of solving a linear system and then solves distributed PCA by some multi-round algorithms. Among them, some make use of the shift-andinvert framework (Garber et al., 2017; Chen et al., 2021), while some use incremental update schemes (Gang et al., 2019; Grammenos et al., 2020; Li et al., 2021).\nNotation. By convention, we use regular letters for scalars and bold letters for both vectors and matrices. Given a vector u \u2208 Rp, denote by \u2016u\u20162 its `2 norm. Given a matrix A \u2208 Rn\u00d7p, we use \u2016A\u2016F, \u2016A\u20162 and \u2016A\u2016max = maxi,j |Aij | to denote its Frobenius norm, `2 norm and max norm, respectively. We use span(A) to represent the subspace spanned by the columns of A. For a symmetric matrix A, denote by \u03bbj(A) its jth largest eigenvalue. For two sequences of real numbers {an}n\u22651 and {bn}n\u22651, we write an . bn (or an & bn) if an \u2264 Cbn (or an \u2265 Cbn) for some constant C > 0 independent of n. For an infinitesimal number , we denote a matrix whose Frobenius norm or max norm is O( ) (i.e., . ) by OF( ) or Omax( ), respectively. Given a random variable x \u2208 R, we define \u2016x\u2016\u03c82 = supp\u22651(E|x|p)1/p/ \u221a p and \u2016x\u2016\u03c81 = supp\u22651(E|x|p)1/p/p. Given two integers p \u2265 K > 0, we denote by Op\u00d7K the set of matrices in Rp\u00d7K whose columns are orthonormal. Denote by S(p,K) the set of all p\u00d7p PSD matrices of rankK. Denote by a \u2228 b = max{a, b}."
        },
        {
            "heading": "2 The Manifold of Restricted PSD Matrices",
            "text": "In this section, we briefly recap the geometry for restricted PSD matrices (Neuman et al., 2021). To start with, any PSD matrix A \u2208 S(p,K) has a unique Cholesky decom-\npositionA = LL> such thatL \u2208 Rp\u00d7p is a lower triangular matrix and has precisely K positive diagonal elements and p \u2212 K zero columns. The jth column of L is zero if and only if the jth column of A is linearly dependent on the previous j \u2212 1 columns of A. Thus, we can rewrite A = NN>, where N \u2208 Rp\u00d7K consists of K non-zero columns of L without changing the order. Note that N is mock lower triangular, i.e., Nij = 0 if i < j. We refer to N as the reduced Cholesky factor ofA. To further develop a geometric structure, Neuman et al. (2021) consider the restricted subset S\u2217(p,K) of S(p,K) such that the first K columns of A \u2208 S\u2217(p,K) are linearly independent. The set of all reduced Cholesky factors of matrices in S\u2217(p,K) is denoted by L\u2217(p,K), which is equivalent to the set of all mock lower triangular matrices in Rp\u00d7K with positive diagonal elements. Neuman et al. (2021) impose a Riemannian structure on S\u2217(p,K) and L\u2217(p,K) such that the following mappings are isometric,\nh : S\u2217(p,K) 7\u2192 L\u2217(p,K), A 7\u2192N , (2.1) g : L\u2217(p,K) 7\u2192 L(p,K), N 7\u2192N \u2032, (2.2)\nwhere N = h(A) is the reduced Cholesky factor of A, L(p,K) = {N \u2032 \u2208 Rp\u00d7K : N \u2032ij = 0, i < j} is endowed with a Euclidean structure, and N \u2032 = g(N) \u2208 L(p,K) is defined by N \u2032ii = log(Nii),\u2200 i and N \u2032ij = Nij ,\u2200 i > j. We refer to N \u2032 = g \u25e6 h(A) as the reduced log-Cholesky factor ofA. The Karcher mean A\u0303 ofM restricted PSD matrices {Am}Mm=1 \u2282 S\u2217(p,K) has a closed-form solution, which is given by\nA\u0303 = h\u22121 \u25e6 g\u22121( 1 M M\u2211 m=1 g \u25e6 h(Am)). (2.3)\nThe algorithm computing A\u0303 is referred to as the Low Rank Cholesky (LRC) algorithm (Neuman et al., 2021)."
        },
        {
            "heading": "3 Statistical Analysis of the Karcher Mean",
            "text": "In this section, we provide the first statistical analysis of the Karcher mean under an intrinsic model on the restricted PSD manifold. Then we consider a general signal-plusnoise model under which a deterministic error bound of the Karcher mean is given."
        },
        {
            "heading": "3.1 An intrinsic model",
            "text": "Inspired by the isometry stated in equations (2.1) and (2.2) between the manifold S\u2217(p,K) of restricted PSD matrices and the Euclidean spaceL(p,K), we propose the following intrinsic model. Suppose A \u2208 S\u2217(p,K) is the signal matrix and denote byN \u2032 = g\u25e6h(A) its reduced log-Cholesky factor. The observations {Am}Mm=1 are generated as follows:\nAm = h\u22121 \u25e6 g\u22121(N \u2032 +Em), m = 1, . . . ,M, (3.1)\nwhere {Em}Mm=1 \u2282 L(p,K) are independent and the lower triangular entries of Em are independent normal variables with mean zero and variance \u03c32. Under this intrinsic model, the Karcher mean of {Am}Mm=1 can be rewritten as\nA\u0303 = h\u22121 \u25e6 g\u22121(N \u2032 + 1 M M\u2211 m=1 Em). (3.2)\nUsing measure concentration, we can obtain a nonasymptotic error bound for the Karcher mean A\u0303.\nTheorem 3.1 (Intrinsic Model). SupposeA \u2208 S\u2217(p,K) is the signal matrix and assume \u2016A\u20162 \u2264 C for some constant C > 0. Assume samples {Am}Mm=1 are generated from the intrinsic model (3.1) and denote by A\u0303 the Karcher mean of {Am}Mm=1. Then there exist some constants c1, c2 > 0 such that the following inequality\n\u2016A\u0303\u2212A\u2016F \u2264 \u221a c2pK\u03c32\nM (3.3)\nholds with probability at least 1\u2212 e\u2212c1pK . Remark 3.2. It is worth noting that (3.3) achieves the optimal rate M\u22121/2. In addition, it only depends on the intrinsic dimension O(pK) of the manifold, which can be much smaller than the ambient dimension p2."
        },
        {
            "heading": "3.2 A general signal-plus-noise model",
            "text": "The intrinsic model may be too restricted, so this subsection introduces a general signal-plus-noise model and then provides a deterministic characterization of the Karcher mean. An application of this deterministic error bound to the distributed PCA problem will be given in Section 4. Similar to the intrinsic model, we denote byA \u2208 S\u2217(p,K) the signal matrix andN = h(A) its reduced Cholesky factor. The observations {Am}Mm=1 \u2282 S\u2217(p,K) are given by\nAm = (N +Em)(N +Em)>, (3.4)\nwhereEm \u2208 Rp\u00d7K represents them-th noise matrix. Here Em is not necessarily a mock lower triangular matrix, so the model is quite general. Also, the reduced Cholesky factor of Am is not necessarily N + Em, but rather (N +Em)Qm for some orthogonal matrixQm \u2208 OK\u00d7K . Denote byNm the reduced Cholesky factor ofAm.\nTo characterize the Karcher mean (2.3) of {Am}Mm=1, we first establish a linear perturbation expansion of QR decomposition below.\nLemma 3.3 (Linear Perturbation Expansion). Suppose Q \u2208 OK\u00d7K and R \u2208 RK\u00d7K is a lower triangular matrix with positive diagonal elements. Given a noise matrix E \u2208 RK\u00d7K , there exist a unique orthogonal matrix qQ \u2208 OK\u00d7K and a lower triangular matrix qR \u2208 RK\u00d7K with\nnon-negative diagonal elements such that qR qQ = RQ+E. When 0 = \u2016E\u2016max is sufficiently small, we have\nqQ = Q+ fR(EQ >)Q+Omax( 20), qR = R+EQ> \u2212RfR(EQ>) +Omax( 20),\nwhere fR : RK\u00d7K 7\u2192 RK\u00d7K is given by\nfR(E) = U(R\u22121E)\u2212 (U(R\u22121E))>, U(P )ij = Pij , i < j, U(P )ij = 0, otherwise.\nIt is worth emphasizing the following properties of fR. First, fR is linear in its argument, i.e., fR(aE + bF ) = afR(E) + bfR(F ) for any a, b \u2208 R and E,F \u2208 RK\u00d7K . Second, fR(E) is a skew-symmetric matrix, i.e., (fR(E))\n> = \u2212fR(E). Last, fR is bounded in the sense that \u2016fR(\u00b7)\u2016F \u2264 \u221a 2\u2016R\u22121\u20162\u2016 \u00b7 \u2016F. Similar first-order perturbation theories exist in the literature for QR, Cholesky, and LU factorization (Chang et al., 1996; Stewart, 1997, 1977; Chang et al., 1997), but none of them provides a linear perturbation expansion with a max-norm control on the remainder term, which is necessary for our development of the error bound on the Karcher mean and the subsequent applications in distributed PCA.\nNow we are ready to present a deterministic characterization of the Karcher mean. For convenience, we write N = (R> B>)> and Em = (E1,m > E2,m > )> such that R,E1,m \u2208 RK\u00d7K and B,E2,m \u2208 R(p\u2212K)\u00d7K . Note that R is a lower triangular matrix with positive diagonal elements since N \u2208 L\u2217(p,K). In the following theorem, we will show that when 0 = maxm \u2016Em\u2016max is sufficiently small, the reduced Cholesky factor N\u0303 of A\u0303 differs from N by a term linear in 1M \u2211M m=1E\nm and an extra term of order Omax( 20). Recall that S\u2217(p,K) is the manifold of restricted PSD matrices. Theorem 3.4 (Karcher Mean on S\u2217(p,K)). When 0 = maxm \u2016Em\u2016max is sufficiently small, the reduced Cholesky factor N\u0303 of the Karcher mean A\u0303 of {Am = (N +Em)(N +Em)>}Mm=1 on S\u2217(p,K) is\nN\u0303 = N + 1\nM M\u2211 m=1 Em \u2212NfR( 1 M M\u2211 m=1 E1,m)\n+Omax( 20),\nwhere fR(\u00b7) is given in Lemma 3.3.\nFrom Theorem 3.4, one may easily derive a deterministic upper bound on \u2016N\u0303 \u2212N\u2016F using the triangular inequality, which depends on \u2016 1M \u2211M m=1E\nm\u2016F and pK 20. Corollary 3.5. Under the same conditions of Theorem 3.4, if \u2016N\u20162 \u2264 C and \u2016R\u22121\u2016 \u2264 C for some constant C > 0, then we have\n\u2016N\u0303 \u2212N\u2016F \u2264 O(\u2016 1\nM M\u2211 m=1 Em\u2016F) +O(pK 20).\nAlgorithm 1: LRC-dPCA Input: {\u03a3\u0302m = 1n \u2211 i x m i x m> i }Mm=1, K;\nOutput: V\u0303 ; Compute V\u0302 m and \u039b\u0302m of \u03a3\u0302m and communicate them to a central server; Compute the Karcher mean A\u0303 of V\u0302 m(\u039b\u0302m)2V\u0302 m> on the manifold of restricted PSD matrices; Compute the top K eigenspace V\u0303 of A\u0303.\nIn applications such as distributed PCA, the Frobenius norm of the average 1M \u2211M m=1E\nm is much smaller than that of Em. Thus, by Corollary 3.5, the Karcher mean N\u0303 is a better approximation of N than any Nm (the reduced Cholesky factor ofAm)."
        },
        {
            "heading": "4 Applications to Distributed PCA",
            "text": "This section applies Theorem 3.4 to show that the distributed PCA algorithm, LRC-dPCA proposed by Neuman et al. (2021), achieves the same performance as the full sample PCA when the local sample size is sufficiently large."
        },
        {
            "heading": "4.1 Distributed PCA and LRC-dPCA",
            "text": "We start with the distributed PCA setting as well as the LRC-dPCA algorithm. For simplicity, we consider a balanced setting, in which we have M machines and the mth machine has n samples {xmi }ni=1 \u2282 Rp. Denote by N = Mn the total number of samples. Assume all samples are i.i.d. sub-Gaussian with mean 0 and covariance \u03a3.\nDefinition 4.1 (sub-Gaussian). We say a random vector x \u2208 Rp is sub-Gaussian with mean 0 and covariance \u03a3 if z = \u03a3\u22121/2x is sub-Gaussian with mean 0 and covariance Ip, i.e., there exists a constant \u03c3 > 0 such that the following inequality holds,\nE[e\u03bb\u3008u,z\u3009] \u2264 e\u03bb 2\u03c32 2 , \u2200\u03bb \u2208 R,\u2200u \u2208 Rp, \u2016u\u20162 = 1.\nRemark 4.2. Fan et al. (2019) and Bhaskara and Wijewardena (2019) use the following equivalent definition of a sub-Gaussian vector: x \u2208 Rd is sub-Gaussian with mean 0 and covariance \u03a3 if there exists a constantC > 0 such that \u2016u>x\u2016\u03c82 \u2264 C \u221a E(u>x)2,\u2200u \u2208 Rd. For more information on the equivalent definitions of sub-Gaussian vectors, one may refer to Vershynin (2012).\nGiven a positive integer K, the goal is to compute the top K eigenspace of \u03a3 using all data on M machines with small communication cost. We consider the LRC-dPCA algorithm proposed by Neuman et al. (2021), collected in Algorithm 1. Following this algorithm, we first compute\n\u03a3\u0302m = 1n \u2211n i=1 x m i x m> i on each local machine and then compute the top K eigenvectors V\u0302 m = (v\u0302m1 , . . . , v\u0302 m K ) \u2208 Op\u00d7K and eigenvalues \u039b\u0302m = diag(\u03bbm1 , . . . , \u03bbm) of \u03a3\u0302m. After communicating these local estimators V\u0302 m, \u039b\u0302m to a central server, we compute the Karcher mean A\u0303 of {V\u0302 m(\u039bm)2V\u0302 m>}Mm=1 on the manifold of restricted PSD matrices1. Finally, the top K eigenvectors V\u0303 \u2208 Op\u00d7K of A\u0303 is returned."
        },
        {
            "heading": "4.2 Theoretical analysis",
            "text": "A statistical analysis of the LRC-dPCA algorithm is missing in its original paper (Neuman et al., 2021). In this subsection, we will utilize our deterministic characterization of the Karcher mean on S\u2217(p,K), i.e., Theorem 3.4, to show that given sufficiently large sub-sample size, LRCdPCA matches the performance of the full sample PCA. Denote by V = (v1, . . . ,vK) \u2208 Op\u00d7K and \u039b = diag(\u03bb1, . . . , \u03bbK) the top K eigenvectors and eigenvalues of \u03a3, respectively. To ensure the uniqueness of span(V ), we assume \u2206K = \u03bbK(\u03a3) \u2212 \u03bbK+1(\u03a3) > 0. Write A = V \u039b2A> and assume the first K columns of A are linearly independent, i.e., A \u2208 S\u2217(p,K). When the subsample size n is sufficiently large, we will show that A\u0302m = V\u0302 m(\u039b\u0302m)2V\u0302 m> also belongs to S\u2217(p,K) with high probability. Here V\u0302 m and \u039b\u0302m denote the top K eigenvectors and eigenvalues of \u03a3\u0302m respectively. Denote by A\u0303 the Karcher mean of {A\u0302m}Mm=1 on S\u2217(p,K). We further denote by N , N\u0302m, N\u0303 the reduced Cholesky factors of A, A\u0302m, A\u0303. In addition, we define Q\u2217 \u2208 OK\u00d7K by the equalityN = V \u039bQ\u2217.\nIn the rest of this subsection, we will apply Theorem 3.4 to study the properties of A\u0303. First, we show that {A\u0302m}Mm=1 follow the general signal-plus-noise model (3.4).\nLemma 4.3. Let E\u0302m = \u03a3\u0302mV\u0302 mH\u0302mQ\u2217\u2212\u03a3V Q\u2217, where Hm = V\u0302 m>V and H\u0302m = sgn(Hm) def= U1U>2 with U1,U2 given by the singular value decomposition Hm = U1\u0393U > 2 ofH m. Then A\u0302m = (N + E\u0302m)(N + E\u0302m)>.\nLet us make several remarks on E\u0302m. It is well-known that\nH\u0302m = argmin O\u2208OK\u00d7K \u2016V\u0302 mO \u2212 V \u2016F\nand thus V\u0302 mH\u0302m is a good estimator of V (Chen et al., 2020). Furthermore, by Lemma H.2, when = maxm \u2016Em\u20162/\u2206K \u2264 1/10 with Em = \u03a3\u0302m\u2212\u03a3, V\u0302 mH\u0302m has the following first-order expansion around V ,\nV\u0302 mH\u0302m = V + g(EmV ) +OF( 2), (4.1)\n1Here we choose (\u039b\u0302m)2 rather than \u039b\u0302m only for technical reasons in the theoretical proofs.\nwhere g is a linear function defined in Lemma H.2. Substituting (4.1) into the definition of E\u0302m, we obtain the following linear expansion of E\u0302m in terms of Em,\nE\u0302m = EmV Q\u2217 + \u03a3g(EmV )Q\u2217 +OF ( 2). (4.2)\nSince g is linear in its argument, the leading term of 1 M \u2211M m=1 E\u0302 m is linear in 1M \u2211M m=1 Em. This enables an\nupper bound for \u2016 1M \u2211M m=1 E\u0302\nm\u2016F, provided by the following lemma.\nLemma 4.4 (Bounding \u2016M\u22121 \u2211M m=1 E\u0302\nm\u2016F). Suppose \u2206K > 0 and \u2016\u03a3\u20162 is bounded. Let Em = \u03a3\u0302m \u2212 \u03a3 and = maxm \u2016Em\u20162/\u2206K . When \u2264 1/10, the following bound\n\u2016 1 M M\u2211 m=1 E\u0302m\u2016F \u2264 C\u2016 1 M M\u2211 m=1 Em\u20162 +O( 2)\nholds for some constant C > 0.\nTo apply Theorem 3.4, we also need to upper bound the max norm 0 = maxm \u2016E\u0302m\u2016max. Again, this is based on the first-order expansion (4.2) of Em.\nLemma 4.5 (Bounding maxm \u2016E\u0302m\u2016max). Assume \u2206K > 0 and \u2016\u03a3\u20162 is bounded. When = maxm \u2016Em\u20162/\u2206K \u2264 1/10, we have with probability at least 1\u2212 2Me\u2212C1n\u03b421 \u2212 Me\u2212C2 \u221a \u03b42n/r that\nmax m \u2016E\u0302m\u2016max \u2264 C3\n\u221a log(p)\nn + \u03b41 + \u03b42,\nfor some constantsC1, C2, C3 > 0 and r = Tr(\u03a3)/\u03bb1(\u03a3). In addition, when n & log3(pM)r2, we have with probability at least 1\u2212 2p\u22121 that\nmax m \u2016E\u0302m\u2016max \u2264 C\n\u221a log(pM)\nn ,\nfor some constant C > 0. In Lemma 4.5, we show that \u2016E\u0302m\u2016max . \u221a log(p)/n with high probability when n & log3(p)r2. By (4.2) and Lemma H.1, we can show that \u2016E\u0302m\u2016F . \u221a p/n with high probability. The upper bound on \u2016E\u0302m\u2016max is thus smaller by a factor of \u221a p/ log(p) than the upper bound\non \u2016E\u0302m\u2016F . This implies that E\u0302m is delocalized across the entries. Moreover, Lemma 4.5 implies that when we apply Theorem 3.4 to the LRC-dPCA algorithm, the remainder termOmax( 20) is negligible compared to the leading term 1M \u2211M m=1 E\u0302 m \u2212 NfR( 1M \u2211M m=1 E\u0302\n1,m). This provides the last key ingredient to the following theorem, which gives an upper bound for \u2016N\u0303 \u2212N\u2016F . Here N\u0303 is the reduced Cholesky factor of the Karcher mean A\u0303.\nTheorem 4.6 (Bounding \u2016N\u0303 \u2212N\u2016F). Assume \u2206K > 0 and \u2016\u03a3\u20162 is bounded. Partition N = (R> B>)> such\nthat R \u2208 RK\u00d7K and B \u2208 R(p\u2212K)\u00d7K and assume \u2016R\u22121\u20162 \u2264 C for some constant C > 0. When = maxm \u2016Em\u20162/\u2206K \u2264 1/10 and 0 = maxm \u2016E\u0302m\u2016max is sufficiently small, the following bound\n\u2016N\u0303 \u2212N\u2016F \u2264 O ( \u2016 1 M M\u2211 m=1 Em\u20162 ) +O( 2) +O(\u221ap 20)\nholds. Define r = Tr(\u03a3)/\u03bb1(\u03a3), r\u03031 = (log2(pM)r) \u2228 (log(pM) \u221a p) and r\u03032 = \u221a p log4(pM)r2. Then we have with probability at least 1\u2212 4p\u22121 that\n\u2016N\u0303 \u2212N\u2016F \u2264 O ( log(p) \u221a r\u221a\nMn\n) +O ( r\u03031 n ) +O ( r\u03032 n2 ) .\nWhen n & r\u03032/r\u03031, the third term is negligible. When we further assume n & Mr\u030321/(log\n2(p)r), the upper bound reduces to\n\u2016N\u0303 \u2212N\u2016F \u2264 O ( log(p) \u221a r\u221a\nMn\n) .\nTheorem 4.6 shows that given sufficiently large local sample size, i.e., n & Mr\u030321/(log\n2(p)r), N\u0303 is as good as the full sample estimator of N in terms of the Frobenius norm. Moreover, \u2016N\u0303 \u2212 N\u2016F is of the same order as \u2016M\u22121 \u2211M m=1 Em\u20162 (see Lemma H.1). Note that the singular vectors of N are equal to V , the singular values of N are equal to \u039b, and LRC-dPCA uses the singular vectors of N\u0303 as an estimator of V . Then it follows from Wedin\u2019s sin(\u0398) theorem (Chen et al., 2020) that LRC-dPCA and full sample PCA share the same performance in eigenvector estimation.\nRemark 4.7. Similar to Lemma 4.5, we can show that \u2016V\u0302 mH\u0302m \u2212 V \u2016max . \u221a log(p)/n with high probability when n & log3(p)r2. Compared to the upper bound \u2016V\u0302 mH\u0302m \u2212 V \u2016F . \u221a p/n, the max norm bound again implies that the residual matrix V\u0302 mH\u0302m \u2212 V does not concentrate on a few coordinates. This has connections to the infinity norm eigenvector perturbation theory (Fan et al., 2018; Chen et al., 2020; Abbe et al., 2020; Damle and Sun, 2020; Cape et al., 2019). However, most applications in their works require incoherence conditions on the eigenvectors. In contrast, we do not require such conditions."
        },
        {
            "heading": "4.3 Manifold selection",
            "text": "As one may notice, A = V \u039b2V > may not belong to S\u2217(p,K), i.e., the first K columns of A may be linearly dependent. If we decompose A = FF> for some F \u2208 Rp\u00d7K and write F = (F>1 F>2 )> with F1 \u2208 RK\u00d7K and F2 \u2208 R(p\u2212K)\u00d7K . Then the smallest singular value \u03c3min(F1) of F1 may be zero or very small depending on p. In these cases, the condition \u2016R\u22121\u20162 \u2264 C for some\nAlgorithm 2: find index in LRC-dPCA Input: V ,\u039b, K Output: I \u2282 [p] Compute T = V \u039b and initialize I = [0, . . . , 0] \u2208 ZK . for k = 1 to K do\nfor i = 1 to p do Set Tk = T [c(I[1 : (k \u2212 1)], i), c(1 : k)]. Compute score[i] = \u03c3k(Tk). end for Set I[k] = argmaxi score[i].\nend for\nconstant C > 0 in Theorem 4.6 may not hold, and it is not suitable to directly use the manifold S\u2217(p,K) in the LRC-dPCA algorithm.\nTo fix this issue, we will utilize p!(p\u2212K)! cousins of the manifold S\u2217(p,K), or equivalently L\u2217(p,K). Let us introduce these cousin manifolds first. Recall that L\u2217(p,K) consists of N \u2208 Rp\u00d7K such that N1:K,1:K is a lower triangular matrix with positive diagonal elements. Here N1:K,1:K \u2208 RK\u00d7K represents the sub-matrix of N with row index [1, . . . ,K] and column index [1, . . . ,K]. Let I = [i1, . . . , iK ] be an ordered index set of size K. A cousin L\u2217I(p,K) of L\u2217(p,K) consists ofN \u2208 Rp\u00d7K such that NI,1:K is a lower triangular matrix with positive diagonal elements. Similarly, we define S\u2217I(p,K) as the set of all matrices in S(p,K) with the I-th rows linearly independent. Similar to the relationship between S\u2217(p,K) and L\u2217(p,K), for anyA \u2208 S\u2217I(p,K), there exists a unique element N \u2208 L\u2217I(p,K) such that A = NN>. Also, we define the Riemannian structure on S\u2217I(p,K) andL\u2217I(p,K) in a way similar to (2.1) and (2.2). In addition, all theory established in Section 3 and 4 can be rephrased in the language of S\u2217I(p,K). The only difference is that the row index set [1, . . . ,K] is replaced by I.\nNow we are in a position to solve the challenge raised at the beginning of this subsection. If A = V \u039b2V > does not belong to S\u2217(p,K), then we should choose a suitable ordered index set I rather than [1, . . . ,K], and then apply the LRC-dPCA algorithm on the manifold S\u2217I(p,K). Motivated by the condition \u2016R\u22121\u2016 \u2264 C in Theorem 4.6, we propose the find index method in Algorithm 2. Given V , \u039b, and K, the algorithm outputs an ordered index set I of size K. To avoid exhaustive search, the algorithm determines I in a sequential manner. In the kth step, we choose an index i \u2208 [p] such that the k-by-k matrix Tk = T [c(I[1 : (k\u22121)], i), c(1 : k)] has the largest \u03c3k(Tk) among all p candidates, where c(\u00b7) indicates the index set. In practice when V and \u039b is unknown, we can use V\u0302 1 and \u039b\u03021 to find a suitable index set and this index set is then shared by all machines."
        },
        {
            "heading": "5 Numerical Experiments",
            "text": "In this section, we present numerical experiments on three synthetic examples: averaging PSD matrices under the intrinsic model, the distributed PCA problems, and averaging PSD matrices under an extrinsic model."
        },
        {
            "heading": "5.1 Averaging PSD matrices",
            "text": "Our first experiment is to illustrate the concentration of the Karcher mean (2.3) under the intrinsic model (3.1), i.e., Theorem 3.1. We set K = 5, \u03c32 = 1 and let p vary across [100, 200, 300, 400] and let M range from 30 to 270 with an increment of 30. For each p, we generate a p \u00d7 p matrix \u03a3 with elements i.i.d. N (0, 1), and then take A = V \u039bV >, where V = (v1, . . . ,vK) and \u039b = (\u03bb1, . . . , \u03bbK) are the top K left singular vectors and singular values of \u03a3, respectively. Given M , we generate {Am}Mm=1 from the intrinsic model (3.1). Then the Karcher mean A\u0303 of {Am}Mm=1 is computed and the error\n\u2016A\u0303 \u2212 A\u2016F is reported in the top figure in Figure 1. As our theory shows, the estimation error turns smaller as M increases or p decreases.\nIn addition, we compare the Karcher mean A\u0303, referred to as LRC, with the usual Euclidean method A\u0303eu, which is defined as the best rank-K approximation of M\u22121 \u2211M m=1A\nm. We take p = 100 and repeat the above data generation processing. The errors \u2016A\u0303 \u2212 A\u2016F and \u2016A\u0303eu \u2212A\u2016F are reported in the bottom figure in Figure 1. As displayed in the figure, under the intrinsic model, the geometry-aware method, LRC, outperforms the Euclidean method. This justifies the intuition that for models with specific geometric structures, it is better to take that geometric information into account."
        },
        {
            "heading": "5.2 Distributed PCA",
            "text": "Our second experiment studies the Karcher mean under a general signal-plus-noise model. Specifically, we consider the distributed PCA problems and numerically verify Theorem 4.6, which shows that LRC-dPCA achieves the same\nperformance as full sample PCA (fPCA). In our setting, p = 100, K = 5, the population covariance \u03a3 is generated by \u03a3 = V V > + 0.3Ip, where V \u2208 Rp\u00d7K with elements i.i.d. N (0, 1). We first fix the number of machines M = 50 and let the sub-sample size n vary across [500, 1000, . . . , 2500]. On the m-th machine, we generate n i.i.d. samples {xmi }ni=1 from N (0,\u03a3) and compute the local sample covariance matrix \u03a3\u0302m = \u2211n i=1 x m i x m> i /n. Then we apply four methods , namely fPCA, LRC-dPCA, dPCA-Fan (Fan et al., 2019), and dPCA-BW (Bhaskara and Wijewardena, 2019), to compute the top K eigenvectors of \u03a3. Let V\u0302 \u2208 Op\u00d7K be the estimated top K eigenvectors. The error is defined as \u2016V\u0302 V\u0302 >\u2212V (V >V )\u22121V >\u2016F , which is the distance between the population projection matrix V (V >V )\u22121V > and the estimated projection matrix V\u0302 V\u0302 >. For each n and each method, the experiment is repeated 100 times and the average of error is recorded. The top figure in Figure 2 displays the relationship between log(error) and log(n) for all methods. It turns out that all methods share similar performance and there is a linear relationship between log(error) and log(n) with slope\u22121/2, which verifies the relationship error \u223c n\u22121/2. Next, we fix the sub-sample size n = 1000 and let M vary across [50, 100, . . . , 200] and repeat the above procedures. The relationship between log(error) and log(M) is reported in the bottom figure of Figure 2. As it displayed, all four methods are almost the same and there is also a linear relationship between log(error) and log(M) with slope \u22121/2, which indicates error \u223c M\u22121/2. Since there is no specific geometric information in the setting, it is expected that LRC-dPCA only matches (rather than surpasses) the performance of the state-of-the-art methods, dPCA-Fan, dPCA-BW, and the optimal method, fPCA."
        },
        {
            "heading": "5.3 Averaging PSD matrices (extrinsic)",
            "text": "Our third experiment considers another signal-plus-noise model, which adds extrinsic noises to the intrinsic model. Specifically, we set p = 100, K = 5, and we generate a p \u00d7 p matrix \u03a3 with elements i.i.d. N (0, 1), and then take A = V \u039bV >, where V = (v1, . . . ,vK) and \u039b = (\u03bb1, . . . , \u03bbK) are the top K left singular vectors and singular values of \u03a3, respectively. Given M and \u03c32, we generate {Am}Mm=1 from the intrinsic model (3.1). Then we add extrinsic noises to Am as follows. For each m, we generate {xmi }2000i=1 i.i.d. from N (0,Am + 0.01Ip), compute \u03a3\u0302m = \u22112000 i=1 x m i x m> i /2000, and setA\n\u2032m as the best rank-K approximation of \u03a3\u0302m. We compute the Karcher mean A\u0303 of {A\u2032m}Mm=1, which is referred to as LRC, and report the error \u2016A\u0303 \u2212A\u2016F. In contrast, we also apply the Euclidean method, which computes the best rank-K approximation A\u0303eu of \u2211M m=1A\n\u2032m/M , and report the error \u2016A\u0303eu \u2212A\u2016F. First, we set \u03c32 = 0.5 and let M vary across [100, 200, . . . , 1000]. The errors of both methods are displayed in the top figure of Figure 3. As shown in the fig-\nure, the geometry-aware method, LRC, still outperforms the Euclidean method even if extrinsic noises are added to the intrinsic model. Next, we fix M = 400 and let \u03c32 range from [0, 0.1 . . . , 0.7]. The errors of both methods are shown in the bottom figure of Figure 3. Recall that \u03c32 denotes the strength of intrinsic noises. The bottom figure indicates that when the intrinsic noises are small, then the geometry-aware method and the Euclidean method are comparable, but when the intrinsic noises becomes large, the geometry-aware method tends to outperform the Euclidean method. Overall, this experiment shows that, in a general signal-plus-noise model, if there exist large intrinsic noises, then it is better to utilize the geometry-aware method."
        },
        {
            "heading": "6 Concluding Remarks",
            "text": "This paper considers the geometry of restricted PSD matrices proposed by Neuman et al. (2021). In particular, we provide a non-asymptotic statistical analysis of the Karcher mean of restricted PSD matrices under an intrinsic model.\nMoreover, for general signal-plus-noise models, we establish a deterministic error bound concerning the Karcher mean. This is based on a linear perturbation expansion of the QR decomposition, which may be of independent interest. As an application, we use the deterministic error analysis of the Karcher mean to prove that the distributed PCA algorithm, LRC-dPCA, achieves the same performance as the full sample PCA. Motivated by the established theory, we propose a manifold selection procedure for the LRCdPCA algorithm. Finally, we carry out three synthetic numerical experiments to verify our theories. One observation in the experiment is that if data model has certain geometric structure, then it is better to utilize the geometryaware method.\nSeveral interesting topics are worth of future studies. In manifold-valued data analysis (Patrangenaru and Ellingson, 2016), it remains to determine which statistical model is more suitable for the given data. For example, the highly anisotropic diffusion tensor images are modelled as PSD matrices (Bonnabel et al., 2013), so it is interesting to investigate the performances of the proposed intrinsic model for such data. Second, it is interesting to extend our study to regression, classification, and clustering problems."
        },
        {
            "heading": "A Proof of Theorem 3.1",
            "text": "Proof of Theorem 3.1. Recall that the Karcher mean A\u0303 of {Am}Mm=1 under the intrinsic model is given by (3.1). First, we give an upper bound on the Frobenius norm of 1M \u2211M m=1E m. By the intrinsic model, we know 1M \u2211M m=1E\nm is a mock lower triangular matrix with lower triangular elements i.i.d.N (0, \u03c32/M). Therefore, by the concentration of \u03c72 ((2.19) in Wainwright (2019)), for all t \u2208 (0, 1), we have\n\u2016 1 M M\u2211 m=1 Em\u20162F \u2264 pK\u03c32 M (1 + t), (A.1)\nwith probability at least 1\u2212 e\u2212pKt2/8. In a similar spirit, using union bound, we have for t \u2208 (0, 1),\nmax i=1,...,K | 1 M M\u2211 m=1 Emii |2 \u2264 \u03c32 M (1 + t) (A.2)\nwith probability at least 1 \u2212 Ke\u2212t2/8. Here Emii is the (i, i)-th element of Em. Thus with high probability, maxi=1,...,K | 1M \u2211M m=1E m ii |2 \u2264 1/2 and\n| exp( 1 M M\u2211 m=1 Emii )\u2212 1| \u2264 2| 1 M M\u2211 m=1 Emii |, (A.3)\nwhere we use the inequality | exp(x) \u2212 1| \u2264 2|x| for x \u2264 1/2. Denote by N the reduced Cholesky factor of A. Then it holds that \u2016N\u20162 = \u2016A\u20161/22 \u2264 C1/2 for some constant C > 0. Furthermore, by (3.1), we have for some constants c1, c2 > 0 that\n\u2016A\u0303\u2212A\u2016F \u2264 \u221a c2pK\u03c32\nM , (A.4)\nwith probability at least 1\u2212 e\u2212c1pK ."
        },
        {
            "heading": "B Proof of Lemma 3.3",
            "text": "Proof of Lemma 3.3. The proof of this lemma is split up into three steps. First, we assume Q = IK and show that qQ has the form of IK + qP + Omax( 20), where qP \u2208 RK\u00d7K is a skew-symmetric matrix of order Omax( 0). Second, by taking upper triangular off-diagonal elements of (R + E) qQ> as zero, we derive a closed-form expression of qP (up to a higher-order term). Motivated by this closed-form expression, we define a function fR : RK\u00d7K 7\u2192 RK\u00d7K satisfying several desired conditions. For example, we have qQ = IK + fR(E) +Omax( 20) and fR is linear in its argument. Third, we extend the results to the general case whenQ \u2208 OK\u00d7K may differ from IK .\nStep 1. When 0 = \u2016E\u2016max is sufficiently small, the matrix R + E is still non-singular and by QR decomposition there exists a unique orthogonal matrix qQ \u2208 OK\u00d7K such that qR = (R + E) qQ> is a lower triangular matrix with positive diagonal elements. In this step, we will show that qQ has a form of IK + qP + Omax( 20), where qP \u2208 RK\u00d7K is a skew-symmetric matrix of order Omax( 0). To that end, we construct qQ as a product of K(K \u2212 1)/2 rotation matrices { qQij , 1 \u2264 i < j \u2264 K}, which set the upper triangular off-diagonal elements as zero in a sequential fashion. In specific, we arrange these K(K \u2212 1)/2 rotation matrices in a prescribed order, i.e., {(1, 2), . . . , (1,K), (2, 3), . . . , (K \u2212 1,K)}. In this way, we may relabel { qQij , 1 \u2264 i < j \u2264 K} as { qQ(s), 1 \u2264 s \u2264 K(K \u2212 1)/2} and write qQ = qQ(K(K\u22121)/2) \u00b7 \u00b7 \u00b7 qQ(1). In the remainder of this proof, we will use s(i, j) to represent the s-index of the (i, j)th rotation matrix qQij .\nFor each (i, j), we set the rotation matrix qQij as\nqQijii = qQijjj = cos(\u03b8 ij), qQijij = \u2212 qQ ij ji = sin(\u03b8 ij), qQijkk = 1, \u2200k 6= i, j, qQ ij kl = 0, otherwise,\nwhere \u03b8ij is chosen in a sequential fashion such that the (i, j)th element of qR(s(i,j)) := (R + E) qQ(1) > \u00b7 \u00b7 \u00b7 qQ(s(i,j))> is zero and the diagonal elements of qR(s(i,j)) keep positive. Note that qQij is by definition an orthogonal matrix. A simple calculation gives that \u03b8ij = \u03b8(s(i,j)) = arctan( qR(s(i,j)\u22121)ij / qR (s(i,j)\u22121) ii ).\nNext, we show that \u03b8ij is a small quantity of order O( 0) via an deductive argument. First, when 0 = \u2016E\u2016max is sufficiently small, \u03b8(1) = \u03b812 = arctan(E12/(R11 +E11)) is a small quantity of orderO( 0). Thus, by definition of qQ12, we have qQ(1) = qQ12 = IK +Omax( 0) and\nqR(1) = (R+E)(IK + qQ (1)> \u2212 IK) = R+E +R( qQ(1) > \u2212 IK) +Omax( 20).\nNote that the error matrix E(1) := qR(1) \u2212 R is again of order Omax( 0). This implies that \u03b8(2) = \u03b813 = arctan(E\n(1) 13 /(R11 + E (1) 11 )) is also a small quantity of order O( 0). Applying this deductive argument K(K \u2212 1)/2\ntimes, we conclude that all \u03b8(s), 1 \u2264 s \u2264 K(K \u2212 1)/2, are small quantities of order O( 0).\nNow we are able to show that qQ has a form of IK+ qP+Omax( 20), where qP \u2208 RK\u00d7K is a skew-symmetric matrix of order Omax( 0). Since \u03b8ij is of order O( 0), by Taylor expansion, we have sin(\u03b8ij) = \u03b8ij +O( 30) and 1 \u2212 cos(\u03b8ij) = O( 20). Thus, we can rewrite qQij as\nqQij = IK + qP ij +Omax( 20),\nwhere qP ijij = \u2212 qP ij ij = \u03b8 ij and qP ijkl = 0 otherwise. Since qP ij is of order Omax( 0), we have\nqQ = qQ(K(K\u22121)/2) \u00b7 \u00b7 \u00b7 qQ(1)\n= (IK + qP (K(K\u22121)/2) +Omax( 20)) \u00b7 \u00b7 \u00b7 (IK + qP (1) +Omax( 20))\n= IK + K(K\u22121)/2\u2211 s=1 qP (s) +Omax( 20)\n= IK + qP +Omax( 20), (B.1)\nwhere qP (s(i,j)) = qP ij and qP = \u2211K(K\u22121)/2 s=1\nqP (s). Since qP (s) is skew-symmetric and of order Omax( 0) for all s, qP is also a skew-symmetric matrix of order Omax( 0), which concludes the proof of step 1.\nStep 2. Now we are ready to derive a closed-form expression of qP (maybe up to a higher-order term) by taking upper triangular off-diagonal elements of (R+E) qQ> as zero. Substituting (B.1) into (R+E) qQ>, we obtain\n(R+E) qQ> = R+E +R qP> +Omax( 20)\n= R+E \u2212R qP +Omax( 20), (B.2)\nwhere the second equality follows from the skew-symmetry of qP . Since R is a lower triangular matrix with positive diagonal elements, R is invertible and R\u22121 is also a lower triangular matrix. As a result, the matrix R\u22121(R +E) qQ> is also a lower triangular matrix. Multiplying LHS and RHS of (B.2) byR\u22121 simultaneously, we obtain\nR\u22121(R+E) qQ> = IK +R \u22121E \u2212 qP +Omax( 20). (B.3)\nFor convenience, we define a function U(\u00b7) : RK\u00d7K 7\u2192 RK\u00d7K ,P 7\u2192 U(P ), where U(P ) takes the upper triangular off-diagonal elements of P , i.e.,\nU(P )ij = Pij , i < j, U(P )ij = 0, otherwise.\nSinceR\u22121(R+E) qQ> is a lower triangular matrix, we have by (B.3) that\nU( qP ) = U(R\u22121E) +Omax( 20).\nSince qP is skew-symmetric, we get the following closed-form solution of qP (up to a higher-order term),\nqP = U(R\u22121E)\u2212 (U(R\u22121E))> +Omax( 20).\nMotivated by the linear expansion of qP , we define the following function,\nfR : RK\u00d7K 7\u2192 RK\u00d7K , E 7\u2192 fR(E) := U(R\u22121E)\u2212 (U(R\u22121E))>.\nNote that fR is linear in the sense that fR(aE + bF ) = afR(E) + bfR(F ) for all a, b \u2208 R and E,F \u2208 RK\u00d7K . Also, the image fR(E) is a skew-symmetric matrix, i.e., (fR(E))> = \u2212fR(E). Moreover, we have \u2016fR(E)\u2016F \u2264\u221a\n2\u2016R\u22121\u20162\u2016E\u2016F. By (B.1), we can rewrite qQ as follows,\nqQ = IK + fR(E) +Omax( 20). (B.4)\nMoreover, by definition of qR, we have\nqR = (R+E) qQ> = R+E \u2212RfR(E) +Omax( 20).\nStep 3. In general, when Q \u2208 OK\u00d7K may differ from IK , we can transform the QR decomposition qR qQ = RQ + E suitably and apply the results in the previous two steps to prove the lemma. In specific, we have\nqR qQQ> = R+EQ>.\nWhen 0 = \u2016E\u2016max is sufficiently small, \u2016EQ>\u2016max \u2264 \u221a K 0 can also be sufficiently small. In addition, qQQ> is still an orthogonal matrix that appears in the QR decomposition ofR+EQ>. Therefore, by (B.4), we have\nqQQ> = IK + fR(EQ >) +Omax(K 20).\nBy multiplying both LHS and RHS of this equation byQ, we obtain that\nqQ = Q+ fR(EQ >)Q+Omax(K3/2 20).\nIn addition, by definition of qR, we have\nqR = (RQ+E) qQ>\n= (R+EQ>)Q qQ>\n= R+EQ> \u2212RfR(EQ>) +Omax( 20),\nwhich concludes the proof."
        },
        {
            "heading": "C Proof of Theorem 3.4",
            "text": "Proof of Theorem 3.4. First, we use Lemma 3.3 to give a first-order perturbation expansion for the reduced Cholesky factor Nm ofAm = (N +Em)(N +Em)>. DefineQm \u2208 OK\u00d7K as an orthogonal matrix such thatNm = (N +Em)Qm>, or equivalently, (R + E1,m)Qm> is a lower triangular matrix with positive diagonal elements. By Lemma 3.3, when \u2016E1,m\u2016max \u2264 0 is sufficiently small, we have\nQm = IK + fR(E 1,m) +Omax( 20),\nwhere fR is defined in Lemma 3.3. By definition ofQm, we have\nNm = (N +Em)Qm> = N +Em \u2212NfR(E1,m) +Omax( 20),\nwhere we use the property fR(E1,m)> = \u2212fR(E1,m). Using this linear perturbation expansion, we are now able to characterize the Karcher mean A\u0303 = N\u0303N\u0303> (or N\u0303 ) of {Am}Mm=1 (or {Nm}Mm=1) on the manifold S\u2217(p,K) (orL\u2217(p,K)). By the LRC algorithm, i.e., (2.3), we have N\u0303 is equal to 1M \u2211M m=1N\nm except that the diagonal elements of N\u0303 are given by\nN\u0303ii = ( M\u220f m=1 Nmii ) 1/M , \u22001 \u2264 i \u2264 K.\nHowever, when 0 is sufficiently small, |Nmii \u2212Nii| is of order O( 0) and thus\nN\u0303ii \u2212 1\nM M\u2211 m=1 Nii = O( 20), \u22001 \u2264 i \u2264 K.\nTherefore, we have\nN\u0303 = 1\nM M\u2211 m=1 Nm +Omax( 20)\n= N + 1\nM M\u2211 m=1 ( Em \u2212NfR(E1,m) ) +Omax( 20)\n= N + 1\nM M\u2211 m=1 Em \u2212NfR( 1 M M\u2211 m=1 E1,m) +Omax( 20),\nwhere the last equality follows from the linear property of fR(\u00b7)."
        },
        {
            "heading": "D Proof of Lemma 4.3",
            "text": "Proof of Lemma 4.3. Since V ,\u039b denote the top K eigenvectors and eigenvalues of \u03a3, respectively, we have \u03a3V = V \u039b and thus N = V \u039bQ\u2217 = \u03a3V Q\u2217. Similarly, we have \u03a3\u0302mV\u0302 m = V\u0302 m\u039b\u0302m. Since H\u0302m and Q\u2217 are both orthogonal matrices, we have\n(N + E\u0302m)(N + E\u0302m)> = (V\u0302 m\u039b\u0302mH\u0302mQ\u2217)(V\u0302 m\u039b\u0302mH\u0302mQ\u2217)>\n= (V\u0302 m\u039b\u0302m)(V\u0302 m\u039b\u0302m)>\n= A\u0302m,\nwhich concludes our proof."
        },
        {
            "heading": "E Proof of Lemma 4.4",
            "text": "Proof of Lemma 4.4. The proof of this lemma is based on a first-order expansion of E\u0302m. Define Em = \u03a3\u0302m \u2212 \u03a3 and = maxm \u2016Em\u20162/\u2206K . When \u2264 1/10, by Lemma H.2, we have\n\u2016V\u0302 mH\u0302m \u2212 V \u2212 g(EmV )\u2016F \u2264 9 \u221a K 2,\nwhere\ng : Rp\u00d7K 7\u2192 Rp\u00d7K , (w1, . . . ,wK) 7\u2192 (\u2212G1w1, . . . ,\u2212GKwK),\nwith Gj = \u2211 i>K(\u03bbi \u2212 \u03bbj)\u22121viv>i for j \u2208 [K] and \u03bbi/vi being the ith eigenvalue/eigenvector of \u03a3. By definition of E\u0302m, we have\nE\u0302m = ((\u03a3 + Em)(V + (V\u0302 mH\u0302m \u2212 V ))\u2212\u03a3V )Q\u2217\n= EmV Q\u2217 + \u03a3(V\u0302 mH\u0302m \u2212 V )Q\u2217 + Em(V\u0302 mH\u0302m \u2212 V )Q\u2217\n= EmV Q\u2217 + \u03a3g(EmV )Q\u2217 +OF( 2).\nSince vec \u25e6 g \u25e6 vec\u22121 is a linear mapping from RpK to RpK , where vec : Rp\u00d7K 7\u2192 RpK is the vectorization mapping, the average of E\u0302m can be expressed as\n1\nM M\u2211 m=1 E\u0302m = 1 M M\u2211 m=1 EmV Q\u2217 + \u03a3g( 1 M M\u2211 m=1 EmV )Q\u2217 +OF( 2).\nThus, by the triangular inequality, we have\n\u2016 1 M M\u2211 m=1 E\u0302m\u2016F \u2264 \u2016 1 M M\u2211 m=1 EmV Q\u2217\u2016F + \u2016\u03a3g( 1 M M\u2211 m=1 EmV )Q\u2217\u2016F +O( 2).\nSince \u2016Q\u2217\u20162 = 1 and \u2016V \u2016F = \u221a K, we have\n\u2016 1 M M\u2211 m=1 EmV Q\u2217\u2016F \u2264 \u221a K\u2016 1 M M\u2211 m=1 Em\u20162.\nIn addition, since \u2016\u03a3Gj\u20162 \u2264 \u2206\u22121K \u03bbK for all j \u2208 [K], we have\n\u2016\u03a3g( 1 M M\u2211 m=1 EmV )Q\u2217\u2016F \u2264 \u2206\u22121K \u03bbK\u2016 1 M M\u2211 m=1 EmV \u2016F\n\u2264 \u2206\u22121K \u03bbK \u221a K\u2016 1\nM M\u2211 m=1 Em\u20162.\nThus, we have\n\u2016 1 M M\u2211 m=1 Em\u2016F \u2264 C\u2016 1 M M\u2211 m=1 Em\u20162 +O( 2)\nfor some constant C > 0."
        },
        {
            "heading": "F Proof of Lemma 4.5",
            "text": "Proof of Lemma 4.5. Similar to Lemma 4.4, the proof of this lemma is also based on the first-order expansion of E\u0302m. Define Em = \u03a3\u0302m \u2212\u03a3 and = maxm \u2016Em\u20162/\u2206K . When \u2264 1/10, by Lemma H.2, we have\n\u2016V\u0302 mH\u0302m \u2212 V \u2212 g(EmV )\u2016F \u2264 9 \u221a K 2,\nwhere\ng : Rp\u00d7K 7\u2192 Rp\u00d7K , (w1, . . . ,wK) 7\u2192 (\u2212G1w1, . . . ,\u2212GKwK),\nwith Gj = \u2211 i>K(\u03bbi \u2212 \u03bbj)\u22121viv>i for j \u2208 [K] and \u03bbi/vi being the ith eigenvalue/eigenvector of \u03a3. By definition of E\u0302m, we have\nE\u0302m = EmV Q\u2217 + \u03a3g(EmV )Q\u2217 +OF( 2).\nBy the triangular inequality and the fact that \u2016 \u00b7 \u2016max \u2264 \u2016 \u00b7 \u2016F, we have\n\u2016E\u0302m\u2016max \u2264 \u2016EmV Q\u2217\u2016max + \u2016\u03a3g(EmV )Q\u2217\u2016max +O( 2)\n\u2264 \u221a K\u2016EmV \u2016max + \u221a K\u2016\u03a3g(EmV )\u2016max +O( 2),\nwhere the second inequality follows from the inequality \u2016\u00b7Q\u2217\u2016max \u2264 \u221a K\u2016\u00b7\u2016max. Thus, to bound \u2016E\u0302m\u2016max, it suffices to bound \u2016EmV \u2016max, \u2016\u03a3g(EmV )\u2016max, and 2 = (maxm \u2016Em\u20162/\u2206K)2 separately. To give an upper bound on the first two terms, we will need Proposition 1 in Pilanci and Wainwright (2015), which is presented below for reader\u2019s convenience.\nProposition F.1 (Proposition 1 in Pilanci and Wainwright (2015)). Let {zi}ni=1 \u2282 Rp be i.i.d. samples generated from a zero-mean sub-Gaussian distribution with Cov(zi) = Ip. Then there exist some universal constants C1, C2 > 0 such that for any subset Y \u2282 Sp\u22121, we have with probability at least 1\u2212 e\u2212C2n\u03b42 ,\nsup \u03b7\u2208Y \u2223\u2223\u2223\u2223\u03b7>(Z>Zn \u2212 Ip ) \u03b7 \u2223\u2223\u2223\u2223 \u2264 C1W(Y)\u221an + \u03b4,\nwhere Z> = (z1, . . . ,zn) \u2208 Rp\u00d7n and W(Y) is the Gaussian width of the subset Y . Specifically, W(Y) is defined by\nW(Y) = E[sup \u03b7\u2208Y |\u3008h,\u03b7\u3009|],\nwhere the expectation is taken on h \u2208 Rp, which is a standard normal random vector.\n1: bound \u2016EmV \u2016max. Denote by el \u2208 Rp the basis vector with value 1 at the lth entry and 0 at other entries. Then the max norm has the following expression,\n\u2016EmV \u2016max = max l\u2208[p],k\u2208[K] |e>l Emvk|.\nLet zmi = \u03a3 \u22121/2xmi and Z m = (zm1 , . . . ,z m n ) >. Since {xmi }ni=1 are i.i.d. sub-Gaussian with mean 0 and covariance \u03a3, {zmi }ni=1 are i.i.d. sub-Gaussian with mean 0 and covariance Ip. By definition of Em, we have\n|e>l Emvk| = |(\u03a31/2el)>( Zm>Zm\nn \u2212 Ip)\u03a31/2vk|.\nBy the polarization equality, we have\n(\u03a31/2el) >( Zm>Zm\nn \u2212 Ip)\u03a31/2vk =\n1\n2\n{ (\u03a31/2el + \u03a3 1/2vk) >( Zm>Zm\nn \u2212 Ip)(\u03a31/2el + \u03a31/2vk)\n\u2212 (\u03a31/2el)>( Zm>Zm\nn \u2212 Ip)(\u03a31/2el)\n\u2212 (\u03a31/2vk)>( Zm>Zm\nn \u2212 Ip)(\u03a31/2vk)\n} .\nThen by the triangular inequality, we have\n\u2016EmV \u2016max \u2264 max l\u2208[p],k\u2208[K]\n1\n2\n{ |(\u03a31/2el + \u03a31/2vk)>( Zm>Zm\nn \u2212 Ip)(\u03a31/2el + \u03a31/2vk)|\n+ |(\u03a31/2el)>( Zm>Zm\nn \u2212 Ip)(\u03a31/2el)|\n+ |(\u03a31/2vk)>( Zm>Zm\nn \u2212 Ip)(\u03a31/2vk)|\n} .\nSince \u2016el\u20162 = \u2016vk\u20162 = 1, we have\n\u2016\u03a31/2(el + vk)\u20162 \u2264 2\u2016\u03a31/2\u20162 = 2\u2016\u03a3\u20161/22 , \u2016\u03a31/2el\u20162 \u2264 \u2016\u03a3\u2016 1/2 2 , \u2016\u03a31/2vk\u20162 \u2264 \u2016\u03a3\u2016 1/2 2 ,\nfor all l \u2208 [p] and k \u2208 [K]. Define Y1 \u2282 Sp\u22121 as follows,\nY1 = { \u03a31/2(el + vk)\n\u2016\u03a31/2(el + vk)\u20162 } l\u2208[p],k\u2208[K] \u222a { \u03a31/2el \u2016\u03a31/2el\u20162 } l\u2208[p] \u222a { \u03a31/2vk \u2016\u03a31/2vk\u20162 } k\u2208[K] ,\nthen we have\n\u2016EmV \u2016max \u2264 C1 \u00b7 sup \u03b7\u2208Y1 \u2223\u2223\u2223\u2223\u03b7>(Zm>Zmn \u2212 Ip ) \u03b7 \u2223\u2223\u2223\u2223 , where C1 > 0 is a constant dependent on \u2016\u03a3\u20162. We remark here that in the proof notations C,C1, C2 represent some universal constants, which may vary according to the context. By Proposition F.1, there exist some universal constants C1, C2 > 0 such that the following inequality holds\n\u2016EmV \u2016max \u2264 C1 W(Y1)\u221a\nn + \u03b4,\nwith probability at least 1\u2212 e\u2212C2n\u03b42 . Since Y1 is a finite set with cardinality |Y1| \u2264 Cp for some constant C > 0, by the maximal inequality (Mohri et al., 2018), the following inquality\nW(Y1) \u2264 C1 \u221a log(p)\nholds for some constant C1 > 0. Thus with probability at least 1\u2212 e\u2212C2n\u03b4 2 , we have\n\u2016EmV \u2016max \u2264 C1\n\u221a log(p)\nn + \u03b4.\n2: bound \u2016\u03a3g(EmV )\u2016max. The proof of this step is similar to that of step one. By definition of g, we have\ng(EmV ) = (\u2212G1Emv1, . . . ,\u2212GKEmvK).\nThen we may write the max norm as\n\u2016\u03a3g(EmV )\u2016max = max l\u2208[p],k\u2208[K] |e>l \u03a3GkEmvk| = max l\u2208[p],k\u2208[K]\n|e>l \u03a3Gk\u03a31/2( Zm>Zm\nn \u2212 Ip)\u03a31/2vk|.\nSimilar to step one, by the polarization equality, the triangular inequality, and the factGk = G>k , we have\n\u2016g(EmV )\u2016max \u2264 max l\u2208[p],k\u2208[K]\n1\n2\n{ |(\u03a31/2Gk\u03a3el + \u03a31/2vk)>( Zm>Zm\nn \u2212 Ip)(\u03a31/2Gk\u03a3el + \u03a31/2vk)|\n+ |(\u03a31/2Gk\u03a3el)>( Zm>Zm\nn \u2212 Ip)(\u03a31/2Gk\u03a3el)|\n+ |(\u03a31/2vk)>( Zm>Zm\nn \u2212 Ip)(\u03a31/2vk)|\n} .\nBy definition ofGk, we have\n\u03a31/2Gk\u03a3 = \u2211 i>K (\u03bbi \u2212 \u03bbk)\u22121\u03bb3/2i viv > i ,\nand thus \u2016\u03a31/2Gk\u03a3\u20162 \u2264 \u03bb3/2K \u2206 \u22121 K . Since \u2016el\u20162 = \u2016vk\u20162 = 1, we have\n\u2016\u03a31/2Gk\u03a3el + \u03a31/2vk\u20162 \u2264 \u2016\u03a31/2Gk\u03a3\u20162 + \u2016\u03a31/2\u20162 \u2264 \u03bb3/2K \u2206 \u22121 K + \u03bb 1/2 1 , \u2016\u03a31/2Gk\u03a3el\u20162 \u2264 \u03bb 3/2 K \u2206 \u22121 K .\nDefine the following set Y2 \u2282 Sp\u22121,\nY2 = { \u03a31/2Gk\u03a3el + \u03a31/2vk \u2016\u03a31/2Gk\u03a3el + \u03a31/2vk\u20162 } l\u2208[p],k\u2208[K] \u222a { \u03a31/2Gk\u03a3el \u2016\u03a31/2Gk\u03a3el\u20162 } l\u2208[p],k\u2208[K] \u222a { \u03a31/2vk \u2016\u03a31/2vk\u20162 } k\u2208[K] .\nThen we have\n\u2016\u03a3g(EmV )\u2016max \u2264 C1 \u00b7 sup \u03b7\u2208Y2 \u2223\u2223\u2223\u2223\u03b7>(Zm>Zmn \u2212 Ip ) \u03b7 \u2223\u2223\u2223\u2223 for some constant C1 > 0 dependent on \u2016\u03a3\u20162 and \u2206K . Again by Proposition F.1, we have with probability at least 1\u2212 e\u2212C2n\u03b42 that\n\u2016\u03a3g(EmV )\u2016max \u2264 C1 W(Y2)\u221a\nn + \u03b4\nfor some universal constants C1, C2 > 0. Since Y2 is a finite set with cardinality |Y2| \u2264 Cp for some constant C > 0, by the maximal inequality (Mohri et al., 2018), we have\nW(Y2) \u2264 C1 \u221a log(p)\nfor some constant C1 > 0. Thus with probability at least 1\u2212 e\u2212C2n\u03b4 2 , we have\n\u2016\u03a3g(EmV )\u2016max \u2264 C1\n\u221a log(p)\nn + \u03b4.\n3: bound 2. We will use the tail bound of \u2016Em\u20162 in Lemma H.1 to bound 2. By Lemma H.1, we have with probability at least 1\u2212 e\u2212C \u221a \u03b4n r that\n\u2016Em\u201622 \u2264 \u03b4\nfor some constant C > 0 and r = Tr(\u03a3)/\u03bb1 \u2264 p. Then by union bound, we have with probability at least 1\u2212Me\u2212C \u221a \u03b4n r that\n2 = max m \u2016Em\u201622/\u22062K \u2264 \u03b4,\nfor some constant C > 0.\nLast: bound maxm \u2016E\u0302m\u2016max. We will combine the results from 1 to 3 and apply a union bound to obtain the upper bound on maxm \u2016E\u0302m\u2016max. In specific, by union bound, we have with probability at least 1\u2212 2Me\u2212C1n\u03b4 2 1 \u2212Me\u2212C2 \u221a \u03b42n/r that\nmax m \u2016E\u0302m\u2016max \u2264 C3\n\u221a log(p)\nn + \u03b41 + \u03b42,\nfor some constants C1, C2, C3 > 0. Take \u03b41 = \u221a\nlog(2Mp) C1n and \u03b42 = log(Mp)2r C22n , then we have with probability at least\n1\u2212 2p\u22121 that,\nmax m \u2016E\u0302m\u2016max \u2264 C1\n\u221a log(pM)\nn + C2\nlog2(pM)r\nn ,\nfor some constants C1, C2 > 0. When n & log3(pM)r2, with probability at least 1\u2212 2p\u22121 the following bound\nmax m \u2016E\u0302m\u2016max \u2264 C\n\u221a log(pM)\nn\nholds for some constant C > 0."
        },
        {
            "heading": "G Proof of Theorem 4.6",
            "text": "Proof of Theorem 4.6. We will combine the results in Theorem 3.4, Lemma 4.3, Lemma 4.4, and Lemma 4.5 to prove this theorem. Recall that E\u0302m = \u03a3\u0302mV\u0302 mH\u0302mQ\u2217 \u2212 \u03a3V Q\u2217, Em = \u03a3\u0302m \u2212 \u03a3, = maxm \u2016Em\u20162/\u2206K , and 0 = maxm \u2016E\u0302m\u2016max. In addition, we partition N = (R> B>)> and E\u0302m = (E\u03021,m > E\u03022,m > )> such that R, E\u03021,m \u2208 RK\u00d7K and B, E\u03022,m \u2208 R(p\u2212K)\u00d7K . By Lemma 4.3, we have A\u0302m = (N + E\u0302m)(N + E\u0302m)> for all m \u2208 [M ], where N is the reduced Cholesky factor of A = V \u039b2V >. By Lemma 4.5, 0 = maxm \u2016E\u0302m\u2016max is sufficiently small with high probability when n is sufficiently large. Thus, we can apply Theorem 3.4 to the LRC-dPCA algorithm to obtain the desired result. In specific, by Theorem 3.4, we have\nN\u0303 = N + 1\nM M\u2211 m=1 E\u0302m \u2212NfR( 1 M M\u2211 m=1 E\u03021,m) +Omax( 20),\nwhere fR is defined in Lemma 3.3. By the triangular inequality, we have\n\u2016N\u0303 \u2212N\u2016F \u2264 \u2016 1\nM M\u2211 m=1 E\u0302m\u2016F + \u2016NfR( 1 M M\u2211 m=1 E\u03021,m)\u2016F +O( \u221a pK 20)\n\u2264 \u2016 1 M M\u2211 m=1 E\u0302m\u2016F + \u221a 2\u2016N\u20162\u2016R\u22121\u20162\u2016 1 M M\u2211 m=1 E\u03021,m\u2016F +O( \u221a pK 20),\nwhere the second inequality is due to the property \u2016fR(\u00b7)\u2016F \u2264 \u221a 2\u2016R\u22121\u20162\u2016 \u00b7 \u2016F. By Lemma 4.4, we have\n\u2016N\u0303 \u2212N\u2016F \u2264 C\u2016 1\nM M\u2211 m=1 Em\u20162 +O( 2) +O( \u221a p 20), (G.1)\nfor some constant C > 0.\nNext, we give a high probability bound on \u2016N\u0303 \u2212N\u2016F in three steps. First, by Lemma H.1, we have with probability at least 1\u2212 e \u2212 \u03b41 C1\u03bb1 \u221a r/(Mn) that\n\u2016 1 M M\u2211 m=1 Em\u20162 \u2264 \u03b41,\nfor some constant C1 > 0 and r = Tr(\u03a3)/\u03bb1(\u03a3). Second, by Lemma H.1 and the union bound, we have with probability at least 1\u2212Me \u2212 \u03b42 C2\u03bb1 \u221a r/n that\n\u2264 \u03b42/\u2206K ,\nfor some constant C2 > 0. Third, by Lemma 4.5, we have with probability at least 1\u2212 2Me\u2212C3n\u03b4 2 3 \u2212Me\u2212C4 \u221a \u03b44n/r that\n0 = max m \u2016E\u0302m\u2016max \u2264 C5\n\u221a log(p)\nn + \u03b43 + \u03b44,\nfor some constants C3, C4, C5 > 0. By the union bound, we combine these three high probability bounds with (G.1) to obtain the desired result. In specific, with probability at least 1 \u2212 e \u2212 \u03b41 C1\u03bb1 \u221a r/(Mn) \u2212Me \u2212 \u03b42 C2\u03bb1 \u221a r/n \u2212 2Me\u2212C3n\u03b423 \u2212 Me\u2212C4 \u221a \u03b44n/r, the following inequality\n\u2016N\u0303 \u2212N\u2016F \u2264 O(\u03b41) +O(\u03b422) +O( \u221a p log(p)\nn ) +O(\u221ap\u03b423) +O( \u221a p\u03b424),\nholds for some constants C1, C2, C3, C4 > 0. Take \u03b41 = C1\u03bb1 log(p) \u221a r/(Mn), \u03b42 = C2\u03bb1 log(pM) \u221a r/n, \u03b43 =\u221a\nlog(2pM) C3n , and \u03b44 = log2(pM)r C24n , then we have with probability at least 1\u2212 4p\u22121 that\n\u2016N\u0303 \u2212N\u2016F \u2264 O( log(p) \u221a r\u221a\nMn ) +O( log\n2(pM)r\nn ) +O(\n\u221a p log(pM)\nn ) +O(\n\u221a p log4(pM)r2\nn2 )\n\u2264 O( log(p) \u221a r\u221a\nMn ) +O( (log2(pM)r) \u2228 (log(pM)\u221ap) n ) +O( \u221a p log4(pM)r2 n2 ).\nWhen n & (log2(pM) \u221a pr) \u2228 (log3(pM)r2), we have with probability at least 1\u2212 4p\u22121 that\n\u2016N\u0303 \u2212N\u2016F \u2264 O( log(p) \u221a r\u221a\nMn ) +O( (log2(pM)r) \u2228 (log(pM)\u221ap) n ).\nIn addition, when n & M((log 4(pM)r2)\u2228(log2(pM)p)) log2(p)r , we have with probability at least 1\u2212 4p\u22121 that\n\u2016N\u0303 \u2212N\u2016F \u2264 O( log(p) \u221a r\u221a\nMn ),\nwhich concludes the proof."
        },
        {
            "heading": "H Auxiliary Lemmas",
            "text": "The following lemma gives a tail bound of \u2016\u03a3\u0302\u2212\u03a3\u20162 in the sub-Gaussian case. Lemma H.1 (Lemma 3 in Fan et al. (2019)). Suppose {xi}ni=1 \u2282 Rp are i.i.d. sub-Gaussian with mean 0 and covariance \u03a3. Let \u03a3\u0302 = 1n \u2211n i=1 xix > i be the sample covariance matrix, {\u03bbj} p j=1 be the eigenvalues of \u03a3 sorted in descending order, and r = Tr(\u03a3)/\u03bb1. There exist constants C1 \u2265 1 and C2 \u2265 0 such that when n \u2265 r, we have\nP(\u2016\u03a3\u0302\u2212\u03a3\u20162 \u2265 s) \u2264 exp ( \u2212 s C1\u03bb1 \u221a r/n ) ,\u2200s \u2265 0,\nand \u2016\u2016\u03a3\u0302\u2212\u03a3\u20162\u2016\u03c81 \u2264 C2\u03bb1 \u221a r/n.\nThe following lemma provides a first-order expansion of V\u0302 H\u0302 around V , where H\u0302 = argminO\u2208OK\u00d7K \u2016V\u0302 O \u2212 V \u2016F.\nLemma H.2 (Lemma 8 in Fan et al. (2019)). Let \u03a3, \u03a3\u0302 \u2208 Rp\u00d7p be symmetric matrices with eigenvalues {\u03bbi}pi=1 and {\u03bb\u0302i}pi=1 (in descending order) and eigenvectors {vj} p j=1, {v\u0302j} p j=1 such that \u03a3vj = \u03bbjvj and \u03a3\u0302v\u0302j = \u03bb\u0302j v\u0302j for j \u2208 [p].\nDefine E = \u03a3\u0302 \u2212 \u03a3, S = {s + 1, . . . , s + K} for some fixed s \u2208 {0, 1, . . . , p \u2212K}, Gj = \u2211 i/\u2208S(\u03bbi \u2212 \u03bbj)\u22121viv>i for j \u2208 [K], and\ng : Rp\u00d7K 7\u2192 Rp\u00d7K , (w1, . . . ,wK) 7\u2192 (\u2212G1w1, . . . ,\u2212GKwK).\nLet V = (vs+1, . . . ,vs+K), V\u0302 = (v\u0302s+1, . . . , v\u0302s+K),H = V\u0302 >V , and H\u0302 = sgn(H) := U1U>2 , whereH = U1\u0393U > 2 is the unique singular value decomposition ofH . If \u2206 = min{\u03bbs\u2212 \u03bbs+1, \u03bbs+K \u2212 \u03bbs+K+1} > 0 and = \u2016E\u20162/\u2206 \u2264 1/10, where \u03bb0 =\u221e and \u03bbp+1 = \u2212\u221e, we have\n\u2016V\u0302 H\u0302 \u2212 V \u2212 g(EV )\u2016F \u2264 9 \u2016g(EV )\u2016F.\nIt is worth noting that since \u2016g(\u00b7)\u2016F \u2264 \u2206\u22121\u2016 \u00b7\u2016F, the Frobenius norm of the remainder term V\u0302 H\u0302\u2212V \u2212g(EV ) is of order\n9 \u2016g(EV )\u2016F \u2264 9 \u2206\u22121\u2016EV \u2016F \u2264 9 \u221a K \u2206\u22121\u2016EV \u20162 \u2264 9 \u221a K 2."
        }
    ],
    "title": "Statistical Analysis of Karcher Means for Random Restricted PSD Matrices",
    "year": 2023
}