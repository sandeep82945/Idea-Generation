{
    "abstractText": "Model selection in linear regression models is a major challenge when dealing with high-dimensional data where the number of available measurements (sample size) is much smaller than the dimension of the parameter space. Traditional methods for model selection such as Akaike information criterion, Bayesian information criterion (BIC) and minimum description length are heavily prone to overfitting in the high-dimensional setting. In this regard, extended BIC (EBIC), which is an extended version of the original BIC and extended Fisher information criterion (EFIC), which is a combination of EBIC and Fisher information criterion, are consistent estimators of the true model as the number of measurements grows very large. However, EBIC is not consistent in high signal-to-noise-ratio (SNR) scenarios where the sample size is fixed and EFIC is not invariant to data scaling resulting in unstable behaviour. In this paper, we propose a new form of the EBIC criterion called EBIC-Robust, which is invariant to data scaling and consistent in both large sample size and high-SNR scenarios. Analytical proofs are presented to guarantee its consistency. Simulation results indicate that the performance of EBIC-Robust is quite superior to that of both EBIC and EFIC.",
    "authors": [
        {
            "affiliations": [],
            "name": "Prakash B. Gohain"
        }
    ],
    "id": "SP:aac709c86c35d319c7f07f3febed58c6cb72edce",
    "references": [
        {
            "authors": [
                "J. Ding",
                "V. Tarokh",
                "Y. Yang"
            ],
            "title": "Model selection techniques: An overview",
            "venue": "IEEE Signal Processing Magazine, vol. 35, no. 6, pp. 16\u201334, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "P. Stoica",
                "Y. Selen"
            ],
            "title": "Model-order selection: a review of information criterion rules",
            "venue": "IEEE Signal Processing Magazine, vol. 21, no. 4, pp. 36\u201347, 2004.",
            "year": 2004
        },
        {
            "authors": [
                "C. Rao",
                "Y. Wu",
                "S. Konishi",
                "R. Mukerjee"
            ],
            "title": "On model selection",
            "venue": "Lecture Notes-Monograph Series, pp. 1\u201364, 2001.",
            "year": 2001
        },
        {
            "authors": [
                "D. Anderson",
                "K. Burnham"
            ],
            "title": "Model selection and multi-model inference",
            "venue": "Second. NY: Springer-Verlag, vol. 63, p. 10, 2004.",
            "year": 2004
        },
        {
            "authors": [
                "A. Chakrabarti",
                "J.K. Ghosh"
            ],
            "title": "AIC, BIC and recent advances in model selection",
            "venue": "Philosophy of statistics, pp. 583\u2013605, 2011.",
            "year": 2011
        },
        {
            "authors": [
                "H. Akaike"
            ],
            "title": "A new look at the statistical model identification",
            "venue": "IEEE transactions on automatic control, vol. 19, no. 6, pp. 716\u2013723, 1974.",
            "year": 1974
        },
        {
            "authors": [
                "G. Schwarz"
            ],
            "title": "Estimating the dimension of a model",
            "venue": "Annals of statistics, vol. 6, no. 2, pp. 461\u2013464, 1978.",
            "year": 1978
        },
        {
            "authors": [
                "J. Rissanen"
            ],
            "title": "Modeling by shortest data description",
            "venue": "Automatica, vol. 14, no. 5, pp. 465\u2013471, 1978.",
            "year": 1978
        },
        {
            "authors": [
                "M.H. Hansen",
                "B. Yu"
            ],
            "title": "Model selection and the principle of minimum description length",
            "venue": "Journal of the American Statistical Association, vol. 96, no. 454, pp. 746\u2013774, 2001.",
            "year": 2001
        },
        {
            "authors": [
                "J. Rissanen"
            ],
            "title": "MDL denoising",
            "venue": "IEEE Transactions on Information Theory, vol. 46, no. 7, pp. 2537\u20132543, 2000.",
            "year": 2000
        },
        {
            "authors": [
                "P. Stoica",
                "P. Babu"
            ],
            "title": "Model order estimation via penalizing adaptively the likelihood (PAL)",
            "venue": "Signal Processing, vol. 93, no. 11, pp. 2865\u20132871, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "J. Chen",
                "Z. Chen"
            ],
            "title": "Extended Bayesian information criteria for model selection with large model spaces",
            "venue": "Biometrika, vol. 95, no. 3, pp. 759\u2013 771, 2008.",
            "year": 2008
        },
        {
            "authors": [
                "A. Owrang",
                "M. Jansson"
            ],
            "title": "A model selection criterion for highdimensional linear regression",
            "venue": "IEEE Transactions on Signal Processing, vol. 66, no. 13, pp. 3436\u20133446, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "S. Kay"
            ],
            "title": "Exponentially embedded families-new approaches to model order estimation",
            "venue": "IEEE Transactions on Aerospace and Electronic Systems, vol. 41, no. 1, pp. 333\u2013345, 2005.",
            "year": 2005
        },
        {
            "authors": [
                "H. Bozdogan"
            ],
            "title": "Model selection and Akaike\u2019s information criterion (AIC): The general theory and its analytical extensions",
            "venue": "Psychometrika, vol. 52, no. 3, pp. 345\u2013370, 1987.",
            "year": 1987
        },
        {
            "authors": [
                "R.R. Picard",
                "R.D. Cook"
            ],
            "title": "Cross-validation of regression models",
            "venue": "Journal of the American Statistical Association, vol. 79, no. 387, pp. 575\u2013583, 1984.",
            "year": 1984
        },
        {
            "authors": [
                "L. de Torrent\u00e9",
                "T. Hastie"
            ],
            "title": "Does cross-validation work when p \u226b n?",
            "year": 2012
        },
        {
            "authors": [
                "S. Kallummil",
                "S. Kalyani"
            ],
            "title": "Signal and noise statistics oblivious orthogonal matching pursuit",
            "venue": "International Conference on Machine Learning. PMLR, 2018, pp. 2429\u20132438.",
            "year": 2018
        },
        {
            "authors": [
                "P.B. Gohain",
                "M. Jansson"
            ],
            "title": "Relative cost based model selection for sparse high-dimensional linear regression models",
            "venue": "ICASSP IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 5515\u20135519.",
            "year": 2020
        },
        {
            "authors": [
                "T.T. Cai",
                "L. Wang"
            ],
            "title": "Orthogonal matching pursuit for sparse signal recovery with noise",
            "venue": "IEEE Transactions on Information theory, vol. 57, no. 7, pp. 4680\u20134688, 2011.",
            "year": 2011
        },
        {
            "authors": [
                "P.B. Gohain",
                "M. Jansson"
            ],
            "title": "New improved criterion for model selection in sparse high-dimensional linear regression models",
            "venue": "ICASSP IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2022, pp. 5692\u20135696.",
            "year": 2022
        },
        {
            "authors": [
                "S.M. Kay"
            ],
            "title": "Fundamentals of statistical signal processing: estimation theory",
            "venue": "Prentice Hall PTR,",
            "year": 1993
        },
        {
            "authors": [
                "P. Stoica",
                "P. Babu"
            ],
            "title": "On the proper forms of BIC for model order selection",
            "venue": "IEEE Transactions on Signal Processing, vol. 60, no. 9, pp. 4956\u20134961, 2012.",
            "year": 2012
        },
        {
            "authors": [
                "P.B. Gohain",
                "M. Jansson"
            ],
            "title": "Scale-invariant and consistent Bayesian information criterion for order selection in linear regression models",
            "venue": "Signal Processing, p. 108499, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "D.F. Schmidt",
                "E. Makalic"
            ],
            "title": "The consistency of MDL for linear regression models with increasing signal-to-noise ratio",
            "venue": "IEEE transactions on signal processing, vol. 60, no. 3, pp. 1508\u20131510, 2011.",
            "year": 2011
        },
        {
            "authors": [
                "P.M. Djuric"
            ],
            "title": "Asymptotic MAP criteria for model selection",
            "venue": "IEEE Transactions on Signal Processing, vol. 46, no. 10, pp. 2726\u20132735, 1998.",
            "year": 1998
        },
        {
            "authors": [
                "Q. Ding",
                "S. Kay"
            ],
            "title": "Inconsistency of the MDL: On the performance of model order selection criteria with increasing signal-to-noise ratio",
            "venue": "IEEE Transactions on Signal Processing, vol. 59, no. 5, pp. 1959\u20131969, 2011.",
            "year": 1959
        },
        {
            "authors": [
                "C.-H. Zhang",
                "J. Huang"
            ],
            "title": "The sparsity and bias of the lasso selection in high-dimensional linear regression",
            "venue": "The Annals of Statistics, vol. 36, no. 4, pp. 1567\u20131594, 2008.",
            "year": 2008
        },
        {
            "authors": [
                "N. Meinshausen",
                "P. B\u00fchlmann"
            ],
            "title": "High-dimensional graphs and variable selection with the lasso",
            "venue": "The annals of statistics, vol. 34, no. 3, pp. 1436\u20131462, 2006.",
            "year": 2006
        },
        {
            "authors": [
                "R. Tibshirani"
            ],
            "title": "Regression shrinkage and selection via the lasso",
            "venue": "Journal of the Royal Statistical Society: Series B (Methodological), vol. 58, no. 1, pp. 267\u2013288, 1996.",
            "year": 1996
        },
        {
            "authors": [
                "B. Efron",
                "T. Hastie",
                "I. Johnstone",
                "R. Tibshirani"
            ],
            "title": "Least angle regression",
            "venue": "The Annals of statistics, vol. 32, no. 2, pp. 407\u2013499, 2004.",
            "year": 2004
        },
        {
            "authors": [
                "A.M. Mathai",
                "S.B. Provost"
            ],
            "title": "Quadratic forms in random variables: theory and applications",
            "year": 1992
        },
        {
            "authors": [
                "B. Laurent",
                "P. Massart"
            ],
            "title": "Adaptive estimation of a quadratic functional by model selection",
            "venue": "Annals of Statistics, pp. 1302\u20131338, 2000.",
            "year": 2000
        }
    ],
    "sections": [
        {
            "text": "ar X\niv :2\n20 6.\n08 73\n1v 1\n[ ee\nss .S\nP] 1\n7 Ju\nn 20\n22 IEEE TRANSACTIONS ON SIGNAL PROCESSING, 2022 1\nIndex Terms\u2014High-dimension, linear regression, data scaling, statistical model selection, subset selection, sparse estimation, scale-invariant, variable selection.\nI. INTRODUCTION\nSelecting the true or best set of covariates from a large pool of potential covariates is a fundamental requirement in\nmany applications of science, engineering and biology. In this paper, our primary focus is on model selection in high-\ndimensional linear regression models associated with the max-\nimum likelihood (ML) method of parameter estimation where the number of measurements, N , is quite small compared to the model space or parameter dimension, p, i.e., N < p. Highdimensional datasets are a common phenomena in many fields of scientific studies, and as such model selection is a central\nelement of data analysis and statistical inference [1].\nConsider the linear model\ny = Ax+ e, (1)\nwhere y \u2208 RN is the measurement vector and A \u2208 RN\u00d7p is the known design matrix. We are considering a highdimensional setting, hence p > N . Also, p can be linked to N as p = Nd, where d > 0 is a real value. e \u2208 RN is the associated noise vector whose elements are assumed to be\nThis research was supported in part by the European Research Council (ERC) under the European Union\u2019s Horizon 2020 research and innovation programme, grant agreement No. 742648.\nThe authors are with the Division of Information Science and Engineering, KTH Royal Institute of Technology, Stockholm SE-10044, Sweden e-mail: pbg@kth.se, janssonm@kth.se.\ni.i.d. following a Gaussian distribution, i.e., e \u223c N (0, \u03c32IN ) where \u03c32 is the unknown true noise power. x \u2208 Rp is the unknown parameter vector. Here, x is assumed to be sparse, which implies that very few of the elements of x are non-zero. We denote S as the true support of x, i.e., S = {i : xi 6= 0} having cardinality card(S) = k0 \u226a N and AS as the set of columns of A corresponding to the support S. The goal of model selection is estimating S given y and A.\nA popular approach for model selection is using information theoretic criteria [2], [3], [4], [5]. A typical information\ncriterion based model selection rule picks the best model that minimizes some statistical metric as shown below\nS\u0302 = argmin I\u2208J {f(MI) + P(I)}, (2)\nwhere S\u0302 is the model estimate, J is the set of candidate models under consideration and MI denotes the model with support I. The statistical metric consists of two parts: (1) f(MI) representing the goodness of fit of model MI and (2) P(I) is the penalty term that compensates for overparameterization. The literature on model selection is quite extensive. Some of the popular classical model selection rules\ninclude Akaike information criterion [6], Bayesian information criterion (BIC)[7], minimum description length (MDL)[8],\ngMDL[9], nMDL[10], and penalizing adaptively the likelihood\n(PAL) [11]. However, these classical methods in their current form fail to handle the large dimension cases and tend to\noverfit the final model [12], [13].\nAmong the classical methods of model selection, BIC has been quite successful due to its simplicity and consistent\nperformance in many fields. BIC is asymptotically consistent in selecting the true model as N grows very large given that p and the true noise variance \u03c32 is fixed. However, its performance in high-dimensional settings when p > N is not satisfactory and it has a tendency to select more co-variates than required, thus overfitting the model [12]. To handle the large-p small-N scenario, the authors in [12] proposed a novel extension to the original BIC called extended BIC (EBIC), that\ntakes into account both the number of unknown parameters and\nthe complexity of the model space. EBIC adds dynamic prior model probabilities to each of the models under consideration\nthat is inversely proportional to the model set dimension.\nThis eliminates the earlier assumption of assigning uniform prior to all models irrespective of their sizes, which goes\nagainst the principle of parsimony. Under a suitable asymptotic identifiability condition, EBIC is consistent such that it selects the true model as N tends to infinity [12]. However, the consistent behaviour of EBIC fails when N is small and fixed\nand \u03c32 tends to zero [13]. This new consistency requirement was first introduced in [14], where the authors highlighted that the original BIC is also inconsistent for fixedN and decreasing noise variance scenarios where N > p.\nTo overcome the drawbacks of EBIC, the authors in [13]\nproposed a novel criterion called extended Fisher information\ncriterion (EFIC) that is inspired by EBIC and the model selection criteria with Fisher information [15]. The authors\nanalyzed the performance of EFIC in the high-dimensional setting for two key cases: (1) when \u03c32 is fixed and N tends to infinity; (2) when N is fixed and \u03c32 tends to zero. In each case, it was shown that EFIC selects the true model with\na probability approaching one. However, as indicated in our simulations, EFIC is not invariant to data scaling and it tends\nto suffer from overfitting issues (and sometimes underfitting) in practical sizes of N when the data is scaled. This scaling problem is a result of the data dependent penalty design that\nmay blow the penalty to extremely small or large values depending on how the data is scaled.\nApart from the criteria mentioned above, there are other\nnon-information theoretic methods available for model selection. One such popular method is cross-validation (CV) [16].\nHowever, the performance of CV is quite poor in sample scarce scenarios with large parameter dimensions and even\nthough CV is unbiased, it can have high variance [17]. Recent\nadditions to the list of model selection methods for highdimensional data are residual ratio thresholding (RRT)[18] and\nmulti-beta-test (MBT) [19]. Both are non-information theoretic\nmethods based on hypothesis testing using a test statistic. They operate along with a greedy variable selection method\nsuch as orthogonal matching pursuit (OMP) [20] and involve a tuning parameter \u2208 [0, 1], that controls the false-discovery rate. However, there is no optimal way to set it and as such\ntheir behaviour may tend to overfit or underfit depending on the chosen tuning parameter value. Moreover, in their current\nform, they can only be used with algorithms that generate\nmonotonic sequences of support estimates such as OMP, which restricts their usability.\nIn this paper, we propose a modified criterion for model selection in high-dimensional linear regression models called\nEBIC-Robust or EBICR in short, where the subscript R stands for robust. EBICR is a scale-invariant and consistent criterion. To guarantee the consistency, we provide analytical proofs to\nshow that under a suitable asymptotic identifiability condition,\nEBICR selects the true model with a probability approaching one as N \u2192 \u221e as well as when \u03c32 \u2192 0. Some preliminary results have been published in [21].\nThroughout the paper, boldface letters denote matrices and vectors. The notation (\u00b7)T stands for transpose. AI denotes a sub-matrix of the full matrix A formed using the columns indexed by the support set I. \u03a0I = AI(ATIAI)\u22121ATI denotes the orthogonal projection matrix on the span of AI and \u03a0\u22a5I = IN \u2212\u03a0I denotes the orthogonal projection matrix on the null space of ATI and IN is an N \u00d7N identity matrix. The notation \u2223\u2223X \u2223\u2223 denotes the determinant of the matrix X and \u2016\u00b7\u20162 denotes the Euclidean norm. X \u223c N (0, 1) denotes a normal distributed random variable with mean 0 and variance 1. X \u223c \u03c72k is a central chi-squared distributed random variable\nwith k degrees of freedom, X \u223c \u03c72k(\u03bb) is a noncentral chisquared distributed random variable with k degrees of freedom and non-centrality parameter \u03bb."
        },
        {
            "heading": "II. BACKGROUND",
            "text": "Given the linear model (1), the entire process of model selection or in other words estimating the true support set S involves two major steps: (i) Predictor/subset selection, which\nincludes finding a competent set of candidate models out of all the (2p \u2212 1) possible models. In our work, we consider the set of competing models as the collection of all plausible combinatorial models up to a maximum cardinality K , under the assumption that k0 \u2264 K \u226a N ; (ii) estimating the true model among the candidate models using a suitable model selection criterion. For a candidate model with support I having cardinality card(I) = k, the linear model in (1) can be reformulated as follows\nHI : y = AIxI + eI , (3) where HI denotes the hypothesis that the data y is truly generated according to (3), AI \u2208 RN\u00d7k is the sub-design matrix consisting of columns from the known design matrix A with support I, xI \u2208 Rk is the corresponding unknown parameter vector and eI \u2208 RN is the associated noise vector following eI \u223c N (0, \u03c32IIN ) where \u03c32I is the unknown noise variance corresponding to the hypothesis HI ."
        },
        {
            "heading": "A. Bayesian Framework for Model Selection",
            "text": "To motivate the proposed criterion we start by describing the Bayesian framework that leads to the maximum a-posteriori\n(MAP) estimator, which in turn forms the backbone for deriving BIC and its extended versions, viz., EBIC, EFIC, as\nwell as the proposed criterion EBICR. Now, for the considered model in (3), the probability density function (pdf) of the data vector y is given as\np(y|\u03b8I ,HI) = exp{\u2212\u2016y\u2212AIxI\u201622/2\u03c32I}\n(2\u03c0\u03c32I) N/2\n, (4)\nwhere \u03b8I = [x T I , \u03c3 2 I ] T comprises of all the parameters of the model. Under hypothesis HI , the maximum likelihood estimates (MLEs) of \u03b8\u0302I = [x\u0302 T I , \u03c3\u0302 2 I ] T are obtained as [22]\nx\u0302I = ( ATIAI )\u22121 ATIy & \u03c3\u0302 2 I = yT\u03a0\u22a5I y\nN . (5)\nLet p(\u03b8I |HI) denote the prior pdf of the parameter vector \u03b8I under HI . Then we have the joint probability\np(y, \u03b8I |HI) = p(y|\u03b8I ,HI)p(\u03b8I |HI) (6) and the marginal distribution of y is\np(y|HI) = \u222b p(y|\u03b8I ,HI)p(\u03b8I |HI)d\u03b8I . (7)\nThe posterior probability Pr(HI |y) is given by\nPr(HI |y) = p(y|HI) Pr (HI)\np(y) , (8)\nwhere Pr(HI) is the prior probability of the model with support I. The MAP estimator picks the model with the largest\nposterior probability Pr(HI |y). However, note that the p(y) is a normalizing factor and independent of I. Hence, the MAP estimate of S is equivalently given by\nS\u0302MAP = argmax I\n{ ln p(y|HI) + lnPr (HI) } . (9)\nTo compute the MAP estimate, we need to evaluate the integral in (7). Traditionally, under the assumption that N and/or SNR are large, we can obtain an approximation of ln p(y|HI) using a second order Taylor series expansion, which gives (see [23],\n[24] for details)\nln p(y|HI) \u2248 ln p(y|\u03b8\u0302I ,HI) + ln p(\u03b8\u0302I |HI)\n+ k + 1 2 ln(2\u03c0)\u2212 1 2 ln \u2223\u2223F\u0302I \u2223\u2223, (10)\nwhere k = card(I) and F\u0302I is the sample Fisher information matrix under HI given as [22]\nF\u0302I = \u2212 \u22022 ln p(y|\u03b8I ,HI)\n\u2202\u03b8I\u2202\u03b8 T I\n\u2223\u2223\u2223\u2223 \u03b8I=\u03b8\u0302I . (11)\nEvaluating (11) using (4) and (5) we get [23]\nF\u0302I =\n[ 1 \u03c3\u03022 I ATIAI 0\n0 N 2\u03c3\u03024\nI\n] . (12)\nNow, for the considered linear model we have\n\u22122 ln p(y|\u03b8\u0302I ,HI) = N ln \u03c3\u03022I + const. (13) Therefore, using (13), we can rewrite (10) as\n\u22122 ln p(y|HI) \u2248 N ln \u03c3\u03022I + ln \u2223\u2223F\u0302I \u2223\u2223\u2212 2 ln p(\u03b8\u0302I |HI) \u2212k ln 2\u03c0 + const. (14)\nFurthermore, it is assumed that the prior term in (10), i.e., ln p(\u03b8\u0302I |HI) is flat and uninformative, and hence disregarded from the analysis. Thus, dropping the constants and the terms independent of the model dimension k, we can equivalently reformulate the MAP based model estimate as\nS\u0302MAP = argmin I\n{ N ln \u03c3\u03022I+ln \u2223\u2223F\u0302I \u2223\u2223\u2212k ln 2\u03c0\u22122 lnPr (HI) } .\n(15)"
        },
        {
            "heading": "B. BIC",
            "text": "The BIC can be obtained from the MAP estimator in (15). The term \u2212k ln 2\u03c0 is ignored as it weakly depends on the model dimension k and hence is typically much smaller than the dominating terms. Moreover, the prior probability of each candidate model is assumed to be equiprobable. Hence, the \u22122 lnPr(HI) term is dropped as well. Now, expanding the |F\u0302I | term of (15) using (12) we have\nln \u2223\u2223F\u0302I \u2223\u2223 = ln(N/2)\u2212 (k + 2) ln \u03c3\u03022I + ln \u2223\u2223ATIAI \u2223\u2223 . (16)\nHere, the following property of the design matrix A is assumed [23], [25]\nlim N\u2192\u221e\n{ N\u22121(ATIAI) } = MI = O(1), (17)\nwhere MI is a k\u00d7 k positive definite matrix and bounded as N \u2192 \u221e. The assumption in (17) is true in many applications\nbut not all (see [26] for more details). Using (17), it is possible to show that for large N\nln \u2223\u2223ATIAI \u2223\u2223 = ln \u2223\u2223\u2223\u2223N \u00b7N\u22121(ATIAI) \u2223\u2223\u2223\u2223 = k lnN +O(1). (18)\nFurthermore, \u03c3\u03022I is considered to be of O(1) as well since it does not grow with N . As such, the O(1) term, (k+2) ln \u03c3\u03022I and ln(N/2) (a constant) are ignored from (16). This leads to the final form of the BIC\nBIC(I) = N ln \u03c3\u03022I + k lnN. (19)\nBIC is consistent when p is fixed and N \u2192 \u221e. However, it is inconsistent when N is fixed and \u03c32 \u2192 0 [27], [24] as well as when p > N and p grows exponentially with N [12]."
        },
        {
            "heading": "C. EBIC",
            "text": "The authors in [12] proposed an extended version of the BIC, i.e., EBIC, to mitigate the drawbacks of BIC for largep small-N scenarios. EBIC can be derived from the MAP estimator in (15), using the same assumptions as in BIC, except for the prior probability term Pr(HI). In EBIC, the idea of equiprobable models is discredited and instead a prior probability is assigned that is inversely proportional to the size of the model space. Thus, a model with dimension k is assigned prior probability of Pr(HI) \u221d ( p k )\u2212\u03b3 , where 0 \u2264 \u03b3 \u2264 1 is a tuning parameter. Thus, the EBIC is\nEBIC(I) = N ln \u03c3\u03022I + k lnN + 2\u03b3 ln ( p\nk\n) . (20)\nWhen \u03b3 = 0, EBIC boils down to BIC (19). Moreover, unlike BIC, EBIC is consistent in selecting the true model for p\u226b N cases where p grows exponentially with N . However, it has been observed in [13] that EBIC is inconsistent when N is fixed and \u03c32 \u2192 0."
        },
        {
            "heading": "D. EFIC",
            "text": "To circumvent the shortcomings of EBIC in high-SNR cases, the authors in [13] proposed EFIC. In EFIC, the\nassumptions imposed on the sample FIM (16) are removed and\nthe entire structure is included as it is in the criterion except for the constant term ln(N/2). Some further simplifications are involved:\nN ln \u03c3\u03022I = N ln \u2225\u2225\u03a0\u22a5I y \u2225\u22252 2 \u2212N lnN (21)\n(k + 2) ln \u03c3\u03022I = (k + 2) [ ln \u2225\u2225\u03a0\u22a5I y \u2225\u22252 2 \u2212 lnN ] . (22)\nThe \u2212N lnN and \u22122 lnN term of (21) and (22) respectively are independent of the model dimension k and hence ignored. Similar to EBIC the prior probability term is assumed to be proportional to the model space, hence Pr(HI) \u221d ( p k )\u2212c , where c > 0 is a tuning parameter. Furthermore, under the large-p approximation and since k \u2264 K \u226a p, the ln ( p k ) term is approximated as\nln\n( p\nk\n) = k\u22121\u2211\ni=0\nln(p\u2212 i)\u2212 ln(k!) \u2248 k ln p. (23)\nHence, for large-p case, we can set \u22122 ln p(HI) \u2248 2ck ln p. Thus, the EFIC is given as\nEFIC(I) = N ln \u2225\u2225\u03a0\u22a5I y \u2225\u22252 2 + k lnN + ln \u2223\u2223ATIAI \u2223\u2223\n\u2212(k + 2) ln \u2225\u2225\u03a0\u22a5I y \u2225\u22252 2 + 2ck ln p.\n(24)\nEFIC is consistent in both large-N and high-SNR scenarios [13]. However, EFIC suffers from a data scaling problem due to the inclusion of the data dependent penalty term and as\nsuch the performance of EFIC is not invariant to data scaling.\nSee further in Section III-A."
        },
        {
            "heading": "III. PROPOSED CRITERION: EBIC-ROBUST (EBICR)",
            "text": "In this section, we present the necessary steps for deriving\nEBICR. EBICR can be seen as a natural extension of BICR [24] for performing model selection in large-p small-N scenarios. Below, we provide a detailed derivation and establish the con-\nnection to BICR. A similar approach as in [23] is considered, but here we perform normalization of F\u0302I under both largeN and high-SNR assumption. It is possible to factorize the ln \u2223\u2223F\u0302I \u2223\u2223 term in (15) in the following manner\nln \u2223\u2223F\u0302I \u2223\u2223 = ln [\u2223\u2223L \u2223\u2223 \u2223\u2223\u2223L\u22121/2F\u0302IL\u22121/2 \u2223\u2223\u2223 ]\n= ln |L|+ ln \u2223\u2223\u2223L\u22121/2F\u0302IL\u22121/2 \u2223\u2223\u2223 \ufe38 \ufe37\ufe37 \ufe38\nT\n. (25)\nThe goal here is to choose a suitable L matrix that normalizes the sample FIM F\u0302I such that the T term in (25) is O(1), i.e., in this case T should be bounded as N \u2192 \u221e and/or \u03c32 \u2192 0. To accomplish this objective, we choose the following L\u22121/2 matrix\nL\u22121/2 =\n  \u221a 1 N \u221a \u03c3\u03022 I \u03c3\u03022 0 Ik 0\n0\n\u221a 1 N \u03c3\u03022 I\n\u03c3\u03022 0\n  , (26)\nwhere \u03c3\u030220 = \u2016y\u201622/N . The factor, \u03c3\u030220 , is used in L\u22121/2 in order to neutralize the data scaling problem and is motivated by the\nfact that given (17), when the SNR is a constant, we have\nE[\u03c3\u030220 ] \u2192 const. & Var[\u03c3\u030220 ] \u2192 0 (27)\nas N \u2192 \u221e. Furthermore, from the considered generating model in (1), when N is fixed, (27) is also satisfied as \u03c32 \u2192 0 (see Appendix B for details on \u03c3\u030220). Now using (12), (26) and the assumptions in (17), (27) it is possible to show that\n\u2223\u2223\u2223L\u22121/2F\u0302IL\u22121/2 \u2223\u2223\u2223 = \u2223\u2223\u2223\u2223\u2223 1 \u03c3\u03022 0 A T I AI N 0\n0 1 2\u03c3\u03024\n0\n\u2223\u2223\u2223\u2223\u2223 = O(1), (28)\nand therefore may be discarded without much effect on the criterion. Furthermore, the ln \u2223\u2223L \u2223\u2223 term can be expanded as\nfollows\nln |L| = ln \u2223\u2223\u2223\u2223\u2223\u2223 N ( \u03c3\u03022 0 \u03c3\u03022 I ) Ik 0 0 N ( \u03c3\u03022 0\n\u03c3\u03022 I\n)2 \u2223\u2223\u2223\u2223\u2223\u2223\n= (k + 1) lnN + (k + 2) ln ( \u03c3\u030220 \u03c3\u03022I ) . (29)\nTherefore, using (28) and (29) we can rewrite (25) as\nln \u2223\u2223F\u0302I \u2223\u2223 = k lnN + (k + 2) ln ( \u03c3\u030220 \u03c3\u03022I ) +O(1) + lnN. (30) Next, for the model prior probability term \u22122 lnPr(HI) in (15), a similar proposition is taken as in EBIC such that Pr(HI) \u221d ( p k )\u2212\u03b6 , where \u03b6 \u2265 0 is a tuning parameter. For large-p, we follow a similar approach as in EFIC by employing the following approximation ln ( p k ) \u2248 k ln p . This gives\n\u22122 lnPr(HI) = 2\u03b6k ln p+ const. (31) Now, substituting (30), (31) in (15) and dropping the O(1), the lnN term (independent of k), the constant and the p(\u03b8\u0302I |HI) term we arrive at the EBICR:\nEBICR(I) =N ln \u03c3\u03022I + k ln ( N\n2\u03c0\n)\n+ (k + 2) ln ( \u03c3\u030220 \u03c3\u03022I ) + 2k\u03b6 ln p.\n(32)\nThe true model is estimated as\nS\u0302EBICR = argmin I\u2208J\n{ EBICR(I) } , (33)\nwhere J denotes the set of candidate models. It can be observed from (32) that the penalty of EBICR is a function of the number of measurements N , the ratio (\u03c3\u030220/\u03c3\u0302 2 I) and the parameter dimension p. Notice that the ratio (\u03c3\u030220/\u03c3\u0302 2 I) is always greater than 1 and independent of the scaling of y. Furthermore, when S 6\u2282 I, the ratio (\u03c3\u030220/\u03c3\u03022I) \u2248 O(1) and for S \u2282 I we have (\u03c3\u030220/\u03c3\u03022I) \u2248 O(SNR+1). Hence, the behaviour of the penalty can be summarized as follows: (i) For fixed p and SNR, as N \u2192 \u221e the penalty grows as O(lnN); (ii) If N and p are constant, as SNR \u2192 \u221e, the penalty grows as O (ln(SNR + 1)) for all I \u2283 S; (iii) when SNR is a constant and given that p grows with N , then as N \u2192 \u221e the penalty grows as O(lnN) +O(ln p)."
        },
        {
            "heading": "A. Scaling Robustness as Compared to EFIC",
            "text": "In this section, we elaborately discuss the data scaling problem. Ideally, any model selection criterion should be\ninvariant to data scaling, which means that if y is scaled by any arbitrary constant C > 0, the equivalent penalty for each of the models I should not change. This property is necessary because otherwise the behaviour of the model selection criterion will be unreliable and may suffer from\noverfitting or underfitting issues when the data is scaled. As\nmentioned before, the penalty of EFIC is not invariant to data scaling. This can be observed from the following analysis. Let \u2206 = card(I) \u2212 card(S). Now, consider the difference assuming I 6= S\nEFIC(I)\u2212 EFIC(S)\n= (N \u2212 2) ln \u2225\u2225\u03a0\u22a5I y \u2225\u22252 2\u2225\u2225\u03a0\u22a5S y \u2225\u22252 2 + ln \u2223\u2223ATIAI \u2223\u2223 \u2223\u2223ATSAS \u2223\u2223 \u2212 k ln \u2225\u2225\u03a0\u22a5I y \u2225\u22252 2\n+ k0 ln \u2225\u2225\u03a0\u22a5S y \u2225\u22252 2 +\u2206(lnN + 2c ln p) = DEFIC (say).\n(34)\nIdeally, for correct model selection, DEFIC > 0 for all I 6= S. Now, if we scale the data y by a constant C > 0, the data dependent term becomes ln\u2016\u03a0\u22a5I Cy\u201622 = lnC2 + ln\u2016\u03a0\u22a5I y\u201622 and the difference becomes\nEFIC(I)\u2212 EFIC(S) = DEFIC \u2212\u2206 lnC2. (35) It is evident that (34) and (35) are unequal and the difference after scaling contains an additional term \u2212\u2206 lnC2. This implies that scaling the data changes the EFIC score difference between any arbitrary model I and the true model S. Hence, depending on the C value (C < 1 or C \u2265 1) and \u2206 > 0 or \u2206 < 0, the difference in (35) may become negative leading to a false model selection. Thus, EFIC is not invariant to data\nscaling. On the contrary, consider the difference for EBICR,\nEBICR(I)\u2212 EBICR(S)\n= (N \u2212 2) ln ( \u03c3\u03022I \u03c3\u03022S ) \u2212 k ln \u03c3\u03022I + k0 ln \u03c3\u03022S +\u2206 ln \u03c3\u030220\n+\u2206(ln(N/2\u03c0) + 2\u03b6 ln p) = DEBICR (say) (36)\nNow, scaling y by C, scales the noise variance estimates \u03c3\u03022I , \u03c3\u03022S and \u03c3\u0302 2 0 by C 2, however, the difference remains the same, i.e., DEBICR . This is because in this case the \u2212\u2206 lnC2 term is cancelled by +\u2206 lnC2 generated by \u2206 ln \u03c3\u030220 . Hence, EBICR is invariant to data scaling, which is a desired property of any model selection criterion."
        },
        {
            "heading": "IV. CONSISTENCY OF EBICR",
            "text": "In this section, we provide the necessary proofs to show that\nEBICR is a consistent criterion. Generally speaking, a model selection criterion with S\u0302 as its estimate of the true model S is consistent if it satisfies the following conditions [13]\nlim \u03c32\u21920 Pr{S\u0302 = S} = 1 & lim N\u2192\u221e Pr{S\u0302 = S} = 1. (37)\nLet us define the set of all overfitted models of dimension k as Iko = {I : card(I) = k,S \u2282 I} and the set of all misfitted models of dimension k as Ikm = {I : card(I) = k,S 6\u2282 I}. Furthermore, let O denote the set of all Iko for k = k0 + 1, . . . ,K , and let M denote the set of all Ikm for k = 1, . . . ,K , i.e.,\nO =\nK\u22c3\nk=k0+1\nIko and M = K\u22c3\nk=1\nIkm , (38)\nwhere K is some upper bound for k0 and k0 \u2264 K \u226a N . In practice, EBICR picks the true model S, if the following conditions are satisfied:\nC1 : EBICR(S) < EBICR(I) \u2200 I \u2208 O (39) C2 : EBICR(S) < EBICR(I) \u2200 I \u2208 M. (40)"
        },
        {
            "heading": "A. Asymptotic Identifiability of the Model",
            "text": "In general, the model is identifiable if no model of comparable size other than the true submodel can predict the noise\nfree response almost equally well [12]. In the context of linear regression, this is equivalent to say y = ASxS 6= AIxI for { I : card(I) \u2264 card(S), I 6= S } . The identifiability\nof the true model in the high-dimensional linear regression\nsetup is uniformly maintained if the minimal eigenvalue of all restricted sub-matrices, ATIAI for {I : card(I) \u2264 2K}, is bounded away from zero [13]. A sufficient assumption on\nthe design matrix A to prove the consistency of EBICR is the sparse Riesz condition [28]:\nlim N\u2192\u221e\n{ N\u22121 ( ATIAI )} = MI , \u2200 card(I) \u2264 2K, (41)\nwhere MI denotes a bounded positive definite matrix."
        },
        {
            "heading": "B. Consistency as \u03c32 \u2192 0 or SNR \u2192 \u221e for fixed N",
            "text": "In this subsection, we examine whether EBICR selects the true model S as \u03c32 goes vanishingly small (or equivalently SNR\u2192 \u221e) under the assumption that N is fixed. We formulate this into a theorem as follows:\nTheorem 1: Assume that N and p are fixed and the matrix A satisfies the condition given by (41). If K \u2265 k0, then Pr {EBICR(S) < EBICR(I)} \u2192 1 as \u03c32 \u2192 0 for all I 6= S and card(I) = 1, . . . ,K .\nProof. The proof consists of two parts. In part (a) we show that the probability of overfitting (S \u2282 S\u0302EBICR) tends to 0 as \u03c32 \u2192 0, which in this case is equivalent to showing lim\u03c32\u21920 Pr(C1) = 1, cf. (39). In part (b) we show that the probability of misfitting (S 6\u2282 S\u0302EBICR) also tends to 0 as \u03c32 \u2192 0, which is equivalent to lim\u03c32\u21920 Pr(C2) = 1, cf. (40). (a) Over-fitting case (S \u2282 S\u0302EBICR): Consider the set of overfitted subsets having cardinality k, which we have denoted as Iko . Let Ij denote the jth subset in the set Iko . The total number of subsets in Iko is ( p\u2212k0 \u2206 ) where \u2206 = k\u2212k0 . For any overfitted subset Ij \u2208 Iko , consider the following inequality EBICR(S) < EBICR(Ij), Ij \u2208 Iko , (42) where j = 1, . . . , ( p\u2212k0 \u2206 ) . Using the relation p = Nd and after some straightforward rearrangement of (42) we get\n(N \u2212 k0 \u2212 2) ln \u03c3\u03022S \u2212 (N \u2212 k \u2212 2) ln \u03c3\u03022Ij \u2212\u2206(1 + 2\u03b6d) lnN \u2212\u2206 ln \u03c3\u030220 +\u2206 ln 2\u03c0 < 0. (43)\nLet us define a random variable XIj = \u03c3\u0302 2 Ij /\u03c32, then\nN \u00b7XIj \u223c \u03c72N\u2212k, \u2200 Ij \u2208 Iko . (44) This implies that the variables XIj are independent of \u03c3 2. Now, we can express\n(N \u2212 k \u2212 2) ln \u03c3\u03022Ij = lnXN\u2212k\u22122Ij + (N \u2212 k\u2212 2) ln\u03c3 2, (45)\nand similarly by defining XS = \u03c3\u0302 2 S/\u03c3 2 we get\n(N\u2212k0\u22122) ln \u03c3\u03022S = lnXN\u2212k0\u22122S +(N\u2212k0\u22122) ln\u03c32. (46) Using (45) and (46) in (43) and after exponentiation we get ( XN\u2212k0\u22122S XN\u2212k\u22122Ij )( 1 N )\u2206(1+2\u03b6d)( 2\u03c0 \u03c3\u030220 )\u2206 < ( 1 \u03c32 )\u2206 . (47)\nLet EkIj denote the entire left hand-side and let \u03b7k denote the right-hand side of the inequality in (47). Let I\u2217 \u2208 Iko denote the subset that produces the maximum value of EkIj among all such subsets Ij \u2208 Iko . Then, let us denote\nEkI\u2217 = max Ij\u2208Iko\n{ EkIj } , j = 1, 2, . . . , ( p\u2212 k0 \u2206 ) . (48)\nThe condition C1 in (39) is satisfied as \u03c32 \u2192 0 under the event EkI\u2217 < \u03b7k, for all k = k0 + 1, . . . ,K. Now, we can express the probability that EkI\u2217 < \u03b7k as follows\nPr ( EkI\u2217 < \u03b7k ) = Pr    (p\u2212k0\u2206 )\u22c2\nj=1\n( EkIj < \u03b7k )   \n= 1\u2212 Pr    (p\u2212k0\u2206 )\u22c3\nj=1\n( EkIj > \u03b7k )   \n\u2265 1\u2212 ( p\u2212 k0 \u2206 ) Pr ( EkIj > \u03b7k )\n=\u21d2 Pr ( EkI\u2217 > \u03b7k ) \u2264 ( p\u2212 k0 \u2206 ) Pr ( EkIj > \u03b7k ) , (49)\nwhere the inequality follows from the union bound. Now consider the following probability Pr { EkIj > \u03b7k } for any arbitrary subset Ij \u2208 Iko , which can be expressed as\nPr {( XN\u2212k0\u22122S XN\u2212k\u22122Ij )( 1 N )\u2206(1+2\u03b6d)( 2\u03c0 \u03c3\u030220 )\u2206 > ( 1 \u03c32 )\u2206} .\n(50)\nLet W = XN\u2212k0\u22122S /X N\u2212k\u22122 Ij . Notice that the random variable W is independent of the noise variance \u03c32 and since N is fixed W is bounded as \u03c32 \u2192 0. Furthermore, lim\n\u03c32\u21920 \u03c3\u030220 = c\n(see Appendix B) and the right-hand side of the inequality in (50) grows unbounded as \u03c32 \u2192 0. Thus, we have\nlim \u03c32\u21920\nPr { EkIj > \u03b7k } = 0. (51)\nTherefore, using (49) and the result in (51), we have\nlim \u03c32\u21920\nPr ( EkI\u2217 > \u03b7k ) = 0, \u2200 k = k0 + 1, . . . ,K. (52)\nFinally, using the union bound, and the result in (52), we get\nPr {C1} =Pr { K\u22c2\nk=k0+1\nEkI\u2217 < \u03b7k\n}\n\u22651\u2212 K\u2211\nk=k0+1\nPr { EkI\u2217 > \u03b7k } \u2192 1, (53)\nas \u03c32 \u2192 0. (b) Misfitting case (S 6\u2282 S\u0302EBICR): Let Ij be any arbitrary jth subset belonging to the set of misfitted subsets of dimension k, i.e., Ikm. We consider the following inequality EBICR(S) < EBICR(Ij), Ij \u2208 Ikm, (54)\nwhere j = 1, . . . , t. Here, t denotes the total number of subsets in the set Ikm and t = ( p k ) if k < k0, otherwise t = ( p k ) \u2212(\np\u2212k0 \u2206 ) if k \u2265 k0, where \u2206 = k\u2212k0. Denoting XS = \u03c3\u03022S/\u03c32, rearranging and applying exponentiation we can express (54) as( XN\u2212k0\u22122S (\u03c3\u03022Ij ) N\u2212k\u22122 )( 1 N )\u2206(1+2\u03b6d)( 2\u03c0 \u03c3\u030220 )\u2206 < ( 1 \u03c32 )N\u2212k0\u22122 .\n(55)\nSimilar to the overfitting case, let EkIj denote the entire lefthand side and \u03b7 the right-hand side of (55). Also, let EkI\u2217 =\nmax Ij\u2208Ikm\n{ EkIj } for j = 1, . . . , t, where I\u2217 is the subset that\nleads to the maximum value of EkIj among all such subsets of dimension k. The condition C2 in (40) is satisfied as \u03c32 \u2192 0 under the event EkI\u2217 < \u03b7, for all k = 1, . . . ,K. Now, we can express the probability that EkI\u2217 < \u03b7 as\nPr ( EkI\u2217 < \u03b7 ) =Pr    t\u22c2\nj=1\n( EkIj < \u03b7 )   \n=\u21d2 Pr ( EkI\u2217 > \u03b7 ) \u2264tPr ( EkIj > \u03b7 ) , (56)\nwhere the inequality follows from the union bound. Now consider the following probability for any arbitrary subset Ij \u2208 Ikm\nPr ( EkIj > \u03b7 ) = Pr {( XN\u2212k0\u22122S (\u03c3\u03022Ij ) N\u2212k\u22122 )( 1 N )\u2206(1+2\u03b6d)\n\u00d7 ( 2\u03c0\n\u03c3\u030220\n)\u2206 > ( 1\n\u03c32\n)N\u2212k0\u22122} .\n(57)\nHere, XN\u2212k0\u22122S is independent of \u03c3 2 and N is fixed, therefore XN\u2212k0\u22122S is bounded as \u03c3 2 \u2192 0. Also \u03c3\u03022Ij \u2192 \u2016\u03a0\u22a5IjASxS\u201622/N in probability as \u03c32 \u2192 0 and since we are in the misfitting scenario, from Lemma 4 in Appendix D we have \u2016\u03a0\u22a5IjASxS\u201622/N > 0. Furthermore, lim\u03c32\u21920 \u03c3\u0302 2 0 = const. (see Appendix B) and the right-hand side of the inequality in (57) grows unbounded as \u03c32 \u2192 0. Hence,\nlim \u03c32\u21920\nPr { EkIj > \u03b7 } = 0. (58)\nUsing (56) and the result in (58) we get\nlim \u03c32\u21920\nPr { EkI\u2217 > \u03b7 } = 0, \u2200 k = 1, . . . ,K. (59)\nFinally, using the union bound and the result in (59), we get\nPr {C2} \u2265 1\u2212 K\u2211\nk=1\nPr { EkI\u2217 > \u03b7 } \u2192 1 as \u03c32 \u2192 0. (60)\nFrom (53) and (60) we can conclude that EBICR is consistent as \u03c32 \u2192 0, which proves Theorem 1."
        },
        {
            "heading": "C. Consistency as N \u2192 \u221e when \u03c32 is fixed (0 < \u03c32 <\u221e)",
            "text": "In this section, we prove the consistency of EBICR as the sample size N \u2192 \u221e given that \u03c32 is fixed and under the setting p = Nd for some d > 0. This is a common setting in the model selection literature (see, e.g., [12], [13], [29]). This leads to the following theorem.\nTheorem 2: Assume that p = Nd for some constant d > 0, the SNR is fixed and the matrix A satisfies (41). If K \u2265 k0, then Pr {EBICR(S) < EBICR(I)} \u2192 1 as N \u2192 \u221e for all I 6= S and card(I) = 1, . . . ,K under the condition \u03b6 > 1\u2212 1/2d.\nProof. As in the previous section, we have two parts of the proof. Part (a) is the overfitting case where we show that\nPr(C1) \u2192 1 as N \u2192 \u221e and part (b) is the misfitting case where we show that Pr(C2) \u2192 1 as N \u2192 \u221e. (a) Overfitting case (S \u2282 S\u0302EBICR): Let Ij \u2208 Iko be any overfitted subset of dimension k. Consider the following inequality\nEBICR(Ij) > EBICR(S), Ij \u2208 Iko . (61)\nDenoting \u2206 = k \u2212 k0 and rearranging (61) we get\n(N \u2212 k \u2212 2) ln ( \u03c3\u03022Ij \u03c3\u03022S ) +\u2206(1 + 2\u03b6d) lnN\n+\u2206 ln ( \u03c3\u030220 \u03c3\u03022S ) \u2212\u2206 ln 2\u03c0 > 0. (62)\nLet EkIj denote the entire left side of the inequality (62) and I\u2217 denote the subset that leads to the minimum value of EkIj among all such subsets of dimension k. Hence,\nEkI\u2217 = min Ij\u2208Iko\n{ EkIj } , j = 1, 2, . . . , ( p\u2212 k0 \u2206 ) . (63)\nThe condition C1 in (39) is satisfied as N \u2192 \u221e under the event EkI\u2217 > 0, for all k = k0 + 1, . . . ,K . Expanding the ratio we have\nln ( \u03c3\u03022Ij \u03c3\u03022S ) = ln ( eT\u03a0\u22a5Ije eT\u03a0\u22a5S e )\n= ln\n[ eT ( I\u2212\u03a0Ij +\u03a0S \u2212\u03a0S ) e\neT\u03a0\u22a5S e\n]\n= ln\n( eT\u03a0\u22a5S e\u2212 eT\u03a0Ij\\Se\neT\u03a0\u22a5S e\n)\n= ln ( 1\u2212 e T\u03a0Ij\\S\neT\u03a0\u22a5S e\n) , (64)\nwhere \u03a0Ij\\S = \u03a0Ij \u2212\u03a0S . Now we can write\nmin 1\u2264j\u2264T\n{ (N \u2212 k \u2212 2) ln ( \u03c3\u03022Ij \u03c3\u03022S )} =\n(N \u2212 k \u2212 2) ln  1\u2212 max 1\u2264j\u2264T {( eT\u03a0Ij\\Se ) /\u03c32 }\n( eT\u03a0\u22a5S e ) /\u03c32\n  , (65)\nwhere T = ( p\u2212k0 \u2206 ) . Now the term, (eT\u03a0Ij\\Se)/\u03c3\n2 \u223c \u03c72\u2206 (see Appendix C). Then from Lemma 2 in Appendix D we have the following upper bound\nmax 1\u2264j\u2264T\n{ (eT\u03a0Ij\\Se)/\u03c3 2 } \u2264 \u2206+ 2 \u221a \u2206\u03c8 lnT + 2\u03c8 lnT,\n(66)\nwith probability approaching one as N \u2192 \u221e if \u03c8 > 1. Now, for sufficiently large p = Nd we can write lnT = ln ( p\u2212k0 \u2206 ) \u2248 \u2206d lnN . This gives\nmax 1\u2264j\u2264T\n{\n(eT\u03a0Ij\\Se)/\u03c3 2\n}\n\u2264 \u2206+ 2\u2206 \u221a \u03c8d lnN + 2\u03c8\u2206d lnN\n= 2\u03c8\u2206d lnN ( 1 +\n1\u221a \u03c8d lnN + 1 2\u03c8d lnN\n)\n\u2248 2\u03c8\u2206d lnN, (67)\nas N grows large. Furthermore, the term in the denominator in (65), (eT\u03a0\u22a5S e)/\u03c3\n2 \u223c \u03c72N\u2212k0 and based on the law of large numbers tends to N \u2212 k0 \u2248 N . Therefore, using (67) in (65) and (N \u2212 k \u2212 2) \u2248 N under the large-N approximation we get\nmin 1\u2264j\u2264T\n{ N ln ( \u03c3\u03022Ij \u03c3\u03022S )} \u2265 N ln ( 1\u2212 2\u2206\u03c8d lnN N )\n\u2248 \u22122\u2206\u03c8d lnN, (68)\nwhere the last approximation follows by linearization of the logarithm for small 2\u2206\u03c8d lnN/N value. Thus, we can write\nEkI\u2217 \u2265 \u22122\u2206\u03c8d lnN +\u2206(1 + 2\u03b6d) lnN +\u2206 ln ( \u03c3\u030220 \u03c3\u03022S )\n\u2212\u2206 ln 2\u03c0\n= \u2206(1 + 2\u03b6d\u2212 2\u03c8d) lnN +\u2206 ln ( \u03c3\u030220 \u03c3\u03022S ) \u2212\u2206 ln 2\u03c0.\n(69)\nSince limN\u2192\u221e \u03c3\u0302 2 0 = const. > 0 (see Appendix B) and limN\u2192\u221e \u03c3\u0302 2 S = \u03c3\n2 (see Appendix C), EkI\u2217 \u2192 \u221e as N \u2192 \u221e for all k = k0+1, . . . ,K under the condition 1+2\u03b6d\u22122\u03c8d > 0 for any \u03c8 > 1. Hence, the lower bound on \u03b6 becomes\n\u03b6 > 1\u2212 1 2d . (70)\nFrom the above analysis we can say that\nlim N\u2192\u221e\nPr { EkI\u2217 < 0 } = 0, \u2200 k = k0 + 1, . . . ,K. (71)\nFinally, using the union bound and the result in (71) we can express the probability of C1 (39) happening as\nPr {C1} =Pr { K\u22c2\nk=k0+1\nEkI\u2217 > 0\n}\n\u2265 1\u2212 K\u2211\nk=k0+1\nPr { EkI\u2217 < 0 } \u2192 1 (72)\nas N \u2192 \u221e. (b) Misfitting case (S 6\u2282 S\u0302EBICR): Let Ij \u2208 Ikm be any misfitted subset of dimension k. Consider the following inequality\nEBICR(Ij) > EBICR(S), Ij \u2208 Ikm. (73)\nDenoting \u2206 = k \u2212 k0 and rearranging (73) we get\n(N \u2212 k \u2212 2) ln ( \u03c3\u03022Ij \u03c3\u03022S ) + (1 + 2\u03b6d)\u2206 lnN\n+\u2206 ln ( \u03c3\u030220 \u03c3\u03022S ) +\u2206 ln ( 1 2\u03c0 ) > 0.\n(74)\nLet EkIj denote the entire left hand side of the inequality in (74) and I\u2217 denote the subset that generates the minimum value of EkIj among all such subsets of dimension k. Then we have\nEkI\u2217 = min Ij\u2208Ikm\n{ EkIj } , j = 1, 2, . . . , T, (75)\nwhere T = ( p k ) if k < k0 otherwise T = ( p k ) \u2212 ( p\u2212k0 \u2206 ) if k \u2265 k0. The condition C2 in (40) is satisfied as N \u2192 \u221e under the event EkI\u2217 > 0, for all k = 1, . . . ,K. Now, let u = E[y] = ASxS . Using this, the ratio \u03c3\u03022 Ij\n\u03c3\u03022 S\ncan be expanded\nas\n\u03c3\u03022Ij \u03c3\u03022S = yT\u03a0\u22a5Ijy yT\u03a0\u22a5S y = (u+ e)T\u03a0\u22a5Ij (u+ e) eT\u03a0\u22a5S e\n= uT\u03a0\u22a5Iju+ 2\u03c3\n\u221a uT\u03a0\u22a5Iju \u00b7 Zj + eT\u03a0 \u22a5 Ije\neT\u03a0\u22a5S e , (76)\nwhere\nZj = uT\u03a0\u22a5Ije\n\u03c3 \u221a uT\u03a0\u22a5Iju \u223c N (0, 1). (77)\nNow\nmin 1\u2264j\u2264T\n{ \u03c3\u03022Ij/\u03c3\u0302 2 S } =\nmin 1\u2264j\u2264T\n{\nu T \u03a0 \u22a5 Iju+ 2\u03c3\n\u221a\nuT\u03a0 \u22a5 Ij u \u00b7 Zj + e T \u03a0 \u22a5 Ije\n}/\ne T \u03a0 \u22a5 S e\n\u2265 [ min\n1\u2264j\u2264T\n{ uT\u03a0\u22a5Iju } + \u03c32 min\n1\u2264j\u2264T\n{ eT\u03a0\u22a5Ije/\u03c3 2 }\n\u2212 2\u03c3 \u221a\nmax 1\u2264j\u2264T\n{ uT\u03a0\u22a5Iju } \u00b7 max 1\u2264j\u2264T\n{ Zj }]/ eT\u03a0\u22a5S e. (78)\nIn the misfitting scenario we have two cases: (i) k < k0 (ii) k \u2265 k0. We consider case (i) in our further analysis, which also encapsulates case (ii). For k < k0 we have lnT = ln ( p k ) \u2248 kd lnN . Therefore, using the result in Lemma 2 we have the following lower bound under large-N approximation\nmin 1\u2264j\u2264T\n{ eT\u03a0\u22a5Ije/\u03c3 2 } = eTe/\u03c32 \u2212 max\n1\u2264j\u2264T\n{ eT\u03a0Ije/\u03c3 2 }\n\u2265 N \u2212 2\u03c8\u2032kd lnN, (79)\nwhere \u03c8\u2032 > 1 and eTe/\u03c32 \u2248 N for large-N . Furthermore, from the result in Lemma 3 we have the following upper bound\nmax 1\u2264j\u2264T\n{Zj} \u2264 \u221a 2\u03c8\u2032kd lnN, (80)\nwhere \u03c8\u2032 > 1. Now, let Cmin = min 1\u2264j\u2264T\n{ uT\u03a0\u22a5Iju } and Cmax =\nmax 1\u2264j\u2264T\n{ uT\u03a0\u22a5Iju } . Also as N \u2192 \u221e we can approximate (N\u2212\nk \u2212 2) \u2248 N and eT\u03a0\u22a5S e \u2248 \u03c32N . Using this, and the results in (79) and (80) we get\nmin 1\u2264j\u2264T\n{ N ln ( \u03c3\u03022Ij \u03c3\u03022S )} = N ln [ min 1\u2264j\u2264T { \u03c3\u03022Ij \u03c3\u03022S }]\n\u2265 N ln [{ Cmin \u2212 2\u03c3 \u221a Cmax \u00b7 \u221a 2\u03c8\u2032kd lnN\n+ \u03c32 (N \u2212 2\u03c8\u2032kd lnN) }/ \u03c32N ] . (81)\nNow, observe that Cmin = u T\u03a0\u22a5I\u2217u = x T SA T S\u03a0 \u22a5 I\u2217ASxS . Since, we are in the misfitting scenario, from Lemma 4, in Appendix D, we can express Cmin = Nbmin where bmin =\nO(1) > 0. Similarly, Cmax = Nbmax where bmax = O(1) > 0 and 0 < bmin \u2264 bmax. Hence, we can rewrite (81) as\nmin 1\u2264j\u2264T\n{ N ln ( \u03c3\u03022Ij \u03c3\u03022S )} \u2265\nN ln ( 1 +\nbmin \u03c32 \u2212 2 \u221a bmax \u03c3 \u221a 2\u03c8\u2032kd lnN N \u2212 2\u03c8 \u2032kd lnN N )\n\u2248 N ln ( 1 +\nbmin \u03c32\n) (82)\nas N grows large. For k < k0, we get \u2206 < 0, therefore, in this case we have\nEkI\u2217 \u2265 N ln ( 1 +\nbmin \u03c32\n) \u2212 |\u2206|(1 + 2\u03b6d) lnN\n\u2212|\u2206| ln (\n\u03c3\u030220 2\u03c0\u03c3\u03022S\n) \u2192 \u221e\n(83)\nas N \u2192 \u221e for all k = 1, . . . ,K , since N ln(1 + bmin/\u03c32) is the dominating term as it tends to infinity much faster than the lnN term and limN\u2192\u221e \u03c3\u0302 2 0 = const. > 0 (see Appendix B) and limN\u2192\u221e \u03c3\u0302 2 S = \u03c3 2 (see Appendix C). From the above analysis we can say that\nlim N\u2192\u221e\nPr { EkI\u2217 < 0 } = 0, \u2200 k = 1, . . . ,K. (84)\nFinally, using the union bound and the result in (84) we can express the probability of C2 (40) happening as\nPr {C2} = Pr { K\u22c2\nk=1\nEkI\u2217 > 0\n}\n\u2265 1\u2212 K\u2211\nk=1\nPr { EkI\u2217 < 0 } \u2192 1 as N \u2192 \u221e. (85)\nFrom (72) and (85) we can conclude that EBICR is consistent as N \u2192 \u221e, which proves Theorem 2."
        },
        {
            "heading": "D. Discussion on the Hyperparameter \u03b6",
            "text": "If \u03b6 is too large, it will lead to underfitting issues. This is evident from (83) where a large value of \u03b6 may force the overall sum to become negative especially for smaller N values. On the contrary, if \u03b6 is too small, it will lead to overfitting issues as the penalty may not be sufficiently\nlarge to compensate for the overparameterization due to large parameter space."
        },
        {
            "heading": "V. PREDICTOR SELECTION ALGORITHMS",
            "text": "In the high-dimensional scenario, when p is large, it is infeasible to perform model selection in the conventional manner. For a design matrix with parameter dimension p, the number of possible candidate models is 2p \u2212 1. Hence, the candidate model space grows exponentially with p and we cannot afford to calculate model score for all possible models. Therefore, to perform model selection, we combine a model\nselection criterion with a predictor selection (support recovery) algorithm such as OMP or LASSO (least absolute shrinkage\nand selection operator) [30]. The goal of predictor selection\nis to pick a subset of important predictors from the entire set\nAlgorithm 1 OMP with K iterations\nInputs: Design matrix A, measurement vector y. Initialization: \u2016aj\u20162 = 1 \u2200j, r0 = y, S0OMP = \u2205 for i = 1 to K do\nFind next column index: di = argmax j\n\u2223\u2223aTj ri\u22121 \u2223\u2223\nAdd current index: SiOMP = Si\u22121OMP \u222a {di} Update residual: ri = ( IN \u2212\u03a0Si\nOMP\n) y\nend for Output: OMP generated index sequence SKOMP\nof p predictors. In this context, the most important predictors refer to the positions of the nonzero elements of the input signal x. Thus, predictor selection reduces the cardinality of the candidate model space to some upper bound K such that k0 \u2264 K \u226a N under the assumption of a sparse parameter vector. This enables us to apply the model selection criterion\non the smaller set of candidate models to pick the best model. The OMP algorithm is shown in Algorithm 1. To perform\nmodel selection, we combine OMP with EBICR as shown in Algorithm 2.\nLASSO is a shrinkage method for variable selection/estimation in linear regression models developed by Tib-\nshirani [30]. Given the linear model in (1), the LASSO solution\nfor x for a particular choice of the regularization parameter \u03bb \u2265 0 is obtained as\nx\u0302lasso(\u03bb) = min x\u2208Rp\n{ 1\n2N \u2016y\u2212Ax\u201622 + \u03bb\u2016x\u20161\n} , (86)\nwhere \u2016\u00b7\u20161 denotes the l1 norm. The parameter \u03bb determines the level of sparsity. When \u03bb \u2192 \u221e the objective function in (86) attains the minimum with x\u0302lasso(\u03bb) being a zero vector. As we gradually lower the \u03bb value, the number of nonzero components in x\u0302lasso(\u03bb) starts increasing. Model selection combining LASSO and EBICR can be performed as shown in Algorithm 3. Gradually decrease \u03bb from a high value so that the number of non-zero components in x\u0302lasso(\u03bb) gradually increases. Therefore, for each decreasing unique value of \u03bb say \u03bbi, we acquire a different solution x\u0302lasso(\u03bbi), with increasing support and thus obtaining a sequence of candidate models with maximum cardinality K . The value of EBICR is computed for each of the candidate models and the model\ncorresponding to the smallest EBICR score is selected as the final model. A most useful method for solving LASSO in our context is the (modified) least angle regression (LARS)\nalgorithm [31], since it also provides the required sequence of regularization parameters for which the support changes.\nAlgorithm 2 Model selection combining EBICR with OMP\nRun OMP for K iterations to obtain SKOMP for k = 1 to K do\nI = SkOMP Compute EBICR(I)\nend for Estimated true support: S\u0302EBICR = argmin I {EBICR(I)}\nAlgorithm 3 Model selection combining EBICR with LASSO\nCompute LASSO estimates {x\u0302lasso(\u03bb1), . . . , x\u0302lasso(\u03bbKmax)} where card(supp (x\u0302lasso(\u03bbKmax))) = K for i = 1 to Kmax do\nI = supp (x\u0302lasso(\u03bbi)) Compute EBICR(I)\nend for Estimated true support: S\u0302EBICR = argmin I {EBICR(I)}"
        },
        {
            "heading": "VI. SIMULATION RESULTS",
            "text": "In this section, we provide numerical simulation results to illustrate the empirical performance of EBICR. The performance of EBICR is compared with the \u2018oracle\u2019, EBIC, EFIC and MBT. However, the performance comparison with the RRT [18] method is dropped since it behaves quite similar\nto MBT (see [19] for details). The \u2018oracle\u2019 criterion assumes a priori knowledge of the true cardinality k0. Thus, the model selection performance of the \u2018oracle\u2019 provides the upper bound\non the model selection performance that can be achieved using a particular predictor selection algorithm and for a given set of\ndata settings. Additionally, we also provide simulation results\nto highlight the drawbacks of classical methods for model selection in high-dimensional linear regression models with\na sparse parameter vector."
        },
        {
            "heading": "A. General Simulation Setup",
            "text": "In the simulations, we consider the model y = Ax+ e, where the design matrix A \u2208 RN\u00d7p is generated with independent entries following normal distribution N (0, 1). Since x is assumed to be sparse, we choose k0 = 5. Furthermore, without loss of generality, we assume that the true support is S = [1, 2, 3, 4, 5], therefore, xS = [x1, x2, x3, x4, x5]T and AS = [a1, a2, a3, a4, a5]. This implies that the elements of x follows xk 6= 0 for k = 1, . . . , k0 and xk = 0 for k > k0. The SNR in dB is SNR (dB) = 10 log10(\u03c3 2 s/\u03c3\n2), where \u03c32s and \u03c32 denote signal and true noise power, respectively. The signal power is computed as \u03c32s = ||ASxS ||22/N . Based on \u03c32s and the chosen SNR (dB), the noise power is set as \u03c32 = \u03c32s/10\nSNR (dB)/10. Using this \u03c32, the noise vector e is generated following N (0, \u03c32IN ). The probability of correct model selection (PCMS) is estimated over 1000 Monte Carlo trials. To maintain randomness in the data, a new design matrix A is generated at each Monte Carlo trial. OMP is used\nfor predictor selection for its simplicity and wider range of\napplicability."
        },
        {
            "heading": "B. Tuning Parameter Selection",
            "text": "An important step in model selection is the choice of the\ntuning parameter. As mentioned earlier, too small or large\nvalues of the tuning parameter can cause severe performance degradation in certain scenarios. Fig. 1 shows a performance comparison of EBICR for four different values of \u03b6 (0.4, 0.6, 1, and 2). Here, we set p = Nd where d = 1.1. Hence, from Theorem 2 we require \u03b6 > 1 \u2212 1/2d = 0.55 to achieve consistency. From the figure we see that for \u03b6 = 0.4, the\nperformance of EBICR degrades after a certain point with increasingN , which justifies the theory. For all other \u03b6 > 0.55, the performances improve with increasing N . For \u03b6 = 0.6, which is very close to the lower bound, the convergence to\ncorrect selection probability one is slow and will require a very large sample size. For, \u03b6 = 2, the performance suffers (due to underfitting) in the low N regime, but do achieve perfect selection as N increases. In this case, \u03b6 = 1 provides a much better overall performance for a broader range of N . A similar trend as in EBICR is observed even in EBIC and EFIC for different choices of \u03b3 and c. Hence, to maintain fairness, the following tuning parameter settings are considered for further analysis: \u03b6 = 1 (EBICR), c = 1 (EFIC) and \u03b3 = 1 (EBIC). For MBT [19], lim\nN\u2192\u221e PCMS \u2192 1 as \u03b2 \u2192 1. Hence, we choose\n\u03b2 = 0.999."
        },
        {
            "heading": "C. Model Selection with Classical Methods in HighDimensional Setting",
            "text": "In this section, we present simulations results for model selection using classical methods in high-dimensional lin-\near regression models and compare their performances with\nEBICR. The purpose of these results is to highlight the limitations of the classical methods in dealing with large-p small-N scenarios. The classical methods used here are BIC [7], B\u0303ICN,SNR[23], BICR[24], gMDL [9], and PAL [11].\nIn the simulation, we consider the true parameter vector to be xS = [5, 4, 3, 2, 1] T . Fig. 2 presents the plot for PCMS versus N for SNR = 30 dB with p = Nd where d = 1.1. The figure shows that EBICR (\u03b6 = 1) clearly surpasses the classical methods with huge differences in performance. In general, when p is fixed and N \u2192 \u221e, the classical methods are consistent [24]. However, when p is varying and grows exponentially with N , the consistency attribute does not hold any longer, hence, we see the decreasing performance trend\nin Fig. 2.\nFig. 3 illustrates the PCMS versus SNR in dB for fixed N = 100 and p = 500. This gives d = log(p)/ log(N) \u2248 1.35, hence, \u03b6 > 1\u22121/2d \u2248 0.63. The first major observation from the figure is that EBICR (\u03b6 = 1) clearly outperforms all the classical methods by a huge margin. Secondly, for the considered setting, the performances of BICR and gMDL are quite similar followed by B\u0303ICN,SNR. The criteria BICR, gMDL and B\u0303ICN,SNR do achieve convergence to detection probability one but at the expense of very high values of SNR. The\nperformances of PAL and BIC are extremely poor in this case, even in the high-SNR regions."
        },
        {
            "heading": "D. Model Selection with the Latest Methods in HighDimensional Setting",
            "text": "In Section VI-C, we highlighted the drawbacks of classical methods in model selection under the high-dimensional\nsetting. We observed that the performance of the classical methods collapses when p grows exponentially with N and the consistency property breaks down. In this section, we present\nsimulation results for model selection comparing EBICR to the existing state-of-the-art methods, designed to deal with the large-p small-N scenarios. 1) Model Selection versus SNR: To highlight the scaleinvariant and consistent behaviour of EBICR, we consider two scenarios. In the first scenario, we assume the true parameter vector to be xS = [0.05, 0.04, 0.03, 0.02, 0.01] T and in the second scenario, we assume xS = [50, 40, 30, 20, 10] T . Note that in the simulations we compute the noise variance \u03c32 based on the chosen SNR level and the current signal power value \u03c32s = \u2016ASxS\u201622 / N . To simulate the probability of correct model selection versus SNR in a high-dimensional setting we\nfixed N = 55 and p = 1000. This gives d = log(p)/ log(N) \u2248 1.724, hence, \u03b6 > 1\u2212 1/2d \u2248 0.71.\nFig. 4 shows the empirical PCMS versus SNR (dB). Fig. 4a and Fig. 4b correspond to xS = [0.05, 0.04, 0.03, 0.02, 0.01] and xS = [50, 40, 30, 20, 10], respectively. Both the figures depict fixed N increasing SNR scenario. Comparing the figures, the first clear observation is that unlike the other\ncriteria, the behaviour of EFIC is not identical for the two different xS given that the other parameters viz, N , p and k0 are constant and the performance is evaluated for the same SNR range. This illustrates the scaling problem present in EFIC that leads to either high underfitting or overfitting\nissues. This behavior or EFIC can be explained as follows. The data dependent penalty term (DDPT) of EFIC is DDPT = \u2212(k + 2) ln\u2016\u03a0\u22a5I y\u201622, whose overall value depends on the value \u2016\u03a0\u22a5I y\u201622, which in turn is influenced by the signal and noise powers \u03c32s and \u03c3\n2, respectively. If \u2016\u03a0\u22a5I y\u201622 \u226a 1, then DDPT \u226b 0, which may blow the overall penalty to a large value leading to underfitting issues. This is most likely the case when xS = [0.05, 0.04, 0.03, 0.02, 0.01] T (Fig. 4a). On the contrary if \u2016\u03a0\u22a5I y\u201622 \u226b 1, then DDPT \u226a 0, thus lowering the overall penalty leading to overfitting issues (when xS = [50, 40, 30, 20, 10] T , Fig. 4b). The second major observation is that EBIC is inconsistent when SNR is high but N is small and fixed. This behaviour of EBIC is already\nreported in [13]. In general, EFIC, MBT (for \u03b2 \u2192 1) and EBICR are consistent for increasing SNR scenarios given that N is fixed, but while EBICR and MBT are invariant to data scaling EFIC is not.\n2) Model Selection versus N : Fig. 5 illustrates the empirical PCMS versus N for SNR = 6 dB, p = 1000 and xS = [50, 40, 30, 20, 10] T . It depicts a low-SNR increasing N scenario. It is clearly seen that compared to the other criteria, EFIC suffers from the scaling issue and requires a\nlarge sample size to achieve detection probability one. Among\nall the criteria, the performance of EBIC and EBICR are closest to the oracle. Furthermore, observe that the performance of\nEBICR and EBIC are more or less alike for the current setting. This is primarily because the SNR is low (6dB) hence the (k + 2) ln(\u03c3\u030220/\u03c3\u0302 2 I) term of EBICR behaves very close to a O(1) quantity for k \u2265 k0. Thus, for low SNR scenarios, the penalties of EBIC and EBICR are similar and as such the behaviour of these two criteria overlaps in this case. However,\nnote that this is not true in the high-SNR cases, which will be evident from the discussion following Fig. 6.\nThe plots shown in Fig. 4 and Fig. 5 represent fixed-N increasing-SNR and low-SNR increasing-N scenarios, respectively. In Fig. 6, we present a high-SNR increasing-N case. Here, we consider a varying parameter space such that p = Nd where d = 1.3. It is clearly observed that for high-SNR\nscenarios, EBICR and MBT provide much faster convergence to oracle behaviour as compared to EBIC that requires higher sample size to achieve detection probability one. Furthermore,\nwe also notice that EFIC suffers from a higher false selection error and performs worse than EBIC in a certain region of the\nsample size. This clearly shows the effects of scaling in the\nbehaviour of EFIC."
        },
        {
            "heading": "VII. CONCLUSION",
            "text": "In this paper, we provided a new criterion, which is an\nextension of BICR, to handle model selection in sparse highdimensional linear regression models employing sparse methods for predictor selection. The extended version is named as\nEBICR, where the subscript \u2018R\u2019 stands for robust and it is a scale-invariant and consistent model selection criterion. Ad-\nditionally, we analytically examined the behaviour of EBICR as \u03c32 \u2192 0 and as N \u2192 \u221e. In both cases, it is shown that the probability of detecting the true model approaches one.\nThe paper further highlighted the data scaling issue present in\nEFIC, which is a consistent criterion for both large sample size and high-SNR scenarios. Extensive simulation results show\nthat the performance of EBICR is either similar or superior to that of EBIC, EFIC and MBT."
        },
        {
            "heading": "APPENDIX A",
            "text": "Lemma 1: Let y be a N \u00d7 1 dimensional vector following y \u223c N (\u00b5, \u03c32IN ) and \u03a0 be a N \u00d7N symmetric, idempotent matrix with rank(\u03a0) = r. Then the ratio yT\u03a0y/\u03c32 has a non-central chi-square distribution \u03c72r(\u03bb) with r degrees of freedom and non-centrality parameter \u03bb = \u00b5T\u03a0\u00b5/\u03c32 (see, e.g., Chapter 5 of [32])."
        },
        {
            "heading": "APPENDIX B",
            "text": "STATISTICAL ANALYSIS OF THE FACTOR \u03c3\u030220\nFrom the generating model (1), the true data vector follows\ny \u223c N ( ASxS , \u03c3 2IN ) . Consider the factor \u03c3\u030220 , which is defined as\n\u03c3\u030220 = \u2016y\u201622 N =\n( \u03c32\nN\n) yT INy\n\u03c32 . (87)\nFrom Lemma 1 in Appendix A we have\nyT INy\n\u03c32 \u223c \u03c72N (\u03bb) where \u03bb = \u2016ASxS\u201622 \u03c32 . (88)\nThis implies that ( N \u03c32 ) \u03c3\u030220 \u223c \u03c72N (\u03bb). Therefore, the mean and variance of \u03c3\u030220 are:\nE[\u03c3\u030220 ] = \u03c32\nN (N + \u03bb) = \u03c32 + \u2016ASxS\u201622 N\nVar[\u03c3\u030220 ] = 2 \u03c34\nN2 (N + 2\u03bb) = 2\n\u03c34 N + 4 \u03c32 N2 \u2016ASxS\u201622.\n(89)\nHence, for a fixed N ,\nlim \u03c32\u21920\nE[\u03c3\u030220 ] = \u2016ASxS\u201622\nN & lim \u03c32\u21920 Var[\u03c3\u030220 ] = 0. (90)\nFurther, when SNR or \u03c32 is fixed, using the assumption\nlimN\u2192\u221e\n{ A\nT S AS N } = MS we get\nlim N\u2192\u221e\nE[\u03c3\u030220 ] = \u03c3 2 + xTSMSxS & lim N\u2192\u221e Var[\u03c3\u030220 ] = 0,\n(91)\nwhere MS is a bounded positive definite matrix and as such xTSMSxS = O(1) as N grows large."
        },
        {
            "heading": "APPENDIX C",
            "text": "STATISTICAL ANALYSIS OF \u03c3\u03022I WHEN S \u2286 I The noise variance estimate under hypothesis HI can be\nrewritten as\n\u03c3\u03022I =\n( \u03c32\nN\n) yT\u03a0\u22a5I y\n\u03c32 . (92)\nThe true model u = ASxS lies in a linear subspace spanned by the columns of AS . Consequently, for I \u2287 S we have \u03a0\u22a5I u = 0. This implies that y T\u03a0\u22a5I y = e T\u03a0\u22a5I e. Thus we have,\nyT\u03a0\u22a5I y\n\u03c32 =\neT\u03a0\u22a5I e\n\u03c32 \u223c \u03c72N\u2212k (Using Lemma 1), (93)\nwhere k = card(I) \u2265 k0. This implies that ( N \u03c32 ) \u03c3\u03022I \u223c \u03c72N\u2212k. Therefore, the mean and variance of \u03c3\u03022I for I \u2287 S are:\nE[\u03c3\u03022I ] = \u03c32\nN (N \u2212 k) & Var[\u03c3\u03022I ] = 2\n\u03c34 N2 (N \u2212 k). (94)\nHence, when \u03c32 is a constant,\nlim N\u2192\u221e\nE[\u03c3\u03022I ] = \u03c3 2 & lim N\u2192\u221e Var[\u03c3\u03022I ] = 0. (95)"
        },
        {
            "heading": "APPENDIX D",
            "text": "Lemma 2: Let Zmax = max i\n{ Zi }m i=1 where Z1, Z2, . . . , Zm\nis a sequence of identically distributed random variables\n(not necessarily independent) having a Chi-square distribution with k degrees of freedom where k < m. Then Zmax \u2264 k + 2 \u221a k\u03c8 lnm + 2\u03c8 lnm for some constant \u03c8 > 1 with probability approaching one as m\u2192 \u221e. Proof: From the union bound we have\nPr (Zmax \u2264 \u03b7) \u2265 1\u2212mPr (Zi \u2265 \u03b7) . (96) Since Zi \u223c \u03c72k, then from the Chi-square tail bound (Lemma 1 of [33]) we have the following result\nPr ( Zi \u2265 k + 2 \u221a kt+ 2t ) \u2264 e\u2212t. (97)\nSetting t = \u03c8 lnm in (97) where \u03c8 > 1 we get\nPr ( Zi \u2265 k + 2 \u221a k\u03c8 lnm+ 2\u03c8 lnm ) \u2264 e\u2212\u03c8 lnm = m\u2212\u03c8.\n(98)\nUsing (98) in (96) we get\nPr ( Zmax \u2264 k + 2 \u221a k\u03c8 lnm+ 2\u03c8 lnm ) \u2265 1\u2212 1\nm\u03c8\u22121 . (99)\nTherefore, Zmax \u2264 k+2 \u221a k\u03c8 lnm+2\u03c8 lnm with probability approaching one as m\u2192 \u221e if \u03c8 > 1. Lemma 3: Let Xmax = max\ni\n{ Xi }m i=1 where\nX1, X2, . . . , Xm is a sequence of identically distributed\nrandom variables (not necessarily independent) having a\nGaussian distribution with zero mean and variance one. Then Xmax \u2264 \u221a 2 lnm with probability approaching one as m\u2192 \u221e. Proof: From the union bound we have\nPr (Xmax \u2264 \u03b7) \u2265 1\u2212mPr (Xi \u2265 \u03b7) . (100) Since Xi \u223c N (0, 1), from the Gaussian tail bound we have\nPr (Xi \u2265 \u03b7) \u2264 1\n\u03b7\ne\u2212\u03b7 2/2\n\u221a 2\u03c0 , (101)\nfor all \u03b7 > 0. Setting \u03b7 = \u221a 2 lnm in (101) we get\nPr ( Xi \u2265 \u221a 2 lnm ) \u2264 m \u22121\n2 \u221a \u03c0 lnm . (102)\nUsing (102) in (100) we get\nPr ( Xmax \u2264 \u221a 2 lnm ) \u2265 1\u2212 1\n2 \u221a \u03c0 lnm . (103)\nTherefore, Xmax \u2264 \u221a 2 lnm with probability approaching one as m\u2192 \u221e. Lemma 4: For any arbitrary support I \u2208 Ikm \u2208 M, under the asymptotic identifiability condition in (41) the following inequality holds \u2225\u2225\u03a0\u22a5I ASxS \u2225\u22252 2 > 0.\nProof: Let S \u2032 = {S \\ I}. The true support S can be split into two disjoint subsets as S = {S \u2229 I} \u222a {S \\ I}. Since span(AS\u2229I) \u2282 span(AI) we have\n\u2016\u03a0\u22a5I ASxS\u201622 =\u2016\u03a0\u22a5IAS\u2032xS\u2032\u201622 =NxTS\u2032 ( N\u22121ATS\u2032\u03a0 \u22a5 I AS\u2032 ) xS\u2032 .\nNow, consider the matrix M = [ AS\u2032 AI ] where card(S \u2032) \u2264 K and card(I) \u2264 K , such that card(S \u2032 \u222a I) \u2264 2K . Under the assumption (41)\nN\u22121MTM = N\u22121 [ ATS\u2032AS\u2032 A T S\u2032AI\nATIAS\u2032 A T IAI\n] (104)\nis a bounded positive definite matrix. Then the Schur complement of the block matrix ATIAI is\nN\u22121 [ ATS\u2032AS\u2032 \u2212ATS\u2032AI(ATIAI)\u22121ATIAS\u2032 ]\n=N\u22121ATS\u2032\u03a0 \u22a5 I AS\u2032\nis also positive definite and bounded as N \u2192 \u221e. Let M\u0303 = N\u22121ATS\u2032\u03a0 \u22a5 IAS\u2032 , then, x T S\u2032M\u0303xS\u2032 = b (say) = O(1) > 0. Hence, \u2016\u03a0\u22a5IASxS\u201622 = Nb > 0 for all I \u2208 Ikm \u2208 M."
        }
    ],
    "title": "Robust Information Criterion for Model Selection in Sparse High-Dimensional Linear Regression Models",
    "year": 2022
}