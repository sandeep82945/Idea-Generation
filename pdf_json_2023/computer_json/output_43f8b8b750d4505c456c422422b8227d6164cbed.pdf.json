{
    "abstractText": "Generating 3D human motion based on textual descriptions has been a research focus in recent years. It requires the generated motion to be diverse, natural, and conform to the textual description. Due to the complex spatio-temporal nature of human motion and the difficulty in learning the cross-modal relationship between text and motion, text-driven motion generation is still a challenging problem. To address these issues, we propose AttT2M, a two-stage method with multi-perspective attention mechanism: body-part attention and global-local motion-text attention. The former focuses on the motion embedding perspective, which means introducing a body-part spatiotemporal encoder into VQ-VAE to learn a more expressive discrete latent space. The latter is from the crossmodal perspective, which is used to learn the sentencelevel and word-level motion-text cross-modal relationship. The text-driven motion is finally generated with a generative transformer. Extensive experiments conducted on HumanML3D and KIT-ML demonstrate that our method outperforms the current state-of-the-art works in terms of qualitative and quantitative evaluation, and achieve finegrained synthesis and action2motion. Our code is in https://github.com/ZcyMonkey/AttT2M.",
    "authors": [
        {
            "affiliations": [],
            "name": "Chongyang Zhong"
        },
        {
            "affiliations": [],
            "name": "Lei Hu"
        },
        {
            "affiliations": [],
            "name": "Zihao Zhang"
        },
        {
            "affiliations": [],
            "name": "Shihong Xia"
        }
    ],
    "id": "SP:bbf2f2a0e9a154508a75237dbf76303ee88dc75d",
    "references": [
        {
            "authors": [
                "Kfir Aberman",
                "Peizhuo Li",
                "Dani Lischinski",
                "Olga Sorkine- Hornung",
                "Daniel Cohen-Or",
                "Baoquan Chen"
            ],
            "title": "Skeletonaware networks for deep motion retargeting",
            "venue": "ACM Transactions on Graphics (TOG),",
            "year": 2020
        },
        {
            "authors": [
                "Hyemin Ahn",
                "Timothy Ha",
                "Yunho Choi",
                "Hwiyeon Yoo",
                "Songhwai Oh"
            ],
            "title": "Text2action: Generative adversarial synthesis from language to action",
            "venue": "IEEE International Conference on Robotics and Automation (ICRA),",
            "year": 2018
        },
        {
            "authors": [
                "Chaitanya Ahuja",
                "Louis-Philippe Morency"
            ],
            "title": "Language2pose: Natural language grounded pose forecasting",
            "venue": "In 2019 International Conference on 3D Vision",
            "year": 2019
        },
        {
            "authors": [
                "Sadegh Aliakbarian",
                "Fatemeh Sadat Saleh",
                "Mathieu Salzmann",
                "Lars Petersson",
                "Stephen Gould"
            ],
            "title": "A stochastic conditioning scheme for diverse human motion prediction",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Nikos Athanasiou",
                "Mathis Petrovich",
                "Michael J Black",
                "G\u00fcl Varol"
            ],
            "title": "Teach: Temporal action composition for 3d humans",
            "venue": "In International Conference on 3D Vision",
            "year": 2022
        },
        {
            "authors": [
                "Emad Barsoum",
                "John Kender",
                "Zicheng Liu"
            ],
            "title": "Hp-gan: Probabilistic 3d human motion prediction via gan",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition workshops,",
            "year": 2018
        },
        {
            "authors": [
                "Uttaran Bhattacharya",
                "Nicholas Rewkowski",
                "Abhishek Banerjee",
                "Pooja Guhan",
                "Aniket Bera",
                "Dinesh Manocha"
            ],
            "title": "Text2gestures: A transformer-based network for generating emotive body gestures for virtual agents",
            "year": 2021
        },
        {
            "authors": [
                "Simon Clavet"
            ],
            "title": "Motion matching and the road to next-gen animation",
            "venue": "In Proc. of GDC,",
            "year": 2016
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova. Bert"
            ],
            "title": "Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805,",
            "year": 2018
        },
        {
            "authors": [
                "Katerina Fragkiadaki",
                "Sergey Levine",
                "Panna Felsen",
                "Jitendra Malik"
            ],
            "title": "Recurrent network models for human dynamics",
            "venue": "In Proceedings of the IEEE International Conference on Computer Vision,",
            "year": 2015
        },
        {
            "authors": [
                "Anindita Ghosh",
                "Noshaba Cheema",
                "Cennet Oguz",
                "Christian Theobalt",
                "Philipp Slusallek"
            ],
            "title": "Synthesis of compositional animations from textual descriptions",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2021
        },
        {
            "authors": [
                "Chuan Guo",
                "Shihao Zou",
                "Xinxin Zuo",
                "Sen Wang",
                "Wei Ji",
                "Xingyu Li",
                "Li Cheng"
            ],
            "title": "Generating diverse and natural 3d human motions from text",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Chuan Guo",
                "Xinxin Zuo",
                "Sen Wang",
                "Li Cheng"
            ],
            "title": "Tm2t: Stochastic and tokenized modeling for the reciprocal generation of 3d human motions and texts",
            "venue": "In Computer Vision\u2013 ECCV 2022: 17th European Conference,",
            "year": 2022
        },
        {
            "authors": [
                "Chuan Guo",
                "Xinxin Zuo",
                "Sen Wang",
                "Shihao Zou",
                "Qingyao Sun",
                "Annan Deng",
                "Minglun Gong",
                "Li Cheng"
            ],
            "title": "Action2motion: Conditioned generation of 3d human motions",
            "venue": "In Proceedings of the 28th ACM International Conference on Multimedia,",
            "year": 2020
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Daniel Holden",
                "Oussama Kanoun",
                "Maksym Perepichka",
                "Tiberiu Popa"
            ],
            "title": "Learned motion matching",
            "venue": "ACM Transactions on Graphics (TOG),",
            "year": 2020
        },
        {
            "authors": [
                "Daniel Holden",
                "Taku Komura",
                "Jun Saito"
            ],
            "title": "Phasefunctioned neural networks for character control",
            "venue": "ACM Transactions on Graphics (TOG),",
            "year": 2017
        },
        {
            "authors": [
                "Daniel Holden",
                "Jun Saito",
                "Taku Komura"
            ],
            "title": "A deep learning framework for character motion synthesis and editing",
            "venue": "ACM Transactions on Graphics (TOG),",
            "year": 2016
        },
        {
            "authors": [
                "Fangzhou Hong",
                "Mingyuan Zhang",
                "Liang Pan",
                "Zhongang Cai",
                "Lei Yang",
                "Ziwei Liu"
            ],
            "title": "Avatarclip: Zero-shot textdriven generation and animation of 3d avatars",
            "venue": "arXiv preprint arXiv:2205.08535,",
            "year": 2022
        },
        {
            "authors": [
                "Lei Hu",
                "Zihao Zhang",
                "Chongyang Zhong",
                "Boyuan Jiang",
                "Shihong Xia"
            ],
            "title": "Pose-aware attention network for flexible motion retargeting by body part",
            "venue": "IEEE Transactions on Visualization and Computer Graphics,",
            "year": 2023
        },
        {
            "authors": [
                "Deok-Kyeong Jang",
                "Soomin Park",
                "Sung-Hee Lee"
            ],
            "title": "Motion puzzle: Arbitrary motion style transfer by body part",
            "venue": "ACM Transactions on Graphics (TOG),",
            "year": 2022
        },
        {
            "authors": [
                "Jogendra Nath Kundu",
                "Maharshi Gor",
                "R Venkatesh Babu"
            ],
            "title": "Bihmp-gan: Bidirectional 3d human motion prediction gan",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Hsin-Ying Lee",
                "Xiaodong Yang",
                "Ming-Yu Liu",
                "Ting-Chun Wang",
                "Yu-Ding Lu",
                "Ming-Hsuan Yang",
                "Jan Kautz"
            ],
            "title": "Dancing to music",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Kyungho Lee",
                "Seyoung Lee",
                "Jehee Lee"
            ],
            "title": "Interactive character animation by learning multi-objective control",
            "venue": "ACM Transactions on Graphics (TOG),",
            "year": 2018
        },
        {
            "authors": [
                "Jiaman Li",
                "Yihang Yin",
                "Hang Chu",
                "Yi Zhou",
                "Tingwu Wang",
                "Sanja Fidler",
                "Hao Li"
            ],
            "title": "Learning to generate diverse dance motions with transformer",
            "venue": "arXiv preprint arXiv:2008.08171,",
            "year": 2020
        },
        {
            "authors": [
                "Zimo Li",
                "Yi Zhou",
                "Shuangjiu Xiao",
                "Chong He",
                "Zeng Huang",
                "Hao Li"
            ],
            "title": "Auto-conditioned recurrent networks for extended complex human motion synthesis",
            "venue": "arXiv preprint arXiv:1707.05363,",
            "year": 2017
        },
        {
            "authors": [
                "Angela S. Lin",
                "Lemeng Wu",
                "Rodolfo Corona",
                "Kevin W.H. Tai",
                "Qixing Huang",
                "Raymond J. Mooney"
            ],
            "title": "Generating animated videos of human activities from natural language",
            "year": 2018
        },
        {
            "authors": [
                "Hung Yu Ling",
                "Fabio Zinno",
                "George Cheng",
                "Michiel Van De Panne"
            ],
            "title": "Character controllers using motion vaes",
            "venue": "ACM Transactions on Graphics (TOG),",
            "year": 2020
        },
        {
            "authors": [
                "Matthew Loper",
                "Naureen Mahmood",
                "Javier Romero",
                "Gerard Pons-Moll",
                "Michael J Black"
            ],
            "title": "Smpl: A skinned multiperson linear model",
            "venue": "ACM transactions on graphics (TOG),",
            "year": 2015
        },
        {
            "authors": [
                "Christian Mandery",
                "\u00d6mer Terlemez",
                "Martin Do",
                "Nikolaus Vahrenkamp",
                "Tamim Asfour"
            ],
            "title": "The kit whole-body human motion database",
            "venue": "In International Conference on Advanced Robotics (ICAR),",
            "year": 2015
        },
        {
            "authors": [
                "Mathis Petrovich",
                "Michael J Black",
                "G\u00fcl Varol"
            ],
            "title": "Actionconditioned 3d human motion synthesis with transformer vae",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Mathis Petrovich",
                "Michael J Black",
                "G\u00fcl Varol"
            ],
            "title": "Temos: Generating diverse human motions from textual descriptions",
            "venue": "In Computer Vision\u2013ECCV 2022: 17th European Conference,",
            "year": 2022
        },
        {
            "authors": [
                "Matthias Plappert",
                "Christian Mandery",
                "Tamim Asfour"
            ],
            "title": "The kit motion-language dataset",
            "venue": "Big data,",
            "year": 2016
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In International conference on machine learning,",
            "year": 2021
        },
        {
            "authors": [
                "Sebastian Starke",
                "He Zhang",
                "Taku Komura",
                "Jun Saito"
            ],
            "title": "Neural state machine for character-scene interactions",
            "venue": "ACM Trans. Graph.,",
            "year": 2019
        },
        {
            "authors": [
                "Sebastian Starke",
                "Yiwei Zhao",
                "Taku Komura",
                "Kazi Zaman"
            ],
            "title": "Local motion phases for learning multi-contact character movements",
            "venue": "ACM Transactions on Graphics (TOG),",
            "year": 2020
        },
        {
            "authors": [
                "Sebastian Starke",
                "Yiwei Zhao",
                "Fabio Zinno",
                "Taku Komura"
            ],
            "title": "Neural animation layering for synthesizing martial arts movements",
            "venue": "ACM Transactions on Graphics (TOG),",
            "year": 2021
        },
        {
            "authors": [
                "Guy Tevet",
                "Brian Gordon",
                "Amir Hertz",
                "Amit H Bermano",
                "Daniel Cohen-Or"
            ],
            "title": "Motionclip: Exposing human motion generation to clip space",
            "venue": "In Computer Vision\u2013ECCV 2022: 17th European Conference,",
            "year": 2022
        },
        {
            "authors": [
                "Guy Tevet",
                "Sigal Raab",
                "Brian Gordon",
                "Yoni Shafir",
                "Daniel Cohen-or",
                "Amit Haim Bermano"
            ],
            "title": "Human motion diffusion model",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Guillermo Valle-P\u00e9rez",
                "Gustav Eje Henter",
                "Jonas Beskow",
                "Andre Holzapfel",
                "Pierre-Yves Oudeyer",
                "Simon Alexanderson"
            ],
            "title": "Transflower: probabilistic autoregressive dance generation with multimodal attention",
            "venue": "ACM Transactions on Graphics (TOG),",
            "year": 2021
        },
        {
            "authors": [
                "Aaron Van Den Oord",
                "Oriol Vinyals"
            ],
            "title": "Neural discrete representation learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Jacob Walker",
                "Kenneth Marino",
                "Abhinav Gupta",
                "Martial Hebert"
            ],
            "title": "The pose knows: Video forecasting by generating pose futures",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2017
        },
        {
            "authors": [
                "Zhiyong Wang",
                "Jinxiang Chai",
                "Shihong Xia"
            ],
            "title": "Combining recurrent neural networks and adversarial training for human motion synthesis and control",
            "venue": "IEEE transactions on visualization and computer graphics,",
            "year": 2019
        },
        {
            "authors": [
                "Will Williams",
                "Sam Ringer",
                "Tom Ash",
                "David MacLeod",
                "Jamie Dougherty",
                "John Hughes"
            ],
            "title": "Hierarchical quantized autoencoders",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Shihong Xia",
                "Lin Gao",
                "Yu-Kun Lai",
                "Ming-Ze Yuan",
                "Jinxiang Chai"
            ],
            "title": "A survey on human performance capture and animation",
            "venue": "Journal of Computer Science and Technology,",
            "year": 2017
        },
        {
            "authors": [
                "Chen Xin",
                "Biao Jiang",
                "Wen Liu",
                "Zilong Huang",
                "Bin Fu",
                "Tao Chen",
                "Jingyi Yu",
                "Gang Yu"
            ],
            "title": "Executing your commands via motion diffusion in latent space",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2023
        },
        {
            "authors": [
                "Sijie Yan",
                "Yuanjun Xiong",
                "Dahua Lin"
            ],
            "title": "Spatial temporal graph convolutional networks for skeleton-based action recognition",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "He Zhang",
                "Sebastian Starke",
                "Taku Komura",
                "Jun Saito"
            ],
            "title": "Mode-adaptive neural networks for quadruped motion control",
            "venue": "ACM Transactions on Graphics (TOG),",
            "year": 2018
        },
        {
            "authors": [
                "Jianrong Zhang",
                "Yangsong Zhang",
                "Xiaodong Cun",
                "Shaoli Huang",
                "Yong Zhang",
                "Hongwei Zhao",
                "Hongtao Lu",
                "Xi Shen"
            ],
            "title": "T2m-gpt: Generating human motion from textual descriptions with discrete representations",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2023
        },
        {
            "authors": [
                "Mingyuan Zhang",
                "Zhongang Cai",
                "Liang Pan",
                "Fangzhou Hong",
                "Xinying Guo",
                "Lei Yang",
                "Ziwei Liu"
            ],
            "title": "Motiondiffuse: Text-driven human motion generation with diffusion model",
            "venue": "arXiv preprint arXiv:2208.15001,",
            "year": 2022
        },
        {
            "authors": [
                "Yan Zhang",
                "Michael J Black",
                "Siyu Tang"
            ],
            "title": "Perpetual motion: Generating unbounded human motion",
            "venue": "arXiv preprint arXiv:2007.13886,",
            "year": 2020
        },
        {
            "authors": [
                "Chongyang Zhong",
                "Lei Hu",
                "Zihao Zhang",
                "Shihong Xia"
            ],
            "title": "Learning uncoupled-modulation cvae for 3d actionconditioned human motion synthesis",
            "venue": "In Computer Vision\u2013 ECCV 2022: 17th European Conference,",
            "year": 2022
        },
        {
            "authors": [
                "Chongyang Zhong",
                "Lei Hu",
                "Zihao Zhang",
                "Yongjing Ye",
                "Shihong Xia"
            ],
            "title": "Spatio-temporal gating-adjacency gcn for human motion prediction",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "With the expansion of application scenarios and demands, cross-modal human motion synthesis has become a hot research topic in computer vision and graphics. Among them, text-driven human motion synthesis aims to synthesize natural human motion that matches a given text description, which can be applied to intelligent animation production, virtual reality, game and film industry, human-robotics interaction, etc.\n*Equal contribution \u2020Corresponding author\nHowever, introducing natural language descriptions as constraints for motion synthesis, which means using crossmodal high-level semantics to control the motion synthesis process, is a difficult interdisciplinary problem involving natural language processing, motion modeling, and crossmodal relationship learning. With the availability of pretrained language models such as CLIP [35], BERT [10], and GPT [36] for natural language processing, there are still two challenges that remained: (1) human motion is high-dimensional data with irregular spatial structure and highly dynamic temporal characteristics. It is challenging\nar X\niv :2\n30 9.\n00 79\n6v 1\n[ cs\n.C V\n] 2\nS ep\n2 02\nto perform text-driven synthesis in pose space directly; (2) there are complex local (specific words correspond to specific motion sub-sequences) and global (the overall text description corresponds to the order and connection of these sub-sequences) correspondence between text and motion. How to better learn this cross-modal relationship is still a problem to be solved.\nCompared to some works that directly generated textdriven motion in the pose space[4, 3, 8, 53, 41], others attempted to learn the low-dimensional representation of motion first using auto-encoder [13], VAE [49] and VQVAE [52] with temporal encoding. However, since human motion has both spatial and temporal characteristics, it is necessary to consider both when dealing with latent representation learning [56, 50]. Therefore, we propose Vector Quantised-Variational AutoEncoder(VQ-VAE) with Body-Part attention-based Spatio-Temporal(BPST) encoder to learn an expressive latent space. As for the motion-text cross-modal relationship, some works used cross-modal translation to learn the shared representation of text and motion and directly generated motion from text feature [4, 3, 33, 6]. However, the vast gaps between motion and text data make learning an adequate shared representation difficult. Other researchers attempted to treat text information as a condition during the motion generation [13, 52, 49, 53, 41, 14]. We observe two levels of correspondence in the cross-modal relationship between motion and text: 1). the local correspondence between words of text and sub-segments of motion sequences; 2). the global correspondence between the overall semantics of text and the whole motion sequence. Previous research typically focused on either local [53] or global [52, 41] text information or combined them by simply concatenating [13]. We propose Global and Local Attention(GLA) to consider both levels more reasonably better to learn the cross-modal relationship between text and motion.\nSpecifically, we propose AttT2M, a two-stage textdriven motion generation model with multi-perspective attention(Figure 2(a)). The first attention focuses on motion embedding perspective. We use a spatial transformer [44] based on body-part attention and TCN to extract the spatio-temporal features, and then a VQ-VAE [43] is used to quantize the features into a discrete codebook. In the text-diven generation stage, we propose global and local attention from the perspective of cross-modal relationship learning. After the global (sentence-level) and local (wordlevel) text features are extracted using CLIP, we calculate the motion-word cross-attention to learn the local crossmodal relationship and learn the global cross-modal relationship by motion-sentence conditional self-attention. Finally, a generative transformer is used to generate motion sequences.\nWe conduct extensive qualitative and quantitative exper-\niments on two widely used datasets, KIT-ML [34] and HumanML3D [13]. The experimental results show that our work achieves better text-driven motion generation results than previous work in qualitative and quantitative comparisons. The generated motions highly match the given natural language descriptions and maintain reliable diversity while being highly realistic and natural(seeing Figure 1). Our work can also achieve fine-grained generation and action to motion.\nOur contributions can be summarized as follows: \u2022 1. We propose a Body-Part attention-based Spatio-\nTemporal VQ-VAE to map motion sequences into a better discrete code book, resulting in a more expressive low-dimensional motion representation.\n\u2022 2. We introduce Global and Local Attention to learn the global-local cross-modal relationship between text and motion, achieving precise correspondences between motion and text.\n\u2022 3. We conduct extensive qualitative and quantitative experiments on KIT-ML and HumanML3D to demonstrate that our method outperforms the previous stateof-the-art methods, which can also handle fine-grained and action-to-motion generation."
        },
        {
            "heading": "2. Related Works",
            "text": "Human motion synthesis is a long-standing research topic [48]. Here we only introduce works based on deep learning and exclude earlier methods. Motion synthesis started with the unconstrained generation, which generated realistic and natural motion sequences in completely random ways [11, 27, 54]. With the development of research and increasing demands, methods based on different conditions have been proposed. One primary condition was motion content. Some works focused on generating motion that satisfied the constraints of low-level control signals (such as speed, direction, trajectory, etc.). They either used a two-stage approach[19, 46, 29] by training a generative model as a prior and then generated motions through optimization, or used an end-to-end manner by parameterizing the control signals as input to predict the control signals and pose for the next frame [25, 18, 51, 37, 38, 39]. Other researchers attempted to use motion graph to synthesize high-quality human motion through searching and blending [9, 17]. In addition, another content constraint was the past motion sequences(referred to as diverse motion prediction). They usually used generative models such as VAE [45, 5] and GAN [7, 23] to encode past motion sequences and then sampled in the latent space to generate multiple prediction results. Recently, motion generation based on the cross-modal condition has been a hot topic. Some works hoped to generate dances from music [24, 26, 42], while other researchers were trying to\ngenerate motion from the given text, which included action labels [32, 15, 55] as well as natural language descriptions [12, 4, 8, 3, 28, 33, 6, 13, 52, 49, 53, 41, 14, 40, 20].\nMotion latent representation learning in motion synthesis is about mapping motions into a low-dimensional latent space due to the high-dimensional and complex nature of the original pose space. Some works utilized autoencoder [13] structures to achieve dimension reduction. Other works attempted to train a generative model based on VAE [49] or VQ-VAE [52] to learn a latent representation of motion with more diversity. Most of these works only focused on temporal characteristics, using RNNs, TCNs, or transformers to learn temporal features. However, in motion prediction and action recognition, researchers had found that the spatial features were also crucial in motion latent representation learning [56, 50]. So we propose Body-Part attention-based SpatiO-Temporal VQ-VAE to learn a more expressive latent representation.\nText-driven motion generation aims to generate 3D human motion based on textual descriptions. We categorize related works into three types. The first type desired to learn shared latent representations of motion and text, and directly converted text to motion. After Text2Action [3] and Language2pose [4] made the first attempts, TEMOS [33] introduced a transformer VAE architecture to make the latent distributions of text and motion as similar as possible, and TEACH [6] achieved coherent motion generation based on multiple action labels. The second type rendered 3D human bodies as images and learned the motion-imagetext relationship using contrastive learning with the help of CLIP. MotionCLIP [40] focused on motion generation, while AvatarCLIP [20] generated human appearance and\nsimple motion simultaneously. The third type treated text as a condition for motion generation. Some works were based on an encoder-decoder framework, using transformers [8] or RNNs [28] to encode motion and concatenating it with text feature as input to the decoder. To increase the diversity of generated motion, generative models were introduced. RNN+CVAE [15], transformer+CVAE [32], and Uncoupled-modulation CVAE [55] are used to generate motion from action labels. With the proposal of a large-scale motion-language dataset called HumanML3D [13], many studies achieved better results. T2M [13] adopted a twostage approach, learning motion clip codes and conducting text-driven motion generation. They also achieved better results by jointly training text-to-motion and motion-to-text tasks [14]. T2M-GPT [52] proposed to learn the discrete representation with VQ-VAE and used a generative transformer for generation. Other works attempted to use the diffusion model [16] in pose space directly [53, 41] or latent space [49] for text-driven motion generation. This paper proposes a Global-Local Attention mechanism to learn multi-level cross-modal relationships between motion and text, achieving better text-driven motion generation."
        },
        {
            "heading": "3. Our Method",
            "text": "The problem we aim to solve is text-driven motion generation, which refers to generating a T frames sequence of human motions X = {x1, x2, ..., xT } based on a given text description \u2126 = {\u21261,\u21261, ...,\u2126N} consisting of N words."
        },
        {
            "heading": "3.1. Method Overview",
            "text": "To generate high-quality and text-corresponding motion, we explore motion latent space learning and textmotion cross-modal relationship learning. The proposed two-stage motion generation framework is shown in Figure 2(b). Stage 1 is motion embedding. We rearrange the original motion representation to adapt the body-part segmentation and use a Body-Part attention-based SpatioTemporal(BPST) motion encoder to learn an expressive spatio-temporal feature. Then a VQ-VAE is used to learn a discrete motion latent space C through quantizer. In Stage 2, we learn the cross-modal relationship between text and motion on the global and local levels to generate motions according to given text \u2126. After the word-level embedding e\u03c9 and sentence-level embedding e\u2126 are extracted by CLIP, we propose motion-word cross attention to learn the local cross-modal relationship. Moreover, we compute motionsentence self-attention to learn the global cross-modal relationship. To generate diverse results, we use a generative transformer to generate motion code Cgen, and the final generated sequence is obtained by the motion decoder."
        },
        {
            "heading": "3.2. The Motion Embedding Module",
            "text": "The pose of motion exhibits infinite variations, leading to a high-dimensional pose space that makes it difficult to model human motions. Therefore, many researchers attempt to learn a latent space to reduce the dimension [13, 49, 52]. However, 3D human motion is a structured time series with spatial and temporal characteristics. Previous text-driven motion generation works paid more attention to temporal modeling using RNN [15], TCN [13, 52], Transformer [33], the understanding of spatial structure was often overlooked. To better learn the spatio-temporal characteristics and map them to a low-dimensional, expressive latent space, we propose a motion embedding module named Body-Part Attention-based Spatio-Temporal VQ-VAE.\nBody-part Attention-based Spatio-Temporal feature extraction Although the human body has many joints, the body part should be the basic unit that best represents the semantic information of motion. The interaction between joints within the same body part ultimately combines into the motion of the entire human. Some previous works have already realized this point [22, 2, 21]. Based on this observation, we propose a spatio-temporal encoder based on body-part attention to extract spatio-temporal features.\nAs shown in Figure 3(a), we divide the human body with n joints into five body parts: {Torso, Left Arm, Right Arm, Left Leg, Right Leg}, each containing its own set of joints. To apply transformer in the spatial dimension, we rearrange the representation of T2M [13] to gather information of each joint, leading to n + 1 motion tokens of each frame: x\u0304i = {jroot, j1, j2, ..., jn\u22121, cf}. Next, we propose a spatial transformer Transenc based on body-part atten-\ntion. Before computing self-attention, we map all tokens to the same dimension through different linear mapping:\nQI = W Iq J ,KI = W Ik J , V I = W Iv J (1)\nwhere the superscript I = {root, others, contact} of Q, K, V and Wq , Wk, Wv is a denotes set representing the root joint, other joints, and foot contact. And J = {jroot, ji, cf} are the motion tokens. Finally, we concatenate the mapping results of different tokens to obtain the final Q, K,V . When calculating body-part attention, we define an adjacency mask M = {mi,j} \u2208 R(n+1)\u00d7(n+1) according to body part division. The dimension of M becomes n+1 because foot contact cf is treated as an extra joint to avoid foot slide. If joint i and j are in the same body part, mi,j = 0, otherwise \u2212\u221ewe add the adjacency relation mask M. The body-part attention is calculated by:\nBPAtt = softmax( QKT \u2295M\u221a\nD )V (2)\nFinally, we repeat the above steps to calculate multi-head self-attention and obtain the final spatial feature fs through a feed-forward network(FFN ). M limits the self-attention calculation within the body part. After extracting the spatial features, we use a temporal encoder TCNenc consisting of multiple temporal convolutional layers to obtain the motion spatio-temporal feature fst.\nLatent space learning based on VQ-VAE Inspired by T2M-GPT [52], we use VQ-VAE to learn the latent motion space. First, we construct a learnable codebook C = {ci} \u2208 RK\u2217DC as a set of discrete latent motion representations. Then, we map the motion sequence X to latent features Fst through our spatio-temporal feature extraction module and then quantize it to a discrete latent representation. We use exponential moving average (EMA) and codebook reset [47] to train VQ-VAE and optimize it with the following loss function:\nL = Lrecon+ \u03b1Lvel + Lemb+ \u03b2Lcom (3)\nwhere:\nLrec = L1(X, X\u0302), Lvel = L1(V, V\u0302 ) Lemb = \u2225sg[Fst]\u2212 C\u22252 ,Lcom = \u2225 Fst \u2212 sg[C]\u22252 (4)\nV is velocity and \u02c6 indicates reconstructed. Lrec, Lvel, Lemb and Lcom are the reconstruction term, smooth velocity term, embedding term and commit term, respectively."
        },
        {
            "heading": "3.3. The Text-driven Motion Generation Module",
            "text": "We observe that there are two-level correspondences in the cross-modal relationship between motion and text: (1) the correspondence between words in text and subsegments in motion sequences. For example, in the sentence\n\u201dWith both arms outstretched at the side, bent at elbow, the person raises arms straight over head\u201d, words such as \u201doutstretched\u201d, \u201dbent\u201d, \u201draises\u201d and \u201darm\u201d are essential. If we do not consider this correspondence, important words may be ignored, resulting in generated motion inconsistent with the given text; (2) the correspondence between the overall semantics of the text and the whole motion sequence. The temporal properties of the motion are continuous, but the textual ones may not be. In the above sentence, \u201doutstretched\u201d and \u201dbent\u201d has a sequential order, while the actions they represent occur simultaneously. Thus the overall correspondence between the sentence and motion also plays an indispensable role in text-driven motion generation. Previous work usually only considered one aspect of the correspondences [53, 41, 33, 52, 49] or simply concatenate two of them [13]. To achieve better text-driven motion generation, we propose a Global and Local Attention Generative Transformer to simultaneously consider these two correspondences, using different levels of attention to better learn the cross-modal relationship between text and motion (as shown in Figure 3(b)).\nLocal cross-modal relationship For a text description \u2126 with N words and the corresponding motion discrete representations C = {ci}mi=1, we first extract word-level features e\u03c9 = {e\u03c9i}Ni=1 using the pre-trained model CLIP [35]. To learn the importance of each word in the cross-modal correspondence, we propose a local cross-modal transformer Translocal to calculate the cross-attention between e\u03c9 and C. Specifically, we use e\u03c9 as the key and value and C as the query:\nQlocal = WqC,Klocal = Wke\u03c9, Vlocal = Wve\u03c9 (5)\nThen, we compute their cross-attention:\nCrossAtt = softmax( QlocalK T local\u221a\nD )Vlocal (6)\nIn this way, we can estimate the importance of each word in the sentence for motion generation(seeing Fig. 8) and learn the cross-modal relationship at the local level. Finally, we input CrossAtt into a FFN to obtain the joint motionwords embedding C\u03c9 = {c\u03c9i}mi=1.\nGlobal cross-modal relationship Similar to extracting word-level features, we also use a CLIP to extract sentence-level features e\u2126 \u2208 R1\u00d7D from the text. Since the size of e\u2126 is 1 \u00d7 D, performing a softmax weighted sum on it is meaningless. To better learn global crossmodal relationship, we adopt the idea of T2M-GPT [52] and use a transformer based on conditional self-attention, called Transglobal. Specifically, we concatenate e\u2126 with C\u03c9 on temporal dimension to calculate self-attention between e\u2126 and C\u03c9 . During this process, e\u2126 is fully involved in the attention calculation with C\u03c9 , which is beneficial for learning the global cross-modal relationship.\nTo obtain more diverse generation results, we input C\u2126 into a generative transformer TCNgen to predict the probability p(cm+1|C\u2126) of the next motion code in the codebook, and optimize the maximum likelihood:\nL = \u2212Ecm+1\u223cp(C)[logp(cm+1|C\u2126)] (7) During testing, we directly input the sentence-level feature e\u2126 into TCNglobal to predict the first motion code c1, then continuously generate a motion code sequence Cgen. Finally, we input it into the motion decoder TCNenc to obtain the human motion sequence X\u0302 = {x\u0302i}Ti=1."
        },
        {
            "heading": "4. Experiments",
            "text": "In this section, we first introduce the dataset and evaluation metrics in Sec.4.1, the implementation details are demonstrated in supplementary materials. Quantitative and qualitative evaluations will be presented in Sec.4.2 and 4.3, respectively. Then, we analyze the main components of the method through ablation study in Sec.4.4. Finally, we further discuss our attention mechanism in Sec.4.5. Due to space limitations, we demonstrate our failure cases and limitations in the supplementary material."
        },
        {
            "heading": "4.1. Dataset and Metrics",
            "text": "KIT Motion-Language [34] is the first 3D human motion dataset with text labels, consisting of a subset of KIT [31] and CMU [1] datasets. It contains 3911 motion sequences and 6353 sequence-level text annotations, with an average of 9.5 words per annotation.\nHumanML3D [13]: HumanML3D added detailed text labels to the motion in AMASS dataset, creating a motionlanguage dataset. It contains 14616 motion segments with\na total duration of 28.59 hours. The average length of per segment is 7.1 seconds, and the longest and shortest motions are 10 and 2 seconds, respectively. The dataset contains 44970 text descriptions with an average length of 12 words, covering 5317 unique words. We follow the split of T2M [13] for our training, validation, and testing on both KIT-ML and HumanML3D.\nEvaluation Metrics We adopt the evaluation metrics following T2M [13], including Frechet Inception Distance(FID.), R-precision, Multi-Modal distance(MM-D.), Diversity(Div.) and Multi-Modality(MM.), which are evaluating the distribution distance between generated motion and ground-truth, the consistency of text and generated motion, the euclidean distance of motion feature and text feature, the diversity of whole generated motion and the diversity of generated motion from the same text, respectively."
        },
        {
            "heading": "4.2. Quantitative Evaluation",
            "text": "We use the metrics mentioned above to evaluate the quantitative effects of our method and compare them with recent state-of-the-art works, including\nHier [12], TM2T [14], T2M [13], MDM [41], MotionDiffuse(MD) [53], MLD [49], and T2M-GPT [52]. The results of these methods were obtained from their pre-trained models or related papers.\nHumanML3D: As shown in Table 1, our method have significant improvements in almost all evaluation metrics compared to previous works. The performance of textdriven motion generation has two aspects: the quality of the generated motion itself and its consistency with the given text description. In terms of text consistency, our method has a significant improvement in the accuracy of top 1, 2, and 3 compared to previous works, which are also very close to the results of real data. This indicates that the consistency of our generated motion with the text is the highest, which is also evidenced by our method\u2019s lowest MM-D. As for the quality of the generated motion itself, our method has the lowest FID of all methods, indicating that the quality of our generated motion is natural, realistic, and close to real data. It is worth noting that Div can only measure the diversity of motion to a certain extent, as Div may also be large when the generated motion has various artifacts.\nThe more compelling diversity metric is MM, of which our method is the second highest.\nKIT-ML: We evaluate the same metrics on KIT-ML. The results are shown in Table 2. Since MD [53], MDM [41], and MLD [49] used ground-truth lengths in their evaluation, it is understandable that they achieved good results whereas our method has better or comparable performance in all metrics(except FID) without the need for ground-truth lengths. And there is a significant improvement over other methods that do not require ground-truth length, like T2M [13] and T2M-GPT [52]."
        },
        {
            "heading": "4.3. Qualitative Evaluation",
            "text": "Text-driven Motion Generation: We present the visualization results of our approach for long text generation with SMPL [30] model(Figure 1 and 4). The figure shows multiple motion results generated by our method, which are high quality and match the textual description very well. The generated motion types include walking on the ground and upstairs, running, crouching, crawling, flipping, and other actions that involve both movements in situ and global displacement, which fully demonstrate the superiority of our method. Moreover, our method can generate diverse sequences for the same input text while ensuring motion consistency. The diversity results and more visualization results are shown in the supplementary materials and demo.\nComparison with Previous Works: To demonstrate the effectiveness of our method, we conduct qualitative comparison on HumanML3D with MD [53] and T2MGPT [52], which achieve the most similar results to us in the quantitative evaluation. The results are shown in Figure 5. The left sequences correspond to the text \u201da person quickly waves with their right hand\u201d, which emphasizes the words \u201dquickly\u201d and \u201dwave right hand\u201d. We can see that the motion generated by MD [53] waves the left hand, while the motion generated by T2M-GPT [52] ignores the word \u201dquickly\u201d and pauses for a moment before waving. Our result matches the text description best. The sequences on\nthe right correspond to the text \u201da person walks in a circle clockwise\u201d. The motion generated by MD [53] stops after only half a circle, while the trajectory generated by T2M-GPT [52] is not a true circle. Our result\u2019s trajectory is a more seamless circle. The better performance on textdriven motion generation relies on the learning of the crossmodal relationship between text and motion, which allows our model to focus more attention on essential words.\nFine-grained Motion Generation: When only one word is changed in a sentence, the resulting change in the text encoding may be very small, but the corresponding motion sequence can be completely different. Thanks to the body-part attention-based spatial features extraction and\nthe local cross-modal relationship learning, our method can achieve fine-grained motion synthesis by changing a single word in the sentence(Figure 6). As we modify the word \u201dleft\u201d to \u201dright\u201d and \u201dboth\u201d, the generated motions change from raising the left hand to raising the right and both arms. In addition to replacing adjectives, when we change the verb by replacing \u201dput up\u201d with \u201dcross\u201d, the model can also generate the motion of crossing both arms. This demonstrates that our method is very sensitive to small changes in the input text, which is more helpful in keeping the consistency between the generated motion and the text.\nAction to Motion Generation: In addition to long textdriven motion generation, our method can also achieve high quality action to motion generation without any extra training, as shown in Figure 7. With only a single word or phrase representing the action as input, our method can still generate motion with high quality."
        },
        {
            "heading": "4.4. Ablation Study",
            "text": "We conducted an ablation study to investigate the effect of each module in our method on two aspects: the quality of the generated motion itself and its consistency with the given text description.\nEffect of BPST: The high-quality low-dimensional motion representation can simplify motion modeling, making the generated motion more realistic and helpful for crossmodal learning between text and motion. To demonstrate the effect of our proposed BPST module, we remove the BPST and directly input the motion representation into the temporal convolutional layer to learn the VQ-VAE. The AS4 in Table 3 shows the results of this ablation study. We can see that the three R-Precision metrics decrease to some extent compared to the complete version(AS6), while FID increases significantly. R-Precision measures the consistency between the generated motion and text description, while FID reflects both the quality of the generated motion itself and the consistency between text and motion. The experimental results indicate that our BPST module significantly impacts the quality of generated motion while also improving the consistency between text and motion to some extent. In addition, we further demonstrated the effective-\nness of BPST by comparing the reconstruction error and commitment error of VQ-VAE with T2M-GPT(Table. 4).\nEffect of GLA: The role of the GLA module is to learn the cross-modal relationship between text and motion, ensuring consistency between the generated motion and the given text. To demonstrate the effectiveness of our GLA, we keep the settings of stage 1 unchanged and remove Translocal and Transglobal in turn. The AS2 and AS3 in Table 3 shows the results of this ablation study. We can see that compared to AS5, the R Precision of AS2 and AS3 decreases significantly, and FID decreases slightly. This indicates that GLA can significantly improve the realism of motion and consistency with the text description. In addition, we found that the performance of AS2 is worse than that of AS3, which indicates that LA plays a more important role in text-driven motion generation than GA.\nFinally, we remove both BPST and GLA, and use vanilla VQ-VAE and concatenation of motion and text in vanilla transformer. The result is shown in AS1 of Table 3. All metrics became much worse, indicating that our method is reasonable and effective."
        },
        {
            "heading": "4.5. Attention Visualization",
            "text": "To further explain the effectiveness and rationality of our method, we visualize the cross-attention between words and motion. We selecte a generated motion sequence of length 41 described by the text \u201dWith both arms outstretched at the side, bent at the elbow, the person raises arms straight overhead, before returning to the original position.\u201d The visualization of attention is shown in Figure 8 using 2D and 3D heatmaps. Rows 4, 15, and 22 of the 2D heatmap correspond to the words \u201doutstretched,\u201d \u201draise,\u201d and \u201dreturning,\u201d which represent the main actions of this motion sequence and have the highest attention weights. Rows 2, 3, 9, 11, and 16 correspond to words such as \u201dboth\u201d, \u201darms\u201d, \u201dbent\u201d, \u201delbow\u201d, and \u201darms\u201d, which are related to the body parts and states of them and relevant to the motion sequence. Other words and punctuation in the sentence have weak relationships with the motion and therefore have correspondingly lower weights. This visualization demonstrates that our method can indeed learn local cross-modal attention for each word, which is helpful for learning cross-modal relationships between text and motion. The body-part attention is shown in the supplementary materials."
        },
        {
            "heading": "4.6. User Perceptual Study",
            "text": "To illustrate the visual superiority of our approach, we did a user perceptual study, and the results are shown in Fig. 9. Specifically, we randomly select 30 texts in testset of HumanML3D and set up 10 manual texts to generate motion using the following 6 models: T2M-GPT, Motiondiffuse, Ours w/o BPS, Ours w/o GA, Ours w/o LA, Ours. We have 30 participants and ask them to rate the generated motion on a scale of 1-6(6 is the best). The evaluation criteria is a combination of motion realism and text-motion consistency. The highest score and lowest std indicate that our method works better and is more robust."
        },
        {
            "heading": "5. Limitations and Failure Cases",
            "text": "Despite achieving the state-of-the-art performance, our method still has some limitations and corresponding failure cases as follows(See the supplemental video for the visualization results).\n(1) Insufficient diversity in long and detailed textdriven generation: Our method can generate motions with diversity. However, for very long texts with very detailed descriptions in the test set, the diversity of the motions generated by our method is insufficient. We observe that there are usually few motion sequences corresponding to long and detailed text in the HumanML3D dataset. When we want to generate multiple motions with them, the generated motions will be very similar.\n(2) Fine-grained generation without ground truth or any similar sample in the dataset: As mentioned in the main paper, our approach enables fine-grained motion generation by changing a word or phrase in a sentence. However, when the motion represented by the changed word does not have ground truth in the dataset, or does not have any similar motion, our method does not yield results that match the text description well.\n(3) Out-of-distribution Generation: When the given text does not lie within the distribution of the dataset, our method is unable to generate motions consistent to the text. But our method still tries very hard to find the motion similar to this text description."
        },
        {
            "heading": "6. Conclusion",
            "text": "We propose AttT2M, a two-stage method with multiperspective attention mechanism for text-driven motion generation. Specifically, we use the spatial transformer based on body-part attention and temporal convolution to extract spatio-temporal motion features, and map them into a discrete latent space using VQ-VAE. Next, we learn the cross-modal relationship between the latent motion representation and the text utilizing global and local attention, which means calculating sentence-level conditional self-attention and word-level cross-attention. Finally, a generative transformer is trained to perform motion generation. Extensive experiments demonstrate that our method outperforms state-of-the-art methods in both qualitative and quantitative evaluations, and can accomplish fine-grained motion synthesis and action to motion.\nAcknowledgements This work was supported by National Key Research and Development Program of China (NO. 2022YFB3303202)."
        }
    ],
    "title": "AttT2M: Text-Driven Human Motion Generation with Multi-Perspective Attention Mechanism",
    "year": 2023
}