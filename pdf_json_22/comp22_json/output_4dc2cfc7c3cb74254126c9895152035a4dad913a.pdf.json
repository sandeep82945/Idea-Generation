{
    "abstractText": "In text summarization and simplification, system outputs must be evaluated along multiple dimensions such as relevance, factual consistency, fluency, and grammaticality, and a wide range of possible outputs could be of high quality. These properties make the development of an adaptable, reference-less evaluation metric both necessary and challenging. We introduce MaskEval, a reference-less metric for text summarization and simplification that operates by performing masked language modeling (MLM) on the concatenation of the candidate and the source texts. It features an attention-like weighting mechanism to modulate the relative importance of each MLM step, which crucially allows it to be adapted to evaluate different quality dimensions. We demonstrate its effectiveness on English summarization and simplification in terms of correlations with human judgments, and explore transfer scenarios between the two tasks.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yu Lu Liu"
        },
        {
            "affiliations": [],
            "name": "Rachel Bawden"
        },
        {
            "affiliations": [],
            "name": "Thomas Scialom"
        },
        {
            "affiliations": [],
            "name": "Beno\u0131\u0302t Sagot"
        },
        {
            "affiliations": [],
            "name": "Jackie Chi"
        },
        {
            "affiliations": [],
            "name": "Kit Cheung"
        }
    ],
    "id": "SP:0578568fd312cb4d4f1601692ce1da5962f09517",
    "references": [
        {
            "authors": [
                "Fernando Alva-Manchego",
                "Louis Martin",
                "Antoine Bordes",
                "Carolina Scarton",
                "Beno\u0131\u0302t Sagot",
                "Lucia Specia"
            ],
            "title": "ASSET: A dataset for tuning and evaluation of sentence simplification models with multiple rewriting transformations",
            "year": 2020
        },
        {
            "authors": [
                "Rachel Bawden",
                "Biao Zhang",
                "Lisa Yankovskaya",
                "Andre T\u00e4ttar",
                "Matt Post."
            ],
            "title": "A study in improving BLEU reference coverage with diverse automatic paraphrasing",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020,",
            "year": 2020
        },
        {
            "authors": [
                "Chris Callison-Burch",
                "Miles Osborne",
                "Philipp Koehn."
            ],
            "title": "Re-evaluating the role of Bleu in machine translation research",
            "venue": "11th Conference of",
            "year": 2006
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association",
            "year": 2019
        },
        {
            "authors": [
                "Esin Durmus",
                "He He",
                "Mona Diab."
            ],
            "title": "FEQA: A question answering evaluation framework for faithfulness assessment in abstractive summarization",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5055\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Esin Durmus",
                "Faisal Ladhak",
                "Tatsunori Hashimoto."
            ],
            "title": "Spurious correlations in reference-free evaluation of text generation",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages",
            "year": 2022
        },
        {
            "authors": [
                "Matan Eyal",
                "Tal Baumel",
                "Michael Elhadad."
            ],
            "title": "Question answering as an automatic evaluation metric for news article summarization",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Lin-",
            "year": 2019
        },
        {
            "authors": [
                "Alexander R. Fabbri",
                "Wojciech Kry\u015bci\u0144ski",
                "Bryan McCann",
                "Caiming Xiong",
                "Richard Socher",
                "Dragomir Radev."
            ],
            "title": "SummEval: Re-evaluating summarization evaluation",
            "venue": "Transactions of the Association for Computational Linguistics, 9:391\u2013409.",
            "year": 2021
        },
        {
            "authors": [
                "Yvette Graham."
            ],
            "title": "Re-evaluating automatic summarization with BLEU and 192 shades of ROUGE",
            "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 128\u2013137, Lisbon, Portugal. Association for Compu-",
            "year": 2015
        },
        {
            "authors": [
                "Karl Moritz Hermann",
                "Tomas Kocisky",
                "Edward Grefenstette",
                "Lasse Espeholt",
                "Will Kay",
                "Mustafa Suleyman",
                "Phil Blunsom."
            ],
            "title": "Teaching machines to read and comprehend",
            "venue": "Advances in neural information processing systems, pages 1693\u20131701.",
            "year": 2015
        },
        {
            "authors": [
                "Matthew Honnibal",
                "Ines Montani."
            ],
            "title": "spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing",
            "venue": "To appear.",
            "year": 2017
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "Proceedings of the 3rd International Conference for Learning Representations, San Diego.",
            "year": 2015
        },
        {
            "authors": [
                "Fajri Koto",
                "Jey Han Lau",
                "Timothy Baldwin."
            ],
            "title": "Evaluating the efficacy of summarization evaluation across languages",
            "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 801\u2013812, Online. Association for Computa-",
            "year": 2021
        },
        {
            "authors": [
                "Guillaume Lample",
                "Alexis Conneau."
            ],
            "title": "Crosslingual language model pretraining",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS).",
            "year": 2019
        },
        {
            "authors": [
                "Hwanhee Lee",
                "Thomas Scialom",
                "Seunghyun Yoon",
                "Franck Dernoncourt",
                "Kyomin Jung."
            ],
            "title": "QACE: Asking questions to evaluate an image caption",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4631\u20134638,",
            "year": 2021
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: Denoising sequence-to-sequence pretraining for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "ROUGE: A package for automatic evaluation of summaries",
            "venue": "Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain. Association for Computational Linguistics.",
            "year": 2004
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Fixing weight decay regularization in adam",
            "venue": "CoRR, abs/1711.05101.",
            "year": 2017
        },
        {
            "authors": [
                "Louis Martin",
                "Samuel Humeau",
                "Pierre-Emmanuel Mazar\u00e9",
                "\u00c9ric de La Clergerie",
                "Antoine Bordes",
                "Beno\u0131\u0302t Sagot"
            ],
            "title": "Reference-less quality estimation of text simplification systems",
            "venue": "In Proceedings of the 1st Workshop on Automatic Text Adaptation",
            "year": 2018
        },
        {
            "authors": [
                "Jekaterina Novikova",
                "Ond\u0159ej Du\u0161ek",
                "Amanda Cercas Curry",
                "Verena Rieser."
            ],
            "title": "Why we need new evaluation metrics for NLG",
            "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2241\u20132252,",
            "year": 2017
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318, Philadelphia,",
            "year": 2002
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-totext transformer",
            "venue": "Journal of Machine Learning Re-",
            "year": 2020
        },
        {
            "authors": [
                "Ricardo Rei",
                "Craig Stewart",
                "Ana C Farinha",
                "Alon Lavie."
            ],
            "title": "COMET: A neural framework for MT evaluation",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2685\u20132702, Online. Associa-",
            "year": 2020
        },
        {
            "authors": [
                "Arij Riabi",
                "Thomas Scialom",
                "Rachel Keraron",
                "Beno\u0131\u0302t Sagot",
                "Djam\u00e9 Seddah",
                "Jacopo Staiano"
            ],
            "title": "Synthetic data augmentation for zero-shot crosslingual question answering",
            "venue": "In Proceedings of the 2021 Conference on Empirical Methods in Natural",
            "year": 2021
        },
        {
            "authors": [
                "Thomas Scialom",
                "Paul-Alexis Dray",
                "Sylvain Lamprier",
                "Benjamin Piwowarski",
                "Jacopo Staiano",
                "Alex Wang",
                "Patrick Gallinari."
            ],
            "title": "QuestEval: Summarization asks for fact-based evaluation",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods",
            "year": 2021
        },
        {
            "authors": [
                "Thomas Scialom",
                "Felix Hill."
            ],
            "title": "Beametrics: A benchmark for language generation evaluation evaluation",
            "venue": "arXiv preprint arXiv:2110.09147.",
            "year": 2021
        },
        {
            "authors": [
                "Thomas Scialom",
                "Sylvain Lamprier",
                "Benjamin Piwowarski",
                "Jacopo Staiano."
            ],
            "title": "Answers unite! unsupervised metrics for reinforced summarization models",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language",
            "year": 2019
        },
        {
            "authors": [
                "Thomas Scialom",
                "Sylvain Lamprier",
                "Benjamin Piwowarski",
                "Jacopo Staiano."
            ],
            "title": "Answers unite! unsupervised metrics for reinforced summarization models",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language",
            "year": 2019
        },
        {
            "authors": [
                "Thomas Scialom",
                "Louis Martin",
                "Jacopo Staiano",
                "\u00c9ric Villemonte de la Clergerie",
                "Beno\u0131\u0302t Sagot"
            ],
            "title": "Rethinking automatic evaluation in sentence simplification",
            "year": 2021
        },
        {
            "authors": [
                "Abigail See",
                "Peter J. Liu",
                "Christopher D. Manning."
            ],
            "title": "Get to the point: Summarization with pointergenerator networks",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1073\u2013",
            "year": 2017
        },
        {
            "authors": [
                "Thibault Sellam",
                "Dipanjan Das",
                "Ankur Parikh."
            ],
            "title": "BLEURT: Learning robust metrics for text generation",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7881\u20137892, Online. Association for Computa-",
            "year": 2020
        },
        {
            "authors": [
                "Elior Sulem",
                "Omri Abend",
                "Ari Rappoport."
            ],
            "title": "BLEU is not suitable for the evaluation of text simplification",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 738\u2013744, Brussels, Belgium. Association",
            "year": 2018
        },
        {
            "authors": [
                "Brian Thompson",
                "Matt Post."
            ],
            "title": "Automatic machine translation evaluation in many languages via zero-shot paraphrasing",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 90\u2013121, Online.",
            "year": 2020
        },
        {
            "authors": [
                "Alex Wang",
                "Kyunghyun Cho",
                "Mike Lewis."
            ],
            "title": "Asking and answering questions to evaluate the factual consistency of summaries",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5008\u20135020, Online.",
            "year": 2020
        },
        {
            "authors": [
                "Wei Xu",
                "Courtney Napoles",
                "Ellie Pavlick",
                "Quanze Chen",
                "Chris Callison-Burch."
            ],
            "title": "Optimizing statistical machine translation for text simplification",
            "venue": "Transactions of the Association for Computational Linguistics, 4:401\u2013415.",
            "year": 2016
        },
        {
            "authors": [
                "Wei Xu",
                "Courtney Napoles",
                "Ellie Pavlick",
                "Quanze Chen",
                "Chris Callison-Burch."
            ],
            "title": "Optimizing statistical machine translation for text simplification",
            "venue": "Transactions of the Association for Computational Linguistics, 4:401\u2013415.",
            "year": 2016
        },
        {
            "authors": [
                "Weizhe Yuan",
                "Graham Neubig",
                "Pengfei Liu."
            ],
            "title": "Bartscore: Evaluating generated text as text generation",
            "venue": "Advances in Neural Information Processing Systems, volume 34, pages 27263\u201327277. Curran Associates, Inc.",
            "year": 2021
        },
        {
            "authors": [
                "Tianyi Zhang",
                "Varsha Kishore",
                "Felix Wu",
                "Kilian Q Weinberger",
                "Yoav Artzi."
            ],
            "title": "BERTScore: Evaluating text generation with BERT",
            "venue": "International Conference on Learning Representations, Online.",
            "year": 2020
        },
        {
            "authors": [
                "Xingxing Zhang",
                "Mirella Lapata."
            ],
            "title": "Sentence simplification with deep reinforcement learning",
            "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 584\u2013594, Copenhagen, Denmark. Association for",
            "year": 2017
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Automatic evaluation metrics are central to measuring progress in natural language generation (NLG) (Callison-Burch et al., 2006; Graham, 2015; Martin et al., 2018). Particularly challenging is the development of metrics for tasks such as summarization and text simplification. Compared to machine translation (MT), where good outputs are limited to those that reproduce all input information, there is a wider range of good summaries/simplifications because the degree of succinctness/simplicity of the output can vary greatly. A further complication is that multiple qualities of the output text must be evaluated, such as factual consistency or fluency.\nFor such tasks, traditional reference-based metrics such as ROUGE (Lin, 2004) and BLEU (Papineni et al., 2002) can therefore be limited by the\n\u2217Most of TS\u2019s contributions were made while he was a PhD student at Sorbonne Universite\u0301.\ndiversity of the available references.1 Indeed, previous work has shown their limited correlation with human quality judgments (Callison-Burch et al., 2006; Novikova et al., 2017; Sulem et al., 2018).\nA promising alternative is reference-less metrics which score a candidate text given only the source text. One such approach makes use of neural language models (LM) (Devlin et al., 2019; Lewis et al., 2020; Zhang et al., 2020; Sellam et al., 2020; Rei et al., 2020; Yuan et al., 2021). For example, BARTScore (Yuan et al., 2021) uses a LM to autoregressively score one text (e.g. a candidate) given another (e.g. the source or the reference). This provides the means to exploit the LM for the task for which it was trained. A second approach consists of question-based metrics (Wang et al., 2020; Durmus et al., 2020; Scialom et al., 2021a), which carry out automatic question generation and answering (QG/QA) based on the candidate and the source text. Typically, answers are assumed to be nouns which are extracted from the text.\nBoth approaches have achieved state-of-the-art correlation scores with human judgments, depending on the specific dataset and dimension of evaluation. However, there has been limited prior work in either paradigm on adapting a reference-less evaluation metric to multiple evaluation dimensions, tasks, and languages.2\nIn this work, we propose MaskEval, an adaptable reference-less LM-based metric which draws on the strengths of both approaches above. Like prior LM-based approaches, it can exploit indomain data for fine-tuning. However, it shares a key assumption of question-based metrics that not all tokens should be equally important for evaluating the output. In fact, we propose to learn this importance to further improve performance.\n1Multiple references, including automatically generated ones (Bawden et al., 2020), can improve this scenario, but cannot cover all possibilities and are also costly to produce.\n2We do not evaluate in the multilingual setting due to current lack of evaluation data.\nar X\niv :2\n20 5.\n12 39\n4v 2\n[ cs\n.C L\n] 1\n3 O\nct 2\n02 2\nMaskEval can be characterized by the following: (i) it features a masked language modeling task (MLM) over both the candidate and source text, inspired by the translation modeling objective (TLM) (Lample and Conneau, 2019), and (ii) learned weights that allow MaskEval to vary the importance given to words. We use this second feature to analyze the contribution of certain classes of words, and to selectively mask inputs to reduce computational cost.\nOur contributions can be summarized as follows: \u2022 We introduce MaskEval, a reference-less\nmetric for text transformation tasks based on a modified MLM framework and a novel learned weighter of words;3 \u2022 We evaluate MaskEval on English summarization, surpassing the best previous questionbased metric (Scialom et al., 2021a) in three out of four dimensions, and the best previous LM-based metric (Yuan et al., 2021) in factual consistency and fluency; \u2022 We show that MaskEval trained on summarization data transfers well to simplification, and vice versa. We also show that weighters trained on summarization can improve the metric\u2019s performance on simplification."
        },
        {
            "heading": "2 Related Work",
            "text": "While n-gram-based reference-based metrics such as ROUGE (Lin, 2004) and BLEU (Papineni et al.,\n3Our code will be made publicly available at: https: //github.com/YuLuLiu/MaskEval\n2002) are perhaps the most established in NLG, two more recent approaches have been shown to provide better correlations with human judgments of quality while being reference-less: those based on pre-trained neural LMs and those based on QG/QA.\nLM-based metrics Pretrained LMs have been used in different ways: (i) by comparing aligned token-level embeddings between the candidate and reference text, as with BERTScore (Zhang et al., 2020), (ii) by fine-tuning them either to directly reproduce human quality judgments (Sellam et al., 2020) or to rerank pairs of candidate texts (Rei et al., 2020), and (iii) by exploiting text-to-text pretrained LMs to score the candidate and source texts, as with BARTScore (Yuan et al., 2021), in a similar way to PRISM (Thompson and Post, 2020), which relies on multilingual paraphrasing as opposed to an LM. BERTScore has been shown to be poorly adapted to summarization (Scialom and Hill, 2021).\nBoth BARTScore and PRISM formulate the evaluation task as text generation, where the score is based on the log probability of the candidate being auto-regressively generated given the source text. While BARTScore achieves good correlations with human judgments for English summarization, it has a few potential disadvantages with respect to the way in which one text is scored based on the other: (i) the model is auto-regressive, and therefore, while the text being scored is conditioned on the entirety of the other text, it is only conditioned on the left context of itself and (ii) it uses a uni-\nform weighting scheme, assigning an equal importance to each generation step (alternative weighting schemes were reported to give lower results).\nWe seek to solve both of these disadvantages with MaskEval, by (i) replacing auto-regressive generation with successive masked language modeling (MLM), with prediction conditioned on both the candidate text and the source text, inspired by the translation language modeling (TLM) objective of XLM (Lample and Conneau, 2019) and (ii) learning a weighter to attribute varying importance to different words in the texts.\nQuestion-based metrics A parallel direction is the development of question-based metrics (Eyal et al., 2019; Scialom et al., 2019a; Durmus et al., 2020; Wang et al., 2020; Scialom et al., 2021a), where the idea is to automatically generate and then answer questions based on the candidate and the source text. The answers to the questions are nouns extracted from the texts. Different versions exist depending on which texts the questions/answers are conditioned on: Scialom et al. (2019a) generate questions by using the source document while Wang et al. (2020) and Durmus et al. (2020) do so using the candidate text. QuestEval unified both approaches, enabling further improvement. Most of the proposed metrics based on question-answering have targeted summarization.\nOne of the major advantages of question-based metrics is their interpretability, producing humanreadable questions and answers, which can offer insights into how a candidate text is either good or bad. However, they are limited by the necessity to have good systems for question generation and answering. This requires large-scale and high-quality data, which are not available in many languages other than English (Riabi et al., 2021).\n3 MaskEval Framework\nMaskEval scores a candidate text with respect to its source text by weighting word-level scores (from both the candidate and source text) in a twostep process (illustrated in Figure 1).\n1. Successive MLM: We perform successive MLM on the words of both the candidate and source text, comparing each prediction to the ground truth to produce one score per word.\n2. Weighted Score Aggregation: We aggregate the scores using a learned weighter (optimized\nto different quality dimensions) in order to vary the importance given to each word."
        },
        {
            "heading": "3.1 Word-level Segmentation in MLM",
            "text": "In both steps, we choose to assign scores (respectively weights) to linguistically meaningful tokens (words as defined by a language-specific word-level tokenizer,4) with the aim of making the method more interpretable and allowing us to perform linguistic analysis on learned weighters. In order to ensure that word-level segmentation is consistent with the existing segmentation of the pretrained MLMs we use (i.e. masking these linguistically defined units does not result in unnatural subword tokenization), we propose a method to reconcile the two by taking the intersection of their segmentation boundaries. An example of this method is shown in Figure 2. We refer to each text segment resulting from this scheme as a \u201cword\u201d.\nGiven candidate text x = (x1, . . . , xN ) containing N words and source text y = (y1, . . . , yM ) containing M words, the aim is to produce N +M scores. For model-internal subword segmentation, we refer to the tokenized candidate text as x = (t (1) x , . . . , t (n) x ) and the tokenized source text as y = (t(1)y , . . . , t (m) y ), where n and m are the number of subword tokens in the candidate and source texts respectively (N \u2264 n and M \u2264 n). We will use the notation t(k)x \u2208 xi to represent the fact that token t(k)x is part of word xi."
        },
        {
            "heading": "3.2 Masked Language Modeling",
            "text": "The goal of this step is to produce a list of scores, each corresponding to an MLM step (i.e. a word). Intuitively, each score evaluates how well a trained model can predict the word when it is masked, given the other words in its text and the other text.\nMasking and prediction We first create a sequence by concatenating x = (x1, x2, ..., xN ) and\n4We use spaCy tokenisers (Honnibal and Montani, 2017).\ny = (y1, y2, ..., yM ), placing a special separator mono-token word <sep> to denote the boundary between the two. In this respect, our MLM resembles the translation language modeling objective introduced in XLM (Lample and Conneau, 2019). Next, for each word position i in the x part of the sequence, we replace it with the mask token, thus creating masked sequence mxi . We take the original word xi as the ground-truth corresponding to this masked sequence. We do the same with each word position j in the y part of the sequence, resulting in masked sequence myj with ground-truth yj . This results in N +M masked sequences, each paired with its ground-truth word. The masked sequences are inputs to our MLM. We predict the masked words in the masked sequences mxi\u2019s and myj \u2019s, denoting the predictions as:\nx\u0302i = MLM(mxi) (1) y\u0302j = MLM(myj ) (2)\nScoring We score predictions x\u0302i and y\u0302j by computing their exact-match score against their corresponding ground-truth words xi and yj .5 We give the score of 1 if the prediction and the ground-truth word are exactly the same and 0 otherwise.6 We denote the scores by:\nsxi = Exact-Match(xi, x\u0302i) (3) syj = Exact-Match(yj , y\u0302j) (4)\nMLM Training When fine-tuning our MLM (we fine-tune pre-trained MLMs), we create examples using the above procedure on existing datasets of document pairs. The only difference is that for each pair, we first randomly choose which text to mask (candidate or source text), and then randomly select one word position within the chosen document. This creates one masked sequence per pair of texts. We train using cross-entropy loss between the predicted word and the ground-truth word."
        },
        {
            "heading": "3.3 Aggregation of Scores",
            "text": "In order to produce a single quality score (which can be adjusted for different dimensions), we aggregate the scores from the previous\n5We considered other scoring functions: i) computing the BERTScore between the prediction and the ground truth; ii) the perplexity score of the predicted word, and iii) the perplexity score of the ground-truth word. These scoring functions result in slightly worse performance than exact-match.\n6Both the prediction and the ground-truth word are lowercased before making the comparison.\nstep by computing a weighted sum as follows:\nMaskEval(x, y) = c N\u2211 i=1 wxisxi + (1\u2212 c) M\u2211 j=1 wyjsyj ,\n(5)\nwhere wxi (resp. wyj ) denotes the weight attributed to each word xi (resp. yj) of the candidate text x (resp. y), learned as described below and normalized such that \u2211N i=1wxi = 1 and \u2211M j=1wyj = 1. 7 c denotes the learned weight (between 0 and 1) attributed to the candidate text. The final MaskEval score is between 0 and 1."
        },
        {
            "heading": "He ais vigil ante He is a mask ed vigil ante",
            "text": ""
        },
        {
            "heading": "3.3.1 Learned weights",
            "text": "The weightswx andwy are learned using a separate attention-like module such that greater attention weights to words of interest, varying the importance given to each MLM step depending on their utility for the final score.\nAs for the MLM step (Section 3.2), the input to the weighter is the concatenation of the tokenized candidate and source texts:( t (1) x , . . . , t (n) x ,<sep>, t (1) y , . . . , t (m) y ) . We en-\ncode this input with a pretrained language model, resulting in contextual embeddings e(k)x (resp. e (k\u2032) y ) for each token in x (resp. y). We then apply an attention-like mechanism over these embeddings, in the form of a linear layer W followed by a separate softmax function for x and for y, such that the\n7Since x and y can be of different lengths, weights are defined and normalized separately for each text to avoid the longer text having more impact in the final score.\nscores for each sums to one. This can be expressed as follows (shown here for x, with 1 \u2264 k \u2264 n):\nvk = \u03c3 ( W \u00b7 ( e(1)x . . . e (n) x )) k\n(6)\nThe weight wxi attributed to each word xi is finally computed as the sum of its tokens\u2019 weights:\nwxi = \u2211\nk s.th. t (k) x \u2208xi\nvk (7)\nThe same process is applied to y, resulting in wyj for each word yj in y. An illustration of the weighter is shown in Figure 3.\nWeighter Training The weighter is a regressor model trained to mimic human judgment scores and therefore can be adapted to evaluate candidate texts along different dimensions of interest (e.g. factual consistency), as long as data annotated for those dimensions exists. Given a candidate paired with a source or reference text, and a human evaluation score for the candidate in the dimension we are optimizing for, we compute the MaskEval score (Equation 5) using the weights in Equation 7. The weighter\u2019s loss function is the mean squared error between the MaskEval score and the human evaluation score.\nBaseline weighting schemes As a baseline, we consider MaskEval with uniform weights, a setup where wxi = 1/N for all words xi in x, wyj = 1/M for all words yj in y, and c = 1/2. Since some quality of a candidate text (e.g. its fluency) should not be influenced by the source text, We also consider candidate-only MaskEval, a setup where wxi = 1/N for all words xi in x, and c = 1."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Experimental Details",
            "text": "We evaluate MaskEval on English summarization and simplification. See Appendix A for additional training details.\nMLMs We train two MLMs: one for summarization and one for simplification. Both are initialized with the T5 base model (Raffel et al., 2020), and then fine-tuned using the data described in Section 4.2, following the process described in Section 3. To be consistent with T5\u2019s training, we continue to use their masking format: the masked word is replaced with the token <extra id 0>, and when fine-tuning we format the ground-truth output\nby placing it between the tokens <extra id 0> and <extra id 1>.\nTo keep the MLM steps reasonably memoryefficient, we use a maximum sequence length of 512 tokens. Each sequence, at both training time and inference time, is modified as follows: a sliding window is applied on the text being masked so that each masked token has a maximum of 24 tokens on each side. Then, the other text is truncated at the token level to respect the sequence length limit.\nAttention Weight Module The weighter takes as input contextual embeddings from the T5 base model. We train two sets of weighters using humanannotated data described in Section 4.2: one for summarization and one for simplification. The data has annotations of quality across different dimensions, and we train a weighter for each quality dimension, using the average score given by human annotators in said dimension as the ground-truth value (scaled to range from 0 to 1)."
        },
        {
            "heading": "4.2 Data",
            "text": "Summarization Our MLM for summarization (MaskEvalSumm) is trained on the train set of CNN/Dailymail (Hermann et al., 2015; See et al., 2017) (\u223c287K documents and their summaries).\nTo train the weighters for summarization and to evaluate our metric on summarization, we use SummEval (Fabbri et al., 2021), one of the largest human-annotated datasets for English summarization. The collection comprises 100 news articles, randomly selected from the test set of CNN/DailyMail (Hermann et al., 2015). It contains 1,600 summary-article pairs, each pair scored by three annotators with respect to four dimensions: consistency (con), coherence (coh), fluency (flu), and relevance (rel).\nWe evaluate MaskEvalSumm with uniform weights on the whole of SummEval, allowing us to compare its performance with existing metrics (listed in Section 4.4). To train the weighters, we use a subset of SummEval (700 randomly selected examples), and then test its performance on the remaining 900 examples. We also evaluate the model with uniform weights on this same test subset to enable a fair comparison.\nSimplification We train our MLM for simplification, MaskEvalSimpl, on WikiLarge\u2019s train set (Zhang and Lapata, 2017) (\u223c296K simplifications).\nTo train the weighters and to evaluate our metric, we use human simplification judgments provided\nin ASSET (Alva-Manchego et al., 2020). This data is composed of randomly selected sentences from TurkCorpus (Xu et al., 2016a), with simplifications generated automatically (162 examples). Each simplification was scored with respect to three dimensions: fluency (flu), simplicity (sim) and meaning preservation (mea).\nWe evaluate MaskEvalSimpl with uniform weights on the whole test set, allowing us to compare to existing metrics (listed in Section 4.4). We train the weighters on a subset of ASSET (62 randomly selected examples), and then test on the remaining 100 examples. We also evaluate the model with uniform weights on this same test subset to enable a fair comparison."
        },
        {
            "heading": "4.3 Task Transfer",
            "text": "To explore transfer between summarization and simplification tasks, we evaluate MaskEval trained for one task on the other task, both with uniform weights and with the set of weighters trained for the other task."
        },
        {
            "heading": "4.4 Comparison to Existing Metrics",
            "text": "As baseline metrics, we first consider the length and the perplexity of the hypothesis summary, as they are reported to perform as well as some evaluation metrics (Durmus et al., 2022).\nReference-based We also consider three reference-based metrics: ROUGE (Lin, 2004) and BLEU (Papineni et al., 2002), and BERTScore (Zhang et al., 2020). They compare a hypothesis text to one or more manually produced groundtruth texts, contrarily to MaskEval, which is reference-less. For simplification, we additionally report SARI (Xu et al., 2016b), a standard n-gram-based metric standard simplification.\nQA-Based We consider three QA-based metrics for summarization: SummaQA (Scialom et al., 2019b), QAGS (Lee et al., 2021), and QuestEval (Scialom et al., 2021a). For simplification, we report QuestEval only since it is the only one, to the best of our knowledge, to have been adapted to evaluate simplification (Scialom et al., 2021b).\nLM-based We compare to the two LM-based metrics closest to ours, in their reference-less variants: PRISM (Thompson and Post, 2020) and BARTScore (Yuan et al., 2021).8\n8The performance of both metrics on SummEval (Pearson correlation) are computed using outputted scores\nSummEval, with reference(s)\n#refs con coh flu rel Ave.\nROUGE-1 11 18.1 20.1 14.9 35.6 22.2 ROUGE-L 11 15.7 15.6 13.8 33.4 19.6 BLEU 11 17.5 22.0 13.7 35.6 22.2 BERTScore-f 11 20.3 18.5 21.6 31.9 23.1\nROUGE-1 1 11.0 9.8 7.5 18.9 11.8 ROUGE-L 1 8.2 7.3 5.7 13.5 8.7 BLEU 1 8.9 3.9 4.0 12.7 7.4 BERTScore-f 1 8.7 9.8 10.6 17.9 11.8\nSummEval, reference-less\ncon coh flu rel Ave.\nPerplexity -3.1 15.7 8.9 19.8 10.3 Length 8.1 8.6 -2.9 26.6 10.1\nBARTScore 37.1 41.3 33.1 44.8 39.1 PRISM 29.7 28.1 24.8 29.7 28.1 SummaQA 8.3 8.0 -2.9 26.2 9.9 QAGS 20.4 7.7 16.8 9.1 13.7 QuestEval 42.0 24.0 28.4 39.2 33.5\nMaskEvalSumm uniform 44.6 27.6 40.6 35.6 37.1 MaskEvalSumm candidate 50.7 25.9 45.9 28.6 37.8\nMaskEvalSimpl uniform 44.6 29.4 37.6 35.2 36.7 opt Flu 41.2 24.5 34.5 32.6 33.2 opt Sim 40.6 24.9 34.5 32.2 33.1 opt Mea 41.5 25.2 34.8 32.3 33.5\nSummEval subset (900 pairs)"
        },
        {
            "heading": "5 Results",
            "text": ""
        },
        {
            "heading": "5.1 Summarization",
            "text": "We report the Pearson correlation between MaskEval scores and human judgments on the SummEval dataset in Table 1.\nMaskEvalSumm achieves good scores on average, the candidate-only variant surpassing QuestEval by 4.3 points, although remaining below BARTscore by 1.3 points. The slightly lower average score for MaskEvalSumm than BARTscore is mainly due to the lower score for coherence, which could be explained by the use of an MLM rather than auto-regressive decoding. However, it per-\nmade available by at https://github.com/neulab/ BARTScore/tree/main/SUM/SummEval. We report BARTScore, with the BART model finetuned with CNN/Dailymail (Hermann et al., 2015; See et al., 2017) and with prompt-tuning.\nASSET, with reference(s)\n#refs flu sim mea\nROUGE-1 10 42.0 42.4 61.8 ROUGE-L 10 40.9 41.0 59.4 BLEU 10 28.9 29.5 47.6 BERTScore-f 10 58.0 54.7 73.4 SARI 10 34.4 29.9 51.9\nROUGE-1 1 33.7 31.2 47.9 ROUGE-L 1 31.8 28.5 43.0 BLEU 1 25.6 23.5 29.9 BERTScore-f 1 48.5 46.8 61.4 SARI 1 30.1 25.2 33.4\nASSET, reference-less\nflu sim mea\nPerplexity 22.9 20.4 23.1 Length 2.5 -0.8 19.4\nBARTScore 57.5 52.0 70.6 PRISM 56.8 45.1 71.0 QuestEval 33.9 32.7 63.4\nMaskEvalSimpl uniform 50.5 43.6 67.5 MaskEvalSimpl candidate 53.6 50.3 63.6\nMaskEvalSumm uniform 48.6 40.3 66.7 opt Con 49.8 42.8 61.0 opt Coh 39.6 31.1 61.9 opt Flu 58.7 51.8 56.9 opt Rel 44.0 34.1 65.0\nAttention weight module\u2019s test set (100 pairs)\nforms better than all previous metrics on two out of four dimensions, outperforming both BARTScore and QuestEval for consistency and fluency. The dimension that benefits most from our approach with respect to the previous best score is fluency (45.9>33.1).\nWith learned weights, MaskEvalSumm is able to improve its performance in all four dimensions with respect to uniform weighting, with relevance being improved the most (+18%)."
        },
        {
            "heading": "5.2 Simplification",
            "text": "We report the Pearson correlation between MaskEval scores and human judgments on the simplification evaluation set in Table 2. Without considering the transfer scenario, The highest performing metric is the reference-based BERTScore, when it has access to 10 references. The best reference-less metric is BARTscore, although PRISM is best for meaning preservation.\nMaskEvalSimpl with uniform weighting has good correlations, behind both BARTscore and PRISM, but outperforming QuestEval. Given the very small number of examples that could be used to train the weighters, MaskEvalSimpl with learned weights is unable to improve its performance in any of the dimensions in ASSET. We nevertheless report these figures for completeness."
        },
        {
            "heading": "5.3 Transfer Between Tasks",
            "text": "We also report scores for transfer between the tasks in Tables 1 and 2. MaskEval with uniform weights trained for the task performs similarly to its counterpart trained for the other task. This shows that transfer is possible between the tasks.\nFor the weighted versions of the metrics, MaskEvalSimp does not provide improvements when transferring, which is not unexpected given its poor performance on simplification. However positive results can be seen when transferring from the weighted version of MaskEvalSumm to simplification. In particular, with weights optimized for summarization fluency, MaskEvalSumm obtains state-of-the-art result on fluency, and greatly improves simplificity. Whilst it is expected that optimizing for summarization fluency improved simplification fluency, the improvements in the simplicity dimension are more surprising, and show the judgments for fluency are easily influenced by other evaluation dimensions. However, these results do show the potential to be able to transfer across dimensions from different tasks, which could be interesting when there are few annotations available."
        },
        {
            "heading": "6 Discussion",
            "text": "We choose to analyze our highest performing set of weighters, those optimized for summarization dimensions. In the following sections, the learned weight of a word xi (resp. yj) includes the candidate weight, thus equaling c wxi (resp. (1\u2212c)wyj )."
        },
        {
            "heading": "6.1 Analysis of the Weighting Function",
            "text": "Figure 4 shows the average weight distribution across parts of speech on summary-source pairs from the test set of SummEval. We can see that by using weights optimized for fluency, MaskEval primarily uses MLM steps masking adpositions, determinants, and other POS tags (i.e. conjunctions, numbers, etc.) in the summary. This is expected since the fluency of a summary does not involve the source document. This equally applies to simplifi-\ncation, which could explain the great performance we obtain during task transfer.\nSome weights behave in an unexpected way: the weights optimized for factual consistency gives more importance to determiners than to nouns, which goes against the assumption commonly held by existing question-based metrics that nouns contain the most salient information."
        },
        {
            "heading": "6.2 Sparsity: Towards Selective Masking",
            "text": "An important factor for an automatic metric to be widely adopted is computational efficiency. This was one of the important concerns with questionbased metrics. We propose to use our weighters to selectively mask the input texts, only calculating scores based on the highest weighted MLM steps, in order to reduce the number of masked predictions while best maintaining performance.\nIn Figure 5, we report the Pearson correlation on the test subset of SummEval when only some weighted MLM steps are used in the computation of the MaskEval score (i.e we apply the weighter before the MLM step). We sort them by their weight , and only keep the top MLM steps whose learned weights sum up to a set threshold, computing the MaskEval score using the retained MLM steps only. We can see that a threshold of \u223c0.70 to preserve over 90% of MaskEval\u2019s original performance, and to match the performance of MaskEval with uniform weights. This threshold corresponds to only considering (on average) 25 to 130 MLM steps of a total of 480. This suggests that with learned weights, the number of MLM steps necessary can be greatly reduced without compromising the performance."
        },
        {
            "heading": "7 Conclusion",
            "text": "We have proposed MaskEval, a metric for summarization and simplification that scores words in source and candidate texts using a MLM, then applies a learned weighting function over those scores, optimized to the task and evaluation dimension. Our analysis shows that different weights are applied depending on the dimension and that some parts of speech such as adpositions are more important than previously suspected. Our weighting function also allows us to produce a light-weight version of the metric, which uses only\u223c20% of the words to reach comparable correlation performance to using uniform weighting over all words.\nIn future work, we plan to test the approach for other text generation tasks (e.g. MT) and for languages other than English, depending on the availability of such data.\nLimitations\nOur LM-based metric can be easily adapted to a multilingual setting by finetuning a multilingual LM instead of the English one used in this paper. However, due to the lack of human annotations in multilingual summarization and simplification, we have not yet tested this capability. The capability of our metric to provide a good evaluation for texts from other tasks, other text generation systems, and other data distributions is also left to future work. Note that We considered using the Multi-SummEval dataset (Koto et al., 2021) to test the multilingual capacity of our metric. However, we decided against this, due to potential problems we identified in the human annotation scheme employed in Multi-SummEval. Notably, (i) annotators did not have access to source documents and annotated on the basis of a single reference text and (ii) automatic evaluation metrics reported having a higher performance than human annotators.\nWe measure the performance of our proposed metric by computing the correlation of its output scores with that given by human annotators. Our results are therefore reliant on the quality of evaluation datasets. For SummEval, for example, we have employed scores given by experts (annotators who have written academic papers on the topic of summarization). The expertise of these annotators may introduce a bias in the evaluation set as their judgments might differ from that of regular users of summarization systems."
        }
    ],
    "title": "MaskEval: Weighted MLM-Based Evaluation for Text Summarization and Simplification",
    "year": 2022
}