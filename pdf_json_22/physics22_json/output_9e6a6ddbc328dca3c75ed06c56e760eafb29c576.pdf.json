{
    "abstractText": "Current proposals for quantum compilers require the synthesis and optimization of linear reversible circuits and among them CNOT circuits. Since these circuits represent a significant part of the cost of running an entire quantum circuit, we aim at reducing their size. In this paper we present a new algorithm for the synthesis of CNOT circuits based on the solution of the syndrome decoding problem. Our method addresses the case of ideal hardware with an all-to-all qubit connectivity and the case of near-term quantum devices with restricted connectivity. For both cases, we present benchmarks showing that our algorithm outperforms existing algorithms.",
    "authors": [
        {
            "affiliations": [],
            "name": "Timoth\u00e9e Goubault de Brugi\u00e8re"
        },
        {
            "affiliations": [],
            "name": "Marc Baboulin"
        },
        {
            "affiliations": [],
            "name": "Beno\u00eet Valiron"
        },
        {
            "affiliations": [],
            "name": "Simon Martiel"
        },
        {
            "affiliations": [],
            "name": "Cyril Allouche"
        }
    ],
    "id": "SP:b6a939eea76356468e92ff7c35dc6bdb85768e2e",
    "references": [
        {
            "authors": [
                "M. Amy",
                "P. Azimzadeh",
                "M. Mosca"
            ],
            "title": "On the controlled-NOT complexity of controlled-NOT\u2013phase circuits",
            "venue": "antum Science and Technology",
            "year": 2018
        },
        {
            "authors": [
                "M. Amy",
                "D. Maslov",
                "M. Mosca"
            ],
            "title": "Polynomial-time T-depth optimization of Cli ord+T circuits via matroid partitioning",
            "venue": "IEEE Transactions on Computer- Aided Design of Integrated Circuits and Systems",
            "year": 2014
        },
        {
            "authors": [
                "S. Arora",
                "L. Babai",
                "J. Stern",
                "Z. Sweedyk"
            ],
            "title": "The hardness of approximate optima in la ices, codes, and systems of linear equations",
            "venue": "Journal of Computer and System Sciences",
            "year": 1997
        },
        {
            "authors": [
                "E. Berlekamp",
                "R. McEliece",
                "H. Van Tilborg"
            ],
            "title": "On the inherent intractability of certain coding problems",
            "venue": "IEEE Transactions on Information Theory",
            "year": 1978
        },
        {
            "authors": [
                "T.G. de Brugiere",
                "M. Baboulin",
                "B. Valiron",
                "S. Martiel",
                "C. Allouche"
            ],
            "title": "antum CNOT circuits synthesis for NISQ architectures using the syndrome decoding problem",
            "venue": "in: International Conference on Reversible Computation,",
            "year": 2020
        },
        {
            "authors": [
                "E.T. Campbell",
                "H. Anwar",
                "D.E. Browne"
            ],
            "title": "Magic-state distillation in all prime dimensions using quantum Reed-Muller codes",
            "venue": "Physical Review X",
            "year": 2012
        },
        {
            "authors": [
                "E.T. Campbell",
                "B.M. Terhal",
                "C. Vuillot"
            ],
            "title": "Roads towards fault-tolerant universal quantum computation",
            "venue": "Nature 549,",
            "year": 2017
        },
        {
            "authors": [
                "A.M. Childs",
                "E. Schoute",
                "C.M. Unsal"
            ],
            "title": "Circuit transformations for quantum architectures, in: 14th Conference on the Theory of antum Computation, Communication and Cryptography (TQC",
            "year": 2019
        },
        {
            "authors": [
                "A. Cowtan",
                "S. Dilkes",
                "R. Duncan",
                "A. Krajenbrink",
                "W. Simmons",
                "S. Sivarajah"
            ],
            "title": "On the qubit routing problem, in: 14th Conference on the Theory of antum Computation, Communication and Cryptography (TQC",
            "year": 2019
        },
        {
            "authors": [
                "M.R. Garey",
                "D.S. Johnson"
            ],
            "title": "Computers and Intractability: A Guide to the Theory of NP-Completeness",
            "year": 1979
        },
        {
            "authors": [
                "G.H. Golub",
                "C.F. Van Loan"
            ],
            "title": "Matrix Computations",
            "year": 1996
        },
        {
            "authors": [
                "D. Go esman"
            ],
            "title": "Stabilizer Codes and antum Error Correction",
            "venue": "Ph.D. thesis. Caltech",
            "year": 1997
        },
        {
            "authors": [
                "M. Haghparast",
                "M. Mohammadi"
            ],
            "title": "Novel quantum compressor designs using new genetic algorithm-based simulator, analyzer and synthesizer so ware in nanotechnology",
            "venue": "International Journal of antum Information",
            "year": 2010
        },
        {
            "authors": [
                "L. Henriet",
                "L. Beguin",
                "A. Signoles",
                "T. Lahaye",
                "A. Browaeys",
                "G.O. Reymond",
                "C. Jurczak"
            ],
            "title": "antum computing with neutral atoms. antum",
            "year": 2020
        },
        {
            "authors": [
                "L.E. Heyfron",
                "E.T. Campbell"
            ],
            "title": "An e icient quantum compiler that reduces T count",
            "venue": "antum Science and Technology",
            "year": 2019
        },
        {
            "authors": [
                "G. Kachigar",
                "J.P. Tillich"
            ],
            "title": "2017. antum information set decoding algorithms",
            "venue": "in: International Workshop on Post- antum Cryptography,",
            "year": 2017
        },
        {
            "authors": [
                "E. Kirshanova"
            ],
            "title": "Improved quantum information set decoding",
            "venue": "in: International Conference on Post- antum Cryptography,",
            "year": 2018
        },
        {
            "authors": [
                "A. Kissinger",
                "A. Meijer-van de Griend"
            ],
            "title": "CNOT circuit extraction for topologically-constrained quantum memories",
            "venue": "antum Information and Computation 20,",
            "year": 2020
        },
        {
            "authors": [
                "A. Kissinger",
                "J. van de Wetering"
            ],
            "title": "PyZX: Large Scale Automated Diagrammatic Reasoning, in: 16th International Conference on antum Physics and Logic, Open Publishing Association",
            "year": 2020
        },
        {
            "authors": [
                "A. Kissinger",
                "J. van de Wetering"
            ],
            "title": "Reducing the number of non-Cli ord gates in quantum circuits",
            "venue": "Physical Review A",
            "year": 2020
        },
        {
            "authors": [
                "R.E. Korf"
            ],
            "title": "Real-time heuristic search",
            "venue": "Artificial intelligence",
            "year": 1990
        },
        {
            "authors": [
                "G. Li",
                "Y. Ding",
                "Y. Xie"
            ],
            "title": "Tackling the qubit mapping problem for NISQera quantum devices",
            "venue": "in: International Conference on Architectural Support for Programming Languages and Operating Systems,",
            "year": 2019
        },
        {
            "authors": [
                "D. Maslov"
            ],
            "title": "Optimal and asymptotically optimal NCT reversible circuits by the gate",
            "venue": "types. antum Information & Computation",
            "year": 2016
        },
        {
            "authors": [
                "B. Nash",
                "V. Gheorghiu",
                "M. Mosca"
            ],
            "title": "antum circuit optimizations for NISQ architectures",
            "venue": "antum Science and Technology",
            "year": 2020
        },
        {
            "authors": [
                "K.N. Patel",
                "I.L. Markov",
                "J.P. Hayes"
            ],
            "title": "Optimal synthesis of linear reversible circuits",
            "venue": "antum Information & Computation",
            "year": 2008
        },
        {
            "authors": [
                "M. Pedram",
                "A. Shafaei"
            ],
            "title": "Layout optimization for quantum circuits with linear nearest neighbor architectures",
            "venue": "IEEE Circuits and Systems Magazine",
            "year": 2016
        },
        {
            "authors": [
                "E. Prange"
            ],
            "title": "The use of information sets in decoding cyclic codes",
            "venue": "IRE Transactions on Information Theory",
            "year": 1962
        },
        {
            "authors": [
                "J. Preskill"
            ],
            "title": "2018. antum computing in the NISQ era and beyond",
            "venue": "antum",
            "year": 2018
        },
        {
            "authors": [
                "A. Vardy"
            ],
            "title": "Algorithmic complexity in coding theory and the minimum distance problem",
            "venue": "in: Symposium on Theory of Computing,",
            "year": 1997
        },
        {
            "authors": [
                "R. Wille",
                "O. Keszocze",
                "M. Walter",
                "P. Rohrs",
                "A. Cha opadhyay",
                "R. Drechsler"
            ],
            "title": "Look-ahead schemes for nearest neighbor optimization of 1Dand 2D quantum circuits, in: Asia and South Pacific",
            "venue": "Design Automation Conference,",
            "year": 2016
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "antum compilers transform a quantum algorithm into an optimized sequence of instructions (elementary gates) directly executable by the hardware. The most common universal set of gates for this task is the Cli ord+T gate set, used in many quantum architectures [8].\nFor fault-tolerant computation, the T gate is considered to be the most costly gate to implement fault-tolerantly (using for instance magic state distillation protocols [7]) and many e orts have been made to reduce their number in quantum circuits [16, 2, 21]. Yet, when implementing complex quantum algorithms it is estimated that the total number of CNOT gates increases much more rapidly with the number of qubits than the number of T gates, and it is likely that the global CNOT cost will not be negligible on large sized registers [16, 24] compared to the global T cost.\nFor the moment, fault-tolerant quantum computers are not available. The current devices are medium-sized chips (less than 100 qubits) called NISQ computers\n\u2217This document is the author\u2019s version of the corresponding research manuscript prior to formal peer review. An updated version is published in Science of Computer Programming, Volume 214, 1 February 2022, 102726, 10.1016/j.scico.2021.102726.\nar X\niv :2\n20 1.\n06 45\n7v 1\n[ qu\nan t-\nph ]\n(Noisy Intermediate Scale antum) [29]. Those computers are prone to errors and especially the two-qubit gates such as the CNOT gate have a much lower fidelity compared to one-qubit gates, including the T gate (see, e.g., the results for Rige i\u2019s Aspen-8 chip [30]). So, for NISQ processors it is a priority to minimize the number of CNOT gates. Moreover, for some technologies like superconducting quantum computers, the execution of the CNOT gates is subject to constraints. A physical qubit on the hardware can only interact with its neighbors, restricting the 2-qubit gates \u2014such as CNOT\u2014 one can apply. Taking into account these constraints is a crucial and di icult task for the design of quantum algorithms and the optimization of the corresponding quantum circuits. In particular, in the literature several works present post-processing techniques to convert with minimum overhead a circuit designed for an ideal hardware to a circuit designed for a specific architecture [9].\nOverall the CNOT gate is a costly resource that has to be optimized, either for NISQ or fault-tolerant computations, and the optimization may be subject to constraints making the task very challenging.\nOne way to optimize the CNOT count in quantum circuits is to focus on circuits consisting solely of CNOT gates, also called linear reversible circuits. They represent a class of quantum circuits playing a fundamental role in quantum compilation. They are part of the so-called Cli ord circuits and the CNOT+T circuits, two classes of circuits that have shown crucial utility in the design of e icient quantum compilers [16, 2] and error correcting codes [13, 7]. For instance the Tpar optimizer [2] takes a Cli ord+T circuit as input and decomposes it into a series of CNOT+T circuits separated by Hadamard gates. Then each CNOT+T circuit is optimized and re-synthesized by successive syntheses of CNOT circuits and applications of T gates.\nHence the synthesis of CNOT circuits naturally occurs in general quantum compilers and giving e icient algorithms for optimizing CNOT circuits will then be of u ermost importance for both short-term and long-term applications.\nContribution and outline of the paper\nIn this paper we focus on the size optimization of linear reversible circuits. We present a new method for the synthesis of CNOT circuits relying on solving a well-known cryptographic problem: the syndrome decoding problem. Our algorithm transforms the synthesis problem into a series of syndrome decoding problems and we propose several methods to solve this particular subproblem. This method, initially designed for a full qubit connectivity, is robust enough to be extended to partial connectivity.\nThe outline of the paper is the following: in Section 2 we present the basic notions and the state of the art in the synthesis of linear reversible circuits. We first present our algorithm in the case of an all-to-all connectivity in Section 3. Then we extend it to the case of restricted connectivities in Section 4. Benchmarks and discussions are given at the end of Sections 3 and 4.\nThis paper is an extended version of a paper published in the conference proceedings of RC\u201920. [6]."
        },
        {
            "heading": "2 Background and state of the art",
            "text": "Synthesis of a linear reversible function Let F2 be the Galois field of two elements. A linear reversible function f on n qubits applies a linear Boolean function on the inputs to each qubit. Given x \u2208 Fn2 as inputs, the output of qubit i is\nfi (x) = \u03b1 i \u22c5 x = \u03b1 i1x1 \u2295 \u03b1 i2x2 \u2295 ... \u2295 \u03b1 inxn where \u2295 is the bitwise XOR operation and the \u03b1 i \u2019s are Boolean vectors also called parities. The action of f can be represented as an n\u00d7n binary matrixAwithA[i , \u2236] = \u03b1 i (using Matlab notation for row selection) and f (x) = Ax . In other words each row of A corresponds to the parity held by the corresponding qubit a er application of A. By reversibility of f , A is also invertible in F2. The application of two successive operators A and B is equivalent to the application of the operator product BA.\nWe are interested in synthesizing general linear reversible Boolean functions into reversible circuits, i.e., series of elementary reversible gates that can be executed on a suitable hardware. To that end we use the CNOT gate, it performs the following 2-qubit operation:\nCNOT(x1, x2) = (x1, x1 \u2295 x2) where x1, resp. x2, is the parity held by the control qubit, resp. the target qubit. If applied a er an operator A, the total operator (A + CNOT) is given from A by adding the row of the control qubit to the row of the target qubit. Such row operations are enough to reduce any invertible Boolean matrix to the identity matrix, so the CNOT gate can be solely used to implement any linear reversible operator. Overall, a CNOT-based circuit can be simulated e iciently: starting from A = I the identity operator, we read sequentially the gates in the circuit and apply the corresponding row operations to A.\nWe use the size of the circuit, i.e., the number of CNOT gates in it to evaluate the quality of our synthesis. The size of the circuit gives the total number of instructions the hardware has to perform during its execution. Due to the presence of noise when executing every logical gate, it is of interest to have the shortest circuit possible.\nConnectivity constraints At the current time, for superconducting technologies, full connectivity between the qubits cannot be achieved. The connections between the qubits are given by a connectivity graph, i.e., an undirected, unweighted graph where 2-qubit operations, such as the CNOT gate, can be performed only between neighbors in the graph. Examples of connectivity graphs from current physical architectures are given on Fig. 1.\nLUdecomposition Given the matrix representationA of a generic linear reversible operator, we can always perform an LU decomposition [12] such that there exists an upper (resp. lower) triangular matrix U (resp. L) and a permutation matrix P such that A = PLU . This decomposition is not unique, several choices of (P , L,U ) are possible. The invertibility of A ensures that the diagonal elements of L and U are all equal to 1. In the remainder of this paper, the term \u201ctriangular operator\u201d stands for\nan operator whose corresponding matrix is either upper or lower triangular. The LU decomposition is at the core of our synthesis of general linear reversible Boolean operators: synthesizingU , L, P and concatenating the circuits gives an implementation of A.\nState of the art In the all-to-all connectivity case the best algorithm is the PatelMarkov-Hayes (PMH) algorithm [26, Algo. 1]. It reaches an asymptotic optimum and produces circuits of size (n2/ log2(n)). This algorithm is for instance used in the Tpar and Gray-Synth algorithms [1, 2] so any improvement over [26, Algo. 1] will also improve any quantum compiler that relies on it.\nFor architectures with restricted connectivity, the first proposed approach has been to transform the circuits given by an unrestricted algorithm with swap insertion algorithms to match the connectivity constraints [23, 32, 27]. To produce more e icient circuits, two concomitant papers proposed a modification of the Gaussian elimination algorithm [19, 25]. They synthesize the operator column by column similarly to the Gaussian Elimination algorithm but they use Steiner trees to compute the shortest sequence of CNOT gates for the synthesis of one column. In [19] the authors compare their method based on Steiner trees against two compilers: Rige i Computing\u2019s ilC and Cambridge antum Computing\u2019s t|ket\u27e9 that both produced state of the art results on benchmarks published by IBM [10]. The benchmarks show a consequent savings in the total number of CNOT gates in favor of the Steiner tree method, so we consider that the work in [19] is state-of-the-art and we will compare solely to their algorithm.\nFor other classes of reversible circuits various methods have been proposed, see e.g. the use of genetic algorithms for the design of compressor trees [14]."
        },
        {
            "heading": "3 Algorithm for an all-to-all connectivity",
            "text": "In this section we present our algorithm in the case of a complete connectivity between the qubits. We focus on the synthesis of a lower triangular operator L \u2208 F n\u00d7n2 . What follows can be straightforwardly extended to the case of upper triangular operators and to general operators using the LU decomposition. With an all-to-all connectivity one can avoid to apply the permutation P by doing a post-processing of the circuit that would transfer the permutation operation directly at the end of the total circuit. This can be done without any overhead in the number of gates.\nA circuit implementing L can solely consist of \u201coriented\u201d CNOTs, whose controlled qubit i and target qubit j satisfy i < j . The circuit given by the Gaussian elimination algorithm is an example. For this particular kind of circuits, a CNOT applied to a qubit k does not have any influence on the operations performed on the first k \u2212 1 qubits: removing such a CNOT will not modify the result of the synthesis of the first k \u2212 1 parities. We use this property to design a new algorithm where we synthesize L parity by parity and where we reuse all the information acquired during the synthesis of the first k parities to synthesize parity k + 1.\nGiven Ln\u22121 = L[1:n \u2212 1, 1:n \u2212 1] (again using Matlab notation), a circuit C implementing the operator ( Ln\u22121 00 1 ) and considering that we want to synthesize the operator L = ( Ln\u22121 0s 1 ) the core of our algorithm consists in adding a sequence of CNOTs to C such that we also synthesize the parity s of the n-th qubit. During the execution of C , applying a CNOT i \u2192 n will add the parity currently held by qubit i to the parity of qubit n without impacting the synthesis of the first n \u2212 1 parities. In other words, if we store in memory all the parities that appeared on all n \u2212 1 qubits during the execution of the circuit C , we want to find the smallest subset of parities such that their sum is equal to s . Then when a parity belonging to this subset appears during the execution of C , on qubit i for instance, we insert in C a CNOT i \u2192 n. We ultimately have a new circuit C \u2032 that implements L.\nThe problem of finding the smallest subset of parities whose sum equals s can be recast as a classical cryptographic problem. Assuming that H \u2208 F n\u22121\u00d7m2 is a Boolean matrix whose columns correspond to the m available parities, any Boolean vector x satisfying Hx = sT gives a solution to our problem and the Hamming weight of x , wt (x), gives the number of parities to add, i.e., the number of CNOTs to add to C . We are therefore interested in an optimal solution of the problem\nminimize x\u2208Fm2 wt (x)\nsuch that Hx = sT . (1) Problem 1 is an instance of the syndrome decoding problem, a well-known problem in cryptography. The link between CNOT circuit synthesis and the syndrome decoding problem has already been established in [1], yet it was used in a di erent problem for proving complexity results (under the name of Maximum Likelihood Decoding problem) and the authors did not pursue the optimization. The syndrome decoding problem is presented in more details in Section 3.1.\nTo summarize, we propose the following algorithm to synthesize a triangular op-\nerator L. Starting from an empty circuit C , for i from 1 to n perform the three following steps:\n1. scan circuit C to compute all the parities available on a single matrix H , 2. solve the syndrome decoding problem Hx = s with s the parity of qubit i , 3. add the relevant CNOT gates to C depending on the solution obtained.\nProvided that the size ofC remains polynomial in n, which will be the case, then steps 1 and 3 can be performed in polynomial time and in practice in a very short amount of time. The core of the algorithm, both in terms of computational complexity and final circuit complexity, lies in Step 2."
        },
        {
            "heading": "3.1 Syndrome decoding problem",
            "text": "In its general form, the syndrome decoding problem is known to be NP-Hard [4] and cannot be approximated by a constant factor [3]. A good overview of how di icult the problem is can be found in [31].\nWe give two methods for solving the syndrome decoding problem. The first one is an optimal one and uses integer programming solvers. The second one is a greedy heuristic for providing sub-optimal results in a short amount of time."
        },
        {
            "heading": "3.1.1 Integer programming formulation",
            "text": "The equality Hx = s is a Boolean equality of n lines. For instance the first line corresponds to\nH1,1x1 \u2295 H1,2x2 \u2295\u2026 \u2295 H1,mxm = s1 . We transform it into an \u201cinteger-like\u201d equality constraint. A standard way to do it is to add an integer variable t and to create the constraint H1,1x1 + H1,2x2 + \u2026 + H1,mxm \u2212 2t = s1 . If we write c = (1, ..., 1, 0, ..., 0)T \u2208 \u2115m+n and A = [H | \u2212 2In] then the syndrome decoding problem is equivalent to the integer linear programming problem\nmin x\u2208Fm2 ,t\u2208\u2115n cT \u22c5 [x ; t ] such that A[x ; t ] = s . (2)"
        },
        {
            "heading": "3.1.2 A cost minimization heuristic",
            "text": "Although the integer programming approach gives optimal results, it is very unlikely that it will scale up to a large number of qubits. Moreover, to our knowledge the other existing algorithms proposed in the literature give exact results, they are complex to implement and their time complexity remains exponential with the size of the problem. We therefore have to consider heuristics to compute an approximate solution in a much shorter amount of time.\nWe use a simple cost minimization approach: starting with the parity s we choose at each iteration the parity v in H that minimizes the Hamming weight of v \u2295 s and we pursue the algorithm with the new parity v \u2295 s . The presence of the canonical vectors in H (as we start with the identity operator) is essential because they ensure that this method will ultimately converge to a solution.\nA simple way to improve our heuristic is to mimic path finding algorithms like Real-Time A* [22]. Instead of directly choosing the parity that minimizes the Hamming weight, we look up to a certain horizon and we make one step in the direction of the most promising path. To control the combinatorial explosion of the number of paths to browse, we only expand the most promising parities at each level. We set the maximum width to m and the depth to k so that it represents at most mk paths to explore. With suitable values of m and k we can control the total complexity of the algorithm. A limitation of such a simple approach is that we can store the same path but with di erent parities order: we decided to ignore this limitation in order to keep a simple implementation.\nLastly, we introduce some randomness by solving the problem PHx = P s for several random change of basis matrices P . Repeating this several times for one syndrome decoding problem increases the chance to find an e icient solution. This technique has been proven to be e icient for a class of cryptographic algorithms called Information Set Decoding [28], even though the complexity of these algorithms remains exponential."
        },
        {
            "heading": "3.2 Benchmarks",
            "text": "All the code is wri en in Julia and executed on a MacBook Air 1.8 GHz Intel Core i5.\nWe generate random operators by generating random circuits with randomly placed CNOT gates. When the number of input gates is su iciently large we empirically note that the operators generated represent the worst case scenario.\nWe first generate an average complexity for di erent problem sizes: for n = 1..150 we generated 20 random operators on n qubits with more than n2 gates to reach with high probability the worst cases.\nWe present our results in three batches, one for each class of methods used for solving the syndrome decoding problem:\n\u2022 the cost minimization methods,\n\u2022 the simplest cost minimization method with random changes of basis (the Information Set Decoding strategy),\n\u2022 the integer programming solver."
        },
        {
            "heading": "3.2.1 Greedy solvers comparison.",
            "text": "We first present the results of the cost minimization techniques. We compare the performance of the greedy methods for di erent values of the width and depth search. For a fair comparison, the size of the search tree must be roughly the same in each case, i.e., at widthdepth fixed. We benchmarked the following cases:\n\u2022 width=Inf, depth=1,\n\u2022 width=60, depth=2,\n\u2022 width=15, depth=3,\n\u2022 width=8, depth=4,\n\u2022 width=5, depth=5.\nThe results are given in Fig. 2. For clarity, instead of plo ing the size of the circuits we plot the ratio between the size of the circuits given by our algorithms and the state of the art algorithm [26, Algo. 1]. We stopped the calculations when the running time was too large for producing benchmarks in several hours.\nOverall, it seems be er to increase the depth of search than the width. Yet, for a depth of 3 and more, the improvements are not as clear. Clearly the worst results are for the depth 1 and depth 2 cases. Then it seems that the decreasing of the width plays a role as the results for depth 5 are not as consistent as for depth 4. This might be due to the fact that we store several times the same solutions: with small width we increase the chances to store only a few di erent solutions but with several vector orders.\nFor problems smaller to 130 qubits we manage to outperform [26, Algo. 1] with more than 30% of gain. Then, as the number of qubits increases our method performs worse. We ran a few computations for much larger problems and the results are that [26, Algo. 1] produces shorter circuits whenever n goes approximately beyond 400. This raises the question of whether it is due to the method in itself or to the solution of the syndrome decoding that becomes less and less optimal as the problem size increases. We leave this question as a future work."
        },
        {
            "heading": "3.2.2 Greedy method + Information Set Decoding (ISD)",
            "text": "We now investigate the role of doing the cost minimization process but with repeated random changes of basis. For computational reasons, we can only repeat the case depth=1, the fastest by far, as repeating the others would take too much time. To investigate how the performance of the algorithm varies with the number of iterations, we considered the following cases:\n\u2022 with 50 iterations,\n\u2022 with 100 iterations,\n\u2022 with 500 iterations,\n\u2022 with 1000 iterations,\n\u2022 with 10000 iterations. The results are given in Fig 3. We focused on the range n = 1...130. Given that the greedy solver with depth=4 approximately provides the best results from the greedy solvers category, and overall the best results, we plot the ratio between the circuit\nsize returned by the ISD strategies and the circuit size returned by this new state-ofthe-art method.\nNot surprisingly, increasing the number of iterations increases the chances to find good solutions to the syndrome decoding problem, thus to find short circuits. What is more surprising maybe is the e iciency of such process. In terms of computational time, the case niter=500 is roughly as costly as using a greedy solver with depth 4. So for an equivalent amount of computational time, the ISD strategy outperforms our greedy solvers with up to 8% of gain. With more iterations we reach up to 10% of savings.\nOverall, the ISD strategy works well until the search space is too large to be e iciently explored with a random method and so few tries. Inevitably, even with niter=1000, the performance of the ISD strategy deteriorates and eventually the performance of the solver is close to the initial greedy solver with only one try. Nonetheless, we believe this gives good insights on the question we asked above, whether the bad performances of the syndrome decoding based methods on large instances are due to the method in itself or in the quality of the solutions of the syndrome decoding problems. When we increase the number of iterations we significantly improve the performance of the method and the range of validity of the method. It seems to go in the sense that it is di icult to optimally solve the syndrome decoding problem and that this has direct consequences on the final result."
        },
        {
            "heading": "3.2.3 Integer programming solvers comparison.",
            "text": "We are able to use an integer programming solver for problems up to 50 qubits. For larger problems, the exponential scalability of such solvers make it impossible to use\nthem in a reasonable amount of time. Nonetheless, it is possible to use integer programming solvers to \"improve\" a solution. Indeed, for some solvers we can ask them to focus on a strategy that tries to find quickly a good solution. Then if we force the solver to stop a er a certain time limit, we can control the overall complexity of our syndrome decoding solver. Given that we can give the solver an initial solution to the problem, we are ensured that the solver will return a valid solution.\nUnfortunately, the results are not significant. For problems on 50 qubits or less, our results are identical to the ones obtained with the ISD strategy, proving that this method can almost optimally solve the syndrome decoding problem on small instances. For larger problems, we gave the solver the solution of the ISD strategy with 500 iterations and we let the solver compute a be er solution during 5 seconds. Given that for an operator on n qubits we have to solve 2n instances of the syndrome decoding problem, this represents a non negligible amount of extra time. Overall, the savings are negligible (< 1%) compared to the ISD strategy with 500 iterations. Increasing the number of iterations in the ISD strategy gives much be er savings and in a much shorter time, so overall integer programming solvers should not be the first option to use."
        },
        {
            "heading": "3.2.4 Experiments on simpler operators",
            "text": "We now look at the performance of the algorithms on a specific number of qubits, here n = 60, but for di erent input circuit sizes. This experiment reveals how close to optimal our algorithm is when we synthesize an operator for which we expect a small output circuit. The results are given in Fig. 4. As the ISD method produces the best results for this size of problem we only plot the results for this method. We\nalso plot the line y = x that shows how far we still are from the optimal solution. Again we outperform the best algorithm in the literature even for small input circuits with more than 50% of savings when the input circuit is of size 100-300 gates, with a maximum saving of 60% for approximately 200 gates."
        },
        {
            "heading": "3.3 Discussion",
            "text": "We now propose some open questions on how to improve the syndrome decoding based algorithm."
        },
        {
            "heading": "3.3.1 Theoretical complexity",
            "text": "Due to the use of heuristics, we cannot give a tight worst-case complexity of our algorithm in terms of circuit size. Notably, we cannot prove that the circuits will be of size O (n2/ log2(n)) in the worst case, as done in [26]. We can only guarantee a complexity in O (n2). It would be interesting as a future work to use cryptographic tools in order to prove theoretical guarantees, even in more restricted cases (for instance if we can solve the syndrome decoding problem exactly)."
        },
        {
            "heading": "3.3.2 A global solver",
            "text": "In the benchmarks presented in Section 3.2, we highlighted a strong correlation between the quality of a solver for the syndrome decoding problem and the quality of\nthe overall CNOT synthesis algorithm. Although such correlation seems quite intuitive, it does not answer the following question: does solving optimally the syndrome decoding problem provide an optimal synthesis algorithm for a triangular operator?\nThere may be cases where making some parities appearing on instances of the syndrome decoding problem will help solve following syndrome decoding problems. If this happens to be true, designing a global version of the syndrome decoding problem would be a challenging task. Notably the computational complexity of such algorithm could be intractable. Can we design a cheap solver for the syndrome decoding algorithm taking into account other future syndrome decoding problems?"
        },
        {
            "heading": "3.3.3 An alternative formulation of the syndrome decoding problem",
            "text": "Navigating through the space of solutions with a generator matrix We propose an alternative formulation of the syndrome decoding problem using the properties of linear codes. A linear code C of length n and rank k , noted [n, k ], is a k -dimensional subspace of the vector space Fn2 . A linear code is characterized notably by two matrices: the generator matrix G \u2208 Fn\u00d7k2 and the parity-check matrix H \u2208 F(n\u2212k )\u00d7n2 . Any codeword y in C is generated by G , i.e., y = Gx for some x , and for any codeword y we also have Hy = 0. When sending a codeword y through a noisy channel, we may recover an altered word z such that z = y + e does not belong to C anymore. e is the error done during the transmission and the decoding process consists in finding which y in C is the closest to z . Equivalently we want to find e with the minimum Hamming weight. Applying H to the received word and we have Hz = H (y + e) = Hy + He = He . Se ing s = Hz and we recover the syndrome decoding problem. In this case any solution of the syndrome decoding problem can be wri en{x0 + Gx | x \u2208 Fk2} where x0 is any solution of the problem. In some sense, the generator matrix G is related to the pseudo-inverse of H . There is a simple way to compute G from H and\nvice-versa: if G = (IkP ) for some matrix P then H = (In\u2212k PT ) and reciprocally. In our case, the length of our linear code is given by the number of parities avail-\nable m and the dimension of our code is k = m \u2212 n (n is the number of qubits on which the parities are encoded). One can easily check that H \u2208 Fm\u2212k\u00d7k2 . Given that we always have H = (Im\u2212k P) (the canonical vectors are always in H as the first parities available) then the computation of G is straightforward. Contrary to our initial formulation of the syndrome decoding problem, we can navigate directly in the space of the solutions of the problem in order to find the best one. Can we exploit this to have a more e icient algorithm for the syndrome decoding problem?\nA graph-oriented formulation In this paper, we give a complementary graphoriented approach to this problem. This will provide new understandings on the\nstructure of the problem and notably this will show a new way to compute the generator matrix G . When we scan a circuit to compute the available parities, each new parity is obtained because we apply a CNOT gate that sums two rows of the current linear reversible operator. So, when we gather the di erent parities in a matrix H , if the gathering is done chronologically, each new column of H can be wri en as a sum of two previous columns of H . We can create a parity graph where each node is a parity and, given three nodes v1, v2, v3 corresponding to three parities p1, p2, p3, we add two edges v1 \u2192 v3, v2 \u2192 v3 if p3 = p1\u2295p2. We call such a three-node structure a triangle. Each node except the first n ones have two in-edges. There is no restriction on the number of out-edges though. A solution to the syndrome decoding problem is given by a subset of nodes: those nodes are considered active. Given such a subset, we can switch to another solution by considering one triangle and taking the complementary in terms of active/inactive nodes. We can then navigate in the space of all the solutions in search of the one with the fewest active nodes: this is the optimal solution of the syndrome decoding problem. One can show that we can always reach the optimal solution with only the elementary operation \"take the complementary of a triangle\" because whatever the set of active nodes, we can always reach a canonical form where the active nodes are solely among the first n nodes. This problem gives a new formulation of the G matrix where each column has only three nonzero elements. The problem is illustrated on 4 qubits in Fig 5. Does there exist an e icient way to solve it?"
        },
        {
            "heading": "4 Extension to an arbitrary connectivity",
            "text": "In this section we extend the algorithm to the case where the connectivity is not complete. First we present how to adapt our algorithm based on the syndrome decoding for the synthesis of triangular operators, then we extend our method to the synthesis of any general operator."
        },
        {
            "heading": "4.1 Synthesis of a triangular operator",
            "text": "LetG be a qubit connectivity graph and L the lower triangular operator to synthesize. We require an ordering on the nodes of G such that the subgraphs containing only the first k nodes, for k = 1..n, are connected. As we need to synthesize both L and U we need in fact this property to be true for an ordering of the qubits and the reverse ordering. An Hamiltonian path in G is enough to have this property so for simplicity we assume that the ordering follows an Hamiltonian path in G .\nEven though the native CNOTs in the hardware are CNOTs between neighbor qubits in the connectivity graph, it is possible to perform an arbitrary CNOT gate but this requires more local CNOT gates. Given a target qubit qt and a qubit control qc and assuming we have a path (qc , q1, ..., qk , qt ) in the graph connecting the two nodes (such path always exists with the assumption we made above), it is possible to perform the CNOT qc \u2192 qt with max(1, 4k ) CNOTs. An example for 4 qubits (with k = 2) is given in Fig 6.\nHence, it is still possible to perform the synthesis parity by parity but we have to\nbe more careful in the se ing and in the solution of the syndrome decoding problem. Not all parities have the same cost, depending on the qubit holding the parity and its position on the hardware.\nTherefore we have to solve a weighted version of the syndrome decoding problem. Namely once we have a set of parities in a matrix H and a cost vector c \u2208 \u2115m , we look for the solution of the optimization problem\nminimize x\u2208Fm2 cT \u22c5 x\nsuch that Hx = sT . (3) Problem 3 can be recasted again as an integer linear programming problem: we only have to change the value of c. We also propose a greedy heuristic for solving quickly and approximately the problem: we define the \u201cbasis cost\u201d of implementing s as the sum of the costs of each canonical vector whose component in s is nonzero. Let bc(s) be this cost. Our greedy approach consists in finding among the parities of H the parity v (column i of H ) that minimizes the cost\nc[i ] + bc(s \u2295 v ). This approach gives a good trade-o between zeroing the most costly components of s and applying parities at a very high cost. Again we can repeat the algorithm with random changes of basis to find a be er solution. Especially we focused on computing bases for which the canonical vectors have the lowest possible costs. In other words, the new canonical vectors correspond to parities hold by qubits that are as close as possible to the target qubit.\nNonetheless, compared to the all-to-all case, solving the weighted syndrome decoding problem is not the only computational core for controlling both the quality of the solution and the computational time. Another key task lies in the enumeration of the available parities. As we will see, it is possible to generate more parities for one syndrome decoding problem instance and this increases the chances to get a low-cost solution."
        },
        {
            "heading": "4.1.1 Listing the parities available.",
            "text": "Until now we set the weighted syndrome decoding instances by computing the parities appearing during the synthesis and by using the template in Fig. 6 to estimate their costs. This is in fact ine icient because it ignores some specificities of the problem:\n\u2022 It is possible to add multiple parities in one shot using the template in Fig. 6.\n\u2022 There is not necessarily one unique path in G between the control qubit and the target qubit.\nMore precisely, the template shown in Fig. 6 is the best to our knowledge, in terms of size, to apply solely the parity on qubit qc to qubit qt . However it is possible to apply any parity\nqt \u2190 qt \u2295 qc \u2295ki=1 \u03b1iqi (4)\nwith \u03b1i \u2208 {0, 1} using less CNOTs than required for applying only qc . In fact the less costly linear combination of parities is the complete combination qc \u2295 q1 \u2295 ... \u2295 qk , for which 2k + 1 CNOTs are enough. Removing any parity from this combination requires 2 additional CNOTs per parity except for the qubit qk that needs only one extra CNOT. An explanatory template on 6 qubits (k = 4) is given on Fig. 7. For any parity at a distance k of the target qubit, there is at most 2k\u22121 di erent linear combinations possible and just as many new parities to consider. Moreover the path between the control qubit and the target qubit ma ers as a di erent path will result in di erent linear combinations of parities. A slight modification of the A* algorithm is enough to compute all the shortest paths between two nodes in a graph.\nEven for a small number of qubits the number of parities becomes quickly intractable. The number of linear combinations along a path increases exponentially with the length of the path as the number of paths for most of the architectures \u2014 a grid for instance. In practice we control the total number of parities by favoring paths over the choices in the linear combinations. This option is empirically justified but a more detailed analysis could be made. For one path we only consider the less costly linear combination, i.e., the one that adds all the parities on the way. On the other hand if possible we go through all the shortest paths between one control qubit and one target qubit."
        },
        {
            "heading": "4.1.2 A faster heuristic",
            "text": "For large problem sizes, even with the minimum number of paths and linear combinations possible, the number of parities can quickly become intractable because one CNOT gate can still introduce up to n \u2212 2 new parities. Indeed, in a straight line for instance given by a path q1 \u2192 q2 \u2192 \u2026 \u2192 qn , modifying qubit qn\u22121 will add all possible linear combinations \u2211n\u22121i=k qi for k = 1...n \u2212 1. This potential factor of n quickly becomes a non negligible computational overhead.\nWe propose therefore a simpler but faster heuristic. Instead of considering linear combinations, we simply consider the parities carried by each qubit as in a complete\nconnectivity case. The trick is to order the parities in terms of distance to the target qubit (depending on which qubit they appear) and find a change of basis such that each canonical vector is at the minimum possible distance from the target qubit. In other words, the parities at distance 1 from the target qubit form a subspace of Fn2 of rank k1 for some k1 and we can choose k1 such parities as new canonical vectors (as long as they are linearly independent). Then the parities at distance 2 will add k2 new dimensions, adding k2 new canonical vectors, etc., until the whole space Fn2 is spanned with a new basis. Now, in this new basis, we consider the components I \u2282 [[1, n]] of the parity to synthesize that correspond to the farthest canonical vectors. By construction, such components can only be zeroed by adding parities from those farthest qubits. Furthermore, adding supplementary parities on the way will not modify the values the components I . So our proposal therefore is to remove such distant components by greedily choosing the parities that minimize at each iteration the number of nonzero components from I . To add such parity we choose to use the template in Fig. 7 with the full linear combination as we said that the intermediate parities do not modify the components I . Once the components I are zeroed, we repeat the process with the new farthest components until the whole parity is synthesized."
        },
        {
            "heading": "4.2 Synthesis of a general operator",
            "text": "The extension of the synthesis from triangular to general operator is not as straightforward as in the all-to-all connectivity case. We cannot simply write A = PLU and concatenate the circuits synthesizing L and U and ultimately permuting the qubits. If we want to use this algorithm as a sub-task of a global circuit optimizer for NISQ architectures we cannot a ord to swap the qubits because it could break the optimizations done in the rest of the circuit.\nTo avoid the permutation of the qubits we have to transform the matrix A by applying a pre-circuit C such that CA = LU . Then the concatenation of C\u22121 and the circuits synthesizing L and U gives a valid implementation of A."
        },
        {
            "heading": "4.2.1 Computation of C",
            "text": "If A is invertible, which is always the case, then it admits an LU factorization if and only if all its leading principal minors are nonzero. We propose an algorithm for computing C exploiting this property while trying to optimize the final size of C . We successively transform A such that every submatrix A[1:i, 1:i] is invertible. By construction when trying to make A[1 \u2236 k , 1 \u2236 k ] invertible for some k we have A[1:k \u2212 1, 1:k \u2212 1] invertible. If A[1 \u2236 k , 1 \u2236 k ] is invertible then we do nothing, otherwise we look in the parities A[k + 1:n,1:k ] those who, added to A[k , 1 \u2236 k ], make A[1 \u2236 k , 1 \u2236 k ] invertible. By assumption A is invertible so there is at least one such row that verifies this property. Then among the valid parities we choose the closest one to qubit k in G . We can add all the parities along the path because by assumption they belong to the span of the first k \u2212 1 rows of A[1 \u2236 k , 1 \u2236 k ] so it has no e ect on the rank of A[1 \u2236 k , 1 \u2236 k ]."
        },
        {
            "heading": "4.2.2 Choice of the qubit ordering",
            "text": "We can further optimize our algorithm by changing the qubits ordering. The algorithm we have presented for synthesizing a triangular operator is still valid up to row and column permutations. Thus, given a permutation P of the qubits, one can synthesize P\u22121LP by applying our algorithm with the order given by P . Then, instead of computing a circuit C such that CA = LU we search for a circuit C satisfying P\u22121CAP = LU and\nCA = PLP\u22121PUP\u22121 = L\u2032U \u2032 where L\u2032 and U \u2032 can be synthesized using our algorithm. Searching for such C can be done using our algorithm on A[P , P ] (in Matlab notation, i.e., the reordering of A along the vector P ).\nThis means that we can choose P such that the synthesis of L and U will yield shorter circuits. Empirically we noticed that when synthesizing the k th parity of L it is preferable to have access to the parities appearing on qubits k \u2212 1, k \u2212 2, etc., in priority for two reasons: first because they can modify more bits on the k th parity and secondly because it is likely that there will be much more parities available, increasing the chance to have an inexpensive solution to the weighted syndrome decoding problem. Intuitively we want the ordering of the qubits to follow at least an Hamiltonian path in G = (V , E ) which would match the previous restriction on the ordering we formulated at the beginning of the section.\nIn practice, we found empirically orderings that provide good results and in fact they are simple. For instance, for any architecture similar to a grid, the ordering we chose is the one starting from the top le qubit, follows the first line, then continues below on the second line, etc., giving a \"snake\" structure to the ordering in the architecture. See Fig. 1 for an illustration on several architectures.\nFinding a good qubit ordering does not have, strictly speaking, to be automatized because it can be done once for all for a given architecture. Still, it would be interesting to be able to find good algorithms for this problem, especially in order to deal with more complex architectures than the ones we consider in this paper. Some proposals and discussions are given in Section 4.4 to tackle this problem."
        },
        {
            "heading": "4.2.3 Exploit the symmetries of the architectures",
            "text": "Lastly, it is possible to use the symmetries of the architectures to obtain other qubit orderings that are as valid as the one initially chosen. For instance, for a grid, starting from one of the three other corners or following a path along the columns instead of the rows gives other qubit orderings that can potentially lead to a be er result. We cannot know in advance which ordering will be the most suited for one particular operator, so the only solution is to try all possibilities (8 in the case of a grid) and keep the best result."
        },
        {
            "heading": "4.3 Benchmarks",
            "text": ""
        },
        {
            "heading": "4.3.1 Fine tuning of the method",
            "text": "By the heuristic nature of our algorithm, its global performance depends on several parameters:\n\u2022 the maximum number of shortest paths between two qubits, noted SPmax,\n\u2022 the number of linear combination we authorize along a certain path, noted LCmax,\n\u2022 the method to solve the syndrome decoding problem,\n\u2022 the number of times we solve the syndrome decoding problem (with random changes of basis), noted Nitersyndrome,\n\u2022 the number of times we repeat the synthesis of one triangular operator, noted Niter,\n\u2022 the ordering of the qubits.\nThe role of each individual parameter is di icult to quantify, but it is even harder to quantify the interdependent roles of all parameters together. A er some numerical experiments, we have observed that the results have a high variance. Notably, the correlation between the quality of the solutions of the syndrome decoding problem and the quality of the overall synthesis algorithm is much weaker than in the all-to-all connectivity case.\nIt seems that the mean performance of the algorithm cannot be changed significantly but we can act on the value of the variance of the results: the larger the variance is, the more likely we will find very good solutions (among very bad ones). To increase the variance, we insist on the parameters that add randomness in the results.\nOverall, here are our current conclusions on the role of each parameter:\nNiter This parameter is probably the most important one. Being able to repeat the experiment a large number of times increases our chance to find a good solution.\nNitersyndrome This parameter improves the solution of the syndrome decoding problem, but has li le e ect on the global quality of the results. In fact, it tends to deteriorate the global performance of our algorithm. This might be due to the fact that there is less variance in the results, thus less chances to reach a good solution.\nThe method to solve the syndrome decoding Using an integer programming solver o ers almost no variance in the results. We noticed that the best results were obtained with the greedy heuristic. When the size of the problem is large, it is preferable to use the faster heuristic notably due to the computational time.\nLCmax This parameter has li le e ect on the results. Furthermore the computational complexity becomes quickly intractable if we increase LCmax: allowing to remove one parity from the linear combination given by Eq. (4) adds (k1) = k parities where k is the length of the path considered, removing two parities adds (k2) \u2248 k 2/2 parities, etc. Moreover, those additional parities have a larger cost and are unlikely to be used in the solution of the syndrome decoding problem.\nSPmax This parameter has a significant e ect on the results. Contrary to the parameter LCmax, this one has to be maximized if possible. It introduces more parities (but this time with identical cost), hence more variance in the results.\nThe ordering of the qubits The ordering has a critical impact on the performance of the algorithm. With some experiments we come up with some good orderings that give in average the best results. Exploiting the symmetries also has a non negligible impact as we increase the chances to find be er solutions. More details will be given in the discussions in Section 4.4."
        },
        {
            "heading": "4.3.2 Numerical results",
            "text": "We compare our method against the best algorithm in the literature [19] whose source code is available on the PyZX Github repository [20]. For each architecture considered in their implementation we generate a set of 50 random operators and perform the synthesis using the Steiner trees. Their algorithm provides an optimization using genetic algorithms but this implements the circuit up to a permutation of the qubits. We first consider exact synthesis so in this case we considered their algorithm without this extra optimization. Then we propose the same experiment but where the synthesis can be done up to a final permutation of the qubits, in this case we added the genetic algorithm optimization in the Steiner tree based algorithm.\nOur own algorithm is implemented in Julia. We set a time limit of 10 minutes for the synthesis of an operator. The values of the parameters for each architecture are summarized in Table 1. Overall, we solely focused on the number of iterations, the number of paths and the method to solve the syndrome decoding problem. When the number of qubits is small enough (n \u2264 25) we can maximize the number of paths considered and the number of iterations with the standard greedy method. For large problem sizes (n \u2265 64), only the fast heuristic can provide results in a reasonable amount of time and we modulate the computational time and the performance of our algorithm with the number of iterations. For intermediate sizes, namely n = 36 and n = 49, the heuristic nature of our algorithm makes it nontrivial to determine the best set of parameters. From our observations, we noticed it was best to have a large number of iterations for the square architectures. However, when adding diagonal connections between the qubits, having a good tradeo between the number of paths and the number of iterations gives be er results.\nExact synthesis Results for exact synthesis are summarized in Table 2. Columns 3 and 4 give the average size of the generated circuits for the method using Steiner\ntrees in [19] and our algorithm based on syndrome decoding. The next columns detail the savings: the mean saving, the minimum saving (negative saving means that our algorithm performs worse), the maximum saving and the proportion of operators for which our circuit is actually shorter than the one provided by the state-of-theart method. The last two columns give the average time required to perform the synthesis of one operator (all iterations included for our algorithm).\nWe can expect our algorithm to behave be er if there are more connections between the qubits. When the connectivity is as limited as possible, for instance with an LNN architecture, our algorithm does not outperform the algorithm based on Steiner trees. The average performance of both algorithms are almost identical and the natural distribution of the relative savings are probably only due to the variance of the results. Similarly, as the Rige i architecture is close to a straight line, we outperform the state of the art but with less savings compared to the other architectures: 10% of savings in average but with almost no savings in some cases.\nFor the remaining architectures the results are more promising. In the case of the 9-qubit square there is a lot of variance in the results: depending on the operator we can have a gain of almost 40% or almost no savings at all. Overall we still manage to produce a shorter circuit for every circuit with an average gain of 23%.\nFor larger architectures, we outperform the state-of-the-art algorithm consistently with at least 17% of savings. First, the more connected the architecture, the be er the results. This is particularly visible if we add diagonal connections in the square architectures: both algorithms provide much shorter circuits but we manage to take more advantage of it, improving our relative savings. We also have almost 30% savings in average on the IBM-Tokyo chip.\nDespite the use of the simpler heuristic for large problems, we still manage to get at least 17% of savings in average. This is less than for smaller architectures for which more optimal techniques are used but the method is more scalable.\nSynthesis up to a permutation In [19] the authors propose to synthesize the operator up to row and column permutations in order to reduce the total number of CNOT gates and they use a genetic algorithm to find suitable permutations. Although allowing to permute the rows and the columns of the operator indeed helps finding shorter circuits, it cannot be used directly in a peep-hole optimization process for global circuit optimization.\nStill, there are cases where permuting the rows and columns may be allowed. Given an operator A, permuting the columns of A is equivalent to applying a permutation matrix on the right: this corresponds in a change of the initial layout of the logical qubits in the hardware. Modifying the layout of the logical qubits can only be done once, at the beginning of the optimization. Hence such operation is not available for all linear reversible operators in the global circuit. Permuting the rows of A is equivalent to applying a permutation matrix on its le , this means that the qubits carry the good parities but are permuted in the hardware. In a context where we want to optimize on the fly a bigger quantum circuit, this will have an impact on the remaining circuit to optimize but the optimization process continues. Therefore permuting the rows might be preferable over permuting the columns as it can be integrated more easily in a global optimization algorithm.\nWe recall that our syndrome decoding based algorithm for restricted connectivities consists in two parts:\n\u2022 First, we compute a short circuit C such that CA = LU , \u2022 Then we synthesize L,U and overall A = C\u22121LU .\nWe know that we can always write A = PLU for some permutation matrix P , so if we authorize ourselves to permute the rows of A we can simply replace C by a permutation matrix and save the cost of implementing C .\nWe repeated the experiments done for an exact synthesis but with the rows permutations. The results are in Table 3. We kept the default values of the parameters of the genetic algorithms given in PyZX. We only modified the number of iterations to ensure that the computational times of our method and the Steiner tree based method are approximately the same. Some finer tuning on the genetic algorithm parameters can be done and this is a limitation in our comparison. Note also that their code optimizes both rows and columns permutations, while our algorithm only allows rows permutations. This is another limitation for a fair comparison.\nOverall, we still manage to outperform [19] for all but three architectures but the savings are not as good as in the exact synthesis case. Notably for sparse or small architectures our results are worse (for the 19 qubits line) or equal (for Rige i\u2019s chip or the 9 qubits square). For the other architectures, the savings range from 12% for the IBM QX5 chip to 20% for the most connected architectures. Note that the gap we noticed between the small and large architectures, which was due to the change of the heuristic for solving the syndrome decoding problem, disappear in this new experiment. We think this is due to the quality of the genetic algorithm that struggles optimizing e iciently the rows and columns permutation when the problem size is too large. This compensates the loss of quality of our own heuristic.\nExperiments with more connected architectures The architectures currently available have a sparse qubit connectivity, making it costly to implement CNOT circuits. Even though full qubit connectivity will not be technologically feasible anytime soon, we can hope that more connected architectures will be designed in a near future. Will this favor our algorithm? In our experiments, we highlighted the fact that the more connected the architecture, the be er the savings over the Steiner tree based algorithm. Besides, the closer we get to a full connectivity the more our algorithm will perform similarly to our original syndrome decoding based algorithm while the Steiner tree based will eventually consist in a standard Gaussian elimination. This is another argument in favor of an increasing outperformance of our algorithm over the state of the art. We propose two experiments to confirm or infirm this behavior.\nThe first experiment is based on an experimental model that can already be found in certain technologies such as some using Rydberg atoms [15]. Starting from a grid layout, we consider that each qubit has a fixed interaction radius and can interact with the qubits within its reach. Here the radius is computed with the L2 norm in a plane. So for instance se ing the radius to 1 and we have the standard square architecture. Se ing the radius to \u221a 2 and we recover the square architecture with diagonal connections. For a 25-qubit square, one can show that the number of interactions increases for the following values of the radius:\n1,\u221a2, 2,\u221a5,\u221a8, 3,\u221a10,\u221a13, 4,\u221a17,\u221a18,\u221a20, 5,\u221a32 and for each of these radius we computed the average CNOT count of both our algorithm and the state of the art with a sample of 50 operators. The results are given in Fig. 8. The behavior of the two algorithms is similar: the CNOT count decreases exponentially with the interaction radius. This is very promising because it shows that only a small improvement in the interaction radius can lead to consequent savings in the cost of CNOT circuits. Interestingly, the di erence between the CNOT counts of the two algorithms is approximately constant. Necessarily this results in an increasing relative gain of our algorithm against the state-of-the-art method.\nThe second experiment aims at showing that the advantage of our algorithm in more connected architectures is still true even in an unstructured architecture. Starting from a 20-qubit LNN architecture, we iteratively randomly add 15 edges to our architecture for a total of 12 increasingly connected architectures. For each new architecture we synthesize 50 random operators and we store the average CNOT count. The results are given in Fig 9. We get similar results to the experiment with the grid and the increasing interaction radius."
        },
        {
            "heading": "4.4 Discussion",
            "text": "We now discuss a crucial problem for the performance of the syndrome decoding algorithm: the choice of the qubit ordering. Indeed, we noticed empirically that a good qubit ordering has a non negligible impact on the overall performance of our algorithm. Unfortunately, the role of the qubit ordering is quite opaque. We propose intuitive mathematical formulations of the best qubit ordering and we discuss their validity.\nWe define an ordering as a map \u03c0 \u2236 V \u2192 [[1, n]], and the distance between two qubits u, v \u2208 V in the ordering is given by | \u03c0(u) \u2212 \u03c0(v )|. We also note d(u, v ) the distance between qubits u and v in the hardware.\nIn Section 4.2.2, we defined a good qubit ordering as an order of the qubits such that each qubit is the closest possible in the hardware to its predecessors and successors in the ordering. In other words, we want at least two successive qubits in the ordering to be neighbors in the hardware. bits at distance 2 in the ordering should also be at distance 2 or less in the hardware, etc. A Hamiltonian path in the hardware guarantees that d(u, v ) \u2264 \u2016\u03c0(u) \u2212 \u03c0(v )\u2016, but among the Hamiltonian paths in the hardware, some may be be er than others. The minLA problem In [6], we initially formulated the best ordering \u03c0 \u2236 V \u2192[[1, n]] as a solution of the Minimum Linear Arrangement problem minimize\n\u03c0 \u2211(u,v )\u2208E wuv |\u03c0(u) \u2212 \u03c0(v )| (5)\nwherewuv is the weight of the edge connecting u and v in the graph. The idea is that we want to give priority to neighbors in the hardware: the nodes must be as close as possible in the hardware if their \u201cnumbers\u201d are also close. A way to do so is to solve the MinLA problem, not in the hardware graph, but in the complete graph with suitable weights. Namely wi j must be large when i , j are neighbors in the hardware andwi j must be smaller if i , j are at distance 2, and even smaller for larger distances. The MinLA problem has already been used for qubit routing [27] and the problem is in general NP-Hard [11]. One way to solve exactly this problem is to encode it in an integer linear program. For conciseness, we will not detail the formulation of the problem but is uses standard encoding techniques. The important result is that, with a time limit given to our integer programming solver, we are able to find solutions quickly, although those solutions are not proven to be optimal.\nUnfortunately, the solutions are in fact quite deceiving, some of them not even being a Hamiltonian path of the hardware graph. We believe this is not because the solver did not find optimal solutions, but rather because the minLA problem is in fact not the most suitable problem to encode what we want. The reason is the following: the minimum value of |\u03c0(u)\u2212\u03c0(v )| = 1 can only be assigned to n\u22121 possible combinations of the (u, v )s. On the other hand, large weights are given to every pair of neighbor nodes in the hardware and there are as many as there are edges in the hardware graph. Even for sparse architectures, like a square, there are approximately four times more maximum weightswi j than minimum possible assignments of |\u03c0(u)\u2212 \u03c0(v )|. In other words, most of the cost to minimize will consist in the portion\ncste \u00d7 \u2211 u,v neighbors |\u03c0(u) \u2212 \u03c0(v )|. (6)\nGiven that we sum over a large number of terms, some of the |\u03c0(u) \u2212 \u03c0(v )| will not be equal to 1 but rather 2, 3 or even more. In these circumstances, as long as the total sum is minimized, the optimizer will not necessarily find a solution where|\u03c0(u) \u2212 \u03c0(v )| = 1 will necessarily correspond to neighbor qubits (u, v ). We illustrate\nthis with the example given in Table 4. We give two di erent orderings for which Eq. (6) is minimized but one of them do not correspond to a Hamiltonian path. The two orderings have the same cost for any weights (wuv )uv and one can show that this cost is minimal for interesting values of the weights (for instance wuv = 1/(10)d(u,v )). Anonlinear integer programming formulation. As a response to the limitation of the minLA problem, we propose a new way to encode our problem of qubit ordering by changing the cost function. To emphasize the role of the small |\u03c0(i ) \u2212 \u03c0(j )| terms, we have to use a nonlinear function. We set wuv = d(u, v ) and the function to minimize is now\u2211\nuv\nwuv e \u2212|\u03c0(u)\u2212\u03c0(v )|. (7)\nWith this cost function, we can see that the terms that have to be minimized in priority are the cases where |\u03c0(u) \u2212\u03c0(v )| = 1 so they will have to correspond to qubits(u, v ) that correspond to small wuv . In other words, for neighbor qubits (u, v ) we will have |\u03c0(u) \u2212 \u03c0(v )| = 1. And the logic is the same for qubits that are at distant 2, 3, etc.\nThis problem can be similarly cast into a nonlinear integer programming. Unfortunately, the solvers at our disposal struggle finding solutions. We have to use an heuristic. We propose this very simple algorithm: we choose a starting permutation and at each iteration we switch two elements of our permutation. We choose one pair among the ones that minimize the cost function and we stop when a local minimum is found. Finally we repeat this process with di erent random starting permutations. An example of ordering obtained with this method for the 16-qubit square is given in Fig. 10. In theory such ordering seems be er suited than the standard \"snake\" we used in our benchmarks. If we check, the cost function is indeed smaller for this ordering than for the snake. It makes sense because of the winding pa ern of the ordering. However, in practice, this does not improve our results (165 CNOT in average against 155). We do not really have an explanation, it is probably due to the fact that our initial definition of a good ordering, i.e., an order of the qubits such that each qubit is the closest possible in the hardware to its predecessors and successors in the ordering, does not catch the whole complexity of the problem. Does there exist a be er formulation of the perfect qubit ordering?"
        },
        {
            "heading": "5 Conclusion",
            "text": "We have presented a new framework for the synthesis of linear reversible circuits. We exploit the specific structure of triangular operators to transform the synthesis into a series of syndrome decoding problems, which are well-known problems in cryptography. Using an LU decomposition we can synthesize any quantum operator in the case of an all-to-all connectivity. Benchmarks show that we outperform the state-of-the-art algorithm for intermediate sized problems (n < 400). Our heuristics\nfor solving the syndrome decoding problem are e icient but could still be improved, both in circuit size and computational time. For instance, some quantum algorithms have been proposed for solving the syndrome decoding problem via the Information Set Decoding algorithm [5, 17, 18], which gives the possibility of designing a hybrid quantum/classical compiler for this particular synthesis problem.\nThen we have highlighted the robustness of our framework by extending it to an arbitrary connectivity graph having a Hamiltonian path. With a suitable preprocessing of the matrix we transform the problem into a series of weighted syndrome decoding problems. Except for the LNN architecture whose connectivity is too sparse, we consistently outperform existing algorithms. As a future work, we can study how to extend our method to the case where the connectivity graph does not have a Hamiltonian path, similarly to [19]. For the moment we only have studied the behavior of our algorithm on random CNOT circuits, but large-scale CNOT circuits are not common. It would be interesting to extend our framework in order to deal with quantum circuits implementing real algorithms, e.g., quantum chemistry-based circuits or arithmetic functions. Besides, with specific quantum circuits instead of random CNOT circuits, the outperformance of direct synthesis methods is not guaranteed and a more in-depth study of various methods (direct synthesis, SWAP insertions, etc.) will be necessary."
        },
        {
            "heading": "Acknowledgment",
            "text": "This work was supported in part by the French National Research Agency (ANR) under the research project So QPRO ANR-17-CE25-0009-02, and by the DGE of the French Ministry of Industry under the research project PIA-GDN/ antEx P163746484124. We thank Bertrand Marchand for comments on the manuscript."
        }
    ],
    "title": "Decoding techniques applied to the compilation of CNOT circuits for NISQ architectures\u2217",
    "year": 2022
}