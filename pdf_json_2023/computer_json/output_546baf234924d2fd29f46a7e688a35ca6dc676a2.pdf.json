{
    "abstractText": "In these pedagogic notes I review the statistical mechanics approach to neural networks, focusing on the paradigmatic example of the perceptron architecture with binary an continuous weights, in the classification setting. I will review the Gardner\u2019s approach based on replica method and the derivation of the SAT/UNSAT transition in the storage setting. Then, I discuss some recent works that unveiled how the zero training error configurations are geometrically arranged, and how this arrangement changes as the size of the training set increases. I also illustrate how different regions of solution space can be explored analytically and how the landscape in the vicinity of a solution can be characterized. I give evidence how, in binary weight models, algorithmic hardness is a consequence of the disappearance of a clustered region of solutions that extends to very large distances. Finally, I demonstrate how the study of linear mode connectivity between solutions can give insights into the average shape of the solution manifold.",
    "authors": [
        {
            "affiliations": [],
            "name": "Enrico M. Malatesta"
        }
    ],
    "id": "SP:6cf6ba29e69398399984ceebe65ab95974c60b5a",
    "references": [
        {
            "authors": [
                "E. Gardner",
                "B. Derrida"
            ],
            "title": "Three unfinished works on the optimal storage capacity of networks",
            "venue": "Journal of Physics A: Mathematical and General 22(12), 1983 ",
            "year": 1989
        },
        {
            "authors": [
                "G. Gy\u00f6rgyi"
            ],
            "title": "First-order transition to perfect generalization in a neural network with binary synapses",
            "venue": "Phys. Rev. A 41, 7097 ",
            "year": 1990
        },
        {
            "authors": [
                "E. Gardner"
            ],
            "title": "Maximum storage capacity in neural networks",
            "venue": "Europhysics Letters 4(4), 481 ",
            "year": 1987
        },
        {
            "authors": [
                "E. Gardner"
            ],
            "title": "The space of interactions in neural network models",
            "venue": "Journal of Physics A: Mathematical and General 21(1), 257 ",
            "year": 1988
        },
        {
            "authors": [
                "W. Krauth",
                "M. M\u00e9zard"
            ],
            "title": "Storage capacity of memory networks with binary couplings",
            "venue": "Journal de Physique 50(20), 3057 ",
            "year": 1989
        },
        {
            "authors": [
                "S. Franz",
                "G. Parisi",
                "M. Sevelev",
                "P. Urbani",
                "F. Zamponi"
            ],
            "title": "Universality of the SAT-UNSAT (jamming) threshold in non-convex continuous constraint satisfaction problems",
            "venue": "SciPost Phys. 2, 019 ",
            "year": 2017
        },
        {
            "authors": [
                "M. M\u00e9zard",
                "G. Parisi",
                "M. Virasoro"
            ],
            "title": "Spin glass theory and beyond: An Introduction to the Replica Method and Its Applications",
            "venue": "vol. 9, World Scientific Publishing Company, doi:10.1142/0271 ",
            "year": 1987
        },
        {
            "authors": [
                "D. Sherrington",
                "S. Kirkpatrick"
            ],
            "title": "Solvable model of a spin-glass",
            "venue": "Phys. Rev. Lett. 35(26), 1792 ",
            "year": 1975
        },
        {
            "authors": [
                "E. Gardner",
                "B. Derrida"
            ],
            "title": "Optimal storage properties of neural network models",
            "venue": "Journal of Physics A: Mathematical and General 21(1), 271 ",
            "year": 1988
        },
        {
            "authors": [
                "C. Baldassi",
                "C. Lauditi",
                "E.M. Malatesta",
                "G. Perugini",
                "R. Zecchina"
            ],
            "title": "Unveiling the structure of wide flat minima in neural networks",
            "venue": "Physical Review Letters 127(27), 278301 ",
            "year": 2021
        },
        {
            "authors": [
                "G. Parisi"
            ],
            "title": "A sequence of approximated solutions to the s-k model for spin glasses",
            "venue": "Journal of Physics A: Mathematical and General 13(4), L115 ",
            "year": 1980
        },
        {
            "authors": [
                "J. Ding",
                "N. Sun"
            ],
            "title": "Capacity lower bound for the ising perceptron",
            "venue": "Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing, STOC 2019, p. 816\u2013827. Association for Computing Machinery, New York, NY, USA, ISBN 9781450367059, doi:10.1145/3313276.3316383 ",
            "year": 2019
        },
        {
            "authors": [
                "C. Baldassi",
                "E.M. Malatesta",
                "G. Perugini",
                "R. Zecchina"
            ],
            "title": "Typical and atypical solutions in nonconvex neural networks with discrete and continuous weights",
            "venue": "Phys. Rev. E 108, 024310 ",
            "year": 2023
        },
        {
            "authors": [
                "T.M. Cover"
            ],
            "title": "Geometrical and statistical properties of systems of linear inequalities with applications in pattern recognition",
            "venue": "IEEE Transactions on Electronic Computers EC-14(3), 326 ",
            "year": 1965
        },
        {
            "authors": [
                "A. Montanari",
                "Y. Zhong",
                "K. Zhou"
            ],
            "title": "Tractability from overparametrization: The example of the negative perceptron",
            "venue": "arXiv preprint arXiv:2110.15824 ",
            "year": 2021
        },
        {
            "authors": [
                "L. Sagun",
                "L. Bottou",
                "Y. LeCun"
            ],
            "title": "Eigenvalues of the hessian in deep learning: Singularity and beyond",
            "venue": "arXiv preprint arXiv:1611.07476 ",
            "year": 2016
        },
        {
            "authors": [
                "L. Sagun",
                "U. Evci",
                "V.U. Guney",
                "Y. Dauphin",
                "L. Bottou"
            ],
            "title": "Empirical analysis of the hessian of over-parametrized neural networks",
            "venue": "arXiv preprint arXiv:1706.04454 ",
            "year": 2017
        },
        {
            "authors": [
                "F. Draxler",
                "K. Veschgini",
                "M. Salmhofer",
                "F. Hamprecht"
            ],
            "title": "Essentially no barriers in neural network energy landscape",
            "venue": "J. Dy and A. Krause, eds., Proceedings of the 35th International Conference on Machine Learning, vol. 80 of Proceedings of Machine Learning Research, pp. 1309\u20131318. PMLR ",
            "year": 2018
        },
        {
            "authors": [
                "R. Entezari",
                "H. Sedghi",
                "O. Saukh",
                "B. Neyshabur"
            ],
            "title": "The role of permutation invariance in linear mode connectivity of neural networks",
            "venue": "International Conference on Learning Representations ",
            "year": 2022
        },
        {
            "authors": [
                "F. Pittorino",
                "A. Ferraro",
                "G. Perugini",
                "C. Feinauer",
                "C. Baldassi",
                "R. Zecchina"
            ],
            "title": "Deep networks on toroids: Removing symmetries reveals the structure of flat regions in the landscape geometry",
            "venue": "K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu and S. Sabato, eds., Proceedings of the 39th International Conference on Machine Learning, vol. 162 of Proceedings of Machine Learning Research, pp. 17759\u201317781. PMLR ",
            "year": 2022
        },
        {
            "authors": [
                "Y. Feng",
                "Y. Tu"
            ],
            "title": "The inverse variance\u2013flatness relation in stochastic gradient descent is critical for finding flat minima",
            "venue": "Proceedings of the National Academy of Sciences 118(9) ",
            "year": 2021
        },
        {
            "authors": [
                "G. Chen",
                "C.K. Qu",
                "P. Gong"
            ],
            "title": "Anomalous diffusion dynamics of learning in deep neural networks",
            "venue": "Neural Networks 149, 18 ",
            "year": 2022
        },
        {
            "authors": [
                "D. Kunin",
                "J. Sagastuy-Brena",
                "L. Gillespie",
                "E. Margalit",
                "H. Tanaka",
                "S. Ganguli",
                "D.L. Yamins"
            ],
            "title": "Rethinking the limiting dynamics of sgd: modified loss",
            "venue": "phase space oscillations, and anomalous diffusion ",
            "year": 2021
        },
        {
            "authors": [
                "H. Li",
                "Z. Xu",
                "G. Taylor",
                "C. Studer",
                "T. Goldstein"
            ],
            "title": "Visualizing the loss landscape of neural nets",
            "venue": "S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi and R. Garnett, eds., Advances in Neural Information Processing Systems, vol. 31. Curran Associates, Inc. ",
            "year": 2018
        },
        {
            "authors": [
                "P. Foret",
                "A. Kleiner",
                "H. Mobahi",
                "B. Neyshabur"
            ],
            "title": "Sharpness-aware minimization for efficiently improving generalization",
            "venue": "International Conference on Learning Representations ",
            "year": 2021
        },
        {
            "authors": [
                "M. Baity-Jesi",
                "L. Sagun",
                "M. Geiger",
                "S. Spigler",
                "G.B. Arous",
                "C. Cammarota",
                "Y. LeCun",
                "M. Wyart",
                "G. Biroli"
            ],
            "title": "Comparing dynamics: deep neural networks versus glassy systems",
            "venue": "Journal of Statistical Mechanics: Theory and Experiment 2019(12), 124013 ",
            "year": 2019
        },
        {
            "authors": [
                "C. Baldassi",
                "A. Ingrosso",
                "C. Lucibello",
                "L. Saglietti",
                "R. Zecchina"
            ],
            "title": "Subdominant dense clusters allow for simple learning and high computational performance in neural networks with discrete synapses",
            "venue": "Phys. Rev. Lett. 115, 128101 ",
            "year": 2015
        },
        {
            "authors": [
                "S. Franz",
                "G. Parisi"
            ],
            "title": "Recipes for metastable states in spin glasses",
            "venue": "Journal de Physique I 5(11), 1401 ",
            "year": 1995
        },
        {
            "authors": [
                "H. Huang",
                "Y. Kabashima"
            ],
            "title": "Origin of the computational hardness for learning with binary synapses",
            "venue": "Phys. Rev. E 90, 052813 ",
            "year": 2014
        },
        {
            "authors": [
                "E. Abbe",
                "S. Li",
                "A. Sly"
            ],
            "title": "Proof of the contiguity conjecture and lognormal limit for the symmetric perceptron",
            "venue": "arXiv preprint arXiv:2102.13069 ",
            "year": 2021
        },
        {
            "authors": [
                "W. Perkins",
                "C. Xu"
            ],
            "title": "Frozen 1-rsb structure of the symmetric ising perceptron",
            "venue": "Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing, pp. 1579\u20131588, doi:10.1145/3406325.3451119 ",
            "year": 2021
        },
        {
            "authors": [
                "D. Gamarnik"
            ],
            "title": "The overlap gap property: A topological barrier to optimizing over random structures",
            "venue": "Proceedings of the National Academy of Sciences 118(41), e2108492118 ",
            "year": 2021
        },
        {
            "authors": [
                "A. El Alaoui",
                "A. Montanari",
                "M. Sellke"
            ],
            "title": "Optimization of mean-field spin glasses",
            "venue": "The Annals of Probability 49(6), 2922 ",
            "year": 2021
        },
        {
            "authors": [
                "A. Braunstein",
                "R. Zecchina"
            ],
            "title": "Learning by message passing in networks of discrete synapses",
            "venue": "Phys. Rev. Lett. 96, 030201 ",
            "year": 2006
        },
        {
            "authors": [
                "C. Baldassi",
                "A. Braunstein",
                "N. Brunel",
                "R. Zecchina"
            ],
            "title": "Efficient supervised learning in networks with binary synapses",
            "venue": "Proceedings of the National Academy of Sciences 104(26), 11079 ",
            "year": 2007
        },
        {
            "authors": [
                "C. Baldassi",
                "C. Lauditi",
                "E.M. Malatesta",
                "R. Pacelli",
                "G. Perugini",
                "R. Zecchina"
            ],
            "title": "Learning through atypical phase transitions in overparameterized neural networks",
            "venue": "Physical Review E 106(1), 014116 ",
            "year": 2022
        },
        {
            "authors": [
                "E. Abbe",
                "S. Li",
                "A. Sly"
            ],
            "title": "Binary perceptron: efficient algorithms can find solutions in a rare well-connected cluster",
            "venue": "arXiv preprint arXiv:2111.03084 ",
            "year": 2021
        },
        {
            "authors": [
                "L. Budzynski",
                "F. Ricci-Tersenghi",
                "G. Semerjian"
            ],
            "title": "Biased landscapes for random constraint satisfaction problems",
            "venue": "Journal of Statistical Mechanics: Theory and Experiment 2019(2), 023302 ",
            "year": 2019
        },
        {
            "authors": [
                "A.G. Cavaliere",
                "T. Lesieur",
                "F. Ricci-Tersenghi"
            ],
            "title": "Optimization of the dynamic transition in the continuous coloring problem",
            "venue": "Journal of Statistical Mechanics: Theory and Experiment 2021(11), 113302 ",
            "year": 2021
        },
        {
            "authors": [
                "C. Baldassi",
                "C. Borgs",
                "J.T. Chayes",
                "A. Ingrosso",
                "C. Lucibello",
                "L. Saglietti",
                "R. Zecchina"
            ],
            "title": "Unreasonable effectiveness of learning neural networks: From accessible states and robust ensembles to basic algorithmic schemes",
            "venue": "Proceedings of the National Academy of Sciences 113(48), E7655 ",
            "year": 2016
        },
        {
            "authors": [
                "C. Baldassi",
                "E.M. Malatesta",
                "R. Zecchina"
            ],
            "title": "Properties of the geometry of solutions and capacity of multilayer neural networks with rectified linear unit activations",
            "venue": "Phys. Rev. Lett. 123, 170602 ",
            "year": 2019
        },
        {
            "authors": [
                "C. Baldassi",
                "F. Pittorino",
                "R. Zecchina"
            ],
            "title": "Shaping the learning landscape in neural networks around wide flat minima",
            "venue": "Proceedings of the National Academy of Sciences 117(1), 161 ",
            "year": 2020
        },
        {
            "authors": [
                "C. Baldassi",
                "E.M. Malatesta",
                "M. Negri",
                "R. Zecchina"
            ],
            "title": "Wide flat minima and optimal generalization in classifying high-dimensional gaussian mixtures",
            "venue": "Journal of Statistical Mechanics: Theory and Experiment 2020(12), 124012 ",
            "year": 2020
        },
        {
            "authors": [
                "A. El Alaoui",
                "M. Sellke"
            ],
            "title": "Algorithmic pure states for the negative spherical perceptron",
            "venue": "Journal of Statistical Physics 189(2), 27 ",
            "year": 2022
        },
        {
            "authors": [
                "I.J. Goodfellow",
                "O. Vinyals",
                "A.M. Saxe"
            ],
            "title": "Qualitatively characterizing neural network optimization problems",
            "venue": "arXiv preprint arXiv:1412.6544 ",
            "year": 2014
        },
        {
            "authors": [
                "J. Frankle"
            ],
            "title": "Revisiting \u201dqualitatively characterizing neural network optimization problems",
            "venue": "NeurIPS 2020 Workshop: Deep Learning through Information Geometry ",
            "year": 2020
        },
        {
            "authors": [
                "T.J. Vlaar",
                "J. Frankle"
            ],
            "title": "What can linear interpolation of neural network loss landscapes tell us",
            "venue": "International Conference on Machine Learning, pp. 22325\u201322341. PMLR ",
            "year": 2022
        },
        {
            "authors": [
                "X. Wang",
                "A.N. Wang",
                "M. Zhou",
                "R. Ge"
            ],
            "title": "Plateau in monotonic linear interpolation \u2014 a \u201dbiased\u201d view of loss landscape for deep networks",
            "venue": "The Eleventh International Conference on Learning Representations ",
            "year": 2023
        },
        {
            "authors": [
                "T. Garipov",
                "P. Izmailov",
                "D. Podoprikhin",
                "D.P. Vetrov",
                "A.G. Wilson"
            ],
            "title": "Loss surfaces",
            "venue": "mode connectivity, and fast ensembling of dnns, In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi and R. Garnett, eds., Advances in Neural Information Processing Systems, vol. 31. Curran Associates, Inc. ",
            "year": 2018
        },
        {
            "authors": [
                "B.L. Annesi",
                "C. Lauditi",
                "C. Lucibello",
                "E.M. Malatesta",
                "G. Perugini",
                "F. Pittorino",
                "L. Saglietti"
            ],
            "title": "The star-shaped space of solutions of the spherical negative perceptron",
            "venue": "arXiv preprint arXiv:2305.10623 ",
            "year": 2023
        }
    ],
    "sections": [
        {
            "text": "In these pedagogic notes I review the statistical mechanics approach to neural networks, focusing on the paradigmatic example of the perceptron architecture with binary an continuous weights, in the classification setting. I will review the Gardner\u2019s approach based on replica method and the derivation of the SAT/UNSAT transition in the storage setting. Then, I discuss some recent works that unveiled how the zero training error configurations are geometrically arranged, and how this arrangement changes as the size of the training set increases. I also illustrate how different regions of solution space can be explored analytically and how the landscape in the vicinity of a solution can be characterized. I give evidence how, in binary weight models, algorithmic hardness is a consequence of the disappearance of a clustered region of solutions that extends to very large distances. Finally, I demonstrate how the study of linear mode connectivity between solutions can give insights into the average shape of the solution manifold.\nContents"
        },
        {
            "heading": "1 Introduction 2",
            "text": ""
        },
        {
            "heading": "2 Gardner\u2019s computation 3",
            "text": "2.1 Statistical mechanics representation 3 2.2 Simple geometric properties of the solution space 4 2.3 The typical Gardner volume 4 2.4 Replica Method 5 2.5 Replica-Symmetric ansatz 7 2.6 SAT/UNSAT transition 9"
        },
        {
            "heading": "3 Landscape geometry 11",
            "text": "3.1 Local Entropy 12 3.2 Algorithmic hardness 15"
        },
        {
            "heading": "4 Linear mode connectivity 17",
            "text": "References 19\nar X\niv :2\n30 9.\n09 24\n0v 1\n[ co\nnd -m\nat .d\nis -n\nn] 1\n7 Se\np 20\n23"
        },
        {
            "heading": "1 Introduction",
            "text": "Suppose we are given a dataset D = \u03be\u00b5, y\u00b5 P \u00b5=1 composed by a set of P, N -dimensional \u201cpatterns\u201d \u03be\u00b5i , i = 1, . . . , N and the corresponding label y \u00b5. The patterns can represent whatever type of data, for example an image, text or audio. In the binary classification setting that we will consider, y\u00b5 = \u00b11; it will represent some particular property of the data that we want to be able to predict, e.g. if in the image there is a cat or a dog. The goal is to learn a function f : RN \u2192 \u00b11 that is able to associate to each input \u03be \u2208 RN the corresponding label. This function should be able to generalize, i.e. to predict the label corresponding to a pattern not in the training set.\nIn the following we will consider, in order to fit training set, the so-called perceptron model, as it is the simplest, yet non-trivial, one-layer neural network that we can study using statistical mechanics tools. Given an input pattern \u03be\u00b5 the perceptron predicts a label y\u0302\u00b5\ny\u0302\u00b5 = sign 1 p\nN\nN \u2211\ni=1\nwi\u03be \u00b5 i\n(1)\nwhere wi are the N parameters that need to be adjusted in order to fit the training set. If the possible values of w lie on the vertices of the hypercube, i.e. wi = \u00b11, \u2200i, the model is called binary perceptron. If instead the weights lie on the N\u2212dimensional sphere of radius p N , i.e. \u2211N i=1 w 2 i = w \u00b7 w = N the model is called spherical perceptron.\nAs usual in statistical mechanics, we consider the case of a synthetic dataset, where the patterns are formed by random i.i.d. N -dimensional Gaussian patterns \u03be\u00b5i \u223cN (0,1), i = 1, . . . , N . Depending on the choice of the label y\u00b5, we can define two different scenarios:\n\u2022 in the so called teacher-student scenario, y\u00b5 is generated by a network, called \u201cteacher\u201d. The simplest case corresponds to a perceptron architecture, i.e.\ny\u00b5 = sign 1 p\nN\nN \u2211\ni=1\nwTi \u03be \u00b5 i\n(2)\nwith random i.i.d. Gaussian or binary weights: wTi \u223c N (0,1) or w T i = \u00b11 with equal probability. This model has been studied by Gardner and Derrida [1] and Gy\u00f6rgyi in the binary case [2].\n\u2022 In real scenarios, sometimes data can be corrupted or the underline rule that generates the label for a given input is noisy. This makes the problem sometimes being unrealizable, since, expecially for large datasets, no student exists that is able to learn the training set. The storage problem, describes the case of extreme noise in the dataset: y\u00b5 is chosen to be \u00b11 with equal probability. As a matter of fact this makes the label completely independent from the input data.\nIn the following we will focus on the storage problem setting. Even if this problem does not present the notion of generalization error, still, it is a scenario where we can try to answer very simple questions using the same statistical mechanics tools that can be also applied to teacher-student settings (see for example [3]). For example: what is the maximum value of the size of the training set P that we are able to fit? It was discovered by the pioneering work by Gardner [4, 5] and Krauth and Mezard [6] that, in both binary and continuous settings, in the large P and N limit, with the ratio \u03b1 \u2261 P/N fixed, it exists a sharp phase transition \u03b1c separating a satisfiable (SAT) phase \u03b1 < \u03b1c where solutions to the problem exists, to an unsatisfiable (UNSAT) phase where the set of solutions is empty. The goal of the next Section 2\nis to introduce the statistical mechanics tools through which we can answer such a question. The same tools, in turn, will provide valuable insights into how the solutions are arranged geometrically."
        },
        {
            "heading": "2 Gardner\u2019s computation",
            "text": ""
        },
        {
            "heading": "2.1 Statistical mechanics representation",
            "text": "Every good statistical mechanics computation starts by writing the partition function of the model under consideration. This is the goal of this section.\nFitting the training set means that we need to satisfy the following set of constraints\n\u2206\u00b5 \u2261 y\u00b5 p\nN\n\u2211\ni\nwi\u03be \u00b5 i \u2265 0 , \u00b5= 1, . . . , P (3)\nindeed if the stability\u2206\u00b5 of example \u00b5 in the training set is positive, it means that the predicted label is equal to the true one. In general we can try to ask to fit the training set in a way that is robust with respect to noisy perturbation of the inputs; this can be done by requiring that the stability of each pattern is larger that a given threshold \u03ba\u2265 0 called margin\n\u2206\u00b5 \u2261 y\u00b5 p\nN\n\u2211\ni\nwi\u03be \u00b5 i \u2265 \u03ba , \u00b5= 1, . . . , P (4)\nThe total amount of noise that we can inject in \u03be\u00b5 without changing the corresponding label y\u00b5 depends on \u03ba. Notice that if \u03ba\u2265 0 the solutions to (4) are also solutions to (3). Sometimes satisfying all the contraints (3) can be hard or even impossible, for example when the problem is not linearly separable. In that case one can relax the problem by counting as \u201csatisfied\u201d certain violated contraints: one way to do that is to impose a negative margin \u03ba. The corresponding model is usually called negative perceptron. As we will see in the next subsection this changes dramatically the nature of the problem.\nWe are now ready to write down the partition function, as\nZD =\n\u222b\nd\u00b5(w )XD(w ;\u03ba) (5)\nwhere\nXD(w ;\u03ba)\u2261 P \u220f\n\u00b5=1\n\u0398 y\u00b5 p\nN\n\u2211\ni\nwi\u03be \u00b5 i \u2212 \u03ba\n(6)\nis an indicator function that selects a solution to the learning problem. Indeed \u0398(x) is the Heaviside Theta function that gives 1 if x > 0 and zero otherwise. d\u00b5(w ) is a measure over the weights that depends on the spherical/binary nature of the problem under consideration\nd\u00b5(w ) = \u220fN i=1 dwi \u03b4 (w \u00b7 w \u2212 N) , spherical case \u220fN\ni=1 dwi 1 2N \u220f i [\u03b4 (wi \u2212 1) +\u03b4 (wi + 1)] , binary case (7)\nThe partition function is called also Gardner volume [5] since it measures the total volume (or number in the binary case) of networks satisfying all the constraints imposed by the training set. Notice that sending \u03be\u00b5i \u2192 y \u00b5\u03be \u00b5 i one does not change the probability measure over the patterns \u03be\u00b5, therefore we can simply set y\u00b5 = 1 for all \u00b5= 1, . . . , P without loss of generality."
        },
        {
            "heading": "2.2 Simple geometric properties of the solution space",
            "text": "We want here to give a simple geometrical interpretation of the space of solutions corresponding to imposing constraints of equation (4). By those simple arguments, we will be able to unravel the convexity/non-convexity properties of the space of solutions.\nLet\u2019s start from the spherical case. For simplicity consider also \u03ba = 0. Initially, when no pattern has been presented, the whole volume of the N -dimensional sphere is a solution. When we present the first pattern \u03be1 uniformly generated on the sphere, the allowed solutions lie on the half-sphere with a positive dot product with \u03be1. Of course, the same would happen if we had presented any other pattern \u03be\u00b5 of the training set. The intersection of those half-spheres form the space of solutions. Since the intersection of half spheres is a convex set, it turns out that the manifold of solutions is always convex, see left panel of Fig. 1 for an example. Notice that this is also true if one is interested in looking to the subset of solutions having \u03ba > 0. If one keeps adding constraints one can obtain an empty set. The minimal density of constraints for which this happens is the SAT/UNSAT transition.\nThe middle panel of Fig. 1, refers to the \u03ba < 0 case, i.e. the spherical negative perceptron [7]. In this problem the space of solutions can be obtained by removing from the sphere a convex spherical cap, one for each pattern (blue and red regions). As a result the manifold of solutions is non-convex and, if one adds too many constraints the space of solutions can become also disconnected, before the SAT/UNSAT transition.\nIn the right panel of Fig. 1 we show the binary case. Since wi = \u00b11, the hypercube (in red) is inscribed in the sphere in N -dimension, i.e. the vertices of the hypercube are contained in the space of solutions of the corresponding spherical problem (green region). As can be readily seen from the example in the figure, the binary weight case is a non-convex problem (even for \u03ba\u2265 0), since in order to go from one solution to another, it can happen that one has to pass through a set of vertices not in the solution space."
        },
        {
            "heading": "2.3 The typical Gardner volume",
            "text": "The Gardner volume ZD introduced in equation 5 is random variable since it explicitly depends on the dataset. The goal of statistical mechanics is to characterize the typical, i.e. the most probable value of this random quantity. A first guess would be to compute the averaged volume \u2329ZD\u232aD. However, since (5) involves a product of many random contributions, the most\nprobable value of ZD and its average are expected to not coincide 1. On the other hand, the log of the product of independent random variables is equivalent to a large sum of independent terms, that, because of the central limit theorem, is Gaussian distributed; in that case we expect that the most probable value coincides with the average. Therefore we expect that for large N\nZD \u223c eN\u03c6 (8)\nwhere\n\u03c6 = lim N\u2192\u221e 1 N \u2329ln ZD\u232aD . (9)\nis the averaged log-volume. Since we are at zero training error \u03c6 coincides with the entropy of solutions. Performing the average over the log is usually called as quenched average in spin glass theory, to distinguish from the log of the average, which is instead called annealed. Annealed averages are much easier than quenched ones; even if they do not give information to the typical value of a random variable, they can still be useful, since they can give an upper bound to the quenched entropy. Indeed due to Jensen\u2019s inequality\n\u03c6 \u2264 \u03c6ann = limN\u2192\u221e 1 N ln\u2329ZD\u232aD . (10)"
        },
        {
            "heading": "2.4 Replica Method",
            "text": "In order to compute the average of the log we use the replica trick\n\u2329ln ZD\u232aD = limn\u21920 \u2329ZnD\u232aD \u2212 1 n = lim n\u21920 1 n ln\u2329ZnD\u232aD . (11)\nThe replica method consists in performing the average over the disorder of ZnD considering n integer (a much easier task with respect to averaging the log of ZD), and then performing an analytic continuation of the result to n \u2192 0 [8]. It has been used firstly in spin glasses models such as the Sherrington-Kirkpatrick model [9] and then applied by E. Gardner to neural networks [5, 10]. Replicating the partition function and introducing an auxiliary variable va\u00b5 \u2261 1p N \u2211 i w a i \u03be \u00b5 i using a Dirac delta function, we have\nZnD =\n\u222b n \u220f\na=1\nd\u00b5(wa) \u220f\n\u00b5a\n\u0398 1 p\nN\n\u2211\ni\nwai \u03be \u00b5 i \u2212 \u03ba\n=\n\u222b\n\u220f\na\u00b5\ndv\u00b5a d v\u0302 \u00b5 a\n2\u03c0\n\u220f\n\u00b5a\n\u0398 v\u00b5a \u2212\u03ba\neiv \u00b5 a v\u0302 \u00b5 a\n\u222b n \u220f\na=1\nd\u00b5(wa) e\u2212i \u2211 \u00b5,a v\u0302 \u00b5 a 1p N \u2211 i w a i \u03be \u00b5 i\n(12)\nwhere we have used the integral representation of the Dirac delta function\n\u03b4(v) =\n\u222b\nd v\u0302 2\u03c0 eiv v\u0302 . (13)\nNow we can perform the average over the patterns in the limit of large N obtaining\n\u220f\n\u00b5i\ne\u2212i \u03be \u00b5 ip N \u2211 a w a i v\u0302 \u00b5 a\n\u03be \u00b5 i\n= \u220f\n\u00b5i\ne\u2212 1 2N \u2211 a w a i v\u0302 a \u00b5\n2\n= \u220f\n\u00b5\ne\u2212 \u2211 a<b v\u0302 \u00b5 a v\u0302 b \u00b5( 1N \u2211 i w a i w b i )\u2212 12 \u2211 a(v\u0302 \u00b5 a )2 .\n(14)\n1In the spin glass literature one says that ZD is not a self-averaging quantity for large N .\nNext we can enforce the definition of the n\u00d7 n matrix of order parameters\nqab \u2261 1 N\nN \u2211\ni=1\nwai w b i (15)\nby using a delta function and its integral representation. qab represents the typical overlap between two replicas a and b, sampled from the Gibbs measure corresponding to (5) and having the same realization of the training set. Due to the binary and spherical normalization of the weights this quantity is bounded \u22121\u2264 qab \u2264 1. We have\n\u2329ZnD\u232aD = \u222b \u220f\na<b\ndqabdq\u0302ab 2\u03c0/N e\u2212N \u2211 a<b qab q\u0302ab\n\u222b\n\u220f\na\nd\u00b5(wa) e \u2211 a<b q\u0302ab \u2211 i w a i w b i\n\u00d7 \u222b \u220f\na\u00b5\ndv\u00b5a d v\u0302 \u00b5 a\n2\u03c0\n\u220f\n\u00b5a\n\u0398 v\u00b5a \u2212 \u03ba\nei \u2211 a\u00b5 v \u00b5 a v\u0302 \u00b5 a\u2212 1 2 \u2211 a\u00b5(v\u0302 \u00b5 a )2\u2212 \u2211 a<b,\u00b5 qab v\u0302 \u00b5 a v\u0302 b \u00b5 .\n(16)\nThe next step of every replica computation is to notice that the terms depending on the patterns \u00b5 = 1, . . . , P and on the index of the weights i = 1, . . . , N have been decoupled (initially they were not), at the price of coupling the replicas (that initially were uncopled). So far the computation was the same independently on the nature of the weights. From now on, however the computation is different, since we need to explicit the form of the measure d\u00b5(w ) in order to factorize the expression over the index i. In the next paragraph we therefore focus on the binary case first, moving then to the spherical case.\nBinary case Setting P = \u03b1N , in the binary weight case we reach the following integral representation of the averaged replicated partition function\nZnD D\u221d \u222b \u220f\na<b\ndqabdq\u0302ab 2\u03c0 eNS(q, q\u0302) (17)\nwhere we have defined\nSbin(q, q\u0302) = GbinS (q, q\u0302) +\u03b1GE (q) (18a)\nGbinS = \u2212 1 2 \u2211\na \u0338=b\nqabq\u0302ab + ln \u2211\n{wa}\ne 1 2 \u2211 a \u0338=b q\u0302abw awb (18b)\nGE = ln\n\u222b\n\u220f\na\ndvad v\u0302a 2\u03c0 \u220f\na\n\u0398 (va \u2212\u03ba) ei \u2211 a va v\u0302a\u2212 1 2 \u2211 ab qab v\u0302a v\u0302b . (18c)\nGS is the so called \u201centropic\u201d term, since it represents the logarithm of the volume at \u03b1 = 0, where there are no constraints induced by the training set. GE is instead called the \u201cenergetic\u201d term and it represents the logarithm of the fraction of solutions. In the energetic term we have also used the fact that, because of equation (15), qaa = 1.\nSpherical case In the spherical case, one needs to do a little more work in order to decouple the i index in the spherical contraints of equation (7): \u222b\n\u220f\na\nd\u00b5(wa) e \u2211 a<b q\u0302ab \u2211 i w a i w b i \u221d \u222b \u220f\na\ndq\u0302aa 2\u03c0\n\u222b\n\u220f\nai\ndwai e \u2212 N2 \u2211 a q\u0302aa+ 1 2 \u2211 ab q\u0302ab \u2211 i w a i w b i\n=\n\u222b\n\u220f\na\ndq\u0302aa 2\u03c0 e\u2212 N 2 \u2211 a q\u0302aa\n\u222b\n\u220f\na\ndwa e 1 2 \u2211 ab q\u0302abw awb N (19)\nTherefore the replicated averaged partition in the spherical case is equal to\nZnD D\u221d \u222b \u220f\na<b\ndqabdq\u0302ab 2\u03c0\n\u222b\ndq\u0302aa 2\u03c0 eNS(q, q\u0302) (20)\nwhere we have defined\nSsph(q, q\u0302) = GsphS (q\u0302) +\u03b1GE (q) (21a)\nGsphS = \u2212 1 2 \u2211\nab\nqabq\u0302ab + ln\n\u222b\n\u220f\na\ndwa e 1 2 \u2211 ab q\u0302abw awb (21b)\n= \u2212 1 2 \u2211\nab\nqabq\u0302ab \u2212 1 2 lndet(\u2212q\u0302)\nand we have used again the definition qaa = 1. Notice how only the entropic term changes with respect to the binary case, whereas the energetic term is the same defined in (18c).\nSaddle points In either case, being the model binary or spherical, the corresponding expressions can be evaluated by using a saddle point approximation, since we are interested in a regime where N is large. The saddle point have to be found by finding two n\u00d7 n matrices qab and q\u0302ab that maximize the action S. This gives access to the entropy \u03c6 of solutions\n\u03c6 = lim N\u2192\u221e 1 N \u2329ln ZD\u232aD = limn\u21920 1 n max {q,q\u0302} S(q, q\u0302) . (22)"
        },
        {
            "heading": "2.5 Replica-Symmetric ansatz",
            "text": "Finding the solution to the maximization procedure is not a trivial task. Therefore one proceeds by formulating a simple parameterization or ansatz on the structure of the saddle points. The simplest ansatz one can formulate is the Replica-Symmetric (RS) one. Considering the binary case, this reads\nqab = \u03b4ab + q(1\u2212\u03b4ab) , (23a) q\u0302ab = q\u0302(1\u2212\u03b4ab) (23b)\nIn the spherical case, instead one has (qab is the same as in binary weights)\nq\u0302ab = \u2212Q\u0302\u03b4ab + q\u0302(1\u2212\u03b4ab) (24)\nThe entropy in both cases can be written as\n\u03c6 = GS +\u03b1GE (25)\nwhere we remind that the entropic term depends on the binary or spherical nature of the weights and are\nGbinS \u2261 limn\u21920 GbinS n = \u2212 q\u0302 2 (1\u2212 q) + \u222b Dx ln 2cosh( \u00c6 q\u0302x) , (26a)\nGsphS \u2261 limn\u21920 GsphS n = 1 2 Q\u0302+ qq\u0302 2 + 1 2 ln 2\u03c0 Q\u0302+ q\u0302 + 1 2 q\u0302 Q\u0302+ q\u0302 . (26b)\nThe energetic term is instead common to both models\nGE \u2261 limn\u21920 GE n =\n\u222b\nDx ln H \u03ba+pqx p\n1\u2212 q\n. (27)\nIn the previous equations we denoted by Dx \u2261 d xp 2\u03c0 e\u2212x 2/2 the standard normal measure and we have defined the function\nH(x)\u2261 \u222b \u221e\nx D y =\n1 2 erfc x p\n2\n(28)\nThe value of the order parameters q\u0302, Q\u0302 and q for the spherical case and q\u0302, q for the binary case can be obtained by differentiating the entropy and equating it to zero.\nSaddle point equations: spherical perceptron The saddle point equations for the spherical case read\nq = q\u0302\n(Q\u0302+ q\u0302)2 (29a)\n1= Q\u0302+ 2q\u0302\n(Q\u0302+ q\u0302)2 (29b)\nq\u0302 = \u2212\u03b1 \u2202 GE \u2202 q = \u03b1 1\u2212 q\n\u222b\nDx GH \u03ba+pqx p\n1\u2212 q\n2\n(29c)\nwhere we have defined the function GH(x)\u2261 G(x)H(x) , G(x) being a standard normal distribution. The saddle point equations can be simplified, by explicitly expressing q\u0302 and Q\u0302 in terms of q. Indeed equations (29a) (29b) are simple algebraic expression for q\u0302 and Q\u0302 in terms of q; they can be explicitly inverted as q\u0302 = q(1\u2212q)2 , Q\u0302 = (1\u2212 2q)/(1\u2212 q)\n2. Inserting the expression of q\u0302 inside (29c) we get an equation for q only\nq = \u03b1 (1\u2212 q) \u222b Dx GH \u03ba+pqx p\n1\u2212 q\n2\n, (30)\nSaddle point equations: binary perceptron In the binary case only the equation involving derivatives of the entropic term changes. The saddle point equations are therefore\nq =\n\u222b\nDx tanh2 \u00c6 q\u0302x\n(31a)\nq\u0302 = \u2212\u03b1 \u2202 GE \u2202 q = \u03b1 1\u2212 q\n\u222b\nDx GH \u03ba+pqx p\n1\u2212 q\n2\n(31b)\nThe saddle point equations (equation (30) for the spherical and (31) for the binary case), can be easily solved numerically by simple recursion with \u03b1 and \u03ba being two external parameters. It is important to notice that the value of the order parameter q has physical meaning. Indeed one can show that q is the typical (i.e. the most probable) overlap between two solutions w 1 and w 2 extracted from the Gibbs measure (6), i.e.\nq =\n\u00ae\u222b\nd\u00b5(w 1) d\u00b5(w 2) 1\nN\n\u2211\ni w 1 i w 2 i XD(w 1;\u03ba)XD(w 2;\u03ba)\nZ2D\n\u00b8\nD (32)\nTherefore solving the saddle point equations gives access to interesting geometrical information: it suggests how distant the solutions extracted from the Gibbs measure (6) are from each other. The distance between solutions can be obtained from the overlap using the relation\nd = 1\u2212 q\n2 \u2208 [0,1] (33)\nIn the binary setting this definition coincides with the Hamming distance between w 1 and w 2, since in that case d is equal to the fraction of indexes i at which the corresponding w1i and w2i are different. The distance between solutions extracted from the Gibbs measure with a given margin \u03ba is shown as a function of \u03b1 in Fig. 2 for the spherical (left panel) and for the binary (right panel) case. In both cases one can clearly see that the distance is monotonically decreasing with \u03b1 it also exists a value of \u03b1 for which the distance goes to zero. In addition is interesting to notice that fixing the value of \u03b1 to a certain value, the distance decreases as the margin is increased, meaning that more robust solutions are located in a smaller region of the solution space [11].\n2.6 SAT/UNSAT transition\nOnce the saddle point equations are solved numerically, we can compute the value of the entropy (i.e. the total number/volume) of solutions using (24) for a given \u03b1 and margin \u03ba. The entropy is plotted in the left panel of Fig. 3 and Fig. 4 for the binary and spherical cases respectively.\nIt is interesting to notice that in both models, for a fixed value of \u03ba, there is a critical value of \u03b1 such that the typical distance between solutions goes to zero, i.e. the solution space shrinks to a point as we approach it. The corresponding entropy diverges to \u2212\u221e at this value of \u03b1. This defines what we have called SAT/UNSAT transition \u03b1c(\u03ba) in the spherical case. This cannot be the true value of \u03b1c(\u03ba) in the binary case: in binary models the entropy cannot be negative, since we are counting solutions (not measuring volumes as in the spherical counterpart). This means that the analytical results obtained are wrong whenever \u03c6 < 0; for this reason in Fig 2 and 4 the unphysical parts of the curves are dashed. Where is the computation wrong in the binary case? As shown by [6], it is the RS ansatz that, although stable, is wrong; at variance with the spherical case for \u03ba \u2265 0, in the binary case the solution space is non-convex, so the solution space can be disconnected; in those cases it is well known that the RS ansatz might fail. As shown by Krauth and M\u00e9zard [6] (see also [3] for a nice \u201cdiscussion\u201d), by using a one-step replica symmetry breaking (1RSB) ansatz [12], in order to compute the SAT/UNSAT transition we should compute the value of \u03b1 for which the RS entropy vanishes\n\u03c6RS(\u03b1c) = 0 . (34)\nThis is known as the zero entropy condition; at this value of \u03b1c , the distance between solutions does not go to 0, for \u03ba = 0 for example d \u2243 0.218. In the right panel of Fig. 3 we plot \u03b1c as\na function of \u03ba. In particular for \u03ba= 0, the value \u03b1c = 0.833 . . . can be obtained numerically. This value is still not rigorously proved, although in recent years [13], proved that the value obtained by the replica formula is lower bound with positive probability.\nIn the spherical perceptron \u03ba \u2265 0 the space of solution is convex and the RS ansatz gives correct results. The critical capacity can be therefore obtained by sending q\u2192 1. In order to get an explicit expression that can be evaluated numerically, one therefore has to do the limit explicitly, so a a little bit of work has still to be done.\nThe entropy is, in both cases, a monotonically decreasing function of \u03b1. This should be expected: when we increase the number of contraints, we should expect that the solution space shrinks. Moreover for a fixed value of \u03b1 the entropy is a monotonically decreasing function of the margin: this means that solutions with larger margin are exponentially fewer in N (remind (8)).\nq\u2192 1 limit in the spherical perceptron In order to perform the limit analytically [4,14] it is convenient to use the following change of variables in the entropy\nq = 1\u2212\u03b4q (35)\nand then send \u03b4q\u2192 0. We now insert this into the RS energetic term (27); using the fact that ln H(x)\u2243 \u221212 ln(2\u03c0)\u2212 ln x \u2212 x2 2 as x \u2192\u221e, retaining only the diverging terms we get\n\u222b\nDx ln H \u03ba+pqx p\n1\u2212 q\n\u2243 \u222b +\u221e\n\u2212\u03ba Dx\n1 2 ln\u03b4q\u2212 (\u03ba+ x)2 2\u03b4q = 1 2 ln(\u03b4q)H (\u2212\u03ba)\u2212 B(\u03ba) 2\u03b4q (36)\nand\nB(\u03ba) =\n\u222b \u221e\n\u2212\u03ba Dz0 (\u03ba+ z0)\n2 = \u03baG (\u03ba) + \u03ba2 + 1 H (\u2212\u03ba) . (37)\nThe free entropy is therefore\n\u03c6 = 1\n2\u03b4q + 1 2 ln\u03b4q+ \u03b1 2 ln(\u03b4q)H (\u2212\u03ba)\u2212 B(\u03ba) \u03b4q\n(38)\nThe derivative with respect to \u03b4q gives the saddle point condition that \u03b4q itself must satisfy\n2 \u2202 \u03c6\n\u2202 \u03b4q = 1 \u03b4q \u2212 1 \u03b4q2 +\u03b1 H (\u2212\u03ba) \u03b4q + B(\u03ba) \u03b4q2 = 0 . (39)\nWhen we send \u03b4q \u2192 0, for a fixed value of \u03ba we can impose that we are near the critical capacity \u03b1= \u03b1c \u2212\u03b4\u03b1 and \u03b4q = C\u03b4\u03b1. We get\n2 \u2202 \u03c6\n\u2202 \u03b4q = 1 C\u03b4\u03b1 \u2212 1 C2\u03b4\u03b12 + (\u03b1c \u2212\u03b4\u03b1) H (\u2212\u03ba) C\u03b4\u03b1 + B(\u03ba) C2\u03b4\u03b12\n= 1\nC\u03b4\u03b1\n1+\u03b1cH (\u2212\u03ba)\u2212 B(\u03ba)\nC\n+ 1\nC2\u03b4\u03b12 [\u03b1cB(\u03ba)\u2212 1] = 0 .\n(40)\nThe first term gives the scaling of \u03b4q, the second gives us the critical capacity in terms of the margin [5,10].\n\u03b1c(\u03ba) = 1\nB(\u03ba) = 1 \u03baG (\u03ba) + (\u03ba2 + 1)H (\u2212\u03ba)\n(41)\nNotice that \u03b1c = 1\nB(\u03ba) is equivalent to imposing that the divergence 1/\u03b4q in the free entropy (38) is eliminated at the critical capacity (so that it correctly goes to \u2212\u221e in that limit). In particular for \u03ba= 0 we get\n\u03b1c(\u03ba= 0) = 2 , (42)\na results that has been derived rigorously by Cover in 1965 [15]. In the right panel of Fig. 4 we plot \u03b1c as a function of \u03ba obtained by (41). It is important to mention that, since in the case \u03ba \u2264 0 the model becomes non-convex, the RS estimate of the critical capacity (41) gives uncorrect results, even if it is un upper bound to the true value. We refer to [16] for a rigorous upper bound to the critical capacity and to [14] for the evaluation of \u03b1c(\u03ba) based on a 1RSB ansatz. However the correct result should be obtained by performing a full-RSB ansatz [7,12]."
        },
        {
            "heading": "3 Landscape geometry",
            "text": "One of the most important open puzzles in deep learning is to understand how the error and the loss landscape look like [17] especially as a function of the number of parameters, and how the shape of the learning landscape impacts the learning dynamics. So far there has been growing empirical evidence that, especially in the overparameterized regime, where the number of parameters is much larger that the size of the training set, the landscape presents a region at low values of the loss with a large number of flat directions. For example, studies of the\nspectrum of the Hessian [18,19] on a local minima or a saddles found by Stochastic Gradient Descent (SGD) optimization, show the presence of a large number of zero and near to zero eigenvalues. This region seem to be attractive for the gradient-based algorithms: the authors of [20] show that different runs of SGD end in the same basin by explicitly finding a path connecting them, in [21, 22] it was shown that very likely even the whole straight path between two different solutions lies at zero training error, i.e. they are linear mode connected. Also, an empirical evidence of an implicit bias of SGD towards flat regions have been shown in [23]. Study of the limiting dynamics of SGD [24, 25] unveil the presence of a diffusive behaviour in weight space, once the training set has been fitted. Simple 2D visualization of the loss landscape [26], provided insights into how commonly employed machine learning techniques for improving generalization also tend also to smooth the landscape. One of the most recent algorithms, Sharpness Aware Minimization (SAW) [27], explicitly designed to target flat regions within the loss landscape, consistently demonstrates improved generalization across a broad spectrum of datasets. In [28] it was suggested numerically that moving from the over to the underparameterized regime, gradient based dynamics suddendly becomes glassy. This observation raises the intriguing possibility of a phase transition occurring between these two regimes.\nIn this section we review some simple results obtained on one layer models concerning the characterization of the flatness of different classes of solutions. We will consider, the paradigmatic case of the binary perceptron, but, as we will see, the definitions of the quantities are general, and can be also used in the case of continuous weights. We will see that in the overparameterized regime (i.e. N \u226b P) solutions located in a very wide and flat region exist. However, as the number of constraints is increased, this region starts to shrink and at a certain point it breaks down in multiple pieces. This, in binary models, is responsible of algorithmic hardness and glassy dynamics."
        },
        {
            "heading": "3.1 Local Entropy",
            "text": "In order to quantify the flatness of a given solutions, several measures can be employed. One such obvious measure is the spectrum of the Hessian; however this quantity is not trivial to study analytically. Here we will employ the so called local entropy [29] measure. Given a configuration w\u0303 (in general it may not be a solution), its local entropy is defined as\nSD(w\u0303 , d;\u03ba) = 1 N lnND(w\u0303 , d;\u03ba) (43)\nwhere ND(w\u0303 , d;\u03ba) is a local Gardner volume\nND(w\u0303 , d;\u03ba)\u2261 \u222b d\u00b5(w )XD(w ;\u03ba)\u03b4(N(1\u2212 2d)\u2212 w \u00b7 w\u0303 ) (44)\nwhich counts2 solutions having margin at least \u03ba at a given distance d from w\u0303 . In the binary perceptron, we will set for simplicity \u03ba= 0 in the above quantity, since for \u03ba= 0 the problem is already non-convex. In (44) we have used the relation between overlap and distance of (33) to impose an hard constraint between w\u0303 and w . For any distance, the local entropy is bounded from above by the total number of configurations at distance d. This value, that we will call Smax, is attained at \u03b1= 0, i.e. when we do not impose any constraints. Moreover, since every point on the sphere is equivalent, Smax does not depend on w\u0303 and \u03ba and it reads3\nSmax(d) = \u2212d ln d \u2212 (1\u2212 d) ln(1\u2212 d) (45) 2In the spherical case it measures a volume. 3In the spherical case an equivalent analytical formula can be derived, see [14].\nwhich is of course always non-negative. In full generality, we are interested in evaluating the local entropy of solutions w\u0303 that have margin \u03ba\u0303 and that are sampled from a probability distribution PD(w\u0303 ; \u03ba\u0303). We are interested in computing the typical local entropy of those class of solutions, that is obtained averaging SD over PD and over the dataset, i.e.\n\u03c6FP(d; \u03ba\u0303,\u03ba) =\n\u222b\nd\u00b5(w\u0303 ) PD(w\u0303 ; \u03ba\u0303)SD(w\u0303 , d;\u03ba) D (46)\nThis \u201caveraged local entropy\u201d is usually called Franz-Parisi entropy in the context of mean field spin glasses [30]. In the following we will consider PD as the flat measure over solutions with margin \u03ba\u0303\nPD(w\u0303 ; \u03ba\u0303) = XD(w\u0303 , \u03ba\u0303) \u222b\nd\u00b5(w\u0303 )XD(w\u0303 , \u03ba\u0303) (47)\nThe first analytical computation of (46) was performed by Huang and Kabashima [31] in the binary perceptron by using the replica method (using steps similar to the ones done in Section 2.4, even if thet are much more involved). They considered the case of typical solutions, i.e. \u03ba\u0303= 0. A plot of \u03c6FP as a function of distance is shown in the left panel of Fig. 5 for several values of \u03b1. It exists a neighborhood of distances d \u2208 [0, dmin] around w\u0303 for which the local entropy is negative, which is unphysical in the binary case. The authors of [31] also showed analytically that \u03c6FP(d = 0) = 04 and that for d \u2192 0\n\u2202 \u03c6FP \u2202 d = \u03b1Cd\u22121/2 +O(ln d) (48)\nwhere C is a negative constant. This tells us that dmin > 0 for any \u03b1 > 0. This suggests that typical solutions are always isolated, meaning that there is a value dmin below which no solution can be found5, no matter what the value of the constraint density is. Notice that, since the overlap is normalized by N as in (32), in order to go from w\u0303 to the closest solution, one should flip an extensive number of weights. The plot of dmin is shown, as a function of \u03b1, in the right panel of Fig. 5.\nThe picture, however is far from being complete. This kind of landscape with point-like solutions suggests that finding such solution should be a hard optimization problem. Indeed, from the rigorous point of view, similar hardness properties has been shown to exist if the problem at hand verifies the so called Overlap Gap Property (OGP) [34, 35]. An optimization problem possesses OGP if picking any two solutions the overlap distribution between them exhibits a gap, i.e. they can be either close or far away from each other, but can\u2019t be in some interval in between. However, this is contrary to the numerical evidence given by simple algorithms such as the ones based on message passing [36,37] or gradient-based methods [14, 38] which find solutions easily. In [29] it was shown that non-isolated solutions indeed exists, but they are exponentially rarer (\u201csubdominant\u201d). In order to target those solutions one should give a larger statistical weight to those w\u0303 that are surrounded by a larger number of solutions. This led [29] to choose the measure\nPD(w\u0303 ; d) = e yNSD(w\u0303 ;d) \u222b\nd\u00b5(w\u0303 ) e yNSD(w\u0303 ;d) . (49)\nwhere y is a parameter (analogous to inverse temperature) that assigns larger statistical weight to solutions with high local entropy the larger it is. In the y \u2192\u221e limit, this measure focuses on\n4This should be expected: if you are at zero distance from w\u0303 , the only solution to be counted is w\u0303 itself! 5In principle, the fact that the local entropy is negative suggest that only a subestensive number of solutions can be found below dmin. Abbe and Sly [32] have also proved that in a slight variation of the model (the so called symmetric binary perceptron), that actually no solution can be found with probability one within a distance dmin. See also [33].\nsolutions with the highest local entropy for a given value of the distance d; notice, indeed, that this probability measure, depends explicitly on d, meaning that the particular type of solution w\u0303 sampled changes depending on the value of d chosen. In the same work it was shown not only that those high local entropy regions exist, but also that, in the teacher-student setting, they have better generalization properties with respect to typical, isolated solutions. For some rigorous results concerning the existence of those regions in similar binary perceptron models, see [39]. It was then shown that similar atypical clustered region play a similar algorithmic role in other optimization problems, such as coloring [40,41]. Those results suggested the design of new algorithms based on message passing that explicitly exploit local entropy maximization in order to find very well-generalizing solutions [42]. One of such algorithms is called focusing Belief-Propagation (fBP).\nA simpler way of finding the high local entropy regions is by using (47) with \u03ba\u0303 stricly larger than zero [11]. Indeed, the property of being robust to small noise perturbation in the input is related to the flatness of the energy landscape in the neighborhood of the solution. This type of approach not only produces a phenomenology similar to [29], but it also helps to unravel the structure of high local entropy regions in neural networks, as we shall see. Increasing the amount of robustness \u03ba\u0303, we therefore intuitively expect to target larger local entropy solutions. As shown in Fig. 6, this is indeed what one finds: as one imposes \u03ba\u0303 > 0, there always exists a neighborhood of d = 0, with positive local entropy (i.e. those solutions are surrounded by an exponential number of solutions). As shown in the inset of the same figure, the cluster is also very dense: for small d, the local entropy curve is indistinguishable from the total log-number of configurations at that distance Smax. As one increases \u03ba\u0303 from 0 one can see that one starts to sample different kind of regions in the solutions space. Firstly, if 0 < \u03ba\u0303 < \u03ba\u0303min(\u03b1) the local entropy is negative in an interval of distances d \u2208 [d1, d2] with d1 > 0: no solutions can be found in a spherical shell of radius d \u2208 [d1, d2]. Secondly, if \u03ba\u0303min(\u03b1)< \u03ba\u0303 < \u03ba\u0303u(\u03b1) the local entropy is positive but non-monotonic. This means that typical solutions with such \u03ba\u0303 are immersed within small regions that have a characteristic size: they can be considered as isolated (for \u03ba\u0303 < \u03ba\u0303min) or nearly isolated (for \u03ba\u0303 > \u03ba\u0303min) balls. Finally, for \u03ba\u0303 > \u03ba\u0303u, the local entropy is monotonic: this suggests that typical solutions with large enough \u03ba\u0303 are immersed in dense regions that do not seem to have a characteristic size and may extend to very large scales. The local entropy curve having the highest local entropy at a given value of \u03b1 is obtained by imposing the maximum possible margin \u03bamax(\u03b1), i.e. the margin for which\nthe entropy computed in section 2.5 vanishes6. Therefore high margin solutions are not only rarer and closer to each other with respect to typical solutions (as examined in section 2), but tend to focus on regions surrounded by lower margin, which in turn are surrounded by many other solutions having even lower margin and so on and so forth. The flat regions in the landscape can be though to be formed by the coalescence of minima corresponding to high-margin classifications.\nIt is worth mentioning that in [14] the same type of approach was applied to the simplest non-convex but continuous weights problem: the negative spherical perceptron. Although in the spherical case the most probable, typical solutions are not completely isolated7, a similar phenomenology is valid: higher margin solutions have always a larger local entropy. Evidence of the existence of these large local entropy regions has also been established in the context of the one hidden layer neural networks [43,44]. These studies also reveal that the use of the cross-entropy loss [44], ReLU activations [43], and even regularization of the weights [45] influence the learning landscape by inducing wider and flatter minima."
        },
        {
            "heading": "3.2 Algorithmic hardness",
            "text": "One can then explore how the highest local entropy curves evolve as a function of \u03b1. The procedure works as follows. Firstly, for a given value of \u03b1, we compute the maximum margin \u03bamax. Secondly, we plot the corresponding local entropy curve \u03c6FP as a function of d. Finally, we repeat the process using another value of \u03b1. The outcome is plotted for the binary perceptron in the left panel of Fig. 7. As expected from the previous section, for low values of \u03b1 the Franz-Parisi entropy is monotonic. However, as we keep increasing \u03b1 the curve becomes\n6The \u03bamax(\u03b1) curve is the inverse of \u03b1c(\u03ba). 7In the context of a spherical model, a solution would be isolated if the Franz-Parisi entropy goes to \u2212\u221e for a\ndmin > 0.\nnon-monotonic: as shown in the right panel of Fig. 7 the derivative of \u03c6FP with respect to d develops a zero at small distances. This critical value of the constrained density has been called \u201clocal entropy\u201d transition \u03b1LE, and it separates a phase where we can find a solution w\u0303 that is located inside a region that extends to very large distance \u03b1 < \u03b1LE, from one \u03b1 > \u03b1LE where it can\u2019t be found. Above another critical value \u03b1OGP of the constrained density, only the \u201ccompletely isolated ball\u201d phase exists: all the high-margin solutions, even if they remain surrounded by an exponential number of lower margin solutions up to the SAT/UNSAT transition, are completely isolated between each other.\nThe local entropy transition has been shown in the binary perceptron [11, 29] to be connected with the onset of algorithmic hardness: no algorithm has currently been found to be able to reach zero training error for \u03b1 > \u03b1LE. Surprisingly, \u03b1LE, which has been derived as a threshold marking a profound change in the geometry of regions of higher local entropy, also acquires the role of an insurmountable algorithmic barrier. Similar algorithmic thresholds have been found to exists in other binary neural network models [38]. Notice that, OGP is expected to hold for \u03b1 > \u03b1OGP; indeed if the Franz-Parisi entropy displays a gap for the \u03bamax curve, it will also exhibit an even larger one for every \u03ba\u0303 \u2208 [0,\u03bamax)8. In the binary perceptron model, \u03b1LE and \u03b1OGP are very close, so it is really difficult to understand which of the two prevents algorithms to find solutions in the infinite size limit.\nIn spherical non-convex models, such as the negative margin perceptron, even if the local entropy transitions can be identified similarly, it has been shown to be not predictive of the behaviour algorithms, but rather, there is numerical evidence showing that it demarks a region where the solution space is connected from one where it is not [14]. In the same work, evidence has been given that smart algorithms are able to reach the SAT/UNSAT transition. In [46] the authors, interestingly, developed an algorithm that is proved to be able to reach the SAT/UNSAT transition, provided that the OGP does not hold. A proof of the lack of OGP\n8To be precise the correct value of \u03b1OGP (and of \u03b1LE), can be obtained by sampling the reference with the largest local entropy at any distance, i.e. by using (49).\nin the spherical negative perceptron, however, is still lacking."
        },
        {
            "heading": "4 Linear mode connectivity",
            "text": "Another important line of research that has recently emerged in machine learning is the characterization of the connectivity between different solutions, i.e. the existence of a path lying a zero training error that connects them. Numerous studies [20, 47\u201351] have been started analyzing the so called linear mode connectivity, i.e. the particular case of a simple straight path.\nThe first statistical mechanics study of the behavior of the training error on the geodesic path connecting two solutions has been done in [52] for the negative spherical perceptron problem. Interestingly, a complete characterization of the shape of the solution space of the spherical negative perceptron has been produced. It is the aim of this section to briefly explain the analytical technique and the main results of the work.\nSuppose that we are given a value of \u03ba < 0 and we need to satisfy the constraints (4) using spherical weights. We sample two solutions w 1, w 2 from the probability distribution (49) using as margin respectively \u03ba1, \u03ba2 \u2265 \u03ba. Since the model is defined on the sphere, the straight path between w 1 and w 2 lies out of the sphere itself; therefore we project it on the sphere, obtaining a set of weights w \u03b3 that lie on the minimum length (i.e. the geodesic) path joining w 1 and w 2 Then we can define the geodesic path between w 1 and w 1:\nw \u03b3 = p\nN (\u03b3w 1 + (1\u2212 \u03b3)w 2) \u2225\u03b3w 1 + (1\u2212 \u03b3)w 2\u2225 , \u03b3 \u2208 [0,1] (50)\nFinally we compute the average training error of w \u03b3 i.e. the fraction of errors on the training set, averaged over the sampled w 1, w 2 and over the realization of the dataset\nE\u03b3 = limN\u2192+\u221e ED\n*\n1 P\nP \u2211\n\u00b5=1\n\u0398 \u2212w \u03b3 \u00b7 \u03be\u00b5 +\u03ba p N\n+\n\u03ba1,\u03ba2\n. (51)\nIn the previous expression the average \u2329\u2022\u232a\u03ba1,\u03ba2 is over the product of the two Gibbs ensembles (49) from which w 1 and w 2 are sampled from. E\u03b3 can be computed by using replica\nmethod. Depending on the values of the margin \u03ba1, \u03ba2 of w 1 and w 2 we can sample different regions of the solution space for a fixed value of \u03b1. We mainly have three cases:\n\u2022 \u03ba1 = \u03ba2 = \u03ba: the two solutions are typical. In this case, for every \u03b3 \u2208 [0, 1], E\u03b3 > 0, see red line in the left panel of Fig. 8.\n\u2022 \u03ba1 = \u03ba2 = \u03ba\u0303 > \u03ba: the two solutions are atypical and have the same margin. As can be seen in the left panel of Fig. 8, if \u03ba\u0303 is slightly above \u03ba, the maximum energy barrier is still non-zero, but in the neighborhood of w 1 and w 2 a region at zero training error appears. The size of this region increases with \u03ba\u0303. If \u03ba\u0303 > \u03ba\u22c6(\u03b1) the maximum barrier is zero, i.e. w 1 and w 2 are linear mode connected.\n\u2022 \u03ba1 = \u03ba and \u03ba2 = \u03ba\u0303 > \u03ba: one solution is typical and the other is atypical and E\u03b3 is asymmetric with respect to \u03b3= 1/2. E\u03b3 is shown in the right panel of Fig. 8, for several values of \u03ba\u0303. As one keeps increasing w\u0303 , the maximum of the barrier decreases, and for \u03ba\u0303 > \u03bakrn(\u03b1) > \u03ba\u22c6(\u03b1) the two solutions become linear mode connected. It can be also shown analytically that if \u03ba\u0303 > \u03bakrn(\u03b1) than w 2 is linear mode connected to another solution having margin \u03ba1 > \u03ba.\nThe picture exposed above holds in the overparameterized regime. In particular, the result obtained in the last point tells us that the solution space A of the spherical negative perceptron is star-shaped, since there exists a subset C \u2282 A, such that for any w \u22c6 \u2208 C then geodesic path from w \u22c6 to any other solution w lies entirely in A, i.e.\n{\u03b3w + (1\u2212 \u03b3)w \u22c6; \u03b3 \u2208 [0,1]} \u2282A . (52)\nThe subset C is called kernel of the star-shaped manifold. In Fig. 9 we show an intuitive, 2D picture of the space of solutions, showing a schematic interpretation of the results exposed. If we sample typical solutions we are basically sampling the tips of the star (see left panel), so that the geodesic path connecting them lies entirely at positive training error. Remind that those solutions are the largest in numbers and are located at a larger typical distance with respect to higher margin ones. As one increases the margin of w 1, w 2 they come closer together,\nfollowing the arms of the star to which they belong to. If the two margin are large enough, i.e. \u03ba1, \u03ba2 > \u03ba\u22c6(\u03b1) they are linear mode connected, and they lie in the blue region of middle panel. If the margin of one of the two solutions is even larger, \u03ba1 > \u03bakrn(\u03b1), then it will be located in the kernel, which is depicted in green in the right panel of Fig. 9. We refer to [52] to a more in-depth discussion of the attractiveness of the kernel region to gradient-based algorithms."
        }
    ],
    "title": "High-dimensional manifold of solutions in neural networks: insights from statistical physics",
    "year": 2023
}