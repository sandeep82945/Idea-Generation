{
    "abstractText": "Transformer architectures have exhibited remarkable performance in image superresolution (SR). Since the quadratic computational complexity of the selfattention (SA) in Transformer, existing methods tend to adopt SA in a local region to reduce overheads. However, the local design restricts the global context exploitation, which is crucial for accurate image reconstruction. In this work, we propose the Recursive Generalization Transformer (RGT) for image SR, which can capture global spatial information and is suitable for high-resolution images. Specifically, we propose the recursive-generalization self-attention (RG-SA). It recursively aggregates input features into representative feature maps, and then utilizes cross-attention to extract global information. Meanwhile, the channel dimensions of attention matrices (query, key, and value) are further scaled to mitigate the redundancy in the channel domain. Furthermore, we combine the RG-SA with local self-attention to enhance the exploitation of the global context, and propose the hybrid adaptive integration (HAI) for module integration. The HAI allows the direct and effective fusion between features at different levels (local or global). Extensive experiments demonstrate that our RGT outperforms recent state-of-the-art methods quantitatively and qualitatively. Code is released at https://github.com/zhengchen1999/RGT.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zheng Chen"
        },
        {
            "affiliations": [],
            "name": "Yulun Zhang"
        },
        {
            "affiliations": [],
            "name": "Jinjin Gu"
        },
        {
            "affiliations": [],
            "name": "Linghe Kong"
        },
        {
            "affiliations": [],
            "name": "Xiaokang Yang"
        }
    ],
    "id": "SP:33f3710a4547a15177bf9176a765229b75fa2170",
    "references": [
        {
            "authors": [
                "Alaaeldin Ali",
                "Hugo Touvron",
                "Mathilde Caron",
                "Piotr Bojanowski",
                "Matthijs Douze",
                "Armand Joulin",
                "Ivan Laptev",
                "Natalia Neverova",
                "Gabriel Synnaeve",
                "Jakob Verbeek"
            ],
            "title": "Xcit: Cross-covariance image transformers",
            "venue": "NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Marco Bevilacqua",
                "Aline Roumy",
                "Christine Guillemot",
                "Marie Line Alberi-Morel"
            ],
            "title": "Lowcomplexity single-image super-resolution based on nonnegative neighbor embedding",
            "venue": "In BMVC,",
            "year": 2012
        },
        {
            "authors": [
                "Richard Chen",
                "Rameswar Panda",
                "Quanfu Fan"
            ],
            "title": "Regionvit: Regional-to-local attention for vision transformers",
            "venue": "In ICLR,",
            "year": 2022
        },
        {
            "authors": [
                "Xiangyu Chen",
                "Xintao Wang",
                "Jiantao Zhou",
                "Chao Dong"
            ],
            "title": "Activating more pixels in image superresolution transformer",
            "venue": "arXiv preprint arXiv:2205.04437,",
            "year": 2022
        },
        {
            "authors": [
                "Zheng Chen",
                "Yulun Zhang",
                "Jinjin Gu",
                "Yongbing Zhang",
                "Linghe Kong",
                "Xin Yuan"
            ],
            "title": "Cross aggregation transformer for image restoration",
            "venue": "NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Zheng Chen",
                "Yulun Zhang",
                "Jinjin Gu",
                "Linghe Kong",
                "Xiaokang Yang",
                "Fisher Yu"
            ],
            "title": "Dual aggregation transformer for image super-resolution",
            "venue": "In ICCV,",
            "year": 2023
        },
        {
            "authors": [
                "Xiangxiang Chu",
                "Zhi Tian",
                "Yuqing Wang",
                "Bo Zhang",
                "Haibing Ren",
                "Xiaolin Wei",
                "Huaxia Xia",
                "Chunhua Shen"
            ],
            "title": "Twins: Revisiting the design of spatial attention in vision transformers",
            "venue": "NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Corinna Cortes",
                "Mehryar Mohri",
                "Afshin Rostamizadeh"
            ],
            "title": "Algorithms for learning kernels based on centered alignment",
            "year": 2012
        },
        {
            "authors": [
                "Tao Dai",
                "Jianrui Cai",
                "Yongbing Zhang",
                "Shu-Tao Xia",
                "Lei Zhang"
            ],
            "title": "Second-order attention network for single image super-resolution",
            "year": 2019
        },
        {
            "authors": [
                "Chao Dong",
                "Chen Change Loy",
                "Kaiming He",
                "Xiaoou Tang"
            ],
            "title": "Learning a deep convolutional network for image super-resolution",
            "venue": "In ECCV,",
            "year": 2014
        },
        {
            "authors": [
                "Xiaoyi Dong",
                "Jianmin Bao",
                "Dongdong Chen",
                "Weiming Zhang",
                "Nenghai Yu",
                "Lu Yuan",
                "Dong Chen",
                "Baining Guo"
            ],
            "title": "Cswin transformer: A general vision transformer backbone with cross-shaped windows",
            "year": 2022
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "In ICLR,",
            "year": 2021
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "year": 2016
        },
        {
            "authors": [
                "Jia-Bin Huang",
                "Abhishek Singh",
                "Narendra Ahuja"
            ],
            "title": "Single image super-resolution from transformed self-exemplars",
            "venue": "In CVPR,",
            "year": 2015
        },
        {
            "authors": [
                "Jiwon Kim",
                "Jung Kwon Lee",
                "Kyoung Mu Lee"
            ],
            "title": "Accurate image super-resolution using very deep convolutional networks",
            "year": 2016
        },
        {
            "authors": [
                "Diederik Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "In ICLR,",
            "year": 2015
        },
        {
            "authors": [
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Honglak Lee",
                "Geoffrey Hinton"
            ],
            "title": "Similarity of neural network representations revisited",
            "venue": "In NeurIPS,",
            "year": 2019
        },
        {
            "authors": [
                "Yawei Li",
                "Yuchen Fan",
                "Xiaoyu Xiang",
                "Denis Demandolx",
                "Rakesh Ranjan",
                "Radu Timofte",
                "Luc Van Gool"
            ],
            "title": "Efficient and explicit modelling of image hierarchies for image restoration",
            "year": 2023
        },
        {
            "authors": [
                "Zhen Li",
                "Jinglei Yang",
                "Zheng Liu",
                "Xiaomin Yang",
                "Gwanggil Jeon",
                "Wei Wu"
            ],
            "title": "Feedback network for image super-resolution",
            "year": 2019
        },
        {
            "authors": [
                "Jingyun Liang",
                "Jiezhang Cao",
                "Guolei Sun",
                "Kai Zhang",
                "Luc Van Gool",
                "Radu Timofte"
            ],
            "title": "Swinir: Image restoration using swin transformer",
            "venue": "ICCVW,",
            "year": 2021
        },
        {
            "authors": [
                "Bee Lim",
                "Sanghyun Son",
                "Heewon Kim",
                "Seungjun Nah",
                "Kyoung Mu Lee"
            ],
            "title": "Enhanced deep residual networks for single image super-resolution",
            "venue": "CVPRW,",
            "year": 2017
        },
        {
            "authors": [
                "Jie Liu",
                "Wenjie Zhang",
                "Yuting Tang",
                "Jie Tang",
                "Gangshan Wu"
            ],
            "title": "Residual feature aggregation network for image super-resolution",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Ze Liu",
                "Yutong Lin",
                "Yue Cao",
                "Han Hu",
                "Yixuan Wei",
                "Zheng Zhang",
                "Stephen Lin",
                "Baining Guo"
            ],
            "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Salma Abdel Magid",
                "Yulun Zhang",
                "Donglai Wei",
                "Won-Dong Jang",
                "Zudi Lin",
                "Yun Fu",
                "Hanspeter Pfister"
            ],
            "title": "Dynamic high-pass filtering and multi-spectral attention for image super-resolution",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "David Martin",
                "Charless Fowlkes",
                "Doron Tal",
                "Jitendra Malik"
            ],
            "title": "A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics",
            "venue": "In ICCV,",
            "year": 2001
        },
        {
            "authors": [
                "Yusuke Matsui",
                "Kota Ito",
                "Yuji Aramaki",
                "Azuma Fujimoto",
                "Toru Ogawa",
                "Toshihiko Yamasaki",
                "Kiyoharu Aizawa"
            ],
            "title": "Sketch-based manga retrieval using manga109 dataset",
            "venue": "Multimedia Tools and Applications,",
            "year": 2017
        },
        {
            "authors": [
                "Yiqun Mei",
                "Yuchen Fan",
                "Yuqian Zhou",
                "Lichao Huang",
                "Thomas S Huang",
                "Humphrey Shi"
            ],
            "title": "Image super-resolution with cross-scale non-local attention and exhaustive self-exemplars mining",
            "year": 2020
        },
        {
            "authors": [
                "Yiqun Mei",
                "Yuchen Fan",
                "Yuqian Zhou"
            ],
            "title": "Image super-resolution with non-local sparse attention",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Ben Niu",
                "Weilei Wen",
                "Wenqi Ren",
                "Xiangde Zhang",
                "Lianping Yang",
                "Shuzhen Wang",
                "Kaihao Zhang",
                "Xiaochun Cao",
                "Haifeng Shen"
            ],
            "title": "Single image super-resolution via a holistic attention",
            "year": 2020
        },
        {
            "authors": [
                "Adam Paszke",
                "Sam Gross",
                "Francisco Massa",
                "Adam Lerer",
                "James Bradbury",
                "Gregory Chanan",
                "Trevor Killeen",
                "Zeming Lin",
                "Natalia Gimelshein",
                "Luca Antiga"
            ],
            "title": "Pytorch: An imperative style, highperformance deep learning library",
            "venue": "NeurIPS,",
            "year": 2019
        },
        {
            "authors": [
                "Maithra Raghu",
                "Thomas Unterthiner",
                "Simon Kornblith",
                "Chiyuan Zhang",
                "Alexey Dosovitskiy"
            ],
            "title": "Do vision transformers see like convolutional neural networks",
            "venue": "NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Wenzhe Shi",
                "Jose Caballero",
                "Ferenc Husz\u00e1r",
                "Johannes Totz",
                "Andrew P Aitken",
                "Rob Bishop",
                "Daniel Rueckert",
                "Zehan Wang"
            ],
            "title": "Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network",
            "year": 2016
        },
        {
            "authors": [
                "Radu Timofte",
                "Eirikur Agustsson",
                "Luc Van Gool",
                "Ming-Hsuan Yang",
                "Lei Zhang",
                "Bee Lim",
                "Sanghyun Son",
                "Heewon Kim",
                "Seungjun Nah",
                "Kyoung Mu Lee"
            ],
            "title": "Ntire 2017 challenge on single image super-resolution: Methods and results",
            "venue": "CVPRW,",
            "year": 2017
        },
        {
            "authors": [
                "Fu-Jen Tsai",
                "Yan-Tsung Peng",
                "Yen-Yu Lin",
                "Chung-Chi Tsai",
                "Chia-Wen Lin"
            ],
            "title": "Stripformer: Strip transformer for fast image deblurring",
            "venue": "In ECCV,",
            "year": 2022
        },
        {
            "authors": [
                "Zhengzhong Tu",
                "Hossein Talebi",
                "Han Zhang",
                "Feng Yang",
                "Peyman Milanfar",
                "Alan Bovik",
                "Yinxiao Li"
            ],
            "title": "Maxvit: Multi-axis vision transformer",
            "year": 2022
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "NeurIPS,",
            "year": 2017
        },
        {
            "authors": [
                "Wenhai Wang",
                "Enze Xie",
                "Xiang Li",
                "Deng-Ping Fan",
                "Kaitao Song",
                "Ding Liang",
                "Tong Lu",
                "Ping Luo",
                "Ling Shao"
            ],
            "title": "Pyramid vision transformer: A versatile backbone for dense prediction without convolutions",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Zhendong Wang",
                "Xiaodong Cun",
                "Jianmin Bao",
                "Wengang Zhou",
                "Jianzhuang Liu",
                "Houqiang Li"
            ],
            "title": "Uformer: A general u-shaped transformer for image restoration",
            "year": 2022
        },
        {
            "authors": [
                "Zhou Wang",
                "Alan C Bovik",
                "Hamid R Sheikh",
                "Eero P Simoncelli"
            ],
            "title": "Image quality assessment: from error visibility to structural similarity",
            "year": 2004
        },
        {
            "authors": [
                "Enze Xie",
                "Wenhai Wang",
                "Zhiding Yu",
                "Anima Anandkumar",
                "Jose M Alvarez",
                "Ping Luo"
            ],
            "title": "Segformer: Simple and efficient design for semantic segmentation with transformers",
            "venue": "NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Rui Yang",
                "Hailong Ma",
                "Jie Wu",
                "Yansong Tang",
                "Xuefeng Xiao",
                "Min Zheng",
                "Xiu Li"
            ],
            "title": "Scalablevit: Rethinking the context-oriented generalization of vision transformer",
            "year": 2022
        },
        {
            "authors": [
                "Syed Waqas Zamir",
                "Aditya Arora",
                "Salman Khan",
                "Munawar Hayat",
                "Fahad Shahbaz Khan",
                "MingHsuan Yang"
            ],
            "title": "Restormer: Efficient transformer for high-resolution image restoration",
            "year": 2022
        },
        {
            "authors": [
                "Roman Zeyde",
                "Michael Elad",
                "Matan Protter"
            ],
            "title": "On single image scale-up using sparserepresentations",
            "venue": "In Proc. 7th Int. Conf. Curves Surf.,",
            "year": 2010
        },
        {
            "authors": [
                "Xindong Zhang",
                "Hui Zeng",
                "Shi Guo",
                "Lei Zhang"
            ],
            "title": "Efficient long-range attention network for image super-resolution",
            "venue": "In ECCV,",
            "year": 2022
        },
        {
            "authors": [
                "Yulun Zhang",
                "Kunpeng Li",
                "Kai Li",
                "Lichen Wang",
                "Bineng Zhong",
                "Yun Fu"
            ],
            "title": "Image superresolution using very deep residual channel attention networks",
            "venue": "In ECCV,",
            "year": 2018
        },
        {
            "authors": [
                "Yulun Zhang",
                "Yapeng Tian",
                "Yu Kong",
                "Bineng Zhong",
                "Yun Fu"
            ],
            "title": "Residual dense network for image super-resolution",
            "venue": "In CVPR,",
            "year": 2018
        },
        {
            "authors": [
                "Yulun Zhang",
                "Kunpeng Li",
                "Kai Li",
                "Bineng Zhong",
                "Yun Fu"
            ],
            "title": "Residual non-local attention networks for image restoration",
            "venue": "In ICLR,",
            "year": 2019
        },
        {
            "authors": [
                "Yulun Zhang",
                "Donglai Wei",
                "Can Qin",
                "Huan Wang",
                "Hanspeter Pfister",
                "Yun Fu"
            ],
            "title": "Context reasoning attention network for image super-resolution",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Shangchen Zhou",
                "Jiawei Zhang",
                "Wangmeng Zuo",
                "Chen Change Loy"
            ],
            "title": "Cross-scale internal graph neural network for image super-resolution",
            "venue": "In NeurIPS,",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Image super-resolution (SR) aims to recover a high-resolution (HR) images from its low-resolution (LR) counterpart. Image SR is an ill-posed problem, as there are multiple solutions that can map to any given LR input. To tackle this challenging inverse problem, researchers have proposed numerous deep convolutional neural networks (CNNs) (Dong et al., 2014; Lim et al., 2017; Zhang et al., 2018b; Mei et al., 2020) in the past few years. Thanks to their impressive performance against conventional approaches, CNNs have almost dominated the field of SR.\nHowever, deep CNNs still suffer from a limitation in global context awareness, due to the local processing principle of the convolution operator. Recently, an alternative method, Transformer, has exhibited considerable performance compared with CNN-based methods on multiple high-level computer vision tasks (Dosovitskiy et al., 2021; Liu et al., 2021; Wang et al., 2021; Chu et al., 2021; Yang et al., 2022). Transformer is first developed in the natural language processing (NLP) field, and its core component is the self-attention (SA) mechanism. This mechanism can directly model longrange dependencies by capturing the interaction between all input data. However, the computational complexity of the vanilla self-attention grows quadratically with image size, limiting its application in high-resolution scenarios, especially low-level vision tasks (e.g., image SR).\nTo apply Transformer in image SR, several methods have been proposed to reduce the computational cost of self-attention. Some researchers apply local (window) self-attention, which divides the feature maps into sub-regions to limit the scope of self-attention. Meanwhile, they utilize the shift mechanism (Liang et al., 2021), overlapping windows (Chen et al., 2022b), or the cross-aggregation operation (Chen et al., 2022c), to enhance the interaction between windows. These methods achieve linear complexity with respect to image size and outperform previous CNN-based methods. However, compared with global attention, the local design needs to stack many blocks to establish global\n\u2217Corresponding authors: Yulun Zhang, yulun100@gmail.com; Linghe Kong, linghe.kong@sjtu.edu.cn\nar X\niv :2\n30 3.\n06 37\n3v 3\n[ cs\n.C V\n] 2\n9 Se\np 20\n23\ndependencies. Furthermore, some methods propose \u201ctransposed\u201d self-attention (Zamir et al., 2022) that operates across the channel dimension instead of the spatial dimension. Although this method can implicitly capture global information, it hinders modeling spatial dependencies, which is crucial to image SR. Therefore, there is a need to develop a method for image SR that can effectively capture global spatial information with low computational cost on high-resolution images.\nIn this paper, we propose the Recursive Generalization Transformer (RGT) for image SR, which can model global spatial information and is suitable for high-resolution images. Specifically, we propose the recursive-generalization self-attention (RG-SA) to explore global information directly in linear computational complexity. The RG-SA first generalizes the input features of arbitrary resolution into representative feature maps with a small, constant size, via the recursive generalization module (RGM). Intuitively, the global information is aggregated into representative maps. Then cross-attention is utilized between input features and representative maps to exchange global information. Since the size of representative maps is much smaller than input features, the whole process is at a low computational cost. Moreover, the RG-SA further adjusts the channel dimension of query, key, and value matrices in SA to mitigate the redundancy in the channel domain.\nFurthermore, considering that the RG-SA aggregates the image features via the RGM, it is inevitable to lose some local details. Thus, we combine the RG-SA with the local self-attention (L-SA) in an alternate arrangement to better utilize the global context. To enhance the integration of two different SA modules, we propose the hybrid adaptive integration (HAI), which acts on the outside of each Transformer block. The HAI directly fuses features at different levels (local or global) before and after the block. Besides, HAI adaptively adjusts the input features through a learnable adaptor for feature alignment. Overall, equipped with the above designs, our RGT can capture global information for accurate image SR while the complexity is manageable.\nOur contributions can be summarized as follows: \u2022 We propose the Recursive Generalization Transformer (RGT) for image SR. The RGT is\ncapable of capturing global spatial information and is suitable for high-resolution images. Our RGT obtains notable SR performance quantitatively and visually.\n\u2022 We propose the recursive-generalization self-attention (RG-SA), utilizing the recursive aggregation module and cross-attention to model global dependency with linear complexity.\n\u2022 We further combine RG-SA with local self-attention to better exploit the global context, and propose the hybrid adaptive integration (HAI) for module integration."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Image Super-Resolution. In recent years, CNN-based methods have shown superior performance over conventional SR approaches. SRCNN (Dong et al., 2014) is the pioneering work, that introduces three convolutional layers for image SR. Following this attempt, many works deepen the architecture to improve performance (Zhang et al., 2019; Magid et al., 2021; Dai et al., 2019). VDSR (Kim et al., 2016) introduces residual learning to build a network with 20 layers. EDSR (Lim et al., 2017) further simplifies residual block, which allows deeper networks. RCAN (Zhang et al., 2018a) proposes a residual-in-residual structure to train a model over 400 layers. Moreover, numerous spatial and channel attention mechanisms (Zhang et al., 2019; Liu et al., 2020; Zhou et al., 2020) are proposed to improve the reconstruction quality. For instance, HAN (Niu et al., 2020) proposes a layer attention module and channel-spatial attention. Although these CNN-based methods produce remarkable results, they still suffer from a limitation in global modeling capability.\nVision Transformer. Transformer is proposed in natural language processing (NLP) and has been adapted to multiple high-level vision tasks, such as image classification (Dosovitskiy et al., 2021; Liu et al., 2021; Dong et al., 2022), semantic segmentation (Xie et al., 2021), and object detection (Yang et al., 2022; Tu et al., 2022). Due to the impressive performance in high-level tasks, Transformer has also been introduced to low-level vision tasks (Liang et al., 2021; Zamir et al., 2022; Wang et al., 2022; Tsai et al., 2022; Chen et al., 2022c; 2023; Li et al., 2023), including image SR. SwinIR (Liang et al., 2021), following the design of Swin Transformer (Liu et al., 2021), utilizes local window self-attention and shift mechanism. ELAN (Zhang et al., 2022) proposes multi-scale self-attention to reduce the computational cost. CAT (Chen et al., 2022c) designs the rectangle-window self-attention to aggregate the features across different windows. These methods all reduce computational complexity by applying self-attention within local regions. However, the local designs restrict the exploitation of global information that is crucial to image SR."
        },
        {
            "heading": "LN RG-SA",
            "text": "Global Attention. To reduce the computational complexity of vanilla self-attention, in addition to local design, several works attempt to propose global attention with low overheads (Wang et al., 2021; Tu et al., 2022; Yang et al., 2022; Chen et al., 2022a; Ali et al., 2021). PVT (Wang et al., 2021) designs a spatial-reduction module to merge tokens of key and value. MaxViT (Tu et al., 2022) proposes the grid attention to gain sparse global attention. ScalableViT (Yang et al., 2022) scales attention matrices from both spatial and channel dimensions. Although these methods reduce the computational complexity to a certain extent, the theoretical complexity remains quadratic, which hinders their effective application on high-resolution images. RegionViT Chen et al. (2022a) captures global information among regional tokens to alleviate the overhead of global attention. Moreover, XCiT (Ali et al., 2021) proposes a \u201ctransposed\u201d version of self-attention that operates across channels dimension rather than the spatial dimension to achieve linear complexity. However, it cannot explicitly model the spatial relationship. In contrast, we propose the recursive-generalization self-attention (RG-SA), which can explore global spatial information in linear complexity."
        },
        {
            "heading": "3 METHOD",
            "text": "We propose the Recursive Generalization Transformer (RGT) for image SR, which is capable of capturing global context and handling high-resolution images effectively. In this section, we first introduce the architecture of RGT. Then, we focus on our proposed recursive-generalization selfattention (RG-SA) and hybrid adaptive integration (HAI)."
        },
        {
            "heading": "3.1 OVERALL ARCHITECTURE",
            "text": "The overall architecture of our proposed RGT is illustrated in Fig. 1, consisting of three modules: shallow feature extraction, deep feature extraction, and image reconstruction. Given a low-resolution (LR) image ILR\u2208RH\u00d7W\u00d73, RGT first leverages a convolutional layer as the shallow feature extraction to get the low-level feature F0\u2208RH\u00d7W\u00d7C , where H , W , and C represent the image height, width, and channel number. F0 is used for the deep feature extraction module, which is composed of N1 residual groups (RGs) and a convolutional layer. Through this module, the F0 is transformed into the deep feature Fd\u2208RH\u00d7W\u00d7C . Finally, the F0 and Fd are fused through a residual connection, and processed by the reconstruction module to generate the high-resolution (HR) image IHR\u2208RH\u0302\u00d7W\u0302\u00d73, where H\u0302 and W\u0302 are the output height and width. The reconstruction module consists of pixel-shuffle (Shi et al., 2016) and convolutional layers.\nEach RG contains N2 Transformer blocks and a convolutional layer. And the residual connection is employed to ensure training stability. As shown in Fig. 1, there are two types of Transformer blocks: Local Self-Attention (L-SA) blocks and RG-SA blocks. The two types of blocks are arranged alternately to organize the topological structure. Each Transformer block is composed of two layer normalization (LN), self-attention, and multilayer perceptron (MLP) (Vaswani et al., 2017). Meanwhile, the HAI acts outside each Transformer block with a learnable adaptor \u03b1. In this work, we apply the recently proposed rectangle-window self-attention (Rwin-SA) (Chen et al., 2022c) as local self-attention by default. Next, we pay more attention to our proposed RG-SA and HAI."
        },
        {
            "heading": "3.2 RECURSIVE-GENERALIZATION SELF-ATTENTION",
            "text": "The vanilla self-attention (SA) mechanism establishes connections between all input tokens. Although it can capture global context, the SA suffers the quadratic computational complexity with image size, limiting its the application of SA in high-resolution scenarios, including image SR. To tackle this problem, we propose the recursive-generalization self-attention (RG-SA), shown in Fig. 2, that can maintain linear computational complexity while capturing global information.\nThe RG-SA first aggregates input image features of arbitrary resolution into compressed maps (denoted as representative feature maps), through the recursive generalization module (RGM). Intuitively, the representative maps aggregate the information of the whole image features, providing a global view of the image. Then, the cross-attention is calculated between input features and representative maps. Therefore, each token in the input image features can obtain a global receptive field. Meanwhile, we further scale the channel dimension of query, key, and value matrices in attention to mitigate the channel redundancy. It improves the performance and reduces consumption.\nRecursive Generalization Module. For simplicity, our RGM only consists of depth-wise and pixelwise convolutions, shown in Fig. 2. Given an input Xin\u2208RH\u00d7W\u00d7C , we first compress spatial size of the features by recursively reusing a single depth-wise convolution T=\u230alogsr H h \u230b times to obtain the rough aggregation maps X\u0302\u2208Rh\u00d7w\u00d7C , where sr is the convolution stride size, h is a constant, and w=W\u00d7 hH . Without loss of generality, we assume W\u2264H , then w \u2264h. Next, we refine the rough aggregation maps to generate the representative maps Xr\u2208Rh\u00d7w\u00d7Cr , through a 3\u00d73 depth-wise convolution and a 1\u00d71 point-wise convolution. The RGM is formulated as:\nX\u0302 = WKr (Xin) = Wr(Wr(. . . (Wr(Xin)))),\nXr = WpWd(X\u0302), (1)\nwhere Wr is the depth-wise convolution with sr stride, Wd is the 3\u00d73 depth-wise convolution, and Wp is the 1\u00d71 point-wise convolution. Also, the 1\u00d71 point-wise convolution scales the channels from C to Cr=C\u00d7cr, where cr is the adjustment factor. Through RGM, we can aggregate the global information of the input image features. Meanwhile, the recursive design is flexible for processing inputs with varying sizes (common in image SR) by dynamically choosing the recursion times T .\nCross-Attention. Subsequently, we reshape and project the input features Xin as the Q\u2208RHW\u00d7Cr (query) and the representative maps as the K\u2208Rhw\u00d7Cr (key), and V\u2208Rhw\u00d7C (value) to compute the cross-attention. The attention matrix A\u2208RHW\u00d7hw is calculated from the dot-product interaction of query and key. Note that we further scale the channel dimensions of query, key, and value. Overall, the whole cross-attention process is defined as:\nQ = WQXin,K = WKXr,V = WV Xr, A = SoftMax(QKT / \u221a\nCr), Cross-Attention(Xin,Xr) = Wm(A \u00b7V), (2)\nwhere WQ\u2208RC\u00d7Cr , WK\u2208RCr\u00d7Cr , and WV \u2208RCr\u00d7C are learnable parameters and biases are omitted for simplification; Wm\u2208RC\u00d7C is the projection matrix for feature fusion. Similar to vanilla self-attention (Vaswani et al., 2017; Dosovitskiy et al., 2021), we divide the channels into\nmultiple \u201cheads\u201d and execute the attention operation in parallel. Finally, we reshape the result of cross-attention to obtain the output features Xout\u2208RH\u00d7W\u00d7C . Through RGM, cross-attention, and channel adjustment, our RG-SA can capture global spatial information while maintaining low computational overheads. Next, we analyze the complexity of the RG-SA in detail.\nComplexity Analysis. Our RG-SA can be divided into two components: RGM and cross-attention. For RGM, the computational complexity is O(HWC). For cross-attention, the computational complexity is O(hwHW (C+Cr)+HWC(C+Cr)+hwCr(C+Cr)). Here, we analyze the single-head self-attention for simplicity. Since h and w are constants, and Cr=C\u00d7cr, the complexity of crossattention is O(HWC2). In general, The total computational complexity of our RG-SA is linear with the input features size (H\u00d7W ). Additionally, by applying the small (<1) adjustment factor cr, we can mitigate the channel redundancy, thus further reducing complexity."
        },
        {
            "heading": "3.3 HYBRID ADAPTIVE INTEGRATION",
            "text": "Alternate Arrangement. In RG-SA, the global information is captured through the cross-attention between input features and representative maps, ensuring low computational overheads. However, the RGM in RG-SA is a coarse-grained design, which leads to losing some local details and ultimately limits modeling global information. To improve the exploitation of the global context, we introduce local self-attention (L-SA) and combine it with our proposed RG-SA. Two attention modules are alternately arranged in each residual group (RG), as illustrated in Fig. 1.\nAnalysis and Motivation. Although the two blocks are integrated, the linear arrangement lacks direct interaction between features at different levels (global or local), thus still cannot exploit global information effectively. For further analysis, under the alternating topology, the input and output of each Transformer block are different level features. Specifically, the input of the RG-SA block is the local features generated from L-SA, while the output is the global features. Correspondingly, the input and output of L-SA are global and local features, respectively. This observation inspires us to enhance information fusion by combining the input and output features of each block.\nSpecific Design. The intuitive idea is directly integrating the input and output features via the vanilla skip connection (He et al., 2016). Nevertheless, since the misalignment between global and local features, simple addition cannot fuse features effectively. To overcome the above issues, we propose hybrid adaptive integration (HAI). As shown in Fig. 1, our HAI acts on the outside of each Transformer block. The input features are adaptively adjusted by a learnable adaptor \u03b1, and added to the output features. The process of the lth Transformer block Bl equipped with HAI is: Zl = Bl(Zl\u22121) + \u03b1l \u00b7 Zl\u22121, (3) where Zl\u22121 and Zl represent the input and the output of the lth Transformer block, \u03b1l\u2208RC is the learnable parameter in the lth block. Overall, the HAI is able to enhance the integration of different SA modules on the basis of the alternate arrangement, which advances the modeling of global information. Moreover, similar to the regular residual connection, our HIA encourages more information flows to the deep network layers, resulting in better performance.\nVisual Results. To intuitively show the effectiveness of HAI, we visualize the relevant features of one RG-SA block (first row, 2nd block in 1st RG) and one L-SA block (second row, 5td block in 1st RG) of RGT in Fig. 3. The deeper color indicates larger weights. First, by observing columns (a) and (b), we can find great differences in the input and output of SA modules in some cases. It reveals the misalignment between global and local features. Secondly, directly fused by the vanilla skip connection, the features are\nover-integrated (top of (c) column), or not changed obviously (bottom of (c) column). In contrast, through HAI, the features shown in column (d) are adaptively changed according to different blocks. It indicates that the input and output features are fused effectively. More discussions on the role of HAI and the value distribution of learnable adaptors \u03b1 are given in Sec. 4.2."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "4.1 EXPERIMENTAL SETTINGS",
            "text": "Data and Evaluation. Following recent works (Zhang et al., 2021; Magid et al., 2021; Liang et al., 2021), we choose DIV2K (Timofte et al., 2017) and Flickr2K (Lim et al., 2017) as the training data. For testing, we use five standard benchmark datasets: Set5 (Bevilacqua et al., 2012), Set14 (Zeyde et al., 2010), B100 (Martin et al., 2001), Urban100 (Huang et al., 2015), and Manga109 (Matsui et al., 2017). We conduct experiments with three upscaling factors: \u00d72, \u00d73, and \u00d74. The lowresolution images are generated by Bicubic (BI) downsampling. To evaluate our method, we use the metrics PSNR and SSIM (Wang et al., 2004) on the Y channel of the YCbCr space.\nImplementation Details. We set two version models, RGT-S and RGT, with different computational complexity. For RGT-S, we set the residual group (RG) number N1 as 6 and the Transformer block number N2 for each RG as 6 for L-SA. The channel dimension number, attention head number, and mlp expansion ratio (Dosovitskiy et al., 2021) are set as 180, 6, and 2, respectively. The window size is set as 8\u00d732. The sr (stride size) and cr (adjustment factor) are set as 4 and 0.5 for RG-SA. The representative map size h is set as 4 for training, and 16 for testing. For RGT, we increase the number of RG from 6 to 8, while other settings remain the same as RGT-S.\nTraining Settings. We train our models with batch size 32, where each input image is randomly cropped to 64\u00d764 size, and the total training iterations are 500K. Training patches are augmented using random horizontal flips and rotations with 90\u25e6, 180\u25e6, and 270\u25e6. To keep fair comparisons, we adopt Adam optimizer (Kingma & Ba, 2015) with \u03b21=0.9 and \u03b22=0.99 to minimize the L1 loss function following previous works (Zhang et al., 2018a; Dai et al., 2019; Liang et al., 2021). The initial learning rate is set as 2\u00d710\u22124 and reduced by half at the milestone [250K,400K,450K,475K]. We use PyTorch (Paszke et al., 2019) to implement our models with 4 Nvidia A100 GPUs."
        },
        {
            "heading": "4.2 ABLATION STUDY",
            "text": "In this section, we study the effects of different components of our method. For fair comparisons, all models adopt the same basic architecture and settings as RGT-S. We conduct experiments on the \u00d72 factor. We adopt the dataset DIV2K (Timofte et al., 2017) and Flickr2K (Lim et al., 2017) to train models, and the iterations are 200K. The dataset Urban100 (Huang et al., 2015) is applied for testing. When we calculate the FLOPs, the input size is 3\u00d7128\u00d7128. Effects of each component. We conduct a break-down ablation experiment to investigate the effects of each component on SR performance. The results are listed in Tab. 1a. First, baseline. The baseline model is derived by replacing all Transformer blocks in RGT-S with local self-attention (LSA) (Liang et al., 2021; Chen et al., 2022b) block and removing HAI. We set the window size of LSA as 8\u00d732, which is consistent with RGT-S. Second, applying RG-SA. We introduce the recursivegeneralization self-attention (RG-SA) into the baseline, and alternately arrange L-SA and RG-SA in successive Transformer blocks. Without changing the structure of the network, the model achieves a 0.09 dB improvement. Meanwhile, compared with the baseline, the FLOPs and parameters of the model are slightly reduced. It shows that our proposed RG-SA is effective regarding parameters and computational complexity. Third, applying HAI. We further adopt the hybrid adaptive integration (HAI) and get the final version of our RGT-S. The model obtains the best performance of 33.68 dB. These results demonstrate the effectiveness of the RG-SA and HAI.\nEffects of RG-SA. We investigate the design of our recursive-generalization self-attention (RG-SA). We implement ablation experiments on the recursive operation and channel adjustment factor. The results are reported in Tab. 1b. First, the impact of recursive operation. We build the model without recursion (w/o Recur) by removing the recursive operation of RGM in RGT-S. Namely, the stride depth-wise convolution is only utilized once to generate the representative maps. We keep sr=4 and cr=0.5 unchanged. Compared with RGT-S, it can be observed that the usage of recursive operation can effectively reduce the FLOPs by 30%, while achieving better performance. Second, the impact of the channel adjustment factor cr. We build and train the model without channel scaling (w/o Scale) by setting the adjustment factor cr in RGT-S as 1 (0.5 by default). Compared with RGT-S, we discover that scaling the channel dimension yields 0.14 dB performance gain, since the smaller cr mitigates the redundancy between channels, thus enhancing the feature expression.\nEffects of HAI. We show the influence of the hybrid adaptive integration (HAI) in Tab. 1c. We compared three models: without HAI (w/o HAI), with vanilla skip connection (He et al., 2016) (w/ Skip), and with HAI (w/ HAI). Both skip connection and HAI act on the outside of each Transformer block. First, the impact of vanilla skip connection. Comparing the model w/o HAI and the model w/ Skip, we can find that the simple application of skip connection seriously degrades the model performance by 0.81 dB. This may be due to the misalignment between different level (global or local) features, which prevents their direct fusion. Second, the impact of HAI. In contrast, our proposed HAI adaptively adjusts the input features through a learnable adaptor \u03b1, thus achieving valid feature integration. With the HAI, the model obtains 0.16 dB gain.\nWe also apply HAI only models only with L-SA in Tab. 1d. Since there is no feature misalignment, the performance does not change much with HAI. These results are consistent with analysis in Sec. 3.3 and demonstrates the effectiveness of HAI in solving feature misalignment.\nFurthermore, we further introduce centered kernel alignment (CKA) (Cortes et al., 2012; Kornblith et al., 2019; Raghu et al., 2021) to study the internal representation structure of the model. The higher the CKA score, the higher the representation similarity. We calculate CKA similarities between all Transformer blocks in RGT-S without and with HAI. The results are shown as heatmaps in Fig. 4. First, enhance module integration. There are obvious differences before and after the 30th block, in the model without HAI. On the contrary, equipping with HAI, it can be found that the transition is more gradual, which indicates that the integration between the modules is more effective. Second, encourage information flow. We observe that there is still a high similarity between the initial blocks (0th~2nd) and the very deep blocks (28th~29th) in the model with HAI. It means that through HAI, more information can flow to the deep layers of the network.\nWe also visualize the value distribution of \u03b1 of RGT-S in Fig. 5. We find that \u03b1 is diverse based on different blocks, indicating its adaptability. Meanwhile, the value of \u03b1 increases at the end block in each RG, i.e., blocks 5th, 11th, 17th, and 23th. It indicates that HAI integrates different modules."
        },
        {
            "heading": "4.3 COMPARISONS WITH STATE-OF-THE-ART METHODS",
            "text": "We compare our two models, RGT-S and RGT, with recent state-of-the-art methods: EDSR (Lim et al., 2017), RCAN (Zhang et al., 2018a), SRFBN (Li et al., 2019), SAN (Dai et al., 2019), HAN (Niu et al., 2020), CSNLN (Mei et al., 2020), NLSA (Mei et al., 2021), CRAN (Zhang et al., 2021), DFSA (Magid et al., 2021), ELAN (Zhang et al., 2022), SwinIR (Liang et al., 2021), and CAT-A (Chen et al., 2022c). Similar to previous works (Lim et al., 2017; Zhang et al., 2018a), we use self-ensemble strategy in testing and mark the model with the symbol \u201c+\u201d.\nQuantitative results. We show the quantitative comparisons for \u00d72, \u00d73, and \u00d74 image SR in Tab. 2. We also report the comparisons of computational complexity (e.g., FLOPs), and parameter numbers in Tab. 3. As we can see, our proposed RGT significantly outperforms other methods on all datasets with all scaling factors. Compared with recent Transformer-based methods, such as SwinIR (Liang et al., 2021) and CAT-A (Chen et al., 2022c), our proposed RGT achieves better results, particularly on the Urban100 and Manga109 datasets. For instance, on the Urban100 dataset (\u00d72), RGT outperforms CAT-A by 0.21 dB. Meanwhile, the model size and computational complexity are lower than CAT-A. Even the small vision model, RGT-S, obtains comparable or better results than compared methods. These comparisons indicate that our proposed RGT can capture more global information compared with previous CNN-based and Transformer-based methods.\nVisual Results. We show visual comparisons (\u00d74) in Fig. 6. We can observe that most compared methods suffer from blurring artifacts and cannot recover accurate textures in some representative challenging cases. In contrast, our RGT can alleviate the blurring artifacts better and recover more image details. For instance, in image img_059, some methods fail to reconstruct most of the strips correctly (e.g., SAN and DFSA), while some only restore part stripes (e.g., SwinIR and CAT-A). In contrast, our method recovers more precise structures. These visual comparisons demonstrate that our RGT is capable of reconstructing high-quality images by modeling global information. Combining with the quantitative comparisons, we further demonstrate the effectiveness of our method."
        },
        {
            "heading": "4.4 MODEL SIZE ANALYSES",
            "text": "We further show the comparison of parameter numbers, FLOPs, and performance with recent image SR methods in Tab. 3. FLOPs are measured when the output size is set as 3\u00d7512\u00d7512, and PSNR values are tested on Urban100 and Manga109 (\u00d74). Our RGT has lower computational complexity and model size than CNN-based methods, EDSR (Lim et al., 2017) and RCAN (Zhang et al., 2018a). Compared with CSNLN (Mei et al., 2020), our RGT only requires 0.3% computational complexity (i.e., FLOPs). Meanwhile, compared with the recent Transformer-based model, CAT-A, our RGT performs better, while FLOPs decreased by 30.39% (109.6G) and parameters decreased by 19.46% (3.23M). Compared with SwinIR (Liang et al., 2021), RGT has comparable computational complexity and model size. Furthermore, to further demonstrate the effectiveness of our method, we provide another version of the model, RGT-S, with lower FLOPs and parameters than SwinIR. Our RGTS still obtains notable SR performance gains compared with other methods. These comparisons indicate that our method achieves a better trade-off between model complexity and performance."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "We propose a new Transformer model, named Recursive Generalization Transformer (RGT), for accurate image SR. Our RGT is capable of modeling global spatial information while maintaining low computational costs. Specifically, we design the recursive-generalization self-attention (RGSA) to extract global information effectively in linear complexity. The RG-SA computes crossattention between the input features and the representative maps recursively aggregated from the input. Meanwhile, the channel dimensions of attention matrices are further scaled to mitigate the redundancy in the channel domain. Furthermore, to improve the exploitation of the global context, we combine RG-SA with local self-attention, and propose the hybrid adaptive integration (HAI) for module integration. The HAI acts on the outside of each Transformer block to directly fuse features at different levels (local or global). Extensive experiments on image SR demonstrate that our proposed RGT achieves superior performance over recent state-of-the-art methods."
        }
    ],
    "year": 2023
}