{
    "abstractText": "Diagnostic results can be radically influenced by the quality of 2D ovarian-tumor ultrasound images. However, clinically processed 2D ovarian-tumor ultrasound images contain many artificially recognized symbols, such as fingers, crosses, dashed lines, and letters which assist artificial intelligence (AI) in image recognition. These symbols are widely distributed within the lesion\u2019s boundary, which can also affect the useful feature-extraction-utilizing networks and thus decrease the accuracy of lesion classification and segmentation. Image inpainting techniques are used for noise and object elimination from images. To solve this problem, we observed the MMOTU dataset and built a 2D ovarian-tumor ultrasound image inpainting dataset by finely annotating the various symbols in the images. A novel framework called mask-guided generative adversarial network (MGGAN) is presented in this paper for 2D ovarian-tumor ultrasound images to remove various symbols from the images. The MGGAN performs to a high standard in corrupted regions by using an attention mechanism in the generator to pay more attention to valid information and ignore symbol information, making lesion boundaries more realistic. Moreover, fast Fourier convolutions (FFCs) and residual networks are used to increase the global field of perception; thus, our model can be applied to high-resolution ultrasound images. The greatest benefit of this algorithm is that it achieves pixel-level inpainting of distorted regions without clean images. Compared with other models, our model achieveed better results with only one stage in terms of objective and subjective evaluations. Our model obtained the best results for 256 \u00d7 256 and 512 \u00d7 512 resolutions. At a resolution of 256 \u00d7 256, our model achieved 0.9246 for SSIM, 22.66 for FID, and 0.07806 for LPIPS. At a resolution of 512 \u00d7 512, our model achieved 0.9208 for SSIM, 25.52 for FID, and 0.08300 for LPIPS. Our method can considerably improve the accuracy of computerized ovarian tumor diagnosis. The segmentation accuracy was improved from 71.51% to 76.06% for the Unet model and from 61.13% to 66.65% for the PSPnet model in clean images.",
    "authors": [
        {
            "affiliations": [],
            "name": "Paolo Zaffino"
        },
        {
            "affiliations": [],
            "name": "Maria Francesca Spadea"
        },
        {
            "affiliations": [],
            "name": "Lijiang Chen"
        },
        {
            "affiliations": [],
            "name": "Changkun Qiao"
        },
        {
            "affiliations": [],
            "name": "Meijing Wu"
        },
        {
            "affiliations": [],
            "name": "Linghan Cai"
        },
        {
            "affiliations": [],
            "name": "Cong Yin"
        },
        {
            "affiliations": [],
            "name": "Mukun Yang"
        },
        {
            "affiliations": [],
            "name": "Xiubo Sang"
        },
        {
            "affiliations": [],
            "name": "Wenpei Bai"
        }
    ],
    "id": "SP:3b2a83f24a0d5d956ed007d5555c3948e9f24075",
    "references": [
        {
            "authors": [
                "W. Al-Dhabyani",
                "M. Gomaa",
                "H. Khaled",
                "A. Fahmy"
            ],
            "title": "Dataset of breast ultrasound images",
            "venue": "Data Brief 2020,",
            "year": 2020
        },
        {
            "authors": [
                "M. George",
                "H. Anita"
            ],
            "title": "Analysis of kidney ultrasound images using deep learning and machine learning techniques: A review",
            "venue": "Pervasive Comput. Soc. Netw. 2022,",
            "year": 2022
        },
        {
            "authors": [
                "S. Sava\u015f",
                "N. Topalo\u011flu",
                "\u00d6. Kazc\u0131",
                "P.N. Ko\u015far"
            ],
            "title": "Classification of carotid artery intima media thickness ultrasound images with deep learning",
            "venue": "J. Med. Syst",
            "year": 2019
        },
        {
            "authors": [
                "H. Li",
                "J. Weng",
                "Y. Shi",
                "W. Gu",
                "Y. Mao",
                "Y. Wang",
                "W. Liu",
                "J. Zhang"
            ],
            "title": "An improved deep learning approach for detection of thyroid papillary cancer in ultrasound",
            "year": 2018
        },
        {
            "authors": [
                "D. Karimi",
                "Q. Zeng",
                "P. Mathur",
                "A. Avinash",
                "S. Mahdavi",
                "I. Spadinger",
                "P. Abolmaesumi",
                "S.E. Salcudean"
            ],
            "title": "Accurate and robust deep learning-based segmentation of the prostate clinical target volume in ultrasound images",
            "venue": "Med Image Anal",
            "year": 2019
        },
        {
            "authors": [
                "Q. Zhao",
                "Y. Ma",
                "S. Lyu",
                "L. Chen"
            ],
            "title": "Embedded self-distillation in compact multibranch ensemble network for remote sensing scene classification",
            "venue": "IEEE Trans. Geosci. Remote Sens",
            "year": 2021
        },
        {
            "authors": [
                "Q. Zhao",
                "S. Lyu",
                "Y. Li",
                "Y. Ma",
                "L. Chen"
            ],
            "title": "MGML: Multigranularity multilevel feature ensemble network for remote sensing scene classification",
            "venue": "IEEE Trans. Neural Netw. Learn. Syst",
            "year": 2021
        },
        {
            "authors": [
                "L. Cai",
                "M. Wu",
                "L. Chen",
                "W. Bai",
                "M. Yang",
                "S. Lyu",
                "Q. Zhao"
            ],
            "title": "Using Guided Self-Attention with Local Information for Polyp Segmentation",
            "venue": "In Proceedings of the International Conference on Medical Image Computing and Computer Assisted Intervention\u2013MICCAI 2022: 25th International Conference, Singapore,",
            "year": 2022
        },
        {
            "authors": [
                "A. Singh",
                "S. Sengupta",
                "V. Lakshminarayanan"
            ],
            "title": "Explainable deep learning models in medical image analysis",
            "venue": "J. Imaging 2020,",
            "year": 2020
        },
        {
            "authors": [
                "V. Chouhan",
                "S.K. Singh",
                "A. Khamparia",
                "D. Gupta",
                "P. Tiwari",
                "C. Moreira",
                "R. Dama\u0161evi\u010dius",
                "V.H.C. De Albuquerque"
            ],
            "title": "A novel transfer learning based approach for pneumonia detection in chest",
            "venue": "X-ray images. Appl. Sci",
            "year": 2020
        },
        {
            "authors": [
                "M. Loey",
                "F.M. Smarandache",
                "N.E. Khalifa"
            ],
            "title": "Within the lack of chest COVID-19 X-ray dataset: A novel detection model based on GAN and deep transfer learning",
            "venue": "Symmetry",
            "year": 2020
        },
        {
            "authors": [
                "M. Frid-Adar",
                "I. Diamant",
                "E. Klang",
                "M. Amitai",
                "J. Goldberger",
                "H. Greenspan"
            ],
            "title": "GAN-based synthetic medical image augmentation for increased CNN performance in liver lesion classification",
            "venue": "Neurocomputing",
            "year": 2018
        },
        {
            "authors": [
                "A. Zhao",
                "G. Balakrishnan",
                "F. Durand",
                "J.V. Guttag",
                "A.V. Dalca"
            ],
            "title": "Data augmentation using learned transformations for one-shot medical image segmentation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Q. Zhao",
                "S. Lyu",
                "W. Bai",
                "L. Cai",
                "B. Liu",
                "M. Wu",
                "X. Sang",
                "M. Yang",
                "L. Chen"
            ],
            "title": "A Multi-Modality Ovarian Tumor Ultrasound Image Dataset for Unsupervised Cross-Domain Semantic Segmentation",
            "venue": "arXiv 2022,",
            "year": 2022
        },
        {
            "authors": [
                "S. Yao",
                "J. Yan",
                "M. Wu",
                "X. Yang",
                "W. Zhang",
                "H. Lu",
                "B. Qian"
            ],
            "title": "Texture synthesis based thyroid nodule detection from medical ultrasound images: Interpreting and suppressing the adversarial effect of in-place manual annotation",
            "venue": "Front. Bioeng. Biotechnol",
            "year": 2020
        },
        {
            "authors": [
                "K. Armanious",
                "Y. Mecky",
                "S. Gatidis",
                "B. Yang"
            ],
            "title": "Adversarial inpainting of medical image modalities",
            "venue": "In Proceedings of the ICASSP 2019\u20132019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Brighton, UK,",
            "year": 2019
        },
        {
            "authors": [
                "E. Xie",
                "P. Ni",
                "R. Zhang",
                "X. Li"
            ],
            "title": "Limited-Angle CT Reconstruction with Generative Adversarial Network Sinogram Inpainting and Unsupervised Artifact Removal",
            "venue": "Appl. Sci. 2022,",
            "year": 2022
        },
        {
            "authors": [
                "H.J. Kwon",
                "S.H. Lee"
            ],
            "title": "A Two-Step Learning Model for the Diagnosis of Coronavirus Disease-19 Based on Chest X-ray Images with 3D Rotational Augmentation",
            "venue": "Appl. Sci. 2022,",
            "year": 2022
        },
        {
            "authors": [
                "O. Ronneberger",
                "P. Fischer",
                "T. Brox"
            ],
            "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
            "venue": "In Proceedings of the Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI 2015: 18th International Conference,",
            "year": 2015
        },
        {
            "authors": [
                "M. Alsalamah",
                "S. Amin"
            ],
            "title": "Medical image inpainting with RBF interpolation technique",
            "venue": "Int. J. Adv. Comput. Sci. Appl. 2016,",
            "year": 2016
        },
        {
            "authors": [
                "N. Guizard",
                "K. Nakamura",
                "P. Coup\u00e9",
                "V.S. Fonov",
                "D.L. Arnold",
                "D.L. Collins"
            ],
            "title": "Non-local means inpainting of MS lesions in longitudinal image processing",
            "venue": "Front. Neurosci",
            "year": 2015
        },
        {
            "authors": [
                "P. Vlanek"
            ],
            "title": "Fuzzy image inpainting aimed to medical imagesl",
            "venue": "In Proceedings of the International Conference in Central Europe on Computer Graphics, Visualization and Computer Vision, Prague, Czech Republic,",
            "year": 2018
        },
        {
            "authors": [
                "M. Arnold",
                "A. Ghosh",
                "S. Ameling",
                "G. Lacey"
            ],
            "title": "Automatic segmentation and inpainting of specular highlights for endoscopic imaging",
            "venue": "EURASIP J. Image Video Process",
            "year": 2010
        },
        {
            "authors": [
                "K. Nazeri",
                "E. Ng",
                "T. Joseph",
                "F. Qureshi",
                "M. Ebrahimi"
            ],
            "title": "Edgeconnect: Structure guided image inpainting using edge prediction",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops, Seoul, Republic of Korea,",
            "year": 2019
        },
        {
            "authors": [
                "Q. Wang",
                "Y. Chen",
                "N. Zhang",
                "Y. Gu"
            ],
            "title": "Medical image inpainting with edge and structure priors",
            "venue": "Measurement",
            "year": 2021
        },
        {
            "authors": [
                "Y. Wei",
                "Z. Gan",
                "W. Li",
                "S. Lyu",
                "M.C. Chang",
                "L. Zhang",
                "J. Gao",
                "P. Zhang"
            ],
            "title": "MagGAN: High-Resolution Face Attribute Editing with Mask-Guided Generative Adversarial Network",
            "venue": "In Proceedings of the Asian Conference on Computer Vision (ACCV), Kyoto, Japan,",
            "year": 2020
        },
        {
            "authors": [
                "Y. Zeng",
                "J. Fu",
                "H. Chao",
                "B. Guo"
            ],
            "title": "Learning Pyramid-Context Encoder Network for High-Quality Image Inpainting",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "year": 2019
        },
        {
            "authors": [
                "Z. Yi",
                "Q. Tang",
                "S. Azizi",
                "D. Jang",
                "Z. Xu"
            ],
            "title": "Contextual Residual Aggregation for Ultra High-Resolution Image Inpainting",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2020
        },
        {
            "authors": [
                "Z. Yan",
                "X. Li",
                "M. Li",
                "W. Zuo",
                "S. Shan"
            ],
            "title": "Shift-net: Image inpainting via deep feature rearrangement",
            "venue": "In Proceedings of the European Conference on Computer Vision (ECCV), Munich, Germany,",
            "year": 2018
        },
        {
            "authors": [
                "C. Wu",
                "Y. Wang",
                "F. Wang"
            ],
            "title": "Deep learning for ovarian tumor classification with ultrasound images",
            "venue": "In Proceedings of the Pacific Rim Conference on Multimedia, Hefei, China,",
            "year": 2018
        },
        {
            "authors": [
                "F. Christiansen",
                "E. Epstein",
                "E. Smedberg",
                "M. \u00c5kerlund",
                "K. Smith"
            ],
            "title": "Ultrasound image analysis using deep neural networks for discriminating between benign and malignant ovarian tumors: Comparison with expert subjective assessment",
            "venue": "Ultrasound Obstet. Gynecol",
            "year": 2021
        },
        {
            "authors": [
                "Z. Zhang",
                "Y. Han"
            ],
            "title": "Detection of ovarian tumors in obstetric ultrasound imaging using logistic regression classifier with an advanced machine learning approach",
            "venue": "IEEE Access 2020,",
            "year": 2020
        },
        {
            "authors": [
                "J. Jin",
                "H. Zhu",
                "J. Zhang",
                "Y. Ai",
                "Y. Teng",
                "C. Xie",
                "X. Jin"
            ],
            "title": "Multiple U-Net-based automatic segmentations and radiomics feature stability on ultrasound images for patients with ovarian cancer",
            "venue": "Front. Oncol",
            "year": 2021
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Ballari, India,",
            "year": 2016
        },
        {
            "authors": [
                "L. Chi",
                "B. Jiang",
                "Y. Mu"
            ],
            "title": "Fast fourier convolution",
            "venue": "Adv. Neural Inf. Process. Syst. 2020,",
            "year": 2020
        },
        {
            "authors": [
                "D. Pathak",
                "P. Krahenbuhl",
                "J. Donahue",
                "T. Darrell",
                "A.A. Efros"
            ],
            "title": "Context encoders: Feature learning by inpainting",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, NV,",
            "year": 2016
        },
        {
            "authors": [
                "J. Yu",
                "Z. Lin",
                "J. Yang",
                "X. Shen",
                "X. Lu",
                "T.S. Huang"
            ],
            "title": "Generative image inpainting with contextual attention",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Salt Lake City, UT,",
            "year": 2018
        },
        {
            "authors": [
                "S. Iizuka",
                "E. Simo-Serra",
                "H. Ishikawa"
            ],
            "title": "Globally and locally consistent image completion",
            "venue": "ACM Trans. Graph. (ToG)",
            "year": 2017
        },
        {
            "authors": [
                "G. Liu",
                "F.A. Reda",
                "K.J. Shih",
                "T.C. Wang",
                "A. Tao",
                "B. Catanzaro"
            ],
            "title": "Image inpainting for irregular holes using partial convolutions",
            "venue": "In Proceedings of the European Conference on Computer Vision (ECCV), Munich, Germany,",
            "year": 2018
        },
        {
            "authors": [
                "J. Yu",
                "Z. Lin",
                "J. Yang",
                "X. Shen",
                "X. Lu",
                "T.S. Huang"
            ],
            "title": "Free-form image inpainting with gated convolution",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision, Seoul, Republic of Korea,",
            "year": 2019
        },
        {
            "authors": [
                "J. Johnson",
                "A. Alahi",
                "L. Fei-Fei"
            ],
            "title": "Perceptual losses for real-time style transfer and super-resolution",
            "venue": "In Proceedings of the European Conference on Computer Vision, Amsterdam, The Netherlands,",
            "year": 2016
        },
        {
            "authors": [
                "R. Suvorov",
                "E. Logacheva",
                "A. Mashikhin",
                "A. Remizova",
                "A. Ashukha",
                "A. Silvestrov",
                "N. Kong",
                "H. Goka",
                "K. Park",
                "V. Lempitsky"
            ],
            "title": "Resolution-robust large mask inpainting with fourier convolutions",
            "venue": "In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer",
            "year": 2022
        },
        {
            "authors": [
                "T.C. Wang",
                "M.Y. Liu",
                "J.Y. Zhu",
                "A. Tao",
                "J. Kautz",
                "B. Catanzaro"
            ],
            "title": "High-resolution image synthesis and semantic manipulation with conditional gans",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Salt Lake City, UT, USA,",
            "year": 2018
        },
        {
            "authors": [
                "Z. Wang",
                "A.C. Bovik",
                "H.R. Sheikh",
                "E.P. Simoncelli"
            ],
            "title": "Image quality assessment: From error visibility to structural similarity",
            "venue": "IEEE Trans. Image Process",
            "year": 2004
        },
        {
            "authors": [
                "M. Heusel",
                "H. Ramsauer",
                "T. Unterthiner",
                "B. Nessler",
                "S. Hochreiter"
            ],
            "title": "Gans trained by a two time-scale update rule converge to a local nash equilibrium",
            "venue": "Adv. Neural Inf. Process. Syst. 2017,",
            "year": 2017
        },
        {
            "authors": [
                "R. Zhang",
                "P. Isola",
                "A.A. Efros",
                "E. Shechtman",
                "O. Wang"
            ],
            "title": "The unreasonable effectiveness of deep features as a perceptual metric",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Salt Lake City, UT,",
            "year": 2018
        },
        {
            "authors": [
                "H. Zhao",
                "J. Shi",
                "X. Qi",
                "X. Wang",
                "J. Jia"
            ],
            "title": "Pyramid Scene Parsing Network",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Honolulu, HI, USA,",
            "year": 2017
        }
    ],
    "sections": [
        {
            "text": "Citation: Chen, L.; Qiao, C.; Wu, M.;\nCai, L.; Yin, C.; Yang, M.; Sang, X.;\nBai, W. Improving the Segmentation\nAccuracy of Ovarian-Tumor\nUltrasound Images Using Image\nInpainting. Bioengineering 2023, 10,\n184. https://doi.org/10.3390/\nbioengineering10020184\nAcademic Editors: Paolo Zaffino,\nMaria Francesca Spadea and\nLarbi Boubchir\nReceived: 18 November 2022\nRevised: 5 January 2023\nAccepted: 28 January 2023\nPublished: 1 February 2023\nCopyright: \u00a9 2023 by the authors.\nLicensee MDPI, Basel, Switzerland.\nThis article is an open access article\ndistributed under the terms and\nconditions of the Creative Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n4.0/).\nKeywords: ovarian tumor; 2D ultrasound image; image inpainting; lesion segmentation; attention mechanism; GAN; deep learning; medical image analysis"
        },
        {
            "heading": "1. Introduction",
            "text": "Medical ultrasonography has turned out to be the preferred imaging technique for many illnesses due to the fact of its simplicity, speed, and safety [1\u20135]. Two-dimensional gray-scale ultrasound and coloration Doppler ultrasound has been broadly used in the diagnostic tasks of ovarian tumors. Doctors can first perceive the benign and malignant nature of tumors. With the non-stop development and improvement of deep learning [6,7], AI, as a riding pressure for intelligent healthcare, has acquired a massive range of achievements in tasks such as clinical image classification and segmentation [8\u201311]. The accuracy\nBioengineering 2023, 10, 184. https://doi.org/10.3390/bioengineering10020184 https://www.mdpi.com/journal/bioengineering\nof the model additionally relies upon the quality of the dataset [12,13]. There is exceedingly little research on the current use of AI for lesion recognition and segmentation of ovarian tumor diseases. In addition, the effectiveness of AI in processing ovarian-tumor images depends on a large-scale AI dataset. Zhao et al. [14] proposed an ovarian-tumor ultrasound image dataset for lesion classification and segmentation. The dataset consists of a complete of 1469 2D ovarian ultrasound images which are divided into eight categories according to tumor types. The giant majority of the images in the dataset contain annotated symbols, which are overwhelmingly allotted to inside the lesion. Nevertheless, hidden but crucial trouble has been recognized in practice: most 2D ovarian-tumor ultrasound images incorporate extra symbols. Actually, in clinical operations where ovarian ultrasound images are acquired, the physician will mark the location, size, and border of the tumor in the ovarian ultrasound image, and observe where the lesion is positioned (left or right ovary). Due to equipment factors and the clinical practice environments, the artificially marked component of these aids to image recognition (symbols such as fingers, crosses, dashes, and letters) cannot be separated from the original image. This phenomenon is also widespread in different medical fields [15\u201318]. The ideal situation would be to train and test deep learning models using clean images without any symbols in lesion areas. We observe that these symbols are centered in ovarian tumor lesions, which negatively affects the training of the model to a positive extent, causing the network to focus more on the symbols in the lesions, which in turn reduces the recognition accuracy of ovarian tumors in the clean images and the segmentation accuracy of the lesions. The different types of images in this paper are shown in Figure 1. The original images with symbols were used as the training set, and two different test sets of clean images and original images with symbols were used as a way to discover the impact of symbols on the segmentation accuracy of the model. Figures 2 and A1 exhibit the effects of our experiments. Fewer training epochs are required to segment more accurate lesion regions in images with symbols, and the segmented regions targeted the yellow line roughly. The clean images, on the other hand, required more epochs and reached lower segmentation accuracy. The results show that the symbols in the images provide additional information to the model enhancing the accuracy of segmentation, which is unrealistic in clinical practice. There is little research on this issue, and it is certainly inappropriate to use the marked ovarian-tumor ultrasound images directly to train the segmentation model. Thus, it is critical for the corrupted areas of the images to be painted, so it is significant for healthcare professionals to use clean images for the artificial intelligence-aided diagnosis of ovarian tumors.\nCurrently, image inpainting in medical images is in the process of booming and has a lot of potential for development. Existing methods are primarily divided into traditional methods and deep learning-based methods. Traditional methods make use of patch-based or diffusion-based methods, the core of which is to use the redundancy of the image itself to fill in the missing areas with low-level texture features of the image. The following four methods are historically used for inpainting: interpolation [20], non-local means [21], diffusion techniques [22], and texture-dependent synthesis [23]. However, ordinary methods cannot learn the deep semantic features of medical images frequently and can not attain excellent results. Deep-learning-based methods use convolutional neural networks to extract and learn high-level semantic features in the image to guide the model to fill the missing parts. Inspired by EdgeConnect [24], Wang et al. [25] migrated the method using edge information to medical images. This paper details the study of these methods and use of an attention mechanism, a pyramid-structured generator, to enforce the inpainting of thyroid ultrasound images, which automatically detects and reconstructs the cross symbols in ultrasound images. However, this method has some limitations: the cross symbols in the thyroid ultrasound images used in this approach are small and few, and the effect is negative for ultrasound images containing many large symbols; the detected cross symbols are labeled with rectangular boxes, and this approach does not apply to different symbols with irregular shapes; the real background is covered by these symbols, and the restoration areas have no real background, so how to guide the generative adversarial network for training and evaluation, in this case, is a very necessary issue. Wei et al. [26] proposed the MagGAN for face-attribute editing. The MagGAN does this by introducing a novel mask-guided adjustment strategy to encourage the affected regions of each target attribute to be positioned in the generator, using the corresponding attributes of the face (eyes, nose, mouth, etc.). The method is applied to the face-attribute editing task, which requires segmentation of the face\u2019s attributes, which is different from our task. However, the motivation of making the results more realistic by bootstrapping the model is similar.\nIn addition, various attention mechanisms have been proposed and are broadly used in image processing. These attention mechanisms have been steadily utilized in the image inpainting task. Zeng et al. [27] expanded on this by proposing a pyramidal structure for contextual attention. Yi et al. [28] proposed a contextual residual aggregation of attention for high-resolution images. The spatial attention mechanism was utilized to solve this problem. To acquire results with a clear structure and texture, the Shift-Net model proposed by Yan et al. [29] replaced the fully detailed layer in the upsampling process with a shiftconnected layer, through which the features in the background region are shifted to fill in the holes. Due to the above issues, in this paper, a one-stage generation model based on GANs is proposed, which swaps the regular convolution with fast Fourier convolutions to enhance the image-wide acceptance field of the model and includes a channel attention mechanism to minimize the model\u2019s focus on symbols to fill the holes using effective features. To the best of our knowledge, we are the first to accomplish image inpainting on 2D ovariantumor ultrasound images with large and irregular masks, and our approach achieves more convincing results than others.\nOur contributions are as follows:\n\u2022 We refined 1469 2D ovarian-tumor ultrasound images for irregular symbols and obtained binary masks to establish a 2D ovarian-tumor ultrasound image inpainting dataset. \u2022 We introduced fast Fourier convolution to enhance the model\u2019s global perceptual field and a channel attention mechanism to enhance the model\u2019s attention to significant features, and the model uses global features and significant channel features to fill the holes. \u2022 Our model achieved better results both subjectively and objectively compared to existing models while for the first time performing image inpainting without clean images. \u2022 We use the restoration images for segmentation training, which significantly enhances the accuracy of the classification and segmentation of clean images.\nThe rest of the paper is organized as follows: Section 2 describes our dataset and model in detail. The associated experiments and results are detailed in Section 3. The conclusions are introduced in Section 4."
        },
        {
            "heading": "2. Methodology",
            "text": ""
        },
        {
            "heading": "2.1. Dataset",
            "text": "In recent years, research about ovarian tumors has increased, and researchers have combined ovarian tumor sonograms with deep learning for ovarian tumor classification and lesion segmentation [30\u201333]. Most of the 2D ovarian-tumor ultrasound images used in these studies contain symbols, which are broadly allotted to the edges or inner parts of the lesions. We experimentally confirmed the negative effect of these symbols on the classification accuracy and lesion segmentation accuracy of tumors. The MMOTU dataset [14] is a publicly available ovarian ultrasound image dataset. We obtained a 2D ovarian-tumor ultrasound image inpainting dataset based on the MMOTU dataset by refining annotation processing. As shown in Figure 3, the green dashed line in the figure is how the MMOTU dataset is annotated. We labeled the fingers and letters (brown boxes), numbers (blue boxes), and yellow lines (yellow boxes) in the figure on this basis. With annotation, a corresponding mask for each image is generated, which masks the various symbols in the image. Figure 4 indicates our pipeline. With these annotations, the corresponding mask for each image was generated to build an inpainting dataset containing 1469 2D ultrasound images of ovarian tumors and masks. We performed experiments about image inpainting on our dataset and the effect of image inpainting on lesion segmentation accuracy in the MMOUT dataset."
        },
        {
            "heading": "2.2. Implementation Details",
            "text": "In this study, we used a complete, 2D ovarian-tumor ultrasound dataset with 1469 images that we produced, of which 1200 images were used for training and 269 images were used for testing. Arbitrarily shaped masks were used during training and testing. To make certain the equity of the experiments, we generated unique irregular masks for the images used for testing. The inputs in our experiments had two specifications: one specification was 256\u00d7 256 (h \u00d7 w), and the other specification was 512\u00d7 512 (h \u00d7 w). We trained and tested our model with both image specifications. The Adam optimizer was chosen to optimize the network. We set the initial learning rate to 0.0001, the batch size for training to 16, and the epoch to 1000. In addition to generating masks using our proposed mask generation strategy, we also performed data enhancement operations on the\nimages during training. The framework was PyTorch, and the devices were two NVIDIA GeForce RTX3090Ti.\n2.3. Proposed Methods 2.3.1. Network Architecture\nWe propose an image inpainting model based on fast Fourier convolutions (FFCs) with a channel attention mechanism. Figure 5 indicates the details of our model. The images are downsampled by three convolutional layers and then encoded with the aid of nine fast Fourier Convolution Residual Network Blocks. The decoder obtains the inpainting image by predicting the output of the encoder. These inpainting and original images are fed into the discriminator for adversarial training. Traditional fully convolutional models, such as ResNet [34], suffer from slow perceptual-field growth due to a small convolutional kernel size and limited receptive fields. Due to this reason, many layers in the network lack global context, such that the result has a lack of global structural consistency. We replaced the regular convolution with the fast Fourier convolution to solve this problem. In addition, due to the presence of symbols such as yellow dashed lines in the images, we added a channel attention layer to our model to permit the model to focus more on useful features and make the results more realistic. Figure 6 suggests the specified architecture of the Fast Fourier Convolution Block.\n2.3.2. Fast Fourier Convolution Block\nRegular convolution is mostly used in deep learning models; however, it cannot capture the global features. Fast Fourier convolutions [35] can be an appropriate solution to this problem. The FFCs divide the input channel into local and global paths: the local path uses regular convolution to capture local information; the global path uses the real fast Fourier transform to obtain information with a global receptive field. The fast Fourier change consists of the following five steps:\n\u2022 Transforming the input tensor to the frequency domain using the real fast Fourier transform: RH\u00d7W\u00d7C \u2192 CH\u00d7W2 \u00d7C. \u2022 Concatenating the real and imaginary parts in the frequency domain: CH\u00d7W2 \u00d7C \u2192 RH\u00d7W2 \u00d72C. \u2022 Obtaining convolution results in the frequency domain through the ReLU layer, Batch-\nNorm layer, and 1 \u00d7 1 convolution layer: RHand2\u00d72C \u2192 RH\u00d7W2 \u00d72C.\n\u2022 Separating the result of frequency domain convolution into real and imaginary parts: RH\u00d7W2 \u00d72C \u2192 CH\u00d7W2 \u00d7C. \u2022 Recovering its spatial structure using Fourier inverse transform: CH\u00d7W2 \u00d7C \u2192 RH\u00d7W\u00d7C.\nAs shown in Figure 6, we add a squeeze-and-excitation (SE) layer after the spectral transform block, which performs the squeeze, excitation, and reweight operations in turn. The SE layer automatically acquires each feature channel\u2019s weight via learning, then boosts the beneficial features and suppresses the ones that are no longer beneficial according to the weight. By using the SE layer, we make the model focus more on the useful features rather than on the features of these symbols in the image. Finally, the output of the local and global paths are merged.\n2.3.3. Generation of masks during training\nThe approach of mask generation during training has been extensively mentioned in previous research, and it is crucial for the inpainting effect of the model. In early studies, the generated masks are rectangular in shape [36], centered on the geometric center of the image. Models trained with these masks have bad results for images with non-centered rectangular masks. Therefore, the method of generating masks at random locations [37] in the image during training was proposed, but this method fails to provide effective and realistic inpainting of images with irregular masks. Subsequently, the strategy of randomly generating irregular masks [38\u201340] at random locations in the image has emerged. There are many symbols in the image that obscure the clean image. If these areas are repaired, the results cannot be evaluated realistically due to the fact there is no clean image. We need to guide the network to learn to use features of the non-symbolic regions to fill holes. In our task, we propose a new mask generation strategy by generating random irregular masks at random locations outside the symbolic regions in the image. The generation formula for the masks is as follows:\nm = mgen \u2212mprior (1)\nwhere mprior is the mask corresponding to the image in the dataset, mgen is the mask generated by the mask generator, and m is the final mask."
        },
        {
            "heading": "2.4. Loss Function",
            "text": "The loss function in the generation task is essential for the training of the model, and it calculates the distinction between the ground truth and the inpainting image as the loss value. The loss values are back-propagated, and the model is penalized to update the parameters of each layer. In the end, the loss value is reduced, and the result is closer to the ground truth. Several extraordinary loss functions were used in our task. In our model, the input uses the corrupted image Iin = Iori (1\u2212m), where Iori denotes the original images and m denotes the corresponding mask, for which one denotes the missing pixels and zero denotes the existing pixels. The symbol denotes the multiplication of the matrix. G denotes the generator, Iinp denotes the final inpainting image generated by the model, and the expressions for the inputs and outputs are shown in Equation (2).\nIinp = G(Iin) (2)\nThe perceptual loss [41] is derived by calculating the distance between features captured by the pre-trained network \u03a8(.) from the generated images and the original images. To enable the network to understand global contextual information, we compute high receptive field perceptual loss [42] using a pre-trained ResNet with global receptive fields. The calculation of LResNet can be expressed as follows:\nLResNet(Iori, Iinp) =M ([ \u03a8ResNet(Iori)\u2212\u03a8ResNet(Iinp) ]2) (3)\nwhere Iori is the original image or the target image of the generator, Iinp is the generated image, and M is the operation of calculating the inter-layer mean after calculating the intra-layer mean. \u03a8ResNet(.) is a pre-trained ResNet implemented with dilated convolution. To make the generated inline images more realistic and natural in detail, we additionally use adversarial loss. The adversarial loss function Ladv is calculated as follows:\nLadv ( Iinp, Iori, Iin ) = max\nD Ex\u2208X [log(D(Iori, Iin)) + log(1\u2212 D(Iinp, Iin))] (4)\nwhere Iori is the target image, Iinp is the inapinting image, Iin is the corrupted image, and D is the adversarial discriminator. In our total loss, we also use the L1 loss and the perceptual loss of the discriminator network LDisc [43]. The formula for the perceptual loss of the discriminator network LDisc is similar to Equation (2). The L1 loss is calculated as follows:\nL1 = 1 N \u2211 |Iori(p)\u2212 Iinp(p)| (5)\nwhere Iori denotes the original image, Iinp denotes the inpainting image, and p represents the pixel at the same location in both images.\nOur total losses are calculated as follows:\nLtotal = \u03b71L1 + \u03b72Ladv + \u03b73LResNet + \u03b74LDisc (6)\nwhere \u03b7 is the weight of each loss function. Following [36,39,42], we set \u03b71 = 10, \u03b72 = 10, \u03b73 = 30, and \u03b74 = 100 in training."
        },
        {
            "heading": "2.5. Evaluation Criterion",
            "text": "We used the evaluation metrics of structural similarity (SSIM) [44], Frechet inception distance score (FID) [45], and learned perceptual image patch similarity (LPIPS) [46] to measure the performance of our model. In addition, we used the mean intersection over union (mIoU) evaluation metric to measure the accuracy of lesion segmentation results.\nThe SSIM is calculated between two windows of size H \u00d7 W. The value of SSIM is between \u22121 and 1, where 1 means the two images are identical and \u22121 means the opposite. The closer the value of SSIM is to one, the better the inpainting effect is. The SSIM calculation formula is defined as follows:\nSSIM = (2\u00b5A\u00b5B + c1)(2\u03c3AB + c2)\n(\u00b5A2 + \u00b5B2 + c1)(\u03c3A2 + \u03c3B2 + c2) (7)\nwhere \u00b5A and \u03c3A2 are the mean and variance of image A, \u00b5B and \u03c3B2 are the mean and variance of image B, \u03c3AB is the covariance of the two images, and c1 and c2 are the constants that maintain stability. The Frechet inception distance score (FID) is a metric to calculate the distance between the real image and the generated image feature vectors. It uses the 2048-dimensional vector of Inception Net-V3 before full concatenation as the feature of the image to evaluate the similarity of the two sets of images. The value of FID is greater than or equal to zero. A lower score means that the two sets of images are more similar, and the FID score in the best case is 0.0, which means that the two sets of images are identical. The FID calculation formula is described as follows:\nFID = \u2225\u2225\u2225\u00b5gt \u2212 \u00b5pred\u2225\u2225\u22252 + Tr(\u03a3gt + \u03a3pred \u2212 2(\u03a3gt\u03a3pred)1/2) (8)\nwhere \u00b5gt and \u03a3gt are the mean and covariance matrices of the real image features, \u00b5pred and \u03a3pred are the mean and covariance matrices of the generated image features, and Tr is the operation to calculate the matrix trace. LPIPS is used to measure the difference between two images in terms of deep-level features, and LPIPS is more consistent with human perception than traditional methods such as `2, PSNR, and FSIM. The value of LPIPS is greater than or equal to zero. A lower value of LPIPS indicates that the two images are more similar, and vice versa. The LPIPS calculation formula is defined as follows:\nd ( Igt, Ipred ) = \u2211\nl\n1 HlWl \u2211 h,w \u2225\u2225\u2225wl (y\u0302lgt\u2212hw \u2212 y\u0302lpred\u2212hw)\u2225\u2225\u222522 (9) where l is the current computed layer; Hl and Wl are the sizes of the patches; and y\u0302lgt\u2212hw and y\u0302lpred\u2212hw \u2208 R\nHl\u00d7Wl\u00d7Cl are the outputs of the current layer. The feature stack is extracted from the L layers and unit-normalized in the channel dimension. The vector wl is used to deflate the number of active channels and calculate the `2 distance. MIoU is a widely used standard metric in semantic segmentation, which calculates the mean of the ratio of intersection and merges sets of all categories. The value is between zero and one. Closer to one means better the segmentation, and closer to zero is the opposite. Its calculation formula is defined as follows:\nMIoU = 1\nk + 1\nk\n\u2211 i=0\nTP FN + FP + TP\n(10)\nwhere k is the number of categories, TP is the number of true positive pixels, FP is the number of false positives, and FN is the number of false negatives."
        },
        {
            "heading": "3. Experiments and Results",
            "text": ""
        },
        {
            "heading": "3.1. Results",
            "text": "3.1.1. Experiments on the Image Inpainting\nFigure 7 indicates the effects of our model on the restoration of the symbolic regions in the ovarian ultrasound images. The boundary, texture, and structure have high similarity to those in the original image. The results show that we have flawlessly removed the symbols from the images. Especially in the lesion area, we removed the yellow line while\nreconstructing the boundary of the lesion and the content filling of the yellow line area very well. This proves the power of our model. Furthermore, we compare our approach with robust baselines that are publicly available on FID, LPIPS, and SSIM metrics. We performed statistical analysis of the inpainting results on 269 images of the test set.\nTable 1 suggests the overall performance of each baseline on our dataset, and the values of the three metrics in the table are the means of the test samples. Smaller FID and LPIPS indicate better performance of the model, and a larger SSIM indicates better performance of the model. Table 2 presents the overall performance of each baseline on our dataset, and the values of the three metrics in the table are the variance of the test samples. The size of the input images in the experiment was 256 \u00d7 256. In the statistical analysis, we observed that our model outperformed all other comparable models in SSIM, FID, and LPIPS metrics. Our model achieved 0.9246 for SSIM, 22.66 for FID, and 0.07806 for LPIPS. Table 3 suggests that the upper and lower limits of our method surpass those of the other methods for all three metrics at a confidence level of 95%. Figure 8 indicates the inpainting results for different models (we show more results in Appendix A). A clear distinction can be found in the blue box area. These baseline models use the learned symbol features to generate the symbol regions, resulting in yellow pixels in the restoration regions. In addition, the regions they generate show significant distortions and folds, with unsatisfactory textures and structures. We address this problem by using an attention mechanism for the model to focus on the features of the fee-symbolic region in the image. Fast Fourier convolution allows the first few layers of the network to quickly increase the receptive field, which allows the model to gain a global receptive field faster and increase the connection between global and local features. The model can better use the global and local features to fill the holes, and the results of the restoration will have the same structural and textural features as the original image, including smoother boundaries and more realistic content. By introducing the channel attention mechanism, our model pays more attention to the features of non-symbolic regions rather than the features of symbolic regions and chooses useful features for image inpainting. Thereby, the restored image is closer to the original image in terms of content, and no yellow pixels appear in the restoration region. In the qualitative comparison, our model showed the best authenticity and details in the results, including smooth edges and high similarity to the original images. Our method better reconstructed the edge structure and content of the lesion in the image, which dramatically improved lesion segmentation accuracy."
        },
        {
            "heading": "GL [38] 0.3026 170.69 0.29589",
            "text": ""
        },
        {
            "heading": "PC [39] 0.6847 79.42 0.13550",
            "text": "3.1.2. Ablation Experiments\nTo verify that our approaches do reduce the capabilities of the model, we designed ablation experiments for the baseline model. The dataset used for the experiments was our inpainting dataset. We used solely FFCs as the baseline in this experiment.\n\u2022 FFCs Fast Fourier convolutions have a larger and more effective field of repetition, which can effectively enhance the field of repetition of our model and improve its capability. We performed quantitative experiments on fast Fourier convolution, dilated convolution, and regular convolution. The convolution kernel size was set to 3 \u00d7 3, and the expansion rate of the dilated convolution was set to 3. Table 4 shows the scores of different types of convolution. FFC performed the best, and dilated convolution was second only to FFC; however, dilated convolution depends on the resolution of the image and has poor generalization. \u2022 Mask generation The types, sizes, and positions of the mask during training impact the generative and generalization capabilities of the model. In our task, we focused on exploring the effect of mask generation location on the model. Regular, irregularly shaped masks will overlap with a variety of symbols in the image, and this part of the region was devoid of realistic background for a realistic inpainting quality assessment. Additionally, we avoided network learning to use the features of these symbols. We compare our mask generation approach with the conventional method, and Tables 5 and 6 show that our method effectively improves the SSIM, LPIPS, and FID. \u2022 Attention mechanism For the network to attenuate the focus on symbolic features in the image and enhance the focus on other features in the real background, we introduced the SE layer. By introducing the channel attention mechanism, our model pays more attention to the features of non-symbolic regions rather than the features of symbolic regions and chooses useful features. By this method, the restored image is more similar to the original image in terms of content and no yellow pixels show up in the restoration region. Tables 5 and 6 show the effects of the experiments.\n3.1.3. Experiments on the Lesion Segmentation\nAs we noted in the introduction, our aim of inpainting of 2D ovarian-tumor ultrasound images is to enhance the accuracy of currently popular segmentation models such as Unet and PSPnet for the segmentation of ovarian lesions. Figures 2 and A1 show the negative effect of symbols in the image on the segmentation of the lesion: they make the model focus more on these symbols. These symbols provide additional information such that the accuracy of segmentation of ovarian-tumor images that are completely clean and without symbols is substantially reduced, which is unacceptable in clinical practice. Therefore, we used the inpainting images and the original images as two training sets, and the clean images as the common test set for experiments on lesion segmentation. Figures 9 and A5 confirm that the segmentation accuracy was improved from 71.51% to 76.06% for the Unet [19] model and from 61.13% to 66.65% for the PSPnet [47] model in clean images. Figure 10 indicates the segmentation results of the Unet model using the clean images as a testing set. Our approach appreciably improves the accuracy of lesion segmentation, and the visualization of segmentation is much better for experiments on lesion segmentation with clean images. These experiments confirm our conjecture and our original aim of performing image inpainting."
        },
        {
            "heading": "4. Conclusions",
            "text": "In this paper, we proposed a 2D ovarian-tumor ultrasound image inpainting dataset to investigate the effect of prevalent symbols in images on ovarian-lesion segmentation. Based on this image inpainting dataset, we proposed a 2D ovarian-tumor ultrasound image inpainting model based on fast Fourier convolution and a channel attention mechanism. Labeled images are used as a priori information to guide the model to focus on features in the non-symbolic regions of the images, and fast Fourier convolution is used to extend the receptive field of the model to make the texture and structure of the inpainting images more realistic and the boundaries smoother. Our model outperformed existing methods in both qualitative and quantitative comparisons. It received the highest scores in all three metrics, LPIPS, FID, and SSIM, which proves the effectiveness of our model. We used the inpainting images for training and validation with Unet and PSPnet models, which appreciably enhanced the accuracy of lesion segmentation in clean images. This additionally demonstrates the great significance of our study for computer-aided diagnosis of ovarian tumors. Our study in this paper did not currently use ground truth of lesion segmentation in the dataset, which may further improve the similarity of lesion boundaries in inpainted images. In future work, we will do further exploration on how to apply the edge information of the lesion to the model to make the boundaries more similar to those in the original image and extend our model to other types of medical images\u2014CT, MRI, etc.\nAuthor Contributions: Conceptualization, C.Q. and L.C. (Lijiang Chen); methodology, C.Q.; writing\u2014original draft preparation, C.Q.; writing\u2014review and editing, L.C. (Linghan Cai) and W.B.; project administration, L.C. (Lijiang Chen) and W.B.; funding acquisition, W.B. and L.C (Lijiang Chen).; data curation, M.W., C.Y., X.S. and M.Y. All authors have read and agreed to the published version of the manuscript.\nFunding: This research was partly supported by the National Natural Science Foundation of China (Grant No. 62072021), the Fundamental Research Funds for the Central Universities (Grant No. YWF-22-L-532), and the Beijing Hospitals Authority\u2019s Ascent Plan (Grant No. DFL20190701).\nInstitutional Review Board Statement: Not applicable.\nInformed Consent Statement: Not applicable.\nData Availability Statement: The 2D ovarian-tumor ultrasound image inpainting dataset we created by annotation can be accessed at https://github.com/freedom423/2D-ovarian-tumor-ultrasoundimage-inpainting, (accessed on 18 November 2022).\nAcknowledgments: The authors of this paper would like to thank all subjects involved in the building these two datasets used in this study.\nConflicts of Interest: The authors declare no conflict of interest."
        },
        {
            "heading": "Appendix A",
            "text": "Figure A1. The accuracy graph of lesion segmentation of the PSPnet [47] model. The blue line represents the accuracy of using the clean images as the testing set. The yellow line represents the accuracy of using the original images with symbols as the testing set. The figure also shows a visualization of the segmentation results for the 50th and 150th epochs.\nFigures A2\u2013A4 show the results of different methods on images containing different types of symbols.\nFigure A2. Comparison between the results of our proposed model and those of other models. (a) The ground truth image. (b) The mask for the ground truth image. (c) The masked image. (d) LaMa method. (e) GL method. (f) DF 1 method. (g) Our method. (h) PC method. (i) DF 2 method.\nFigure A3. Comparison between the results of our proposed model and those of other models. (a) The ground truth image. (b) The mask for the ground truth image. (c) The masked image. (d) LaMa method. (e) GL method. (f) DF 1 method. (g) Our method. (h) PC method. (i) DF 2 method.\nFigure A4. Comparison between the results of our proposed model and those of other models. (a) The ground truth image. (b) The mask for the ground truth image. (c) The masked image. (d) LaMa method. (e) GL method. (f) DF 1 method. (g) Our method. (h) PC method. (i) DF 2 method.\nFigure A5. The accuracy graph of lesion segmentation of the PSPnet [47] model. The blue line represents the accuracy of using the inpainting images as the training set. The yellow line represents the accuracy of using the original images as the training set."
        }
    ],
    "title": "Improving the Segmentation Accuracy of Ovarian-Tumor Ultrasound Images Using Image Inpainting",
    "year": 2023
}