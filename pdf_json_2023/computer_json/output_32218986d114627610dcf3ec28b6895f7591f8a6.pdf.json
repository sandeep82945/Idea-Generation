{
    "abstractText": "We study online learning in episodic constrained Markov decision processes (CMDPs), where the goal of the learner is to collect as much reward as possible over the episodes, while guaranteeing that some long-term constraints are satisfied during the learning process. Rewards and constraints can be selected either stochastically or adversarially, and the transition function is not known to the learner. While online learning in classical (unconstrained) MDPs has received considerable attention over the last years, the setting of CMDPs is still largely unexplored. This is surprising, since in real-world applications, such as, e.g., autonomous driving, automated bidding, and recommender systems, there are usually additional constraints and specifications that an agent has to obey during the learning process. In this paper, we provide the first best-of-both-worlds algorithm for CMDPs with longterm constraints. Our algorithm is capable of handling settings in which rewards and constraints are selected either stochastically or adversarially, without requiring any knowledge of the underling process. Moreover, our algorithm matches state-of-the-art regret and constraint violation bounds for settings in which constraints are selected stochastically, while it is the first to provide guarantees in the case in which they are chosen adversarially.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jacopo Germano"
        },
        {
            "affiliations": [],
            "name": "Francesco Emanuele Stradi"
        }
    ],
    "id": "SP:b4c2cd2f4a9e1ca6fd2a3aaeba6ba435984eb006",
    "references": [
        {
            "authors": [
                "E. Altman."
            ],
            "title": "Constrained Markov Decision Processes",
            "venue": "Chapman and Hall.",
            "year": 1999
        },
        {
            "authors": [
                "Peter Auer",
                "Thomas Jaksch",
                "Ronald Ortner."
            ],
            "title": "Near-optimal Regret Bounds for Reinforcement Learning",
            "venue": "Advances in Neural Information Processing Systems, D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou (Eds.), Vol. 21. Curran Associates, Inc. https://proceedings.neurips.cc/paper/2008/file/e4a6222cdb5b34375400904f03d8e6a5-Paper.pdf",
            "year": 2008
        },
        {
            "authors": [
                "Mohammad Gheshlaghi Azar",
                "Ian Osband",
                "R\u00e9mi Munos."
            ],
            "title": "Minimax Regret Bounds for Reinforcement Learning",
            "venue": "https://doi.org/10.48550/ARXIV.1703.05449",
            "year": 2017
        },
        {
            "authors": [
                "Qinbo Bai",
                "Vaneet Aggarwal",
                "Ather Gattami."
            ],
            "title": "Provably efficient model-free algorithm for MDPs with peak constraints",
            "venue": "arXiv preprint arXiv:2003.05555 (2020).",
            "year": 2020
        },
        {
            "authors": [
                "Santiago R Balseiro",
                "Yonatan Gur."
            ],
            "title": "Learning in repeated auctions with budgets: Regret minimization and equilibrium",
            "venue": "Management Science 65, 9 (2019), 3952\u20133968.",
            "year": 2019
        },
        {
            "authors": [
                "Matteo Castiglioni",
                "Andrea Celli",
                "Christian Kroer."
            ],
            "title": "Online Learning with Knapsacks: the Best of Both Worlds",
            "venue": "(2022). https://doi.org/10.48550/ARXIV.2202.13710",
            "year": 2022
        },
        {
            "authors": [
                "Matteo Castiglioni",
                "Andrea Celli",
                "Alberto Marchesi",
                "Giulia Romano",
                "Nicola Gatti."
            ],
            "title": "A Unifying Framework for Online Optimization with Long-Term Constraints",
            "venue": "https://doi.org/10.48550/ARXIV.2209.07454",
            "year": 2022
        },
        {
            "authors": [
                "Nicolo Cesa-Bianchi",
                "G\u00e1bor Lugosi."
            ],
            "title": "Prediction, learning, and games",
            "venue": "Cambridge university press.",
            "year": 2006
        },
        {
            "authors": [
                "Yonathan Efroni",
                "Shie Mannor",
                "Matteo Pirotta."
            ],
            "title": "Exploration-Exploitation in Constrained MDPs",
            "venue": "https://doi.org/10.48550/ARXIV.2003.02189",
            "year": 2020
        },
        {
            "authors": [
                "Eyal Even-Dar",
                "Sham M Kakade",
                "Yishay Mansour."
            ],
            "title": "Online Markov decision processes",
            "venue": "Mathematics of Operations Research 34, 3 (2009), 726\u2013736.",
            "year": 2009
        },
        {
            "authors": [
                "Ramakrishna Gummadi",
                "Peter Key",
                "Alexandre Proutiere."
            ],
            "title": "Repeated auctions under budget constraints: Optimal bidding strategies and equilibria",
            "venue": "the Eighth Ad Auction Workshop, Vol. 4. Citeseer.",
            "year": 2012
        },
        {
            "authors": [
                "Yue He",
                "Xiujun Chen",
                "Di Wu",
                "Junwei Pan",
                "Qing Tan",
                "Chuan Yu",
                "Jian Xu",
                "Xiaoqiang Zhu."
            ],
            "title": "A Unified Solution to Constrained Bidding in Online Display Advertising",
            "venue": "Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining. 2993\u20133001.",
            "year": 2021
        },
        {
            "authors": [
                "David Isele",
                "Alireza Nakhaei",
                "Kikuo Fujimura."
            ],
            "title": "Safe reinforcement learning on autonomous vehicles",
            "venue": "2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 1\u20136.",
            "year": 2018
        },
        {
            "authors": [
                "Chi Jin",
                "Tiancheng Jin",
                "Haipeng Luo",
                "Suvrit Sra",
                "Tiancheng Yu."
            ],
            "title": "Learning Adversarial MDPs with Bandit Feedback and Unknown Transition",
            "venue": "https://doi.org/10.48550/ARXIV.1912.01192",
            "year": 2019
        },
        {
            "authors": [
                "Nikolaos Liakopoulos",
                "Apostolos Destounis",
                "Georgios Paschos",
                "Thrasyvoulos Spyropoulos",
                "Panayotis Mertikopoulos."
            ],
            "title": "Cautious regret minimization: Online optimization with long-term budget constraints",
            "venue": "International Conference on Machine Learning. PMLR, 3944\u20133952.",
            "year": 2019
        },
        {
            "authors": [
                "Shie Mannor",
                "John N. Tsitsiklis",
                "Jia Yuan Yu."
            ],
            "title": "Online Learning with Sample Path Constraints",
            "venue": "Journal of Machine Learning Research 10, 20 (2009), 569\u2013590. http://jmlr.org/papers/v10/mannor09a.html",
            "year": 2009
        },
        {
            "authors": [
                "Gergely Neu",
                "Andras Antos",
                "Andr\u00e1s Gy\u00f6rgy",
                "Csaba Szepesv\u00e1ri."
            ],
            "title": "Online Markov decision processes under bandit feedback",
            "venue": "Advances in Neural Information Processing Systems 23 (2010).",
            "year": 2010
        },
        {
            "authors": [
                "Francesco Orabona."
            ],
            "title": "A Modern Introduction to Online Learning",
            "venue": "CoRR abs/1912.13213 (2019). arXiv:1912.13213 http://arxiv.org/abs/1912.13213",
            "year": 2019
        },
        {
            "authors": [
                "Martin L Puterman."
            ],
            "title": "Markov decision processes: discrete stochastic dynamic programming",
            "venue": "John Wiley & Sons.",
            "year": 2014
        },
        {
            "authors": [
                "Shuang Qiu",
                "Xiaohan Wei",
                "Zhuoran Yang",
                "Jieping Ye",
                "Zhaoran Wang."
            ],
            "title": "Upper Confidence Primal-Dual Reinforcement Learning for CMDP with Adversarial Loss",
            "venue": "Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates, Inc., 15277\u201315287. https://proceedings.neurips.cc/paper/2020/file/ae95296e27d7f695f891cd26b4f37078-Paper.pdf",
            "year": 2020
        },
        {
            "authors": [
                "Aviv Rosenberg",
                "Yishay Mansour."
            ],
            "title": "Online Convex Optimization in Adversarial Markov Decision Processes",
            "venue": "Proceedings of the 36th International Conference on Machine Learning (Proceedings of Machine Learning Research), Kamalika Chaudhuri and Ruslan Salakhutdinov (Eds.), Vol. 97. PMLR, 5478\u20135486. https://proceedings.mlr.press/v97/rosenberg19a.html",
            "year": 2019
        },
        {
            "authors": [
                "Aviv Rosenberg",
                "Yishay Mansour."
            ],
            "title": "Online Stochastic Shortest Path with Bandit Feedback and Unknown Transition Function",
            "venue": "Advances in Neural Information Processing Systems, H. Wallach, H. Larochelle, A. Beygelzimer, F. d Alch\u00e9-Buc, E. Fox, and R. Garnett (Eds.), Vol. 32. Curran Associates, Inc. https://proceedings.neurips.cc/paper/2019/file/a0872cc5b5ca4cc25076f3d868e1bdf8-Paper.pdf",
            "year": 2019
        },
        {
            "authors": [
                "Ashudeep Singh",
                "Yoni Halpern",
                "Nithum Thain",
                "Konstantina Christakopoulou",
                "E Chi",
                "Jilin Chen",
                "Alex Beutel."
            ],
            "title": "Building healthy recommendation sequences for everyone: A safe reinforcement learning approach",
            "venue": "Proceedings of the FAccTRec Workshop, Online. 26\u201327.",
            "year": 2020
        },
        {
            "authors": [
                "Richard S Sutton",
                "Andrew G Barto."
            ],
            "title": "Reinforcement learning: An introduction",
            "venue": "MIT press.",
            "year": 2018
        },
        {
            "authors": [
                "Xiaohan Wei",
                "Hao Yu",
                "Michael J. Neely."
            ],
            "title": "Online Learning in Weakly Coupled Markov Decision Processes: A Convergence Time Study",
            "venue": "Proc. ACM Meas. Anal. Comput. Syst. 2, 1, Article 12 (apr 2018), 38 pages. https://doi.org/10.1145/3179415",
            "year": 2018
        },
        {
            "authors": [
                "Lu Wen",
                "Jingliang Duan",
                "Shengbo Eben Li",
                "Shaobing Xu",
                "Huei Peng."
            ],
            "title": "Safe reinforcement learning for autonomous vehicles through parallel constrained policy optimization",
            "venue": "2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC). IEEE, 1\u20137.",
            "year": 2020
        },
        {
            "authors": [
                "Di Wu",
                "Xiujun Chen",
                "Xun Yang",
                "Hao Wang",
                "Qing Tan",
                "Xiaoxun Zhang",
                "Jian Xu",
                "Kun Gai."
            ],
            "title": "Budget constrained bidding by model-free reinforcement learning in display advertising",
            "venue": "Proceedings of the 27th ACM International Conference on Information and Knowledge Management. 1443\u20131451.",
            "year": 2018
        },
        {
            "authors": [
                "Liyuan Zheng",
                "Lillian J. Ratliff."
            ],
            "title": "Constrained Upper Confidence Reinforcement Learning",
            "venue": "https://doi.org/10.48550/ARXIV.2001.09377",
            "year": 2020
        },
        {
            "authors": [
                "Rosenberg",
                "Mansour"
            ],
            "title": "2019a), by Lemmas 6, 7 and 8 we obtain that with probability at least 1\u2212 2\u03b4 Event E(\u03b4) holds and",
            "venue": "ARXIV PREPRINT - APRIL",
            "year": 2023
        }
    ],
    "sections": [
        {
            "text": "ar X\niv :2\n30 4.\n14 32\n6v 1\nWe study online learning in episodic constrained Markov decision processes (CMDPs), where the goal of the learner is to collect as much reward as possible over the episodes, while guaranteeing that some long-term constraints are satisfied during the learning process. Rewards and constraints can be selected either stochastically or adversarially, and the transition function is not known to the learner. While online learning in classical (unconstrained) MDPs has received considerable attention over the last years, the setting of CMDPs is still largely unexplored. This is surprising, since in real-world applications, such as, e.g., autonomous driving, automated bidding, and recommender systems, there are usually additional constraints and specifications that an agent has to obey during the learning process. In this paper, we provide the first best-of-both-worlds algorithm for CMDPs with longterm constraints. Our algorithm is capable of handling settings in which rewards and constraints are selected either stochastically or adversarially, without requiring any knowledge of the underling process. Moreover, our algorithm matches state-of-the-art regret and constraint violation bounds for settings in which constraints are selected stochastically, while it is the first to provide guarantees in the case in which they are chosen adversarially."
        },
        {
            "heading": "1 Introduction",
            "text": "The framework of Markov decision processes (MDPs) (Puterman, 2014) has been extensively employed to model sequential decision-making problems. In reinforcement learning (Sutton and Barto, 2018), the goal is to learn an optimal policy for an agent interacting with an environment modeled as an MDP. A different line of work (Even-Dar et al., 2009; Neu et al., 2010) is concerned with problems in which an agent interacts with an unknown MDP with the goal of guaranteeing that the overall reward achieved during the learning process is as much as possible. This approach is more akin to online learning problems, and it is far less investigated than classical reinforcement learning approaches.\nIn real-world applications, there are usually additional constraints and specifications that an agent has to obey during the learning process, and these cannot be captured by the classical definition of MDP. For instance, autonomous vehicles must avoid crashing while navigating (Wen et al., 2020; Isele et al., 2018), bidding agents in ad auctions\n\u2217Equal contribution.\nare constrained to a given budget (Wu et al., 2018; He et al., 2021), while recommender systems should not present offending items to users (Singh et al., 2020). In order to model such features of real-world problems, Altman (1999) introduced constrained MDPs (CMDPs) by extending classical MDPs with cost constraints which the agent has to satisfy.\nWe study online learning in episodic CMDPs in which the agent is subject to long-term constraints. In such a setting, the goal of the agent is twofold. On the one hand, the agent wants to minimize their (cumulative) regret, which is how much reward they lose compared to that achieved by playing the same best-in-hindsight, constraint-satisfying policy in all the episodes. On the other hand, while the agent is allowed to violate the constraints in a given episode, they want that the (cumulative) constraint violation over all the episodes stays under control by growing sublinearly in the number of episodes. Long-term constraints can naturally model many features of real world problems, such as, e.g., budget depletion in automated bidding (Balseiro and Gur, 2019; Gummadi et al., 2012).\nAll the existing works studying online learning problems in CMDPs with long-term constraints address settings in which the constraints are selected stochastically according to an unknown (stationary) probability distribution. While these works address both the case where the rewards are stochastic (see, e.g., (Zheng and Ratliff, 2020)) and the one in which they are chosen adversarially (see, e.g., (Wei et al., 2018)), to the best of our knowledge there is no work addressing settings with adversarially-selected constraints.\nIn this paper, we pioneer the study of CMDPs in which the constraints are selected adversarially. In doing so, we introduce a best-of-both-worlds algorithm that provides optimal (in the number of episodes T ) regret and constraint violation bounds when rewards and constraints are selected either stochastically or adversarially, without requiring any knowledge of the underling process. While algorithms of this kind have been recently introduced in online learning settings (see, e.g., (Liakopoulos et al., 2019)), to the best of our knowledge ours is the first of its kind in CMDPs. When the constraints are selected stochastically, we show that our algorithm provides O\u0303( \u221a T ) cumulative regret and constraint violation under a suitably-defined Slater-like condition on the constraints. Moreover, whenever such a condition is not satisfied, our algorithm still ensures O\u0303(T 3/4) regret and constraint violation. Instead, whenever the constraints are chosen adversarially, our analysis revolves around a parameter \u03c1 which is related to our Slater-like condition, and in particular to the \u201cmargin\u201d by which it is possible to strictly satisfy the constraints. We prove that our algorithm achieves no-\u03b1-regret with \u03b1 = \u03c1/(1+\u03c1), while guaranteeing that the cumulative constraint violation is sublinear in the number of episodes. This matches the regret guarantees derived for other best-of-both-worlds algorithms in online learning settings (Castiglioni et al., 2022a). Indeed, under adversarial constraints, Mannor et al. (2009) show that it is impossible to simultaneously achieve sublinear regret and sublinear cumulative constraint violation."
        },
        {
            "heading": "2 Related Works",
            "text": "In the following, we survey some previous works that are tightly related to ours. In particular, we first describe works dealing with the online learning problem in MDPs, and, then, we discuss some works studying the constrained version of the classical online learning problem.\nOnline Learning in MDPs. There is a considerable literature on online learning problems (Cesa-Bianchi and Lugosi, 2006) in MDPs (see (Auer et al., 2008; Even-Dar et al., 2009; Neu et al., 2010) for some initial results on the topic). In such settings, two types of feedback are usually investigated: in the full-information feedback model, the entire loss function is observed after the learner\u2019s choice, while in the bandit feedback model, the learner only observes the loss due to the chosen action. Azar et al. (2017) study the problem of optimal exploration in episodic MDPs with unknown transitions and stochastic losses when the feedback is bandit. The authors present an algorithm whose regret upper bound is O\u0303( \u221a T ), thus matching the lower bound for this class of MDPs and improving the previous result by Auer et al. (2008). Rosenberg and Mansour (2019a) study the online learning problem in episodic MDPs with adversarial losses and unknown transitions when the feedback is full information. The authors present an online algorithm exploiting entropic regularization and providing a regret upper bound of O\u0303( \u221a T ). The same setting is investigated by Rosenberg and Mansour (2019b) when the feedback is bandit. In such a case, the authors provide a regret upper bound of the order of O\u0303(T 3/4), which is improved by Jin et al. (2019) by providing an algorithm that achieves in the same setting a regret upper bound of O\u0303( \u221a T ).\nOnline Learning in CMDPs with Long-term Constraints. All the previous works on the topic study settings in which constraints are selected stochastically. In pareticular, Zheng and Ratliff (2020) deal with episodic CMDPs with stochastic losses and constraints, where the transition probabilities are known and the feedback is bandit. The regret upper bound of their algorithm is of the order of O\u0303(T 3/4), while the cumulative constraint violation is guaranteed to\nbe below a threshold with a given probability. Wei et al. (2018) deal with adversarial losses and stochastic constraints, assuming the transition probabilities are known and the feedback is full information. The authors present an algorithm that guarantees an upper bound of the order of O\u0303( \u221a T ) on both regret and constraint violation. Bai et al. (2020) provide the first algorithm that achieves sublinear regret when the transition probabilities are unknown, assuming that the rewards are deterministic and the constraints are stochastic with a particular structure. Efroni et al. (2020) propose two approaches to deal with the exploration-exploitation dilemma in episodic CMDPs. These approaches guarantee sublinear regret and constraint violation when transition probabilities, rewards, and constraints are unknown and stochastic, while the feedback is bandit. Qiu et al. (2020) provide a primal-dual approach based on optimism in the face of uncertainty. This work shows the effectiveness of such an approach when dealing with episodic CMDPs with adversarial losses and stochastic constraints, achieving both sublinear regret and constraint violation with fullinformation feedback.\nOnline Learning with Long-term Constraints. A central result is provided by Mannor et al. (2009), who show that it is impossible to suffer from sublinear regret and sublinear constraint violation when an adversary chooses losses and constraints. Liakopoulos et al. (2019) try to overcome such an impossibility result by defining a new notion of regret. They study a class of online learning problems with long-term budget constraints that can be chosen by an adversary. The learner\u2019s regret metric is modified by introducing the notion of a K-benchmark, i.e., a comparator that meets the problem\u2019s allotted budget over any window of length K . Castiglioni et al. (2022a,b) deal with the problem of online learning with stochastic and adversarial losses, providing the first best-of-both-worlds algorithm for online learning problems with long-term constraints."
        },
        {
            "heading": "3 Preliminaries",
            "text": ""
        },
        {
            "heading": "3.1 Constrained Markov Decision Processes",
            "text": "We study episodic loop-free constrained MDPs (Altman, 1999), which we call CMDPs for short and are defined as tuples M = ( X,A, P, {rt}Tt=1 , {Gt} T t=1 ) , where:\n\u2022 T is a number of episodes, with t \u2208 [T ] denoting a specific episode.2\n\u2022 X and A are the finite state and action spaces, respectively. By the loop-free property, X is partitioned into L layers X0, . . . , XL such that the first and the last layers are singletons, i.e., X0 = {x0} and XL = {xL}. \u2022 P : X \u00d7A \u00d7X \u2192 [0, 1] is the transition function, where, for ease of notation, we denote by P (x\u2032|x, a) the probability of going from state x \u2208 X to x\u2032 \u2208 X by taking action a \u2208 A. By the loop-free property, it holds that P (x\u2032|x, a) > 0 only if x\u2032 \u2208 Xk+1 and x \u2208 Xk for some k \u2208 [0 . . . L\u2212 1].\n\u2022 {rt}Tt=1 is a sequence of vectors describing the rewards at each episode t \u2208 [T ], namely rt \u2208 [0, 1]|X\u00d7A|. We refer to the reward of a specific state-action pair x \u2208 X, a \u2208 A for a specific episode t \u2208 [T ] as rt(x, a). Rewards may be stochastic, in that case rt is a random variable distributed according to a probability distributionR for every t \u2208 [T ], or chosen by an adversary.\n\u2022 {Gt}Tt=1 is a sequence of constraint matrices describing the m constraint violations at each episode t \u2208 [T ], namely Gt \u2208 [\u22121, 1]|X\u00d7A|\u00d7m, where non-positive violation values stand for satisfaction of the constraints. For i \u2208 [m], we refer to the violation of the i-th constraint for a specific state-action pair x \u2208 X, a \u2208 A at episode t \u2208 [T ] as gt,i(x, a). Constraint violations may be stochastic, in that case Gt is a random variable distributed according to a probability distribution G for every t \u2208 [T ], or chosen by an adversary.\nNotice that any episodic CMDP with horizon L that is not loop-free can be cast into a loop-free one by suitably duplicating the state space L times, i.e., a state x is mapped to a set of new states (x, k), where k \u2208 [0 . . . L]. The learner chooses a policy \u03c0 : X \u00d7 A \u2192 [0, 1] at each episode, defining a probability distribution over actions at each state. For ease of notation, we denote by \u03c0(\u00b7|x) the probability distribution for a state x \u2208 X , with \u03c0(a|x) denoting the probability of action a \u2208 A. In Algorithm 1, we report the interaction between the learner and the environment in a CMDP. Notice that, in this paper, we assume that the learner knows X and A, but they do not know anything about the transition function P .\n2In this paper, we denote with [a . . . b] the set of all consecutive integers beginning with a and ending with b, while [b] = [1 . . . b].\nAlgorithm 1 Learner-Environment Interaction\n1: for t = 1, . . . , T do 2: rt and Gt are chosen stochastically or adversarially 3: the learner chooses a policy \u03c0t : X \u00d7A\u2192 [0, 1] 4: the state is initialized to x0 5: for k = 0, . . . , L\u2212 1 do 6: the learner plays ak \u223c \u03c0t(\u00b7|xk) 7: the environment evolves to xk+1 \u223c P (\u00b7|xk, ak) 8: the learner observes xk+1 9: the learner is revealed rt, Gt"
        },
        {
            "heading": "3.2 Occupancy Measures",
            "text": "Next, we introduce the notion of occupancy measure (Rosenberg and Mansour, 2019b). Given a transition function P and a policy \u03c0, the occupancy measure qP,\u03c0 \u2208 [0, 1]|X\u00d7A\u00d7X| induced by P and \u03c0 is such that, for every x \u2208 Xk, a \u2208 A, and x\u2032 \u2208 Xk+1 with k \u2208 [0 . . . L\u2212 1]:\nqP,\u03c0(x, a, x\u2032) = Pr[xk = x, ak = a, xk+1 = x \u2032|P, \u03c0]. (1)\nMoreover, we also define:\nqP,\u03c0(x, a) = \u2211\nx\u2032\u2208Xk+1\nqP,\u03c0(x, a, x\u2032), (2)\nqP,\u03c0(x) = \u2211\na\u2208A qP,\u03c0(x, a). (3)\nThen, we can introduce the following lemma, which characterizes when an occupancy measure is valid. Lemma 1 (Rosenberg and Mansour (2019a)). For every q \u2208 [0, 1]|X\u00d7A\u00d7X|, it holds that q is a valid occupancy measure of an episodic loop-free MDP if and only if, for every k \u2208 [0 . . . L\u2212 1], the following three conditions hold:\n   \u2211 x\u2208Xk \u2211 a\u2208A \u2211 x\u2032\u2208Xk+1 q(x, a, x\u2032) = 1 \u2211 a\u2208A \u2211 x\u2032\u2208Xk+1 q(x, a, x\u2032) = \u2211 x\u2032\u2208Xk\u22121 \u2211 a\u2208A q(x\u2032, a, x) \u2200x \u2208 Xk\nP q = P\nwhere P is the transition function of the MDP and P q is the one induced by q (see Equation (4)).\nNotice that any valid occupancy measure q induces a transition function P q and a policy \u03c0q as:\nP q(x\u2032|x, a) = q(x, a, x \u2032)\nq(x, a) , \u03c0q(a|x) = q(x, a) q(x) . (4)"
        },
        {
            "heading": "3.3 Offline CMDPs Optimization",
            "text": "We define the parametric linear program LPr,G (5) with parameters r and G we aim at solving as follows:\nOPTr,G :=\n{ maxq\u2208\u2206(M) r \u22a4q\ns.t. G\u22a4q \u2264 0 , (5)\nwhere q \u2208 [0, 1]|X\u00d7A| is the occupancy measure vector whose values are defined in Equation (2), \u2206(M) is the set of valid occupancy measures, r is the reward vector, and G is the constraint matrix. Furthermore, we introduce the following condition. Condition 1. Given a constraint matrix G, the Slater\u2019s Condition holds when there is a strictly feasible solution q\u22c4 s.t. G\u22a4q\u22c4 < 0.\nThen, we define the Lagrangian function for Problem (5).\nDefinition 1 (Lagrangian Function). Given a reward vector r and a constraint matrix G, the Lagrangian function Lr,G : \u2206(M)\u00d7 Rm\u22650 \u2192 R of Problem (5) is defined as:\nLr,G(q, \u03bb) := r\u22a4q \u2212 \u03bb\u22a4(G\u22a4q). (6)\nIt is well-known by Altman (1999) that strong duality holds for CMDPs when assuming the Slater\u2019s condition. Therefore, we have that the following corollary holds.\nCorollary 1. Given a reward vector r and a constraint matrix G such that the Slater\u2019s condition holds, we have:\nOPTr,G = min \u03bb\u2208Rm\n\u22650\nmax q\u2208\u2206(M)\nLr,G(q, \u03bb)\n= max q\u2208\u2206(M) min \u03bb\u2208Rm\n\u22650\nLr,G(q, \u03bb).\nNotice that the min-max problem in Corollary 1 corresponds to the optimization problem associated with a 0-sum Lagrangian game."
        },
        {
            "heading": "3.4 Cumulative Regret and Constraint Violation",
            "text": "We introduce the notion of cumulative regret and cumulative constraint violation as functions in T . In particular, we define the cumulative regret as follows.\nDefinition 2 (Cumulative Regret). It is:\nRT := T OPTr,G \u2212 T\u2211\nt=1\nr\u22a4t q P,\u03c0t , (7)\nwhere:\nr := { Er\u223cR[r] if the rewards are stochastic 1 T \u2211T t=1 rt if the rewards are adversarial ,\nG := { EG\u223cG [G] if the constraints are stochastic 1 T \u2211T t=1 Gt if the constraints are adversarial .\nNotice that the regret is computed with respect to the optimal safe strategy in hindsight in the adversarial case. We will refer to the optimal occupancy measure (the one associated with OPTr,G) as q \u2217, so that OPTr,G = r \u22a4q\u2217 and Definition 2 reduces to RT := \u2211T t=1 r \u22a4(q\u2217 \u2212 qP,\u03c0t). Now, we focus on the cumulative constraint violation.\nDefinition 3 (Cumulative Constraint Violation). It is:\nVT := max i\u2208[m]\nT\u2211\nt=1\n[ G\u22a4t q P,\u03c0t ] i . (8)\nFor the sake of notation, we will refer to qP,\u03c0t using qt, thus omitting the dependency on P and \u03c0."
        },
        {
            "heading": "3.5 Feasibility Parameter",
            "text": "We introduce a problem-specific parameter \u03c1 \u2208 [0, L], which is strictly related to the feasibility of Problem (5). Formally, in the stochastic constraint setting, that is, when constraints are chosen from a fixed distribution, parameter \u03c1 is defined as \u03c1 := maxq\u2208\u2206(M) mini\u2208[m]\u2212 [ G\n\u22a4 q ] i . Instead, in the adversarial constraint setting, parameters \u03c1 is\ndefined as \u03c1 := maxq\u2208\u2206(M) mint\u2208[T ] mini\u2208[m]\u2212 [ G\u22a4t q ] i . In both cases, the occupancy measure leading to the value of \u03c1 is denoted with q\u25e6. We state the following condition on the value of \u03c1. Condition 2. It holds that \u03c1 \u2265 T\u2212 18L \u221a 20m.\nAs discussed in the next sections, the above condition plays a central role when providing guarantees to our algorithms.\nAlgorithm 2 Primal-Dual Gradient Descent Online Policy Search (PDGD-OPS)\nRequire: T , X , A, \u03b4 1: q\u03021 \u2190 UC-O-GDPS.INIT(X,A, \u03b4) 2: \u03bb1 \u2190 OGD.INIT ([ 0, T 1/4 ]m , \u03b7 )\n3: for t = 1 to T do 4: Play \u03c0q\u0302t and observe trajectory (xk, ak) L\u22121 k=0 , reward vector rt, and constraint matrix Gt 5: \u2113t \u2190 Gt\u03bbt \u2212 rt 6: \u03b7t =\n1 \u2113tC \u221a T with \u2113t = max{||\u2113\u03c4 ||\u221e}t\u03c4=1 7: q\u0302t+1 \u2190 UC-O-GDPS.UPDATE ( \u2113t, \u03b7t, (xk, ak) L\u22121 k=0 )\n8: \u03bbt+1 \u2190 OGD.UPDATE ( \u2212G\u22a4t q\u0302t )"
        },
        {
            "heading": "4 Best-of-Both-Worlds CMDP Optimization Algorithm",
            "text": "In this section, we present our algorithm named Primal-Dual Gradient Descent Online Policy Search (PDGD-OPS). The rationale of PDGD-OPS is to instantiate two no-regret algorithms, referred to as primal and dual player, respectively. Precisely, the primal player optimizes on the primal variable space of the Lagrangian function, namely on the set \u2206(M), while the dual player does it on the dual variable space Rm\u22650, which, in our algorithm, is properly shrunk to [ 0, T 1/4 ]m . As concerns the objective functions, the primal player aims to maximize the Lagrangian function, while the dual one to minimize it, as described in the Lagrangian 0-sum game defined in Corollary 1. Notice that, while the space of the dual variables is known apriori, the occupancy measure space needs be estimated online as the transition probabilities are unknown. It is thus is necessary to employ an adversarial MDP no-regret algorithm for the primal player. Moreover, in order to provide guarantees on the dynamics of the Lagrange multipliers \u2014 necessary to bound the cumulative regret and cumulative constraint violation \u2014 we require the primal player to satisfy the weak no-interval regret property (see Definition 5 in Section 5).\nAlgorithm 2 provides the pseudo-code of PDGD-OPS. As mentioned before, the algorithm employs two regret minimizers, named UC-O-GDPS and OGD, working on the space of the primal and dual variables, respectively. The occupancy measure is initialized uniformly (Line 1) by the primal player. We refer to Section 5 for the description of the UC-O-GDPS initialization. The dual player is initialized by the OGD.INIT procedure which takes as input the decision space D and a learning rate \u03b7, and it returns the vector \u03bb1 = 0 associated with the dual variable (Line 2). During the learning process, at each episode t \u2208 [T ], PDGD-OPS plays the policy \u03c0q\u0302t induced by the occupancy measure q\u0302t computed in the previous episode (Line 4). The feedback received by the learner once the episode is concluded concerns the trajectory (xk, ak) L\u22121 k=0 traversed in the CMDP, the reward vector and constraint matrix for that specific episode.\nGiven the observed feedback, the algorithm builds the Lagrangian objective function (Line 5) as the loss \u2113t = Gt\u03bbt\u2212rt and feeds it to the primal player along with the trajectory and the adaptive learning rate (Line 6). The trajectory is needed to estimate the transition probabilities, while the rationale of the adaptive learning rate is to remove the quadratic dependence from \u2016\u03bb\u20161 in the regret bound of the primal player. We refer to Section 5 for the description of UC-O-GDPS.UPDATE (Line 7).\nTo conclude, we notice that the dual player receives only the loss \u2212G\u22a4t q\u0302t, as, r\u22a4t q\u0302t has no dependence on the optimization variable \u03bbt, thus, it does not affect the optimization process. For the sake of completeness, we report the OGD update of the dual player, namely OGD.UPDATE (Line 8) :\n\u03bbt+1 := \u03a0D ( \u03bbt + \u03b7G \u22a4 t q\u0302t ) , (9)\nwhere \u03a0D is the Euclidean projection on the decision space D, \u03b7 = [ K \u221a T ln ( T 2\n\u03b4\n)]\u22121 and K is an instance-\ndependent quantity that does not depend on T and \u03b4. From here on, we refer to the regret suffered by OGD with respect to a general Lagrange multiplier \u03bb as RDT (\u03bb), where D stands for dual. Please notice that, thanks to the prop-\nerties of OGD Orabona (2019) and using the aforementioned learning rate \u03b7, we obtain RDT (\u03bb) \u2264 O\u0303 ( (1 + ||\u03bb||22) \u221a T ) ."
        },
        {
            "heading": "5 Adversarial MDP Optimization Algorithm",
            "text": "We focus on the UC-O-GDPS algorithm employed by the primal player. As aforementioned, this algorithm resorts to online learning techniques as the decision space of the primal player is not known beforehand. In particular, the algorithm is an online adversarial MDP optimizer as Algorithm 2 deals with both stochastic and adversarial settings."
        },
        {
            "heading": "5.1 UC-O-GDPS Algorithm",
            "text": "Upper Confidence Online Gradient Descent Policy Search (UC-O-GDPS) follows the rationale of the UC-O-REPS algorithm by Rosenberg and Mansour (2019a), from which we highlight two major differences. The first difference concerns the update step. In particular, while in UC-O-REPS the update is performed by Online Mirror Descent when the unnormalized KL is used as Bregman divergence, in UC-O-GDPS such a step is performed by Online Gradient Descent. The use of Online Gradient Descent allows the UC-O-GDPS algorithm to satisfy the weak no-interval regret property (see Definition 5) which plays a central role in our regret analysis. We also notice that, to the best of our knowledge, the weak no-interval regret property has never been studied in episodic adversarial MDPs, and thus our result may be of independent interest. The second difference concerns the design of an adaptive learning rate which depends on the losses previously observed. The satisfaction of weak no-interval regret property and the adoption of our adaptive learning rate allow us to attain a regret bound of O\u0303 (\u221a T ) for PDGD-OPS in place of O\u0303 ( T 3 4 ) .\nTransition Probability Confidence Set. Initially, we discuss how UC-O-GDPS updates the confidence set, denoted with P , on the transition probabilities P . This is done by following the approach prescribed by Rosenberg and Mansour (2019a). However, for the sake of clarity, we summarize the functioning. In particular, the update of the confidence set requires a non-negligible computational effort, however it is possible to update the confidence set at a subset of episodes to make the UC-O-GDPS algorithm more efficient without worsening the regret bounds. More precisely, the episodes are divided dynamically in epochs depending of the observed feedback, and the update of the confidence bound is only performed at the first episode of every epoch. UC-O-GDPS adopts counters of visits for each state-action pair (x, a) and each state-action-state triple (x, a, x\u2032) to estimate the empirical transition function as:\nP i (x \u2032 | x, a) = Mi (x \u2032 | x, a) max {1, Ni(x, a)} ,\nwhere Ni(x, a) and Mi (x \u2032 | x, a) are the initial values of the counters, that is, the total number of visits of pair (x, a) and triple (x, a, x\u2032), respectively, observed in the epochs preceding epoch i. Furthermore, a new epoch starts whenever there is a state-action pair whose counter is doubled compared to its initial value at the beginning of the epoch. The confidence set Pi is updated at every epoch i as, \u2200 (x, a) \u2208 X \u00d7A:\nPi = { P\u0302 : \u2225\u2225\u2225P\u0302 (\u00b7|x, a) \u2212 P i (\u00b7|x, a) \u2225\u2225\u2225 1 \u2264 \u01ebi (x, a) } , (10)\nwhere \u01ebi (x, a) is defined as:\n\u01ebi (x, a) =\n\u221a\u221a\u221a\u221a2|Xk(x)+1| ln ( T |X||A| \u03b4 )\nmax {1, Ni(x, a)} ,\nand k(x) denotes the index of the layer to which x belongs and \u03b4 \u2208 (0, 1) is the given confidence. The next result, which directly follows from Rosenberg and Mansour (2019a), shows that the cumulative error due to the estimation of the transition probabilities grows sublinearly during time. Lemma 2. When the confidence set P is updated as in Equation (10), with probability at least 1\u2212 2\u03b4, it holds: T\u2211\nt=1\n||qt \u2212 q\u0302t||1 \u2264 Eq\u03b4 ,\nwhere Eq\u03b4 \u2264 O\u0303( \u221a T ).\nInitialization. UC-O-GDPS.INIT procedure (Line 1 of Algorithm 2) initializes the epoch index as i = 1 and confidence set P1 as the set of all possible transition functions. For all k \u2208 [0..L\u2212 1] and all (x, a, x\u2032) \u2208 Xk \u00d7A\u00d7 Xk+1, the counters are initialized as N0(x, a) = N1(x, a) = M0 (x\n\u2032 | x, a) = M1 (x\u2032 | x, a) = 0. Finally, the following occupancy measure:\nq\u03021 (x, a, x \u2032) =\n1\n|Xk\u2016A| |Xk+1| is returned as output by the initialization procedure for all k \u2208 [0..L\u2212 1] and all (x, a, x\u2032) \u2208 Xk \u00d7A\u00d7Xk+1.\nUpdate. The pseudo-code of UC-O-GDPS.UPDATE procedure (used in Line 7 of Algorithm 2) is provided in Algorithm 3. Initially, it updates the estimate of the confidence set P (Lines 1\u20136) as described above, and, subsequently, it performs an update step according to projected online gradient descent (Line 7).\nAlgorithm 3 UC-O-GDPS.UPDATE\nRequire: \u2113t, \u03b7t, (xk, ak) L\u22121 k=0\n1: for k \u2208 [0..L\u2212 1] do 2: Update counters:\nNi (xk, ak)\u2190 Ni (xk, ak) + 1, Mi (xk+1 | xk, ak)\u2190Mi (xk+1 | xk, ak) + 1\n3: if \u2203k,Ni (xk, ak) \u2265 max {1, 2Ni\u22121 (xk, ak)} then 4: Increase epoch index i\u2190 i+ 1 5: Initialize new counters: for all (x, a, x\u2032),\nNi(x, a) = Ni\u22121(x, a)\nMi (x \u2032 | x, a) = Mi\u22121 (x\u2032 | x, a)\n6: Update confidence set Pi as in Equation (10) 7: Update occupancy measure:\nq\u0302t+1 = \u03a0\u2206(Pi) (q\u0302t \u2212 \u03b7t\u2113t)"
        },
        {
            "heading": "5.2 Interval Regret",
            "text": "Initially, we provide the definition of interval regret for adversarial online MDPs.\nDefinition 4 (Interval Regret). Given an interval [t1..t2] \u2286 [1..T ], the Interval Regret with respect to a general occupancy measure q is:\nRt1,t2(q) :=\nt2\u2211\nt=t1\n\u2113\u22a4t (qt \u2212 q).\nNow, we define the notion of weak no-interval regret. This notion plays a crucial role when proving the properties of Algorithm 2, and it is defined as follows.\nDefinition 5 (Weak No-Interval Regret). An online MDP optimizer satisfies the weak no-interval regret property if:\nRt1,t2(q) \u2264 O\u0303 (\u221a T ) \u2200[t1..t2] \u2286 [1..T ].\nFor the sake of clarity, in the following, we use the superscript P in the regret to distinguish the regret associated with the primal optimizer (RP) from the regret associated with the dual optimizer (RD), and we use RPT (q) in place of RP1,T (q). Next, we state the main result of this section. Theorem 3. With probability at least 1\u2212 2\u03b4, when \u03b7t = ( \u2113tC \u221a T )\u22121 , UC-O-GDPS satisfies for any q \u2208 \u2229i\u2206(Pi):\nRPt1,t2(q) \u2264 \u2113t1,t2E q \u03b4 + \u2113t2LC \u221a T + \u2113t1,t2 |X ||A| 2 (t2 \u2212 t1 + 1) C \u221a T\nwhere \u2113t1,t2 := max{||\u2113t||\u221e}t2t=t1 , \u2113t := \u21131,t and \u03b4 \u2208 [0, 1].\nFurthermore, it directly follows from Theorem 3 that, when t1 = 1, t2 = T , it holds R P T \u2264 O\u0303 ( \u2113T \u221a T ) ."
        },
        {
            "heading": "6 Theoretical Results",
            "text": "In this section we provide the theoretical results attained by Algorithm 2 in terms of cumulative regret and cumulative constraint violation. We start providing a fundamental result on the Lagrange multiplier dynamics. Then, our study\ndistinguishes two cases, as they require a different treatment. In the first case the constraints are stochastic (Section 6.1), while in the second case they are adversarial (Section 6.2).\nThe main technical challenge when bounding the cumulative regret and constraint violation concerns bounding the space of the dual variables. We recall that, when employing standard no-regret techniques, an unbounded dual space would lead to an unbounded loss for the primal regret minimizer, resulting in a linear regret. Our choice D = [0, T 1/4]m of the dual decision space allows us to circumvent such an issue and PDGD-OPS to achieve a cumulative regret bound of RT \u2264 O\u0303 ( T 3 4 ) , while keeping the cumulative violation sublinear. Nevertheless, when \u03c1 is large enough (namely, Condition 2 holds), the O\u0303 ( T 3 4 ) dependency in the upper bounds is not optimal. In particular, in this case, we can show that the Lagrangian vector never touches the boundaries of D, and this property can be used to show that the regret and violation bounds are O\u0303 (\u221a T ) . In the following, we present our result on how the Lagrange\nmultipliers can be bounded, providing a proof sketch and referring to Appendix C for the complete proof.\nTheorem 4. If Condition 2 holds and PDGD-OPS is used, then, when \u03b6 := 20mL 2\n\u03c12 , the following:\n||\u03bbt||1 \u2264 \u03b6 \u2200t \u2208 [T + 1] holds with probability at least 1 \u2212 2\u03b4 in the stochastic constraint setting and with probability at least 1 \u2212 \u03b4 in the adversarial constraint setting.\nProof Sketch. The proof exploits the fact that both the primal and dual player satisfy the weak no-interval regret property. Precisely, the sum of the values of the Lagrangian function in [t1..t2] can be lower bounded by using the interval regret of UC-O-GDPS, while the same quantity can be upper bounded with the interval regret of OGD, showing a contradiction concerning the value Lagrange multipliers can achieve for an opportune choice of constants and learning rates."
        },
        {
            "heading": "6.1 Stochastic Constraint Setting",
            "text": "The peculiarity of this setting is that, at every episode t \u2208 [T ] the constraint matrix G is sampled from a fixed distribution, namely, Gt \u223c G. Instead, rewards rt can be sampled from a fixed distributionR or chosen adversarially.\nAzuma-Hoeffding Bounds. Initially, we bound the error between the realizations of reward vectors and their corresponding mean values when the rewards are chosen stochastically. The proof is provided in Appendix C.\nLemma 3. If the rewards are stochastic, then, with probability at least 1\u2212 \u03b4, it holds: \u2223\u2223\u2223\u2223\u2223 T\u2211\nt=1\n(rt \u2212 r)\u22a4 q\u2217 \u2223\u2223\u2223\u2223\u2223 \u2264 E r \u03b4\nwhere Er\u03b4 := L\u221a2 \u221a T ln ( 2 \u03b4 ) .\nNow, we bound the error between the realizations of constraint violations and their corresponding mean values.\nLemma 4. If the constraints are stochastic, given a sequence of occupancy measures (qt) T t=1, then with probability at least 1\u2212 \u03b4, for all [t1..t2] \u2286 [1..T ], it holds: \u2223\u2223\u2223\u2223\u2223 t2\u2211\nt=t1\n\u03bb\u22a4t ( G\u22a4t \u2212G \u22a4) qt \u2223\u2223\u2223\u2223\u2223 \u2264 \u03bbt1,t2E G t1,t2,\u03b4\nwhere EGt1,t2,\u03b4 := 2L \u221a 2(t2 \u2212 t1 + 1) ln ( T 2 \u03b4 ) and \u03bbt1,t2 := max{\u2016\u03bbt\u20161}t2t=t1 . For the sake of notation, we use EG\u03b4 in place of EG1,T,\u03b4. Let us remark that Er\u03b4 , EG\u03b4 \u2264 O\u0303( \u221a T ).\nAnalysis when Condition 2 holds. We start by analyzing the case in which Condition 2 holds. By Theorem 4, we know that the maximum 1-norm of the dual vectors selected by OGD during the learning process is upper-bounded by the constant \u03b6. Since \u03b6 essentially determines the range of the Lagrangian function, we can prove optimal regret and violation bounds of order O\u0303 ( \u03b6 \u221a T ) for PDGD-OPS, as stated in the following theorem.\nTheorem 5. In the stochastic constraint setting, when Condition 2 holds, the cumulative regret and constraint violation incurred by PDGD-OPS are upper bounded as follows. If the rewards are adversarial, then with probability at least 1\u2212 4\u03b4 it holds:\nRT \u2264 \u03b6EG\u03b4 + \u03b6Eq\u03b4 +RDT (0) +RPT (q\u2217), VT \u2264 1\n\u03b7 \u03b6 + Eq\u03b4 ;\nif the rewards are stochastic, then with probability at least 1\u2212 5\u03b4 it holds: RT \u2264 Er\u03b4 + \u03b6EG\u03b4 + \u03b6Eq\u03b4 +RDT (0) +RPT (q\u2217),\nVT \u2264 1\n\u03b7 \u03b6 + Eq\u03b4 .\nIn both cases, it holds:\nRT \u2264 O\u0303 ( \u03b6 \u221a T ) , VT \u2264 O\u0303 ( \u03b6 \u221a T ) .\nNotice that, if Condition 2 does not hold, the bounds stated in Theorem 5 can become of order O\u0303 ( T 3 4 ) or even linear.\nWe conclude the analysis of the stochastic constraint setting when Condition 2 holds with the following remark.\nRemark 1. In Theorem 5, the regret bound when the rewards are stochastic is better than the one when the rewards are chosen adversarially. This result may seem counter-intuitive as the adversarial setting is the hardest setting a learner might face. Informally, this is due to the different definition of the optimization baseline used in the stochastic and adversarial settings.\nAnalysis when Condition 2 does not hold. We focus on the case in which Condition 2 does not hold. As previously observed, in this case the regret and violation bounds given in Theorem 5 are not meaningful anymore, as they could become linear in T (in fact, this is exactly the case when \u03c1 \u221d T\u2212 14 ). Nevertheless, by constraining the dual player to the decision space D = [0, T 1/4]m, we are able to prove worst-case regret and violation bounds of order O\u0303 ( T 3 4 ) .\nThis result is formalized in the following Theorem.\nTheorem 6. In the stochastic constraint setting, when Condition 2 does not hold, the cumulative regret and constraint violations incurred by PDGD-OPS are upper bounded as follows. If the rewards are adversarial, then with probability at least 1\u2212 4\u03b4 it holds:\nRT \u2264 mT 1 4 EG\u03b4 +mT 1 4 Eq\u03b4 +RDT (0) +RPT (q\u2217), VT \u2264 (2 + 2L) 1\n\u03b7 T\n1 4 + Eq\u03b4 ;\nif the rewards are stochastic, then with probability at least 1\u2212 5\u03b4 it holds:\nRT \u2264 Er\u03b4 +mT 1 4 EG\u03b4 +mT 1 4 Eq\u03b4 +RDT (0) +RPT (q\u2217), VT \u2264 (2 + 2L) 1\n\u03b7 T\n1 4 + Eq\u03b4 .\nIn both cases, it holds:\nRT \u2264 O\u0303 ( T 3 4 ) , VT \u2264 O\u0303 ( T 3 4 ) ."
        },
        {
            "heading": "6.2 Adversarial Constraint Setting",
            "text": "We recall that in this setting, at every episode t \u2208 [T ], the constraint matrix Gt is chosen adversarially. Instead, rewards rt can be sampled from a fixed distribution R or chosen adversarially. This case corresponds to the hardest scenario the learner can face. As stated in Section 3.5, the treatment of this case requires a definition of \u03c1 stronger than that used in the stochastic constraint setting. Thanks to such a redefinition, it is possible to achieve guarantees on the\ncumulative constraint violation of the same order of those attainable in the stochastic setting, while obtaining at least a constant fraction of the optimal reward. Such a result can be achieved when Condition 2 holds. Notice that both sublinear cumulative regret and sublinear cumulative constraint violation cannot be achieved in our setting, as shown by Mannor et al. (2009).\nThe following theorem summarizes our result for the adversarial constraint setting.\nTheorem 7. In the adversarial constraint setting, when Condition 2 holds, the cumulative regret and constraint violations incurred by PDGD-OPS are upper bounded as follows. If the rewards are adversarial, then with probability at least 1\u2212 2\u03b4 it holds:\nRT \u2264 1\n1 + \u03c1 T \u00b7 OPTr,G + \u03b6E q \u03b4 +R D T (0) +R P T (q\u0303),\nVT \u2264 1\n\u03b7 \u03b6 + Eq\u03b4 ;\nif the rewards are stochastic, then with probability at least 1\u2212 3\u03b4 it holds:\nRT \u2264 1\n1 + \u03c1 T \u00b7 OPTr,G + Er\u03b4 + \u03b6E q \u03b4 +R D T (0) +R P T (q\u0303),\nVT \u2264 1\n\u03b7 \u03b6 + Eq\u03b4 .\nIn both cases, it holds:\nT\u2211\nt=1\nr\u22a4t qt \u2265 \u2126 ( \u03c1\n1 + \u03c1 T \u00b7 OPTr,G\n) ,\nVT \u2264 O\u0303 ( \u03b6 \u221a T ) ."
        },
        {
            "heading": "A Events",
            "text": "Here we state the events that we use in the rest of the Appendix.\nThe following event states that the true occupancy measure space is always contained in the confidence set:\nEvent E\u2206(\u03b4): \u2206(M) \u2286 \u2229i\u2206(Pi).\nIn particular, under E\u2206(\u03b4), we have that q\u25e6, q\u2217 \u2208 \u2229i\u2206(Pi). E\u2206(\u03b4) holds with probability at least 1\u2212\u03b4 (See Lemma 5).\nThe following event states that the cumulative error after T episodes due to the difference between qP,\u03c0t and qP q\u0302t ,\u03c0t is small enough:\nEvent E q\u0302(\u03b4): \u2211T\nt=1 ||qt \u2212 q\u0302t||1 \u2264 E q \u03b4 , where E q \u03b4 := 4L|X |\n\u221a 2T ln ( 1 \u03b4 ) + 6L|X | \u221a 2T |A| ln ( T |X||A| \u03b4 ) \u2264 O\u0303( \u221a T ).\nIn the next sections we will often condition on the intersection of the previous events:\nEvent E\u2206,q\u0302(\u03b4): E q\u0302(\u03b4) \u2229 E\u2206(\u03b4)\nE\u2206,q\u0302(\u03b4) holds with probability at least 1\u2212 2\u03b4 (See Lemma 2). The next event states that, in case the rewards are stochastic, the reward accumulated is not too far from the mean reward accumulated. Event Erq\u2217(\u03b4): \u2223\u2223\u2223 \u2211T t=1 (rt \u2212 r) \u22a4 q\u2217 \u2223\u2223\u2223 \u2264 Er\u03b4 , where Er\u03b4 = L\u221a2 \u221a T ln ( 2 \u03b4 ) \u2264 O\u0303 (\u221a T )\nErq\u2217(\u03b4) holds with probability at least 1\u2212 \u03b4 (See Lemma 3).\nFor the stochastic constraint setting, we define the quantity EGt1,t2,\u03b4 := 2L \u221a 2(t2 \u2212 t1 + 1) ln ( T 2 \u03b4 ) and then two events bounding the cumulative difference between the dual utility with the average constraints and that with the sampled constraints. Event EGq\u25e6(\u03b4): for all [t1..t2] \u2286 [1..T ], \u2223\u2223\u2223 \u2211t2 t=t1 \u03bb\u22a4t (G \u22a4 t \u2212G \u22a4 )q\u25e6 \u2223\u2223\u2223 \u2264 \u03bbt1,t2EGt1,t2,\u03b4 Event EGq\u2217(\u03b4): for all [t1..t2] \u2286 [1..T ], \u2223\u2223\u2223 \u2211t2 t=t1 \u03bb\u22a4t (G \u22a4 t \u2212G \u22a4 )q\u2217 \u2223\u2223\u2223 \u2264 \u03bbt1,t2EGt1,t2,\u03b4\nEGq\u25e6(\u03b4), E G q\u2217(\u03b4) each hold with probability at least 1\u2212 \u03b4 (See Lemma 4). We denote EG\u03b4 := EG1,T,\u03b4"
        },
        {
            "heading": "B Additional Details and Omitted Proof of Section 5",
            "text": "B.1 Algorithm\nConfidence Set. The description of how Confidence Set on the Transition Probability functions are built and used, follows precisely the description of Rosenberg and Mansour (2019a). We report the functioning for completeness. UC-O-GDPS keeps counters of visits of each state-action pair (x, a) and each state-action-state triple (x, a, x\u2032), in order to estimate the empirical transition function as:\nP i (x \u2032 | x, a) = Mi (x \u2032 | x, a) max {1, Ni(x, a)}\nwhere Ni(x, a) and Mi (x \u2032 | x, a) are the initial values of the counters, that is, the total number of visits of pair (x, a) and triple (x, a, x\u2032) respectively, before epoch i. Epochs are used to reduce the computational complexity; in particular, a new epoch starts whenever there exists a state-action whose counter is doubled compared to its initial value at the beginning of the epoch. Next, the confidence set for epoch i is defined as:\nPi = { P\u0302 : \u2225\u2225\u2225P\u0302 (\u00b7|x, a)\u2212 P i (\u00b7|x, a) \u2225\u2225\u2225 1 \u2264 \u01ebi (x, a) \u2200 (x, a) \u2208 X \u00d7A } (11)\nwith \u01ebi (x, a) defined as:\n\u01ebi (x, a) =\n\u221a\u221a\u221a\u221a2|Xk(x)+1| ln ( T |X||A| \u03b4 )\nmax {1, Ni(x, a)}\nAlgorithm 4 Upper Confidence Online Gradient Descent Policy Search (UC-O-GDPS)\nRequire: state space X , action space A, episode number T , and confidence parameter \u03b4 1: Initialize epoch index i = 1 and confidence set P1 as the set of all transition functions. For all k \u2208 [0..L\u2212 1] and\nall (x, a, x\u2032) \u2208 Xk \u00d7 A\u00d7 Xk+1, initialize counters N0(x, a) = N1(x, a) = M0 (x\u2032 | x, a) = M1 (x\u2032 | x, a) = 0 and occupancy measure\nq\u03021 (x, a, x \u2032) =\n1\n|Xk\u2016A| |Xk+1| Initialize policy \u03c01 = \u03c0 q\u03021\n2: for t \u2208 [T ] do 3: Execute policy \u03c0t for L steps and obtain trajectory xk, ak for k \u2208 [0..L\u2212 1] and loss \u2113t 4: for k \u2208 [0..L\u2212 1] do 5: Update counters:\nNi (xk, ak)\u2190 Ni (xk, ak) + 1, Mi (xk+1 | xk, ak)\u2190Mi (xk+1 | xk, ak) + 1\n6: if \u2203k,Ni (xk, ak) \u2265 max {1, 2Ni\u22121 (xk, ak)} then 7: Increase epoch index i\u2190 i+ 1 8: Initialize new counters: for all (x, a, x\u2032),\nNi(x, a) = Ni\u22121(x, a)\nMi (x \u2032 | x, a) = Mi\u22121 (x\u2032 | x, a)\n9: Update confidence set Pi based on Equation (10) 10: Update occupancy measure: 11: \u03b7t = 1\n\u2113tC \u221a T with \u2113t = max{||\u2113t||\u221e}tt=1 q\u0302t+1 = \u03a0\u2206(Pi) (q\u0302t \u2212 \u03b7t\u2113t)\n12: Update policy \u03c0t+1 = \u03c0 q\u0302t+1\nusing k(x) for the index of the layer that x belongs to and for some confidence parameter \u03b4 \u2208 (0, 1). We state the following Lemma by Rosenberg and Mansour (2019a), which provides the results related to the confidence set \u01ebi(x, a).\nLemma 5. Rosenberg and Mansour (2019a) For any \u03b4 \u2208 [0, 1]:\n\u2225\u2225P (\u00b7|x, a)\u2212 P i (\u00b7|x, a) \u2225\u2225 1 \u2264\n\u221a\u221a\u221a\u221a2|Xk(x)+1| ln ( T |X||A| \u03b4 )\nmax {1, Ni(x, a)}\nholds with probability at least 1\u2212 \u03b4 simultaneously for all (x, a) \u2208 X \u00d7A and all epochs.\nLemma 5 implies that, with high probability, the occupancy measure space \u2206(M) is included in the estimated one \u2206(Pi) \u2200i.\nOccupancy Measure Update. The update of the occupancy measure is performed on the space \u2206(Pi), which is built on the estimated transition function set Pi. More formally:\nq\u0302t+1 = \u03a0\u2206(Pi) (q\u0302t \u2212 \u03b7t\u2113t)\nwith \u03b7t = 1 \u2113tC \u221a T with \u2113t = max{||\u2113t||\u221e}tt=1, and C constant. The employment of Online Gradient Descent has been necessary to achieve the interval regret results, while the adaptive learning rate was chosen to improve the performance in terms of Regret bounds.\nB.2 Interval Regret\nIn the following subsections, we prove the theorem related to the interval regret of Algorithm 4. First, we will present the main theorem, then, all the necessary lemmas.\nTheorem 3. With probability at least 1\u2212 2\u03b4, when \u03b7t = ( \u2113tC \u221a T )\u22121 , UC-O-GDPS satisfies for any q \u2208 \u2229i\u2206(Pi):\nRPt1,t2(q) \u2264 \u2113t1,t2E q \u03b4 + \u2113t2LC \u221a T + \u2113t1,t2 |X ||A| 2 (t2 \u2212 t1 + 1) C \u221a T\nwhere \u2113t1,t2 := max{||\u2113t||\u221e}t2t=t1 , \u2113t := \u21131,t and \u03b4 \u2208 [0, 1].\nProof. Assume Event E\u2206,q\u0302(\u03b4) holds. By definition 4:\nRt1,t2(q) =\nt2\u2211\nt=t1\n\u2113\u22a4t (qt \u2212 q)\n=\nt2\u2211\nt=t1 \u2113\u22a4t (qt \u2212 q\u0302t) \ufe38 \ufe37\ufe37 \ufe38\n1\n+\nt2\u2211\nt=t1 \u2113\u22a4t (q\u0302t \u2212 q) \ufe38 \ufe37\ufe37 \ufe38\n2\n\u2264 \u2113t1,t2Eq\u03b4 + \u2113t2LC \u221a T + \u2113t1,t2 |X ||A| 2 (t2 \u2212 t1 + 1) C \u221a T\nwhere the Inequality holds by Lemmas 9 and 10. We focus on bounding the first term 1 and the second term 2 .\nB.2.1 Bound on the First Term\nIn order to bound the first term of the Interval Regret, we state some useful Lemmas by Rosenberg and Mansour (2019a).\nLemma 6. Rosenberg and Mansour (2019a) Let {\u03c0t}Tt=1 be policies and let {Pt}Tt=1 be transition functions. Then, T\u2211\nt=1\n||qPt,\u03c0t \u2212 qP,\u03c0t ||1 \u2264 T\u2211\nt=1\n\u2211\nx\u2208X\n\u2211 a\u2208A |qPt,\u03c0t(x, a)\u2212 qP,\u03c0t(x, a)|+ T\u2211 t=1 \u2211 x\u2208X \u2211 a\u2208A qP,\u03c0t(x, a)||Pt(\u00b7|x, a)\u2212P (\u00b7|x, a)||1\n(12) where Pt = P q\u0302t .\nThe following Lemma, shows how to bound the first term in Equation (12) with the second one.\nLemma 7. Rosenberg and Mansour (2019a) Let {\u03c0t}Tt=1 be policies and let {Pt}Tt=1 be transition functions. Then, for every k \u2208 [1..L\u2212 1] and every t = 1, ..., T it holds that:\n\u2211\nxk\u2208Xk\n\u2211\nak\u2208A |qPt,\u03c0t(xk, ak)\u2212 qP,\u03c0t(xk, ak)| \u2264\nk\u22121\u2211\ns=0\n\u2211\nxs\u2208Xs\n\u2211\nas\u2208A qP,\u03c0t(xs, as)||Pt(\u00b7|xs, as)\u2212 P (\u00b7|xs, as)||1\nwhere Pt = P q\u0302t .\nand finally, Equation (12) is upper bounded given:\nLemma 8. Rosenberg and Mansour (2019a) Let {\u03c0t}Tt=1 be policies and let {Pt}Tt=1 be transition functions such that qPt,\u03c0t \u2208 \u2206(Pi) for every t. Then, with probability at least 1\u2212 2\u03b4 Event E\u2206(\u03b4) holds and: T\u2211\nt=1\nL\u22121\u2211\nk=0\nk\u22121\u2211\ns=0\n\u2211\nxs\u2208Xs\n\u2211\nas\u2208A qP,\u03c0t(xs, as)||Pt(\u00b7|xs, as)\u2212P (\u00b7|xs, as)||1 \u2264 2L|X |\n\u221a 2T ln ( 1\n\u03b4\n) +3L|X | \u221a 2T |A| ln ( T |X ||A|\n\u03b4\n)\nwhere Pt = P q\u0302t .\nFrom the previous Lemmas, it easy to show that:\nLemma 2. When the confidence set P is updated as in Equation (10), with probability at least 1\u2212 2\u03b4, it holds: T\u2211\nt=1\n||qt \u2212 q\u0302t||1 \u2264 Eq\u03b4 ,\nwhere Eq\u03b4 \u2264 O\u0303( \u221a T ).\nProof. Following Rosenberg and Mansour (2019a), by Lemmas 6, 7 and 8 we obtain that with probability at least 1\u2212 2\u03b4 Event E\u2206(\u03b4) holds and: \u2211T t=1 ||qPt,\u03c0t \u2212 qP,\u03c0t ||1 \u2264 4L|X | \u221a 2T ln ( 1 \u03b4 ) + 6L|X | \u221a 2T |A| ln ( T |X||A| \u03b4 )\nNow, we are ready to bound 1 .\nLemma 9. Under Event E\u2206,q\u0302(\u03b4) it holds:\nt2\u2211\nt=t1\n\u2113\u22a4t (qt \u2212 q\u0302t) \u2264 \u2113t1,t2Eq\u03b4\nwith \u2113t1,t2 := max{||\u2113t||\u221e}t2t=t1\nProof.\nt2\u2211\nt=t1\n\u2113\u22a4t (qt \u2212 q\u0302t) \u2264 t2\u2211\nt=t1\n||\u2113t||\u221e||qt \u2212 q\u0302t||1\n\u2264 \u2113t1,t2 t2\u2211\nt=t1\n||qt \u2212 q\u0302t||1\n\u2264 \u2113t1,t2 T\u2211\nt=1\n||qt \u2212 q\u0302t||1\n\u2264 \u2113t1,t2Eq\u03b4 (13)\nwith \u2113t1,t2 := max{||\u2113t||\u221e}t2t=t1 and where Inequality (13) holds under the event E q\u0302(\u03b4).\nB.2.2 Bound on the Second Term\nLemma 10. For any q \u2208 \u2229i\u2206(Pi), the Projected OGD update:\nq\u0302t+1 = \u03a0\u2206(Pi) (q\u0302t \u2212 \u03b7t\u2113t)\nwith \u03b7t = 1 \u2113tC \u221a T and \u2113t = max{||\u2113t||\u221e}tt=1 ensures:\nt2\u2211\nt=t1\n\u2113\u22a4t (q\u0302t \u2212 q) \u2264 U1 \u2113t2 2 C \u221a T + U2 \u2113t1,t2 2 (t2 \u2212 t1 + 1) C \u221a T\nwhere U1 = 2L, U2 = |X ||A|, \u2113t1,t2 = max{||\u2113t||\u221e}t2t=t1 .\nProof. By the standard analysis of Projected Online Gradient Descent [Lemma 2.12 Orabona (2019)] we have:\n\u2113\u22a4t (q\u0302t \u2212 q) \u2264 1\n2\u03b7t ||q\u0302t \u2212 q||22 \u2212\n1\n2\u03b7t ||q\u0302t+1 \u2212 q||22 + \u03b7t 2 ||\u2113t||22.\nObserve that for any two occupancy measures q1, q2 it holds:\n||q1 \u2212 q2||22 \u2264 ||q1||22 + ||q2||22 \u2264 ||q1||1 + ||q2||1 \u2264 2L\nwhere the second Inequality follows from q(x, a) \u2208 [0, 1] \u2200x, a. Then, summing over the interval [t1.. t2] we get: t2\u2211\nt=t1\n\u2113\u22a4t (q\u0302t \u2212 q) \u2264 1\n2\u03b7t1 ||q\u0302t1 \u2212 q||22\u2212\n1\n2\u03b7t2 ||q\u0302t2+1 \u2212 q||22 \ufe38 \ufe37\ufe37 \ufe38 \u22640\n+ 1\n2\nt2\u22121\u2211\nt=t1\n( 1\n\u03b7t+1 \u2212 1 \u03b7t\n) ||q\u0302t+1 \u2212 q||22 + 1\n2\nt2\u2211\nt=t1\n\u03b7t||\u2113t||22\n\u2264 L \u03b7t1 + L\nt2\u22121\u2211\nt=t1\n( 1\n\u03b7t+1 \u2212 1 \u03b7t\n) +\n1\n2C \u221a T\nt2\u2211\nt=t1\n1\n\u2113t\n\u2211\nx,a\n\u2113t(x, a) 2 (14)\n\u2264 L \u03b7t1 + L\nt2\u22121\u2211\nt=t1\n( 1\n\u03b7t+1 \u2212 1 \u03b7t\n)\n\ufe38 \ufe37\ufe37 \ufe38 = 1\n\u03b7t2 \u2212 1 \u03b7t1\n+ 1\n2C \u221a T\nt2\u2211\nt=t1 ||\u2113t||\u221e max{||\u2113\u03c4 ||\u221e}t\u03c4=1\ufe38 \ufe37\ufe37 \ufe38\n\u22641\n||\u2113t||\u221e \u2211\nx,a\n1\n\u2264L\u2113t2C \u221a T + |X ||A| 2 \u2113t1,t2 (t2 \u2212 t1 + 1) C \u221a T\n(15)\nwhere Inequality (14) follows from the definition of \u03b7t, and from \u03b7t > \u03b7t+1, while Inequality (15) comes from the telescopic sum over [t1..t2] and from the definition of \u03b7t2 ."
        },
        {
            "heading": "C Omitted Proof of Section 6",
            "text": "C.1 Interval Regrets\nIn this section, we show the Interval Regrets, attained by both primal and dual player, in our specific framework.\nC.1.1 Interval Regret of the dual\nIn this subsection, we show the Interval Regret obtained by dual player. Recall that the dual variables are updated with Projected Online Gradient Descent as shown in (9) or equivalently:\n\u03bbt+1,i = min { max { 0, \u03bbt,i + \u03b7[G \u22a4 t ]iq\u0302t } , T 1/4 } (16)\nwith \u03b7 = [ K \u221a T ln ( T 2\n\u03b4\n)]\u22121 .\nLet\nRDt1,t2(\u03bb) :=\nt2\u2211\nt=t1\n(\u03bb\u2212 \u03bbt)\u22a4 G\u22a4t q\u0302t\ndenote the regret accumulated by OGD from episode t1 to episode t2 with respect to the constant multiplier \u03bb. By standard analysis of OGD Orabona (2019) we have that:\nRDt1,t2(\u03bb) \u2264 ||\u03bbt1 \u2212 \u03bb||22\n2\u03b7 +\n\u03b7\n2\nt2\u2211\nt=t1\n||G\u22a4t q\u0302t||22\nWe can upper-bound the quantity ||G\u22a4t q\u0302t||22 as:\n||G\u22a4t q\u0302t||22 = m\u2211\ni=1\n( \u2211\nx,a\ngt,i(x, a)q\u0302t(x, a)\n)2 \u2264 m\u2211\ni=1\n( \u2211\nx,a\nq\u0302t(x, a)\n)2 \u2264 mL2\nobtaining:\nRDt1,t2(\u03bb) \u2264 D1 ||\u03bbt1 \u2212 \u03bb||22\n\u03b7 +D2\u03b7(t2 \u2212 t1 + 1)\nwith D1 = 1 2 , D2 =\nmL2\n2 .\nWe bound the distance between lagrange multipliers for consecutive episodes.\nLemma 11. If the dual player employs Projected Online Gradient Descent as in Update (16), it holds:\n||\u03bbt+1||1 \u2212 ||\u03bbt||1 \u2264 m\u03b7L\nProof. Since the dual minimizer is performing projected gradient descent with learning rate \u03b7, and the gradient of the Lagrangian at time t with respect to \u03bb is equal to q\u0302\u22a4t G \u22a4 t , element-wise it holds that:\n\u03bbt+1,i = min { max { 0, \u03bbt,i + \u03b7[G \u22a4 t ]iq\u0302t } , T 1 4 }\n\u2264 max { 0, \u03bbt,i + \u03b7[G \u22a4 t ]iq\u0302t } \u2264 max { 0, \u03bbt,i + \u03b7||[G\u22a4t ]i||\u221e||q\u0302t||1 } \u2264 max {0, \u03bbt,i + \u03b7L} = \u03bbt,i + \u03b7L\nThus,\n||\u03bbt+1||1 \u2212 ||\u03bbt||1 = m\u2211\ni=1\n\u03bbt+1,i \u2212 m\u2211\ni=1\n\u03bbt,i \u2264 m\u2211\ni=1\n\u03bbt,i +\nm\u2211\ni=1\n\u03b7L \u2212 m\u2211\ni=1\n\u03bbt,i = m\u03b7L\nC.1.2 Interval Regret of the primal\nWe restate Lemma 10:\nLemma 10. For any q \u2208 \u2229i\u2206(Pi), the Projected OGD update:\nq\u0302t+1 = \u03a0\u2206(Pi) (q\u0302t \u2212 \u03b7t\u2113t)\nwith \u03b7t = 1 \u2113tC \u221a T and \u2113t = max{||\u2113t||\u221e}tt=1 ensures:\nt2\u2211\nt=t1\n\u2113\u22a4t (q\u0302t \u2212 q) \u2264 U1 \u2113t2 2 C \u221a T + U2 \u2113t1,t2 2 (t2 \u2212 t1 + 1) C \u221a T\nwhere U1 = 2L, U2 = |X ||A|, \u2113t1,t2 = max{||\u2113t||\u221e}t2t=t1 .\nLet\n\u03bbt1,t2 := max{||\u03bbt||1}t2t=t1 .\nThen it holds \u2113t1,t2 \u2264 1 + \u03bbt1,t2 and we can restate the interval regret of the primal in terms of the 1-norm of the Lagrange multipliers as:\nt2\u2211\nt=t1\nrLt \u22a4 (q \u2212 q\u0302t) \u2264 U1 (1 + \u03bb1,t2) 2 C \u221a T + U2 (1 + \u03bbt1,t2) 2 (t2 \u2212 t1 + 1) C \u221a T . (17)\nC.2 Bound on the Lagrange multipliers\nWe prove Theorem 4, which we restate for convenience.\nTheorem 4. If Condition 2 holds and PDGD-OPS is used, then, when \u03b6 := 20mL 2\n\u03c12 , the following:\n||\u03bbt||1 \u2264 \u03b6 \u2200t \u2208 [T + 1]\nholds with probability at least 1 \u2212 2\u03b4 in the stochastic constraint setting and with probability at least 1 \u2212 \u03b4 in the adversarial constraint setting.\nProof. Suppose event E\u2206(\u03b4) holds. If the constraints are stochastic, suppose event EGq\u25e6(\u03b4) holds too. Let M > 1 be a constant. We prove the statement by absurd. Suppose by absurd that there exists t2 \u2208 [T ] such that:\n\u2200t \u2264 t2 ||\u03bbt||1 \u2264 2LM\n\u03c12 \u2227 ||\u03bbt2+1||1 >\n2LM\n\u03c12\nand let t1 < t2 be such that:\n||\u03bbt1\u22121||1 \u2264 2L\n\u03c1 \u2227 \u2200t : t1 \u2264 t \u2264 t2 ||\u03bbt||1 \u2265\n2L\n\u03c1 .\nBy construction it holds that 1 < 2L\u03c1 \u2264 ||\u03bbt||1 \u2264 2LM\u03c12 for all t1 \u2264 t \u2264 t2. Also notice that by Lemma 11, for \u03b7 \u2264 1mL it holds that:\n||\u03bbt1 ||1 \u2264 ||\u03bbt1\u22121||1 +m\u03b7L \u2264 2L \u03c1 +m\u03b7L \u2264 4L \u03c1\nFocus on the quantity \u2211t2\nt=t1 \u2212\u03bb\u22a4t G\u22a4t q\u25e6: in the stochastic constraint setting we have, under the event EGq\u25e6(\u03b4):\nt2\u2211\nt=t1\n\u2212\u03bb\u22a4t G\u22a4t q\u25e6 \u2265 t2\u2211\nt=t1\n\u2212\u03bb\u22a4t G \u22a4 q\u25e6 \u2212 \u03bbt1,t2EGt1,t2\n\u2265 t2\u2211\nt=t1\nm\u2211\ni=1\n\u2212\u03bbt,i [ G \u22a4 q\u25e6 ] i \u2212 \u03bbt1,t2EGt1,t2\n\u2265 \u03c1 t2\u2211\nt=t1\nm\u2211\ni=1\n\u03bbt,i \u2212 \u03bbt1,t2EGt1,t2\n= \u03c1\nt2\u2211\nt=t1\n||\u03bbt||1 \u2212 \u03bbt1,t2EGt1,t2\n\u2265 \u03c12L \u03c1 (t2 \u2212 t1 + 1)\u2212 \u03bbt1,t2EGt1,t2\n= 2L(t2 \u2212 t1 + 1)\u2212 \u03bbt1,t2EGt1,t2 While in the adversarial setting it holds:\nt2\u2211\nt=t1\n\u2212\u03bb\u22a4t G\u22a4t q\u25e6 \u2265 t2\u2211\nt=t1\nm\u2211\ni=1\n\u2212\u03bbt,i [ G\u22a4t q \u25e6] i\n\u2265 \u03c1 t2\u2211\nt=t1\nm\u2211\ni=1\n\u03bbt,i\n= \u03c1\nt2\u2211\nt=t1\n||\u03bbt||1\n\u2265 \u03c12L \u03c1 (t2 \u2212 t1 + 1) = 2L(t2 \u2212 t1 + 1)\nIn particular, we have that: t2\u2211\nt=t1\n\u2212\u03bb\u22a4t G\u22a4t q\u25e6 \u2265 2L(t2 \u2212 t1 + 1)\u2212 \u03bbt1,t2EGt1,t2\nis true in both settings under the required events.\nWe can lower bound the cumulative value of the Lagrangian function, namely rLt \u22a4 q\u0302t, from t1 to t2 by that achievable by the primal minimizer by always playing the feasible occupancy measure q\u25e6:\nt2\u2211\nt=t1\nrLt \u22a4 q\u0302t =\nt2\u2211\nt=t1\nrLt \u22a4 q\u25e6 \u2212\nt2\u2211\nt=t1\nrLt \u22a4 (q\u25e6 \u2212 q\u0302t)\n=\nt2\u2211\nt=t1\nr\u22a4t q \u25e6\n\ufe38 \ufe37\ufe37 \ufe38 \u22650\n+\nt2\u2211\nt=t1\n\u2212\u03bb\u22a4t G\u22a4t q\u25e6 \u2212 t2\u2211\nt=t1\nrLt \u22a4 (q\u25e6 \u2212 q\u0302t)\n\u2265 2L(t2 \u2212 t1 + 1)\u2212 \u03bbt1,t2EGt1,t2,\u03b4 \u2212 t2\u2211\nt=t1\nrLt \u22a4 (q\u25e6 \u2212 q\u0302t)\nApplying Lemma 10 and observing that by construction 1 \u2264 \u03bbt1,t2 \u2264 2LM\u03c12 , we can bound 1 + \u03bbt1,t2 \u2264 4LM\u03c12 and obtain:\nt2\u2211\nt=t1\nrLt \u22a4 q\u0302t \u2265 2L(t2 \u2212 t1 + 1)\u2212 2LM\n\u03c12 EGt1,t2,\u03b4 \u2212 U1\n2LM \u03c12 C \u221a T \u2212 U2 2LM \u03c12 (t2 \u2212 t1 + 1) C \u221a T\nsince under E\u2206(\u03b4) we have that q\u25e6 \u2208 \u2229i\u2206(Pi). We can upper-bound the same quantity with the value achievable by the dual by always playing a vector of zeroes.\nt2\u2211\nt=t1\nrLt \u22a4 q\u0302t =\nt2\u2211\nt=t1\nr\u22a4t q\u0302t \u2212 t2\u2211\nt=t1\n\u03bb\u22a4t G \u22a4 t q\u0302t\n\u2264 t2\u2211\nt=t1\nr\u22a4t q\u0302t \u2212 t2\u2211\nt=t1\n0\u22a4G\u22a4t q\u0302t +R D t1,t2(0)\n\u2264 t2\u2211\nt=t1\nL+D1 ||\u03bbt1 ||22\n\u03b7 +D2\u03b7(t2 \u2212 t1 + 1)\n\u2264 t2\u2211\nt=t1\nL+D1 ||\u03bbt1 ||21\n\u03b7 +D2\u03b7(t2 \u2212 t1 + 1)\n\u2264 L(t2 \u2212 t1 + 1) +D3 L2\n\u03c12\u03b7 +D2\u03b7(t2 \u2212 t1 + 1)\nWith D3 = 4D1.\nCombining the bounds on the cumulative value of the Lagrangian, we have:\n2L(t2 \u2212 t1 + 1)\u2212 2LM\n\u03c12 EGt1,t2,\u03b4\u2212U1\n2LM \u03c12 C \u221a T \u2212 U2 2LM \u03c12 (t2 \u2212 t1 + 1) C \u221a T\n\u2264\nL(t2 \u2212 t1 + 1) +D3 L2\n\u03c12\u03b7 +D2\u03b7(t2 \u2212 t1 + 1)\nObserving that EGt1,t2,\u03b4 = 2L \u221a 2(t2 \u2212 t1 + 1) ln ( T 2 \u03b4 ) \u2264 U3l1 \u221a t2 \u2212 t1 + 1 with l1 = \u221a ln ( T 2 \u03b4 ) and U3 = 2L \u221a 2 and rearranging the terms we obtain:\nL(t2 \u2212 t1 + 1) \u2264 U3 2LM \u03c12 l1 \u221a t2 \u2212 t1 + 1 +\n+ U1 2LM \u03c12 C \u221a T +\n+ U2 2LM \u03c12 (t2 \u2212 t1 + 1) C \u221a T + +D2\u03b7(t2 \u2212 t1 + 1) +\n+D3 1\n\u03b7\nL2\n\u03c12\nWe will make use of the following lemma:\nLemma 12. For \u03b7 \u2264 1mL and M\u03c1 > 4 it holds:\n(t2 \u2212 t1 + 1) > M\n\u03c12m\u03b7\nProof. By Lemma 11 we have:\nt2\u2211\nt=t1\n(||\u03bbt+1||1 \u2212 ||\u03bbt||1) \u2264 t2\u2211\nt=t1\nm\u03b7L\nwhich, since the sum in the LHS is telescopic, implies:\n||\u03bbt2+1||1 \u2212 ||\u03bbt1 ||1 \u2264 (t2 \u2212 t1 + 1)m\u03b7L.\nAlso note that:\n2LM \u03c12 \u2212 4L \u03c1 \u2264 ||\u03bbt2+1||1 \u2212 ||\u03bbt1 ||1.\nRearranging the terms, we obtain, for M\u03c1 > 4:\nM\n\u03c12m\u03b7 < 2L(M\u03c1 \u2212 2) \u03c1m\u03b7L \u2264 (t2 \u2212 t1 + 1)\nApplying Lemma 12 we show that the above leads to a contradiction for some choices of C, M and \u03b7, namely, we show that:\nL(t2 \u2212 t1 + 1) > U3 2LM \u03c12 l1 \u221a t2 \u2212 t1 + 1 + (1)\n+ U1 2LM \u03c12 C \u221a T + (2)\n+ U2 2LM \u03c12 (t2 \u2212 t1 + 1) C \u221a T + (3) +D2\u03b7(t2 \u2212 t1 + 1) + (4)\n+D3 1\n\u03b7\nL2 \u03c12 (5)\nIn the followings, we prove that each of the terms on the RHS is upper bounded by 15L(t2 \u2212 t1 + 1):\n1. By trivial computations and applying Lemma 12:\n1 5 L(t2 \u2212 t1 + 1) > U3 2LM \u03c12 l1 \u221a T \u2265 U3 2LM \u03c12 l1 \u221a t2 \u2212 t1 + 1\n(t2 \u2212 t1 + 1) > U3 10M \u03c12 l1 \u221a T\n(t2 \u2212 t1 + 1) > M\n\u03c12m\u03b7 \u2265 U3\n10M \u03c12 l1 \u221a T\n1\nm\u03b7 \u2265 10U3l1\n\u221a T\nwhich is ensured by:\n\u03b7 \u2264 1 10mU3l1 \u221a T\n2. Then applying again Lemma 12:\n1 5 L(t2 \u2212 t1 + 1) > U1 2LM \u03c12 C \u221a T\n(t2 \u2212 t1 + 1) > M\n\u03c12m\u03b7 \u2265 10U1\nM \u03c12 C \u221a T\nwhich is true for:\n\u03b7 \u2264 1 10mU1C \u221a T\n3. We solve the third term with respect to C.\n1 5 L(t2 \u2212 t1 + 1) \u2265 U2 2LM \u03c12 (t2 \u2212 t1 + 1) C \u221a T\nwhich is ensured by:\nC \u2265 10U2 M \u03c12 1\u221a T\n4.\n1 5 L(t2 \u2212 t1 + 1) > D2\u03b7(t2 \u2212 t1 + 1)\n1 5 L > D2\u03b7\nWhich is ensured by\n\u03b7 < L\n5D2\n5. Applying Lemma 12, we solve the Inequality with respect to M:\n1 5 L(t2 \u2212 t1 + 1) > D3 1 \u03b7\nL2\n\u03c12\n(t2 \u2212 t1 + 1) > M\n\u03c12m\u03b7 \u2265 5D3\n1\n\u03b7\nL\n\u03c12\nM m \u2265 5D3L\nfrom which: M \u2265 5mD3L\nWe recall all the constants: D2 = mL2 2 , D3 = 2, U1 = 2L, U2 = |X ||A|, U3 = 2L \u221a 2. We choose M = 10mL and recall Condition 2:\n\u03c1 \u2265 T\u221218L \u221a 20m \u21d2 20mL 2\n\u03c12 \u2264 T 14 \u2264\n\u221a T\nWe now focus on the condition on C:\nC \u2265 10U2 10mL \u03c12 1\u221a T\n= 5 U2 L\n20mL2\n\u03c12 1\u221a T\nis thus always ensured by C = 5U2L . The conditions on \u03b7 are satisfied if:\n\u03b7 \u2264 min { L\n5D2 ,\n1\n10mU1C \u221a T ,\n1\n10mU3l1 \u221a T\n} .\nObserve that:\nmin\n{ L\n5D2 ,\n1\n10mU1C \u221a T ,\n1\n10mU3l1 \u221a T\n}\n= min\n{ 1\n2.5mL ,\n1\n10mU1 ( 5U2 L )\u221a T ,\n1\n20 \u221a 2mLl1 \u221a T\n}\nwhich, if we plug in the value of l1, leads to the choice:\n\u03b7 = 1\n50mmax {\nU1U2 L , L\n}\u221a T ln ( T 2\n\u03b4\n)\nThe remaining conditions M\u03c1 > 4, \u03b7 \u2264 1mL are trivially satisfied. Summing the conditions (1 \u2212 5) proves the contradiction. If we plug the values of U1 and U2 corresponding to UC-O-GDPS, we have max { U1U2 L , L } = max {2|X ||A|, L} = 2|X ||A| and thus obtain:\n\u03b7 = 1\n100m|X ||A| \u221a T ln ( T 2\n\u03b4\n)\nC.3 Analysis with Stochastic constraints\nC.3.1 Lower bound on the dual cumulative utility\nWe start proving a useful Lemma in which we lower bound the dual cumulative utility. This Lemma holds both for the stochastic constraints and the adversarial constraint setting. Lemma 13. Under the event E q\u0302(\u03b4), the cumulative dual utility \u2211T\nt=1 \u03bb \u22a4 t G \u22a4 t qt is lower bounded as:\nT\u2211\nt=1\n\u03bb\u22a4t G \u22a4 t qt \u2265 \u2212\u03bb1,TEq\u03b4 \u2212RDT (0)\nwhere \u03bbt1,t2 := max{\u2016\u03bbt\u20161}t2t=t1 .\nProof. We exploit the fact that the dual is no-regret with respect to the 0 vector:\nT\u2211\nt=1\n\u03bb\u22a4t G \u22a4 t qt =\nT\u2211\nt=1\n\u03bb\u22a4t G \u22a4 t (qt \u2212 q\u0302t) +\nT\u2211\nt=1\n\u03bb\u22a4t G \u22a4 t q\u0302t\n\u2265 T\u2211\nt=1\n\u03bb\u22a4t G \u22a4 t (qt \u2212 q\u0302t) +\nT\u2211\nt=1\n0\u22a4G\u22a4t q\u0302t \u2212RDT (0)\n\u2265 T\u2211\nt=1 \u2212\u2016\u03bbt\u20161\ufe38 \ufe37\ufe37 \ufe38 \u2264\u03bb1,T\n\u2225\u2225G\u22a4t \u2225\u2225 \u221e\ufe38 \ufe37\ufe37 \ufe38\n\u22641\n\u2016qt \u2212 q\u0302t\u20161 \u2212RDT (0)\n\u2265 \u2212\u03bb1,T T\u2211\nt=1\n\u2016qt \u2212 q\u0302t\u20161 \u2212RDT (0)\n\u2265 \u2212\u03bb1,TEq\u03b4 \u2212RDT (0) where the last Inequality holds under E q\u0302(\u03b4).\nC.3.2 Analysis when Condition 2 holds\nWe start by introducing the notation v\u0302t,i := [G \u22a4 t ]iq\u0302t, that is the violation of the i-th constraint incurred by q\u0302t. We further denote V\u0302t,i := \u2211t \u03c4=1 v\u0302\u03c4,i. Observe that, when Condition 2 holds, thanks to Theorem 4 we have ||\u03bbt||1 \u2264 T 1 4 for all t and thus \u03bbt,i \u2264 T 1 4 . This means that \u03bbt,i never gets past the upper extreme and the update of the dual is effectively equivalent to that of OGD working on the set Rm\u22650: \u03bbt,i = max{\u03bbt,i + \u03b7v\u0302t,i, 0} Lemma 14. If Condition 2 holds, then for each episode t \u2208 [T ] and each constraint i it holds:\n\u03bbt,i \u2265 \u03b7V\u0302t\u22121,i\nProof. We prove the result by induction. Suppose that the statement holds for episode t. Then\n\u03bbt+1,i = max{\u03bbt,i + \u03b7v\u0302t,i, 0} \u2265 \u03bbt,i + \u03b7v\u0302t,i \u2265 \u03b7V\u0302t\u22121,i + \u03b7v\u0302t,i = \u03b7V\u0302t,i\nObserve that for t = 1 the statement holds as the sum on the RHS evaluates to 0.\nLemma 15. If Condition 2 holds, under the events E\u2206(\u03b4), E q\u0302(\u03b4) and EGq\u25e6(\u03b4) for the stochastic constraint setting and under the events E\u2206(\u03b4) and E q\u0302(\u03b4) for the adversarial constraints one, it holds:\nVT \u2264 V\u0302T,i\u2217 + Eq\u03b4\nProof. Let i\u2217 denote the most violated constraint, e.g. i\u2217 = argmaxi \u2211T t=1[G \u22a4 t qt]i. Then we have:\nVT =\nT\u2211\nt=1\n[G\u22a4t qt]i\u2217\n=\nT\u2211\nt=1\n[G\u22a4t q\u0302t]i\u2217 + T\u2211\nt=1\n[G\u22a4t (qt \u2212 q\u0302t)]i\u2217\n= V\u0302T,i\u2217 +\nT\u2211\nt=1\n[G\u22a4t ]i\u2217(qt \u2212 q\u0302t)\n\u2264 V\u0302T,i\u2217 + T\u2211\nt=1\n||[G\u22a4t ]i\u2217 ||\u221e||qt \u2212 q\u0302t||1\n\u2264 V\u0302T,i\u2217 + Eq\u03b4\nWhere the last step holds under E q\u0302(\u03b4) since ||[G\u22a4t ]i\u2217 ||\u221e \u2264 1.\nWe are now ready to prove the regret and violation bounds for the stochastic constraint setting.\nTheorem 5. In the stochastic constraint setting, when Condition 2 holds, the cumulative regret and constraint violation incurred by PDGD-OPS are upper bounded as follows. If the rewards are adversarial, then with probability at least 1\u2212 4\u03b4 it holds:\nRT \u2264 \u03b6EG\u03b4 + \u03b6Eq\u03b4 +RDT (0) +RPT (q\u2217), VT \u2264 1\n\u03b7 \u03b6 + Eq\u03b4 ;\nif the rewards are stochastic, then with probability at least 1\u2212 5\u03b4 it holds: RT \u2264 Er\u03b4 + \u03b6EG\u03b4 + \u03b6Eq\u03b4 +RDT (0) +RPT (q\u2217),\nVT \u2264 1\n\u03b7 \u03b6 + Eq\u03b4 .\nIn both cases, it holds:\nRT \u2264 O\u0303 ( \u03b6 \u221a T ) , VT \u2264 O\u0303 ( \u03b6 \u221a T ) .\nProof. Assume events EGq\u25e6(\u03b4), E G q\u2217(\u03b4), E \u2206(\u03b4) and E q\u0302(\u03b4) hold.\nRecall that \u03bb1,T \u2264 \u03b6 under the events E\u2206(\u03b4) and EGq\u25e6(\u03b4) since Condition 2 holds (see proof of Theorem 4). By Lemma 15 we have:\nVT \u2264 V\u0302T,i\u2217 + Eq\u03b4 \u2264 1\n\u03b7 \u03bbT+1,i\u2217 + Eq\u03b4\n\u2264 1 \u03b7 ||\u03bbT+1||1 + Eq\u03b4\n\u2264 1 \u03b7 \u03b6 + Eq\u03b4\nWhere the third Inequality holds for Lemma 14. By the definition of regret of the primal:\nT\u2211\nt=1\nr\u22a4t qt \u2265 T\u2211\nt=1\nr\u22a4t q \u2217 \u2212\nT\u2211\nt=1\n\u03bb\u22a4t G \u22a4 t q\n\u2217 + T\u2211\nt=1\n\u03bb\u22a4t G \u22a4 t qt \u2212RPT (q\u2217)\n\u2265 T\u2211\nt=1\nr\u22a4t q \u2217 \u2212\nT\u2211\nt=1\n\u03bb\u22a4t G \u22a4 t q \u2217 \u2212 \u03bb1,TEq\u03b4 \u2212RDT (0)\u2212RPT (q\u2217) (18)\n\u2265 T\u2211\nt=1\nr\u22a4t q \u2217 \u2212\nT\u2211\nt=1\n\u03bb\u22a4t G \u22a4 q\u2217 \u2212 \u03bb1,TEG\u03b4 \u2212 \u03bb1,T Eq\u03b4 \u2212RDT (0)\u2212RPT (q\u2217) (19)\n\u2265 T\u2211\nt=1\nr\u22a4t q \u2217 \u2212\nT\u2211\nt=1\n\u2211\ni\n\u03bbt,i (G)iq \u2217\n\ufe38 \ufe37\ufe37 \ufe38 \u22640\n\u2212\u03bb1,T EG\u03b4 \u2212 \u03bb1,T Eq\u03b4 \u2212RDT (0)\u2212RPT (q\u2217) (20)\n\u2265 T\u2211\nt=1\nr\u22a4t q \u2217 \u2212 \u03b6EG\u03b4 \u2212 \u03b6Eq\u03b4 \u2212RDT (0)\u2212RPT (q\u2217)\nwhere Inequality (18) holds for Lemma 13, and Inequality (19) holds under Event EGq\u2217(\u03b4). We now focus on the case in which the rewards are adversarial. We have:\nT\u2211\nt=1\nr\u22a4t q \u2217 = T \u00b7 r\u22a4q\u2217 = T \u00b7 OPTr,G\nand thus we obtain the stated bound:\nT\u2211\nt=1\nr\u22a4t qt \u2265 T \u00b7 OPTr,G \u2212 \u03b6EG\u03b4 \u2212 \u03b6E q \u03b4 \u2212RDT (0)\u2212RPT (q\u2217)\nBy union bound on EGq\u25e6(\u03b4), E G q\u2217(\u03b4) and E \u2206,q\u0302(\u03b4), the result holds with probability at least 1\u2212 4\u03b4.\nFor the stochastic rewards case, we require also event Erq\u2217(\u03b4) to hold. Thus,\nT\u2211\nt=1\nr\u22a4t q \u2217 \u2265\nT\u2211\nt=1\nr\u22a4q\u2217 \u2212 Er\u03b4 = T \u00b7 OPTr,G \u2212 Er\u03b4\nand thus we obtain the stated bound:\nT\u2211\nt=1\nr\u22a4t qt \u2265 T \u00b7 OPTr,G \u2212 Er\u03b4 \u2212 \u03b6EG\u03b4 \u2212 \u03b6E q \u03b4 \u2212RDT (0)\u2212RPT (q\u2217)\nBy union bound on EGq\u25e6(\u03b4), E G q\u2217(\u03b4), E \u2206,q\u0302(\u03b4) and Erq\u2217(\u03b4), the result holds with probability at least 1\u2212 5\u03b4.\nObserve that under E\u2206,q\u0302(\u03b4) it holds:\nRPT (q \u2217) \u2264 O\u0303 ( (1 + \u03bb1,T ) \u221a T ) = O\u0303 ( \u03b6 \u221a T )\nand\nRDT (0) \u2264 mL2\n2\n1\n100m|X ||A| \u221a ln ( T 2\n\u03b4\n) \u221a T \u2264 O (\u221a T )\nC.3.3 Analysis when Condition 2 does not hold\nLemma 16. If Condition 2 does not hold, then\nV\u0302T,i \u2264 (2 + 2L) 1\n\u03b7 T\n1 4 \u2200T, i\nholds under the event E\u2206(\u03b4) in the adversarial constraint setting and under the events E\u2206(\u03b4), EGq\u25e6(\u03b4), in the stochastic constraint setting.\nProof. Assume events E\u2206(\u03b4), EGq\u25e6(\u03b4) hold and suppose by absurd that V\u0302T,i = (2 + 2L + \u01eb) 1 \u03b7T 1 4 , with \u01eb > 0, for some T and i.\nWe can lower bound the quantity \u2211T\nt=1 r L t \u22a4 q\u0302t:\nT\u2211\nt=1\nrLt \u22a4 q\u0302t =\nT\u2211\nt=1\nr\u22a4t q \u25e6 \ufe38 \ufe37\ufe37 \ufe38 \u22650\n\u2212 T\u2211\nt=1\n\u03bb\u22a4t G \u22a4 t q\n\u25e6 \u2212 T\u2211\nt=1\nrLt \u22a4 (q\u25e6 \u2212 q\u0302t)\n\u2265 \u2212 T\u2211\nt=1\n\u03bb\u22a4t G \u22a4 q\u25e6\n\ufe38 \ufe37\ufe37 \ufe38 \u22650\n\u2212\u03bb1,TEG\u03b4 \u2212 T\u2211\nt=1\nrLt \u22a4 (q\u25e6 \u2212 q\u0302t)\n\u2265 \u2212mT 14 EG\u03b4 \u2212 T\u2211\nt=1\nrLt \u22a4 (q\u25e6 \u2212 q\u0302t) (21)\nWhere Inequality (21) holds since ||\u03bbt||1 \u2264 mV 1 4 by construction of the dual space. Observe that, if we are in the Adversarial setting, then from the (stronger) definition of \u03c1 and q\u25e6 it holds \u2212 \u2211T\nt=1 \u03bb \u22a4 t G \u22a4 t q \u25e6 \u2265 0 and we obtain the tighter bound\nT\u2211\nt=1\nrLt \u22a4 q\u0302t \u2265 \u2212\nT\u2211\nt=1\nrLt \u22a4 (q\u25e6 \u2212 q\u0302t)\nThe dual is no regret with respect to the vector \u03bb\u0303, whose elements are 0 for j 6= i and T 14 in position j = i: T\u2211\nt=1\nrLt \u22a4 q\u0302t =\nT\u2211\nt=1\nr\u22a4t q\u0302t \u2212 T\u2211\nt=1\n\u03bb\u22a4t G \u22a4 t q\u0302t\n\u2264 T\u2211\nt=1\nr\u22a4t q\u0302t \u2212 T\u2211\nt=1\n\u03bb\u0303\u22a4G\u22a4t q\u0302t +R D T (\u03bb\u0303)\n=\nT\u2211\nt=1\nr\u22a4t q\u0302t \u2212 T 1 4\nT\u2211\nt=1\n[G\u22a4t q\u0302t]i +R D T (\u03bb\u0303)\n\u2264 LT \u2212 T 14 V\u0302T,i +RDT (\u03bb\u0303)\nCombining the bounds we have:\n\u2212mT 14 EG\u03b4 \u2212 T\u2211\nt=1\nrLt \u22a4 (q\u25e6 \u2212 q\u0302t) \u2264 LT \u2212 T 1 4 V\u0302T,i +R D T (\u03bb\u0303)\nT 1 4 V\u0302T,i \u2264 LT +mT 1 4 EG\u03b4 +\nT\u2211\nt=1\nrLt \u22a4 (q\u25e6 \u2212 q\u0302t) +RDT (\u03bb\u0303)\n\u221a T\n\u03b7 (2 + 2L+ \u01eb) \u2264 LT +mT 14 EG\u03b4 +\nT\u2211\nt=1\nrLt \u22a4 (q\u25e6 \u2212 q\u0302t) +RDT (\u03bb\u0303) (22)\nObserve that:\nRDT (\u03bb\u0303) \u2264 1\n2 ||\u03bb\u0303||22 \u03b7 + mL2 2 \u03b7T =\n\u221a T\n2\u03b7 +\nmL2\n2\n1\n100m|X ||A| \u221a T ln ( T 2\n\u03b4\n)T \u2264 L \u221a T\n\u03b7\nSince |X | \u2265 L.\nFor the primal it holds by Lemma 10:\nT\u2211\nt=1\nrLt \u22a4 (q\u25e6 \u2212 q\u0302t) =\nT\u2211\nt=1\n\u2113t \u22a4(q\u0302t \u2212 q\u25e6)\n\u2264 \u03bb1,TU1C \u221a T + \u03bb1,TU2\n\u221a T\nC\n\u2264 mT 14 \u221a T ( U1C +\nU2 C\n)\n= m ( U1\nU2 5 + 5\n)\u221a T T 1 4\n= m ( 2L |X ||A|\n5 + 5\n)\u221a T T 1 4\n\u2264 6mL|X ||A| \u221a T T 1 4\n\u2264 L \u03b7 T 1 4 \u2264 L\n\u221a T\n\u03b7\nAnd for the Azuma-Hoeffding term it holds:\nmT 1 4 EG\u03b4 = mT 1 4 2L\n\u221a 2T ln ( T 2\n\u03b4\n) \u2264 1\n\u03b7 T\n1 4 =\n\u221a T\n\u03b7\nObserve that LT \u2264 \u221a T \u03b7 holds trivially. Dividing both the terms in Equation (22) by \u221a T \u03b7 , we obtain\n2 + 2L+ \u01eb \u2264 2 + 2L which is absurd.\nWe are now ready to prove the Regret and Violation bounds when Assumption 2 does not hold:\nTheorem 6. In the stochastic constraint setting, when Condition 2 does not hold, the cumulative regret and constraint violations incurred by PDGD-OPS are upper bounded as follows. If the rewards are adversarial, then with probability at least 1\u2212 4\u03b4 it holds:\nRT \u2264 mT 1 4 EG\u03b4 +mT 1 4 Eq\u03b4 +RDT (0) +RPT (q\u2217), VT \u2264 (2 + 2L) 1\n\u03b7 T\n1 4 + Eq\u03b4 ;\nif the rewards are stochastic, then with probability at least 1\u2212 5\u03b4 it holds:\nRT \u2264 Er\u03b4 +mT 1 4 EG\u03b4 +mT 1 4 Eq\u03b4 +RDT (0) +RPT (q\u2217), VT \u2264 (2 + 2L) 1\n\u03b7 T\n1 4 + Eq\u03b4 .\nIn both cases, it holds:\nRT \u2264 O\u0303 ( T 3 4 ) , VT \u2264 O\u0303 ( T 3 4 ) .\nProof. Assume events E\u2206(\u03b4), E q\u0302(\u03b4), EGq\u2217(\u03b4), E G q\u25e6(\u03b4) hold. We avoid the computations and restart from line (20), since the previous part of the proofs are identical:\nT\u2211\nt=1\nr\u22a4t qt \u2265 T\u2211\nt=1\nr\u22a4t q \u2217 \u2212\nT\u2211\nt=1\n\u2211\ni\n\u03bbt,i (G)iq \u2217\n\ufe38 \ufe37\ufe37 \ufe38 \u22640\n\u2212\u03bb1,T EG\u03b4 \u2212 \u03bb1,T Eq\u03b4 \u2212RDT (0)\u2212RPT (q\u2217)\n\u2265 T\u2211\nt=1\nr\u22a4t q \u2217 \u2212mT 14 EG\u03b4 \u2212mT 1 4 Eq\u03b4 \u2212RDT (0)\u2212RPT (q\u2217)\nBy the same reasoning as in the proof of Theorem 5, we obtain that if the rewards are adversarial then\nT\u2211\nt=1\nr\u22a4t qt \u2265 T \u00b7 OPTr,G \u2212mT 1 4 EG\u03b4 \u2212mT 1 4 Eq\u03b4 \u2212RDT (0)\u2212RPT (q\u2217)\nwith probability at least 1 \u2212 4\u03b4 by union bound on E\u2206,q\u0302(\u03b4), EGq\u2217(\u03b4) and EGq\u25e6(\u03b4), while if the rewards are stochastic, under the event Erq\u2217(\u03b4) we have that:\nT\u2211\nt=1\nr\u22a4t qt \u2265 T \u00b7 OPTr,G \u2212 Er\u03b4 \u2212mT 1 4 EG\u03b4 \u2212mT 1 4 Eq\u03b4 \u2212RDT (0)\u2212RPT (q\u2217)\nwith probability at least 1\u2212 5\u03b4 by union bound on E\u2206,q\u0302(\u03b4), EGq\u2217(\u03b4), EGq\u25e6(\u03b4) and Erq\u2217(\u03b4).\nObserve that:\nRPT (q \u2217) \u2264 O\u0303\n( T 3 4 )\nand\nRDT (0) = mL2\n2 \u03b7T \u2264 O\u0303\n(\u221a T ) .\nIn order to bound the violation, we apply Lemma 16:\nVT \u2264 V\u0302T,i\u2217 + Eq\u03b4 \u2264 (2 + 2L) 1\n\u03b7 T\n1 4 + Eq\u03b4\nC.4 Analysis with Adversarial Constraints\nC.4.1 Analysis when Condition 2 holds\nTheorem 7. In the adversarial constraint setting, when Condition 2 holds, the cumulative regret and constraint violations incurred by PDGD-OPS are upper bounded as follows. If the rewards are adversarial, then with probability at least 1\u2212 2\u03b4 it holds:\nRT \u2264 1\n1 + \u03c1 T \u00b7 OPTr,G + \u03b6E q \u03b4 +R D T (0) +R P T (q\u0303),\nVT \u2264 1\n\u03b7 \u03b6 + Eq\u03b4 ;\nif the rewards are stochastic, then with probability at least 1\u2212 3\u03b4 it holds:\nRT \u2264 1\n1 + \u03c1 T \u00b7 OPTr,G + Er\u03b4 + \u03b6E q \u03b4 +R D T (0) +R P T (q\u0303),\nVT \u2264 1\n\u03b7 \u03b6 + Eq\u03b4 .\nIn both cases, it holds:\nT\u2211\nt=1\nr\u22a4t qt \u2265 \u2126 ( \u03c1\n1 + \u03c1 T \u00b7 OPTr,G\n) ,\nVT \u2264 O\u0303 ( \u03b6 \u221a T ) .\nProof. Assume events E\u2206(\u03b4) and E q\u0302(\u03b4) hold.\nRecall that \u03bb1,T \u2264 \u03b6 under the event E\u2206(\u03b4) since Condition 2 holds (see the proof of Theorem 4). Following the same steps of the proof of Theorem 5, we obtain:\nVT \u2264 1\n\u03b7 \u03b6 + Eq\u03b4\nLet q\u0303 = \u03c11+\u03c1q \u2217 + 11+\u03c1q \u25e6, observe that it holds for all t and for all i:\n[G\u22a4t q\u0303]i = \u03c1 1 + \u03c1 [G\u22a4t q \u2217]i\ufe38 \ufe37\ufe37 \ufe38 \u22641 + 1 1 + \u03c1 [G\u22a4t q \u25e6]i\ufe38 \ufe37\ufe37 \ufe38 \u2264\u2212\u03c1 \u2264 0\nr\u22a4t q\u0303 = \u03c1 1 + \u03c1 r\u22a4t q \u2217 + 1 1 + \u03c1 r\u22a4t q \u25e6 \u2265 \u03c1 1 + \u03c1 r\u22a4t q \u2217\nBy the definition of regret of the primal:\nT\u2211\nt=1\nr\u22a4t qt \u2265 T\u2211\nt=1\nr\u22a4t q\u0303 \u2212 T\u2211\nt=1\n\u03bb\u22a4t G \u22a4 t q\u0303 +\nT\u2211\nt=1\n\u03bb\u22a4t G \u22a4 t qt \u2212RPT (q\u0303)\n\u2265 \u03c1 1 + \u03c1\nT\u2211\nt=1\nr\u22a4t q \u2217 \u2212\nT\u2211\nt=1\n\u2211\ni \u03bbt,i [G \u22a4 t q\u0303]i\ufe38 \ufe37\ufe37 \ufe38 \u22640 +\nT\u2211\nt=1\n\u03bb\u22a4t G \u22a4 t qt \u2212RPT (q\u0303)\n\u2265 \u03c1 1 + \u03c1\nT\u2211\nt=1\nr\u22a4t q \u2217 \u2212 \u03bb1,T Eq\u03b4 \u2212RDT (0)\u2212RPT (q\u0303)\n\u2265 \u03c1 1 + \u03c1\nT\u2211\nt=1\nr\u22a4t q \u2217 \u2212 \u03b6Eq\u03b4 \u2212RDT (0)\u2212RPT (q\u0303)\nwhere the third Inequality holds for Lemma 13. By the same reasoning as in the proof of Theorem 5, we obtain that if the rewards are adversarial it holds:\nT\u2211\nt=1\nr\u22a4t qt \u2265 \u03c1\n1 + \u03c1 T \u00b7 OPTr,G \u2212 \u03b6E q \u03b4 \u2212RDT (0)\u2212RPT (q\u0303)\n= T \u00b7 OPTr,G \u2212 1\n1 + \u03c1 T \u00b7 OPTr,G \u2212 \u03b6E q \u03b4 \u2212RDT (0)\u2212RPT (q\u0303)\nwith probability at least 1\u2212 2\u03b4, since we are conditioning on E\u2206,q\u0302(\u03b4). If the rewards are stochastic, requiring also event Erq\u2217(\u03b4) to hold we obtain:\n\u03c1\n1 + \u03c1\nT\u2211\nt=1\nr\u22a4t q \u2217 \u2265 \u03c1\n1 + \u03c1\nT\u2211\nt=1\nr\u22a4q\u2217 \u2212 \u03c1 1 + \u03c1 Er\u03b4 \u2265 \u03c1 1 + \u03c1 T \u00b7 OPTr,G \u2212 Er\u03b4\nAnd thus,\nT\u2211\nt=1\nr\u22a4t qt \u2265 T \u00b7 OPTr,G \u2212 1\n1 + \u03c1 T \u00b7 OPTr,G \u2212 Er\u03b4 \u2212 \u03b6E q \u03b4 \u2212RDT (0)\u2212RPT (q\u0303)\nwith probability at least 1\u2212 3\u03b4. Finally observe that, under Assumption 2 and event E\u2206,q\u0302(\u03b4), it holds: RPT (q\u0303) \u2264 O\u0303 ( (1 + \u03bb1,T ) \u221a T ) \u2264 O\u0303 ( \u03b6 \u221a T )\nand\nRDT (0) \u2264 mL2\n2\n1\n100m|X ||A| \u221a ln ( T 2\n\u03b4\n) \u221a T \u2264 O (\u221a T )\nC.5 Azuma-Hoeffding Bounds and Proofs\nIn this subsection we prove that events Erq\u2217(\u03b4), E G q\u25e6(\u03b4), E G q\u2217(\u03b4) each hold with probability at least 1\u2212 \u03b4. Lemma 3. If the rewards are stochastic, then, with probability at least 1\u2212 \u03b4, it holds:\u2223\u2223\u2223\u2223\u2223 T\u2211\nt=1\n(rt \u2212 r)\u22a4 q\u2217 \u2223\u2223\u2223\u2223\u2223 \u2264 E r \u03b4\nwhere Er\u03b4 := L\u221a2 \u221a T ln ( 2 \u03b4 ) .\nProof. Observe that:\nmax t\u2208[t1..t2]\n\u2223\u2223\u2223(rt \u2212 r)\u22a4 q\u2217 \u2223\u2223\u2223 \u2264 max\nt\u2208[t1..t2] \u2016rt \u2212 r\u2016\u221e\ufe38 \ufe37\ufe37 \ufe38\n\u22641\n\u2016q\u2217\u20161\n\u2264 L\nwhere the second Inequality holds since since q\u2217(x, a) \u2265 0. By the Azuma-Hoeffding inequality for martingales we have that:\nP [\u2223\u2223\u2223\u2223\u2223 t2\u2211\nt=t1\n(rt \u2212 r)\u22a4 q\u2217 \u2223\u2223\u2223\u2223\u2223 \u2265 L\u221a 2 \u221a T ln ( 2 \u03b4 )] \u2264 \u03b4.\nWe perform the same analysis for the constraints, obtaining:\nLemma 4. If the constraints are stochastic, given a sequence of occupancy measures (qt) T t=1, then with probability at least 1\u2212 \u03b4, for all [t1..t2] \u2286 [1..T ], it holds: \u2223\u2223\u2223\u2223\u2223 t2\u2211\nt=t1\n\u03bb\u22a4t ( G\u22a4t \u2212G \u22a4) qt \u2223\u2223\u2223\u2223\u2223 \u2264 \u03bbt1,t2E G t1,t2,\u03b4\nwhere EGt1,t2,\u03b4 := 2L \u221a 2(t2 \u2212 t1 + 1) ln ( T 2 \u03b4 ) and \u03bbt1,t2 := max{\u2016\u03bbt\u20161}t2t=t1 .\nProof. Observe that:\nmax t\u2208[t1..t2]\n\u2223\u2223\u2223\u03bb\u22a4t (G\u22a4t \u2212G \u22a4 )qt \u2223\u2223\u2223 \u2264 max t\u2208[t1..t2] \u2016\u03bbt\u20161 \u2225\u2225\u2225G\u22a4t \u2212G \u22a4\u2225\u2225\u2225 \u221e\ufe38 \ufe37\ufe37 \ufe38\n\u22642\n\u2016qt\u20161\n\u2264 max t\u2208[t1..t2] 2||\u03bbt||1L\n= 2\u03bbt1,t2L\nwhere the second Inequality holds since qt(x, a) \u2265 0 and \u03bbt,i \u2265 0. By the Azuma-Hoeffding inequality for martingales we have that:\nP [\u2223\u2223\u2223\u2223\u2223 t2\u2211\nt=t1\n\u03bb\u22a4t (G \u22a4 t \u2212G \u22a4 )qt \u2223\u2223\u2223\u2223\u2223 \u2265 2\u03bbt1,t2L \u221a 2(t2 \u2212 t1 + 1) ln ( 2 \u03b4 T 2 2 )] \u2264 2\u03b4/T 2.\nA union bound over all the t1, t2 such that [t1..t2] \u2286 [1..T ] concludes the proof."
        }
    ],
    "year": 2023
}