{
    "abstractText": "In this paper, we present our submission to 3rd Affective Behavior Analysis in-the-wild (ABAW) challenge. Learning complex interactions among multimodal sequences is critical to recognise dimensional affect from in-the-wild audiovisual data. Recurrence and attention are the two widely used sequence modelling mechanisms in the literature. To clearly understand the performance differences between recurrent and attention models in audiovisual affect recognition, we present a comprehensive evaluation of fusion models based on LSTM-RNNs, self-attention and cross-modal attention, trained for valence and arousal estimation. Particularly, we study the impact of some key design choices: the modelling complexity of CNN backbones that provide features to the the temporal models, with and without endto-end learning. We trained the audiovisual affect recognition models on in-the-wild ABAW corpus by systematically tuning the hyper-parameters involved in the network architecture design and training optimisation. Our extensive evaluation of the audiovisual fusion models shows that LSTM-RNNs can outperform the attention models when coupled with low-complex CNN backbones and trained in an end-to-end fashion, implying that attention models may not necessarily be the optimal choice for continuous-time multimodal emotion recognition.",
    "authors": [
        {
            "affiliations": [],
            "name": "Vincent Karas"
        },
        {
            "affiliations": [],
            "name": "Mani Kumar Tellamekala"
        },
        {
            "affiliations": [],
            "name": "Michel Valstar"
        },
        {
            "affiliations": [],
            "name": "Bj\u00f6rn W. Schuller"
        }
    ],
    "id": "SP:5b28a0ff0b1dde5ad900ec8fd02c3848c5467716",
    "references": [
        {
            "authors": [
                "Panagiotis Antoniadis",
                "Ioannis Pikoulis",
                "Panagiotis P Filntisis",
                "Petros Maragos"
            ],
            "title": "An audiovisual and contextual approach for categorical and continuous emotion recognition in-the-wild",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Michael Braun",
                "Jonas Schubert",
                "Bastian Pfleging",
                "Florian Alt"
            ],
            "title": "Improving driver emotions with affective strategies",
            "venue": "Multimodal Technologies and Interaction,",
            "year": 2019
        },
        {
            "authors": [
                "Cong Cai",
                "Yu He",
                "Licai Sun",
                "Zheng Lian",
                "Bin Liu",
                "Jianhua Tao",
                "Mingyu Xu",
                "Kexin Wang"
            ],
            "title": "Multimodal sentiment analysis based on recurrent neural network and multimodal attention",
            "venue": "In Proceedings of the 2nd on Multimodal Sentiment Analysis Challenge,",
            "year": 2021
        },
        {
            "authors": [
                "Qiong Cao",
                "Li Shen",
                "Weidi Xie",
                "Omkar M. Parkhi",
                "Andrew Zisserman"
            ],
            "title": "Vggface2: A dataset for recognising faces across pose and age",
            "venue": "In 2018 13th IEEE International Conference on Automatic Face Gesture Recognition (FG",
            "year": 2018
        },
        {
            "authors": [
                "H. Chen",
                "D. Jiang",
                "H. Sahli"
            ],
            "title": "Transformer encoder with multi-modal multi-head attention for continuous affect recognition",
            "venue": "IEEE Transactions on Multimedia,",
            "year": 2020
        },
        {
            "authors": [
                "Sheng Chen",
                "Yang Liu",
                "Xiang Gao",
                "Zhen Han"
            ],
            "title": "Mobilefacenets: Efficient cnns for accurate real-time face verification on mobile devices",
            "venue": "Biometric Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "D. Deng",
                "Z. Chen",
                "B.E. Shi"
            ],
            "title": "Multitask Emotion Recognition with Incomplete Labels",
            "venue": "In 2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG",
            "year": 2020
        },
        {
            "authors": [
                "Didan Deng",
                "Liang Wu",
                "Bertram E. Shi"
            ],
            "title": "Iterative distillation for better uncertainty estimates in multitask emotion recognition",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops,",
            "year": 2021
        },
        {
            "authors": [
                "Nhu-Tai Do",
                "Tram-Tran Nguyen-Quynh",
                "Soo-Hyung Kim"
            ],
            "title": "Affective expression analysis in-the-wild using multitask temporal statistical deep learning model",
            "venue": "In 2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG",
            "year": 2020
        },
        {
            "authors": [
                "Florian Eyben",
                "Martin W\u00f6llmer",
                "Tony Poitschke",
                "Bj\u00f6rn Schuller",
                "Christoph Blaschke",
                "Berthold F\u00e4rber",
                "Nhu Nguyen-Thien"
            ],
            "title": "Emotion on the road\u2014necessity, acceptance, and feasibility of affective computing in the car",
            "venue": "Advances in human-computer interaction,",
            "year": 2010
        },
        {
            "authors": [
                "Ziwang Fu",
                "Feng Liu",
                "Hanyang Wang",
                "Jiayin Qi",
                "Xiangling Fu",
                "Aimin Zhou",
                "Zhibin Li"
            ],
            "title": "A cross-modal fusion network based on self-attention and residual structure for multimodal emotion recognition",
            "venue": "arXiv preprint arXiv:2111.02172,",
            "year": 2021
        },
        {
            "authors": [
                "Albert Gu",
                "Karan Goel",
                "Christopher R\u00e9"
            ],
            "title": "Efficiently modeling long sequences with structured state spaces",
            "venue": "arXiv preprint arXiv:2111.00396,",
            "year": 2021
        },
        {
            "authors": [
                "S. Hershey",
                "S. Chaudhuri",
                "D.P.W. Ellis",
                "J.F. Gemmeke",
                "A. Jansen",
                "R.C. Moore",
                "M. Plakal",
                "D. Platt",
                "R.A. Saurous",
                "B. Seybold",
                "M. Slaney",
                "R.J. Weiss",
                "K. Wilson"
            ],
            "title": "Cnn architectures for large-scale audio classification",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2017
        },
        {
            "authors": [
                "Sepp Hochreiter",
                "J\u00fcrgen Schmidhuber"
            ],
            "title": "Long short-term memory",
            "venue": "Neural computation,",
            "year": 1997
        },
        {
            "authors": [
                "J. Huang",
                "J. Tao",
                "B. Liu",
                "Z. Lian",
                "M. Niu"
            ],
            "title": "Multimodal transformer fusion for continuous emotion recognition",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2020
        },
        {
            "authors": [
                "Giancarlo Kerg",
                "Bhargav Kanuparthi",
                "Anirudh Goyal",
                "Kyle Goyette",
                "Yoshua Bengio",
                "Guillaume Lajoie"
            ],
            "title": "Untangling tradeoffs between recurrence and self-attention in neural networks",
            "venue": "arXiv preprint arXiv:2006.09471,",
            "year": 2020
        },
        {
            "authors": [
                "Dimitrios Kollias"
            ],
            "title": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & multi-task learning challenges",
            "venue": "arXiv preprint arXiv:2202.10659,",
            "year": 2022
        },
        {
            "authors": [
                "D. Kollias",
                "A. Schulc",
                "E. Hajiyev",
                "S. Zafeiriou"
            ],
            "title": "Analysing affective behavior in the first abaw 2020 competition",
            "venue": "In 2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG",
            "year": 2020
        },
        {
            "authors": [
                "Dimitrios Kollias",
                "Viktoriia Sharmanska",
                "Stefanos Zafeiriou"
            ],
            "title": "Face behavior a la carte: Expressions, affect and action units in a single network",
            "venue": "arXiv preprint arXiv:1910.11111,",
            "year": 1910
        },
        {
            "authors": [
                "Dimitrios Kollias",
                "Viktoriia Sharmanska",
                "Stefanos Zafeiriou"
            ],
            "title": "Distribution matching for heterogeneous multitask learning: a large-scale face study",
            "venue": "arXiv preprint arXiv:2105.03790,",
            "year": 2021
        },
        {
            "authors": [
                "Dimitrios Kollias",
                "Panagiotis Tzirakis",
                "Mihalis A Nicolaou",
                "Athanasios Papaioannou",
                "Guoying Zhao",
                "Bj\u00f6rn Schuller",
                "Irene Kotsia",
                "Stefanos Zafeiriou"
            ],
            "title": "Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond",
            "venue": "International Journal of Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Dimitrios Kollias",
                "Stefanos Zafeiriou"
            ],
            "title": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
            "venue": "arXiv preprint arXiv:1910.04855,",
            "year": 1910
        },
        {
            "authors": [
                "Dimitrios Kollias",
                "Stefanos Zafeiriou"
            ],
            "title": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
            "venue": "arXiv preprint arXiv:2103.15792,",
            "year": 2021
        },
        {
            "authors": [
                "Dimitrios Kollias",
                "Stefanos Zafeiriou"
            ],
            "title": "Analysing affective behavior in the second abaw2 competition",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "DN Krishna",
                "Ankita Patil"
            ],
            "title": "Multimodal emotion recognition using cross-modal attention and 1d convolutional neural networks",
            "venue": "In Interspeech,",
            "year": 2020
        },
        {
            "authors": [
                "F. Kuhnke",
                "L. Rumberg",
                "J. Ostermann"
            ],
            "title": "Two-stream aural-visual affect analysis in the wild",
            "venue": "IEEE International Conference on Automatic Face and Gesture Recognition (FG",
            "year": 2020
        },
        {
            "authors": [
                "Lawrence I.-Kuei Lin"
            ],
            "title": "A Concordance Correlation Coefficient to Evaluate Reproducibility",
            "year": 1989
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "arXiv preprint arXiv:1711.05101,",
            "year": 2017
        },
        {
            "authors": [
                "Ziyu Ma",
                "Fuyan Ma",
                "Bin Sun",
                "Shutao Li"
            ],
            "title": "Hybrid mutimodal fusion for dimensional emotion recognition",
            "venue": "In Proceedings of the 2nd on Multimodal Sentiment Analysis Challenge,",
            "year": 2021
        },
        {
            "authors": [
                "Danny Merkx",
                "Stefan L Frank"
            ],
            "title": "Human sentence processing: Recurrence or attention",
            "venue": "arXiv preprint arXiv:2005.09471,",
            "year": 2020
        },
        {
            "authors": [
                "Rosalind W Picard"
            ],
            "title": "Affective computing: from laughter to ieee",
            "venue": "IEEE Transactions on Affective Computing,",
            "year": 2010
        },
        {
            "authors": [
                "Soujanya Poria",
                "Erik Cambria",
                "Rajiv Bajpai",
                "Amir Hussain"
            ],
            "title": "A review of affective computing: From unimodal analysis to multimodal fusion",
            "venue": "Information Fusion,",
            "year": 2017
        },
        {
            "authors": [
                "Vandana Rajan",
                "Alessio Brutti",
                "Andrea Cavallaro"
            ],
            "title": "Is cross-attention preferable to self-attention for multi-modal emotion recognition",
            "venue": "arXiv preprint arXiv:2202.09263,",
            "year": 2022
        },
        {
            "authors": [
                "Fabien Ringeval",
                "Bj\u00f6rn Schuller",
                "Michel Valstar",
                "Nicholas Cummins",
                "Roddy Cowie",
                "Leili Tavabi",
                "Maximilian Schmitt",
                "Sina Alisamir",
                "Shahin Amiriparian",
                "Eva-Maria Messner",
                "Siyang Song",
                "Shuo Liu",
                "Ziping Zhao",
                "Adria Mallol-Ragolta",
                "Zhao Ren",
                "Mohammad Soleymani",
                "Maja Pantic"
            ],
            "title": "Avec 2019 workshop and challenge: State-of-mind, detecting depression with ai, and cross-cultural affect recognition",
            "venue": "In Proceedings of the 9th International on Audio/Visual Emotion Challenge and Workshop,",
            "year": 2019
        },
        {
            "authors": [
                "F. Ringeval",
                "A. Sonderegger",
                "J. Sauer",
                "D. Lalanne"
            ],
            "title": "Introducing the recola multimodal corpus of remote collaborative and affective interactions",
            "venue": "In 2013 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG),",
            "year": 2013
        },
        {
            "authors": [
                "Mark Sandler",
                "Andrew Howard",
                "Menglong Zhu",
                "Andrey Zhmoginov",
                "Liang-Chieh Chen"
            ],
            "title": "Mobilenetv2: Inverted residuals and linear bottlenecks",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Florian Schroff",
                "Dmitry Kalenichenko",
                "James Philbin"
            ],
            "title": "Facenet: A unified embedding for face recognition and clustering",
            "venue": "In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2015
        },
        {
            "authors": [
                "Licai Sun",
                "Zheng Lian",
                "Jianhua Tao",
                "Bin Liu",
                "Mingyue Niu"
            ],
            "title": "Multi-modal continuous dimensional emotion recognition using recurrent neural network and self-attention mechanism",
            "venue": "In Proceedings of the 1st International on Multimodal Sentiment Analysis in Real-life Media Challenge and Workshop,",
            "year": 2020
        },
        {
            "authors": [
                "Licai Sun",
                "Mingyu Xu",
                "Zheng Lian",
                "Bin Liu",
                "Jianhua Tao",
                "Meng Wang",
                "Yuan Cheng"
            ],
            "title": "Multimodal emotion recognition and sentiment analysis via attention enhanced recurrent model",
            "venue": "In Proceedings of the 2nd on Multimodal Sentiment Analysis Challenge,",
            "year": 2021
        },
        {
            "authors": [
                "Antoine Toisoul",
                "Jean Kossaifi",
                "Adrian Bulat",
                "Georgios Tzimiropoulos",
                "Maja Pantic"
            ],
            "title": "Estimation of continuous valence and arousal levels from faces in naturalistic conditions",
            "venue": "Nature Machine Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Yao-Hung Hubert Tsai",
                "Shaojie Bai",
                "Paul Pu Liang",
                "J. Zico Kolter",
                "Louis-Philippe Morency",
                "Ruslan Salakhutdinov"
            ],
            "title": "Multimodal transformer for unaligned multimodal language sequences",
            "venue": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2019
        },
        {
            "authors": [
                "Panagiotis Tzirakis",
                "Stefanos Zafeiriou",
                "Bjorn W Schuller"
            ],
            "title": "End2you\u2013the imperial toolkit for multimodal profiling by end-to-end learning",
            "venue": "arXiv preprint arXiv:1802.01115,",
            "year": 2018
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "\u0141. ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need. pages 5998\u20136008",
            "venue": "Curran Associates, Inc,",
            "year": 2017
        },
        {
            "authors": [
                "Stefanos Zafeiriou",
                "Dimitrios Kollias",
                "Mihalis A Nicolaou",
                "Athanasios Papaioannou",
                "Guoying Zhao",
                "Irene Kotsia"
            ],
            "title": "Aff-wild: Valence and arousal \u2018in-the-wild\u2019challenge",
            "venue": "In Computer Vision and Pattern Recognition Workshops (CVPRW),",
            "year": 2017
        },
        {
            "authors": [
                "Su Zhang",
                "Yi Ding",
                "Ziquan Wei",
                "Cuntai Guan"
            ],
            "title": "Continuous emotion recognition with audio-visual leader-follower attentive fusion",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Yuan-Hang Zhang",
                "Rulin Huang",
                "Jiabei Zeng",
                "Shiguang Shan"
            ],
            "title": "M 3 f: Multi-modal continuous valence-arousal estimation in the wild",
            "venue": "In 2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG",
            "year": 2020
        },
        {
            "authors": [
                "Jianfeng Zhao",
                "Xia Mao",
                "Lijiang Chen"
            ],
            "title": "Speech emotion recognition using deep 1d & 2d cnn lstm networks",
            "venue": "Biomedical signal processing and control,",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "The growing market penetration of smart devices is radically increasing the number of scenarios where we interact with machines. Nowadays, such interactions take place in a wide range of environments, including the workplace, at\nhome, or even inside our vehicles. If technology is going to accompany us in all aspect of our lives, powering machines with affective capabilities is a requirement to humanise technology towards a more natural Human-Computer Interaction (HCI). Creating more human-like technology is one of the objectives of Affective Computing [31].\nThis paper focuses on the automatic recognition of valence and arousal with the aim to develop Emotional Artificial Intelligence solutions that could allow machines to adapt to the users\u2019 affective states. For instance, in the vehicle environment, if the car detects that the driver has been showing high levels of arousal and negative levels of valence, the system could interpret that the driver is experiencing some sort of anger. Alternatively, low levels of arousal and negative valence may indicate sadness or fatigue [10]. In this case, the car could suggest playing calm music or even pulling over to take some rest and relax for the safety of the own driver and those in the surroundings. Mood improvement and relaxation systems already exist on the market for some premium brands, but knowing when to suggest them and adapting them based on the detected emotions could greatly enhance the user experience [2].\nHowever, automatically detecting emotions and moods in a setting as described above, or any scenario in an uncontrolled environment, remains an open problem. It is commonly referred to as emotion recognition in the wild, and presents several challenges: Data is often noisy, e.g. for the visual modality, a person\u2019s face may be partially occluded, or there may be rapid changes in illumination. Audio from the voice may be indistinct due to background noise, or missing if the person is silent. Another issue lies in cross-cultural emotion recognition [34], i.e. automatic affect recognition systems needing to perform reliably for\nar X\niv :2\n20 3.\n13 28\n5v 2\n[ cs\n.S D\n] 2\npeople of very diverse backgrounds, who may express their feelings differently.\nIn order to tackle this difficult problem, various methods have been proposed. These frequently involve fusing multiple modalities in order to better judge the emotional state from complementary information [32]. Another common strategy is to make use of temporal information, since the emotional state fluctuates over time.\nFor the purpose of processing time series, recurrent neural networks (RNNs) continue to be popular. RNNs look at each element of the input sequentially and update their hidden state. Recent works in emotion recognition have also made use of networks based on self-attention and crossmodal attention [5, 15]. While self-attention finds relations between the elements of one sequence, cross-modal attention relates two sequences from different modalities to each other [41]. Compared to RNNs, attention-based network architectures have the advantage of allowing for parallel computation. However, adding recurrence may still improve an attention-based network [15].\nAlthough the recurrence and attention models widely applied to the multimodal fusion for affect recognition and sentiment analysis [3, 29, 38, 39], it is not very clear how their performance vary depending on the quality of input feature embeddings when modelling complex interactions among the modalities, particularly in end-to-end learning approaches. Specifically, to the best of our knowledge, not much attention is paid to comprehensively analysing the performance of RNNs and attention models based on the underlying CNN backbones\u2019 characteristics. To this end, we consider two commonly used CNN backbone models of two different complexity levels for extracting face image features: FaceNet based on InceptionResNetV1 architecture and MobileFaceNet based on MobileNetV2 architecture. Using the visual features extracted using these two CNN backbones and systematically tuning the hyperparameters of network design and optimisation, we comprehensively evaluate the performance of LSTM-RNNs, selfattention and cross-modal attention models on the task of audiovisual affect recognition.\nHerein, we present this comparative analysis of RNNs, self-attention and cross-modal attention as part of our entry to the third Affective Behavior in the Wild (ABAW) challenge. While similar comparisons have been performed, we focus our analysis specifically on the task of continuous emotion recognition in the wild. The Affwild2 dataset used in the challenge is the largest in the wild corpus annotated in terms of valence and arousal [22]. Its data presents many of the difficulties listed above, including a high diversity of subjects, varying illumination and occlusions, and frequently noisy audio or silence. We believe that it is beneficial to benchmark the algorithms on such a dataset.\nOur main contributions are:\n1. We investigate the impact of CNN backbones with different complexities on the performance of LSTMRNNs for audiovisual affect recognition in the wild, and show the effectiveness of end-to-end-learning.\n2. We contrast the performance of LSTM-RNNs with self-attention and cross-modal attention, and show that recurrent models can outperform attention models in combination with low-complexity CNN backbones.\nThe rest of the paper is structured as follows: We present our methodology in Sec. 3, and describe our experimental settings and results in Sec. 4. A discussion of the results follows in Sec. 5, and make suggestions for future work in Sec. 6. Finally, Sec. 7 concludes this paper."
        },
        {
            "heading": "2. Related Work",
            "text": "Recurrence vs. Attention for Sequence Modelling. To model the underlying temporal dynamics embedded in the continuous-time data, recurrent [14] and attention [43] mechanisms have been widely used. While the recurrence models rely on gated sequential propagation of temporal dynamics encoded into a latent state, the attention models bypass the sequential propagation of information and directly attend to the past inputs. Thus, the attention models can easily capture long-range temporal contingencies by circumventing the problem of vanishing gradients. Although, LSTM-RNNs [14] are designed to capture the long-range dependencies by controlling the information flow, they still fall short in practice due to their fixed dimensional latent state to hold the past information, unlike in the attention models. However, this advantage with attention models comes at the cost of poor (quadratic) scalability with the sequence length, which is not the case with RNNs. Furthermore, attention models can operate only within a fixed temporal context window whereas the RNNs can easily handle unbounded context [12].\nSome recent works [16, 30] made systematic efforts to understand the trade-offs between the recurrence and attention mechanisms. However, in the case of continuous-time multimodal fusion which requires modelling complex interactions among different modalities, not much is known about how their performance is influenced by some key design choices, for instance, the CNN backbone modelling complexity and the resultant input features quality. This observation motivates our attempt to study the impact of CNN backbones on the performance of LSTMs, self-attention and cross-modal attention models, by systematically tuning the hyper-parameters involved in the network architecture design and training optimisation. In-The-Wild Audiovisual Affect Recognition. The first affect in the wild challenge based on the Aff-wild dataset was introduced at CVPR 2017 [44]. In [21], the dataset and the challenge are described. Aff-wild has 298 videos\nsourced from YouTube. Shown in it are subjects reacting to a variety of stimuli, e.g. film trailers. Subsequently, the corpus was extended with additional videos, and renamed to Aff-wild2 dataset [22]. Aff-wild2 has 548 videos, with a total of about 2, 78 M frames. The total number of subjects is 455, 277 of them male. The dataset is annotated with three sets of labels: continuous affect (valence and arousal), basic expressions (six emotions and neutral), and facial action units (FAUs). 545 videos have annotations for valence and arousal.\nThe first ABAW challenge was held as a workshop at FG2020 [18]. It consisted of three sub-challenges for estimating valence-arousal (VA track), classifying facial expressions (EXPR track), and detecting 8 facial action units (AU track). The winning team of the VA track [7] relied on a multi-task learning approach. To deal with the problem of incomplete labels in Aff-wild2 data used for the first ABAW competition, i.e. not all samples being annotated for each task, Deng et al. [7] proposed a teacher-student framework. An ensemble of deep models was trained with semisupervised learning, where the teacher predicted missing labels to guide the student.\nIn 2021, the second ABAW challenge took place in conjunction with ICCV 2021 [24]. Compared to the previous year, the database had been extended with more annotations. The challenge tracks were identical, but the AU track now included 12 AUs. The winner of the VA track [8] was the same team as in the previous year, again utilising a multi-task teacher-student framework. The approach also included the prediction uncertainty of an ensemble of student models to further improve performance.\nSeveral multi-task learning models [7, 9, 26] effectively leveraged the availability of Aff-wild2 data jointly annotated with the labels of dimensional affect, categorical expressions, and AUs. A holistic multi-task, multi-domain network for facial emotion analysis named FaceBehaviorNet was developed on Aff-wild2 and validated in a crosscorpus setting in [19, 20, 23].\nBuilding on the success of attention mechanism [43] in sequence data modelling in recent years, cross-modal attention based audiovisual fusion has been widely applied to the emotion recognition tasks [11,25,33,45,46]. Unlike the aforementioned works that solely rely on the fusion of facial and vocal expressions for affect recognition, Antoniadis et al. [1] proposed to use the features of body and background visual context additionally."
        },
        {
            "heading": "3. Methodology",
            "text": "Since we want to compare fusion methods for timecontinuous emotion recognition, our method is based on deep neural networks operating on sequences of features extracted from the visual and audio modalities. We use the cropped and aligned faces from the videos as visual inputs\nand fixed-length clips as audio inputs. Our approach is illustrated in Fig. 1."
        },
        {
            "heading": "3.1. Visual Features",
            "text": "Visual features are extracted with the help of 2D-CNNs. We use pre-trained networks trained on facial recognition tasks. Specifically, we use FaceNet [37] based on the InceptionResnetv1 architecture, and trained on VGGFace2 [4]. Alternatively, we employ MobileFaceNet [6], a lightweight architecture designed for facial recognition in embedded devices. MobileFaceNet is built upon residual blocks used in the MobileNetv2 network [36] Its usefulness as a feature extractor for emotion recognition was demonstrated in [8]. Both CNNs return 512-dimensional feature embeddings. The FaceNet has approximately 27M parameters, while the MobileFaceNet has 0.99M parameters."
        },
        {
            "heading": "3.2. Audio Features",
            "text": "For audio feature extraction, we choose a 1D CNN network based on the architecture proposed by Zhao et al. [47]. The CNN encoder has 4 local feature learning blocks consisting of 1D convolutions and maxpooling layers. The kernel sizes and output channels are [3,3,3,3] and [64, 64, 128, 128]. The choice of this architecture is motivated by its low parameter count (about 88k) and proven effectiveness for speech emotion recognition on a number of corpora.\nWe use the RECOLA dataset [35], a corpus of spontaneous affective interactions between French speakers, to pre-train the audio network. For this purpose, we combine the 1D-CNN with a 2-layer LSTM and a fully connected output layer and train the model end-to-end using the the End2You toolkit 1 [42]. Then, the LSTM and output layers were removed to obtain the convolutional feature extractor. We then added a global average pooling layer at the end so the network returns 128-dimensional embeddings."
        },
        {
            "heading": "3.3. Sequence Modelling",
            "text": "Recurrence Models are widely used for sequential data modelling, whose fundamental strength lies in their ability to learn the underlying temporal context in the form of a hidden state i.e. ht = f(ht\u22121, ..). This approach based on maintaining the hidden states is a natural solution to model the sequential data that is irregularly sampled from an underlying continuous-time series phenomenon [12] such as dimensional affect recognition. However, the limitations of recurrence models in terms of capturing cross-modal interactions in multimodal temporal data, which is critical for audiovisual emotion recognition, is not very clear. In this work, we consider the canonical Long-Short Term Memory (LSTM) RNNs [14], using both unidirectional and bidirectional models, for a comprehensive evaluation on valence and arousal estimation from face and speech data.\n1https://github.com/end2you/end2you\nSelf-Attention (SA). Second, we use networks based on the Transformer architecture [43]. Specifically, we use multiheaded scaled dot-product attention blocks with feedforward networks as employed in the transformer encoder. The scaled dot-product attention is defined as:\nAttention(Q,K, V ) = softmax ( QKT\u221a dk ) V (1)\nMulti-head attention linearly projects the query, key and value pairs into different sub-spaces and performs attention on them in parallel, before recombining and projecting into the output dimension. It is defined as:\nMHA(Q,K, V ) = Concat (head1, ..., headn)WO where headi = Attention ( QWQi ,KW K i , V W V i ) (2) In order to fuse modalities within our models, we either use a simple concatenation of our feature embeddings, or a cross-modal fusion architecture. Cross-Modal Attention (CMA) Fusion is proposed in Tsai et al. [41] to implement the Multimodal Transformer network in which pair-wise attention modelling across different modalities is performed. On the task of discrete emotion recognition from multimodal signals, CMA demonstrate\nsuperior generalisation performance compared to LSTMRNNs [41]. However, when it comes to the continuous emotion recognition from multimodal data, the performance gains that CMA can achieve over the canonical RNNs is unclear. To delineate the trade-offs between the CMA and the other aforementioned sequence models, in this work we evaluate different CMA-based audiovisual fusion models. We implemented audiovisual CMA models by tailoring the multimodal transformer architecture2 which was originally designed for text, audio and visual modalities.\nOur cross-modal architecture is based on the crossmodal attention blocks introduced by [41]. In self-attention used in the transformer encoder, Q, K and V are identical. In the cross-modal attention however, the queries and the key, value pairs come from two different modalities, where Q is denoted as the target and K,V as the source respectively. It is similar to the transformer decoder, but does not involve self-attention. At each layer, the target modality is reinforced with the low-level information of the source modality [41].\nWhen employing concatenation of feature vectors, we pass the result through either a stack of recurrent layers or a self-attention stack. When using cross-modal fusion, we pass the features through two cross-modal blocks in paral-\n2https://github.com/yaohungt/Multimodal-Transformer\nlel, one of them using the audio features to attend to the visual features and the other vice versa. We then concatenate the outputs of the cross-modal blocks before passing them to a self-attention stack.\nWe use fully connected and 1D convolutional layers to reduce the dimension features returned by our extractor networks before passing them to our sequence models. When using 1D-CNNs with kernel size larger than 1, this also serves to encode the local temporal context. For the transformer networks, we add additional position embedding layers with fixed sinusoidal patterns, since they would otherwise not be able to distinguish the order of the sequence passed to them [43]."
        },
        {
            "heading": "3.4. Loss Functions",
            "text": "We use fully connected layers to return the outputs of our. Each model has two output heads. The first head has size 2 and is used for prediction of valence and arousal scores.\nWe use two losses for the regression head. The first is based on the concordance correlation coefficient (CCC) [27], which is defined as in Eq. (3). It measures the correlation between two sequences, and ranges between -1 and 1, where -1 means perfect anti-correlation, 0 means no correlation, and 1 means perfect correlation. The loss is calculated as 1\u2212 CCC.\nCCC(x, y) = 2 \u2217 cov(x, y)\n\u03c32x + \u03c3 2 y + (\u00b5x \u2212 \u00b5y)\n2 where cov(x, y) = \u2211 (x\u2212 \u00b5x) \u2217 (y \u2212 \u00b5y) (3)\nWe also compute the mean square error (MSE), which is defined as Eq. (4). The reasoning behind adding an additional regression loss is that CCC loss alone proved to be less stable during training in our experiments.\nMSE(x, y) = \u2211 (x\u2212 y)2 (4)\nIn addition to regressing the scores, we also add a classification head that predicts the category the scores belong to. Jointly estimating continuous and categorical emotions from faces has been shown to be effective for facial affect analysis in the wild [40]. While the Affwild2 dataset is annotated in terms of both continuous and categorical emotions, the rules of the ABAW challenge do not allow using multiple annotations for the valence-arousal track. Therefore, we discretise the labels, by dividing the twodimensional affect space into 24 sections. These are derived by plotting valence and arousal in polar coordinates, with 3 equidistant radial subdivisions and 8 angular subdivisions.\nCrossentropy loss is used as loss function for the classification head. Since the Affwild2 dataset is imbalanced towards positive arousal and valence, we weigh the logits to emphasise minority classes.\nOur total loss is thus composed of three terms. We add weights to the MSE and crossentropy losses to adjust their contribution, leading to our loss function Eq. (5)\nL = Lccc + \u03bbmse \u2217 Lmse + \u03bbce \u2217 Lce (5)"
        },
        {
            "heading": "4. Experiments and Results",
            "text": "We describe or experimental settings and the obtained results on the validation set of the challenge."
        },
        {
            "heading": "4.1. Dataset",
            "text": "We use subset of Affwild2 annotated for the ValenceArousal (VA) Estimation task. The training set consists of 341 videos, the validation set consists of 71 videos and the test set consists of 152 videos. Several videos have more than one person in them, those videos are annotated separately for each person and are considered like multiple videos. Frames are annotated with valence and arousal in the range [-1, 1].\nWe use the cropped and aligned faces from the videos provided by the challenge organisers. Some frames are annotated as invalid, after discarding them, we create sequences from the remaining frames. We use a fixed sequence length of 16 frames for our experiments. Audio clips are extracted at a fixed window length of 0.5s, centered at the frame timestamps. We convert the audio of the entire dataset to 16 kHz mono, 16 bit PCM.\nThe frame rate of the Affwild2 dataset is 30fps for the majority of videos. Thus, consecutive frames are very similar. In order to provide our model with more temporal information, one option would be to increase sequence length, at the cost of additional computational resources. We choose instead an approach similar to [26] and use dilated sampling, i.e. we select only 1 in N frames. With sequence length T , this gives a temporal context t of:\nt = N\n30 \u2217 T (6)\nIn order to not reduce the size of the training set, we also apply an interleaved sampling method to select the remaining frames.\nWe do not apply this dilated sampling method for the validation set. While this introduces some discrepancy with the training, it maintains equal conditions to the test set.\nThe images are resized to the the shape required by the CNN feature extractor. We use randomly affine transformations and changes in saturation, brightness and contrast as data augmentation on the images. We also apply gaussian noise to the audio frames."
        },
        {
            "heading": "4.2. Training",
            "text": "We implement out models in the PyTorch framework and train them on servers with Nvidia RTX3090 and A40 GPUs.\nPer model training, we allocate 40 CPUs and 40GB RAM in order to accelerate the loading of batches. The batch size is 64.\nModels are trained using the AdamW optimiser [28]. We apply cosine annealing with warm restarts as learning rate scheduling, setting it to restart after 200 steps.\nIn order to find the best configurations for our models, we perform extensive hyperparameter optimisation. We train our models in groups, choosing first the feature extractors and the general architecture (recurrent or transformer), then varying the architecture\u2019s parameters as well as the learning rate for our optimiser and the contributions of our losses. A listing of the hyperparameters used is given in.\nSince the potential number of hyperparameter combinations is very large, a simple grid search would be inefficient. Instead, we make use of a tuning algorithm to cover a larger number of choices efficiently. For this, we choose Ray Tune 3, a flexible tuning toolkit supports parallel training on multiple GPUs. We use the ASHA scheduling algorithm to quickly discover suitable configurations and stop trials early if they are not performing well.\nIn a first round of experiments, we freeze the layer weights of the feature extraction networks to limit the num-\n3https://www.ray.io/ray-tune\nber of trainable parameters. Then, we test end-to-end learning with the full set of parameters. For these experiments, we restrict the choice of the visual encoder network to MobileFaceNet to avoid overfitting."
        },
        {
            "heading": "4.3. Validation Results",
            "text": "The validation results for preliminary experiments on models with frozen feature extraction networks are reported in Tab. 1. We denote the three types of architectures employed as Audiovisual-[RNN, SA, CMA] for recurrent, self-attention, and cross-modal attention respectively. The second column specifies the feature extraction network used for visual information, as Inception or Mobile for InceptionResnetv1 and MobileFaceNet, respectively. For comparison, we also state the results of unimodal models trained with self-attention and RNN.\nIt can be seen from Sec. 4.3 that our audiovisual models outperform the challenge baseline by a wide margin.\nWe report the validation results of the best models per architecture, trained end-to-end, in Tab. 3. All models share the same feature encoders, i.e., MobileFaceNet and the 1D CNN pre-trained on RECOLA. The hyperparameter config-\nurations of the best models are given in Tab. 4. In addition, we report the number of parameters for the best performing audiovisual models to allow for comparison of computational costs."
        },
        {
            "heading": "5. Discussion",
            "text": "When judging performance, we analyse the mean value of CCC for valence and arousal, which is the metric used in the VA Track of the ABAW 2022 challenge. We first discuss how the choice of the visual CNN impacts the RNN models, and the impact of end-to-end learning. We then compare the performance of self-attention and cross-attention, before contrasting RNNs and attention models."
        },
        {
            "heading": "5.1. RNN performance",
            "text": "When using RNN as the sequence model, performance decreases significantly when replacing the FaceNet feature encoder with the less complex MobileFaceNet (0.413 to 0.378). At the same time, the number of parameters in the trainable part of the model increases sharply from 109 k to 4.4 M. We interpret this as the model having difficulty to learn valence and arousal effectively from the features returned by the smaller CNN.\nHowever, using the lightweight architecture together with end-to-end-learning presents a very different picture. When the feature extractors are fully trainable, the performance of the recurrent model was greatly increased, yielding an average CCC of 0.456. At the same time, the number of model parameters decreased, to merely 76 k in the sequence part. Examining the hyperparameter configuration of this winning model showed that it has a single, unidirectional LSTM layer, with a hidden dimension d = 64. We conclude from this that the RNN architecture is very efficient in learning representations of the emotional state if it is trained end-to-end in combination with shallow CNN encoders."
        },
        {
            "heading": "5.2. Comparing self-attention and cross-attention",
            "text": "When comparing the self-attention (SA) models with different visual CNNs, it can be seen that the model with MobileFaceNet performs better than the one with FaceNet, with average CCC scores of 0.389 and 0.374, respectively. At the same time, the size of the attention model is smaller for the architecture with MobileFaceNet (482 k parameters\ncompared to 765 k parameters). For the cross-modal attention (CMA) models, replacing FaceNet with MobileFaceNet also increases performance, from 0.378 to 0.392. However, the transformer network becomes much larger, going from 134 k parameters to 2.1 M parameters. As can be seen from these scores, the performance of self-attention and cross-attention appears to be similar if the feature extractors are frozen, with CMA performing slightly better.\nWhen using end-to-end learning, performance increases significantly for the self-attention model, with an average validation CCC of 0.450. At the same time, the number of parameters in the sequence part of the network shrinks to 193 k. The cross-attention model also benefits greatly from end-to-end learning, achieving a score of 0.44. The number of parameters in the sequence part of the model is 2.4 M.\nComparing the two best models trained end-to-end shows that the self-attention model outperforms the crossattention, while being significantly smaller. We hypothesise that the lower complexity of the self-attention model helped discover a more efficient architecture during end-toend training."
        },
        {
            "heading": "5.3. Comparison between RNN and attention for",
            "text": "sequence modeling\nWe now compare the performances of our RNN models and attention models directly based on the results discussed in the two previous sections. In the case the feature extractors are frozen, for the larger FaceNet, the RNN outperforms the attention models. If frozen MobileFaceNet is used, the attention models outperform RNN. With end-to-end learning, RNN beats both self-attention and cross-attention, while also having fewer parameters.\nWe conclude from this that our initial hypothesis that attention-based models consistently outperform RNNs for emotion recognition in the wild has not been confirmed. When end-to-end learning is used in combination with shallow CNNs for feature encoding, RNNs perform superior to the attention-based models investigated in this paper."
        },
        {
            "heading": "6. Outlook",
            "text": "We compared fusion performance using two CNNs of different sizes as visual feature extractors, while using a small 1D-CNN for extracting audio features. Another study could focus on choosing different audio networks, e.g. a larger model like VGGish [13], and comparing the effects.\nThe models used in this work had limited temporal context due to computational constraints. Future studies could extend towards greater sequence lengths to investigate how well the models capture long-term dependencies.\nOur analysis has focused on the average of valence and arousal as a metric, in order to judge the overall perfor-\nmance of the models. We leave the analysis of trade-offs between valence and arousal for future work."
        },
        {
            "heading": "7. Conclusion",
            "text": "On a wide range of sequence modelling tasks, attention models demonstrated superior generalisation performance than recurrent models in recent years. However, it is worth noting that the recurrent models have the natural ability to cope with the challenges in learning from time-continuous sequence data, by inferring the latent states with unbounded context, at least in principle. Therefore, in the case of continuous-time multimodal affect recognition, a recurrent neural network architecture may still be a natural choice to model the latent states of face and voice data and their interactions in a time-continuous manner. Extensive evaluation of LSTM-RNNs, self-attention and cross-modal attention on in-the-wild audiovisual affect recognition, suggests that attention models may not necessarily be the optimal choice to perform continuous-time multimodal information fusion."
        }
    ],
    "title": "Continuous-Time Audiovisual Fusion with Recurrence vs. Attention for In-The-Wild Affect Recognition",
    "year": 2022
}