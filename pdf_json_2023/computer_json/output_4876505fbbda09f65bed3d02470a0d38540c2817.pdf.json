{
    "abstractText": "Monocular scene understanding is a foundational component of autonomous systems. Within the spectrum of monocular perception topics, one crucial and useful task for holistic 3D scene understanding is semantic scene completion (SSC), which jointly completes semantic information and geometric details from RGB input. However, progress in SSC, particularly in large-scale street views, is hindered by the scarcity of high-quality datasets. To address this issue, we introduce SSCBench, a comprehensive benchmark that integrates scenes from widely used automotive datasets (e.g., KITTI-360, nuScenes, and Waymo). SSCBench follows an established setup and format in the community, facilitating the easy exploration of SSC methods in various street views. We benchmark models using monocular, trinocular, and point cloud input to assess the performance gap resulting from sensor coverage and modality. Moreover, we have unified semantic labels across diverse datasets to simplify cross-domain generalization testing. We commit to including more datasets and SSC models to drive further advancements in this field.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yiming Li"
        },
        {
            "affiliations": [],
            "name": "Sihang Li"
        },
        {
            "affiliations": [],
            "name": "Xinhao Liu"
        },
        {
            "affiliations": [],
            "name": "Moonjun Gong"
        },
        {
            "affiliations": [],
            "name": "Kenan Li"
        },
        {
            "affiliations": [],
            "name": "Nuo Chen"
        },
        {
            "affiliations": [],
            "name": "Zijun Wang"
        },
        {
            "affiliations": [],
            "name": "Zhiheng Li"
        },
        {
            "affiliations": [],
            "name": "Tao Jiang"
        },
        {
            "affiliations": [],
            "name": "Fisher Yu"
        },
        {
            "affiliations": [],
            "name": "Yue Wang"
        },
        {
            "affiliations": [],
            "name": "Hang Zhao"
        },
        {
            "affiliations": [],
            "name": "Zhiding Yu"
        },
        {
            "affiliations": [],
            "name": "Chen Feng"
        }
    ],
    "id": "SP:525f2fb048ff975941fc7c669c18edee97de58cb",
    "references": [
        {
            "authors": [
                "Mehmet Aygun",
                "Aljosa Osep",
                "Mark Weber",
                "Maxim Maximov",
                "Cyrill Stachniss",
                "Jens Behley",
                "Laura"
            ],
            "title": "LealTaix\u00e9. 4d panoptic lidar segmentation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Tim Bailey",
                "Hugh Durrant-Whyte"
            ],
            "title": "Simultaneous localization and mapping (slam): Part ii",
            "venue": "IEEE robotics & automation magazine,",
            "year": 2006
        },
        {
            "authors": [
                "Jens Behley",
                "Martin Garbade",
                "Andres Milioto",
                "Jan Quenzel",
                "Sven Behnke",
                "Cyrill Stachniss",
                "Jurgen Gall"
            ],
            "title": "Semantickitti: A dataset for semantic scene understanding of lidar sequences",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Jens Behley",
                "Andres Milioto",
                "Cyrill Stachniss"
            ],
            "title": "A benchmark for lidar-based panoptic segmentation based on kitti",
            "venue": "IEEE International Conference on Robotics and Automation (ICRA),",
            "year": 2021
        },
        {
            "authors": [
                "Shariq Farooq Bhat",
                "Ibraheem Alhashim",
                "Peter Wonka"
            ],
            "title": "Adabins: Depth estimation using adaptive bins",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Sean L Bowman",
                "Nikolay Atanasov",
                "Kostas Daniilidis",
                "George J Pappas"
            ],
            "title": "Probabilistic data association for semantic slam",
            "venue": "IEEE international conference on robotics and automation (ICRA),",
            "year": 2017
        },
        {
            "authors": [
                "Holger Caesar",
                "Varun Bankiti",
                "Alex H Lang",
                "Sourabh Vora",
                "Venice Erin Liong",
                "Qiang Xu",
                "Anush Krishnan",
                "Yu Pan",
                "Giancarlo Baldan",
                "Oscar Beijbom"
            ],
            "title": "nuscenes: A multimodal dataset for autonomous driving",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Anh-Quan Cao",
                "Raoul de Charette"
            ],
            "title": "Monoscene: Monocular 3d semantic scene completion",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Ming-Fang Chang",
                "John Lambert",
                "Patsorn Sangkloy",
                "Jagjeet Singh",
                "Slawomir Bak",
                "Andrew Hartnett",
                "De Wang",
                "Peter Carr",
                "Simon Lucey",
                "Deva Ramanan"
            ],
            "title": "Argoverse: 3d tracking and forecasting with rich maps",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Chao Chen",
                "Xinhao Liu",
                "Yiming Li",
                "Li Ding",
                "Chen Feng"
            ],
            "title": "Deepmapping2: Self-supervised large-scale lidar map optimization",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Ran Cheng",
                "Christopher Agia",
                "Yuan Ren",
                "Xinhai Li",
                "Liu Bingbing"
            ],
            "title": "S3cnet: A sparse semantic scene completion network for lidar point clouds",
            "venue": "In Conference on Robot Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Ran Cheng",
                "Ryan Razani",
                "Ehsan Taghavi",
                "Enxu Li",
                "Bingbing Liu"
            ],
            "title": "2-s3net: Attentive feature fusion with adaptive feature selection for sparse semantic segmentation network",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Tiago Cortinhal",
                "George Tzelepis",
                "Eren Erdal Aksoy"
            ],
            "title": "Salsanext: Fast, uncertainty-aware semantic segmentation of lidar point clouds",
            "venue": "In Proceedings of the Advances in Visual Computing,",
            "year": 2020
        },
        {
            "authors": [
                "Carlos A Diaz-Ruiz",
                "Youya Xia",
                "Yurong You",
                "Jose Nino",
                "Junan Chen",
                "Josephine Monica",
                "Xiangyu Chen",
                "Katie Luo",
                "Yan Wang",
                "Marc Emond"
            ],
            "title": "Ithaca365: Dataset and driving perception under repeated and challenging weather conditions",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Xingshuai Dong",
                "Matthew A Garratt",
                "Sreenatha G Anavatti",
                "Hussein A Abbass"
            ],
            "title": "Towards real-time monocular depth estimation for robotics: A survey",
            "venue": "IEEE Transactions on Intelligent Transportation Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Markus Enzweiler",
                "Dariu M Gavrila"
            ],
            "title": "Monocular pedestrian detection: Survey and experiments",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2008
        },
        {
            "authors": [
                "Scott Ettinger",
                "Shuyang Cheng",
                "Benjamin Caine",
                "Chenxi Liu",
                "Hang Zhao",
                "Sabeek Pradhan",
                "Yuning Chai",
                "Ben Sapp",
                "Charles R Qi",
                "Yin Zhou"
            ],
            "title": "Large scale interactive motion forecasting for autonomous driving: The waymo open motion dataset",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Whye Kit Fong",
                "Rohit Mohan",
                "Juana Valeria Hurtado",
                "Lubing Zhou",
                "Holger Caesar",
                "Oscar Beijbom",
                "Abhinav Valada"
            ],
            "title": "Panoptic nuscenes: A large-scale benchmark for lidar panoptic segmentation and tracking",
            "venue": "IEEE Robotics and Automation Letters,",
            "year": 2022
        },
        {
            "authors": [
                "Jannik Fritsch",
                "Tobias Kuehnl",
                "Andreas Geiger"
            ],
            "title": "A new performance measure and evaluation benchmark for road detection algorithms",
            "venue": "In International Conference on Intelligent Transportation Systems (ITSC),",
            "year": 2013
        },
        {
            "authors": [
                "Andreas Geiger",
                "Philip Lenz",
                "Raquel Urtasun"
            ],
            "title": "Are we ready for autonomous driving? the kitti vision benchmark suite",
            "venue": "In 2012 IEEE conference on computer vision and pattern recognition,",
            "year": 2012
        },
        {
            "authors": [
                "Andreas Geiger",
                "Philip Lenz",
                "Christoph Stiller",
                "Raquel Urtasun"
            ],
            "title": "Vision meets robotics: The kitti dataset",
            "venue": "International Journal of Robotics Research (IJRR),",
            "year": 2013
        },
        {
            "authors": [
                "Junru Gu",
                "Chenxu Hu",
                "Tianyuan Zhang",
                "Xuanyao Chen",
                "Yilun Wang",
                "Yue Wang",
                "Hang Zhao"
            ],
            "title": "Vip3d: End-to-end visual trajectory prediction via 3d agent queries",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Fangzhou Hong",
                "Hui Zhou",
                "Xinge Zhu",
                "Hongsheng Li",
                "Ziwei Liu"
            ],
            "title": "Lidar-based panoptic segmentation via dynamic shifting network",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Hou-Ning Hu",
                "Yung-Hsu Yang",
                "Tobias Fischer",
                "Trevor Darrell",
                "Fisher Yu",
                "Min Sun"
            ],
            "title": "Monocular quasi-dense 3d object tracking",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Qingyong Hu",
                "Bo Yang",
                "Linhai Xie",
                "Stefano Rosa",
                "Yulan Guo",
                "Zhihua Wang",
                "Niki Trigoni",
                "Andrew Markham"
            ],
            "title": "Randla-net: Efficient semantic segmentation of large-scale point clouds",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Yihan Hu",
                "Jiazhi Yang",
                "Li Chen",
                "Keyu Li",
                "Chonghao Sima",
                "Xizhou Zhu",
                "Siqi Chai",
                "Senyao Du",
                "Tianwei Lin",
                "Wenhai Wang"
            ],
            "title": "Planning-oriented autonomous driving",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Xinyu Huang",
                "Xinjing Cheng",
                "Qichuan Geng",
                "Binbin Cao",
                "Dingfu Zhou",
                "Peng Wang",
                "Yuanqing Lin",
                "Ruigang Yang"
            ],
            "title": "The apolloscape dataset for autonomous driving",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition workshops,",
            "year": 2018
        },
        {
            "authors": [
                "Xinyu Huang",
                "Peng Wang",
                "Xinjing Cheng",
                "Dingfu Zhou",
                "Qichuan Geng",
                "Ruigang Yang"
            ],
            "title": "The apolloscape open dataset for autonomous driving and its application",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Yuanhui Huang",
                "Wenzhao Zheng",
                "Yunpeng Zhang",
                "Jie Zhou",
                "Jiwen Lu"
            ],
            "title": "Tri-perspective view for vision-based 3d semantic occupancy prediction",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "R. Kesten",
                "M. Usman",
                "J. Houston",
                "T. Pandya",
                "K. Nadhamuni",
                "A. Ferreira",
                "M. Yuan",
                "B. Low",
                "A. Jain",
                "P. Ondruska",
                "S. Omari",
                "S. Shah",
                "A. Kulkarni",
                "A. Kazakova",
                "C. Tao",
                "L. Platinsky",
                "W. Jiang",
                "V. Shet"
            ],
            "title": "Woven planet perception dataset 2020",
            "venue": "https://woven.toyota/en/perception-dataset,",
            "year": 2019
        },
        {
            "authors": [
                "Lars Kreuzberg",
                "Idil Esen Zulfikar",
                "Sabarinath Mahadevan",
                "Francis Engelmann",
                "Bastian Leibe"
            ],
            "title": "4d-stop: Panoptic segmentation of 4d lidar using spatio-temporal object proposal generation and aggregation",
            "venue": "In European Conference on Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Jiale Li",
                "Hang Dai",
                "Yong Ding"
            ],
            "title": "Self-distillation for robust lidar semantic segmentation in autonomous driving",
            "venue": "In European Conference on Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Yiming Li",
                "Congcong Wen",
                "Felix Juefei-Xu",
                "Chen Feng"
            ],
            "title": "Fooling lidar perception via adversarial trajectory perturbation",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Yiming Li",
                "Dekun Ma",
                "Ziyan An",
                "Zixun Wang",
                "Yiqi Zhong",
                "Siheng Chen",
                "Chen Feng"
            ],
            "title": "V2x-sim: Multi-agent collaborative perception dataset and benchmark for autonomous driving",
            "venue": "IEEE Robotics and Automation Letters,",
            "year": 2022
        },
        {
            "authors": [
                "Yiming Li",
                "Zhiding Yu",
                "Christopher Choy",
                "Chaowei Xiao",
                "Jose M Alvarez",
                "Sanja Fidler",
                "Chen Feng",
                "Anima Anandkumar"
            ],
            "title": "Voxformer: Sparse voxel transformer for camera-based 3d semantic scene completion",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Yiyi Liao",
                "Jun Xie",
                "Andreas Geiger"
            ],
            "title": "Kitti-360: A novel dataset and benchmarks for urban scene understanding in 2d and 3d",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Daniel Maturana",
                "Sebastian Scherer"
            ],
            "title": "Voxnet: A 3d convolutional neural network for real-time object recognition",
            "venue": "In 2015 IEEE/RSJ international conference on intelligent robots and systems (IROS),",
            "year": 2015
        },
        {
            "authors": [
                "Moritz Menze",
                "Andreas Geiger"
            ],
            "title": "Object scene flow for autonomous vehicles",
            "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2015
        },
        {
            "authors": [
                "Andres Milioto",
                "Ignacio Vizzo",
                "Jens Behley",
                "Cyrill Stachniss"
            ],
            "title": "Rangenet++: Fast and accurate lidar semantic segmentation",
            "venue": "In IEEE/RSJ international conference on intelligent robots and systems (IROS),",
            "year": 2019
        },
        {
            "authors": [
                "Quang-Hieu Pham",
                "Pierre Sevestre",
                "Ramanpreet Singh Pahwa",
                "Huijing Zhan",
                "Chun Ho Pang",
                "Yuda Chen",
                "Armin Mustafa",
                "Vijay Chandrasekhar",
                "Jie Lin"
            ],
            "title": "A 3d dataset: Towards autonomous driving in challenging environments",
            "venue": "IEEE International Conference on Robotics and Automation (ICRA),",
            "year": 2020
        },
        {
            "authors": [
                "Matthew Pitropov",
                "Danson Evan Garcia",
                "Jason Rebello",
                "Michael Smart",
                "Carlos Wang",
                "Krzysztof Czarnecki",
                "Steven Waslander"
            ],
            "title": "Canadian adverse driving conditions dataset",
            "venue": "The International Journal of Robotics Research,",
            "year": 2021
        },
        {
            "authors": [
                "Charles R Qi",
                "Hao Su",
                "Kaichun Mo",
                "Leonidas J Guibas"
            ],
            "title": "Pointnet: Deep learning on point sets for 3d classification and segmentation",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Shi Qiu",
                "Saeed Anwar",
                "Nick Barnes"
            ],
            "title": "Semantic segmentation for real point cloud scenes via bilateral augmentation and adaptive fusion",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Christoph B Rist",
                "David Emmerichs",
                "Markus Enzweiler",
                "Dariu M Gavrila"
            ],
            "title": "Semantic scene completion using local deep implicit functions on lidar data",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Luis Roldao",
                "Raoul de Charette",
                "Anne Verroust-Blondet"
            ],
            "title": "Lmscnet: Lightweight multiscale 3d semantic completion",
            "venue": "In 2020 International Conference on 3D Vision (3DV), pp. 111\u2013119",
            "year": 2020
        },
        {
            "authors": [
                "Luis Roldao",
                "Raoul De Charette",
                "Anne Verroust-Blondet"
            ],
            "title": "3d semantic scene completion: a survey",
            "venue": "International Journal of Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Faranak Shamsafar",
                "Samuel Woerz",
                "Rafia Rahim",
                "Andreas Zell"
            ],
            "title": "Mobilestereonet: Towards lightweight deep networks for stereo matching",
            "venue": "In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Shuran Song",
                "Fisher Yu",
                "Andy Zeng",
                "Angel X Chang",
                "Manolis Savva",
                "Thomas Funkhouser"
            ],
            "title": "Semantic scene completion from a single depth image",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Pei Sun",
                "Henrik Kretzschmar",
                "Xerxes Dotiwalla",
                "Aurelien Chouard",
                "Vijaysai Patnaik",
                "Paul Tsui",
                "James Guo",
                "Yin Zhou",
                "Yuning Chai",
                "Benjamin Caine"
            ],
            "title": "Scalability in perception for autonomous driving: Waymo open dataset",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Haotian Tang",
                "Zhijian Liu",
                "Shengyu Zhao",
                "Yujun Lin",
                "Ji Lin",
                "Hanrui Wang",
                "Song Han"
            ],
            "title": "Searching efficient 3d architectures with sparse point-voxel convolution",
            "venue": "In European conference on computer vision,",
            "year": 2020
        },
        {
            "authors": [
                "Sebastian Thrun"
            ],
            "title": "Probabilistic robotics",
            "venue": "Communications of the ACM,",
            "year": 2002
        },
        {
            "authors": [
                "Xiaoyu Tian",
                "Tao Jiang",
                "Longfei Yun",
                "Yue Wang",
                "Yilun Wang",
                "Hang Zhao"
            ],
            "title": "Occ3d: A large-scale 3d occupancy prediction benchmark for autonomous driving",
            "venue": "arXiv preprint arXiv:2304.14365,",
            "year": 2023
        },
        {
            "authors": [
                "Tai Wang",
                "Jiangmiao Pang",
                "Dahua Lin"
            ],
            "title": "Monocular 3d object detection with depth from motion",
            "venue": "In European Conference on Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Xiaofeng Wang",
                "Zheng Zhu",
                "Wenbo Xu",
                "Yunpeng Zhang",
                "Yi Wei",
                "Xu Chi",
                "Yun Ye",
                "Dalong Du",
                "Jiwen Lu",
                "Xingang Wang"
            ],
            "title": "Openoccupancy: A large scale benchmark for surrounding semantic occupancy perception",
            "venue": "arXiv preprint arXiv:2303.03991,",
            "year": 2023
        },
        {
            "authors": [
                "Pengchuan Xiao",
                "Zhenlei Shao",
                "Steven Hao",
                "Zishuo Zhang",
                "Xiaolin Chai",
                "Judy Jiao",
                "Zesong Li",
                "Jian Wu",
                "Kai Sun",
                "Kun Jiang"
            ],
            "title": "Pandaset: Advanced sensor suite dataset for autonomous driving",
            "venue": "IEEE International Intelligent Transportation Systems Conference (ITSC),",
            "year": 2021
        },
        {
            "authors": [
                "Chenfeng Xu",
                "Bichen Wu",
                "Zining Wang",
                "Wei Zhan",
                "Peter Vajda",
                "Kurt Keutzer",
                "Masayoshi Tomizuka"
            ],
            "title": "Squeezesegv3: Spatially-adaptive convolution for efficient point-cloud segmentation",
            "venue": "In Proceedings of the European Conference on Computer Vision (ECCV),",
            "year": 2020
        },
        {
            "authors": [
                "Runsheng Xu",
                "Xin Xia",
                "Jinlong Li",
                "Hanzhao Li",
                "Shuo Zhang",
                "Zhengzhong Tu",
                "Zonglin Meng",
                "Hao Xiang",
                "Xiaoyu Dong",
                "Rui Song"
            ],
            "title": "V2v4real: A real-world large-scale dataset for vehicle-to-vehicle cooperative perception",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Xu Yan",
                "Jiantao Gao",
                "Jie Li",
                "Ruimao Zhang",
                "Zhen Li",
                "Rui Huang",
                "Shuguang Cui"
            ],
            "title": "Sparse single sweep lidar point cloud segmentation via learning contextual shape priors from scene completion",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Li Yi",
                "Vladimir G Kim",
                "Duygu Ceylan",
                "I-Chao Shen",
                "Mengyan Yan",
                "Hao Su",
                "Cewu Lu",
                "Qixing Huang",
                "Alla Sheffer",
                "Leonidas Guibas"
            ],
            "title": "A scalable active framework for region annotation in 3d shape collections",
            "venue": "ACM Transactions on Graphics (ToG),",
            "year": 2016
        },
        {
            "authors": [
                "Georges Younes",
                "Daniel Asmar",
                "Elie Shammas",
                "John Zelek"
            ],
            "title": "Keyframe-based monocular slam: design, survey, and future directions",
            "venue": "Robotics and Autonomous Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Weihao Yuan",
                "Xiaodong Gu",
                "Zuozhuo Dai",
                "Siyu Zhu",
                "Ping Tan"
            ],
            "title": "Newcrfs: Neural window fully-connected crfs for monocular depth estimation",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Xiaoshuai Zhang",
                "Abhijit Kundu",
                "Thomas Funkhouser",
                "Leonidas Guibas",
                "Hao Su",
                "Kyle Genova"
            ],
            "title": "Nerflets: Local radiance fields for efficient structure-aware 3d scene representation from 2d supervision",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Yunpeng Zhang",
                "Zheng Zhu",
                "Dalong Du"
            ],
            "title": "Occformer: Dual-path transformer for vision-based 3d semantic occupancy prediction",
            "venue": "In Proceedings of the IEEE International Conference on Computer Vision (ICCV),",
            "year": 2023
        },
        {
            "authors": [
                "Yin Zhou",
                "Oncel Tuzel"
            ],
            "title": "Voxelnet: End-to-end learning for point cloud based 3d object detection",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Zixiang Zhou",
                "Yang Zhang",
                "Hassan Foroosh"
            ],
            "title": "Panoptic-polarnet: Proposal-free lidar point cloud panoptic segmentation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Xinge Zhu",
                "Hui Zhou",
                "Tai Wang",
                "Fangzhou Hong",
                "Yuexin Ma",
                "Wei Li",
                "Hongsheng Li",
                "Dahua Lin"
            ],
            "title": "Cylindrical and asymmetrical 3d convolution networks for lidar segmentation",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Understanding 3D scenes from a single RGB image is crucial and meaningful in vision and robotics, with monocular perception tasks like object detection (Wang et al., 2022), tracking (Hu et al., 2022), and depth estimation (Yuan et al., 2022) garnering significant attention. The emerging field of 3D semantic scene completion (SSC) (Roldao et al., 2022) seeks to jointly infer complete 3D semantics and geometry from a sparse and partial observation (e.g., an RGB image). The resulting volumetric representation seamlessly integrates occupancy and semantic information, facilitating robotic scene understanding and planning capabilities in street views.\nOne critical challenge in SSC is to generate accurate ground truth labels, especially in street views. Given the current limitations of 3D sensing technology, achieving a perfectly comprehensive 3D representation is impossible. The pioneering SemanticKITTI benchmark (Behley et al., 2019) proposes to leverage the temporal information through the aggregation of different LiDAR sweeps, which can effectively reveal previously occluded 3D surfaces. Meanwhile, it excludes 3D voxels not observed from all viewpoints during driving. Consequently, SemanticKITTI provides relatively comprehensive and accurate ground truth labels for SSC tasks.\nWhile SemanticKITTI is a valuable resource for learning sparse-to-dense mapping, its limited scale and diversity impede the development of more powerful and generalizable SSC models. Another significant limitation of SemanticKITTI is the omission of dynamic objects during ground truth generation, resulting in inaccurate labels. Hence, there is an urgent need for a large-scale SSC dataset with reliable ground truth to advance learning-based scene understanding in street views.\nTo this end, we introduce SSCBench, a large-scale benchmark comprising diverse street views sourced from well-established automotive datasets, including KITTI-360 (Liao et al., 2022), nuScenes (Caesar et al., 2020), and Waymo (Sun et al., 2020), as illustrated in Fig. 1. To enhance label accuracy, we utilize the 3D bounding box labels provided in these datasets to synchronize measurements of dynamic objects. Our key features include (a) accessibility: we provide datasets in a format\n\u2217Equal contribution \u2020The corresponding author is Chen Feng cfeng@nyu.edu\nar X\niv :2\n30 6.\n09 00\n1v 2\n[ cs\n.C V\n] 3\n0 Se\np 20\n23\ncompatible with SemanticKITTI, facilitating seamless usage within the community; (b) large scale: we offer an extensive dataset with \u223c8 times more frames than SemanticKITTI, encompassing diverse geographic locations across six cities; (c) comprehensiveness: we mainly focus on SSC methods with monocular input. Additionally, we utilize trinocular input to compare the single-view and panoramic-view methods and use point cloud input to show the gap between camera-based and LiDAR-based methods. Furthermore, we have unified semantic labels across different datasets in SSCBench, facilitating cross-domain generalization experiments. We plan to continually incorporate novel automotive datasets and SSC algorithms to drive further advancements in the field."
        },
        {
            "heading": "2 RELATED WORKS",
            "text": "Monocular Perception and 3D Semantic Scene Completion. The simplicity, efficiency, affordability, and accessibility of monocular cameras have made monocular perception a focal point of attention in the vision and robotics community. This has resulted in extensive research into various tasks, including depth estimation (Dong et al., 2022), 3D object detection and tracking (Enzweiler & Gavrila, 2008), as well as localization and mapping (Younes et al., 2017). Song et al. (2017) introduce the concept of monocular 3D semantic scene completion (SSC), which seeks to reconstruct and complete the semantics as well as geometry within a 3D volume from a single depth image. However, they only consider the bounded indoor scenarios due to the lack of outdoor datasets. Behley et al. (2019) build the first outdoor dataset based on KITTI (Geiger et al., 2012) for 3D semantic scene completion in street views. Existing approaches usually depend on 3D inputs, such as LiDAR point clouds (Roldao et al., 2020; Cheng et al., 2021a; Rist et al., 2021; Yan et al., 2021), while recent monocular vision-based solutions also emerge (Cao & de Charette, 2022; Li et al., 2023). However, the development of outdoor SSC is hindered by the lack of datasets, with SemanticKITTI (Behley et al., 2019) being the only dataset supporting SSC in street views. Building diverse datasets is imperative to unlock the full potential of SSC for autonomous systems.\nPoint Cloud Segmentation in Street Views. 3D LiDAR segmentation aims to assign point-wise semantic labels for point clouds, including a range of specific tasks, like LiDAR semantic (Hu et al., 2020; Qiu et al., 2021; Milioto et al., 2019; Xu et al., 2020; Cortinhal et al., 2020), panoptic (Zhou et al., 2021; Hong et al., 2021; Behley et al., 2021), and 4D panoptic segmentation (Kreuzberg et al.,\n2022; Aygun et al., 2021). In this field, point-based methods, stemming from PointNet++ (Qi et al., 2017), perform well on small synthetic point cloud (Yi et al., 2016) rather than sparse LiDAR point cloud, with sampling and gathering disordered neighbors. Voxel-based approaches (Zhu et al., 2021; Tang et al., 2020; Li et al., 2022a; Cheng et al., 2021b) process point clouds by initially partitioning 3D space into voxels through Cartesian coordinates. Note that 3D LiDAR segmentation aims to classify and understand the scenes based on raw LiDAR scans, while 3D semantic scene completion includes the completion of occluded areas, with input of camera or LiDAR.\nAutonomous Driving Dataset and Benchmark. Autonomous driving research thrives on highquality datasets, which serve as the lifeblood for training and evaluating perception (Caesar et al., 2020), prediction (Ettinger et al., 2021), and planning algorithms (Hu et al., 2023). In 2012, the pioneering KITTI dataset sparked a revolution in autonomous driving research, unlocking a multitude of tasks including object detection, tracking, mapping, and optical/depth estimation (Geiger et al., 2012; 2013; Fritsch et al., 2013; Menze & Geiger, 2015; Chen et al., 2023). Since then, the research community has embraced the challenge, giving rise to a wealth of datasets. These datasets push the boundaries of autonomous driving research by addressing challenges posed by multimodal fusion (Caesar et al., 2020), multi-tasking learning (Huang et al., 2018; Liao et al., 2022), adverse weather (Pitropov et al., 2021), collaborative driving (Agarwal et al., 2020; Li et al., 2022b; Xu et al., 2023), repeated driving (Diaz-Ruiz et al., 2022), and dense traffic scenarios (Pham et al., 2020; Xiao et al., 2021), etc. There are several impactful and widely-used driving datasets such as KITTI-360 (Liao et al., 2022), nuScenes (Caesar et al., 2020), and Waymo (Sun et al., 2020). They provide LiDAR and camera recordings as well as point cloud semantics and bounding annotations, as summarized in Tab. 1. Therefore, we can create accurate ground truth labels for SSC by aggregating multiple semantic point clouds and leveraging the 3D boxes to handle dynamic objects.\nOcc3D and OpenOccupancy. We compare SSCBench with the concurrent relevant work Occ3D (Tian et al., 2023). The differences lie in: (a) setup: Occ3D uses surrounding-view images as input, and only considers the reconstruction of 3D voxels visible to the camera. SSCBench considers a more challenging yet meaningful setup (also a well-established one): how to reconstruct and complete 3D semantics in both visible and occluded areas only with monocular visual input. This task requires reasoning about temporal information and 3D geometric relationships to get rid of the limited field of view; (b) scale: SSCBench provides more datasets than Occ3D and plans to add more due to the abundance of monocular driving recordings; (c) accessibility: we inherit the widely-used setup from the pioneer KITTI, thus making SSCBench more accessible to the community; (d) comprehensiveness: we benchmark SSC methods with monocular, trinocular, and point cloud input and provide unified labels for cross-domain generalization tests. Another relevant benchmark, OpenOccupancy (Wang et al., 2023), exhibits similar differences, notably its exclusive use of the nuScenes dataset (Caesar et al., 2020), which results in a limitation of diversity."
        },
        {
            "heading": "3 DATASET CURATION",
            "text": ""
        },
        {
            "heading": "3.1 REVISIT OF SEMANTICKITTI",
            "text": "SemanticKITTI (Behley et al., 2019) extends the odometry dataset of the KITTI vision benchmark (Geiger et al., 2012) by providing point-wise semantic annotations for 22 driving sequences in Karlsruhe, Germany. SemanticKITTI not only supports 3D semantic segmentation but also serves as the first outdoor SSC benchmark. Similar to indoor SSC (Song et al., 2017), SemanticKITTI uses voxelized 3D representation widely employed in robotics such as occupancy grid mapping (Thrun, 2002). SemanticKITTI generates ground truth labels through voxelization of a dense semantic point cloud given by rigid registration of multiple LiDAR scans.\nSemanticKITTI has two limitations. First, rigid registration with sensor poses can only handle measurements for static environments, resulting in traces produced by dynamic objects such as moving cars as shown in Fig. 2, which can confuse the 3D representation learning (Rist et al., 2021). Secondly, it is constrained by the limited scale and lack of diverse geographical coverage. The data collection is confined to a single city, resulting in training, validation, and test sets composed of 3,834, 815, and 3,992 frames, respectively, amounting to a total of 8,641 frames. However, this falls short of the large-scale benchmark necessary for comprehensive evaluation and generalization in the field."
        },
        {
            "heading": "3.2 SSCBENCH",
            "text": "We aim to establish a large-scale SSC benchmark in street views that facilitates the training of robust and generalizable SSC models. To achieve this, we harness well-established and widely-used datasets and integrate them into a unified setup and format. Overall, our SSCBench, consisting of three subsets, includes 38,562 frames for training, 15,798 frames for validation, and 12,553 frames for testing respectively, amounting totally to 66,913 frames (\u223c67K), which greatly exceeds the scale of SemanticKITTI mentioned above by \u223c7.7 times. In the following, we introduce three carefully designed datasets, all based on existing data sources, that collectively contribute to our SSCBench.\nSSCBench-KITTI-360. KITTI-360 (Liao et al., 2022) represents a significant advancement in autonomous driving research, building upon the renowned KITTI dataset (Geiger et al., 2012). It introduces an enriched data collection framework with diverse sensor modalities as well as panoramic viewpoints (a perspective stereo camera plus a pair of fisheye cameras) and provides comprehensive annotations including consistent 2D and 3D semantic instance labels as well as 3D bounding primitives. The dense and coherent labels not only support established tasks such as segmentation and detection but also enable novel applications like semantic SLAM (Bowman et al., 2017) and novel view synthesis (Zhang et al., 2023a). While KITTI-360 includes point cloud-based semantic scene completion, the prevalent methodology for SSC remains centered around voxelized representations (Roldao et al., 2022), which exhibit broader applicability in robotics.\nRemark. KITTI-360 covers a driving distance of 73.7km, comprising 300K images and 80K laser scans. While adhering to KITTI\u2019s forward-facing camera setup, it offers greater geographical diversity\nand demonstrates minimal trajectory overlap with KITTI. Leveraging the open-source training and validation set, we build SSCBench-KITTI-360 consisting of 9 long sequences. To reduce redundancy, we sample every 5 frames following the SemanticKITTI SSC benchmark. The training set includes 8,487 frames from scenes 00, 02-05, 07, and 10, while the validation set comprises 1,812 frames from scene 06. The testing set comprises 2,566 frames from scene 09. In total, the dataset contains 12,865 (\u223c13K) frames, surpassing the scale of SemanticKITTI by \u223c1.5 times. SSCBench-nuScenes. Unlike KITTI\u2019s forward-facing camera setup, nuScenes (Caesar et al., 2020) captures a complete 360-degree view around the ego vehicle. It provides a diverse range of multimodal sensory data, including camera images, LiDAR point clouds, and radar data, gathered in Boston and Singapore. nuScenes offers meticulous annotations for complex urban driving scenarios, including diverse weather conditions, construction zones, and varying illumination. Later on, panoptic nuScenes (Fong et al., 2022) extends the original nuScenes dataset with semantic and instance labels. With comprehensive metrics and evaluation protocols, nuScenes is widely employed in autonomous driving research (Gu et al., 2023; Hu et al., 2023; Li et al., 2021; Huang et al., 2023).\nRemark. The nuScenes dataset consists of 1K 20-second scenes with labels provided only for the training and validation set, totaling 850 scenes. From the available 850 scenes, we allocate 500 scenes for training, 200 scenes for validation, and 150 scenes for testing. This distribution results in 20,064 frames for training, 8,050 frames for validation, and 5,949 frames for testing, totaling 34,078 frames (\u223c34K). This scale is approximately four times that of SemanticKITTI. As nuScenes only provides annotations for keyframes at a frequency of 2Hz, there is no downsampling in SSCBench-nuScenes.\nSSCBench-Waymo. The Waymo dataset (Sun et al., 2020), collected from various locations in the US, offers a large-scale collection of multimodal sensor recordings. Waymo provides 5 cameras with a combined horizontal field of view of \u223c230 degrees, slightly smaller than nuScenes. The data is captured in diverse conditions across multiple cities, including San Francisco, Phoenix, and Mountain View, ensuring broad geographical coverage within each city. It consists of 1000 scenes for training and validation, as well as 150 scenes for testing, with each scene spanning 20 seconds.\nRemark. To construct SSCBench-Waymo, we utilize the open-source training and validation scenes and redistribute them into sets of 500, 298, and 202 scenes for training, validation, and testing, respectively. In order to reduce redundancy and training time for our benchmark, we downsample the original data by a factor of 10. This downsampling results in a training set of 10,011 frames, a validation set of 5,936 frames, and a test set of 4,038 frames, totaling 19,985 frames (\u223c20K)."
        },
        {
            "heading": "3.3 CONSTRUCTION PIPELINE",
            "text": "Prerequisites. To establish SSCBench, a driving dataset with multimodal recordings is required for LiDAR-based or camera-based SSC. The dataset should include sequentially collected 3D LiDAR point clouds with accurate sensor poses for geometry completion, per-point semantic annotations for semantic scene understanding, and 3D bounding annotations to handle dynamic instances.\nAggregation of Point Clouds. To generate a complete representation, our approach involves superimposing an extensive set of laser scans within a defined region in front of the vehicle. In short sequences like nuScenes and Waymo, we utilize future scans with measurements from the corresponding region to create a dense semantic point cloud. In long sequences like KITTI-360, which feature multiple loop closures, we incorporate all spatial neighboring point clouds in addition to the temporal neighborhood. Accurate sensor poses, provided by advanced SLAM systems (Bailey & Durrant-Whyte, 2006), greatly facilitate the aggregation of point clouds for the static environment. As for dynamic objects, we avoid the spatial-temporal tubes by synchronization. We utilize the instance label to transform dynamic objects to their spatial alignment within the current frame. As shown in Fig. 2, the spatial-temporal tubes are removed and the objects have a denser shape.\nVoxelization of Aggregated Point Clouds. Voxelization is to discretize a continuous 3D space into a regular grid structure composed of volumetric elements called voxels, enabling the conversion of unstructured data into a structured format that can be efficiently processed by convolutional neural networks (CNNs) or vision transformers (ViTs). Voxelization introduces a trade-off between spatial resolution and memory consumption and offers a flexible and scalable representation for 3D perception (Maturana & Scherer, 2015; Zhou & Tuzel, 2018; Li et al., 2023). For easy integration, SSCBench adheres to SemanticKITTI\u2019s setup, with a volume extending 51.2m ahead, 25.6m on each side, and 6.4m in height. Voxel resolution is 0.2m, resulting in a 256\u00d7256\u00d732 voxel volume. Labels\nfor each voxel are determined by majority voting among labeled points within it, while empty voxels are marked accordingly if no points are present.\nExclusion of Unknown Voxels. Capturing complete 3D outdoor dynamic scenes is nearly impossible without ubiquitous scene sensing. While it is possible to utilize spatial or prior knowledge-based inference, our intention is to ensure the fidelity of ground truth by minimizing errors originating from these steps. Hence, we only consider visible and probed voxels from all viewpoints during training and evaluation. Specifically, we first employ ray tracing from different perspectives to identify and remove occluded voxels within objects or behind walls. Furthermore, in datasets with sparse sensing, where numerous voxels remain unprobed, we remove these unknown voxels during training and evaluation to enhance the reliability of the ground truth labels as shown in Fig. 2."
        },
        {
            "heading": "3.4 DATASET STATISTICS",
            "text": "We show dataset statistics in Fig. 3. The dataset label distribution reveals noticeable domain gaps among various cities. Specifically, KITTI-360 and SemanticKITTI exhibit similar label distributions due to being captured within the same city in Germany. However, nuScenes and Waymo collected in the US and Singapore demonstrate distinct label distributions. Furthermore, SSCBench stands out for its larger scale in comparison to SemanticKITTI. For instance, SSCBench comprises 7.7 times more frames than SemanticKITTI, and its collection of driving sequences is also more diverse."
        },
        {
            "heading": "4 EXPERIMENTAL SETUP",
            "text": "Benchmark Methods. Our benchmark employs four camera-based methods, i.e., MonoScene (Cao & de Charette, 2022), VoxFormer (Li et al., 2023), TPVFormer (Huang et al., 2023), and OccFormer (Zhang et al., 2023b), as well as two LiDAR-based methods, i.e., SSCNet (Song et al., 2017) and LMSCNet (Roldao et al., 2020), due to their widespread adoption and cutting-edge performance. Using their public codebases, we run them under default settings but appropriately adjust data loaders and batch sizes to align with our SSCBench. We separately train, validate, and test these methods on SSCBench subsets, as reported in Sec. 5. Furthermore, we provide a unified benchmark for evaluating cross-domain generalizability in Sec. 6, where models are trained on one subset and tested on others, e.g., trained on SSCBench-KITTI-360 and tested on SSCBench-Waymo.\nEvaluation Metrics. We adopt the intersection over union (IoU) for evaluating geometry completion and the mean IoU (mIoU) of each class for evaluating semantic segmentation, following SemanticKITTI. We also report the ratio of different classes in the dataset to better understand the relationship between IoU and mIoU. We report the performances within the volume extending 51.2m ahead, 25.6m on each side, and 6.4m in height, and the voxel resolution is 0.2m. The design of this front-view evaluation emphasizes the area directly in the vehicle\u2019s anticipated forward trajectory. Experimental results across different ranges, i.e., short-range areas (12.8\u00d712.8\u00d76.4m3) and medium-\nrange areas (25.6\u00d725.6\u00d76.4m3), are reported in the Appendix. This range-based evaluation provides valuable insights into performance concerning spatial distance."
        },
        {
            "heading": "5 SEPARATE BENCHMARKING RESULTS",
            "text": ""
        },
        {
            "heading": "5.1 QUANTITATIVE COMPARISONS",
            "text": "Camera-based Methods. On SSCBench-KITTI-360, TPVFormer and OccFormer demonstrate superior geometry completion performance (IoU) compared to VoxFormer and MonoScene, as illustrated in Tab. 2. Improved geometry completion contributes to enhancing semantic segmentation (mIoU). On SSCBench-Waymo, MonoScene marginally outperforms VoxFormer across the majority of evaluation metrics. Due to the absence of stereo data in SSCBench-Waymo, the utilization of off-the-shelf self-supervised depth estimation modules (Bhat et al., 2021; Shamsafar et al., 2022), primarily trained on KITTI, results in suboptimal depth knowledge, leading to less competitive performance by VoxFormer in SSC. This trend becomes more pronounced in SSCBench-nuScenes, where the IoU and mIoU metrics for MonoScene significantly surpass those of VoxFormer (IoU, 29.63 \u2192 25.16 and mIoU, 9.60 \u2192 4.96). It is evident that accurate depth estimation plays a crucial role in scene geometry estimation within camera-based methods.\nLiDAR-based Methods. SSCNet consistently outperforms LMSCNet across all three subsets, mainly due to its larger number of parameters (1.03M compared to 0.35M). Additionally, it is worth noting that SSCNet exhibits superior recognition capabilities for small objects, such as motorcycles ( , 2.24 \u2194 0.00 in SSCBench-nuScenes) and bicycles ( , 13.23 \u2194 0.00 in SSCBench-Waymo). This demonstrates SSCNet\u2019s advantage in handling sparse LiDAR data compared to LMSCNet. When comparing results between SSCBench-KITTI-360 and SSCBench-nuScenes to SSCBench-Waymo, SSCNet and LMSCNet consistently deliver significantly better performance on the SSCBenchWaymo dataset, with IoU values approaching 90%. This improvement can be attributed to the denser LiDAR input available in Waymo data. However, it is crucial to emphasize that while dense LiDAR input leads to satisfactory performance, implementing this 5-LiDAR setup remains costly for most common autonomous driving solutions.\nCamera vs. LiDAR. As demonstrated in Tab. 2, on SSCBench-KITTI-360, LiDAR-based methods outperform camera-based approaches in terms of geometry metrics and most semantic metrics. This outcome is expected since camera-based methods must infer 3D scene geometry from 2D images, while LiDAR-based methods directly extract scene geometry from LiDAR input. However, the scenario changes in SSCBench-nuScenes, where camera-based methods generally surpass LiDARbased methods in terms of IoU. This difference can be attributed to the use of a sparse LiDAR sensor (Velodyne HDL32E) in the nuScenes dataset. These results indicate that LiDAR-based methods are sensitive to the sparsity of input. Specifically, while dense input has the potential for significant\nperformance improvement, sparse input can lead to significant degradation. This observation is further confirmed in SSCBench-Waymo, where the Waymo dataset contains point cloud data collected from one mid-range and four short-range LiDARs. As seen in Tab. 2, the two LiDAR-based methods outperform camera-based methods by a significant margin on all metrics in SSCBench-Waymo.\nHowever, camera-based methods outperform LiDAR-based ones for smaller objects that comprise a minuscule fraction of samples (< 0.5%). For classes such as bicycles ( , 0.00 \u2192 1.16), persons ( , 0.26 \u2192 4.54), poles ( , 1.09 \u2192 12.93), and traffic signs ( , 0.90 \u2192 14.25) in SSCBench-KITTI360, as well as motorcycles ( , 0.00 \u2192 3.80) and bicycles ( , 0.00 \u2192 1.70) in SSCBench-nuScenes, camera-based methods significantly outperform LiDAR-based methods. Despite the low frequency of small objects, their identification is vitally important for collision avoidance and traffic understanding."
        },
        {
            "heading": "5.2 DISCUSSIONS AND ANALYSES",
            "text": "Impact of Point Cloud Density. Our experiments illuminate the impact of LiDAR input density on model performance. In the SSCBench-nuScenes dataset, which features relatively sparse LiDAR input (32 channels), camera-based methods outperform LiDAR-based methods on geometric metrics. However, in the SSCBench-Waymo dataset, which benefits from dense LiDAR input (64 channels, 5 LiDARs), LiDAR-based methods vastly outperform camera-based methods. The sensitivity of LiDAR-based methods to input becomes evident, with advantages observed in dense input and notable performance degradation in sparse input. This highlights the need for future research in developing robust LiDAR-based methods that can mitigate degradation while capitalizing on the benefits.\nMonocular vs. Trinocular. Table 3 displays the performance of TPVFormer with monocular and trinocular input. While a trinocular setup offers a broader field of view that can help enhance overall performance in terms of both IoU (36.78 \u2192 39.06) and mIoU (10.91 \u2192 13.70), achieving excellent results using only a single camera remains a compelling academic challenge. There is still significant research value in developing monocular methods that can match the performance of models with panoramic views, as they are memory-efficient, computation-efficient, and easy to deploy.\nComparison with SemanticKITTI. We observe significant discrepancies when comparing our experimental results on SSCBench to those from SemanticKITTI (Behley et al., 2019) (for more details, we refer readers to VoxFormer (Li et al., 2023)). While VoxFormer performs admirably well on SemanticKITTI in metrics such as IoU and mIoU, it faces challenges with the diversity of our SSCBench dataset. This challenge primarily arises from its depth estimation module\u2019s inability to generalize beyond SemanticKITTI. Furthermore, LMSCNet, which typically exhibits superior geometric performance compared to SSCNet on SemanticKITTI, demonstrates the opposite trend on SSCBench. These discrepancies underscore two essential points. First, they highlight the significance of SSCBench, which provides diverse and demanding real-world scenarios for comprehensive evaluations. Second, they emphasize the necessity for robust methods capable of maintaining high performance across various environments."
        },
        {
            "heading": "6 UNIFIED BENCHMARKING RESULTS",
            "text": "To assess the domain gap and compare the cross-domain generalizability of state-of-the-art algorithms, we established a unified benchmark for cross-validation on SSCBench. Specifically, we employed two LiDAR-based methods, LMSCNet (Roldao et al., 2020) and SSCNet (Song et al., 2017), and one camera-based method, MonoScene (Cao & de Charette, 2022), for experiments on SSCBench-KITTI-\n360 and SSCBench-Waymo. To ensure consistent evaluation metrics, we standardized the labels of SSCBench-KITTI-360 and SSCBench-Waymo to a unified set comprising 10 common objects. All other experimental settings and evaluation metrics adhere to the guidelines outlined in Sec. 4.\nOverall Performance. As shown in Tab. 4, all three methods exhibit a notable decline in performance when cross-validated on another dataset across both the geometric metric (IoU) as well as the semantic metric (mIoU), regardless of the training dataset. Specifically, the model trained on SSCBenchWaymo and tested on SSCBench-KITTI-360 suffers a more severe decline for LiDAR-based methods than the other way around. This is because SSCBench-Waymo has a very dense point cloud input from five LiDARs, which effectively reduces the performance degradation caused by domain differences. Interestingly, the deterioration trend in terms of mIoU for MonoScene is more severe when transferring from SSCBench-KITTI-360 to SSCBench-Waymo than the other way around. This can be partially explained by the higher in-domain mIoU on SSCBench-KITTI-360 than that on SSCBench-Waymo and the difference in input resolutions (1408\u00d7376 \u2194 960\u00d7640), which is magnified by the fixed model parameters, and thereby affects feature representation.\nClass-Specific Performance. We observe the most significant performance drop in the \"road\" class ( ) for both transfer directions in all three methods. This suggests that ground types may be represented differently across datasets, causing challenges for both camera-based and LiDAR-based methods to adapt to these variations. It is worth noting that after unifying the labels in SSCBenchKITTI-360, the score for \"motorcycle\" ( ) drops to 0 (from 1.02). This demonstrates that a relatively smaller number of classes can impact feature extraction and classification, particularly for uncommon small objects. Throughout cross-domain evaluations, a recurring phenomenon of deterioration can be observed. This phenomenon underscores the value and necessity of our proposed SSCBench dataset. It is envisioned that models trained on this dataset would be better equipped to handle the variations and complexities encountered in cross-domain scenarios. Moreover, it also serves as motivation for the development of models that are more robust and capable of generalizing across different domains."
        },
        {
            "heading": "7 CONCLUSION",
            "text": "Limitations and Future Work. SSCBench only encompasses 3D data following the convention of the SSC problem. This limits evaluations of 4D methods with temporal dimensions. Future work will aim to expand SSCBench to include temporal information.\nSummary. In this paper, we introduce SSCBench, a large-scale benchmark composed of diverse street views, aimed at facilitating the development of robust and generalizable semantic scene completion models. Through meticulous curation and comprehensive benchmarking, we identify the bottlenecks of existing methods and offer valuable insights into future research directions. Our ambition is for SSCBench to stimulate advancements in 3D semantic scene completion, ultimately enhancing perception capabilities for the next-generation autonomous systems."
        },
        {
            "heading": "A OVERVIEW",
            "text": "We present the necessary licenses in Appendix B, because our benchmark is constructed based on the previous works (Liao et al., 2022; Caesar et al., 2020; Sun et al., 2020). We also exhaustively analyze the impact of perception distance on the evaluation performance in Appendix C, where we present detailed results comparison from short-range to long-range under different settings. In Appendix D, we display representative qualitative visualization results from our SSCBench, along with the input image and LiDAR. Finally, the visualization comparison between the monocular method and trinocular method has also been presented in Appendix D for better qualitative evaluation."
        },
        {
            "heading": "B DATASET DETAILS",
            "text": "Complying with the original dataset we used, different parts of the SSCBench dataset are released under different licenses. In particular:\n\u2022 SSCBench-KITTI-360: CC BY-NC-SA 3.0 \u2022 SSCBench-nuScenes: CC BY-NC-SA 4.0 \u2022 SSCBench-Waymo: Waymo Dataset License Agreement for Non-Commercial Use\nThe authors bear all responsibilities for the licensing, distribution, and maintenance of our datasets.\nTo improve the usability and readability of our SSCBench, we will release the complete dataset and detailed configurations upon acceptance, along with the codes and corresponding models for the evaluation methods on our SSCBench, based on their official codebases."
        },
        {
            "heading": "C ANALYSIS OF RANGE",
            "text": "We report the performances across three distinct scales of volume, namely 12.8\u00d712.8\u00d76.4m3, 25.6\u00d725.6\u00d76.4m3, and 51.2\u00d751.2\u00d76.4m3. These scales have been deliberately selected and provide an in-depth analysis of performance across varying perception ranges. Such an understanding is paramount for a variety of subsequent tasks, especially those associated with vehicle planning and navigation (Hu et al., 2023). Notably, all the aforementioned scales lie within the observable range of onboard sensors, such as cameras and LiDAR. The design choice of our evaluation from short-range to long-range underscores its significance in assessing the vehicle\u2019s forward anticipation and the associated implications on performance in relation to the perception range.\nC.1 PER SUBSET\nIn Tab. I, II, and III, comprehensive performances of the proposed three subsets SSCBench-KITTI360, SSCBench-nuScenes, and SSCBench-Waymo have been demonstrated, especially for the three distinct evaluation ranges. As the perception range expands, all LiDAR-based and camera-based methods generally experience a decrease in performance due to increased complexity and uncertainty in larger volumes. However, LiDAR-based methods, specifically in SSCBench-Waymo, demonstrate minimal performance degradation. This trend, likely attributed to the dense LiDAR input, highlights their ability to maintain stable performance across varying perception ranges. It underscores the potential benefits of dense LiDAR data collection, albeit with increased cost and computational demand. Additionally, this trend emphasizes the need for further research to enhance LiDAR data density and advance depth estimation for camera-based methods, improving their scalability to larger perception ranges.\nC.2 TRINOCULAR vs. MONOCULAR\nMirroring the degradation with SSCBench-Waymo, trinocular setting can also confront a decrease in performance along with the increase of evaluation range, shown in Tab. IV. However, it is worth noting that while trinocular method exhibits a comparable decline in geometric metrics (IoU) to that of monocular method, it experiences a more pronounced deterioration in semantic metrics (mIoU). One possible explanation might be that trinocular method becomes more reliant on semantic information due to the influx of additional visual information from triple inputs. As the evaluation range expands,\nthere is a potential misalignment in the features from the visual data captured by the cameras heading to different view angles, which can have a more adverse effect on the recognition of distant objects.\nC.3 UNIFIED BENCHMARK RESULTS\nTable V, VI, and VII demonstrate the cross-domain evaluation across three different evaluation ranges, following the unified setting mentioned above. It is noteworthy that for LMSCNet, after training on SSCBench-KITTI-360 and testing on SSCBench-Waymo, there is a subtle improvement in performance as the evaluation distance increases. This is likely attributed to the denser LiDAR data in the SSCBench-Waymo. While a similar trend can be observed for SSCNet, a distinction emerges when trained on SSCBench-Waymo and tested on SSCBench-360: SSCNet demonstrates greater robustness to the change of distance compared to LMSCNet, which is likely due to the aforementioned number of parameters. Interestingly, for MonoScenes, the model showcases an insensitivity to the evaluation range across different datasets. Specifically, the semantic metric (mIoU) for MonoScenes experiences a decline of less than 1%."
        },
        {
            "heading": "D QUALITATIVE RESULTS",
            "text": "In Fig. I, we show the completion results of different methods on the SSCBench dataset. We can observe comparable results between LMSCNet (Roldao et al., 2020) and SSCNet (Song et al., 2017), while MonoScene (Cao & de Charette, 2022) has a varying performance depending on the input. On SSCBench-KITTI-360, MonoScene has comparable results to the two LiDAR-based methods due to the large FOV. It also shows a superior completion ability when there is occlusion near the ego-vehicle\n(see the 3rd row). However, when the FOV is limited, MonoScene fails to predict the scene outside the visible input (the last two rows). Additionally, in order to qualitatively analyze results under different camera settings, We show the comparison of semantic scene completion results between monocular and trinocular methods on our proposed SSCBench-Waymo dataset, as Fig. II, III, IV and V. Benefiting from the more visual input of the supplementary perspectives, trinocular method can also have a better perception of the environment next to the ego vehicle. Moreover, multi-view input also contributes to more reasonable reasoning for the front view, especially the details in long-range, as shown in the second column of Fig. II."
        }
    ],
    "title": "SSCBENCH: MONOCULAR 3D SEMANTIC SCENE COMPLETION BENCHMARK IN STREET VIEWS",
    "year": 2023
}