{
    "abstractText": "In recent years, deep learning-based hashing techniques have garnered significant attention within the academic community, owing to their impressive performance achievements. However, the current deep hashing method is limited by its use of neural networks cannot make good use of the important feature information of the image, thus limiting the retrieval accuracy. In this paper, we design a two-channel feature extraction network called the local and global feature fusion network (LGDH). Specifically, the features extracted by visual geometry group (VGG) network and con-volution-enhanced image transformer (CeiT) network are deeply orthogonal fusion of local and global features, which solves the problem of lack of semantic correlation in existing deep hash methods. To fully harness the insights gained from previous hash function learning, we employ long short-term memory (LSTM) decoder to proficiently shape and create a resilient public binary code space. Through detailed experiments, our proposed method has achieved the Sota level in three public datasets CIFAR10, NUS-WIDE and MIRFLICKR compared with multiple cutting-edge algorithms, which can prove the effectiveness of our pro-posed method. INDEX TERMS Deep learning, deep hashing, local and global feature fusion network, visual geometry group, convolution-enhanced image transformer, long short-term memory.",
    "authors": [
        {
            "affiliations": [],
            "name": "Xiaoxiao Wang"
        },
        {
            "affiliations": [],
            "name": "Lin Zhang"
        },
        {
            "affiliations": [],
            "name": "Nanzhen Yao"
        },
        {
            "affiliations": [],
            "name": "Peng Qian"
        }
    ],
    "id": "SP:6a8f70742c717255b96f74f0aa581356bbd029cb",
    "references": [
        {
            "authors": [
                "G. Montavon",
                "W. Samek",
                "K.-R. M\u00fcller"
            ],
            "title": "Methods for interpreting and understanding deep neural networks,",
            "venue": "Digital signal processing,",
            "year": 2018
        },
        {
            "authors": [
                "J. Gu"
            ],
            "title": "Recent advances in convolutional neural networks,",
            "venue": "Pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep residual learning for image recognition,",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "S. Ren",
                "K. He",
                "R. Girshick",
                "J. Sun"
            ],
            "title": "Faster r-cnn: Towards realtime object detection with region proposal networks,\" Advances in neural information processing",
            "year": 2015
        },
        {
            "authors": [
                "H. Liu",
                "R. Wang",
                "S. Shan",
                "X. Chen"
            ],
            "title": "Deep supervised hashing for fast image retrieval,",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Z. Cao",
                "M. Long",
                "J. Wang",
                "P.S. Yu"
            ],
            "title": "Hashnet: Deep learning to hash by continuation,",
            "venue": "Proceedings of the IEEE international conference on computer vision,",
            "year": 2017
        },
        {
            "authors": [
                "S. Su",
                "C. Zhang",
                "K. Han",
                "Y. Tian"
            ],
            "title": "Greedy hash: Towards fast optimization for accurate hash coding in cnn,",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Z. Zhang",
                "Q. Zou",
                "Y. Lin",
                "L. Chen",
                "S. Wang"
            ],
            "title": "Improved deep hashing with soft pairwise similarity for multi-label image retrieval,",
            "venue": "IEEE Transactions on Multimedia,",
            "year": 2019
        },
        {
            "authors": [
                "L. Yuan"
            ],
            "title": "Central similarity quantization for efficient image and video",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2020
        },
        {
            "authors": [
                "L. Fan",
                "K.W. Ng",
                "C. Ju",
                "T. Zhang",
                "C.S. Chan"
            ],
            "title": "Deep Polarized Network for Supervised Learning of Accurate Binary Hashing",
            "venue": "Codes,\" in IJCAI,",
            "year": 2020
        },
        {
            "authors": [
                "M. Yang"
            ],
            "title": "Dolg: Single-stage image retrieval with deep orthogonal fusion of local and global features,",
            "venue": "Proceedings of the IEEE/CVF International conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Z. Wang",
                "Z. Zhang",
                "Y. Luo",
                "Z. Huang",
                "H.T. Shen"
            ],
            "title": "Deep collaborative discrete hashing with semantic-invariant structure construction,",
            "venue": "IEEE Transactions on Multimedia,",
            "year": 2020
        },
        {
            "authors": [
                "W. Shi",
                "Y. Gong",
                "B. Chen",
                "X. Hei"
            ],
            "title": "Transductive semisupervised deep hashing,",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Z. Zhang",
                "J. Wang",
                "L. Zhu",
                "Y. Luo",
                "G. Lu"
            ],
            "title": "Deep collaborative graph hashing for discriminative image retrieval,",
            "venue": "Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "A. Vaswani"
            ],
            "title": "Attention is all you need,",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "J. Devlin",
                "M.-W. Chang",
                "K. Lee",
                "K. Toutanova"
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding,",
            "venue": "arXiv preprint arXiv:1810.04805,",
            "year": 2018
        },
        {
            "authors": [
                "A. Graves"
            ],
            "title": "Long short-term memory,\" Supervised sequence labelling with recurrent neural networks, pp",
            "year": 2012
        },
        {
            "authors": [
                "A. Dosovitskiy"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale,",
            "venue": "arXiv preprint arXiv:2010.11929,",
            "year": 2020
        },
        {
            "authors": [
                "K. Yuan",
                "S. Guo",
                "Z. Liu",
                "A. Zhou",
                "F. Yu",
                "W. Wu"
            ],
            "title": "Incorporating convolution designs into visual transformers,",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "K. Simonyan",
                "A. Zisserman"
            ],
            "title": "Very deep convolutional networks for large-scale image recognition,",
            "venue": "arXiv preprint arXiv:1409.1556,",
            "year": 2014
        },
        {
            "authors": [
                "J. Zhang",
                "Y. Peng",
                "Z. Ye"
            ],
            "title": "Deep reinforcement learning for image hashing,",
            "venue": "arXiv preprint arXiv:1802.02904,",
            "year": 2018
        },
        {
            "authors": [
                "J. Deng",
                "J. Guo",
                "N. Xue",
                "S. Zafeiriou"
            ],
            "title": "Arcface: Additive angular margin loss for deep face recognition,",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2019
        },
        {
            "authors": [
                "A. Krizhevsky",
                "G. Hinton"
            ],
            "title": "Learning multiple layers of features from tiny",
            "year": 2009
        },
        {
            "authors": [
                "T.-S. Chua",
                "J. Tang",
                "R. Hong",
                "H. Li",
                "Z. Luo",
                "Y. Zheng"
            ],
            "title": "Nus-wide: a real-world web image database from national university of singapore,",
            "venue": "Proceedings of the ACM international conference on image and video retrieval,",
            "year": 2009
        },
        {
            "authors": [
                "M.J. Huiskes",
                "M.S. Lew"
            ],
            "title": "The mir flickr retrieval evaluation,",
            "venue": "Proceedings of the 1st ACM international conference on Multimedia information retrieval,",
            "year": 2008
        },
        {
            "authors": [
                "J. Deng",
                "W. Dong",
                "R. Socher",
                "L.-J. Li",
                "K. Li",
                "L. Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database,",
            "venue": "IEEE conference on computer vision and pattern recognition,",
            "year": 2009
        },
        {
            "authors": [
                "Y. Peng",
                "J. Zhang",
                "Z. Ye"
            ],
            "title": "Deep reinforcement learning for image hashing,",
            "venue": "IEEE Transactions on Multimedia,",
            "year": 2019
        },
        {
            "authors": [
                "Z. Qiu",
                "Y. Pan",
                "T. Yao",
                "T. Mei"
            ],
            "title": "Deep semantic hashing with generative adversarial networks,",
            "venue": "Proceedings of the 40th international ACM SIGIR conference on research and development in information retrieval,",
            "year": 2017
        },
        {
            "authors": [
                "H. Lai",
                "Y. Pan",
                "Y. Liu",
                "S. Yan"
            ],
            "title": "Simultaneous feature learning and hash coding with deep neural networks,",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2015
        },
        {
            "authors": [
                "R. Xia",
                "Y. Pan",
                "H. Lai",
                "C. Liu",
                "S. Yan"
            ],
            "title": "Supervised hashing for image retrieval via image representation learning,",
            "venue": "Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2014
        },
        {
            "authors": [
                "F. Shen",
                "C. Shen",
                "W. Liu",
                "H. Tao Shen"
            ],
            "title": "Supervised discrete hashing,",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2015
        },
        {
            "authors": [
                "Y. Weiss",
                "A. Torralba",
                "R. Fergus"
            ],
            "title": "Spectral hashing,\" Advances in neural information processing",
            "year": 2008
        },
        {
            "authors": [
                "A. Gionis",
                "P. Indyk",
                "R. Motwani"
            ],
            "title": "Similarity search in high dimensions via hashing,",
            "venue": "Vldb,",
            "year": 1999
        }
    ],
    "sections": [
        {
            "text": "VOLUME XX, 2017 1\nwithin the academic community, owing to their impressive performance achievements. However, the current deep hashing method is limited by its use of neural networks cannot make good use of the important feature information of the image, thus limiting the retrieval accuracy. In this paper, we design a two-channel feature extraction network called the local and global feature fusion network (LGDH). Specifically, the features extracted by visual geometry group (VGG) network and con-volution-enhanced image transformer (CeiT) network are deeply orthogonal fusion of local and global features, which solves the problem of lack of semantic correlation in existing deep hash methods. To fully harness the insights gained from previous hash function learning, we employ long short-term memory (LSTM) decoder to proficiently shape and create a resilient public binary code space. Through detailed experiments, our proposed method has achieved the Sota level in three public datasets CIFAR10, NUS-WIDE and MIRFLICKR compared with multiple cutting-edge algorithms, which can prove the effectiveness of our pro-posed method.\nINDEX TERMS Deep learning, deep hashing, local and global feature fusion network, visual geometry group, convolution-enhanced image transformer, long short-term memory.\nI. INTRODUCTION In the realm of academia, the exponential growth of the Internet and the proliferation of digital media have underscored the escalating significance of image storage, retrieval, and transmission. In this era of information explosion, image hashing has become a powerful tool for image management, content recognition, image retrieval, image com-pression and other applications. The primary objective of image hashing entails the transformation of images into a condensed binary representation, facilitating their efficient storage and subsequent comparative analysis, while preserving the semantics and characteristics of the images. However, the traditional image hashing method has some limitations in dealing with complex image content and changes, so more powerful methods are needed to meet the needs of modern multimedia applications.\nAs an important research direction in image processing and retrieval, depth image hashing aims to map complex depth images into a low dimensional binary code space for efficient storage and fast retrieval. Compared with traditional image hashing methods, depth image hashing\nmethods need to consider the special properties of depth information, such as depth value, distance, surface normal, etc., and how to encode this information into the hash code effectively. By converting deep images into compact hash codes, storage space can be drastically reduced, image retrieval and matching tasks can be per-formed faster, and storage and computational costs can be reduced. Driven by deep learning, deep image hashing has made remarkable progress. The emergence of deep neural networks (DNN) [1, 2] has greatly improved the capability of feature representation, enabling the deep image hashing method to learn more discriminative features. Network structures such as convolutional neural networks (CNN) [3, 4] perform well on tasks such as image classification [5] and object detection [6], this also fosters an academic inclination towards their application in the realm of deep image hashing. Through deep learning technology, the deep image hashing method can learn useful feature representation from large-scale data and improve the quality and retrieval effect of the hash code. Nonetheless, they autonomously acquire the complete set of hash\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n8 VOLUME XX, 2017\nfunctions as a cohesive unit, neglecting to consider the interplay and correlations among the distinct hash functions in their learning process. In addition, traditional image hashing algorithms such as can use the method of sequential learning hash code for error correction, and the existence of this ability can well increase the accuracy of subsequent hash code.\nTherefore, this paper combines the above existing problems to solve one by one and make the following contributions: \u2022 We designed a two-channel feature extraction\nnetwork that outputs global features and local features. We applied an Orthogonal Fusion Model to amalgamate these characteristics, thereby enhancing the retrieval accuracy of the hash method.\n\u2022 We employed LSTM to serve as a modeling tool for\nthe hash function, enabling the dynamic adaptation of the hash code in response to the current image content and target. This approach effectively augments the model's learning capability.\n\u2022 We conducted a comparative analysis across various\nalgorithms using three publicly available datasets, namely CIFAR10, NUS-WIDE, and MIRFLICKR. Our results demonstrate the efficacy of our approach, substantiating its academic merit."
        },
        {
            "heading": "II. Related work",
            "text": "In the domain of computer vision, the evolution of image hashing technology has spurred innovation and marked significant progress in areas such as image retrieval, matching, and various related research endeavors, enriching the academic discourse. Image hashing is a technique that converts an image into a fixed length binary or hexadecimal string. It generates a unique hash value, which is used to represent the features of the image through specific processing and calculation of the image. Image hashing can be used for image search, image retrieval, duplicate image elimination, copyright protection and image similarity comparison. In recent years, the convergence of state-of-the-art techniques, including deep learning and reinforcement learning, has spawned a succession of pioneering deep image hashing methods. These methods have demonstrated notable success in augmenting retrieval performance and curtailing storage overhead. From an academic standpoint, this represents a noteworthy advancement in the field."
        },
        {
            "heading": "1) DEPTH IMAGE HASHING ALGORITHM",
            "text": "Deep reinforcement learning has emerged as a focal point of scholarly attention within the domain of image hashing research. Deep supervised hashing [7] uses regularizes to generate discrete binary values on realvalued network outputs to generate binary hashes. HashNet [8] also incorporated the inverse tangent function into its algorithm, establishing an efficient smooth conversion of\nreal-valued features to binary code. In addition, HashNet uses weighted cross-entropy losses to capture the sparsity of the learning data. Compared with greedy hashing algorithm [9], HashNet can avoid the gradient disappearance problem. Greedy hashing, on the other hand, is binary encoded with the Sign function at the hash level. With the development of technology, evolutionary techniques have gradually emerged. Zhang et al. [10]. We have introduced a novel hashing algorithm in the academic context, which integrates both cross-entropy loss and mean square error loss functions. This algorithm has been specifically designed to address the challenges posed by both hard comparability and soft comparability in the context of multi-label image retrieval. The quantified central similarity hash proposed by Yuan et al. [11] also utilizes central similarity optimization among data points to improve the uniqueness of hash code in image retrieval. Fan et al. [12] proposed that polarization loss was used as a position gauge loss, and a longer distance output channel was used to generate separable features between classes. In the paper titled \"Deep Reinforcement Learning for Image Hashing,\" scholars incorporate deep reinforcement learning techniques into the realm of image hashing, facilitating the autonomous acquisition of optimal strategies for generating hashing codes. Through the interaction between the agent and the environment, the model can learn the hashing code generation strategy that ADAPTS to different image distribution and retrieval requirements. This method not only reduces the dependence of manual feature engineering, but also improves the generalization ability of the model on multi-task and multi-data sets. On the other hand, Yang et al. [13] proposed a single-stage image retrieval method that combines local and global features by deep orthogonal fusion through a depth model. By using the complementarity of local features and global features, this method achieves more accurate image representation and coding, thus improving the performance of image hashing. Wang et al. [14] integrated concepts from collaborative approaches and structural construction to introduce a novel deep hashing technique rooted in discrete representation. This method generates a compact hash code by learning the semantic invariance and structure information of images. Through collaboration and structure construction, this method achieves more accurate image similarity measurement and improves the effectiveness of image retrieval. Shi et al. [15] discussed the application of semisupervised learning in deep image hashing. Through the utilization of both annotated and unannotated data, this approach facilitates enhanced hash function learning, thereby elevating the caliber and resilience of the resulting hash codes. This approach demonstrates its effectiveness not only under conditions of constrained labeled data but also when confronted with vast amounts of unlabeled data in the context of transfer learning, illustrating its versatility and robustness from a scholarly perspective. Zhang et al.\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n8 VOLUME XX, 2017\n[16] combined graph embedding method with deep learning to propose a deep collaborative graph hashing method. By constructing a graph model to capture the relationship between images, this method can achieve more discriminative image hash coding and improve the accuracy and robustness of image retrieval."
        },
        {
            "heading": "2) TRANSFORMER",
            "text": "Transformer [17] stands as a seminal NLP model pioneered by Google's research team in 2017, with BERT [18] being an evolution built upon the Transformer framework. Notably, the Transformer architecture employs a SelfAttention mechanism while eschewing the need for traditional RNN sequential structures. This design choice enables the model to undergo parallelized training and facilitates the incorporation of global information. Long term memory network (LSTM) [19], as a recurrent neural network, has also been applied in the field of image hashing. By using LSTM to model the features of image sequences, more accurate image coding and hashing can be achieved. LSTM plays an important role in \"Deep Reinforcement Learning for Image Hashing\" and other methods to help the model learn the temporal relationship and context information between images. In addition, Vision Transformer (ViT) [20] applied Transformer to computer vision for the first time in 2020, and subsequently achieved significant results in various areas of computer vision. CeiT [21] is an improvement on ViT, which can be transformed into a single transformer with little increase in computing cost. Get higher performance and better convergence.\nThe research of depth image hashing is evolving towards higher performance, lower storage cost and stronger generalization ability. Researchers in the academic realm have consistently pioneered the development of efficient image hashing methods by integrating state-of-the-art techniques from deep learning and reinforcement learning.\nThese methods are not only widely used in image retrieval, matching and other tasks, but also provide new ideas and solutions for the research and application of computer vision. In the future, with the continuous development of technology, the depth image hashing method is expected to achieve more remarkable achievements in more fields."
        },
        {
            "heading": "III. Method",
            "text": "Compared with existing depth image hashing algorithms, we use a feature fusion model with two-branch structure, which uses CeiT (green background part) to extract global features such as image semantic features, and VGG19 [22] (orange background part) to extract local feature map image point and line features. Then, global, and local features are fed into the Orthogonal Fusion Model (blue background part) for feature fusion. Finally, the fused features are fed into a decoder composed of LSTM for decoding, thus building a robust public binary code space. They are represented Final Descriptor."
        },
        {
            "heading": "A. Overview",
            "text": "Our LDGH framework is depicted in Figure 1. To elevate the neural network's capacity for expressing features, we introduce the Fusion Model and Decoder module into our design. As the previous image hashing algorithm [14] used Reset and other neural networks, resulting in insufficient feature information extraction. In our academic approach, we employ CeiT as the overarching global feature extraction network to capture and distill the holistic information present within the image, while concurrently utilizing the VGG19 network as the local feature extraction network to scrutinize and extract intricate local-level features embedded within the image. We feed both global and local features into the orthogonal Fusion Model to facilitate feature fusion.\nThis amalgamation of global and local features serves to bolster the network's feature expression prowess, with the overarching goal of enhancing the algorithm's performance through more informed utilization of feature information, we design a decoder composed of LSTM, which can build\na robust public binary code space. Therefore, the overall flow of our network is shown in formula (1):\nFD=Decoder(OFM(CeiT(x),VGG(x))) (1)\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n8 VOLUME XX, 2017\nIn the formula, FD is the Final Descriptor, Decoder is the decoder composed of LSTM, OFM is the Orthogonal Fusion Model, CeiT is employed as the network responsible for global feature extraction, whereas VGG fulfills the role of the network specialized in local feature extraction. In the academic context, CeiT assumes the role of the macro-level feature extractor, capturing broader contextual information, while VGG concentrates on microlevel feature extraction, emphasizing local patterns and finer details."
        },
        {
            "heading": "B. Fusion Model",
            "text": "The Fusion Model can be dissected into two essential constituents: the feature extraction network and the feature fusion process. Previous deep image hashing algorithms used a single-stream convolutional neural network as the feature extraction network, which resulted in limited feature extraction information and weak network learning ability. Hence, to augment feature expression capabilities and bolster the model's learning prowess, we've adopted a dual-stream network architecture for enhanced feature extraction. This comprises the integration of a global feature extraction network known as CeiT alongside a local feature extraction network referred to as Vgg19 from an academic perspective.\nWe did not choose to use the ViT model in the global feature extraction network, be-cause ViT has disadvantages such as large data demand and large amount of calculation. Because we use CeiT network as the global extraction network, it can effectively combine the advantages of CNN and Transformer, and in the case of a small increase in computing cost, we can use the VIT model. Get higher performance and better convergence. Since we ultimately need to use the features extracted by the CeiT network rather than the final classification score, we did not use the yellow background of the class token (yellow dot) and the final MSA and FFN in Figure 2.\nTo minimize the computational overhead of the network, we opted to replace the ResNet50 network with the VGG19 network, serving as both the local and global feature extraction network. We do not need to obtain classification results, so we deleted the FFN structure in VGG19. As a result, both the global feature extraction network and the\nlocal feature extraction network generate feature representations for the input image.\nIn response to the absence of semantic coherence within current deep hashing methodologies, we harnessed the Orthogonal Fusion Module for the amalgamation of global and local feature representations. As shown in Figure 3.\nMainly, we projected local features and global features \ud835\udc87\ud835\udc8d,\ud835\udc91\ud835\udc93\ud835\udc90\ud835\udc8b (\ud835\udc8a,\ud835\udc8b) , Therefore. This formula can be expressed as formula 2:\n\ud835\udc87\ud835\udc8d,\ud835\udc91\ud835\udc93\ud835\udc90\ud835\udc8b (\ud835\udc8a,\ud835\udc8b) = \ud835\udc87\ud835\udc8d\n(\ud835\udc8a,\ud835\udc8b) \u22c5\ud835\udc87\ud835\udc88 \u2223\ud835\udc87\ud835\udc88\u2223 \ud835\udfd0 \ud835\udc87\ud835\udc88 (2)\nTherefore, the workflow of the Fusion Model is shown in Formula 3:\n\ud835\udc76\ud835\udc96\ud835\udc95\ud835\udc6d\ud835\udc74 = \ud835\udc76\ud835\udc6d\ud835\udc74(\ud835\udc6a\ud835\udc86\ud835\udc8a\ud835\udc7b(\ud835\udc99), \ud835\udc7d\ud835\udc6e\ud835\udc6e(\ud835\udc99)) (3)\nOFM in the above formula is the Orthogonal Fusion Model"
        },
        {
            "heading": "C. Decoder",
            "text": "Since hash functions sequentially map images into binary code, it becomes crucial for the current hash function learning process to consider the errors introduced by preceding hash functions [23]. RNN is used to optimize algorithm performance. However, RNN has good memory effect for short sequences, but limited memory ability for long sequences, which leads to performance degradation when processing long sequence data, because RNN cannot effectively capture important long-term information. Therefore, we employ LSTM to capture the hash function dynamics and adjust the hash code value in response to the current image content and target. Figure 4 shows the LSTM cell.\nThe LSTM model is composed of the input word \ud835\udc17\ud835\udc2d, cell state \ud835\udc6a\ud835\udc95, temporary cell state ?\u0303?\ud835\udc2d, hidden state \ud835\udc21\ud835\udc2d and so on.\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n8 VOLUME XX, 2017\nThe LSTM computation process can be succinctly described as follows: it involves the deliberate act of both disregarding outdated information within the cell state and assimilating novel information, resulting in the retention of pertinent data for subsequent computational steps, while concurrently discarding extraneous information. and the hidden layer state \ud835\udc89\ud835\udc95 is output at each time step. Each cell contains three door units: First the forget door, then the update door, and finally the output door. The memory of distant information can be realized by relying on the memory relationship between the three gates. For this purpose, we model the hash function using LSTM as the decoder input."
        },
        {
            "heading": "D. Training Loss",
            "text": "Our method outputs the result through the N-class prediction head, so we use Arc Face margin loss [24] to train the whole network, as shown in Formula 4:\n\ud835\udc73 = \u2212 \ud835\udc8d\ud835\udc90\ud835\udc88 ( \ud835\udc86\ud835\udc99\ud835\udc91(\ud835\udf38\u00d7\ud835\udc68\ud835\udc6d(?\u0302?\ud835\udc95\n\ud835\udc7b?\u0302?\ud835\udc88,\ud835\udfcf))\n\u2211 \ud835\udc86\ud835\udc99\ud835\udc91(\ud835\udf38\u00d7\ud835\udc68\ud835\udc6d(?\u0302?\ud835\udc8f \ud835\udc7b?\u0302?,\ud835\udc9a\ud835\udc8f))\n\ud835\udc8f\n) (4)\nHere ?\u0302?\ud835\udc8a \ud835\udc7b represents the \ud835\udc8a line of the final output \ud835\udc6d\ud835\udc6b \u2208\n\ud835\udc79\ud835\udfd3\ud835\udfcf\ud835\udfd0\u00d7\ud835\udc75 , ?\u0302?\ud835\udc88 represents the global feature after \ud835\udc73\ud835\udfd0 normalization y is the one-hot label vector, \ud835\udf38 is the scale factor, \ud835\udc00\ud835\udc05 is the cosine similarity after ArcFace adjustment, as shown in formula 5:\n\ud835\udc68\ud835\udc6d(\ud835\udc94, \ud835\udc84) = { \ud835\udc84\ud835\udc90\ud835\udc94(\ud835\udc82\ud835\udc84\ud835\udc90\ud835\udc94(\ud835\udc94) + \ud835\udc8e) , \ud835\udc8a\ud835\udc87 \ud835\udc84 = \ud835\udfcf\n\ud835\udc94, \ud835\udc8a\ud835\udc87 c = \ud835\udfce (5)\nWhere \ud835\udc2c is the cosine similarity, m is the ArcFace margin and c = 1 means this is the ground truth class."
        },
        {
            "heading": "IV. Experiment",
            "text": "This section describes our test experiments, comparison experiments, and ablation experiments. Three public datasets, CIFAR10 [25], NUS-WIDE [26] and MIRFLICKR [27], compare the performance of our algorithm with that of several existing algorithms. The ablation study analyzed the influence of each module on our algorithm."
        },
        {
            "heading": "A. Experimental details",
            "text": "This article utilizes the Ubuntu 20.04 operating system, along with computer hardware, with a particular emphasis on the repeated utilization of recursion, aiming to delve into the application and advantages of this crucial technology. (1) CPU is Inter i9-13900k; (2) GPU is 1 Nvidia GeForce RTX 4090, memory 24G. Fusion network We used the weights pre-trained in ImageNet [28] to initialize the\nnetwork VGG19, and the CeiT network. The input image is first flipped horizontally and randomly cropped to a size of 512\u00d7512. We set batch to 64 for training, and a full training phase takes 2.6 days. The weight attenuation factor is set to 0.0001 and the cosine learning rate attenuation strategy is adopted. In the Arc Face marginal loss, we set the scale \u03b3 to 30."
        },
        {
            "heading": "1) DATASETS:",
            "text": "Our proposed methods were trained and tested using CIFAR10, NUS-WIDE, and MIRFLICKR datasets. CIFAR10 dataset is an image classification dataset used in the field of machine vision. Comprising a total of 60,000 color images distributed across 10 distinct categories such as aircraft, cars, birds, cats, deer, dogs, frogs, horses, boats, and trucks, this dataset features uniform dimensions of 32x32 pixels. The dataset is further organized into five training sets and one test set, each comprising 10,000 images, making it a comprehensive resource for machine learning and computer vision tasks. NUS-WIDE (National University of Singapore-WIDE) is a large-scale image database widely used for image retrieval and multi-label image classification research. Created by a research team at the National University of Singapore, the dataset contains 269,648 images, 5,018 specific tags and is widely used in the computer vision and machine learning community. NUS-WIDE stands out primarily for its extensive collection of image samples, distinguished by the presence of multiple labels associated with each image. As such, it proves exceptionally well-suited for conducting research in the domains of multi-label image classification and image retrieval tasks. MIRFLICKR (Music Information Retrieval - FLICKR) is a data set for multimodal (audio and image) Information Retrieval research, mainly for Music information retrieval (Music Information retrieval). Research in the field of MIR. The dataset is based on images and audio clips uploaded by users on the Flickr site, and therefore contains multiple types of multimodal data."
        },
        {
            "heading": "2) EVALUATION CRITERIA:",
            "text": "1. The Mean Average Precision (MAP) serves as a\nmetric for assessing the overall effectiveness of retrieval systems. It calculates the average precision (AP) across different queries or categories, offering a comprehensive evaluation of accuracy. MAP finds common usage in evaluating the performance of search engines, document retrieval systems, and recommendation systems, capturing their collective precision.\n2. Precision@48 bits is a crucial performance metric\nused to assess the accuracy and efficiency of hashbased search. In this measurement, 48 bits represent the number of bits used to encode the hash codes for data. This metric finds widespread applications in fields such as information retrieval, data compression,\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n8 VOLUME XX, 2017\nand database queries. Its significance lies in its ability to not only gauge the quality of search results but also directly impact the utilization efficiency of computational and storage resources, thereby exerting a profound influence on the performance of various data-driven applications. Therefore, accurately evaluating and optimizing precision@48 bits is of paramount importance in enhancing the efficacy of various data-driven applications.\n3. Precision@hamm.dist<=2 is a metric used to evaluate\nthe performance of hash retrieval, commonly used in Deep Hashing or binary-coded image retrieval tasks. Precision is a commonly used categorical performance metric to measure how many positive examples of a model's predictions are correct. In binary classification problems, accuracy is a crucial performance metric that reflects the ratio of the number of samples correctly predicted as positive by the model to the total number of samples predicted as positive by the model. Accuracy is commonly used to assess the overall performance of a classification model and helps us understand how well the model performs in correctly identifying positives and negatives. This part means that when calculating Precision, only those sample pairs whose Hamming Distance is less than or equal to 2 are considered. Hamming Distance is a measure used to compare the degree of difference be-tween two binary encodings. It calculates the dissimilarity between two binary codes by counting the number of differing bits. In image hashing applications, Hamming Distance is commonly employed to assess the likeness of two images, with a minimal distance suggesting a high degree of similarity between their hashing codes..\n4. The precision of the top k retrieved results (TopK-\nprecision) is a metric for measuring accuracy when retrieving different numbers of samples from a ranking list. This metric informs us of the likelihood that these items are relevant when searching for the\ntop k items in the ranking list, thereby aiding us in assessing the quality of retrieval results."
        },
        {
            "heading": "B. Comparison with State-of-the-art Methods",
            "text": "We compare the retrieval performance of LGDH with 16 state-of-the-art hashing methods and with other hashing methods on a variety of criteria. In the comparison data, we directly cited the results reported in their original paper.\nTable 1 displays the MAP scores achieved across various hash code lengths when applied to the CIFAR10 dataset. Our LGDH proposal stands out with an impressive average MAP score of 0.917, consistently surpassing the state-ofthe-art techniques for hash code lengths, except for the 12- bit case. To provide a clearer overview, we categorize the results table into three distinct groups: deep hashing methods, traditional methods utilizing depth features, and traditional methods utilizing manual features."
        },
        {
            "heading": "1) EXPERIMENT RESULTS ON CIFAR10 DATASET.",
            "text": "TABLE \u2160\nMAP OF 12 METHODS ON CIFAR10 WITH VARYING HASH CODE LENGTHS.\nMethod 12bit 24bit 32bit 48bit\nLGDH(ours) 0.890 0.918 0.928 0.933 DCGH[16] 0.901 0.913 0.919 0.914 TSSDH[15] 0.834 0.855 0.858 0.860 DCDH[14] 0.769 0.835 0.853 0.883 DRLIH[29] 0.816 0.843 0.855 0.853 HashNet[8] 0.765 0.823 0.840 0.843 DSH[30] 0.708 0.712 0.751 0.720 NINH[31] 0.792 0.818 0.832 0.830 CNNH[32] 0.683 0.692 0.667 0.623 SDH-VGG19[33] 0.430 0.652 0.653 0.665 ITQ-VGG19[34] 0.339 0.361 0.368 0.375 SH-VGG19[35] 0.244 0.213 0.213 0.209 LSH-VGG19[36] 0.133 0.171 0.178 0.198 SDH 0.255 0.330 0.344 0.360 ITQ 0.158 0.163 0.168 0.169\nSH 0.124 0.125 0.125 0.126\nLSH 0.116 0.121 0.124 0.131\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nFigure 5(a) shows Precision-Recall curves obtained from Hamming Ranking with a 48-bit hash code length. The precision of LGDH proposed in (a), (b) and (c) is always optimal. Most traditional methods are less accurate when using longer hash codes. This is because for longer hash codes (48-bit), the number of images sharing the same Hamming code decreases exponentially, which will cause some queries to fail to return images with a Hamming code radius of 2. However, our proposed LGDH is still the best, which indicates that our proposed algorithm has strong robustness on longer hash codes. Figure 5(b) shows Precision achieved within a Hamming radius of 2 utilizing hash lookup. Figure 5(c) shows Top-k Precision for Retrieved Results, and we can also observe that our proposed LGDH achieves the best precision compared to the most advanced methods."
        },
        {
            "heading": "2) EXPERIMENT RESULTS ON NUS-WIDE DATASET.",
            "text": "with an average of 0.886, thereby reinforcing its exceptional efficacy in the realm of information retrieval assignments. TABLE \u2161\nMAP OF 12 METHODS ON NUS-WIDE WITH VARYING HASH CODE LENGTHS.\nMethod 12bit 24bit 32bit 48bit\nLGDH(ours) 0.861 0.882 0.890 0.911 DCGH 0.850 0.878 0.887 0.892 TSSDH 0.841 0.856 0.865 0.870 DCDH 0.789 0.816 0.830 0.851 DRLIH 0.823 0.846 0.845 0.853 HashNet 0.812 0.833 0.830 0.840 DSH 0.793 0.804 0.815 0.880 NINH 0.808 0.827 0.827 0.827 CNNH 0.768 0.784 0.790 0.740 SDH-VGG19 0.730 0.797 0.818 0.830 ITQ-VGG19 0.777 0.800 0.806 0.817 SH-VGG19 0.712 0.697 0.689 0.682 LSH-VGG19 0.518 0.567 0.618 0.651 SDH 0.460 0.510 0.519 0.525 ITQ 0.472 0.478 0.483 0.476\nSH 0.452 0.445 0.443 0.437\nLSH 0.436 0.414 0.432 0.442\nFigures 6(a), (b), and (c) show Precision-Recall curves obtained from Hamming Ranking with a 48-bit hash code length, Precision achieved within a Hamming radius of 2 utilizing hash lookup and Top-k Precision for Retrieved Results. As can be seen from the figure, our algorithm is still in the optimal effect, which further demonstrates the effectiveness of our proposed LGDH."
        },
        {
            "heading": "3) EXPERIMENT RESULTS ON MIRFLICKR DATASET.",
            "text": "LGDH(ours) 0.869 0.878 0.886 0.887 DCGH 0.859 0.870 0.874 0.880 TSSDH 0.840 0.851 0.858 0.863 DCDH 0.830 0.850 0.858 0.856 DRLIH 0.796 0.811 0.810 0.814 HashNet 0.777 0.782 0.785 0.785 DSH 0.651 0.681 0.684 0.686 NINH 0.772 0.756 0.760 0.778 CNNH 0.763 0.757 0.758 0.755 SDH-VGG19 0.732 0.739 0.737 0.747 ITQ-VGG19 0.686 0.685 0.687 0.689 SH-VGG19 0.618 0.604 0.598 0.595 LSH-VGG19 0.575 0.584 0.604 0.614 SDH 0.595 0.601 0.608 0.605 ITQ 0.576 0.579 0.579 0.580\n8 VOLUME XX, 2017\nFIGURE 7. The comparative outcomes on CIFAR10. (a) Precision-Recall curves obtained from Hamming Ranking with a 48-bit hash code length; (b) Precision achieved within a Hamming radius of 2 utilizing hash lookup; (c) Top-k Precision for Retrieved Results.\nFigures 7(a), (b), and (c) still show Precision-Recall curves obtained from Hamming Ranking with a 48-bit hash code length, Precision achieved within a Hamming radius of 2 utilizing hash lookup and Top-k Precision for Retrieved Results."
        },
        {
            "heading": "C. Ablation Studies",
            "text": "In order that the components proposed by our method are valid, detailed ablation experiments are performed on Cifar10, NUS-WIDE, and MIRFlickr datasets. As shown in\nLGDH method without Fusion Model: To assess the effectiveness of the fusion feature module in academic research context, we conducted comparative experiments by systematically removing the fusion model component from three distinct datasets. This experimental design facilitates the evaluation of the fusion feature module's impact on performance across diverse contexts.\nLGDH placement without Decoder: We removed the Decoder module and performed ablation experiments on three data sets to prove the validity of the Decoder module. Because it effectively constructs a robust public binary code space, the Decoder module is effective from the experimental results."
        },
        {
            "heading": "V. Conclusion",
            "text": "In this paper, we introduce a depth image hashing approach that leverages both local feature fusion and global feature fusion. Firstly, we design a two-channel feature extraction\nnetwork called global feature fusion network. To address the issue of insufficient se-mantic correlation in current deep hashing approaches, we introduce a novel approach: deep orthogonal fusion, which leverages the feature embeddings extracted from both the VGG network and the CeiT network. In response to the issue of effectively utilizing prior knowledge in hash function learning, we employ LSTM modeling to construct a resilient public binary code space. Compared with multiple cutting-edge algorithms in three public datasets of CIFAR10, NUSWIDE and MIRFLICKR, our proposed method has achieved the Sota level, which can prove the effectiveness of our proposed method."
        },
        {
            "heading": "ACKNOWLEDGMENT",
            "text": "The authors are deeply grateful to our research collaborators and colleagues whose invaluable insights, discussions, and support have played a pivotal role in shaping the direction of this work."
        }
    ],
    "title": "Depth image hashing algorithm based on local global feature fusion",
    "year": 2023
}