{
    "abstractText": "Federated Learning is an evolving machine learning paradigm, in which multiple clients perform computations based on their individual private data, interspersed by communication with a remote server. A common strategy to curtail communication costs is Local Training, which consists in performing multiple local stochastic gradient descent steps between successive communication rounds. However, the conventional approach to local training overlooks the practical necessity for client-specific personalization, a technique to tailor local models to individual needs. We introduce Scafflix, a novel algorithm that efficiently integrates explicit personalization with local training. This innovative approach benefits from these two techniques, thereby achieving doubly accelerated communication, as we demonstrate both in theory and practice. 1 ar X iv :2 30 5. 13 17 0v 1 [ cs .L G ] 2 2 M ay 2 02 3",
    "authors": [
        {
            "affiliations": [],
            "name": "Kai Yi"
        },
        {
            "affiliations": [],
            "name": "Laurent Condat"
        },
        {
            "affiliations": [],
            "name": "Peter Richt\u00e1rik"
        },
        {
            "affiliations": [],
            "name": "King Abdullah"
        }
    ],
    "id": "SP:9f237dd3d632f1d3f864b7e5d61195e52bc9a637",
    "references": [
        {
            "authors": [
                "S. Alam",
                "L. Liu",
                "M. Yan",
                "M. Zhang"
            ],
            "title": "Fedrolex: Model-heterogeneous federated learning with rolling sub-model extraction",
            "venue": "arXiv preprint arXiv:2212.01548,",
            "year": 2022
        },
        {
            "authors": [
                "M.G. Arivazhagan",
                "V. Aggarwal",
                "A.K. Singh",
                "S. Choudhary"
            ],
            "title": "Federated learning with personalization layers",
            "venue": "arXiv preprint arXiv:1912.00818,",
            "year": 2019
        },
        {
            "authors": [
                "H.H. Bauschke",
                "P.L. Combettes"
            ],
            "title": "Convex Analysis and Monotone Operator Theory in Hilbert Spaces",
            "year": 2017
        },
        {
            "authors": [
                "D. Bui",
                "K. Malik",
                "J. Goetz",
                "H. Liu",
                "S. Moon",
                "A. Kumar",
                "K.G. Shin"
            ],
            "title": "Federated user representation learning",
            "year": 1909
        },
        {
            "authors": [
                "S. Caldas",
                "P. Wu",
                "T. Li",
                "J. Kone\u010dn\u00fd",
                "H.B. McMahan",
                "V. Smith",
                "A. Talwalkar"
            ],
            "title": "LEAF: A benchmark for federated settings",
            "venue": "arXiv preprint arXiv:1812.01097,",
            "year": 2018
        },
        {
            "authors": [
                "C.-C. Chang",
                "C.-J. Lin"
            ],
            "title": "LibSVM: A library for support vector machines",
            "venue": "ACM Transactions on Intelligent Systems and Technology (TIST),",
            "year": 2011
        },
        {
            "authors": [
                "Y. Chen",
                "X. Qin",
                "J. Wang",
                "C. Yu",
                "W. Gao"
            ],
            "title": "Fedhealth: A federated transfer learning framework for wearable healthcare",
            "venue": "IEEE Intelligent Systems,",
            "year": 2020
        },
        {
            "authors": [
                "L. Condat",
                "P. Richt\u00e1rik"
            ],
            "title": "RandProx: Primal-dual optimization algorithms with randomized proximal updates",
            "venue": "In Proc. of Int. Conf. Learning Representations (ICLR),",
            "year": 2023
        },
        {
            "authors": [
                "L. Condat",
                "I. Agarsky",
                "P. Richt\u00e1rik"
            ],
            "title": "Provably doubly accelerated federated learning: The first theoretically successful combination of local training and compressed communication",
            "venue": "arXiv preprint arXiv:2210.13277,",
            "year": 2022
        },
        {
            "authors": [
                "L. Condat",
                "G. Malinovsky",
                "P. Richt\u00e1rik"
            ],
            "title": "TAMUNA: Accelerated federated learning with local training and partial participation",
            "year": 2023
        },
        {
            "authors": [
                "E. Gasanov",
                "A. Khaled",
                "S. Horv\u00e1th",
                "P. Richt\u00e1rik"
            ],
            "title": "Flix: A simple and communication-efficient alternative to local methods in federated learning",
            "venue": "In 24th International Conference on Artificial Intelligence and Statistics (AISTATS",
            "year": 2022
        },
        {
            "authors": [
                "E. Gorbunov",
                "F. Hanzely",
                "P. Richt\u00e1rik"
            ],
            "title": "Local SGD: unified theory and new efficient methods",
            "venue": "In NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "E. Gorbunov",
                "F. Hanzely",
                "P. Richt\u00e1rik"
            ],
            "title": "A unified theory of SGD: Variance reduction, sampling, quantization and coordinate descent",
            "venue": "In Proc. of 23rd Int. Conf. Artificial Intelligence and Statistics (AISTATS),",
            "year": 2020
        },
        {
            "authors": [
                "R.M. Gower",
                "N. Loizou",
                "X. Qian",
                "A. Sailanbayev",
                "E. Shulgin",
                "P. Richt\u00e1rik"
            ],
            "title": "SGD: General analysis and improved rates",
            "venue": "In Proc. of 36th Int. Conf. Machine Learning (ICML), volume PMLR",
            "year": 2019
        },
        {
            "authors": [
                "M. Grudzie\u0144",
                "G. Malinovsky",
                "P. Richt\u00e1rik"
            ],
            "title": "Can 5th Generation Local Training Methods Support Client Sampling? Yes",
            "venue": "In Proc. of Int. Conf. Artificial Intelligence and Statistics (AISTATS),",
            "year": 2023
        },
        {
            "authors": [
                "F. Haddadpour",
                "M. Mahdavi"
            ],
            "title": "On the convergence of local descent methods infederated learning",
            "venue": "arXiv preprint arXiv:1910.14425,",
            "year": 2019
        },
        {
            "authors": [
                "F. Hanzely",
                "P. Richt\u00e1rik"
            ],
            "title": "Federated learning of a mixture of global and local models",
            "venue": "arXiv preprint arXiv:2002.05516,",
            "year": 2020
        },
        {
            "authors": [
                "S. Horvath",
                "S. Laskaridis",
                "M. Almeida",
                "I. Leontiadis",
                "S. Venieris",
                "N. Lane"
            ],
            "title": "Fjord: Fair and accurate federated learning under heterogeneous targets with ordered dropout",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "M. Raykova",
                "H. Qi",
                "D. Ramage",
                "R. Raskar",
                "D. Song",
                "W. Song",
                "S.U. Stich",
                "Z. Sun",
                "A.T. Suresh",
                "F. Tram\u00e8r",
                "P. Vepakomma",
                "J. Wang",
                "L. Xiong",
                "Z. Xu",
                "Q. Yang",
                "F.X. Yu",
                "H. Yu",
                "S. Zhao"
            ],
            "title": "Advances and open problems in federated learning",
            "venue": "Foundations and Trends\u00aein Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "S. Karimireddy",
                "S. Kale",
                "M. Mohri",
                "S. Reddi",
                "S. Stich",
                "A. Suresh"
            ],
            "title": "SCAFFOLD: Stochastic controlled averaging for on-device federated learning",
            "venue": "In ICML,",
            "year": 2020
        },
        {
            "authors": [
                "A. Khaled",
                "K. Mishchenko",
                "P. Richt\u00e1rik"
            ],
            "title": "First analysis of local GD on heterogeneous data",
            "venue": "In NeurIPS Workshop on Federated Learning for Data Privacy and Confidentiality,",
            "year": 2019
        },
        {
            "authors": [
                "A. Khaled",
                "K. Mishchenko",
                "P. Richt\u00e1rik"
            ],
            "title": "Tighter theory for local SGD on identical and heterogeneous data",
            "venue": "In The 23rd International Conference on Artificial Intelligence and Statistics (AISTATS",
            "year": 2020
        },
        {
            "authors": [
                "D. Li",
                "J. Wang"
            ],
            "title": "Fedmd: Heterogenous federated learning via model distillation",
            "venue": "arXiv preprint arXiv:1910.03581,",
            "year": 2019
        },
        {
            "authors": [
                "Q. Li",
                "B. He",
                "D. Song"
            ],
            "title": "Model-contrastive federated learning",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "T. Li",
                "A.K. Sahu",
                "M. Zaheer",
                "M. Sanjabi",
                "A. Talwalkar",
                "V. Smith"
            ],
            "title": "Federated optimization in heterogeneous networks",
            "venue": "Proceedings of Machine learning and systems,",
            "year": 2020
        },
        {
            "authors": [
                "X. Li",
                "K. Huang",
                "W. Yang",
                "S. Wang",
                "Z. Zhang"
            ],
            "title": "On the conver-gence of FedAvg on non-IID data",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "G. Malinovsky",
                "D. Kovalev",
                "E. Gasanov",
                "L. Condat",
                "P. Richt\u00e1rik"
            ],
            "title": "From local SGD to local fixed point methods for federated learning",
            "venue": "In Proc. of 37th Int. Conf. Machine Learning (ICML),",
            "year": 2020
        },
        {
            "authors": [
                "G. Malinovsky",
                "K. Yi",
                "P. Richt\u00e1rik"
            ],
            "title": "Variance reduced Proxskip: Algorithm, theory and application to federated learning",
            "venue": "arXiv preprint arXiv:2207.04338,",
            "year": 2022
        },
        {
            "authors": [
                "A. Maranjyan",
                "M. Safaryan",
                "P. Richt\u00e1rik"
            ],
            "title": "Gradskip: Communication-accelerated local gradient methods with better computational complexity",
            "venue": "arXiv preprint arXiv:2210.16402,",
            "year": 2022
        },
        {
            "authors": [
                "B. McMahan",
                "E. Moore",
                "D. Ramage",
                "B. Ag\u00fcera y Arcas"
            ],
            "title": "Federated learning of deep networks using model averaging",
            "venue": "arXiv preprint arXiv:1602.05629,",
            "year": 2016
        },
        {
            "authors": [
                "H.B. McMahan",
                "E. Moore",
                "D. Ramage",
                "S. Hampson",
                "B. Ag\u00fcera y Arcas"
            ],
            "title": "Communicationefficient learning of deep networks from decentralized data",
            "venue": "In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics (AISTATS),",
            "year": 2017
        },
        {
            "authors": [
                "Y. Mei",
                "P. Guo",
                "M. Zhou",
                "V. Patel"
            ],
            "title": "Resource-adaptive federated learning with all-in-one neural composition",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "K. Mishchenko",
                "G. Malinovsky",
                "S. Stich",
                "P. Richt\u00e1rik"
            ],
            "title": "ProxSkip: Yes! Local gradient steps provably lead to communication acceleration! Finally",
            "venue": "In 39th International Conference on Machine Learning",
            "year": 2022
        },
        {
            "authors": [
                "A. Mitra",
                "R. Jaafar",
                "G. Pappas",
                "H. Hassani"
            ],
            "title": "Linear convergence in federated learning: Tackling client heterogeneity and sparse gradients",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2021
        },
        {
            "authors": [
                "P. Moritz",
                "R. Nishihara",
                "I. Stoica",
                "M.I. Jordan"
            ],
            "title": "SparkNet: Training deep networks in Spark",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2016
        },
        {
            "authors": [
                "D. Povey",
                "X. Zhang",
                "S. Khudanpur"
            ],
            "title": "Parallel training of DNNs with natural gradient and parameter averaging",
            "venue": "In ICLR Workshop,",
            "year": 2015
        },
        {
            "authors": [
                "J.H. Ro",
                "A.T. Suresh",
                "K. Wu"
            ],
            "title": "Fedjax: Federated learning simulation with jax",
            "venue": "arXiv preprint arXiv:2108.02117,",
            "year": 2021
        },
        {
            "authors": [
                "C. T Dinh",
                "N. Tran",
                "J. Nguyen"
            ],
            "title": "Personalized federated learning with moreau envelopes",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "walkar",
                "H. Wang",
                "B. worth",
                "S. Wu",
                "F.X. Yu",
                "H. Yuan",
                "M. Zaheer",
                "M. Zhang",
                "T. Zhang",
                "C. Zheng",
                "C. Zhu",
                "W. Zhu"
            ],
            "title": "A field guide to federated optimization",
            "venue": "arXiv preprint arXiv:2107.06917,",
            "year": 2021
        },
        {
            "authors": [
                "H. Yang",
                "H. He",
                "W. Zhang",
                "X. Cao"
            ],
            "title": "Fedsteg: A federated transfer learning framework for secure image steganalysis",
            "venue": "IEEE Transactions on Network Science and Engineering,",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "ar X\niv :2\n30 5.\n13 17\n0v 1\n[ cs\n.L G\n] 2\n2 M\nContents"
        },
        {
            "heading": "1 Introduction 3",
            "text": ""
        },
        {
            "heading": "2 Related work 4",
            "text": "2.1 Local Training (LT) methods in Federated Learning (FL) . . . . . . . . . . . . . . . 4 2.2 Personalization in FL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5"
        },
        {
            "heading": "3 Proposed algorithm Scafflix and convergence analysis 6",
            "text": ""
        },
        {
            "heading": "4 Experiments 9",
            "text": "4.1 Prelude: Convex Logistic Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 4.2 Neural Network Training: Datasets and Baselines for Evaluation . . . . . . . . . . . 10 4.3 Analysis of Generalization with Limited Communication Rounds . . . . . . . . . . . 11 4.4 Key Ablation Studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11"
        },
        {
            "heading": "5 Conclusion 12",
            "text": ""
        },
        {
            "heading": "A Proposed i-Scaffnew algorithm 16",
            "text": ""
        },
        {
            "heading": "B From i-Scaffnew to Scafflix 20",
            "text": ""
        },
        {
            "heading": "C Proof of Corollary 2 20",
            "text": "D Additional Experimental Results 21"
        },
        {
            "heading": "1 Introduction",
            "text": "Due to privacy concerns and limited computing resources on edge devices, centralized training with all data first gathered in a datacenter is often impossible in many real-world applications of data science and artificial intelligence. As a result, Federated Learning (FL) has gained increasing interest as a framework that enables multiple clients to do local computations, based on their personal data kept private, and to communicate back and forth with a server. FL is classically formulated as an empirical risk minimization problem of the form\nmin x\u2208Rd\n[ f(x) := 1\nn n\u2211 i=1 fi(x)\n] , (ERM)\nwhere fi is the local objective on client i, n is the total number of clients, x is the global model. Thus, the usual approach is to solve (ERM) and then to deploy the obtained globally optimal model x? := arg minx\u2208Rd f(x) to all clients. To reduce communication costs between the server and the clients, the practice of updating the local parameters multiple times before aggregation, known as Local Training (LT) [Povey et al., 2015, Moritz et al., 2016, McMahan et al., 2017, Li et al., 2020b, Haddadpour and Mahdavi, 2019, Khaled et al., 2019, 2020, Karimireddy et al., 2020, Gorbunov et al., 2020a, Mitra et al., 2021], is widely used in FL. LT, in its most modern form, is a communication-acceleration mechanism, as we detail in Section 2.1.\nMeanwhile, there is a growing interest in providing personalization to the clients, by providing them more-or-less customized models tailored to their individual needs and heterogeneous data, instead of the one-size-fits-all model x?. We review existing approaches to personalization in Section 2.2. If personalization is pushed to the extreme, every client just uses its private data to learn its own locally-optimal model\nx?i := arg min x\u2208Rd fi(x)\nand no communication at all is needed. Thus, intuitively, more personalization means less communication needed to reach a given accuracy. In other words, personalization is a communication-acceleration mechanism, like LT.\nTherefore, we raise the following question:"
        },
        {
            "heading": "Is it possible to achieve double communication acceleration in FL by jointly leveraging",
            "text": "the acceleration potential of personalization and local training?\nFor this purpose, we first have to formulate personalized FL as an optimization problem. A compelling interpretation of LT [Hanzely and Richt\u00e1rik, 2020] is that it amounts to solve an implicit personalization objective of the form:\nmin x1,...,xn\u2208Rd\n1\nn n\u2211 i=1 fi(xi) + \u03bb 2n n\u2211 i=1 \u2016x\u0304\u2212 xi\u20162, (1)\nwhere xi \u2208 Rd denotes the local model at client i \u2208 [n] := {1, . . . , n}, x\u0304 := 1n \u2211n\ni=1 xi is the average of these local models, and \u03bb \u2265 0 is the implicit personalization parameter that controls the amount of personalization. When \u03bb is small, the local models tend to be trained locally. On the other hand, a larger \u03bb puts more penalty on making the local models xi close to their mean x\u0304, or equivalently in\nmaking all models close to each other, by pushing towards averaging over all clients. Thus, LT is not only compatible with personalization, but can be actually used to implement it, though implicitly: there is a unique parameter \u03bb in (1) and it is difficult evaluate the amount of personalization for a given value of \u03bb.\nThe more accurate model FLIX for personalized FL was proposed by Gasanov et al. [2022]. It consists for every client i to first compute locally its personally-optimal model x?i , and then to solve the problem\nmin x\u2208Rd\nf\u0303(x) := 1\nn n\u2211 i=1 fi ( \u03b1ix+ (1\u2212 \u03b1i)x?i ) , (FLIX)\nwhere \u03b1i \u2208 [0, 1] is the explicit and individual personalization factor for client i. At the end, the personalized model used by client i is the explicit mixture\nx\u0303?i := \u03b1ix ? + (1\u2212 \u03b1i)x?i ,\nwhere x? is the solution to (FLIX). A smaller value of \u03b1i gives more weight to x?i , which means more personalization. On the other hand, if \u03b1i = 1, the client i uses the global model x? without personalization. Thus, if all \u03b1i are equal to 1, there is no personalization at all and (FLIX) reverts to (ERM). So, (FLIX) is a more general formulation of FL than (ERM). The functions in (FLIX) inherit smoothness and strong convexity from the fi, so every algorithm appropriate for (ERM) can also be applied to solve (FLIX). Gasanov et al. [2022] proposed an algorithm also called FLIX to solve (FLIX), which is simply vanilla distributed gradient descent (GD) applied to (FLIX).\nIn this paper, we first redesign and generalize the recently-proposed Scaffnew algorithm [Mishchenko et al., 2022], which features LT and has an accelerated communication complexity, and propose Individualized-Scaffnew (i-Scaffnew), wherein the clients can have different properties. We then apply and tune i-Scaffnew for the problem (FLIX) and propose our new algorithm for personalized FL, which we call Scafflix. We answer positively to the question above and prove that Scafflix enjoys a doubly accelerated communication complexity, by jointly harnessing the acceleration potential of LT and personalization. That is, its communication complexity depends on the square root of the condition number of the functions fi and on the \u03b1i. In addition to establishing the new state of the art for personalized FL with our theoretical guarantees, we show by extensive experiments that Scafflix is efficient in real-world learning setups and outperforms existing algorithms."
        },
        {
            "heading": "2 Related work",
            "text": ""
        },
        {
            "heading": "2.1 Local Training (LT) methods in Federated Learning (FL)",
            "text": "Theoretical evolutions of LT in FL have been long-lasting, spanning five generations from empirical results to accelerated communication complexity. The celebrated FedAvg algorithm proposed by McMahan et al. [2017] showed the feasibility of communication-efficient learning from decentralized data. It belongs to the first generation of LT methods, where the focus was on empirical results and practical validations [Povey et al., 2015, Moritz et al., 2016, McMahan et al., 2017].\nThe second generation of studies on LT for solving (ERM) was based on homogeneity assumptions, such as bounded gradients ( \u2203c < +\u221e, \u2016\u2207fi(x)\u2016 \u2264 c, x \u2208 Rd, i \u2208 [n] ) [Li et al., 2020b] and bounded\ngradient diversity ( 1 n \u2211n i=1 \u2016\u2207fi(x)\u2016 2 \u2264 c\u2016\u2207f(x)\u20162 ) [Haddadpour and Mahdavi, 2019]. However, these assumptions are too restrictive and do not hold in practical FL settings [Kairouz et al., 2019, Wang et al., 2021].\nThe third generation of approaches, under generic assumptions on the convexity and smoothness of the functions, exhibited sublinear convergence [Khaled et al., 2019, 2020] or linear convergence to a neighborhood [Malinovsky et al., 2020].\nRecently, popular algorithms have emerged, such as Scaffold [Karimireddy et al., 2020], S-LocalGD [Gorbunov et al., 2020a], and FedLin [Mitra et al., 2021], successfully correcting for the client drift and enjoying linear convergence to an exact solution under standard assumptions. However, their communication complexity remains the same as with GD, namely O(\u03ba log \u22121), where \u03ba := L/\u00b5 is the condition number.\nFinally, Scaffnew was proposed by Mishchenko et al. [2022], with accelerated communication complexity O( \u221a \u03ba log \u22121). This is a major achievement, which proves for the first time that LT is a communication acceleration mechanism. Thus, Scaffnew is the first algorithm in what can be considered the fifth generation of LT-based methods with accelerated convergence. Subsequent works have further extended Scaffnew with features such as variance-reduced stochastic gradients [Malinovsky et al., 2022], compression [Condat et al., 2022], partial client participation [Condat et al., 2023], asynchronous communication of different clients [Maranjyan et al., 2022], and to a general primal\u2013dual framework [Condat and Richt\u00e1rik, 2023]. The fifth generation of LT-based methods also includes the 5GCS algorithm [Grudzie\u0144 et al., 2023], based on a different approach: the local steps correspond to an inner loop to compute a proximity operator inexactly. Our proposed algorithm Scafflix generalizes Scaffnew and enjoys even better accelerated communication complexity, thanks to a better dependence on the possibly different condition numbers of the functions fi."
        },
        {
            "heading": "2.2 Personalization in FL",
            "text": "We can distinguish three main approaches to achieve personalization: a) One-stage training of a single global model using personalization algorithms. One common scheme is to design a suitable regularizer to balance between current and past local models [Li et al., 2021] or between global and local models [Li et al., 2020a, Hanzely and Richt\u00e1rik, 2020]. The FLIX model [Gasanov et al., 2022] achieves explicit personalization by balancing the local and global model using interpolation. Meta-learning is also popular in this thread, as evidenced by T Dinh et al. [2020], which proposes a federated meta-learning framework that utilizes Moreau envelopes and a regularizer to balance personalization and generalization.\nb) Training a global model and fine-tuning every local client or knowledge transfer/distillation. This approach allows knowledge transfer from a source domain trained in the FL manner to target domains [Li and Wang, 2019], which is especially useful for personalization in healthcare domains [Chen et al., 2020, Yang et al., 2020].\nc) Collaborative training between the global model and local models. The basic idea behind this approach is that each local client trains some personalized parts of a large model, such as the last few layers of a neural network. Parameter decoupling enables learning of task-specific representations for better personalization [Arivazhagan et al., 2019, Bui et al., 2019], while channel sparsity encourages each local client to train the neural network with sparsity based on their limited computation resources [Horvath et al., 2021, Alam et al., 2022, Mei et al., 2022].\nDespite the significant progress made in FL personalization, many approaches only present empirical results. Our approach benefits from the simplicity and efficiency of the FLIX framework and enjoys accelerated convergence.\nAlgorithm 1 Scafflix for (FLIX)\n1: input: stepsizes \u03b31 > 0, . . . , \u03b3n > 0; probability p \u2208 (0, 1]; initial estimates x01, . . . , x0n \u2208 Rd and h01, . . . , h 0 n \u2208 Rd such that \u2211n i=1 h 0 i = 0, personalization weights \u03b11, . . . , \u03b1n\n2: at the server, \u03b3 := ( 1 n \u2211n i=1 \u03b1 2 i \u03b3 \u22121 i )\u22121 \u03b3 is used by the server at Step 11 3: at clients in parallel, x?i := arg min fi not needed if \u03b1i = 1 4: for t = 0, 1, . . . do 5: flip a coin \u03b8t := {1 with probability p, 0 otherwise} 6: for i = 1, . . . , n, at clients in parallel, do 7: x\u0303ti := \u03b1ix t i + (1\u2212 \u03b1i)x?i estimate of the personalized model x\u0303?i 8: compute an estimate gti of \u2207fi(x\u0303ti) 9: x\u0302ti := x t i \u2212 \u03b3i \u03b1i ( gti \u2212 hti ) local SGD step\n10: if \u03b8t = 1 then 11: send \u03b1\n2 i \u03b3i x\u0302ti to the server, which aggregates x\u0304 t := \u03b3n \u2211n j=1 \u03b12i \u03b3i x\u0302tj and broadcasts it to all\nclients communication, but only with small probability p 12: xt+1i := x\u0304 t 13: ht+1i := h t i + p\u03b1i \u03b3i ( x\u0304t \u2212 x\u0302ti ) update of the local control variate hti 14: else 15: xt+1i := x\u0302 t i 16: ht+1i := h t i 17: end if 18: end for 19: end for"
        },
        {
            "heading": "3 Proposed algorithm Scafflix and convergence analysis",
            "text": "We generalize Scaffnew [Mishchenko et al., 2022] and propose Individualized-Scaffnew (i-Scaffnew), shown as Algorithm 2 in the Appendix. Its novelty with respect to Scaffnew is to make use of different stepsizes \u03b3i for the local SGD steps, in order to exploit the possibly different values of Li and \u00b5i, as well as the different properties Ai and Ci of the stochastic gradients. This change is not straightforward and requires to rederive the whole proof with a different Lyapunov function and to formally endow Rd with a different inner product at every client.\nWe then apply and tune i-Scaffnew for the problem (FLIX) and propose our new algorithm for personalized FL, which we call Scafflix, shown as Algorithm 1.\nWe analyze Scafflix in the strongly convex case, because the analysis of linear convergence rates in this setting gives clear insights and allows us to deepen our theoretical understanding of LT and personalization. And to the best of our knowledge, there is no analysis of Scaffnew in the nonconvex setting. But we conduct several nonconvex deep learning experiments to show that our theoretical findings also hold in practice.\nAssumption 1 (Smoothness and strong convexity). In the problem (FLIX) (and (ERM) as the particular case \u03b1i \u2261 1), we assume that for every i \u2208 [n], the function fi is Li-smooth and \u00b5i-strongly convex,1 for some Li \u2265 \u00b5i > 0. This implies that the problem is strongly convex, so that its solution x? exists and is unique.\n1A function f : Rd \u2192 R is said to be L-smooth if it is differentiable and its gradient is Lipschitz continuous with constant L; that is, for every x \u2208 Rd and y \u2208 Rd, \u2016\u2207f(x) \u2212 \u2207f(y)\u2016 \u2264 L\u2016x \u2212 y\u2016, where, here and throughout the\nWe also make the two following assumptions on the stochastic gradients gti used in Scafflix (and i-Scaffnew as a particular case with \u03b1i \u2261 1).\nAssumption 2 (Unbiasedness). We assume that for every t \u2265 0 and i \u2208 [n], gti is an unbiased estimate of \u2207fi(x\u0303ti); that is,\nE [ gti | x\u0303ti ] = \u2207fi(x\u0303ti).\nTo characterize unbiased stochastic gradient estimates, the modern notion of expected smoothness is well suited [Gower et al., 2019, Gorbunov et al., 2020b]:\nAssumption 3 (Expected smoothness). We assume that, for every i \u2208 [n], there exist constants Ai \u2265 Li 2 and Ci \u2265 0 such that, for every t \u2265 0,\nE [\u2225\u2225gti \u2212\u2207fi(x\u0303?i )\u2225\u22252 | x\u0303ti] \u2264 2AiDfi(x\u0303ti, x\u0303?i ) + Ci, (2)\nwhere D\u03d5(x, x\u2032) := f(x)\u2212 f(x\u2032)\u2212 \u3008\u2207f(x\u2032), x\u2212 x\u2032\u3009 \u2265 0 denotes the Bregman divergence of a function \u03d5 at points x, x\u2032 \u2208 Rd.\nThus, unlike the analysis in Mishchenko et al. [2022][Assumption 4.1], where the same constants are assumed for all clients, since we consider personalization, we individualize the analysis: we consider that each client can be different and use stochastic gradients characterized by its own constants Ai and Ci. This is more representative of practical settings. Assumption 3 is general and covers in particular the following two important cases [Gower et al., 2019]:\n1. (bounded variance) If gti is equal to \u2207fi(x\u0303ti) plus a zero-mean random error of variance \u03c32i (this covers the case of the exact gradient gti = \u2207fi(x\u0303ti) with \u03c3i = 0), then Assumption 3 is satisfied with Ai = Li and Ci = \u03c32i .\n2. (sampling) If fi = 1ni \u2211ni j=1 fi,j for some Li-smooth functions fi,j and g t i = \u2207fi,jt(x\u0303ti) for\nsome jt chosen uniformly at random in [ni], then Assumption 3 is satisfied with Ai = 2Li and Ci = ( 2 ni \u2211ni j=1 \u2016\u2207fi,j(x\u0303?i )\u2016 2 ) \u2212 2 \u2016\u2207fi(x\u0303?i )\u20162 (this can be extended to minibatch and nonuniform sampling).\nWe now present our main convergence result:\nTheorem 1 (fast linear convergence). In (FLIX) and Scafflix, suppose that Assumptions 1, 2, 3 hold and that for every i \u2208 [n], 0 < \u03b3i \u2264 1Ai . For every t \u2265 0, define the Lyapunov function\n\u03a8t := 1\nn n\u2211 i=1 \u03b3min \u03b3i \u2225\u2225x\u0303ti \u2212 x\u0303?i \u2225\u22252 + \u03b3minp2 1n n\u2211 i=1 \u03b3i \u2225\u2225hti \u2212\u2207fi(x\u0303?i )\u2225\u22252 , (3)\npaper, the norm is the Euclidean norm. f is said to be \u00b5-strongly convex if f \u2212 \u00b5 2 \u2016 \u00b7 \u20162 is convex. We refer to Bauschke and Combettes [2017] for such standard notions of convex analysis. 2We can suppose Ai \u2265 Li. Indeed, we have the bias-variance decomposition E [\u2225\u2225gti \u2212\u2207fi(x\u0303?i )\u2225\u22252 | x\u0303ti] =\u2225\u2225\u2207fi(x\u0303ti)\u2212\u2207fi(x\u0303?i )\u2225\u22252 + E[\u2225\u2225gti \u2212\u2207fi(x\u0303ti)\u2225\u22252 | x\u0303ti] \u2265 \u2225\u2225\u2207fi(x\u0303ti)\u2212\u2207fi(x\u0303?i )\u2225\u22252. Assuming that Li is the best known smoothness constant of fi, we cannot improve the constant Li such that for every x \u2208 Rd, \u2016\u2207fi(x)\u2212\u2207fi(x\u0303?i )\u20162 \u2264 2LiDfi(x, x\u0303 ? i ). Therefore, Ai in (2) has to be \u2265 Li.\nwhere \u03b3min := mini\u2208[n] \u03b3i. Then Scafflix converges linearly: for every t \u2265 0,\nE [ \u03a8t ] \u2264 (1\u2212 \u03b6)t\u03a80 + \u03b3min\n\u03b6\n1\nn n\u2211 i=1 \u03b3iCi, (4)\nwhere \u03b6 = min ( min i\u2208[n] \u03b3i\u00b5i, p 2 ) . (5)\nIt is important to note that the range of the stepsizes \u03b3i, the Lyapunov function \u03a8t and the convergence rate in (4)\u2013(5) do not depend on the personalization weights \u03b1i; they only play a role in the definition of the personalized models x\u0303ti and x\u0303 ? i . Indeed, the convergence speed essentially\ndepends on the conditioning of the functions x 7\u2192 fi ( \u03b1ix+ (1\u2212 \u03b1i)x?i ) , which are independent from the \u03b1i. More precisely, let us define, for every i \u2208 [n],\n\u03bai := Li \u00b5i \u2265 1 and \u03bamax = max i\u2208[n] \u03bai,\nand let us study the complexity of of Scafflix to reach -accuracy, i.e. E [ \u03a8t ] \u2264 . If, for every i \u2208 [n], Ci = 0, Ai = \u0398(Li), and \u03b3i = \u0398( 1Ai ) = \u0398( 1 Li ), the iteration complexity of Scafflix is\nO (( \u03bamax + 1\np2\n) log(\u03a80 \u22121) ) . (6)\nAnd since communication occurs with probability p, the communication complexity of Scafflix is O (( p\u03bamax + 1\np\n) log(\u03a80 \u22121) ) . (7)\nNote that \u03bamax can be much smaller than \u03baglobal := maxi Limini \u00b5i , which is the condition number that appears in the rate of Scaffnew with \u03b3 = 1maxi Ai . Thus, Scafflix is much more versatile and adapted to FL with heterogeneous data than Scaffnew.\nCorollary 1 (case Ci \u2261 0). In the conditions of Theorem 1, if p = \u0398 (\n1\u221a \u03bamax\n) and, for every i \u2208 [n],\nCi = 0, Ai = \u0398(Li), and \u03b3i = \u0398( 1Ai ) = \u0398( 1 Li ), the communication complexity of Scafflix is O (\u221a \u03bamax log(\u03a8 0 \u22121) ) . (8) Corollary 2 (general stochastic gradients). In the conditions of Theorem 1, if p = \u221a\nmini\u2208[n] \u03b3i\u00b5i and, for every i \u2208 [n],\n\u03b3i = min\n( 1\nAi , \u00b5min 2Ci\n) (9)\n(or \u03b3i := 1Ai if Ci = 0), where \u00b5min := minj\u2208[n] \u00b5j, the iteration complexity of Scafflix is O ((\nmax i\u2208[n] max ( Ai \u00b5i , Ci \u00b5min\u00b5i )) log(\u03a80 \u22121) ) =O ( max ( max i\u2208[n] Ai \u00b5i ,max i\u2208[n] Ci \u00b5min\u00b5i ) log(\u03a80 \u22121) ) (10)\nand its communication complexity is\nO ( max ( max i\u2208[n] \u221a Ai \u00b5i ,max i\u2208[n] \u221a Ci \u00b5min\u00b5i ) log(\u03a80 \u22121) ) . (11)\nIf Ai = \u0398(Li) uniformly, we have maxi\u2208[n] \u221a\nAi \u00b5i\n= \u0398( \u221a \u03bamax). Thus, we see that thanks to LT,\nthe communication complexity of Scafflix is accelerated, as it depends on \u221a \u03bamax and 1\u221a .\nIn the expressions above, the acceleration effect of personalization is not visible: it is \u201chidden\u201d in \u03a80, because every client computes xti but what matters is its personalized model x\u0303\nt i, and\u2225\u2225x\u0303ti \u2212 x\u0303?i \u2225\u22252 = \u03b12i \u2225\u2225xti \u2212 x?\u2225\u22252. In particular, assuming that x01 = \u00b7 \u00b7 \u00b7 = x0n = x0 and h0i = \u2207fi(x\u03030i ), we have\n\u03a80 \u2264 \u03b3min n \u2225\u2225x0 \u2212 x?\u2225\u22252 n\u2211 i=1 \u03b12i ( 1 \u03b3i + \u03b3iL 2 i p2 ) \u2264 ( max i \u03b12i )\u03b3min n \u2225\u2225x0 \u2212 x?\u2225\u22252 n\u2211 i=1 ( 1 \u03b3i + \u03b3iL 2 i p2 ) ,\nand we see that the contribution of every client to the initial gap \u03a80 is weighted by \u03b12i . Thus, the smaller the \u03b1i, the smaller \u03a80 and the faster the convergence. This is why personalization is an acceleration mechanism in our setting."
        },
        {
            "heading": "4 Experiments",
            "text": "We first consider a convex logistic regression problem to show that the empirical behavior of Scafflix is in accordance with the theoretical convergence guarantees available in the convex case. Then, we make extensive experiments of training neural networks on large-scale distributed datasets.3\n3Code is available at https://github.com/WilliamYi96/Scafflix."
        },
        {
            "heading": "4.1 Prelude: Convex Logistic Regression",
            "text": "We begin our evaluation by considering the standard convex logistic regression problem with an l2 regularizer. This benchmark problem is takes the form (ERM) with\nfi(x) := 1\nni ni\u2211 j=1 log ( 1 + exp(\u2212bi,jxTai,j) ) + \u00b5 2 \u2016x\u20162, (12)\nwhere \u00b5 represents the regularization parameter, ni is the total number of data points present at client i; ai,j are the training vectors and the bi,j \u2208 {\u22121, 1} are the corresponding labels. Every function fi is \u00b5-strongly convex and Li-smooth with Li = 14ni \u2211ni j=1 \u2016ai,j\u2016\n2 + \u00b5. We set \u00b5 to 0.1 for this experiment. We employ the mushrooms, a6a, and w6a datasets from the LibSVM library [Chang and Lin, 2011] to conduct these tests. The data is distributed evenly across all clients, and the \u03b1i are set to the same value. The results are shown in Fig. 1. We can observe the double acceleration effect of our approach, which combines explicit personalization and accelerated local training. Lower \u03b1i values, i.e. more personalization, yield faster convergence for both GD and Scafflix. Moreover, Scafflix is much faster than GD, thanks to its specialized local training mechanism."
        },
        {
            "heading": "4.2 Neural Network Training: Datasets and Baselines for Evaluation",
            "text": "To assess the generalization capabilities of Scafflix, we undertake a comprehensive evaluation involving the training of neural networks using two widely-recognized large-scale FL datasets.\nDatasets. Our selection comprises two notable large-scale FL datasets: Federated Extended MNIST (FEMNIST) [Caldas et al., 2018], and Shakespeare [McMahan et al., 2017]. FEMNIST is a character recognition dataset consisting of 671,585 samples. In accordance with the methodology outlined in FedJax [Ro et al., 2021], we distribute these samples randomly across 3,400 devices. For all algorithms, we employ a Convolutional Neural Network (CNN) model, featuring two convolutional layers and one fully connected layer. The Shakespeare dataset, used for next character prediction tasks, contains a total of 16,068 samples, which we distribute randomly across 1,129 devices. For all algorithms applied to this dataset, we use a Recurrent Neural Network (RNN) model, comprising two Long Short-Term Memory (LSTM) layers and one fully connected layer.\nBaselines. The performance of our proposed Scafflix algorithm is benchmarked against prominent baseline algorithms, specifically FLIX [Gasanov et al., 2022] and FedAvg [McMahan et al., 2016]. The FLIX algorithm optimizes the FLIX objective utilizing the SGD method, while FedAvg is designed to optimize the ERM objective. We employ the official implementations for these benchmark algorithms. Comprehensive hyperparameter tuning is carried out for all algorithms, including Scafflix, to ensure optimal results. For both FLIX and Scafflix, local training is required to achieve the local minima for each client. By default, we set the local training batch size at 100 and employ SGD with a learning rate selected from the set Cs := {10\u22125, 10\u22124, \u00b7 \u00b7 \u00b7 , 1}. Upon obtaining the local optimum, we execute each algorithm with a batch size of 20 for 1000 communication rounds. The model\u2019s learning rate is also selected from the set Cs."
        },
        {
            "heading": "4.3 Analysis of Generalization with Limited Communication Rounds",
            "text": "In this section, we perform an in-depth examination of the generalization performance of Scafflix, particularly in scenarios with a limited number of training epochs. This investigation is motivated by our theoretical evidence of the double acceleration property of Scafflix. To that aim, we conduct experiments on both FEMNIST and Shakespeare. These two datasets offer a varied landscape of complexity, allowing for a comprehensive evaluation of our algorithm. In order to ensure a fair comparison with other baseline algorithms, we conducted an extensive search of the optimal hyperparameters for each algorithm. The performance assessment of the generalization capabilities was then carried out on a separate, held-out validation dataset. The hyperparameters that gave the best results in these assessments were selected as the most optimal set.\nIn order to examine the impact of personalization, we assume that all clients have same \u03b1i \u2261 \u03b1 and we select \u03b1 in {0.1, 0.3, 0.5, 0.7, 0.9}. We present the results corresponding to \u03b1 = 0.1 in Fig. 2. Additional comparative analyses with other values of \u03b1 are available in the Appendix. As shown in Fig. 2, it is clear that Scafflix outperforms the other algorithms in terms of generalization on both the FEMNIST and Shakespeare datasets. Interestingly, the Shakespeare dataset (next-word prediction) poses a greater challenge compared to the FEMNIST dataset (digit recognition). Despite the increased complexity of the task, Scafflix not only delivers significantly better results but also achieves this faster. Thus, Scafflix is superior both in speed and accuracy."
        },
        {
            "heading": "4.4 Key Ablation Studies",
            "text": "In this section, we conduct several critical ablation studies to verify the efficacy of our proposed Scafflix method. These studies investigate the optimal personalization factor for Scafflix, assess the impact of the number of clients per communication round, and examine the influence of the communication probability p in Scafflix.\nOptimal Personalization Factor. In this experiment, we explore the effect of varying personalization factors on the FEMNIST dataset. The results are presented in Fig. 3a. We set the batch size\nto 128 and determine the most suitable learning rate through a hyperparameter search. We consider linearly increasing personalization factors within the set {0.1, 0.3, 0.5, 0.7, 0.9}. An exponential scale for \u03b1 is also considered in the Appendix, but the conclusion remains the same.\nWe note that the optimal personalization factor for the FEMNIST dataset is 0.3. Interestingly, personalization factors that yield higher accuracy also display a slightly larger variance. However, the overall average performance remains superior. This is consistent with expectations as effective personalization may emphasize the representation of local data, and thus, could be impacted by minor biases in the model parameters received from the server.\nNumber of Clients Communicating per Round. In this ablation study, we examine the impact of varying the number of participating clients in each communication round within the Scafflix framework. By default, we set this number to 10. Here, we conduct extensive experiments with different client numbers per round, choosing \u03c4 from {1, 5, 10, 20}. The results are presented in Fig. 3b. We can observe that Scafflix shows minimal sensitivity to changes in the batch size for local training. However, upon closer examination, we find that larger batch sizes, specifically \u03c4 = 10 and 20, demonstrate slightly improved generalization performance.\nSelection of Communication Probability p. In this ablation study, we explore the effects of varying the communication probability p in Scafflix. We select p from {0.1, 0.2, 0.5}, and the corresponding results are shown in Fig. 3c. We can clearly see that a smaller value of p, indicating reduced communication, facilitates faster convergence and superior generalization performance. This highlights the benefits of LT, which not only makes FL faster and more communication-efficient, but also improves the learning quality."
        },
        {
            "heading": "5 Conclusion",
            "text": "In the contemporary era of artificial intelligence, improving federated learning to achieve faster convergence and reduce communication costs is crucial to enhance the quality of models trained on huge and heterogeneous datasets. To address this challenge, we introduced Scafflix, a novel algorithm that achieves double communication acceleration by redesigning the objective to support explicit personalization for individual clients, while leveraging a state-of-the-art local training mechanism.\nWe provided complexity guarantees in the convex setting, and also validated the effectiveness of our approach in the nonconvex setting through extensive experiments and ablation studies. We believe that our work is a significant contribution to the important topic of communication-efficient federated learning and offers valuable insights for further investigation in the future."
        },
        {
            "heading": "A Proposed i-Scaffnew algorithm",
            "text": "We consider solving (ERM) with the proposed i-Scaffnew algorithm, shown as Algorithm 2 (applying i-Scaffnew to (FLIX) yields Scafflix, as we discuss subsequently in Section B).\nTheorem 2 (fast linear convergence). In (ERM) and i-Scaffnew, suppose that Assumptions 1, 2, 3 hold and that for every i \u2208 [n], 0 < \u03b3i \u2264 1Ai . For every t \u2265 0, define the Lyapunov function\n\u03a8t := n\u2211 i=1 1 \u03b3i \u2225\u2225xti \u2212 x?\u2225\u22252 + 1p2 n\u2211 i=1 \u03b3i \u2225\u2225hti \u2212\u2207fi(x?)\u2225\u22252 . (13)\nThen i-Scaffnew converges linearly: for every t \u2265 0,\nE [ \u03a8t ] \u2264 (1\u2212 \u03b6)t\u03a80 + 1\n\u03b6 n\u2211 i=1 \u03b3iCi, (14)\nwhere \u03b6 = min ( min i\u2208[n] \u03b3i\u00b5i, p 2 ) . (15)\nProof. To simplify the analysis of i-Scaffnew, we introduce vector notations: the problem (ERM) can be written as\nfind x? = arg min x\u2208X f(x) s.t. Wx = 0, (16)\nwhere X := Rd\u00d7n, an element x = (xi)ni=1 \u2208 X is a collection of vectors xi \u2208 Rd, f : x \u2208 X 7\u2192\u2211n i=1 fi(xi), the linear operator W : X \u2192 X maps x = (xi)ni=1 to (xi \u2212 1 n \u2211n j=1 \u03b3 \u03b3j xj) n i=1, for given\nvalues \u03b31 > 0, . . . , \u03b3n > 0 and their harmonic mean \u03b3 = ( 1 n \u2211n i=1 \u03b3 \u22121 i )\u22121. The constraint Wx = 0 means that x minus its weighted average is zero; that is, x has identical components x1 = \u00b7 \u00b7 \u00b7 = xn. Thus, (16) is indeed equivalent to (ERM). x? := (x?)ni=1 \u2208 X is the unique solution to (16), where x? is the unique solution to (ERM).\nMoreover, we introduce the weighted inner product in X : (x,y) 7\u2192 \u3008x,y\u3009\u03b3 := \u2211n\ni=1 1 \u03b3i \u3008xi, yi\u3009.\nThen, the orthogonal projector P onto the hyperspace {y \u2208 X : y1 = \u00b7 \u00b7 \u00b7 = yn}, with respect to this weighted inner product, is P : x \u2208 X 7\u2192 x\u0304 = (x\u0304)ni=1 with x\u0304 = \u03b3 n \u2211n i=1 1 \u03b3i xi (because x\u0304 minimizes \u2016x\u0304\u2212 x\u20162\u03b3 , so that 1 n \u2211n i=1 1 \u03b3i\n(x\u0304\u2212 xi) = 0). Thus, P , as well as W = Id\u2212 P , where Id denotes the identity, are self-adjoint and positive linear operators with respect to the weighted inner product. Moreover, for every x \u2208 X ,\n\u2016x\u20162\u03b3 = \u2016Px\u2016 2 \u03b3 + \u2016Wx\u2016 2 \u03b3 = \u2016x\u0304\u2016 2 \u03b3 + \u2016Wx\u2016 2 \u03b3 =\nn \u03b3 \u2016x\u0304\u20162 + \u2016Wx\u20162\u03b3 ,\nwhere x\u0304 = (x\u0304)ni=1 and x\u0304 = \u03b3 n \u2211n i=1 1 \u03b3i xi.\nLet us introduce further vector notations for the variables of i-Scaffnew: for every t \u2265 0, we define the scaled concatenated control variate ht := (\u03b3ihti) n i=1, h ? := (\u03b3ih ? i ) n i=1, with h ? i := \u2207fi(x?), x\u0304t := (x\u0304t)ni=1, w t := (wti) n i=1, with w t i := x t i \u2212 \u03b3igti , w? := (w?i )ni=1, with w?i := x?i \u2212 \u03b3i\u2207fi(x?i ), h\u0302t := ht \u2212 pW x\u0302t. Finally, we denote by F t0 the \u03c3-algebra generated by the collection of X -valued random variables x0,h0, . . . ,xt,ht and by F t the \u03c3-algebra generated by these variables, as well as the stochastic gradients gti .\nWe can then rewrite the iteration of i-Scaffnew as: x\u0302t := wt + ht if \u03b8t = 1 then xt+1 := x\u0304t\nht+1 := ht \u2212 pW x\u0302t else\nxt+1 := x\u0302t ht+1 := ht\nend if We suppose that \u2211n\ni=1 h 0 i = 0. Then, it follows from the definition of x\u0304 t that \u03b3n \u2211n j=1 1 \u03b3i\n(x\u0304t\u2212x\u0302tj) = 0, so that for every t \u2265 0, \u2211n i=1 h t i = 0; that is, Wh t = ht.\nLet t \u2265 0. We have E [\u2225\u2225xt+1 \u2212 x?\u2225\u22252\n\u03b3 | F t\n] = p \u2225\u2225x\u0304t \u2212 x?\u2225\u22252 \u03b3 + (1\u2212 p) \u2225\u2225x\u0302t \u2212 x?\u2225\u22252 \u03b3 ,\nwith \u2225\u2225x\u0304t \u2212 x?\u2225\u22252 \u03b3 = \u2225\u2225x\u0302t \u2212 x?\u2225\u22252 \u03b3 \u2212 \u2225\u2225W x\u0302t\u2225\u22252 \u03b3 .\nMoreover,\u2225\u2225x\u0302t \u2212 x?\u2225\u22252 \u03b3 = \u2225\u2225wt \u2212w?\u2225\u22252 \u03b3 + \u2225\u2225ht \u2212 h?\u2225\u22252 \u03b3 + 2\u3008wt \u2212w?,ht \u2212 h?\u3009\u03b3\n= \u2225\u2225wt \u2212w?\u2225\u22252 \u03b3 \u2212 \u2225\u2225ht \u2212 h?\u2225\u22252 \u03b3 + 2\u3008x\u0302t \u2212 x?,ht \u2212 h?\u3009\u03b3\n= \u2225\u2225wt \u2212w?\u2225\u22252 \u03b3 \u2212 \u2225\u2225ht \u2212 h?\u2225\u22252 \u03b3 + 2\u3008x\u0302t \u2212 x?, h\u0302t \u2212 h?\u3009\u03b3 \u2212 2\u3008x\u0302t \u2212 x?, h\u0302t \u2212 ht\u3009\u03b3\n= \u2225\u2225wt \u2212w?\u2225\u22252 \u03b3 \u2212 \u2225\u2225ht \u2212 h?\u2225\u22252 \u03b3 + 2\u3008x\u0302t \u2212 x?, h\u0302t \u2212 h?\u3009\u03b3 + 2p\u3008x\u0302t \u2212 x?,W x\u0302t\u3009\u03b3\n= \u2225\u2225wt \u2212w?\u2225\u22252 \u03b3 \u2212 \u2225\u2225ht \u2212 h?\u2225\u22252 \u03b3 + 2\u3008x\u0302t \u2212 x?, h\u0302t \u2212 h?\u3009\u03b3 + 2p \u2225\u2225W x\u0302t\u2225\u22252 \u03b3 .\nHence, E [\u2225\u2225xt+1 \u2212 x?\u2225\u22252\n\u03b3 | F t\n] = \u2225\u2225x\u0302t \u2212 x?\u2225\u22252\n\u03b3 \u2212 p \u2225\u2225W x\u0302t\u2225\u22252 \u03b3\n= \u2225\u2225wt \u2212w?\u2225\u22252 \u03b3 \u2212 \u2225\u2225ht \u2212 h?\u2225\u22252 \u03b3 + 2\u3008x\u0302t \u2212 x?, h\u0302t \u2212 h?\u3009\u03b3 + p \u2225\u2225W x\u0302t\u2225\u22252 \u03b3 .\nOn the other hand, we have E [\u2225\u2225ht+1 \u2212 h?\u2225\u22252\n\u03b3 | F t\n] = p \u2225\u2225\u2225h\u0302t \u2212 h?\u2225\u2225\u22252 \u03b3 + (1\u2212 p) \u2225\u2225ht \u2212 h?\u2225\u22252 \u03b3\nand \u2225\u2225\u2225h\u0302t \u2212 h?\u2225\u2225\u22252 \u03b3 = \u2225\u2225\u2225(ht \u2212 h?) + (h\u0302t \u2212 ht)\u2225\u2225\u22252 \u03b3\n= \u2225\u2225ht \u2212 h?\u2225\u22252 \u03b3 + \u2225\u2225\u2225h\u0302t \u2212 ht\u2225\u2225\u22252\n\u03b3 + 2\u3008ht \u2212 h?, h\u0302t \u2212 ht\u3009\u03b3\n= \u2225\u2225ht \u2212 h?\u2225\u22252 \u03b3 \u2212 \u2225\u2225\u2225h\u0302t \u2212 ht\u2225\u2225\u22252\n\u03b3 + 2\u3008h\u0302t \u2212 h?, h\u0302t \u2212 ht\u3009\u03b3\n= \u2225\u2225ht \u2212 h?\u2225\u22252 \u03b3 \u2212 \u2225\u2225\u2225h\u0302t \u2212 ht\u2225\u2225\u22252\n\u03b3 \u2212 2p\u3008h\u0302t \u2212 h?,W (x\u0302t \u2212 x?)\u3009\u03b3\n= \u2225\u2225ht \u2212 h?\u2225\u22252\n\u03b3 \u2212 p2 \u2225\u2225W x\u0302t\u2225\u22252 \u03b3 \u2212 2p\u3008W (h\u0302t \u2212 h?), x\u0302t \u2212 x?\u3009\u03b3\n= \u2225\u2225ht \u2212 h?\u2225\u22252\n\u03b3 \u2212 p2 \u2225\u2225W x\u0302t\u2225\u22252 \u03b3 \u2212 2p\u3008h\u0302t \u2212 h?, x\u0302t \u2212 x?\u3009\u03b3 .\nHence, E [\u2225\u2225xt+1 \u2212 x?\u2225\u22252\n\u03b3 | F t\n] + 1 p2 E [\u2225\u2225ht+1 \u2212 h?\u2225\u22252 \u03b3 | F t ] = \u2225\u2225wt \u2212w?\u2225\u22252 \u03b3 \u2212 \u2225\u2225ht \u2212 h?\u2225\u22252 \u03b3 + 2\u3008x\u0302t \u2212 x?, h\u0302t \u2212 h?\u3009\u03b3 + p \u2225\u2225W x\u0302t\u2225\u22252 \u03b3\n+ 1 p2 \u2225\u2225ht \u2212 h?\u2225\u22252 \u03b3 \u2212 p \u2225\u2225W x\u0302t\u2225\u22252 \u03b3 \u2212 2\u3008h\u0302t \u2212 h?, x\u0302t \u2212 x?\u3009\u03b3\n= \u2225\u2225wt \u2212w?\u2225\u22252\n\u03b3 +\n1 p2 ( 1\u2212 p2 ) \u2225\u2225ht \u2212 h?\u2225\u22252 \u03b3 . (17)\nMoreover, for every i \u2208 [n],\u2225\u2225wti \u2212 w?i \u2225\u22252 = \u2225\u2225xti \u2212 x? \u2212 \u03b3i(gti \u2212\u2207fi(x?))\u2225\u22252 = \u2225\u2225xti \u2212 x?\u2225\u22252 \u2212 2\u03b3i\u3008xti \u2212 x?, gti \u2212\u2207fi(x?)\u3009+ \u03b32i \u2225\u2225gti \u2212\u2207fi(x?)\u2225\u22252 ,\nand, by unbiasedness of gti and Assumption 2,\nE [\u2225\u2225wti \u2212 w?i \u2225\u22252 | F t0] = \u2225\u2225xti \u2212 x?\u2225\u22252 \u2212 2\u03b3i\u3008xti \u2212 x?,\u2207fi(xti)\u2212\u2207fi(x?)\u3009\n+ \u03b32i E [\u2225\u2225gti \u2212\u2207fi(x?)\u2225\u22252 | F t]\n\u2264 \u2225\u2225xti \u2212 x?\u2225\u22252 \u2212 2\u03b3i\u3008xti \u2212 x?,\u2207fi(xti)\u2212\u2207fi(x?)\u3009+ 2\u03b32i AiDfi(xti, x?) + \u03b32i Ci.\nIt is easy to see that \u3008xti \u2212 x?,\u2207fi(xti)\u2212\u2207fi(x?)\u3009 = Dfi(xti, x?) +Dfi(x?, xti). This yields\nE [\u2225\u2225wti \u2212 w?i \u2225\u22252 | F t0] \u2264 \u2225\u2225xti \u2212 x?\u2225\u22252 \u2212 2\u03b3iDfi(x?, xti)\u2212 2\u03b3iDfi(xti, x?) + 2\u03b32i AiDfi(xti, x?)\n+ \u03b32i Ci.\nIn addition, the strong convexity of fi implies that Dfi(x ?, xti) \u2265 \u00b5i 2 \u2225\u2225xti \u2212 x?\u2225\u22252, so that E [\u2225\u2225wti \u2212 w?i \u2225\u22252 | F t0] \u2264 (1\u2212 \u03b3i\u00b5i) \u2225\u2225xti \u2212 x?\u2225\u22252 \u2212 2\u03b3i(1\u2212 \u03b3iAi)Dfi(xti, x?) + \u03b32i Ci,\nand since we have supposed \u03b3i \u2264 1Ai ,\nE [\u2225\u2225wti \u2212 w?i \u2225\u22252 | F t0] \u2264 (1\u2212 \u03b3i\u00b5i) \u2225\u2225xti \u2212 x?\u2225\u22252 + \u03b32i Ci.\nTherefore,\nE [\u2225\u2225wt \u2212w?\u2225\u22252\n\u03b3 | F t0\n] \u2264 max\ni\u2208[n] (1\u2212 \u03b3i\u00b5i)\n\u2225\u2225xt \u2212 x?\u2225\u22252 \u03b3 + n\u2211 i=1 \u03b3iCi\nand\nE [ \u03a8t+1 | F t0 ] = E [\u2225\u2225xt+1 \u2212 x?\u2225\u22252 \u03b3 | F t0 ] + 1 p2 E [\u2225\u2225ht+1 \u2212 h?\u2225\u22252 \u03b3 | F t0 ] \u2264 max\ni\u2208[n] (1\u2212 \u03b3i\u00b5i)\n\u2225\u2225xt \u2212 x?\u2225\u22252 \u03b3 + 1 p2 ( 1\u2212 p2 ) \u2225\u2225ht \u2212 h?\u2225\u22252 \u03b3 + n\u2211 i=1 \u03b3iCi\n\u2264 (1\u2212 \u03b6) (\u2225\u2225xt \u2212 x?\u2225\u22252\n\u03b3 +\n1 p2 \u2225\u2225ht \u2212 h?\u2225\u22252 \u03b3 ) + n\u2211 i=1 \u03b3iCi\n= (1\u2212 \u03b6)\u03a8t + n\u2211 i=1 \u03b3iCi, (18)\nwhere \u03b6 = min ( min i\u2208[n] \u03b3i\u00b5i, p 2 ) .\nUsing the tower rule, we can unroll the recursion in (18) to obtain the unconditional expectation of \u03a8t+1."
        },
        {
            "heading": "B From i-Scaffnew to Scafflix",
            "text": "We suppose that Assumptions 1, 2, 3 hold. We define for every i \u2208 [n] the function f\u0303i : x \u2208 Rd 7\u2192 fi ( \u03b1ix+ (1\u2212 \u03b1i)x?i ) . Thus, (FLIX) takes the form of (ERM) with fi replaced by f\u0303i.\nWe want to derive Scafflix from i-Scaffnew applied to (ERM) with fi replaced by f\u0303i. For this, we first observe that for every i \u2208 [n], f\u0303i is \u03b12iLi-smooth and \u03b12i\u00b5i-strongly convex. This follows easily from the fact that \u2207f\u0303i(x) = \u03b1i\u2207fi ( \u03b1ix+ (1\u2212 \u03b1i)x?i ) .\nSecond, for every t \u2265 0 and i \u2208 [n], gti is an unbiased estimate of \u2207fi(x\u0303ti) = \u03b1 \u22121 i \u2207f\u0303i(xti).\nTherefore, \u03b1igti is an unbiased estimate of \u2207f\u0303i(xti) satisfying\nE [\u2225\u2225\u2225\u03b1igti \u2212\u2207f\u0303i(x?)\u2225\u2225\u22252 | xti] = \u03b12iE[\u2225\u2225gti \u2212\u2207fi(x\u0303?i )\u2225\u22252 | x\u0303ti] \u2264 2\u03b12iAiDfi(x\u0303ti, x\u0303?i ) + \u03b12iCi.\nMoreover,\nDfi(x\u0303 t i, x\u0303 ? i ) = fi(x\u0303 t i)\u2212 fi(x\u0303?i )\u2212 \u3008\u2207fi(x\u0303?i ), x\u0303ti \u2212 x\u0303?i \u3009\n= f\u0303i(x t i)\u2212 f\u0303i(x?)\u2212 \u3008\u03b1\u22121i \u2207f\u0303i(x ?), \u03b1i(x t i \u2212 x?)\u3009 = f\u0303i(x t i)\u2212 f\u0303i(x?)\u2212 \u3008\u2207f\u0303i(x?), xti \u2212 x?\u3009 = Df\u0303i(x t i, x ?).\nThus, we obtain Scafflix by applying i-Scaffnew to solve (FLIX), viewed as (ERM) with fi replaced by f\u0303i, and further making the following substitutions in the algorithm: gti is replaced by \u03b1ig t i , h t i is replaced by \u03b1ihti (so that h t i in Scafflix converges to \u2207fi(x\u0303?i ) instead of \u2207f\u0303i(x?) = \u03b1i\u2207fi(x\u0303?i )), \u03b3i is replaced by \u03b1\u22122i \u03b3i (so that the \u03b1i disappear in the theorem). Accordingly, Theorem 1 follows from Theorem 2, with the same substitutions and with Ai, Ci and \u00b5i replaced by \u03b12iAi, \u03b1 2 iCi and \u03b1 2 i\u00b5i, respectively. Finally, the Lyapunov function is multiplied by \u03b3min/n to make it independent from when scaling the \u03b3i by in Corollary 2.\nWe note that i-Scaffnew is recovered as a particular case of Scafflix if \u03b1i \u2261 1, so that Scafflix is indeed more general."
        },
        {
            "heading": "C Proof of Corollary 2",
            "text": "We place ourselves in the conditions of Theorem 1. Let > 0. We want to choose the \u03b3i and the number of iterations T \u2265 0 such that E [ \u03a8T ] \u2264 . For this, we bound the two terms (1\u2212 \u03b6)T\u03a80 and \u03b3min \u03b6n \u2211n i=1 \u03b3iCi in (4) by /2.\nWe set p = \u221a\nmini\u2208[n] \u03b3i\u00b5i, so that \u03b6 = mini\u2208[n] \u03b3i\u00b5i. We have\nT \u2265 1 \u03b6 log(2\u03a80 \u22121)\u21d2 (1\u2212 \u03b6)T\u03a80 \u2264 2 . (19)\nMoreover,\n(\u2200i \u2208 [n] s.t. Ci > 0) \u03b3i \u2264 \u00b5min 2Ci \u21d2 \u03b3min \u03b6n n\u2211 i=1 \u03b3iCi \u2264 2\n( minj\u2208[n] \u03b3j ) ( minj\u2208[n] \u00b5j ) minj\u2208[n] \u03b3j\u00b5j \u2264 2 .\nTherefore, we set for every i \u2208 [n]\n\u03b3i := min\n( 1\nAi , \u00b5min 2Ci ) (or \u03b3i := 1Ai if Ci = 0), and we get from (19) that E [ \u03a8T ] \u2264 after\nO ((\nmax i\u2208[n] max ( Ai \u00b5i , Ci \u00b5min\u00b5i )) log(\u03a80 \u22121) ) iterations."
        },
        {
            "heading": "D Additional Experimental Results",
            "text": ""
        }
    ],
    "title": "Explicit Personalization and Local Training: Double Communication Acceleration in Federated Learning",
    "year": 2023
}