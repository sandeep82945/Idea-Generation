{
    "abstractText": "Implicit Neural Representation (INR) is an innovative approach for representing complex shapes or objects without explicitly defining their geometry or surface structure. Instead, INR represents objects as continuous functions. Previous research has demonstrated the effectiveness of using neural networks as INR for image compression, showcasing comparable performance to traditional methods such as JPEG. However, INR holds potential for various applications beyond image compression. This paper introduces Rapid-INR, a novel approach that utilizes INR for encoding and compressing images, thereby accelerating neural network training in computer vision tasks. Our methodology involves storing the whole dataset directly in INR format on a GPU, mitigating the significant data communication overhead between the CPU and GPU during training. Additionally, the decoding process from INR to RGB format is highly parallelized and executed on-the-fly. To further enhance compression, we propose iterative and dynamic pruning, as well as layer-wise quantization, building upon previous work. We evaluate our framework on the image classification task, utilizing the ResNet-18 backbone network and three commonly used datasets with varying image sizes. Rapid-INR reduces memory consumption to only 5% of the original dataset size and achieves a maximum 6\u00d7 speedup over the PyTorch training pipeline, as well as a maximum 1.2\u00d7 speedup over the DALI training pipeline, with only a marginal decrease in accuracy. Importantly, Rapid-INR can be readily applied to other computer vision tasks and backbone networks with reasonable engineering efforts. Our implementation code is publicly available at https://github.com/sharc-lab/Rapid-INR.",
    "authors": [
        {
            "affiliations": [],
            "name": "Hanqiu Chen"
        },
        {
            "affiliations": [],
            "name": "Hang Yang"
        },
        {
            "affiliations": [],
            "name": "Stephen Fitzmeyer"
        },
        {
            "affiliations": [],
            "name": "Cong Hao"
        }
    ],
    "id": "SP:8ef59b85dca0a32d602d4a352237adc3074c49bc",
    "references": [
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "2015.",
            "year": 2015
        },
        {
            "authors": [
                "R. Girshick"
            ],
            "title": "Fast r-cnn",
            "venue": "2015.",
            "year": 2015
        },
        {
            "authors": [
                "X. Sun",
                "D. Yang",
                "X. Li",
                "T. Zhang",
                "Y. Meng",
                "H. Qiu",
                "G. Wang",
                "E. Hovy",
                "J. Li"
            ],
            "title": "Interpreting deep learning models in natural language processing: A review",
            "venue": "2021.",
            "year": 2021
        },
        {
            "authors": [
                "P. Dinesh",
                "V. Sujitha",
                "C. Salma",
                "B. Srijayapriya"
            ],
            "title": "A review on natural language processing: back to basics",
            "venue": "Innovative Data Communication Technologies and Application: Proceedings of ICIDCA 2020, pp. 655\u2013661, 2021.",
            "year": 2020
        },
        {
            "authors": [
                "R.J. Chen",
                "M.Y. Lu",
                "T.Y. Chen",
                "D.F. Williamson",
                "F. Mahmood"
            ],
            "title": "Synthetic data in machine learning for medicine and healthcare",
            "venue": "Nature Biomedical Engineering, vol. 5, no. 6, pp. 493\u2013497, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "S. Grigorescu",
                "B. Trasnea",
                "T. Cocias",
                "G. Macesanu"
            ],
            "title": "A survey of deep learning techniques for autonomous driving",
            "venue": "Journal of Field Robotics, vol. 37, no. 3, pp. 362\u2013386, apr 2020. [Online]. Available: https://doi.org/10.1002%2Frob.21918",
            "year": 2020
        },
        {
            "authors": [
                "L. Liu",
                "Y. Wang",
                "W. Shi"
            ],
            "title": "Understanding time variations of dnn inference in autonomous driving",
            "venue": "2022.",
            "year": 2022
        },
        {
            "authors": [
                "J. Ramirez",
                "J. Gallego-Posada"
            ],
            "title": "L 0 onie: Compressing coins with l 0-constraints",
            "venue": "arXiv preprint arXiv:2207.04144, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Str\u00fcmpler",
                "J. Postels",
                "R. Yang",
                "L.V. Gool",
                "F. Tombari"
            ],
            "title": "Implicit neural representations for image compression",
            "venue": "Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XXVI. Springer, 2022, pp. 74\u201391.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Zhang",
                "T. van Rozendaal",
                "J. Brehmer",
                "M. Nagel",
                "T. Cohen"
            ],
            "title": "Implicit neural video compression",
            "venue": "arXiv preprint arXiv:2112.11312, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "E. Dupont",
                "A. Goli\u0144ski",
                "M. Alizadeh",
                "Y.W. Teh",
                "A. Doucet"
            ],
            "title": "Coin: Compression with implicit neural representations",
            "venue": "2021.",
            "year": 2021
        },
        {
            "authors": [
                "E. Dupont",
                "H. Loya",
                "M. Alizadeh",
                "A. Goli\u0144ski",
                "Y.W. Teh",
                "A. Doucet"
            ],
            "title": "Coin++: Neural compression across modalities",
            "venue": "2022.",
            "year": 2022
        },
        {
            "authors": [
                "B. Mildenhall",
                "P.P. Srinivasan",
                "M. Tancik",
                "J.T. Barron",
                "R. Ramamoorthi",
                "R. Ng"
            ],
            "title": "Nerf: Representing scenes as neural radiance fields for view synthesis",
            "venue": "Communications of the ACM, vol. 65, no. 1, pp. 99\u2013106, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "J.J. Park",
                "P. Florence",
                "J. Straub",
                "R. Newcombe",
                "S. Lovegrove"
            ],
            "title": "Deepsdf: Learning continuous signed distance functions for shape representation",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp. 165\u2013174.",
            "year": 2019
        },
        {
            "authors": [
                "L. Mescheder",
                "M. Oechsle",
                "M. Niemeyer",
                "S. Nowozin",
                "A. Geiger"
            ],
            "title": "Occupancy networks: Learning 3d reconstruction in function space",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp. 4460\u20134470.",
            "year": 2019
        },
        {
            "authors": [
                "E.R. Chan",
                "M. Monteiro",
                "P. Kellnhofer",
                "J. Wu",
                "G. Wetzstein"
            ],
            "title": "pi-gan: Periodic implicit generative adversarial networks for 3d-aware image synthesis",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2021, pp. 5799\u20135809.",
            "year": 2021
        },
        {
            "authors": [
                "V. Sitzmann",
                "J.N.P. Martel",
                "A.W. Bergman",
                "D.B. Lindell",
                "G. Wetzstein"
            ],
            "title": "Implicit neural representations with periodic activation functions",
            "venue": "2020.",
            "year": 2020
        },
        {
            "authors": [
                "M.-E. Nilsback",
                "A. Zisserman"
            ],
            "title": "Automated flower classification over a large number of classes",
            "venue": "2008 Sixth Indian Conference on Computer Vision, Graphics & Image Processing. IEEE, 2008, pp. 722\u2013729.",
            "year": 2008
        },
        {
            "authors": [
                "A. Krizhevsky",
                "G. Hinton"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "venue": "2009.",
            "year": 2009
        }
    ],
    "sections": [
        {
            "text": "I. INTRODUCTION\nIn recent years, Deep Neural Networks (DNNs) have gained significant attention for their effectiveness in various AI tasks, such as computer vision [1], [2], natural language processing [3], [4], healthcare [5], and autonomous driving [6], [7]. The growing demand for on-device machine learning model training is driven by several factors, including the need to maintain data privacy, enable personalized models and lifelong learning, and enhance energy efficiency by reducing reliance on cloud-based data transmission.\nHowever, as neural networks and datasets continue to grow in size and complexity, there are significant challenges in directly offloading the end-to-end training of deep neural networks (DNNs) onto a single device. First, effectively reducing storage requirements for large training datasets while preserving essential information for training the neural networks is difficult. This is crucial to avoid the need for external memory data communication, which can slow down training and introduce latency. Second, developing on-the-fly image\n\u00a7 Equal contribution.\ndecoding techniques is challenging, as it requires real-time and parallel decoding of compressed images during the training process.\nTo address these challenges, researchers are focusing on developing advanced image compression techniques that strike a balance between reducing storage requirements and preserving important training information. Implicit Neural Representation (INR) has emerged as a promising method in computer vision for reducing image size. However, existing studies [8]\u2013[10] are mainly focusing on optimizing INR for high compression rates and minimal quality loss, overlooking its broader applications. Therefore, it is essential to thoroughly explore and leverage the full potential of INR to maximize its benefits.\nMotivated by the considerable overhead associated with external memory access in computer vision tasks, as well as the impressive image compression capabilities of INR, this paper proposes an innovative framework Rapid-INR. The main purpose of this framework is to overcome the data transmission bottleneck by utilizing INR to compress the entire image dataset in multilayer perceptron (MLP) weights and achieve end-to-end training only on GPU. This compression enables on-device storage without the need for external memory access, while also facilitating on-the-fly decoding. The contributions of this paper can be summarized as follows: \u2022 CPU-free training with exceptional speedup. In contrast to\nconventional approaches that rely on powerful host devices (such as CPUs) and multiple data-loader workers to expedite image data pre-processing and transmission, Rapid-INR offers the advantage of offloading the entire training process onto GPU without the need for CPU and external memory accesses. This benefit stems from two key factors. Firstly, the INR format results in smaller image sizes. Secondly, images are stored in INR weight tensors, which are more compatible with GPU CUDA memory as compared to the JPEG format. This approach effectively reduces the data communication overhead and significantly improves training speed. \u2022 High decoding parallelism without specialized hardware. In the backbone training process, on-the-fly decoding of a batch of images from the INR weight format to RGB format is made possible by leveraging pixel-level parallelism, enabling simultaneous processing of individual pixels. By fully utilizing the CUDA cores in the INR decoding stage, we achieve optimal decoding speed without the need for specialized hardware. \u2022 Optimized compression method for efficient storage. To ar X iv :2\n30 6.\n16 69\n9v 2\n[ cs\n.C V\n] 2\n0 A\nug 2\n02 3\nfurther reduce memory consumption in existing INR-based image compression methods [11], [12] while maintaining image quality, we introduce dynamic pruning and layerwise quantization. Dynamic pruning selectively prunes INR weights based on reconstructed image quality, ensuring efficient memory usage. Layer-wise quantization involves quantizing weights to 8 bits in layers with minimal impact on final performance. By leveraging these techniques, we achieve significant memory savings while preserving image quality within acceptable limits. \u2022 Ease of use with high generality. One of the key advantages of Rapid-INR is its seamless integration with mainstream computer vision tasks training pipelines, requiring only minor modifications. Additionally, Rapid-INR is compatible with common data augmentation techniques. Unlike traditional approaches, Rapid-INR decouples the data representation from the spatial resolution. The memory needed to parameterize the signal is no longer dependent on the spatial resolution but rather scales with the complexity of the underlying signal. This enables INR-encoded images to be decoded to arbitrary spatial resolutions, offering greater flexibility and adaptability in downstream tasks. \u2022 Neural architecture search. We conduct a comprehensive study by exploring different numbers of layers and hidden dimensions in the MLP used for INR to investigate the relationship between MLP architecture and reconstructed image quality. Our goal is to optimize the MLP architecture when the size of MLP used for INR is fixed while maintaining high-quality reconstructions. \u2022 Experiment results. Rapid-INR outperforms existing frameworks with significant performance improvements. It achieves a maximum speedup of 6\u00d7 compared to the PyTorch training pipeline with one data-loader worker. Additionally, RapidINR achieves a maximum speedup of 1.2\u00d7 compared to the NVIDIA DALI framework with one CPU thread. Moreover, it maintains a high level of accuracy in image classification tasks, with only about a 2% accuracy drop, while utilizing just about 5% of the original JPEG storage space."
        },
        {
            "heading": "II. BACKGROUND AND MOTIVATION",
            "text": ""
        },
        {
            "heading": "A. JPEG and Implicit Neural Representation",
            "text": "JPEG (Joint Photographic Experts Group) is a popular image compression standard renowned for reducing image file sizes while maintaining visual quality. It achieves compression by employing mathematical transformations such as color space transformation, discrete cosine transform, and quantization. The quantization matrix used during the quantization stage is crucial in determining the size and quality of JPEG images. The choice of quality settings directly impacts the selection of the quantization matrix. Additionally, downsampling is an alternative method for JPEG compression.\nImplicit Neural Representations (INR) present a novel approach to representing a wide range of signals [10], [13]\u2013 [16]. They employ a continuous function that maps the signal\u2019s domain (e.g., pixel coordinates in an image) to its corresponding value (e.g., the R, G, B color of that pixel). The goal of this\nstudy is to compress images that can be expressed using a set of coordinates x \u2208 X and RGB values y \u2208 Y . Each data point consists of a collection of coordinate and RGB value pairs d = {(xi, yi)}ni=1. The objective is to train a neural network f\u03b8 : X \u2192 Y with parameters \u03b8 by minimizing the following loss function:\nL(\u03b8, d) = n\u2211 i=1 ||f\u03b8(xi)\u2212 yi||2 (1)\nINRs typically utilize periodic activation functions that are ideal for representing complex natural signals [17]. These activation functions have continuous derivatives, making them well-suited for capturing continuous signals.\nB. INR Compression Advantages INR offers several advantages over other compression methods: \u2022 Flexibility and generality. INR provides exceptional flex-\nibility, enabling effective modeling of diverse complex functions. It can handle various types of data, including images, 3D shapes, and time series. This versatility makes it suitable for tasks like image synthesis, generative modeling, and data reconstruction. Moreover, INR exhibits strong generalization capabilities, performing well on unseen or out-of-distribution data, as the network learns meaningful representations capturing the underlying structure of the data. \u2022 Storage efficiency. INR compresses data into compact forms by encoding the entire function within network weights. This results in more storage-efficient representations compared to explicit representations that require individual storage for each data point. It facilitates the storage and retrieval of large-scale models and datasets. \u2022 Continuity and Smoothness. INR\u2019s continuity and smoothness properties are desirable for animation, interpolation, and solving partial differential equations in scientific problems.\nC. INR Related Works We highlight two prior works that explore the utilization of INR for image compression: COIN [11] and COIN++ [12]. COIN, which is the pioneering paper in this area, systematically examines the performance of INR in image compression. It achieves a higher image reconstruction quality than JPEG when the compression rate is high by encoding each image using an overfitted MLP that maps pixel locations to RGB values. COIN achieves this without employing entropy coding or learning a weight distribution, and by only quantizing the weights to 16 bits. However, it cannot achieve a better reconstruction quality than JPEG when the compression rate is small.\nCOIN++ builds upon COIN by incorporating meta-learning techniques to expedite INR image encoding. Additionally, it explores various quantization strategies to enhance compression. COIN++ goes further in reducing the size of network weights, resulting in a higher compression rate under the same reconstruction quality. However, it should be noted that the decoding speed of COIN++ is three times slower compared to COIN."
        },
        {
            "heading": "D. Two Common Training Pipelines",
            "text": "Two widely used computer vision neural network training pipelines are the PyTorch pipeline [18] and the recently introduced new training pipeline using Data Loading Library (DALI) [19] developed by NVIDIA, as shown in Fig. 1.\nThe PyTorch pipeline involves prefetching image batches from the disk to the CPU during training. The image decoding, resizing, and augmentation are performed on the CPU. While PyTorch\u2019s data loader can largely increase the speed of image preprocessing before sending the images to the GPU, it often requires multiple CPU workers to run concurrently. The overall performance of this pipeline is highly dependent on the CPU\u2019s capabilities. The presence of a weak CPU can prevent full utilization of the GPU\u2019s CUDA cores.\nIn contrast, DALI pipeline accelerates the JPEG decoding by leveraging both the CPU and the GPU. The resizing and data augmentation tasks are shifted to the GPU for maximizing its computational capabilities. DALI exhibits promising speedup compared to the PyTorch pipeline. However, it still heavily depends on the presence of powerful CPUs to ensure efficient data processing."
        },
        {
            "heading": "E. Key Insights",
            "text": "To the best of our knowledge, there are no existing works that have combined INR image compression and training acceleration. By leveraging the high image compression rate of INR, it becomes possible to compress the entire dataset into the memory of a single device. This enables offloading the whole training process to GPU, achieving on-the-fly decoding of images on the GPU during training, eliminating the need for repeated external memory access, data transmission, and powerful CPUs for data loading and preprocessing.\nFor instance, the training set of ImageNet [?] occupies approximately 138GB, which exceeds the on-chip memory capacity of most training devices. However, by encoding the dataset in the INR format, it only requires around 14GB, which can be directly stored in the CUDA memory. Furthermore, storing the MLP weights for INR in tensor format ensures compatibility with a wide range of GPUs and offers convenience during usage. The differences between our Rapid-INR pipeline and two previous pipelines are shown in Fig. 1."
        },
        {
            "heading": "III. ENCODER-DECODER ARCHITECTURE",
            "text": "Rapid-INR utilizes an encoder-decoder architecture for image compression, as shown in Fig. 2. The encoder takes images in the JPEG format and encodes them into the INR weights format, saving the encoded dataset on disk. On the other hand, the decoder decodes the images from INR weights format back to RGB format. These decoded RGB images are then used for backbone training."
        },
        {
            "heading": "A. Encoder Architecture",
            "text": "Rapid-INR utilizes a simple multilayer perceptron (MLP) as the INR encoder. The MLP takes a two-dimensional input representing the spatial location (x, y) of each pixel in an image. The output dimension of the MLP is set to 3, representing the RGB value of a pixel. The choice of the number of layers N and hidden dimensions H is determined based on the image size. Larger images require more layers and hidden dimensions for optimal performance.\nTo determine the optimal architecture, we incorporate neural architecture search into our methodology, which will be explained in detail in Section V. During training, each image in the dataset is paired with a dedicated MLP. The number of\ncopies of INR weights matches the total number of images in the dataset.\nDuring the training process, the MLP takes the normalized spatial location of each pixel within an image, ranging from 0 to 1, as the input. The RGB values associated with each pixel serve as the training labels. The MLP effectively learns the spatial color information, which is implicitly encoded in the MLP weights. After the offline training phase, the learned weights are transferred to disk for subsequent backbone training."
        },
        {
            "heading": "B. Decoder Architecture",
            "text": "The INR decoder is positioned before the backbone network to transform images from the INR weights format to the RGB format. The decoder utilizes the same MLP architecture as the encoder. Before commencing the backbone training, the entire dataset in the INR weights format is transferred from disk to the GPU\u2019s CUDA memory at one time. The compression of images into the INR format enables the accommodation of the entire dataset within the CUDA memory, avoiding external data access during training.\nDuring backbone network training, the decoder selects a batch of images in the INR weights format from the CUDA memory and then mapped to the MLP for decoding. Thanks to the continuous nature of the INR\u2019s representation function, which can support an arbitrary image resolution, there is no need for image resizing during the decoding process. This implies that the images can be decoded to any desired size without loss of information or quality.\nTo fully harness the computational power of the CUDA cores, the decoding process is optimized to leverage pixel-level parallelism. This enables simultaneous decoding of each image within the batch, as well as parallel decoding of individual pixels within each image. GPU acceleration is also utilized for data augmentation, where the augmented images are then forwarded to the backbone for training. Minimizing external memory access is prioritized, ensuring that communication\noverhead is minimized. As a result, a significant portion of the computation takes place on the GPU, reducing the need for frequent data communication and optimizing overall performance.\nIV. INR COMPRESSION TECHNIQUES\nAlthough INR can achieve similar image reconstruction quality to JPEG, it is important to consider that without employing any network compression techniques on the MLP weights for INR may result in larger file sizes compared to JPEG. To address this concern, Rapid-INR integrates two network compression methods: iterative and dynamic pruning, along with layer-wise quantization. These techniques are specifically designed to reduce the size of MLP weights for INR while minimizing the potential impact on reconstructed image quality. The goal is to strike a balance between achieving compact representations and maintaining satisfactory image reconstruction quality. We use three image datasets: MiniImageNet [20], 102flowers [21] and CIFAR-10 [22] with varied image sizes to evaluate our proposed techniques. We use peak signal-to-noise ratio (PSNR) as the image reconstruction quality evaluation metric. Our analysis results are shown in Fig. 3.\nA. Iterative and Dynamic Pruning\nInsights. Our preliminary study reveals a linear relationship between the size of the INR and the reconstructed image quality (PSNR). Additionally, different images exhibit varying reconstructed quality using the same sized INR due to color richness and variety. Moreover, our investigation indicates that low-quality reconstructed images have a larger impact on the final backbone training accuracy. Based on these findings, we propose iterative and dynamic pruning, where the pruning ratio is dynamically selected based on the PSNR of the reconstructured images.\nTechnical details. Our approach for Mini-ImageNet and 102flowers datasets involves two rounds of iterative pruning to\nachieve an optimal balance between reconstructed image quality and INR size. In the first round, we employ L1-unstructured pruning to set 20% of the weights to 0. Subsequently, we apply the pruning mask to the weights and retrain the MLP with a smaller learning rate to help network to recover. This pruning of redundant weights facilitates network convergence, and in some cases, we even observed a slight improvement in PSNR. For the CIFAR-10 dataset with smaller image sizes, we omit the first round of pruning and directly proceed to the second round.\nThe second round of pruning is referred to as dynamic pruning. The pruning ratios Pr for the CIFAR-10, 102flowers, and Mini-ImageNet datasets are defined by Eq. 2, 3 and 4 respectively:\nPr =  0 PSNR < 30\n0.05 \u2217 PSNR\u2212 1.5 30 \u2264 PSNR \u2264 35 0.25 PSNR > 35 (2)\nPr =  0.2 PSNR < 35\n0.04 \u2217 PSNR\u2212 1.2 35 \u2264 PSNR \u2264 40 0.4 PSNR > 40 (3)\nPr =  0.2 PSNR < 35\n0.04 \u2217 PSNR\u2212 1.2 35 \u2264 PSNR \u2264 40 0.4 PSNR > 40 (4)\nAfter the dynamic pruning, the average pruning ratios achieved on the CIFAR-10, 102flowers and Mini-ImageNet datasets are 15.6%, 24.2% and 25.5% respectively."
        },
        {
            "heading": "B. Layer-wise Quantization",
            "text": "Insights. We conduct a weight distribution analysis on different layers, and the results for the Mini-ImageNet dataset are depicted in Figure 4. The MLP layers exhibit distinct weight distributions after completing INR encoding training. The first layer of the MLP focuses on extracting initial features and rescaling input values. Consequently, the weights in this layer are distributed across a wide range, resembling a uniform distribution. The last layer of the MLP is responsible for regression to output normalized RGB values. Hence, the weight distribution in this layer is characterized by weights that are not concentrated around zero. The hidden layers of the MLP are involved in feature extraction and learning. The weight distribution in these layers follows a Gaussian distribution, with the majority of weights centered around zero.\nThe varied distribution patterns of different types of weights are not suitable for the same quantization strategy, and also\nweights with a Gaussian distribution within a small scale are more suitable for quantization. To reduce the performance degradation caused by quantization, we adopt a layer-wise quantization approach, quantizing only the weights of the hidden layers. This preserves the first and last layer weights in full precision, mitigating the performance degradation effectively.\nTechnical details. In our layer-wise quantization, only the weights of the hidden layers are quantized to 8 bits while retaining the weights in the first and last layers in their full precision of 32 bits. By implementing layer-wise quantization, we can effectively prevent a substantial drop in the PNSR and maintain storage efficiency."
        },
        {
            "heading": "V. NEURAL ARCHITECTURE SEARCH AND HYPERPARAMETER TUNING",
            "text": "To optimize the reconstruction quality of INR-compressed images, the selection of the MLP architecture and hyperparameters plays a crucial role. In order to find the optimal MLP architecture within the given storage constraints and determine the best hyperparameters such as learning rate and activation function frequency, we conduct a neural architecture search and hyperparameter tuning. These processes involve systematically exploring different architectural configurations and hyperparameter settings to identify the combination that yields the highest performance. By employing these techniques, we were able to improve the encoding capabilities of INR."
        },
        {
            "heading": "A. Neural Architecture Search",
            "text": "We conduct experiments with two different sizes of INR: 13.5KB and 29KB. To explore the effects of different MLP architectures, we vary the number of hidden layers and hidden dimensions in tandem, while maintaining a fixed size constraint. We further consider two types of architectures: uniform and tapered. The uniform architecture has the same dimension\nfor all hidden layers, while the tapered architecture gradually increases and then decreases the dimensions of consecutive hidden layers.\nBased on our findings, as shown in Fig. 5 (a), the uniform architecture outperforms the tapered architecture in terms of PSNR. We also use different training iterations for these two MLP architectures. Increasing the training iterations improves the PSNR for the 13.5KB INR variant. We also discover that excessively deep MLP architectures (with more than 10 layers) result in decreased PSNR. Therefore, it is recommended to limit the number of layers to less than 10 for optimal performance."
        },
        {
            "heading": "B. Hyperparameter Tuning",
            "text": "We also investigate the impact of two key parameters: the activation function frequency and the learning rate used during INR encoding training. Fig. 5 (b) presents the PSNR results obtained by varying the activation function frequency and hidden layer dimensions. Our findings indicate that an activation function frequency of 30 yields the best performance in terms of PSNR.\nFurthermore, we examine the influence of different learning rates in conjunction with varying numbers of training iterations. The results are depicted in Fig. 5 (c). Starting at a learning rate of 1e-3 and above, image compression fails for a subset of the images. From these observations, it is suggested to select a learning rate within the range of 2e-4 to 5e-4 to achieve optimal performance."
        },
        {
            "heading": "VI. EXPERIMENT RESULTS",
            "text": ""
        },
        {
            "heading": "A. Experiment Setup",
            "text": "Our experiment is conducted using PyTorch with an NVIDIA A6000 GPU and an Intel 6226R CPU. The architecture of the INR MLP varys for each dataset: 3 layers with 15 hidden dimensions for CIFAR-10, 10 layers with 32 hidden dimensions for 102flowers, and 10 layers with 40 hidden dimensions for\nMini-ImageNet. The MLP employs the sine function as the activation function with a frequency value of 30. The training of the INR MLP consists of three rounds. In the first round, we train the MLP to overfit using full precision. The learning rate for this stage was 5e-4, and we train for 5,000 epochs. In the second and third rounds, we apply iterative and dynamic pruning techniques. The learning rate for these rounds is set to 2e-4, and we train for 10,000 epochs. To optimize gradient descent, we utilize the Adam optimizer and incorporated cosine learning rate decay. This technique progressively decreases the learning rate as the gradient diminished during the training process.\nFor the image classification task, we select ResNet-18 as the backbone network. It is important to note that this choice is specific to our case study, as Rapid-INR can be easily extended to other vision tasks and backbone architectures. During the backbone training, we employ a batch size of 128 for CIFAR10 and 64 for both 102flowers and Mini-ImageNet datasets. The learning rate used for backbone training is set to 1e-2, with a moment decay of 5e-4. We utilize the SGD optimizer for gradient descent and train for 200 epochs."
        },
        {
            "heading": "B. Reconstruction Quality",
            "text": "In our experiment, we compare three different image compression techniques: INR, JPEG with different qualities, and JPEG downsampling. Fig. 6 showcases the results of reconstructing an image from the 102flowers dataset using these methods. The images presented include the original image, the reconstructed image using each technique, and the corresponding residual image.\nWhen considering the same memory space, the INRreconstructed image exhibits the highest quality among the three techniques. However, all of them share a common issue: the sharp boundaries in the images have been blurred. This blurring effect is more evident in the residual image, where the boundary of the flower appears clearer compared to other regions.\nThe blurring effect in the INR-reconstructed image can be attributed to the nature of the learned function in INR, which is\ncontinuous. Although INR can capture some useful information about the boundary, it struggles to accurately model the abrupt color changes that occur at boundaries. On the other hand, JPEG compression involves approximation techniques, and since the boundary region occupies only a small portion of the image, the color information in this region is often blurred due to the influence of surrounding regions during compression."
        },
        {
            "heading": "C. Backbone Training Accuracy",
            "text": "We further analyze the accuracy of training the backbone network by utilizing images decoded from various compression techniques. The experimental results are presented in Fig. 7. We compare the accuracy of training the backbone using images decoded from different types of INR-encoded images, as well as raw JPEG images. Although there is only a slight drop in accuracy, when combining dynamic pruning and 8-bit layerwise quantization, a significant amount of storage space can\nbe saved. Moreover, we ensure a fair comparison by fixing the size of the compressed INR and JPEG images. Notably, INR with dynamic pruning and 8-bit layer-wise quantization outperforms both JPEG compression techniques in terms of backbone training accuracy."
        },
        {
            "heading": "D. Hardware Speedup",
            "text": "We also conduct an end-to-end training time profiling to demonstrate the accelerated performance of the INR training pipeline. It compares INR with two robust baselines (Fig. 1) across three datasets. The measured training time includes data transfer costs, decoding, augmentation, and neural network training.\nThe PyTorch and DALI pipelines share the same training flow and configuration as the INR pipeline, but differ significantly in the data preparation phase. Firstly, the baselines require frequent data transfers between CPU and GPU memory, while INR only requires a single transmission due to its superior compression rate. Secondly, the baselines heavily rely on CPU power for training since GPU memory cannot store JPEG format data. This means the CPU must decode small JPEG images into large tensor format, increasing the CPU\u2019s transmission workload. Furthermore, the decoding algorithm itself can become a bottleneck due to limited parallelization. Thirdly, the baselines struggle to fully utilize GPU resources, including GPU memory and parallel computation, unlike the INR pipeline. INR achieves higher GPU memory utilization by accommodating all training data within the GPU and keeping it fixed. Additionally, INR\u2019s decoding process leverages onthe-fly GPU parallel computation instead of sequential CPU processing.\nFig. 8 presents profiling results from three datasets, showcasing the scalability of INR across different scenarios. The datasets have varying characteristics: CIFAR-10 has small-sized images but a large dataset size, 102flowers has large-sized images but a small dataset size, and Mini-ImageNet contains large-sized images and a substantial dataset size. (Note that we use Mini-ImageNet for profiling convenience, but the original\nImageNet dataset can also fit into the NVIDIA A6000 card.) In the experiment, we increase the number of threads to accelerate the pipeline until it reaches the hardware limits. On 102flowers and Mini-ImageNet datasets, INR outperforms the PyTorch pipeline with eight threads and four threads, respectively, and both outperform DALI with one thread. However, on CIFAR10, INR only surpasses the PyTorch pipeline when using one thread. The key distinction between the 102flowers/MiniImageNet datasets and CIFAR-10 is the image size. Larger image sizes exploit parallelism more effectively, which explains the decreasing INR training time with increasing CIFAR-10 batch sizes. However, using large batch sizes may compromise training accuracy despite achieving speedup."
        },
        {
            "heading": "VII. CONCLUSIONS",
            "text": "In this paper, we propose Rapid-INR, an INR-based image compression technique that effectively reduces image size while preserving acceptable quality. Rapid-INR enables accelerated training in computer vision tasks by compressing the training dataset, storing it on GPU, and performing on-the-fly decoding. We also introduce dynamic pruning and layer-wise quantization techniques to achieve further compression with minimal quality loss, building upon previous research.\nWhile Rapid-INR demonstrates improved storage efficiency compared to the JPEG image format and achieves speedup in image classification training compared to PyTorch and DALI pipelines, it still faces several challenges, like the lack of interpretability, computational complexity, memory efficiency in INR encoding, and sensitivity to hyperparameters. These factors need to be carefully evaluated to determine the suitability of INR for specific tasks or applications."
        },
        {
            "heading": "VIII. ACKNOWLEDGEMENTS",
            "text": "This work and its authors are partially supported by the National Science Foundation under Grant No.2202329, Cisco, and Samsung. The authors would also like to thank Xuebin Yao and Pradeep Subedi from Samsung for their insightful discussions."
        }
    ],
    "title": "Rapid-INR: Storage Efficient CPU-free DNN Training Using Implicit Neural Representation",
    "year": 2023
}