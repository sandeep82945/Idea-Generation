{
    "abstractText": "In recent years, the collection and sharing of resting-state functional magnetic resonance imaging (fMRI) datasets across multiple centers have enabled studying psychiatric disorders at scale, and prompted the application of statistically powerful tools such as deep neural networks. Yet, multi-center datasets introduce non-biological heterogeneity that can confound the biological signal of interest and produce erroneous findings. To mitigate this problem, the neuroimaging community has adopted harmonization techniques previously proposed in other domains to remove site-effects from fMRI data. The reported success of these approaches in improving the generalization of the models have varied significantly. It remains unclear whether harmonization techniques could boost the final outcome of multi-site fMRI studies, to what extent, and which approaches are best suited for this task. In an attempt to objectively answer these questions, we conduct a standardized rigorous evaluation of seven different harmonization techniques from the neuroimaging and computer vision literature on two large-scale multi-site datasets (N = 2169 and N = 2366) to diagnose autism spectrum disorder and major depression disorder from static and dynamic representations of fMRI data. Interestingly, while all harmonization techniques removed site-effects from the data, they had little influence on disorder classification performance in standard k-fold and leave-one-site-out validation settings over a well-tuned baseline. Further investigation shows that the baseline model implicitly learns site-invariant features which could well explain its competitiveness with explicit harmonization techniques and suggest orthogoPreprint submitted to Elsevier June 14, 2023 (which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. The copyright holder for this preprint this version posted June 14, 2023. ; https://doi.org/10.1101/2023.06.14.544758 doi: bioRxiv preprint",
    "authors": [
        {
            "affiliations": [],
            "name": "Ahmed El-Gazzar"
        },
        {
            "affiliations": [],
            "name": "Rajat Mani Thomas"
        },
        {
            "affiliations": [],
            "name": "Guido van Wingen"
        }
    ],
    "id": "SP:2b787c5a0e58dcd3e78f331ba555ab2392fb728f",
    "references": [
        {
            "authors": [
                "A. Di Martino",
                "C.-G. Yan",
                "Q. Li",
                "E. Denio",
                "F.X. Castellanos",
                "K. Alaerts",
                "J.S. Anderson",
                "M. Assaf",
                "S.Y. Bookheimer"
            ],
            "title": "M",
            "venue": "Dapretto, et al., The autism brain imaging data exchange: towards a large-scale evaluation of the intrinsic brain architecture in autism, Molecular psychiatry 19 ",
            "year": 2014
        },
        {
            "authors": [
                "C.R. Jack Jr",
                "M.A. Bernstein",
                "N.C. Fox",
                "P. Thompson",
                "G. Alexander",
                "D. Harvey",
                "B. Borowski",
                "P.J. Britson",
                "J.L. Whitwell"
            ],
            "title": "C",
            "venue": "Ward, et al., The alzheimer\u2019s disease neuroimaging initiative (adni): Mri methods, Journal of Magnetic Resonance Imaging: An Official Journal of the International Society for Magnetic Resonance in Medicine 27 ",
            "year": 2008
        },
        {
            "authors": [
                "S. Smith",
                "C. Beckmann",
                "J. Andersson",
                "E. Auerbach",
                "J. Bijsterbosch",
                "G. Douaud",
                "E. Duff",
                "D. Feinberg",
                "L. Griffanti"
            ],
            "title": "M",
            "venue": "Harms, others",
            "year": 2013
        },
        {
            "authors": [
                "G.G. Brown",
                "D.H. Mathalon",
                "H. Stern",
                "J. Ford",
                "B. Mueller",
                "D.N. Greve"
            ],
            "title": "26 (which was not certified by peer review) is the author/funder",
            "venue": "All rights reserved. No reuse allowed without permission. The copyright holder for this preprint this version posted June 14, 2023. ; https://doi.org/10.1101/2023.06.14.544758 doi: bioRxiv preprint G. McCarthy, J. Voyvodic, G. Glover, M. Diaz, et al., Multisite reliability of cognitive bold data, Neuroimage 54 ",
            "year": 2011
        },
        {
            "authors": [
                "D.N. Greve",
                "G.G. Brown",
                "B.A. Mueller",
                "G. Glover"
            ],
            "title": "T",
            "venue": "T. Liu, et al., A survey of the sources of noise in fmri, Psychometrika 78 ",
            "year": 2013
        },
        {
            "authors": [
                "M.R. Arbabshirani",
                "S. Plis",
                "J. Sui",
                "V.D. Calhoun"
            ],
            "title": "Single subject prediction of brain disorders in neuroimaging: Promises and pitfalls",
            "venue": "Neuroimage 145 ",
            "year": 2017
        },
        {
            "authors": [
                "E. Olivetti",
                "S. Greiner",
                "P. Avesani"
            ],
            "title": "Adhd diagnosis from multiple data sources with batch effects",
            "venue": "Frontiers in systems neuroscience 6 ",
            "year": 2012
        },
        {
            "authors": [
                "M. Yu",
                "K.A. Linn",
                "P.A. Cook",
                "M.L. Phillips",
                "M. McInnis",
                "M. Fava",
                "M.H. Trivedi",
                "M.M. Weissman",
                "R.T. Shinohara",
                "Y.I. Sheline"
            ],
            "title": "Statistical harmonization corrects site effects in functional connectivity measurements from multi-site fmri data",
            "venue": "Human brain mapping 39 ",
            "year": 2018
        },
        {
            "authors": [
                "W.E. Johnson",
                "C. Li",
                "A. Rabinovic"
            ],
            "title": "Adjusting batch effects in microarray expression data using empirical bayes methods",
            "venue": "Biostatistics 8 ",
            "year": 2007
        },
        {
            "authors": [
                "R. Vega",
                "R. Greiner"
            ],
            "title": "Finding effective ways to (machine) learn fmri-based classifiers from multi-site data",
            "venue": "in: Understanding and Interpreting Machine Learning in Medical Image Computing Applications, Springer",
            "year": 2018
        },
        {
            "authors": [
                "A. Yamashita",
                "N. Yahata",
                "T. Itahashi",
                "G. Lisi",
                "T. Yamada",
                "N. Ichikawa",
                "M. Takamura",
                "Y. Yoshihara",
                "A. Kunimatsu"
            ],
            "title": "N",
            "venue": "Okada, et al., Harmonization of resting-state functional mri data across multiple imaging sites via the separation of site differences into sampling bias and measurement bias, PLoS biology 17 ",
            "year": 2019
        },
        {
            "authors": [
                "P. Faure",
                "H. Korn"
            ],
            "title": "Is there chaos in the brain? i",
            "venue": "concepts of nonlinear dynamics and methods of investigation, Comptes Rendus de l\u2019Acad\u00e9mie des Sciences-Series III-Sciences de la Vie 324 ",
            "year": 2001
        },
        {
            "authors": [
                "K.J. Friston"
            ],
            "title": "The labile brain",
            "venue": "i. neuronal transients and nonlinear coupling, Philosophical Transactions of the Royal Society of London. Series B: Biological Sciences 355 ",
            "year": 2000
        },
        {
            "authors": [
                "H. Li",
                "S.J. Pan",
                "S. Wang",
                "A.C. Kot"
            ],
            "title": "Domain generalization with adversarial feature learning",
            "venue": "in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "year": 2018
        },
        {
            "authors": [
                "Y. Ganin",
                "E. Ustinova",
                "H. Ajakan",
                "P. Germain",
                "H. Larochelle",
                "F. Laviolette",
                "M. Marchand",
                "V. Lempitsky"
            ],
            "title": "Domain-adversarial training of neural networks",
            "venue": "The journal of machine learning research 17 ",
            "year": 2016
        },
        {
            "authors": [
                "M. Arjovsky",
                "L. Bottou",
                "I. Gulrajani",
                "D. Lopez-Paz"
            ],
            "title": "Invariant risk minimization",
            "venue": "arXiv preprint arXiv:1907.02893 ",
            "year": 2019
        },
        {
            "authors": [
                "B.E. Dewey",
                "C. Zhao",
                "J.C. Reinhold",
                "A. Carass",
                "K.C. Fitzgerald",
                "E.S. Sotirchos",
                "S. Saidha",
                "J. Oh",
                "D.L. Pham"
            ],
            "title": "P",
            "venue": "A. Calabresi, et al., Deepharmony: A deep learning approach to contrast harmonization across scanner changes, Magnetic resonance imaging 64 ",
            "year": 2019
        },
        {
            "authors": [
                "R. Garcia-Dias",
                "C. Scarpazza",
                "L. Baecker",
                "S. Vieira",
                "W.H. Pinaya",
                "A. Corvin",
                "A. Redolfi",
                "B. Nelson",
                "B. Crespo-Facorro"
            ],
            "title": "C",
            "venue": "McDonald, et al., Neuroharmony: A new tool for harmonizing volumetric mri data from unseen scanners, Neuroimage 220 ",
            "year": 2020
        },
        {
            "authors": [
                "R. Pomponio",
                "G. Erus",
                "M. Habes",
                "J. Doshi",
                "D. Srinivasan",
                "E. Mamourian",
                "V. Bashyam",
                "I.M. Nasrallah",
                "T.D. Satterthwaite"
            ],
            "title": "Y",
            "venue": "Fan, et al., Harmonization of large mri datasets for the analysis of brain imaging patterns throughout the lifespan, NeuroImage 208 ",
            "year": 2020
        },
        {
            "authors": [
                "M.E. Torbati",
                "D.S. Minhas",
                "G. Ahmad"
            ],
            "title": "E",
            "venue": "E. O\u2019Connor, J. Muschelli, C. M. Laymon, Z. Yang, A. D. Cohen, H. J. Aizenstein, W. E. Klunk, et al., A multi-scanner neuroimaging data harmonization using ravel and combat, Neuroimage 245 ",
            "year": 1870
        },
        {
            "authors": [
                "M. Quaak"
            ],
            "title": "L",
            "venue": "van de Mortel, R. M. Thomas, G. van Wingen, Deep learning applications for the classification of psychiatric disorders using neuroimaging data: systematic review and meta-analysis, NeuroImage: Clinical 30 ",
            "year": 2021
        },
        {
            "authors": [
                "I. Gulrajani",
                "D. Lopez-Paz"
            ],
            "title": "In search of lost domain generalization",
            "venue": "arXiv preprint arXiv:2007.01434 ",
            "year": 2020
        },
        {
            "authors": [
                "C. Craddock",
                "S. Sharad",
                "C. Brian",
                "K. Ranjeet",
                "G. Satrajit",
                "Y. Chaogan"
            ],
            "title": "l",
            "venue": "Qingyang, L. Daniel, J. Vogelstein, R. Burns, C. Stanley, M. Mennes, K. Clare, D. Adriana, F. Castellanos, M. Michael, Towards automated analysis of connectomes: The configurable pipeline for the analysis of connectomes (c-pac), Frontiers in Neuroinformatics 7 ",
            "year": 2013
        },
        {
            "authors": [
                "R.C. Craddock",
                "G.A. James",
                "P.E. Holtzheimer III",
                "X.P. Hu",
                "H.S. Mayberg"
            ],
            "title": "A whole brain fmri atlas generated via spatially constrained spectral clustering",
            "venue": "Human brain mapping 33 ",
            "year": 2012
        },
        {
            "authors": [
                "C.-G. Yan",
                "X. Chen",
                "L. Li",
                "F.X. Castellanos",
                "T.-J. Bai",
                "Q.-J. Bo",
                "J. Cao",
                "G.-M. Chen",
                "N.-X. Chen"
            ],
            "title": "W",
            "venue": "Chen, et al., Reduced default mode network functional connectivity in patients with recurrent major depressive disorder, Proceedings of the National Academy of Sciences 116 ",
            "year": 2019
        },
        {
            "authors": [
                "C. Yan",
                "Y. Zang"
            ],
            "title": "Dparsf: a matlab toolbox for\u201d pipeline\u201d data analysis of resting-state fmri",
            "venue": "Frontiers in systems neuroscience 4 ",
            "year": 2010
        },
        {
            "authors": [
                "R.S. Desikan",
                "F. S\u00e9gonne",
                "B. Fischl",
                "B.T. Quinn",
                "B.C. Dickerson",
                "D. Blacker",
                "R.L. Buckner",
                "A.M. Dale",
                "R.P. Maguire"
            ],
            "title": "B",
            "venue": "T. Hyman, et al., An automated labeling system for subdividing the human cerebral cortex on mri scans into gyral based regions of interest, Neuroimage 31 ",
            "year": 2006
        },
        {
            "authors": [
                "T. Menara",
                "G. Lisi",
                "F. Pasqualetti",
                "A. Cortese"
            ],
            "title": "Brain network dynamics fin- 29 (which was not certified by peer review) is the author/funder",
            "venue": "All rights reserved. No reuse allowed without permission. The copyright holder for this preprint this version posted June 14, 2023. ; https://doi.org/10.1101/2023.06.14.544758 doi: bioRxiv preprint gerprints are resilient to data heterogeneity, Journal of Neural Engineering 18 ",
            "year": 2021
        },
        {
            "authors": [
                "A.S. Heinsfeld",
                "A.R. Franco",
                "R.C. Craddock",
                "A. Buchweitz",
                "F. Meneguzzi"
            ],
            "title": "Identification of autism spectrum disorder using deep learning and the abide dataset",
            "venue": "NeuroImage: Clinical 17 ",
            "year": 2018
        },
        {
            "authors": [
                "S. Bai",
                "J.Z. Kolter",
                "V. Koltun"
            ],
            "title": "An empirical evaluation of generic convolutional and recurrent networks for sequence modeling",
            "venue": "arXiv preprint arXiv:1803.01271 ",
            "year": 2018
        },
        {
            "authors": [
                "A. El Gazzar",
                "L. Cerliani"
            ],
            "title": "G",
            "venue": "van Wingen, R. M. Thomas, Simple 1-d convolutional networks for resting-state fmri based classification in autism, in: 2019 International Joint Conference on Neural Networks (IJCNN), IEEE",
            "year": 2019
        },
        {
            "authors": [
                "A. v. d. Oord",
                "S. Dieleman",
                "H. Zen",
                "K. Simonyan",
                "O. Vinyals",
                "A. Graves",
                "N. Kalchbrenner",
                "A. Senior",
                "K. Kavukcuoglu"
            ],
            "title": "Wavenet: A generative model for raw audio",
            "year": 2016
        },
        {
            "authors": [
                "S. Ioffe",
                "C. Szegedy"
            ],
            "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
            "venue": "in: International conference on machine learning, PMLR",
            "year": 2015
        },
        {
            "authors": [
                "A.A. Chen",
                "J.C. Beer",
                "N.J. Tustison",
                "P.A. Cook",
                "R.T. Shinohara",
                "H. Shou"
            ],
            "title": "A",
            "venue": "D. N. Initiative, et al., Removal of scanner effects in covariance improves multivariate pattern analysis in neuroimaging data, bioRxiv ",
            "year": 2020
        },
        {
            "authors": [
                "M. Xu",
                "J. Zhang",
                "B. Ni",
                "T. Li",
                "C. Wang",
                "Q. Tian",
                "W. Zhang"
            ],
            "title": "Adversarial domain adaptation with domain mixup",
            "venue": "in: Proceedings of the AAAI Conference on Artificial Intelligence, volume 34",
            "year": 2020
        },
        {
            "authors": [
                "B. Sun",
                "K. Saenko"
            ],
            "title": "Deep coral: Correlation alignment for deep domain adaptation",
            "venue": "in: European conference on computer vision, Springer, 2016, pp. 443\u2013450. 30 (which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. The copyright holder for this preprint this version posted June 14",
            "year": 2023
        },
        {
            "authors": [
                "N.R. Winter",
                "J. Blanke",
                "R. Leenings",
                "J. Ernsting",
                "L. Fisch",
                "K. Sarink",
                "C. Barkhau",
                "K. Thiel",
                "K. Flinkenfl\u00fcgel"
            ],
            "title": "A",
            "venue": "Winter, et al., A systematic evaluation of machine learning-based biomarkers for major depressive disorder across modalities, medRxiv ",
            "year": 2023
        },
        {
            "authors": [
                "Z. Sherkatghanad",
                "M. Akhondzadeh",
                "S. Salari",
                "M. Zomorodi-Moghadam",
                "M. Abdar",
                "U.R. Acharya",
                "R. Khosrowabadi",
                "V. Salari"
            ],
            "title": "Automated detection of autism spectrum disorder using a convolutional neural network",
            "venue": "Frontiers in neuroscience 13 ",
            "year": 2020
        },
        {
            "authors": [
                "M. Khosla",
                "K. Jamison",
                "A. Kuceyeski",
                "M.R. Sabuncu"
            ],
            "title": "3d convolutional neural networks for classification of functional connectomes",
            "venue": "in: Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support, Springer",
            "year": 2018
        },
        {
            "authors": [
                "S. Gallo",
                "A. El-Gazzar",
                "P. Zhutovsky",
                "R.M. Thomas",
                "N. Javaheripour",
                "M. Li",
                "L. Bartova",
                "D. Bathula",
                "U. Dannlowski"
            ],
            "title": "C",
            "venue": "Davey, et al., Functional connectivity signatures of major depressive disorder: machine learning analysis of two multicenter neuroimaging studies, Molecular Psychiatry ",
            "year": 2023
        },
        {
            "authors": [
                "R.M. Thomas",
                "S. Gallo",
                "L. Cerliani",
                "P. Zhutovsky",
                "A. El-Gazzar",
                "G. Van Wingen"
            ],
            "title": "Classifying autism spectrum disorder using the temporal statistics of resting-state functional mri data with 3d convolutional neural networks",
            "venue": "Frontiers in psychiatry 11 ",
            "year": 2020
        },
        {
            "authors": [
                "J. Pearl"
            ],
            "title": "Causality",
            "venue": "Cambridge university press",
            "year": 2009
        },
        {
            "authors": [
                "J. Peters",
                "P. B\u00fchlmann",
                "N. Meinshausen"
            ],
            "title": "Causal inference by using invariant prediction: identification and confidence intervals",
            "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology) 78 ",
            "year": 2016
        },
        {
            "authors": [
                "S. Beery",
                "G. Van Horn",
                "P. Perona"
            ],
            "title": "Recognition in terra incognita",
            "venue": "in: Proceedings of the European conference on computer vision (ECCV)",
            "year": 2018
        },
        {
            "authors": [
                "L. Zhang",
                "X. Wang",
                "D. Yang",
                "T. Sanford",
                "S. Harmon",
                "B. Turkbey",
                "H. Roth",
                "A. Myronenko",
                "D. Xu",
                "Z. Xu"
            ],
            "title": "When unseen domain generalization is unnecessary? rethinking data augmentation",
            "venue": "arXiv preprint arXiv:1906.03347 ",
            "year": 2019
        },
        {
            "authors": [
                "H. Zhao",
                "R.T. Des Combes",
                "K. Zhang",
                "G. Gordon"
            ],
            "title": "On learning invariant representations for domain adaptation",
            "venue": "in: International Conference on Machine Learning, PMLR",
            "year": 2019
        },
        {
            "authors": [
                "F.D. Johansson",
                "D. Sontag",
                "R. Ranganath"
            ],
            "title": "Support and invertibility in domain-invariant representations",
            "venue": "in: The 22nd International Conference on Artificial Intelligence and Statistics, PMLR",
            "year": 2019
        },
        {
            "authors": [
                "M.E. Raichle",
                "A.M. MacLeod",
                "A.Z. Snyder",
                "W.J. Powers",
                "D.A. Gusnard",
                "G.L. Shulman"
            ],
            "title": "A default mode of brain function",
            "venue": "Proceedings of the National Academy of Sciences 98 ",
            "year": 2001
        },
        {
            "authors": [
                "A. Abraham",
                "M.P. Milham",
                "A. Di Martino",
                "R.C. Craddock",
                "D. Samaras",
                "B. Thirion",
                "G. Varoquaux"
            ],
            "title": "Deriving reproducible biomarkers from multisite resting-state data: An autism-based example",
            "venue": "NeuroImage 147 ",
            "year": 2017
        },
        {
            "authors": [
                "A. El-Gazzar",
                "R.M. Thomas",
                "G. Van Wingen"
            ],
            "title": "fmri-s4: learning shortand long-range dynamic fmri dependencies using 1d convolutions and state 32 (which was not certified by peer review) is the author/funder",
            "venue": "All rights reserved. No reuse allowed without permission. The copyright holder for this preprint this version posted June 14, 2023. ; https://doi.org/10.1101/2023.06.14.544758 doi: bioRxiv preprint space models, in: Machine Learning in Clinical Neuroimaging: 5th International Workshop, MLCN 2022, Held in Conjunction with MICCAI 2022, Singapore, September 18, 2022, Proceedings, Springer",
            "year": 2022
        },
        {
            "authors": [
                "J. Richiardi",
                "S. Achard",
                "H. Bunke"
            ],
            "title": "D",
            "venue": "Van De Ville, Machine learning with brain graphs: predictive modeling approaches for functional imaging in systems neuroscience, IEEE Signal processing magazine 30 ",
            "year": 2013
        },
        {
            "authors": [
                "L. Friedman",
                "G.H. Glover"
            ],
            "title": "F",
            "venue": "Consortium, et al., Reducing interscanner variability of activation in a multicenter fmri study: controlling for signalto-fluctuation-noise-ratio (sfnr) differences, Neuroimage 33 ",
            "year": 2006
        },
        {
            "authors": [
                "S.J. Broyd",
                "C. Demanuele",
                "S. Debener",
                "S.K. Helps",
                "C.J. James",
                "E.J. Sonuga-Barke"
            ],
            "title": "Default-mode brain dysfunction in mental disorders: a systematic review",
            "venue": "Neuroscience & biobehavioral reviews 33 ",
            "year": 2009
        },
        {
            "authors": [
                "M. Wang",
                "D. Zhang",
                "J. Huang",
                "P.-T. Yap",
                "D. Shen",
                "M. Liu"
            ],
            "title": "Identifying autism spectrum disorder with multi-site fmri via low-rank domain adaptation",
            "venue": "IEEE transactions on medical imaging 39 ",
            "year": 2019
        },
        {
            "authors": [
                "D.C. Van Essen",
                "K. Ugurbil",
                "E. Auerbach",
                "D. Barch",
                "T.E. Behrens",
                "R. Bucholz",
                "A. Chang",
                "L. Chen",
                "M. Corbetta"
            ],
            "title": "S",
            "venue": "W. Curtiss, et al., The human connectome project: a data acquisition perspective, Neuroimage 62 ",
            "year": 2012
        },
        {
            "authors": [
                "Q. Liu",
                "C. Chen",
                "J. Qin",
                "Q. Dou",
                "P.-A. Heng"
            ],
            "title": "Feddg: Federated domain generalization on medical image segmentation via episodic learning in continuous frequency space",
            "venue": "in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "year": 2021
        },
        {
            "authors": [
                "K. Ahuja",
                "J. Wang",
                "A. Dhurandhar",
                "K. Shanmugam",
                "K.R. Varshney"
            ],
            "title": "Empirical or invariant risk minimization? a sample complexity perspective",
            "venue": "arXiv preprint arXiv:2010.16412 ",
            "year": 2020
        },
        {
            "authors": [
                "V. Vapnik"
            ],
            "title": "The nature of statistical learning theory",
            "venue": "Springer science & business media, 1999. 33 (which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. The copyright holder for this preprint this version posted June 14",
            "year": 2023
        }
    ],
    "sections": [
        {
            "text": "In recent years, the collection and sharing of resting-state functional magnetic resonance imaging (fMRI) datasets across multiple centers have enabled studying psychiatric disorders at scale, and prompted the application of statistically powerful tools such as deep neural networks. Yet, multi-center datasets introduce non-biological heterogeneity that can confound the biological signal of interest and produce erroneous findings. To mitigate this problem, the neuroimaging community has adopted harmonization techniques previously proposed in other domains to remove site-effects from fMRI data. The reported success of these approaches in improving the generalization of the models have varied significantly. It remains unclear whether harmonization techniques could boost the final outcome of multi-site fMRI studies, to what extent, and which approaches are best suited for this task. In an attempt to objectively answer these questions, we conduct a standardized rigorous evaluation of seven different harmonization techniques from the neuroimaging and computer vision literature on two large-scale multi-site datasets (N = 2169 and N = 2366) to diagnose autism spectrum disorder and major depression disorder from static and dynamic representations of fMRI data. Interestingly, while all harmonization techniques removed site-effects from the data, they had little influence on disorder classification performance in standard k-fold and leave-one-site-out validation settings over a well-tuned baseline. Further investigation shows that the baseline model implicitly learns site-invariant features which could well explain its competitiveness with explicit harmonization techniques and suggest orthogo-\nPreprint submitted to Elsevier June 14, 2023\nnality between latent disease features and site discrminative features. However, additional experiments show that harmonization methods could be critical to report faithful results in settings where there is high intra-site class imbalance and the learning algorithm is prone to overfit on spurious features confounding the final outcome of the study.\nKeywords: Functional magnetic resonance imaging, Multi-site datasets, Domain generalization, Data harmonization, Deep Learning, Psychiatric disorders"
        },
        {
            "heading": "1. Introduction",
            "text": "Clinical machine learning (ML) studies on fMRI data, where models are trained to predict the diagnosis of brain disorders from resting-state functional activity, are at the core of recent research efforts in the field of psychiatry in an attempt to understand the pathophysiology of mental diseases, find objective biomarkers and improve the robustness of diagnosis in psychiatry. A central hurdle in this direction is the limited sample size a research group can collect at a single site to study a certain disorder. This hinders the generalization of the results and prevents the application of more statically powerful, yet less sample-efficient learning methods such as deep neural networks. In order to mitigate this problem, many researcher groups collaborate to create larger datasets, formed by aggregating fMRI scans obtained at different locations into a single multi-site dataset targeting a specific psychiatric disorder [1, 2, 3]. Unfortunately, inter-scanner variability, possibly caused by field strength of the magnet, manufacturer and parameters of the MRI scanner or radio-frequency noise environments [4, 5], creates a second problem known as batch effects, which is technical noise that might confound the real biological signal. The main consequence of batch effects is that researchers have observed a decrease in performance of the ML models on the larger multi-site datasets compared with performance obtained when using a dataset collected at a single site [6, 7]. Various approaches have been recently proposed to remove site-effects in the\n2\ninput fMRI signal post-acquisition. For example, [8] applied the Combat [9] harmonization technique, originally proposed to remove batch effects in genetic data, to estimate and regress site effects in functional connectivity while preserving the biological signal. [10] investigated the application of multiple linear transformations on the functional connectivity matrices to harmonize the data. [11] adopted a travelling subject dataset to estimate and remove measurement bias and improve the signal to noise ratio of the data. While these methods reported improvements in the generalization performance for their respective datasets and learning algorithms, further adoption of these techniques by multiple multi-site ML studies have not shown the same success. Data harmonization therefore remains a central area of controversy in the neuroimaging community.\nA major limitation of popular harmonization techniques used for fMRI data such as Combat and Covbat is that they are only constrained to static functional connectivity representations and can not be further applied to other fMRI imaging derivatives (e.g. 4D volumes, timecourses, spatio-temporal graphs). Such representations capture the dynamic nature of the signal which have been shown as a more suitable candidate for finding disease biomarkers versus pre-defined static correlations [12, 13]. Moreover, fMRI harmonization studies have only employed linear models or simple kernel-based methods as the baseline classifiers to evaluate the effectiveness of the proposed approaches. A sub-optimal choice of the learning model could influence the results, as the baseline model could be more prone to learn spurious correlations in the data, which might not be the case if a more powerful baseline is used in the first place. Finally, stateof-the-art harmonization methods [14, 15, 16] in other domains, have shifted to harmonizing data in the latent space while optimizing for the label of interest, thus harmonizing the discrimnative features in an end-to-end manner. This is in contrast to harmonizing the data in the input space as a pre-processing step as is the current practice with fMRI data. The application of such techniques have transferred to different medical domains including structural MRI[17], yet, their success in the fMRI domain is yet to be assessed. This is in general a common theme in the literature where the large majority of harmonization studies\n3\nfocus on structural MRI volumes and derivatives [18, 19, 17, 20]. The problem is different for fMRI data and we argue is more difficult to tackle. The dynamic and stochastic nature of fMRI, the high dimensionality of the data, lack of standardized learning architectures, high noise in the labels, and limited sample sizes deems the evaluation of different harmonization methods a very challenging task. However we conjecture that it is an essential step moving forward if we are to find generalizable biomarkers for psychiatric disorders.\nIn this work, our aim is to build a reproducible standardized test-bed to measure and compare the effect of applying different harmonization techniques on static and dynamic representations of the fMRI data while using powerful deep neural networks for end-to-end diagnosis of psychiatric disorders. Namely, we implement 7 different harmonization techniques from the computer vision and neuroimaging literature (including ComBat, CovBat, Domain adversarial methods and deep feature harmonization) across two multi-site datasets on both static and dynamic fMRI representations using suitable choices of deep neural network architectures. The motivation to use deep learning models in our framework is i) the flexibility of neural networks (NN) to approximate any function and to incorporate different objectives simultaneously, enabling the implementation of several modern harmonization techniques. ii) The powerful representations capacity of NN as baseline models to enable a fair evaluation of the advantage of harmonization methods. iii) A main motivation behind data-pooling is the ability to use powerful tools like deep neural networks. Given the rapid developments of research efforts in building deep learning models for diagnosing psychiatric disorders[21], we believe it would be more useful to the community, especially since we designed our framework in an architecture-agnostic manner, and one could easily swap the neural networks used in the framework with a customized architecture.\nIn addition to the challenging nature of the data, we acknowledge that it is difficult to systematically compare different harmonization techniques given the stochastic nature of training neural networks and the variability in models sizes introduced by additional modules for some harmonization techniques. We strive\n4\nfor a fair comparison by standardizing the model architecture across methods, conducting extensive hyperparameter search for each method, and using multiple different seeds in running the experiments to report the best average test metrics under the best hyper-parameters for each method optimized on an independent validation set. This work was developed on top of DomainBed[22] framework to ensure a fair, reproducible and streamlined comparison of the algorithms against each other and against a strong well-tuned baseline. Our implementation is open-source and can be easily extended to incorporate and evaluate new harmonization methods, different phenotypes and different representations of fMRI data along with their respective learning architectures of choice.\nWe summarize our contributions as follows:\n\u2022 First, we investigate the ability of the deep neural networks to classify\nsites from static and dynamic representations of the data. We conduct further experiments to understand how these effects are manifested in the imaging derivatives.\n\u2022 We implement different harmonization algorithms from the neuroimag-\ning and computer vision literature and conduct a rigorous evaluation to benchmark the utility of these algorithms in improving the generalizability of the trained model measured as improvement of phenotype prediction on leave-one-site-out and site-stratified k-fold validation schemes.\n\u2022 We propose a simple empirical method to estimate site differences in the\nlatent features used for classifcation of the biological label of interest.\n\u2022 We study the effect of ML method selection, number of available training\nsites, and intra-site class imbalance on the generalization performance of harmonization methods.\n5"
        },
        {
            "heading": "2. Methods",
            "text": ""
        },
        {
            "heading": "2.1. Datasets and models",
            "text": "For the purpose of our analysis, we chose two publicly available multi-site datasets. Each one of the datasets targets a different psychiatric disorder, with a different demographic background and different pre-processing pipeline. This experimental setup eliminates results biases towards these variables and support the generalization of the empirical results across pre-processing pipelines, demographics, and learning objectives.\nABIDE I+II. contains a collection of rs-fMRI brain images aggregated across 29 institutions[1]. It includes 1,028 participants with a diagnosis of autism, Asperger or pervasive developmental disorder (called ASD from now on), and 1,141 typically developing participants (TD). In this study, we select the largest 9 sites in terms of number of participants (N > 50) to conduct our analysis. Figure 1 shows the distribution of participants between sites. Demographic information are reported in Table S1. The 4D volumes were pre-processed according to the CPAC pipeline [23](See supplementary materials for details of the pre-processing steps). Further, A craddock-200[24] atlas was used to parcellate the volume into 200 regions of interest (ROIs) and the timecourse of each region was calculated as the mean timecourse of all the voxels within the ROI.\nRest-Meta-MDD. is currently the largest resting-state fMRI database for studying major depression disorder (MDD) [25], including 1,255 patients and 1,083 HC from 25 cohorts in China. In this study we also select the largest 9 sites that contain at least 20 participants from each group. Figure 1B shows the distribution of participants between sites of the datasets. Demographic data are reported in Table S2 (see Supplementary Information for more details about sample composition). Standard preprocessing of the data was done at each site using the Data Processing Assistant for Resting-State fMRI (DPARSF) [26].\n6\n116 time-courses of cortical and sub-cortical regions as defined by the HarvardOxford atlas [27] were extracted to obtain the input representations for the ML models."
        },
        {
            "heading": "2.2. Input representations and learning models",
            "text": "Another variable of interest in this study is the data representation used in the analysis. Different imaging derivatives can exhibit different levels of heterogeneity [28] and subsequently, display different results when evaluating the efficacy of harmonization methods. Thus, we chose two input representations to conduct our analysis, and for each representation we used a suitable learning model architecture.\nStatic - Pearson\u2019s correlations. are the pairwise correlations between ROI timecourses. This results in a matrix of N \u00d7 N matrix where N is the number of ROIs. The lower triangular matrix is then flattened and fed to the learning algorithm. This representation constitutes the most popular approach in ML in neuroimaging due to its simplicity, reduced dimensionality and the improvement of signal to noise ratio. In developing learning models for this representation, we chose multi-layer perceptrons (MLPs) []. Correlation vectors do not include any geometric inductive biases and thus fully connected layers offer a general statistical tool for feature extraction and classification, and have been successfully used for phenotype classification of correlations vectors [29]. Further, MLPs of-\n7\nfer a flexible building block to implement different modern deep harmonization techniques as we further discuss in section 4.\nDynamic - ROI Time-courses. Unlike static correlations, feeding the input as the time-courses preserves the dynamic nature of the temporal signal. This can be challenging to model due to the sequential nature of the data and its high dimensionality coupled with the limited sample size of the dataset. A promising solution to model this signal is 1D convolutional neural networks (1D-CNNs)[30]. 1D-CNNs offer a computationally efficient approach to extract spatio-temporal features from fMRI data and have shown success in characterizing psychiatric disorders and phenotypes [31]. 1D-CNNs consists of a cascade of 1D convolutional blocks (1D Conv Layer[32] + Batch-Norm[33] + Max-Pooling + activation function) used for feature extraction followed by global average pooling and a fully connected layer for classifcation. Similarly, this model offers a flexible solution for architecture-agnostic implementations of deep harmonization techniques. Figure 2 offers a schematic of our learning pipelines.\n8"
        },
        {
            "heading": "2.3. Site classification",
            "text": "Before delving into any harmonization methods, we first evaluated the performance of our pipelines in estimating the acquisition site on unseen data. The purpose of this experiment is three folds. 1) These results provide an empirical metric of heterogeneity in each dataset and each data representation. This metric can later be used to evaluate the efficacy of harmonization methods when applied and can provide a correlation measure of the effect of site heterogeneity on the performance of the main task in question. 2) Validate the ability of the proposed architectures to extract site-dependent features when supervised with the sites label as this is a perquisite for the implementation of some harmonization techniques. 3) Gain a better understanding of the nature of site differences in the imaging derivatives. (e.g. global vs local and linear vs non-linear)\nTo conduct this experiment, we trained the models as multi-class classifiers on the largest 4 sites in terms of number of samples in both datasets using both representations and their corresponding model architecture. Training details for both models is provided in the supplementary material. We report 5-fold cross-validation results on Table 1. The results show that acquisition site can be estimated with high accuracy (chance level = 25%) in both datasets. Further, it shows higher accuracy metrics when conducting the experiment on the timecourses indicating a higher heterogeneity in the dynamic signal over static connectivity. This makes intuitive sense as the temporal resolution of the timecourses is different across sites. This variability is absent in the static representations as the features only constitute the correlations of ROI signals. Another area of interest is the correlation of test metrics to the model\u2019s linearity/non-linearity. This is evaluated using exclusively linear or non-linear activation functions after each feature extraction layer in the models. i.e. a linear 1D CNN only applies an identity transformation after each convolution layer while a non-linear 1D CNN applies a non-linear transformation (rectified linear unit here) after each convolution layer. Significantly higher test metrics when training using non-linear models would suggest that site differences are intrinsically non-linear and non-linear functions are more optimal to represent\n9\nthis relationship. However, this behaviour is not observed in the results as linear and non-linear models perform on-par on both datasets and using both representations. This could suggest that site differences manifest as a linear transformation and a linear classification model is sufficient to capture such differences. To further investigate if the reason for this high heterogeneity is local (i.e. a subset of features) or global, we conduct a simple ablation study, where only a subset of N regions is used to train and test the model. For each N , we randomly sample the regions included in the subset, conduct the site classification experiment, repeat the sampling and training 200 times, and report the mean test accuracy and standard deviations for each N . Results of this experiment conducted on the dynamic representations of the top-4 sites of the ABIDE dataset show that site effects can be captured (Acc. \u2265 30%) across any N \u2265 5 regions in the signal, indicating the site effects is global across the input. Saliency maps of the trained MLP on the static representation also show high gradients across the entire signal indicating that site effects are global (See supplementary materials for figure and exact details to generate the saliency maps)."
        },
        {
            "heading": "2.4. Harmonization techniques",
            "text": "Strong site differences on the pre-processed imaging derivatives further confirm the existence of non-biological variability across sites and shows that standardized acquisition protocols and standardized pre-processing pipelines fail to eliminate such heterogeneity. This drives the need to find harmonization solutions to avoid erroneous findings. In this section we briefly describe some of the popular approaches in the computer vision and neuroimaging literature\n10\nimplemented in our framework. For further details about the methods, we refer the reader to the supplementary materials and the respective references of each method. We group the methods under two distinct categories and a baseline."
        },
        {
            "heading": "2.4.1. Baseline: no explicit harmonization",
            "text": "To validate the efficacy of harmonization techniques, we conduct experiments where the model is trained on the pre-processed imaging derivatives without any explicit harmonization either in the input space or feature space and solely with the objective of minimizing the prediction error of the label of interest. A key aspect to ensure a fair evaluation of the baseline is to deploy equivalent computational resources spent on tuning the harmonization methods to find the most optimal baseline under the same computational time and budget. This for example include model size, training time, hyperparameter search space, data standardization, etc. Failing to guarantee a fair playground is a common pitfall in harmonization studies and can lead to overoptimistic conclusions about the efficacy of some harmonization methods in improving the final outcome over an under-powered baseline."
        },
        {
            "heading": "2.4.2. Input harmonization",
            "text": "This group of techniques rely on the assumption that the distributional shift between sites is a linear transformation and could be mitigated via linear transformations of X into a shared harmonized input space. Common approaches in neuroimaging include z-scoring and whitening to remove batch effects caused by translation, scaling and rotation of input features. More general approaches to mitigate linear transformations that have been introduced and are quite popular in multiple medical and biological domains are ComBat[9] and CovBat[34]. Combat uses a multivariate linear mixed effects regression with terms for biological variables and scanner to model imaging feature measurements. The method uses empirical Bayes to simultaneously model and estimate biological and non biological terms and algebraically removes the estimated additive and multiplicative site effects. CovBat operates similarly to ComBat where in addition\n11\nto correction of mean and variance across sites, it also corrects for covariance. Another recent approach from the computer vision and deep learning literature is Inter-domain Mixup[35], which performs linear interpolations between random pairs samples from different domains with the same label of interest. These interpolated examples are then fed to the learning model and the models are trained with no further explicit harmonization in the objective function. The intuition behind this approach is that the training distribution becomes siteagnostic while preserving the biological signal of interest presumably existing within data samples with the same label. In our analysis we include ComBat, CovBat and Inter-domain Mixup for the static input representation, and only Inter-domain Mixup to the the dynamic representation. Note that combat and covbat approaches can not be applied to dynamic timecourses given that there are no 1-to-1 associations between the input features across individual samples."
        },
        {
            "heading": "2.4.3. Deep features harmonization",
            "text": "Recent advances in the domain generalization and domain adaption literature in deep learning rely on aligning the distribution of multiple domains in the latent space while training the network for the task of interest in an endto-end manner. This also acts as a regulization technique and has been shown successful in multiple tasks. In this work we selected four of the most popular approaches for deep features harmonization.\n\u2022 DANN [15]: Domain adversarial neural networks employ an adversarial\nnetwork to match feature distributions across sites.\n\u2022 IRM [16]: Independent Risk Minimization learns a feature representation\n\u03c6(X) such that the optimal linear classifier on top of that representation matches across domains.\n\u2022 CORAL [36]: matches the mean and covariance of feature distributions.\n\u2022 MMD [14]: matches the maximum mean discrepancy of feature distribu-\ntions.\n12"
        },
        {
            "heading": "3. Experiments",
            "text": "In order to evaluate the performance of a harmonization method, we would like answer two question in a quantitative manner; 1) How successful is each harmonization method in removing site-induced differences? 2)What is the impact of applying a specific harmonization technique on the final outcome of the DL model?\nTo answer the first question, we can revert back to the acquisition site prediction experiment we conducted on the imaging derivatives in Section 2.3 but now after harmonizing the data. The quantitative measure can thus be the difference between the test metric of the site classification pre- and post- applying harmonization. To recap, this means that we again train a new site classifcation model from scratch on the harmonized data and validate the performance using a 5-fold validation scheme. While this is straightforward for harmonization methods in the input space (e.g. ComBat, CovBat), it is more tricky for feature harmonization methods which harmonize the data in the feature space while training for the label of interest simultaneously. To enable conducting this experiment for these methods, we first train the models on the desired label of interest (while incorporating the harmonization method in the objective function), then only use the output of the trained featurizer to train the site classifier model without back-propagating the learning gradients of the sites through the featurizer. This means that the site classifier is trained only on the latent harmonized features and not the entire input data. We use the same approach to evaluate the features of the baseline method where we first train for the label of interest (without any explicit harmonization). Figure 3 illustrates the training process for the site classification experiment for both harmonized inputs and features.\nTo answer the second question, we used two evaluation scenarios for the label of interest. The first is when there is access to data from all sites during training and the objective is to utilize the labelled data to build a model that generalizes on unseen data from the same sites. This is evaluated using a site-\n13\nstratified k-fold validation scheme and we refer to it as the domain adaption problem. The second scenario is when we only have access to some sites during training and the objective is to train a model to generalize to unseen sites. This evaluated using a leave-one-site-out (LOSO) validation scheme and we refer to it as the domain generalization problem. Domain generalization is typically more challenging as there is no access to the test-set data distribution during training, yet, it represents a more realistic clinical scenario when the trained models are to be deployed on new unseen sites.\nTo ensure a fair comparison for all the methods, we followed a rigorous framework that accounts for variability that might arise due to hyperparameter configuration, model selection (i.e. when to stop training and which model to select for the prediction on the test set), and training stochasticity (randomness due to models initialization and data splits). To find the optimal hyperparameters for each method, we conducted a random search of 20 trials over the hyperparameter distribution of each method. We fixed the hyperparameter search distributions between the methods if possible barring some methodspecific hyperparameters. The best hyperparameter configuration (measured as the configuration yielding the highest test scores on a independent validation) were then used to train the final model and report the test results. For model selection during training and to stop training, we used an inner-validation split of (85% train - 15% validation) to monitor the convergence of the training and select the model for testing.\nIn reporting the test metrics for each method, we ran the entire experiment three times using three different seeds, thus making every random choice anew: hyperparameters random sampler, weights initializations, and dataset innerand outer-splits. Every number we report is a mean over these repetitions together with their estimated standard error.\n14"
        },
        {
            "heading": "4. Results",
            "text": ""
        },
        {
            "heading": "4.1. Post-harmonization site classification",
            "text": "We present site classifcation results after harmonization as described in Section 3. Figure 4 shows the test accuracy of the site classifcation experiment on the harmonized inputs/features on the largest four sites of the Rest-Metamdd dataset using static FC representation. The 5-fold accuracy dropped from 64.8\u00b13.9% pre-harmonization to approximately chance level after data harmonization using the adopted methods described in Section 2.4. These empirical results suggest that the harmonization methods successfully removed site-effects that were originally found in the imaging derivatives and that drove high classification accuracies when predicting acquisition site. We observed similar behaviour for the ABIDE dataset and the dynamic representations of the data (See supplementary materials). Most interestingly in the results is that that latent features learned by the baseline model with no explicit harmonization contained no predictive information of the acquisition site, resulting in chance level performance in the site classification experiment akin to methods that explicitly harmonize the data/features. We discuss in Section 5 why this behaviour might occur.\n15"
        },
        {
            "heading": "4.2. Domain adaptation",
            "text": "In this section, we present and compare the test results for the DL models optimized for the diagnosis of psychiatric disorders using a baseline model and harmonization methods in the domain adaptation scenario. To reiterate, domain adaption refers to the setup where there is availability of a number of samples from all the sites during training time. At inference, no data from unseen sites are presented to the model. We used site-stratified k-fold validation for reporting domain adaption results to ensure that all the data points are considered for testing and that all the sites are present at training and test time. The results in Figure 5 show the test performance for both the Rest-Meta-MDD and ABIDE datasets using static and dynamic representations when trained using different explicit harmonization techniques versus a baseline model trained on the unharmonized inputs. The results show that for all representation-dataset configurations, no harmonization method outperformed the baseline more than one accuracy point. Note that our results are generally on-par with the current reported state-of-the art performances for both datasets [37, 31, 38, 25, 39, 40, 41, 42].\n16"
        },
        {
            "heading": "4.3. Domain generalization",
            "text": "We present the test results for the DL models optimized for the diagnosis of psychiatric disorders using a baseline model and harmonization methods in the domain generalization scenario. Tables 2-5 show the test accuracy for each leftout-site in both datasets using both static and dynamic representations. Again, the average LOSO accuracy for all dataset/representation combination showed no superior metric for any of the harmonization methods over the baseline. Interestingly, the average LOSO accuracy is on-par with the k-fold results even though in the LOSO setup, the model has no access to data from the test site during training. This behaviour further confirms that the models learn site-invariant features and the performance does not drop due to introduced heterogeneity from unseen sites."
        },
        {
            "heading": "4.4. Further exploration",
            "text": "While the empirical results show that the studied harmonization methods did not improve the final outcome in domain generalization and domain adaption\n17\nsettings under our experimental setup, they open the door for more questions about other factors that can influence the results. Here we zoom into three factors that often vary across multi-site ML studies and can potentially lead to different conclusions about the efficacy of harmonization methods. Namely we study the implications of learning algorithm selection, number of sites within the dataset, and intra-site class imbalance. We conducted experiments for each variable with and without harmonization. For the sake of concision, we selected the ComBat technique in the experiments as a proxy for all the harmonization methods as it is currently the most popular approach within the field to harmonize the data."
        },
        {
            "heading": "4.4.1. Effect of ML model choice & number of sites on generalization",
            "text": "In our experiments, we have used DL models to evaluate the efficacy of harmonization techniques on performance of diagnosis prediction. A natural question then arises, does the choice of learning model influence the effect of data harmonization? A possible explanation to the performance of the baseline\n18\nis that DL models encode the data into a lower-dimensional latent feature space where site-discriminative features are lost. In this case, would a more simple linear model such as a logistic regression classifier benefit from harmonizing the data?\nAnother variable of interest is the number of acquisition sites in the dataset. Intuitively, as the number of sites in the dataset grow, the risk that a learning model optimized to minimize the disorder prediction error would over-fit on site information diminishes. In the experiments conducted thus far, we used nine sites (each with N\u2265 40) to evaluate the generalizability of the performance. This is however typically not the case for the majority of multi-site datasets where the number of sites is much smaller. Our setup could thus under-estimate the efficacy of harmonization in such datasets.\nTo account for these potential confounding variables in our framework, we compared the performance of a logistic regression classifier with and without combat harmonization on the tasks of MDD and ASD diagnosis from static FC in the domain generalization setup. Further, we conducted the experiments\n19\nusing different numbers of sites M \u2208 [3,11], where for each M the experiments are repeated 10 times each using randomly sampled sites. We report the results in Figure 6. Our results show that the LR model marginally benefited from combat harmonization at all sampled M configurations on the ABIDE dataset while the pattern is not clear for MDD-rest where harmonization appears to benefit the model at low M and then the baseline eventually catches up as M increases beyond five sites. A notable observation from this experiment orthogonal to harmonization is that test accuracy does not improve much as the number of sites in the training dataset increases (and hence the number of training samples). We suspect this could be due to the noise in the data and clinical labels which create an upper bound on the performance of the learning model at the current order of magnitude of available training samples."
        },
        {
            "heading": "4.5. Effect of intra-site class imbalance on generalization",
            "text": "A topic of controversy in neuroimaging multi-site studies, is that disease classifiers can overfit on site information if the class distribution across sites is highly non-uniform, resulting in unrealistic evaluation of the model performance. This is even more pronounced when the label of interest is challenging for the model and it is easier to predict the site to minimize its prediction error. This is often the case with psychiatric ML studies if researchers fail to account for high intra-site class imbalance either in the objective function of\n20\nthe model or data sampling. The datasets used in this study have a nearly uniform class distribution across most of the sites (see Figure 1) and in our experiments, we ensured a perfect uniform class distribution by under-sampling the majority class within each site during training. In this section, we aimed to investigate the effect of harmonization on model performance under different intra-site class distributions. Ideally, for datasets with highly class-imbalanced sites, harmonization methods should prevent erroneous convergence, represent a fair evaluation of the model, and even enable the use of the full dataset without under-sampling. To investigate this, we designed an experiment where class distribution is controlled by a parameter \u03b1 \u2208 [0.5, 1]. When sampling data from each site, we sampled class A with a probability \u03b1, and class B is thus sampled with probability 1 \u2212 \u03b1. Given that the base distribution in our sites is nearly uniform, we can thus control the class distribution by \u03b1. The class balance of the entire dataset is maintained by alternating classes A and B at each site. (i.e, under sampling class A at site 1, under sampling class B at site 2, and so on.). We present the results for both MDD and ASD classifcation on the MLP at different \u03b1 in Figure 7. The results confirm that for unharmonized data, increasing intra-site class imbalance linearly increases the performance of the model from the baseline results observed throughout all our experiments. This indicates that at higher alpha the model converges to learning site differences instead of the biological label of interest. But when using ComBat, the performance does not diverge with class imbalance, indicating the learning is not confounded at high intra-site class imbalance."
        },
        {
            "heading": "5. Discussion",
            "text": "The goal of this work was to evaluate the efficacy of applying harmonization techniques on the performance of learning models to diagnose psychiatric disorders from rs-fMRI data. Specifically, we compared the performance of seven different popular harmonization techniques against an equally powerful baseline on static and dynamic representations of fMRI data for the tasks of diagnosing\n21\nMDD vs HC and ASD vs TD in two publicly available multi-site datasets using deep learning models. The main finding of this work is that while harmonization techniques successfully remove site-effects in the data, they do not improve the final outcome on the desired tasks against the baseline for both static and dynamic representations of the data and in both domain adaptation and domain generalization settings. Investigating the behaviour of the baseline showed that when then model is optimized solely for the task of disease diagnosis and intrasite class distribution is balanced, it learns site-invariant feature representations which could well explain the empirical performance against explicit harmonization methods. Further investigative experiments showed no effect of the number of sites in the dataset or chosen learning algorithm on this conclusion. A key insight however is the advantage of harmonization in experimental setups with high inter-site class imbalance, where harmonization methods reduce the risk of\n22\nthe disease classifier diverging from learning the desired task. We observed this behaviour with unharmonized data: where intra-site class imbalance increased, the tendency of the model to learn spurious site features, and hence reporting erroneous inflated results increased. Harmonization methods could thus be used to \u201dcombat\u201d this behaviour and ensure that the reported results are that of the biological label of interest. For the remainder of the discussion, we examine the empirical results from a theoretical perspective, i.e. why does the baseline model learn site-invariant features? Further we highlight some of the common pitfalls in harmonization studies. Finally we make some remarks about our framework including its utility to the community and some of the limitations of our problem setup.\nWhy does the baseline model learn site-invariant features?. The need for harmonization stems from the fact there exists a distributional shift within the training dataset or between the training and test sets. As we have seen in our site experiment, data distribution P (X) is indeed different between sites and one can easily train a classifier to identify the source of the distribution. A special case however, is when the joint distribution between domains is different PA(Y,X) 6= PB(Y,X) but the conditional distribution is similar PA(Y |X) = PB(Y |X). This is commonly referred to in the literature as covariate shift and also encapsulates transformations to the data PA(Y |\u03c6(X)) = PB(Y |\u03c6(X)[43]. The goal of training supervised ML classifiers is to find such transformations of the data \u03c6(X) that maximizes the log likelihood of probability of the correct label. If there is indeed a conditional independence between the label and domain, an optimal model trained to minimize the empirical risk of prediction error of the label has no incentive to learn a transformed feature space which contains spurious site information [44].\nIn one often cited example, [45] trained a convolutional neural network to classify camels from cows. In the training dataset, most of the pictures of cows had green pastures, while most pictures of camels were in the desert. The model picked up the spurious correlation and associated green pastures with cows thus\n23\nfailing to classify pictures of cows on beaches correctly. This is an example of covariate shift, as the casual features of the label we are interested should be independent of where the picture is taken. However because the training dataset is biased, the model fails to learn the desired causal features. This is simply fixed by collecting a dataset that contains diverse examples or by data augmentation. This behaviour has been observed in multiple domains [46, 47, 48, 22] where advanced harmonization methods did not improve on baseline methods when a suitable learning algorithm is chosen and the datasets are unbiased towards the domain.\nWe believe that this is similar to the case in our setup. Despite strong siteeffects, because the datasets are balanced and the label is orthogonal to the site, the results suggest that the baseline model learns site-invariant features as seen in Section 4.1 and thus eliminating the need for explicit harmonization methods. This is not the case however in unbalanced sites where the baseline exhibit similar behaviour as in [45] and learns spurious domain features.\nCommon pitfalls in evaluating harmonization methods. As we alluded earlier, data harmonization is a major area of controversy in the neuroimaging community. In this section we review some of the common pitfalls that hinders the reproducability of harmonization methods across research groups and fuels this controversy.\nUnder-optimized baselines: A number of harmonization studies typically spend time and computational resources on optimizing a proposed harmonization algorithm, and do not allocate the same resources to build an equally powerful baseline model. For example, in our experiments, we observed that evaluating a baseline before training convergence or without the use of an inner validation to avoid overfitting, hampers its generalization performance on the test set. Other examples include training the baselines off-the-shelf without any search for optimal hyperparameters, while doing a exhaustive hyperparameter search for the models used in harmonization experiments. The role of ML practitioners is to ensure a fair comparison for all the methods for a faithful\n24\nevaluation of the efficacy of harmonization methods.\nInconsistencies in experimental conditions & model selection Similarly, experimental conditions should not vary between the methods. [22] finds that model selection (when to stop training) plays an important role in evaluating domain generalization algorithms and recommends explicit specification and standardization of model selection method for any harmonization algorithm. Further, the experiments should be run using several random seeds and data splits should be consistent between the methods.\nData Leakage Several open-source online implementations of harmonization methods utilize the entire dataset including the test samples to harmonize the data. This behaviour is a concrete example of double dipping. This does not only leak test-data statistics into training, but for some harmonization methods where the label of interest is required, it also leaks the test labels. This often leads to unrealistic optimistic results only due to the leakage and not because of data harmonization.\nThe utilities and limitations of our framework. To conduct the experiments reported in this work, we have built an open-source framework on top of the Domain-bed[22] library to ensure standardization of architectures/experimental setups and enable the reproducability of the results. While we have studied harmonization on two datasets with two different clinical targets, our framework can easily be extended to any fMRI dataset and for any phenotype using a few lines of code. This also includes integrating different representations of the data (e.g. 4D volumes, Graphs) along with any custom neural network architectures. We encourage researchers working on this topic to include their harmonization methods or architectures in our framework for evaluating the efficacy of new proposed methods against existing solutions.\nWhile the empirical results in our work guide suggest that harmonization algorithms do not improve the generalization performance of fMRI phenotype prediction models, it remains possible that harmonization may be useful in other settings than those we have investigated. And although our results sug-\n25\ngest that deep learning models appear to handle site heterogeneity intrinsically by learning site-invariant features, a small benefit of harmonization was observed for logistic regression for one of the two datasets. The main use for harmonization appears to be the situation with class-imbalance across sites, when algorithms could otherwise use site information to predict the label of interest. And although harmonization did not improve disorder prediction, it did not reduce performance either. Correct fMRI data harmonization therefore does not appear to come at a costs. However, we identified several situations where incorrect implementation of data harmonization could result in erroneous conclusions. We therefore advice to use fMRI data harmonization primarily to combat class imbalance, rather than incorporating it as standard strategy for multi-site studies."
        }
    ],
    "title": "Harmonization techniques for machine learning studies using multi-site functional MRI data",
    "year": 2023
}