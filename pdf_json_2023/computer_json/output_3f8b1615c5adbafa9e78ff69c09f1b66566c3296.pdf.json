{
    "abstractText": "Nowadays, statistical arbitrage is one of the most attractive fields of study for researchers, and its applications are widely used also in the financial industry. In this work, we propose a new approach for statistical arbitrage based on clustering stocks according to their exposition on common risk factors. A linear multifactor model is exploited as theoretical background. The risk factors of such a model are extracted via Principal Component Analysis by looking at different time granularity. Furthermore, they are standardized to be handled by a feature selection technique, namely the Adaptive Lasso, whose aim is to find the factors that strongly drive each stock\u2019s return. The assets are then clustered by using the information provided by the feature selection, and their exposition on each factor is deleted to obtain the statistical arbitrage. Finally, the Sequential Least SQuares Programming is used to determine the optimal weights to construct the portfolio. The proposed methodology is tested on the Italian, German, American, Japanese, Brazilian, and Indian Stock Markets. Its performances, evaluated through a Cross-Validation approach, are compared with three benchmarks to assess the robustness of our strategy.",
    "authors": [
        {
            "affiliations": [],
            "name": "Federico Gatta"
        },
        {
            "affiliations": [],
            "name": "Carmela Iorio"
        },
        {
            "affiliations": [],
            "name": "Diletta Chiaro"
        },
        {
            "affiliations": [],
            "name": "Fabio Giampaolo"
        },
        {
            "affiliations": [],
            "name": "Salvatore Cuomo"
        }
    ],
    "id": "SP:2a3d9790e32e3a03e511df933875d2423cc448b4",
    "references": [
        {
            "authors": [
                "L Horv\u00e1th",
                "G Rice"
            ],
            "title": "Asymptotics for empirical eigenvalue processes in high-dimensional linear factor models",
            "venue": "J Multivar Anal 169:138\u2013165",
            "year": 2019
        },
        {
            "authors": [
                "B Williams"
            ],
            "title": "Identification of the linear factor model",
            "venue": "Economet Rev",
            "year": 2020
        },
        {
            "authors": [
                "R Salmer\u00f3n",
                "C Garc\u0131\u0301a",
                "J Garc\u0131\u0301a"
            ],
            "title": "Variance inflation factor and condition number in multiple linear regression",
            "venue": "J Stat Comput Simul",
            "year": 2018
        },
        {
            "authors": [
                "V Ressel",
                "D Berati",
                "C Raselli",
                "K Birrer",
                "R Kottke",
                "HJ van Hedel",
                "RO Tuura"
            ],
            "title": "Magnetic resonance imaging markers reflect cognitive outcome after rehabilitation in children with acquired brain injury",
            "venue": "Eur J Radiol",
            "year": 2020
        },
        {
            "authors": [
                "R Mozun",
                "C Ardura-Garcia",
                "ES Pedersen",
                "M Goutaki",
                "J Usemann",
                "F Singer",
                "P Latzin",
                "A Moeller",
                "CE Kuehni"
            ],
            "title": "Agreement of parent-and child-reported wheeze and its association with measurable asthma",
            "venue": "traits. Pediatr Pulmonol",
            "year": 2021
        },
        {
            "authors": [
                "G Connor"
            ],
            "title": "The three types of factor models: a comparison of their explanatory power",
            "venue": "Financ Anal J",
            "year": 1995
        },
        {
            "authors": [
                "EF Fama",
                "KR French"
            ],
            "title": "Dissecting anomalies with a fivefactor model",
            "venue": "Rev Financ Stud",
            "year": 2016
        },
        {
            "authors": [
                "EF Fama",
                "KR French"
            ],
            "title": "Common risk factors in the returns on stocks and bonds",
            "year": 2021
        },
        {
            "authors": [
                "SJ Koopman",
                "M van der Wel"
            ],
            "title": "Forecasting the us term structure of interest rates using a macroeconomic smooth dynamic factor model",
            "venue": "Int J Forecast",
            "year": 2013
        },
        {
            "authors": [
                "JJ Szczygielski",
                "L Br\u00fcmmer",
                "HP Wolmarans"
            ],
            "title": "An augmented macroeconomic linear factor model of south African industrial sector returns",
            "venue": "J Risk Financ",
            "year": 2020
        },
        {
            "authors": [
                "Yip",
                "Fung",
                "Xu",
                "Lei"
            ],
            "title": "An application of independent component analysis in the arbitrage pricing theory",
            "venue": "Proceedings of the IEEE-INNS-ENNS international joint conference on neural networks. IJCNN",
            "year": 2000
        },
        {
            "authors": [
                "Lam C",
                "Yao Q (2012) Factor modeling for high-dimensional time series"
            ],
            "title": "inference for the number of factors",
            "venue": "Ann Statist, pp 694\u2013726 11730 Neural Computing and Applications",
            "year": 2023
        },
        {
            "authors": [
                "J Fan",
                "Y Liao",
                "W Wang"
            ],
            "title": "Projected principal component analysis in factor models",
            "venue": "Ann Stat",
            "year": 2016
        },
        {
            "authors": [
                "Y Ding",
                "Y Li",
                "X Zheng"
            ],
            "title": "High dimensional minimum variance portfolio estimation under statistical factor models",
            "venue": "J Econom",
            "year": 2021
        },
        {
            "authors": [
                "F Giordano",
                "ML Rocca",
                "ML Parrella"
            ],
            "title": "Clustering complex time-series databases by using periodic components",
            "venue": "Stat Anal Data Min ASA Data Sci J",
            "year": 2017
        },
        {
            "authors": [
                "H Li"
            ],
            "title": "Multivariate time series clustering based on common principal component analysis",
            "venue": "Neurocomputing",
            "year": 2019
        },
        {
            "authors": [
                "AM Alonso",
                "D Pe\u00f1a"
            ],
            "title": "Clustering time series by linear dependency",
            "year": 2019
        },
        {
            "authors": [
                "F TRIGGIANO"
            ],
            "title": "Gaussian processes and expected signature for time series classification",
            "year": 2022
        },
        {
            "authors": [
                "TW Liao"
            ],
            "title": "Clustering of time series data-a survey",
            "venue": "Pattern Recogn",
            "year": 2005
        },
        {
            "authors": [
                "D Le\u00f3n",
                "A Arag\u00f3n",
                "J Sandoval",
                "G Hern\u00e1ndez",
                "A Ar\u00e9valo",
                "J Ni\u00f1o"
            ],
            "title": "Clustering algorithms for risk-adjusted portfolio construction",
            "venue": "Proc Comput Sci",
            "year": 2017
        },
        {
            "authors": [
                "J Puerto",
                "M Rodr\u0131\u0301guez-Madrena",
                "A Scozzari"
            ],
            "title": "Clustering and portfolio selection problems: a unified framework",
            "venue": "Comput Op Res",
            "year": 2020
        },
        {
            "authors": [
                "C Iorio",
                "G Frasso",
                "A D\u2019Ambrosio",
                "R Siciliano"
            ],
            "title": "A p-spline based clustering approach for portfolio selection",
            "venue": "Expert Syst Appl",
            "year": 2018
        },
        {
            "authors": [
                "K Imajo",
                "K Minami",
                "K Ito",
                "K Nakagawa"
            ],
            "title": "Deep portfolio optimization via distributional prediction of residual factors",
            "venue": "Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Avellaneda M",
                "Lee J-H"
            ],
            "title": "Statistical arbitrage in the us equities market",
            "venue": "Quant Financ",
            "year": 2010
        },
        {
            "authors": [
                "N Huck"
            ],
            "title": "Large data sets and machine learning: applications to statistical arbitrage",
            "venue": "Eur J Op Res",
            "year": 2019
        },
        {
            "authors": [
                "E L\u00fctkebohmert",
                "J Sester"
            ],
            "title": "Robust statistical arbitrage strategies",
            "venue": "Quant Financ",
            "year": 2021
        },
        {
            "authors": [
                "Z Zhao",
                "R Zhou",
                "DP Palomar"
            ],
            "title": "Optimal mean-reverting portfolio with leverage constraint for statistical arbitrage in finance",
            "venue": "IEEE Trans Signal Process",
            "year": 2019
        },
        {
            "authors": [
                "LR Sant\u2019Anna",
                "JF Caldeira",
                "TP Filomena"
            ],
            "title": "Lasso-based index tracking and statistical arbitrage long-short strategies",
            "venue": "North Am J Econom Financ",
            "year": 2020
        },
        {
            "authors": [
                "H Zou"
            ],
            "title": "The adaptive lasso and its oracle properties",
            "venue": "J Am Stat Assoc 101(476):1418\u20131429",
            "year": 2006
        },
        {
            "authors": [
                "K Balladares",
                "JP Ramos-Requena",
                "JE Trinidad-Segovia"
            ],
            "title": "S\u00e1nchezGranero MA (2021) Statistical arbitrage in emerging markets: a global test of efficiency",
            "year": 2021
        },
        {
            "authors": [
                "SM Carta",
                "S Consoli",
                "AS Podda",
                "DR Recupero",
                "MM Stanciu"
            ],
            "title": "Ensembling and dynamic asset selection for risk-controlled statistical arbitrage",
            "venue": "IEEE Access",
            "year": 2021
        },
        {
            "authors": [
                "S Demir",
                "B Stappers",
                "K Kok",
                "NG Paterakis"
            ],
            "title": "Statistical arbitrage trading on the intraday market using the asynchronous advantage actor-critic method",
            "venue": "Appl Energy",
            "year": 2022
        },
        {
            "authors": [
                "MC Massi",
                "F Gasperoni",
                "F Ieva"
            ],
            "title": "Paganoni AM Feature selection for imbalanced data with deep sparse autoencoders ensemble",
            "venue": "Stat Anal Data Min ASA Data Sci J. https://doi.org/10",
            "year": 2022
        },
        {
            "authors": [
                "UM Khaire",
                "R Dhanalakshmi"
            ],
            "title": "Stability of feature selection algorithm: a review",
            "venue": "J King Saud Univ Comput Inform Sci",
            "year": 2019
        },
        {
            "authors": [
                "J Fan",
                "R Li"
            ],
            "title": "Variable selection via nonconcave penalized likelihood and its oracle properties",
            "venue": "J Am Stat Assoc 96(456):1348\u20131360",
            "year": 2001
        },
        {
            "authors": [
                "S Tian",
                "Y Yu"
            ],
            "title": "Financial ratios and bankruptcy predictions: an international evidence",
            "venue": "Int Rev Econom Financ",
            "year": 2017
        },
        {
            "authors": [
                "C Dong",
                "S Li"
            ],
            "title": "Specification lasso and an application in financial markets",
            "year": 2021
        },
        {
            "authors": [
                "S Cuomo",
                "F Gatta",
                "F Giampaolo",
                "C Iorio",
                "F Piccialli"
            ],
            "title": "An unsupervised learning framework for marketneutral portfolio",
            "venue": "Expert Syst Appl",
            "year": 2022
        },
        {
            "authors": [
                "M Gupta",
                "B Gupta"
            ],
            "title": "An ensemble model for breast cancer prediction using sequential least squares programming method (slsqp)",
            "venue": "eleventh international conference on contemporary computing",
            "year": 2018
        },
        {
            "authors": [
                "P Fracas",
                "KV Camarda",
                "E Zondervan"
            ],
            "title": "Shaping the future energy markets with hybrid multimicrogrids by sequential least squares programming",
            "venue": "Phys Sci Rev",
            "year": 2021
        },
        {
            "authors": [
                "J Xie",
                "H Zhang",
                "Y Shen",
                "M Li"
            ],
            "title": "Energy consumption optimization of central air-conditioning based on sequential-leastsquare-programming",
            "year": 2020
        },
        {
            "authors": [
                "B Li",
                "SC Hoi"
            ],
            "title": "Online portfolio selection: a survey",
            "venue": "ACM Comput Surv (CSUR)",
            "year": 2014
        },
        {
            "authors": [
                "F Bucci",
                "F Lillo",
                "J-P Bouchaud",
                "M Benzaquen"
            ],
            "title": "Are trading invariants really invariant? Trading costs matter",
            "venue": "Quant Financ",
            "year": 2020
        },
        {
            "authors": [
                "M Schneider",
                "F Lillo"
            ],
            "title": "Cross-impact and no-dynamic-arbitrage. Quantit Financ 19(1):137\u2013154 Publisher\u2019s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations",
            "venue": "Neural Computing and Applications",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "used also in the financial industry. In this work, we propose a new approach for statistical arbitrage based on clustering stocks according to their exposition on common risk factors. A linear multifactor model is exploited as theoretical background. The risk factors of such a model are extracted via Principal Component Analysis by looking at different time granularity. Furthermore, they are standardized to be handled by a feature selection technique, namely the Adaptive Lasso, whose aim is to find the factors that strongly drive each stock\u2019s return. The assets are then clustered by using the information provided by the feature selection, and their exposition on each factor is deleted to obtain the statistical arbitrage. Finally, the Sequential Least SQuares Programming is used to determine the optimal weights to construct the portfolio. The proposed methodology is tested on the Italian, German, American, Japanese, Brazilian, and Indian Stock Markets. Its performances, evaluated through a Cross-Validation approach, are compared with three benchmarks to assess the robustness of our strategy.\nKeywords Machine learning Time series Cluster analysis Market neutral portfolio"
        },
        {
            "heading": "1 Introduction",
            "text": "Nowadays, Artificial Intelligence approaches in Finance are becoming dominant. This is due to the broad discussion about the analysis of financial data developed over the years. In fact, since the earliest works related to time series, the subject has pulled in many academics and practitioners. Among the others, one of the most exciting application fields is the development of investment strategies and risk management. In particular, statistical arbitrage is concerned with creating trading strategies that exploit hidden patterns in the behavior of related assets. Currently, most of the works in this field are based on future price predictions. However, the reliability of such forecasting approaches is\nhugely discussed. Furthermore, one can also argue that risk hedging is sometimes inefficient because it does not consider that some risk factors are specific to only an asset subset.\nIn this work, we propose a methodology to overcome these limitations. The main task we are involved in is building a portfolio that is able to reduce investment risk. In more detail, we consider a set of time T \u00bc f1; :::; Tg and a universe of stocks J \u00bc f1; :::; Jg. As a common practice in the financial literature, we work with the stocks returns, i.e., R \u00bc frjgj2J . Each return is a time series indexed in T , that is rj \u00bc frjtgt2T . A portfolio is a linear combination of stock returns obtained with a weight vector / \u00bc \u00f0/1; :::;/J\u00de 2 \u00bd 1; 1 J such that the norm l1 of / is equal to 1. In other words, taking into account that each /j can be considered as the portfolio exposition on stock jth, we require that the full exposition, on both long and short positions, is unitary. It should be noted that by exploiting the above formulation, two assumptions are made: i) the assets are infinitely divisible, and ii) short positions are allowed. Furthermore, for convenience, we also assume iii) & Federico Gatta federico.gatta@sns.it 1 Scuola Normale Superiore, Pisa, Italy 2 Department of Economics and Statistics, University of Naples Federico II, Naples, Italy 3 Department of Mathematics and Applications, University of\nNaples Federico II, Naples, Italy\nthere are not transaction costs and iv) our trades have no impact on prices.\nAmong all the possible portfolios, we are interested in finding the one which cuts back on the risk related to the investment. Actually, this is not a straightforward task, starting from the definition and the type of risk we aim to minimize. In the following, we evaluate portfolio strategies with several performance measures through Cross-Validation (CV), measuring the mean and standard deviation (std) of each investment. Then, we consider a portfolio as robust if its mean is optimal and its deviation is low. This is the main task of this work: we want to find an investment strategy that exhibits good performance and is reliable, i.e., whose results do not change a lot according to the time in which it works.\nTo achieve our goal, we exploit a multi-step procedure. Firstly, we represent each stock return with a convenient linear factor model by using the Principal Component Analysis (PCA). We aim to extract risk factors at different time granularities to have a complete overview of both short-term and long-term risk factors. In this stage, a multicollinearity filter is applied to avoid the presence of multicollinearity, which is a linear dependence between two or more regressors that introduces a bias in the parameters estimate. Then, we exploit this representation and the Adaptive-Lasso to perform feature selection and to partition stocks universe J by grouping those stocks whose behavior is affected by similar factors. Finally, we work inside each cluster to obtain a local portfolio that deletes the exposition on such factors, and we aggregate the portfolios resulting from each cluster to get the final one.\nConsequently, the main contributions of this paper can\nbe summarized as follows:\n\u2022 we propose a novel multi-horizons methodology for stocks clustering; \u2022 we propose a statistical arbitrage strategy based on the previous clustering procedure; \u2022 we prove the stability of our strategy by carefully comparing it with three well-known benchmarks, i.e.,\nthe minimum variance portfolio, the mean-var portfolio, and the Exponential Gradient, on both the Italian, German, American, Japanese, Brazilian, and Indian stock markets.\nThe following of this paper can be summarized as follows. In Sect. 2 a brief literature review of the building blocks of our proposal is offered. Section 3 introduces our methodology. Section 4 describes the experimental stage by providing detailed information about the exploited datasets, the experimental setup, and the obtained results. Finally, Sect. 5 concludes this work by summarizing limitations and findings and providing further analysis directions."
        },
        {
            "heading": "2 Contextualization and related work",
            "text": "This section provides a short overview of related work and state-of-the-art approaches linked to our proposal. In this way, it is provided a contextualization of the problem and, in particular, of our framework."
        },
        {
            "heading": "2.1 Model determination",
            "text": "Linear factor models play a crucial role in finance ranging from asset pricing theory to portfolio optimization. In the literature, there are different types of linear factor models (e.g., dominant residuals, systematic-idiosyncratic, and pure exogenous). In this manuscript, we focus on the systematic-idiosyncratic class of the linear factor model. It relates the rate of return on an asset j th to the values of a limited number of factors by a linear equation, as in Eq. 1:\nrjt \u00bc aj \u00fe b j 1F1;t \u00fe :::\u00fe b j nFN;t \u00fe ejt 8j 2 J ; 8t 2 T\n\u00f01\u00de\nwe can also rewrite Eq. 1 in matrix form, as:\nR \u00bc IaT \u00fe Fb\u00fe e\nwhere R \u00bc \u00f0r1; . . .; rJ\u00de 2 RT J is the matrix whose columns are the stock returns time series, I \u00bc \u00f01; . . .; 1\u00de 2 RT is the unitary vector, a \u00bc \u00f0a1; . . .; aJ\u00de 2 RJ are J constants and aT is the transpose of a, F \u00bc \u00f0F1; . . .;FN\u00de 2 RT N is the matrix whose columns are the N risk factors Fi \u00bc fFi;tgt2T , b \u00bc fbjng 2 RN J is a matrix of factor loadings and e \u00bc \u00f0e1; . . .; eJ\u00de 2 RT J is the matrix whose columns are the residuals time series ej \u00bc fejtgt2T . In this model, the factors and the residuals satisfy two types of constraints. More specifically, the residual is assumed to be uncorrelated with each factor, E\u00f0ej;Fn\u00de \u00bc 0; j \u00bc 1; . . .; J; n \u00bc 1; . . .N. In addition, the residual for one asset\u2019s return is assumed to be uncorrelated with that of any other, E\u00f0ej; eq\u00de \u00bc 0; j 6\u00bc q \u00bc 1; . . .; J. Since the risk factors are systematic, the only sources of correlations among asset returns are given by their exposures to the factors and the covariances among the factors. So, in this model, we assume asset return residual components are unrelated. Hence, residual components are particular to each asset. Thus, the risk associated with the residual return is idiosyncratic for that asset.\nSeveral works employ and discuss such a type of model. For example, [1] studies the asymptotic properties of the covariance structure, as both the size of the time-series universe and the number of available observations tend to infinity. Instead, [2] is related to determining the risk factors in a context that allows risk factors to be correlated with each other.\nBy contrast, in constructing our framework, we need independence between the regressors in the factor model. In particular, as common in the standard financial literature, we assume the risk factors are the observations of independent, identically distributed random variables with 0 mean and unitary variance. In particular, we are assuming the absence of multicollinearity among them. This hypothesis is central to our work, as both the clustering and the statistical arbitrage strongly depend on the model parameters. So, we verify it by applying condition number measures that is usually applied to detect the presence of collinearity (see, for example, [3]). It is a widely used approach in the recent literature, see for example [4, 5].\nAnother critical point related to applying a linear factor model as that in Eq. 1 is the determination of the risk factors. As pointed out by [6], three different approaches exist to solve this task. The fundamental approach uses the fundamentals of the stocks considered as risk factors, e.g., P/E Ratio. It is exploited by several researchers, including the Nobel Prize Fama. In particular, there are a series of articles, such as [7, 8], which develop a five-factor model for stock pricing. Similarly, the macroeconomic approach exploits as risk factors some macroeconomic variables like the return of market indexes or the inflation rate. An example of this approach is [9], where the yield curve is approximated with a dynamic factor model obtained with dimensionality reduction techniques applied to many macroeconomic features. Instead, in [10] South Africa stocks returns are analyzed with the help of both national and international variables. The main aim is to understand how different features impact the national industrial process.\nIn the two approaches above, the risk factors are searched outside the data. In contrast, the statistical approach employs feature extraction techniques to extract risk factors from the stocks universe itself. Usually, these techniques belong to both the fields of statistical and machine learning. The aim is to rely on data analysis instruments to obtain factors highly representative of the data we are working with, particularly their variance. [11] work with time series from the Japanese Stock Market by applying the Independent Component Analysis to extract risk factors fed into a linear factor model, on the background of Arbitrage Pricing Theory. Instead, several other works focused on applying PCA, thanks to its simplicity, speed, and reliability. In [12], the asymptotic properties of the factors obtained via PCA are analyzed, under the stationary condition, as the dimension of the sample and of the time series go to infinity. Furthermore, the results are tested on stocks belonging to the S&P index. Instead, in [13], risk factors are obtained by applying the PCA on the projection of the input matrix on an appropriate space. The proposal is then evaluated on the S&P constituent stocks. Finally, [14]\nexploits the PCA to extract risk factors for a linear model, which is then used as a starting point for constructing a minimum variance portfolio. Also, the experimental stage of this study is carried out on the stocks in the S&P index."
        },
        {
            "heading": "2.2 Clustering",
            "text": "The linear factor models built with the PCA performed on multiple granularities are then exploited to cluster the stocks. Time-series clustering is an open debate widely discussed in the literature, which is far more complex than static data. Due to the enormous complexity of the task, several works for specific-purpose goals have been proposed, such as [15\u201318]. Furthermore, several papers have also been concerned with reviewing and classifying the existing methodologies. For example, [19] divides clustering methodologies according to the way they operate. In particular, raw-data-based approaches directly work with time series. This goal is often achieved by exploiting some particular distance metrics that considers the input\u2019s temporal evolution. Instead, model-based approaches work with a specific time-series model for each series by clustering the fitted coefficients. Finally, features-based approaches extract from each time series a feature vector, and the clustering is performed on those vectors.\nActually, our proposal can be placed in the framework of features-based clustering. In fact, from each stock j, it is extracted the binary vector of features hj that represents if the stock is significantly affected by the corresponding risk factor. So, indicating with bj \u00bc \u00f0bj1; :::; b j N\u00de the estimated coefficients of the linear factor model representing the stock j, the feature vector hj 2 RN can be written as:\nhj \u00bc \u00f0hj1; :::; h j N\u00de 2 RN ; s:t: h j i \u00bc 1 if bji 6\u00bc 0 0 if bji \u00bc 0\n\u00f02\u00de\nIn the recent literature, several works try to exploit clustering methodologies for portfolio optimization or trading/ investment strategies. For example, [20] uses different clustering techniques such as K-means to partition the assets universe. Then, standard portfolio optimization techniques are applied to each cluster. The strategy is tested on the high-frequency data of the Russell 1000 stocks. Instead, [21] exploits the correlation between stocks as a critical feature to cluster assets and create optimal portfolios. Furthermore, the authors develop a framework that unifies the typical two stages of this strategy, i.e., clustering and portfolio construction. However, several other methods for portfolio construction based on a clustering and unsupervised learning approach have been proposed in the financial literature. See, for example, [22] which shows in detail different state-of-the-art\nmethodologies and how they can impact academic financial research."
        },
        {
            "heading": "2.3 Statistical arbitrage and portfolio construction",
            "text": "Once the clustering is obtained, our methodology constructs a market-neutral portfolio within each cluster, which is a portfolio such that the exposition on each risk factor is 0. In other words, the return of such a portfolio type is not related to the overall market conditions, and it is affected only by the weighted sum of aj and ej. According to the classical financial literature, in a well-diversified portfolio, the idiosyncratic risks should delete themselves for the diversification effect. However, it has been shown in recent works that they could contain valuable information and undiscovered patterns. Such information should be taken into account to improve investment strategies significantly. For example, [23] constructs a portfolio by exploiting the residual of a linear factor model with a fundamental approach. Their proposal is compared with the portfolio obtained without considering the idiosyncratic risks. The experiments on the American market show the effectiveness of their proposal, with a Sharpe Ratio significantly higher than the benchmark one, thanks to the reduced portfolio variance. In [24], the authors develop a strategy to exploit hidden patterns in the residuals to construct a zero investment portfolio in a deep learning framework. Their proposal has been accurately tested on stocks markets over the years, showing good robustness also during the financial crisis.\nAs for statistical arbitrage, several experiments have been carried out through the years. Among them, [25] compares the statistical and macroeconomic approaches to constructing a market-neutral portfolio. In more detail, the authors test the strategy obtained with PCA and that obtained by using the Exchange Traded Funds as a proxy of risk factors. The experiments are carried out on the American stock market data between 1997 and 2007. In particular, the reliability of the strategies is tested during both bull and bear periods (the so-called Dot-com Bubble). The final results show the profitability of both approaches in the considered period. [26] contains a comparison among the applications of different machine learning techniques for constructing statistical arbitrage and portfolio optimization strategies. In particular, to provide a reliable analysis of these state-of-the-art methods, the author exploits a dataset made up of hundreds of American stocks over about two decades. Also, [27] discusses the properties behind the statistical arbitrage to provide a theoretical background and a strong characterization of this strategy. Finally, Table 1 contains a comparison among\ndifferent statistical arbitrage approaches presented in the last years. In particular, we highlight the peculiarities of each work and its weaknesses when compared to our proposal."
        },
        {
            "heading": "3 The methodology",
            "text": "In this section, we briefly show the primary data analysis tools we exploit. Then, we describe the proposed methodology by providing both pseudo-codes and illustrative images. Finally, we discuss some issues related to our proposal."
        },
        {
            "heading": "3.1 Feature selection: adaptive lasso",
            "text": "In our framework, a key role is played by the feature selection, which should identify the risk factors which actually drive stock returns. Several approaches for feature selection are discussed in the literature. For example, see [34, 35] for a comprehensive review of the several methodologies, their application field, and their statistical properties. Among them, we exploit the Adaptive Lasso (A-Lasso) [30]. It is a linear regression technique with weighted l1 regularization terms, so the loss function can be written as:\nL \u00bc 1 T XT t\u00bc1 r jt XN i\u00bc1 Fi;tb j i\n\" #2\n\u00fek XN i\u00bc1 jwji b j i j w j i \u00bc 1 b\u0302 ji\n!s \u00f03\u00de\nwhere the weights wi are obtained from the inverse of the Ordinary Least Square coefficients b\u0302ji and k; s are two nonnegative hyperparameters.\nIt can be shown that thanks to the l1 regularization terms, the parameters related to the negligible regressors are set to zero. In this way, we can effectively know which risk factors play a role in the return of each stock. Furthermore, the reason behind our choice of using A-Lasso is twofold. From one side, it is designed to handle a linear relationship between the target and the predictors, such as that in Eq. 1, with little computational requirements. On the other side, it has been shown that A-Lasso satisfies the oracle properties specified in [36]. These properties ensure the asymptotic consistency of the estimator in terms of both relevant features detected and parameters estimate.\nSo, thanks to its reliability and accuracy in determining relevant features, A-Lasso has been widely used in the modern financial literature; see, for example, the works [37, 38]. The former is related to bankruptcy prediction. Several markets from Europe and Japan are analyzed, and\nthe A-Lasso is applied to determine which features are relevant for this task. The experimental stage proves that, in almost all the considered study cases, the feature selection can improve the performance of the prediction. The latter is concerned with explaining excess returns in the stock market. To face this task, the authors propose the Specification-Lasso, a modified version of Lasso and A-Lasso. The strategy\u2019s validity is shown by both simulated and real experiments, where the regressors are several fundamentals related to the stocks under observation."
        },
        {
            "heading": "3.2 Risk factors extraction",
            "text": "In this work, the dataset of each experiment we carry out is made up of daily observations of stocks in six different markets: Italian, German, American, Japanese, Brazilian, Indian (see Sect. 4 for further information on the datasets). So, for each experiment, we use a set of risk factors obtained starting from the daily returns dataset. Furthermore, as our investment strategies have monthly horizons, we also exploit risk factors obtained from the monthly returns dataset, which is the dataset obtained by aggregating daily returns each month.\nWe extract the risk factors in Eq. 1 via PCA. Actually, there exist several feature extraction techniques. PCA is a linear approach, while more sophisticated nonlinear approaches are the Neural Network PCA (NNPCA) or the Variational AutoEncoder (VAE). However, previous study [39] shows that in the stock markets, PCA performs as well as the nonlinear methods, with the strong advantage of being computationally cheaper. The great advantage in computational time, while the output results are almost similar, is the reason for our choice.\nOnce extracted, the risk factors are standardized to obtain values distributed as a zero-mean random variable with unitary variance. As already pointed out, we focus on extracting two different sets of factors: daily and monthly. The formers are obtained by applying the PCA to the daily returns dataset. For the latter, we first apply the PCA on the monthly returns dataset to obtain the weights of the Principal Components (PCs). Then, the weights matrix is applied to the daily returns dataset in order to extract the monthly risk factors on a daily basis. Figure 1 describes the process of feature extraction.\nIn other words, consider the overall set of risk factors in Eq. 1, i.e., F \u00bc fFigi2f1;:::;Ng. We aim to split it into two subsets. The first one is Fd \u00bc fFigi2f1;:::;Ndg and it is referred to the risk factors extracted on daily basis. The second one is Fm \u00bc fFigi2fNd\u00fe1;:::;Ng and it contains the PCs obtained on monthly basis.\nOne of the significant issues for this type of approach is determining how many PCs have to be considered. The choice of the number of PCs to consider is empirically made by considering the results of previous experiments carried out in similar contexts. Another issue is related to the multicollinearity that could affect parameters estimate. If we extracted risk factors on a singular basis, this would not be a problem as the PCs, for construction, are independent of each other. Instead, in our framework, multicollinearity could seriously harm strategy performances. For example, let us consider the first PC in the daily and monthly settings. As shown in the example in Fig. 2, they are almost the same.\nWe handle this problem by applying a multicollinearity filter, i.e., we add the risk factors to the set F in three stages with a threshold rule. In the first stage, the daily PCs, which are referred to as PCsd \u00bc fD1; :::;DNdg \u00bc Fd, are added without any restriction. In the second stage, the monthly PCs are computed PCsm \u00bc fM1; :::;MNmg. For each one, a score is obtained as the maximum absolute correlation of the monthly component with the daily ones, that is scoreM \u00bc maxD2PCsd j corr\u00f0M;D\u00de j; 8M 2 PCsm. In the third stage, the monthly PCs whose score is lower than a fixed threshold th are added to F, so Fm \u00bc fM 2 PCsm s:t: scoreM\\thg. Finally, F is defined as the union Fd [ Fm. In this way, we can avoid multicollinearity. The generation and selection of the risk factors are definitely described by Algorithm 1. Finally, observe that the number of PCs which survive the multicollinearity filter could vary according to time. This should ensure our strategy has the necessary flexibility to catch temporal evolution in the covariance of the assets. Anyway, we do not notice a significant variation in the risk factors set through our experiments.\nFig. 1 The extraction of the risk factors. Two different granularities are considered: daily and monthly. The raw data are fed into the PCA, which extracts the weights of the Principal Components (PCs). In particular, two sets of weights are obtained: one from the PCA applied to daily data and the other from the PCA applied to monthly data.\nThen, the daily and monthly PCs are obtained by multiplying these weights with the daily dataset. Finally, the Risk Factors set is constructed by considering all the daily PCs and the monthly PCs that pass the Multicollinearity Filter"
        },
        {
            "heading": "3.3 Clustering approach",
            "text": "Once extracted the risk factors in F as described in the previous Subsection, A-Lasso is applied in order to obtain, for each stock j 2 J , a subset Aj F made up of the more relevant risk factors, i.e., those which have the most significant impact on j. As already pointed out, by applying the A-Lasso, we obtain an estimate for each coefficient in\nEq. 1. Furthermore, thanks to the l1 regularization, there are some coefficients set to zero. In this way, we can work with only the most important risk factors, which are contained in the subset Aj defined as Aj \u00bc fi 2 f1; :::;Ngs:t:bji 6\u00bc 0g where b j i is the coefficient estimate provided by A-Lasso.\nTo achieve our goal, the two hyperparameters of A-Lasso, k; s, have to be set. This task is done by exploiting Grid Search and 5 folds CV. In particular, we set five folds whose length is equal to that of the investment, and we search the hyperparameters couple that minimizes the average mean square error (mse) among the folds. Furthermore, we consider only hyperparameter combinations that save between 2 and 4 PCs. In this way, we avoid too strong regularization (number of PCs 2) and too complex models (so we set the number of PCs 4). This stage is the most computationally expensive of the whole procedure. In fact, as a Grid Search is executed for each asset, more than 10000 CVs are performed. This highlight the needing for a fast feature selection technique. However, some tricks can be used to reduce the time, as discussed in the Conclusion.\nFinally, the clusters are constructed by grouping stocks with similar expositions to the same risk factors. More\nformally, once the sets Aj have been determined, we define the equivalence relationship in the stocks universe J in this way:\nj i () Aj \u00bc Ai 8i; j 2 J \u00f04\u00de\nThen, the clusters are defined as the equivalence classes associated with , that is, the clusters set C is the quotient set J = . Algorithm 2 describes the clustering methodology. The computational time of this algorithm is negligible compared to that of the total procedure."
        },
        {
            "heading": "3.4 Statistical arbitrage strategy",
            "text": "Once the clusters are obtained, a statistical arbitrage strategy, specifically a market-neutral portfolio, is constructed within each cluster. The starting point is the equation representing the stocks in a fixed cluster. Let us consider C 2 C, let c be the cardinality of C, and letAC and\na, respectively, be the set of risk factors relevant for the stocks in C and its cardinality. We can represent each stock in C by using Eq. 5: rjt \u00bc aj \u00fe X i2AC bjiFi;t \u00fe ejt 8j 2 C; 8t 2 T \u00f05\u00de\nThe first issue related to Eq. 5 is the estimate of the coefficients. We accomplish this task by applying the Pooled Ordinary Least Squares (OLS) Regression. That is, we separately estimate aj and the vector bj in each time window, and then we average the single estimates. In more detail, we split the data into Time Windows (TW) whose length is coherent with the investment temporal horizon. Then, for each time window tw 2 TW , we obtain an estimate of the parameters ajtw and b j tw by applying the OLS. Finally, we average the estimates in each window to obtain the final parameters, as in 6. aj \u00bc 1j TW j X\ntw2TW ajtw b\nj \u00bc 1j TW j X\ntw2TW bjtw \u00f06\u00de\nAfter determining the model coefficients, we aim to delete the exposition on each risk factor. In other words, we want to create a portfolio such that the weighted sum of the coefficients associated with the risk factor Fi is zero for each i 2 AC. As mentioned in the Introduction, we define a portfolio as a linear combination of stock returns where each component of the weights vector / \u00bc \u00f0/1; :::;/c\u00de represents the exposition on the related stock. Furthermore, as we allow for both long and short positions and we require the invested amount to not exceed the total capital, we impose the l1 norm of the weights to be 1. Accordingly, we can represent a portfolio made up of the stocks in C as: Portt \u00bc X j2C /jrjt \u00bc X j2C /jaj \u00fe X i2AC X j2C /jbji Fi;t\n\u00fe X j2C /jejt 8t 2 T \u00f07\u00de\nObserve that as there is a one-to-one correspondence between admissible weights vectors and portfolios, we sometimes overlap the two concepts in the following.\nIf we impose the market neutral condition, then we require the terms into the curved brackets to be zero, so we have a homogeneous linear system of a equations in the c variables /1; :::;/c. Furthermore, with the l1 condition, we obtain an optimization problem with both linear (8) and nonlinear (9) constraints:X j2C /jbji \u00bc0 8i 2 AC \u00f08\u00de\nX j2C j /j j\u00bc1 \u00f09\u00de\nAssuming that there are at least a\u00fe 1 stocks in C, we can construct a market-neutral portfolio. We indicate with PC the set of all the weights vectors such that both 8 and 9 are satisfied. For a generic portfolio in PC, the return at time t can be reduced as the sum of aPort and ePortt : Portt \u00bc X j2C /jrjt \u00bc X j2C /jaj \u00fe X j2C /jejt \u00bc aPort \u00fe ePortt 8t 2 T\n\u00f010\u00de\nwhere aPort is a constant and ePortt is the sum of c Gaussian random variables. As already discussed above, several works in the recent literature have assessed the utility of the idiosyncratic risks in constructing an investment strategy. In other words, it has been shown that there are hidden patterns in the residual sum that can improve the quality and the performance of a strategy for portfolio construction. So, we consider them by searching in PC the portfolio PC that optimizes a specific criterion.\nAs there are infinite portfolios that satisfy the constraints, which are both linear and nonlinear, we apply a nonlinear optimization algorithm to find the optimal one. In particular, the Sequential Least SQuares Programming (SLSQP) is used (see [40] and [41] for further references). The choice for this algorithm is due to its global convergence property [42] and super-linear speed. It works by repeatedly splitting the main problem into subproblems solved by linearizing the constraints. Regarding the objective function, which is the criterion used to choose the portfolios to invest in, we carry out experiments by trying to minimize the variance, that is, PC \u00bc argmin P2PCVar\u00f0P\u00de. Once a portfolio for each cluster is obtained, we apply the same criterion to select the three optimal ones. Finally, we invest in them. We choose three portfolios and not just one to increase the diversification effect and to reduce the investment risk by making the strategy more robust. The entire investment strategy is reported in Algorithm 3.\nThe computational time is contained, in the order of a\nfew seconds for the whole Algorithm 3."
        },
        {
            "heading": "4 Experimental results",
            "text": "This Section is concerned with the experimental stage. Firstly, we describe the datasets used and the preprocessing stage. Then, we describe the evaluation strategy used for the comparison and show and discuss the experimental results obtained in the various markets."
        },
        {
            "heading": "4.1 The dataset",
            "text": "We assess our proposal in six different stock markets in the experimental stage. In particular, the datasets cover both developed and emerging markets. In this way, we are able to analyze our proposal performances in different situations. The experiments are carried out individually, without any interactions between each other. The period considered is the same for all the experiments: from 2011-12-21 to 2021-12-20. The employed datasets are:\n\u2022 Italian Stock Market: the dataset contains stocks from the FTSE Italia All Share. The data are publicly\navailable on the website of Il Sole 24 Ore, an Italian newspaper. The exact link is provided at the end of this article. \u2022 German Stock Market: the dataset is referred to the stocks belonging to the index Classic All Share -\nGerman and whose time series are available on Investing.com, one of the most significant sources for public financial data. The exact link can be found at the end of this article. \u2022 American Stock Market: the dataset is created starting from the stocks belonging to the index S&P100. The data are provided by Investing.com, see the Data &\nCode Availability Statement.\n\u2022 Japanese Stock Market: the stocks belonging to the Topix 100 index are grouped in this dataset. The data\nprovider is Investing.com and further information can be found at the end of this work. \u2022 Brazilian Stock Market: the dataset collects stocks from the Bovespa index. For the data link, see the Data\n& Code Availability Statement.\n\u2022 Indian Stock Market: the dataset is related to the Nifty 100 index. The data are provided by Investing.com, as\nstated at the end of the paper.\nAll the datasets considered can be viewed as matrices whose rows represent the time axis (i.e., the observations) and whose columns are the stocks considered. The preprocessing stage is done through multiple steps. Firstly, from the prices dataset, we calculate the returns one. After that the rows representing the weekends and holidays are removed from the dataset, as for the columns corresponding to stocks with poor data, i.e., full of missing values. Moreover, in the case of the Italian dataset, some initial rows are deleted as many missing values for several stocks occurred in the first observations. Then, the remaining missing values are imputed with 0 (that is, no price change has occurred these days). Finally, in the train set and for the computation of the investment strategy, the values are standardized columns by columns. That is, if r\u0302j is the row return corresponding to stock j, we consider rj \u00bc\n1ffiffiffiffiffiffiffiffiffiffi Var\u00f0r\u0302j\u00de p \u00f0r\u0302j Mean\u00f0r\u0302j\u00de\u00de in place of r\u0302j. The data used for the comparison are not handled in any way.\nAs for the number of PCs we consider, it is empirically determined by looking at the results obtained in similar previous experiments. The same is true also for the threshold th, which is chosen in such a way to preserve many monthly components and avoid multicollinearity. In this direction, we set th \u00bc 0:5 for all the experiments. The condition number gives output values lower than 10 (specifically, values lower than 5 in all the experiments), which means negligible multicollinearity among variables. Table 2 summarizes the final datasets we work with after the preprocessing stage and the number of PCs considered, which is determined as a function of the overall number of stocks. Furthermore, the condition number results are also shown (in particular, for each dataset, the highest value obtained among all the folds is reported). Finally, also the number of resulting PCs is displayed. As already stated in Sect. 3.2, within the same dataset, the considered risk factors could vary as time progress, so our proposal can follow covariance shifts. However, the risk factors set exhibits certain robustness among the 12 folds we consider for CV in that it does not show huge variations. We can interpret this result as no considerable changes have occurred in such a short time. In other words, significant variations in the patterns among assets are noticeable at bigger time intervals."
        },
        {
            "heading": "4.2 Results and discussion",
            "text": "Before describing the results obtained, we briefly show some details about the evaluation of the proposal and the comparison with other benchmarks. Firstly, we describe the performance measures used for the comparison. In the following, we indicate the value of the portfolio to analyze with P \u00bc fPtgt2Te0 and its returns with Pr \u00bc fPrtgt2Te0 , where Te0 \u00bc f1; :::;Ng is the test set. The performance measures are:\n\u2022 Percentage Profit P % It is the percentage profit obtained by the strategy. It can be defined as the ratio\nP% \u00bc PNP1 1. In comparing different strategies, we prefer high values. \u2022 Max Percentage Drawdown (MD %) It is the maximum percentage loss the portfolio suffered. For-\nmally, it can be viewed as MD% \u00bc 1 mint\\s2Te\u00f0PsPt\u00de. In comparing different strategies, we prefer low values. \u2022 Recovery Factor (RF) It is a proxy of the capability of the portfolio to recover losses. It can be defined as the\nratio between the final profit and the max suffered loss. Formally, RF \u00bc ProfitLoss with Profit \u00bc PN P1 and Loss \u00bc maxt\\s2Te\u00f0Pt Ps\u00de. In comparing different strategies, we prefer high values. \u2022 Profit Factor (PF) It is the ratio between the sum of the profits and the losses computed daily. That is, PF \u00bc ProfitsSum LossesSum with ProfitsSum \u00bc PN t\u00bc2 maxfPt Pt 1; 0g\nand LossesSum \u00bc PN\nt\u00bc2 maxfPt 1 Pt; 0g. In comparing different strategies, we prefer a high value. \u2022 Sharpe Ratio (ShR) It is a measure of how the risk is rewarded in terms of extra gain. It is formally defined as\nthe ratio between the difference of the expected return and the risk-free rate, and the standard deviation of the portfolio returns, which is used as a proxy for the riskiness, i.e., ShR \u00bc Mean\u00f0Pr\u00de irffiffiffiffiffiffiffiffiffiffiffi Var\u00f0Pr\u00de p where ir is the risk-free rate. Observe that, in the computation of ShR, we approximate ir with zero. This assumption is not relevant to the comparison as ShR is a monotonic function with respect to ir. In comparing different strategies, we prefer a high value. \u2022 Sortino Ratio (SoR) It is a measure of the reward for the risk, as ShR. The only difference is that, as a\nriskiness proxy, the downside deviation, i.e., the standard deviation referred only to negative returns, is considered. So, we can write SoR \u00bc Mean\u00f0Pr\u00de irffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi Down Var\u00f0Pr\u00de p . The\nTable 2 Summary statistics of the datasets after the preprocessing stage. For each of the considered markets, we report: the number of stocks in the universe J (Num Stocks); the number of observations in the whole dataset (Num Obs.); the number of observations in the first train set and the first test set (Tr1=Te0); the number of PCs\nextracted on both daily and monthly basis (Num PCs); the number of total risk factors Fi, obtained by merging the daily and monthly PCs as described above (Num Fi); the highest result (with respect to the 12 folds) obtained in the condition number (Cond. Num.)\nItalian German American Japanese Brazilian Indian\nNum Stocks 151 65 86 85 63 71\nNum Obs. 2423 2506 2515 2444 2471 2465\nTr1 / Te0 2168/255 2252/254 2263/252 2199/245 2225/246 2218/247 Num PCs 7 5 6 6 5 6\nNum Fi 11 8 7 7 7 8 Cond. Num. 4.359 2.913 2.278 2.653 3.802 3.683\nassumption regarding ir is the same as in ShR. In comparing different strategies, we prefer high values.\nAs already stated in the Introduction, we remind that our aim is the construction of a strategy that is robust through time. To achieve this task, we perform the CV to evaluate some of the most commonly used metrics for investment strategies. In more detail, we apply a block W-fold CV, so each fold preserves the temporal dimension. So, the dataset is split into train and test sets, which are indicated with Tr1 and Te0, respectively. The test set is further split into W consecutive folds according to the temporal dimension. So,\nwhere is the disjoint union of sets.\nThen, for the first iteration, we consider Tr1 as the train set and Te1 as the test set. For the \u00f0w\u00fe 1\u00de-th iteration, we exploit as the train set and Tew\u00fe1 as the test set. Figure 3 graphically explains the procedure.\nAfter applying the CV, we have a vector of results for each metric, one for each fold. Then, the mean and the standard deviation of these vectors are computed. Finally, the mean minus the std is considered the proxy of the lower bound confidence interval, which is a proxy of the worstcase scenario (wcs). By evaluating this quantity, we expect to assess the robustness of the strategy. That is, it not only has to be profitable, i.e., with a high mean, but it also has to be stable with respect to the time, i.e., its std among the fold should be as low as possible. Finally, note that, only in the case of MD%, as for this metric a lower value is better, we consider as target measure for the wcs, the upper bound, i.e., the sum between mean and std.\nRegarding the number and the length of the folds used for the comparison, as we extract monthly risk factors as long-term ones, the investment strategy has a monthly time horizon. So, the length of each fold is one month. Furthermore, we use one year of data as the test set. So, there\nareW \u00bc 12 folds, and each one is made up of one month of data.\nThe last detail to be clarified is the benchmarks used for the comparison. As we work in a context where both long and short positions are allowed, we use as benchmarks the minimum variance and the mean-var portfolios. These portfolios are constructed by looking at the historical data. A vector of weights is optimized through the SLSQP by means of minimum variance or maximal ratio between mean and variance, respectively. These weights form the portfolios in which we invest. Moreover, also the Exponential Gradient is used, as done in [32, 43].\nNow, we show the results obtained in the experimental stage. For each fold in the CV, we simulate the performance obtained by a portfolio with an initial amount of wealth equal to 1000. Table 3 shows the results obtained in the Italian, German and American markets. Instead, the results for the Japanese, Brazilian and Indian stock markets are reported in Table 4.\nFor each table, we report the mean, the variance, and the wcs. Furthermore, the results obtained by both our strategy (Port), the minimum variance portfolio (MinVar) and the mean-variance portfolio (M-V) are reported for comparison. Finally, the best result in each field is reported in bold, and the second one is underlined.\nFor a visual inspection, we also report the plots of the strategies in the evaluation stage. In particular, Figures from 4 to 9 show the results in each market.\nAs already mentioned above, all the strategies in each fold are considered to start with an initial capital of 1000. The proposed strategy is reported in blue, the minimum variance benchmark is represented by the green line, and the mean-variance portfolio is shown by the red line. Furthermore, the black dotted line represents the value 1000, which corresponds to an overall return of 0.\nAs the results show, our portfolio optimization strategy seems promising. In fact, despite the mean value often is not the best, the wcs, which is our target, is very often the optimal one, with the only exception of the American dataset. This happens because the std is almost ever the lowest or the second-lowest, in all the considered datasets.\nIn particular, it can be interesting to compare the American and Brazilian Stock Markets from one side, and the others on the other side. In fact, in the second case, the mean across the folds is not very exciting. Indeed, the benchmark strategies obtain better results. However, the strategy shows its robustness by obtaining a low variance. In the Italian and Japanese cases, this allows it to overcome the benchmarks when evaluating the worst-case scenario in all performance measures except for the Profit and the Max Drawdown, where the results obtained are still far from the best. In the Indian dataset, the wcs performances are significantly better than benchmark approaches. The only\nand tested on Tew\u00fe1\nTa bl e 3\nT h e re su lt s w e o b ta in ed\nd u ri n g th e ex p er im\nen ta l st ag e in\nth e It al ia n , G er m an , an d A m er ic an\nm ar k et s. O u r p ro p o sa l is re p o rt ed\nu n d er\nth e co lu m n s P o rt . T h e b en ch m ar k s ar e M in V a r,\nM -V\n, an d E x p G ra d fo r th e p o rt fo li o w it h m in im\nu m\nv ar ia n ce , th e p o rt fo li o w h ic h m ax im\niz es\nth e ra ti o\nm ea n va r , an d th e E x p o n en ti al\nG ra d ie n t, re sp ec ti v el y . F u rt h er m o re , al so\nth e E x p o n en ti al\nG ra d ie n t st ra te g y E x p G ra d is co n si d er ed . W e re p o rt b o th\nth e m ea n (m\nea n ) an d th e st an d ar d d ev ia ti o n (s td ) o f th e p er fo rm\nan ce\nin d ic at o rs\nam o n g th e fo ld s. F u rt h er m o re , w e al so\nre p o rt th e\nw o rs tca se\nsc en ar io\n(w cs ) in\nth e co n fi d en ce\nin te rv al . S u ch\nin te rv al is d efi n ed\nas th e m ea n m in u s th e st an d ar d d ev ia ti o n fo r al l th e p er fo rm\nan ce\nm ea su re s co n si d er ed\nex ce p t fo r M D % , w h er e it\nis th e su m\no f th em\n. T h e b et te r re su lt fo r ea ch\nm et ri c is re p o rt ed\nin b o ld , an d th e se co n d b es t is u n d er li n ed\nIt al ia n P o rt\nM in V ar\nM -V\nE x p G ra d\nM ea n\nS td\nW cs\nM ea n\nS td\nW cs\nM ea n\nS td\nW cs\nM ea n\nS td\nW cs\nP %\n0 .2 5 5\n0 .9 4 3\n0 .6 8 8\n0 .6 1 7\n0 .9 5 7\n0 .3 4\n0 .2 7 1\n1 .1 6 8\n0 .8 9 7\n1 .7 1 8\n5 .7 7 3\n4 .0 5 5\nM D %\n1 .0 0 9\n0 .5 5 6\n1 .5 6 5\n0 .6 5 5\n0 .4 0 2\n1 .0 5 7\n1 .0 7 8\n0 .5 5 2\n1 .6 3 1\n3 .6 7 2\n2 .0 5 0\n5 .7 2 2\nR F\n0 .8 8 9\n1 .6 8 5\n0 .7 9 7\n2 .7 5 8\n4 .9 5 6\n2 .1 9 8\n1 .3 4 1\n2 .4 9 4\n1 .1 5 4\n1 .2 5 4\n2 .4 0 4\n1 .1 5 1\nP F\n1 .2 9 1\n0 .5 9 4\n0 .6 9 7\n2 .0 8 6\n2 .2 5 8\n0 .1 7 2\n1 .6 5 8\n1 .2 4 7\n0 .4 1 1\n1 .6 4 3\n1 .4 7 7\n0 .1 6 5\nS h R\n1 .1 5 2\n3 .0 1 1\n1 .8 5 9\n2 .9 6 8\n4 .8 7 4\n1 .9 0 6\n1 .8 0 1\n4 .5 6 3\n2 .7 6 2\n3 .4 8 5\n7 .7 4 2\n4 .2 5 7\nS o R\n2 .3 8 8\n5 .2 0 7\n2 .8 1 8\n5 .8 7 7\n8 .8 1 3\n2 .9 3 5\n3 .6 6 1\n8 .7 2 5\n5 .0 6 4\n6 .1 8 3\n1 2 .3 4 4\n6 .1 6 1\nG er m an P o rt\nM in V ar\nM -V\nE x p G ra d\nM ea n\nS td\nW cs\nM ea n\nS td\nW cs\nM ea n\nS td\nW cs\nM ea n\nS td\nW cs\nP %\n0 .7 1 3\n1 .2 6 4\n0 .5 5 1\n0 .1 8 2\n1 .1 6 1\n0 .9 7 8\n0 .5 0 1\n0 .8 1 8\n1 .3 1 9\n1 .1 0 3\n5 .5 6 9\n6 .6 7 2\nM D %\n1 .2 2 6\n0 .5 4\n1 .7 6 5\n0 .9 7 8\n0 .4 8 1\n1 .4 5 8\n1 .3 0 3\n0 .6 1 5\n1 .9 1 8\n5 .3 8 9\n3 .0 2 6\n8 .4 1 5\nR F\n1 .1 0 9\n1 .5 1 8\n0 .4 0 9\n1 .0 6 1\n3 .3 9 7\n2 .3 3 6\n0 .2 2 6\n0 .6 7 3\n0 .8 9 9\n0 .2 1 6\n1 .1 6 8\n0 .9 5 2\nP F\n1 .5 2\n0 .6 4 9\n0 .8 7 1\n1 .4 8\n1 .5 3\n0 .0 5\n0 .8 5 2\n0 .3 4 6\n0 .5 0 6\n1 .0 5 1\n0 .5 8 0\n0 .4 7 1\nS h R\n2 .0 1\n2 .7 8 2\n0 .7 7 3\n1 .0 3\n4 .7 7 2\n3 .7 4 2\n1 .1 1 1\n2 .0 8 8\n3 .2\n0 .3 8 1\n2 .9 9 3\n2 .6 1 2\nS o R\n2 .7 2 2\n4 .8 3 8\n2 .1 1 6\n2 .1 4 9\n7 .4 5 2\n5 .3 0 3\n1 .2 2 2\n2 .9 0 9\n4 .1 3 2\n1 .1 9 6\n4 .9 8 8\n3 .7 9 2\nA m er ic an P o rt\nM in V ar\nM -V\nE x p G ra d\nM ea n\nS td\nW cs\nM ea n\nS td\nW cs\nM ea n\nS td\nW cs\nM ea n\nS td\nW cs\nP %\n0 .0 4 1\n0 .8 4 1\n0 .8\n0 .1 5 5\n1 .1 4 6\n0 .9 9 1\n0 .1 5 5\n0 .9 4 5\n0 .7 9\n0 .7 5 8\n8 .6 5 1\n7 .8 9 3\nM D %\n0 .9 8\n0 .3 7 6\n1 .3 5 6\n1 .1 1 1\n0 .5 8 4\n1 .6 9 5\n0 .9 8 9\n0 .5 5 6\n1 .5 4 5\n5 .6 7 8\n2 .8 3 5\n8 .5 1 2\nR F\n0 .3 0 1\n1 .1 3 7\n0 .8 3 6\n0 .6 7 9\n1 .5 6 9\n0 .8 9\n1 .3 0 4\n3 .0 1 5\n1 .7 1 1\n0 .5 9 6\n1 .6 6 8\n1 .0 7 2\nP F\n1 .0 9 6\n0 .6 0 6\n0 .4 9\n1 .2 6 8\n0 .7 2 7\n0 .5 4 1\n1 .5 1 1\n1 .2 8\n0 .2 3 1\n1 .2 2 2\n0 .8 1 8\n0 .4 0 4\nS h R\n0 .2 7 6\n3 .3 7 9\n3 .6 5 5\n0 .7 7 9\n3 .4 8 8\n2 .7 1\n1 .2 5 9\n4 .2 8 3\n3 .0 2 3\n1 .5 6 5\n5 .0 8 7\n3 .5 2 3\nS o R\n0 .2 3\n5 .0 9 7\n5 .3 2 7\n1 .3 3 2\n5 .6 0 2\n4 .2 7\n1 .7 9 7\n6 .5 5 4\n4 .7 5 7\n7 .7 0 0\n2 0 .4 9 2\n1 2 .7 9 2\nTa bl e 4\nT h e re su lt s w e o b ta in ed\nin th e Ja p an es e, B ra zi li an , an d In d ia n st o ck\nm ar k et s. T h er e ar e fo u r co lu m n s re p re se n ti n g o u r m et h o d o lo g y (P o rt ), th e m in im\nu m\nv ar ia n ce\n(M in V a r)\np o rt fo li o ,\nth e m ea n -v ar\n(M -V\n) p o rt fo li o , an d th e E x p o n en ti al\nG ra d ie n t st ra te g y (E\nx p G ra d ). E ac h co lu m n is sp li t in to\nth re e su b -c o lu m n s re p re se n ti n g th e m ea n (m\nea n ), th e st an d ar d d ev ia ti o n (s td ) an d\nth e w o rs tca se\nsc en ar io\n(w cs ) in\nth e co n fi d en ce\nin te rv al . W e h ig h li g h t th e b es t re su lt s an d th e se co n d o n e b y re p o rt in g th em\nin b o ld\nan d u n d er li n ed , re sp ec ti v el y\nJa p an es e P o rt\nM in V ar\nM -V\nE x p G ra d\nM ea n\nS td\nW cs\nM ea n\nS td\nW cs\nM ea n\nS td\nW cs\nM ea n\nS td\nW cs\nP %\n0 .0 0 1\n0 .7 6 4\n0 .7 6 2\n0 .1 6 6\n0 .7 0 4\n0 .5 3 9\n0 .0 3 6\n1 .7 6 6\n1 .7 3\n1 .9 7 2\n4 .0 9 5\n2 .1 2 3\nM D %\n1 .1 4 4\n0 .6 2\n1 .7 6 3\n0 .7 8 7\n0 .2 6 3\n1 .0 5\n1 .4 0 2\n1 .0 5 9\n2 .4 6 2\n3 .7 6 7\n1 .8 7 8\n5 .6 4 5\nR F\n0 .3 5 3\n0 .8 4 9\n0 .4 9 6\n0 .4 4 6\n1 .1 2 3\n0 .6 7 7\n1 .6 7 1\n3 .7 0 3\n2 .0 3 1\n1 .3 0 8\n2 .4 9 8\n1 .1 9\nP F\n1 .0 9 3\n0 .3 7 8\n0 .7 1 5\n1 .2 7\n0 .6 1 8\n0 .6 5 2\n1 .8 0 3\n1 .9 3 9\n0 .1 3 7\n1 .6 2 5\n1 .2 1 3\n0 .4 1 3\nS h R\n0 .3 1 3\n2 .3 4 3\n2 .0 3 1\n0 .8 0 8\n3 .1 5 5\n2 .3 4 7\n1 .6 6 2\n5 .6 1 7\n3 .9 5 6\n3 .4 2\n6 .5 2 4\n3 .1 0 4\nS o R\n0 .7 8 6\n4 .0 3 6\n3 .2 5\n1 .3 6 8\n5 .5 9 9\n4 .2 3\n2 .7 7 3\n9 .3 1 1\n6 .5 3 8\n5 .7 6 5\n1 1 .0 4 5\n5 .2 8\nB ra zi li an P o rt\nM in V ar\nM -V\nE x p G ra d\nM ea n\nS td\nW cs\nM ea n\nS td\nW cs\nM ea n\nS td\nW cs\nM ea n\nS td\nW cs\nP %\n0 .9 6 4\n1 .1 9 2\n0 .2 2 9\n0 .6 9 5\n1 .0 5 9\n0 .3 6 4\n0 .1 0 6\n1 .4 6 7\n1 .3 6 1\n0 .6 8 2\n4 .6 7 7\n3 .9 9 5\nM D %\n1 .3 4 3\n0 .5 0 2\n1 .8 4 5\n0 .9 6 5\n0 .5 4 1\n1 .5 0 5\n1 .3 2 3\n0 .6 4 9\n1 .9 7 2\n5 .2 5 6\n2 .4 9 2\n7 .7 4 8\nR F\n0 .8 9\n0 .9 2 9\n0 .0 3 9\n1 .1 6 3\n1 .3 2 9\n0 .1 6 6\n1 .0 8 1\n2 .7 0 7\n1 .6 2 6\n0 .6 6 4\n1 .6 4 9\n0 .9 8 5\nP F\n1 .4 9 1\n0 .5 1 8\n0 .9 7 3\n1 .4 6 2\n0 .5 9\n0 .8 7 2\n1 .2 5 3\n0 .8 7 5\n0 .3 7 8\n1 .3 6 2\n1 .0 1 5\n0 .3 4 7\nS h R\n2 .1 9 6\n2 .6 2 3\n0 .4 2 7\n1 .9 6 8\n2 .7 5 8\n0 .7 9\n0 .7 0 3\n4 .0 5 9\n3 .3 5 5\n1 .9 3 3\n5 .3 2 1\n3 .3 8 8\nS o R\n3 .6\n4 .6 1 9\n1 .0 1 8\n3 .2 3 1\n5 .2 1 1\n1 .9 8\n1 .9 8 7\n7 .6 5 1\n5 .6 6 4\n2 .3 2 2\n7 .4 4 6\n5 .1 2 4\nIn d ia n P o rt\nM in V ar\nM -V\nE x p G ra d\nM ea n\nS td\nW cs\nM ea n\nS td\nW cs\nM ea n\nS td\nW cs\nM ea n\nS td\nW cs\nP %\n0 .4 1 1\n1 .2 2\n0 .8 0 9\n0 .1 2\n1 .3 5 9\n1 .2 3 8\n0 .4 6 8\n1 .5 8 7\n1 .1 1 9\n2 .9 0 3\n5 .2 2 1\n2 .3 1 8\nM D %\n1 .3 3 4\n0 .4 8\n1 .8 1 4\n1 .2 7 8\n0 .6 1 3\n1 .8 9 1\n1 .1 3 3\n0 .6 7 4\n1 .8 0 7\n3 .9 1 2\n1 .6 5 6\n5 .5 6 8\nR F\n0 .5 3 6\n1 .2 2 1\n0 .6 8 4\n0 .9 1 2\n2 .2 5 4\n1 .3 4 2\n1 .2 8 6\n2 .1 7 8\n0 .8 9 2\n1 .0 5 5\n1 .8 9 4\n0 .8 3 9\nP F\n1 .2 0 8\n0 .4 6 6\n0 .7 4 2\n1 .5 4 5\n1 .4 0 3\n0 .1 4 1\n1 .5 9 6\n1 .0 1 6\n0 .5 8\n1 .7 7 9\n1 .3 6 6\n0 .4 1 3\nS h R\n0 .8 9 6\n2 .2 7\n1 .3 7 4\n1 .1 2 7\n4 .7 0 3\n3 .5 7 6\n1 .8 3 1\n4 .2 1 4\n2 .3 8 4\n4 .5 6 1\n8 .1 9 4\n3 .6 3 3\nS o R\n1 .8 8 2\n4 .3 1 6\n2 .4 3 5\n1 .5 1 7\n6 .8 9 1\n5 .3 7 3\n3 .2 1 8\n7 .3 7 3\n4 .1 5 5\n6 .1 6 5\n1 0 .4 9 1\n4 .3 2 6\nexception is Max Drawdown, where the difference between our proposal and the mean-variance portfolio is very low, less than 0.007%. Instead, in the American example, the proposal fails to overcome the competitors. However, it obtains discrete results, especially in the Max Drawdown and the Recovery Factors, where it is the best, and in the\nProfit and Profit Factor, where it is very close to the best result. Finally, in the German and Brazilian datasets, the strategy achieves the best mean in almost all the considered metrics, and also, the std is relatively low. This means the worst-case scenario overcomes that obtained by competitors in all the cases except that Max Drawdown."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this work, a framework for statistical arbitrage is discussed. We proposed a cluster-based multi-step data-driven strategy that considers risk factors related to different temporal horizons. Our proposal is contextualized in the\nliterature, and its performance is repeatedly assessed through several experiments on several stock markets. We find that this kind of strategy seems to be quite robust and profitable in various stock markets belonging to both emerging and developed countries. Furthermore, this finding holds also when comparing our proposal with other benchmark strategies. In fact, the comparison shows that\nour methodology obtains almost every good performance, superior to those obtained by the benchmarks.\nTo summarize, we can analyze more in detail the proposed framework by highlighting its strengths and weakness, thus providing possible directions for further studies. Firstly, the assumptions we made, although classical in the financial literature, can represent an obstacle in applying\nthe proposal in a real-world scenario. In fact, there is an open debate on their reliability ([44, 45]). So, it can be worth investigating what happens when some of the assumptions are relaxed.\nThen, regarding the feature extraction, the proposed factor model, and the feature selection, we have chosen to stay in the linear case. In fact, a previous empirical study\nhas shown the reliability of such a model and the complexity and the computational times are lower than in nonlinear environments. However, a careful study of the problem through nonlinear methods could show hidden patterns that can improve the performance of our methodology. Moreover, the patterns among asset time series could change accordingly to the considered time granularity. In other words, it is also noteworthy to investigate hybrid approaches where different extraction methods are used in different granularities.\nRegarding hyperparameter optimization, it has been observed it is the most expensive part of the whole framework. A trick to reduce the number of iterations and alleviate its computational cost can be the introduction of the Randomized Grid Search. Another idea is the shrinkage of the searching space to a narrow boundary surrounding the solution in the previous period. However, they should be further investigated in the future. As for the clustering strategy, it shows both pros and cons. For example, it does not need to explicitly define a distance between time series, which can be a difficult task. In contrast, some of the clusters show little significance in that they are made up of a small number of stocks, so they are unusable for the investment strategy. Furthermore, the number of PCs to consider is currently empirically determined, which could lead to a bias. Future works could try to fix these disadvantages. For example, it can be worth investigating the optimal number of PCs to consider by analyzing the tradeoff between representation accuracy and sensitivity to noise.\nRegarding the investment strategy and the idiosyncratic risks, other experiments have been carried out to find the optimal portfolio by means of the Sharpe ratio instead of minimum variance. However, these experiments have shown no profitability. In more detail, it seems that a meanreverting process in such a case is more convenient for describing the price dynamic. In the future, it could be helpful to accurately investigate what type of dynamic (mean-reverting rather than momentum) better fits the particular context under evaluation.\nFunding Open access funding provided by Scuola Normale Superiore within the CRUI-CARE Agreement.\nData availability The data used in this paper for assessing the proposed methodology is publicly available. In particular, data related to the Italian Stock Market have been downloaded from https://mercati. ilsole24ore.com/azioni/borsa-italiana/ftse-all-share, while data relating to German, American, Japanese, Brazilian, and Indian Stock Markets have been downloaded from the following links: https:// www.investing.com/indices/classic-all-share, https://www.investing. com/indices/s-p-100, https://www.investing.com/indices/topix-100, https://www.investing.com/indices/bovespa, and https://www.invest ing.com/indices/cnx-100 The codes used for the construction of the\nstrategy and the comparison are available at the GitHub repository https://github.com/fgt996/Clustering4Investment.\nDeclarations\nConflict of interest No potential conflict of interest was reported by the authors.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article\u2019s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article\u2019s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons. org/licenses/by/4.0/."
        }
    ],
    "title": "Statistical arbitrage in the stock markets by the means of multiple time horizons clustering",
    "year": 2023
}