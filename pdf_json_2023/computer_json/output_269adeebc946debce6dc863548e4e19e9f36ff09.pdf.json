{
    "abstractText": "Fringe projection profilometry (FPP) and digital image correlation (DIC) are widely applied in three-dimensional (3D) measurements. The combination of DIC and FPP can effectively overcome their respective shortcomings. However, the speckle on the surface of an object seriously affects the quality and modulation of fringe images captured by cameras, which will lead to non-negligible errors in the measurement results. In this paper, we propose a fringe image extraction method based on deep learning technology, which transforms speckle-embedded fringe images into speckle-free fringe images. The principle of the proposed method, 3D coordinate calculation, and deformation measurements are introduced. Compared with the traditional 3D-DIC method, the experimental results show that this method is effective and precise.",
    "authors": [
        {
            "affiliations": [],
            "name": "Chuang Zhang"
        },
        {
            "affiliations": [],
            "name": "Cong Liu"
        },
        {
            "affiliations": [],
            "name": "Zhihong Xu"
        }
    ],
    "id": "SP:ba1cc6036c2c556d6d73c0edbdcdb0a7c5bd5566",
    "references": [
        {
            "authors": [
                "Z. Gao",
                "T. Kato",
                "H. Takahashi",
                "A. Doi"
            ],
            "title": "3D Measurement and Feature Extraction for Metal Nuts",
            "year": 2022
        },
        {
            "authors": [
                "D. Midgett",
                "S. Thorn",
                "S. Ahn",
                "S. Uman",
                "R. Avendano",
                "I. Melvinsdottir",
                "T. Lysyy",
                "J. Kim",
                "J. Duncan",
                "J Humphrey"
            ],
            "title": "CineCT platform for in vivo and ex vivo measurement of 3D high resolution Lagrangian strains in the left ventricle following myocardial infarction and intramyocardial delivery of theranostic hydrogel",
            "venue": "J. Mol. Cell. Cardiol",
            "year": 2022
        },
        {
            "authors": [
                "J. Herr\u00e1ez",
                "J.C. Mart\u00ednez",
                "E. Coll",
                "M.T. Mart\u00edn",
                "J. Rodr\u00edguez"
            ],
            "title": "3D modeling by means of videogrammetry and laser scanners for reverse engineering",
            "venue": "Measurement",
            "year": 2016
        },
        {
            "authors": [
                "J. Geng"
            ],
            "title": "Structured-light 3D surface imaging: A tutorial",
            "venue": "Adv. Opt. Photon",
            "year": 2011
        },
        {
            "authors": [
                "C. Yu",
                "F. Ji",
                "J. Xue",
                "Y. Wang"
            ],
            "title": "Adaptive Binocular Fringe Dynamic Projection Method for High Dynamic Range Measurement",
            "venue": "Sensors 2019,",
            "year": 2019
        },
        {
            "authors": [
                "S. Zhang"
            ],
            "title": "Recent progresses on real-time 3D shape measurement using digital fringe projection techniques",
            "venue": "Opt. Lasers Eng",
            "year": 2010
        },
        {
            "authors": [
                "C. Zuo",
                "Q. Chen",
                "G. Gu",
                "S. Feng",
                "F. Feng"
            ],
            "title": "High-speed three-dimensional profilometry for multiple objects with complex shapes",
            "venue": "Opt. Express",
            "year": 2012
        },
        {
            "authors": [
                "Y. Hu",
                "Q. Chen",
                "S. Feng",
                "C. Zuo"
            ],
            "title": "Microscopic fringe projection profilometry: A review",
            "venue": "Opt. Lasers Eng",
            "year": 2020
        },
        {
            "authors": [
                "S. Yang",
                "H. Huang",
                "G. Wu",
                "Y. Wu",
                "T. Yang",
                "F. Liu"
            ],
            "title": "High-speed three-dimensional shape measurement with inner shifting-phase fringe projection profilometry",
            "venue": "Chin. Opt. Lett. 2022,",
            "year": 2022
        },
        {
            "authors": [
                "L. Yu",
                "B. Pan"
            ],
            "title": "Single-camera high-speed stereo-digital image correlation for full-field vibration measurement",
            "venue": "Mech. Syst. Signal Process",
            "year": 2017
        },
        {
            "authors": [
                "M. Tekieli",
                "S. De Santis",
                "G. de Felice",
                "A. Kwiecie\u0144",
                "F. Roscini"
            ],
            "title": "Application of Digital Image Correlation to composite reinforcements testing",
            "venue": "Compos. Struct",
            "year": 2017
        },
        {
            "authors": [
                "S.S. Gorthi",
                "P. Rastogi"
            ],
            "title": "Fringe projection techniques: Whither we are",
            "venue": "Opt. Lasers Eng",
            "year": 2010
        },
        {
            "authors": [
                "D. Zheng",
                "F. Da",
                "Q. Kemao",
                "H.S. Seah"
            ],
            "title": "Phase-shifting profilometry combined with Gray-code patterns projection: Unwrapping error removal by an adaptive median filter",
            "venue": "Opt. Express 2017,",
            "year": 2017
        },
        {
            "authors": [
                "Y. Wei",
                "L. Lu",
                "J. Xi"
            ],
            "title": "Reconstruction of moving object with single fringe pattern based on phase shifting profilometry",
            "year": 2021
        },
        {
            "authors": [
                "Y. Ding",
                "P. Lu",
                "B. He",
                "X. Huang",
                "G. Li",
                "Z. Wang",
                "Y. Zhou",
                "Z. Zhu"
            ],
            "title": "Speckle Deformation Measurement Based on Pixel Correlation Search Method",
            "venue": "In Proceedings of the 2021 IEEE 5th Advanced Information Technology, Electronic and Automation Control Conference (IAEAC), Chongqing,",
            "year": 2021
        },
        {
            "authors": [
                "Q. Liu",
                "D.T.-W. Looi",
                "H.H. Chen",
                "C. Tang",
                "R.K.L. Su"
            ],
            "title": "Framework to optimise two-dimensional DIC measurements at different orders of accuracy for concrete structures",
            "venue": "Structures",
            "year": 2020
        },
        {
            "authors": [
                "K. Zhou",
                "D. Lei",
                "J. He",
                "P. Zhang",
                "P. Bai",
                "F. Zhu"
            ],
            "title": "Real-time localization of micro-damage in concrete beams using DIC technology and wavelet packet analysis",
            "venue": "Cem. Concr. Compos",
            "year": 2021
        },
        {
            "authors": [
                "H. Shi",
                "H. Ji",
                "G. Yang",
                "X. He"
            ],
            "title": "Shape and deformation measurement system by combining fringe projection and digital image correlation",
            "venue": "Opt. Lasers Eng",
            "year": 2013
        },
        {
            "authors": [
                "L. Felipe-Ses\u00e9",
                "E. L\u00f3pez-Alba",
                "P. Siegmann",
                "F.A. D\u00edaz"
            ],
            "title": "Integration of fringe projection and two-dimensional digital image correlation for three-dimensional displacements measurements",
            "year": 2016
        },
        {
            "authors": [
                "H. Yu",
                "D. Zheng",
                "J. Fu",
                "Y. Zhang",
                "C. Zuo",
                "J. Han"
            ],
            "title": "Deep learning-based fringe modulation-enhancing method for accurate fringe projection profilometry",
            "venue": "Opt. Express",
            "year": 2020
        },
        {
            "authors": [
                "S. Zhang",
                "S.-T. Yau"
            ],
            "title": "High-resolution, real-time 3D absolute coordinate measurement based on a phase-shifting method",
            "venue": "Opt. Express",
            "year": 2006
        },
        {
            "authors": [
                "I. Yamaguchi"
            ],
            "title": "Digital simulation of speckle patterns",
            "venue": "In Proceedings of the Speckle 2018: VII International Conference on Speckle Metrology, Jano\u0301w Podlaski, Poland,",
            "year": 2018
        },
        {
            "authors": [
                "O. Ronneberger",
                "P. Fischer",
                "T. Brox"
            ],
            "title": "U-Net: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted Intervention\u2014MICCAI",
            "year": 2015
        },
        {
            "authors": [
                "S. Zhang",
                "P.S. Huang"
            ],
            "title": "Novel method for structured light system calibration",
            "year": 2006
        }
    ],
    "sections": [
        {
            "text": "Citation: Zhang, C.; Liu, C.; Xu, Z.\nHigh-Accuracy Three-Dimensional\nDeformation Measurement System\nBased on Fringe Projection and\nSpeckle Correlation. Sensors 2023, 23,\n680. https://doi.org/10.3390/\ns23020680\nAcademic Editor: Lei Huang\nReceived: 5 December 2022\nRevised: 1 January 2023\nAccepted: 5 January 2023\nPublished: 6 January 2023\nCopyright: \u00a9 2023 by the authors.\nLicensee MDPI, Basel, Switzerland.\nThis article is an open access article\ndistributed under the terms and\nconditions of the Creative Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n4.0/).\nKeywords: fringe projection profilometry; digital image correlation; fringe and speckle separation; neural network; high accuracy\n1. Introduction\nOptical metrology is widely used in biomedicine, reverse engineering, bridge monitoring and other fields [1\u20133], because of its non-contact, speed and high-accuracy advantages [4\u20137]. Fringe projection profilometry (FPP) [8,9] and digital image correlation (DIC) [10,11] are two common non-interference measurement methods. FPP uses a projector to project fringes onto the measured object [12]. The phase information [13,14] can be solved from the deformed fringe images, and the three-dimensional (3D) shape of the object can be reconstructed by the phase. However, it is only sensitive to the out-of-plane displacement of the measured object in deformation measurements. It cannot achieve the tracking of object points. DIC employs a speckle texture on the surface of the measured object as the deformation information carrier [15]. Two-dimensional DIC (2D-DIC) [16] adopts a single camera to capture images, making it is easy to operate, but it can only measure in-plane deformation. Three-dimensional DIC (3D-DIC) [17] can measure the 3D deformation of objects with high accuracy, but it requires synchronous triggering of multiple cameras. Furthermore, the filtering effects of subset windows will lower the accuracy of non-uniform deformation fields. The combination of DIC and FPP can overcome the disadvantages of their respective methods. However, the separation of fringe and speckle images is the key procedure. Therefore, Shi et al. [18] obtained a surface texture image of a measured object from phaseshifting fringe images. Because the grey gradient of the speckle texture on the surface of the object changes little, it is not necessary to separate the fringe images to realize the combination of DIC and FPP for the measurement. However, in practice, in order to improve the accuracy of DIC measurement, it is necessary to spray or transfer the speckle with large grey gradient changes on the measured surface. This also leads to poor quality of the captured fringe images, which cannot be directly used for calculation. This requires us to separate the fringe and speckle images. Felipe Sese et al. [19] used a multisensory camera and laser structural illumination to separate the color encoding of characterized fringe\nSensors 2023, 23, 680. https://doi.org/10.3390/s23020680 https://www.mdpi.com/journal/sensors\nSensors 2023, 23, 680 2 of 12\nand speckle patterns, and realized the measurement of 3D displacement. However, the measurement errors of this method are large, and this method is not suitable for high-speed measurements. Therefore, in this paper, a fringe and speckle image separation method based on deep learning technology is proposed, which can significantly improve the modulation of fringe images [20], and the accuracy of 3D deformation measurements. A set of three-step phaseshifting speckle-embedded fringe images is converted into speckle-free fringe images using a convolutional neural network (CNN). This method with high efficiency can automatically produce speckle-free images using the trained CNN model.\n2. Principle\nA flow chart of the proposed measurement method is shown in Figure 1. Firstly, speckle images are extracted from the background light grey intensity of the phase-shifting fringe images. DIC is applied to calculate the sub-pixel displacement of the measured object before and after deformation. At the same time, high-quality fringe images are separated from the speckle-embedded fringe images using CNN. The 3D coordinates of the measured object before and after deformation can be obtained via FPP and the parameterization results of the system. Secondly, the least-squares method is used to solve the integer pixel 3D coordinates before deformation and the 3D coordinates corresponding to the sub-pixel after deformation. The 3D displacement can be achieved by subtracting the corresponding 3D coordinates. At last, the difference method is used to solve the full-field strain. It is necessary to establish local coordinate systems. The 3D coordinates before deformation and 3D displacement are converted to the corresponding local coordinate systems, and the strain tensor can be obtained by fitting the 3D displacement in the local system. The above contents will be introduced in the following four subsections.\nSensors 2023, 23, x FOR PEER REVIEW 2 of 13\nus to separate e fringe and speckle image . Felipe Sese et al. [19] used a ul isensory camera and laser structural illumination to separate the color encoding of characterized fringe and speckle patterns, and realized the measurement of 3D displacement. However, the measurement errors of this method are large, and this method is not suitable for highspeed measurements.\nTherefore, in this paper, a fringe and speckle image separation method based on deep learning technology is propo ed, w ich can significantly improve the modulation of fringe images [20], and the accuracy of 3D deformation measurements. A set of three-step phase-shifting speckle-embedded fringe images is converted into speckle-free fringe images using a convolutional neural network (CNN). This method with high efficiency can automatically produce speckle-free images using the trained CNN model.\n2. Principle\nA flow chart of the proposed measurement method is shown in Figure 1. Firstly, speckle images are extracted from the background light grey intensity of the phase-shifting fringe images. DIC is applied to calculate the sub-pixel displacement of the measured object before and after deformation. At the same time, high-quality fringe images are separated from the speckle-embedded fringe images u ing CNN. The 3D coordinates of the measured object before and after deformation can be obtained via FPP and the parameterization results of the system. Secondly, the least-squares method is used to solve the integer pixel 3D coordinates before deformation and the 3D coordinates corresponding to the sub-pixel after deformation. The 3D displacement can be achieved by subtracting the corresponding 3D coordinates. At last, the difference method is used to solve the full-fiel strain. It is nec ssary to establish local coordinate systems. The 3D coordinates before deformation and 3D displacement are converted to the corresponding local coordinate systems, and the strain tensor can be obtained by fitting the 3D displacement in the local system. The above contents will be introduced in the following four subsections.\n2.1. Analysis of the Influence of Speckle\nThe phase-shifting profilometry (PSP) method calculates the phase information of the surface of the measured objects using fringe images. Ideally, a set of sinusoidal fringes are projected by the projector, and the intensity of the image captured by the camera can be expressed as:\n ( , ) ( , ) ( , ) cos ( , ) = + + n nI x y A x y B x y x y (1)\nwhere ( , ) [0,255]nI x y is the grey intensity of the standard sinusoidal fringe images captured by the camera, ( , )A x y is the background grey intensity, ( , )B x y is the surface reflectivity, ( , ) x y is the phase to be obtained, which contains the 3D information\nof the object,  n is the phase-shifting phase, and 1,2,3 ,=n N represents the number of phase-shifting steps.\nFigure 1. Flow chart of the proposed measurement method.\n2.1. Analysis of the Influence of Speckle\nT ph s -shifting p ofilometry (PSP) method calcul tes the phase information of the surface of the measured bjects using fri ge images. Ideally, a set of sinusoidal fringes are projected by the projector, and the intensity of the image captured by the camera can be expressed as:\nIn(x, y) = A(x, y) + B(x, y) cos[\u03d5(x, y) + \u2206\u03d5n] (1)\nwhere In(x, y) \u2208 [0, 255] is the grey intensity of the standard sinusoidal f inge images captured by the camera, A(x, y) is the background grey intensity, B(x, y) is the surface reflectivity, \u03d5(x y) is the ph to be obtained, which contains the 3D infor ation of the object, \u2206\u03d5n is the phase-shifting phase, and n = 1, 2, 3 \u00b7 \u00b7 \u00b7 , N represents the number of phase-shifting steps. The least-squares phase solution method for N-step phase-shifting can be expressed as [21]:\n\u03d5(x, y) = arctan  \u2212 N \u2211 n=1 In sin(\u2206\u03d5n)\nN \u2211\nn=1 In cos(\u2206\u03d5n)\n (2)\nSensors 2023, 23, 680 3 of 12\nIn order to establish the speckle model of images, a random number of speckles [22] in In(x, y) are added. Then, the grey intensity of the speckle-embedded fringe images is expressed as:\nI\u2032n(x, y) = {\nIn(x, y) Sn(x, y) = 255 0 Sn(x, y) = 0\n(3)\nwhere I\u2032n(x, y) is the grey intensity of speckle-embedded fringe images, and S(x, y) is the random number of speckles. According to Equations (1) and (3), a speckle-free fringe image with an initial phase value of 0 rad and a speckle-embedded fringe image with an initial phase value of 0 rad, generated via computer simulation, are shown in Figures 2a1 and 2b1, respectively. In the middle row of the image, there is a red line, and the grey intensity distribution of the two cases is shown in Figures 2a2 and 2b2, respectively. The grey intensity distribution of the speckle-free fringe image is a standard sinusoidal curve, while the speckle-embedded fringe image is irregular. If the PSP is applied to solve the phase value of the image directly, as shown in Figure 2b1, it will inevitably induce large phase errors. Therefore, Section 2.2 will describe, in detail, how to extract the fringe pattern based on the deep learning model.\nSensors 2023, 23, x FOR PEER REVIEW 3 of 13 The least-squares phase solution method for N-step phase-shifting can be expressed as [21]: 1\n1\nsin( ) ( , ) arctan\ncos( )\n \n\n=\n=   \u2212    =        N n n n N n n n I x y I\n(2)\nIn order to establish the speckle model of images, a random number of speckles [22]\nin ( , )nI x y are added. Then, the grey intensity of the speckle-embedded fringe images\nis expressed as:\n( , ) ( , ) 255 ( , )\n0 ( , ) 0\nn n\nn\nn\nI x y S x y I x y\nS x y\n=  = \n=\n(3)\nwhere ( , )nI x y is the grey intensity of speckle-embedded fringe images, and ( , )S x y\nis the random number of speckles.\nAccording to Equations (1) and (3), a speckle-free fringe image with an initial phase\nvalue of 0 rad and a speckle-embedded fringe image with an initial phase value of 0 rad,\ngenerated via computer simulation, are hown in Figure 2a1 and b1, respectively. In the\nmiddl row of the image, there is a r li e, and the grey intensity distrib ti n of the two\ncases is shown in Figure 2a2 and b2, respectively. The grey intensity distribution of the\nspeckle-free fringe image is a standard sinusoidal curve, while the speckle-embedded\nfringe image is irregular. If the PSP is applied to solve the phase value of the image di-\nrectly, as shown in Figure 2b1, it will inevitably induce large phase errors. Therefore, Sec-\ntion 2.2 will describe, in detail, how to extract the fringe pattern based on the deep learning\nmodel.\n2.2. Fringe and Speckle Pattern Separation Based on Deep Learning Model\nAs shown in Figure 3, a U-shaped convolutional neural network (CNN) structure\nwith great feature extraction performance is constructed [23]. Three-step phase-shifting\nspeckle-embedded fringe images are used as the input of the CNN, while speckle-free\nfringe images are used as the output of the CNN. The U-shaped structure is divided into\n2.2. Fringe and Speckle Pattern Separation Based on Deep Learning Model\nAs shown in Figure 3, a U-shaped convolutio al neural network (CNN) structure with great feature extraction performance is constructed [23]. Three-step phase-shifting speckleembedded fringe images are used as the input of the CNN, while speckle-free fringe images are used as the output of the CNN. The U-shaped structure is divided into two parts: feature extraction and feature fusion. The feature extraction part consists of the following operations: convolution (Conv) and batch normalization (BN), and Conv, BN and Dropout by four times. The feature fusion part consists of the following operations: transposed convolution (T-Conv), BN and rectified linear unit (ReLU) by four times; T-Conv; and a group of residual structures and Conv. At the same time, in order to retrieve the missing information, the method of skip connection is used to connect the high-level information and the low-level information to achieve pixel-level information acquisition. The residual block structure includes Conv, residual block and Conv. This structure alleviates the gradient disappearance problem caused by increasing the depth in neural networks.\nSensors 2023, 23, 680 4 of 12\nSensors 2023, 23, x FOR PEER REVIEW 4 of 13 two parts: feature extraction and feature fusion. The feature extraction part consists of the following operations: convolution (Conv) and batch normalization (BN), and Conv, BN and Dropout by four times. The feature fusion part consists of the following operations: transposed convolution (T-Conv), BN and rectified linear unit (ReLU) by four times; T-\nConv; and a group of residual structures and Conv. At the same time, in order to retrieve\nthe missing information, the method of skip connection is used to connect the high-level\ninformation and the low-level information to achieve pixel-level information acquisition.\nThe residual block structure includes Conv, residual block and Conv. This structure alle-\nviates the gradient disappearance problem caused by increasing the depth in neural net-\nworks.\nwhere H and W are the height and width of the image, respectively, out\nnI and nI are\nthe output of the network and the given real output.\nThe adaptive moment estimation (ADAM) learning algorithm is applied in CNN. For\nparameter selection, the batch size is 2, the starting learning rate is 1 \u00d7 10\u22123, and the learn-\ning rate will be multiplied by 0.1 after every 200 training rounds.\n2.3. Three-Dimensional Displacement Calculation\nIn the combination of DIC and FPP, DIC is mainly utilized to solve the sub-pixel high-\naccuracy image coordinate displacement of objects before and after deformation. There-\nfore, the corresponding points before and after deformation can be found. However, since\nthe accuracy of DIC can be reached at around 0.01 pixels, the least-squares curve fitting\nalgorithm is employed to solve the corresponding 3D coordinates of the image sub-pixel\ncoordinates. The first-order polynomial is selected if the measured object with a complex-\nsurface, higher-order polynomial can be employed. The 3D coordinates (X, Y, Z) of the\nreference subset center and the deformed subset center can be expressed as:\n0 1 2\n0 1 2\n0 1 2\n( , )\n( , )\n( , )\n= + +  = + +  = + + X x y a a x a y Y x y b b x b y Z x y c c x c y\n(5)\nwhere H and W are the height and width of the image, respectively, Ioutn and In are the output of the network and the given real output. The adaptive moment estimation (ADAM) learning algorithm is applied in CNN. For parameter selection, the batch size is 2, the starting learning rate is 1 \u00d7 10\u22123, and the learning rate will be multiplied by 0.1 after every 200 training rounds.\n2.3. Three-Dimensional Displacement Calculation\nIn the combination of DIC and FPP, DIC is mainly utilized to solve the sub-pixel high-accuracy image coordinate displacement of objects before and after deformation. Therefore, the correspo ding points before and after deformation can be found. Howev r, since the accuracy of DIC can be reached at around 0.01 pixels, the least-squares curv fitting algorithm is employed to solve the corresponding 3D coordinates of th image sub-pixel coordinates. The first-order polyn mial is selecte f the measured object with a\nmplex-surfac , highe -order polynom al can be employed. Th 3D coordinates (X, Y, Z) of the reference subset center and the defor ed subset c nter an be expressed as:\nX(x, y) = a0 + a1x + a2y Y(x, y) = b0 + b1x + b2y Z(x, y) = c0 + c1x + c2y\n(5)\nwhere (x, y) denote the local image coordinates of the subset window, and a0, a1, a2, b0, b1, b2, c0, c1 and c2 are the fitting coefficients. A (2M + 1) \u00d7 (2M + 1) subset window area is selected to the nearest integer pixel around the calculation point as the center for fitting.\nPS = W (6)\nP =  1 x0 \u2212M y0 \u2212M 1 x0 \u2212M y0 \u2212M + 1 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 1 x0 + M y0 + M\u2212 1 1 x0 + M y0 + M  S =  a0 b0 c0a1 b1 c1 a2 b2 c2  (7)\nSensors 2023, 23, 680 5 of 12\nW =  X(x0 \u2212M, y0 \u2212M) Y(x0 \u2212M, y0 \u2212M) Z(x0 \u2212M, y0 \u2212M) X(x0 \u2212M, y0 \u2212M + 1) Y(x0 \u2212M, y0 \u2212M + 1) Z(x0 \u2212M, y0 \u2212M + 1) \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7\nX(x0 + M, y0 + M\u2212 1) Y(x0 + M, y0 + M\u2212 1) Z(x0 + M, y0 + M\u2212 1) X(x0 + M, y0 + M) Y(x0 + M, y0 + M) Z(x0 + M, y0 + M)  (8) where (x0, y0) denote the nearest integral pixel coordinate of (x, y), P is the coordinate matrix of all integral pixels in the selected subset window area, S is the fitting coefficient matrix, and W is the 3D coordinate matrix corresponding to all integer pixel coordinates in the selected subset window area. When the fitting coefficients are already calculated, the 3D coordinates of the corresponding reference and deformed object points can be obtained using Equation (5). The 3D displacement (U, V, W) in the global world coordinate system can be expressed as:UV\nW\n = X2Y2\nZ2\n\u2212 X1Y1\nZ1  (9) where (X1, Y1, Z1) and (X2, Y2, Z2) represent the 3D coordinates of the reference subset center and the deformed subset center calculated using Equations (5)\u2013(8), respectively.\n2.4. Full-Field Strain Measurements\nIn Section 2.3, the 3D coordinates of the reference subset center are (X, Y, Z), and the 3D displacement (U, V, W) in the global world coordinate system are calculated. In order to calculate the full-field strain of the measured surface, the first step is to establish a local coordinate system through the plane fitting of the local region centered on the point. The local coordinate system takes the vertical fitting plane facing outward as the positive direction of the Z axis; the X axis and Y axis are perpendicular to the Z axis, respectively, and can be artificially specified. Then, we calculate the rotation matrix R and the translation vector T from the world coordinate system to the local coordinate system. The 3D coordinates before deformation and the 3D displacement are transformed into the corresponding local coordinate systems through the rotation matrix R and translation vector T. XeYe\nZe\n = R XY\nZ + T (10) UeVe\nWe\n = R UV\nW + T (11) where (Xe, Ye, Ze) are the 3D coordinates in the local coordinate system, and (Ue, Ve, We) are the 3D displacement in the local coordinate system. The displacement field function is obtained via quadric surface fitting, and the strain tensor can be calculated using the field functions. The fitting forms of the field function are as follows:  Ue = aX1 X 2 e + aX2 Y 2 e + aX3 XeYe + a X 4 Xe + a X 5 Ye + a X 6 Ve = aY1 X 2 e + aY2 Y 2 e + aY3 XeYe + a Y 4 Xe + a Y 5 Ye + a Y 6\nWe = aZ1 X 2 e + aZ2 Y 2 e + aZ3 XeYe + a Z 4 Xe + a Z 5 Ye + a Z 6\n(12)\nwhere (aX1 , a X 2 , a X 3 , a X 4 , a X 5 , a X 6 ), (a Y 1 , a Y 2 , a Y 3 , a Y 4 , a Y 5 , a Y 6 ) and (a Z 1 , a Z 2 , a Z 3 , a Z 4 , a Z 5 , a Z 6 ) are the coef-\nficients of each field function. Then, the Lagrange strain tensor parameters are calculated based on following equations.\nSensors 2023, 23, 680 6 of 12\n\n\u03b5xx = \u2202Ue \u2202Xe + 12 [( \u2202Ue \u2202Xe )2 + ( \u2202Ve \u2202Xe )2 + ( \u2202We \u2202Xe )2] \u03b5yy =\n\u2202Ve \u2202Ye + 12 [( \u2202Ue \u2202Ye )2 + ( \u2202Ve \u2202Ye )2 + ( \u2202We \u2202Ye )2] \u03b5zz =\n\u2202We \u2202Ze + 12 [( \u2202Ue \u2202Ze )2 + ( \u2202Ve \u2202Ze )2 + ( \u2202We \u2202Ze )2] \u03b5xy = 1 2 ( \u2202Ue \u2202Ye + \u2202Ve\u2202Xe ) + 12 [( \u2202Ue \u2202Xe \u2202Ue \u2202Ye ) + ( \u2202Ve \u2202Xe \u2202Ve \u2202Ye ) + ( \u2202We \u2202Xe \u2202We \u2202Ye\n)] \u03b5yz = 1 2 ( \u2202Ve \u2202Ze + \u2202We\u2202Ye ) + 12 [( \u2202Ue \u2202Ye \u2202Ue \u2202Ze ) + ( \u2202Ve \u2202Ye \u2202Ve \u2202Ze ) + ( \u2202We \u2202Ye \u2202We \u2202Ze\n)] \u03b5zx = 1 2 ( \u2202We \u2202Xe + \u2202Ue\u2202Ze ) + 12 [( \u2202Ue \u2202Ze \u2202Ue \u2202Xe ) + ( \u2202Ve \u2202Ze \u2202Ve \u2202Xe ) + ( \u2202We \u2202Ze \u2202We \u2202Xe\n)] (13)\n3. Experiments and Results\nIn order to verify the effectiveness of the proposed method, three sets of experiments were conducted. They are fringe image extraction, 3D displacement and full-field strain measurements.\n3.1. Fringe Image Extraction\nIn Figure 4, the experimental system includes a DLP projector (LightCrafter 4500, Manufacturer Texas Instruments, Headquartered in Dallas, TX, USA) with a resolution of 912 \u00d7 1140 pixels2, and an IDS UI-3370CP (Manufacturer IDS, Headquartered in Obersulm, Germany) camera with a resolution of 2048 \u00d7 2048 pixels2 and that operates at 80 frame rates per second. A customized acrylic circular plate is employed as the measured object in the experiment, with a radius of 90 mm. The distances from the camera and projector to the circular plate are about 750 mm, and the angle between the camera and the projector is about 30\u25e6.\nSensors 2023, 23, x FOR PEER REVIEW 7 of 13\nIn Figure 4, the experi ental syste includes a DLP projector (LightCrafter 4500,\nanufacturer Texas Instru ents, eadquartered in Dallas, TX, USA) ith a resolution of"
        },
        {
            "heading": "912 \u00d7 1140 pixels2, and an IDS UI-3370CP ( anufacturer I S, eadquartered in bersul ,",
            "text": "er any) ca era ith a resolution of 2048 \u00d7 2048 pixels2 and that operates at 80 fra e\nr tes per second. A customized acryli circular plate is employed as the measured object\nin the experiment, with radius of 90 mm. The distances from the camera and projector\nto the ircular plate are about 750 mm, and the angl between the camera and the projector\nis about 30\u00b0.\nIn order to simulate the real situation, real experimental images are captured by the\ncamera as the training and testing datasets of the CNN. The projector projects a set of\nthree-step phase-shifting speckle-embedded fringe images and a set of three-step phase-\nshifting speckle-free fringe images to the circular plate, respectively. Two-hundred and\nforty training image datasets are captured by adjusting the projected speckle size. Next,\nspeckles are generated using Equation (3), and the speckles are transferred to the circular\nplate via water transfer, as shown in Figure 4b. The projector projects a set of three-step\nphase-shifting speckle-free fringe images, and the camera captures them as testing da-\ntasets. GPU (NVIDIA GeForce RTX 2080TI, 32 GB of RAM, Manufacturer NVIDIA, Head-\nquartered in Santa Clara, California, USA) and CPU (Intel Xeon Platinum, 96 GB of RAM,\nManufacturer intel, Headquartered in Santa Clara, California, USA) multithreading tech-\nniques are employed to accelerate the process of training.\nIn Figure 5, (a) shows the input fringe images of CNN and grey intensity at the green\nline, and (b) shows the fringe images predicted by CNN and grey intensity at the green\nline. It is obvious that CNN can effectively separate fringe images and improve the quality\nof the fringe images.\ni re 4. e experi ental setup. (a) Circular plate without speckles; (b) circular plate with speckles; (c) the experimental setup.\nI r er t si l t t real sit ati , real ex eri t l i c t r t c t tr i i t ti t . r j t j s t f t r - t s ifti s l f i i t t circ l r l t , res ecti l . o- r a f rt tr i i i t t j ti t r j t i . s c l s re r t si ti ( ), t l tr f t t i l r\nlat i at r tr sf , as sho in Fig re 4b. r j t j t s t f t re -st hase-shifting speckle-fre fringe images, and the camera captures them as testing datasets. GPU (NVIDIA GeForce RTX 2080 I, 32 GB of RAM, Manufacturer NVIDIA, Headquartere in Santa Clara, CA, USA) and CPU (Intel Xeon Platinum, 96 GB of RAM, Manufacturer intel, Headquartered in Santa Clara, CA, USA) multithreading techniques are employed to accelerate the process of training.\nSensors 2023, 23, 680 7 of 12\nIn Figure 5, (a) shows the input fringe images of CNN and grey intensity at the green line, and (b) shows the fringe images predicted by CNN and grey intensity at the green line. It is obvious that CNN can effectively separate fringe images and improve the quality of the fringe images.\nSensors 2023, 23, x FOR PEER REVIEW 7 of 13 In Figure 4, the experimental system includes a DLP projector (LightCrafter 4500, Manufacturer Texas Instruments, Headquartered in Dallas, TX, USA) with a resolution of"
        },
        {
            "heading": "912 \u00d7 1140 pixels2, and an IDS UI-3370CP (Manufacturer IDS, Headquartered in Obersulm,",
            "text": "Germany) camera with a resolution of 2048 \u00d7 2048 pixels2 and that operates at 80 frame rates per second. A customized acrylic circular plate is employed as the measured object in the experiment, with a radius of 90 mm. The distances from the camera and projector to the circular plate are about 750 mm, and the angle between the camera and the projector is about 30\u00b0. Figure 4. The experimental setup. (a) Circular plate without speckles; (b) circular plate with speckles; (c) the experimental setup. In order to simulate the real situation, real experimental images are captured by the camera as the training and testing datasets of the CNN. The projector projects a set of three-step phase-shifting speckle-embedded fringe images and a set of three-step phaseshifting speckle-free fringe images to the circular plate, respectively. Two-hundred and forty training image datasets are captured by adjusting the projected speckle size. Next,\nspeckles are generated using Equation (3), and the speckles are transferred to the circular\nplate via water transfer, as shown in Figure 4b. The projector projects a set of three-step\nphase-shifting speckle-free fringe images, and the camera captures them as testing da-\ntasets. GPU (NVIDIA GeForce RTX 2080TI, 32 GB of RAM, Manufacturer NVIDIA, Head-\nquartered in Santa Clara, California, USA) and CPU (Intel Xeon Platinum, 96 GB of RAM,\nManufacturer intel, Headquartered in Santa Clara, California, USA) multithreading tech-\nniques are employed to accelerate the process of training.\nIn Figure 5, (a) shows the input fringe images of CNN and grey intensity at the green\nline, and (b) shows the fringe images predicted by CNN and grey intensity at the green\nline. It is obvious that CNN can effectively separate fringe images and improve the quality\nof the fringe images.\nSensors 2023, 23, x FOR PEER REVIEW 8 of 13\nFigure 5. Experimental results. (a) The input speckle-embedded fringe images of CNN and grey intensity; (b) the output speckle-free fringe images of CNN and grey intensity.\n3.2. Three-Dimensional Displacement Measurements\nThe circular plate is rotated about 5 degrees, and speckle-embedded fringe images\nbefore and after rotation are captured. The fringe images can be extracted by the CNN.\nThe phase value of the circular plate before and after deformation can be calculated using\nthe 3-step phase-shifting algorithm. The parameters of the experimental system can be\ncalibrated using the method proposed by Zhang [24], and the coordinates of the X, Y and\nZ directions of the circular plate in the global world coordinate system can be obtained.\nSpeckle images can be obtained from the background grey intensity of the phase-\nshifting speckle-embedded fringe images. The circular area in the middle of the circular\nplate is selected as the calculation area. As shown in Figure 6, (a) shows x-direction dis-\nplacement of the circular plate along the y direction in the image, and (b) shows y-direction\ndisplacement of the circular plate along the x direction in the image. Their units are pixels.\nFigure 6. The displacement of integer pixel position. (a) Displacement in x direction; (b) displacement in y direction.\nThe corresponding relationship before and after rotation can be found using the DIC\nalgorithm according to Section 2.3. The fitting subset window area is 29 \u00d7 29 pixels2. Ac-\ncording to Equations (5)\u2013(8), polynomial fitting is performed to find the corresponding"
        },
        {
            "heading": "3D coordinate relationship before and after rotation. The displacement in the three directions can be obtained by subtracting the 3D coordinates of the corresponding points. The",
            "text": "full-field displacement in the three directions are shown in Figure 7.\nFigure 5. Experimental results. (a) The input speckle-embe ded fringe images of CNN and grey intensity; (b) the output speckle-free fringe i ages of C N and grey intensity.\n3.2. Three-Dimensional Displacement Measurements\nThe circular plate is rotated about 5 degrees, and speckle-embedded fringe images before and after rotation are captured. The fringe images can be extracted by the CNN. The phase value of the circular plate before and after deformation can be calculated using the 3-step phase-shifting algorithm. The parameters of the experimental system can be calibrated using the method proposed by Zhang [24], and the coordinates of the X, Y and Z directions of the circular plate in the global world coordinate system can be obtained. Speckle images can be obtained from the background grey intensity of the phaseshifting speckle-embedded fringe images. The circular area in the middle of the circular plate is selected as the calculation area. As shown in Figure 6, (a) shows x-direction displacement of the circular plate along the y direction in the image, and (b) shows ydirection displacement of the circular plate along the x direction in the image. Their units are pixels.\nSensors 2023, 23, x FOR PEER REVIEW 8 of 13\nFigure 5. Experimental results. (a) The input speckle-embedded fringe images of CNN and grey intensity; (b) the output speckle-free fringe images of CNN and grey intensity.\n3.2. Three-Dimensional Displacement Measurements\nThe circular plate is rotated about 5 degrees, and speckle-embedded fringe images\nbefore and after rotation are captured. The fringe images can be extracted by the CNN.\nThe phase valu of the circular plate before and after deformation can be calculated using\nthe 3-step phase-shifting algorithm. The p rameters of t experimental system can be\ncalibrated using the method proposed by Zhang [24], and he coordinates of the X, Y and\nZ directions of the circular plate the global world co rdinate system can be obtained.\nSpeckle images can be obtained from t e backg ound grey intensity of the phase-\nshifting sp kle-embedded fringe images. The circular area in the middle of the circular\nplate is selected as the alculation rea. As shown in Figure 6, (a) shows x-directi n dis\nplacement of the circular plate along the y direction in the image, and (b) shows y-d ection\ndis lacement of the circular plate along the x direction in the image. Their units are pixels.\nFigure 6. The displacement of integer pixel position. (a) Displacement in x direction; (b) displacement in y direction.\nThe corresponding relationship before and after rotation can be found using the DIC\nalgorithm according to Section 2.3. The fitting subset window area is 29 \u00d7 29 pixels2. Ac-\ncording to Equations (5)\u2013(8), polynomial fitting is performed to find the corresponding"
        },
        {
            "heading": "3D coordinate relationship before and after rotation. The displacement in the three directions can be obtained by subtracting the 3D coordinates of the corresponding points. The",
            "text": "full-field displacement in the three directions are shown in Figure 7.\nFigure 6. The displacement of integer pixel position. (a) Displacem nt i x direction; (b) displacement in y direction.\nhe corresponding relationship before and after rotation can be found using the I algorith accor ing to Section 2.3. The fitting subset window area is 29 \u00d7 29 pixels2.\nSensors 2023, 23, 680 8 of 12\nAccording to Equations (5)\u2013(8), polynomial fitting is performed to find the corresponding 3D coordinate relationship before and after rotation. The displacement in the three directions can be obtained by subtracting the 3D coordinates of the corresponding points. The full-field displacement in the three directions are shown in Figure 7. Sensors 2023, 23, x FOR PEER REVIEW 9 of 13\nFigure 7. Full-field displacement in the X, Y and Z directions.\nThe total displacement of the circular plate can be expressed as:\n2 2 2= + +x y zD D D D (14)\nwhere D is the total displacement, and Dx, Dy and Dz denote the displacement in the X, Y\nand Z directions, respectively.\nFigure 8a shows a stereogram of the full-field displacement, and Figure 8b shows a\nplanar graph of the full-field displacement. The full-field displacement is funnel-shaped\nwith nearly zero displacement in the middle and large displacement in the periphery. The\ndisplacement increases linearly along the radial direction. Figure 9a shows the total dis-\nplacement on the red dashed line. Accurate displacement can be obtained via Fourier\ncurve fitting. Figure 9b shows the error between total displacement and fitting displace-\nment. It can be seen that the error values are mainly distributed in the range of \u00b10.02 mm,\nwith only a few exceeding this range. In this experiment, the circular plate takes up about\nhalf of the camera\u2019s field of view. The measurement error is only about 0.006%.\nFigure 7. Full-field displacement in the X, Y and Z directions.\nThe total displace ent of the circular plate can be expressed as:\nD = \u221a\nD2x + D2y + D2z (14)\nwhere D is the total displacement, and Dx, Dy and Dz denote the displacement in the X, Y and Z directions, respectively. Figure 8a shows a stereogram of the full-field displacement, and Figure 8b shows a planar graph of the full-field displacement. The full-field displacement is funnel-shaped with nearly zero displacement in the middle and large displacement in the periphery. The displacement increases linearly along the radial direction. Figure 9a shows the total displacement on the red dashed line. Accurate displacement can be obtained via Fourier curve fitting. Figure 9b shows the error between total displacement and fitting displacement. It can be seen that the error values are mainly distributed in the range of \u00b10.02 mm, with only a few exceeding this range. In this experiment, the circular plate takes up about half of the camera\u2019s field of view. The measurement error is only about 0.006%.\nSensors 2023, 23, x FOR PEER REVIEW 9 of 13\nFigure 7. Full-field displacement in the X, Y and Z directions.\nThe total displacement of the circul r plate can be expressed as:\n2 2 2= + +x y zD D D D (14)\nwhere D is the total displacement, and Dx, Dy and Dz denote the displacement in the X, Y\nand Z directions, respectively.\nFigure 8a shows a stereogram of the full-field displacement, and Figure 8b shows a\nplanar graph of the full-field displacement. The full-field displacement is funnel-shaped\nwith ne rly zero displacement in the middle and large isplacement in the periphery. The\ndisplacement inc eases in arly along the radi l irection. Figure 9a shows the total dis-\nplacement on the red dashed line. Accurate displacement can be obtained via Fourier\ncurve fitting. Figure 9b shows the error between total displacement and fitting displace-\nment. It can be seen that the error values are mainly distributed in the range of \u00b10.02 mm,\nwith only a few exceeding this range. In this experi ent, the circular plate takes up about\nhalf of the camera\u2019s field of view. The ea t e ror is only about .006%.\nFigure 8. The total displacement. (a) The stereogram of the full-field displacement; (b) the planar graph of the full-field displacement.\nFigure 8. The total displacement. (a) The st of the full-field isplacem nt; (b) the planar graph of the full-fiel displacement.\nSensors 2023, 23, 680 9 of 12 Sensors 2023, 23, x FOR PEER REVIEW 10 of 13\nFigure 9. The displacement at the dashed line. (a) Comparison of experimental results and fitting results; (b) error.\n3.3. Full-Field Strain Measurements\nStrain reflects the relative deformation of an object under stress. It is an essential\nphysical quantity used to measure the mechanical properties of objects. As shown in Fig-\nure 10, camera 1 and the projector form FPP combined with a DIC measurement system.\nCamera 1 and camera 2 constitute a 3D-DIC measurement system. They are placed sym-\nmetrically about the projector. The distance from camera 1 to the three-point bending\nspecimen is about 750 mm. The distance from the projector to the three-point bending\nspecimen is about 850 mm. The angle between camera 1 and the projector is about 30\u00b0.\nTwo methods are used to measure the strain in the red dotted area of the three-point\nFigure 10. Experimental setup.\nSince it is impossible to know the changes of the surface below the measured surface,\nonly \u0510xx, \u0510xy and \u0510yy can be calculated. Figure 11a1,b1,c1 are the full-field strain distribution\ncalculated using 3D-DIC, Figure 11a2,b2,c2 are the full-field strain distributions calculated\nusing the method of this paper, and Figure 11a3,b3,c3 are the strain solved using two\nmethods at the red dotted line. By comparison, the distribution of full-field strain calcu-\nlated in this paper is basically the same as that calculated using 3D-DIC. The strain difference between the two methods is only 1 \u00d7 10\u22124. Therefore, this method can measure accu-\nrate strain results.\n(a)\n(b)\nFigure 9. The displacement at the dashed line. (a) Comparison of experimental results and fitting results; (b) error.\n3.3. Full-Field Strain Measurements\nStrain reflects the relative deformation of an object under stress. It is an essential physical quantity used to measure the mechanical properties of objects. As shown in Figure 10, camera 1 and the projector form FPP combined with a DIC measurement system. Camera 1 and camera 2 constitute a 3D-DIC measurement system. They are placed symmetrically about the projector. The distance from camera 1 to the three-point bending specimen is about 750 mm. The distance from the projector to the three-point bending specimen is about 850 mm. The angle between camera 1 and the projector is about 30\u25e6. Two methods are used to measure the strain in the red dotted area of the three-point bending specimen.\nSensors 2023, 23, x FOR PEER REVIEW 10 of 13\nFigure 9. The displacement at the dashed line. (a) Comparison of experimental results and fitting\nresults; (b) error.\n3.3. Full-Field Strain Measurements\nStrain reflects the relative deformation of an object under stress. It is an essential\nphysical quantity used to asure the mechanical properties of objects. As shown in Fig-\nure 10, camera 1 and the projector form FPP combined with a DIC measurement system.\nCamera 1 and camera 2 constitute a 3D-DIC measurement system. They are placed sym-\nmetrically about the projector. The distance from camera 1 to the thr e-point bending\nspecimen is about 750 mm. The distance from the projector to the three-point bending\nspecimen is about 850 mm. The angle between ca era 1 and the projector is about 30\u00b0.\nTwo ethods ar used to measure the strain in th red dotted area of the three-point\nbending specimen.\nSi i po sible to know the changes of the surface below the mea red surface,\nonly \u0510x , \u0510 y can be calculated. Figure 1a1,b1,c1 are the full-field strain distribution\ncalculate sing 3D-DIC, Figure 11a2,b2,c2 are the full-field strain distributions calculated\nusing the method of this paper, and Figure 11a3,b3,c3 are the strain solved using two\nmethods at the red dotted line. By comparison, the distribution of full-field strain calcu-\nlated in this paper is basically the same as that calculated using 3D-DIC. The strain differ-\nence between the two methods is only 1 \u00d7 10\u22124. Therefore, this method can measure accu-\nrate strain results.\n(a)\n(b)\nFig . peri ental setup.\nSince it is impossible to know the changes of the surface below the measured surface, only \u03b5xx, \u03b5xy and \u03b5yy can be calculated. Figure 11a1,b1,c1 are the full-field strain distribution calculated using 3D-DIC, Figure 11a2,b2,c2 are the full-field strain distributions calculated using the method of this paper, and Figure 11a3,b3,c3 are the strain solved using two methods at the red dotted line. By comparison, the distribution of full-field strain calculated in t is paper is basically the same as that calculated using 3D-DIC. The strain difference between the two method is only 1 \u00d7 10\u22124. Therefore, this method can measure accu te strain results.\nSensors 2023, 23, 680 10 of 12Sensors 2023, 23, x FOR PEER REVIEW 11 of 13\n4. Discussion of Measurement Results under Different Exposure Times\nNext, the applicability of the proposed measurement method in high- and low-expo-\nsure experimental scenes is discussed.\nIn the experiment of this paper, when the camera exposure time is 40 ms, the gray\nintensity of the image is in a reasonable range. The camera exposure time was adjusted to"
        },
        {
            "heading": "20 ms for the low-exposure experimental scene, and the camera exposure time was adjusted to 80 ms for the high-exposure experimental scene. The experimental images collected under different exposure times are shown in Figure 12.",
            "text": "Figure 13 shows an error comparison of 3D displacement of a line under the exposure\ntimes of 20 ms, 40 ms and 80 ms. It can be clearly seen that the error reaches \u00b10.05 mm\nwith high and low exposure. In Table 1, the RMSE of displacement measured with low\nexposure time (20 ms) is 0.02 mm, the RMSE measured with normal exposure (40 ms) is\n0.01 mm, and the RMSE measured with high exposure (80 ms) is 0.03 mm. Therefore, the\ni re 11. i tri ti f f ll-fiel strai . ( , , ) t f ll-fi l str i istri ti s c lc late using 3 - IC; (a2,b2,c2) the full-field strain distributions calculated using the ethod of this paper; (a3,b3,c3) the strain solved using two methods at the red dotted line.\n. i r t s lts r iffere t Exposure i es\nt, t applicability of the pr posed measur ment method in high- and low exposure exp rimental scenes is di cussed. I t e ri t f t is r, e t c r s r ti is s, t r i te sity of t e i age is i a reaso able ra ge. e ca era ex os re ti e as a j ste to 20 ms for the low-exposure experimental scene, and the camera exposure time was adjusted to 80 ms for the high-exposure experimental scene. The experimental images collected under different exposure times are shown in Figure 12.\ni re 12. Images acquired at differ nt exposure times. (a) Exposure time of 20 ms; (b) exposure time of 40 ms; (c) exposure time of 80 ms.\ni re 13 s s rr r c arison of is lace ent of a li r re ti es of 20 ms, 40 ms and 80 ms. It can be cl ar y seen that e error reaches\u00b10.05 mm with high and low exposure. In Table 1, the RMSE of displacement m asured with low exposure time (20 ms) is 0. 2 mm, the RMSE measured with normal exp sure (40 ms) is 0. 1 mm, and the RMSE measured with high exposure (80 ms) is 0.03 mm. Therefore, the method\nSensors 2023, 23, 680 11 of 12\npresented in this paper can be used to measure both high- and low-exposure experimental scenes and has high accuracy.\nSensors 2023, 23, x FOR PEER REVIEW 12 of 13\nmethod presented in this paper can be used to measure both high- and low-exposure ex-\nperimental scenes and has high accuracy.\nFigure 13. Comparison of displacement measurement error under different exposure times.\nTable 1. Root mean square error of displacement measurement (mm).\nExposure Time 20 ms 40 ms 80 ms\nRMSE 0.02 0.01 0.03\n5. Conclusions\nIn this paper, a high-accuracy 3D deformation measurement system based on fringe\nprojection and speckle correlation is proposed, which can effectively eliminate the influ-\nence of speckles on fringe image quality. The difficulty of combining DIC with FPP is\nsolved. The method has high accuracy in displacement measurement and can be effec-\ntively applied to high- and low-exposure experimental scenes. A similar trend to 3D-DIC\ncan be obtained in strain measurements.\nBefore each measurement, the measurement system in this paper needs to be cali-\nbrated, and the high-accuracy calibration plate should be selected as far as possible. In\naddition, the measurement system in this paper can also be calibrated by measuring the\nstandard specimen or the standard deformation.\nAuthor Contributions: Conceptualization, C.Z. and C.L.; methodology, C.Z.; software, C.L.; validation, C.Z., C.L. and Z.X.; formal analysis, C.Z.; investigation, C.Z.; resources, C.Z. and Z.X.; data curation, C.Z.; writing\u2014original draft preparation, C.Z.; writing\u2014review and editing, C.Z.; visualization, C.Z.; supervision, C.L.; project administration, C.L.; funding acquisition, C.Z., C.L. and Z.X. All authors have read and agreed to the published version of the manuscript.\nFunding: This study was funded by the National Natural Science Foundation of China (Grant No. 11802132), the Natural Science Foundation of Jiangsu Province (Grant No. BK20180446), the China Postdoctoral Science Foundation (Grant Nos. 2020M671493 and 2019M652433), the Jiangsu Planned Projects for Postdoctoral Research Funds, and the Natural Science Foundation of Shandong Province (Grant No. ZR2018BF001).\nInstitutional Review Board Statement: Not applicable.\nInformed Consent Statement: Not applicable.\nData Availability Statement: Not applicable.\nAcknowledgments: We thank the High-Performance Computing Center of Nanjing University of Science and Technology for providing computing assistance.\nConflicts of Interest: The authors declare no conflicts of interest.\nFigure 13. Comparison of displacement measurement error under different exposure ti es.\nTable 1. Root mean square error of displace ent easure ent ( ).\nExposure Ti 20 ms 40 ms 80 ms\nRMSE 0.02 0.01 0.03\n5. Conclusions\nIn this paper, a high-accuracy 3D deformation measurement system based on fringe projection and speckle correlation is proposed, which can effectively eliminate the influence of speckles on fringe image quality. The difficulty of combining DIC with FPP is solved. The method has high accuracy in displacement measurement and can be effectively applied to high- and low-exposure experimental scen s. A similar tr nd to 3D-DIC can be obtained in strain measurements. Before e e t, the measurement system in this paper needs to be calibr ted, and the high-accuracy alibration plate should be se ected as far as possible. In addition, the measure nt system in this paper can also be c ibrated by measuring the standard specimen or the standard deformation.\nAuthor Contributions: Conceptualization, C Z. and C.L.; meth dology, C.Z.; software, C.L.; v lidation, C.Z , C.L. and Z.X.; form l nalysis, C Z.; investigation, C Z.; resources, C.Z. and Z.X.; data curation, C.Z.; writing ri i l , . .; riting review and editing, C.Z.; visualization, C.Z ; supervision, C.L ; project administration, C.L.; funding acquisition, C.Z., C.L. and Z.X. All authors have r ad nd agreed to the published version f the manuscript.\nFunding: This study was funded by the National Natural Science Foundation of China (Grant No. 1802132), the Natural Science Foundation of Jiangsu Province (Grant No. BK20180 46), the China Postdoctoral Science Foundation (Grant Nos. 2020M671493 and 2019M6524 3), the Jiangsu Pla ned Projects for Post oct r l s rc s, a the Natural Science Foundation of Shandong Province (Grant No. ZR2018BF001).\nInstitutional Review Board Statement: Not a plicable.\nInformed Consent Statement: Not applicable.\nData Availability Statement: Not applicable.\nAcknowledgments: We thank the igh-Performance Computing Center of Nanjing University of Science and Technology for providing co puting assistance.\nConflicts of Interest: The authors declare no conflict of interest.\nSensors 2023, 23, 680 12 of 12\nReferences 1. Gao, Z.; Kato, T.; Takahashi, H.; Doi, A. 3D Measurement and Feature Extraction for Metal Nuts. In Advances in Networked-Based\nInformation Systems; Barolli, L., Chen, H.C., Enokido, T., Eds.; Lecture Notes in Networks and Systems; Springer International Publishing: Cham, Switzerland, 2022; Volume 313, pp. 299\u2013305. [CrossRef]\n2. Midgett, D.; Thorn, S.; Ahn, S.; Uman, S.; Avendano, R.; Melvinsdottir, I.; Lysyy, T.; Kim, J.; Duncan, J.; Humphrey, J.; et al. CineCT platform for in vivo and ex vivo measurement of 3D high resolution Lagrangian strains in the left ventricle following myocardial infarction and intramyocardial delivery of theranostic hydrogel. J. Mol. Cell. Cardiol. 2022, 166, 74\u201390. [CrossRef] [PubMed] 3. Herr\u00e1ez, J.; Mart\u00ednez, J.C.; Coll, E.; Mart\u00edn, M.T.; Rodr\u00edguez, J. 3D modeling by means of videogrammetry and laser scanners for reverse engineering. Measurement 2016, 87, 216\u2013227. [CrossRef] 4. Geng, J. Structured-light 3D surface imaging: A tutorial. Adv. Opt. Photon. 2011, 3, 128\u2013160. [CrossRef] 5. Yu, C.; Ji, F.; Xue, J.; Wang, Y. Adaptive Binocular Fringe Dynamic Projection Method for High Dynamic Range Measurement. Sensors 2019, 19, 4023. [CrossRef] [PubMed] 6. Zhang, S. Recent progresses on real-time 3D shape measurement using digital fringe projection techniques. Opt. Lasers Eng. 2010, 48, 149\u2013158. [CrossRef] 7. Zuo, C.; Chen, Q.; Gu, G.; Feng, S.; Feng, F. High-speed three-dimensional profilometry for multiple objects with complex shapes. Opt. Express 2012, 20, 19493. [CrossRef] 8. Hu, Y.; Chen, Q.; Feng, S.; Zuo, C. Microscopic fringe projection profilometry: A review. Opt. Lasers Eng. 2020, 135, 106192. [CrossRef] 9. Yang, S.; Huang, H.; Wu, G.; Wu, Y.; Yang, T.; Liu, F. High-speed three-dimensional shape measurement with inner shifting-phase fringe projection profilometry. Chin. Opt. Lett. 2022, 20, 112601. [CrossRef] 10. Yu, L.; Pan, B. Single-camera high-speed stereo-digital image correlation for full-field vibration measurement. Mech. Syst. Signal Process. 2017, 94, 374\u2013383. [CrossRef] 11. Tekieli, M.; De Santis, S.; de Felice, G.; Kwiecien\u0301, A.; Roscini, F. Application of Digital Image Correlation to composite reinforcements testing. Compos. Struct. 2017, 160, 670\u2013688. [CrossRef] 12. Gorthi, S.S.; Rastogi, P. Fringe projection techniques: Whither we are? Opt. Lasers Eng. 2010, 48, 133\u2013140. [CrossRef] 13. Zheng, D.; Da, F.; Kemao, Q.; Seah, H.S. Phase-shifting profilometry combined with Gray-code patterns projection: Unwrapping error removal by an adaptive median filter. Opt. Express 2017, 25, 4700\u20134713. [CrossRef] [PubMed] 14. Wei, Y.; Lu, L.; Xi, J. Reconstruction of moving object with single fringe pattern based on phase shifting profilometry. Opt. Eng. 2021, 60, 084106. [CrossRef] 15. Ding, Y.; Lu, P.; He, B.; Huang, X.; Li, G.; Wang, Z.; Zhou, Y.; Zhu, Z. Speckle Deformation Measurement Based on Pixel\nCorrelation Search Method. In Proceedings of the 2021 IEEE 5th Advanced Information Technology, Electronic and Automation Control Conference (IAEAC), Chongqing, China, 12\u201314 March 2021; pp. 2525\u20132529. [CrossRef]\n16. Liu, Q.; Looi, D.T.-W.; Chen, H.H.; Tang, C.; Su, R.K.L. Framework to optimise two-dimensional DIC measurements at different orders of accuracy for concrete structures. Structures 2020, 28, 93\u2013105. [CrossRef] 17. Zhou, K.; Lei, D.; He, J.; Zhang, P.; Bai, P.; Zhu, F. Real-time localization of micro-damage in concrete beams using DIC technology and wavelet packet analysis. Cem. Concr. Compos. 2021, 123, 104198. [CrossRef] 18. Shi, H.; Ji, H.; Yang, G.; He, X. Shape and deformation measurement system by combining fringe projection and digital image correlation. Opt. Lasers Eng. 2013, 51, 47\u201353. [CrossRef] 19. Felipe-Ses\u00e9, L.; L\u00f3pez-Alba, E.; Siegmann, P.; D\u00edaz, F.A. Integration of fringe projection and two-dimensional digital image correlation for three-dimensional displacements measurements. Opt. Eng. 2016, 55, 121711. [CrossRef] 20. Yu, H.; Zheng, D.; Fu, J.; Zhang, Y.; Zuo, C.; Han, J. Deep learning-based fringe modulation-enhancing method for accurate fringe projection profilometry. Opt. Express 2020, 28, 21692. [CrossRef] 21. Zhang, S.; Yau, S.-T. High-resolution, real-time 3D absolute coordinate measurement based on a phase-shifting method. Opt. Express 2006, 14, 2644. [CrossRef] [PubMed] 22. Yamaguchi, I. Digital simulation of speckle patterns. In Proceedings of the Speckle 2018: VII International Conference on Speckle Metrology, Jan\u00f3w Podlaski, Poland, 9\u201312 September 2018; Volume 10834, p. 1083409. [CrossRef] 23. Ronneberger, O.; Fischer, P.; Brox, T. U-Net: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted Intervention\u2014MICCAI 2015; Navab, N., Hornegger, J., Wells, W.M., Frangi, A.F., Eds.; Lecture Notes in Computer Science; Springer International Publishing: Cham, Switzerland, 2015; Volume 9351, pp. 234\u2013241. [CrossRef] 24. Zhang, S.; Huang, P.S. Novel method for structured light system calibration. Opt. Eng. 2006, 45, 083601. [CrossRef]\nDisclaimer/Publisher\u2019s Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content."
        }
    ],
    "title": "High-Accuracy Three-Dimensional Deformation Measurement System Based on Fringe Projection and Speckle Correlation",
    "year": 2023
}