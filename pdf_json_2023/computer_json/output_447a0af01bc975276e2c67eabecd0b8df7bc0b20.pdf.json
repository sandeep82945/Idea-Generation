{
    "abstractText": "We propose a new class of generative models that naturally handle data of varying dimensionality by jointly modeling the state and dimension of each datapoint. The generative process is formulated as a jump diffusion process that makes jumps between different dimensional spaces. We first define a dimension destroying forward noising process, before deriving the dimension creating time-reversed generative process along with a novel evidence lower bound training objective for learning to approximate it. Simulating our learned approximation to the time-reversed generative process then provides an effective way of sampling data of varying dimensionality by jointly generating state values and dimensions. We demonstrate our approach on molecular and video datasets of varying dimensionality, reporting better compatibility with test-time diffusion guidance imputation tasks and improved interpolation capabilities versus fixed dimensional models that generate state values and dimensions separately.",
    "authors": [
        {
            "affiliations": [],
            "name": "Andrew Campbell"
        },
        {
            "affiliations": [],
            "name": "William Harvey"
        },
        {
            "affiliations": [],
            "name": "Christian Weilbach"
        },
        {
            "affiliations": [],
            "name": "Valentin De Bortoli"
        },
        {
            "affiliations": [],
            "name": "Tom Rainforth"
        },
        {
            "affiliations": [],
            "name": "Arnaud Doucet"
        }
    ],
    "id": "SP:f69d20c2ae331867dd0e8f22b7870e12baf7d2ff",
    "references": [
        {
            "authors": [
                "Jascha Sohl-Dickstein",
                "Eric Weiss",
                "Niru Maheswaranathan",
                "Surya Ganguli"
            ],
            "title": "Deep unsupervised learning using nonequilibrium thermodynamics",
            "venue": "International Conference on Machine Learning,",
            "year": 2015
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Yang Song",
                "Jascha Sohl-Dickstein",
                "Diederik P Kingma",
                "Abhishek Kumar",
                "Stefano Ermon",
                "Ben Poole"
            ],
            "title": "Score-based generative modeling through stochastic differential equations",
            "venue": "International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Chitwan Saharia",
                "William Chan",
                "Saurabh Saxena",
                "Lala Li",
                "Jay Whang",
                "Emily L Denton",
                "Kamyar Ghasemipour",
                "Raphael Gontijo Lopes",
                "Burcu Karagol Ayan",
                "Tim Salimans",
                "Jonathan Ho",
                "Mohammad Fleet",
                "David J aand Norouzi"
            ],
            "title": "Photorealistic text-to-image diffusion models with deep language understanding",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Aditya Ramesh",
                "Prafulla Dhariwal",
                "Alex Nichol",
                "Casey Chu",
                "Mark Chen"
            ],
            "title": "Hierarchical text-conditional image generation with clip latents",
            "venue": "arXiv preprint arXiv:2204.06125,",
            "year": 2022
        },
        {
            "authors": [
                "Zhifeng Kong",
                "Wei Ping",
                "Jiaji Huang",
                "Kexin Zhao",
                "Bryan Catanzaro"
            ],
            "title": "Diffwave: A versatile diffusion model for audio synthesis",
            "venue": "International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Joseph L Watson",
                "David Juergens",
                "Nathaniel R Bennett",
                "Brian L Trippe",
                "Jason Yim",
                "Helen E Eisenach",
                "Woody Ahern",
                "Andrew J Borst",
                "Robert J Ragotte",
                "Lukas F Milles"
            ],
            "title": "Broadly applicable and accurate protein design by integrating structure prediction networks and diffusion generative models",
            "venue": "bioRxiv,",
            "year": 2022
        },
        {
            "authors": [
                "Emiel Hoogeboom",
                "V\u0131ctor Garcia Satorras",
                "Cl\u00e9ment Vignac",
                "Max Welling"
            ],
            "title": "Equivariant diffusion for molecule generation in 3d",
            "venue": "International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Jonathan Ho",
                "Tim Salimans",
                "Alexey Gritsenko",
                "William Chan",
                "Mohammad Norouzi",
                "David J Fleet"
            ],
            "title": "Video diffusion models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Jonathan Ho",
                "William Chan",
                "Chitwan Saharia",
                "Jay Whang",
                "Ruiqi Gao",
                "Alexey Gritsenko",
                "Diederik P Kingma",
                "Ben Poole",
                "Mohammad Norouzi",
                "David J Fleet",
                "Tim Saliman"
            ],
            "title": "Imagen video: High definition video generation with diffusion models",
            "venue": "arXiv preprint arXiv:2210.02303,",
            "year": 2022
        },
        {
            "authors": [
                "Ilia Igashov",
                "Hannes St\u00e4rk",
                "Cl\u00e9ment Vignac",
                "Victor Garcia Satorras",
                "Pascal Frossard",
                "Max Welling",
                "Michael Bronstein",
                "Bruno Correia"
            ],
            "title": "Equivariant 3d-conditional diffusion models for molecular linker design",
            "venue": "arXiv preprint arXiv:2210.05274,",
            "year": 2022
        },
        {
            "authors": [
                "Prafulla Dhariwal",
                "Alexander Nichol"
            ],
            "title": "Diffusion models beat GANs on image synthesis",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Katherine Crowson"
            ],
            "title": "Clip guided diffusion",
            "venue": "Web Demo, https://huggingface.co/spaces/EleutherAI/clip-guided-diffusion,",
            "year": 2021
        },
        {
            "authors": [
                "Hengtong Zhang",
                "Tingyang Xu"
            ],
            "title": "Towards controllable diffusion models via reward-guided exploration",
            "venue": "arXiv preprint arXiv:2304.07132,",
            "year": 2023
        },
        {
            "authors": [
                "Chin-Wei Huang",
                "Jae Hyun Lim",
                "Aaron C Courville"
            ],
            "title": "A variational perspective on diffusionbased generative models and score matching",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Tero Karras",
                "Miika Aittala",
                "Timo Aila",
                "Samuli Laine"
            ],
            "title": "Elucidating the design space of diffusion-based generative models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Joe Benton",
                "Yuyang Shi",
                "Valentin De Bortoli",
                "George Deligiannidis",
                "Arnaud Doucet"
            ],
            "title": "From denoising diffusions to denoising Markov models",
            "venue": "arXiv preprint arXiv:2211.03595,",
            "year": 2022
        },
        {
            "authors": [
                "Brian DO Anderson"
            ],
            "title": "Reverse-time diffusion equation models",
            "venue": "Stochastic Processes and their Applications,",
            "year": 1982
        },
        {
            "authors": [
                "Ulrich G Haussmann",
                "Etienne Pardoux"
            ],
            "title": "Time reversal of diffusions",
            "venue": "The Annals of Probability,",
            "year": 1986
        },
        {
            "authors": [
                "Pascal Vincent"
            ],
            "title": "A connection between score matching and denoising autoencoders",
            "venue": "Neural Computation,",
            "year": 2011
        },
        {
            "authors": [
                "Yang Song",
                "Conor Durkan",
                "Iain Murray",
                "Stefano Ermon"
            ],
            "title": "Maximum likelihood training of score-based diffusion models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Patrick Cheridito",
                "Damir Filipovi\u0107",
                "Marc Yor"
            ],
            "title": "Equivalent and absolutely continuous measure changes for jump-diffusion processes",
            "venue": "Annals of applied probability,",
            "year": 2005
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Olaf Ronneberger",
                "Philipp Fischer",
                "Thomas Brox"
            ],
            "title": "U-net: Convolutional networks for biomedical image segmentation",
            "venue": "Medical Image Computing and Computer-Assisted Intervention,",
            "year": 2015
        },
        {
            "authors": [
                "Tomer Weiss",
                "Luca Cosmo",
                "Eduardo Mayo Yanes",
                "Sabyasachi Chakraborty",
                "Alex M Bronstein",
                "Renana Gershoni-Poranne"
            ],
            "title": "Guided diffusion for inverse molecular design",
            "venue": "ChemrXiv,",
            "year": 2023
        },
        {
            "authors": [
                "Bowen Jing",
                "Gabriele Corso",
                "Renato Berlinghieri",
                "Tommi Jaakkola"
            ],
            "title": "Subspace diffusion generative models",
            "venue": "European Conference on Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Han Zhang",
                "Ruili Feng",
                "Zhantao Yang",
                "Lianghua Huang",
                "Yu Liu",
                "Yifei Zhang",
                "Yujun Shen",
                "Deli Zhao",
                "Jingren Zhou",
                "Fan Cheng"
            ],
            "title": "Dimensionality-varying diffusion process",
            "venue": "arXiv preprint arXiv:2211.16032,",
            "year": 2022
        },
        {
            "authors": [
                "Andrew Campbell",
                "Joe Benton",
                "Valentin De Bortoli",
                "Tom Rainforth",
                "George Deligiannidis",
                "Arnaud Doucet"
            ],
            "title": "A continuous time framework for discrete denoising models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Ulf Grenander",
                "Michael I Miller"
            ],
            "title": "Representations of knowledge in complex systems",
            "venue": "Journal of the Royal Statistical Society: Series B (Methodological),",
            "year": 1994
        },
        {
            "authors": [
                "David B Phillips",
                "Adrian F M Smith"
            ],
            "title": "Bayesian model comparison via jump diffusions",
            "venue": "Markov Chain Monte Carlo in Practice,",
            "year": 1995
        },
        {
            "authors": [
                "Michael I Miller",
                "Ulf Grenander",
                "Joseph A O\u2019Sullivan",
                "Donald L Snyder"
            ],
            "title": "Automatic target recognition organized via jump-diffusion algorithms",
            "venue": "IEEE Transactions on Image Processing,",
            "year": 1997
        },
        {
            "authors": [
                "Peter J Green"
            ],
            "title": "Reversible jump Markov chain Monte Carlo computation and Bayesian model determination",
            "year": 1995
        },
        {
            "authors": [
                "Peter J Green"
            ],
            "title": "Trans-dimensional Markov chain Monte Carlo",
            "venue": "Highly Structured Stochastic Systems,",
            "year": 2003
        },
        {
            "authors": [
                "Lars Ruddigkeit",
                "Ruud Van Deursen",
                "Lorenz C Blum",
                "Jean-Louis Reymond"
            ],
            "title": "Enumeration of 166 billion organic small molecules in the chemical universe database gdb-17",
            "venue": "Journal of chemical information and modeling,",
            "year": 2012
        },
        {
            "authors": [
                "Raghunathan Ramakrishnan",
                "Pavlo O Dral",
                "Matthias Rupp",
                "O Anatole Von Lilienfeld"
            ],
            "title": "Quantum chemistry structures and properties of 134 kilo molecules",
            "venue": "Scientific Data,",
            "year": 2014
        },
        {
            "authors": [
                "Eric V Anslyn",
                "Dennis A Dougherty"
            ],
            "title": "Modern physical organic chemistry",
            "venue": "University Science Books,",
            "year": 2005
        },
        {
            "authors": [
                "Stephen Tian",
                "Chelsea Finn",
                "Jiajun Wu"
            ],
            "title": "A control-centric benchmark for video prediction",
            "venue": "arXiv preprint arXiv:2304.13723,",
            "year": 2023
        },
        {
            "authors": [
                "Harini Kannan",
                "Danijar Hafner",
                "Chelsea Finn",
                "Dumitru Erhan"
            ],
            "title": "Robodesk: A multi-task reinforcement learning benchmark. https: // github. com/ google-research/ robodesk",
            "year": 2021
        },
        {
            "authors": [
                "Michael Janner",
                "Yilun Du",
                "Joshua Tenenbaum",
                "Sergey Levine"
            ],
            "title": "Planning with diffusion for flexible behavior synthesis",
            "venue": "International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "William Harvey",
                "Saeid Naderiparizi",
                "Vaden Masrani",
                "Christian Weilbach",
                "Frank Wood"
            ],
            "title": "Flexible diffusion modeling of long videos",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Stewart N Ethier",
                "Thomas G Kurtz"
            ],
            "title": "Markov processes: Characterization and convergence",
            "year": 2009
        },
        {
            "authors": [
                "Giovanni Conforti",
                "Christian L\u00e9onard"
            ],
            "title": "Time reversal of Markov processes with jumps under a finite entropy condition",
            "venue": "Stochastic Processes and their Applications,",
            "year": 2022
        },
        {
            "authors": [
                "Clement Vignac",
                "Pascal Frossard"
            ],
            "title": "Top-n: Equivariant set and graph generation without exchangeability",
            "venue": "International Conference on Learning Representations,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "We propose a new class of generative models that naturally handle data of varying dimensionality by jointly modeling the state and dimension of each datapoint. The generative process is formulated as a jump diffusion process that makes jumps between different dimensional spaces. We first define a dimension destroying forward noising process, before deriving the dimension creating time-reversed generative process along with a novel evidence lower bound training objective for learning to approximate it. Simulating our learned approximation to the time-reversed generative process then provides an effective way of sampling data of varying dimensionality by jointly generating state values and dimensions. We demonstrate our approach on molecular and video datasets of varying dimensionality, reporting better compatibility with test-time diffusion guidance imputation tasks and improved interpolation capabilities versus fixed dimensional models that generate state values and dimensions separately."
        },
        {
            "heading": "1 Introduction",
            "text": "Generative models based on diffusion processes [1\u20133] have become widely used in solving a range of problems including text-to-image generation [4, 5], audio synthesis [6] and protein design [7]. These models define a forward noising diffusion process that corrupts data to noise and then learn the corresponding time-reversed backward generative process that generates novel datapoints from noise.\nIn many applications, for example generating novel molecules [8] or videos [9, 10], the dimension of the data can also vary. For example, a molecule can contain a varying number of atoms and a video can contain a varying number of frames. When defining a generative model over these data-types, it is therefore necessary to model the number of dimensions along with the raw values of each of its dimensions (the state). Previous approaches to modeling such data have relied on first sampling the number of dimensions from the empirical distribution obtained from the training data, and then sampling data using a fixed dimension diffusion model (FDDM) conditioned on this number of dimensions [8]. For conditional modeling, where the number of dimensions may depend on the observations, this approach does not apply and we are forced to first train an auxiliary model that predicts the number of dimensions given the observations [11].\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\nar X\niv :2\n30 5.\n16 26\n1v 2\n[ st\nat .M\nL ]\nThis approach to trans-dimensional generative modeling is fundamentally limited due to the complete separation of dimension generation and state value generation. This is exemplified in the common use case of conditional diffusion guidance. Here, an unconditional diffusion model is trained that end-users can then easily and cheaply condition on their task of interest through guiding the generative diffusion process [3, 12\u201314] without needing to perform any further training or fine-tuning of the model on their task of interest. Since the diffusion occurs in a fixed dimensional space, there is no way for the guidance to appropriately guide the dimension of the generated datapoint. This can lead to incorrect generations for datasets where the dimension greatly affects the nature of the datapoint created, e.g. small molecules have completely different properties to large molecules.\nTo generate data of varying dimensionality, we propose a jump diffusion based generative model that jointly generates both the dimension and the state. Our model can be seen as a unification of diffusion models which generate all dimensions in parallel with autoregressive type models which generate dimensions sequentially. We derive the model through constructing a forward noising process that adds noise and removes dimensions and a backward generative process that denoises and adds dimensions, see Figure 1. We derive the optimum backward generative process as the time-reversal of the forward noising process and derive a novel learning objective to learn this backward process from data. We demonstrate the advantages of our method on molecular and video datasets finding our method achieves superior guided generation performance and produces more representative data interpolations across dimensions."
        },
        {
            "heading": "2 Background",
            "text": "Standard continuous-time diffusion models [3, 15\u201317] define a forward diffusion process through a stochastic differential equation (SDE) where x0 \u223c pdata and, for t > 0,\ndxt = \u2212\u2192 b t(xt)dt+ gtdwt, (1)\nwhere xt \u2208 Rd is the current state, \u2212\u2192 b t : Rd \u2192 Rd is the drift and gt \u2208 R is the diffusion coefficient. dwt is a Brownian motion increment on Rd. This SDE can be understood intuitively by noting that in each infinitesimal timestep, we move slightly in the direction of the drift \u2212\u2192 b t and inject a small amount of Gaussian noised governed by gt. Let pt(xt) denote the distribution of xt for the forward diffusion process (1) so that p0(x0) = pdata(x0). \u2212\u2192 b t and gt are set such that at time t = T , pT (xT ) is close to pref(xT ) = N (xT ; 0, Id); e.g. \u2212\u2192 b t(xt) = \u2212 12\u03b2txt, gt = \u221a \u03b2t for \u03b2t > 0 [2, 3].\nThe time-reversal of the forward diffusion (1) is also a diffusion [18, 19] which runs backwards in time from pT (xT ) to p0(x0) and satisfies the following reverse time SDE\ndxt = \u2190\u2212 b t(xt)dt+ gtdw\u0302t,\nwhere \u2190\u2212 b t(xt) = \u2212\u2192 b t(xt) \u2212 g2t\u2207xt log pt(xt), dt is a negative infinitesimal time step and dw\u0302t is a Brownian motion increment when time flows backwards. Unfortunately, both the terminal\ndistribution, pT (xT ), and the score, \u2207xt log pt(xt), are unknown in practice. A generative model is obtained by approximating pT with pref and learning an approximation s\u03b8t (xt) to \u2207xt log pt(xt) typically using denoising score matching [20], i.e.\nmin \u03b8 EU(t;0,T )p0,t(x0,xt)[\u2225s \u03b8 t (xt)\u2212\u2207xt log pt|0(xt|x0)\u22252]. (2)\nFor a flexible model class, s\u03b8, we get s\u03b8t (xt) \u2248 \u2207xt log pt(xt) at the minimizing parameter."
        },
        {
            "heading": "3 Trans-Dimensional Generative Model",
            "text": "Instead of working with fixed dimension datapoints, we will instead assume our datapoints consist of a variable number of components. A datapoint X consists of n components each of dimension d. For ease of notation, each datapoint will explicitly store both the number of components, n, and the state values, x, giving X = (n,x). Since each datapoint can have a variable number of components from n = 1 to n = N , our overall space that our datapoints live in is the union of all these possibilities, X \u2208 X = \u22c3N n=1{n} \u00d7 Rnd. For example, for a varying size point cloud dataset, components would refer to points in the cloud, each containing (x, y, z) coordinates giving d = 3 and the maximum possible number of points in the cloud is N .\nBroadly speaking, our approach will follow the same framework as previous diffusion generative models. We will first define a forward noising process that both corrupts state values with Gaussian noise and progressively deletes dimensions. We then learn an approximation to the time-reversal giving a backward generative process that simultaneously denoises whilst also progressively adding dimensions back until a synthetic datapoint of appropriate dimensionality has been constructed."
        },
        {
            "heading": "3.1 Forward Process",
            "text": "Our forward and backward processes will be defined through jump diffusions. A jump diffusion process has two components, the diffusion part and the jump part. Between jumps, the process evolves according to a standard SDE. When a jump occurs, the process transitions to a different dimensional space with the new value for the process being drawn from a transition kernel Kt(Y|X) : X \u00d7 X \u2192 R\u22650. Letting Y = (m,y), the transition kernel satisfies \u2211 m \u222b y Kt(m,y|X)dy = 1 and\u222b\ny Kt(m = n,y|X)dy = 0. The rate at which jumps occur (jumps per unit time) is given by a rate\nfunction \u03bbt(X) : X \u2192 R\u22650. For an infinitesimal timestep dt, the jump diffusion can be written as Jump X\u2032t = { Xt with probability 1\u2212 \u03bbt(Xt)dt Y \u223c Kt(Y|Xt) with probability \u03bbt(Xt)dt\nDiffusion xt+dt = x\u2032t + bt(X \u2032 t)dt+ gtdwt nt+dt = n \u2032 t\nwith Xt \u225c (nt,xt) and Xt+dt \u225c (nt+dt,xt+dt) and dwt being a Brownian motion increment on Rn\u2032td. We provide a more formal definition in Appendix A. With the jump diffusion formalism in hand, we can now construct our forward noising process. We will use the diffusion part to corrupt existing state values with Gaussian noise and the jump part to destroy dimensions. For the diffusion part, we use the VP-SDE introduced in [2, 3] with \u2212\u2192 b t(X) = \u2212 12\u03b2tx and \u2212\u2192g t = \u221a \u03b2t with \u03b2t \u2265 0.\nWhen a jump occurs in the forward process, one component of the current state will be deleted. For example, one point in a point cloud or a single frame in a video is deleted. The rate at which these deletions occur is set by a user-defined forward rate\u2192\u03bb t(X). To formalize the deletion, we need to introduce some more notation. We let Kdel(i|n) be a user-defined distribution over which component of the current state to delete. We also define del : X\u00d7N\u2192 X to be the deletion operator that deletes a specified component. Specifically, (n\u22121,y) = del((n,x), i) where y \u2208 R(n\u22121)d has the same values as x \u2208 Rnd except for the d values corresponding to the ith component which have been removed. We can now define the forward jump transition kernel as \u2212\u2192 K t(Y|X) = \u2211n i=1 K\ndel(i|n)\u03b4del(X,i)(Y). We note that only one component is ever deleted at a time meaning \u2212\u2192 K t(m,y|X) = 0 for m \u0338= n\u2212 1. Further, the choice of Kdel(i|n) will dictate the behaviour of the reverse generative process. If we set Kdel(i|n) = I{i = n} then we only ever delete the final component and so in the reverse generative\ndirection, datapoints are created additively, appending components onto the end of the current state. Alternatively, if we set Kdel(i|n) = 1/n then components are deleted uniformly at random during forward corruption and in the reverse generative process, the model will need to pick the most suitable location for a new component from all possible positions.\nThe forward noising process is simulated from t = 0 to t = T and should be such that at time t = T , the marginal probability pt(X) should be close to a reference measure pref(X) that can be sampled from. We set pref(X) = I{n = 1}N (x; 0, Id) where I{n = 1} is 1 when n = 1 and 0 otherwise. To be close to pref, for the jump part, we set \u2192 \u03bb t high enough such that at time t = T there is a high probability that all but one of the components in the original datapoint have been deleted. For simplicity, we also set\u2192\u03bb t to depend only on the current dimension \u2192 \u03bb t(X) = \u2192 \u03bb t(n) with\u2192\u03bb t(n = 1) = 0 so that the forward process stops deleting components when there is only 1 left. In our experiments, we demonstrate the trade-offs between different rate schedules in time. For the diffusion part, we use the standard diffusion \u03b2t schedule [2, 3] so that we are close to N (x; 0, Id)."
        },
        {
            "heading": "3.2 Backward Process",
            "text": "The backward generative process will simultaneously denoise and add dimensions back in order to construct the final datapoint. It will consist of a backward drift\n\u2190\u2212 b t(X), diffusion coefficient\u2190\u2212g t, rate\u2190\u2212\n\u03bb t(X) and transition kernel \u2190\u2212 K t(Y|X). We would like these quantities to be such that the backward\nprocess is the time-reversal of the forward process. In order to find the time-reversal of the forward process, we must first introduce some notation to describe \u2190\u2212 K t(Y|X). \u2190\u2212 K t(Y|X) should undo the forward deletion operation. Since \u2212\u2192 K t(Y|X) chooses a component and then deletes it, \u2190\u2212 K t(Y|X) will need to generate the state values for a new component, decide where the component should be placed and then insert it at this location. Our new component will be denoted yadd \u2208 Rd. The insertion operator is defined as ins : X \u00d7 Rd \u00d7 N \u2192 X . It takes in the current value X, the new component yadd and an index i \u2208 {1, . . . , n+ 1} and inserts yadd into X at location i such that the resulting value Y = ins(X,yadd, i) has del(Y, i) = X. We denote the joint conditional distribution over the newly added component and the index at which it is inserted as At(yadd, i|X). We therefore have \u2190\u2212 K t(Y|X) = \u222b yadd \u2211n+1 i=1 At(y\nadd, i|X)\u03b4ins(X,yadd,i)(Y)dyadd. Noting that only one component is ever added at a time, we have \u2190\u2212 K t(m,y|X) = 0 for m \u0338= n+ 1.\nThis backward process formalism can be seen as a unification of diffusion models with autoregressive models. The diffusion part \u2190\u2212 b t denoises the current set of components in parallel, whilst the autoregressive part At(yadd, i|X) predicts a new component and its location. \u2190\u2212 \u03bb t(X) is the glue between these parts controlling when and how many new components are added during generation.\nWe now give the optimum values for \u2190\u2212 b t(X),\u2190\u2212g t, \u2190\u2212 \u03bb t(X) and At(yadd, i|X) such that the backward process is the time-reversal of the forward process.\nProposition 1. The time reversal of a forward jump diffusion process given by drift \u2212\u2192 b t, diffusion coefficient \u2212\u2192g t, rate \u2192 \u03bb t(n) and transition kernel \u2211n i=1 K\ndel(i|n)\u03b4del(X,i)(Y) is given by a jump diffusion process with drift \u2190\u2212 b \u2217t (X), diffusion coefficient \u2190\u2212g \u2217t , rate \u2190\u2212 \u03bb \u2217t (X) and transition kernel\n\u222b yadd \u2211n+1 i=1 A \u2217 t (y\nadd, i|X)\u03b4ins(X,yadd,i)(Y)dyadd as defined below \u2190\u2212 b \u2217t (X) = \u2212\u2192 b t(X)\u2212\u2207x log pt(X), \u2190\u2212g \u2217t = \u2212\u2192g t,\n\u2190\u2212 \u03bb \u2217t (X) = \u2192 \u03bb t(n+ 1)\n\u2211n+1 i=1 K del(i|n+ 1) \u222b yadd pt(ins(X,yadd, i))dyadd\npt(X) ,\nA\u2217t (y add, i|X) \u221d pt(ins(X,yadd, i))Kdel(i|n+ 1).\nAll proofs are given in Appendix A. The expressions for \u2190\u2212 b \u2217t and \u2190\u2212g \u2217t are the same as for a standard diffusion except for replacing\u2207x log pt(x) with\u2207x log pt(X) = \u2207x log pt(x|n) which is simply the score in the current dimension. The expression for \u2190\u2212 \u03bb \u2217t can be understood intuitively by noting that the numerator in the probability ratio is the probability that at time t, given a deletion occurs, the forward process will arrive at X. If this is higher than the raw probability at time t that the forward process is at X (the denominator) then we should have high \u2190\u2212 \u03bb \u2217t because X is likely the result of a deletion of a larger datapoint. Finally the optimum A\u2217t (y add, i|X) is simply the conditional distribution of yadd and i given X when the joint distribution over yadd, i,X is given by pt(ins(X,yadd, i))Kdel(i|n+ 1)."
        },
        {
            "heading": "3.3 Objective for Learning the Backward Process",
            "text": "The true \u2190\u2212 b \u2217t , \u2190\u2212 \u03bb \u2217t and A \u2217 t are unknown so we need to learn approximations to them, \u2190\u2212 b \u03b8t , \u2190\u2212 \u03bb \u03b8t and A\u03b8t . Following Proposition 1, we set \u2190\u2212 b \u03b8t (X) = \u2212\u2192 b t(X) \u2212 s\u03b8t (X) where s\u03b8t (X) approximates \u2207x log pt(X). The forward and parameterized backward processes are summarized in Table 1. Standard diffusion models are trained using a denoising score matching loss which can be derived from maximizing an evidence lower bound on the model probability for Epdata(x0)[log p\u03b80(x0)] [21]. We derive here an equivalent loss to learn s\u03b8t , \u2190\u2212 \u03bb \u03b8t and A \u03b8 t for our jump diffusion process by leveraging the results of [17] and [22]. Before presenting this loss, we first introduce some notation. Our objective for s\u03b8t (Xt) will resemble denoising score matching (2) but instead involve the conditional score \u2207xt log pt|0(Xt|X0) = \u2207xt log pt|0(xt|X0, nt). This is difficult to calculate directly due to a combinatorial sum over the different ways the components of X0 can be deleted to get to Xt. We avoid this problem by equivalently conditioning on a mask variable Mt \u2208 {0, 1}n0 that is 0 for components of X0 that have been deleted to get to Xt and 1 for components that remain in Xt. This makes our denoising score matching target easy to calculate: \u2207xt log pt|0(xt|X0, nt,Mt) = \u221a \u03b1tMt(x0)\u2212xt 1\u2212\u03b1t where \u03b1t = exp(\u2212 \u222b t 0 \u03b2(s)ds) [3]. Here Mt(x0) is the vector removing any components in x0 for which Mt is 0, thus Mt(x0) and xt have the same dimensionality. We now state our full objective. Proposition 2. For the backward generative jump diffusion process starting at pref(XT ) and finishing at p\u03b80(X0), an evidence lower bound on the model log-likelihood Ex0\u223cpdata [log p\u03b80(x0)] is given by\nL(\u03b8) = \u2212T 2 E [ g2t \u2225s\u03b8t (Xt)\u2212\u2207xt log pt|0(xt|X0, nt,Mt)\u22252 ] + (3)\nTE [ \u2212 \u2190\u2212 \u03bb \u03b8t (Xt) + \u2192 \u03bb t(nt) log \u2190\u2212 \u03bb \u03b8t (Y) + \u2192 \u03bb t(nt) logA \u03b8 t (x add t , i|Y) ] + C, (4)\nwhere expectations are with respect to U(t; 0, T )p0,t(X0,Xt,Mt)Kdel(i|nt)\u03b4del(Xt,i)(Y), C is a constant term independent of \u03b8 and Xt = ins(Y,xaddt , i). This evidence lower bound is equal to the log-likelihood when \u2190\u2212 b \u03b8t = \u2190\u2212 b \u2217t , \u2190\u2212 \u03bb \u03b8t = \u2190\u2212 \u03bb \u2217t and A \u03b8 t = A \u2217 t .\nWe now examine the objective to gain an intuition into the learning signal. Our first term (3) is an L2 regression to a target that, as we have seen, is a scaled vector between xt and \u221a \u03b1tMt(x0). As the solution to an L2 regression problem is the conditional expectation of the target, s\u03b8t (Xt) will learn to predict vectors pointing towards x0 averaged over the possible correspondences between dimensions of xt and dimensions of x0. Thus, during sampling, s\u03b8t (Xt) provides a suitable direction to adjust the current value Xt taking into account the fact Xt represents only a noisy subpart of a clean whole X0. The second term (4) gives a learning signal for \u2190\u2212 \u03bb \u03b8t and A \u03b8 t . For A \u03b8 t , we simply have a maximum likelihood objective, predicting the missing part of Xt (i.e. xaddt ) given the observed part of Xt (i.e. Y). The signal for \u2190\u2212 \u03bb \u03b8t comes from balancing two terms: \u2212 \u2190\u2212 \u03bb \u03b8t (Xt) and \u2192 \u03bb t(nt) log \u2190\u2212 \u03bb \u03b8t (Y)\nwhich encourage the value of \u2190\u2212 \u03bb \u03b8t to move in opposite directions. For a new test input Z, \u2190\u2212 \u03bb \u03b8t (Z)\u2019s value needs to trade off between the two terms by learning the relative probability between Z being the entirety of a genuine sample from the forward process, corresponding to the \u2190\u2212 \u03bb \u03b8t (Xt) term in (4), or Z being a substructure of a genuine sample, corresponding to the \u2190\u2212 \u03bb \u03b8t (Y) term in (4). The optimum trade-off is found exactly at the time reversal \u2190\u2212 \u03bb \u2217t as we show in Appendix A.5.\nWe optimize L(\u03b8) using stochastic gradient ascent, generating minibatches by first sampling t \u223c U(0, T ), X0 \u223c pdata and then computing Xt from the forward process. This can be done analytically for the\u2192\u03bb t(n) functions used in our experiments. We first sample nt by analytic integration of the dimension deletion Poisson process with time inhomogeneous rate\u2192\u03bb t(n). We then add Gaussian noise independently to each dimension under pt|0(xt|X0, nt,Mt) using a randomly drawn mask variable Mt. See Appendix B for further details on the efficient evaluation of our objective.\n3.4 Parameterization\ns\u03b8t (Xt), A \u03b8 t (y add, i|Xt) and \u2190\u2212 \u03bb \u03b8t (Xt) will all be parameterized by neural networks. In practice, we have a single backbone network suited to the problem of interest e.g. a Transformer [23], an EGNN [24] or a UNet [25] onto which we add prediction heads for s\u03b8t (Xt), A \u03b8 t (y add, i|Xt) and \u2190\u2212 \u03bb \u03b8t (Xt). s\u03b8t (Xt) outputs a vector in Rntd. A\u03b8t (yadd, i|Xt) outputs a distribution over i and mean and standard deviation statistics for a Gaussian distribution over yadd. Finally, having \u2190\u2212 \u03bb \u03b8t (Xt) \u2208 R\u22650 be the raw output of a neural network can cause optimization issues due to the optimum \u2190\u2212 \u03bb \u2217t including a probability ratio which can take on very large values. Instead, we learn a component prediction network p\u03b80|t(n0|Xt) that predicts the number of components in X0 given Xt. To convert this into\u2190\u2212 \u03bb \u03b8(Xt), we show in Proposition 3 how the optimum \u2190\u2212 \u03bb \u2217t (Xt) is an analytic function of the true p0|t(n0|Xt). We then plug p\u03b80|t(n0|Xt) into Proposition 3 to obtain an approximation of \u2190\u2212 \u03bb \u2217t (Xt).\nProposition 3. We have\n\u2190\u2212 \u03bb \u2217t (Xt) = \u2192 \u03bb t(nt + 1) N\u2211 n0=1 pt|0(nt + 1|n0) pt|0(nt|n0) p0|t(n0|Xt),\nwhere Xt = (nt,xt) and pt|0(nt + 1|n0) and pt|0(nt|n0) are both easily calculable distributions from the forward dimension deletion process.\n3.5 Sampling Algorithm 1: Sampling the Generative Process t\u2190 T X \u223c pref(X) = I{n = 1}N (x; 0, Id) while t > 0 do\nif u < \u2190\u2212 \u03bb \u03b8t (X)\u03b4t with u \u223c U(0, 1) then\nSample xadd, i \u223c A\u03b8t (xadd, i|X) X\u2190 ins(X,xadd, i)\nend x\u2190 x\u2212 \u2190\u2212 b \u03b8t (X)\u03b4t+ gt \u221a \u03b4t\u03f5 with \u03f5 \u223c N (0, Ind)\nX\u2190 (n,x), t\u2190 t\u2212 \u03b4t end To sample the generative process, we numerically integrate the learned backward jump diffusion process using time-step \u03b4t. Intuitively, it is simply the standard continuous time diffusion sampling scheme [3] but at each timestep we check whether a jump has occurred and if it has, sample the new component and insert it at the chosen index as explained by Algorithm 1."
        },
        {
            "heading": "4 Related Work",
            "text": "Our method jointly generates both dimensions and state values during the generative process whereas prior approaches [8, 11] are forced to first sample the number of dimensions and then run the diffusion process in this fixed dimension. When diffusion guidance is applied to these unconditional models [14, 26], users need to pick by hand the number of dimensions independent of the conditioning information even though the number of dimensions can be correlated with the conditioning parameter.\nInstead of automatically learning when and how many dimensions to add during the generative process, previous work focusing on images [27, 28] hand pick dimension jump points such that the\nresolution of images is increased during sampling and reaches a certain pre-defined desired resolution at the end of the generative process. Further, rather than using any equivalent of A\u03b8t , the values for new dimensions are simply filled in with Gaussian noise. These approaches mainly focus on efficiency rather than flexible generation as we do here.\nThe first term in our learning objective in Proposition 2 corresponds to learning the continuous part of our process (the diffusion) and the second corresponds to learning the discrete part of our process (the jumps). The first term can be seen as a trans-dimensional extension of standard denoising score matching [20] whilst the second bears similarity to the discrete space ELBO derived in [29].\nFinally, jump diffusions also have a long history of use in Bayesian inference, where one aims to draw samples from a trans-dimensional target posterior distribution based on an unnormalized version of its density [30]: an ergodic jump diffusion is designed which admits the target as the invariant distribution [30\u201332]. The invariant distribution is not preserved when time-discretizing the process. However, it was shown in [33, 34] how general jump proposals could be built and how this process could be \u201cMetropolized\u201d to obtain a discrete-time Markov process admitting the correct invariant distribution, yielding the popular Reversible Jump Markov Chain Monte Carlo algorithm. Our setup differs significantly as we only have access to samples in the form of data, not an unnormalized target."
        },
        {
            "heading": "5 Experiments",
            "text": ""
        },
        {
            "heading": "5.1 Molecules",
            "text": "We now show how our model provides significant benefits for diffusion guidance and interpolation tasks. We model the QM9 dataset [35, 36] of 100K varying size molecules. Following [8], we consider each molecule as a 3-dimensional point cloud of atoms, each atom having the features: (x, y, z) coordinates, a one-hot encoded atom type, and an integer charge value. Bonds are inferred from inter-atomic distances. We use an EGNN [24] backbone with three heads to predict s\u03b8t , p \u03b8 0|t(n0|Xt), and A\u03b8t . We uniformly delete dimensions, K del(i|n) = 1/n, and since a point cloud is permutation invariant, A\u03b8t (y add|Xt) need only predict new dimension values. We set \u2192 \u03bb t to a constant except for t < 0.1T , where we set\u2192\u03bb t<0.1T = 0. This ensures that all dimensions are added with enough generation time remaining for the diffusion process to finalize all state values.\nWe visualize sampling from our learned generative process in Figure 2; note how the process jointly creates a suitable number of atoms whilst adjusting their positions and identities. Before moving on to apply diffusion guidance which is the focus of our experiments, we first verify our unconditional sample quality in Table 2 and find we perform comparably to the results reported in [8] which use an FDDM. We ablate our choice of\u2192\u03bb t by comparing with setting \u2192 \u03bb t to a constant for all t and with setting \u2192\u03bb t = 0 for t < 0.9T (rather than just for t < 0.1T ). We find that the constant \u2192 \u03bb t performs worse due to the occasional component being added late in the generation process without enough time for the diffusion process to finalize its value. We find the\u2192\u03bb t<0.9T = 0 setting to have satisfactory sample quality however this choice of \u2192\u03bb t introduces issues during diffusion guided generation as we see next. Finally, we ablate the parameterization of Proposition 3 by learning \u2190\u2212 \u03bb \u03b8t (Xt) \u2208 R directly as the output of a neural network head. We find that this reduces sample quality due to the more well-behaved nature of the target, p\u03b80|t(n0|Xt) when using Proposition 3. We note pure autoregressive models perform significantly worse than diffusion based models as found in [8]."
        },
        {
            "heading": "5.1.1 Trans-Dimensional Diffusion Guidance",
            "text": "We now apply diffusion guidance to our unconditional model in order to generate molecules that contain a certain number of desired atom types, e.g. 3 carbons or 1 oxygen and 2 nitrogens. The distribution of molecule sizes changes depending on these conditions. We generate molecules conditioned on these properties by using the reconstruction guided sampling approach introduced in [9]. This method augments the score s\u03b8t (Xt) such that it approximates \u2207xt log pt(Xt|y) rather than \u2207xt log pt(Xt) (where y is the conditioning information) by adding on a term approximating \u2207xt log pt(y|Xt) with pt(y|Xt) = \u2211 n0 \u222b x0\np(y|X0)p0|t(X0|Xt)dx0. This guides xt such that it is consistent with y. Since \u2190\u2212 \u03bb \u03b8t (Xt) has access to xt, it will cause nt to automatically also be consistent with y without the user needing to input any information on how the conditioning information relates to the size of the datapoints. We give further details on diffusion guidance in Appendix C.\nWe show our results in Table 3. In order to perform guidance on the FDDM baseline, we implement the model from [8] in continuous time and initialize the dimension from the empirically observed dimension distribution in the dataset. This accounts for the case of an end user attempting to guide a unconditional model with access to no further information. We find that TDDM produces samples whose dimensions much more accurately reflect the true conditional distribution of dimensions given the conditioning information. The\u2192\u03bb t<0.9T = 0 ablation on the other hand only marginally improves the dimension error over FDDM because all dimensions are added in the generative process at a time when Xt is noisy and has little relation to the conditioning information. This highlights the necessity of allowing dimensions to be added throughout the generative process to gain the trans-dimensional diffusion guidance ability. The ablation with constant\u2192\u03bb t has increased dimension error over TDDM as we find that when\u2192\u03bb t > 0 for all t, \u2190\u2212 \u03bb \u03b8t can become very large when t is close to 0 when the model has perceived a lack of dimensions. This occasionally results in too many dimensions being added hence an increased dimension error. Not using the Proposition 3 parameterization also increases dimension error due to the increased difficulty in learning \u2190\u2212 \u03bb \u03b8t .\n5.1.2 Trans-Dimensional Interpolation\nInterpolations are a unique way of gaining insights into the effect of some conditioning parameter on a dataset of interest. To create an interpolation, a conditional generative model is first trained and then sampled with a sweep of the conditioning parameter but using fixed random noise [8]. The resulting series of synthetic datapoints share similar features due to the fixed random noise but vary in ways that are very informative as to the effect of the conditioning parameter. Attempting to interpolate with an FDDM is fundamentally limited because the entire interpolation occurs in the same dimension which is unrealistic when the conditioning parameter is heavily correlated with the dimension of the datapoint. We demonstrate this by following the setup of [8] who train a conditional FDDM conditioned on polarizability. Polariz-\nability is the ability of a molecule\u2019s electron cloud to distort in response to an external electric field [38] with larger molecules tending to have higher polarizability. To enable us to perform a trans-dimensional interpolation, we also train a conditional version of our model conditioned on polarizability. An example interpolation with this model is shown in Figure 4. We find that indeed the size of the molecule increases with increasing polarizability, with some molecular substructures e.g. rings, being maintained across dimensions. We show how the dimension changes with polarizability during 3 interpolations in Figure 3. We find that these match the true dataset statistics much more accurately than interpolations using FDDM which first pick a dimension and carry out the entire interpolation in that fixed dimension."
        },
        {
            "heading": "5.2 Video",
            "text": "We finally demonstrate our model on a video modeling task. Specifically we model the RoboDesk dataset [39], a video benchmark to measure the applicability of video models for planning and control problems. The videos are renderings of a robotic arm [40] performing a variety of different tasks including opening drawers and moving objects. We first train an unconditional model on videos of varying length and then perform planning by applying diffusion guidance to generate videos conditioned on an initial starting frame and a final goal frame [41]. The planning problem is then reduced to \u201cfilling in\u201d the frames in between. Our trans-dimensional model automatically varies the number of in-filled frames during generation so that the final length of video matches the length of time the task should take, whereas the fixed dimension model relies on the unrealistic assumption that the length of time the task should take is known before generation.\nWe model videos at 32\u00d7 32 resolution and with varying length from 2 to 35 frames. For the network backbone, we use a UNet adapted for video [42]. In contrast to molecular point clouds, our data is no longer permutation invariant hence A\u03b8t (y\nadd, i|Xt) includes a prediction over the location to insert the new frame. Full experimental details are provided in Appendix D. We evaluate our approach on three planning tasks, holding stationary, sliding a door and pushing an object. An example generation conditioned on the first and last frame for the slide door task is shown in Figure 5, with\nthe model in-filling a plausible trajectory. We quantify our model\u2019s ability to generate videos of a length appropriate to the task in Table 4 finding on all three tasks we generate a more accurate length of video than FDDM which is forced to sample video lengths from the unconditional empirically observed length distribution in the training dataset."
        },
        {
            "heading": "6 Discussion",
            "text": "In this work, we highlighted the pitfalls of performing generative modeling on varying dimensional datasets when treating state values and dimensions completely separately. We instead proposed a trans-dimensional generative model that generates both state values and dimensions jointly during the generative process. We detailed how this process can be formalized with the time-reversal of a jump diffusion and derived a novel evidence lower bound training objective for learning the generative process from data. In our experiments, we found our trans-dimensional model to provide significantly better dimension generation performance for diffusion guidance and interpolations when conditioning on properties that are heavily correlated with the dimension of a datapoint. We believe our approach can further enable generative models to be applied in a wider variety of domains where previous restrictive fixed dimension assumptions have been unsuitable."
        },
        {
            "heading": "7 Acknowledgements",
            "text": "The authors are grateful to Martin Buttenschoen for helpful discussions. AC acknowledges support from the EPSRC CDT in Modern Statistics and Statistical Machine Learning (EP/S023151/1). AD acknowledges support of the UK Dstl and EPSRC grant EP/R013616/1. This is part of the collaboration between US DOD, UK MOD and UK EPSRC under the Multidisciplinary University Research Initiative. He also acknowledges support from the EPSRC grants CoSines (EP/R034710/1) and Bayes4Health (EP/R018561/1). WH and CW acknowledge the support of the Natural Sciences and Engineering Research Council of Canada (NSERC), the Canada CIFAR AI Chairs Program. This material is based upon work supported by the United States Air Force Research Laboratory (AFRL) under the Defense Advanced Research Projects Agency (DARPA) Data Driven Discovery Models (D3M) program (Contract No. FA8750-19-2-0222) and Learning with Less Labels (LwLL) program (Contract No.FA8750-19-C-0515). Additional support was provided by UBC\u2019s Composites Research Network (CRN), Data Science Institute (DSI) and Support for Teams to Advance Interdisciplinary Research (STAIR) Grants. This research was enabled in part by technical support and computational resources provided by WestGrid (https://www.westgrid.ca/) and Compute Canada (www.computecanada.ca). The authors would like to acknowledge the use of the University of Oxford Advanced Research Computing (ARC) facility in carrying out this work. http://dx.doi.org/10.5281/zenodo.22558"
        },
        {
            "heading": "Appendix",
            "text": "This appendix is organized as follows. In Section A, we present proofs for all of our propositions. Section A.1 presents a rigorous definition of our forward process using a more specific notation. This is then used in Section A.2.1 to prove the time reversal for our jump diffusions. We also present an intuitive proof of the time reversal using notation from the main text in Section A.2.2. In Section A.3 we prove Proposition 2 using the notation from the main text. We prove Proposition 3 in Section A.4 and we analyse the optimum of our objective directly without using stochastic process theory in Section A.5. In Section B we give more details on our objective and in Section C we detail how we apply diffusion guidance to our model. We give the full details for our experiments in Section D and finally, in Section E, we discuss the broader impacts of our work."
        },
        {
            "heading": "A Proofs",
            "text": ""
        },
        {
            "heading": "A.1 Notation and Setup",
            "text": "We here introduce a more rigorous notation for defining our trans-dimensional notation that will be used in a rigorous proof for the time-reversal of our jump diffusion. First, while it makes sense from a methodological and experimental point of view to present our setting as a transdimensional one, we slightly change the point of view in order to derive our theoretical results. We extend the space Rd to R\u0302d = Rd \u222a {\u221e} using the one-point compactification of the space. We refer to [43] for details on this space. The point\u221e will be understood as a mask. For instance, let x1, x2, x3 \u2208 Rd. Then X = (x1, x2, x3) \u2208 (R\u0302d)N with N = 3 corresponds to a vector for which all components are observed whereas X \u2032 = (x1,\u221e, x3) \u2208 (R\u0302d)N corresponds to a vector for which only the components on the first and third dimension are observed. The second dimension is masked in that case. Doing so, we will consider diffusion models on the space X = (R\u0302d)N with d,N \u2208 N. In the case of a video diffusion model, N can be seen as the max number of frames. We will always consider that this space is equipped with its Borelian sigma-field X and all probability measures will be defined on X . We denote dim : X\u2192 {0, 1}N which is given for any X = {xi}Ni=1 \u2208 X by\ndim(X) = {\u03b4Rd(xi)}Ni=1.\nIn other words, dim(X) is a binary vector identifying the \u201cdimension\u201d of the vector X , i.e. which frames are observed. Going back to our example X = (x1, x2, x3) \u2208 (R\u0302d)N and X \u2032 = (x1,\u221e, x3) \u2208 (R\u0302d)N , we have that dim(X) = {1, 1, 1} and dim(X \u2032) = {1, 0, 1}. For any vector u \u2208 {0, 1}N we denote |u| = \u2211N i=1 ui, i.e. the active dimensions of u (or equivalently the nonmasked frames). For any X \u2208 X and D \u2208 {0, 1}N , we denote XD = {X \u2032i}Ni=1 with X \u2032i = Xi if Di = 1 and X \u2032i =\u221e if Di = 0.\nWe denote Ckb (Rd,R) the set of functions which are k differentiable and bounded. Similarly, we denote Ckb (Rd,R) the set of functions which are k differentiable and compactly supported. The set Ck0(Rd,R) denotes the functions which are k differentiable and vanish when \u2225x\u2225 \u2192 +\u221e. We note that f \u2208 C(R\u0302d), if f \u2208 C(Rd) and f \u2212 f(\u221e) \u2208 C0(Rd) and that f \u2208 Ck(R\u0302d) for any k \u2208 N if the restriction of f to Rd is in Ck(Rd) and f \u2208 C(R\u0302d)."
        },
        {
            "heading": "A.1.1 Transdimensional infinitesimal generator",
            "text": "To introduce rigorously the transdimensional diffusion model defined in Section 3.1, we will introduce its infinitesimal generator. The infinitesimal generator of a stochastic process can be roughly defined as its \u201cprobabilistic derivative\u201d. More precisely, assume that a stochastic process (Xt)t\u22650 admits a transition semigroup (Pt)t\u22650, i.e. for any t \u2265 0, A \u2208 X and X \u2208 X we have P(Xt \u2208 A | X0 = x) = Pt(x,A), then the infinitesimal generator is defined asA(f) = limt\u21920(Pt(f)\u2212f)/t, for every f for which this quantity is well-defined.\nHere, we start by introducing the infinitesimal generator of interest and give some intuition about its form. Then, we prove a time-reversal formula for this infinitesimal generator.\nWe consider b : Rd \u2192 Rd, \u03b1 : {0, 1}NM \u2192 R+. For any f \u2208 C2(X) and X \u2208 X we define A(f)(X) = \u2211N\ni=1{\u27e8b(Xi),\u2207xif(X)\u27e9+ 1 2\u2206xif(X)}\u03b4Rd(Xi) (5) \u2212 \u2211\nD1\u2282D \u22060 0 \u00b7 \u00b7 \u00b7 \u2211 DM\u2282D \u2206M\u22121 M\u22121 \u03b1(D0, . . . ,DM ) \u2211M\u22121 i=0 (f(X)\u2212 f(XDi+1))\u03b4Di(dim(X)),\nwhere M \u2208 N, D0 = {1}N , {\u2206j}M\u22121j=0 \u2208 NM such that \u2211M\u22121\nj=0 \u2206j < N and for any j \u2208 {0, . . . ,M\u22121}, D\u2206jj is the subset of {0, 1}{1,...,N} such that Dj+1 \u2208 D \u2206j j if and only if Dj \u00b7Dj+1 = Dj+1, where \u00b7 is the pointwise multiplication operator, and |Dj | = |Dj+1| + \u2206j . The condition Dj \u00b7 Dj+1 = Dj+1 means that the non-masked dimensions in Dj+1 are also non-masked dimensions in Dj . The condition |Dj | = |Dj+1|+\u2206j means that in order to go from Dj to Dj+1, one needs to mask exactly \u2206j dimensions.\nTherefore, a sequence {\u2206j}M\u22121j=0 \u2208 NM such that \u2211M\u22121 j=0 \u2206j < N can be interpreted as a sequence\nof drops in dimension. At the core level, we have that |DM | = N \u2212 \u2211M\u22121\nj=0 \u2206j . For instance if |DM | = 1, we have that at the end of the process, only one dimension is considered. We choose \u03b1 such that \u2211 D1\u2282D \u22060 0 \u00b7 \u00b7 \u00b7 \u2211 DM\u2282D \u2206M\u22121 M\u22121 \u03b1(D0, . . . ,DM ) = 1. Therefore, \u03b1(D0, . . . ,DM ) corresponds to the probability to choose the dimension path D0 \u2192 \u00b7 \u00b7 \u00b7 \u2192 DM . The part X 7\u2192 \u27e8b(Xi),\u2207xif(X)\u27e9+ 12\u2206xif(X) is more classical and corresponds to the continuous part of the diffusion process. We refer to [44] for a thorough introduction on infinitesimal generators. For simplicity, we omit the schedule coefficients in (5)."
        },
        {
            "heading": "A.1.2 Justification of the form of the infinitesimal generator",
            "text": "For any dimension path P = D0 \u2192 \u00b7 \u00b7 \u00b7 \u2192 DM (recall that D0 = {1}N ), we define the jump kernel JP as follows. For any x \u2208 X, we have JP(X,dY ) = \u2211M\u22121 i=0 \u03b4Di(dim(X))\u03b4XDi+1 (dY ). This operator corresponds to the deletion operator introduced in Section 3.1 . Hence, for any dimension path P = D0 \u2192 \u00b7 \u00b7 \u00b7 \u2192 DM , we can define the associated infinitesimal generator: for any f \u2208 C2(X) and X \u2208 X we define\nAP(f)(X) = \u2211N\ni=1{\u27e8b(xi),\u2207xif(X)\u27e9+ 1 2\u2206xif(X)}\u03b4Rd(Xi) + \u222b X (f(Y )\u2212 f(X))JP(X,dY ).\nWe can define the following jump kernel J = \u2211\nD1\u2282D \u22060 0 \u00b7 \u00b7 \u00b7 \u2211 DM\u2282D \u2206M\u22121 M\u22121 \u03b1(D0, . . . ,DM )JP.\nThis corresponds to averaging the jump kernel over the different possible dimension paths. We have that for any f \u2208 C2(X) and X \u2208 X\nA(f)(X) = \u2211N\ni=1{\u27e8b(xi),\u2207xif(X)\u27e9+ 1 2\u2206xif(X)}\u03b4Rd(Xi) + \u222b X (f(Y )\u2212 f(X))J(X,dY ).\n(6) In other words, A = \u2211\nD1\u2282D \u22060 0 \u00b7 \u00b7 \u00b7 \u2211 DM\u2282D \u2206M\u22121 M\u22121 \u03b1(D0, . . . ,DM )AP.\nIn what follows, we assume that there exists a Markov process (Xt)t\u22650 with infinitesimal generator A. In order to sample from (Xt)t\u22650, one choice is to first sample the dimension path P according to the probability \u03b1. Second sample from the Markov process associated with the infinitesimal generator AP. We can approximately sample from this process using the Lie-Trotter-Kato formula [44, Corollary 6.7, p.33].\nDenote (Pt)t\u22650 the semigroup associated with AP, (Qt)t\u22650 the semigroup associated with the continuous part of AP and (Jt)t\u22650 the semigroup associated with the jump part of AP. More precisely, we have that, (Qt)t\u22650 is associated with Acont such that for any f \u2208 C2(X) and X \u2208 X\nAcont(f)(X) = \u2211N i=1{\u27e8b(Xi),\u2207xif(X)\u27e9+ 1 2\u2206xif(X)}.\nIn addition, we have that, (Qt)t\u22650 is associated with APjump such that for any f \u2208 C2(X) and X \u2208 X APjump(f)(X) = \u222b X (f(Y )\u2212 f(X))JP(X,dY ).\nFirst, note that Acont corresponds to the infinitesimal generator of a classical diffusion on the components which are not set to\u221e. Hence, we can approximately sample from (Qt)t\u22650 by sampling according to the Euler-Maruyama discretization of the associated diffusion, i.e. by setting\nXt \u2248 X0 + tb(X0) + \u221a tZ, (7)\nwhere Z is a Gaussian random variable.\nSimilarly, in order to sample from (Jt)t\u22650, one should sample from the jump process defined as follows. On the interval [0, \u03c4), we have Xt = X0. At time \u03c4 , we define X1 \u223c J(X0, \u00b7) and repeat the procedure. In this case \u03c4 is defined as an exponential random variable with parameter 1. For t > 0 small enough the probability that t > \u03c4 is of order t. Therefore, we sample from J, i.e. the deletion kernel, with probability t. Combining this approximation and (7), we get approximate samplers for (Qt)t\u22650 and (Jt)t\u22650. Under mild assumptions, the Lie-Trotter-Kato formula ensures that for any t \u2265 0\nPt = lim n\u2192+\u221e\n(Qt/nJt/n) n.\nThis justifies sampling according to Algorithm 1 (in the case of the forward process)."
        },
        {
            "heading": "A.2 Proof of Proposition 1",
            "text": "For the proof of Proposition 1, we first provide a rigorous proof using the notation introduced in A.1. We then follow this with a second proof that aims to be more intuitive using the notation used in the main paper.\nA.2.1 Time-reversal for the transdimensional infinitesimal generator and Proof of Proposition 1\nWe are now going to derive the formula for the time-reversal of the transdimensional infinitesimal generatorA, see (5). This corresponds to a rigorous proof of Proposition 1. We refer to Section A.2.2 for a more intuitive, albeit less-rigorous, proof. We start by introducing the kernel KP given for any dimension path D0 \u2192 \u00b7 \u00b7 \u00b7 \u2192 DM , for any i \u2208 {0, . . . ,M \u2212 1}, Y \u2208 Di+1 and A \u2208 X by\nKP(Y,A) = \u2211M\u22121 i=0 \u03b4Di+1(dim(Y )) \u222b A\u2229Di pt((XDi\\Di+1 ,YDi+1 )|dim(Xt)=Di)P(dim(Xt)=Di) pt(YDi+1 |dim(Xt)=Di+1)P(dim(Xt)=Di+1) dXDi\\Di+1 .\nNote that this kernel is the same as the one considered in Proposition 1. It is well-defined under the following assumption. Assumption 1. For any t > 0 and D \u2282 {0, 1}N , we have that Xt conditioned to dim(Xt) = D admits a density w.r.t. the |D|d-dimensional Lebesgue measure, denoted pt(\u00b7|dim(Xt) = D).\nThe following result will be key to establish the time-reversal formula. Lemma 1. Assume A1. Let A,B \u2208 X . Let P be a dimension path D0 \u2192 \u00b7 \u00b7 \u00b7 \u2192 DM with M \u2208 N. Then, we have\nE[1A(Xt)JP(Xt,B)] = E[1B(Xt)KP(Xt,A)].\nProof. Let A,B \u2208 X . We have E[1A(Xt)JP(Xt,B)] = \u2211M\u22121 i=0 E[1A(Xt)\u03b4Di(dim(Xt))1B((Xt)Di+1)]\n= \u2211M\u22121\ni=0 \u222b A\u2229Di pt(XDi |dim(Xt) = Di)P(dim(Xt) = Di)1B(XDi+1)dXDi\n= \u2211M\u22121\ni=0 \u222b A\u2229Di pt(XDi |dim(Xt) = Di)P(dim(Xt) = Di)1B(XDi+1)dXDi+1dXDi\\Di+1\n= \u2211M\u22121\ni=0 \u222b B\u2229Di+1 1B(XDi+1)\n\u00d7 ( \u222b A\u2229Di 1A(XDi)pt(XDi |dim(Xt) = Di)P(dim(Xt) = Di)dXDi\\Di+1)dXDi+1\n= \u2211M\u22121\ni=0 \u222b B\u2229Di+1 1B(XDi+1)\n\u00d7KP(XDi+1 ,A)pt(XDi+1 |dim(Xt) = Di+1)P(dim(Xt) = Di+1)dXDi+1 = \u2211M\u22121 i=0 E[\u03b4Di+1(dim(Xt))KP(Xt,A)1B(Xt)],\nwhich concludes the proof.\nLemma 1 shows that KP verifies the flux equation associated with JP. The flux equation is the discrete state-space equivalent of the classical time-reversal formula for continuous state-space. We refer to [45] for a rigorous treatment of time-reversal with jumps under entropic conditions.\nWe are also going to consider the following assumption which ensures that the integration by part formula is valid. Assumption 2. For any t > 0 and i \u2208 {1, . . . , N}, Xt admits a smooth density w.r.t. the Nddimensional Lebesgue measure denoted pt and we have that for any f, h \u2208 C2b((Rd)N ) for any u \u2208 [0, t] and i \u2208 {1, . . . , N}\nE[\u03b4Rd((Xu)i)\u27e8\u2207xif(Xu),\u2207xih(Xu)\u27e9] = \u2212E[\u03b4Rd((Xu)i)h(Xu)(\u2206xif(Xu) + \u27e8\u2207xi log pu(Xu),\u2207xif(Xu)\u27e9)].\nThe second assumption ensures that we can apply the backward Kolmogorov evolution equation. Assumption 3. For any g \u2208 C2(X) and t > 0, we have that for any u \u2208 [0, t] and X \u2208 X, \u2202ug(u,X)+A(g)(u,X) = 0, where for any u \u2208 [0, t] and X \u2208 X, g(u,X) = E[g(Xt) |Xu = X].\nWe refer to [19] for conditions under A2 and A3 are valid in the setting of diffusion processes. Proposition 4. Assume A1, A2 and A3. Assume that there exists a Markov process (Xt)t\u22650 solution of the martingale problem associated with (6). Let T > 0 and consider (Yt)t\u2208[0,T ] = (XT\u2212t)t\u2208[0,T ]. Then (Yt)t\u2208[0,T ] is solution to the martingale problem associated withR, where for any f \u2208 C2(X), t \u2208 (0, T ) and x \u2208 X we have\nR(f)(t,X) = \u2211N\ni=1{\u2212\u27e8b(Xi) +\u2207xi log pt(X),\u2207xif(X)\u27e9+ 1 2\u2206xif(X)}\u03b4Rd(Xi) + \u222b X (f(Y )\u2212 f(X))K(X,dY ).\nProof. Let f, g \u2208 C2(X). In what follows, we show that for any s, t \u2208 [0, T ] with t \u2265 s E[(f(Yt)\u2212 f(Ys))g(Ys)] = E[g(Ys) \u222b t s R(f)(u,Yu)du].\nMore precisely, we show that for any s, t \u2208 [0, T ] with t \u2265 s E[(f(Xt)\u2212 f(Xs))g(Xt)] = E[\u2212g(Xt) \u222b t s R(f)(u,Xu)du].\nLet s, t \u2208 [0, T ], with t \u2265 s. Next, we denote for any u \u2208 [0, t] and X \u2208 X, g(u,X) = E[g(Xt) | Xu = X]. Using A3, we have that for any u \u2208 [0, t] and X \u2208 X, \u2202ug(u,X) + A(g)(u,X) = 0, i.e. g satisfies the backward Kolmogorov equation. For any u \u2208 [0, t] and X \u2208 X, we have\nA(fg)(u,X) = \u2202ug(u,X)f(X) + \u2211N i=1(\u27e8b(Xi),\u2207xig(u,X)\u27e9+ 1 2\u2206xig(u,Xi))f(X)\u03b4Rd(Xi)\n+ \u2211N\ni=1(\u27e8b(Xi),\u2207xif(X)\u27e9+ 1 2\u2206xif(X))g(u,X)\u03b4Rd(Xi) + \u2211N\ni=1 \u03b4Rd(Xi)\u27e8\u2207xif(X),\u2207xig(u,X)\u27e9+ J(X, fg) = \u2202ug(u,X)f(X) +A(g)(u,X)f(X) + J(X, fg)\u2212 J(X, g)f(X)\n+ \u2211N\ni=1(\u27e8b(Xi),\u2207xif(X)\u27e9+ 1 2\u2206xif(X))g(u,X)\u03b4Rd(Xi) + \u2211N\ni=1 \u03b4Rd(Xi)\u27e8\u2207xif(X),\u2207xig(u,X)\u27e9 = \u2211N\ni=1(\u27e8b(Xi),\u2207xif(X)\u27e9+ 1 2\u2206xif(X))g(u,X)\u03b4Rd(Xi) + \u2211N\ni=1 \u03b4Rd(Xi)\u27e8\u2207xif(X),\u2207xig(u,X)\u27e9+ J(X, fg)\u2212 J(X, g)f(X). (8)\nUsing A2, we have that for any u \u2208 [0, t] and i \u2208 {1, . . . , N}\nE[\u03b4Rd((Xu)i)\u27e8\u2207xif(Xu),\u2207xig(u,Xu)\u27e9] = \u2212E[\u03b4Rd((Xu)i)g(u,Xu)(\u2206xif(Xu) + \u27e8\u2207xi log pu(Xu),\u2207xif(Xu)\u27e9)]. (9)\nIn addition, we have that for any X \u2208 X and u \u2208 [0, t], J(X, fg) \u2212 J(X, g)f(X) =\u222b X g(u, Y )(f(Y )\u2212 f(X))J(X,dY ). Using Lemma 1, we get\nE[J(Xu, fg)\u2212 J(Xu, f)g(u,Xu)] = \u2212E[g(u,Xu)K(Xu, f)]. (10)\nTherefore, using (8), (9) and (10), we have E[A(fg)(u,Xu)] = E[\u2212R(f)(u,Xu)g(u,Xu)].\nFinally, we have E[(f(Xt)\u2212 f(Xs))g(Xt)] = E[g(t,Xt)f(Xt)\u2212 f(Xs)g(s,Xs)]\n= E[ \u222b t s A(fg)(u,Xu)du]\n= \u2212E[ \u222b t s R(f)(u,Xu)g(u,Xu)du] = \u2212E[g(Xt) \u222b t s R(f)(u,Xu)du],\nwhich concludes the proof.\nA.2.2 Intuitive Proof of Proposition 1\nWe recall Proposition 1. Proposition 1. The time reversal of a forward jump diffusion process given by drift \u2212\u2192 b t, diffusion coefficient \u2212\u2192g t, rate \u2192 \u03bb t(n) and transition kernel \u2211n i=1 K\ndel(i|n)\u03b4del(X,i)(Y) is given by a jump diffusion process with drift \u2190\u2212 b \u2217t (X), diffusion coefficient \u2190\u2212g \u2217t , rate \u2190\u2212 \u03bb \u2217t (X) and transition kernel\u222b\nyadd \u2211n+1 i=1 A \u2217 t (y\nadd, i|X)\u03b4ins(X,yadd,i)(Y)dyadd as defined below \u2190\u2212 b \u2217t (X) = \u2212\u2192 b t(X)\u2212\u2207x log pt(X), \u2190\u2212g \u2217t = \u2212\u2192g t,\n\u2190\u2212 \u03bb \u2217t (X) = \u2192 \u03bb t(n+ 1)\n\u2211n+1 i=1 K del(i|n+ 1) \u222b yadd pt(ins(X,yadd, i))dyadd\npt(X) ,\nA\u2217t (y add, i|X) \u221d pt(ins(X,yadd, i))Kdel(i|n+ 1).\nDiffusion part. Using standard diffusion models arguments such as [18] [45], we get \u2190\u2212 b \u2217t (X) = \u2212\u2192 b t(X)\u2212\u2207x log pt(X|n).\nJump part. We use the flux equation from [45] which intuitively relates the probability flow going in the forward direction with the probability flow going the backward direction with equality being achieved at the time reversal.\npt(X) \u2190\u2212 \u03bb \u2217t (X) \u2190\u2212 K\u2217t (Y|X) = pt(Y) \u2192 \u03bb t(Y) \u2212\u2192 K t(X|Y) pt(X) \u2190\u2212 \u03bb \u2217t (X) \u222b yadd \u2211n+1 i=1 A \u2217 t (y add, i|X)\u03b4ins(X,yadd,i)(Y)dyadd\n= pt(Y) \u2192 \u03bb t(Y) \u2211n+1 i=1 K del(i|n+ 1)\u03b4del(Y,i)(X). (11)\nTo find \u2190\u2212 \u03bb \u2217t (X), we sum and integrate both sides over m and y, with Y = (m,y),\u2211N\nm=1 \u222b y\u2208Rmd pt(X) \u2190\u2212 \u03bb \u2217t (X) \u222b yadd \u2211n+1 i=1 A \u2217 t (y add, i|X)\u03b4ins(X,yadd,i)(Y)dyadddy\n= \u2211N\nm=1 \u222b y\u2208Rmd pt(Y) \u2192 \u03bb t(Y) \u2211n+1 i=1 K\ndel(i|n+ 1)\u03b4del(Y,i)(X)dy. Now we use the fact that \u03b4del(Y,i)(X) is 0 for any m \u0338= n+ 1,\npt(X) \u2190\u2212 \u03bb \u2217t (X) = \u2192 \u03bb t(n+ 1) \u222b y\u2208R(n+1)d pt(Y) \u2211n+1 i=1 K del(i|n+ 1)\u03b4del(Y,i)(X)dy\n= \u2192 \u03bb t(n+ 1) \u2211n+1 i=1 K del(i|n+ 1) \u222b y\u2208R(n+1)d pt(Y)\u03b4del(Y,i)(X)dy.\nNow letting Y = ins(X,yadd, i),\npt(X) \u2190\u2212 \u03bb \u2217t (X) = \u2192 \u03bb t(n+ 1) \u2211n+1 i=1 K del(i|n+ 1) \u222b yadd pt(ins(X,yadd, i))dyadd\n\u2190\u2212 \u03bb \u2217t (X)x = \u2192 \u03bb t(n+ 1)\n\u2211n+1 i=1 K del(i|n+1) \u222b yadd pt(ins(X,y add,i))dyadd\npt(X) .\nTo find A\u2217t (y add, i|X), we start from (11) and set Y = ins(X, zadd, j) to get\npt(X) \u2190\u2212 \u03bb \u2217t (X)A \u2217 t (z add, j|X) = pt(Y) \u2192 \u03bb t(n+ 1)K\ndel(j|n+ 1). By inspection, we see immediately that\nA\u2217t (z add, j|X) \u221d pt(ins(X, zadd, j)Kdel(j|n+ 1).\nWith a re-labeling of zadd and j we achieve the desired form A\u2217t (y add, i|X) \u221d pt(ins(X,yadd, i))Kdel(i|n+ 1)."
        },
        {
            "heading": "A.3 Proof of Proposition 2",
            "text": "In this section we prove Proposition 2 using the notation from the main paper by following the framework of [17]. We operate on a state space X = \u22c3N n=1{n} \u00d7 Rnd. On this space the gradient operator \u2207 : C(X ,R) \u2192 C(X ,X ) is defined as \u2207f(X) = \u2207(nd)x f(X) where \u2207(nd)x is the standard gradient operator defined as C(Rnd,R) \u2192 C(Rnd,Rnd) with respect to x \u2208 Rnd. We will write integration with respect to a probability measure defined on X as an explicit sum over the number of components and integral over Rnd with respect to a probability density defined on Rnd i.e. \u222b X f(X)\u00b5(dX) = \u2211N n=1 \u222b x\u2208Rnd f(X)p(n)p(x|n)dx where, for A \u2282 R\nnd,\u222b (n,A) \u00b5(dX) = \u222b x\u2208A p(n)p(x|n)dx. We will write p(X) as shorthand for p(n)p(x|n).\nFollowing, [17], we start by augmenting our space with a time variable so that operators become time inhomegeneous on the extended space. We write this as X\u0304 = (X, t) where X\u0304 lives in the extended space S = X \u00d7 R\u22650. In the proof, we use the infinitesimal generators for the the forward and backward processes. An infinitesimal generator is defined as\nA(f)(X\u0304) = lim t\u21920 Ept|0(Y\u0304|X\u0304)[f(Y\u0304)]\u2212 f(X\u0304) t\nand can be understood as a probabilistic version of a derivative. For our process on the augmented space S , our generators decompose asA = \u2202t+A\u0302t where A\u0302t operates only on the spatial components of X\u0304 i.e. X [17].\nWe now define the spatial infinitesimal generators for our forward and backward process. We will change our treatment of the time variable compared to the main text. Both our forward and backward processes will run from t = 0 to t = T , with the true time reversal of X following the forward process satisfying (Yt)t\u2208[0,T ] = (XT\u2212t)t\u2208[0,T ]. Further, we will write\n\u2212\u2192g t as gt and\u2190\u2212g t = gT\u2212t as we do not learn g and this is the optimal relation from the time reversal. We define L\u0302t(f)(X) = \u2212\u2192 b t(X)\u00b7\u2207f(X)+ 12g 2 t\u2206f(X)+ \u2192 \u03bb t(X) \u2211N m=1 \u222b y\u2208Rmd f(Y)( \u2212\u2192 K t(Y|X)\u2212\u03b4X(Y))dy,\nas well as K\u0302t(f)(X) = \u2190\u2212 b \u03b8t (X)\u00b7\u2207f(X)+ 12g 2 T\u2212t\u2206f(X)+ \u2190\u2212 \u03bb \u03b8t (X) \u2211N m=1 \u222b y\u2208Rmd f(Y)( \u2190\u2212 K\u03b8t (Y|X)\u2212\u03b4X(Y))dy\nwhere \u2206 = (\u2207\u00b7\u2207) is the Laplace operator and \u03b4 is a dirac delta onX i.e. \u2211N\nm=1 \u222b y\u2208Rmd \u03b4X(Y)dy =\n1 and \u2211N\nm=1 \u222b y\u2208Rmd f(Y)\u03b4X(Y)dy = f(X).\nVerifying Assumption 1. The first step in the proof is to verify Assumption 1 in [17]. Letting \u03bdt(X) = pT\u2212t(X), we assume we can write \u2202tpt(X) = K\u0302\u2217t pt(X) in the formM\u03bd + c\u03bd = 0 for some function c : S \u2192 R, whereM is the generator of another auxiliary process on S and K\u0302\u2217t is the adjoint operator which satisfies \u27e8K\u0302\u2217t f, h\u27e9 = \u27e8f, K\u0302th\u27e9 i.e.\u2211N\nn=1 \u222b x\u2208Rnd h(X)K\u0302 \u2217 t (f)(X)dx = \u2211N n=1 \u222b x\u2208Rnd f(X)K\u0302t(h)(X)dx\nWe now find K\u0302\u2217t . We start by substituting in the form for K\u0302t,\u2211N n=1 \u222b x\u2208Rnd f(X)K\u0302t(h)(X)dx = \u2211N n=1 \u222b x\u2208Rnd f(X){( \u2190\u2212 b \u03b8t (x) \u00b7 \u2207h)(X) + 12g 2 T\u2212t\u2206h(X)+\n\u2190\u2212 \u03bb \u03b8t (X) \u2211N m=1 \u222b y\u2208Rmd h(Y)( \u2190\u2212 K\u03b8t (Y|X)\u2212 \u03b4X(Y))dy}dx\nWe first focus on the RHS terms corresponding to the diffusion part of the process\u2211N n=1 \u222b x\u2208Rnd f(X){( \u2190\u2212 b \u03b8t \u00b7 \u2207h)(X) + 12g 2 T\u2212t\u2206h(X)}dx\n= \u2211N\nn=1 \u222b x\u2208Rnd f(X)( \u2190\u2212 b \u03b8t \u00b7 \u2207h)(X) + 12g 2 T\u2212tf(X)\u2207 \u00b7 \u2207h(X)dx\n= \u2211N\nn=1 \u222b x\u2208Rnd f(X)( \u2190\u2212 b \u03b8t \u00b7 \u2207h)(X) + 12g 2 T\u2212th(X)\u2207 \u00b7 \u2207f(X)dx\n= \u2211N\nn=1 \u222b x\u2208Rnd \u2212h(X)\u2207 \u00b7 (f \u2190\u2212 b \u03b8t )(X) + 1 2g 2 T\u2212th(X)\u2207 \u00b7 \u2207f(X)dx\n= \u2211N\nn=1 \u222b x\u2208Rnd h(X){\u2212\u2207 \u00b7 (f \u2190\u2212 b \u03b8t )(X) + 1 2g 2 T\u2212t\u2207 \u00b7 \u2207f(X)}dx\n= \u2211N\nn=1 \u222b x\u2208Rnd h(X){\u2212f(X)\u2207 \u00b7 \u2190\u2212 b \u03b8t (X)\u2212\u2207f(X) \u00b7 \u2190\u2212 b \u03b8t (X) + 1 2g 2 T\u2212t\u2207 \u00b7 \u2207f(X)}dx.\nwhere we apply integration by parts twice to arrive at the third line and once to arrive at the fourth line. We now focus on the RHS term corresponding to the jump part of the process\u2211N\nn=1 \u222b x\u2208Rnd f(X){ \u2190\u2212 \u03bb \u03b8t (X) \u2211N m=1 \u222b y\u2208Rmd h(Y)( \u2190\u2212 K\u03b8t (Y|X)\u2212 \u03b4X(Y))dy}dx\n= \u2211N\nm=1 \u222b y\u2208Rmd h(Y){ \u2211N n=1 \u222b x\u2208Rnd f(X) \u2190\u2212 \u03bb \u03b8t (X)( \u2190\u2212 K\u03b8t (Y|X)\u2212 \u03b4X(Y))dx}dy\n= \u2211N\nn=1 \u222b x\u2208Rnd h(X){ \u2211N m=1 \u222b y\u2208Rmd f(Y) \u2190\u2212 \u03bb \u03b8t (Y)( \u2190\u2212 K\u03b8t (X|Y)\u2212 \u03b4Y(X))dy}dx,\nwhere on the last line we have relabelled X to Y and Y to X. Putting both re-arranged forms for the RHS together, we obtain\u2211N\nn=1 \u222b x\u2208Rnd h(X)K\u0302\n\u2217 t (f)(X)dx =\u2211N\nn=1 \u222b x\u2208Rnd h(X){\u2212f(X)\u2207 \u00b7 \u2190\u2212 b \u03b8t (X)\u2212\u2207f(X) \u00b7 \u2190\u2212 b \u03b8t (X) + 1 2g\n2 T\u2212t\u2207 \u00b7 \u2207f(X)+\u2211N\nm=1 \u222b y\u2208Rmd f(Y) \u2190\u2212 \u03bb \u03b8t (Y)( \u2190\u2212 K\u03b8t (X|Y)\u2212 \u03b4Y(X))dy}dx.\nWe therefore have\nK\u0302\u2217t (f)(X) =\u2212 f(X)\u2207 \u00b7 \u2190\u2212 b \u03b8t (X)\u2212\u2207f(X) \u00b7 \u2190\u2212 b \u03b8t (X) + 1 2g\n2 T\u2212t\u2206f(X)+\u2211N\nm=1 \u222b y\u2208Rmd f(Y) \u2190\u2212 \u03bb \u03b8t (Y)( \u2190\u2212 K\u03b8t (X|Y)\u2212 \u03b4X(Y))dy.\nNow we re-write \u2202tpt(X) = K\u0302\u2217t pt(x) in the formM\u03bd + c\u03bd = 0. We start by re-arranging\n\u2202tpt(X) = K\u0302\u2217t pt(X) =\u21d2 0 = \u2202t\u03bdt(X) + K\u0302\u2217T\u2212t\u03bdt(X).\nSubstituting in our form for K\u0302\u2217t we obtain\n0 =\u2202t\u03bdt(X)\u2212 \u03bdt(X)\u2207 \u00b7 \u2190\u2212 b \u03b8T\u2212t(X)\u2212 \u2190\u2212 b \u03b8T\u2212t(X) \u00b7 \u2207\u03bdt(X) + 12g 2 t\u2206\u03bdt(X)\n+ \u2211N\nm=1 \u222b y\u2208Rmd \u03bdt(Y) \u2190\u2212 \u03bb \u03b8T\u2212t(Y)( \u2190\u2212 K\u03b8T\u2212t(X|Y)\u2212 \u03b4X(Y))dy (12)\nWe define our auxiliary process to have generatorM = \u2202t + M\u0302t with\nM\u0302t(f)(X) = bMt (X)\u00b7\u2207f(X)+ 12g 2 t\u2206f(X)+\u03bb M t (X) \u2211N m=1 \u222b y\u2208Rmd f(Y)(K M t (Y|X)\u2212\u03b4X(Y))dy\nwhich is a jump diffusion process with drift bMt , diffusion coefficient gt, rate \u03bb M t and transition kernel KMt . Then if we have b M t = \u2212 \u2190\u2212 b \u03b8T\u2212t,\n\u03bbMt (X) = \u2211N m=1 \u222b y\u2208Rmd \u2190\u2212 \u03bb \u03b8T\u2212t(Y) \u2190\u2212 K\u03b8T\u2212t(X|Y)dy, (13)\nand KMt (Y|X) \u221d \u2190\u2212 \u03bb \u03b8T\u2212t(Y) \u2190\u2212 K\u03b8T\u2212t(X|Y). (14)\nThen we have (12) can be rewritten as\n0 =\u2202t\u03bdt(X)\u2212 \u03bdt(X)\u2207 \u00b7 \u2190\u2212 b \u03b8T\u2212t(X) + b M t (X) \u00b7 \u2207\u03bdt(X) + 12g 2 t\u2206\u03bdt(X)\n\u2212 \u03bdt(X) \u2190\u2212 \u03bb \u03b8T\u2212t(X) + \u03bdt(X) \u2211N m=1 \u222b y\u2208Rmd \u2190\u2212 \u03bb \u03b8T\u2212t(Y) \u2190\u2212 K\u03b8T\u2212t(X|Y)dy+\n\u03bbMt (x) \u2211N m=1 \u222b y\u2208Rmd \u03bdt(Y)(K M t (Y|X)\u2212 \u03b4X(Y))dy\nwhich is in the formM(\u03bd)(X\u0304) + c(X\u0304)\u03bd(X\u0304) = 0 if we let\nc(X\u0304) = \u2212\u2207 \u00b7 \u2190\u2212 b \u03b8T\u2212t(X)\u2212 \u2190\u2212 \u03bb \u03b8T\u2212t(X) + \u2211N m=1 \u222b y\u2208Rmd \u2190\u2212 \u03bb \u03b8T\u2212t(Y) \u2190\u2212 K\u03b8T\u2212t(X|Y)dy.\nVerifying Assumption 2. Now that we have verified Assumption 1, the second step in the proof is Assumption 2 from [17]. We assume there is a bounded measurable function \u03b1 : S \u2192 (0,\u221e) such\nthat \u03b1Mf = L(f\u03b1) \u2212 fL\u03b1 for all functions f : X \u2192 R such that f \u2208 D(M) and f\u03b1 \u2208 D(L). Substituting inM and L we get\n\u03b1t(X)[\u2202tf(X)\u2212 \u2190\u2212 b \u03b8T\u2212t(X) \u00b7 \u2207f(X)\n+ 12g 2 t\u2206f(X) + \u03bb M t (X) \u2211N m=1 \u222b y\u2208Rmd f(Y)(K M t (Y|X)\u2212 \u03b4X(Y))dy]\n= \u2202t(f\u03b1t)(X) + \u2212\u2192 b t(X) \u00b7 \u2207(f\u03b1t)(X) + 12g 2 t\u2206(f\u03b1t)(X)\n+ \u2192 \u03bb t(X) \u2211N m=1 \u222b y\u2208Rmd f(Y)\u03b1t(Y)( \u2212\u2192 K t(Y|X)\u2212 \u03b4X(Y))dy\n\u2212 f(X)[\u2202t\u03b1t(X) + \u2212\u2192 b t(X) \u00b7 \u2207\u03b1t(X) + 12g 2 t\u2206\u03b1t(X)\n+ \u2192 \u03bb t(X) \u2211N m=1 \u222b y\u2208Rmd \u03b1t(Y)( \u2212\u2192 K t(Y|X)\u2212 \u03b4X(Y))dy] (15)\nSince f does not depend on time, \u2202tf(X) = 0 and \u2202t(f\u03b1t)(X) = f(X)\u2202t\u03b1t(X) thus the \u2202t terms on the RHS also cancel out. Comparing terms on the LHS and RHS relating to the diffusion part of the process we obtain\n\u2212 \u03b1t(X)( \u2190\u2212 b \u03b8T\u2212t(X) \u00b7 \u2207f(X)) + 12\u03b1t(X)g 2 t\u2206f(X) =\n\u2212\u2192 b t(X) \u00b7 \u2207(f\u03b1t)(X) + 12g 2 t\u2206(f\u03b1t)(X)\u2212 f(X) \u2212\u2192 b t(X) \u00b7 \u2207\u03b1t(X)\u2212 12f(X)g 2 t\u2206\u03b1t(X).\nTherefore, we get\n\u2212 \u03b1t(X)( \u2190\u2212 b \u03b8T\u2212t(X) \u00b7 \u2207f(X)) + 12\u03b1t(X)g 2 t\u2206f(X) =\n\u2212\u2192 b t(X) \u00b7 (f(X)\u2207\u03b1t(X) + \u03b1t(X)\u2207f(X)) + 12g 2 t ( 2\u2207f(X) \u00b7 \u2207\u03b1t(X) + f(X)\u2206\u03b1t(X) + \u03b1t(X)\u2206f(X) ) \u2212 f(X)\n\u2212\u2192 b t((X) \u00b7 \u2207\u03b1t(X)\u2212 12f(X)g 2 t\u2206\u03b1t(X).\nSimplifying the above expression, we get\n\u2212\u03b1t(X)( \u2190\u2212 b \u03b8T\u2212t(X) \u00b7 \u2207f(X)) = \u03b1t(X) \u2212\u2192 b t(X) \u00b7 \u2207f(X) + g2t\u2207f(X) \u00b7 \u2207\u03b1t(X) (\u2212\u03b1t(X) \u2190\u2212 b \u03b8T\u2212t(X)) \u00b7 \u2207f(X) = (\u03b1t(X) \u2212\u2192 b t(X) + g 2 t\u2207\u03b1t(X)) \u00b7 \u2207f(X).\nThis is true for any f implying\n\u2212\u03b1t(X) \u2190\u2212 b \u03b8T\u2212t(X) = \u03b1t(X) \u2212\u2192 b t(X) + g 2 t\u2207\u03b1t(X).\nThis implies that \u03b1t(X) satisfies\n\u2207 log\u03b1t(X) = \u2212 1g2t ( \u2212\u2192 b t(X) + \u2190\u2212 b \u03b8T\u2212t(X)) (16)\nComparing terms from the LHS and RHS of (15) relating to the jump part of the process we obtain\n\u03b1t(X)\u03bb M t (X) \u2211N m=1 \u222b y\u2208Rmd f(Y)(K M t (Y|X)\u2212 \u03b4X(Y))dy =\n\u2192 \u03bb t(X) \u2211N m=1 \u222b y\u2208Rmd f(Y)\u03b1t(Y)( \u2212\u2192 K t(Y|X)\u2212 \u03b4X(Y))dy\n\u2212 f(X)\u2192\u03bb t(X) \u2211N\nm=1 \u222b y\u2208Rmd \u03b1t(Y)( \u2212\u2192 K t(Y|X)\u2212 \u03b4X(Y))dy.\nHence, we have \u03b1t(X) \u2211N\nm=1 \u222b y\u2208Rmd f(Y)\u03bb M t (X)K M t (Y|X)dy \u2212 \u03b1t(X)\u03bbMt (X)f(X) =\n\u2192 \u03bb t(X) \u2211N m=1 \u222b y\u2208Rmd f(Y)\u03b1t(Y) \u2212\u2192 K t(Y|X)dy\n\u2212 f(X)\u2192\u03bb t(X) \u2211N\nm=1 \u222b y\u2208Rmd \u03b1t(Y) \u2212\u2192 K t(Y|X)dy.\nRecalling the definitions of \u03bbMt (X) and K M (Y|X), (13) and (14), we get \u03b1t(X) \u2211N\nm=1 \u222b y\u2208Rmd f(Y) \u2190\u2212 \u03bb \u03b8T\u2212t(Y) \u2190\u2212 K\u03b8T\u2212t(X|Y)dy\n\u2212 \u03b1t(X)f(X) \u2211N\nm=1 \u222b y\u2208Rmd \u2190\u2212 \u03bb \u03b8T\u2212t(Y) \u2190\u2212 K\u03b8T\u2212t(X|Y)dy =\n\u2192 \u03bb t(X) \u2211N m=1 \u222b y\u2208Rmd f(Y)\u03b1t(Y) \u2212\u2192 K t(Y|X)dy\n\u2212 f(X)\u2192\u03bb t(X) \u2211N\nm=1 \u222b y\u2208Rmd \u03b1t(Y) \u2212\u2192 K t(Y|X)dy.\nThis equality is satisfied if \u03b1t(X) follows the following relation\n\u03b1t(Y) = \u03b1t(X)\n\u2190\u2212 \u03bb \u03b8T\u2212t(Y) \u2190\u2212 K\u03b8T\u2212t(X|Y)\n\u2192 \u03bb t(X) \u2212\u2192 K t(Y|X)\nfor n \u0338= m (17)\nWe only require this relation to be satisfied for n \u0338= m because both \u2212\u2192 K t(Y|X) and \u2190\u2212 K\u03b8T\u2212t(X|Y) are 0 for n = m. We note at this point, as in [17], that if we have \u03b1t(X) = 1/pt(X) and \u2190\u2212 \u03bb \u03b8T\u2212t and \u2190\u2212 K\u03b8T\u2212t(X|Y) equal to the true time-reversals, then both (16), and (17) are satisfied. However, \u03b1t(X) = 1/pt(X) is not the only \u03b1t to satisfy these equations. (16) and (17) can be thought of as enforcing a certain parameterization of the generative process in terms of \u03b1t [17].\nConcluding the proof. Now for the final part of the proof, we substitute our value for \u03b1 into the IISM loss from [17] which is equal to the negative of the evidence lower bound on Epdata(X0)[log p\u03b80(X0)] up to a constant independent of \u03b8. Defining \u03b2t(Xt) = 1/\u03b1t(Xt), we have\nIISM(\u03b2) = \u222b T 0 Ept(Xt)[ L\u0302\u2217t \u03b2t(Xt) \u03b2t(Xt) + L\u0302t log \u03b2t(Xt)]dt.\nWe split the spatial infintesimal generator of the forward process into the generator corresponding to the diffusion and the generator corresponding to the jump part, L\u0302 = L\u0302difft + L\u0302Jt with\nL\u0302difft (f)(X) = \u2212\u2192 b t(X) \u00b7 \u2207f(X) + 12g 2 t\u2206f(X).\nand L\u0302Jt(f)(X) = \u2192 \u03bb t(X) \u2211N m=1 \u222b y\u2208Rmd f(Y)( \u2212\u2192 K t(Y|X)\u2212 \u03b4X(Y))dy.\nBy comparison with the approach to find the adjoint K\u0302\u2217t , we also have L\u0302\u2217t = L\u0302diff\u2217t + L\u0302J\u2217t with\nL\u0302diff\u2217t (f)(X) = \u2212f(X)\u2207 \u00b7 \u2212\u2192 b t(X)\u2212\u2207f(X) \u00b7 \u2212\u2192 b t(X) + 1 2g 2 t\u2206f(X).\nIn addition, we get L\u0302J\u2217t (f)(X) = \u2211N m=1 \u222b y\u2208Rmd f(Y) \u2192 \u03bb t(Y)( \u2212\u2192 K t(X|Y)\u2212 \u03b4X(Y))dy.\nFinally, IISM becomes\nIISM(\u03b2) = \u222b T 0 Ept(Xt)[ L\u0302diff\u2217t \u03b2t(Xt) \u03b2t(Xt) + L\u0302difft log \u03b2t(Xt)]dt+ \u222b T 0 Ept(Xt)[ L\u0302J\u2217t \u03b2t(Xt) \u03b2t(Xt) + L\u0302J log \u03b2t(Xt)]dt\n= IdiffISM(\u03b2) + IJISM(\u03b2),\nwhere we have named the two terms corresponding to the diffusion and jump part of the process as IdiffISM, IJISM respectively. For the diffusion part of the loss, we use the denoising form of the objective proven in Appendix E of [17] which is equivalent to IdiffISM up to a constant independent of \u03b8\nIdiffISM(\u03b2) = \u222b T 0 Ep0,t(X0,Xt)[ L\u0302difft (pt|0(\u00b7|X0)\u03b1t(\u00b7))(Xt) pt|0(Xt|X0)\u03b1t(Xt) \u2212 L\u0302difft log(pt|0(\u00b7|X0)\u03b1t(\u00b7))(Xt)]dt+ const.\nTo simplify this expression, we first re-arrange L\u0302difft (h) for some general function h : S \u2192 R.\nL\u0302difft (h) h \u2212 L\u0302difft (log h) = \u2212\u2192 b t\u00b7\u2207h h + 1 2g 2 t \u2206h h \u2212 \u2212\u2192 b t \u00b7 \u2207 log h\u2212 12g 2 t\u2206 log h\n= 12g 2 t ( \u2207\u00b7\u2207h h \u2212\u2207 \u00b7 \u2207 log h) = 12g 2 t \u2225\u2207 log h\u22252.\nSetting h = pt|0(\u00b7|X0)\u03b1t(\u00b7), our diffusion part of the loss becomes\nIdiffISM(\u03b2) = 12 \u222b T 0 g2tEp0,t(X0,Xt)[\u2225\u2207 log pt|0(Xt|X0) +\u2207 log\u03b1t(Xt)\u22252]dt+ const\nWe then directly parameterize\u2207 log\u03b1t(Xt) as \u2212s\u03b8t (Xt) IdiffISM(\u03b2) = 12 \u222b T 0 g2tEp0,t(X0,Xt)[\u2225\u2207 log pt|0(Xt|X0)\u2212 s\u03b8t (Xt)\u22252]dt+ const.\nWe now focus on the expectation within the integral to re-write it in an easy to calculate form\nEp0,t(X0,Xt)[\u2225\u2207 log pt|0(Xt|X0)\u2212 s\u03b8t (Xt)\u22252] = Ep0,t(X0,Xt)[\u2225s\u03b8t (Xt)\u22252 \u2212 2s\u03b8t (Xt)T\u2207 log p0,t(X0,Xt)] + const\nNow we note that we can re-write \u2207 log p0,t(X0,Xt) using Mt where Mt is a mask variable Mt \u2208 {0, 1}n0 that is 0 for components of X0 that have been deleted to get to Xt and 1 for components that remain in Xt.\n\u2207 log p0,t(X0,Xt) = 1p0,t(X0,Xt)\u2207p0,t(X0,Xt) = 1p0,t(X0,Xt)\u2207 \u2211 Mt p0,t(X0,Xt,Mt)\n= \u2211\nMt 1 p0,t(X0,Xt) \u2207p0,t(X0,Xt,Mt) = \u2211\nMt p(nt,Mt,X0) p0,t(X0,Xt) \u2207pt|0(xt|nt,X0,Mt) = \u2211\nMt p(Mt|X0,Xt) p(xt|nt,X0,Mt)\u2207pt|0(xt|nt,X0,Mt)\n= Ep(Mt|X0,Xt)[\u2207 log pt|0(xt|nt,X0,Mt)] Substituting this back in we get\nEp0,t(X0,Xt)[\u2225\u2207 log pt|0(Xt|X0)\u2212 s\u03b8t (Xt)\u22252] = Ep0,t(X0,Xt)[\u2225s\u03b8t (Xt)\u22252 \u2212 2s\u03b8t (Xt)TEp(Mt|X0,Xt)[\u2207 log pt|0(xt|nt,X0,Mt)]] + const = Ep0,t(X0,Xt,Mt)[\u2225\u2207 log pt|0(xt|nt,X0,Mt)\u2212 s\u03b8t (Xt)\u22252] + const.\nTherefore, the diffusion part of IISM can be written as IdiffISM(\u03b2) = T2 EU(t;0,T )p0,t(X0,Xt,Mt)[g 2 t \u2225\u2207 log pt|0(xt|nt,X0,Mt)\u2212 s\u03b8t (Xt)\u22252] + const.\nWe now focus on the jump part of the loss IJISM. We first substitute in L\u0302Jt and L\u0302J\u2217t IJISM = \u222b T 0 Ept(Xt)[ \u2211 m \u222b y\u2208Rmd \u2192 \u03bb t(Y) \u03b2t(Y) \u03b2t(Xt) ( \u2212\u2192 K t(Xt|Y)\u2212 \u03b4Y(Xt))dy+\n\u2192 \u03bb t(Xt) \u2211N m=1 \u222b y\u2208Rmd \u2212\u2192 K t(Y|Xt) log \u03b2t(Y)dy \u2212 \u2192 \u03bb t(Xt) log \u03b2t(Xt)]dt.\n(18)\nNoting that \u03b2t(Xt) = 1/\u03b1t(Xt), we get\n\u03b2t(Xt) \u03b2t(Y)\n= \u2190\u2212 \u03bb \u03b8T\u2212t(Y) \u2190\u2212 K\u03b8T\u2212t(Xt|Y)\u2192\n\u03bb t(Xt) \u2212\u2192 Kt(Y|Xt)\nfor nt \u0338= m (19)\nor swapping labels for Xt and Y,\n\u03b2t(Y) \u03b2t(Xt)\n= \u2190\u2212 \u03bb \u03b8T\u2212t(Xt) \u2190\u2212 K\u03b8T\u2212t(Y|Xt)\u2192\n\u03bb t(Y) \u2212\u2192 Kt(Xt|Y)\nfor nt \u0338= m (20)\nSubstituting (19) into the second line and (20) into the first line of (18) and using the fact that \u2212\u2192 K t(Xt|Y) = 0 for nt = m, we obtain\nIJISM = \u222b T 0 Ept(Xt)[ \u2211N m=1\\nt \u222b y\u2208Rmd \u2192 \u03bb t(Y) \u2190\u2212 \u03bb \u03b8T\u2212t(Xt) \u2190\u2212 K\u03b8T\u2212t(Y|Xt)\u2192\n\u03bb t(Y) \u2212\u2192 Kt(Xt|Y)\n\u2212\u2192 K t(Xt|Y)dy\n\u2212 \u2211N\nm=1 \u222b y\u2208Rmd \u2192 \u03bb t(Y) \u03b2t(Y) \u03b2t(Xt) \u03b4Y(Xt)dy\n+ \u2192 \u03bb t(Xt) \u2211N m=1\\nt \u222b y\u2208Rmd \u2212\u2192 K t(Y|Xt){log \u03b2t(Xt)\u2212 log \u2190\u2212 \u03bb \u03b8T\u2212t(Y) \u2212 log \u2190\u2212 K\u03b8T\u2212t(Xt|Y) + log \u2192 \u03bb t(Xt) + log \u2212\u2192 K t(Y|Xt)}dy\n\u2212\u2192\u03bb t(Xt) log \u03b2t(Xt)]dt. Hence, we have\nIJISM = \u222b T 0 Ept(Xt)[ \u2190\u2212 \u03bb \u03b8T\u2212t(Xt) \u2211N m=1\\nt \u222b y\u2208Rmd \u2190\u2212 K\u03b8T\u2212t(Y|Xt)dy \u2212 \u2192 \u03bb t(Xt) \u03b2t(Xt) \u03b2t(Xt) +\n\u2192 \u03bb t(Xt) \u2211N m=1\\nt \u222b y\u2208Rmd \u2212\u2192 K t(Y|Xt){\u2212 log \u2190\u2212 \u03bb \u03b8T\u2212t(Y)\u2212 log \u2190\u2212 K\u03b8T\u2212t(Xt|Y)}dy]dt+ const.\nThis can be rewritten as IJISM = \u222b T 0 Ept(Xt)[ \u2190\u2212 \u03bb \u03b8T\u2212t(Xt) + \u2192 \u03bb t(Xt)E\u2212\u2192Kt(Y|Xt)[\u2212 log \u2190\u2212 \u03bb \u03b8T\u2212t(Y)\u2212 log \u2190\u2212 K\u03b8T\u2212t(Xt|Y)]]dt+ const. Therefore, we have IJISM = TEU(t;0,T )pt(Xt)\u2212\u2192Kt(Y|Xt)[ \u2190\u2212 \u03bb \u03b8T\u2212t(Xt)\u2212 \u2192 \u03bb t(Xt) log \u2190\u2212 \u03bb \u03b8T\u2212t(Y)\u2212 \u2192 \u03bb t(Xt) log \u2190\u2212 K\u03b8T\u2212t(Xt|Y)] + const. Finally, using the definition of the forward and backward kernels, i.e. \u2212\u2192 K t(Y|Xt) =\u2211n\ni=1 K del(i|n)\u03b4del(X,i)(Y) and \u2190\u2212 K\u03b8T\u2212t(Xt|Y) = \u222b xadd \u2211n i=1 A \u03b8 t (x\nadd, i|Y)\u03b4ins(Y,xadd,i)(Xt)dxadd, we get\nIJISM = TEU(t;0,T )pt(Xt)[ \u2211N m=1 \u222b y\u2208Rmd \u2211nt i=1 K del(i|nt)\u03b4del(Xt,i)(Y)( \u2190\u2212 \u03bb \u03b8T\u2212t(Xt)\u2212 \u2192 \u03bb t(Xt) log \u2190\u2212 \u03bb \u03b8T\u2212t(Y)\n\u2212\u2192\u03bb t(Xt) log \u2190\u2212 K\u03b8T\u2212t(Xt|Y))dy] + const\nWe get IJISM = TEU(t;0,T )pt(Xt)Kdel(i|nt)\u03b4del(Xt,i)(Y)\n[ \u2190\u2212 \u03bb \u03b8T\u2212t(Xt)\u2212 \u2192 \u03bb t(Xt) log \u2190\u2212 \u03bb \u03b8T\u2212t(Y)\u2212 \u2192 \u03bb t(Xt) log \u2190\u2212 K\u03b8T\u2212t(Xt|Y)] + const.\nTherefore, we have IJISM = TEU(t;0,T )pt(Xt)Kdel(i|nt)\u03b4del(Xt,i)(Y)\n[ \u2190\u2212 \u03bb \u03b8T\u2212t(Xt)\u2212 \u2192 \u03bb t(Xt) log \u2190\u2212 \u03bb \u03b8T\u2212t(Y)\u2212 \u2192 \u03bb t(Xt) logA \u03b8 T\u2212t(x\nadd, i|Y)] + const. Putting are expressions for IdiffISM and IJISM together we obtain IISM =T2 E[g 2 t \u2225\u2207 log pt|0(xt|nt,X0,Mt)\u2212 s\u03b8t (Xt)\u22252]+\nTE[ \u2190\u2212 \u03bb \u03b8T\u2212t(Xt)\u2212 \u2192 \u03bb t(Xt) log \u2190\u2212 \u03bb \u03b8T\u2212t(Y)\u2212 \u2192 \u03bb t(Xt) logA \u03b8 T\u2212t(x\nadd, i|Y)] + const. We get that \u2212IISM gives us our evidence lower bound on Epdata(X0)[log p\u03b80(X0)] up to a constant that does not depend on \u03b8. In the main text we have used a time notation such that the backward process runs backwards from t = T to t = 0. To align with the notation of time used in the main text we change T \u2212 t to t on subscripts for \u2190\u2212 \u03bb \u03b8T\u2212t and A \u03b8 T\u2212t. We also will use the fact that \u2192 \u03bb t(Xt) depends only on the number of components in Xt, \u2192 \u03bb t(Xt) = \u2192 \u03bb t(nt).\nL(\u03b8) =\u2212 T2 E[g 2 t \u2225s\u03b8t (Xt)\u2212\u2207xt log pt|0(xt|nt,X0,Mt)\u22252]+\nTE[\u2212 \u2190\u2212 \u03bb \u03b8t (Xt) + \u2192 \u03bb t(nt) log \u2190\u2212 \u03bb \u03b8t (Y) + \u2192 \u03bb t(nt) logA \u03b8 t (x add, i|Y)] + const.\nTightness of the lower bound Now that we have derived the ELBO as in Proposition 2, we show that the maximizers of the ELBO are tight, i.e. that they close the variational gap. We do this by proving the general ELBO presented in [17] has this property and therefore ours, which is a special case of this general ELBO, also has that the optimum parameters close the variational gap.\nTo state our proposition, we recall the setting of [17]. The forward noising process is denoted (Yt)t\u22650 and associated with an infinitesimal generator L\u0302 its extension (t,Yt)t\u22650 is associated with the infinitesimal generator L, i.e. L = \u2202t + L\u0302. We also define the score-matching operator \u03a6 given for any f for which it is defined by\n\u03a6(f) = L(f)/f \u2212 L(log(f)). We recall that according to [17, Equation (8)] and under [17, Assumption 1, Assumption2], we have\nlog pT (Y0) \u2265 E[log p0(YT )\u2212 \u222b T 0 L(v/\u03b2)/(v/\u03b2) + L(log \u03b2)dt],\nwith vt = pT\u2212t for any t \u2208 [0, T ]. We define the variational gap Gap as follows Gap = E[log pT (Y0)\u2212 log p0(YT ) + \u222b T 0 L(v/\u03b2)/(v/\u03b2) + L(log \u03b2)dt].\nIn addition, using It\u00f4 Formula, we have that log vT (YT )\u2212 log v0(Y0) = \u222b T 0 L(v)dt. Assuming that E[| log vT (YT )\u2212 log v0(Y0)|] < +\u221e, we get Gap = E[ \u222b T 0 \u2212L(log v) + L(v/\u03b2)/(v/\u03b2) + L(log \u03b2)dt] = E[ \u222b T 0 \u03a6(v/\u03b2)dt]. In particular, using [17, Proposition 1], we get that Gap \u2265 0 and Gap = 0 if and only if \u03b2 \u221d v. In addition, the ELBO is maximized if and only if \u03b2 \u221d v, see [17, Equation 10] and the remark that follows. Therefore, we have that: if we maximize the ELBO then the ELBO is tight. Combining this with the fact that the ELBO is maximized at the time reversal [17], then we have that when our jump diffusion parameters match the time reversal, our variational gap is 0.\nOther approaches. Another way to derive the ELBO is to follow the steps of [15] directly, since [17] is a general framework extending this approach. The key formulae to derive the result and the ELBO is 1) a Feynman-Kac formula 2) a Girsanov formula. In the case of jump diffusions (with jump in Rd) a Girsanov formula has been established by [22]. Extending this result to one-point compactification space would allow us to prove directly Proposition 2 without having to rely on the general framework of [17]."
        },
        {
            "heading": "A.4 Proof of Proposition 3",
            "text": "We start by recalling the form for the time reversal given in Proposition 1 \u2190\u2212 \u03bb \u2217t (X) = \u2192 \u03bb t(n+ 1) \u2211n+1 i=1 K del(i|n+ 1) \u222b yadd pt(ins(X,yadd, i))dyadd/pt(X).\nWe then introduce a marginalization over X0 \u2190\u2212 \u03bb \u2217t (X) = \u2192 \u03bb t(n+ 1) \u2211n+1 i=1 K del(i|n+ 1) \u222b yadd \u2211 n0 \u222b x0 p0,t(X0, ins(X,yadd, i))dx0dyadd/pt(X)\n= \u2192 \u03bb t(n+ 1) \u2211n+1 i=1 K del(i|n+ 1) \u222b yadd \u2211 n0 \u222b x0 p0(X0) pt(X) pt|0(ins(X,yadd, i)|X0)dx0dyadd = \u2192 \u03bb t(n+ 1) \u2211n+1 i=1 K del(i|n+ 1) \u222b yadd \u2211 n0 \u222b x0 p0|t(X0|X) pt|0(X|X0) pt|0(ins(X,yadd, i)|X0)dx0dyadd = \u2192 \u03bb t(n+ 1) \u2211n+1 i=1 K del(i|n+ 1) \u222b yadd \u2211 n0 \u222b x0 p0|t(n0|X)p0|t(x0|X,n0) pt|0(n|X0)pt|0(x|X0,n) \u00d7\npt|0(n+ 1|X0)pt|0(z(X,yadd, i)|X0, n+ 1)dx0dyadd\nwhere (n+ 1, z(X,yadd, i)) = ins(X,yadd, i). Now using the fact the forward component deletion process does not depend on x0, only n0, we have pt|0(n|X0) = pt|0(n|n0) and pt|0(n + 1|X0) = pt|0(n+ 1|n0). Using this result, we get\n\u2190\u2212 \u03bb \u2217t (X) = \u2192 \u03bb t(n+ 1) \u2211 n0 {pt|0(n+1|n0)pt|0(n|n0) p0|t(n0|X)\u00d7\u222b\nx0\n\u2211n+1 i=1 K del(i|n+1) \u222b yadd pt|0(z(X,y add,i)|X0,n+1)dyadd\npt|0(x|X0,n) p0|t(x0|X, n0)dx0}. (21)\nWe now focus on the probability ratio within the integral over x0. We will show that this ratio is 1. We start with the numerator, introducing a marginalization over possible mask variables between X0 and (n+ 1, z), denoted M (n+1) with M (n+1) having n+ 1 ones and n0 \u2212 (n+ 1) zeros.\u2211n+1\ni=1 K del(i|n+ 1) \u222b yadd pt|0(z(X,y add, i)|X0, n+ 1)dyadd\n= \u2211n+1\ni=1 K del(i|n+ 1) \u2211 M(n+1) \u222b yadd pt|0(M (n+1), z(X,yadd, i)|X0, n+ 1)dyadd\n= \u2211 M(n+1) \u2211n+1 i=1 K del(i|n+ 1)pt|0(M (n+1)|X0, n+ 1) \u222b yadd pt|0(z(X,y add, i)|X0, n+ 1,M (n+1))dyadd\nNow, for our forward process we have\npt|0(z(X,y add, i)|X0, n+ 1,M (n+1)) = \u220fn+1 j=1 N (z(j); \u221a \u03b1tM (n+1)(X0) j , (1\u2212 \u03b1t)Id)\nwhere z is shorthand for z(X,yadd, i), z(j) is the vector in Rd for the jth component of z and M (n+1)(X0)\nj is the vector in Rd corresponding to the component in X0 corresponding to the jth one in the M (n+1) mask. Integrating out yadd we have\u222b yadd pt|0(z(X,y add, i)|X0, n+1,M (n+1))dyadd = \u220fn j=1N (x(j); \u221a \u03b1tM (n+1)\\i(X0) j , (1\u2212\u03b1t)Id),\nwhere M (n+1)\\i denotes a mask variable obtained by setting the ith one of M (n+1) to zero. Hence, we have \u2211n+1\ni=1 K del(i|n+ 1) \u222b yadd pt|0(z(X,y add, i)|X0, n+ 1)dyadd\n= \u2211 M(n+1) \u2211n+1\ni=1 K del(i|n+ 1)pt|0(M (n+1)|X0, n+ 1)\u220fn\nj=1N (x(j); \u221a \u03b1tM (n+1)\\i(X0) j , (1\u2212 \u03b1t)Id). (22)\nWe now re-write the denominator from (21) introducing a marginalization over mask variables, M (n) pt|0(x|X0, n) = \u2211 M(n) pt|0(M (n)|X0, n)pt|0(x|M (n),X0, n). (23)\nWe use the following recursion for the probabilities assigned to mask variables\npt|0(M (n)|X0, n) = \u2211 M(n+1) \u2211n+1 i=1 I{M (n+1)\\i = M (n)}Kdel(i|n+1)pt|0(M (n+1)|X0, n+1).\nSubstituting this into (23) gives pt|0(x|X0, n) = \u2211 M(n) \u2211 M(n+1) \u2211n+1\ni=1 I{M (n+1)\\i = M (n)}Kdel(i|n+ 1)\u00d7 pt|0(M (n+1)|X0, n+ 1)pt|0(x|M (n),X0, n)\n= \u2211 M(n) \u2211 M(n+1) \u2211n+1\ni=1 I{M (n+1)\\i = M (n)}Kdel(i|n+ 1) \u00d7 pt|0(M (n+1)|X0, n+ 1) \u220fn j=1N (x(j); \u221a \u03b1tM (n)(X0) j , (1\u2212 \u03b1t)Id)\n= \u2211 M(n+1) \u2211n+1 i=1 K del(i|n+ 1)pt|0(M (n+1)|X0, n+ 1)\n\u00d7 \u220fn j=1N (x(j); \u221a \u03b1tM (n+1)\\i(X0) j , (1\u2212 \u03b1t)Id).\nBy comparing with (22), we can see that pt|0(x|X0, n) = \u2211n+1 i=1 K del(i|n+ 1) \u222b yadd pt|0(z(X,y add, i)|X0, n+ 1)dyadd.\nThis shows that the probability ratio in (21) is 1. Therefore, we have \u2190\u2212 \u03bb \u2217t (X) = \u2192 \u03bb t(n+ 1) \u2211 n0 {pt|0(n+1|n0)pt|0(n|n0) p0|t(n0|X) \u222b x0 p0|t(x0|X, n0)dx0}\n= \u2192 \u03bb t(n+ 1) \u2211 n0 pt|0(n+1|n0) pt|0(n|n0) p0|t(n0|X),\nwhich concludes the proof.\npt|0(n|n0) can be analytically calculated when \u2192 \u03bb t(n) is of a simple enough form. When \u2192 \u03bb t(n) does not depend on n then the dimension deletion process simply becomes a time inhomogeneous Poisson process. Therefore, we would have\npt|0(n|n0) = ( \u222b t 0 \u2192 \u03bb sds)n0\u2212n (n0\u2212n)! exp(\u2212 \u222b t 0 \u2192 \u03bb sds).\nIn our experiments we set \u2192\u03bb t(n = 1) = 0 to stop the dimension deletion process when we reach a single component. If we have \u2192\u03bb t(n) = \u2192 \u03bb t(m) for all n,m > 1 then we can still use the time inhomogeneous Poisson process formula for n > 1 and find the probability for n = 1, pt|0(n = 1|n0) by requiring pt|0(n|n0) to be a valid normalized distribution. Therefore, for the case that\u2192\u03bb t(n) = \u2192 \u03bb t(m) for all n,m > 1 and \u2192 \u03bb t(n = 1) = 0, we have\npt|0(n|n0) =  ( \u222b t 0 \u2192 \u03bb sds)n0\u2212n (n0\u2212n)! exp(\u2212 \u222b t 0 \u2192 \u03bb sds) 1 < n \u2264 n0 1\u2212 \u2211n0 m=2 ( \u222b t 0 \u2192 \u03bb sds)n0\u2212m (n0\u2212m)! exp(\u2212 \u222b t 0 \u2192 \u03bb sds) n = 1\nIn cases where \u2192\u03bb t(n) depends on n not just for n = 1, pt|0(n|n0) can become more difficult to calculate analytically. However, since the probability distributions are all 1-dimensional over n, it is very cheap to simply simulate the forward dimension deletion process many times and empirically estimate pt|0(n|n0) although we do not need to do this for our experiments."
        },
        {
            "heading": "A.5 The Objective is Maximized at the Time Reversal",
            "text": "In this section, we analyze the objective L(\u03b8) as a standalone object and determine the optimum values for s\u03b8t , \u2190\u2212 \u03bb \u03b8t and A \u03b8 t directly. This is in order to gain intuition directly into the learning signal of L(\u03b8) without needing to refer to stochastic process theory. The definition of L(\u03b8) as in the main text is\nL(\u03b8) = \u2212T2 E[g 2 t \u2225s\u03b8t (Xt)\u2212\u2207xt log pt|0(xt|X0, nt,Mt)\u22252]+\nTE[\u2212 \u2190\u2212 \u03bb \u03b8t (Xt) + \u2192 \u03bb t(nt) log \u2190\u2212 \u03bb \u03b8t (Y) + \u2192 \u03bb t(nt) logA \u03b8 t (x add t , i|Y)] + C.\nwith the expectations taken over U(t; 0, T )p0,t(X0,Xt,Mt)Kdel(i|nt)\u03b4del(Xt,i)(Y).\nContinuous optimum. We start by analysing the objective for s\u03b8t . This part of L(\u03b8) can be written as \u2212 12 \u222b T 0 g2tEp0,t(X0,Xt,Mt)[\u2225s\u03b8t (Xt)\u2212\u2207xt log pt|0(xt|X0, nt,Mt)\u22252]dt\nWe now use the fact that the function that minimizes an L2 regression problem min f Ep(x,y)[\u2225f(x)\u2212 y\u22252] is the conditional expectation of the target f\u2217(x) = Ep(y|x)[y]. Therefore the optimum value for s\u03b8t (Xt) is\ns\u2217t (Xt) = Ep(Mt,X0|Xt)[\u2207xt log pt|0(xt|X0, nt,Mt)] = \u2211\nMt \u2211N n0=1 \u222b x0\u2208Rn0d p(Mt, n0,x0|Xt)\u2207xt log pt|0(xt|x0, n0, nt,Mt)dx0\n= \u2211\nMt \u2211N n0=1 \u222b x0\u2208Rn0d p(Mt,n0,x0|Xt) pt|0(xt|x0,n0,nt,Mt) \u2207xtpt|0(xt|x0, n0, nt,Mt)dx0\n= \u2211\nMt \u2211N n0=1 \u222b x0\u2208Rn0d p(x0,n0,nt,Mt) p(nt,xt) \u2207xtpt|0(xt|x0, n0, nt,Mt)dx0\n= 1p(nt,xt) \u2211 Mt \u2211N n0=1 \u222b x0\u2208Rn0d \u2207xtp(xt,x0, n0, nt,Mt)dx0\n= 1p(nt,xt)\u2207xt \u2211 Mt \u2211N n0=1 \u222b x0\u2208Rn0d p(xt,x0, n0, nt,Mt)dx0\n= 1p(nt,xt)\u2207xtp(xt, nt) = \u2207xt log p(Xt).\nTherefore, the optimum value for s\u03b8t (Xt) is\u2207xt log p(Xt) which is the value that gives \u2190\u2212 b t to be the time reversal of \u2212\u2192 b t as stated in Proposition 1.\nJump rate optimum. The learning signal for \u2190\u2212 \u03bb \u03b8t comes from these two terms in L(\u03b8)\nTE[\u2212 \u2190\u2212 \u03bb \u03b8t (Xt) + \u2192 \u03bb t(nt) log \u2190\u2212 \u03bb \u03b8t (Y)] (24)\nThis expectation is maximized when for each test input Z and test time t, we have the following expression maximized\n\u2212pt(Z) \u2190\u2212 \u03bb \u03b8t (Z) + \u2211nz+1 i= \u222b yadd pt(ins(Z,yadd, i))Kdel(i|nz + 1)dyadd \u00d7 \u2192 \u03bb t(nz + 1) log \u2190\u2212 \u03bb \u03b8t (Z),\nbecause pt(Z) is the probability Z gets drawn as a full sample from the forward process and\u2211nz+1 i= \u222b yadd\npt(ins(Z,yadd, i))Kdel(i|nz + 1)dyadd is the probability that a sample one component bigger than Z gets drawn from the forward process and then a component is deleted to get to Z. Therefore the first probability is the probability that test input Z and test time t appear as the first term in (24) whereas the second probability is the probability that test input Z and test time t appear as the second term in (24).\nWe now use the fact that, for constants b and c,\nargmaxa \u2212 ba+ c log a = cb .\nWe therefore have the optimum \u2190\u2212 \u03bb \u03b8t (Z) as\n\u2190\u2212 \u03bb \u2217t (Z) = \u2192 \u03bb t(nz + 1)\n\u2211nz+1 i= \u222b yadd pt(ins(Z,y add,i))Kdel(i|nz+1)dyadd\npt(Z)\nwhich is the form for the time-reversal given in Proposition (1).\nJump kernel optimum. Finally, we analyse the part of L(\u03b8) for learning A\u03b8t (xaddt , i|Y),\nTE[\u2192\u03bb t(nt) logA\u03b8t (xaddt , i|Y)]\n= \u222b T 0 Ept(Xt)Kdel(i|nt)\u03b4del(Xt,i)(Y)[ \u2192 \u03bb t(nt) logA \u03b8 t (x add t , i|Y)]dt\n= \u222b T 0 Ept(nt)[ \u2192 \u03bb t(nt)Ept(xt|nt)Kdel(i|nt)\u03b4del(Xt,i)(Y)[logA \u03b8 t (x add t , i|Y)]]dt.\nWe now re-write the joint probability distribution that the inner expectation is taken with respect to,\npt(xt|nt)Kdel(i|nt)\u03b4del(Xt,i)(Y) = p\u0303(Y|nt)p(x add t , i|Y)\u03b4y(xbaset ).\nwith p\u0303(Y|nt) = \u2211nt i=1 \u222b xt pt(xt|nt)Kdel(i|nt)\u03b4del(xt,i)(Y)dxt,\nand p(xaddt , i|Y) \u221d pt(xt|nt)Kdel(i|nt),\nand xbaset \u2208 R(nt\u22121)d referring to the nt \u2212 1 components of xt, that are not xaddt i.e. Xt = ins((xbaset , nt \u2212 1),xaddt , i). We then have\nTE[\u2192\u03bb t(nt) logA\u03b8t (xaddt , i|Y)]\n= \u222b T 0 Ept(nt)[ \u2192 \u03bb t(nt)Ep\u0303(Y|nt)p(xaddt ,i|Y)\u03b4y(xbaset )[logA \u03b8 t (x add t , i|Y)]]dt\n= \u222b T 0 Ept(nt)[ \u2192 \u03bb t(nt)Ep\u0303(Y|nt)p(xaddt ,i|Y)\u03b4y(xbaset )[logA \u03b8 t (x add t , i|Y)]]dt\n\u2212 \u222b T 0 Ept(nt)[ \u2192 \u03bb t(nt)Ep\u0303(Y|nt)p(xaddt ,i|Y)\u03b4y(xbaset )[log p(x add t , i|Y)]]dt+ const\n= \u222b T 0 Ept(nt)[ \u2192 \u03bb t(nt)Ep\u0303(Y|nt)\u03b4y(xbaset )[\u2212KL(p(x add t , i|Y) ||A\u03b8t (xaddt , i|Y)]]dt+ const.\nTherefore, the optimum A\u03b8t (x add t , i|Y) which maximizes this part of L(\u03b8) is\nA\u2217t (x add t , i|Y) = p(xaddt , i|Y) \u221d pt(Xt)Kdel(i|nt).\nwhich is the same form as given in Proposition 1."
        },
        {
            "heading": "B Training Objective",
            "text": "We estimate our objective L(\u03b8) by taking minibatches from the expectation U(t; 0, T )p0,t(X0,Xt,Mt)Kdel(i|nt)\u03b4del(Xt,i)(Y). We first sample t \u223c U(t; 0, T ) and then take samples from our dataset X0 \u223c pdata(X0). In order to sample pt|0(Xt,Mt|X0) we need to both add noise, delete dimensions and sample a mask variable. Since the Gaussian noising process is isotropic, we can add a suitable amount of noise to all dimensions of X0 and then delete dimensions of that noised full dimensional value. More specifically, we first sample X\u0303t = (n0, x\u0303t) with x\u0303t \u223c N (x\u0303t; \u221a \u03b1tx0, (1 \u2212 \u03b1t)In0d) for \u03b1t = exp ( \u2212 \u222b t 0 \u03b2(s)ds ) using the analytic forward equations for the VP-SDE derived in [3]. Then we sample the number of dimensions to delete. This is simple to do when our rate function is independent of n except for the case when n = 1 at which it is zero. We simply sample a Poisson random variable with mean parameter \u222b t 0 \u2192 \u03bb sds and then clamp its value such that the maximum number of possible components that are deleted is n0 \u2212 1. This gives the appropriate distribution over n, pt|0(n|n0) as given in Section A.4. To sample which dimensions are deleted, we can sample Kdel(i1|n0)Kdel(i2|n0 \u2212 1) . . .Kdel(in0\u2212nt |nt + 1) from which we can create the mask Mt and apply it to X\u0303t to obtain Xt, Xt = Mt(X\u0303t). When Kdel(i|n) = 1/n this is especially simple to do by simply randomly permuting the components of X\u0303t, and then removing the final n0 \u2212 nt components.\nAs is typically done in standard diffusion models, we parameterize s\u03b8t in terms of a noise prediction network that predicts \u03f5 where xt = \u221a \u03b1tMt(x0) + \u221a 1\u2212 \u03b1t\u03f5, \u03f5 \u223c N (0, Intd). We then re-weight the score loss in time such that we have a uniform weighting in time rather than the \u2018likelihood weighting\u2019 with g2t [3, 21]. Our objective to learn s \u03b8 t then becomes\n\u2212EU(t;0,T )pdata(X0)p(Mt,nt|X0)N (\u03f5;0,Intd) [ \u2225\u03f5\u03b8t (Xt)\u2212 \u03f5\u22252 ] with xt = \u221a \u03b1tMt(x0) + \u221a 1\u2212 \u03b1t\u03f5, s\u03b8t (Xt) = \u22121\u221a1\u2212\u03b1t \u03f5 \u03b8 t (Xt).\nFurther, by using the parameterization given in Proposition 3, we can directly supervise the value of p\u03b80|t(n0|Xt) by adding an extra term to our objective. We can treat the learning of p \u03b8 0|t(n0|Xt) as a standard prediction task where we aim to predict n0 given access to Xt. A standard objective for learning p\u03b80|t(n0|Xt) is then the cross entropy\nmax \u03b8\nEp0,t(X0,Xt) [ log p\u03b80|t(n0|Xt) ]\nOur augmented objective then becomes\nL\u0303(\u03b8) = TE[\u22121 2 \u2225\u03f5\u03b8t (Xt)\u2212\u03f5\u22252\u2212 \u2190\u2212 \u03bb \u03b8t (Xt)+ \u2192 \u03bb t(nt) log \u2190\u2212 \u03bb \u03b8t (Y)+ \u2192 \u03bb t(nt) logA \u03b8 t (x add t , i|Y)+\u03b3 log p\u03b80|t(n0|Xt)] (25) where the expectation is taken with respect to\nU(t; 0, T )pdata(X0)p(Mt, nt|X0)N (\u03f5; 0, Intd)Kdel(i|nt)\u03b4del(Xt,i)(Y)\nwhere xt = \u221a \u03b1tMt(x0) + \u221a 1\u2212 \u03b1t\u03f5 and \u03b3 is a loss weighting term for the cross entropy loss."
        },
        {
            "heading": "C Trans-Dimensional Diffusion Guidance",
            "text": "To guide an unconditionally trained model such that it generates datapoints consistent with conditioning information, we use the reconstruction guided sampling approach introduced in [9]. Our conditioning information will be the values for some of the components of X0, and thus the guidance should guide the generative process such that the rest of the components of the generated datapoint are consistent with those observed components. Following the notation of [9], we denote the observed components as xa \u2208 Rnad and the components to be generated as xb \u2208 Rnbd. Our trained score function s\u03b8t (Xt) approximates \u2207xt log pt(Xt) whereas we would like the score to approximate \u2207xt log pt(Xt|xa0). In order to do this, we will need to augment our unconditional score s\u03b8t (Xt) such that it incorporates the conditioning information.\nWe first focus on the dimensions of the score vector corresponding to xa. These can be calculated analytically from the forward process\n\u2207xat log p(Xt|x a 0) = \u2207xat log pt|0(x a t |xa0 , nt)\nwith pt|0(xat |xa0 , nt) = N (xat ; \u221a \u03b1tx a 0 , (1 \u2212 \u03b1t)Inad). Note that we assume a correspondence between xat and x a 0 . For example, in video if we condition on the first and last frame, we assume that the first and last frame of the current noisy xt correspond to xa0 and guide them towards their observed values. For molecules, the point cloud is permutation invariant and so we can simply assume the first na components of xt correspond to xa0 and guide them to their observed values.\nNow we analyse the dimensions of the score vector corresponding to xb. We split the score as\n\u2207xbt log p(Xt|x a 0) = \u2207xbt log p(x a 0 |Xt) +\u2207xbt log pt(Xt)\np(xa0 |Xt) is intractable to calculate directly and so, following [9], we approximate it with N (xa0 ; x\u0302\u03b8a0 (Xt), 1\u2212\u03b1t\u03b1t Inad) where x\u0302 \u03b8a 0 (Xt) is a point estimate of x a 0 given from s \u03b8 t (Xt) calculated as\nx\u0302\u03b8a0 (Xt) = xat + (1\u2212 \u03b1t)s\u03b8t (Xt)a\u221a\n\u03b1t\nwhere again we have assumed a correspondence between xat and x a 0 . Our approximation for \u2207xbt log p(x a 0 |Xt) is then\n\u2207xbt log p(x a 0 |Xt) \u2248 \u2212\u2207xbt \u03b1t 2(1\u2212 \u03b1t) \u2225xa0 \u2212 x\u0302\u03b8a0 (Xt)\u22252\nwhich can be calculated by differentiating through the score network s\u03b8t .\nWe approximate \u2190\u2212 \u03bb \u2217t (Xt|xa0) and A\u2217t (yadd, i|Xt,xa0), with their unconditional forms \u2190\u2212 \u03bb \u03b8t (Xt) and A\u03b8t (y add, i|Xt). We find this approximation still leads to valid generations because the guidance of the score network s\u03b8t , results in Xt containing the conditioning information which in turn leads to\u2190\u2212 \u03bb \u03b8t (Xt) guiding the number of components in Xt to be consistent with the conditioning information too as verified in our experiments. Further, any errors in the approximation for A\u03b8t (y\nadd, i|Xt) are fixed by further applications of the guided score function, highlighting the benefits of our combined autoregressive and diffusion based approach."
        },
        {
            "heading": "D Experiment Details",
            "text": "Our code is available at https://github.com/andrew-cr/jump-diffusion"
        },
        {
            "heading": "D.1 Molecules",
            "text": ""
        },
        {
            "heading": "D.1.1 Network Architecture",
            "text": "Backbone For our backbone network architecture, we used the EGNN used in [8]. This is a specially designed graph neural network applied to the point cloud treating it as a fully connected graph. A special equivariant update is used, operating only on distances between atoms. We refer to [8] for the specific details on the architecture. We used the same size network as used in [8]\u2019s QM9 experiments, specifically there are 9 layers, with a hidden node feature size of 256. The output of the EGNN is fed into a final output projection layer to give the score network output s\u03b8t (Xt).\nComponent number prediction To obtain p\u03b80|t(n0|Xt), we take the embedding produced by the EGNN before the final output embedding layer and pass it through 8 transformer layers each consisting of a self-attention block and an MLP block applied channel wise. Our transformer model dimension is 128 and so we project the EGNN embedding output down to 128 before entering into the transformer layers. We then take the output of the transformer and take the average embedding over all nodes. This embedding is then passed through a final projection layer to give softmax logits over the p\u03b80|t(n0|Xt) distribution.\nAutoregressive Distribution Our A\u03b8t (yadd, i|Xt) network has to predict the position and features for a new atom when it is added to the molecule. Since the point cloud is permutation invariant, we do not need to predict i and so we just need to parameterize A\u03b8t (y\nadd|Xt). We found the network to perform the best if the network first predicts the nearest atom to the new atom and then a vector from that atom to the location of the new atom. To achieve this, we first predict softmax logits for a distribution over the nearest atom by applying a projection to the embedding output from the previously described transformer block. During training, the output of this distribution can be directly supervised by a cross entropy loss. Given the nearest atom, we then need to predict the position and features of the new atom to add. We do this by passing in the embedding generated by the EGNN and original point cloud features into a new transformer block of the same size as that used for p\u03b80|t(n0|Xt). We also input the distances from the nearest atom to all other atoms in the molecule currently as an additional feature. To obtain the position of the new atom, we will take a weighted sum of all the vectors between the nearest atom and other atoms in the molecule. This is to make it easy for the network to create new atoms \u2018in plane\u2019 with existing atoms which is useful for e.g. completing rings that have to remain in the same plane. To calculate the weights for the vectors, we apply an output projection to the output of the transformer block. The new atom features (atom type and charge) are generated by a separate output projection from the transformer block. For the position and features, A\u03b8t (y\nadd|Xt) outputs both a mean and a standard deviation for a Gaussian distribution. For the position distribution, we set the standard deviation to be isotropic to remain equivariant to rotations. In total our model has around 7.3 million parameters."
        },
        {
            "heading": "D.1.2 Training",
            "text": "We train our model for 1.3 million iterations at a batch size of 64. We use the Adam optimizer with learning rate 0.00003. We also keep a running exponential moving average of the network weights that is used during sampling as is standard for training diffusion models [2, 3, 16] with a decay parameter of 0.9999. We train on the 100K molecules contained in the QM9 training split. We model hydrogens explicitly. Training a model requires approximately 7 days on a single GPU which was done on an Academic cluster.\nIn [8] the atom type is encoded as a one-hot vector and diffused as a continuous variable along with the positions and charge values for all atoms. They found that multiplying the one-hot vectors by 0.25 to boost performance by allowing the atom-type to be decided later on in the diffusion process. We instead multiply the one-hot vectors by 4 so that atom-type is decided early on in the diffusion process which improves our guided performance when conditioning on certain atom-types being\npresent. We found our model is robust to this change and achieves similar sample quality to [8] as shown in Table 2.\nWhen deleting dimensions, we first shuffle the ordering of the nodes and then delete the final n0 \u2212 nt nodes. The cross entropy loss weighting in (25) is set to 1.\nFollowing [8] we train our model to operate within the center of mass (CoM) zero subspace of possible molecule positions. The means, throughout the forward and backward process, the average position of an atom is 0. In our transdimensional framework, this is achieved by first deleting any atoms required under the forward component deletion process. We then move the molecule such that its CoM is 0. We then add CoM free noise such that the noisy molecule also has CoM= 0. Our score model s\u03b8t is parameterized through a noise prediction model \u03f5 \u03b8 t which is trained to predict the CoM free noise that was added. Therefore, our score network learns suitable directions to maintain the process on the CoM= 0 subspace. For the position prediction from A\u03b8t (y\nadd|Xt) we train it to predict the new atom position from the current molecules reference frame. When the new atom is added, we then update all atom positions such that CoM= 0 is maintained."
        },
        {
            "heading": "D.1.3 Sampling",
            "text": "During sampling we found that adding corrector steps [3] improved sample quality. Intuitively, corrector steps form a process that has pt(X) as its stationary distribution rather than the process progressing toward p0(X). We use the same method to determine the corrector step size \u03b6 as in [3]. For the conditional generation tasks, we also found it useful to include corrector steps for the component generation process. As shown in [29], corrector steps in discrete spaces can be achieved by simulating with a rate that is the addition of the forward and backward rates. We achieve this in the context of trans-dimensional modeling by first simulating a possible insertion using \u2190\u2212 \u03bb \u03b8t and then simulating a possible deletion using\u2192\u03bb t. We describe our overall sampling algorithm in Algorithm 2.\nAlgorithm 2: Sampling the Generative Process with Corrector Steps Input: Number of corrector steps C t\u2190 T X \u223c pref(X) = I{n = 1}N (x; 0, Id) while t > 0 do\nif u < \u2190\u2212 \u03bb \u03b8t (X)\u03b4t with u \u223c U(0, 1) then\nSample xadd, i \u223c A\u03b8t (xadd, i|X) X\u2190 ins(X,xadd, i)\nend x\u2190 x\u2212 \u2190\u2212 b \u03b8t (X)\u03b4t+ gt \u221a \u03b4t\u03f5 with \u03f5 \u223c N (0, Ind) for c = [1, . . . , C] do x\u2190 x+ \u03b6s\u03b8t\u2212\u03b4t(X) + \u221a 2\u03b6\u03f5 with \u03f5 \u223c N (0, Ind)\nif u < \u2190\u2212 \u03bb \u03b8t\u2212\u03b4t(X)\u03b4t with u \u223c U(0, 1) then\nSample xadd, i \u223c A\u03b8t\u2212\u03b4t(xadd, i|X) X\u2190 ins(X,xadd, i)\nend if u <\u2192\u03bb t\u2212\u03b4t(n)\u03b4t with u \u223c U(0, 1) then\nX\u2190 del(X, i) with i \u223c Kdel(i|n) end\nend X\u2190 (n,x), t\u2190 t\u2212 \u03b4t\nend"
        },
        {
            "heading": "D.1.4 Evaluation",
            "text": "Unconditional For our unconditional sampling evaluation, we start adding corrector steps when t < 0.1T in the backward process and use 5 corrector steps without the corrector steps on the number of components. We set \u03b4 = 0.05 for t > 0.5T and \u03b4 = 0.001 for t < 0.5T such that the total number\nof network evaluations is 1000. We show the distribution of sizes of molecules generated by our model in Figure 6 and show more unconditional samples in Figure 7. We find our model consistently generates realistic molecules and achieves a size distribution similar to the training dataset even though this is not explicitly trained and arises from sampling our backward rate \u2190\u2212 \u03bb \u03b8t . Since we are numerically integrating a continuous time process and approximating the true time reversal rate \u2190\u2212 \u03bb \u2217t , some approximation error is expected. For this experiment, sampling all of our models and ablations takes approximately 2 GPU days on Nvidia 1080Ti GPUs.\nConditional For evaluating applying conditional diffusion guidance to our model, we choose 10 conditioning tasks that each result in a different distribution of target dimensions. The task is to produce molecules that include at least a certain number of target atom types. We then guide the first set of atoms generated by the model to have these desired atom types. The tasks chosen are given in Table 5. Molecules in the training dataset that meet the conditions in each task have a different distribution of sizes. The tasks were chosen so that we have an approximately linearly increasing mean number of atoms for molecules that meet the condition. We also require that there are at least 100 examples of molecules that meet the condition within the training dataset.\nFor sampling when using conditional diffusion guidance, we use 3 corrector steps throughout the backward process with \u03b4t = 0.001. For these conditional tasks, we include the corrector steps on the number of components. We show the distribution of dimensions for each task from the training dataset and from our generated samples in Figure 8. Our metrics are calculated by first drawing 1000 samples for each conditioning task and then finding the Hellinger distance between the size distribution generated by our method (orange diagonal hashing in Figure 8) and the size distribution for molecules in the training dataset that match the conditions of the task (green no hashing in Figure 8). We find that indeed our model when guided by diffusion guidance can automatically produce a size distribution close to the ground truth size distribution found in the dataset for that conditioning value. We show samples generated by our conditionally guided model in Figure 9. We can see that our model can generate realistic molecules that include the required atom types and are of a suitable size. For this experiment, sampling all of our models and ablations takes approximately 13 GPU days on Nvidia 1080Ti GPUs.\nInterpolations For our interpolations experiments, we follow the set up of [8] who train a new model conditioned on the polarizability of molecules in the dataset. We train a conditional version of our model which can be achieved by simply adding in the polarizability as an additional feature input to our backbone network and re-using all the same hyperparameters. We show more examples of interpolations in Figure 10."
        },
        {
            "heading": "D.1.5 Ablations",
            "text": "For our main model, we set \u2192\u03bb t<0.1T = 0 to ensure that all dimensions are added with enough generation time remaining for the diffusion process to finalize all state values. To verify this setting, we compare its performance with\u2192\u03bb t<0.03T = 0 and \u2192 \u03bb t<0.3T = 0. We show our results in Table 6. We find that the \u2192\u03bb t<0.03T = 0 setting to generate reasonable sample quality but incur some extra dimension error due to the generative process sometimes observing a lack of dimensions near t = 0 and adding too many dimensions. We observed the same effect in the paper for when setting \u2192 \u03bb t to be constant for all t in Table 3. Further, the setting \u2192 \u03bb t<0.3T = 0 also results in increased dimension error due to there being less opportunity for the guidance model to supervise the number of dimensions. We find that\u2192\u03bb t<0.1T = 0 to be a reasonable trade-off between these effects."
        },
        {
            "heading": "D.1.6 Uniqueness and Novelty Metrics",
            "text": "We here investigate sample diversity and novelty of our unconditional generative models. We measure uniqueness by computing the chemical graph corresponding to each generated sample and measure what proportion of the 10000 produced samples have a unique chemical graph amongst this set of 10000 as is done in [8]. We show our results in Table 7 and find our TDDM method to have slightly lower levels of uniqueness when compared to the fixed dimension diffusion model baseline.\nMeasuring novelty on generative models trained on the QM9 dataset is challenging because the QM9 dataset contains an exhaustive enumeration of all molecules that satisfy certain predefined constraints [46], [8]. Therefore, if a novel molecule is produced it means the generative model has failed to capture some of the physical properties of the dataset and indeed it is found in [8] that during training, as the model improved, novelty decreased. Novelty is therefore not typically included in evaluating molecular diffusion models. For completeness, we include the novelty scores in Table 7 as a comparison to the results presented in [8] Appendix C. We find that our samples are closer to the statistics of the training dataset whilst still producing \u2018novel\u2019 samples at a consistent rate.\nD.2 Video"
        },
        {
            "heading": "D.2.1 Dataset",
            "text": "We used the VP2 benchmark, which consists of 35 000 videos, each 35 frames long. The videos are evenly divided among seven tasks, namely: push {red, green, blue} button, open {slide, drawer}, push {upright block, flat block} off table. The 5000 videos for each task were collected using a scripted task-specific policy operating in the RoboDesk environment [40]. They sample an action vector at every step during data generation by adding i.i.d. Gaussian\nnoise to each dimension of the action vector output by the scripted policy. For each task, they sample 2500 videos with noise standard deviation 0.1 and 2500 videos with standard deviation 0.2. We filter out the lower-quality trajectories sampled with noise standard deviation 0.2, and so use only the 17 500 videos (2500) per task with noise standard deviation 0.1. We convert these videos to 32\u00d7 32 resolution and then, so that the data we train on has varying lengths, we create each training example by sampling a length l from a uniform distribution over {2, . . . , 35} and then taking a random l-frame subset of the video."
        },
        {
            "heading": "D.2.2 Forward Process",
            "text": "The video domain differs from molecules in two important ways. The first is that videos cannot be reasonably treated as a permutation-invariant set. This is because the order of the frames matters. Secondly, generating a full new component for the molecules with a single pass autoregressive network is feasible, however, a component for the videos is a full frame which is challenging for a single pass autoregressive network to generate. We design our forward process to overcome these challenges.\nWe define our forward process to delete frames in a random order. This means that during generation, frames can be generated in any order in the reverse process, enabling more conditioning tasks since we can always ensure that whichever frames we want to condition on are added first. Further, we use a non-isotropic noise schedule by adding noise just to the frame that is about to be deleted. Once it is deleted, we then start noising the next randomly chosen frame. This is so that, in the backward direction, when a new frame is added, it is simply Gaussian noise. Then the score network will fully denoise that new frame before the next new frame is added. We now specify exactly how our forward process is constructed.\nWe enable random-order deletion by applying an initial shuffling operation occurring at time t = 0. Before this operation, we represent the video x as an ordered sequence of frames, x0 = [x1,x2, . . . ,xn0 ]. During shuffling, we sample a random permutation \u03c0 of the integers 1, . . . , n0. Then the frames are kept in the same order, but annotated with an index variable so that we have x0+ = [(x (1) 0+ , \u03c0(1)), (x (2) 0+ , \u03c0(2)), . . . , (x (n0) 0+ , \u03c0(n0))].\nWe will run the forward process from t = 0 to t = 100N . We will set the forward rate such we delete down from nt to nt \u2212 1 at time (N \u2212 nt + 1)100. This is achieved heuristically by setting\n\u2192 \u03bb t(nt) = { 0 for t < (N \u2212 nt + 1)100, \u221e for t \u2265 (N \u2212 nt + 1)100.\nWe can see that at time t = (N \u2212nt +1)100 we will quickly delete down from nt to nt\u2212 1 at which point\u2192\u03bb t(nt) will become 0 thus stopping deletion until the process arrives at the next multiple of 100 in time. When we hit a deletion event, we delete the frame from Xt that has the current highest index variable \u03c0(n). In other words\nKdel(i|Xt) =\n{ 1 for nt = x (i) t [2],\n0 otherwise\nwhere we use x(i)t [2] to refer to the shuffle index variable for the ith current frame in xt.\nWe now provide an example progression of the forward deletion process. Assume we have n0 = 4, N = 5 and sample a permutation such that \u03c0(1) = 3, \u03c0(2) = 2, \u03c0(3) = 4, and \u03c0(4) = 1. Initially\nthe state is augmented to include the shuffle index. Then the forward process progresses from t = 0 to t = 500 with components being deleted in descending order of the shuffle index\nx0+ = [(x (1) t , 3), (x (2) t , 2), (x (3) t , 4), (x (4) t , 1)]\nx100+ = [(x (1) t , 3), (x (2) t , 2), (x (3) t , 4), (x (4) t , 1)]\nx200+ = [(x (1) t , 3), (x (2) t , 2), (x (4) t , 1)]\nx300+ = [(x (2) t , 2), (x (4) t , 1)]\nx400+ = [(x (4) t , 1)]\nIn this example, due to the random permutation sampled, the final video frame remained after all others had been deleted. Note that the order of frames is preserved as we delete frames in the forward process although the spacing between them can change as we delete frames in the middle.\nBetween jumps, we use a noising process to add noise to frames. The noising process is non-isotropic in that it adds noise to different frames at different rates such that the a frame is noised only in the time window immediately preceding its deletion. For component i \u2208 [1, . . . , nt], we set the forward noising process such that pt|0(x (i) t |x (i) 0 ,Mt) = N (x (i) t ;x (i) 0 , \u03c3t(x (i) t )\n2) where x(i)0 is the clean frame corresponding to x(i)t as given by the mask Mt and \u03c3t(x (i) t ) follows\n\u03c3t(x (i) t ) =  0 for t < (N \u2212 x(i)t [2])100, 100 for t > (N \u2212 x(i)t [2])100, t\u2212 (N \u2212 x(i)t [2])100 for (N \u2212 x (i) t [2])100 \u2264 t \u2264 (N \u2212 x (i) t [2] + 1)100\nwhere we again use x(i)t [2] for the shuffle index of component i. This is the VE-SDE from [3] applied to each frame in turn. We note that we only add noise to the state values on not the shuffle index itself. The SDE parameters that result in the VE-SDE are \u2212\u2192 b t = 0 and \u2212\u2192g t = \u221a 2t\u2212 2(N \u2212 x(i)t [2])100."
        },
        {
            "heading": "D.2.3 Sampling the Backward Process",
            "text": "When t is not at a multiple of 100, the forward process is purely adding Gaussian noise, and so the reverse process is also purely operating on the continuous dimensions. We use the Heun sampler proposed by [16] to update the continuous dimensions in this case, and also a variation of their discretisation of t - specifically to update from e.g. t = 600 to t = 500, we use their discretization of t as if the maximum value was 100 and then offset all values by 500.\nTo invert the dimension deletion process, we can use Proposition 3 to derive our reverse dimension generation process. We re-write our parameterized \u2190\u2212 \u03bb \u03b8t using Proposition 3 as\n\u2190\u2212 \u03bb \u03b8t (Xt) = \u2192 \u03bb t(nt + 1)Ep\u03b8\n0|t(n0|Xt) [ pt|0(nt + 1|n0) pt|0(nt|n0) ] At each time multiple of 100 in the backward process, we will have an opportunity to add a component. At this time point, we estimate the expectation with a single sample n0 \u223c p\u03b80|t(n0|Xt). If n0 > nt then \u2190\u2212 \u03bb \u03b8t (Xt) =\u221e. The new component will then be added at which point \u2190\u2212 \u03bb \u03b8t (Xt) becomes 0 for the remainder of this block of time due to nt becoming nt + 1. If n0 = nt then \u2190\u2212 \u03bb \u03b8t (Xt) = 0 and no new component is added. \u2190\u2212 \u03bb \u03b8t (Xt) will continue to be 0 for the remainder of the backward process once an opportunity to add a component is not used.\nWhen a new frame is added, we use A\u03b8t (y add, i|Xt) to decide where the frame is added and its initial value. Since when we delete a frame it is fully noised, A\u03b8t (y add, i|Xt) can simply predict Gaussian noise for the new frame yadd. However, A\u03b8t (y add, i|Xt) will still learn to predict a suitable location i to place the new frame such that backward process is the reversal of the forward.\nWe give an example simulation from the backward generative process in Figure 11."
        },
        {
            "heading": "D.2.4 Network Architecture",
            "text": "Our video diffusion network architecture is based on the U-net used by [42], which takes as input the index of each frame within the video, and uses the differences between these indices to control the interactions between frames via an attention mechanism. Since, during generation, we do not know the final position of each frame within the x0, we instead pass in its position within the ordered sequence xt.\nOne further difference is that, since we are perform non-isotropic diffusion, the standard deviation of the added noise will differ between frames. We adapt to this by performing preconditioning, and inputting the timestep embedding, separately for each frame x(i)t based on \u03c3t(x (i) t ) instead of basing them on the global diffusion timestep t. Our timestep embedding and pre- and post-conditioning of network inputs/outputs are as suggested by [16], other than being done on a per-frame basis. The architecture from [42] with these changes applied then gives us our score network s\u03b8t .\nWhile it would be possible to train a single network that estimates the score and all quantities needed for modelling jumps, we chose to train two separate networks in order to factorize our exploration of the design space. These were the score network s\u03b8t , and the rate and index prediction network modeling p\u03b80|t(n0|Xt) and A \u03b8 t (i|Xt). The rate and index prediction network is similar to the first half of the score network, in that it uses all U-net blocks up to and including the middle one. We then flatten the 512\u00d7 4\u00d7 4 hidden state for each frame after this block such that, for an nt frame input, we obtain a nt \u00d7 8192 hidden state. These are fed through a 1D convolution with kernel size 2 and zero-padding of size 1 on each end, reducing the hidden state to (nt + 1)\u00d7 128, which is in turn fed\nthrough a ReLU activation function. This hidden state is then fed into three separate heads. One head maps it to the parameters of A\u03b8t (i|Xt) via a 1D convolution of kernel size 3. The output of size (nt + 1) is fed through a softmax to provide the categorical distribution A\u03b8t (i|Xt). The second head averages the hidden state over the \u201cframe\u201d dimension, producing a 128-dimensional vector. This is fed through a single linear layer and a softmax to parameterize p\u03b80|t(n0|Xt). Finally, the third head consists of a 1D convolution of kernel size 3 with 35 output channels. The (nt + 1)\u00d7 35 output is fed through a softmax to parameterize distributions over the number of frames that were deleted from X0 which came before the first in xt, the number of frames from X0 which were deleted between each pair of frames in xt, and the number deleted after the last frame in xt. We do not use this head at inference-time but found that including it improved the performance of the other heads by helping the network learn better representations.\nFor a final performance improvement, we note that under our forward process there is only ever one \u201cnoised\u201d frame in xt, while there are sometimes many clean frames. Since the cost of running our architecture scales with the number of frames, running it on many clean frames may significantly increase the cost while providing little improvement to performance. We therefore only feed into the architecture the \u201cnoised\u201d frame, the two closest \u201cclean\u201d frames before it, and the two closest \u201cclean\u201d frames after it. See our released source code for the full implementation of this architecture."
        },
        {
            "heading": "D.2.5 Training",
            "text": "To sample t during training, we adapt the log-normal distribution suggested by [16] in the context of isotropic diffusion over a single image. To apply it to our non-isotropic video diffusion, we first sample which frames have been deleted, which exist with no noise, and which have had noise added, by sampling the timestep from a uniform distribution and simulating our proposed forward process. We then simply change the noise standard deviation for the noisy frame, replacing it with a sample from the log-normal distribution. The normal distribution underlying our log-normal has mean \u22120.6 and standard deviation 1.8. This can be interpreted as sampling the timestep from a mixture of log-normal distributions, 1N \u2211N\u22121 i=0 LN (t\u2212 100i;\u22120.6, 1.82). Here, the mixture index i can be interpreted as controlling the number of deleted frames.\nWe use the same loss weighting as [16] but, similarly to our use of preconditioning, compute the weighting separately for each frame x(i)t as a function of \u03c3t(x (i) t ) to account for the non-isotropic noise."
        },
        {
            "heading": "D.2.6 Perceptual Quality Metrics",
            "text": "We now verify that our reverse process does not have any degradation in quality during the generation as more dimensions are added. We generate 10000 videos and throw away the 278 that were sampled to have only two frames. We then compute the FID score for individual frames in each of the remaining 9722 videos. We group together the scores for all the first frames to be generated in the reverse process and then for the second frame to be generated and so on. We show our results in Table 8. We find that when a frame is inserted has no apparent effect on perceptual quality and conclude that there is no overall degradation in quality as our sampling process progresses. We note that the absolute value of these FID scores may not be meaningful due to the RoboDesk dataset being far out of distribution for the Inception network used to calculate FID scores. We can visually confirm good sample quality from Figure 5."
        },
        {
            "heading": "E Broader Impacts",
            "text": "In this work, we presented a general method for performing generative modeling on datasets of varying dimensionality. We have not focused on applications and instead present a generic method.\nAlong with other generic methods for generative modeling, we must consider the potential negative social impacts that these models can cause when inappropriately used. As generative modeling capabilities increase, it becomes simpler to generate fake content which can be used to spread misinformation. In addition to this, generative models are becoming embedded into larger systems that then have real effects on society. There will be biases present within the generations created by the model which in turn can reinforce these biases when the model\u2019s outputs are used within wider systems. In order to mitigate these harms, applications of generative models to real world problems must be accompanied with studies into their biases and potential ways they can be misused. Further, public releases of models must be accompanied with model cards [47] explaining the biases, limitations and intended uses of the model."
        }
    ],
    "title": "Trans-Dimensional Generative Modeling via Jump Diffusion Models",
    "year": 2023
}