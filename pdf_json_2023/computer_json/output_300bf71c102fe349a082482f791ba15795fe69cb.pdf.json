{
    "abstractText": "Complex, long-horizon planning and its combinatorial nature pose steep challenges for learning-based agents. Difficulties in such settings are exacerbated in low data regimes where over-fitting stifles generalization and compounding errors hurt accuracy. In this work, we explore the use of an often unused source of auxiliary supervision: language. Inspired by recent advances in transformer-based models, we train agents with an instruction prediction loss that encourages learning temporally extended representations that operate at a high level of abstraction. Concretely, we demonstrate that instruction modeling significantly improves performance in planning environments when training with a limited number of demonstrations on the BabyAI and Crafter benchmarks. In further analysis we find that instruction modeling is most important for tasks that require complex reasoning, while understandably offering smaller gains in environments that require simple plans. More details and code can be found at https://github.com/jhejna/instruction-prediction.",
    "authors": [
        {
            "affiliations": [],
            "name": "Joey Hejna"
        },
        {
            "affiliations": [],
            "name": "Pieter Abbeel"
        },
        {
            "affiliations": [],
            "name": "Lerrel Pinto"
        }
    ],
    "id": "SP:6aaf1f8bce432d349d774c12b2940dfbb7d56ba2",
    "references": [
        {
            "authors": [
                "A. Akakzia",
                "C. Colas",
                "P.-Y. Oudeyer",
                "M. Chetouani",
                "O. Sigaud"
            ],
            "title": "Grounding Language to AutonomouslyAcquired Skills via Goal Generation",
            "venue": "ICLR 2021.",
            "year": 2021
        },
        {
            "authors": [
                "A. Anand",
                "E. Racah",
                "S. Ozair",
                "Y. Bengio",
                "M.-A. C\u00f4t\u00e9",
                "R.D. Hjelm"
            ],
            "title": "Unsupervised state representation learning in atari",
            "venue": "Proceedings of the 33rd International Conference on Neural Information Processing Systems, 8769\u20138782.",
            "year": 2019
        },
        {
            "authors": [
                "P. Anderson",
                "Q. Wu",
                "D. Teney",
                "J. Bruce",
                "M. Johnson",
                "N. S\u00fcnderhauf",
                "I. Reid",
                "S. Gould",
                "A. van den Hengel"
            ],
            "title": "Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments",
            "venue": "In Proceedings of the IEEE Conference on Computer",
            "year": 2018
        },
        {
            "authors": [
                "J. Andreas",
                "D. Klein",
                "S. Levine"
            ],
            "title": "Modular multitask reinforcement learning with policy sketches",
            "venue": "International Conference on Machine Learning, 166\u2013175. PMLR.",
            "year": 2017
        },
        {
            "authors": [
                "J. Andreas",
                "D. Klein",
                "S. Levine"
            ],
            "title": "Learning with Latent Language",
            "venue": "NAACL-HLT.",
            "year": 2018
        },
        {
            "authors": [
                "D. Bahdanau",
                "F. Hill",
                "J. Leike",
                "E. Hughes",
                "A. Hosseini",
                "P. Kohli",
                "E. Grefenstette"
            ],
            "title": "Learning to Understand Goal Specifications by Modelling Reward",
            "venue": "International Conference on Learning Representations.",
            "year": 2018
        },
        {
            "authors": [
                "H. Chan",
                "Y. Wu",
                "J. Kiros",
                "S. Fidler",
                "J. Ba"
            ],
            "title": "ACTRCE: Augmenting Experience via Teacher\u2019s Advice For Multi-Goal Reinforcement Learning",
            "venue": "1st Workshop on Goal Specifications for Reinforcement Learning, Workshop held jointly at ICML, IJCAI, AAMAS.",
            "year": 2018
        },
        {
            "authors": [
                "D.S. Chaplot",
                "K.M. Sathyendra",
                "R.K. Pasumarthi",
                "D. Rajagopal",
                "R. Salakhutdinov"
            ],
            "title": "Gated-attention architectures for task-oriented language grounding",
            "venue": "ThirtySecond AAAI Conference on Artificial Intelligence.",
            "year": 2018
        },
        {
            "authors": [
                "H. Chen",
                "A. Suhr",
                "D. Misra",
                "N. Snavely",
                "Y. Artzi"
            ],
            "title": "Touchdown: Natural language navigation and spatial reasoning in visual street environments",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 12538\u201312547.",
            "year": 2019
        },
        {
            "authors": [
                "K. Chen",
                "J.K. Chen",
                "J. Chuang",
                "M. V\u00e1zquez",
                "S. Savarese"
            ],
            "title": "Topological Planning with Transformers for Vision-and-Language Navigation",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 11276\u201311286.",
            "year": 2021
        },
        {
            "authors": [
                "L. Chen",
                "K. Lu",
                "A. Rajeswaran",
                "K. Lee",
                "A. Grover",
                "M. Laskin",
                "P. Abbeel",
                "A. Srinivas",
                "I. Mordatch"
            ],
            "title": "Decision transformer: Reinforcement learning via sequence modeling",
            "venue": "arXiv preprint arXiv:2106.01345.",
            "year": 2021
        },
        {
            "authors": [
                "V. Chen",
                "A. Gupta",
                "K. Marino"
            ],
            "title": "Ask Your Humans: Using Human Instructions to Improve Generalization in Reinforcement Learning",
            "venue": "International Conference on Learning Representations.",
            "year": 2021
        },
        {
            "authors": [
                "X. Chen",
                "S. Toyer",
                "C. Wild",
                "S. Emmons",
                "I. Fischer",
                "K.-H. Lee",
                "N. Alex",
                "S.H. Wang",
                "P. Luo",
                "S. Russell",
                "P. Abbeel",
                "R. Shah"
            ],
            "title": "An Empirical Investigation of Representation Learning for Imitation",
            "venue": "Thirty-fifth Conference on Neural Information Processing Systems Datasets",
            "year": 2021
        },
        {
            "authors": [
                "M. Chevalier-Boisvert",
                "D. Bahdanau",
                "S. Lahlou",
                "L. Willems",
                "C. Saharia",
                "T.H. Nguyen",
                "Y. Bengio"
            ],
            "title": "BabyAI: First Steps Towards Grounded Language Learning With a Human In the Loop",
            "venue": "International Conference on Learning Representations.",
            "year": 2019
        },
        {
            "authors": [
                "G. Cideron",
                "M. Seurin",
                "F. Strub",
                "O. Pietquin"
            ],
            "title": "Higher: Improving instruction following with hindsight generation for experience replay",
            "venue": "2020 IEEE Symposium Series on Computational Intelligence (SSCI), 225\u2013232. IEEE.",
            "year": 2020
        },
        {
            "authors": [
                "A. Dosovitskiy",
                "L. Beyer",
                "A. Kolesnikov",
                "D. Weissenborn",
                "X. Zhai",
                "T. Unterthiner",
                "M. Dehghani",
                "M. Minderer",
                "G. Heigold",
                "S. Gelly",
                "J. Uszkoreit",
                "N. Houlsby"
            ],
            "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
            "venue": "International Conference on Learn-",
            "year": 2021
        },
        {
            "authors": [
                "M. Edmonds",
                "F. Gao",
                "H. Liu",
                "X. Xie",
                "S. Qi",
                "B. Rothrock",
                "Y. Zhu",
                "Y.N. Wu",
                "H. Lu",
                "S.-C. Zhu"
            ],
            "title": "A tale of two explanations: Enhancing human trust by explaining robot behavior",
            "venue": "Science Robotics, 4(37): eaay4663.",
            "year": 2019
        },
        {
            "authors": [
                "M. Edmonds",
                "F. Gao",
                "X. Xie",
                "H. Liu",
                "S. Qi",
                "Y. Zhu",
                "B. Rothrock",
                "S.-C. Zhu"
            ],
            "title": "Feeling the force: Integrating force and pose for fluent discovery through imitation learning to open medicine bottles",
            "venue": "2017 IEEE/RSJ International Conference on Intelligent Robots and Systems",
            "year": 2017
        },
        {
            "authors": [
                "D. Fried",
                "R. Hu",
                "V. Cirik",
                "A. Rohrbach",
                "J. Andreas",
                "L.-P. Morency",
                "T. Berg-Kirkpatrick",
                "K. Saenko",
                "D. Klein",
                "T. Darrell"
            ],
            "title": "Speaker-follower models for visionand-language navigation",
            "venue": "Proceedings of the 32nd International Conference on Neural Information Processing Sys-",
            "year": 2018
        },
        {
            "authors": [
                "J. Fu",
                "A. Korattikara",
                "S. Levine",
                "S. Guadarrama"
            ],
            "title": "From language to goals: Inverse reinforcement learning for vision-based instruction following",
            "venue": "arXiv preprint arXiv:1902.07742.",
            "year": 2019
        },
        {
            "authors": [
                "L. Gleitman",
                "A. Papafragou"
            ],
            "title": "Language and thought",
            "venue": "Cambridge University Press.",
            "year": 2005
        },
        {
            "authors": [
                "P. Goyal",
                "R.J. Mooney",
                "S. Niekum"
            ],
            "title": "Zero-shot Task Adaptation using Natural Language",
            "venue": "arXiv preprint arXiv:2106.02972.",
            "year": 2021
        },
        {
            "authors": [
                "P. Goyal",
                "S. Niekum",
                "R.J. Mooney"
            ],
            "title": "Using natural language for reward shaping in reinforcement learning",
            "venue": "arXiv preprint arXiv:1903.02020.",
            "year": 2019
        },
        {
            "authors": [
                "K.M. Hermann",
                "F. Hill",
                "S. Green",
                "F. Wang",
                "R. Faulkner",
                "H. Soyer",
                "D. Szepesvari",
                "W.M. Czarnecki",
                "M. Jaderberg",
                "D Teplyashin"
            ],
            "title": "Grounded language learning in a simulated 3d world",
            "venue": "arXiv preprint arXiv:1706.06551",
            "year": 2017
        },
        {
            "authors": [
                "F. Hill",
                "S. Mokra",
                "N. Wong",
                "T. Harley"
            ],
            "title": "Human instruction-following with deep reinforcement learning via transfer-learning from text",
            "venue": "arXiv preprint arXiv:2005.09382.",
            "year": 2020
        },
        {
            "authors": [
                "H. Hu",
                "D. Yarats",
                "Q. Gong",
                "Y. Tian",
                "M. Lewis"
            ],
            "title": "Hierarchical Decision Making by Generating and Following Natural Language Instructions",
            "venue": "Advances in Neural Information Processing Systems, 32: 10025\u201310034.",
            "year": 2019
        },
        {
            "authors": [
                "M. Jaderberg",
                "V. Mnih",
                "W.M. Czarnecki",
                "T. Schaul",
                "J.Z. Leibo",
                "D. Silver",
                "K. Kavukcuoglu"
            ],
            "title": "Reinforcement learning with unsupervised auxiliary tasks",
            "venue": "International Conference on Learning Representations.",
            "year": 2017
        },
        {
            "authors": [
                "Y. Jiang",
                "S.S. Gu",
                "K.P. Murphy",
                "C. Finn"
            ],
            "title": "Language as an Abstraction for Hierarchical Deep Reinforcement Learning",
            "venue": "Advances in Neural Information Processing Systems, 32: 9419\u20139431.",
            "year": 2019
        },
        {
            "authors": [
                "L.P. Kaelbling",
                "M.L. Littman",
                "A.R. Cassandra"
            ],
            "title": "Planning and acting in partially observable stochastic domains",
            "venue": "Artificial intelligence, 101(1-2): 99\u2013134.",
            "year": 1998
        },
        {
            "authors": [
                "J. Kanu",
                "E. Dessalene",
                "X. Lin",
                "C. Fermuller",
                "Y. Aloimonos"
            ],
            "title": "Following instructions by imagining and reaching visual goals",
            "venue": "arXiv preprint arXiv:2001.09373.",
            "year": 2020
        },
        {
            "authors": [
                "J. Krantz",
                "E. Wijmans",
                "A. Majumdar",
                "D. Batra",
                "S. Lee"
            ],
            "title": "Beyond the nav-graph: Vision-and-language navigation in continuous environments",
            "venue": "European Conference on Computer Vision, 104\u2013120. Springer.",
            "year": 2020
        },
        {
            "authors": [
                "M. Laskin",
                "A. Srinivas",
                "P. Abbeel"
            ],
            "title": "Curl: Contrastive unsupervised representations for reinforcement learning",
            "venue": "International Conference on Machine Learning, 5639\u20135650. PMLR.",
            "year": 2020
        },
        {
            "authors": [
                "X. Lin",
                "G. Bertasius",
                "J. Wang",
                "S.-F. Chang",
                "D. Parikh",
                "L. Torresani"
            ],
            "title": "VX2TEXT: End-to-End Learning of Video-Based Text Generation From Multimodal Inputs",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 7005\u20137015.",
            "year": 2021
        },
        {
            "authors": [
                "J. Luketina",
                "N. Nardelli",
                "G. Farquhar",
                "J. Foerster",
                "J. Andreas",
                "E. Grefenstette",
                "S. Whiteson",
                "T. Rockt\u00e4schel"
            ],
            "title": "A survey of reinforcement learning informed by natural language",
            "venue": "arXiv preprint arXiv:1906.03926.",
            "year": 2019
        },
        {
            "authors": [
                "S. Mirchandani",
                "S. Karamcheti",
                "D. Sadigh"
            ],
            "title": "ELLA: Exploration through Learned Language Abstraction",
            "venue": "arXiv preprint arXiv:2103.05825.",
            "year": 2021
        },
        {
            "authors": [
                "S. Nair",
                "E. Mitchell",
                "K. Chen",
                "B. Ichter",
                "S. Savarese",
                "C. Finn"
            ],
            "title": "Learning Language-Conditioned Robot Behavior from Offline Data and Crowd-Sourced Annotation",
            "venue": "Conference on Robot Learning (CoRL).",
            "year": 2021
        },
        {
            "authors": [
                "J. Oh",
                "S. Singh",
                "H. Lee",
                "P. Kohli"
            ],
            "title": "Zero-shot task generalization with multi-task deep reinforcement learning",
            "venue": "International Conference on Machine Learning, 2661\u2013 2670. PMLR.",
            "year": 2017
        },
        {
            "authors": [
                "E. Parisotto",
                "F. Song",
                "J. Rae",
                "R. Pascanu",
                "C. Gulcehre",
                "S. Jayakumar",
                "M. Jaderberg",
                "R.L. Kaufman",
                "A. Clark",
                "S Noury"
            ],
            "title": "Stabilizing transformers for reinforcement learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "A. Pashevich",
                "C. Schmid",
                "C. Sun"
            ],
            "title": "Episodic transformer for vision-and-language navigation",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, 15942\u201315952.",
            "year": 2021
        },
        {
            "authors": [
                "A. Radford",
                "K. Narasimhan",
                "T. Salimans",
                "I Sutskever"
            ],
            "title": "Improving language understanding by generative pre-training",
            "year": 2018
        },
        {
            "authors": [
                "S. Ross",
                "G. Gordon",
                "D. Bagnell"
            ],
            "title": "A reduction of imitation learning and structured prediction to no-regret online learning",
            "venue": "Proceedings of the fourteenth international conference on artificial intelligence and statistics, 627\u2013635. JMLR Workshop and Conference Proceedings.",
            "year": 2011
        },
        {
            "authors": [
                "M. Schwarzer",
                "A. Anand",
                "R. Goel",
                "R.D. Hjelm",
                "A. Courville",
                "P. Bachman"
            ],
            "title": "Data-Efficient Reinforcement Learning with Self-Predictive Representations",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "M. Shridhar",
                "L. Manuelli",
                "D. Fox"
            ],
            "title": "CLIPort: What and Where Pathways for Robotic Manipulation",
            "venue": "5th Annual Conference on Robot Learning.",
            "year": 2021
        },
        {
            "authors": [
                "M. Shridhar",
                "J. Thomason",
                "D. Gordon",
                "Y. Bisk",
                "W. Han",
                "R. Mottaghi",
                "L. Zettlemoyer",
                "D. Fox"
            ],
            "title": "ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks",
            "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR).",
            "year": 2020
        },
        {
            "authors": [
                "M. Shridhar",
                "X. Yuan",
                "M.-A. C\u00f4t\u00e9",
                "Y. Bisk",
                "A. Trischler",
                "M. Hausknecht"
            ],
            "title": "ALFWorld: Aligning Text and Embodied Environments for Interactive Learning",
            "venue": "Proceedings of the International Conference on Learning Representations (ICLR).",
            "year": 2021
        },
        {
            "authors": [
                "T. Shu",
                "C. Xiong",
                "R. Socher"
            ],
            "title": "Hierarchical and interpretable skill acquisition in multi-task reinforcement learning",
            "venue": "arXiv preprint arXiv:1712.07294.",
            "year": 2017
        },
        {
            "authors": [
                "S. Stepputtis",
                "J. Campbell",
                "M. Phielipp",
                "S. Lee",
                "C. Baral",
                "H. Ben Amor"
            ],
            "title": "Language-Conditioned Imitation Learning for Robot Manipulation Tasks",
            "venue": "Advances in Neural Information Processing Systems, 33.",
            "year": 2020
        },
        {
            "authors": [
                "A. Stooke",
                "K. Lee",
                "P. Abbeel",
                "M. Laskin"
            ],
            "title": "Decoupling representation learning from reinforcement learning",
            "venue": "International Conference on Machine Learning, 9870\u20139879. PMLR.",
            "year": 2021
        },
        {
            "authors": [
                "A. Vaswani",
                "N. Shazeer",
                "N. Parmar",
                "J. Uszkoreit",
                "L. Jones",
                "A.N. Gomez",
                "L. u. Kaiser",
                "I. Polosukhin"
            ],
            "title": "Attention is All you Need",
            "venue": "Guyon, I.; Luxburg, U. V.; Bengio, S.; Wallach, H.; Fergus, R.; Vishwanathan, S.; and Garnett, R., eds., Advances in Neural Information Process-",
            "year": 2017
        },
        {
            "authors": [
                "H. Wang",
                "K. Narasimhan"
            ],
            "title": "Grounding Language to Entities and Dynamics for Generalization in Reinforcement Learning",
            "venue": "arXiv preprint arXiv:2101.07393.",
            "year": 2021
        },
        {
            "authors": [
                "R. Wightman"
            ],
            "title": "PyTorch Image Models",
            "venue": "https://github. com/rwightman/pytorch-image-models.",
            "year": 2019
        },
        {
            "authors": [
                "E. Wijmans",
                "A. Kadian",
                "A. Morcos",
                "S. Lee",
                "I. Essa",
                "D. Parikh",
                "M. Savva",
                "D. Batra"
            ],
            "title": "DD-PPO: Learning near-perfect pointgoal navigators from 2.5 billion frames. arXiv preprint arXiv:1911.00357",
            "year": 2019
        },
        {
            "authors": [
                "E.C. Williams",
                "N. Gopalan",
                "M. Rhee",
                "S. Tellex"
            ],
            "title": "Learning to parse natural language to grounded reward functions with weak supervision",
            "venue": "2018 IEEE International Conference on Robotics and Automation (ICRA), 4430\u20134436. IEEE.",
            "year": 2018
        },
        {
            "authors": [
                "V. Zambaldi",
                "D. Raposo",
                "A. Santoro",
                "V. Bapst",
                "Y. Li",
                "I. Babuschkin",
                "K. Tuyls",
                "D. Reichert",
                "T. Lillicrap",
                "E Lockhart"
            ],
            "title": "Deep reinforcement learning",
            "year": 2018
        },
        {
            "authors": [
                "X. Representations. Zang",
                "A. Pokle",
                "M. V\u00e1zquez",
                "K. Chen",
                "J.C. Niebles",
                "A. Soto",
                "S. Savarese"
            ],
            "title": "Translating Navigation Instructions in Natural Language to a High-Level Plan for Be",
            "year": 2018
        },
        {
            "authors": [
                "RL setting in (Chen"
            ],
            "title": "2021b), which makes sense as rewards often depend on both states and actions",
            "year": 2021
        },
        {
            "authors": [
                "Pashevich",
                "Schmid",
                "Sun"
            ],
            "title": "2021) have also found that conditioning on entire action sequences leads to a degradation",
            "year": 2021
        },
        {
            "authors": [
                "Gupta",
                "Marino"
            ],
            "title": "2021), the authors train a high-level RNN to output the current language instruction. They then condition their low-level policy on the latent representation fed to the RNN that predicts instructions. While they show latent condition to be effective, transformers purposefully avoid encoding entire streams of data into a single vector, and instead operate on a token level",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "Introduction",
            "text": "Intelligent agents ought to be able to complete complex, long horizon tasks and generalize to new scenarios. Unfortunately, policies learned by modern deep-learning techniques often struggle to acquire either of these abilities. This is particularly true in planning regimes where multiple, complex, steps must be completed correctly in sequence to complete a task. Realistic constraints, such as partial observability, the underspecification of goals, or the sparse reward nature of many planning problems make learning even harder. Reinforcement learning approaches often struggle to effectively learn policies and require billions of environment interactions to produce effective solutions (Wijmans et al. 2019; Parisotto et al. 2020). Imitation learning is an alternative approach based on learning from expert data, but can still require millions of demonstrations to learn effective planners (Chevalier-Boisvert et al. 2019). Such high data constraints make learning difficult and expensive.\nUnfortunately the aforementioned issues with behavior learning are only exacerbated in the low data regime. First, with limited training data agents are less likely to act perfectly at each environment step, leading to small errors\nCopyright \u00a9 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nthat compound overtime in the offline setting. Ultimately, this leads to sub-par performance over long horizons that can usually only be improved by carefully collecting additional expert data (Ross, Gordon, and Bagnell 2011). Second, deep-learning based policies are more likely to overfit small training datasets, making them unable to generalize to new test-time scenarios. On the other hand, humans have the remarkable ability to interpolate previous knowledge and solve unseen long-horizon tasks. After observing an environment, we might deduce plan or sequence of the steps to follow to complete our objective. However, imitation learning agents are not required to construct plans by default \u2013 they are usually trained to only output action sequences given seen observations. This begs the question: how can we make agents reason better in long-horizon tasks?\nAn attractive solution lies in language instructions, the same medium humans use for mental planning (Gleitman and Papafragou 2005). Several prior works directly provide agents with language instructions to follow (Anderson et al. 2018; Shridhar et al. 2020; Chen et al. 2019). Unfortunately, such approaches require the specification of exhaustive instructions at test time for systems to function. A truly intelligent agent ought to be able to devise its own plan and execute it, with only a handful of demonstrations. We propose improving policy learning in the low-data regime by having agents predict planning instructions in addition to their immediate next action. As we do not input instructions to the policy, we can plan without their specification at test time. Though prior works have used hierarchical structures that generate their own instructions to condition on (Chen, Gupta, and Marino 2021; Hu et al. 2019; Jiang et al. 2019), we surprisingly find that just predicting language instructions is in itself a powerful objective to learn good representations for planning. Teaching agents to output language instructions for completing tasks has two concrete benefits. First, it forces them to learn at a higher level of abstraction where generalization is easier. Second, by outputting multi-step instructions agents explicitly consider the future. Practically, we teach agents to output instructions by adding an auxiliary instruction prediction network to transformerbased policy networks, as in seq2seq translation (Vaswani et al. 2017). Our approach can be interpreted as translating observations or trajectories into instructions.\nWe test our representation learning method in limited data\nar X\niv :2\n30 6.\n12 55\n4v 1\n[ cs\n.L G\n] 2\n1 Ju\nn 20\n23\nsettings and combinatorially complex enviornments. We find that in many settings higher performance can be attained by relabeling existing demonstrations with language instructions instead of collecting new ones, creating a new, scalable type of data collection for practitioners. Furthermore, our method is conceptually simple and easy to implement. This work is the first to show that direct representation learning with language can accelerate imitation learning.\nTo summarize, our contributions are as follows. First, we introduce a method for training transformer based planning networks on paired demonstration and instruction data via an auxiliary instruction prediction loss. Second, we test our objective in long-horizon planning based environments with limited data and find that it substantially outperforms contemporary approaches. Finally, we analyze the scenarios in which predicting instructions provides fruitful training signal, concluding that instruction modeling is a valuable objective when tasks are sufficiently complex."
        },
        {
            "heading": "Related Work",
            "text": "Language in the context of policy learning has been heavily studied (Luketina et al. 2019), usually to communicate a task objective. Uniquely, we use natural language instructions to aid in learning via an auxiliary objective. Here we survey the most relevant works to our approach.\nLanguage Goals. Language offers a natural medium to communicate goals to intelligent agents. As such, several prior work have focused on learning language goal conditioned policies, particularly for robotics (Nair et al. 2021; Stepputtis et al. 2020; Kanu et al. 2020; Hill et al. 2020; Akakzia et al. 2021; Goyal, Mooney, and Niekum 2021; Shridhar, Manuelli, and Fox 2021), or for games (ChevalierBoisvert et al. 2019; Chaplot et al. 2018; Hermann et al. 2017), and sometimes even with hindsight relabeling (Chan et al. 2018; Cideron et al. 2020). Others in the area of inverse reinforcement learning use language to specify reward functions (Fu et al. 2019; Bahdanau et al. 2018; Williams et al. 2018) or shape them (Mirchandani, Karamcheti, and Sadigh 2021; Goyal, Niekum, and Mooney 2019). These works use language to give humans an easy way to specify the desired goal conditions of an environment. Unlike these works, we use language instructions that dictate the steps to reach a desired goal condition instead of just using language goals that specify the desired state. Other works, particularly in the visual navigation space, provide agents with step-by-step instructions similar to those we use, sometimes in addition to language goals. Anderson et al. (2018); Fried et al. (2018); Chen et al. (2019); Krantz et al. (2020); Chen et al. (2021a, 2019); Zang et al. (2018) condition policies on step-by-step language instructions for visual navigation, while Shridhar et al. (2020); Pashevich, Schmid, and Sun (2021); Shridhar et al. (2021) use instructions for household tasks. Many of these benchmarks ask agents to simply follow instructions, like \u201cturn right at the end of the hallway\u201d instead of achieving overarching goals like \u201cgo to the kitchen\u201d. Critically unlike our method, these approaches require laboriously providing the agent with step-by-step instructions at test-time. By using instructions for representation learning instead of policy inputs, we additionally avoid needing to label entire\ndatasets with language instructions and can train on partially labeled datasets. Other proposed environments (Wang and Narasimhan 2021) assess understanding by prompting agents with necessary information about task dynamics, precluding the removal of text-prompting at test time.\nLanguage and Hierarchical Learning. Instead of directly using instructions as policy inputs, other works use language instructions as an intermediary representations for hierarchical policies. Usually, a high-level planner outputs language instructions for a low-level executor to follow. Andreas, Klein, and Levine (2017) and Oh et al. (2017) provide agents with hand-designed high-level language instructions or policy \u201csketches\u201d. Again unlike our method such approaches require instruction labels for every training task and for every new task at test-time. Jiang et al. (2019) and Shu, Xiong, and Socher (2017) provide interactive language labels to agents to train hierarchical policies with reinforcement learning. In the imitation learning setting, Hu et al. (2019) learn a hierarchical policy using behavior cloning for a strategy game. Unlike the planning problems we consider, their environment has no oracle solution and does not consider generalization to unseen tasks. Most related to our work, Chen, Gupta, and Marino (2021) use latent representations from a learned high-level instruction predictor to aid a low-level policy. However, unlike Chen, Gupta, and Marino (2021), we learn latent representations that can predict instructions, but do not explicitly condition on them at test-time. While these hierarchical approaches have shown promise, the quality of learned policies is inherently limited by the amount of language data available for training. Even with a perfect low-level policy, inaccurate languages commands will yield poor overall performance. For example, a small mis-specification in a subgoal, like changing \u201cblue door\u201d to \u201cred door\u201d would likely cause complete policy failure. This is not an issue for our loss-based approach, as our instruction prediction network can be detached from the policy. As previously mentioned, this structure lets us learn on a mix of instruction annotated and unannotated data, letting it more easily scale than hierarchical approaches particularly in data-limited scenarios. Other approaches in robotics similar to hierarchy use high-level discrete action labels alongside demonstrations to learn planning grammars (Edmonds et al. 2017, 2019). While different in flavor than our approach, such methods also share similar data limitations to the hierarchical methods preivously discussed.\nAuxiliary Objectives. The use of auxiliary objectives in policy learing has been extensively studied. Though to our knowledge no prior have used instructions, auxiliary objectives in general have been found to aid policy learning (Jaderberg et al. 2017). Laskin, Srinivas, and Abbeel (2020) and Stooke et al. (2021) demonstrated the success of contrastive auxiliary objectives in robotic reinforcement learning domains. Schwarzer et al. (2020) and Anand et al. (2019) did the same in the Atari game-playing environments. We were inspired by their effectiveness. Additionally, works like Andreas, Klein, and Levine (2018) have previously used language question and answering for representation leaning in visual domains.\nTransformers. Our approach is based on several innova-\ntions involving transformer networks. Vaswani et al. (2017) previously showed state of the art results in machine translation using transformers. While the application of transformers has extended to behavior learning (Zambaldi et al. 2018; Parisotto et al. 2020; Chen et al. 2021b), prior works in the area have not leveraged the transformer decoder. Closest to our domain, Lin et al. (2021) generate captions from video. The architecture of our policy networks take inspiration from recent works adapting transformers to mediums beyond text, namely in vision (Dosovitskiy et al. 2021) and offline reinforcement learning (Chen et al. 2021b)."
        },
        {
            "heading": "Method",
            "text": "In this section we formally describe imitation learning with instruction prediction, then detail our implementation for both Markovian and non-Markovian environments."
        },
        {
            "heading": "Problem Setup",
            "text": "The standard learning from demonstrations setup assumes access to a dataset of expert trajectory sequences containing paired observations and actions o1, a1, o2, a2, ..., oT , aT . The goal of imitation learning is to learn a policy \u03c0(at|\u00b7) that predicts the correct actions an agent should take. In our work we consider both Markovian and partially observed nonMarkovian settings. In the non-Markovian observed case, policies are given access to previous observations in order to infer state information (Kaelbling, Littman, and Cassandra 1998), and we denote the policy as \u03c0(at|o1, ...ot). In the Markovian setting this is unnecessary, and the policy is simply \u03c0(at|ot). In imitation learning it is common for policies to be goal conditioned, or even conditioned on language goals as is the case in our experiments. In goal conditioned settings an encoding of the desired task or goal g is additional input to the policy. As our approach works with or without goal conditioning we omit it from the rest of this section for brevity. A standard imitation learning technique is behavior cloning, which in discrete domains maximizes the likelihood of the actions in the dataset using a negative log likelihood objective, Laction = \u2212 \u2211 t log \u03c0(at|\u00b7).\nIn this work, we assume access to oracle language instructions that tell an agent how it should complete a task to provide useful training signal. As mentioned in Section 2, for the purposes of our method we distinguish goals from instructions. Language goals describe the desired final state of the environment environment, specifying what to do, whereas language instructions communicate how an agent should reach the desired state in a step-by-step manner. Each trajectory may have several language instructions x(1), x(2), ..., x(n) corresponding to each of the n different steps to reach the desired goal configuration. For example, a language instruction like \u201copen the door\u201d only applies to the part of the demonstration before the agent opens the door and after it completes the last instruction. The i-th instruction x(i) thus corresponds to an interval [Ti, Ti+1) where Ti marks the time the instruction was given and Ti+1 denotes the start of the next instruction. A depiction of an example instruction sequence can be found in Figure ??. While language instructions are an additional data require-\nment, they can be cheap to obtain, particularly in scenarios where demonstrations are expensive to collect. If demonstrations are collected in the real world, providing simultaneous instruction annotations for a demonstration is likely less labor-intensive than collecting an additional demonstration. Moreover, humans can easily re-label existing demonstrations with instructions. Video data could easily be captioned with voice-over. Similar statements can be made for simulators \u2013 if one can code an oracle policy, instructions are likely easy to generate along the way. These modifications are easy for simulators with planning stacks, as in BabyAI. Moreover, we focus on low to medium data regime, where the cost of setting up an environment and collecting more demonstrations is likely be higher than annotating an existing small set of demonstrations. Next, we describe how we train agents to predict language instructions to aid in imitation learning."
        },
        {
            "heading": "Instruction Prediction for Imitation Learning",
            "text": "The central hypothesis of this work is that predicting language instructions will force agents to learn representations beneficial for long-horizon planning. In our framework, we first construct an observation encoder f\u03b8 that produces latent representations z. As in behavior cloning we predict actions from latents z using a policy network \u03c0\u03d5, but we additionally use a language decoder g\u03c8 to predict the current instruction from z. Our general setup is shown in left half of Figure 1. We consider both non-Markovian environments, where sequences of observations must be provided to the model so it can infer the underlying state. In the non-Markovian setting, the encoder produces z1, ..., zt = f\u03b8(o1, ..., ot), the policy is \u03c0\u03d5(at|z1, ..., zt), and the language model is g\u03c8(x\n(i)|z1, ..., zTi\u22121). For standard fully observed Markovian environments, conditioning on past observations is unnecessary and the encoder, policy, and language decoder can be written as zt = f(ot), \u03c0\u03d5(at|zt), and g\u03c8(x(i)|zt) respectively. As is common in natural language processing, we treat each language instruction x(i) as a sequence of multiple text tokens x(i)1 , x (i) 2 , ..., x (i) li\nwhere li is the length of the i-th instruction. The decoder is trained using the standard language modeling loss. We construct our total imitation learning objective by maximizing the log-likelihood of both the action and instruction data. For a given trajectory in the non-Markovian case, this is written as follows\nL = \u2212 T\u2211 t=1 log \u03c0\u03d5(at|z1, ..., zt) (1)\n\u2212 \u03bb n\u2211 i=1 li\u2211 j=1 log g\u03c8(x (i) j |x (i) 1 , ..., x (i) j\u22121, z1, ..., zTi\u22121)\nwhere latent representations z are all produced by the shared encoder f\u03b8. The MDP case is formulated by removing past conditioning on z1, ..., zt\u22121. The first term of the loss is the standard classification loss used for behavior cloning in discrete domains. The second term of the loss corresponds to the negative log-likelihood of the language instructions. We index the language loss by instructions via the first sum. The second sum over token log\nlikelihoods is from the standard auto-regressive language modeling framework, where the likelihood of an instruction is the product of the conditional probabilities p(x(i)) =\u220fli j=1 p(x (i) j |x (i) 1 , ...x (i) j\u22121). Note that when predicting the likelihood of language tokens for instruction i, the model can only condition on latents up to zTi\u22121. This ensures that we compute the likelihood of instruction i using only observations during or before its execution. Finally, \u03bb is a weighting coefficient that trades off the importance of instruction prediction and action modeling. During training, we optimize all parameters \u03d5, \u03c8, and \u03b8 jointly, meaning that gradients from both behavior cloning and language prediction are propagated to the encoder weights \u03b8. In some of our experiments we test additional learning objectives which are also trained on top of the same latent representations z as is standard in the literature (Jaderberg et al. 2017).\nThough our method is general to any network architecture, we train transformer based policies since they have been shown to be extremely effective at natural language processing tasks (Vaswani et al. 2017) and carry a good inductive bias for combinatorial planning problems (Zambaldi et al. 2018). For details on the transformer architectures we use, we defer to (Dosovitskiy et al. 2021; Chen et al. 2021b). In the following sections we describe our transformer-based models for both Markovian and non-Markovian settings.\nNon-Markovian Settings. For environments that are non-Markovian or partially observed we use a transformer based sequence model as our policy network, similar to those employed in (Chen et al. 2021b). We operate in the entire sequence at once: z1, ...zT = f\u03b8(o1, ..., oT ). Causal masking similar to that in (Radford et al. 2018) ensures that at time t the representation zt only depends on current and previous observations o1, ...ot. The same policy network \u03c0\u03d5(at|zt) is applied to each latent to produce actions for each timestep. The language decoder g\u03c8 is also a transformer model and employs both causal attention masks to the language inputs and cross attention masks to the latents. Causal-self attention masks on the language inputs enforce the auto-regressive modeling of the instruction tokens. Cross attention masks to the latent representations ensure that predictions for the ith instruction cannot attend to latents from\ntimesteps after its execution as is depicted by the red \u201cx\u201ds in Figure 1. This forces language prediction during training to mirror test-time as the agent cannot use future information to predict the instruction.\nMarkovian settings. For environments that are fully Markovian, we use only the most recent observation ot. As sequence modeling is unnecessary, we use a transformer to encode individual states, leveraging their success in combinatorial environments (Zambaldi et al. 2018). Specifically, we a Vision Transformer architecture (Dosovitskiy et al. 2021) that predict actions only for a single timestep. Observations are preprocessed into tokens and prepended with a special CLS token: ot \u2192 CLS, ot,1, ot,2, ot,3, .... As we do not input future observations, the transformer encoder uses full unmasked self attention. At the end of the network we take the latent representation corresponding to the CLS token and use it to predict the action \u03c0\u03d5(at|zt,CLS). We use all latent tokens zt,CLS, zt,1, zt,2, zt,3, ... to predict the current language instruction with g\u03c8 . An architecture figure can be found in Appendix C."
        },
        {
            "heading": "Experiments",
            "text": "In this section we detail our experimental setup and empirical results. In particular, we investigate the benefits of instruction modeling for planning in limited data regimes. We seek to answer the following questions: How effective is instruction modeling loss? How does instruction modeling scale with both data and instruction annotations? What architecture choices are important? And finally, when is instruction modeling a fruitful objective?"
        },
        {
            "heading": "Environments",
            "text": "To evaluate the effectiveness of instruction prediction at enabling long-horizon planning and generalization, we test our method on BabyAI (Chevalier-Boisvert et al. 2019) and the Crafting Environment from Chen, Gupta, and Marino (2021) which both provide coarse instructions. They cover challenges in partial observability, human generated text, and more. We later examine the ALFRED environment to understand where instruction prediction is useful. Full model hyperparameters can be found in the Appendix.\nBabyAI: Agents must navigate partially observable gridworlds to complete arbitrarily complex goals specified through procedurally generated language such as moving objects, opening locked doors, and more. Agents are evaluated on their ability to complete unseen missions in unseen environment configurations. We modify the BabyAI oracle agent to output language instructions based on its planning logic. We focus our experiments on the hardest environment, BossLevel, and up to 10% of the million demos in ChevalierBoisvert et al. (2019). Because of partial observability, we employ a transformer sequence model as described in Section with the same encoder from Chevalier-Boisvert et al. (2019). The language goals from the environment are tokenized and fed as additional inputs to the policies. We evaluate on five hundred unseen tasks.\nCrafting: This environment from Chen, Gupta, and Marino (2021) tests how well an agent can generalize to new tasks using instructions collected from humans. The original dataset contains around 5.5k trajectories with human instruction labels. Each task is specified by a specific goal item the agent should craft, encoded via language. The agent must complete from one to five independent steps to obtain the final item. As this environment is fully observed, we employ the Vision Transformer based model described in Section with the benchmark\u2019s original state encoder."
        },
        {
            "heading": "Baselines",
            "text": "We compare the effectiveness of our instruction modeling auxiliary loss to a number of baselines. The text in parenthesis indicates how we refer to the method in Tables 1, 2, 4, and 5. None of our models are pretrained, though we explore this and more additional baselines in the Appendix.\n1. Original Architecture (Orig): The original state of the art model architectures proposed for each environment. The crafting environment uses a language-instruction hierarchy. In BabyAI, we use convolutions and FiLM layers as in Chevalier-Boisvert et al. (2019).\n2. Transformer (Xformer): Our transformer based models\nwithout any auxiliary objectives to determine the effectiveness of our architectures.\n3. Transformer Hierarchy (Hierarchy): A high-level model outputs instructions for a low level executor for comparison to to hierarchical approaches.\n4. Transformer with Forward Prediction (Forward): Instead of predicting instructions, we use the decoder to predict future actions. This baseline demonstrates the importance of using grounded information.\n5. Transformer with ATC (ATC): Our transformer model with the active temporal contrast (ATC) self-supervised objective proposed in Stooke et al. (2021). This compares vision and instruction based representation learning.\n6. Transformer with Lang (Lang): Our transformer based models with just instruction prediction loss.\n7. Transformer with ATC and Lang (Lang + ATC): Our transformer based models with both instruction modeling and constrastive auxiliary losses."
        },
        {
            "heading": "How Effective Is Instruction Prediction?",
            "text": "Our main experimental results can be found in Table 1, where we compare the performance of all methods on both environments with three differing dataset sizes. We find that for all environments and dataset sizes our instruction modeling objective improves or has no effect in the worst case. In BabyAI, we achieve a 70% success rate on the hardest level with fifty thousand demonstrations and instructions. For comparison, it is worth noting that the original BabyAI implementation (Chevalier-Boisvert et al. 2019) achieved a success rate of 77% with one million demonstrations. In the crafting environment, using instruction modeling boosts the success rate by about 5% or more in the 1.1k and 2.2k demonstration setting. To our knowledge our results are state of art in this environment, exceeding the reported 69% success rate on unseen tasks in Chen, Gupta, and Marino (2021) where RL is additionally used. We also find that the model is able to accurately predict instructions (Figure 2), which ap-\npears to be correlated with performance. See the Appendix for analysis of the language outputs of the models.\nVisual representation learning was not as fruitful as language based representation learning overall. The combination of ATC and instruction modeling was unfortunately not constructive in all scenarios: it performed better in some instances and worse than just language loss in others. This is consistent with results found in Chen et al. (2021c) that show that observation based auxiliary objectives often yield mixed results in the imitation learning setting. We find that our hierarchical implementations do not perform very well in comparison to plain transformer models. This is likely because with only a few demonstrations high-level language policies are likely to output incorrect instructions for unseen tasks leading low-level instruction conditioned policies to output sub-optimal actions. More analysis of the hierarchical baselines is in the Appendix. Finally, we see that the forward prediction used in the forward baseline hardly contributes to performance, indicating that grounded instructions do more than just combat compounding errors."
        },
        {
            "heading": "How Does Instruction Prediction Scale with Data and Annotations?",
            "text": "Overall, we find that instruction modeling reduces the amount of data required for policies to begin to generalize well. This is particularly evident in the low to medium data regime. With too little data, agents are likely to overfit quickly and only see a minor benefit from instruction prediction. With a significant amount of data instruction modeling may become unnecessary, and the policy can learn good rep-\n% w/ Instr 0% 50% 100% 50k Demos 40.2\u00b12.2 68.6\u00b11.4 70.3\u00b11.3 25k Demos 39.9\u00b10.5 50.3\u00b11.3 55.4\u00b17.0\nTable 2: We ablate the amount of demonstrations annotated with language instructions. Values are % success rates with standard deviations.\nresentations from action labels alone. However, in between these regimes we find that instruction prediction reduces the amount of data needed to generalize by forcing the model to learn more ammendable representations to long-horizon planning. Figure 3 depicts how model performance changes with dataset size. In BabyAI, instruction modeling does not appear to significantly help with the smallest number of demos, however, after twelve and a half thousand demonstrations that we find that policy performance with language scales almost linearly with data before it experiences diminishing returns at two-hundred thousand demonstrations. Policies without language are unable to perform substantially better until we provide one hundred thousands demonstrations. This is not just because training with instructions helps overcome partial observability \u2013 we show similar results on a fully observed version of BabyAI in the Appendix. The Crafting environment has only fourteen training tasks versus BabyAI\u2019s potentially infinite number, causing it to require fewer demos before performance saturates. Thus, we observe the opposite problem: instruction modeling helps when the policy is data constrained, and then is neutral when more data is introduced. In Section 4.6, we show that this saturation happens rather quickly for 5-step Crafting tasks, as there is only one in the training dataset.\nA benefit of our loss-based approach is that it can easily be applied to mixed datasets that have only some instruction labels. To additionally study the scaling properties of our language prediction objective, we construct datasets in BabyAI where only half of the trajectories have paired instructions. Results can be found in Table 2. Surprisingly, one is better off collecting 12.5k language annotations than collecting an additional 25k demonstrations in the BabyAI environments. A similar statement can be made in the crafting environment for 1.1k demonstrations. This means that collecting instruction annotations is a feasible alternative to demonstrations."
        },
        {
            "heading": "What Modeling Decisions Are Important?",
            "text": "We ablate the use of our instruction decoder cross-attention masking in BabyAI. We find that the omission of the masking scheme leads to a 20% drop in performance, from 70.3 \u00b1 1.3% to 50.1 \u00b1 12.1%. Without masking the language decoder has an easier time predicting an instruction as it can attend to observations from after the instruction finished, creating a disparity between train and test time, ultimately leading to lower quality representations. Overall, the transformer architecture appears to be critical to high performance, likely because of its good inductive bias for reasoning about objects and their interactions. This is especially evident in the Crafting environment. As stated in Chen, Gupta, and Marino (2021), the imitation learning approaches with the original model were unable to achieve a meaningful success rate on any of the unseen tasks, whereas our baseline transformer achieves a success rate of around 70%. Our architecture choice is also extremely parameter efficient as shown in the Appendix.\nWhen Is Instruction Prediction Useful? We hypothesize that instruction prediction is particularly useful for combinatorially complex, long horizon tasks. Many simple tasks, like \u201copen the door\u201d or \u201cgrab a cup and put it in the coffee maker\u201d communicate all required steps and consequently stand to gain little from instruction modeling. Conversely, tasks in both environments we study do not communicate all required steps to agents. Thus, as task horizon and difficulty increase one would expect instruction modeling to be more important. In BabyAI we consider two additional levels \u2013 GoTo, which only requires object localization, and SynthLoc which uses a subset of the BossLevel goals. Results in Table 4 indicate that instruction modeling is indeed more important for harder tasks. The same trend approximately holds in the Crafting environment (Table 5). All policies are able to complete two steps tasks near or above 90% success, but models with language prediction perform around 5% better with fewer than 3.3k demonstrations. The difference in performance is ever greater for the three-step tasks, where instruction prediction boosts performance from around 58% to closer to 75% in most cases. Evaluations of the five-step tasks were noisy, which we attribute to the existence of only one five-step training task with which no model was able to adequately generalize. The takeaway from these observations is that instructions offer less training signal for combinatorially simple tasks, where reaching the goal requries only a few, obvious logical steps. Thus, we expect instruction modeling to not matter when instructions can easily be predicted from goals alone.\nTo test this hypothesis, we train transformer models to predict instructions with and without access to observations in our primary environments and additionally in a modified version of the ALFRED benchmark (Shridhar et al. 2020). Table 3 shows the token prediction accuracy of instructions using text goals alone versus text goals and observations. While token prediction accuracies are relatively high, particularly for BabyAI, accuracy differences of 5% or more can make a large impact as instructions can share similar structure, but have a few critical tokens specifying objects. In the benchmarks where our method is impactful instructions cannot be as accurately predicted from text goals alone. However, in the ALFRED environment, which is largely based on visual understanding, instructions can easily predicted from just the goal. This indicates that while the visual complexity of ALFRED may be high, it does not pose significant challenges in logical understanding. Further analysis of the ALFRED benchmark is provided in the Appendix. In the future as tasks become more combinatorially complex, we expect instructions to provide a more critical modeling component."
        },
        {
            "heading": "Conclusion",
            "text": "We introduce an auxiliary objective that predicts language instructions for imitation learning and associated transformer based architectures. Our instruction modeling objective consistently improves generalization to unseen tasks with few demonstrations, and scales efficiently with instruction labels. We further analyze the domains where our method is successful, and make recommendations for when to apply it."
        },
        {
            "heading": "Fully-Observed BabyAI",
            "text": "In addition to the standard partially observed BabyAI environment, we created a fully observed version where the agent can view the entire world grid. In this fully observed setting we employ the same model architecture as in the Crafting environment, but with the hyperparameters from BabyAI. Results for a single seed on the BossLevel in the fully observed setting can be found in Table 6, and look largely similar to those of the partially observed BabyAI environment.\nAnalyzing Instruction Information and ALFRED In section 4.6 we find that instruction prediction is more useful as task difficulty increases. In this section, we try to measure how useful provided instructions are and additionally analyze instruction prediction in the ALFRED visual environment. Instruction prediction is likely to only provide a strong learning signal when they provide information not already available to the agent. Logically this makes sense: if all of the requisite information has been given to the agent in its task, instructions will add nothing new. For example, one goal from the ALFRED environment is \u201cput a watch on the table\u201d. If the provided instructions for this task were \u201cpick up the watch\u201d, \u201cgo to the table\u201d, and \u201cput down the watch\u201d, the instructions would provide very little useful signal as all of their information was already conveyed by the goal.\nBy default, ALFRED provides language instructions and goals as inputs to the agent. We remove the instructions from the input so they can be used for our auxiliary instruction prediction objective. Following the methodology and architecture choices of (Pashevich, Schmid, and Sun 2021), we generate an additional 42K demonstrations in ALFRED and label them with vocabulary from the planner. Overall, we found that instruction prediction had little impact on performance across three seeds as seen in Table 7. Based on these results, we assess why instruction prediction was not fruitful in ALFRED.\nWe can measure how much information instructions are able to provide to an agent by measuring how easy it is to predict them from just the goal. If instructions can easily be predicted from the goal alone, then they are unlikely to provide any additional learning signal to the agent. If the agent can only accurately predict instructions by observing the behavior in the demonstrations and the goal, then instructions prediction is more likely to encourage salient representation learning or logical reasoning about the task. For each benchmark, we take our transformer based architecture for instruction prediction and train only the language head to predict instructions with and without access to observations from the demonstrations. We report the best attained prediction accuracies in Table 8. Our results indicate that the instructions in ALFRED can be nearly predicted perfectly from the text goals (96.9%), indicating that the tasks are too easy and do not require instructions. This result has also been verified by the community. As of writing, one of the top models on the ALFRED leaderboard does not even use the language instructions as inputs. Conversely, we see lower accuracy overall and much larger gaps in accuracy for both BabyAI BossLevel (86.4% to 92.4%) and the Crafting environments (43.8% to 49.9%). We hope this result will drive the community to develop more logically challenging benchmarks with complex tasks where the scaling of instruction prediction can be further studied. Additionally, one can begin to assess the effectiveness of instruction prediction before using it by following this methodology."
        },
        {
            "heading": "Additional Baselines",
            "text": "We ran a additional baselines in the BabyAI environment. 1. GPT Enc: In order to demonstrate that instruction prediction is not just aiding in the agent\u2019s understanding of text goals, we\nconstruct a baseline that encodes the text goals in BabyAI using a pretrained GPT-2 Model before giving them to the agent. As the text-embeddings have been pretrained, this simulates the case where we have a maximal understanding of the text goal before interacting with the environment.\n2. XFormer AC: Our original architecture does not use previous actions as input due to the substantial increase in input tokens it causes. This baseline inputs both observation and action sequences into the transformer based model.\n3. Goal Prediction: Our architecture with language prediction, except instead of predicting the unseen instructions we predict the goal text that is used as input to the policy. This is a type of reconstruction objective in the text regime.\nWe ran these additional baselines for two seeds. Results for these new baselines and XFormer and Lang can be found in Table 9. We find that encoding text goals with GPT does not lead to performance gas as large as language prediction. This indicates that our instruction prediction helps with learning good representations for planning, and not just language understanding. The transformer with action inputs (XFormer AC) does not perform better than the regular transformer and in fact performs slightly worse, indicating that action inputs are not an important modeling component in the imitation domain and may just make learning harder by adding additional modalities and doubling sequence length. This is different than results found in the Offline RL setting in (Chen et al. 2021b), which makes sense as rewards often depend on both states and actions. Moreover, inverse models in the discrete action spaces in BabyAI are relatively easy to learn. Previous works with transformers in imitation (Pashevich, Schmid, and Sun 2021) have also found that conditioning on entire action sequences leads to a degradation of performance as policies can more easily overfit."
        },
        {
            "heading": "Extended Training",
            "text": "Due to space constraints, we have included results training on more demonstrations for BabyAI (100k and 200k demos) and Crafting (5k demos) here. These were already included in Figure 3. In BabyAI, we were only able to run one seed for each model as they took far longer to converge with more data.\nHierarchical Baselines Here we provide further details on our hierarchical baselines. The two prior works relevant on hierarchical language most relevant to our investigations are (Chen, Gupta, and Marino 2021) and (Hu et al. 2019). Both of these works learn Markovian, or nearly-Markovian models.\nIn the crafting environment (Chen, Gupta, and Marino 2021), the authors train a high-level RNN to output the current language instruction. They then condition their low-level policy on the latent representation fed to the RNN that predicts instructions. While they show latent condition to be effective, transformers purposefully avoid encoding entire streams of data into a single vector, and instead operate on a token level. Thus, we found it impractical to attempt this approach with our significantly more effective transformer models.\nThe environment in (Hu et al. 2019) is a partially observed multi-player strategy game. As alluded to in the related work, this environment has multiple viable strategies and is thus distinct from the oracle imitation learning we mostly consider. Though\nthe environment is partially observed, the authors do not train a sequence model. Instead, they concatenate command data from previous time-steps to the model input. As information from the very beginning of the trajectory is necessary for some BabyAI tasks, we found it impractical to scale this concatenation based approach. In (Hu et al. 2019), a discriminative high level policy is trained to select an instruction from a fixed set. A low-level is trained with ground-truth human labeled instructions to output actions. In their strategy game, hierarchical approaches perform very well, unlike in our experiments where hierarchical models do not perform the best. One explanation for this comes from the nature of the strategy game environment. The distribution of optimal actions from a given state may be multi-modal, as different strategies may dictate different actions from the same state. Conditioning on an instruction would remove this multi-modality. Below we describe the hierarchical approaches we tried. For all approaches we used the same architectures as detailed in Section 3.\nFully-Observed Setting. In the fully observed setting, we adopt a similar strategy to (Hu et al. 2019). A high-level policy takes as input the observation o and goal g and predicts the current instruction x(i). We train a low-level policy that predicts actions from the current instruction, goal, and observation \u03c0(at|ot, x(i), g). At test time, the high-level auto-regressively generates instructions that are then given to the low level.\nPartially-Observed Setting. Unfortunately, we find that there is no clear cut way to train a hierarchical model using only transformers where instructions and actions operate at different time scales. Here are the methods we tried: 1. Sequences for Each Instruction. We take each trajectory take slices of it up until the completion of each instruction. Our\nhigh-level model predicts only the language instruction corresponding to that trajectory slice. This essentially means that the high-level predicts one instruction conditioned on all the history before the instruction. The same sequences are used to train the low-level, conditioned on the single instruction. The low-level policy can be written as \u03c0(at|o1, ..., ot, g, x(i). Because training sequence models with only individual losses is very inefficient, we train the low-level model to output the correct actions at all points in time corresponding to the instruction it is conditioned on.\n2. All Instructions. Instead of conditioning a policy on a single instruction, we train the high level policy to output all of the instructions for the entire task. This is especially challenging at the beginning of an episode when there are few frames. The low-level policy is then conditioned on the entire sequence of instructions and can be written as \u03c0(at|o1, ..., ot, g, x(1), ..., x(n)). As this performs relatively well, we hypothesize that the model learns to ignore instructions far in the future when deciding which actions to take at the current timestep.\n3. All Instructions, Aggressive Mask. This is the same as the above, except we use an aggressive masking scheme when training the high level that only allows plans to be predicted from observations strictly preceding the time frame of the current instruction.\nIn Table 11 we give results for each of the sequence-style hierarchical approaches we tried on two seeds. In Table 1 we report the accuracy for the All Instructions method which we found to perform best. Other potential methods could include providing an encoding of the current instruction after each timestep to the transformer. However, such approaches would either employ an RNN or bag-of-words style model to generate the encoding and not be purely transformer based like the rest of our models.\nAdditional Figures See Figure 4 for the Fully Observed ViT-based architecture.\nLanguage Analysis Below we include metrics on the performance of the instruction prediction component of our models. As is in Table 12, the\ninstruction prediction models attain high accuracy and BLEU scores which tend to increase with more data. We find that the"
        },
        {
            "heading": "Seq2Seq Transformer Decoder",
            "text": "synthetically generated instructions in babyAI are much easier to model than the real human language in the crafter benchmark, which tends to be more multi-modal. Below we also include example outputs of our language model trained with varying amounts on data. Trajectories are fed into the model, and it is asked to reproduce the instructions associated with them as during training. For the BabyAI model, causal masking is still used. Overall, the model trained with more data more accurately predicts the instructions. We did not use any beam-search style methods to produce the outputs.\nHyperparameters Here we include all hyper-parameters we used. We used pytorch for our experiments. Our implementation for the partially observed sequence models was based on MinGPT by Andrej Karpathy. Our implementation of the Vision Transformer for fully observed environments was based on (Wightman 2019). We determined parameters for ATC by testing different frame skip values in both environments. In the crafting environment we tested using \u03bb coefficients of 0.5 and 0.25 and found 0.25 to perform better. We also tested using weight decay and dropout in BabyAI with 50k demonstrations and found it to have no significant impact and thus did not use it for the other experiments. When evaluating models we found those for the Crafting environment to perform better in Pytorch \u201ctrain\u201d mode, meaning with dropout on, while those for BabyAI worked better in \u201ceval\u201d mode. We ran our experiments on NVIDIA GTX 1080 Ti GPUs. In BabyAI we train all models for two seeds and in the crafting environment we train all models for four. For 100k or more demonstrations in BabyAI, we train only one seed as is done in (Chevalier-Boisvert et al. 2019). This was done because we found that training transformers until convergence on\nBabyAI takes around 4 days on GPU with 100k demonstrations, and even longer for 200k.\nGround Truth 12.5K Demos 50K Demos 100K Demos open the grey door open the grey door open the grey door open the grey door\nopen the yellow door open the yellow door open the yellow door open the yellow door go to the red ball open to the red ball open to the red ball open to the red ball open the purple door open the yellow door open the yellow door open the yellow door open the yellow door open the purple door open the yellow door open the yellow door pick up the red key open up the red key open up the red key open up the red key\npick up the purple key pick up the blue key pick up the purple key pick up the purple key open the yellow door open the yellow door open the yellow door open the yellow door\nmove the blue key open the blue key open the blue key open the blue key open the purple door open the grey door open the purple door open the purple door go to the green door open to the green door open to the green door open to the green door go to the blue key open to the blue key go to the blue key go to the blue key open the yellow door open the yellow door open the yellow door open the yellow door open the blue door open the blue door open the blue door open the blue door open the yellow door open the red door open the yellow door open the green door open the green door open the red door open the green door open the green door open the yellow door open the red door open the yellow door open the yellow door open the blue door open the red door pick the blue door open the green door open the green door open the blue door pick the blue door open the green door open the blue door pick the blue door pick the blue door pick the blue door\npick up the purple box pick up the purple box pick up the purple box pick up the purple box go to the yellow key go to the yellow key go to the yellow key go to the yellow key drop the purple box drop the purple box drop the purple box drop the purple box pick up the grey key pick up the grey key pick up the grey key pick up the grey key\nopen the red door open the red door open the red door open the red door open the green door open the green door open the green door open the green door open the green door open the blue door open the purple door open the purple door go to the purple box open to the purple box open to the purple box open to the purple box pick up the blue box pick up the red box pick up the blue box pick up the blue box go to the green ball go to the green ball go to the green ball go to the green ball drop the blue box drop the grey box drop the blue box drop the blue box open the blue door open the blue door open the blue door open the blue door open the purple door open the purple door pick the purple door pick the purple door open the yellow door pick the blue door pick the green door pick the green door open the green door pick the blue door pick the green door pick the green door pick up the yellow box pick up the green box pick up the purple box pick up the purple box go to the grey ball go to the green ball go to the grey ball go to the grey ball\ndrop the yellow box drop the green box drop the yellow box drop the yellow box open the green door go the green door go the yellow door open the green door\npick up the green box move up the green box open up the yellow box open up the green box go to the green door go to the green door go to the green door go to the green ball drop the green box drop the green box drop the green box drop the green box open the purple door open the purple door open the purple door open the purple door open the red door open the red door open the red door open the red door open the red door open the grey door open the red door open the red door\nopen the green door open the grey door pick the red door open the red door pick up the green ball pick up the green ball pick up the green ball pick up the green ball\ngo to the blue key go to the blue key go to the blue key go to the blue key go to the grey door go to the grey key go to the grey door go to the grey door close the grey door close the grey door close the grey door close the grey door open the grey door open the grey door open the grey door open the grey door\npick up the grey ball pick up the yellow ball pick up the blue ball pick up the red ball go to the purple key go to the blue key go to the purple key go to the purple key drop the grey ball drop the grey ball drop the grey ball drop the grey ball open the grey door open the grey door open the grey door open the grey door open the yellow door open the yellow door open the yellow door open the yellow door open the grey door open the blue door open the yellow door open the blue door"
        }
    ],
    "title": "Improving Long-Horizon Imitation through Instruction Prediction",
    "year": 2023
}