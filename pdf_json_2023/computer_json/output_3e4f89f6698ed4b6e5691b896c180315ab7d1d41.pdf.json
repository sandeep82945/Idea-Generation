{
    "abstractText": "Machine Learning-as-a-Service, a pay-as-you-go business pattern, is widely accepted by third-party users and developers. However, the open inference APIs may be utilized by malicious customers to conduct model extraction attacks, i.e., attackers can replicate a cloud-based black-box model merely via querying exquisitely sampled or crafted examples. Existing model extraction attacks mainly depend on the posterior knowledge (i.e., predictions of query samples) fromOracle. Thus, they either require high query overhead to simulate the decision boundary, or suffer from generalization errors and overfitting problems due to query budget limitations. To mitigate it, this work proposes an efficient model extraction attack based on prior knowledge for the first time. The insight is that prior knowledge of unlabeled proxy datasets such as intrinsic property and natural relationships is conducive to the search for the decision boundary (e.g., informative samples). Specifically, we leverage self-supervised learning including autoencoder and contrastive learning to pre-compile the prior knowledge of the proxy dataset into the feature extractor of the substitute model. Then we adopt entropy to measure and sample the most informative examples to query the target model. Our design leverages both prior and posterior knowledge to extract the model and thus eliminates generalizability errors and overfitting problems. We conduct extensive experiments on open APIs like Traffic Recognition, Flower Recognition, Moderation Recognition, and NSFW Recognition from real-world platforms, Azure and Clarifai. The experimental results demonstrate the effectiveness and efficiency of our attack. For example, our attack achieves 95.1% fidelity with merely 1.8K queries (cost 2.16$) on the NSFW Recognition API from the Clarifai platform. Also, the adversarial examples generated with our substitute model have better transferability than others, which reveals that our scheme is more conducive to downstream attacks.",
    "authors": [
        {
            "affiliations": [],
            "name": "Shiqian Zhao"
        },
        {
            "affiliations": [],
            "name": "Kangjie Chen"
        },
        {
            "affiliations": [],
            "name": "Meng Hao"
        },
        {
            "affiliations": [],
            "name": "Jian Zhang"
        },
        {
            "affiliations": [],
            "name": "Guowen Xu"
        },
        {
            "affiliations": [],
            "name": "Hongwei Li"
        },
        {
            "affiliations": [],
            "name": "Tianwei Zhang"
        }
    ],
    "id": "SP:17187a13c2bdcf2544133e74a25bc0195cf03736",
    "references": [
        {
            "authors": [
                "Yossi Adi",
                "Carsten Baum",
                "Moustapha Cisse",
                "Benny Pinkas",
                "Joseph Keshet"
            ],
            "title": "Turning your weakness into a strength: Watermarking deep neural networks by backdooring",
            "venue": "In 27th {USENIX} Security Symposium ({USENIX} Security",
            "year": 2018
        },
        {
            "authors": [
                "Madhu S Advani",
                "Andrew M Saxe",
                "Haim Sompolinsky"
            ],
            "title": "Highdimensional dynamics of generalization error in neural networks",
            "venue": "Neural Networks",
            "year": 2020
        },
        {
            "authors": [
                "Buse Gul Atli",
                "Sebastian Szyller",
                "Mika Juuti",
                "Samuel Marchal",
                "N Asokan"
            ],
            "title": "Extraction of complex dnn models: Real threat or boogeyman",
            "venue": "In Engineering Dependable and Secure Machine Learning Systems: Third International Workshop,",
            "year": 2020
        },
        {
            "authors": [
                "Dana H Ballard"
            ],
            "title": "Modular learning in neural networks",
            "venue": "In Aaai,",
            "year": 1987
        },
        {
            "authors": [
                "Antonio Barbalau",
                "Adrian Cosma",
                "Radu Tudor Ionescu",
                "Marius Popescu"
            ],
            "title": "Black-Box Ripper: Copying black-box models using generative evolutionary algorithms",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2020
        },
        {
            "authors": [
                "Herv\u00e9 Chabanne",
                "Jean-Luc Danger",
                "Linda Guiga",
                "Ulrich K\u00fchne"
            ],
            "title": "Side channel attacks for architecture extraction of neural networks",
            "venue": "CAAI Transactions on Intelligence Technology",
            "year": 2021
        },
        {
            "authors": [
                "Varun Chandrasekaran",
                "Kamalika Chaudhuri",
                "Irene Giacomelli",
                "Somesh Jha",
                "Songbai Yan"
            ],
            "title": "Exploring connections between active learning and model extraction",
            "venue": "In Proceedings of the 29th USENIX Conference on Security Symposium",
            "year": 2020
        },
        {
            "authors": [
                "Kangjie Chen",
                "Shangwei Guo",
                "Tianwei Zhang",
                "Xiaofei Xie",
                "Yang Liu"
            ],
            "title": "Stealing deep reinforcement learning models for fun and profit",
            "venue": "In Proceedings of the 2021 ACM Asia Conference on Computer and Communications Security",
            "year": 2021
        },
        {
            "authors": [
                "Ting Chen",
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Geoffrey Hinton"
            ],
            "title": "A simple framework for contrastive learning of visual representations",
            "venue": "In International conference on machine learning",
            "year": 2020
        },
        {
            "authors": [
                "Xinlei Chen",
                "Kaiming He"
            ],
            "title": "Exploring simple siamese representation learning",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition",
            "year": 2021
        },
        {
            "authors": [
                "Adam Coates",
                "Andrew Ng",
                "Honglak Lee"
            ],
            "title": "An analysis of single-layer networks in unsupervised feature learning",
            "venue": "In Proceedings of the fourteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings,",
            "year": 2011
        },
        {
            "authors": [
                "Jacson Rodrigues Correia-Silva",
                "Rodrigo F Berriel",
                "Claudine Badue",
                "Alberto F de Souza",
                "Thiago Oliveira-Santos"
            ],
            "title": "Copycat cnn: Stealing knowledge by persuading confession with random non-labeled data",
            "venue": "In 2018 International Joint Conference on Neural Networks (IJCNN)",
            "year": 2018
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "year": 2018
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition",
            "year": 2020
        },
        {
            "authors": [
                "Adam Dziedzic",
                "Muhammad Ahmad Kaleem",
                "Yu Shen Lu",
                "Nicolas Papernot"
            ],
            "title": "Increasing the cost of model extraction with calibrated proof of work",
            "venue": "arXiv preprint",
            "year": 2022
        },
        {
            "authors": [
                "Ian J Goodfellow",
                "Jonathon Shlens",
                "Christian Szegedy"
            ],
            "title": "Explaining and harnessing adversarial examples",
            "venue": "arXiv preprint arXiv:1412.6572",
            "year": 2014
        },
        {
            "authors": [
                "Jean-Bastien Grill",
                "Florian Strub",
                "Florent Altch\u00e9",
                "Corentin Tallec",
                "Pierre Richemond",
                "Elena Buchatskaya",
                "Carl Doersch",
                "Bernardo Avila Pires",
                "Zhaohan Guo",
                "Mohammad Gheshlaghi Azar"
            ],
            "title": "Bootstrap your own latent-a new approach to self-supervised learning",
            "venue": "Advances in neural information processing systems",
            "year": 2020
        },
        {
            "authors": [
                "Kaiming He",
                "Xinlei Chen",
                "Saining Xie",
                "Yanghao Li",
                "Piotr Doll\u00e1r",
                "Ross Girshick"
            ],
            "title": "Masked autoencoders are scalable vision learners",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "year": 2022
        },
        {
            "authors": [
                "Kaiming He",
                "Haoqi Fan",
                "Yuxin Wu",
                "Saining Xie",
                "Ross Girshick"
            ],
            "title": "Momentum contrast for unsupervised visual representation learning",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition",
            "year": 2020
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Mantas Mazeika",
                "Saurav Kadavath",
                "Dawn Song"
            ],
            "title": "Using self-supervised learning can improvemodel robustness and uncertainty",
            "venue": "Advances in neural information processing systems",
            "year": 2019
        },
        {
            "authors": [
                "Matthew Jagielski",
                "Nicholas Carlini",
                "David Berthelot",
                "Alex Kurakin",
                "Nicolas Papernot"
            ],
            "title": "High accuracy and high fidelity extraction of neural networks",
            "venue": "In 29th USENIX security symposium (USENIX Security",
            "year": 2020
        },
        {
            "authors": [
                "Hengrui Jia",
                "Christopher A Choquette-Choo",
                "Varun Chandrasekaran",
                "Nicolas Papernot"
            ],
            "title": "Entangled Watermarks as a Defense against Model Extraction",
            "venue": "In USENIX Security Symposium",
            "year": 2021
        },
        {
            "authors": [
                "Mika Juuti",
                "Sebastian Szyller",
                "Samuel Marchal",
                "N Asokan"
            ],
            "title": "PRADA: protecting against DNNmodel stealing attacks",
            "venue": "IEEE European Symposium on Security and Privacy (EuroS&P)",
            "year": 2019
        },
        {
            "authors": [
                "Sanjay Kariyappa",
                "Atul Prakash",
                "andMoinuddin K Qureshi"
            ],
            "title": "Maze: Data-free model stealing attack using zeroth-order gradient estimation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "year": 2021
        },
        {
            "authors": [
                "Sanjay Kariyappa",
                "Moinuddin K Qureshi"
            ],
            "title": "Defending against model stealing attacks with adaptive misinformation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "year": 2020
        },
        {
            "authors": [
                "Manish Kesarwani",
                "Bhaskar Mukhoty",
                "Vijay Arya",
                "Sameep Mehta"
            ],
            "title": "2018. Model extraction warning in mlaas paradigm",
            "venue": "In Proceedings of the 34th Annual Conference\u201917,",
            "year": 2017
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Geoffrey Hinton"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "year": 2009
        },
        {
            "authors": [
                "Yupei Liu",
                "Jinyuan Jia",
                "Hongbin Liu",
                "Neil Zhenqiang Gong"
            ],
            "title": "StolenEncoder: Stealing Pre-trained Encoders in Self-supervised Learning",
            "venue": "In Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications",
            "year": 2022
        },
        {
            "authors": [
                "Yiyong Liu",
                "Zhengyu Zhao",
                "Michael Backes",
                "Yang Zhang"
            ],
            "title": "Membership inference attacks by exploiting loss trajectory",
            "venue": "In Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications",
            "year": 2022
        },
        {
            "authors": [
                "Yunhui Long",
                "Vincent Bindschaedler",
                "Lei Wang",
                "Diyue Bu",
                "XiaofengWang",
                "Haixu Tang",
                "Carl A Gunter",
                "Kai Chen"
            ],
            "title": "Understanding membership inferences on well-generalized learning models",
            "year": 2018
        },
        {
            "authors": [
                "Ishan Misra",
                "Laurens van der Maaten"
            ],
            "title": "Self-supervised learning of pretext-invariant representations",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition",
            "year": 2020
        },
        {
            "authors": [
                "Takayuki Miura",
                "Satoshi Hasegawa",
                "Toshiki Shibahara"
            ],
            "title": "MEGEX: Datafree model extraction attack against gradient-based explainable AI",
            "venue": "arXiv preprint",
            "year": 2021
        },
        {
            "authors": [
                "Maria-Elena Nilsback",
                "Andrew Zisserman"
            ],
            "title": "n. d.",
            "venue": "Category Flower Dataset. [EB/OL]. https://www.robots.ox.ac.uk/~vgg/data/flowers/102/ Accessed October",
            "year": 2022
        },
        {
            "authors": [
                "Mehdi Noroozi",
                "Ananth Vinjimoor",
                "Paolo Favaro",
                "Hamed Pirsiavash"
            ],
            "title": "Boosting self-supervised learning via knowledge transfer",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition",
            "year": 2018
        },
        {
            "authors": [
                "Seong Joon Oh",
                "Bernt Schiele",
                "andMario Fritz"
            ],
            "title": "Towards reverse-engineering black-box neural networks. Explainable AI: Interpreting",
            "venue": "Explaining and Visualizing Deep Learning",
            "year": 2019
        },
        {
            "authors": [
                "Aaron van den Oord",
                "Yazhe Li",
                "Oriol Vinyals"
            ],
            "title": "Representation learning with contrastive predictive coding",
            "venue": "arXiv preprint arXiv:1807.03748",
            "year": 2018
        },
        {
            "authors": [
                "Tribhuvanesh Orekondy",
                "Bernt Schiele",
                "Mario Fritz"
            ],
            "title": "Knockoff nets: Stealing functionality of black-box models",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition",
            "year": 2019
        },
        {
            "authors": [
                "Soham Pal",
                "Yash Gupta",
                "Aditya Kanade",
                "Shirish Shevade"
            ],
            "title": "Stateful detection of model extraction attacks",
            "venue": "arXiv preprint",
            "year": 2021
        },
        {
            "authors": [
                "Soham Pal",
                "Yash Gupta",
                "Aditya Shukla",
                "Aditya Kanade",
                "Shirish Shevade",
                "Vinod Ganapathy"
            ],
            "title": "A framework for the extraction of deep neural networks by leveraging public data",
            "year": 2019
        },
        {
            "authors": [
                "Soham Pal",
                "Yash Gupta",
                "Aditya Shukla",
                "Aditya Kanade",
                "Shirish Shevade",
                "Vinod Ganapathy"
            ],
            "title": "Activethief: Model extraction using active learning and unannotated public data",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Nicolas Papernot",
                "Patrick McDaniel",
                "Ian Goodfellow",
                "Somesh Jha",
                "Z Berkay Celik",
                "Ananthram Swami"
            ],
            "title": "Practical black-box attacks against machine learning",
            "venue": "In Proceedings of the 2017 ACM on Asia conference on computer and communications security",
            "year": 2017
        },
        {
            "authors": [
                "Steffano Psathas"
            ],
            "title": "Decreasing Model Stealing Querying for Black Box Adversarial Attacks",
            "year": 2022
        },
        {
            "authors": [
                "Aditi Raghunathan",
                "SangMichael Xie",
                "Fanny Yang",
                "John CDuchi",
                "Percy Liang"
            ],
            "title": "Adversarial training can hurt generalization",
            "venue": "arXiv preprint arXiv:1906.06032",
            "year": 2019
        },
        {
            "authors": [
                "Adnan Siraj Rakin",
                "Md Hafizul Islam Chowdhuryy",
                "Fan Yao",
                "Deliang Fan"
            ],
            "title": "Deepsteal: Advanced model extractions leveraging efficient weight stealing in memories",
            "venue": "IEEE Symposium on Security and Privacy (SP). IEEE,",
            "year": 2022
        },
        {
            "authors": [
                "Ahmed Salem",
                "Yang Zhang",
                "Mathias Humbert",
                "Pascal Berrang",
                "Mario Fritz",
                "Michael Backes"
            ],
            "title": "Ml-leaks: Model and data independent membership inference attacks and defenses on machine learning models",
            "year": 2018
        },
        {
            "authors": [
                "Zeyang Sha",
                "Xinlei He",
                "Ning Yu",
                "Michael Backes",
                "Yang Zhang"
            ],
            "title": "Can\u2019t Steal? Cont-Steal! Contrastive Stealing Attacks Against Image Encoders",
            "venue": "arXiv preprint",
            "year": 2022
        },
        {
            "authors": [
                "Reza Shokri",
                "Marco Stronati",
                "Congzheng Song",
                "Vitaly Shmatikov"
            ],
            "title": "Membership inference attacks against machine learning models",
            "venue": "IEEE symposium on security and privacy (SP)",
            "year": 2017
        },
        {
            "authors": [
                "Xuxiang Sun",
                "Gong Cheng",
                "Hongda Li",
                "Lei Pei",
                "Junwei Han"
            ],
            "title": "Exploring effective data for surrogate training towards black-box attack",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "year": 2022
        },
        {
            "authors": [
                "Yu Sun",
                "Shuohuan Wang",
                "Yukun Li",
                "Shikun Feng",
                "Xuyi Chen",
                "Han Zhang",
                "Xin Tian",
                "Danxiang Zhu",
                "Hao Tian",
                "Hua Wu"
            ],
            "title": "Ernie: Enhanced representation through knowledge integration",
            "year": 2019
        },
        {
            "authors": [
                "Christian Szegedy",
                "Wojciech Zaremba",
                "Ilya Sutskever",
                "Joan Bruna",
                "Dumitru Erhan",
                "Ian Goodfellow",
                "Rob Fergus"
            ],
            "title": "Intriguing properties of neural networks",
            "year": 2013
        },
        {
            "authors": [
                "Florian Tram\u00e8r",
                "Fan Zhang",
                "Ari Juels",
                "Michael K Reiter",
                "Thomas Ristenpart"
            ],
            "title": "Stealing machine learning models via prediction {APIs",
            "venue": "In 25th USENIX security symposium (USENIX Security",
            "year": 2016
        },
        {
            "authors": [
                "Jean-Baptiste Truong",
                "Pratyush Maini",
                "Robert J Walls",
                "Nicolas Papernot"
            ],
            "title": "Data-free model extraction",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition",
            "year": 2021
        },
        {
            "authors": [
                "Hsiao-Yu Tung",
                "Hsiao-Wei Tung",
                "Ersin Yumer",
                "Katerina Fragkiadaki"
            ],
            "title": "Self-supervised learning of motion capture",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2017
        },
        {
            "authors": [
                "Binghui Wang",
                "Neil Zhenqiang Gong"
            ],
            "title": "Stealing hyperparameters in machine learning",
            "venue": "IEEE symposium on security and privacy (SP)",
            "year": 2018
        },
        {
            "authors": [
                "Wenxuan Wang",
                "Bangjie Yin",
                "Taiping Yao",
                "Li Zhang",
                "Yanwei Fu",
                "Shouhong Ding",
                "Jilin Li",
                "Feiyue Huang",
                "Xiangyang Xue"
            ],
            "title": "Delving into data: Effectively substitute training for black-box attack",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "year": 2021
        },
        {
            "authors": [
                "YaqingWang",
                "Quanming Yao",
                "James T Kwok",
                "Lionel M Ni"
            ],
            "title": "Generalizing from a few examples: A survey on few-shot learning",
            "venue": "ACM computing surveys (csur) 53,",
            "year": 2020
        },
        {
            "authors": [
                "Yuanshun Yao",
                "Huiying Li",
                "Haitao Zheng",
                "Ben Y Zhao"
            ],
            "title": "Latent backdoor attacks on deep neural networks",
            "venue": "In Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications",
            "year": 2019
        },
        {
            "authors": [
                "Honggang Yu",
                "Kaichen Yang",
                "Teng Zhang",
                "Yun-Yun Tsai",
                "Tsung-Yi Ho",
                "Yier Jin"
            ],
            "title": "CloudLeak: Large-Scale Deep Learning Models Stealing Through Adversarial Examples",
            "venue": "In NDSS",
            "year": 2020
        },
        {
            "authors": [
                "Xiaoyong Yuan",
                "Leah Ding",
                "Lan Zhang",
                "Xiaolin Li",
                "Dapeng Oliver Wu"
            ],
            "title": "2022. Es attack: Model stealing against deep neural networks without data hurdles",
            "venue": "IEEE Transactions on Emerging Topics in Computational Intelligence 6,",
            "year": 2022
        },
        {
            "authors": [
                "Jialong Zhang",
                "Zhongshu Gu",
                "Jiyong Jang",
                "Hui Wu",
                "Marc Ph Stoecklin",
                "Heqing Huang",
                "Ian Molloy"
            ],
            "title": "Protecting intellectual property of deep neural networks with watermarking",
            "venue": "In Proceedings of the 2018 on Asia Conference on Computer and Communications Security",
            "year": 2018
        },
        {
            "authors": [
                "Jie Zhang",
                "Bo Li",
                "Jianghe Xu",
                "Shuang Wu",
                "Shouhong Ding",
                "Lei Zhang",
                "Chao Wu"
            ],
            "title": "Towards efficient data free black-box adversarial attack",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "year": 2022
        },
        {
            "authors": [
                "Zhanyuan Zhang",
                "Yizheng Chen",
                "David Wagner"
            ],
            "title": "Seat: similarity encoder by adversarial training for detecting model extraction attack queries",
            "venue": "In Proceedings of the 14th ACM Workshop on Artificial Intelligence and Security",
            "year": 2021
        },
        {
            "authors": [
                "Benjamin Zi Hao Zhao",
                "Aviral Agrawal",
                "Catisha Coburn",
                "Hassan Jameel Asghar",
                "Raghav Bhaskar",
                "Mohamed Ali Kaafar",
                "Darren Webb",
                "Peter Dickinson"
            ],
            "title": "On the (in) feasibility of attribute inference attacks on machine learning models",
            "venue": "IEEE European Symposium on Security and Privacy (EuroS&P). IEEE,",
            "year": 2021
        },
        {
            "authors": [
                "Huadi Zheng",
                "Qingqing Ye",
                "Haibo Hu",
                "Chengfang Fang",
                "Jie Shi"
            ],
            "title": "Bdpl: A boundary differentially private layer against machine learning model extraction attacks",
            "venue": "In Computer Security\u2013ESORICS 2019: 24th European Symposium on Research in Computer Security,",
            "year": 2019
        },
        {
            "authors": [
                "Mingyi Zhou",
                "JingWu",
                "Yipeng Liu",
                "Shuaicheng Liu",
                "Ce Zhu"
            ],
            "title": "Dast: Datafree substitute training for adversarial attacks",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "To mitigate it, this work proposes an efficient model extraction attack based on prior knowledge for the first time. The insight is that prior knowledge of unlabeled proxy datasets such as intrinsic property and natural relationships is conducive to the search for the decision boundary (e.g., informative samples). Specifically, we leverage self-supervised learning including autoencoder and contrastive learning to pre-compile the prior knowledge of the proxy dataset into the feature extractor of the substitute model. Then we adopt entropy to measure and sample the most informative examples to query the target model. Our design leverages both prior and posterior knowledge to extract the model and thus eliminates generalizability errors and overfitting problems. We conduct extensive experiments on open APIs like Traffic Recognition, Flower Recognition, Moderation Recognition, and NSFW Recognition from real-world platforms, Azure and Clarifai. The experimental results demonstrate the effectiveness and efficiency of our attack. For example, our attack achieves 95.1% fidelity with merely 1.8K queries (cost 2.16$) on the NSFW Recognition API from the Clarifai platform. Also, the adversarial examples generated with our substitute model have better transferability than others, which reveals that our scheme is more conducive to downstream attacks."
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Machine Learning as a Service (MLaaS) is an emerging business paradigm that enables individual and cell corporations to enjoy\nhigh-performance models. In pay-as-you-go MLaaS solutions, companies or platforms train large-scale models and provide their Application Programming Interfaces (APIs) to the public, and customers obtain the prediction results from APIs at a reasonable cost. For instance, ChatGPT-4 charges 0.09$ on average for every 1k tokens for the 32K context model [42] (see Table 1 for more platforms). Despite its popularity, this pattern reveals such a risk: due to the wide availability of model interfaces, a malicious user could conduct attacks against cloud-based models via APIs, such as membership inference [34, 51, 53], backdoor [32, 64] and model extraction [7, 24, 58, 61, 65].\nAmong all the attacks, model extraction attack (MEA) [7, 24, 58, 61, 65] is one of the most influential ones. As shown in Figure 1, the attacker can extract a cloud-based model via interactions with APIs merely: (i) the attacker queries the victim API with carefully crafted or sampled samples from the proxy dataset, and obtains the predictions from the victim model; (ii) the attacker trains a substitute model based on the obtained knowledge for high accuracy and/or fidelity. Apart from the harm caused to intellectual property (IP), MEAs also facilitate downstream attacks, such as adversarial attacks [26, 40, 47, 62] and membership inference attacks [33], which capitalize on the similarity and transferability between the target and substitute models. Therefore, it is crucial to study the\nar X\niv :2\n30 6.\n04 19\n2v 4\n[ cs\n.C R\n] 1\n3 Ju\npotential risks resulting from MEAs, so as to protect the IP of the cloud-based services [31, 47, 52, 65].\nExisting MEAs mainly adopt a sample-query-train paradigm to extract the information of the victim model\u2019s decision boundary, and strike a trade-off between high-performance and low-cost requirements. These works can be divided into two types: generationbased and search-based strategies. The generation-based MEAs like Data Free [27, 59, 72] utilize Generative Adversary Networks to generate query examples from noises. The limitation of this method is that it requires millions of expensive queries. Recent efforts [37, 54] introduce proxy datasets to address this inefficiency. The searchbased MEAs are also based on proxy datasets. Different from the generation-based MEAs, they utilize a searching strategy to search for the most informative examples [26, 47, 65]. For example, Yu et al. [65] use adversarial active learning to iteratively craft adversary samples and query victims with them, thereby reducing the number of queries.\nDespite the impressive progress, however, there are three major drawbacks as shown in Table 2. First, most of them are based on an ideal assumption, that is, there is a gradually converging consistency with regard to the decision boundary between the substitute model and the victim model. Unfortunately, under the requirement of low query overhead, it is an intractable problem due to the eternal existence of generalization error and over-fitting for few-shot training [2, 49, 63]. Second, they all concentrate on enhancing the posterior information of queried data from victims and tend to ignore the prior knowledge from abundant unlabelled data of proxy datasets. The lack of such knowledge makes it difficult to search for the decision boundary and thus limits the performance. Third, existing fidelity-oriented MEAs are usually evaluated locally under some hypothesis. For example, Jagielski et al. [24] achieves highfidelity model extraction when the substitute model is initialized with the same weights as the victim. It is still unclear how effective these attacks are when applied in real-world scenarios.\nTo fill these gaps, in this paper, we propose a novel model extraction attack to take the first step to steal a cloud-based model with high fidelity. The insight behind our scheme is that the posterior prediction (e.g., label and score) from Oracle is far from adequate, and the prior knowledge of unlabelled proxy datasets such as intrinsic property and natural relationships is conducive to MEAs [9, 20, 22]. Motivated by this, we leverage the prior knowledge of unlabelled proxy data and posterior knowledge of labeled query data together to alleviate the generalization error and over-fitting problem. However, it is non-trivial to incorporate the prior knowledge into MEAs. On one hand, the impact of different prior knowledge on the attack performance remains unexplored, and it is vital to select a feasible solution from various approaches. In this paper, we consider both generative methods and contrastive methods to comprehensively study the effect of prior knowledge on MEAs. On the other hand, how coordinating the prior knowledge and posterior knowledge is also challenging since they may have an inclusion or opposition relationship. Therefore, we propose to first utilize self-supervised learning to leach the prior knowledge from the full unlabeled proxy datasets, and then we design an active stealing framework based on entropy for sampling, which combines the label information obtained from Oracle and the knowledge learned in the previous\nphase. At the beginning of the extraction loop, the query examples are determined by the pre-compiled prior knowledge. This enables us to make good use of both the prior knowledge in the unlabeled proxy dataset. As the queried data increases, the substitute model learns more posterior knowledge from the victim. Consequently, the newly sampled data increasingly relies on the posterior derived from the Oracle. In this way, the prior knowledge and posterior knowledge contribute together to the generalizability of query examples and the overall extraction performance.\nWe conduct extensive experiments and compare our scheme with state-of-the-art model extraction attacks for both Independent Identically Distributed (IID) and Out of Distribution (OOD) cases. We first evaluate the fidelity of the substitute model obtained by our method and other MEAs [13, 43, 46, 47, 65]. Compared with these methods, our attack can achieve up to 35% fidelity improvement, as well as a significant reduction of the query budget. Then we investigate how the stolenmodel can be utilized to implement downstream attacks such as adversarial example attacks. Compared with other methods [13, 43, 46, 47, 65], the transferability of adversarial examples generated by our substitute model surpasses the existing methods with a margin of 26%. Also, for the same transferability, our adversarial examples require a much smaller perturbation so that are less likely to be detected.\nWe also evaluate the performance of our attack in real-world MLaaS products by conducting four case studies, i.e., Traffic Recognition API, Flower Recognition API, Moderation Recognition API, and NSFW Recognition API. Our attack achieves high fidelity for stealing the NSFW Recognition model (95.1% fidelity with 2.16$ budget). Notably, our attack is much more effective than existing methods, which demonstrates the practicality in the real world.\nIn summary, we mainly make the following contributions:\n\u2022 Our work for the first time incorporates prior knowledge to model extraction attacks, which can better utilize proxy datasets as a starter for searching decision boundaries. \u2022 We design a novel high-fidelity model extraction framework with self-supervising learning (auto-encoder and contrastive learning). Our framework can well combine the prior knowledge of the proxy dataset and the posterior knowledge from the Oracle to achieve a significant improvement in extraction fidelity. \u2022 We conduct extensive experiments on popular commercial platforms andAPIs such as Traffic Recognition, Flower Recognition, Moderation Recognition, and NSFW Recognition API from Microsoft and Clarifai. All the experimental results demonstrate the effectiveness of our approach."
        },
        {
            "heading": "2 BACKGROUND",
            "text": "Revolving around the data labeling, we introduce the background of this work by answering the following four questions: 1). why do we need labeled data in \u00a7 2.1; 2). how to solve the expensive labeling problem in \u00a7 2.2; 3). is the current labeling paradigm secure in \u00a7 2.3; 4). how to improve the labeling efficiency for malicious use in \u00a7 2.4."
        },
        {
            "heading": "2.1 DNN Training",
            "text": "We first introduce the general paradigm of deep neural network training. LetD = X \u00d7 Y denotes training dataset, where X \u2286 R\ud835\udc40 is the sample space and Y \u2286 R\ud835\udc41 is the label space. The goal of deep learning is to learn a functionmap (also called a classifier) \ud835\udc53\ud835\udf03 (\u00b7) from the sample spaceX to the label spaceY, where\ud835\udf03 is the learnedmodel parameter. To ensure a better generalization, the classifier \ud835\udc53\ud835\udf03 (\u00b7) usually has a multi-layer structure \ud835\udc53\ud835\udf03 (\u00b7) = \ud835\udc53\ud835\udc3e \u25e6 \ud835\udc53\ud835\udc3e\u22121 \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 \ud835\udc532 \u25e6 \ud835\udc531 (\u00b7).\nIn the training phase, the stochastic gradient descent (SGD) algorithm is used to iteratively optimize a loss function, e.g. the cross entropy (CE). Specifically, the training process consists of two steps: 1) computing the CE loss:\nLCE ((x, y);\ud835\udf03 ) = \u2212 1 \ud835\udc41 \ud835\udc41\u2211\ufe01 \ud835\udc56=1 y\ud835\udc56 log exp(\ud835\udc53\ud835\udf03 (x)\ud835\udc56 )\u2211\ud835\udc41 \ud835\udc57=1 exp(\ud835\udc53\ud835\udf03 (x) \ud835\udc57 ) , (1)\nand 2) updating the model parameter with SGD:\n\ud835\udf03\ud835\udc61+1 = \ud835\udf03\ud835\udc61 \u2212 \ud835\udf02\n\ud835\udc35 \ud835\udc35\u2211\ufe01 \ud835\udc56=1 \u2207\ud835\udf03LCE (\ud835\udc53\ud835\udf03 (x\ud835\udc56 ), y\ud835\udc56 );\ud835\udf03 ), (2)\nwhere \ud835\udc35 is the batch size and \ud835\udf02 is the learning rate."
        },
        {
            "heading": "2.2 MLaaS",
            "text": "As learned from \u00a7 2.1, training a well-performed model requires a mass of labeled data. However, the exorbitant price for labeling impedes individuals and cell corporations from obtaining their personalized model. Under this circumstance, Machine learning as a Service (MLaaS) gradually becomes a feasible and popular choice for everyman to enjoy high-performance large-scale deep learning models. Generally speaking, this pay-as-you-go pattern works in such a way: users submit query samples to platforms, which process data and return prediction results. To date, there are many platforms and companies offering MLaaS products as shown in Table 1.\nAccording to the ownership of the cloud-based model, MLaaS can be divided into two categories: 1). Platforms train and own. A mainstream MLaaS mode is that platforms train models with exclusive massive data, and open their APIs to the public for profit. Nonetheless, these APIs are often limited to common tasks such as Face Detection1 and General Object Recognition2. 2). Platforms\n1https://www.faceplusplus.com/face-detection/ 2https://clarifai.com/clarifai/main/models/general-image-recognition-vit\ntrain but the third party owns. For some customized requirements, a third party who is short of computing power, could train and deploy its own private model with the assistance of platforms. Then the third party can monetize its models and open its APIs to downstream customers. To sum up, although the training data is provided by a third party in the second circumstance, the training processes for both scenarios are not transparent for either the third party or users. Therefore, all models trained by platforms, as well as the hyper-parameters, are completely black boxes to others."
        },
        {
            "heading": "2.3 Model Extraction Attacks",
            "text": "Tramer et al. [58] proposes the first concept of model extraction attack (MEA) and implements this attack on decision trees, logistic regression, and neural networks [58]. The objective of MEA is to extract a functionally equivalent or similar substitute for the cloudbased victim model. In practice, model extraction attacks should meet three conditions: 1) Weak attacker. The adversary only has black-box access to the cloud-based model and has zero knowledge about the structure and hyperparameters of this model. In other words, attackers could only obtain information about the cloudbased model via APIs. 2) Low query overhead. The adversary can query the target model within a small budget since the number of queries represents the economic cost for the adversary and also the risk of being detected. 3) High functional equivalence requirement. The substitute model stolen from the platform requires achieving a high similarity with the victim model.\nWithout losing generality, the mainstream MEA strategies adopt an active framework to iteratively mimic the oracle [7, 46, 65]. More specifically, the attacker first randomly initializes a query dataset, and then he circularly 1) queries the victim and obtains prediction results from the victim, 2) uses the labeled data to train a substitute model, and 3) determines the query data with the locally trained model for the next round. This circulation could repeat several times until the query budget is exhausted or termination conditions are triggered such as the functional equivalence requirement. According to the method of determining data for the next query, MEAs can be further classified into Generation-based MEAs and Search-based MEAs. Generation-based MEAs. The generation-based strategy assumes that adversaries do not possess any query dataset, which is also called data-free MEAs. In this setting, several works [27, 59, 66, 68, 72] utilize Generative Adversarial Networks (GAN) to craft\nadversarial examples located near the boundary of the target model. Despite this data-free advantage, these works cause a costly query overhead [37]. For example, Truong et al. [59] utilize 20M queries to improve the accuracy of the substitute model from 76.8% to 88.1% for the CIFAR10 task. Therefore, this setting violates the requirement of low query overhead in model extraction attacks. With consideration of the key issue of generation-based MEAs, works [5, 37, 54] are proposed to address the problem of high query consumption. More specifically, Sun et al. [54] utilizes a proxy dataset to guide the generation of the synthesized dataset, and Barbalau et al. [5] directly adopts a generative model pre-trained with semantically unrelated proxy dataset in the general data-free framework. Search-based MEAs. On the other hand, the search-based strategy considers a more practical scenario where a positive adversary gathers a surrogate dataset and then samples the most informative examples to conduct model extraction. Several recent works follow this strategy and propose query-efficient attacks based on a variety of search strategies, such as Active Learning [7, 24, 45, 46, 48, 65], Reinforcement Learning [8, 43], JacobianAugmentation [26, 47]. Despite the intuitive feasibility of this method, the crucial overfitting issue for few-shot active learning is remaining to be addressed under a constraint of low query overhead. This problem is inextricable for the current mainstream framework of MEAs while considering reducing the query budget. Unlike previous works, this paper proposes to combine the prior knowledge of data intrinsic property and posterior information obtained from oracle to effectively alleviate this symptom."
        },
        {
            "heading": "2.4 Self-supervised Learning",
            "text": "As the data-hungry property of supervised learning, the model may suffer from generalization error and over-fitting problems while the training data is deficient [2, 49]. In reality, however, most of the collected data is unlabelled while high-quality human annotation is particularly expensive. For example, data labeling company Scale3 charges 6 dollars for every image annotation. Under this circumstance, self-supervised Learning (SSL) is proposed to assist the model in getting rid of heavy Oracle labeling [23, 36, 39, 60]. The insight behind SSL is that the prior knowledge of data such as intrinsic property and natural relationships are independent of but beneficial to acquired semantic learning. Generally, SSL can be\n3https://scale.com/pricing\nsummarised into generative methods and contrastive methods. Generative methods such as Masked Autoencoder [21] mainly adopt dimensionality reduction and reconstruction to compile the most important feature of data into an encoder, as well as filter the noise contained in data [14, 55]. Different from the reconstruction principle, contrastive methods learn the distribution of datasets by increasing the distance between similar examples and estranging distinct examples [9, 20, 22]."
        },
        {
            "heading": "3 PROBLEM FORMULATION",
            "text": "In this section, we provide the threat model for model extraction attacks and the formal definition of fidelity-oriented MEA."
        },
        {
            "heading": "3.1 Threat Model",
            "text": "Attack\u2019s goal. We consider a malicious user who tries to steal the real-world cloud-based model O merely via the query API. Following previous work [6, 24, 50], we target our model extraction attack to achieve high fidelity between the substitute model O\u0302 and the victim model O. Different from the accuracy-oriented MEAs [7, 58, 65], the fidelity goals ensure a consistent boundary with the victim so that it benefits the downstream attacks. Attacker\u2019s capability.We consider a weak attacker who has blackbox access to the cloud-based model in real-world scenarios. The attacker could only query carefully sampled examples to the victim model for inference and analyze the predictions. As we investigate the commercialized APIs, we find that most of the MLaaS products provide a detailed functionality of the model. Given these facts, in this paper, we study the vulnerability of cloud-based models to MEAs under different real-world scenarios. We characterize the attacker\u2019s ability along these two dimensions: \u2022 Model Structure and Hyper-parameter. As O is a black box for the adversary, the attacker knows nothing about the model structures of the victim. Although some APIs open their architecture choice4, we restrict the attacker\u2019s knowledge to make sure the attack has a fabulous generalization ability. In this paper, we adopt ResNet50 as the architecture of the substitute model. As for hyper-parameters, we use the regular setting for DNN training. For example, we use SGD as the optimization algorithm. \u2022 Surrogate Dataset. Depending on the accessibility to the training data of the victim model, the proxy dataset D\ud835\udc5d\ud835\udc5f\ud835\udc5c\ud835\udc65\ud835\udc66 of the attacker could be categorized as two folds. a). Independent Identically Distributed (IID). It is possible that O is trained with some datasets that are so unique that have to be used or are uploaded from users. Model owners may disclose the information of datasets out of censorship from the government or consumers. In this circumstance, the attacker could have full access to the training dataset [24, 65]. b).Out of Distribution (OOD). More commonly, the MLaaS providers keep the training datasets secret. However, to facilitate the transaction with customers, platforms usually offer a clear description of the model\u2019s functionality, i.e., the concepts of categories and the scope of service. Therefore, in practice, a positive adversary could collect task-related OOD data from the public.\n4https://clarifai.com/clarifai/main/models/moderation-recognition"
        },
        {
            "heading": "3.2 Fidelity Model Extraction",
            "text": "While most of the existing works [7, 58, 65] are keen on improving the extracted model\u2019s accuracy, Jagielski et al. [24] and the following works [6, 50] explore fidelity-orientated MEAs and illustrate the critical advantages, especially when using model extraction to launch downstream attacks such as black-box adversarial example [26, 47], membership inference [33], and attribute inference attacks [70]. In this paper, we also concentrate on the fidelity goal, which can be formally defined as:\nF = 1 |X| \u2211\ufe01 x\u2208X Argmax(O(x)) = Argmax(O\u0302(x)), (3)\nwhere O(x) and O\u0302(x) denote the posterior probabilities of the victim and substitute models, respectively. Given the adversary\u2019s goal and fidelity metric, we formally define our model extraction attack as follows:\nDefinition 1 ((\ud835\udc61, \ud835\udc5e)-Model ExtractionAttack (MEA)). Given a black-box access to a cloud-based target model O, a query budget of \ud835\udc5e, a fidelity goal of \ud835\udc61 , and a validate dataset D\ud835\udc63\ud835\udc4e\ud835\udc59 , (\ud835\udc61, \ud835\udc5e)-MEA aims to obtain a substitute model O\u0302 within \ud835\udc5e queries to O such that\nF(O\u0302(D\ud835\udc63\ud835\udc4e\ud835\udc59 ),O(D\ud835\udc63\ud835\udc4e\ud835\udc59 )) \u2265 \ud835\udc61 . (4) where F is the fidelity metric as above."
        },
        {
            "heading": "4 OUR ATTACK",
            "text": ""
        },
        {
            "heading": "4.1 Problem Analysis",
            "text": "Active Learning forMEA. In active learning-basedMEA, given an Oracle (well-performed cloud-based model), active learning helps alleviate the heavy human annotation burden by sampling the most informative samples D\ud835\udc5e\ud835\udc62\ud835\udc52\ud835\udc5f\ud835\udc66 from unlabelled datasets D\ud835\udc5d\ud835\udc5f\ud835\udc5c\ud835\udc65\ud835\udc66 to tag. Note that the most informative examples are those with the highest uncertainty for locally trained substitute model O\u0302. Therefore, when the posterior (annotated data) is little, unfortunately, the decision boundary of the substitute model could have a colossal variance from that of the victim modelO. The current solutions aim to increase the information contained within query samples while ignoring the abundant prior knowledge included in the large-scale unlabelled proxy dataset. To counter such symptoms, we leverage the new emerging technique and propose a novel prior-guided model extraction scheme with entropy information. Prior-guided MEA. The prior knowledge in context is the information existing within the surrogate dataset of the attacker. Intuitively, the samples with semblable semantic annotation should have similar activation in the representation space, and vice versa. Such prior knowledge inspires the development of Self-supervision Learning, which advocates focusing on the data itself. Motivated by the great achievement made by SSL, we introduce the prior knowledge learning to the MEA task and propose a novel entropy-based MEA scheme. The key idea of our method is that in the initial phase of a MEA attack, the informative samples can be determined by the prior knowledge of the proxy dataset; and then as the query budget increases, the decision-making power gradually shifts to the posterior. Through the corporation and competition of prior and posterior knowledge, the sampled points could have the best transferability to the victim model."
        },
        {
            "heading": "4.2 Prior Knowledge Learning",
            "text": "To leverage a learning-based method to mount our model extraction attack, we consider compiling the prior knowledge into the feature extractor of the substitute model. More specifically, we utilize diverse self-supervised learning algorithms to explore the inherent property and natural relationship within the proxy dataset. Theoretically, the paradigm of SSL can learn a good representation space that is conducive to downstream supervised learning tasks. In this paper, we consider five prior knowledge including four SSL methods: Random Sampling (RS): We first consider a simple solution, random sampling, where an attacker stochastically selects examples from his proxy dataset without replacement. Then he queries the APIs with sampled points all at once and constructs a training dataset using the query results for the substitute model training. The advantages of RS are two-fold: on one hand, it guarantees an identical distribution from the query dataset to the surrogate dataset, and on the other hand, its sampling decision does not require interaction with Oracle and thus is hard to detect. Basic Autoencoder (BAE): For high-dimensional data, it is common sense that there are too many trashy or even negative features existing in raw data or embeddings. To mitigate their influence, Ballard et al. [4] propose to utilize an autoencoder to pre-train the artificial neural networks. After decades of development, a typical autoencoder usually includes an encoder and a decoder in series. As a typical paradigm, autoencoders attempt to filter out unimportant features by dimension reduction and reconstructing primitive images as formulation:\n\ud835\udc5a\ud835\udc56\ud835\udc5b\ud835\udc56\ud835\udc5a\ud835\udc56\ud835\udc67\ud835\udc52 \ud835\udc51 (\ud835\udc65, \ud835\udc53\ud835\udc51\ud835\udc52\ud835\udc50 (\ud835\udc53\ud835\udc52\ud835\udc5b\ud835\udc50 (\ud835\udc65))), s.t. |\ud835\udc53\ud835\udc52\ud835\udc5b\ud835\udc50 (\ud835\udc65) | \u226a |\ud835\udc65 |\n(5)\nwhere \ud835\udc53\ud835\udc52\ud835\udc5b\ud835\udc50 , \ud835\udc53\ud835\udc51\ud835\udc52\ud835\udc50 are the encoder and decoder of AE, respectively, and \ud835\udc51 denotes the similarity measurement. To control the degree of filtering quantificationally, we can change the output dimensionality of \ud835\udc53\ud835\udc52\ud835\udc5b\ud835\udc50 (\ud835\udc65). The prior knowledge behind BAE is that the raw data has inessential features which are deleterious for model training. Denoising Autoencoder (DAE): DAE is a popular and widely studied generative method in self-supervised learning. With the intuition that the representation of data should be robust to noise, Devlin et al. [14] propose to randomly mask some tokens from the input and attempt to reconstruct the previous data based on the context information inside for the NLP task. Then He generalizes this method to pre-train an encoder for an image task [21]. Formally, the optimization objective of DAE can be written as:\n\ud835\udc5a\ud835\udc56\ud835\udc5b\ud835\udc56\ud835\udc5a\ud835\udc56\ud835\udc67\ud835\udc52 \ud835\udc51 (\ud835\udc65, \ud835\udc53\ud835\udc51\ud835\udc52\ud835\udc50 (\ud835\udc53\ud835\udc52\ud835\udc5b\ud835\udc50 (\ud835\udc65 + \ud835\udeff))), s.t. |\ud835\udc53\ud835\udc52\ud835\udc5b\ud835\udc50 (\ud835\udc65 + \ud835\udeff) | \u226a |\ud835\udc65 |\n(6)\nwhere \ud835\udc51 denotes the \ud835\udc3f2 distance between the recovered data and raw data. In the general paradise of DAE, noise \ud835\udeff in Function 6 is randomly patched to the tokens or positional embeddings [14, 21], and its additive proportion can be adjusted to simulate different levels of noise pollution. The bottleneck design of DAE guarantees the elimination of noise contained in data, as well as enhances the robustness of the feature extractor. The prior knowledge from DAE is that through denoising, the model can be more robust and have better generalizability.\nAlgorithm 1:MEA Based on Prior Knowledge. Input: Proxy Dataset: D\ud835\udc5d\ud835\udc5f\ud835\udc5c\ud835\udc65\ud835\udc66 ; Query Budget: \ud835\udc35; Victim: O;\nInteraction Times: Itera; Output: Substitute Model: O\u0302;\n1 \ud835\udc53\ud835\udc52\ud835\udc5b\ud835\udc50 = \ud835\udc46\ud835\udc46\ud835\udc3f(D\ud835\udc5d\ud835\udc5f\ud835\udc5c\ud835\udc65\ud835\udc66) 2 O\u0302\u2190 \ud835\udc53\ud835\udc52\ud835\udc5b\ud835\udc50 + \ud835\udc53 \ud835\udc50 3 D\ud835\udc56\ud835\udc61\ud835\udc52\ud835\udc5e\ud835\udc62\ud835\udc52\ud835\udc5f\ud835\udc66 \u2190 Select \ud835\udc35/\ud835\udc3c\ud835\udc61\ud835\udc52\ud835\udc5f\ud835\udc4e samples with the highest\n\ud835\udc38\ud835\udc5b\ud835\udc61\ud835\udc5f\ud835\udc5c\ud835\udc5d\ud835\udc66 (O\u0302(D\ud835\udc5d\ud835\udc5f\ud835\udc5c\ud835\udc65\ud835\udc66)), \ud835\udc56\ud835\udc61\ud835\udc52 = 0 4 while \ud835\udc56\ud835\udc61\ud835\udc52 \u2264 \ud835\udc3c\ud835\udc61\ud835\udc52\ud835\udc5f\ud835\udc4e do 5 D\ud835\udc56\ud835\udc61\ud835\udc52\n\ud835\udc5e\ud835\udc62\ud835\udc52\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc51 = \ud835\udc36\ud835\udc4e\ud835\udc59\ud835\udc59\ud835\udc34\ud835\udc43\ud835\udc3c (D\ud835\udc56\ud835\udc61\ud835\udc52\ud835\udc5e\ud835\udc62\ud835\udc52\ud835\udc5f\ud835\udc66)\n6 D\ud835\udc56\ud835\udc61\ud835\udc52 \ud835\udc5e\ud835\udc62\ud835\udc52\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc51 = D\ud835\udc56\ud835\udc61\ud835\udc52\u22121 \ud835\udc5e\ud835\udc62\ud835\udc52\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc51 \u222aD\ud835\udc56\ud835\udc61\ud835\udc52 \ud835\udc5e\ud835\udc62\ud835\udc52\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc51 7 O\u0302\u2190 \ud835\udc53\ud835\udc52\ud835\udc5b\ud835\udc50 + \ud835\udc53 \ud835\udc50 8 Train O\u0302 with D\ud835\udc56\ud835\udc61\ud835\udc52\n\ud835\udc5e\ud835\udc62\ud835\udc52\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc51\n9 D\ud835\udc56\ud835\udc61\ud835\udc52+1\ud835\udc5e\ud835\udc62\ud835\udc52\ud835\udc5f\ud835\udc66 \u2190 Select \ud835\udc35/\ud835\udc3c\ud835\udc61\ud835\udc52\ud835\udc5f\ud835\udc4e samples with the highest \ud835\udc38\ud835\udc5b\ud835\udc61\ud835\udc5f\ud835\udc5c\ud835\udc5d\ud835\udc66 (O\u0302(D\ud835\udc5d\ud835\udc5f\ud835\udc5c\ud835\udc65\ud835\udc66 \\D\ud835\udc56\ud835\udc61\ud835\udc52\ud835\udc5e\ud835\udc62\ud835\udc52\ud835\udc5f\ud835\udc66))\n10 \ud835\udc56\ud835\udc61\ud835\udc52 = \ud835\udc56\ud835\udc61\ud835\udc52 + 1"
        },
        {
            "heading": "11 end",
            "text": ""
        },
        {
            "heading": "12 return O\u0302",
            "text": "Momentum Contrast (MoCo): MoCo is a well-performed contrastive method proposed by He et al. [22] for self-supervised learning. In short, the desired encoder in MoCo is parallelled with a key encoder which encodes the positive and negative pairs onthe-fly. Unlike traditional memory bank design, MoCo adopts the momentum-updated mechanism to renew the target encoder, which can be formulated as:\n\ud835\udf03\ud835\udc58 \u2190\ud835\udc5a\ud835\udf03\ud835\udc58 + (1 \u2212\ud835\udc5a)\ud835\udf03\ud835\udc5e, (7) where \ud835\udc5a \u2208 [0, 1) is a momentum coefficient that represents the evolving speed of the paired keys. \ud835\udf03\ud835\udc58 , \ud835\udf03\ud835\udc5e are the parameters of the target encoder and momentum encoder, respectively. Given the optimization method in Function 7, the goal of MoCo is to minimize InfoNCE [41] as:\nL\ud835\udc5e,\ud835\udc58+,\ud835\udc58\u2212 = \u2212 log exp(\ud835\udc5e \u00b7 \ud835\udc58+/\ud835\udf0f) exp(\ud835\udc5e \u00b7 \ud835\udc58+/\ud835\udf0f) +\u2211\ud835\udc58\u2212 exp(\ud835\udc5e \u00b7 \ud835\udc58\u2212/\ud835\udf0f) . (8) The optimization result for Function 8 is that the similarity between sample \ud835\udc5e and positive key \ud835\udc58+ (\u21132 (\ud835\udc5e, \ud835\udc58+)) increases, and the dot product of \ud835\udc5e and negative key \ud835\udc58\u2212 (\u21132 (\ud835\udc5e, \ud835\udc58\u2212)) decreases. The prior knowledge from MoCo is that clustering similar sample pairs and alienating dissimilar sample pairs are conducive to supervised learning. Simple Contrastive Learning Representation (SimCLR): Chen et al. introduce a novel contrastive learning algorithm in [9]. Different from previous contrastive learning methods, SinCLR abandons thememory bank and adopts a straightforwardmethod: it randomly samples \ud835\udc41 samples and leverages stochastic augmentations (i.e., random cropping, random color distortions, and random Gaussian blur) to enlarge the size of this minibatch to 2\ud835\udc41 , then it compares the difference between positive and negative samples with:\nL\ud835\udc56, \ud835\udc57 = \u2212 log exp(\ud835\udc60\ud835\udc56\ud835\udc5a(\ud835\udc67\ud835\udc56 , \ud835\udc67 \ud835\udc57 )/\ud835\udf0f)\u22112\ud835\udc41\n\ud835\udc58=1 1[\ud835\udc58\u2260\ud835\udc56 ] exp((\ud835\udc60\ud835\udc56\ud835\udc5a(\ud835\udc67\ud835\udc56 , \ud835\udc67 \ud835\udc57 )/\ud835\udf0f) , (9)\nwhere \ud835\udf0f denotes a temperature parameter and \ud835\udc60\ud835\udc56\ud835\udc5a(\ud835\udc67\ud835\udc56 , \ud835\udc67 \ud835\udc57 ) is the cosine similarity between \ud835\udc67\ud835\udc56 and \ud835\udc67 \ud835\udc57 , i.e., \ud835\udc60\ud835\udc56\ud835\udc5a(\ud835\udc62, \ud835\udc63) = \ud835\udc62\ud835\udc47 \ud835\udc63/\u2225\ud835\udc62\u2225\u2225\ud835\udc63 \u2225.\nIn SimCLR, the sample combined with its augmented sample is positive pair, while the other 2(\ud835\udc41 \u22121) augmented samples are treated as negative samples. The prior knowledge behind SimCLR is that clustering the data after the composition of multiple data augmentation operations is in favor of yielding effective representations."
        },
        {
            "heading": "4.3 Attack Scheme",
            "text": "We conduct our model extraction attack with four steps: Collecting Proxy Dataset. To mount our attack, we first gather our proxy datasetD\ud835\udc5d\ud835\udc5f\ud835\udc5c\ud835\udc65\ud835\udc66 from the Internet in accordance with the functionality of theMLaaS products. Considering the circumstances where users can upload data to the cloud for training customized models, we consider three cases: 1). The attacker has the same training dataset as the victim; 2). The attacker has no knowledge of the training data, but he knows the functionality of API and collects task-related data from the Internet; 3). The attacker is completely blind to the training set and task, he just gathers an OOD surrogate dataset which is task-irrelevant. Embedding the Prior Knowledge. After we gathered the proxy dataset, we first leverage self-supervised learning to train an encoder based on different principles (RS, BAE, DAE, MoCo, SinCLR). To improve the robustness of the encoder \ud835\udc53\ud835\udc52\ud835\udc5b\ud835\udc50 , we adopt multiple data augmentations to transform the input image into five augmented views including RandomCrop, ColorJitter, RandomGrayscale, GaussianBlur, and RandomFlip. After embedding the prior knowledge into \ud835\udc53\ud835\udc52\ud835\udc5b\ud835\udc50 , the attacker replaces the feature extractor of the substitute model O\u0302 with \ud835\udc53\ud835\udc52\ud835\udc5b\ud835\udc50 . For all SSL methods, we utilize ResNet50 as the backbone to eliminate the effect of different architectures. We leave other structures such as Masked Autoencoder [21] adopting Vision Transformer [15] for further exploration. Initialing the Start-up Query Dataset. Different from previous methods which randomly sample points as their start-up query dataset, ours yields the most informative examples based on its prior knowledge learned from the whole D\ud835\udc5d\ud835\udc5f\ud835\udc5c\ud835\udc65\ud835\udc66 . That is, as the \ud835\udc53\ud835\udc52\ud835\udc5b\ud835\udc50 estranges the dissimilar samples, the feature vectors of two distinct samples should have diverse patterns that result in low information density. This automatic clustering mechanism finds out the uncertain samples according to their natural relation. Extracting the Victim Model with Interactions. After startup, our attack consists of iteratively 1). querying the cloud-based model with sampled informative data, 2). training local substitute model with annotations from the victim, and 3). determining the most informative points for the next round in accordance with their entropy. Recall that the prior knowledge of the proxy dataset is pre-compiled into the substitute model, hence the newly selected examples are determined by the prior and posterior knowledge together. This paradigm efficiently addresses the overfitting problem when the annotated data is little and has great generalizability when the query budget increases."
        },
        {
            "heading": "5 EVALUATION",
            "text": ""
        },
        {
            "heading": "5.1 Experimental Setup",
            "text": "Attack Scenarios: We conduct our experiments in both experimental and real-world scenarios.\nLocal Experiments: To better study the effect of the distribution of proxy dataset and architecture choice of the victim model, we first conduct extensive experiments locally. Strictly following the paradigm of cloud APIs, we first train DNN models with private datasets and then encapsulate them as black boxes. Same to realworld scenarios, the model architecture and hyper-parameters are unknowable to the attacker. We use two widely used benchmark datasets CIFAR10 and STL10 in this scenario:\n\u2022 CIFAR10 [30]: CIFAR10 is a widely used balanced dataset for object classification tasks. In particular, this dataset consists of 60,000 color 32 \u00d7 32 images (50,000 training images and 10,000 testing images) from 10 categories including airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck. In this case study, we set both the surrogate dataset of the attacker and the training dataset of the victim model to the training set of CIFAR10. \u2022 STL10 [12]: STL10 is a massive dataset for developing selftaught learning algorithms. This dataset contains 5,000 training images and 8,000 testing images from 10 categories including airplane, bird, car, cat, deer, dog, horse, monkey, ship, and truck. In addition, STL10 incorporates an unlabelled subset with a total capacity of 100,000 from classes other than the aforementioned ten classes, such as chicken, snake, beer, and so on. The unlabelled set is out-of-distribution from the training dataset. For this case study, we set the training dataset as a proprietary dataset for the victim and treat the unlabelled dataset as the proxy dataset to mount our attack.\nReal-world APIs: We also conduct four case studies (Traffic Recognition, Flower Recognition, NSFW Recognition, and Moderation Recognition) in real-world scenarios. As elucidated in Table 1, the commercialized APIs could be categorized into customization and non-customization. For each condition, we implement our evaluations on two products hosted by popular MLaaS platforms Microsoft and Clarifai, respectively. For customization conditions, a third party can upload his well-labeled dataset to the cloud and the platform will deploy a model which is trained on his data. Then the model owner can open his models\u2019 APIs to downstream customers. In this case, the third party has access to the full training dataset while the public or users do not. For another condition, different from the customized MLaaS provider like Microsoft and Google, platforms such as Clarifai and \ud835\udc39\ud835\udc4e\ud835\udc50\ud835\udc52++ only allow users to query their deployed models and do not support customization. Generally speaking, these APIs usually have a better performance for common tasks while performing poorly on niche tasks. To study the vulnerability of non-customized APIs, we conduct attacks on the Moderation Recognition API and NSFW Moderation API from Clarifai. For both APIs, the training data and model are completely unknown to the public including the attacker. The only operation that an attacker can do is querying the provided APIs and then observe the prediction from these APIs.\n\u2022 Traffic Recognition: The Traffic Sign Recognition API is provided by the Microsoft Azure AI platform, and is used to recognize traffic signs in autonomous driving. We upload the well-labeled GTSRB dataset [57] to train the victim model online, and then query this API with samples. GTSRB has a total volume of 39,209 labeled data from 43 types of traffic\nDataset Query Budget\nPrior Knowledge\nRS Generative Contrastive\nBAE DAE MoCo SimCLR\nCIFAR10 1k 35.38 38.19 38.13 71.75 63.60 2k 40.96 41.27 45.02 76.40 70.19 4k 46.54 51.72 49.57 80.25 79.12\nSTL10 1k 30.26 33.45 33.64 62.69 51.64 2k 34.78 36.80 38.22 74.48 62.39 4k 41.25 42.63 46.00 81.94 71.85\nEvaluation metrics: Focusing on the fidelity goal, we measure the performance of our attack with metric defined in Formula 3. To measure the performance of our attack for downstream attacks, we use Attack Success Rate (ASR) to quantify the transferability of adversarial examples generated based on the substitute model."
        },
        {
            "heading": "5.2 Attack Effectiveness",
            "text": "IID and OOD Cases: We first evaluate the effectiveness of our attack on IID (CIFAR10 task) and OOD (STL10 task) cases. We adopt VGGNet13 architecture as the victim model and ResNet50 as the backbone of the substitute model. After training the victim models, we take them as black boxes and open their APIs only. The experimental results are reported in Table 3. We use five strategies to sample the most informative examples from the proxy dataset. In both IID and OOD cases, the substitute model trained with MoCo achieves the highest fidelity among all query budgets. For the CIFAR10 task, with 1k queries, our substitute model obtains 35.38% fidelity with RS strategy, 38.19% fidelity with BAE, 38.13% fidelity with DAE, 71.75% fidelity with MoCo, and 63.60% fidelity with SimCLR. It is worth noting that the MoCo strategy achieves 71.75% fidelity with 1k queries only, indicating that the effectiveness of our attack is rather high. As the query overhead increases, the agreement between the victim and the substitute also improves. For example, the fidelity obtained by RS raises from 40.96% to 46.54% when the query budget increases from 2K to 4K. For the STL10 task, there are similar experimental phenomena to the CIFAR10 task. After querying 4k OOD data, the substitute model achieves 81.94% fidelity with MoCo and 71.85% with SimCLR, indicating that our attack is still effective when the distribution of the proxy dataset is different from that of the victim training dataset. Case Study 2: Traffic Recognition API. We upload the GTSRB dataset to the AI platform Azure of Microsoft, and Azure automatically trains and deploys a model on the cloud for us. We conduct model extraction attacks on this API and report the experimental results in Table 4. As illustrated, with 4.00k queries, our substitute model achieves 41.84% fidelity with RS strategy, 42.11% fidelity with BAE strategy, 45.47% fidelity with DAE strategy, 47.21% fidelity with MoCo strategy and 46.55% fidelity with SimCLR strategy. Case Study 3: Flower Recognition API. In this case, the cloudbased model is a black box for the attackers, and attackers utilize Flower102 to conduct model extraction attacks. The experimental results of MEAs are shown in Table 4. As we can see, with 1.50k queries, our attack achieves 22.35%, 28.24%, 32.94%, 25.88%, and 50.00% fidelity with RS, BAE, DAE, MoCo, and SimCLR strategies, respectively. Among all sampling strategies, SimCLR achieves the highest fidelity at all query budgets. Case Study 4: Moderation Recognition API.We also implement model extraction attacks on the non-customizable models from MLaaS platforms. Here we consider the Moderation Recognition API from Clarifai, which is used to audit the content of images and videos. As demonstrated in Table 4, with 0.24k queries, our attack achieves 73.52% fidelity with RS strategy, 76.46% fidelity with BAE strategy, 78.46% fidelity with DAE strategy, 73.92% fidelity with MoCo strategy, and 83.42% with SimCLR strategy, which indicate that our attack can efficiently steal the Moderation Recognition\nmodel with a high agreement. The experimental results show that the SimCLR strategy is better than the other four strategies.\nCase Study 5: NSFW Recognition API. Besides the Moderation Recognition API, we also conduct MEAs on another official API NSFW Recognition from Clarifai. As we can see from Table 4, with 1.08k queries and 1.30$, our substitute model could achieve 91.50% agreement with the victim model, outperforming the other four strategies, i.e., 50.25% fidelity with RS, 65.70% with BAE, 84.45% with DAE, and 58.75% with MoCo. This extraction performance is alarming as the attacker extracts a cloud-based model with over 90% fidelity under a few budgets."
        },
        {
            "heading": "5.3 Comparisons with Others",
            "text": "Previous Works. We compare our attack with five state-of-theart MEAs, i.e., Correia-Silva attack [13], Pal attack [46], Orekondy attack [43], Papernot attack [47] and Yu attack [65]. Specifically, Correia-Silva et al. [13] adopts a random strategy to sample query data from the adversary\u2019s proxy data D\ud835\udc5d\ud835\udc5f\ud835\udc5c\ud835\udc65\ud835\udc66 , and then trains a substitute model on these data and the query results from the victim model. Pal et al. [46] utilize multiple strategies combining active learning to steal the victim model. In this paper, we compare our method with the Uncertainty method proposed in [46]. Similar to the Pal attack, Orekondy et al. [43] also use an active sampling\nstrategy to query from the victim model. However, the Orekondy attack leverages reinforcement learning to select query samples. Papernot attack et al. [47] utilizes the Jacobian Matrix of the substitute model to deploy data augmentation and uses the newly generated data to query the victim model. The FeatureFool (FF) strategy in Yu attack [65] adopts L-BFGS with triplet loss to generate adversarial examples to deduce the decision boundary of the victim model. We reproduce the FF method and compare our attack with it. In our attack, we adopt SimCLR to compile the prior knowledge into the substitute model. Comparisons in Real-world Scenarios. Firstly, we compare our attack with the above schemes (Correia-Silva, Pal, Orekondy, Papernot, and Yu) on commercialized APIs including Flower Recognition, Traffic Recognition, NSFW Recognition, and Moderation Recognition API. To ensure fairness, we set the substitute model architectures and hyper-parameters for all attacks to the same. We show the comparison result in Figure 3. As demonstrated, our attack outperforms other attacks for all real-world scenarios. Especially for Flower Recognition and NSFW Recognition case studies, our method surpasses other schemes by a large margin. For example, with 1.80K queries and 2.16 $, our attack achieves 95.1% fidelity to the NSFW Recognition API, ahead of the SOTA performance obtained by the Correia-Silva attack. More Comparisons. In addition to the real-world scenarios, we also conduct extensive experiments on the distribution of proxy datasets. We compare the efficiency of our attack with other attacks with different query budgets and victim architecture. Different from the CIFAR10 scenario, where the proxy dataset is the same as the victim\u2019s training set, the proxy dataset of the STL scenario is OOD. As we can see from Figure 4 and Figure 5, our attack surpasses the existing state-of-the-art attacks by large margins, demonstrating that our scheme is much more efficient. In Figure 4, our attack achieves the highest fidelity 79.41%/79.51%/79.79%/78.52% when the architecture of the victim model is VGGNet13, DenseNet121, ResNet50, and MobileNet V2, respectively. This result illustrates\nthat our attack generalizes across different victim model architectures better than other attacks. Moreover, as the query budget increases from 1000 to 5000, the margins between our attack and others grow, which indicates the efficiency of our attack in all budgets. The same experimental phenomenon also appears in the STL10 scenario, which demonstrates that our attack still outperforms other attacks even when the proxy dataset is completely irrelevant to the victim training set."
        },
        {
            "heading": "5.4 Other Impacts",
            "text": "In this section, we further study the impacts of the architecture choice of the victim model and prior knowledge. Impact of Victim Architecture. The models with the same architecture are more likely to have a similar performance. Therefore, we explore the impact of the architecture choice of the victim model. Figure 6 shows our experimental results when we consider different victim model structures, i.e., VGGNet13, DenseNet121, ResNet50, and MobileNetV3. For the substitute model, we adopt ResNet50 as its architecture. As illustrated, in both CIFAR10 and STL10 tasks, the fidelity performance of our attack is similar. Therefore, our attack generalizes well when the victim model adopts different structures. This is of great significance when considering real-world scenarios where the architecture of the cloud-based model is unknown to the attacker. Impact of Prior Knowledge. The principle of self-supervised learning determines the prior knowledge embedded in the substitute model. Roughly, self-supervised learning can be categorized into the generative method and the contrastive method. We study the impact of different self-supervised learning methods and report their results in Figure 7. Getting rid of multifarious settings, we conduct this ablation study in four real-world APIs as introduced before. We find that for Flower Recognition, NSFW Recognition, and Moderation Recognition API, SimCLR achieves the highest extraction fidelity at all costs, while another contrastive method MoCo obtains a similar performance with the generative methods BAE and DAE or even lags behind. We suspect the reason is that the self-taught process of SimCLR is more like supervised learning. Therefore, the prior knowledge it learns is transferable to the model extraction attack."
        },
        {
            "heading": "5.5 Transferability of Adversarial Example",
            "text": "This section evaluates the transferability of the adversarial example (AE) generated based on the stolen substitute model. Up to date, the AE generation of existing adversarial example attacks mainly depends on the gradient of the victim model. However, in practice, it is an extremely high requirement as the victim models are black boxes for the attackers most of the time. Considering such a predicament, Papernot et al. [47] propose a new attack paradigm where the attacker first utilizes Jacobian Matrix to steal a cloudbased model, and then leverages the substitute model to craft AEs to make the victim misclassify [46]. Intuitively, a substitute model with high fidelity should have a similar decision boundary to the target model. Therefore, the transferability of AE generated on the basis of a substitute model should be rather high.\nBudget A Budget B Budget C Budget D 0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFi de\nlit y\nC IF\nAR 10\nVGGNet\nBudget A Budget B Budget C Budget D 0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFi de\nlit y\nDenseNet\nBudget A Budget B Budget C Budget D 0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFi de\nlit y\nResNet\nBudget A Budget B Budget C Budget D 0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFi de\nlit y\nMobileNet\nBudget A Budget B Budget C Budget D 0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFi de\nlit y\nST L1\n0\nVGGNet\nBudget A Budget B Budget C Budget D 0.0\n0.2\n0.4\n0.6\n0.8 1.0 Fi de lit y DenseNet\nBudget A Budget B Budget C Budget D 0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFi de\nlit y\nResNet\nBudget A Budget B Budget C Budget D 0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFi de\nlit y\nMobileNet\nBAE DAE MoCo SimCLR\nFigure 6: Ablation study on the architecture of victim model (Budget A = 1K, Budget B = 2K, Budget C = 3K, Budget D = 4K).\n0.50 K 1.00 K 1.50 K 2.00 K 0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFi de\nlit y\nFlower Recognition API\n1.00 K 2.00 K 3.00 K 4.00 K 0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFi de\nlit y\nTraffic Recognition API\n0.36 K 0.72 K 1.08 K 1.44 K 0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFi de\nlit y\nNSFW Recognition API\n0.24 K 0.48 K 0.72 K 0.96 K 0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFi de\nlit y\nModeration Recognition API\nBAE DAE MoCo SimCLR\nFigure 7: Ablation study on the choice of prior knowledge.\nIn this paper, we utilize the Fast Gradient SignMethod (FGSM) [56] to generate adversarial examples, and then measure their transferability to the victim model. FGSM [18] is a one-step AE-generating attack that adopts the gradient ascent method to optimize the adversarial example. Formally, the FGSM generates AE in accordance with:\n\ud835\udc65 = \ud835\udc65 + \ud835\udf16\ud835\udc60\ud835\udc56\ud835\udc54\ud835\udc5b(\u2207\ud835\udc65L(\ud835\udf03, \ud835\udc65,\ud835\udc66)), (10)\nwhere \ud835\udf16 determines the magnitude of disturbance. Here, we define the transferability of AE as the percentage of the crafted AEs that are misclassified by the victim. We evaluate the performance of the generated AEs at the rates of \ud835\udf16 = 0.03, 0.06, 0.09, 0.12, 0.15, 0.18, 0.24. Transferability Comparisons.We conduct model extraction attacks and adversarial example attacks on IID and OOD cases. In the first stage of the attack, i.e., MEA, we set the query budget as 2,000 for both cases. After extracting the victim model, we leverage the substitute model to generate adversarial examples with the FGSM attack for the downstream attack. As demonstrated in Figure 8, for the IID case, the AE generated by our method has better transferability across a wide range of perturbations. Our method outperforms other schemes by a large margin, especially when the perturbation is small. For example, our attack achieves 82.70% ASR when the perturbation size is only 0.06. Moreover, we also conduct adversarial example attacks with an OOD proxy dataset. As we can see from Figure 9, our attack maintains the highest transferability as the perturbation \ud835\udf16 increases from 0.03 to 0.24. Our attack is clearly ahead of other attacks when the perturbation is about 0.09 in terms of transferability. Another significant superiority of our attack is that when achieving the same ASR, our attack requires a much smaller perturbation, indicating that our AEs are harder to be visually detected. Therefore, our model extraction attack is more conducive to downstream attacks than the existing MEAs.\nImpact of SSL. Table 5 and 6 show the ASR of AEs generated from the stolen model with different strategies. As we can see from Table 5, the substitute model extracted with SimCLR achieves the highest ASR for \ud835\udf16 = 0.06, 0.09, 0.12, 0.15. Another contrastive method MoCo obtains the highest ASR when the perturbation is 0.03. Totally speaking, both the AEs crafted by generative methods and contrastive methods achieve high transferability when the \ud835\udf16 is rather high. However, the contrastive methods behave better at a small perturbation. For the OOD case, there is a similar phenomenon to the IID case. It is noteworthy that MoCo dominates at most perturbations except \ud835\udf16 = 0.03 as shown in Table 6."
        },
        {
            "heading": "6 DISCUSSION",
            "text": ""
        },
        {
            "heading": "6.1 Defenses",
            "text": "To defend existing MEAs, there is a variety of works providing countermeasures. We give a detailed analysis of these defenses and the possible effect on our attack. DetectMaliciousQueries.A simple and straightforward approach to defend MEAs is to analyze the adversary\u2019s query examples and block her/his account once being determined to be malicious by the platform. For example, in this line, PRADA [26] proposes a detection method, which studies the distribution of consecutive query samples and sounds the alarm if the distribution is abnormal from benign behavior. SEAT [69] presents a similarity encoder to measure the context of query samples\u2019 resemblance, and then decides whether to terminate the user\u2019s account based on the similarity results. Besides, other works such as [3, 29, 44] also explore several detection-based strategies. These defenses could be the most likely countermeasure for attacks with adversarial examples including ours. ObstructAttacker\u2019sAcquisition.Different from the above detectionbased solutions, obstruction-based strategies attempt to slow down the adversary\u2019s process of obtaining useful information from the victim\u2019s API. For example, Dziedzic et al. [16] impede MEAs with a calibrated proof-of-work strategy, in which users are required to complete a proof-of-work before they can obtain the model\u2019s predictions. This increases the difficulty for adversaries to get the victim\u2019s prediction results. However, this method is not effective to defend our attack since we only consume a small query budget. Perturb Adversary\u2019s Training. Another angle of defense is to limit the information leakage of each query such that adversaries could not obtain useful information to train a substitute model. To this end, lots of works propose perturbation-based methods. For example, Zheng et al. [71] leverage differential privacy to perturb prediction responses near the decision boundary. Kariyappa et al. [28] add adaptive misinformation into OOD queries to defend adversarial examples-based MEAs. However, as this defense needs to modify the prediction of the query sample, it would bear a loss of accuracy. Therefore, it is necessary to consider a better trade-off between prediction accuracy and defensive effect. Discriminate Intellectual Property. Besides the above defenses, some works also propose methods for protecting the model\u2019s IP, which can be regarded as another defense for MEAs. Generally speaking, these methods protect a model\u2019s IP via planting a watermark into the model such that the authentic owner could declare possession of the model [1, 67]. For example, Jia et al. [25] propose a watermark-based method, called EWE, to guarantee that the watermark cannot be removed even when retraining. However, as the cultivation of a watermark would affect the accuracy of the model, the defender needs to sacrifice the performance to achieve higher security."
        },
        {
            "heading": "6.2 Future Work",
            "text": "Further exploration of prior knowledge to MEAs. Our attack introduces self-supervised learning to the model extraction attack. As a hot topic, abundant SSL algorithms are proposed and proved to be efficient when the training data is few such as BYOL [20] and SimSiam [10]. Adopting the general scheme proposed in this\npaper, the existing SSL methods can replace the module of prior knowledge learning directly. Moreover, another potential direction is to explore the likelihood of combining unsupervised learningwith MEAs. Unsupervised learning clusters unlabeled data in accordance with the distance between samples. In this case, the samples located at the border of clusters are more likely to be sorted as another category. Extension to more API types. During our investigation of commercialized APIs, we find that there are some other kinds of MLaaS products, such as Body Outlining Model5 from \ud835\udc39\ud835\udc4e\ud835\udc50\ud835\udc52++ and Audio Transcription Model6 from Clarifai are not studied in previous works. However, there is luxuriant commercial value contained in those APIs thus worth considering their security problem. Due to the differences in modal and task, the existing model extraction attacks may not be used to steal the functionality of those commercialized APIs. An interesting direction is to explore the vulnerability of these models."
        },
        {
            "heading": "6.3 Responsible exposure",
            "text": "Before we conduct our attacks on those APIs, we informed the MLaaS providersMicrosoft and Clarifai. After we have implemented MEAs, we report the attack effect to them and provide them with some useful countermeasures to protect the IP of cloud-based models."
        },
        {
            "heading": "7 CONCLUSION",
            "text": "In this work, we propose a novel and efficient fidelity-oriented extraction framework to steal the cloud-basedmodel with prior knowledge. We pre-compile the prior knowledge of the proxy dataset into the substitute model and explore the utilization efficiency of unlabeled data to the extreme. Different from existing MEA attacks, which heavily rely on the posterior knowledge of victim models and hence result in generalization error and overfitting problems under query budget limitations, we combine the prior and posterior knowledge together achieving high efficiency in large-scale cloudbased model stealing attacks. The experimental results demonstrate that our attack achieves remarkable extraction performance and surpasses other methods by a gargantuan margin. We hope this work could inspire the study of introducing more prior knowledge to the model stealing attacks and raise the alarm about the security of MLaaS."
        }
    ],
    "title": "Extracting Cloud-based Model with Prior Knowledge",
    "year": 2023
}