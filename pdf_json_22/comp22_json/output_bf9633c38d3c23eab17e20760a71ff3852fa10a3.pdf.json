{
    "abstractText": "Previous works on font generation mainly focus on the standard print fonts where character\u2019s shape is stable and strokes are clearly separated. There is rare research on brush handwriting font generation, which involves holistic structure changes and complex strokes transfer. To address this issue, we propose a novel GAN-based image translation model by integrating the skeleton information. We first extract the skeleton from training images, then design an image encoder and a skeleton encoder to extract corresponding features. A self-attentive refined attention module is devised to guide the model to learn distinctive features between different domains. A skeleton discriminator is involved to first synthesize the skeleton image from the generated image with a pre-trained generator, then to judge its realness to the target one. We also contribute a large-scale brush handwriting font image dataset with six styles and 15,000 high-resolution images. Both quantitative and qualitative experimental results demonstrate the competitiveness of our proposed model.",
    "authors": [
        {
            "affiliations": [],
            "name": "Shaozu Yuan"
        },
        {
            "affiliations": [],
            "name": "Ruixue Liu"
        },
        {
            "affiliations": [],
            "name": "Meng Chen"
        },
        {
            "affiliations": [],
            "name": "Baoyang Chen"
        },
        {
            "affiliations": [],
            "name": "Zhijie Qiu"
        },
        {
            "affiliations": [],
            "name": "Xiaodong He"
        }
    ],
    "id": "SP:89ac30b5c51ac368b59a3108a289457b0afd0323",
    "references": [
        {
            "authors": [
                "Yuchen Tian"
            ],
            "title": "Master chinese calligraphy with conditional adversarial networks",
            "venue": "https://github. com/kaonashi-tyc/zi2zi, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "Danyang Sun",
                "Tongzheng Ren",
                "Chongxuan Li",
                "Hang Su",
                "Jun Zhu"
            ],
            "title": "Learning to write stylized chinese characters by reading a handful of examples",
            "venue": "Proceedings of the 27th International Joint Conference on Artificial Intelligence, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "Bo Chang",
                "Qiong Zhang",
                "Shenyi Pan",
                "Lili Meng"
            ],
            "title": "Generating handwritten chinese characters using cyclegan",
            "venue": "2018 IEEE Winter Conference on Applications of Computer Vision (WACV). IEEE, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "Yue Gao",
                "Yuan Guo",
                "Zhouhui Lian",
                "Yingmin Tang",
                "Jianguo Xiao"
            ],
            "title": "Artistic glyph image synthesis via onestage few-shot learning",
            "venue": "ACM Transactions on Graphics (TOG), vol. 38, no. 6, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "Chuan Wen",
                "Yujie Pan",
                "Jie Chang",
                "Ya Zhang",
                "Siheng Chen",
                "Yanfeng Wang",
                "Mei Han",
                "Qi Tian"
            ],
            "title": "Handwritten chinese font generation with collaborative stroke refinement",
            "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Ian Goodfellow",
                "Jean Pouget-Abadie",
                "Mehdi Mirza",
                "Bing Xu",
                "David Warde-Farley",
                "Sherjil Ozair",
                "Aaron Courville",
                "Yoshua Bengio"
            ],
            "title": "Generative adversarial nets",
            "venue": "Advances in neural information processing systems, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "Phillip Isola",
                "Jun-Yan Zhu",
                "Tinghui Zhou",
                "Alexei A Efros"
            ],
            "title": "Image-to-image translation with conditional adversarial networks",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "Jun-Yan Zhu",
                "Taesung Park",
                "Phillip Isola",
                "Alexei A Efros"
            ],
            "title": "Unpaired image-to-image translation using cycle-consistent adversarial networks",
            "venue": "Proceedings of the IEEE international conference on computer vision, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "Yunjey Choi",
                "Minje Choi",
                "Munyoung Kim",
                "Jung-Woo Ha",
                "Sunghun Kim",
                "Jaegul Choo"
            ],
            "title": "Stargan: Unified generative adversarial networks for multi-domain image-to-image translation",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "Junho Kim",
                "Minjae Kim",
                "Hyeonwoo Kang",
                "Kwang Hee Lee"
            ],
            "title": "U-gat-it: Unsupervised generative attentional networks with adaptive layer-instance normalization for image-to-image translation",
            "venue": "International Conference on Learning Representations, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Yexun Zhang",
                "Ya Zhang",
                "Wenbin Cai"
            ],
            "title": "Separating style and content for generalized style transfer",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 8447\u20138455.",
            "year": 2018
        },
        {
            "authors": [
                "Yangchen Xie",
                "Xinyuan Chen",
                "Li Sun",
                "Yue Lu"
            ],
            "title": "Dgfont: Deformable generative networks for unsupervised font generation",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 5130\u20135140.",
            "year": 2021
        },
        {
            "authors": [
                "Shaozu Yuan",
                "Ruixue Liu",
                "Meng Chen",
                "Baoyang Chen",
                "Zhijie Qiu",
                "Xiaodong He"
            ],
            "title": "Learning to compose stylistic calligraphy artwork with emotions",
            "venue": "Proceedings of the 29th ACM International Conference on Multimedia, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Bo Chang",
                "Qiong Zhang",
                "Shenyi Pan",
                "Lili Meng"
            ],
            "title": "Generating handwritten chinese characters using cyclegan",
            "venue": "2018 IEEE Winter Conference on Applications of Computer Vision (WACV). IEEE, 2018, pp. 199\u2013207.",
            "year": 2018
        },
        {
            "authors": [
                "Jinshan Zeng",
                "Qi Chen",
                "Mingwen Wang"
            ],
            "title": "Diversity regularized stargan for multi-style fonts generation of chinese characters",
            "venue": "Journal of Physics: Conference Series. IOP Publishing, 2021, vol. 1880, p. 012017.",
            "year": 2021
        },
        {
            "authors": [
                "Yiming Gao",
                "Jiangqin Wu"
            ],
            "title": "Gan-based unpaired chinese character image translation via skeleton transformation and stroke rendering",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Yue Jiang",
                "Zhouhui Lian",
                "Yingmin Tang",
                "Jianguo Xiao"
            ],
            "title": "Scfont: Structure-guided chinese font generation via deep stacked networks",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, 2019, vol. 33.",
            "year": 2019
        },
        {
            "authors": [
                "Jinshan Zeng",
                "Qi Chen",
                "Yunxin Liu",
                "Mingwen Wang",
                "Yuan Yao"
            ],
            "title": "Strokegan: Reducing mode collapse in chinese font generation via stroke encoding",
            "venue": "arXiv preprint arXiv:2012.08687, 2020.",
            "year": 2012
        },
        {
            "authors": [
                "TY Zhang",
                "Ching Y. Suen"
            ],
            "title": "A fast parallel algorithm for thinning digital patterns",
            "venue": "Communications of the ACM, vol. 27, no. 3, pp. 236\u2013239, 1984.",
            "year": 1984
        },
        {
            "authors": [
                "Bolei Zhou",
                "Aditya Khosla",
                "Agata Lapedriza",
                "Aude Oliva",
                "Antonio Torralba"
            ],
            "title": "Learning deep features for discriminative localization",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "Jungbeom Lee",
                "Eunji Kim",
                "Sungmin Lee",
                "Jangho Lee",
                "Sungroh Yoon"
            ],
            "title": "Ficklenet: Weakly and semisupervised semantic image segmentation using stochastic inference",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "Aditya Chattopadhay",
                "Anirban Sarkar",
                "Prantik Howlader",
                "Vineeth N Balasubramanian"
            ],
            "title": "Gradcam++: Generalized gradient-based visual explanations for deep convolutional networks",
            "venue": "2018 IEEE Winter Conference on Applications of Computer Vision (WACV). IEEE, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "Martin Heusel",
                "Hubert Ramsauer",
                "Thomas Unterthiner",
                "Bernhard Nessler",
                "Sepp Hochreiter"
            ],
            "title": "Gans trained by a two time-scale update rule converge to a local nash equilibrium",
            "venue": "arXiv preprint arXiv:1706.08500, 2017.",
            "year": 2017
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014 Font Generation, Generative Adversarial Network, Brush Handwriting Font Dataset\n1. INTRODUCTION\nDuring thousands-year history of Chinese calligraphy, many styles of writing or chirography came into being. The chirography style can be defined as the skeleton structure and stroke style. The skeleton contains the basic information of character, such as the composition and position of strokes, writing direction, etc., while the stroke style means the deformation of the skeleton, such as the thickness, shape, writing strength, etc. Intuitively, it\u2019s essential to ensure structure correct and keep style consistent when generating brush handwriting font automatically.\nRecent works [1\u20135] formulate the Chinese font generation as an image style transfer problem, where characters in the reference style are transferred to a specific style. As it\u2019s time-consuming and labor-intensive to create a handwriting Chinese font library, most of the researches focus on Standard Print Font Generation (SPFG), however, there is rare research on Brush Handwriting Font Generation (BHFG). As shown in\n\u2020 Corresponding author, email: chenmeng20@jd.com.\nFig. 1, BHFG is much more challenging than SPFG. It\u2019s observed that, even for the same character, the character images written in different styles look quite different. Especially for the characters written in cursive or semi-cursive styles, their images are heavily distorted. On one hand, the basic structure and layout of separated strokes share some similarities so that the character can be recognisable. One the other hand, there exist large geometric variations in the shape so the impressive styles can be easily distinguished. Based on these observation and analysis, we argue that the skeleton of character is critical for preserving the character content among different styles. However, most existing approaches designed for SPFG neglect the importance of skeleton.\nTo address above issue, we first collect a large-scale brush handwriting Chinese font dataset, which contains six different chirography styles and more than 15,000 high-resolution images. Then we propose a novel end-to-end Skeleton Enhanced Generative Adversarial Network (denoted as SEGAN) for BHFG which can handle the large geometric variations between different styles. SE-GAN is a one-stage model, which means that the generator can directly output stylized images with user-specified input character. SE-GAN includes two encoders to catch the features from both source image and corresponding skeleton image. To extract and fuse the features from two sources effectively, a novel Self-attentive Refined Attention Module (SAttRAM) is devised and applied in the generator. To further ensure the structure preservation, we also design an extra skeleton discriminator to keep the skeleton of generated image close to the skeleton of target image. Extensive experiments were conducted on six different stylized brush handwriting font generation tasks. The experimental results show the competitiveness of our proposed model compared to the baselines. ar X iv :2 20 4.\n10 48\n4v 1\n[ cs\n.C V\n] 2\n2 A\npr 2\n02 2\nThe contributions of this paper can be summarized as follows: 1) We propose a novel end-to-end GAN-based model (SE-GAN) for BHFG. Besides, a novel Self-attentive Refined Attention Module (SAttRAM) is devised and applied in the generator to extract and fuse the features from source image and skeleton image effectively. 2) Extensive experiments were conducted on six different styles of Chinese font generation tasks. Both automatic and human evaluations demonstrate the efficacy of SE-GAN compared to strong baselines. 3) To facilitate future research on BHFG, we also contribute a large-scale font image dataset and will release it soon.\n2. RELATED WORK\nImage-to-Image Translation. Since the propose of Generative Adversarial Network (GAN) [6], many works have been proposed for image-to-image translation tasks. Pix2pix [7] is a conditional GAN based on supervised learning with paired data. Then unsupervised image translation models such as CycleGAN [8] is proposed to use the cycle consistency to improve the training stability. Apart from one-to-one domain image translation, StarGAN [9] introduces a domain classifier or a shared multi-domain embedding to achieve many-tomany domain translation with a single model. Furthermore, U-GAT-IT [10] proves the effectiveness of attention module in image translation task, which can guide the model to focus on more important regions between different domains. However, these works are formulated as pixel-to-pixel translation where the source and target images contain little deformations, which cannot be directly applied to font generation tasks with huge holistic changes. Chinese Font Generation. Most previous works formulate character image generation as image translation tasks [1, 11\u201313]. Zi2zi [1] generates stylized Chinese font generation with paired data. However, it requires large-scale fontpair corpus for pre-training and fine-tuning. To overcome the challenge of insufficient data, [14] apply cycle consistency to generate fonts from unpaired data, and [11, 12] separate con-\ntent and style as two irrelevant domains and learn the latent style from font images. Recently, [15] utilize StarGAN [9] equipped with diversity regularizer to achieve multi-style Chinese font generation. Some other works [5, 16\u201318] try to integrate more domain knowledge of character into multi-stage model for character generation. Most of these works concentrate on the SPFG, and the images from source and target domain usually share very similar styles or shapes. In this paper, we focus on the BHFG task, which contains large stroke style difference and layout changes. We propose a onestage model instead of multi-stage model, thus all modules are trained jointly and the generation process is more efficient.\n3. APPROACH"
        },
        {
            "heading": "3.1. Overall Framework",
            "text": "Figure 2 shows our framework with two encoders: the image encoderEi and the skeleton encoderEs, which are composed of four residual blocks. TheEi is employed to extract character image features from Xi, including content and style information from source domain whereas the skeleton encoder Es aims to preserve the structure feature from Xs. To enhance the feature extraction and fusion of two different features, a novel self-attentive refined attention module (SAttRAM) with two variants, SRAM and CRAM, are stacked sequentially to extract the skeleton enhanced image representations. Then, the generator takes the refined image representations which include both content and style information as input to generate the target style image. Following the adversarial training strategy in GANs, we employ two discriminators. The first Di is used to discriminate the target image and generated image. The second Ds tries to detect whether the skeleton of generated image is coherent to the skeleton image extracted from the target domain character image. For skeleton extraction, inspired by [19], we adopt an simple but effective skeletonization algorithm to extract the skeleton image by eroding and dilating the binarized character image iteratively."
        },
        {
            "heading": "3.2. Self-attentive Refined Attention Module",
            "text": "The previous works [10,20,21] demonstrate the effectiveness of class activation map (CAM) in localizing the important regions of input images in both image generation and classification tasks. Inspired by this, CAM can be used to extract style-discriminative attention heatmaps in the font generation task. To obtain style-discriminative features M(x), the decoded feature maps F (x) \u2208 RC\u00d7W\u00d7H of source font x are first fed into the classifier, a fully connected (FC) layer with weights \u2126 \u2208 RC , for domain classification. Then CAM computes the attention heatmaps by linearly weighted summation of all channels:\nM(x) = C\u2211 c=k \u2126kF (x)k (1)\nwhere M(x) \u2208 RW\u00d7H\u00d7C indicates the attention heatmap at spatial locationH,W , \u2126k represents the weight for channel k in feature maps, and F (x)k \u2208 RW\u00d7H represents the feature map of channel k from the last convolutional layer at spatial location HW .\nHowever, the CAM lacks the spatial attention and usually leads to over-activation issues for feature capturing [22]. Moreover, it\u2019s difficult to integrate multi-modal features with a CAM module. To refine the style-discriminative feature and integrate multi-modal features, we propose a self-attentive refined attention module (SAttRAM) as shown in Fig 3. We bring self attention as an efficient module to refine the pixelwise attention heatmaps by capturing spatial dependency. Hence, the refined feature maps can be defined as follows:\nM\u0302(x) = f(F (x), F (x))g(M(x)) +M(x) (2)\nf(F (x), F (x)) = \u03c3(\u03b8(F (x))T )\u03c6(F (x))) (3)\nHere f is a pairwise embedding function that computes dot-product pixel affinity as self-refined attention weight normalised by softmax function \u03c3 in an embedding space. The embedding functions \u03b8, \u03c6 are implemented by individual 1 \u00d7 1 convolution layers, where \u03b8(F (x)) \u2208 RC1\u00d7WH and \u03c6(F (x)) \u2208 RC1\u00d7WH . The function g reshapes the input feature F (x) to g(F (x)) \u2208 RWH , all of which are aggregated with this similarity weights given by function f(F (x), F (x)) \u2208 RC\u00d7WH . This self-refined attention weight is normalised by softmax function \u03c3 and the output is M\u0302(x) \u2208 RW\u00d7H\u00d7C .\nTo handle the multimodal input features for image generation, we build two variant attention units on top of the refined attention feature maps, namely the self-refined attention module (SRAM) unit and the cross-refined attention module (CRAM) unit. SRAM takes a group of images features xi = Ei(x) as input to obtain attended features, since image features are basic information in image translation. CRAM catches intra-modal interactions between image features xi = Ei(Xi) and skeleton features xs = Es(Xs),\nwhich can further refine the feature maps extracted from character images.\nM\u0302(xi, xs)) = f(F (xi), F (xs))g(M(xi)) +M(xi) (4)\nIn Equation 4, the character encoding feature is embedded into the residual space by function g. Whereas f represents the pixel-level feature aggregation between image feature xi and skeleton feature xs."
        },
        {
            "heading": "3.3. Discriminators",
            "text": "We design two discriminators for learning target font style. Following the traditional setting, the first discriminator Di is employed to calculate how similar the generated image is to the font image of target domain, and the encoder for Di also exploits the refined attention feature maps as mentioned in previous section. Under the assumption that the generated character image should have similar skeleton to its target font image, we also design an extra skeleton discriminator Ds. Considering that the skeletonization is a non-differential function, we first apply a pre-trained skeleton generator (skeletonGAN) to generate the corresponding skeleton from a given character image. Then the skeleton discriminator Ds is applied to distinguish the skeleton of generated image from that of ground truth image. For skeletonGAN, we train a simple CycleGAN model [8] without SAttRAM module, as skeletonization task appears to be much easier than the character generation task. With the help of skeleton discriminator Ds, the model is encouraged to preserve the content and structure of character during training, meanwhile, Ds is also served as a regularizer for SE-GAN to prevent model from overfitting."
        },
        {
            "heading": "3.4. Loss Design",
            "text": "The loss function of SE-GAN contains content loss, adversarial loss, cycle-consistency loss and classification loss. The content loss consists Lcon of two parts: the pixel loss Lpix which forces the generated image GF (Xi, Xs) to be similar to the target image Yt, and the skeleton consistency\nloss Lsc which ensures the skeletons consistency between SG(GF (Xi, Xs)) and SG(Yi).\nLpix(GF ) = EX [||GF (Xi, Xs)\u2212 Yt||1] (5)\nLsc(GF , SG) = EX [||SG(GF (Xi, Xs))\u2212 SG(Yt)||1] (6) where the SG represents the pre-trained skeletonGAN.\nThe cycle-consistency loss Lcycle is identical to that used in [8], which guarantees that the cycle transformation is able to bring the image back to the original state. In order to distinguish that the image Xi belongs to source or target style ycls and promote the style transformation in the refined attention module, we use an auxiliary loss Lcls following [10].\nBesides, the adversarial loss Ladv combines discriminative loss LDi and skeleton-level discriminative loss LDs , and these two losses aim to catch different properties of the desired generated image Xi.\nLDi(GF , Di) =EY [logDi(GF (Xi, Xs))] + EX [log(1\u2212Di(GF (Xi, Xs)))] (7)\nLDs(GF , Ds) =EY [logDs(Ys)] + EX [log(1\u2212Ds(SG(GF (Xi, Xs))))]\n(8) where Di, Ds are pixel-level discriminator and skeleton discriminator respectively.\nFinally, we jointly train the generators, discriminators, and classifiers by using the full objective as follows:\nmin GF ,GB ,RAMp max D1,D2 \u03bb1Ladv + \u03bb2Lcycle + \u03bb3Lcls + \u03bb4Lcon (9) where \u03bbi controls the weights of different losses. Here, we omit the corresponding backward loss functions for simplicity because they are also defined in the same way.\n4. EXPERIMENTS"
        },
        {
            "heading": "4.1. Experimental Setup",
            "text": "As the deficiency of public brush handwriting font generation dataset, we collect a large-scale image dataset for experiments with six different styles. The statistics of each style is\npresented in Table 1. The total number of images is 15,799, and the size of each subset ranges from 1,419 to 3,958. During experiments, we split each subset into train/dev/test set by ratio of 8:1:1. We choose the standard print font Liukai1 as the source domain, and take each of the six styles as the target domain respectively. We adopt content accuracy [3] and Fre\u0301chet Inception Distance (FID) [23] as evaluation metrics. For baselines, we compare our model with four representative font generation models, including zi2zi [1], CycleGAN [14], StarGAN [15], and DGFont [12]."
        },
        {
            "heading": "4.2. Experimental Results",
            "text": "Quantitative analysis. We calculate both the Top 1 content accuracy (Acc) and FID score for all baselines and our proposed model on six chirography styles. Table 1 illustrates that our proposed model SE-GAN achieves the best accuracy and the lowest FID score for nearly all six styles. Compared with supervised methods like zi2zi, our model can still generate high-quality images. Compared with unsupervised models CycleGAN and StarGAN, which have unstable performances among different styles, SE-GAN still has very competitive performance and significant improvements. For DF-Font, we notice that it is inclined to generate similar font images to the source images, however, the distinctive styles are not well learnt. Although the content accuracy of DF-Font is high, the FID score is the worst. Qualitative analysis. We show some generated examples of all models in Fig. 4 for case study. Generally, the font images from SE-GAN are much easier to recognize and the chirography styles are more consistent with the original styles mentioned. For other baselines, there exists obvious flaws in the generated font images. For zi2zi, there are missing strokes for Style 2 and Style 6, and the structures are tied to each other for Style 1 and Style 5. For CycleGAN, the structures are wrong\n1https://www.foundertype.com/index.php\nand the characters are hard to recognize for Style 1 and 3. For StarGAN, there are some extra and erroneous strokes in Style 2 and 3, which damage the overall appearance of characters. All above examples demonstrate that the skeleton information can facilitate the font generation.\nAblation Study. We conduct ablation experiments by removing the skeleton input Es from generator and removing the skeleton discriminator Ds separately. As shown in Table 1, after removing Es, the accuracy drops evidently compared with SE-GAN, which indicates the skeleton information is helpful for learning the structure of characters. However, it still outperforms CycleGAN for most of the styles, proving the effectiveness of SAttRAM. Besides, removingDs also degrades the model performance over all styles, indicating the necessity of skeleton discriminator. Fig. 5 (a) illustrates the generated images of different ablated models. Due to the lack of skeleton information, some strokes in the generated characters are either missing or over exaggerated. When Ds is removed, some components are mixed together, hurting the readability of the generated images. Fig. 5 (b) compares the heated attention maps of CAM, SRAM and CRAM. It\u2019s ob-\nserved that, compared with CAM, SRAM and CRAM have fewer over-activations and more complete activation coverage. Besides, the font shape learnt by CRAM is more accurate than SRAM and closer to the ground-truth font image, which verified the contribution of skeleton. User Study. We conduct two kinds of user study (we skip evaluating DF-Font considering its bad performance in Table 1). The first is to evaluate preference score for the generated font images of different models. The second is to pick out the more visually pleasing font image by mixing the generated images with human-written font images. Totally sixty students majored in fine arts with more than 3-year experience of calligraphy writing were invited to finish the human evaluation. Table 3 reveals our model obtains the highest user preference score and winning rate from the human experts.\nIn this paper, we propose SE-GAN, a novel end-to-end framework for brush handwriting font generation. As the task involves holistic structure changes and complex stroke transfer, we propose to integrate the skeleton information for character image generation. We design two encoders to extract the character and skeleton features respectively. To efficiently fuse the information from both sides, a novel self-attentive attention module is devised in the generator. Besides, we also employ a skeleton discriminator to ensure the content consistency between generated and target images. The experiments demonstrate the advantages of our proposed model over several strong baseline methods. In the future, we will explore the pre-trained image translation models to facilitate this task.\n6. REFERENCES\n[1] Yuchen Tian, \u201cMaster chinese calligraphy with conditional adversarial networks,\u201d https://github. com/kaonashi-tyc/zi2zi, 2017.\n[2] Danyang Sun, Tongzheng Ren, Chongxuan Li, Hang Su, and Jun Zhu, \u201cLearning to write stylized chinese characters by reading a handful of examples,\u201d in Proceedings of the 27th International Joint Conference on Artificial Intelligence, 2018.\n[3] Bo Chang, Qiong Zhang, Shenyi Pan, and Lili Meng, \u201cGenerating handwritten chinese characters using cyclegan,\u201d in 2018 IEEE Winter Conference on Applications of Computer Vision (WACV). IEEE, 2018.\n[4] Yue Gao, Yuan Guo, Zhouhui Lian, Yingmin Tang, and Jianguo Xiao, \u201cArtistic glyph image synthesis via onestage few-shot learning,\u201d ACM Transactions on Graphics (TOG), vol. 38, no. 6, 2019.\n[5] Chuan Wen, Yujie Pan, Jie Chang, Ya Zhang, Siheng Chen, Yanfeng Wang, Mei Han, and Qi Tian, \u201cHandwritten chinese font generation with collaborative stroke refinement,\u201d in Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2021.\n[6] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio, \u201cGenerative adversarial nets,\u201d in Advances in neural information processing systems, 2014.\n[7] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros, \u201cImage-to-image translation with conditional adversarial networks,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017.\n[8] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros, \u201cUnpaired image-to-image translation using cycle-consistent adversarial networks,\u201d in Proceedings of the IEEE international conference on computer vision, 2017.\n[9] Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, and Jaegul Choo, \u201cStargan: Unified generative adversarial networks for multi-domain image-to-image translation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018.\n[10] Junho Kim, Minjae Kim, Hyeonwoo Kang, and Kwang Hee Lee, \u201cU-gat-it: Unsupervised generative attentional networks with adaptive layer-instance normalization for image-to-image translation,\u201d in International Conference on Learning Representations, 2020.\n[11] Yexun Zhang, Ya Zhang, and Wenbin Cai, \u201cSeparating style and content for generalized style transfer,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 8447\u20138455.\n[12] Yangchen Xie, Xinyuan Chen, Li Sun, and Yue Lu, \u201cDgfont: Deformable generative networks for unsupervised\nfont generation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 5130\u20135140.\n[13] Shaozu Yuan, Ruixue Liu, Meng Chen, Baoyang Chen, Zhijie Qiu, and Xiaodong He, \u201cLearning to compose stylistic calligraphy artwork with emotions,\u201d in Proceedings of the 29th ACM International Conference on Multimedia, 2021. [14] Bo Chang, Qiong Zhang, Shenyi Pan, and Lili Meng, \u201cGenerating handwritten chinese characters using cyclegan,\u201d in 2018 IEEE Winter Conference on Applications of Computer Vision (WACV). IEEE, 2018, pp. 199\u2013207. [15] Jinshan Zeng, Qi Chen, and Mingwen Wang, \u201cDiversity regularized stargan for multi-style fonts generation of chinese characters,\u201d in Journal of Physics: Conference Series. IOP Publishing, 2021, vol. 1880, p. 012017. [16] Yiming Gao and Jiangqin Wu, \u201cGan-based unpaired chinese character image translation via skeleton transformation and stroke rendering,\u201d in Proceedings of the AAAI Conference on Artificial Intelligence, 2020. [17] Yue Jiang, Zhouhui Lian, Yingmin Tang, and Jianguo Xiao, \u201cScfont: Structure-guided chinese font generation via deep stacked networks,\u201d in Proceedings of the AAAI Conference on Artificial Intelligence, 2019, vol. 33. [18] Jinshan Zeng, Qi Chen, Yunxin Liu, Mingwen Wang, and Yuan Yao, \u201cStrokegan: Reducing mode collapse in chinese font generation via stroke encoding,\u201d arXiv preprint arXiv:2012.08687, 2020. [19] TY Zhang and Ching Y. Suen, \u201cA fast parallel algorithm for thinning digital patterns,\u201d Communications of the ACM, vol. 27, no. 3, pp. 236\u2013239, 1984. [20] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba, \u201cLearning deep features for discriminative localization,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016. [21] Jungbeom Lee, Eunji Kim, Sungmin Lee, Jangho Lee, and Sungroh Yoon, \u201cFicklenet: Weakly and semisupervised semantic image segmentation using stochastic inference,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019. [22] Aditya Chattopadhay, Anirban Sarkar, Prantik Howlader, and Vineeth N Balasubramanian, \u201cGradcam++: Generalized gradient-based visual explanations for deep convolutional networks,\u201d in 2018 IEEE Winter Conference on Applications of Computer Vision (WACV). IEEE, 2018. [23] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter, \u201cGans trained by a two time-scale update rule converge to a local nash equilibrium,\u201d arXiv preprint arXiv:1706.08500, 2017."
        }
    ],
    "title": "SE-GAN: SKELETON ENHANCED GAN-BASED MODEL FOR BRUSH HANDWRITING FONT GENERATION",
    "year": 2022
}