{
    "abstractText": "Abstract The coronavirus disease (COVID-19) is an infectious disease caused by the SARSCoV-2 virus. COVID-19 is found to be the most infectious disease in last few decades. This disease has infected millions of people worldwide. The inadequate availability and the limited sensitivity of the testing kits have motivated the clinicians and the scientist to use Computer Tomography (CT) scans to screen COVID-19. Recent advances in technology and the availability of deep learning approaches have proved to be very promising in detecting COVID-19 with increased accuracy. However, deep learning approaches require a huge labeled training dataset, and the current availability of benchmark COVID-19 data is still small. For the limited training data scenario, the CNN usually overfits after several iterations. Hence, in this work, we have investigated different pre-trained network architectures with transfer learning for COVID-19 detection that can work even on a small medical imaging dataset. Various variants of the pre-trained ResNet model, namely ResNet18, ResNet50, and ResNet101, are investigated in the current paper for the detection of COVID-19. The experimental results reveal that transfer learned ResNet50 model outperformed other models by achieving a recall of 98.80% and an F1-score of 98.41%. To further improvise the results, the activations from different layers of best performing model are also explored for the detection using the support vector machine, logistic regression andK-nearest neighbor classifiers. Moreover, a classifier fusion strategy is also proposed that fuses the predictions from the different classifiers via majority voting. Experimental results reveal that via using learned image features and classification fusion strategy, the recall, and F1-score have improvised to 99.20% and 99.40%.",
    "authors": [
        {
            "affiliations": [],
            "name": "Taranjit Kaur"
        },
        {
            "affiliations": [],
            "name": "Tapan Kumar Gandhi"
        }
    ],
    "id": "SP:9612e95a74f4d979d4eb6ecff1579292ef508402",
    "references": [
        {
            "authors": [
                "A. Bernheim",
                "X. Mei",
                "M. Huang",
                "Y. Yang",
                "Z.A. Fayad",
                "N. Zhang",
                "K. Diao",
                "B. Lin",
                "X. Zhu",
                "K. Li et al.",
                "Chest CT findings in coronavirus disease-19 (COVID-19"
            ],
            "title": "relationship to duration of infection",
            "venue": "Radiology 66, 200463 (2020) Circuits, Systems, and Signal Processing",
            "year": 2022
        },
        {
            "authors": [
                "C.M. Bishop"
            ],
            "title": "Pattern Recognition and Machine Learning",
            "year": 2006
        },
        {
            "authors": [
                "J. Chen",
                "L. Wu",
                "J. Zhang",
                "L. Zhang",
                "D. Gong",
                "Y. Zhao",
                "S. Hu",
                "Y. Wang",
                "X. Hu",
                "B. Zheng et al.",
                "Deep learning-based model for detecting 2019 novel coronavirus pneumonia on high-resolution computed tomography"
            ],
            "title": "a prospective study",
            "venue": "Sci. Rep. 10(1), 1\u201311",
            "year": 2020
        },
        {
            "authors": [
                "T. Cover",
                "P. Hart"
            ],
            "title": "Nearest neighbor pattern classification",
            "venue": "IEEE Trans. Inf. Theory 13(1),",
            "year": 1967
        },
        {
            "authors": [
                "S. Fouladi",
                "M.J. Ebadi",
                "A.A. Safaei",
                "M.Y. Bajuri",
                "A. Ahmadian",
                "Efficient deep neural networks for classification of COVID-19 based on CT images"
            ],
            "title": "virtualization via software defined radio",
            "venue": "Comput. Commun. 176, 234\u2013248",
            "year": 2021
        },
        {
            "authors": [
                "T. Goel",
                "R. Murugan",
                "S. Mirjalili",
                "D.K. Chakrabartty"
            ],
            "title": "Automatic screening of covid-19 using an optimized generative adversarial network",
            "venue": "Cognit. Comput",
            "year": 2021
        },
        {
            "authors": [
                "O. Gozes",
                "M. Frid-Adar",
                "H. Greenspan",
                "P.D. Browning",
                "H. Zhang",
                "W. Ji",
                "A. Bernheim",
                "E. Siegel"
            ],
            "title": "Rapid ai development cycle for the coronavirus (covid-19) pandemic: initial results for automated detection & patient monitoring using deep learning ct image analysis (2020)",
            "venue": "arXiv Prepr arXiv:200305037",
            "year": 2003
        },
        {
            "authors": [
                "H.P. Graf",
                "E. Cosatto",
                "L. Bottou",
                "I. Dourdanovic",
                "V. Vapnik"
            ],
            "title": "Parallel support vector machines: the cascade SVM, in Advances in Neural Information",
            "venue": "Processing Systems",
            "year": 2005
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "year": 2016
        },
        {
            "authors": [
                "X. He",
                "X. Yang",
                "S. Zhang",
                "J. Zhao",
                "Y. Zhang",
                "E. Xing",
                "P. Xie"
            ],
            "title": "Sample-efficient deep learning for COVID-19 diagnosis based on CT scans",
            "venue": "medRxiv 66,",
            "year": 2020
        },
        {
            "authors": [
                "C. Huang",
                "Y.Wang",
                "X. Li",
                "L. Ren",
                "J. Zhao",
                "Y. Hu",
                "L. Zhang",
                "G. Fan",
                "J. Xu",
                "X. Gu"
            ],
            "title": "Clinical features of patients infectedwith 2019 novel coronavirus inWuhan, China",
            "venue": "Lancet 395(10223),",
            "year": 2020
        },
        {
            "authors": [
                "T. Kaur",
                "T.K. Gandhi"
            ],
            "title": "Automated diagnosis of COVID-19 from CT scans based on concatenation of Mobilenetv2 and ResNet50 features, in CVIP (No",
            "year": 2020
        },
        {
            "authors": [
                "T. Kaur",
                "T.K. Gandhi"
            ],
            "title": "Deep convolutional neural networks with transfer learning for automated brain image classification",
            "venue": "Mach. Vis. Appl. 31(3),",
            "year": 2020
        },
        {
            "authors": [
                "T. Kaur",
                "T.K. Gandhi",
                "B.K. Panigrahi"
            ],
            "title": "Automated diagnosis of COVID-19 using deep features and parameter free BAT optimization",
            "venue": "IEEE J. Transl. Eng. Heal. Med",
            "year": 2021
        },
        {
            "authors": [
                "H.J. Koo",
                "S. Lim",
                "J. Choe",
                "S.-H. Choi",
                "H. Sung",
                "K.-H. Do"
            ],
            "title": "Radiographic and CT features of viral pneumonia",
            "venue": "Radiographics 38(3),",
            "year": 2018
        },
        {
            "authors": [
                "R. Kundu",
                "H. Basak",
                "P.K. Singh",
                "A. Ahmadian",
                "M. Ferrara",
                "R. Sarkar"
            ],
            "title": "Fuzzy rank-based fusion of CNN models using Gompertz function for screening COVID-19 CT-scans",
            "year": 2021
        },
        {
            "authors": [
                "D.Y. Liu",
                "H.L. Chen",
                "B. Yang",
                "X.E. Lv",
                "L.N. Li",
                "J. Liu"
            ],
            "title": "Design of an enhanced fuzzy k-nearest neighbor classifier based computer aided diagnostic system for thyroid disease",
            "venue": "J. Med. Syst",
            "year": 2012
        },
        {
            "authors": [
                "M. Loey",
                "F. Smarandache",
                "N.E.M. Khalifa"
            ],
            "title": "A deep transfer learning model with classical data augmentation and CGAN to detect COVID-19 from chest CT radiography digital images",
            "venue": "Neural Comput. Appl. 66,",
            "year": 2020
        },
        {
            "authors": [
                "H. Mohammad-Rahimi",
                "M. Nadimi",
                "A. Ghalyanchi-Langeroudi",
                "M. Taheri",
                "S. Ghafouri-Fard",
                "Application of machine learning in diagnosis of COVID-19 through X-ray",
                "CT images"
            ],
            "title": "a scoping review",
            "venue": "Front. Cardiovasc. Med. 8, 185",
            "year": 2021
        },
        {
            "authors": [
                "M. Nour",
                "Z. C\u00f6mert",
                "K. Polat"
            ],
            "title": "A novel medical diagnosis model for COVID-19 infection detection based on deep features and Bayesian optimization",
            "venue": "Appl. Soft. Comput",
            "year": 2020
        },
        {
            "authors": [
                "H. Panwar",
                "P.K. Gupta",
                "M.K. Siddiqui",
                "R. Morales-Menendez",
                "P. Bhardwaj",
                "V. Singh"
            ],
            "title": "A deep learning and grad-CAM based color visualization approach for fast detection of COVID-19 cases using chest X-ray and CT-Scan images",
            "venue": "Chaos Solitons Fract. 140,",
            "year": 2020
        },
        {
            "authors": [
                "Y. Pathak",
                "P.K. Shukla",
                "K.V. Arya"
            ],
            "title": "Deep bidirectional classification model for COVID-19 disease infected patients",
            "venue": "IEEE/ACM Trans. Comput. Biol. Bioinf. 6,",
            "year": 2020
        },
        {
            "authors": [
                "O.A.B. Penatti",
                "K. Nogueira",
                "J.A. Dos Santos"
            ],
            "title": "Do deep features generalize from everyday objects to remote sensing and aerial scenes domains",
            "venue": "Proceedings of the IEEEConference on Computer Vision and Pattern Recognition Workshops",
            "year": 2015
        },
        {
            "authors": [
                "A. Sayg\u0131l\u0131"
            ],
            "title": "A new approach for computer-aided detection of coronavirus (COVID-19) from CT and X-ray images using machine learning methods",
            "venue": "Appl. Soft Comput. 105,",
            "year": 2021
        },
        {
            "authors": [
                "S. Sen",
                "S. Saha",
                "S. Chatterjee",
                "S. Mirjalili",
                "R. Sarkar"
            ],
            "title": "A bi-stage feature selection approach for COVID19 prediction using chest",
            "venue": "CT images. Appl. Intell",
            "year": 2021
        },
        {
            "authors": [
                "P. Silva",
                "E. Luz",
                "G. Silva",
                "G.Moreira",
                "R. Silva",
                "D. Lucio",
                "D.Menotti",
                "COVID-19 detection inCT images with deep learning"
            ],
            "title": "a voting-based scheme and cross-datasets analysis",
            "venue": "Inform. Med. Unlocked 66, 100427",
            "year": 2020
        },
        {
            "authors": [
                "E. Soares",
                "P. Angelov",
                "S. Biaso",
                "M.H. Froes",
                "D.K. Abe",
                "SARS-CoV-2 CT-scan dataset"
            ],
            "title": "a large dataset of real patients CT scans for SARS-CoV-2 identification",
            "venue": "medRxiv 66, 1\u20138",
            "year": 2020
        },
        {
            "authors": [
                "Y. Song",
                "S. Zheng",
                "L. Li",
                "X. Zhang",
                "Z. Huang",
                "J. Chen",
                "H. Zhao",
                "Y. Jie",
                "R. Wang"
            ],
            "title": "Deep learning enables accurate diagnosis of novel coronavirus (COVID-19) with CT images",
            "venue": "medRxiv 6,",
            "year": 2020
        },
        {
            "authors": [
                "Z. Tan",
                "X. Li",
                "M. Gao",
                "L. Jiang"
            ],
            "title": "The environmental story during the COVID-19 lockdown: how human activities affect PM2.5 concentration in China",
            "venue": "IEEE Geosci. Remote. Sens. Lett. 6,",
            "year": 2020
        },
        {
            "authors": [
                "T.P. Velavan",
                "C.G. Meyer"
            ],
            "title": "The COVID-19 epidemic",
            "venue": "Trop. Med. Int. Heal. 25(3),",
            "year": 2020
        },
        {
            "authors": [
                "S. Wang",
                "B. Kang",
                "J. Ma",
                "X. Zeng",
                "M. Xiao",
                "J. Guo",
                "M. Cai",
                "J. Yang",
                "Y. Li",
                "X. Meng"
            ],
            "title": "A deep learning algorithm using CT images to screen for corona virus disease (COVID-19)",
            "venue": "Eur. Radiol",
            "year": 2021
        },
        {
            "authors": [
                "J. Yosinski",
                "J. Clune",
                "Y. Bengio",
                "H. Lipson"
            ],
            "title": "How transferable are features in deep neural networks",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2014
        },
        {
            "authors": [
                "J. Zhao",
                "Y. Zhang",
                "X. He",
                "P. Xie"
            ],
            "title": "COVID-CT-Dataset: a CT scan dataset about COVID-19 (2020). arXiv Prepr arXiv:200313865 Publisher\u2019s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations",
            "year": 2003
        }
    ],
    "sections": [
        {
            "text": "Keywords CT images \u00b7 COVID-19 \u00b7 Diagnosis \u00b7 Transfer learning \u00b7 Activations \u00b7 Classifier fusion\nB Taranjit Kaur Taranjit.Kaur@ee.iitd.ac.in\nTapan Kumar Gandhi tgandhi@ee.iitd.ac.in\n1 Department of Electrical Engineering, Indian Institute of Technology, Delhi (IIT Delhi), Hauz Khas, New Delhi 110016, India"
        },
        {
            "heading": "1 Introduction",
            "text": "The emergence of COVID-19 infection has critically affected the social and economic structures of both the developing and the developed countries since December 2019 [12]. Researchers and health care workers around the globe are trying to apprehend the COVID-19 etiology and its effect on the quality of life [30].\nNowadays, Computer Tomography (CT) scans are emerging as an alternative for screening in contrast to the conventional reverse transcription-polymerase chain reaction (RT-PCR). This is owing to the limitations associated with the availability, reproducibility, and significant false-negative outcomes produced by RT-PCR kits.\nDistinctive CT scans having patchy ground-glass opacities are important biomarkers that can aid in speedy detection and isolation of the subject [1, 16, 31].\nAs the cases are increasing at an alarming rate, an automated detection system for COVID-19 is the need of the hour that can assist in faster virus detection at different stages thereby relieving the healthcare professionals from the manual annotation task.\nSeveral artificial intelligence (AI) based methods have evolved that automatically provide a diagnosis whether a CT image is COVID +ve or not [3, 4, 8, 19\u201321, 29, 32, 34].\nSoares et al. [28] build a publically available CT scan data for severe acute respiratory syndrome coronavirus 2 (SARS-COV-2) comprising a total of 2482 images. The authors proposed a non-iterative algorithm named eXplainable deep network (xDNN) based on recursive calculation for the categorization of data as non-COVID and COVID+ve. The authors attained a promising F1 value of 97.31% over the validation data.\nNour et al. [21] developed a diagnosis model for SARS-COV-2 infection detection using deep discriminative features and a Bayesian algorithm. The authors used a five convolutional layered model as a deep feature extractor; thereafter, the features were fed to the standard K-nearest neighbor (KNN), support vector machine (SVM), and decision trees whose hyperparameters were fine-tuned with Bayesian optimization procedure. The authors validated their method on the publically available X-ray image dataset and concluded that the SVM algorithm provided the best prediction resulting in an accuracy of 98.97% under the data partitioning ratio of 7:3.\nHe et al. [11] introduced a dataset of CT scan images with hundreds of COVID+ve images freely available for research. The authors also developed the Self-Transmethod integrating self-supervised contrastive learning with the transfer learning-basedmechanism for the separation of infected scans from normal ones. The researchers attained an F1-value of 0.85 using a split proportion of 0.6, 0.25, and 0.15 for training, testing, and validation.\nSayg\u0131l\u0131 [25] proposed an automated system for separating COVID +ve CT scans from the non-infected scan images. The authors followed the pipeline of data set acquisition, image pre-processing that includes rgb2gray transformation, image resizing & image sharpening, Feature extraction that includes Local Binary Patterns & histograms of the Oriented Gradients, Feature reduction using Principal Component Analysis, and classification using standard machine learning (ML) algorithms. They achieved an accuracy of 98.11% on the dataset provided by Soares et al. [28] using the handcrafted features with a tenfold data partitioning scheme.\nKaur andGandhi [13] designed amethod for COVID detection based upon concatenation of deep features from the learned ResNet50 and learned MobileNetv2 model. The deep features were deduced by taking activations from the \u2018avg_pool\u2019 and the \u2018Conv1\u2019 layer of the learned models. Thereafter, the features were concatenated and given as input to the SVM for classification. The feature fusion approach, although with the high dimensionality of 63,232 yielded a validation accuracy of 98.35% on the benchmark COVID CT dataset.\nIn another work via Kaur et al. [15], they developed a diagnosis scheme for COVID-19 signature detection using deep features and Parameter Free-BAT (PFBAT) enhanced Fuzzy-KNN (FKNN). Firstly, the pre-trained MobileNetv2 model was fine-tuned on CT chest radiographs. Thereafter, the features were extracted by performing activations onto the fully connected layer of the fine-tuned model. The features along with the corresponding label were fed to the FKNN classifier whose hyperparameters, i.e., nearest neighbor (\u2018k\u2019) and fuzzy strength measure (\u2018m\u2019) were fine-tuned via PF-BAT. Experimenting on the dataset by Soares et al. [28] reveals that the system achieved an average accuracy of 99.18%.\nGoel et al. [7] designed an automated method for SARS-COV-2 detection using a framework that employs Generative Adversarial Network (GAN) for augmentation, Whale optimization for hyperparameter tuning of GAN network, and classification using transfer learned Inception V3 model. The researchers achieved a prediction accuracy value of 99.22% on the benchmark CT scan dataset using train test splits as 7:3.\nSen et al. [26] developed a feature selection approach for COVID-19 signature detection from lung CT scans. The detection framework uses a Convolutional Neural Network (CNN) architecture as a deep feature extractor. Thereafter, feature selection was done in two stages, i.e., firstly filter-based method is used then the Dragonfly optimization algorithm was applied over the ranked features. The selected features were then used by SVM for classification. The prediction rate was 90% and 98.39% on the benchmark CT scan image datasets.\nSurveying the recent literature divulges that the use of CT images is evolving rapidly for COVID-19 detection owing to the shortage and the limited sensitivity of the RT-PCR detection kits [17]. Furthermore, to automate and expedite the screening of distinctive CT scans, the concept of transfer learning has proven to be quite advantageous, especially if the size of the dataset is limited [13, 15]. Additionally, to generalize the performance of the different algorithms on the benchmark CT scan dataset, a public database was provided by Soares et al. [28]. The survey also highlights that the predicted accuracy over this dataset is still limited, and there is a scope to improvise this rate further via effective mathematical models.\nWith the inspiration to improvise the prediction results, variants of the ResNet model are explored for detection. ResNet model variants are selected because of their proven better performance for the SARS-COV-2 detection task using CT scans [19].\nAdditionally, the potential of the learned image features from the transfer learned model is also investigated for COVID-19 detection when combined with the establishedML algorithms (SVM. KNN, Logistic Regression (LR)). A classification fusion\nmechanism is also proposed that combines the predictions from the differentML algorithms via majority voting. Summarizing the vital contributions of the proposed work are:\n1. A comparative study is performed with various architectures of Deep CNN\u2019s (DCNN) like ResNet18, ResNet50, and ResNet101 for COVID-19 detection using the transfer learning concept. 2. Results revealed that the transfer learning-based deep ResNet50 model exhibited the best performance w.r.t the other residual network variants. 3. Thepotential of the image attributes extracted from thevarious layers of the transfer learned model is also investigated by combining them with well-established ML algorithms like SVM, KNN, and LR. 4. A classification fusion scheme is also proposed that combines the predictions from the different classifiers via majority voting to further boost the classification performance.\nIn this presentwork, Sect. 2 provides thematerials andmethods, followed bySect. 3, which describes the experimental results. Discussion and conclusions are illustrated in Sects. 4 and 5, respectively."
        },
        {
            "heading": "2 Materials andMethods",
            "text": ""
        },
        {
            "heading": "2.1 COVID Dataset Depiction",
            "text": "The proposed method has been analyzed on the publically available benchmark COVID CT scan dataset provided by Soares et al. [28]. The statistics relating to the data are provided in Table 1. Sample CT scans are shown in Fig. 1."
        },
        {
            "heading": "2.2 PerformanceMeasures",
            "text": "In the present paper, Accuracy, Recall, Precision, F-score, and Area under the Curve (AUC) are selected to quantitatively validate the competence of the designed method."
        },
        {
            "heading": "2.3 Transfer Learning",
            "text": "One of the effective mechanisms making use of pre-trained deep models for image classification under limited data scenarios is transfer learning. In the present work, variants of a pre-trained ResNet model are used [11]. ResNet models follow a forward neural network architecture with \u201cshortcut connections\u201d to train CNN [10, 24].\nThrough these \u201cshortcut connections\u201d gradients can easily propagate, which makes the training faster. Primarily, ResNet18 [24], ResNet50 [10], and ResNet101 [10] with transfer learning are used in this work as they have exhibited better performance than other computational models [19]. Only the last three layers of the model variants are fine-tuned to accommodate new image categories, as shown in Fig. 2 [33]."
        },
        {
            "heading": "2.4 Feature Classification Using Conventional Machine Learning Algorithms",
            "text": "Apart from using the transfer learned model for classification, the potential of learned image features is also explored for COVID-19 detection. The network activations from different layers of the learned model are taken, and then, they are fed to the proven state-of-the-art classifiers like SVM, KNN, and the LR. The following section briefly outlines the mathematical formulation for SVM, KNN, and LR."
        },
        {
            "heading": "2.4.1 Support Vector Machine (SVM)",
            "text": "SVM is among themost popular supervisedML algorithms that function by constructing an optimal hyperplane [9]. Let M training examples (pi, zi) be represented in an N-dimensional sample space, where pi is an example pattern and zi \u2208 {\u22121, 1} is the label. Let the kernel value matrix be represented by K and \u03b1i be the Lagrange coefficients to be calculated via optimization procedure. Solving the quadratic equation given below results in an optimal separating hyperplane\nmax W (\u03b1) = \u22121 2\nM\u2211\ni\nM\u2211\nj\n\u03b1i\u03b1 j zi z j K (pi ; p j ) + M\u2211\ni\n\u03b1i (1)\nThe equation given above is subject to the constrain 0 \u2264 \u03b1i \u2264 C,\u2200i and\u2211Mi \u03b1i zi = 0."
        },
        {
            "heading": "2.4.2 K-Nearest Neighbor (KNN)",
            "text": "KNN is one of the most simplistic nonparametric pattern recognition technique [5, 18]. In the KNN algorithm, a label is assigned according to the most common labels from its k-nearest neighbors. The main advantages of the KNN classifier are its simple implementation and fewer parameters to tune, i.e., distance metric and k.\nStep by Step procedure for KNN algorithm:\n(i) Initialize the number of nearest neighbors (k). (ii) Compute the distance between the test image and all the training images. Any\ndistance criteria could be used. E.g. Euclidean distance is primarily used and it is governed by the equation\nDistance(a, b) = \u2016a \u2212 b\u2016 (2)\nwhere (a, b) are two different samples in the feature space. (iii) Sort the distances and calculate the nearest neighbor based on the kth minimum distance. (iv) Get the corresponding labels of the training data which falls under k for the sorted condition. (v) Take the majority of k-nearest neighbors as the output label."
        },
        {
            "heading": "2.4.3 Logistic Regression (LR)",
            "text": "LR method uses a sigmoid or a logistic function. Considering a 2 class problem with f 0(X), f 1(X) as the class conditional densities, q0(X), q1(X) as posterior probabilities, and p0(X), p1(X) as prior probabilities, then according to Bayes rule [2]\nq0(X) = f0(X)p0 f0(X)p0 + f1(X)p1 =\n1\n1 + exp(\u2212\u03be) (3)\nwhere \u03be is defined as\n\u03be = \u2212 ln (\nf1(X)p1 f0(X)p0\n) = ln ( f0(X)p0 f1(X)p1 ) (4)\nAlternatively,\nln ( f0(X)p0 f1(X)p1 ) = WTX + w0 (5)\nThe above equation holds if f 0 and f 1 are Gaussian with similar covariance matrix. This is the case of logistic regression where it would result in an optimal classifier. In logistic regression, the goal is to find W and w0 that minimizes 1 2 \u2211n i=1 ( h ( WT Xi + w0 ) \u2212 yi )2\nwhere h(a) = (1 + exp(\u2212a))\u22121 is the sigmoid or logistic function and yi \u2208 {0, 1} are the targets."
        },
        {
            "heading": "3 Experimental Results",
            "text": ""
        },
        {
            "heading": "3.1 Experimental Setup",
            "text": "The experimental settings for the proposed work include using an \u2018Adam\u201d optimizer with the initial learning rate of 0.0006 to minimize the cross-entropy loss. The variant models were fitted for 20-epochs with 150 as the batch size. The image data were processedonan IntelCore i7-4500UCPUhaving8GBRAM,with a1.8GHzprocessor in a MATLAB 19a platform. Other training options include shuffling the data before each epoch using L2-regularizer with a weight decay value of 0.05 to circumvent overfitting. Moreover, the data splits, i.e., train/validation were used as per the base paper by Soares et al. [28]. The researchers in Soares et al. [28] have given the .mat files for train-validation splits that do not contain subject-related information, i.e., without any metadata on the images. The images are only designated by numbers without any indication of whether they are subject-independent or not."
        },
        {
            "heading": "3.2 Results",
            "text": "The experimental results via deploying the learned ResNet model variants for the COVID-19 detection task are given in Table 2. The tabulated entries show that the best\nresults are attainedby theResNet50model. It has achieved aprecisionof 98.02%, recall of 98.80%, AUC of 0.9994, F1-score of 98.41%, and a validation accuracy of 98.35%. Other models that are taken for comparison, i.e., ResNet18 and ResNet101 achieved a validation accuracy of 97.12% and 96.71%, respectively. The confusion matrix for all the three learned variants is given in Fig. 4. The smallest misclassification error is achieved by ResNet50 followed by ResNet18, and then ResNet101. The accuracy/loss versus epoch plot for the best performing model is given in Fig. 5, which indicates that validation closely follows the training. The AUC plot for ResNet50 is shown in Fig. 6, which specifies that the model attains a value of 0.9994 for True Positive Rate Vs False Positive fraction.\nFigure 7 shows the occlusion sensitivity maps for the learned ResNet50 model. It gives us an idea about which area of the scan is most decisive for classification, i.e., occluding which results in a maximum drop in the probability score. The regions that are positively contributing to the probability score are shown in red color.\nIn the present work, we also tried to investigate the efficacy of the activations from the several layers of the transfer learned ResNet50 model for the COVID-19 detection task. These activations from the various layers of the trained model are used in conjunction with well-established classifiers such as SVM, KNN, and LR.\nBeginning from the fully connected layer, activations from specific layers are taken and the detection results are reported in Table 3. Interestingly, e.g., the activations extracted from the layer \u2018res5b_branch2c\u2019 proved to be decisive for the classification using SVM and the LR classifier. It rendered a value of 99.20% for precision, recall, F1-score, and a value of 99.18%, 0.9997 for accuracy, and AUC.\nTo further improve the classification performance, a fusion strategy is also proposed as outlined in Fig. 8, where predictions from the three different classifiers are fused according to the majority voting rule. The confusion matrix resulting from the classification fusion is shown in Fig. 4d indicating that the misclassifications are reduced to merely three samples.\nOn fusing the predictions from SVM, KNN, and LR using features from the \u2018res5b_branch2c\u2019 layer, a validation accuracy of 99.38% and F1-score of 99.40% is achieved. The advantage of classification fusion is also highlighted for the activations from \u2018avg_pool\u2019 layer where a validation accuracy of 99.38% is achieved using a feature dimension of 2048 rather than using a feature space of 100,352."
        },
        {
            "heading": "4 Discussion",
            "text": "The proposed fusion approach has also been compared with other model architectures that have used the same dataset. As apparent from Table 4, the proposed prediction fusion mechanism yields a precision of 99.60%, F-score of 99.40%, Recall of 99.20%, Accuracy of 99.38% that is higher than the existing network architectures (detection accuracy of 97.38%, 98.35%, 98.39%, 98.37%, 98.99%, 94.04%, 92% reported in Kaur and Gandhi [13], Panwar et al. [22], Pathak et al. [23], Silva et al. [27], Soares et al. [28], Fouladi et al. [6], Sen et al. [26]). In contrast to the reported works in literature, the proposed prediction fusion mechanism offers some benefits in addition to achieving a promising level of detection performance: (1) It employs a single pretrained network architecture for COVID-19 classification differing from the usage of both VGG-16 and xDNN proposed in Soares et al. [28]. (2) It does not employ any optimization procedure for fine-tuning the model hyperparameters as in Kaur et al. [15], Pathak et al. [23] where the authors have employed PF-BAT and Memetic Adaptive Differential Evolution optimization for FKNN and deep bidirectional long short-term memory network with a Mixture Density model (DBM) hyperparameter tuning.\nTa bl e 3 Pe rf or m an ce\nm et ri cs\nfo r tr an sf er\nle ar ne d R es N et 50\nus in g ac tiv\nat io ns\nfr om\nsp ec ifi c la ye rs\nL ay er\nC la ss ifi er\nPr ec is io n (%\n) R ec al l( % )\nA cc ur ac y (%\n) F1\n-s co re\n(% )\nA U C\nD im\nen si on of tr ai ni ng fe at ur e sp ac e\nD im\nen si on of va lid at io n fe at ur e sp ac e\n\u2018f c\u2019\nSV M\n98 .0 2\n98 .8 0\n98 .3 5\n98 .4 1\n0. 99\n32 19\n86 \u00d7\n2 48\n6 \u00d7\n2\nK N N\n97 .2 4\n98 .8 0\n97 .9 4\n98 .0 2\n0. 97 92\nL R\n98 .0 2\n98 .8 0\n98 .3 5\n98 .4 1\n0. 99\n94\nC la ss ifi er\nfu si on\n98 .0 2\n98 .8 0\n98 .3 5\n98 .4 1\n0. 99\n94\n\u2018a vg\n_p oo\nl\u2019 SV\nM 98\n.4 1\n99 .2 0\n98 .7 7\n98 .8 0\n0. 99\n95 19\n86 \u00d7\n20 48\n48 6\n\u00d7 20\n48\nK N N\n98 .8 0\n99 .2 0\n98 .9 7\n99 .0 0\n0. 98 96\nL R\n99 .2 0\n99 .2 0\n99 .1 8\n99 .2 0\n0. 99\n93\nC la ss ifi er\nfu si on\n99 .6 0\n99 .2 0\n99 .3 8\n99 .4 0\n0. 99\n97\n\u2018r es 5c _b\nra nc h2\nc\u2019 SV\nM 98\n.0 2\n98 .8 0\n98 .3 5\n98 .4 1\n0. 99\n75 19\n86 \u00d7\n10 0, 35\n2 48\n6 \u00d7\n10 0, 35\n2\nK N N\n99 .5 9\n97 .6 0\n98 .5 6\n98 .5 9\n0. 98 59\nL R\n98 .4 1\n99 .2 0\n98 .7 7\n98 .8 0\n0. 99\n93\nC la ss ifi er\nfu si on\n98 .4 1\n99 .2 0\n98 .7 7\n98 .8 0\n0. 99 93\n\u2018r es 5c _b\nra nc h2\nb\u2019 SV\nM 98\n.4 2\n99 .6 0\n98 .9 7\n99 .0 1\n0. 99\n97 19\n86 \u00d7\n25 ,0 88\n48 6\n\u00d7 25\n,0 88\nK N N\n98 .8 1\n99 .6 0\n99 .1 8\n99 .2 0\n0. 99 16\nL R\n99 .2 0\n99 .2 0\n99 .1 8\n99 .2 0\n0. 99\n95\nC la ss ifi er\nfu si on\n99 .6 0\n99 .2 0\n99 .3 8\n99 .4 0\n0. 99\n97\n\u2018r es 5c _b\nra nc h2\na\u2019 SV\nM 98\n.4 1\n98 .8 0\n98 .5 6\n98 .6 0\n0. 99\n97 19\n86 \u00d7\n25 ,0 88\n48 6\n\u00d7 25\n,0 88\nK N N\n98 .0 2\n99 .2 0\n98 .5 6\n98 .6 1\n0. 98 54\nTa bl e 3 (c on\ntin ue d)\nL ay er\nC la ss ifi er\nPr ec is io n (%\n) R ec al l( % )\nA cc ur ac y (%\n) F1\n-s co re\n(% )\nA U C\nD im\nen si on of tr ai ni ng fe at ur e sp ac e\nD im\nen si on of va lid at io n fe at ur e sp ac e\nL R\n98 .4 1\n99 .2 0\n98 .7 7\n98 .8 0\n0. 99\n97\nC la ss ifi er\nfu si on\n98 .4 1\n99 .2 0\n98 .7 7\n98 .8 0\n0. 99\n97\n\u2018r es 5b\n_b ra nc h2\nc\u2019 SV\nM 99\n.2 0\n99 .2 0\n99 .1 8\n99 .2 0\n0. 99\n97 19\n86 \u00d7\n10 0, 35\n2 48\n6 \u00d7\n10 0, 35\n2\nK N N\n98 .0 2\n99 .2 0\n98 .5 6\n98 .6 1\n0. 98 54\nL R\n99 .6 0\n98 .8 0\n99 .1 8\n99 .2 0\n0. 99\n97\nC la ss ifi er\nfu si on\n99 .6 0\n99 .2 0\n99 .3 8\n99 .4 0\n0. 99\n97\n\u2018r es 5b\n_b ra nc h2\nb\u2019 SV\nM 98\n.4 1\n99 .2 0\n98 .7 7\n98 .8 0\n0. 99\n97 19\n86 \u00d7\n25 ,0 88\n48 6\n\u00d7 25\n,0 88\nK N N\n97 .6 6\n10 0\n98 .7 7\n98 .8 1\n0. 98 73\nL R\n97 .6 4\n99 .2 0\n98 .3 5\n98 .4 1\n0. 99\n96\nC la ss ifi er\nfu si on\n97 .6 4\n99 .2 0\n98 .3 5\n98 .4 1\n0. 99 96\n\u2018r es 5b\n_b ra nc h2\na\u2019 SV\nM 98\n.0 2\n99 .2 0\n98 .5 6\n98 .6 1\n0. 99\n97 19\n86 \u00d7\n25 ,0 88\n48 6\n\u00d7 25\n,0 88\nK N N\n99 .2 0\n99 .2 0\n99 .1 8\n99 .2 0\n0. 99 97\nL R\n98 .4 1\n99 .2 0\n98 .7 7\n98 .8 0\n0. 99\n97\nC la ss ifi er\nfu si on\n98 .4 1\n99 .2 0\n98 .7 7\n98 .8 0\n0. 99 97\n\u2018r es 5a _b\nra nc h1\n\u2019 SV\nM 98\n.0 2\n99 .2 0\n98 .5 6\n98 .6 1\n0. 99\n94 19\n86 \u00d7\n10 0, 35\n2 48\n6 \u00d7\n10 0, 35\n2\nK N N\n97 .2 7\n99 .6 0\n98 .3 5\n98 .4 2\n0. 98 32\nL R\n96 .8 8\n99 .2 0\n97 .9 4\n98 .0 2\n0. 99\n97\nC la ss ifi er\nfu si on\n98 .0 2\n99 .2 0\n98 .5 6\n98 .6 1\n0. 99\n94\n\u2018r es 5a _b\nra nc h2\nc\u2019 SV\nM 98\n.0 2\n99 .2 0\n98 .5 6\n98 .6 1\n0. 99\n96 19\n86 \u00d7\n10 0, 35\n2 48\n6 \u00d7\n10 0, 35\n2\nK N N\n98 .8 0\n99 .2 0\n98 .9 7\n99 .0 0\n0. 98 96\nL R\n98 .4 1\n99 .2 0\n98 .7 7\n98 .8 0\n0. 99\n97\nTa bl e 3 (c on\ntin ue d)\nL ay er\nC la ss ifi er\nPr ec is io n (%\n) R ec al l( % )\nA cc ur ac y (%\n) F1\n-s co re\n(% )\nA U C\nD im\nen si on of tr ai ni ng fe at ur e sp ac e\nD im\nen si on of va lid at io n fe at ur e sp ac e\nC la ss ifi er\nfu si on\n98 .4 1\n99 .2 0\n98 .7 7\n98 .8 0\n0. 99 97\n\u2018r es 5a _b\nra nc h2\nb\u2019 SV\nM 98\n.8 0\n99 .2 0\n98 .9 7\n99 .0 0\n0. 99\n94 19\n86 \u00d7\n25 ,0 88\n48 6\n\u00d7 25\n,0 88\nK N N\n97 .6 5\n99 .6 0\n98 .5 6\n98 .6 1\n0. 98 53\nL R\n98 .8 0\n98 .8 0\n98 .7 7\n98 .8 0\n0. 99\n92\nC la ss ifi er\nfu si on\n98 .8 0\n99 .2 0\n98 .9 7\n99 .0 0\n0. 98\n96\n\u2018r es 5a _b\nra nc h2\na\u2019 SV\nM 98\n.8 0\n99 .2 0\n98 .9 7\n99 .0 0\n0. 99\n94 19\n86 \u00d7\n25 ,0 88\n48 6\n\u00d7 25\n,0 88\nK N N\n96 .8 8\n99 .2 0\n97 .9 4\n98 .0 2\n0. 97 91\nL R\n99 .1 9\n98 .4 0\n98 .7 7\n98 .8 0\n0. 99\n94\nC la ss ifi er\nfu si on\n99 .2 0\n99 .2 0\n99 .1 8\n99 .2 0\n0. 99\n97\n\u2018r es 4f _b\nra nc h2\nc\u2019 SV\nM 98\n.8 0\n98 .8 0\n98 .7 7\n98 .8 0\n0. 99\n91 19\n86 \u00d7\n20 0, 70\n4 48\n6 \u00d7\n20 0, 70\n4\nK N N\n93 .8 0\n90 .8 0\n92 .1 8\n92 .2 8\n0. 92 22\nL R\n98 .4 1\n98 .8 0\n98 .5 6\n98 .6 0\n0. 99\n92\nC la ss ifi er\nfu si on\n98 .8 0\n98 .8 0\n98 .7 7\n98 .8 0\n0. 99\n91\n\u2018r es 4f _b\nra nc h2\nb\u2019 SV\nM 98\n.0 2\n98 .8 0\n98 .3 5\n98 .4 1\n0. 99\n89 19\n86 \u00d7\n50 ,1 76\n48 6\n\u00d7 50\n,1 76\nK N N\n92 .2 8\n95 .6 0\n93 .6 2\n93 .9 1\n0. 93 56\nL R\n98 .4 0\n98 .4 0\n98 .3 5\n98 .4 0\n0. 99\n88\nC la ss ifi er\nfu si on\n98 .4 0\n98 .4 0\n98 .3 5\n98 .4 0\n0. 99\n88\n\u2018r es 4f _b\nra nc h2\na\u2019 SV\nM 94\n.3 0\n99 .2 0\n96 .5 0\n96 .6 9\n0. 99\n77 19\n86 \u00d7\n50 ,1 76\n48 6\n\u00d7 50\n,1 76\nK N N\n84 .9 6\n90 .4 0\n86 .8 3\n87 .6 0\n0. 86 73\nL R\n94 .6 4\n98 .8 0\n96 .5 0\n96 .6 7\n0. 99\n74\nC la ss ifi er\nfu si on\n88 .5 5\n87 .2 2\n86 .8 3\n87 .8 8\n0. 86 69\nB ol d si gn ifi es\nth e be st re su lts\nTable 4 (continued)\nMethod Accuracy (%) Precision (%) Recall (%) F1 Score (%)\nAUC\nU-Net++ [6] 92 \u2013 100 \u2013 \u2013\nProposed 98.35 98.02 98.80 98.41 0.9994\nProposed (with features from \u2018res5b branch2c\u2019) 99.18 99.20 99.20 99.20 0.9997\nProposed (with classification fusion)\n99.38 99.60 99.20 99.40 0.9997"
        },
        {
            "heading": "5 Conclusion",
            "text": "In the present paper, we have investigated the efficacy of different pre-trained network architectures with transfer learning for COVID-19 detection using a limited CT scan dataset. Investigation reveals that the transfer learned ResNet50model turned out to be the finest by achieving an accuracy value of 98.35% that is superior to the considered models and the existing state-of-the-art works in the literature. Moreover, the potential of the activations from different layers of the learned ResNet50 network is also explored for detection using the established ML algorithms. The exploration reveals that the activations from some of the specific layers of the learned ResNet50 model are quite decisive for classification yielding an accuracy of 99.18% using SVM and LR classifiers. A classification fusion strategy is also proposed that further improvised the accuracy to 99.38% by combining the predictions from the different classifiers via majority voting.\nThe proposed automated system can assist the healthcare professionals in rapid detection of the virus at different stages.\nFunding This research did not receive any specific grant from funding agencies in the public, commercial, or not-for-profit sectors.\nData Availability The datasets generated during and/or analyzed during the current study are available in the repository available at the following link: https://www.kaggle.com/plameneduardo/sarscov2-ctscandataset.\nDeclarations\nConflict of interest The authors declare that they have no competing interests."
        }
    ],
    "title": "Classifier Fusion for Detection of COVID-19 from CT Scans",
    "year": 2021
}