{
    "abstractText": "The increasing use of deep learning techniques has reduced interpretation time and, ideally, reduced interpreter bias by automatically deriving geological maps from digital outcrop models. However, accurate validation of these automated mapping approaches is a significant challenge due to the subjective nature of geological mapping and the difficulty in collecting quantitative validation data. Additionally, many state-of-the-art deep learning methods are limited to 2D image data, which is insufficient for 3D digital outcrops, such as hyperclouds. To address these challenges, we present Tinto, a multi-sensor benchmark digital outcrop dataset designed to facilitate the development and validation of deep learning approaches for geological mapping, especially for non-structured 3D data like point clouds. Tinto comprises two complementary sets: 1) a real digital outcrop model from Corta Atalaya (Spain), with spectral attributes and ground-truth data, and 2) a synthetic twin that uses latent features in the original datasets to reconstruct realistic spectral data (including sensor noise and processing artifacts) from the ground-truth. The point cloud is dense and contains 3,242,964 labeled points. We used these datasets to explore the abilities of different deep learning approaches for automated geological mapping. By making Tinto publicly available, we hope to foster the development and adaptation of new deep learning tools for 3D applications in Earth sciences. The dataset can be accessed through this link: https://doi.org/10.14278/rodare.2256.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ahmed J. Afifi"
        },
        {
            "affiliations": [],
            "name": "Samuel T. Thiele"
        },
        {
            "affiliations": [],
            "name": "Aldino Rizaldy"
        },
        {
            "affiliations": [],
            "name": "Sandra Lorenz"
        },
        {
            "affiliations": [],
            "name": "Pedram Ghamisi"
        },
        {
            "affiliations": [],
            "name": "Raimon Tolosana-Delgado"
        },
        {
            "affiliations": [],
            "name": "Moritz Kirsch"
        },
        {
            "affiliations": [],
            "name": "Richard Gloaguen"
        },
        {
            "affiliations": [],
            "name": "Michael Heizmann"
        }
    ],
    "id": "SP:713c6802d58653e16072c8e9576d4a822ca3ee71",
    "references": [
        {
            "authors": [
                "M. Schmitt",
                "P. Ghamisi",
                "N. Yokoya",
                "R. H\u00e4nsch"
            ],
            "title": "Eod: the ieee grss earth observation database",
            "venue": "IGARSS 2022-2022 IEEE International Geoscience and Remote Sensing Symposium. IEEE, 2022, pp. 5365\u2013 5368.",
            "year": 2022
        },
        {
            "authors": [
                "O. Ghorbanzadeh",
                "Y. Xu",
                "P. Ghamisi",
                "M. Kopp",
                "D. Kreil"
            ],
            "title": "Landslide4sense: Reference benchmark data and deep learning models for landslide detection",
            "venue": "IEEE Transactions on Geoscience and Remote Sensing, vol. 60, pp. 1\u201317, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "J.A. Hurt",
                "G.J. Scott",
                "D.T. Anderson",
                "C.H. Davis"
            ],
            "title": "Benchmark meta-dataset of high-resolution remote sensing imagery for training robust deep learning models in machine-assisted visual analytics",
            "venue": "2018 IEEE Applied Imagery Pattern Recognition Workshop (AIPR). IEEE, 2018, pp. 1\u20139.",
            "year": 2018
        },
        {
            "authors": [
                "A.X. Chang",
                "T. Funkhouser",
                "L. Guibas",
                "P. Hanrahan",
                "Q. Huang",
                "Z. Li",
                "S. Savarese",
                "M. Savva",
                "S. Song",
                "H. Su"
            ],
            "title": "Shapenet: An information-rich 3d model repository",
            "venue": "arXiv preprint arXiv:1512.03012, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "G. Sumbul",
                "M. Charfuelan",
                "B. Demir",
                "V. Markl"
            ],
            "title": "Bigearthnet: A large-scale benchmark archive for remote sensing image understanding",
            "venue": "IGARSS 2019- 2019 IEEE International Geoscience and Remote Sensing Symposium. IEEE, 2019, pp. 5901\u20135904.",
            "year": 2019
        },
        {
            "authors": [
                "P. Ghamisi",
                "K.R. Shahi",
                "P. Duan",
                "B. Rasti",
                "S. Lorenz",
                "R. Booysen",
                "S. Thiele",
                "I.C. Contreras",
                "M. Kirsch",
                "R. Gloaguen"
            ],
            "title": "The potential of machine learning for a more responsible sourcing of critical raw materials",
            "venue": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, vol. 14, pp. 8971\u20138988, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "S. Lorenz",
                "S.T. Thiele",
                "M. Kirsch",
                "G. Unger",
                "R. Zimmermann",
                "P. Guarnieri",
                "N. Baker",
                "E.V. S\u00f8rensen",
                "D. Rosa",
                "R. Gloaguen"
            ],
            "title": "Three-dimensional, km-scale hyperspectral data of well-exposed zn\u2013pb mineralization at black angel mountain, greenland",
            "venue": "Data, vol. 7, no. 8, p. 104, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "S. Li",
                "W. Song",
                "L. Fang",
                "Y. Chen",
                "P. Ghamisi",
                "J.A. Benediktsson"
            ],
            "title": "Deep learning for hyperspectral image classification: An overview",
            "venue": "IEEE Transactions on Geoscience and Remote Sensing, vol. 57, no. 9, pp. 6690\u20136709, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "M. Ibrahim",
                "N. Akhtar",
                "K. Ullah",
                "A. Mian"
            ],
            "title": "Exploiting structured cnns for semantic segmentation of unstructured point clouds from lidar sensor",
            "venue": "Remote Sensing, vol. 13, no. 18, p. 3621, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "B. Chen",
                "S. Shi",
                "J. Sun",
                "W. Gong",
                "J. Yang",
                "L. Du",
                "K. Guo",
                "B. Wang",
                "B. Chen"
            ],
            "title": "Hyperspectral lidar point cloud segmentation based on geometric and spectral information",
            "venue": "Optics express, vol. 27, no. 17, pp. 24 043\u201324 059, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "C. Laukamp",
                "M. Haest",
                "T. Cudahy"
            ],
            "title": "The rocklea dome 3d mineral mapping test data set",
            "venue": "Earth System Science Data, vol. 13, no. 3, pp. 1371\u20131383, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "S.T. Thiele",
                "S. Lorenz",
                "M. Kirsch",
                "I.C.C. Acosta",
                "L. Tusa",
                "E. Herrmann",
                "R. M\u00f6ckel",
                "R. Gloaguen"
            ],
            "title": "Multi-scale, multi-sensor data integration for automated 3-d geological mapping",
            "venue": "Ore Geology Reviews, vol. 136, p. 104252, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "E. Grilli",
                "F. Menna",
                "F. Remondino"
            ],
            "title": "A review of point clouds segmentation and classification algorithms",
            "venue": "The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences, vol. 42, pp. 339\u2013344, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "Y. Xie",
                "J. Tian",
                "X.X. Zhu"
            ],
            "title": "Linking points with labels in 3d: A review of point cloud semantic segmentation",
            "venue": "IEEE Geoscience and remote sensing magazine, vol. 8, no. 4, pp. 38\u201359, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "T. Rabbani",
                "F. Van Den Heuvel",
                "G. Vosselmann"
            ],
            "title": "Segmentation of point clouds using smoothness constraint",
            "venue": "International archives of photogrammetry, remote sensing and spatial information sciences, vol. 36, no. 5, pp. 248\u2013253, 2006.",
            "year": 2006
        },
        {
            "authors": [
                "E. Castillo",
                "J. Liang",
                "H. Zhao"
            ],
            "title": "Point cloud segmentation and denoising via constrained nonlinear least squares normal estimates",
            "venue": "Innovations for Shape Analysis: Models and Algorithms, pp. 283\u2013299, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "P.J. Besl",
                "R.C. Jain"
            ],
            "title": "Segmentation through variableorder surface fitting",
            "venue": "IEEE Transactions on pattern analysis and machine intelligence, vol. 10, no. 2, pp. 167\u2013192, 1988.",
            "year": 1988
        },
        {
            "authors": [
                "A.-V. Vo",
                "L. Truong-Hong",
                "D.F. Laefer",
                "M. Bertolotto"
            ],
            "title": "Octree-based region growing for point cloud segmentation",
            "venue": "ISPRS Journal of Photogrammetry and Remote Sensing, vol. 104, pp. 88\u2013100, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "A. Nurunnabi",
                "D. Belton",
                "G. West"
            ],
            "title": "Robust segmentation in laser scanning 3d point cloud data",
            "venue": "2012 International Conference on Digital Image Computing Techniques and Applications (DICTA). IEEE, 2012, pp. 1\u20138.",
            "year": 2012
        },
        {
            "authors": [
                "J. Zhang",
                "X. Lin",
                "X. Ning"
            ],
            "title": "Svm-based classification of segmented airborne lidar point clouds in urban areas",
            "venue": "Remote sensing, vol. 5, no. 8, pp. 3749\u20133775, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "N. Chehata",
                "L. Guo",
                "C. Mallet"
            ],
            "title": "Airborne lidar feature selection for urban classification using random forests",
            "venue": "Laserscanning, 2009.",
            "year": 2009
        },
        {
            "authors": [
                "E.H. Lim",
                "D. Suter"
            ],
            "title": "3d terrestrial lidar classifications with super-voxels and multi-scale conditional random fields",
            "venue": "Computer-Aided Design, vol. 41, no. 10, pp. 701\u2013 710, 2009.",
            "year": 2009
        },
        {
            "authors": [
                "Y. Lu",
                "C. Rasmussen"
            ],
            "title": "Simplified markov random fields for efficient semantic labeling of 3d point clouds",
            "venue": "2012 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEE, 2012, pp. 2690\u20132697.",
            "year": 2012
        },
        {
            "authors": [
                "H. Su",
                "S. Maji",
                "E. Kalogerakis",
                "E. Learned-Miller"
            ],
            "title": "Multi-view convolutional neural networks for 3d shape This article has been accepted for publication in IEEE Transactions on Geoscience and Remote Sensing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TGRS.2023.3340293 This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/ 14 recognition",
            "venue": "Proceedings of the IEEE international conference on computer vision, 2015, pp. 945\u2013953.",
            "year": 2015
        },
        {
            "authors": [
                "D. Maturana",
                "S. Scherer"
            ],
            "title": "Voxnet: A 3d convolutional neural network for real-time object recognition",
            "venue": "2015 IEEE/RSJ international conference on intelligent robots and systems (IROS). IEEE, 2015, pp. 922\u2013928.",
            "year": 2015
        },
        {
            "authors": [
                "C.R. Qi",
                "H. Su",
                "K. Mo",
                "L.J. Guibas"
            ],
            "title": "Pointnet: Deep learning on point sets for 3d classification and segmentation",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 652\u2013660.",
            "year": 2017
        },
        {
            "authors": [
                "C.R. Qi",
                "L. Yi",
                "H. Su",
                "L.J. Guibas"
            ],
            "title": "Pointnet++: Deep hierarchical feature learning on point sets in a metric space",
            "venue": "Advances in neural information processing systems, vol. 30, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "Y. Wang",
                "Y. Sun",
                "Z. Liu",
                "S.E. Sarma",
                "M.M. Bronstein",
                "J.M. Solomon"
            ],
            "title": "Dynamic graph cnn for learning on point clouds",
            "venue": "Acm Transactions On Graphics (tog), vol. 38, no. 5, pp. 1\u201312, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Lin",
                "G. Vosselman",
                "M.Y. Yang"
            ],
            "title": "Weakly supervised semantic segmentation of airborne laser scanning point clouds",
            "venue": "ISPRS journal of photogrammetry and remote sensing, vol. 187, pp. 79\u2013100, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "R. Huang",
                "Y. Xu",
                "U. Stilla"
            ],
            "title": "Granet: Global relationaware attentional network for semantic segmentation of als point clouds",
            "venue": "ISPRS Journal of Photogrammetry and Remote Sensing, vol. 177, pp. 1\u201320, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Lin",
                "G. Vosselman",
                "Y. Cao",
                "M.Y. Yang"
            ],
            "title": "Local and global encoder network for semantic segmentation of airborne laser scanning point clouds",
            "venue": "ISPRS journal of photogrammetry and remote sensing, vol. 176, pp. 151\u2013 168, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "W. Wu",
                "Z. Qi",
                "L. Fuxin"
            ],
            "title": "Pointconv: Deep convolutional networks on 3d point clouds",
            "venue": "Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition, 2019, pp. 9621\u20139630.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Li",
                "R. Bu",
                "M. Sun",
                "W. Wu",
                "X. Di",
                "B. Chen"
            ],
            "title": "Pointcnn: Convolution on x-transformed points",
            "venue": "Advances in neural information processing systems, vol. 31, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "G. Cai",
                "Z. Jiang",
                "Z. Wang",
                "S. Huang",
                "K. Chen",
                "X. Ge",
                "Y. Wu"
            ],
            "title": "Spatial aggregation net: Point cloud semantic segmentation based on multi-directional convolution",
            "venue": "Sensors, vol. 19, no. 19, p. 4329, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "X. Ma",
                "C. Qin",
                "H. You",
                "H. Ran",
                "Y. Fu"
            ],
            "title": "Rethinking network design and local geometry in point cloud: A simple residual mlp framework",
            "venue": "arXiv preprint arXiv:2202.07123, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Q. Xu",
                "X. Sun",
                "C.-Y. Wu",
                "P. Wang",
                "U. Neumann"
            ],
            "title": "Grid-gcn for fast and scalable point cloud learning",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 5661\u20135670.",
            "year": 2020
        },
        {
            "authors": [
                "T. Hackel",
                "N. Savinov",
                "L. Ladicky",
                "J.D. Wegner",
                "K. Schindler",
                "M. Pollefeys"
            ],
            "title": "Semantic3d. net: A new large-scale point cloud classification benchmark",
            "venue": "arXiv preprint arXiv:1704.03847, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "A. Quadros",
                "J.P. Underwood",
                "B. Douillard"
            ],
            "title": "An occlusion-aware feature for range images",
            "venue": "2012 IEEE International Conference on Robotics and Automation. IEEE, 2012, pp. 4428\u20134435.",
            "year": 2012
        },
        {
            "authors": [
                "W. Tan",
                "N. Qin",
                "L. Ma",
                "Y. Li",
                "J. Du",
                "G. Cai",
                "K. Yang",
                "J. Li"
            ],
            "title": "Toronto-3D: A large-scale mobile lidar dataset for semantic segmentation of urban roadways",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, 2020, pp. 202\u2013203.",
            "year": 2020
        },
        {
            "authors": [
                "J. Behley",
                "M. Garbade",
                "A. Milioto",
                "J. Quenzel",
                "S. Behnke",
                "C. Stachniss",
                "J. Gall"
            ],
            "title": "SemanticKITTI: A Dataset for Semantic Scene Understanding of LiDAR Sequences",
            "venue": "Proc. of the IEEE/CVF International Conf. on Computer Vision (ICCV), 2019.",
            "year": 2019
        },
        {
            "authors": [
                "J. Sun",
                "Q. Zhang",
                "B. Kailkhura",
                "Z. Yu",
                "C. Xiao",
                "Z.M. Mao"
            ],
            "title": "Benchmarking robustness of 3d point cloud recognition against common corruptions",
            "venue": "arXiv preprint arXiv:2201.12296, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "A. L\u00f3pez",
                "J.M. Jurado",
                "J.R. Jim\u00e9nez-P\u00e9rez",
                "F.R. Feito"
            ],
            "title": "Generation of hyperspectral point clouds: Mapping, compression and rendering",
            "venue": "Computers & Graphics, vol. 106, pp. 267\u2013276, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "M. Brell",
                "K. Segl",
                "L. Guanter",
                "B. Bookhagen"
            ],
            "title": "3d hyperspectral point cloud generation: Fusing airborne laser scanning and hyperspectral imaging sensors for improved object-based information extraction",
            "venue": "ISPRS Journal of Photogrammetry and Remote Sensing, vol. 149, pp. 200\u2013214, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "P. Gader",
                "A. Zare",
                "R. Close",
                "J. Aitken",
                "G. Tuell"
            ],
            "title": "Muufl gulfport hyperspectral and lidar airborne data set",
            "venue": "Univ. Florida, Gainesville, FL, USA, Tech. Rep. REP-2013-570, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "Y. Xu",
                "B. Du",
                "L. Zhang",
                "D. Cerra",
                "M. Pato",
                "E. Carmona",
                "S. Prasad",
                "N. Yokoya",
                "R. H\u00e4nsch",
                "B. Le Saux"
            ],
            "title": "Advanced multi-sensor optical remote sensing for urban land use and land cover classification: Outcome of the 2018 ieee grss data fusion contest",
            "venue": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, vol. 12, no. 6, pp. 1709\u20131724, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "M. Weinmann",
                "M. Weinmann"
            ],
            "title": "Fusion of hyperspectral, multispectral, color and 3d point cloud information for the semantic interpretation of urban environments",
            "venue": "The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences, vol. 42, pp. 1899\u20131906, 2019.",
            "year": 1899
        },
        {
            "authors": [
                "M. Weinmann"
            ],
            "title": "Geospatial computer vision based on multi-modal data\u2014how valuable is shape information for the extraction of semantic information?",
            "venue": "Remote Sensing,",
            "year": 2017
        },
        {
            "authors": [
                "M. Weinmann",
                "P. Maier",
                "J. Florath",
                "U. Weidner"
            ],
            "title": "Investigations on the potential of hyperspectral and sentinel-2 data for land-cover/land-use classification",
            "venue": "ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences, vol. 4, pp. 155\u2013162, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "B. Chen",
                "S. Shi",
                "J. Sun",
                "W. Gong",
                "J. Yang",
                "L. Du",
                "K. Guo",
                "B. Wang",
                "B. Chen"
            ],
            "title": "Hyperspectral lidar point cloud segmentation based on geometric and spectral information",
            "venue": "Optics express, vol. 27, no. 17, pp. 24 043\u201324 059, 2019. This article has been accepted for publication in IEEE Transactions on Geoscience and Remote Sensing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TGRS.2023.3340293 This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/ 15",
            "year": 2019
        },
        {
            "authors": [
                "L. Weidner",
                "G. Walton",
                "A. Krajnovich"
            ],
            "title": "Classifying rock slope materials in photogrammetric point clouds using robust color and geometric features",
            "venue": "ISPRS Journal of Photogrammetry and Remote Sensing, vol. 176, pp. 15\u201329, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "K.T. Decker",
                "B.J. Borghetti"
            ],
            "title": "Composite style pixel and point convolution-based deep fusion neural network architecture for the semantic segmentation of hyperspectral and lidar data",
            "venue": "Remote Sensing, vol. 14, no. 9, p. 2113, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "K.T. Decker",
                "B.J. Borghetti"
            ],
            "title": "Hyperspectral point cloud projection for the semantic segmentation of multimodal hyperspectral and lidar data with point convolution-based deep fusion neural networks",
            "venue": "Applied Sciences, vol. 13, no. 14, p. 8210, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "I. Mitschke",
                "T. Wiemann",
                "F. Igelbrink",
                "J. Hertzberg"
            ],
            "title": "Hyperspectral 3d point cloud segmentation using randlanet",
            "venue": "International Conference on Intelligent Autonomous Systems. Springer, 2022, pp. 301\u2013312.",
            "year": 2022
        },
        {
            "authors": [
                "M. Kirsch",
                "S. Lorenz",
                "S. Thiele",
                "R. Gloaguen"
            ],
            "title": "Characterisation of massive sulphide deposits in the iberian pyrite belt based on the integration of digital outcrops and multi-scale, multi-source hyperspectral data",
            "venue": "2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS. IEEE, 2021, pp. 126\u2013129.",
            "year": 2021
        },
        {
            "authors": [
                "S.T. Thiele",
                "S. Lorenz",
                "M. Kirsch",
                "R. Gloaguen"
            ],
            "title": "A novel and open-source illumination correction for hyperspectral digital outcrop models",
            "venue": "IEEE Transactions on Geoscience and Remote Sensing, vol. 60, pp. 1\u201312, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "A.J. Afifi",
                "S.T. Thiele",
                "S. Lorenz",
                "P. Ghamisi",
                "R. Tolosana-Delgado",
                "M. Kirsch",
                "R. Gloaguen",
                "M. Heizmann"
            ],
            "title": "Tinto: Multisensor Benchmark for 3D Hyperspectral Point Cloud Segmentation in the Geosciences",
            "venue": "Apr. 2023. [Online]. Available: https: //doi.org/10.14278/rodare.2256",
            "year": 2023
        },
        {
            "authors": [
                "M. Sch\u00fctz"
            ],
            "title": "Potree: Rendering large point clouds in web browsers",
            "venue": "Technische Universit\u00e4t Wien, Wiede\u0144, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "J. Aitchison"
            ],
            "title": "The Statistical Analysis of Compositional Data, ser",
            "venue": "Monographs on Statistics and Applied Probability. London (UK): Chapman & Hall Ltd. (Reprinted",
            "year": 2003
        },
        {
            "authors": [
                "R. Kokaly",
                "R. Clark",
                "G. Swayze",
                "K. Livo",
                "T. Hoefen",
                "N. Pearson",
                "R. Wise",
                "W. Benzel",
                "H. Lowers",
                "R. Driscoll"
            ],
            "title": "Usgs spectral library version 7 data: Us geological survey data release",
            "venue": "United States Geological Survey (USGS): Reston, VA, USA, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "M. Oren",
                "S.K. Nayar"
            ],
            "title": "Generalization of lambert\u2019s reflectance model",
            "venue": "Proceedings of the 21st annual conference on Computer graphics and interactive techniques, 1994, pp. 239\u2013246.",
            "year": 1994
        },
        {
            "authors": [
                "M.-C. Popescu",
                "V.E. Balas",
                "L. Perescu-Popescu",
                "N. Mastorakis"
            ],
            "title": "Multilayer perceptron and neural networks",
            "venue": "WSEAS Transactions on Circuits and Systems, vol. 8, no. 7, pp. 579\u2013588, 2009.",
            "year": 2009
        },
        {
            "authors": [
                "A. Boulch"
            ],
            "title": "Convpoint: Continuous convolutions for point cloud processing",
            "venue": "Computers & Graphics\u201d, vol. 88, pp. 24 \u2013 34, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "H. Zhao",
                "L. Jiang",
                "J. Jia",
                "P.H. Torr",
                "V. Koltun"
            ],
            "title": "Point transformer",
            "venue": "Proceedings of the IEEE/CVF international conference on computer vision, 2021, pp. 16 259\u201316 268.",
            "year": 2021
        },
        {
            "authors": [
                "M.-H. Guo",
                "J.-X. Cai",
                "Z.-N. Liu",
                "T.-J. Mu",
                "R.R. Martin",
                "S.-M. Hu"
            ],
            "title": "Pct: Point cloud transformer",
            "venue": "Computational Visual Media, vol. 7, pp. 187\u2013199, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "M. Abadi",
                "P. Barham",
                "J. Chen",
                "Z. Chen",
                "A. Davis",
                "J. Dean",
                "M. Devin",
                "S. Ghemawat",
                "G. Irving",
                "M. Isard"
            ],
            "title": "Tensorflow: a system for large-scale machine learning.",
            "venue": "in Osdi,",
            "year": 2016
        },
        {
            "authors": [
                "X. Glorot",
                "Y. Bengio"
            ],
            "title": "Understanding the difficulty of training deep feedforward neural networks",
            "venue": "Proceedings of the thirteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings, 2010, pp. 249\u2013256.",
            "year": 2010
        },
        {
            "authors": [
                "D.P. Kingma",
                "J. Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "I. Sutskever",
                "J. Martens",
                "G. Dahl",
                "G. Hinton"
            ],
            "title": "On the importance of initialization and momentum in deep learning",
            "venue": "Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28, ser. ICML\u201913. JMLR.org, 2013, p. III\u20131139\u2013III\u20131147.",
            "year": 2013
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014point cloud, hyperspectral, hypercloud, deep learning, point cloud segmentation, synthetic data, digital outcrop, remote sensing.\nI. INTRODUCTION\nTHE need for annotated datasets to train and assessdeep learning models has become essential in numerous advanced fields of research, including remote sensing and Earth observation [1]. Although there are several (ongoing) attempts to promote benchmarking and open science in the remote sensing field by developing exhaustive lists of available datasets [1]1, evaluation servers (e.g., DASE2), cloud services (e.g., Amazon Web Services3, Microsoft\u2019s Planetary Computer4, Radiant Earth\u2019s MLHub5), and benchmark datasets\nAhmed J. Afifi, Samuel T. Thiele, Aldino Rizaldy, Sandra Lorenz, Pedram Ghamisi, Raimon Tolosana-Delgado, Moritz Kirsch, and Richard Gloaguen are with the Helmholtz-Zentrum Dresden-Rossendorf (HZDR), Helmholtz Institute Freiberg for Resource Technology (HIF), 09599 Freiberg, Germany.\nAhmed J. Afifi & Michael Heizmann are with the Institute of Industrial Information Technology (IIIT), Karlsruhe Institute of Technology (KIT), 76187 Karlsruhe, Germany.\nPedram Ghamisi is also with the Institute of Advanced Research in Artificial Intelligence (IARAI), 1030 Vienna, Austria.\n1https://github.com/satellite-image-deep-learning/datasets 2http://dase.grss-ieee.org/ 3https://registry.opendata.aws/ 4https://planetarycomputer.microsoft.com/catalog 5https://www.mlhub.earth/\n(e.g., [2, 3, 4, 5]), novel applications within the fields of geomorphology, and geology remain sparse and unclear. Also, generally applicable reference data in remote sensing and geosciences for evaluating machine learning approaches are still not available in sufficient quantity and quality [6, 7].\nHyperspectral remote sensing has emerged as a powerful tool for detecting subtle spectral differences in mineralogical composition. The ability to apply this technology from a range of platforms, including satellites, airplanes, autonomous vehicles, and tripods, supports geological mapping at various scales. Integrating surface spectral data with topographic data enables the creation of hyperclouds, which are geometrically and radiometrically accurate point cloud representations of the target. Depending on the analysed range of the electromagnetic spectrum, a variety of minerals can be detected and mapped in their original 3D context. The visible and near-infrared (VNIR) and shortwave infrared range (SWIR) are useful for detecting spectral features of alteration minerals, such as oxides and hydroxides. In contrast, the long-wave infrared region (LWIR) allows for the detection of many rock-forming minerals, such as quartz and feldspars. Integrating both information sources enables comprehensive lithological mapping and has numerous geological applications, including greenfield exploration of critical raw materials, geological mapping of open-pit and underground mines, compositional mapping of rock wastes and stockpiles, and soil contamination mapping in post-mining landscapes.\nDespite the vast potential of deep learning for hyperspectral mapping [8], it has found limited applicability for geoscientific applications, primarily due to the challenges associated with validation and the benchmarking of adapted algorithms [6]. Ground truth is difficult to establish for large-scale datasets, due to the limited accessibility of geological outcrops, their inherently complex and heterogeneous mineralogical composition as well as the traditionally rather subjective definition of lithological domains by geological experts based on mostly visual criteria. While simulated datasets have been used in the past, they do not yet capture the complexity and variability of real-world geological environments. The rather novel concept of 3D digital spectral outcrops (e.g., hyperclouds) is causing further complications due to the unstructured and complex nature of spectral point cloud data, which is incompatible with most state-of-the-art algorithms. Current approaches partially solve these issues by simplifying the dataset either spatially (working in the 2D image space [9]) or spectrally (segmentation based on selected features [10]) before applying segmentation in 2D or 3D, respectively. For both ways, the\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n2 potential of the dataset is not fully exploited and important information might be lost in the process. Geological scenes are preferred over highly structured scenes for benchmarking deep learning models in geosciences due to their complexity, variability, and real-world relevance. Geological scenes exhibit various features like rock formations, mineralogy, and topography, creating a realistic and challenging testing environment for deep learning algorithms. Class boundaries are diffused in the geological scenes and the classes are highly mixed. Benchmarking geological scenes enhance the models\u2019 generalization capability and learning new patterns and features from the training data, allowing them to adapt to new unseen geological scenarios and making them more robust and reliable in real-world applications. Additionally, it enables researchers to understand deep learning models\u2019 performance in capturing geological information and aligns with the needs of geoscientific applications. Overall, benchmarking on geological scenes provides a robust evaluation environment and facilitates tailored deep learning solutions in geosciences. Lately, few 3D hyperspectral datasets with geoscientific application context have been openly published (e.g., [7, 11]). Nevertheless, none of them offers an adequate amount of ground truth data to qualify as a benchmark dataset.\nIn this contribution, we present a large and geologically complex but well-understood real-world benchmark dataset, and a synthetic (reconstructed) equivalent, designed for testing and comparing deep learning methods for hyperspectral geological mapping. The real-world dataset covers Corta Atalaya, an abandoned open pit mine within the Minas de Rio Tinto copper mining district in Andalusia, Spain. The hyperspectral data have been acquired using plane, drone, and tripod-based acquisition and cover the VNIR, SWIR, and LWIR range of the electromagnetic spectrum. Lithology class labels have been defined for the whole dataset based on a combination of detailed laboratory analysis and derived supervised classification [12] and were adjusted based on an expert interpretation of the geology. However, due to the complex nature of geological datasets, this labeling cannot be treated with 100% confidence. To address this shortcoming, we used the realworld benchmark dataset to derive a realistic synthetic dataset in which class labels (and associated spectral endmembers and abundances) are known with certainty. This approach allows us to develop data for which the class and abundance properties are known with confidence while retaining the spatial statistical properties and complexity of a real dataset.\nTo facilitate established and emerging deep learning approaches, we present these datasets both in 2D raster form (as is conventional for remote sensing applications) and 3D point cloud form (for emerging approaches that are beginning to move beyond the topological limitations imposed by 2D rasters). Challenges and limitations associated with each data representation, and a selection of tools available for working with them, are discussed.\nThe rest of the paper is structured as follows. Section II reviews some related work and the available datasets for 3D point cloud processing. Section III describes the proposed Tinto benchmark dataset and how it is collected, labeled, and synthesised in detail. In Section IV, we discuss the\nbaseline deep learning models that are used to evaluate the Tinto dataset, the experimental setup, and the experimental outcomes. Finally, the conclusion and remarks are drawn in Section V."
        },
        {
            "heading": "II. BACKGROUND AND RELATED WORK",
            "text": "A plethora of sophisticated methodologies has been advanced to segment 3D point clouds, often utilizing publicly available datasets to evaluate their efficacy. These methodologies encompass a broad spectrum of techniques, from traditional methods to cutting-edge machine learning and deep learning approaches [13]. In the following subsections, we provide a very brief overview of the mature field of some diverse point cloud segmentation approaches by highlighting a few relevant examples and presenting a comprehensive collection of 3D datasets commonly employed for this specific task."
        },
        {
            "heading": "A. Point Cloud Segmentation",
            "text": "Traditional point cloud segmentation methods rely on strict hand-crafted geometric constraints and rules. The main goal of the segmentation process is to group 3D points into nonoverlapping regions. The generated regions have common semantic meanings and geometric structures [14]. With the introduction of machine learning and deep learning models in solving 2D tasks and the availability of large-scale labeled datasets, many researchers proposed machine/deep learning models to segment point clouds from the object and scene levels. Generally, deep learning models achieved remarkable performance compared to traditional and machine learning methods. Following, we discuss different point cloud segmentation methods.\n1) Edge-based: Edge-based methods try to detect points close to the edge by calculating the rapid changes in the intensity (the feature associated with the points), normals, or the gradient. This will create boundaries between two different regions. Then, the points are grouped inside the same region where changes are small. These methods perform segmentation quickly but struggle to achieve accurate results when dealing with point clouds from large areas due to issues like noise and uneven point distribution [15, 16].\n2) Region growing: Region growing-based methods involve the random selection of seed points and the measure of geometrical or feature similarity between the seeds and neighboring points. Points with similar features are merged to create one region. This process is performed iteratively until all points are merged into similar regions. These methods were firstly applied to 2.5D LiDAR data and they were widely applied for the segmentation of building structures. Similar points that belong to the same region can be selected by comparing their features or calculating the Euclidean similarity. The segmented points are selected for example by fitting a plane to a number of points in a given volume and then points with the minimum distance to that plane are merged [17, 18, 19].\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n3 3) Shallow supervised Machine Learning: Shallow supervised machine learning refers to non-deep algorithms that use labeled data to train a model. These methods allow classifying points in a cloud based on predefined features such as maximum likelihood based on support vector machine (SVM) [20], random forests (RF) [21], and Bayesian discriminant classifiers [22]. Other groups of methods depend on statistical contextual models such as Conditional Random Fields (CRF) [23] and Markov Random Fields (MRF) [24]. These methods focus on the statistics and the relational information of the points over different scales. Machine learning models applied for point cloud segmentation perform a neighborhood point selection, then feature extraction from the grouped points, feature selection to reduce the feature dimensionality and then segment the points semantically.\n4) Deep Learning: Deep learning has become the most influential and hottest technique in different research fields such as computer vision, medical imaging, autonomous driving, and robotics. Deep learning is a special branch of machine learning where the models are deeper, more complex and the extracted features generally have higher dimensions than the ones extracted from traditional machine learning methods. Applicable methods for applying deep learning on 3D data depend on how the data is represented. With multi-view data, a normal 2D Convolutional Neural Network (CNN) can be easily applied, such as the MVCNN model [25]. Voxel-based data can be used with 3D CNN, where the normal 2D CNN can be easily extended to 3D [26]. The drawback of using voxel-based representation is the memory and the computation cost to train a model. To overcome the voxel-based and multiview methods, models that can be applied directly on point cloud data were recently proposed as a promising solution. Models applied directly on point clouds such as the pioneer model PointNet [27] were followed by the improved version PointNet++ [28] and the Dynamic Graph CNN (DGCNN) [29]. Other approaches of point cloud segmentation in the field of remote sensing and 3D laser scanning can be found in [30, 31, 32].\nThe research on point cloud segmentation using deep learning is a hot research topic. Different models are proposed with either sophisticated layers [33, 34, 35] to deal with the point cloud as an unordered set or with a simple architecture using MLP as a backbone of the model [36] to achieve improved performance with less computation and memory cost."
        },
        {
            "heading": "B. Available 3D Datasets",
            "text": "To our knowledge, no 3D hyperspectral benchmark datasets have been published for a geoscientific application context yet. Available benchmark datasets for point cloud segmentation deal with indoor scenes, such as Stanford Large-Scale 3D Indoor Spaces (S3DIS) [37] and Semantic3D.Net [38], or urban scenes, such as Sydney Urban Objects Dataset [39], Toronto3D [40], and SemanticKITTI [41], for semantic segmentation. Other datasets focus on the objects, for instance segmentation or object parts segmentation, such as ShapeNet [4] and ModelNet40-C [42]. In most cases, point cloud attributes are limited to the 3D coordinates (X, Y, Z), intensity or RGB\ncolor values. The Maarmorilik Dataset [7] is an open-source 3D hyperspectral dataset capturing the complex geology of the Black Angel Mountain in Maarmorilik, West Greenland, alongside a detailed and interactive tutorial documenting relevant processing workflows for hypercloud data. It includes RGB and VNIR-SWIR hyperspectral data but does not provide ground truth and thus cannot be defined as a benchmark.\nIn [43, 44], similar work has been conducted by generating hyperspectral point clouds, but the datasets are not available to the public. The MUUFL Gulfport dataset [45] and GRSS18 dataset [46], on the other hand, provide the ground truth and are publicly open. They consist of LiDAR and hyperspectral data. However, the ground truth is in a 2D format rather than in a 3D point format as offered by our dataset. Some work [47, 48, 49, 50] have also been done investigating the utilization of hyperspectral data with LiDAR for classification task but neglecting the use of deep learning. Moreover, these works [47, 48, 49] performed the classification in a 2D style. The work in [51] investigated the use of smartphone and UAV photogrammetry to assess rock slope hazards in mountainous regions. Different datasets were created according to lighting conditions, slope morphologies, and seasons. They were manually labeled and were classified using Random Forest into geologically relevant categories. The research compares 12 different point cloud feature sets, finding that feature sets focused on geometry, slope, and texture perform significantly better than those incorporating absolute color features, which are sensitive to lighting changes and struggle to distinguish between geological materials. More recent works [52, 53, 54] use deep learning models for hyperspectral point cloud segmentation and proved that hyperspectral data improved the performance of the models. These results motivated us to employ various deep learning models in our work.\nTo the best of the authors\u2019 knowledge, the Tinto dataset will be the first-ever dataset that provides the following features: (1) a 3D point cloud of a real outcrop, (2) the corresponding ground truth, (3) the same scene captured using different sensors (RGB, VNIR, SWIR, and LWIR), (4) hyperspectral information attached to each point in the point cloud, (5) two types of corresponding synthetic data (clean and noisy data), and (6) 2D views of the scene from three different directions."
        },
        {
            "heading": "III. TINTO DATASET",
            "text": ""
        },
        {
            "heading": "A. Data Acquisition and Correction",
            "text": "Several steps of the acquisition and processing of the Corta Atalaya hyperclouds have been previously described by [12, 55]. To summarise briefly, a tripod-mounted hyperspectral Specim AisaFenix camera was used to capture oblique VNIR and SWIR imagery from three locations on the edge of the Corta Atalaya open-pit. Each of these rasters was then backprojected onto a dense 3D point cloud derived from 488 RGB photographs (captured using a Nikon D850 DSLR camera and Nikkor 85 mm f/1.8G lens) using the structure from motion multi-view stereo method implemented in Agisoft Metashape Professional v1.6. Atmospheric effects, which result largely from (i) the spectral signature of sunlight, (ii) interactions between this light and the atmosphere, and (iii) uneven illumination across the complex surface of the Corta Atalaya mine,\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n4 were corrected using the method described by [56] during the back-projection step.\nThe long-wave infrared hyperspectral data was collected in August 2020 during a larger hyperspectral airborne mapping campaign [55]. A Hyper-Cam FTIR hyperspectral camera from Telops was deployed, covering the electromagnetic spectrum between 7.7 and 11.8 micrometers. The collected raw data were processed using a standard workflow: The individual data cubes were orthorectified using a 2.5 m resolution Lidarbased terrain model, acquired within the same campaign, and subsequently stitched to a mosaic (average ground sampling distance of 1.2 m) using Telops\u2019 Reveal Airborne geolocation tool (version 2). Initial radiometric correction to at-sensor radiance was performed using the Telops Reveal Calibrate Software Version 5.2.8. Atmospheric correction was done by an In-Scene Atmospheric Compensation algorithm, while the separation of temperature and emissivity was performed based on emissivity normalization from the radiance data [55]. The resulting calibrated emissivity mosaic was then sampled onto the same dense 3D point cloud that was used with the VNIR-SWIR data to create an LWIR hypercloud to be included in this benchmark dataset. Figure 1 visualizes an overview of the Tinto benchmark dataset with the various datasets it contains [57]. The 3D visualization of the Tinto point clouds on Potree [58] can be accessed through this link: https://www.hzdr.de/FWG/FWGE/Hyperclouds/Tinto.html"
        },
        {
            "heading": "B. Data labelling and synthetic twin",
            "text": "Geological maps, i.e., classifications that show the spatial distribution of rock types, are generally subjective interpretations of the map author due to (i) cover by vegetation\nor soil, (ii) ambiguous rock type definitions, and (iii) the thematic or purpose the author has in mind when creating the map. For example, a map intended to constrain geotechnical aspects of a mine would often differ significantly from one made to quantify the composition and distribution of ore, due to subjective choices made when defining and classifying different rock units. While this ambiguity is an important justification for automated methods offered by e.g., machine learning approaches, which can improve objectivity while simultaneously allowing for data to be reprocessed for various purposes, it presents significant challenges when developing meaningful and reliable benchmarks. In this contribution, we have mitigated these challenges using two radically different approaches: (1) deriving a manually vetted but largely datadriven classification for ground-truthing purposes, and (2) back-calculating a realistic dataset (synthetic twin) from this classification result to derive a hyperspectral dataset for which the original rock composition is known for each pixel. These two approaches are described in the following sections.\n1) Geological ground-truth: In the first approach, which aims at a meaningful ground-truth classification for the real hyperspectral data, we have integrated and synthesised hyperspectral information, sample mineralogy, field mapping and published geological understanding of Corta Atalaya. The spectral classification results of [12] were used as a base for the ground truth and manually corrected where field data, ground sampling and expert interpretation of the highresolution photogrammetric model showed clear mislabelling. These results (Figure 2) were subsequently checked by mine geologists on-site, resulting in a labelled data set that we consider to be as accurate as practically possible for geological applications.\nSeveral classes in this classification are spectrally and geologically related (e.g., classes defined by the presence of different but related alteration minerals; [12]). Lumping these together, we derive a simplified classification containing 6 rock types. While we encourage people to use the full label set, this simplified version could be useful for evaluating approaches that perform poorly with a large number of classes (e.g., unsupervised methods).\nWe have also defined a suggested training subset (Figure 2) to ensure consistent results between studies. This has been selected such that (1) it covers all classes in the dataset, and (2) matches with what could be realistically achieved in practice, with training data distributed along three bench-traverses that are typical for geological mapping in open-pit environments. Note that this geometry results in a highly imbalanced training set, a common challenge for hyperspectral classification problems. Table I and Table II present the number of points per class in the basic and complete ground truth, respectively.\n2) Synthetic twin: Potential issues associated with remaining biases or inconsistencies in the ground-truth labels have been addressed by generating an entirely synthetic suite of spectral data by forward modelling. These share the same labels as the real dataset, as well as several latent variables and spatial relationships, but are derived using a spectral mixing model and a spatial distribution of mineral abundances simulated using spectral proxies and sample measurements for\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n5 (a) Basic (6-lithology) segmentation.\n(b) Complete (10-lithology) segmentation.\nFig. 2: Simplified (top) and complete (bottom) ground truth labels provided for this benchmark dataset. The suggested training subset is outlined in black and follows traverses that match roughly with how a field geologist would collect data.\neach class from [12]. We suggest that these synthetic spectra are suited for comparing learning approaches, as the ground truth is known with certainty, while the real spectra can be used to evaluate performance on realistic data. The procedure followed to generate this synthetic twin is outlined below.\nFirstly, three latent features known to correlate with specific mineral abundances (spectral proxies) were extracted from the real dataset using established minimum wavelength mapping and band-ratio techniques [12]. These were normalised to have a mean of zero and standard deviation of one and assembled into a vector L containing the latent feature at every point, ensuring that spatial associations present in the real dataset (and potentially informative for deep machine learning methods) are preserved in the synthetic one.\nNext, mineral abundances from x-ray diffraction measurements on the ground-truth samples [12] were used to define a mean composition for each class. To ensure the synthetic abundances sum to one, the so called additive log ratio transformation (ALR) [59] was used. As reference phase, an abundant phase was chosen, generally quartz. Sulphide was used for the massive sulphide class. Hence, the ALR transformed abundance \u03b1 of the remaining phases was computed for each point x by\n\u03b1i,j(x) = log\n( A\u0302i,j\nA\u03020,j\n) +\u03c3MTi \u00b7\u039bj(x), i = 1, 2, . . . , n (1)\nwhere A\u0302i,j denotes the average abundance of mineral i \u2208 {0, 1, . . . , n} in class j\u2014A\u03020,j being then the abundance of the reference mineral for class j\u2014, the vector \u039bj(x) contains the values of the three latent variables described previously for class j at location x, and Mi contains a manually defined mapping vector that determines the contribution of each latent variable to the log-abundance of the i-th mineral. Finally, \u03c3 scales the log standard deviation of the mineral abundances within each class, and was kept at a constant value of 0.3 after some experimentation.\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n6\nA vector of closed abundances A(x) was then calculated for each point x by inverting the additive log transform [59]\nAj(x) = exp\n[ 0 \u03b11,j(x) . . . \u03b1n,j(x) ] \u2225 exp [ 0 \u03b11,j(x) . . . \u03b1n,j(x) ] \u22251 , (2)\nwith exp(\u00b7) the component-wise exponential function, resulting in a set of realistic mineral abundance maps (Figure 3). Following the real geology exposed in Corta Atalaya, a degree of endmember variability was then introduced by splitting the abundance of three mineral groups (muscovite, chlorite and clay) into compositional endmembers, based on the position of the 2200, 2250 and 2160 nm absorption features respectively. This extended the number of phases in A from seven to ten, a realistic degree of complexity for geological outcrops. A pure endmember spectral library E assembled using spectra from the USGS [60] was then used to derive a synthetic reflectance spectra S for each data point, assuming linear mixing,\nS = E \u00b7A. (3)\nThese synthetic reflectance spectra, and the mineral abundances used to derive them, are also included in the benchmark dataset, and could be used for testing e.g., endmember identification and unmixing methods.\n3) Degraded twin: In reality, sensor noise and other unwanted effects (e.g., atmospheric and topographic distortions, coating, vegetation) mean that no dataset will contain perfect reflectance spectra. Hence, as a final step, the synthetic reflectance spectra were degraded to simulate realistic measurement, preprocessing and data-correction procedures. First, the reflectance spectra were converted to at-target radiance estimates using the two-light-source atmospheric model described by [56] and the Oren-Nayar BRDF [61]. Simulating the real acquisition procedure, these radiance data were projected onto 2D rasters using three different camera poses, and pathradiance added to the corresponding spectra proportional to the target-sensor distance, resulting in three at-sensor radiance rasters. For the LWIR dataset, light emitted by the target (and by air between the target and the sensor) was also calculated,\nnoting that emissivity = 1 \u2212 reflectance following Kirchhoff\u2019s law, and added to the at-sensor radiance.\nEach raster was then transformed according to the inverse of the sensor-specific lens calibration and converted to digital numbers by dividing by the lab-determined spectral calibration values. Sensor noise was added using dark-current data acquired during the acquisition of the real hyperspectral data, resulting in a set of three simulated raw rasters with realistic noise.\nA degraded synthetic reflectance dataset (Figure 1) was then derived by correcting the simulated raw data using the same routine as was applied to the real data (cf., Section IIIA)."
        },
        {
            "heading": "C. Accompanying 2D data",
            "text": "Although this manuscript focuses on 3-D point cloud data attributed with reflectance spectra to create hyperclouds, it is worth noting that we have included a set of 2-D rasters derived by projecting the class labels, real, synthetic and degraded spectra onto nadir, oblique perspective and oblique panoramic views (Figure 4). These will not be discussed further here, but could serve as a useful benchmark for image segmentation or unmixing methods."
        },
        {
            "heading": "IV. EVALUATION",
            "text": ""
        },
        {
            "heading": "A. Baseline Models",
            "text": "Many deep learning models designed for processing raw point clouds are primarily focused on classification and segmentation tasks. Point coordinates are typically the most common input to the network, and in some cases, normals and RGB values can also be incorporated. Some models proposed sophisticated layers to effectively process point clouds directly [33, 34, 35]. They learn the geometrical features of the points to perform the classification and segmentation tasks. Tinto dataset includes multiple sources of information, but the emphasis is on learning hyperspectral information for point cloud segmentation. To evaluate the Tinto dataset on deep learning models, different deep learning models designed for point cloud processing were selected and trained from\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n7 (a) View1: nadir orthoimage (LWIR: 10114.0, 9181.0, 8545.0 nm.) (b) View2: oblique perspective (VNIR: 850.0, 650.0, 525.0 nm).\n(c) View3: oblique panorama (SWIR: 2200.0, 2250.0, 2350.0 nm).\nFig. 4: False-colour visualisations of the real LWIR (top-left), VNIR (top-right) and SWIR (bottom) hyperspectral datasets from the three viewpoints used to derive the Tinto2D benchmark images.\nscratch. They are categorized into three different architectures according to the main building layers. The first category is the multi-layer perceptron (MLP) based models [62] where the MLPs are the main building layers in the models. The second category is the convolutional-based models where convolutional-like layers are used to process the point clouds. The third category is the Transformers-based models where the Natural Language Processing (NLP) Transformer models are adopted and modified to process raw point clouds for different tasks. Following, we will discuss each category and explain the models related to the category.\n1) MLP-based Models: MLP-based models for point cloud processing refer to architectures that utilize MLPs as their fundamental building blocks for analyzing and extracting information from 3D point cloud data. The first baseline model is a 10-layer MLP. The model was implemented to classify the point cloud according to its hyperspectral information. The output layer is equal to the number of classes. Our network consists of 10 hidden layers between the input and the output layers to extract useful features and help in the classification process. The hidden layers have to extract features of different sizes. The input layer takes the hyperspectral information of the point as input information. The output layer has the same size as the number of classes in the ground-truth. Another MLP-based model is the PointNet model [27]. It is the first pioneering deep learning model for direct point cloud processing. As the points are unordered in the point cloud data, it processes each point in isolation through a shared MLP to extract local features. Specifically, PointNet applies point-wise operations using several MLP layers to extract independent features separately and uses max-pooling operation to capture the global features of the point cloud. The aggregated global feature extracted from the point cloud can be used for various tasks such as classification and segmentation. The drawback of the PointNet model is that features are learned independently and then the global feature is aggregated. So, the local structure of the point cloud between points is not captured. To overcome this limitation, PointNet++ [28] was proposed as a hierarchical network. PointNet++ consists of three main layers: the sampling layer, the grouping layer, and the PointNet-based learning layer. The sampling layer uses the farthest point sampling algorithm to select centroids. The grouping layer uses the selected centroids to find the nearest neighbor points of each centroid. The PointNet layer is then applied on the local region to learn and extract the feature vector. This process is repeated in a hierarchical form and the points\u2019 resolution is reduced as the network goes deeper. In the last layer, the global feature is produced. However, the calculated KNN is not updated as the input goes deeper into the network. The Dynamic Graph CNN (DGCNN) [29]\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n8 creates dynamic graphs that capture the relationships between points. It utilizes EdgeConv layers to extract the characteristics of individual points while employing non-linear MLPs on neighboring points. Subsequently, the edges are iteratively adjusted in subsequent layers based on the input from prior layers, creating a dynamic workflow.\n2) Convolutional-based Models: The success of CNNs for image classification inspired researchers to use a custom convolutional-like operation for point clouds. PointCNN [34] is designed to process the point cloud with the ability to efficiently learn and capture patterns from point clouds without relying on predefined grids or structures. It achieves this by introducing a unique convolution operation called X-Conv that adapts to the local geometry of the points by learning the weights of the input features and then permuting the points into canonical order, making it highly effective for tasks like 3D object classification and segmentation. ConvPoint [63] is another model where a customized continuous convolution operation was introduced to process the unordered point clouds. This operation can be extended easily to build a CNN model to process the point clouds similar to 2D CNNs. The convolution operation separates the kernel into spatial and feature parts. A unit sphere is used to select the location of the spacial parts randomly and the weighted function of the layers is learned by a simple MLP.\nThese proposed approaches and others underscore the adaptability and effectiveness of custom convolutional-like operations in extracting meaningful features from point cloud data, further expanding the horizons of 3D data analysis.\n3) Transformer-based Models: Inspired by the popularity of the transformer models in NLP, researchers proposed different Transformer models to process the point clouds. It is under the assumption that the point cloud format suits the selfattention operator due to invariance to permutation and cardinality. Point Transformer (PT) [64] proposed a transformer model with self-attention layers as a set operator that can process the point clouds for various tasks. The attention layers learn the relationship between selected central points and the corresponding neighboring points. Those attention layers serve as the backbone for the feature encoder block, which gradually downsamples the number of points in each consecutive layer. Point Cloud Transformer (PCT) [65] is permutation invariant and it enhances the input embedding by applying the farthest point sampling for centroids selecting and nearest neighbors calculating. PCT proposed an attention mechanism where the final output features from the attention layer are the offset features that are the difference between the input and the original attention features.\nIn summary, Transformer-based models tailored for point clouds leverage the self-attention mechanism to exploit the unique characteristics of point cloud data, demonstrating promising potential for a wide range of applications in 3D perception and analysis.\nB. Implementation Details We implemented, trained, and evaluated the MLP model on TensorFlow [66]. The weights of the model were initialized using the Xavier initialization method [67]. For the\nremaining models, we used the original implementation codes from the GitHub repositories, all with Adam optimizer [68], except DGCNN and PCT used Stochastic Gradient Descent (SGD) [69] with the momentum of 0.9. The dataset contains 3, 187, 785 points (excluding the Vegetation class). For the complete label scenario, the dataset is split into a training set (297, 968 points) and a testing set (2, 889, 817 points) with the ratio of 10% and 90%, respectively. The inputs were the hyperspectral information of the points. The input size depends on the sensor used to acquire the data (VNIR = 51 bands, SWIR = 141 bands, and LWIR = 126 bands). The learning rate was set to 0.001, except PCT used 0.1. All models used a batch size of 16, except PointNet and PointNet++ used 24 and DGCNN used 32. The models were implemented in PyTorch [70], and only PointCNN was applied in TensorFlow."
        },
        {
            "heading": "C. Experimental Results",
            "text": "We conducted experiments on the testing split of the Tinto dataset to assess the performance of the baseline models. All hyperspectral point clouds of the VNIR, SWIR, and LWIR (the real, clean synthetic, and noisy synthetic) are utilized for the quantitative and qualitative evaluation. The baseline models trained and tested on the Tinto dataset are MLP, PointNet [27], PointNet++ [28], DGCNN [29], PointCNN [34], ConvPoint [63], PT [64], and PCT [65].\nFirstly, we trained the baseline models separately on each sensor data using the training set of real data with complete labels. The trained models were then evaluated on the testing set and their accuracies were computed. Table III (Real Data) reports the overall accuracy of the baseline models on the real data for the complete labels. The results indicated that the PointCNN model achieved the highest accuracy on the LWIR data, the PCT model achieved the highest accuracy on the SWIR data, and the PointNet model achieved the highest accuracy on the VNIR data. Most baseline models were proposed and designed to capture the geometric information from 3D point clouds of shapes and performed well with objects and scenes that can be segmented into grids. These models mainly trained on the coordinates of the points and other information (e.g., normals and RGB values) as input features to extract geometric information from the point cloud. However, we modified the baseline models and trained them on the hyperspectral data only. Their performance is lower on this dataset compared to the models\u2019 performance on other datasets. Moreover, it\u2019s worth mentioning that the accuracy of the labels associated with the real data is not guaranteed to be 100%, which can impact the models\u2019 performance to some extent. This is because of the nature of the dataset as there are no sharp boundaries between the classes as the rocks are highly overlapped in reality.\nTo address the issue of inaccurate ground truth labels, the dataset has a synthetic part where each point in the point cloud has a synthetic hyperspectral feature and is associated with a correct class label. We trained the baseline models on the training set and evaluated them on the testing set of various sensors. Table III (Clean Synthetic Data) reports the overall accuracy of the baseline models on the clean synthetic data\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n9\nfor the complete labels. We found that the performance of all models on the clean synthetic data scored higher accuracies compared to the performance on the real data with a large margin. Interestingly, the majority of models performed similarly on clean synthetic data from different hyperspectral sensors. Our observation is that precise ground truth data can greatly enhance model performance and enable accurate segmentation of the point cloud. Also, Table III shows that the models that consider the neighboring points when extracting the features scored the best accuracies. The DGCNN model, which relies on the local neighboring points to create and update the graph for learning the point features, outperforms other models on all sensors data. Then, it is followed by PCT and PointNet++ models. Both models learn the local features by considering the neighboring points and merge them with the global features computed by the self-attention layer for PCT and the maxpooling layer for PointNet++. Our experiment demonstrated that those models are better suited for our synthetic dataset.\nIn order to increase the realism of the synthetic data and challenge the models further, we added real noise information (sensor noise and processing artifacts) to the synthetic point cloud. We evaluated the baseline models on the noisy synthetic data and found that their accuracy decreased compared to those trained on clean synthetic data. Table III (Noisy Synthetic Data) reports the overall accuracy of the baseline models on the noisy synthetic data for the basic and complete labels. While all models achieved higher performance compared to the models trained on the real data, DGCNN and PointNet++ models are consistently the leading models on the synthetic data. The DGCNN model achieved the highest accuracy on the SWIR and VNIR data while PointNet++ achieved the highest accuracy on the LWIR data.\nIn conclusion, the baseline models proved that they can be adapted to learn hyperspectral information and perform the point cloud segmentation task on the geological data. This opens a new direction of applying deep learning models to generate segmented maps on the geological data using hyperspectral information and propose new models that can fuse information from different sources.\nFigure 5, Figure 6, and Figure 7 showcase the qualitative outcomes of segmented point clouds generated by the trained baseline models using the testing split of the dataset on the LWIR, SWIR, and VNIR data, respectively in different scenarios (real data, clean synthetic data, noisy synthetic data). These illustrations highlight the baseline models\u2019 ability to produce segmented point clouds with a reasonably high degree of accu-\nracy. When applied to clean synthetic data, most models excel in accurately segmenting the point cloud. However, it\u2019s crucial to acknowledge that real-world data typically contains noise, and our models exhibit decreased performance when noise is introduced into the synthetic data. Furthermore, the presence of imbalanced data can lead to misclassifications, particularly in instances where certain classes have a disproportionately smaller number of training samples compared to others. This issue becomes particularly noticeable in the case of the purple shale class, both in real data and noisy synthetic data scenarios."
        },
        {
            "heading": "V. CONCLUSION",
            "text": "This paper introduces the first-ever fully-labeled multisensor hyperspectral benchmark dataset and we expect it to be a valuable resource for researchers working on point cloud segmentation in geosciences. The dataset is comprehensive and diverse, covering real and synthetic data with different levels of labeling for the classes. The Tinto dataset is a suitable benchmark for developing and evaluating point cloud segmentation algorithms for geological applications. We believe that the Tinto dataset will serve as a benchmark for future studies and contribute to the development of innovative solutions for point cloud segmentation in Earth sciences. Interestingly, deep models that leverage information from neighboring points demonstrate superior performance, thanks to their ability to extract both local and global features for point cloud segmentation. The strength of this dataset is that it is versatile and allows testing architecture robustness under different conditions (e.g. noise, data quality). Overall, the Tinto benchmark dataset represents a significant contribution to the field and holds promise for a broad range of applications in Earth sciences such as mineral exploration, geological mapping, and natural resources management."
        },
        {
            "heading": "ACKNOWLEDGMENT",
            "text": "The authors would like to acknowledge extensive support from Atalaya Mining during fieldwork conducted for this publication, and subsequent validation of the results. This research received funding from the Initiative and Networking Fund (INF) of the Hermann von Helmholtz Association of German Research Centres in the framework of the Helmholtz Imaging Platform (HIP) under grant agreement No ZT-I-PF4-021 and from the European Union\u2019s Horizon 2020 research and innovation programme under grant agreement No 776487. The experiments were conducted on the high specification\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n10\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n11\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n12\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n13\nNvidia A100 GPUs server which was provided by the European Regional Development Fund and the Land of Saxony."
        }
    ],
    "title": "Tinto: Multisensor Benchmark for 3D Hyperspectral Point Cloud Segmentation in the Geosciences",
    "year": 2023
}