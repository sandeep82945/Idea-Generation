{
    "abstractText": "Multi-agent Pathfinding (MAPF) problem generally asks to find a set of conflict-free paths for a set of agents confined to a graph and is typically solved in a centralized fashion. Conversely, in this work, we investigate the decentralized MAPF setting, when the central controller that posses all the information on the agents\u2019 locations and goals is absent and the agents have to sequientially decide the actions on their own without having access to a full state of the environment. We focus on the practically important lifelong variant of MAPF, which involves continuously assigning new goals to the agents upon arrival to the previous ones. To address this complex problem, we propose a method that integrates two complementary approaches: planning with heuristic search and reinforcement learning through policy optimization. Planning is utilized to construct and re-plan individual paths. We enhance our planning algorithm with a dedicated technique tailored to avoid congestion and increase the throughput of the system. We employ reinforcement learning to discover the collision avoidance policies that effectively guide the agents along the paths. The policy is implemented as a neural network and is effectively trained without any reward-shaping or external guidance. We evaluate our method on a wide range of setups comparing it to the state-of-the-art solvers. The results show that our method consistently outperforms the learnable competitors, showing higher throughput and better ability to generalize to the maps that were unseen at the training stage. Moreover our solver outperforms a rulebased one in terms of throughput and is an order of magnitude faster than a state-of-the-art search-based solver.",
    "authors": [
        {
            "affiliations": [],
            "name": "Alexey Skrynnik"
        },
        {
            "affiliations": [],
            "name": "Anton Andreychuk"
        },
        {
            "affiliations": [],
            "name": "Maria Nesterova"
        },
        {
            "affiliations": [],
            "name": "Konstantin Yakovlev"
        },
        {
            "affiliations": [],
            "name": "Aleksandr Panov"
        }
    ],
    "id": "SP:7fa49a7c0f08569337c1cd39b371cf42587a974e",
    "references": [
        {
            "authors": [
                "D.S. Bernstein",
                "R. Givan",
                "N. Immerman",
                "S. Zilberstein"
            ],
            "title": "The complexity of decentralized control of Markov decision processes",
            "venue": "Mathematics of operations research, 27(4): 819\u2013840.",
            "year": 2002
        },
        {
            "authors": [
                "Z. Chen",
                "J. Alonso-Mora",
                "X. Bai",
                "D.D. Harabor",
                "P.J. Stuckey"
            ],
            "title": "Integrated task assignment and path planning for capacitated multi-agent pickup and delivery",
            "venue": "IEEE Robotics and Automation Letters, 6(3): 5816\u20135823.",
            "year": 2021
        },
        {
            "authors": [
                "J. Chung",
                "C. Gulcehre",
                "K. Cho",
                "Y. Bengio"
            ],
            "title": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
            "venue": "arXiv preprint arXiv:1412.3555.",
            "year": 2014
        },
        {
            "authors": [
                "M. Damani",
                "Z. Luo",
                "E. Wenzel",
                "G. Sartoretti"
            ],
            "title": "PRIMAL 2: Pathfinding via reinforcement and imitation multi-agent learning-lifelong",
            "venue": "IEEE Robotics and Automation Letters, 6(2): 2666\u20132673.",
            "year": 2021
        },
        {
            "authors": [
                "B. de Wilde",
                "A.W. ter Mors",
                "C. Witteveen"
            ],
            "title": "Push and rotate: cooperative multi-agent path planning",
            "venue": "In Proceedings of the 12th International Conference on Autonomous Agents and Multiagent Systems",
            "year": 2013
        },
        {
            "authors": [
                "D. Hafner",
                "T. Lillicrap",
                "M. Norouzi",
                "J. Ba"
            ],
            "title": "Mastering atari with discrete world models",
            "venue": "ICLR.",
            "year": 2021
        },
        {
            "authors": [
                "D. Hafner",
                "J. Pasukonis",
                "J. Ba",
                "T. Lillicrap"
            ],
            "title": "Mastering diverse domains through world models",
            "venue": "arXiv preprint arXiv:2301.04104.",
            "year": 2023
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, 770\u2013778.",
            "year": 2016
        },
        {
            "authors": [
                "L.P. Kaelbling",
                "M.L. Littman",
                "A.R. Cassandra"
            ],
            "title": "Planning and acting in partially observable stochastic domains",
            "venue": "Artificial Intelligence, 101(1-2): 99\u2013134.",
            "year": 1998
        },
        {
            "authors": [
                "K. Kansky",
                "S. Vaidyanath",
                "S. Swingle",
                "X. Lou",
                "M. LazaroGredilla",
                "D. George"
            ],
            "title": "PushWorld: A benchmark for manipulation planning with tools and movable obstacles",
            "venue": "arXiv:2301.10289.",
            "year": 2023
        },
        {
            "authors": [
                "J. Li",
                "A. Tinka",
                "S. Kiesel",
                "J.W. Durham",
                "T.S. Kumar",
                "S. Koenig"
            ],
            "title": "Lifelong multi-agent path finding in large-scale warehouses",
            "venue": "Proceedings of the 35th AAAI Conference on Artificial Intelligence (AAAI 2021), 11272\u2013 11281.",
            "year": 2021
        },
        {
            "authors": [
                "W. Li",
                "H. Chen",
                "B. Jin",
                "W. Tan",
                "H. Zha",
                "X. Wang"
            ],
            "title": "Multi-Agent Path Finding with Prioritized Communication Learning",
            "venue": "2022 International Conference on Robotics and Automation (ICRA), 10695\u201310701.",
            "year": 2022
        },
        {
            "authors": [
                "M. Liu",
                "C. Amato",
                "E.P. Anesta",
                "J.D. Griffith",
                "J.P. How"
            ],
            "title": "Learning for Decentralized Control of Multiagent Systems in Large, Partially-Observable Stochastic Environments",
            "venue": "AAAI Conference on Artificial Intelligence.",
            "year": 2016
        },
        {
            "authors": [
                "M. Liu",
                "H. Ma",
                "J. Li",
                "S. Koenig"
            ],
            "title": "Task and path planning for multi-agent pickup and delivery",
            "venue": "Proceedings of the 18th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2019), 1152\u20131160.",
            "year": 2019
        },
        {
            "authors": [
                "V.J. Lumelsky",
                "K. Harinarayan"
            ],
            "title": "Decentralized motion planning for multiple mobile robots: The cocktail party model",
            "venue": "Autonomous Robots, 4(1): 121\u2013135.",
            "year": 1997
        },
        {
            "authors": [
                "H. Ma",
                "D. Harabor",
                "P.J. Stuckey",
                "J. Li",
                "S. Koenig"
            ],
            "title": "Searching with consistent prioritization for multiagent path finding",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, 7643\u20137650.",
            "year": 2019
        },
        {
            "authors": [
                "H. Ma",
                "W. H\u00f6nig",
                "T.K.S. Kumar",
                "N. Ayanian",
                "S. Koenig"
            ],
            "title": "Lifelong Path Planning with Kinematic Constraints for Multi-Agent Pickup and Delivery",
            "venue": "Proceedings of the 33rd AAAI Conference on Artificial Intelligence (AAAI 2019), 7651\u20137658.",
            "year": 2019
        },
        {
            "authors": [
                "H. Ma",
                "J. Li",
                "T. Kumar",
                "S. Koenig"
            ],
            "title": "Lifelong Multi-Agent Path Finding for Online Pickup and Delivery Tasks",
            "venue": "Proceedings of the 16th Conference on Autonomous Agents and MultiAgent Systems (AAMAS 2017), 837\u2013845.",
            "year": 2017
        },
        {
            "authors": [
                "Z. Ma",
                "Y. Luo",
                "H. Ma"
            ],
            "title": "Distributed heuristic multi-agent path finding with communication",
            "venue": "2021 IEEE International Conference on Robotics and Automation (ICRA), 8699\u20138705. IEEE.",
            "year": 2021
        },
        {
            "authors": [
                "S. Milani",
                "N. Topin",
                "B. Houghton",
                "W.H. Guss",
                "S.P. Mohanty",
                "O. Vinyals",
                "N.S. Kuno"
            ],
            "title": "The MineRL Competition on Sample-Efficient Reinforcement Learning Using Human Priors: A Retrospective",
            "venue": "NeurIPS Competition track.",
            "year": 2020
        },
        {
            "authors": [
                "V. Mnih",
                "K. Kavukcuoglu",
                "D. Silver",
                "A. a. Rusu",
                "J. Veness",
                "M.G. Bellemare",
                "A. Graves",
                "M. Riedmiller",
                "A.K. Fidjeland",
                "G. Ostrovski",
                "S. Petersen",
                "C. Beattie",
                "A. Sadik",
                "I. Antonoglou",
                "H. King",
                "D. Kumaran",
                "D. Wierstra",
                "S. Legg",
                "D. Hassabis"
            ],
            "title": "Human-level control through",
            "year": 2015
        },
        {
            "authors": [
                "K. Okumura",
                "M. Machida",
                "X. D\u00e9fago",
                "Y. Tamura"
            ],
            "title": "Priority inheritance with backtracking for iterative multi-agent path finding",
            "venue": "Proceedings of the 28th International Joint Conference on Artificial Intelligence (IJCAI 2019), 535\u2013542.",
            "year": 2019
        },
        {
            "authors": [
                "K. Okumura",
                "M. Machida",
                "X. D\u00e9fago",
                "Y. Tamura"
            ],
            "title": "Priority inheritance with backtracking for iterative multi-agent path finding",
            "venue": "Artificial Intelligence, 310: 103752.",
            "year": 2022
        },
        {
            "authors": [
                "T. Rashid",
                "M. Samvelyan",
                "C.S. De Witt",
                "G. Farquhar",
                "J. Foerster",
                "S. Whiteson"
            ],
            "title": "QMIX: Monotonic value function factorisation for deep multi-agent reinforcement Learning",
            "venue": "35th International Conference on Machine Learning, ICML 2018, volume 10, 6846\u20136859. ISBN",
            "year": 2018
        },
        {
            "authors": [
                "B. Riviere",
                "W. H\u00f6nig",
                "Y. Yue",
                "S.-J. Chung"
            ],
            "title": "Glas: Global-to-local safe autonomy synthesis for multi-robot motion planning with end-to-end learning",
            "venue": "IEEE Robotics and Automation Letters, 5(3): 4249\u20134256.",
            "year": 2020
        },
        {
            "authors": [
                "M. Samvelyan",
                "T. Rashid",
                "C.S. De Witt",
                "G. Farquhar",
                "N. Nardelli",
                "T.G. Rudner",
                "C.M. Hung",
                "P.H. Torr",
                "J. Foerster",
                "S. Whiteson"
            ],
            "title": "The StarCraft multi-agent challenge",
            "venue": "Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AA-",
            "year": 2019
        },
        {
            "authors": [
                "G. Sartoretti",
                "J. Kerr",
                "Y. Shi",
                "G. Wagner",
                "T.S. Kumar",
                "S. Koenig",
                "H. Choset"
            ],
            "title": "Primal: Pathfinding via reinforcement and imitation multi-agent learning",
            "venue": "IEEE Robotics and Automation Letters, 4(3): 2378\u20132385.",
            "year": 2019
        },
        {
            "authors": [
                "J. Schulman",
                "F. Wolski",
                "P. Dhariwal",
                "A. Radford",
                "O. Klimov"
            ],
            "title": "Proximal policy optimization algorithms",
            "venue": "arXiv preprint arXiv:1707.06347.",
            "year": 2017
        },
        {
            "authors": [
                "A. Skrynnik",
                "A. Yakovleva",
                "V. Davydov",
                "K. Yakovlev",
                "A.I. Panov"
            ],
            "title": "Hybrid Policy Learning for Multi-Agent Pathfinding",
            "venue": "IEEE Access, 9: 126034\u2013126047.",
            "year": 2021
        },
        {
            "authors": [
                "R. Stern",
                "N.R. Sturtevant",
                "A. Felner",
                "S. Koenig",
                "H. Ma",
                "T.T. Walker",
                "J. Li",
                "D. Atzmon",
                "L. Cohen",
                "T. S Kumar"
            ],
            "title": "Multi-agent pathfinding: Definitions, variants, and benchmarks",
            "venue": "In Proceedings of the 12th Annual Symposium on Combinatorial Search (SoCS",
            "year": 2019
        },
        {
            "authors": [
                "H. Tang",
                "J. Hao",
                "T. Lv",
                "Y. Chen",
                "Z. Zhang",
                "H. Jia",
                "C. Ren",
                "Y. Zheng",
                "C. Fan",
                "L. Wang"
            ],
            "title": "Hierarchical Deep Multiagent Reinforcement Learning",
            "venue": "ArXiv, abs/1809.09332.",
            "year": 2018
        },
        {
            "authors": [
                "J. Van den Berg",
                "M. Lin",
                "D. Manocha"
            ],
            "title": "Reciprocal velocity obstacles for real-time multi-agent navigation",
            "venue": "Proceedings of The 2008 IEEE International Conference on Robotics and Automation (ICRA 2008), 1928\u2013 1935. IEEE.",
            "year": 2008
        },
        {
            "authors": [
                "K.-H.C. Wang",
                "A. Botea"
            ],
            "title": "MAPP: a scalable multi-agent path planning algorithm with tractability and completeness guarantees",
            "venue": "Journal of Artificial Intelligence Research, 42: 55\u201390.",
            "year": 2011
        },
        {
            "authors": [
                "X. Wang",
                "S. Wang",
                "X. Liang",
                "D. Zhao",
                "J. Huang",
                "X. Xu",
                "B. Dai",
                "Q. Miao"
            ],
            "title": "Deep Reinforcement Learning: A Survey",
            "venue": "IEEE transactions on neural networks and learning systems, PP.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Wang",
                "B. Xiang",
                "S. Huang",
                "G. Sartoretti"
            ],
            "title": "SCRIMP: Scalable Communication for Reinforcement-and Imitation-Learning-Based Multi-Agent Pathfinding",
            "venue": "Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems, 2598\u20132600.",
            "year": 2023
        },
        {
            "authors": [
                "A. Wong",
                "T. B\u00e4ck",
                "A.V. Kononova",
                "A. Plaat"
            ],
            "title": "Deep multiagent reinforcement learning: Challenges and directions",
            "venue": "Artificial Intelligence Review, 56(6): 5023\u20135056.",
            "year": 2023
        },
        {
            "authors": [
                "C. Yu",
                "A. Velu",
                "E. Vinitsky",
                "J. Gao",
                "Y. Wang",
                "A. Bayen",
                "Y. Wu"
            ],
            "title": "The surprising effectiveness of ppo in cooperative multi-agent games",
            "venue": "Advances in Neural Information Processing Systems, 35: 24611\u201324624.",
            "year": 2022
        },
        {
            "authors": [
                "H. Zhu",
                "B. Brito",
                "J. Alonso-Mora"
            ],
            "title": "Decentralized probabilistic multi-robot collision avoidance using buffered uncertainty-aware Voronoi cells",
            "venue": "Autonomous Robots, 46(2): 401\u2013420.",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "Introduction",
            "text": "Multi-agent pathfinding (MAPF) (Stern et al. 2019) is a challenging problem that has been getting increasing attention recently. It is often studied in the AI community with the following assumptions. The agents are confined to a graph, and at each timestep, an agent can either move to an adjacent vertex or stay at the current one. A central controller possesses information about the graph and the agents\u2019 start and goal locations. This unit is in charge of constructing a set of conflict-free plans for all the agents. Thus, a typical setting for MAPF can be attributed as centralized and fully observable.\n*Preprint. Under review\nIn many real-world domains, however, it is not possible, from the engineering perspective, to design such a central controller that has a stable connection to all the agents (robots) and obtains a full knowledge of the environment all the time. For example, consider a fleet of service robots delivering some items in a human-shared environment, e.g., the robots delivering medicine in the hospital. Each of these robots is likely to have access to the global map of the environment (e.g., the floor plan), possibly refined through the robot\u2019s sensors. However, the connection to the central controller may not be consistent. Thus, the latter may not have accurate data on the robots\u2019 locations and, consequently, cannot provide valid MAPF solutions. In such scenarios, decentralized approaches to the MAPF problems, when the robots themselves have to decide their future paths based on their local observations, as depicted in Fig. 1, are essential. In this work, we aim to develop such an efficient decentralized approach.\nIt is natural to frame the decentralized Multi-Agent Path Finding (MAPF) problem as a sequential decision-making problem where, at each timestep, each agent must choose and execute an action that will advance it toward its goal while ensuring that other agents can also reach their goals. The result of solving this problem is a policy that, at each moment, specifies which action to execute. To form such\nar X\niv :2\n31 0.\n01 20\n7v 1\n[ cs\n.A I]\n2 O\nct 2\n02 3\na policy, learnable methods are commonly used nowadays, such as reinforcement learning (RL), which proves particularly beneficial in tasks with incomplete information (Mnih et al. 2015; Rashid et al. 2018; Hafner et al. 2021). However, even state-of-the-art RL methods generally struggle with solving long-horizon problems with the involved casual structure (Milani et al. 2020; Hafner et al. 2023), and they are often inferior to the search-based, planning methods when solving problems with hard combinatorial structure (Kansky et al. 2023).\nIndeed, numerous learnable methods tailored to MAPF settings are already known, such as PRIMAL (Sartoretti et al. 2019), PRIMAL2 (Damani et al. 2021), DHC (Ma, Luo, and Ma 2021), PICO (Li et al. 2022), SCRIMP (Wang et al. 2023) to name a few. These methods either rely on the complex training procedures that typically involve manual reward-shaping, external demonstrations etc., or on the communication (data sharing) between the agents. Moreover these methods often do not generalize well, i.e. their performance degrades significantly when they solve problem instances on the maps that are not alike the ones used for training.\nTo this end, the current paper suggests that the MAPF problem should not be solved directly by RL but rather in combination and vivid interaction with the heuristic search algorithm. This idea is put into practice via the following pipeline. Each agent plans an individual path to its goal by a heuristic search algorithm without taking the other agents into account. Moreover, an additional technique is introduced for planning that is dedicated specifically to dispersing the agents over the workspace via penalizing the paths that are likely to cause deadlocks. Upon path construction, a learnable policy is invoked to reach the first waypoint of this path. This policy is obtained through a decentralized training and is aimed at following the path on the one hand, while making necessarily detours to avoid collisions and let the other agents progress toward their goals on the other hand.\nEmpirically, we compare our method, which we name FOLLOWER, to a range of both learnable and non-learnable state-of-the-art competitors and show that it i) consistently outperforms the learnable competitors in terms of solution quality; ii) better generalizes to the unseen environments compared to the other learnable solvers; iii) outperforms a state-of-the-art rule-based centralized solver in terms of solution quality; iv) scales much better to the large numbers of agents in terms of computation time compared to the stateof-the-art search-based centralized solver."
        },
        {
            "heading": "Related Works",
            "text": "Lifelong MAPF LMAPF is an extension of MAPF when the agents are assigned new goals upon reaching their current ones. Similarly, in (online) multi-agent pickup and delivery (MAPD), agents are continuously assigned tasks comprising two locations that the agent has to visit in a strict order: pickup location and delivery location. Typically, the assignment problem is not considered in LMAPF/MAPD. However, some works also consider task assignment, such as (Liu et al. 2019; Chen et al. 2021).\nMa et al. (2017) propose several variants to tackle MAPD differing in the amount of data the agents share. Yet, even the decoupled (as attributed by the authors) algorithms based on Token Swapping rely on global information, i.e., the one provided by the central unit. An enhanced Token Swapping variant that considers kinematic constraints was introduced in (Ma et al. 2019b). In (Okumura et al. 2019) an efficient rule-based re-planning approach to solve MAPF that is naturally capable of solving LMAPF/MAPD problems is introduced \u2013 PIBT. It does not rely on the several restrictive assumptions of Token Swapping and is empirically shown to outperform the latter. We compare with PIBT and demonstrate that our method is better regarding solution quality.\nFinally, one of the most recent and effective LMAPF solvers is the RHCR algorithm presented in (Li et al. 2021). It draws upon the idea of bounded planning, i.e., constructing not a complete plan but rather its initial part. RHCR is a centralized solver that relies on the full knowledge of the agents\u2019 locations, current paths, goals, etc. In this work, we empirically compare with RHCR and show that our method scales much better to many agents in computation time.\nDecentralized MAPF This setting entails that the paths/actions of the agents are not decided by a central unit but by the agents themselves. Numerous approaches, especially the ones tailored to the robotics applications, boil this problem down to reactive control (Lumelsky and Harinarayan 1997; Van den Berg, Lin, and Manocha 2008; Zhu, Brito, and Alonso-Mora 2022). These methods, however, are often prone to deadlocks. Several MAPF algorithms can also be implemented in a decentralized manner. For example, Wang and Botea (2011) introduce MAPP algorithm that relies on individual pathfinding for each agent and a set of rules to determine priorities and choose actions to avoid conflicts when they occur along the paths. In general, most rule-based MAPF solvers, like the previously mentioned PIBT (Okumura et al. 2019), or another seminal MAPF solver Push And Rotate (de Wilde, ter Mors, and Witteveen 2013), can be implemented in such a way that each agent locally decides its actions. However, in this case, the implicit assumption is that the agents can communicate to share relevant information (or that they have access to the global MAPFrelated data). By contrast, our work assumes that the agents cannot reliably communicate with each other or a central unit, which significantly increases the complexity of the problem.\nLearnable MAPF This direction has recently received increased attention. In (Sartoretti et al. 2019), a seminal PRIMAL method was introduced. It utilizes reinforcement learning and imitation learning to solve MAPF in a decentralized fashion. Later in (Damani et al. 2021), it was enhanced and tailored explicitly to LMAPF. The new version was named PRIMAL2. Since numerous learningbased MAPF solvers have emerged, it has become common to compare against PRIMAL/PRIMAL2 (we also compare with it in our work). For example, Riviere et al. (2020) propose another learning-based approach tailored explicitly to agents with a non-trivial dynamic model, such as quadrotors. Ma, Luo, and Ma (2021) describe DHC \u2013 a method\nthat efficiently utilizes the agents\u2019 communications to solve decentralized MAPF. Another communication-based learnable approach, PICO, is presented in (Li et al. 2022) and yet another in the most recent paper by (Wang et al. 2023). Overall, currently, there is a wide range of learnable decentralized MAPF solvers. In this work, we compare our method with the state-of-the-art learnable competitors and show that the former produces better quality solutions and better generalizes to the unseen maps, which is crucial for the learnable solvers.\nMARL and HRL Multi-Agent Reinforcement Learning (MARL) (Wong et al. 2023) is a separate direction in RL that specifically considers the multi-agent setting. Mainly, MARL approaches consider game environments (like Starcraft (Samvelyan et al. 2019)) in which pathfinding is not of primary importance. However, several MARL methods, such as QMIX (Rashid et al. 2018) and MAPPO (Yu et al. 2022), have been adapted specifically for the MAPF task (Skrynnik et al. 2021). However, they rely on information sharing between the agents.\nLearnable low-level policies and heuristic sub-goal allocation procedures are commonplace in many hierarchical RL (HRL) approaches tailored to single-agent problems. However, such techniques are rarely explored in MARL (Wang et al. 2022). Existing studies primarily demonstrate their results within simplistic environments (Tang et al. 2018), leaving ample room for further research. Among these, PoEM (Liu et al. 2016), a method closely related to ours, utilizes preexisting demonstrations to identify sub-goals, which poses a significant limitation. In contrast to our approach, all the methods we know of present their findings using scenarios with a few agents."
        },
        {
            "heading": "Background",
            "text": "Multi-agent Pathfinding In (Classical) Multi-agent pathfinding (Stern et al. 2019), the timeline is discretized to the timesteps and the workspace, where M agents operate, is discretized to a graph G = (V,E), whose vertices correspond to the locations and the edges to the transitions between these locations. M start and goal vertices are given, and each agent i has to reach its goal gi \u2208 V from the start si \u2208 V . At each timestep, an agent can either stay in its current vertex or move to an adjacent one. An individual plan for an agent pi1 is a sequence of actions that transfers it between two designated vertices. The plan\u2019s cost is equal to the number of actions comprising it.\nThe MAPF problem asks to find a set of M plans s.t. each agent reaches the goal without colliding with other agents. Formally, two collisions are typically distinguished: a vertex collision, where the agents occupy the same vertex at the same timestep, and an edge collision, where the agents use the same edge at the same timestep.\nLifelong MAPF (LMAPF) is a variant of MAPF where immediately after an agent reaches its goal, it is assigned to\n1In MAPF literature, a plan is typically denoted with \u03c0. However, in RL, this is reserved to denote the policy. As we use both MAPF and RL approaches in this work, we denote a plan as p.\nanother one (via an external assignment procedure) and has to continue its operation.\nThe Considered Decentralized LMAPF Problem Consider a set of agents operating in the shared environment, represented as a graph G = (V,E). The timeline is discretized to timesteps T = 0, 1, ..., Tmax, where Tmax is the episode length. Each agent is located initially at the start vertex and is assigned to the current goal vertex. If it reaches the latter before the episode ends, it is immediately assigned another goal vertex. We assume that the goal assignment unit is external to the system, and the agents\u2019 behavior does not affect the goal assignments. Each agent is allowed to perform the following actions: wait at the current vertex and move to an adjacent vertex. The duration of each action is uniform, i.e., one timestep. We assume that the outcomes of the actions are deterministic and no inaccuracies occur when executing the actions.\nEach agent has complete knowledge of the graph G. However, it can observe the other agents only locally. When observing them, no communication is happening. Thus, an agent does not know the current goals or intended paths of the other agents. It only observes their locations. The observation function can be defined differently depending on the type of graph. In our experiments, we use 4-connected grids and assume that an agent observes the other agents in the area of the size m \u00d7 m, centered at the agent\u2019s current position.\nOur task is to construct an individual policy \u03c0 for each agent, i.e., the function that takes as input a graph (global information) and (a history of) observations (local information) and outputs a distribution over actions. Equipped with such policy, an agent at each time step samples an action from the distribution suggested by \u03c0 and executes it in the environment. This continues until timestep Tmax is reached when the episode ends. Upon that, we compute the throughput as the ratio of the number of goals achieved by all agents to episode length. We use it to compare different policies: we assert that \u03c01 outperforms \u03c02 if the throughput of the former is higher.\nPartially Observable Markov Decision Process We consider a partially observable multi-agent Markov decision process (Bernstein et al. 2002; Kaelbling, Littman, and Cassandra 1998): M = \u27e8S,A,U, P,R,O,O, \u03b3\u27e9. At each timestep, each agent u \u2208 U , where U = 1, . . . , n, chooses an action a(u) \u2208 A, forming a joint action j \u2208 J = An. This joint action leads to a (stochastic) change of the environment according to the transition function P (s\u2032|s, j) : S\u00d7J\u00d7S \u2192 [0, 1].\nAfter that, each agent receives an individual observation o(u) \u2208 O based on the global observation function O(s, a) : S\u00d7A \u2192 O, and an individual scalar reward R(s, u, j) : S\u00d7 U \u00d7J \u2192 R, which depends on the current state, joint action and may be different for different agents. Discount factor 0 \u2264 \u03b3 \u2264 0 determines the importance of future rewards.\nTo make decisions, each agent maintains an actionobservation history \u03c4 (u) \u2208 T = (O\u00d7A)\u2217. The latter is used to condition a stochastic policy \u03c0(u)(a(u)|\u03c4 (u)) : T \u00d7 A \u2192\n[0, 1]. The aim is to obtain (to learn) a policy \u03c0(u) for each individual agent that maximizes the expected cumulative reward over time."
        },
        {
            "heading": "Learn to Follow",
            "text": "The suggested approach, which we dub FOLLOWER, is comprised of the two complementary modules combined into a coherent pipeline shown in Fig. 2. First, a Heuristic Path Decider is used to construct an individual path to the goal. Then, a Learnable Follower is invoked to reach the first waypoint on this path. This module decides which actions to take until the waypoint is reached or until the agent gets away from the path. In both cases, the sub-goal decider is called again, and the cycle repeats."
        },
        {
            "heading": "Heuristic Path Decider",
            "text": "The aim of this module is to build a path from the current location of the agent to the goal one. The static obstacles are taken into account for pathfinding, while the locations of the other agents (even the currently visible) are not; therefore, the constructed path may go through them. The rationale behind this is that the collision avoidance will be handled later on by the learnable path following policy.\nA crucial design choice for this module is which individual path to build. On the one hand, A* finds the shortest (individual) path to the goal. On the other hand, when the number of agents is high and each agent is following the shortest path, congestion often arises in the bottleneck parts of the map, such as corridors or doors. This degrades the overall performance dramatically. To this end, we suggest searching not for the shortest paths but rather for the evenly dispersed paths. Intuitively, we wish to distribute the agents across the grid to decrease congestion and increase the throughput. This technique is implemented as follows.\nInstead of assuming that the transition costs used by A* are uniform, we compute the individual varying transition costs associated with the cells. The individual cost of a (transition to a) cell is the sum of two components, the static and the dynamic one:\ncost(c, t) = costst(c) + costdyn(c, t). (1)\nThe static cost component depends on the topology of the map and does not change through the episode. The dynamic cost component, conversely, is based on the history of the observations of the agent and is dynamically updated.\nTo estimate the static cost of each cell, we, first, compute the average cost of the paths starting in this cell and ending in all other free cells (we use BFS algorithm for that):\navg cost(c) = \u2211\nc\u2032\u2208Vfree(c)\npath cost(c, c\u2032)\n|Vfree(c)| , (2)\nwhere Vfree(c) denotes the vertices reachable from c. Intuitively, the lower values of avg cost(c) indicate that a higher number of (the shortest) paths pass through c, and, thus, the latter is a potential congestion attractor. Consecutively, the transition to c should be penalized. This is implemented as follows:\ncostst(c) = maxc\u2032\u2208V (avg cost(c\n\u2032))\navg cost(c) , (3)\nIn other words, the static transition cost to a cell is 1 only if it is the \u201cmost rarely used\u201d cell of the grid, while the transition costs to the other (more frequently used) cells are higher.\nThe dynamic cost, costdyn(c, t), is based on the personal experience of an agent and changes during the episode. It is computed as follows.\ncostdyn(c, t) = \u2211\nt\u2032\u2208[0,t]\nAgentAtCell(c, t\u2032), (4)\nwhere AgentAtCell(c, t\u2032) is a function that returns 1 iff some agent was observed (by the current agent) at cell c at timestep t\u2032 and returns 0 otherwise.\nIntuitively, the dynamic cost penalizes transitions to the cells that are frequently used by the other agents. Indeed, each agent maintains its own dynamic costs. Moreover, to avoid the negative impact of over-accumulating the dynamic penalties, whenever an agent reaches its goal it resets the dynamic costs of all grid cells.\nEmpirically, both the precomputed transition costs and the individual dynamic costs contribute toward greater efficiency of our solver as will be shown later."
        },
        {
            "heading": "Learnable Follower",
            "text": "This module implements a learnable policy tailored to achieve the nearest waypoint of the path while avoiding a collision with the other agents. The policy function is approximated by a (deep) neural network and, as the agents are assumed to be homogeneous, a single network is utilized during training (a technique referred to as policy sharing). This approach is beneficial for complex tasks and large maps where it would be infeasible to learn a separate neural network for each agent, as the number of parameters increases linearly with the number of agents.\nThe input to the neural network represents the local observation of an agent and is comprised of a 2\u00d7m\u00d7m tensor, where m is the observation range. The channels of the tensor encode the locations of the static obstacles combined with the current path and the other agents; see Fig. 2.\nThe input goes through the Spatial Encoder first, and then the network is split into the actor and critic heads, with the RNN blocks designed to memorize the observation history. The output of the actor is the Action Decoder, which produces an action distribution. The Critic Head generates a value estimate, which is needed for training purposes only.\nThe pipeline employs a policy optimization algorithm, rewarding the agent with +r for reaching the first waypoint. If the agent deviates from or approaches the waypoint, the heuristic path decider is reactivated. This is advantageous in situations where taking a detour to avoid congestion with other agents is beneficial in achieving the overall goal. The focus on reaching the first waypoint provides a dense reward signal.\nWhile the agent is rewarded for reaching the nearest waypoint, its decision-making extends beyond the immediate\nvicinity of that waypoint. It\u2019s important to note that the FOLLOWER aims to maximize rewards by navigating through multiple waypoints en route to the global goal. It takes into account potential long-term cumulative rewards, such as allowing another agent to pass and then following the path, instead of obstructing each other.\nThe task of the learning process is to optimize the shared policy \u03c0u\u03b8 (i.e. the same policy for each agent) to maximize the expected cumulative reward. During the training process, rollouts (sequences of observations, rewards, and actions) are gathered asynchronously from multiple environments with varying numbers of agents. The shared policy \u03c0\u03b8 (actor network) is continually updated using the PPO clipped loss (Schulman et al. 2017).\nIn practice, the observation history \u03c4u is effectively modeled using a recurrent neural network (RNN) integrated into the actor and critic heads. The actor network is parameterized by \u03b8, while the critic network is parameterized by \u03d5. In our approach, we specifically utilize the GRU architecture (Chung et al. 2014).\nDuring the decentralized inference, each agent uses a copy of the trained weights, and the other parameters remain unchanged. The proposed FOLLOWER scheme, despite its simplicity, allows the agent to separate the two components of the overall policy transparently and does not require the involvement of any expert data for training. Finally, the reward function used is simple and does not require involved manual shaping."
        },
        {
            "heading": "Experimental Evaluation",
            "text": "To evaluate the efficiency of the proposed method2, we conduct a set of experiments, comparing it with the state-of-the-\n2We are committed to open-source FOLLOWER.\nart LMAPF algorithms on different maps. The training and evaluation of the presented approaches is held in fast and scalable POGEMA3 environment.\nThe learnable policy of FOLLOWER is implemented as the neural network of the following architecture. The Spatial Encoder is a ResNet (He et al. 2016) with an additional Multi-Layer Perceptron (MLP) in the output layer. The Action Decoder and the Critic Head are recurrent neural networks, based on the GRU architecture (Chung et al. 2014). The total number of parameters is 5M. Moreover, we additionally create a trimmed version of the network that only contains 3678 parameters and completely omits the RNN part (see Appendix for more details). We call a solver using it FOLLOWERLITE. The latter is implemented purely in C++ while the regular FOLLOWER \u2013 using both C++ (for pathfinding) and Python (for neural network machinery).\nFor training the episode length is set to 512. The agent\u2019s field-of-view is 11\u00d711, the number of agents varies in range: 128, 256. The reward for following the planned path is a small positive number, i.e. r = 0.01. More details about the parameters of the neural network are reported in Appendix. Upon fixing the parameters, the final policy of FOLLOWER is trained for 1 billion steps using a single NVIDIA A100 in approximately 18 hours. FOLLOWERLITE is trained for 20 million steps with a single NVIDIA TITAN RTX GPU in approximately 30 minutes."
        },
        {
            "heading": "Comparison With the Learnable Methods",
            "text": "In the first series of experiments, we compare FOLLOWER and FOLLOWERLITE with the state-of-the-art learnable MAPF solvers \u2013 SCRIMP (Wang et al. 2023), PRIMAL2 (Damani et al. 2021) and PICO (Li et al. 2022).\n3https://github.com/AIRI-Institute/pogema\nPRIMAL2 is a seminal approach specifically tailored for solving LMAPF problems. SCRIMP and PICO are the modern decentralized MAPF solvers that were (straightforwardly) adopted by us to handle LMAPF setting. All of these solvers are indeed, decentralized, and rely on the local observations. In the experiments we utilize the conflict-handling mechanism from PRIMAL2 \u2013 when two or more agents decide to move to the same cell, only one of them succeeds while the rest stay put. Noteworthy, SCRIMP has a dedicated negotiation procedure for that, which we did not modify.\nAs learnable methods assume training on a certain type of maps, we use the maps suggested by the authors of the respective baselines for a fair comparison. Specifically, we made a comparison on two types of maps \u2013 the maze-like maps of size 65 \u00d7 65 on which PRIMAL2 was originally trained, and 20 \u00d7 20 grids with randomly placed obstacles, that were used for training PICO and SCRIMP. The visualizations of the maps are given in Appendix. We use the readily available weights for PRIMAL2 and SCRIMP neural networks from the authors\u2019 repositories. PICO was trained by us using the open-source code of its authors. Our solvers, FOLLOWER and FOLLOWERLITE, were trained on the maze-like maps only.\nFor evaluation, each solver is faced with 10 different maze-like and 40 random maps that were not used during training. Each map is populated with an increasing number of agents, ranging from 32 to 256 agents for maze-like maps and from 8 to 64 for random ones. Start and goal locations for the agents are generated randomly. The length of the episode is set to 512.\nThe results of the first series of experiments are depicted in Fig. 3. The x-axis shows the number of agents, and the yaxis demonstrates the average throughput. Overall, on both types of maps FOLLOWER demonstrates the best results, notably outperforming all the competitors. The main competitor on the maze-like maps, PRIMAL2, shows almost twice less throughput on the instances with 256 agents. The main competitor on random maps, SCRIMP, shows results equal to the lightweight version of FOLLOWER, i.e. FOLLOWERLITE. However, the results of SCRIMP on the maze-like maps are much worse, that indicates its low ability to generalize. PICO demonstrates the worst results on random maps\nout of all the evaluated approaches, though it was trained on this type of maps. Thus, it is excluded from the rest of the evaluations.\nOut-of-distribution evaluation. An important quality of any learnable algorithm is its generalization, i.e. the ability to solve problem instances that are not similar to the ones used for training. We have already seen that FOLLOWER generalizes well and can outperform SCRIMP on random maps though FOLLOWER was not trained on this type of maps. Now we run an additional test where we compare FOLLOWER, FOLLOWERLITE, PRIMAL2 and SCRIMP on two (unseen during learning) maps from the well-known in the MAPF community MovingAI benchmark (Stern et al. 2019): den520d and Paris 1. The former map was taken from a video game, while the latter one correspond to the part of a real city. Their topologies are quite different from the one of the maps used for training. Both of the maps were downscaled to the size of 64\u00d7 64.\nThe results of these experiments are presented in Fig. 4. Again FOLLOWER significantly outperforms all the competitors. PRIMAL2 demonstrates very low throughput on both maps, that indicates its bad ability of generalization. Compared to PRIMAL2, SCRIMP shows itself much better in terms of generalization, but in the best case it is only able to demonstrate results comparable to the lightweight version of the proposed approach, i.e. FOLLOWERLITE. Additional results of the out-of-distribution experiments are presented in Appendix."
        },
        {
            "heading": "Comparison With Non-Learnable Approaches",
            "text": "We compare FOLLOWER and FOLLOWERLITE with two non-learnable approaches \u2013 RHCR4 (Li et al. 2021) and PIBT5 (Okumura et al. 2022). These two approaches are in two different polarities \u2013 RHCR is the state-of-the-art search-based approach aiming at the high-quality (i.e. highthroughput) LMAPF solutions at the expense of the limited scalability, while PIBT is the state-of-the-art rule-based approach that is extremely fast, but provides solutions of a moderate quality.\nRHCR solver requires setting a time limit for planning. We set it either to 1 or 10 seconds (both variants are reported with the according names). We chose PBS (Ma et al. 2019a)\n4https://github.com/Jiaoyang-Li/RHCR 5https://github.com/Kei18/pibt2\nas the planning approach since it showed the best results in the original paper. We have also tuned the planning horizon (2, 5, 10, 20), the re-planning rate (1, 5) and found that the best throughput is achieved by RHCR when the first parameter is set to 20 and the second one to 5 (see Appendix for more details). We use these values in our experiments. The other parameters of RHCR are left default.\nThe comparison is performed on the warehouse map from the original RHCR paper (Li et al. 2021). The maximum number of agents is limited to 192, according to restrictions for starting locations introduced in (Li et al. 2021). We generated 10 random instances for each number of agents for evaluation.\nThe results are presented in Fig. 5. As can be seen on the left part of the Fig. 5, both versions of RHCR significantly outperform the other solvers when the number of agents is up to 128. However, when this number increases to 160 and 192, the performance of RHCR with a 1s time cap degrades dramatically. It is then outperformed by both FOLLOWER and FOLLOWERLITE. This highlights the principal limitation of the centralized approach: it does not scale well to a large number of agents, especially when a strict time limit for finding a MAPF solution is imposed. PIBT does not have such a problem with scalability but its throughput is the lowest among all the evaluated methods.\nTo better understand how the runtime of the evaluated methods is affected by the increasing number of agents, see the right pane of the Fig. 5. In this plot each data point indicates how much time on average is spent to decide the next action. Indeed, FOLLOWER needs much less time to choose an action and, consequtively, scales better to the increasing number of agents compared to RHCR. As expected, PIBT is the fastest approach, as its rule-based procedures are computationally cheap compared to the ones used by RHCR and FOLLOWER. Recall, however that the throughput of PIBT is inferior. Moreover, in practice our method can be effectively parallelized (to run individually on each agent) and this will (as we expect) lower down its runtime further on."
        },
        {
            "heading": "Ablations",
            "text": "In this experiment, we investigate the impact of different components on the performance of FOLLOWER. To this end we turn them off and run the resultant solver on the warehouse map from the RHCR paper. Specifically, FOLLOWER (no RL) omits the learnable policy. At each\ntimestep, it plans a path, taking into account other observable agents as static obstacles, and selects its first action for the execution. If no path is found a random action is picked. FOLLOWER (no cost accumulation) and FOLLOWER (no precalculated cost) use both planning and learning components, but they do not utilize one of the introduced techniques that penalize transitions to the frequently used cells.\nThe results are shown in Fig. 6 (left pane). First, note that the performance of FOLLOWER (no RL) is inferior, which justifies the importance of the learnable policy and its contribution to the efficiency of FOLLOWER. The same can be said about the cost-penalizing techniques. Overall, it is clear that all components of FOLLOWER are crucial; omitting any of them results in a notable degradation of performance.\nIn addition, we run FOLLOWER with different episode lengths (up to 10, 000), as the initial distribution of the agents can be very different from the distribution that happens after some time. The results are shown in Fig. 6 (right pane). Notably, the absense of RL policy and dynamic cost accumulation lead to a very low throughput in the limit. We explain this by the congestion that occurs and grows like a rolling snowball and prevent the agents from reaching their goals. Indeed, FOLLOWER copes well with this as its throughput monotonically increases with the episode length and then plateaus."
        },
        {
            "heading": "Summary",
            "text": "The proposed approach surpasses learnable decentralized competitors, especially when the number of agents is large or when dealing with maps different from the training ones. Both the learnable component and the cost-penalizing techniques are essential to FOLLOWER\u2019s performance. Furthermore, FOLLOWER scales much better than the state-of-theart search-based solver and provides solutions of the better quality compared to modern rule-based solver."
        },
        {
            "heading": "Conclusion",
            "text": "In this study, we addressed the challenging problem of decentralized lifelong multi-agent pathfinding. We proposed a solution that leverages heuristic search for long-term planning and reinforcement learning for short-term conflict resolution. Our method consistently outperforms decentralized learnable competitors. Moreover, it provides the better tradeoff between the scalability and the solution quality compared to the modern search-based and rule-based planners. Direc-\ntions for future research may include: enriching the action space of the agents, handling uncertain observations and external stochastic events."
        },
        {
            "heading": "Appendix",
            "text": ""
        },
        {
            "heading": "Source Code",
            "text": "The source code related to this paper will be made publicly available upon publication. Until then, we invite individuals interested in accessing the code to contact us directly via email."
        },
        {
            "heading": "Limitations",
            "text": "Similarly to a wide range of other works focusing on learnable (L)MAPF solvers (including the ones we compare with), we rely on the following assumptions. The map of the environment is accurate and does not change. The agents have perfect localization and mapping abilities and execute actions accurately (and their moves are synchronized). All these may be considered as the limitations, as in real world, e.g. in robotic applications, many of the assumptions do not hold.\nAnother limitation is that the suggested approach, as any other (known to us) learnable (L)MAPF solver, is not able to provide theoretical guarantees of completeness/optimality."
        },
        {
            "heading": "The Effect Of Penalizing The Transition Costs",
            "text": "As explained in the main body of the paper, for pathfinding with A* each agent computes an individual transition cost to each cell of the grid that is updated during the episode. Each time some other agent is observed at a certain cell its transition cost is increased. This helps to avoid areas that are often used by many agents and thus to pro-actively avoid congestion, as shown in the main body of the paper.\nHere we wish to demonstrate the effect of the introduced cost penalizing technique visually. In Fig. 7 one of the problem instances from our dataset in shown. The left part of the figure shows the result of solving this instance (that contained 128 simultaneously moving agents for 512 timesteps) by FOLLOWER and FOLLOWERLITE that omit the cost penalizing technique (i.e. all the cells have transition cost 1 and it never changes). The more intense the (red) color is \u2013 the more frequently the corresponding cell was visited by the agents. Clearly, in this case the agents tend to use the central part of the map frequently. This makes it hard for the agents to avoid each other in the narrow passages that are plentiful on this map. On the other hand, when the individual varying transition costs are employed (right part of Fig. 7), the agents get evenly distributed across the map, which prevents congestion and increase the performance (as confirmed by the experiments, reported in the main body of the paper)."
        },
        {
            "heading": "Additional Out-Of-Distribution Evaluation",
            "text": "In the main body of the paper we have already reported the results obtained on the maps that are not alike the ones used for training the considered learnable solvers. Here we provide the additional results of FOLLOWER, FOLLOWERLITE, PRIMAL2 and SCRIMP on 4 additional out-of-distribution maps taken from the MovingAI benchmark: Boston 0, den312d, room-64-64-16 and room-64-64-8.\nThe results presented in Fig. 8 confirm the claims made in the main text of the paper. First, PRIMAL2 is the worst solver in terms of generalization and ability to achieve high\ntroughput on the maps that are not alike the ones it was trained for. Second, neither SCRIMP nor PRIMAL2 are able to compete with FOLLOWER when the number/density of agents is high. Third, the throughput of FOLLOWERLITE is lower than the one of FOLLOWER, which is expected as the neural network of the former has much less parameters (3K vs. 5M) and does not contain the RNN (GRU) blocks."
        },
        {
            "heading": "Tuning RHCR Parameters",
            "text": "As it was mentioned in the main text, we have tuned the parameters of RHCR before conducting the empirical comparison to our method. We varied the values of such RHCR parameters as planning horizon (2, 5, 10, 20), re-planning rate (1,5) and time limit for each re-planning attempt (1s, 10s). Planning horizon parameter controls for how many time steps the resultant plans will be collision-free. E.g. when it equals 10 it is guaranteed that for the next 10 time steps the agents following the constructed plans will not collide. Replanning rate determines how frequently (in time steps) reconstruction of the plans (for all agents) occurs. Time limit parameter restricts the amount of time (in seconds) which is alotted for each re-planning attempt.\nFig. 9 demonstrates the results of different versions of RHCR (note that planning horizon cannot be lower than replanning rate). The best average throughput is achieved by RHCR with re-planning rate 5 and planning horizon 20, denoted as (w = 5, h = 20) in the figure. Noteworty, the same values of these parameters were used for the experimental evaluation on the warehouse map in the original RHCR paper. Thus, the results of this version are included into the\nmain part of our paper."
        },
        {
            "heading": "Additional Training Details",
            "text": "As reported in the main body of the paper, after tuning and fixing the hyper-parameters required by the learnable component of FOLLOWER, the latter was trained for one billion steps on A100 GPU (18 hours of training). Only maze-like maps were used for training. Fig. 10 illustrates how the performance changed while training.\nFig. 10 (a) shows the reward plot. Some perturbations of the reward are noticeable, which can be explained by the random sampling of maps and problem instances, among which both simple and complex setups are present. Overall, the reward consistently grows (and then stabilizes) which means that learning is, indeed, happening.\nDuring training we also regularly saved the current weights of the neural network and invoked FOLLOWER on the subset of the training maps with the fixed seeds and 256 agents. The results are shown in Fig. 10 (b). Clearly, the throughput is increasing, indicating that a single agent is not only learning to follow its path effectively but also acquiring the skills needed to cooperatively solve non-trivial LMAPF instances.\nMoreover, during learning, we tested FOLLOWER on random maps that were not part of the training dataset. The results are shown in Fig. 10 (c). Again, we observed an\nincrease in throughput, indicating that our solver does not overfit to the training maps but is capable of adapting to and solving arbitrary LMAPF problems effectively."
        },
        {
            "heading": "Hyperparameters",
            "text": "Table 1 presents the hyperparameters of FOLLOWER and FOLLOWERLITE.\nThe hyperparameters for which the tuning range is given (e.g. learning rate, GRU hidden size, etc.) are optimized using Bayesian search.\nThe observation radius has been set to 11 \u00d7 11 as it is commonly used in similar learning-based methods (with whom we compare). The parameters for the number of rollout workers, environments per worker, and training steps are empirically determined to decrease the overall learning time of the algorithm. For the remaining paramaters (value loss coefficient, GAE\u03bb, activation function, network initialization), we have used the default values provided in the Sam-\npleFactory framework6. We conducted a hyperparameter sweep with approximately 150 runs, amounting to around 200 GPU hours. The models for FOLLOWER were trained for 100 million steps. Our optimization target was the average throughput on the validation set, which consisted of 15 mazes from the training set with fixed seeds.\nMaps Visualizations Fig. 11 illustrates examples of the maps used for testing. The presented names of the maps correspond to the names in our repository.\nInitial positions of the agents are shown by the filled circles, while their (initial) goals are represented by the empty ones. Each agent is assigned a unique goal initially. Subsequent LMAPF goals are randomly generated, ensuring that a feasible path from the agent\u2019s current location to the goal exists. The goals for each agent are generated independently using a fixed seed, ensuring consistency and enabling fair testing of the algorithms (i.e. each algorithm gets the same start/goals locations).\n6https://github.com/alex-petrenko/sample-factory"
        }
    ],
    "title": "Learn to Follow: Decentralized Lifelong Multi-agent Pathfinding via Planning and Learning",
    "year": 2023
}