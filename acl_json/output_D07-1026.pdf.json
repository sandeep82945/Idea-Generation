{
    "abstractText": "In this paper we propose an instance based method for lexical entailment and apply it to automatic ontology population from text. The approach is fully unsupervised and based on kernel methods. We demonstrate the effectiveness of our technique largely surpassing both the random and most frequent baselines and outperforming current state-of-the-art unsupervised approaches on a benchmark ontology available in the literature.",
    "authors": [
        {
            "affiliations": [],
            "name": "Claudio Giuliano"
        },
        {
            "affiliations": [],
            "name": "Alfio Gliozzo"
        }
    ],
    "id": "SP:8178bb1ab494c81e029f132ce8c025b3dfbebd11",
    "references": [
        {
            "authors": [
                "Philipp Cimiano",
                "Johanna V\u00f6lker."
            ],
            "title": "Towards large-scale, open-domain and ontology-based named entity classification",
            "venue": "Proceedings of RANLP\u201905, pages 66\u2013 166\u2013172, Borovets, Bulgaria.",
            "year": 2005
        },
        {
            "authors": [
                "I. Dagan",
                "O. Glickman."
            ],
            "title": "Probabilistic textual entailment: Generic applied modeling of language variability",
            "venue": "Proceedings of the PASCAL Workshop on Learning Methods for Text Understanding and Mining, Grenoble.",
            "year": 2004
        },
        {
            "authors": [
                "Ido Dagan",
                "Oren Glickman",
                "Bernardo Magnini"
            ],
            "title": "The PASCAL recognising textual entailment",
            "year": 2005
        },
        {
            "authors": [
                "Ido Dagan",
                "Oren Glickman",
                "Alfio Gliozzo",
                "Efrat Marmorshtein",
                "Carlo Strapparava."
            ],
            "title": "Direct word sense matching for lexical substitution",
            "venue": "Proceedings ACL-2006, pages 449\u2013456, Sydney, Australia, July.",
            "year": 2006
        },
        {
            "authors": [
                "I. Dagan."
            ],
            "title": "Contextual word similarity",
            "venue": "Rob Dale, Hermann Moisl, and Harold Somers, editors, Handbook of Natural Language Processing, chapter 19, pages 459\u2013476. Marcel Dekker Inc.",
            "year": 2000
        },
        {
            "authors": [
                "C. Fellbaum."
            ],
            "title": "WordNet",
            "venue": "An Electronic Lexical Database. MIT Press.",
            "year": 1998
        },
        {
            "authors": [
                "Michael Fleischman",
                "Eduard Hovy."
            ],
            "title": "Fine grained classification of named entities",
            "venue": "Proceedings of ACL-2002, pages 1\u20137, Morristown, NJ, USA.",
            "year": 2002
        },
        {
            "authors": [
                "William A. Gale",
                "Kenneth W. Church",
                "David Yarowsky."
            ],
            "title": "Work on statistical methods for word sense disambiguation",
            "venue": "R. Goldman et al., editor, Working Notes of the AAAI Fall Symposium on Probabilistic Approaches to Natural Language, pages 54\u2013",
            "year": 1992
        },
        {
            "authors": [
                "Claudio Giuliano",
                "Alfio Massimiliano Gliozzo",
                "Carlo Strapparava"
            ],
            "title": "Syntagmatic kernels: a word sense disambiguation case study",
            "venue": "In Proceedings of the EACL-2006 Workshop on Learning Structured Information in Natural Language Applications,",
            "year": 2006
        },
        {
            "authors": [
                "O. Glickman",
                "E. Shnarch",
                "I. Dagan."
            ],
            "title": "Lexical reference: a semantic matching subtask",
            "venue": "proceedings of EMNLP 2006.",
            "year": 2006
        },
        {
            "authors": [
                "Oren Glickman",
                "Ido Dagan",
                "Mikaela Keller",
                "Samy Bengio",
                "Walter Daelemans."
            ],
            "title": "Investigating lexical substitution scoring for subtitle generation",
            "venue": "Proceedings of CoNLL-2006.",
            "year": 2006
        },
        {
            "authors": [
                "A. Gliozzo",
                "C. Giuliano",
                "C. Strapparava."
            ],
            "title": "Domain kernels for word sense disambiguation",
            "venue": "Proceedings of ACL-2005, pages 403\u2013410, Ann Arbor, Michigan, June.",
            "year": 2005
        },
        {
            "authors": [
                "John Lafferty",
                "Andrew McCallum",
                "Fernando Pereira."
            ],
            "title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
            "venue": "Proceedings of ICML-2002, pages 282\u2013289, Williams College, MA. Morgan Kaufmann, San Francisco, CA.",
            "year": 2001
        },
        {
            "authors": [
                "Dekang Lin."
            ],
            "title": "Automatic retrieval and clustering of similar words",
            "venue": "Proceedings of ACL-98, pages 768\u2013 774, Morristown, NJ, USA.",
            "year": 1998
        },
        {
            "authors": [
                "B. Magnini",
                "C. Strapparava",
                "G. Pezzulo",
                "A. Gliozzo."
            ],
            "title": "The role of domain information in word sense disambiguation",
            "venue": "Natural Language Engineering, 8(4):359\u2013373.",
            "year": 2002
        },
        {
            "authors": [
                "Andrew Kachites McCallum."
            ],
            "title": "Mallet: A machine learning for language toolkit",
            "venue": "http://mallet.cs.umass.edu.",
            "year": 2002
        },
        {
            "authors": [
                "Diana McCarthy",
                "Rob Koeling",
                "Julie Weeds",
                "John Carroll."
            ],
            "title": "Finding predominant word senses in untagged text",
            "venue": "Proceedings of ACL-2004, Barcelona, Spain, July.",
            "year": 2004
        },
        {
            "authors": [
                "J. Shawe-Taylor",
                "N. Cristianini."
            ],
            "title": "Kernel Methods for Pattern Analysis",
            "venue": "Cambridge University Press.",
            "year": 2004
        },
        {
            "authors": [
                "Hristo Tanev",
                "Bernardo Magnini."
            ],
            "title": "Weakly supervised approaches for ontology population",
            "venue": "Proceedings of EACL-2006, Trento, Italy.",
            "year": 2006
        },
        {
            "authors": [
                "Erik Tjong Kim Sang",
                "Sabine Buchholz."
            ],
            "title": "Introduction to the CoNLL-2000 shared task: Chunking",
            "venue": "Proceedings of CoNLL-2000, Lisbon, Portugal.",
            "year": 2000
        },
        {
            "authors": [
                "Erik F. Tjong Kim Sang",
                "Fien De Meulder."
            ],
            "title": "Introduction to the CoNLL-2003 shared task: Languageindependent named entity recognition",
            "venue": "Proceedings of CoNLL-2003, pages 142\u2013147, Edmonton, Canada.",
            "year": 2003
        },
        {
            "authors": [
                "Erik F. Tjong Kim Sang."
            ],
            "title": "Introduction to the CoNLL-2002 shared task: Language-independent named entity recognition",
            "venue": "Proceedings of CoNLL2002, pages 155\u2013158, Taipei, Taiwan.",
            "year": 2002
        },
        {
            "authors": [
                "D. Yarowsky."
            ],
            "title": "Decision lists for lexical ambiguity resolution: Application to accent restoration in Spanish and French",
            "venue": "Proceedings of ACL-94, pages 88\u2013 95, Las Cruces, New Mexico.",
            "year": 1994
        },
        {
            "authors": [
                "Shubin Zhao",
                "Ralph Grishman."
            ],
            "title": "Extracting relations with integrated information using kernel methods",
            "venue": "Proceedings of ACL 2005, Ann Arbor, Michigan, June.",
            "year": 2005
        }
    ],
    "sections": [
        {
            "text": "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp. 248\u2013256, Prague, June 2007. c\u00a92007 Association for Computational Linguistics"
        },
        {
            "heading": "1 Introduction",
            "text": "Textual entailment is formally defined as a relationship between a coherent text T and a language expression, the hypothesis H . T is said to entail H , denoted by T \u2192 H , if the meaning of H can be inferred from the meaning of T (Dagan et al., 2005; Dagan and Glickman., 2004). Even though this notion has been recently proposed in the computational linguistics literature, it has already attracted a great attention due to the very high generality of its settings and to the indubitable usefulness of its (potential) applications.\nIn this paper, we concentrate on the problem of lexical entailment, a textual entailment subtask in which the system is asked to decide whether the substitution of a particular word w with the word e in a coherent text Hw = H lwHr generates a sentence He = H leHr such that Hw \u2192 He, where H l and Hr denote the left and the right context of w, respectively. For example, given the word \u2018weapon\u2019 a\nsystem may substitute it with the synonym \u2018arm\u2019, in order to identify relevant texts that denote the sought concept using the latter term. A particular case of lexical entailment is recognizing synonymy, where both Hw \u2192 He and He \u2192 Hw hold.\nIn the literature, slight variations of this problem are also referred to as sense matching (Dagan et al., 2006), lexical reference (Glickman et al., 2006a) and lexical substitution (Glickman et al., 2006b). They have been applied to a wide variety of tasks, such as semantic matching, subtitle generation and Word Sense Disambiguation (WSD). Modeling lexical entailment is also a prerequisite to approach the SemEval-2007 lexical substitution task1, consisting of finding alternative words that can occur in given context.\nIn this paper, we propose to apply an approach for lexical entailment to the ontology population task. The basic idea is that if a word entails another one in a given context then the former is an instance or a subclass of the latter. This approach is intuitively appealing because lexical entailment is intrinsically an unsupervised task, therefore it does not require lexical resources, seed examples or manually annotated data sets. Unsupervised approaches are particularly suited for ontology population, whose goal is to find instances of concepts from corpora, because both corpus and the ontology sizes can scale up to millions of documents and thousands of concepts, preventing us from applying supervised learning. In addition, the top level part of the ontology (i.e., the Tbox in the Description Logics terminology) is very\n1http://nlp.cs.swarthmore.edu/semeval/ tasks/task10/description.shtml\n248\noften modified during the ontology engineering lifecycle, for example by introducing new concepts and restructuring the subclass of hierarchy according to the renewed application needs required by the evolution of the application domain. It is evident that to preserve the consistency between the Tbox and the Abox (i.e., the set of instances and their relations) in such a dynamic ontology engineering process, supervised approaches are clearly inadequate, as small changes in the TBox will be reflected into dramatic annotation effort to keep instances in the Abox aligned.\nThe problem of populating a predefined ontology of concepts with novel instances implies a WSD task, as the entities in texts are ambiguous with respect to the domain ontology. For example, the entity Washington is both the name of a state and the name of a city. In the ontology population settings traditional WSD approaches cannot be directly applied since entities are not reported into dictionaries, making the lexical entailment alternative more viable. In particular, we model the problem of ontology population as the problem of recognizing for each mention of an entity of a particular coarsegrained type (e.g., location) the fine-grained concept (e.g., lake or mountain) that can be substituted in texts preserving the meaning. For example, in the sentence \u201cthe first man to climb the Everest without oxygen\u201d, \u201cEverest\u201d can be substituted with the word mountain preserving the meaning, while the sentence is meaningless when \u201cEverest\u201d is replaced with the word lake. Following the lexical entailment approach, the ontology population task is transformed into the problem of recognizing the term from a fine-grained set of categories (e.g., city, country, river, lake and mountain) that can be substituted in the contexts where the entity is mentioned (e.g., Everest in the example above).\nThe main contributions of this paper are summarized as follows. First, we propose a novel approach to lexical entailment, called Instance Based Lexical Entailment (IBLE), that allows approaching the problem as a classification task, in which a given target word (i.e., the entailing word) in a particular context is judged to entail a different word taken from a (pre-defined) set of (possible) candidate entailed words (see Section 3). Second, we exploit the IBLE approach to model the ontology population\ntask as follows. Given a set of candidate concepts belonging to generic ontological types (e.g., people or locations), and a set of pre-recognized mentions of entities of these types in the corpus (e.g., Newton, Ontario), we assign the entity to the class whose lexicalization is more frequently entailed in the corpus. In particular, as training set to learn the fine-grained category models, we use all the occurrences of their corresponding expressions in the same corpus (e.g., we collected all occurrences in context of the word scientist to describe the concept scientist). Then, we apply the trained model to classify the pre-recognized coarse-grained entities into the fine-grained categories.\nOur approach is fully unsupervised as for training it only requires occurrences of the candidate entailed words taken in their contexts. Restricted to the ontology population task, for each coarse-grained entity (e.g., location), the candidate entailed words are the terms corresponding to the fine-grained classes (e.g., lake or mountain) and the entailing words are mentions of entities (e.g., New York, Ontario) belonging to the coarse-grained class, recognized by an entity tagger.\nExperiments show that our method for recognizing lexical entailment is effective for the ontology population task, reporting improvements over a state-of-the-art unsupervised technique based on contextual similarity measures (Cimiano and Vo\u0308lker, 2005). In addition, we also compared it to a supervised approach (Tanev and Magnini, 2006), that we regarded as an upper bound, obtaining comparable results."
        },
        {
            "heading": "2 The Ontology Population Task",
            "text": "Populating concepts of a predefined ontology with instances found in a corpus is a primary goal of knowledge management systems. As concepts in the ontology are generally structured into hierarchies belonging to a common ontological type (e.g., people or locations), the problem of populating ontologies can be solved hierarchically, firstly identifying instances in texts as belonging to the topmost concepts, and then assigning them to a fine-grained class. Supervised named entity recognition (NER) systems can be used for accomplishing the first step. State-of-the-art NER systems are characterized by\nhigh accuracy, but they require a large amount of training data. However, domain specific ontologies generally contains many \u201cfine-grained\u201d categories (e.g., particular categories of people, such as writers, scientists, and so on) and, as a consequence, supervised methods cannot be used because the annotation costs would become prohibitive.\nTherefore, in the literature, the fine-grained classification task has been approached by adopting weakly supervised (Tanev and Magnini, 2006; Fleischman and Hovy, 2002) or unsupervised methods (Cimiano and Vo\u0308lker, 2005). Tanev and Magnini (2006) proposed a weakly supervised method that requires as training data a list of terms without context for each class under consideration. Such list can be automatically acquired from existing ontologies or other sources (i.e., database fields, web sites like Wikipedia, etc.) since the approach imposes virtually no restrictions on them. Given a generic syntactically parsed corpus containing at least each training entity twice, the algorithm learns, for each class, a feature vector describing the contexts where those entities occur. Then it compares the new (unknown) entity with the so obtained feature vectors, assigning it to the most similar class. Fleischman and Hovy (2002) approached the ontology population problem as a classification task, providing examples of instances in their context as training examples for their respective fine-grained categories.\nThe aforementioned approaches are clearly inadequate to recognize such fine-grained distinctions, as they would require a time consuming and costly annotation process for each particular class, that is clearly infeasible when the number of concepts in the ontology scales up. Therefore, most of the present research in ontology population is focusing on either unsupervised approaches (Cimiano and Vo\u0308lker, 2005) or weakly supervised approaches (Tanev and Magnini, 2006).\nUnsupervised approaches are mostly based on term similarity metrics. Cimiano and Vo\u0308lker (2005) assign a particular entity to the fine-grained class such that the contextual similarity is maximal among the set of fine-grained subclasses of a coarse-grained category. Contextual similarity has been measured by adopting lexico-syntactic features provided by a dependency parser, as proposed in (Lin, 1998)."
        },
        {
            "heading": "3 Instance Based Lexical Entailment",
            "text": "Dagan et al. (2006) adapted the classical supervised WSD setting to approach the sense matching problem (i.e., the binary lexical entailment problem of deciding whether a word, such as position, entails a different word, such as job, in a given context) by defining a one-class learning algorithm based on support vector machines (SVM). They train a oneclass model for each entailed word (e.g., all the occurrences of the word job in the corpus) and, then, apply it to classify all the occurrences of the entailing words (e.g., the word position), providing a binary decision criterion2. Similarly to the WSD case, examples are represented by feature vectors describing their contexts, and then compared to the feature vectors describing the context of the target word.\nIn this paper, we adopt a similar strategy to approach a multi-class lexical entailment problem. The basic hypothesis is that if a word w entails e in a particular context (Hw \u2192 He), then some of the contexts T je in which e occurs in the training corpus are similar to Hw. Given a word w and an (exhaustive) set of candidate entailed words E = {e1, e2, . . . , en}, to which we refer hereafter with the expression \u201csubstitution lexica\u201d, our goal is to select the word ei \u2208 E that can be substituted to w in the context Hw generating a sentence He such that Hw \u2192 He. In the multi-class setting, supervised learning approaches can be used. In particular, we can apply a one-versus-all learning methodology, in which each class ei is trained from both positive (i.e., all the occurrences of ei in the corpus) and negative examples (i.e., all the occurrences of the words in the set {ej |j 6= i}).\nOur approach is clearly a simplification of the more general lexical entailment settings, where given two generic words w and e, and a context H = H lwHr, the system is asked to decide whether w entails e or not. In fact, the latter is a binary classification problem, while the former is easier as the system is required to select \u201cthe best\u201d option among the substitution lexicon. Of course providing such set could be problematic in many cases (e.g., it could be incomplete or simply not available for\n2This approach resembles the pseudo-words technique proposed to evaluate WSD algorithms at the earlier stages of the WSD studies (Gale et al., 1992), when large scale sense tagged corpora were not available for training supervised algorithms.\nmany languages or rare words). On the other hand, such a simplification is practically effective. First of all, it allows us to provide both positive and negative examples, avoiding the use of one-class classification algorithms that in practice perform poorly (Dagan et al., 2006). Second, the large availability of manually constructed substitution lexica, such as WordNet (Fellbaum, 1998), or the use of repositories based on statistical word similarities, such as the database constructed by Lin (1998), allows us to find an adequate substitution lexicon for each target word in most of the cases.\nFor example, as shown in Table 1, the word job has different senses depending on its context, some of them entailing its direct hyponym position (e.g., \u201clooking for permanent job\u201d), others entailing the word task (e.g., \u201cthe job of repairing\u201d). The problem of deciding whether a particular instance of job can be replaced by position, and not by the word place, can be solved by looking for the most similar contexts where either position or place occur in the training data, and then selecting the class (i.e., the entailed word) characterized by the most similar ones, in an instance based style. In the first example (see row 1), the word job is strongly associated to the word position, because the contexts of the latter in the examples 1 and 2 are similar to the context of the former, and not to the word task, whose contexts (4, 5 and 6) are radically different. On the other hand, the second example (see row 2) of the word job is similar to the occurrences 4 and 5 of the word task, allowing its correct substitution.\nIt is worthwhile to remark that, due to the ambiguity of the entailed words (e.g., position could also entail either perspective or place), not every occurrence of them should be taken into account, in order to avoid misleading predictions caused by the irrelevant senses. Therefore, approaches based on a more classical contextual similarity technique (Lin, 1998; Dagan, 2000), where words are described \u201cglobally\u201d by context vectors, are doomed to fail. We will provide empirical evidence of this in the evaluation section.\nChoosing an appropriate similarity function for the contexts of the words to be substituted is a primary issue. In this work, we exploited similarity functions already defined in the WSD literature, relying on the analogy between the lexical entail-\nment and the WSD task. The state-of-the-art supervised WSD methodology, reporting the best results in most of the Senseval-3 lexical sample tasks in different languages, is based on a combination of syntagmatic and domain kernels (Gliozzo et al., 2005) in a SVM classification framework. Therefore, we adopted exactly the same strategy for our purposes.\nA great advantage of this methodology is that it is totally corpus based, as it does not require neither the availability of lexical databases, nor the use of complex preprocessing steps such as parsing or anaphora resolution, allowing us to apply it on different languages and domains once large corpora are available for training. Therefore, we exploited exactly the same strategy to implement the IBLE classifier required for our purposes, defining a kernel composed by n simple kernels, each representing a different aspect to be considered when estimating contextual similarity among word occurrences. In fact, by using the closure properties of the kernel functions, it is possible to define the kernel combination schema as follows3:\nKC(xi, xj) = n\u2211\nl=1\nKl(xi, xj)\u221a Kl(xj , xj)Kl(xi, xi) , (1)\nwhere Kl are valid kernel functions, measuring similarity between the objects xi and xj from different perspectives4.\nOne means to satisfy both the WSD and the lexical entailment requirements is to consider two different aspects of similarity: domain aspects, mainly related to the topic (i.e., the global context) of the texts in which the word occurs, and syntagmatic aspects, concerning the lexico-syntactic pattern in the local context. Domain aspects are captured by the domain kernel, described in Section 3.1, while syntagmatic aspects are taken into account by the syntagmatic kernel, presented in Section 3.2.\n3Some recent works (Zhao and Grishman, 2005; Gliozzo et al., 2005) empirically demostrate the effectiveness of combining kernels in this way, showing that the combined kernel always improves the performance of the individual ones. In addition, this formulation allows evaluating the individual contribution of each information source.\n4An exhaustive discussion about kernel methods for NLP can be found in (Shawe-Taylor and Cristianini, 2004)."
        },
        {
            "heading": "3.1 The Domain Kernel",
            "text": "(Magnini et al., 2002) claim that knowing the domain of the text in which the word is located is a crucial information for WSD. For example the (domain) polysemy among the Computer Science and the Medicine senses of the word virus can be solved by simply considering the domain of the context in which it is located. Domain aspects are also crucial in recognizing lexical entailment. For example, the term virus entails software agent in the Computer Science domain (e.g., \u201cThe laptop has been infected by a virus\u201d), while it entails bacterium when located in the Medicine domain (e.g., \u201cHIV is a virus\u201d). As argued in (Magnini et al., 2002), domain aspects can be considered by analyzing the lexicon in a large context of the word to be disambiguated, regardless of the actual word order. We refer to (Gliozzo et al., 2005) for a detailed description of the domain kernel. The simplest methodology to estimate the domain similarity among two texts is to represent them by means of vectors in the Vector Space Model (VSM), and to exploit the cosine similarity. The VSM is a kdimensional space Rk, in which the text tj is represented by means of the vector ~tj such that the ith component of ~tj is the term frequency of the term wi in it. The similarity between two texts in the VSM is estimated by computing the cosine between them, providing the kernel function KV SM that can be used as a basic tool to estimate domain similarity between texts5.\n5In (Gliozzo et al., 2005), in addition to the standard VSM, a domain kernel, exploiting external information acquired from unlabeled data, has been also used to reduce the amount of (labeled) training data. Here, given that our approach is fully unsupervised, i.e., we can obtain as many examples as we need, we do not use the domain kernel."
        },
        {
            "heading": "3.2 The Syntagmatic Kernel",
            "text": "Syntagmatic aspects are probably the most important evidence for recognizing lexical entailment. In general, the strategy adopted to model syntagmatic relations in WSD is to provide bigrams and trigrams of collocated words as features to describe local contexts (Yarowsky, 1994). The main drawback of this approach is that non contiguous or shifted collocations cannot be identified, decreasing the generalization power of the learning algorithm. For example, suppose that the word job has to be disambiguated into the sentence \u201c. . . permanent academic job in. . . \u201d, and that the occurrence \u201cWe offer permanent positions. . . \u201d is provided for training. A traditional feature mapping would extract the context words w\u22121:academic, w\u22122:permanent to represent the former, and w\u22121:permanent, w\u22122:offer to index the latter. Evidently such features will not match, leading the algorithm to a misclassification.\nThe syntagmatic kernel, proposed by Gliozzo et al. (2005), is an attempt to solve this problem. It is based on a gap-weighted subsequences kernel (Shawe-Taylor and Cristianini, 2004). In the spirit of kernel methods, this kernel is able to compare sequences directly in the input space, avoiding any explicit feature mapping. To perform this operation, it counts how many times a (non-contiguous) subsequence of symbols u of length n occurs in the input string s, and penalizes non-contiguous occurrences according to the number of the contained gaps. To define our syntagmatic kernel, we adapted the generic definition of the sequence kernels to the problem of recognizing collocations in local word contexts. We refer to (Giuliano et al., 2006) for a detailed description of the syntagmatic kernel."
        },
        {
            "heading": "4 Lexical Entailment for Ontology Population",
            "text": "In this section, we apply the IBLE technique, described in Section 3, to recognize lexical entailment for ontology population. To this aim, we cast ontology population as a lexical entailment task, where the fine-grained categories are the candidate entailed words, and the named entities to be subcategorized are the entailing words. Below, we present the main steps of our algorithm in details.\nStep 1 By using a state-of-the-art supervised NER system, we recognize the named entities belonging to a set of coarse-grained categories (e.g., location and people) of interest for the domain.\nStep 2 For all fine-grained categories belonging to the same coarse-grained type, we extract from a domain corpus all the occurrences of their lexicalizations in context (e.g., for the category actor, we extract all contexts where the term actor occurs), and use them as input to train the IBLE classifier. In this way, we obtain a multi-class classifier for each ontological type. Then, we classify all the occurrences of the named entities recognized in the first step. The output of this process is a list of tagged named entities; where the elements of the list could have been classified into different fine-grained categories even though they refer to the same phrase (e.g., the occurrences of the entity \u201cJack London\u201d could have been classified both as writer and actor, depending on the contexts where they occur).\nStep 3 A distinct category is finally assigned to the entities referring to the same phrase in the list. This is done on the basis of the tags that have been assigned to all its occurrences during the previous step. To this purpose, we implemented a voting mechanism. The basic idea is that an entity belongs to a specific category if its occurrences entail a particular superclass \u201cmore often than expected by chance\u201d, where the expectation is modeled on the basis of the overall distribution of fine-grained category labels, assigned during the second step, in the corpus. This intuition is formalized by applying a statistical reliability measure, that depends on the distribution of positive assignments for each class, defined by the\nfollowing formula:\nR(e, c) = P (c|e)\u2212 \u00b5c\n\u03c3c , (2)\nwhere P (c|e) is estimated by the relative frequency of the fine-grained class c among the different occurrences of the entity e, \u00b5c and \u03c3c measure the mean and the standard deviation of the distribution P (c|E), and E is an (unlabeled) training set of instances of the coarse-grained type classified by the IBLE algorithm. Finally, each entity is assigned to the category c\u2217 such that\nc\u2217 = argmax c R(e, c). (3)"
        },
        {
            "heading": "5 Evaluation",
            "text": "Evaluating a lexical entailment algorithm in itself is rather complex. Therefore, we performed a task driven evaluation of our system, measuring its usefulness in an ontology population task, for which evaluation benchmarks are available, allowing us to compare our technique to existing state-of-the-art approaches.\nAs introduced in Section 4, the ontology population task can be modeled as a lexical entailment problem, in which the fine-grained classes are the entailed words and the named entities belonging to the coarse-grained ontological type are the entailing words.\nIn the following, we first introduce the experimental settings (Section 5.1). Then we evaluate our technique by comparing it to state-of-the-art unsupervised approaches for ontology population (Section 5.2)."
        },
        {
            "heading": "5.1 Experimental Settings",
            "text": "For all experiments, we adopted the evaluation benchmark proposed in (Tanev and Magnini, 2006). It considers two high-level named entity categories both having five fine-grained sub-classes (i.e., mountain, lake, river, city, and country as subtypes of LOCATION; statesman, writer, athlete, actor, and inventor are subtypes of PERSON). The authors used WordNet and Wikipedia as primary data sources for populating the evaluation ontology. In total, the ontology is populated with 280 instances which were not ambiguous (with respect to the ontology) and appeared at least twice in\nthe English CLEF corpus6. Even the evaluation task is rather small and can be perceived as an artificial experimental setting, it is the best available benchmark we can use to compare our system to existing approaches in the literature, as we are not aware of other available resources.\nTo perform NER we used CRFs (Lafferty et al., 2001). We trained a first-order CRF on the MUC data set to annotate locations and people. In our experiments, we used the implementation provided in MALLET (McCallum, 2002). We used a standard feature set inspired by the literature on text chunking and NER (Tjong Kim Sang and Buchholz, 2000; Tjong Kim Sang and De Meulder, 2003; Tjong Kim Sang, 2002) to train a first-order CRFs. Each instance is represented by encoding all the following families of features, all time-shifted by - 2,-1,0,1,2: (a) the word itself, (b) the PoS tag of the token, (c) orthographic predicates, such as capitalization, upper-case, numeric, single character, and punctuation, (d) gazetteers of locations, people names and organizations, (e) character-n-gram predicates for 2 6 n 6 3.\nAs an (unsupervised) training set for the finegrained categories, we exploited all occurrences in context of their corresponding terms we found in the CLEF corpus (e.g., for the category actor we used all the occurrences of the term actor). We did not use any prior estimation of the class frequency, adopting a pure unsupervised approach. Table 2 lists the fine-grained concepts and the number of the training examples found for each of them in the CLEF corpus.\nAs a reference for a comparison of the outcomes of this study, we used the results presented in (Tanev and Magnini, 2006) for the Class-Word and ClassExample approaches. The Class-Word approach exploits a similarity metric between terms and concepts based on the comparison of the contexts where they appear. Details of this technique can be found in (Cimiano and Vo\u0308lker, 2005). Tanev and Magnini (2006) proposed a variant of the Class-Word algorithm, called Class-Example, that relies on syntactic features extracted from corpus and uses as an additional input a set of training examples for each class. Overall, it required 1, 194 examples to accomplish\n6http://www.clef-campaign.org\nthis task. All experiments were performed using the SVM package LIBSVM7 customized to embed our own kernel. In all the experiments, we used the default parameter setting."
        },
        {
            "heading": "5.2 Results",
            "text": "Table 4 shows our results compared with two baselines (i.e., random and most frequent, estimated from the test data) and the two alternative approaches for ontology population described in the previous section. Our system outperforms both baselines and largely surpasses the Class-Word unsupervised method.\nIt is worthwhile to remark here that, being the IBLE algorithm fully unsupervised, improving the most frequent baseline is an excellent result, rarely achieved in the literature on unsupervised methods for WSD (McCarthy et al., 2004). In addition, our system is also competitive when compared to supervised approaches, being it only 5 points lower than the Class-Example method, while it does not require seed examples and syntactic parsing. This characteristic makes our system flexible and adaptable to different languages and domains.\n7http://www.csie.ntu.edu.tw/\u223ccjlin/ libsvm/\nFinally, we performed a disaggregated evaluation of our system, assessing the performance for different ontological types and different concepts. Results show that our method performs better on larger fine-grained classes (i.e., writer and country), while the results on smaller categories are affected by low recall, even if the predictions provided by the system tends to be highly accurate. Taking into consideration that our system is fully unsupervised, this behavior is highly desirable because it implies that it is somehow able to identify the predominant class. In addition the high precision on the smaller classes can be explained by our instance based approach."
        },
        {
            "heading": "6 Conclusions and Future Work",
            "text": "In this paper, we presented a novel unsupervised technique for recognizing lexical entailment in texts, namely instance based lexical entailment, and we exploited it to approach an ontology population task. The basic assumption is that if a word is entailed by another in a given context, then some of the\ncontexts of the entailed word should be similar to that of the word to be disambiguated. Our technique is effective, as it largely surpasses both the random and most frequent baselines. In addition, it improves over the state-of-the-art for unsupervised approaches, achieving performances close to the supervised rivaling techniques requiring hundreds of examples for each class.\nOntology population is only one of the possible applications of lexical entailment. For the future, we plan to apply our instance based approach to a wide variety of tasks, e.g., lexical substitution, word sense disambiguation and information retrieval. In addition, we plan to exploit our lexical entailment as a subcomponent of a more complex system to recognize textual entailment. Finally, we are going to explore more elaborated kernel functions to recognize lexical entailment and more efficient learning strategies to apply our method to web-size corpora."
        },
        {
            "heading": "Acknowledgments",
            "text": "The authors would like to thank Bernardo Magnini and Hristo Tanev for providing the benchmark, Ido Dagan for useful discussions and comments regarding the connections between lexical entailment and ontology population, and Alberto Lavelli for his thorough review. Claudio Giuliano is supported by the X-Media project (http://www. x-media-project.org), sponsored by the European Commission as part of the Information Society Technologies (IST) program under EC grant number IST-FP6-026978. Alfio Gliozzo is supported by the FIRB-Israel research project N. RBIN045PXH."
        }
    ],
    "title": "Instance Based Lexical Entailment for Ontology Population",
    "year": 2007
}