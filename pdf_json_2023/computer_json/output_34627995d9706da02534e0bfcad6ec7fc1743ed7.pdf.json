{
    "abstractText": "This article presents a proximal policy optimization (PPO) based reinforcement learning (RL) approach for DC-DC boost converter control, which is compared to traditional control methods. The performance of the PPO algorithm is evaluated using MATLAB Simulink co-simulation, and the results demonstrate that the most efficient approach for achieving short settling time and stability is to combine the PPO algorithm with reinforcement learning based control method. The simulation results indicate that the step response characteristics provided using the control method based on RL with the PPO algorithm outperform traditional control approaches, which can be used to improve DC-DC boost converter control. This research also highlights the inherent capability of the reinforcement learning method to enhance the performance of boost converter control.",
    "authors": [
        {
            "affiliations": [],
            "name": "Utsab Saha"
        },
        {
            "affiliations": [],
            "name": "Shakib Shahria"
        },
        {
            "affiliations": [],
            "name": "A.B.M Harun-Ur Rashid"
        }
    ],
    "id": "SP:538f914a3598fd75477d967901072173b918b784",
    "references": [
        {
            "authors": [
                "M. Khursheed",
                "M. Mallick",
                "A. Iqbal"
            ],
            "title": "Tuning of controllers for a boost converter used to interface battery source to bts load of a telecommunication site",
            "venue": "Renewable Power for Sustainable Growth: Proceedings of International Conference on Renewal Power (ICRP",
            "year": 2020
        },
        {
            "authors": [
                "A. Janabi",
                "B. Wang"
            ],
            "title": "Switched-capacitor voltage boost converter for electric and hybrid electric vehicle drives",
            "venue": "IEEE Transactions on Power Electronics",
            "year": 2019
        },
        {
            "authors": [
                "K. Kumar",
                "R. Tiwari",
                "P.V. Varaprasad",
                "C. Babu",
                "K.J. Reddy"
            ],
            "title": "Performance evaluation of fuel cell fed electric vehicle system with reconfigured quadratic boost converter",
            "venue": "International Journal of Hydrogen Energy 46(11),",
            "year": 2021
        },
        {
            "authors": [
                "S. Hasanpour",
                "Y.P. Siwakoti",
                "A. Mostaan",
                "F. Blaabjerg"
            ],
            "title": "New semiquadratic 21 high step-up dc/dc converter for renewable energy applications",
            "venue": "IEEE Transactions on Power Electronics",
            "year": 2020
        },
        {
            "authors": [
                "M.L. Alghaythi",
                "R.M. O\u2019Connell",
                "N.E. Islam",
                "M.M.S. Khan",
                "J.M. Guerrero"
            ],
            "title": "A high step-up interleaved dc-dc converter with voltage multiplier and coupled inductors for renewable energy systems",
            "venue": "IEEE Access",
            "year": 2020
        },
        {
            "authors": [
                "O. Djamel",
                "G. Dhaouadi",
                "S. Youcef",
                "M. Mahmoud"
            ],
            "title": "Hardware implementation of digital pid controller for dc\u2013dc boost converter",
            "venue": "4th International Conference on Power Electronics and Their Applications (ICPEA),",
            "year": 2019
        },
        {
            "authors": [
                "S. Arulselvi",
                "G. Uma",
                "M. Chidambaram"
            ],
            "title": "Design of pid controller for boost converter with rhs zero",
            "venue": "The 4th International Power Electronics and Motion Control Conference,",
            "year": 2004
        },
        {
            "authors": [
                "B. Dhivya",
                "V. Krishnan",
                "R. Ramaprabha"
            ],
            "title": "Neural network controller for boost converter",
            "venue": "International Conference on Circuits, Power and Computing Technologies (ICCPCT),",
            "year": 2013
        },
        {
            "authors": [
                "S.S. Koduru",
                "V.S.P. Machina",
                "S. Madichetty"
            ],
            "title": "Real-time implementation of deep learning technique in microcontroller-based dc-dc boost converter-a design approach",
            "venue": "IEEE Delhi Section Conference (DELCON),",
            "year": 2022
        },
        {
            "authors": [
                "Kim",
                "S.-K",
                "C.R. Park",
                "J.-S",
                "Y.I. Lee"
            ],
            "title": "A stabilizing model predictive controller for voltage regulation of a dc/dc boost converter",
            "venue": "IEEE Transactions on Control Systems Technology",
            "year": 2014
        },
        {
            "authors": [
                "N.N. Ismail",
                "I. Musirin",
                "R. Baharom",
                "D. Johari"
            ],
            "title": "Fuzzy logic controller on dc/dc boost converter",
            "venue": "IEEE International Conference on Power and Energy,",
            "year": 2010
        },
        {
            "authors": [
                "K. Bendaoud",
                "S. Krit",
                "M. Kabrane",
                "H. Ouadani",
                "M. Elaskri",
                "K. Karimi",
                "H. Elbousty",
                "L. Elmaimouni"
            ],
            "title": "Implementation of fuzzy logic controller (flc) for dcdc boost converter using matlab/simulink",
            "venue": "Int. J. Sensors Sens. Networks, Spec. Issue Smart Cities Using a Wirel. Sens. Networks 5(5-1),",
            "year": 2017
        },
        {
            "authors": [
                "H. Guldemir"
            ],
            "title": "Sliding mode control of dc-dc boost converter",
            "venue": "Journal of Applied Sciences",
            "year": 2005
        },
        {
            "authors": [
                "R.P. Aguilera",
                "P. Acuna",
                "G. Konstantinou",
                "S. Vazquez",
                "J.I. Leon"
            ],
            "title": "Basic control principles in power electronics: Analog and digital control design",
            "venue": "Control of Power Electronic Converters and Systems,",
            "year": 2018
        },
        {
            "authors": [
                "M.A. Denai",
                "F. Palis",
                "A. Zeghbib"
            ],
            "title": "Anfis based modelling and control of nonlinear systems : a tutorial",
            "venue": "IEEE International Conference on Systems,",
            "year": 2004
        },
        {
            "authors": [
                "U. Saha",
                "S. Shahria",
                "Rashid"
            ],
            "title": "A.H.-U.: Intelligent control strategies for dc-dc boost converter: Performance analysis and optimization",
            "venue": "International Conference on Smart Systems for Applications in Electrical Sciences (ICSSES),",
            "year": 2023
        },
        {
            "authors": [
                "B.K. Bose"
            ],
            "title": "Artificial neural network applications in power electronics. In: IECON\u201901",
            "venue": "Annual Conference of the IEEE Industrial Electronics Society (Cat. No. 37243),",
            "year": 2001
        },
        {
            "authors": [
                "S. Zhao",
                "F. Blaabjerg",
                "H. Wang"
            ],
            "title": "An overview of artificial intelligence applications for power electronics",
            "venue": "IEEE Transactions on Power Electronics",
            "year": 2020
        },
        {
            "authors": [
                "D. Abel"
            ],
            "title": "A theory of abstraction in reinforcement learning",
            "venue": "arXiv preprint",
            "year": 2022
        },
        {
            "authors": [
                "D. Cao",
                "W. Hu",
                "J. Zhao",
                "G. Zhang",
                "B. Zhang",
                "Z. Liu",
                "Z. Chen",
                "F. Blaabjerg"
            ],
            "title": "Reinforcement learning and its applications in modern power and energy systems: A review. Journal of modern power systems and clean energy",
            "year": 2020
        },
        {
            "authors": [
                "M. Hajihosseini",
                "M. Andalibi",
                "M. Gheisarnejad",
                "H. Farsizadeh",
                "Khooban",
                "M.- H"
            ],
            "title": "Dc/dc power converter control-based deep machine learning techniques: Realtime implementation",
            "venue": "IEEE Transactions on Power Electronics",
            "year": 2020
        },
        {
            "authors": [
                "L. Bu\u015foniu",
                "T. De Bruin",
                "D. Toli\u0107",
                "J. Kober",
                "I. Palunko"
            ],
            "title": "Reinforcement learning for control: Performance, stability, and deep approximators",
            "venue": "Annual Reviews in Control",
            "year": 2018
        },
        {
            "authors": [
                "K. Prag",
                "M. Woolway",
                "T. Celik"
            ],
            "title": "Data-driven model predictive control of dc-to-dc buck-boost converter",
            "venue": "IEEE Access",
            "year": 1019
        },
        {
            "authors": [
                "S. Singh",
                "V. Kumar",
                "D. Fulwani"
            ],
            "title": "Mitigation of destabilising effect of cpls in island dc micro-grid using non-linear control",
            "venue": "IET Power Electronics",
            "year": 2017
        },
        {
            "authors": [
                "M. Andalibi",
                "M. Hajihosseini",
                "S. Teymoori",
                "M. Kargar",
                "M. Gheisarnejad"
            ],
            "title": "A time-varying deep reinforcement model predictive control for dc power converter systems",
            "venue": "IEEE 12th International Symposium on Power Electronics for Distributed Generation Systems (PEDG),",
            "year": 2021
        },
        {
            "authors": [
                "Z. Wu",
                "J. Zhao",
                "J. Zhang"
            ],
            "title": "Cascade pid control of buck-boost-type dc/dc power converters",
            "venue": "6th World Congress on Intelligent Control and Automation,",
            "year": 2006
        },
        {
            "authors": [
                "M.A. Mohamed",
                "Q. Guan",
                "M. Rashed"
            ],
            "title": "Control of dc-dc converter for interfacing supercapcitors energy storage to dc micro grids",
            "venue": "IEEE International Conference on Electrical Systems for Aircraft, Railway, Ship Propulsion and Road Vehicles & International Transportation Electrification Conference (ESARS-ITEC),",
            "year": 2018
        },
        {
            "authors": [
                "R. Sumita",
                "T. Sato"
            ],
            "title": "Pid control method using predicted output voltage for digitally controlled dc/dc converter",
            "venue": "1st International Conference on Electrical, Control and Instrumentation Engineering (ICECIE),",
            "year": 2019
        },
        {
            "authors": [
                "T. Kobaku",
                "R. Jeyasenthil",
                "S. Sahoo",
                "R. Ramchand",
                "T. Dragicevic"
            ],
            "title": "Quantitative feedback design-based robust pid control of voltage mode controlled dc-dc boost converter",
            "venue": "IEEE Transactions on Circuits and Systems II: Express Briefs 68(1),",
            "year": 2020
        },
        {
            "authors": [
                "Q. Xu",
                "Y. Yan",
                "C. Zhang",
                "T. Dragicevic",
                "F. Blaabjerg"
            ],
            "title": "An offset-free composite model predictive control strategy for dc/dc buck converter feeding constant power loads",
            "venue": "IEEE Transactions on Power Electronics",
            "year": 2019
        },
        {
            "authors": [
                "N. Boutchich",
                "A. Moufid",
                "N. Bennis",
                "S. El Hani"
            ],
            "title": "A constrained mpc approach applied to buck dc-dc converter for greenhouse powered by photovoltaic source",
            "venue": "International Conference on Electrical and Information Technologies (ICEIT),",
            "year": 2020
        },
        {
            "authors": [
                "X. Zhang",
                "W. He",
                "Y. Zhang"
            ],
            "title": "An adaptive output feedback controller for boost converter",
            "venue": "Electronics 11(6),",
            "year": 2022
        },
        {
            "authors": [
                "J. Fan",
                "S. Li",
                "J. Wang",
                "Z. Wang"
            ],
            "title": "A gpi based sliding mode control method for boost dc-dc converter",
            "venue": "IEEE International Conference on Industrial Technology (ICIT),",
            "year": 2016
        },
        {
            "authors": [
                "K. Louassaa",
                "A. Chouder",
                "C. Rus-Casas"
            ],
            "title": "Robust nonsingular terminal sliding mode control of a buck converter feeding a constant power load",
            "venue": "Electronics 12(3),",
            "year": 2023
        },
        {
            "authors": [
                "S. Singh",
                "D. Fulwani",
                "V. Kumar"
            ],
            "title": "Robust sliding-mode control of dc/dc boost converter feeding a constant power load",
            "venue": "IET Power Electronics",
            "year": 2015
        },
        {
            "authors": [
                "M.M. Mardani",
                "N. Vafamand",
                "M.H. Khooban",
                "T. Dragi\u010devi\u0107",
                "F. Blaabjerg"
            ],
            "title": "Design of quadratic d-stable fuzzy controller for dc microgrids with multiple cpls",
            "venue": "IEEE Transactions on Industrial Electronics",
            "year": 2018
        },
        {
            "authors": [
                "R.F. Bastos",
                "C.R. Aguiar",
                "A.F. Gon\u00e7alves",
                "Machado"
            ],
            "title": "R.Q.: An intelligent control system used to improve energy production from alternative sources with dc/dc integration",
            "venue": "IEEE Transactions on Smart Grid",
            "year": 2014
        },
        {
            "authors": [
                "M. Gheisarnejad",
                "H. Farsizadeh",
                "Tavana",
                "M.-R",
                "M.H. Khooban"
            ],
            "title": "A novel deep learning controller for dc\u2013dc buck\u2013boost converters in wireless power transfer feeding cpls",
            "venue": "IEEE Transactions on Industrial Electronics",
            "year": 2020
        },
        {
            "authors": [
                "C. Cui",
                "N. Yan",
                "C. Zhang"
            ],
            "title": "An intelligent control strategy for buck dcdc converter via deep reinforcement learning",
            "year": 2020
        },
        {
            "authors": [
                "G. Balta",
                "N. G\u00fcler",
                "N Altin"
            ],
            "title": "Modified fast terminal sliding mode control for dc-dc buck power converter with switching frequency regulation",
            "venue": "International Transactions on Electrical Energy Systems",
            "year": 2022
        },
        {
            "authors": [
                "J. Schulman",
                "F. Wolski",
                "P. Dhariwal",
                "A. Radford",
                "O. Klimov"
            ],
            "title": "Proximal policy optimization algorithms",
            "year": 2017
        },
        {
            "authors": [
                "B. Liu",
                "Q. Cai",
                "Z. Yang",
                "Z. Wang"
            ],
            "title": "Neural proximal/trust region policy optimization attains globally optimal policy",
            "venue": "arXiv preprint arXiv:1906.10306",
            "year": 2019
        },
        {
            "authors": [
                "M. Juneja",
                "S.K. Nagar"
            ],
            "title": "Particle swarm optimization algorithm and its parameters: A review",
            "venue": "International Conference on Control, Computing, Communication and Materials (ICCCCM), pp",
            "year": 2016
        },
        {
            "authors": [
                "M.I. Solihin",
                "L.F. Tack",
                "M.L. Kean"
            ],
            "title": "Tuning of pid controller using particle swarm optimization (pso)",
            "venue": "Proceeding of the International Conference on Advanced Science, Engineering and Information Technology,",
            "year": 2011
        },
        {
            "authors": [
                "X. Meng",
                "B. Song"
            ],
            "title": "Fast genetic algorithms used for pid parameter optimization",
            "venue": "IEEE International Conference on Automation and Logistics,",
            "year": 2007
        },
        {
            "authors": [
                "S. Katoch",
                "S.S. Chauhan",
                "V. Kumar"
            ],
            "title": "A review on genetic algorithm: past, present, and future",
            "venue": "Multimedia Tools and Applications",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "Keywords: Reinforcement Learning, Proximal Policy Optimization, Artificial Neural Network, Boost Converter Control\nar X\niv :2\n31 0.\n02 94\n5v 1\n[ ee\nss .S"
        },
        {
            "heading": "1 Introduction",
            "text": "The performance of power electronic systems, particularly boost converters, can be greatly improved by using proper control methods. The converter output voltage and current are regulated as part of these control schemes to provide the necessary power at the desired condition. Boost converters are widely utilised in a variety of industries, including telecommunications [1], electric vehicles [2], [3], and renewable energy systems [4], [5] where the main function is to step up the voltage level to fulfil the needed power demand at the specified condition. However, proper control circuit is essential for guaranteeing dependable performance, increasing efficiency, and extending system lifespan of the converter. Boost converters are controlled using several control techniques such as proportional integral derivative (PID) [6], [7], artificial neural network (ANN) [8], [9], model predictive control (MPC) [10], fuzzy logic [11], [12], adaptive neuro-fuzzy inference system (ANFIS) control, sliding mode control [13] etc.\nTraditional control techniques, such as PI control, have been widely used due to their simplicity and robustness. PI control works by comparing the output voltage or current with the reference value and adjusting the duty cycle of the converter switch accordingly. However, PI control is a linear control technique and may not always be optimal for improving the performance of nonlinear systems [14]. Another popular control algorithm is the adaptive neuro-fuzzy inference system (ANFIS), which is described in [15], [16]. ANFIS is a type of fuzzy logic control that uses a neural network to tune the fuzzy logic system parameters. ANFIS combines the benefits of fuzzy logic and neural networks to provide a more accurate and efficient control approach. Artificial neural networks (ANN) is another machine learning technique used in power electronics control [17], [18]. ANN is a type of supervised learning that uses a back propagation algorithm to optimize the control policy. ANN has shown promising results in improving the control of various power electronic systems, including boost converters, but to get an accurate model, it requires more training time and a slower speed of learning. Fuzzy logic control is another control technique that uses linguistic rules to control nonlinear systems. It works by defining the input and output variables in linguistic terms and using a set of fuzzy rules to map the inputs to the outputs. But there are few issues with fuzzy logic controllers. Fuzzy control methods rely on human ability and understanding. Defining specific fuzzy sets or membership functions takes time and effort.\nA potential solution to address these issues is the implementation of an intelligent controller capable of interacting with the system and learning to effectively handle both normal and abruptly changing system dynamics. By adapting and acquiring knowledge through interaction with the system, such intelligent controller can improve the control performance in various operating conditions. Reinforcement learning can be a valuable approach for addressing these challenges in boost converter control. Reinforcement learning is a machine learning technique that has gained significant attention in the field of power electronics control. It has been extensively explored in recent studies [19], [20], [21]. RL operates by learning an optimal control policy through iterative interaction with the environment. This approach has shown promising outcomes in improving the control of diverse nonlinear systems, including several power converters. RL is a model-free framework that is capable of solving optimum control problems.\nThe controller in the general feedback control structure receives information about the condition of the system and responds correspondingly. Similarly, the decision rule used in reinforcement learning (RL) is a control law known as the policy [22]. This policy governs the actions taken by the system based on state feedback. The state of the system is changed by the applied actuation, and when it does so, the transition to the updated state is assessed using a reward function. Increasing the cumulative reward from each initial state is the main objective of optimal control. The ultimate objective is to optimise the system\u2019s long-term performance because decision-making in this process is sequential. There are many reliable model-free RL methods. In this paper, a specific algorithm called proximal policy optimization (PPO) is implemented, which directly optimizes the policy parameters using observed data [22], [23].\nThe issue of voltage instability caused by a constant power load (CPL) in DCmicrogrids (MGs) has been extensively investigated and documented in existing literature [24], [25]. Over time, control techniques addressing this challenge have transitioned from model-based approaches to model-free methods. These techniques involve various controller systems, including traditional state feedback controllers, and more recently, the utilization of machine learning techniques. Popular power converter control strategies employed for buck-boost converters with constant power loads (CPLs) encompass state-feedback controllers [26], [27], proportional-integral-derivative (PID) control [28], [29], model predictive control (MPC) [30], [31], adaptive feedback controller [32] and sliding model control (SMC) [33], [34], [35]. Recent research endeavors focusing on voltage regulation and stabilization employing intelligent controllers have explored various techniques. These include the utilization of a quadratic D-stable fuzzy controller [36], fuzzy-PID control [37], as well as machine learning and reinforcement learning approaches such as deep deterministic policy gradient (DDPG) [38], and deep reinforcement learning (DRL) methods like markov decision process (MDP) and deep Q network (DQN) algorithms [39]. Additionally, a modified fast terminal sliding mode control (FTSMC) with a fixed switching frequency has been proposed to regulate the output voltage of DC-DC buck converters [40].\nIn this paper, an approach for boost converter control is proposed based on proximal policy optimization (PPO), which is a state-of-the-art reinforcement learning (RL) algorithm renowned for its successful implementation in various control applications. The performance of the PPO-based control approach is compared with traditional control techniques, including optimized proportional-integral (PI) control and artificial neural network (ANN) control. The effectiveness of the proposed control approach in improving the performance of the boost converter control system is evaluated through simulations using the MATLAB Simulink solver. To evaluate the reliability of the control performance, simulations are performed under both fixed and varying input conditions. It is observed that, in both scenarios, the step response characteristics remained consistently similar and almost better than other two control methods."
        },
        {
            "heading": "2 Boost Converter Model",
            "text": "The boost converter operates on the principle of complementary switching. It encompasses two complementary modes: the closed mode and the open mode. In the closed\nmode, energy is stored in the inductor while being released from the capacitor. Conversely, in the open mode, energy is released from the inductor while being stored in the capacitor. The operation of a boost converter relies on two fundamental principles, namely the energy balance principle and the charge balance principle, which ensure its ideal functioning. The circuit diagram of boost converter is shown in Fig. 1.\nThe energy balance that the input energy equals to the output energy.\nPin = Pout (1)\n\u21d2 Vin \u00d7 Iin = Vout \u00d7 Iout The charge balance implies the input charge equal to output charge.\nQin = Qout (2)\n\u21d2 Iin \u00d7 (1\u2212 d)\u00d7 T = Iout \u00d7 T Based on the charge balance and energy balance principles, the basic relationship between the input voltage and output voltage of boost converter is given by eqn. 3\nVo = Vin 1\u2212 d\n(3)\nThe averaging method is employed for the analysis and design of controllers in power electronics circuits. In order to utilize the averaging method for modeling purposes, the closed and open modal operations are represented as follows:\nx\u0307 = A1x+B1u\nx\u0307 = A2x+B2u\nThen the average state space model is-\nx\u0307 = Ax+Bu\nwhere, A = A1d+A2(1\u2212 d)\nB = B1d+B2(1\u2212 d)"
        },
        {
            "heading": "2.1 State Space Modelling",
            "text": "Initially, the state space models are derived for the open mode and closed mode. Subsequently, these two modes are combined utilizing the averaging method to obtain the average state space model. Circuit diagram for closed mode conduction is shown in Fig. 2:\nThe resulting dynamic state equations are given by:\nL diL dt = Vin (4)\nC dvc dt = \u2212vc R\n(5)\nwhere, iL = iin = the current of inductor vc = vout = the voltage of the capacitor\nLet the state variable, x1 = iL and x2 = vc output equation, y = x2 = vc Therefore, the state equation for closed mode state space can be written as: [\nx\u03071 x\u03072\n] = [ 0 0 0 \u2212 1RC ] [ x1 x2 ] + [ 1 L 0 ] Vin\nx\u0307 = A1x+B1u\nResulting dynamic state equation from Fig. 3 are given by-\nL diL dt = Vin \u2212 vc (6)\nC dvc dt = iL \u2212 vc R\n(7)\nTherefore, the state space equations for open mode state space can be written as-[ x\u03071 x\u03072 ] = [ 0 \u2212 1L 1 C \u2212 1 RC ] [ x1 x2 ] + [ 1 L 0 ] Vin\nx\u0307 = A2x+B2u\nThe averaging method combines the state space models of the two modes to obtain:\nx\u0307 = Ax+Bu\nwhere, A = A1d+A2(1\u2212 d)\nB = B1d+B2(1\u2212 d)\nx\u0307 =\n[ 0 \u2212 1\u2212dL\n1\u2212d C \u2212 1 RC\n] x+ [ 1 L 0 ] Vin"
        },
        {
            "heading": "3 Proposed Method",
            "text": ""
        },
        {
            "heading": "3.1 Reinforcement Learning",
            "text": "Reinforcement learning (RL) is a machine learning technique where an agent learns to make optimal decisions by continuously interacting with its environment and maximizing a reward signal. This approach takes inspiration from the learning mechanisms\nobserved in humans and animals, involving a trial-and-error process to acquire and improve decision-making skills. RL is applied in various domains where making effective decisions under challenging conditions is crucial. In practical scenarios, an agent encounters situations where it needs to make decisions,\nand it relies on feedback from the environment in the form of rewards or penalties to adjust and refine those decisions. To optimize the long-term cumulative reward, the agent must acquire a policy, a set of rules or strategies, that guides its decision-making process. Importantly, the agent does not receive explicit instructions or guidance on how to achieve this objective; instead, it learns by actively engaging with the environment, gaining knowledge through repeated attempts and learning from both successful and unsuccessful outcomes. Reinforcement learning (RL) is a framework devoid of explicit models, capable of solving optimal control problems. The controller inside the general feedback control structure gets feedback in the form of state signals from the plant and acts accordingly. In RL, the decision rule is denoted as a policy, which operates based on state feedback control principles [22]. The system\u2019s state is changed via actuation, and the subsequent transition to the new state is evaluated using a reward function. The goal of optimal control is to increase the total reward from each initial state. The purpose of this process, which involves sequential decision-making, is to optimise the system\u2019s long-term performance. Various robust model-free RL algorithms exist, and this paper focuses on a policy gradient method called proximal policy optimization (PPO) [22]. PPO directly optimizes policy parameters using observed data.\nIn this study, the environment corresponds to the DC-DC boost converter system. The action, at taken by the agent involves generating the gate pulse that regulates the converter\u2019s operation as shown in Fig. 4. The data analyzer block continuously monitors important parameters such as the output voltage, vout error (deviation from the desired value), e(t) and the rate of change of error, e\u2032(t). These monitored signals are processed as state, st\u22121 and reward, rt\u22121 and provided to the RL agent, which is shown in Fig. 4. Using the analyzed state, st\u22121 and reward information, rt\u22121 the RL agent makes informed decisions regarding the appropriate gate pulse to be applied. The agent leverages its learned policy to select an action, at that is expected to optimize the cumulative reward over time. Through a trial-and-error process, the agent explores different gate pulse actions and learns from the feedback received through the reward signal. By repeatedly adjusting its actions based on the observed outcomes, the agent adapts its decision-making strategy, gradually improving its control performance in regulating the boost converter."
        },
        {
            "heading": "3.2 Proximal Policy Optimization",
            "text": "The proximal policy optimization (PPO) algorithm is a model-free, online, or onpolicy reinforcement learning (RL) method. It updates the decision-making policy using small batches of experiences obtained from interacting with the environment. PPO keeps a balance between important factors like ease of implementation, tuning, sample complexity, and efficiency, while minimizing the deviation from the previous policy. It learns from online data and ensures low variance in training [23]. The PPO algorithm follows an iterative process where it alternates between two key steps: sampling data by interacting with the environment, and optimizing a clipped surrogate objective function using stochastic gradient ascent [41]. The algorithm ensures stability during agent training by utilizing the clipped surrogate objective function and constraining the magnitude of policy changes at each iteration [42]. This approach helps to prevent drastic policy updates and contributes to smoother and more reliable training of the agent.\nPPO is commonly implemented using the actor-critic Model, which consists of two deep neural networks - one for action selection (actor) and the other for reward evaluation (critic). In reinforcement learning, the actor network is responsible for decision-making by selecting actions based on the current state of the environment. Its goal is to optimize a policy that maps states to actions effectively. On the other hand, the critic network evaluates the actions chosen by the actor network and provides feedback on the value of state-action pairs. This feedback helps the actor network refine and improve its policy.\nThe objective of the actor-critic network is to optimize the surrogate objective function, L(\u03b8) with the aim of maximizing performance.\nL(\u03b8) = E \u2032 t[min(rt(\u03b8))A \u2032 t, clip(rt(\u03b8), 1\u2212 \u2208, 1+ \u2208)A \u2032 t] (8)\nThis function represents the expectation of the advantage function, where the advantage function is dependent on the estimated advantage At, the policy parameters \u03b8,\nand the probability ratio rt(\u03b8).\nrt(\u03b8) = \u03c0\u03b8 (at | st)\n\u03c0\u03b8old (at | st) (9)\nThe probability ratio, rt(\u03b8) in the context of the actor-critic network refers to the comparison between the likelihood of taking a specific action at when in state st at time t, based on the current policy parameters \u03c0\u03b8, and the likelihood of taking the same action at in the same state st at time t, utilizing the past or previous policy parameters \u03b8old from the previous epoch. During training, the PPO-based RL agent learns by sampling actions according to its updated policy. This policy starts with random actions to explore the state-action space and gradually becomes more focused on actions that result in higher rewards. The PPO agent determines the probabilities for each action in its set of possible actions. It randomly chooses an action using these probabilities. The agent updates its actor and critic properties using mini-batches of data over multiple training sessions while interacting with the environment. The objective of the PPO agent is to train the coefficients of the actor-critic neural networks to minimize the difference between the desired output Vref and the actual value Vout.\nThe general algorithmic structure of the PPO algorithm [21], [23] is as follows: The PPO algorithm follows a specific set of steps. Initially, the parameters of the actor-critic network are initialized. Next, a sequence of experiences is generated, consisting of state-action pairs and their corresponding reward values. For each time instance, the action-value function, Q\u03c0 and advantage function, A\u03c0 are calculated.\nThe action-value function represents the expected return when starting from a particular state and taking a specific action according to the policy. It is computed as the sum of the expected rewards associated with the state-action pair. On the other hand, the value function estimates the expected return of being in a particular state and reflects its desirability. It is computed as the sum of the expected rewards given the state as shown in algorithm 1. The advantage function, A\u03c0 captures the difference between the action-value function and the value function. It provides insights into the advantage or disadvantage of choosing a specific action in a given state. During the training process, the PPO algorithm learns from a set of mini-batch experiences over a specified number of epochs, denoted as K. The critic network\u2019s parameters are updated by minimizing the critic loss function, denoted as Lc, which aims to minimize the loss over the sampled mini-batch of experiences.\nOn the other hand, the actor network\u2019s parameters are updated by repeating a series of steps until a terminating criterion is met. These steps involve maximizing a surrogate objective function that balances the exploration and exploitation tradeoff. The objective is to find the policy that maximizes the expected reward. The training process continues iteratively, with the critic and actor networks being updated based on their respective loss functions and objectives. This iterative process allows the PPO algorithm to progressively refine the actor-critic networks and improve the overall decision-making capabilities. The training continues until a specific termination criterion, such as reaching a maximum number of iterations or achieving a desired level of performance, is met. At this point, the PPO algorithm concludes its training process.\nAlgorithm 1: Proximal Policy Optimization Algorithm\nInitialize the parameter of actor-critic network while termination criteria is not satisfied do\nStep 1: Generate N experiences{ st1 , at1 , rt1 } , { st2 , at2 , rt2 } ... { stn , atn , rtn } ; Step 2: Calculate action-value function and advantage function at each time step t,\nQ\u03c0(s, a) = \u2211 t E\u03c0\u03b8[R(st, at)|s, a]\nV\u03c0(s) = \u2211 t E\u03c0\u03b8[R(st, at)|s]\nA\u03c0(s, a) = Q\u03c0(s, a)\u2212V\u03c0(s) while k \u0338= epoch do\n1. The critic network\u2019s parameters are updated by:\nLc(\u03b8v) = 1\nM M\u2211 t=1 (Q\u03c0(s, a)\u2212 V (s|\u03b8v))2\n2. The actor network\u2019s parameters are updated by:\nLa(\u03b8v) = \u2212 1\nM [min(rt(\u03b8)At, clip(rt(\u03b8), 1\u2212 \u03f5, 1 + \u03f5)At)]\nend\nend"
        },
        {
            "heading": "3.3 PI Control",
            "text": "The proportional integral (PI) controller is a feedback control loop that computes an error signal by determining the disparity between the system\u2019s output and a reference or desired value. This algorithm continuously monitors the output voltage and adjusts the duty cycle of the converter to maintain the desired output voltage level. In simpler terms, it keeps a close eye on the output voltage and makes necessary changes to ensure it stays at the right level.\nThe PI controller employs a feedback control loop mechanism to mitigate the impact of disturbances in a system, guide the system towards a desired state, and establish clear relationships between system variables. It takes the error at a specific time, e(t), as its input, which indicates the disparity between the measured and reference values. The PI controller generates an output termed as \u201dactuation\u201d, a(t), which specifies the action to be taken for the considered plant or system. The actuation, a(t), is calculated as the sum of two components: the proportional gain (kp) multiplied by the magnitude of the error, and the integral gain (ki) multiplied by the integral of the\nerror over time as shown in Fig. 5. The action, a(t) can be written as,\na(t) = kp \u2217 e(t) + ki \u222b t 0 e(t)dt (10)\nThe PI controller generates a control signal, a(t) at time t by summing the proportional (P) and integral (I) terms. The control signal exhibits a proportional increase in response to the error, aiming to reduce the steady-state error. This error correction is based on the current steady-state error. The proportional gain (kp) alone can lead to oscillations due to quick reactions. To address this, the integral gain (ki) contributes to the control signal based on accumulating past steady-state errors, helping to reduce steady-state errors over time."
        },
        {
            "heading": "3.4 PI Controller Optimization",
            "text": ""
        },
        {
            "heading": "3.4.1 Particle Swarm Optimization",
            "text": "The particle swarm optimization (PSO) algorithm is an optimization algorithm inspired by swarm behaviour observed in nature, such as fish schooling and bird flocking [43], [44]. Introduced in 1995, particle swarm optimization (PSO) has evolved to enhance it\u2019s performance. PSO uses particles to iteratively adapt flight based on personal and social experiences.. The best previous position (pbest) for each particle and the best position among all particles (gbest) are tracked. The particles are updated based on equations that involve velocity and position adjustments. These updates consider the particle\u2019s previous velocity, its distances from its best experience and the group\u2019s best experience. The algorithm\u2019s performance is measured using a predefined fitness function which is mean absolute error (MAE) used in this work. An inertia weight, introduced to balance global and local search capabilities, is incorporated into the equations. The velocity of each particle at time t, denoted as vi, is calculated according to equations (11) and position xi is calculated using (12). As a result, the\nparticles are updated based on these equations.\nvt+1i = w.v t i + c1 \u2217 u1 \u2217 (pbesti \u2212 x t i) + c2 \u2217 u2 \u2217 (gbest \u2212 xti) (11)\nxt+1i = x t i + v t+1 i (12)\nIn equation (11), the particle\u2019s new velocity is calculated based on its previous velocity, the distances between its current position and its own best experience, and the group\u2019s best experience. This update guides the particle to fly towards a new position, as described in equation (12). The performance of each particle is evaluated using mean absolute error (MAE). The inertia weight, denoted as w is introduced to balance the trade-off between global and local search capabilities. It can be a positive constant or a time-dependent function, allowing for adjustments during the optimization process.\nThe primary goal of employing the Particle Swarm Optimization (PSO) algorithm in this context is to obtain the optimized values of kp and ki. These control parameters are crucial for the effective regulation and control of the boost converter. By leveraging PSO, we aim to determine the optimal values of kp and ki that will enable the PI controller to achieve efficient and accurate control over the boost converter system. The implementation begins by defining the objective function as the mean absolute error (MAE), which serves as a performance metric for evaluating solution quality. The problem dimension is specified as having two variables (kp and ki). Lower bounds and upper bounds are set to constrain the search space. The particle swarm optimization (PSO) algorithm is implemented using the particleswarm function in MATLAB, incorporating parameters such as swarm size, maximum iterations, display options, and plot function. The PSO algorithm is executed by invoking the particleswarm function with the objective function, number of variables, lower and upper bounds, and the designated options. Finally, the optimized values of the variables are assigned to specific parameters (kp and ki)."
        },
        {
            "heading": "3.4.2 Genetic Algorithm",
            "text": "genetic algorithm (GA) is an optimization method inspired by natural selection and genetics [45], [46]. GA uses parallel and random search to find high-performing solutions. It employs reproduction, crossover, and mutation operators for optimization. GA is effective for complex non-linear and non-convex problems.\nThe flowchart of the genetic algorithm (GA) is illustrated in Fig. 6. The fitness function employed in this algorithm is the mean absolute error (MAE). The MAE serves as a quantitative measure to evaluate the quality or fitness of each potential solution generated by the GA. The GA commences by initializing a population of potential solutions. Following the initialization step, the GA progresses through iterative generations, aiming to improve the quality of the solutions. The iterative process of generation, selection, crossover, and mutation continues until a termination criterion is met.\nThe methodology employed in this study utilizes the genetic algorithm (GA) to optimize the values of two control parameters, namely kp and ki, that are vital for regulating a dc-dc boost converter. The GA is implemented using the ga function in\nMATLAB with carefully chosen parameters and constraints. The objective function, mean absolute error (MAE) is employed as a measure of fitness to guide the optimization process. The GA is configured by specifying the population size, maximum number of generations, display options, and a plot function using the optimoptions function. To initiate the optimization process, the ga function is invoked, providing the objective function, number of variables, lower and upper bounds for the variables. The resulting optimized solution obtained represents the values of kp and ki."
        },
        {
            "heading": "3.5 ANN Control",
            "text": "A DC-DC boost converter equipped with artificial neural network (ANN) control is a power converter that raises the voltage of a DC input source using an advanced control technique based on neural network technology. The ANN control method emulates the behavior of a human neural network, allowing the converter to adapt and improve its performance in real-time. To ensure optimal operation, the ANN control algorithm\ncontinually monitors both the input of the converter and the desired output, adjusting its control signals accordingly, as illustrated in Fig. 7.\nIn the process of developing an artificial neural network (ANN) model, a significant amount of data of (100000 data samples), is generated using the MATLAB editor, forming the dataset. The dataset consists of data samples that include input voltage and output voltage, which serve as the input features for the ANN model. For training purpose 80% data is used and rest of the data (20%) is used for testing purpose. Subsequently, the neural network is constructed and trained using MATLAB\u2019s graphical user interfaces (GUIs) specifically designed for NN applications. These GUIs facilitate the creation and training of the Neural Network after the dataset has been established within the MATLAB workspace. Enhancing the performance of the neural network (NN) model involves fine-tuning the training parameters. The specifications of the NN model are presented in TABLE 1. Once the NN model is trained, it is exported to the MATLAB workspace and subsequently transformed into a Simulink block. This Simulink block governs the duty cycle of the boost converter, exerting control over its operation."
        },
        {
            "heading": "4 Result",
            "text": ""
        },
        {
            "heading": "4.1 Experiment",
            "text": "The design specification of DC-DC boost converter used in this work is shown in Table 2. In this paper, three control methods are evaluated namely optimized PI, ANN and RL. Two experiments are conducted to examine the controller\u2019s performance under different conditions. In the first case, the input voltage is held constant at 24 volts. In the second case, the input voltage is varied between 24 volts and 26 volts to assess the controller\u2019s ability to handle varying input behavior. To introduce the input variation, a step change is applied at 0.5 seconds.\nThe RL agent maintains consistent performance across three different reference voltage cases: 48V, 54V, and 60V. The simulation duration is set to 1 seconds, with a sample time of 0.0002 seconds. The specific parameters of the PPO algorithm and the actor-critic networks used in the experiments can be found in Table 3. These parameters governed the training and operation of the RL agent in regulating the\nboost converter. The error value at each sample time (t) in the PPO implementation is defined as the difference between the reference voltage value (Vref ) and the output voltage (Vout).\nTo construct the state vector (st) at each sample time step, the PPO agent calculates the output voltage (Vout), the error value (et), and the rate of change of the error (e\u2032t). During the training of the RL agent, a fixed number of sample steps are used unless the termination criterion is met. If the output voltage exceeds the upper limit (Vup) or greater than Vref and then crossing the lower limit (Vlow), the training for that episode is terminated. This logic can be considered as a limiting mechanism that help to reach and stay at desired voltage level. At each sample time step (t) during training, the RL agent takes the state, (st\u22121) and the reward value, (rt\u22121) as inputs to generate the next action, at as shown in Fig. 4. The reward function, specified in algorithm 2, determines the reward value based on the current state and desired outcomes, guiding the agent\u2019s learning process."
        },
        {
            "heading": "4.2 Experimental result",
            "text": "In this work the PI control parameters (kp and ki) as optimized using PSO and GA algorithm. The design parameters of PI controller after optimizations are shown in Table 4. In the case of a fixed input voltage, three control methods, namely RL, ANN, and optimized PI, are evaluated and compared. The evaluation focuses on key performance metrics obtained from analyzing the step response characteristics of the system. These metrics included parameters such as rise time, settling time, overshoot, and undershoot.\nBy observing and analyzing the step response characteristics, important insights are gained into the dynamic behavior and performance of each control method. The rise time, which measures the time taken for the system\u2019s output to reach a specified percentage of its final value, provides information about the speed of response. The settling time, representing the time required for the system\u2019s output to stabilize within a specified tolerance band around the desired value, indicates the system\u2019s ability to achieve steady-state operation. Overshoot and undershoot, on the other hand, provides insights into the presence and magnitude of any oscillatory behavior or deviation from the desired response. By considering these evaluation metrics, a comprehensive understanding of the strengths and weaknesses of each control method is obtained, enabling informed comparisons and the identification of the most effective approach for regulating the boost converter under fixed input voltage conditions.\nAlgorithm 2: Reward calculation\nVout \u2190 Output Voltage Vref \u2190 Desired Output eth \u2190 Error Threshold Vup \u2190 Upper Limit Vlow \u2190 Lower Limit\nOptimization kp ki\nPSO 0.002 0.315"
        },
        {
            "heading": "GA 0.0021 0.314",
            "text": "When the input voltage remains constant, the optimized PI controller consistently achieves the shortest settling time compared to other control methods. Interestingly, for the PI controller, the settling time decreases as the desired voltage increases as shown in Fig. 8.\nThe quantitative information of the settling time for different control methods under fixed input voltage conditions is presented in Table 5. The optimized PI controller consistently exhibited the shortest settling time across various scenarios, with the settling time decreasing as the desired voltage increased. This trend indicates the controller\u2019s ability to achieve faster stabilization as the system becomes more aggressive in its response. This behavior can be attributed to the inherent dynamics and characteristics of the system being controlled. This could be due to the system becoming more responsive and active as the desired voltage increased.\nHowever, it\u2019s important to note that this behavior may not be the same in all cases and can depend on the system\u2019s characteristics and control parameters chosen. Different trends are observed when using RL and ANN control methods as depicted in Fig. 8. Among the RL and ANN control methods, the boost converter with RL control performed the better in terms of settling time which is quite evident in Table\n6 and 7. This is because RL has unique learning and decision-making abilities that allow it to adapt and optimize the control policy according to the specific needs and dynamics of the boost converter system.\nWhen the input voltage is varied, the step response characteristics of the system are observed to remain relatively consistent for both ANN control and RL control methods depicted in Fig. 9. This implies that these methods are capable of adapting to the varying input voltage and maintaining stable step response characteristics.\nHowever, in the case of PI control, an undesired voltage wave shapes are evident in Fig. 9. The step response characteristics are significantly degraded, indicating that the\nPI control method struggled to handle the input voltage variation effectively. Quantitatively comparing the performance of these control methods, RL control emerged as the superior method as depicted in Tables 9 and 10. It exhibited the ability to seamlessly handle the input voltage variation and maintain step response characteristics similar to those observed under fixed input voltage conditions. This quality positions RL control as the best control method among the three in terms of adapting to varying input voltage and preserving stable step response characteristics. The RL control method\u2019s success can be attributed to its capacity to learn and optimize the control policy based on the specific dynamics and requirements of the boost converter system. This adaptability allows RL control to consistently deliver reliable and robust performance, even in the presence of changing input voltage conditions."
        },
        {
            "heading": "4.3 Discussion",
            "text": "The struggle of the PI control method in handling the varying input voltage can be attributed to its inherent limitations and design characteristics. PI control relies on a fixed set of proportional and integral gains to regulate the system\u2019s output.\nWhen the input voltage varies, the system dynamics change, and the fixed gains may no longer be optimal for achieving desired performance. The PI control method lacks the adaptability and flexibility to effectively adjust its parameters in response to changing conditions. As a result, it struggles to accurately track the desired output and maintain stable step response characteristics when the input voltage varies. The undesired voltage waveshape observed in the step response is a clear indication of this struggle.\nIn contrast, RL control and ANN control methods have the advantage of learning and adapting their control policies based on the observed system behavior. This allows them to dynamically adjust their actions and responses, leading to improved performance and the ability to handle input voltage variations more effectively.\nOverall, the limitations of fixed gains and lack of adaptability make the PI control method less suited for applications where the input voltage can vary. Alternative control methods that offer more flexibility and adaptive capabilities, such as RL and ANN control, are better equipped to handle such dynamic scenarios."
        },
        {
            "heading": "5 Conclusion",
            "text": "In summary, the comparative analysis revealed that the reinforcement learning (RL) control method almost outperforms than other methods in fixed and variable input\ncondition, exhibiting superior performance in terms of step response characteristics. Noticeably, PI control method with particle swarm optimization (PSO) and genetic algorithm (GA) optimization exhibited similar performance in both cases. In case of ANN control the system maintains an adequate performance, showing the significance of artificial intelligence in converter control systems. Overall, the RL control method emerged as the preferred choice among the boost converter controllers, as it maintained consistent and better step response characteristics in both fixed and varying input voltage cases. These findings underscore the significance of selecting the appropriate control method and optimizing the controllers for achieving optimal control system performance."
        }
    ],
    "title": "Proximal Policy Optimization-Based Reinforcement Learning Approach for DC-DC Boost Converter Control: A Comparative Evaluation Against Traditional Control Techniques",
    "year": 2023
}