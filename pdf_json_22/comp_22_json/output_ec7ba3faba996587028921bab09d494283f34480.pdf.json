{
    "abstractText": "The COVID-19 outbreak has resulted in the loss of human life worldwide and has increased worry concerning life, public health, the economy, and the future. With lockdown and social distancing measures in place, people turned to social media such as Twitter to share their feelings and concerns about the pandemic. Several studies have focused on analyzing Twitter users\u2019 sentiments and emotions. However, little work has focused on worry detection at a fine-grained level due to the lack of adequate datasets. Worry emotion is associated with notions such as anxiety, fear, and nervousness. In this study, we built a dataset for worry emotion classification called \u201cWorryCov\u201d. It is a relatively large dataset derived from Twitter concerning worry about COVID-19. The data were annotated into three levels (\u201cnoworry\u201d, \u201cworry\u201d, and \u201chigh-worry\u201d). Using the annotated dataset, we investigated the performance of different machine learning algorithms (ML), including multinomial Na\u00efve Bayes (MNB), support vector machine (SVM), logistic regression (LR), and random forests (RF). The results show that LR was the optimal approach, with an accuracy of 75%. Furthermore, the results indicate that the proposed model could be used by psychologists and researchers to predict Twitter users\u2019 worry levels during COVID-19 or similar crises. Keywords\u2014COVID-19; sentiment analysis; emotion analysis; worry dataset; concern analysis",
    "authors": [
        {
            "affiliations": [],
            "name": "Tahani Soud Alharbi"
        },
        {
            "affiliations": [],
            "name": "Fethi Fkih"
        }
    ],
    "id": "SP:b8850804a0556db8096f92c7290f11fc669f0104",
    "references": [
        {
            "authors": [
                "C. Sohrabi"
            ],
            "title": "World Health Organization declares global emergency: A review of the 2019 novel coronavirus (COVID-19)",
            "venue": "International Journal of Surgery, vol. 76. Elsevier Ltd, pp. 71\u201376, Apr. 01, 2020. doi: 10.1016/j.ijsu.2020.02.034.",
            "year": 2020
        },
        {
            "authors": [
                "S. Li",
                "Y. Wang",
                "J. Xue",
                "N. Zhao",
                "T. Zhu"
            ],
            "title": "The impact of covid-19 epidemic declaration on psychological consequences: A study on active weibo users",
            "venue": "Int J Environ Res Public Health, vol. 17, no. 6, Mar. 2020, doi: 10.3390/ijerph17062032.",
            "year": 2020
        },
        {
            "authors": [
                "S. Zhang"
            ],
            "title": "The COVID-19 Pandemic and Mental Health Concerns on Twitter in the United States",
            "venue": "Health Data Science, vol. 2022, pp. 1\u2013 9, Feb. 2022, doi: 10.34133/2022/9758408.",
            "year": 2022
        },
        {
            "authors": [
                "V. Sideropoulos",
                "H. Kye",
                "D. Dukes",
                "A.C. Samson",
                "O. Palikara",
                "J. van Herwegen"
            ],
            "title": "Anxiety and Worries of Individuals with Down Syndrome During the COVID-19 Pandemic: A Comparative Study in the UK",
            "venue": "J Autism Dev Disord, 2022, doi: 10.1007/s10803-022-05450- 0.",
            "year": 2022
        },
        {
            "authors": [
                "X. Ji",
                "S.A. Chun",
                "J. Geller"
            ],
            "title": "Monitoring public health concerns using twitter sentiment classifications",
            "venue": "Proceedings - 2013 IEEE International Conference on Healthcare Informatics, ICHI 2013, no. September, pp. 335\u2013344, 2013, doi: 10.1109/ICHI.2013.47.",
            "year": 2013
        },
        {
            "authors": [
                "U. Qazi",
                "M. Imran",
                "F. Ofli"
            ],
            "title": "GeoCoV19: A Dataset of Hundreds of Millions of Multilingual COVID-19 Tweets with Location Information",
            "venue": "SIGSPATIAL Special, vol. 12, no. 1, pp. 6\u201315, Jun. 2020, doi: 10.1145/3404820.3404823.",
            "year": 2020
        },
        {
            "authors": [
                "L. Sinnenberg",
                "A.M. Buttenheim",
                "K. Padrez",
                "C. Mancheno",
                "L. Ungar",
                "R.M. Merchant"
            ],
            "title": "Twitter as a tool for health research: A systematic review",
            "venue": "American Journal of Public Health, vol. 107, no. 1. American Public Health Association Inc., pp. e1\u2013e8, Jan. 01, 2017. doi: 10.2105/AJPH.2016.303512.",
            "year": 2017
        },
        {
            "authors": [
                "Q. Yang"
            ],
            "title": "SenWave: Monitoring the Global Sentiments under the COVID-19 Pandemic",
            "venue": "pp. 1\u201314, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "A.H. Alamoodi"
            ],
            "title": "Sentiment analysis and its applications in fighting COVID-19 and infectious diseases: A systematic review",
            "venue": "Expert Systems with Applications, vol. 167. Elsevier Ltd, Apr. 01, 2021. doi: 10.1016/j.eswa.2020.114155.",
            "year": 2021
        },
        {
            "authors": [
                "A.S. Imran",
                "S.M. Daudpota",
                "Z. Kastrati",
                "R. Batra"
            ],
            "title": "Cross-cultural polarity and emotion detection using sentiment analysis and deep learning on covid-19 related tweets",
            "venue": "IEEE Access, vol. 8, pp. 181074\u2013 181090, 2020, doi: 10.1109/ACCESS.2020.3027350.",
            "year": 1810
        },
        {
            "authors": [
                "B. Kleinberg",
                "I. van der Vegt",
                "M. Mozes"
            ],
            "title": "Measuring Emotions in the COVID-19 Real World Worry Dataset",
            "venue": "vol. 1, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "X. Li",
                "M. Zhou",
                "J. Wu",
                "A. Yuan",
                "F. Wu",
                "J. Li"
            ],
            "title": "Analyzing COVID- 19 on Online Social Media: Trends, Sentiments and Emotions",
            "venue": "May 2020.",
            "year": 2020
        },
        {
            "authors": [
                "R. Verma",
                "C. von der Weth",
                "J. Vachery",
                "M. Kankanhalli"
            ],
            "title": "Identifying Worry in Twitter: Beyond Emotion Analysis",
            "venue": "pp. 72\u201382, 2020, doi: 10.18653/v1/2020.nlpcss-1.9.",
            "year": 2020
        },
        {
            "authors": [
                "R.A. Faisal",
                "M.C. Jobe",
                "O. Ahmed",
                "T. Sharker"
            ],
            "title": "Replication analysis of the COVID-19 Worry Scale",
            "venue": "Death Stud, vol. 46, no. 3, pp. 574\u2013580, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "P.J. Schulz",
                "E.M. Andersson",
                "N. Bizzotto",
                "M. Norberg"
            ],
            "title": "Using Ecological momentary assessment to study the development of COVid- 19 worries in Sweden: Longitudinal study",
            "venue": "J Med Internet Res, vol. 23, no. 11, p. e26743, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "B. Mandel",
                "A. Culotta",
                "J. Boulahanis",
                "D. Stark",
                "B. Lewis",
                "J. Rodrigue"
            ],
            "title": "A demographic analysis of online sentiment during Hurricane Irene",
            "venue": "Proceedings of the 2012 Workshop on Language in Social Media, no. Lsm, pp. 27\u201336, 2012.",
            "year": 2012
        },
        {
            "authors": [
                "X. Ji",
                "S.A. Chun",
                "Z. Wei",
                "J. Geller"
            ],
            "title": "Twitter sentiment classification for measuring public health concerns",
            "venue": "Soc Netw Anal Min, vol. 5, no. 1, pp. 1\u201325, 2015, doi: 10.1007/s13278-015-0253-5.",
            "year": 2015
        },
        {
            "authors": [
                "J. Pei",
                "G. Yu",
                "X. Tian",
                "M.R. Donnelley"
            ],
            "title": "A new method for early detection of mass concern about public health issues",
            "venue": "J Risk Res, vol. 20, no. 4, pp. 516\u2013532, Apr. 2017, doi: 10.1080/13669877.2015.1100655. 0.69 0.75 0.78 0.72 0.74 0.73 0.73 0.6 0.71 0.74 0.73 0.73 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 MNB SVM LR RF no-worry worry high-worry 0.76 0.79 0.8 0.74 0.66 0.69 0.69 0.61 0.72 0.75 0.75 0.69 0 0.2 0.4 0.6 0.8 1 MNB SVM LR RF no-worry worry high-worry (IJACSA) International Journal of Advanced Computer Science and Applications, Vol. 13, No. 8, 2022 652 | P a g e www.ijacsa.thesai.org",
            "year": 2017
        },
        {
            "authors": [
                "A. Abd-Alrazaq",
                "D. Alhuwail",
                "M. Househ",
                "M. Hai",
                "Z. Shah"
            ],
            "title": "Top concerns of tweeters during the COVID-19 pandemic: A surveillance study",
            "venue": "J Med Internet Res, vol. 22, no. 4, Apr. 2020, doi: 10.2196/19016.",
            "year": 2020
        },
        {
            "authors": [
                "M Et al. Song",
                "L Emilsson",
                "SR Bozorg",
                "LH Nguyen",
                "AD Joshi",
                "K Staller"
            ],
            "title": "Understanding Weekly COVID-19 Concerns through Dynamic Content-Specific LDA Topic Modeling",
            "venue": "Lancet Gastroenterol Hepatol, vol. 5, no. 6, pp. 537\u2013547, 2020, doi: 10.18653/v1/2020.nlpcss- 1.21.Understanding.",
            "year": 2020
        },
        {
            "authors": [
                "S.A. Chun",
                "A.C.Y. Li",
                "A. Toliyat",
                "J. Geller"
            ],
            "title": "Tracking citizen\u2019s concerns during COVID-19 pandemic",
            "venue": "ACM International Conference Proceeding Series, Jun. 2020, pp. 322\u2013323. doi: 10.1145/3396956.3397000.",
            "year": 2020
        },
        {
            "authors": [
                "N.A. Hasanah",
                "N. Suciati",
                "D. Purwitasari"
            ],
            "title": "Identifying degree-ofconcern on covid-19 topics with text classification of twitters",
            "venue": "Register: Jurnal Ilmiah Teknologi Sistem Informasi, vol. 7, no. 1, pp. 50\u201362, 2021, doi: 10.26594/register.v7i1.2234.",
            "year": 2021
        },
        {
            "authors": [
                "O.T. Aduragba",
                "J. Yu",
                "A.I. Cristea",
                "L. Shi"
            ],
            "title": "Detecting Fine- Grained Emotions on Social Media during Major Disease Outbreaks: Health and Well-being before and during the COVID-19 Pandemic",
            "venue": "AMIA Annu Symp Proc, vol. 2021, pp. 187\u2013196, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "J.H. Shen",
                "F. Rudzicz"
            ],
            "title": "Detecting Anxiety through Reddit",
            "venue": "Association for Computational Linguistics, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "D. Owen",
                "J.C. Collados",
                "L. Espinosa-Anke"
            ],
            "title": "Towards Preemptive Detection of Depression and Anxiety in Twitter",
            "venue": "Nov. 2020.",
            "year": 2020
        },
        {
            "authors": [
                "S.M. Mohammad",
                "F. Bravo-Marquez",
                "M. Salameh",
                "S. Kiritchenko"
            ],
            "title": "SemEval-2018 Task 1: Affect in Tweets",
            "venue": "NAACL HLT 2018 - International Workshop on Semantic Evaluation, SemEval 2018 - Proceedings of the 12th Workshop, pp. 1\u201317, 2018, doi: 10.18653/v1/s18-1001.",
            "year": 2018
        },
        {
            "authors": [
                "J. Roesslein"
            ],
            "title": "Tweepy: Twitter for Python",
            "venue": "URL: https://github.com/tweepy/tweepy, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "U. Naseem",
                "I. Razzak",
                "M. Khushi",
                "P.W. Eklund",
                "J. Kim"
            ],
            "title": "COVIDSenti: A Large-Scale Benchmark Twitter Data Set for COVID- 19 Sentiment Analysis",
            "venue": "IEEE Trans Comput Soc Syst, vol. 8, no. 4, pp. 976\u2013988, Aug. 2021, doi: 10.1109/TCSS.2021.3051189.",
            "year": 2021
        },
        {
            "authors": [
                "Y.S. Malik"
            ],
            "title": "Coronavirus Disease Pandemic (COVID-19): Challenges and a Global Perspective",
            "venue": "Pathogens, vol. 9, no. 7, 2020, doi: 10.3390/pathogens9070519.",
            "year": 2020
        },
        {
            "authors": [
                "M. Saif",
                "S. Kiritchenko"
            ],
            "title": "Understanding emotions: A dataset of tweets to study interactions between affect categories",
            "venue": "Proceedings of the Eleventh International Conference on Language Resources and Evaluation, Miyazaki, Japan, 2018, pp. 7\u201312.",
            "year": 2018
        },
        {
            "authors": [
                "T. Sosea",
                "C. Pham",
                "A. Tekle",
                "C. Caragea",
                "J.J. Li"
            ],
            "title": "Emotion analysis and detection during COVID-19",
            "venue": "ArXiv, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "S.M. Mohammad",
                "F. Bravo-Marquez"
            ],
            "title": "Emotion intensities in tweets",
            "venue": "arXiv preprint arXiv:1708.03696, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "D. Ramyachitra",
                "P. Manikandan"
            ],
            "title": "Imbalanced dataset classification and solutions: a review",
            "venue": "International Journal of Computing and Business Research (IJCBR), vol. 5, no. 4, pp. 1\u201329, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "S. Kusal",
                "S. Patil",
                "K. Kotecha",
                "R. Aluvalu",
                "V. Varadarajan"
            ],
            "title": "Ai based emotion detection for textual big data: Techniques and contribution",
            "venue": "Big Data and Cognitive Computing, vol. 5, no. 3, 2021, doi: 10.3390/bdcc5030043.",
            "year": 2021
        },
        {
            "authors": [
                "N. Babanejad",
                "A. Agrawal",
                "A. An",
                "M. Papagelis"
            ],
            "title": "A Comprehensive Analysis of Preprocessing for Word Representation Learning in Affective Tasks",
            "venue": "pp. 5799\u20135810, 2020, doi: 10.18653/v1/2020.aclmain.514.",
            "year": 2020
        },
        {
            "authors": [
                "U. Naseem",
                "I. Razzak",
                "S.K. Khan",
                "M. Prasad"
            ],
            "title": "A Comprehensive Survey on Word Representation Models: From Classical to State-of-the- Art Word Representation Language Models",
            "venue": "ACM Trans. Asian Low- Resour. Lang. Inf. Process., vol. 20, no. 5, pp. 1\u201346, Jun. 2021, doi: 10.1145/3434237.",
            "year": 2021
        },
        {
            "authors": [
                "K. Kowsari",
                "K.J. Meimandi",
                "M. Heidarysafa",
                "S. Mendu",
                "L. Barnes",
                "D. Brown"
            ],
            "title": "Text classification algorithms: A survey",
            "venue": "Information (Switzerland), vol. 10, no. 4, pp. 1\u201368, 2019, doi: 10.3390/info10040150.",
            "year": 2019
        },
        {
            "authors": [
                "S. Adhikari"
            ],
            "title": "Exploiting linguistic information from Nepali transcripts for early detection of Alzheimer\u2019s disease using natural language processing and machine learning techniques",
            "venue": "International Journal of Human Computer Studies, vol. 160, Apr. 2022, doi: 10.1016/j.ijhcs.2021.102761.",
            "year": 2022
        },
        {
            "authors": [
                "E. Loper",
                "S. Bird"
            ],
            "title": "NLTK: The Natural Language Toolkit",
            "venue": "2002. [Online]. Available: http://nltk.sf.net/.",
            "year": 2002
        },
        {
            "authors": [
                "H. Du"
            ],
            "title": "Twitter vs News: Concern analysis of the 2018 California wildfire event",
            "venue": "Proceedings - International Computer Software and Applications Conference, vol. 2, pp. 207\u2013212, 2019, doi: 10.1109/COMPSAC.2019.10208.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Didi",
                "A. Walha",
                "A. Wali"
            ],
            "title": "COVID-19 Tweets Classification Based on a Hybrid Word Embedding Method",
            "venue": "Big Data and Cognitive Computing, vol. 6, no. 2, p. 58, 2022, doi: 10.3390/bdcc6020058.",
            "year": 2022
        },
        {
            "authors": [
                "K.S. Kalaivani",
                "S. Uma",
                "C.S. Kanimozhiselvi"
            ],
            "title": "A Review on Feature Extraction Techniques for Sentiment Classification",
            "venue": "Proceedings of the 4th International Conference on Computing Methodologies and Communication, ICCMC 2020, no. Iccmc, pp. 679\u2013 683, 2020, doi: 10.1109/ICCMC48092.2020.ICCMC-000126.",
            "year": 2020
        },
        {
            "authors": [
                "R. Ahuja",
                "A. Chug",
                "S. Kohli",
                "S. Gupta",
                "P. Ahuja"
            ],
            "title": "The impact of features extraction on the sentiment analysis",
            "venue": "Procedia Comput Sci, vol. 152, pp. 341\u2013348, 2019, doi: 10.1016/j.procs.2019.05.008.",
            "year": 2019
        },
        {
            "authors": [
                "O. Kramer"
            ],
            "title": "Scikit-learn",
            "venue": "Machine learning for evolution strategies, Springer, 2016, pp. 45\u201353.",
            "year": 2016
        }
    ],
    "sections": [
        {
            "text": "645 | P a g e\nwww.ijacsa.thesai.org\nhuman life worldwide and has increased worry concerning life, public health, the economy, and the future. With lockdown and social distancing measures in place, people turned to social media such as Twitter to share their feelings and concerns about the pandemic. Several studies have focused on analyzing Twitter users\u2019 sentiments and emotions. However, little work has focused on worry detection at a fine-grained level due to the lack of adequate datasets. Worry emotion is associated with notions such as anxiety, fear, and nervousness. In this study, we built a dataset for worry emotion classification called \u201cWorryCov\u201d. It is a relatively large dataset derived from Twitter concerning worry about COVID-19. The data were annotated into three levels (\u201cnoworry\u201d, \u201cworry\u201d, and \u201chigh-worry\u201d). Using the annotated dataset, we investigated the performance of different machine learning algorithms (ML), including multinomial Na\u00efve Bayes (MNB), support vector machine (SVM), logistic regression (LR), and random forests (RF). The results show that LR was the optimal approach, with an accuracy of 75%. Furthermore, the results indicate that the proposed model could be used by psychologists and researchers to predict Twitter users\u2019 worry levels during COVID-19 or similar crises.\nKeywords\u2014COVID-19; sentiment analysis; emotion analysis;\nworry dataset; concern analysis\nI. INTRODUCTION\nAt the end of the year 2019, China reported cases of pneumonia caused by an unknown virus in Wuhan City. Later, this pneumonia was defined by the World Health Organization (WHO) as the coronavirus disease 2019 (COVID-19)[1]. It was then declared a pandemic that has had multiple consequences, including the death and long-term effects of infected people. According to WHO, as of July 2022, the total number of reported COVID-19 cases was approximately 545 million, with a total of 6.3 million deaths 1 . The uncertainty and low predictability of COVID-19 threaten people\u2019s both physical and mental health, especially in terms of emotions and cognition [2]. The most challenging effects of the pandemic, especially during lockdowns, are depression, anxiety, and worries due to unemployment, losing loved ones, or being personally affected by the disease [3]. While there are several programs that psychologists and therapists carry out to enable recovery from these issues, there is an immense need to study worry using other sources [4]. Traditional methods of public health monitoring, like questionnaires and clinical tests, have certain limitations; for example, they only cover a\n1https://covid19.who.int\nlimited number of participants and are restricted to the data collection period[5].\nIn contrast, social media are becoming a significant source of rich real-time information during crises, including disease outbreaks and natural disasters [6]. Twitter is a unique source of big data for public health researchers due to the real-time nature of the content and the ease of searching and accessing publicly available data [7]. In this vein, COVID-19-related behaviors and sentiments are available on social media. Twitter users continuously post about their feelings and worries regarding these unusual circumstances[8]. This situation drew the attention of computer scientists and researchers, leading to numerous studies on the understanding of the emotional states during current events, especially those related to the pandemic [9].\nThe research problem is related to the discrimination of the worry analysis studies. Most of the researchers have focused on discrete emotion theories, like Ekman\u2019s emotion classification schema [10], by annotating texts to the six basic emotions (i.e., anger, disgust, fear, happiness, sadness, and surprise) [11]. As the most dominant emotions during crises are worry and anxiety [12], [13], the existing methods for emotion detection are insufficient to capture the emotion of worry accurately [14].\nDetecting worry is complex as people are either unwilling to disclose worries to medical personnel or prefer sharing their feelings on social media. Thus, there is a lack of datasets that could be used for worry analysis, as many studies depend on surveys and interviews [15], [16]. To the best of the authors\u2019 knowledge, this is the first study to build a to-date dataset about COVID-19-related worries that is to be applied to machine learning (ML) models. In the context of this paper, worry about COVID-19 is classified into three fine-grained levels: \u201cno-worry\u201d, \u201cworry\u201d, and \u201chigh-worry\u201d. The \u201cnoworry\u201d category includes people discussing the news and politics about the virus or content-containing statistics and figures. On the other hand, people expressing high levels of feelings such as panic or fear (\u201chigh worry\u201d category) are distraught. Between these two categories (\u201cworry\u201d category), there are people expressing concern about the virus, who are considered stressed about the present and the future.\nThe contribution of this paper is two-fold. First, the WorryCov 2 dataset was built based on three classes: \u201cnoworry\u201d, \u201cworry\u201d, and \u201chigh-worry\u201d. It was built with experts\n2Dataset is available from the authors upon reasonable request.\n646 | P a g e\nwww.ijacsa.thesai.org\nin linguistics and followed an annotation scheme under strict quality control. Then, several ML classification models were used to test the dataset.\nThe paper is outlined as follows. The related works are discussed in Section 2. Section 3 introduces the proposed approach. Section 4 provides the results and discussion, while Section 5 concludes the paper.\nII. RELATED WORK\nWorry analysis is considered one dimension of emotion analysis frequently studied in the literature. Therefore, this study focuses on concern, sentiment, and emotion analysis towards or during disasters or pandemics such as the COVID19 pandemic.\nMuch previous research was carried out to determine the public health concerns toward disasters or epidemics based on sentiment analysis results. For example, the work in [17] aimed to analyze Twitter messages relating to Hurricane Irene and trained a dataset based on sentiment analysis classifiers to categorize tweets into levels of concern. They evaluated the impact of various tokenization strategies and feature choices like a bag of words (BOW) and lexicons on classification accuracy. With 84.27% accuracy, the best settings for the maximum entropy classifier were removing punctuation, converting the text to lowercase, removing stop words, and building a worry lexicon. The Epidemic Sentiment Monitoring System [5] provides visualization tools for Twitter posts responding to public concerns about different diseases. The degree of concern reported that multinomial Na\u00efve Bayes (MNB) achieved the highest F1-score using term frequencyinverse document frequency (TF-IDF) features. To measure and monitor public health concerns about communicable diseases, a sentiment classification approach was applied to Twitter data by measuring different levels of concern [18]. The classifier was trained with a dataset automatically generated by a programming system using an emotionoriented and clue-based method. Three ML classifiers were evaluated, with the NB classifier achieving the best accuracy for the epidemic-related dataset.\nRegression is often used to detect public health concerns. For instance, in [19], a strategy to predict to what extent news about a public health issue can be disseminated was proposed using a data collection of microblog news posts. This ML method relies on the logistic regression (LR) algorithm that automatically categorized news posts into two classes: normal news or news posts that resonated with widespread public anxiety.\nAs for COVID-19, abundant works have already been published studying the effects of this pandemic on various aspects. For example, most research focused on analyzing Twitter data and finding the main critical topics that raise concerns for individuals regarding the COVID-19 pandemic. In [20], [21] used the topic modeling technique LDA (an unsupervised machine learning model) to identify the most common topics in the tweets and performed sentiment analysis. Furthermore, analyzing citizens\u2019 concerns during the COVID-19 epidemic has been studied in [22]. 30,000 COVID-19-related tweets were collected from March 14,\n2020. Each tweet was labeled as very negative, negative, neutral, very positive, and positive by using the natural language processing (NLP) library. Then, the authors used sentiment analysis on pre-processed tweets to show the level of concern in various US states. They presented an approach for measuring citizens\u2019 concern levels through Twitter data by using the ratio of very negative and negative tweet counts over the total number of tweets in the dataset. As a result, school closing-related tweets cause the highest level of concern among citizens. Similarly, the study [23] presented a method to identify the COVID-19 topic's degree of concern through user conversations on Twitter based on two phases of the classification process. The first classification step is to separate tweets into two classes, namely COVID-19 and nonCOVID-19. The second step is to classify the COVID-19 data into seven topics: donations, emotional support, warnings and suggestions, hoaxes, notification of information, seeking help, and criticism. Six pairs of combinations of word-level and character-level word embeddings, namely Word2Vec and fastText, with three deep learning models, CNN, RNN, and LSTM, were used to apply the text classification model. The best accuracy was achieved when fastText and LSTM were used together for both stages of classification, with 97.3% and 99.4%, respectively.\nSignificant research in public health has applied emotion analysis using social media-derived information to monitor public emotions during disease outbreaks. Emotions such as anxiety, anger, happiness, desire, disgust, fear, relaxation, and sadness have been widely studied. Emotions are often linked with topic modeling to identify the topics and their intensity level. For example, findings in [12] indicate that the longer texts gave insights into what people worry about during the pandemic: the economy and the family. In the SenWave system [8], seven fine-grained sentiment categories, namely, optimistic, thankful, empathetic, pessimistic, anxious, sad, annoyed, denial, official, and joking, are used to study the concern of Twitter users from different countries. The labeled tweets are used to train the deep learning language models such as XLNet, AraBert, and ERNIE, while over 105 million unlabeled tweets are used for the testing process. An XLNet pre-trained language model was used for English tweets. The classifier achieved an 80% accuracy, which proves the efficiency of the models. However, emotion analysis studies are minimal compared with sentiment research due to the lack of annotated data [24]. The EmoBERT model [24] was used to capture emotions related to emotional health (annoyed, anxious, empathetic, sad) to compare emotions expressed on social media before and during the COVID-19 epidemic. In comparison to BERT and XLNet, EmoBERT achieved better results.\nOur review shows that little research has addressed worry detection. However, many studies address anxiety as an issue of mental health, for instance, this study [25]utilized personal narratives from Reddit to detect anxiety disorders and classified anxiety-related posts into a binary level of anxiety. Using various linguistic features, including vector-space representations (Word2Vec and Doc2Vec), topic (LDA) models, Linguistic Inquiry and Word Count (LIWC) dictionary, and n-gram language models. Overall, all features\n647 | P a g e\nwww.ijacsa.thesai.org\nthat have been used succeeded in classifying the level of anxiety, for single-source features, using Neural Network with N-gram probabilities achieved slightly better accuracy (92%) compare with using SVM with word-vector embeddings (word2vec), and for combined features, Neural Network has produced the highest accuracy of 98% by aggregating LIWC with word2vec embeddings and by aggregating N-gram features with LIWC. Moreover, this paper [26] developed its own binary classification dataset for detecting anxiety and depression users on social media who have not yet been diagnosed with mental illness. The authors have presented a comparative experimental evaluation using the traditional linear model and pre-trained LMs (language models). Their results showed that LMs (BERT and ALBERT) performed relatively well with balanced training data. However, in unbalanced training sets, Support Vector Machine (SVM) with word embeddings and TF-IDF features performed slightly better overall, with 0.750 F1-score, 0.747 for accuracy, and 0.740 for precision.\nTo our knowledge, Verma et al.\u2019s study [14] is the most relevant to the prediction of worry using Twitter data. Using crowdsourcing, they re-annotated an existing dataset that contains four emotions (joy, anger, fear, and sadness) [27] for worry classification. A wide range of machine learning and deep learning models were evaluated. For traditional ML approaches, Multinomial Naive Bayes (MNB), and Support Vector Machine (SVM) are implemented by using featurebased models. For deep learning based on word embeddings, they used Hierarchical Attention Network (HAN) and CNNstatic with combined Glove emoji2vec embeddings. While deep learning approaches based on contextual embeddings were also applied like RoBERT and XLNet. The results showed that deep learning methods outperform as compared to the traditional models for worry identification with 0.61 F1score.\nThe gap in the current studies is related to the lack of a large new dataset for worry level detection related to COVID19 tweets. Despite the many works, most previous results focus on the sentiment classification of tweets as positive, negative, and natural.\nIII. PROPOSED APPROACH\nThe proposed approach is shown in Fig. 1. Due to the limited dataset related to worry identification from text, we built a dataset and chose the classification task. We decided to select some machine learning models to validate the credibility of the collected dataset. The approach first described the data collection and the annotation process into three levels of worry using COVID-19-related tweets. The dataset was then used to extract features, run ML models, and evaluate the results."
        },
        {
            "heading": "A. Building the Benchmark Dataset",
            "text": "1) Dataset collection and filtering: To build the\nbenchmark dataset, tweets were collected, filtered, and\nannotated. Twitter is one of the most popular social media and\nhas a wide range of content including rich text, emojis, and\nhashtags [14]. The tweets related to COVID-19 were collected\nusing Tweepy, the Python Twitter API library [28]. Initially,\nwe used unified query keywords (i.e., coronavirus, covid-19,\n#coronavirus, and #covid-19), previously used in other studies\n[29], to identify the tweets related to COVID-19. The tweets\nwere collected over three periods to ensure that they covered\nsignificant milestones during the pandemic. The three periods\nare consistent with [30] and are the following:\n First period: from January 30 to February 28, 2020. During this period, the first COVID-19-induced death was reported in China, and WHO announced a public health emergency.\n Second period: from March 29 to April 29, 2020. During this period, WHO declared COVID-19 a worldwide pandemic, leading many governments to impose restrictions on citizens in an attempt to reduce the spread of the virus.\n Third period: from May 10 to June 30, 2020. During this period, COVID-19 had spread globally, with an increased number of confirmed cases and deaths.\n648 | P a g e\nwww.ijacsa.thesai.org\nFollowing these periods and using the aforementioned keywords, 270,000 tweets were collected. Each tweet had 24 columns, including data and time, username, tweet text, and location. Since we wanted to detect feelings of worry at the tweet level, we removed the rest of the columns and only retained the text column. However, a large proportion of COVID-19-related tweets were probably not associated with one emotion; thus, annotating them would be costly and ineffective [31], [32]. To meet our objective, we focused solely on the worry emotion and used worry-related keywords to create a dataset of tweets representing this emotion. Following [33], we selected keywords (terms) to filter the collected data. The terms were extracted from Thesaurus.com by finding synonyms and terms related to worry; the dictionary is one of the trusted, free online dictionaries. The synonym keywords are shown in Table I.\nOften, datasets contain noise and irrelevant text. Therefore, the following rules were applied to reduce the dataset to more concise and related tweets: (1) deleting duplicate tweets (i.e., retweeted by other users), (2) deleting non-English language tweets, and (3) deleting all tweets less than 40 characters (short tweet).\ndata is challenging and requires dedicated time from domain\nexperts (time-consuming). However, it is a critical part of the\ndata preparation process in supervised learning. We annotated\nthe data for not just coarse classes (such as worry or no-worry)\nbut also for fine-grained levels indicating the intensity or\ndegrees of emotion. However, annotating instances for degrees\nof emotions is a more difficult task to ensure annotation\nconsistency [33]. Therefore, this study followed a set of rules\nto overcome this challenge: (1) tweets were annotated to three\nclasses only: \u201cno-worry\u201d, \u201cworry\u201d, and \u201chigh-worry\u201d,(2) Three\nEnglish speakers with more than three years of experience in\nlinguistics were employed; (3) the majority vote was used to\nannotate an individual tweet, and when the three experts\ndisagreed, the tweet was considered irrelevant and was\nremoved from the dataset; moreover, a newly developed\nwebsite application was used to help the annotators accomplish\ntheir work; and (4) each annotator got the same number of\ntweets (2,700) for each month of the three periods (8,100\ntweets in total). This process was slow but ensured results in\naccordance with the following guidelines to classify each\ntweet:\n \u201cNo-worry\u201d class:\no News or politics (i.e., conspiration theories, Chinarelated discussion where the Chinese are blamed for the virus, US politics, critics of Donald Trump, etc.) and facts (e.g., numbers, statistics).\nExample: \u201cChina's outbreak is serious. But flu killed *5000 Americans* in the 1st 2wks of 2020 coronavirus infected 6, killed 0. Not sure what info you have that CDC director doesn't \"The immediate risk to the US public is low.\" Our US readers deserve to know they don't need to panic.\u201d\no Other diseases (i.e., tweets comparing COVID with other diseases, discussing symptoms and mortality rates).\nExample: \u201cAids is a killer disease Cancer is a killer disease Ebola is a killer disease Swine flu is a killer disease The only thing that divides Coronavirus to this other diseases is the fact that it is just the latest, stop with the panic and take care of yourself! #coronavirus.\u201d\no Expressing some other emotions (i.e., tweets denying the existence of the virus or expressing any optimistic/positive attitude toward it).\nExample: \u201cmarkets are full of pads soap Dettol, etc. People are not freaking out as they know there's enough. They aren't crazy buying. Let's hope the panic ends soon all over the world and we live happily again #covid-19.\u201d\n \u201cWorry\u201d class:\no Expressing general concern (i.e., mentioning being worried/stressed/concerned about the present and future of COVID-19).\nExample: \u201cAlso, I\u2019m young and healthy and unlikely to die from covid-19, so no reason to be afraid at all for me. I\u2019m nervous about infecting those who are less likely to survive though, so I will do my best to prevent that of at all possible if I get infected.\u201d\n \u201cHigh-worry\u201d class:\no Expressing concern (i.e. tweets expressing feelings of panic, fear, etc.).\nExample: \u201ci'm tired of crying. i'm tired of the anxiety, and panic attacks. i want to go outside again. please - STAY HOME. #COVID-19 #COVID19Ontario.\u201d\no Frequent use of intensifiers (e.g., extremely, so, very) and featuring content related to (fear of) death.\nExample: \u201cSo much stress, so much anxiety, AND I\u2019M PREGNANT. Headaches all day, puking many times a day, quarantined. People are dying, this is not cool. #coronavirus\u201d\n649 | P a g e\nwww.ijacsa.thesai.org\nThe annotation resulted in 7,861 instances corresponding to the three classes. The \u201cno-worry\u201d class included 3,158 instances, the \u201cworry\u201d class had 3,127 instances, and the \u201chigh-worry\u201d class included 1,576 instances. The remaining 239 tweets were eliminated as the annotators disagreed with their classification (not sure). However, we noticed that the WorryCov dataset is imbalanced. So, it should be solved to reduce skewness and increase the performance of ML models [34]. Therefore, we decided to expand the dataset using other external datasets. To our knowledge, no dataset focuses on only worry emotion. Therefore, we selected the intensity of anxiety based on [11] since it was considered a synonym for worry. Anxiety levels in [11] ranged from 1 to 9, where 1 was considered the lowest and 9 the highest. Considering this range, we chose the intensity levels 7, 8, and 9 as descriptive of the \u201chigh-worry\u201d class, resulting in a total of 3,127 instances in the \u201chigh-worry\u201d class."
        },
        {
            "heading": "B. Prediction of Worry Levels",
            "text": "The balanced benchmark dataset was used to evaluate the performance of different ML models. In this section, the data preprocessing, feature extraction, classification, and evaluation steps of this dataset are discussed.\n1) Data preprocessing: Preprocessing generally improves\nthe data quality by extracting meaningful fragments from a\ngiven text excluding the noise [35], [36]. Preprocessing steps\ninclude text cleaning such as URL, digit, punctuation removal,\netc., and lemmatization.\nIn the cleaning step, we removed URLs, user mentions, and hashtags. Previous research on sample datasets shows that these items do not provide any evidence of the level of worry in tweets or useful information [37]. Next, each tweet was converted to lowercase to avoid considering the exact words as unique features, such as \u201cHELP\u201d, \u201cHelp\u201d, or \u201chelp\u201d will be converted to \"help\" [38]. Then, the contractions (i.e., \u201cI\u2019m\u201d instead of \u201cI am\u201d) were replaced by the original phrase as described in [37]. Next, digits, punctuation marks, and extra spaces that do not provide any semantic information to the text were removed. NLP classification tasks often involve removing stop words to improve performance metrics [39]. However, in this dataset, worry feelings were frequently expressed as ideas about oneself, leading to the use of the \"I\" and \"my\" pronouns. Therefore, stop words were not removed to retain the linguistic characteristics of worried users. Finally, each word was lemmatized using Wordnet Lemmatizer available in the natural language toolkit (NLTK) library [40].\n2) Feature extraction: Often called a features vector [37], this step refers to transforming raw data into numerical data\nthat machines can understand. Term frequency-inverse\ndocument frequency (TF-IDF) is a popular text vectorization\ntechnique to generate vector representations of a text [41] and\nwas employed in this experiment. The TF-IDF weighting\nscheme is based on two parts: term frequency (TF) and inverse\ndocument frequency (IDF). TF-IDF is mathematically\nformulated in the following (1) [42]:\nTF \u2013 IDF (t, d) = TF (t, d) \u00d7 IDF(t) (1)\nwhere t denotes a term and d denotes a document.\nTF is the frequency of any term within a given document and is calculated by dividing the number of mentions of a given word by the total number of words in the document [37], TF is defined by (2) [43]:\nTF (t, d) = Number of times the term t appears in the (2)\ndocument / Total number of terms in the document\nIDF represents the importance of a term in the corpus of the text. It is a technique that combined with TF reduces the impact of common words. There are some words, like \u201cthe\u201d, \u201cis\u201d, \u201cand\u201d, etc., that occur frequently but are void of information. IDF is defined by Eq. (3) [44]:\nIDF (t) = log (Number of documents / Documents (3)\ncontaining the term t)"
        },
        {
            "heading": "C. ML-Based Classifiers",
            "text": "Four ML-based classifiers were used in the multiclassification task. These methods were multinomial NB (MNB), logistic regression (LR), Support Vector Machine (SVM), and Random Forest (RF). The default settings of these methods were taken from the scikit-learn library [45]. MNB is suitable for classifying discrete features or fractional counts such as TFIDF. LR calculates the likelihood of a target variable based on a collection of independent variables and a given dataset. SVM is a classification algorithm for two-group classification problems (in our case one-vs-rest scheme is used). Finally, the RF algorithm builds many random decision trees using bagging and feature randomness for each tree."
        },
        {
            "heading": "D. Evaluation Metrics",
            "text": "Each classifier was evaluated using the following performance measurements: accuracy, precision, recall, and F1-score. These standard metrics are defined as follows:\nAccuracy is the ratio of the number of correct predictions to the overall number of predictions:\nAccuracy = (TN + TP) / (TN + TP + FP + FN) (4)\nPrecision is the ratio of the correctly predicted positive instances to the total positive instances:\nPrecision (P) = TP / (TP + FP) (5)\nRecall is the ratio of the correctly predicted positive instances to the total of all instances in the actual class:\nRecall (R) = TP / (TP + FN) (6)\nF1-score is the harmonic average of precision and recall:\nF1-score = (2 \u00d7 P \u00d7 R) / (P + R) (7)\nwhere TP, TN, FP, and FN denote true positive, true negative, false positive, and false negative, respectively.\nIV. RESULTS AND DISCUSSION\nAfter filtering (see Section 3.1), we obtain 15,000 tweets. Fig. 2 presents the word cloud of the most commonly used words in the WorryCov dataset. The most frequent keywords are related to the COVID-19 pandemic, such as \"Covid\",\n\"corona\", \"coronavirus\", and \"scared\". The distribution of tweets among the three worry levels is shown in Fig. 3. The figure demonstrates that the three classes are balanced. Fig. 4 includes tweets representative of the three levels of worry. The figure shows that the \u201chigh-worry\u201d class shows fear and stress behavior. While the \u201cworry\u201d class indicates familiar people\u2019s behavior during any pandemic. In contrast, the \u201cno-worry\u201d indicates informative or news content or an optimistic feeling.\nTo predict the performance of the selected ML models, the dataset was split into 80% for training and 20% for testing. Next, data preprocessing and feature extraction (see Section 3.2) were employed to extract relevant features. Finally, the accuracy, precision, recall, and F1-score results were reported for the average class (Table II).\nAs shown in Table II, the classification performance of LR (reported in bold) performed better than the other models in terms of accuracy, precision, recall, and F1-score. It yielded the highest accuracy of 75%, a precision of 0.751, a recall of 0.747, and an F1-score of 0.748. On the contrary, RF acquired the lowest values with an accuracy of 68%, 0.683 for recall, 0.682 for precision, and F1-score of 0.682.\nAn investigation of the dataset shows that the LR algorithm was able to build features better than others because the training algorithm of LR uses the one-vs-rest scheme in the multiclass option and the cross-entropy loss. However, the results cannot be generalized as the absolute difference among the high-performing models in Table II is less than 4%.\nFig. 5, 6, and 7 show the precision, recall, and F1-score measurements, respectively, for all the applied algorithms according to the three worry levels. The results indicate that although the dataset was balanced, the TFIDF feature extraction method did not provide sufficient information to the classifiers. The feature sets did not detect the worry classes due to embedded semantic features within this textual class label, which TFIDF could not capture.\nThe \u201cno-worry\u201d class was the highest-performing class label, while the \u201cworry\u201d class was the lowest-performing class label. However, for the \u201cno-worry\u201d and \u201chigh-worry\u201d classes, other information was available. For example, in the \u201cnoworry\u201d class, some terms related to blame, news, and politics are present. As for the \u201chigh-worry\u201d class, intensifiers were present. These results indicate the possible usefulness of the TFIDF feature set.\n651 | P a g e\nwww.ijacsa.thesai.org\nIn general, the current method, compared to Verma et al.\u2019s study [14] is based on a new dataset. The new approach is also much more focused on the worry levels compared to 9 anxiety levels in [11], ranging from 1 to 9, where one was considered the lowest and nine the highest.\nV. CONCLUSION\nIn this paper, we compiled a fine-grained benchmark dataset for the classification of worry levels concerning the COVID-19 pandemic. The dataset was collected from Twitter and was annotated using a majority vote among three experts. The WorryCov dataset was used to classify and predict the level of worry among Twitter users during the pandemic. Several experiments were conducted using the following ML algorithms: NB, LR, RF, and SVM. The optimal performance was achieved by LR, with an accuracy of 75%. It is recommended that the proposed approach be used for decision-making in healthcare entities to plan programs for the affected people. However, the current work has a few limitations. For example, the dataset is relatively small and was collected based on a short period from 2020\u20132021. Moreover, human behavior changes over time due to interaction with infected people, vaccination initiatives, and governments' health policies. Therefore, a new set of keywords that represent the new set of tweets might be needed to uncover the new trends in human worry levels. In the future, several deep learning models could be used to enhance the performance of the current approaches."
        }
    ],
    "title": "Building and Testing Fine-Grained Dataset of COVID-19 Tweets for Worry Prediction",
    "year": 2022
}