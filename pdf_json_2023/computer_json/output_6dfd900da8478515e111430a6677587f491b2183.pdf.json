{
    "abstractText": "This essay discusses a potential method for predicting the behavior of various physical processes and uses the COVID-19 outbreak to demonstrate its applicability. This study assumes that the current data set reflects the output of a dynamic system that is governed by a nonlinear ordinary differential equation. This dynamic system may be described by a Differential Neural Network (DNN)with time-varying weights matrix parameters. A new hybrid learning scheme based on the decomposition of the signal to be predicted. The decomposition considers the slow and fast components of the signal which is more natural to signals such as the ones corresponding to the number of infected and deceased patients who suffered of COVID 2019 sickness. The paper results demonstrate the recommended method offers competitive performance (70 days of COVID prediction) in comparison to similar studies.",
    "authors": [
        {
            "affiliations": [],
            "name": "A. Poznyak"
        },
        {
            "affiliations": [],
            "name": "I. Chairez"
        },
        {
            "affiliations": [],
            "name": "A. Anyutin"
        }
    ],
    "id": "SP:9b6ac0f8edaa95b26fe1a1762a53deb1e9ec0dc4",
    "references": [
        {
            "authors": [
                "D Adam"
            ],
            "title": "Special report: the simulations driving the world\u2019s response to COVID-19",
            "venue": "Nature",
            "year": 2020
        },
        {
            "authors": [
                "D Balcan"
            ],
            "title": "2010)Modeling the spatial spread of infectious diseases: the global epidemic and mobility computational model",
            "venue": "J Comput Sci",
            "year": 2010
        },
        {
            "authors": [
                "R da Costa Barros",
                "TP Nascimento"
            ],
            "title": "Robotic mobile fulfillment systems: a survey on recent developments and research opportunities",
            "venue": "Robot Auton Syst",
            "year": 2021
        },
        {
            "authors": [
                "DL Deangelis",
                "WM Mooij"
            ],
            "title": "Individual-based modeling of ecological and evolutionary processes",
            "venue": "Ann Rev Ecol Evol Syst",
            "year": 2005
        },
        {
            "authors": [
                "Goutsias John"
            ],
            "title": "Classical versus stochastic kinetics modeling of biochemical reaction systems",
            "venue": "Biophys J",
            "year": 2007
        },
        {
            "authors": [
                "S Haykin"
            ],
            "title": "Neural networks and learning machines, 3rd edn",
            "year": 2008
        },
        {
            "authors": [
                "NP Jewell",
                "JA Lewnard",
                "BL Jewell"
            ],
            "title": "Predictive mathematical models of the COVID-19 pandemic: underlying principles and value of projections",
            "venue": "JAMA 323(19):1893\u20131894",
            "year": 2020
        },
        {
            "authors": [
                "R Priyanka",
                "A Kumari",
                "M Sood"
            ],
            "title": "Implementation of simple RNN and LSTMs based prediction model for coronavirus disease",
            "venue": "IOP Conf. Ser.: Mater. Sci. Eng.,",
            "year": 2021
        },
        {
            "authors": [
                "A Poznyak",
                "E Sanchez",
                "W Yu"
            ],
            "title": "Differential neural networks for robust nonlinear control: identification, state estimation, and trajectory tracking",
            "venue": "World Scientific,",
            "year": 2001
        },
        {
            "authors": [
                "T Poznyak",
                "I Chairez",
                "A Poznyak"
            ],
            "title": "Ozonation and biodegradation in environmental engineering: dynamic neural network approach",
            "year": 2019
        },
        {
            "authors": [
                "A Poznyak",
                "I Chairez",
                "T Poznyak"
            ],
            "title": "A survey on artificial neural networks application for identification and control in environmental engineering: biological and chemical systems with uncertain models",
            "venue": "Annu Rev Control",
            "year": 2019
        },
        {
            "authors": [
                "JT Reason"
            ],
            "title": "Motion sickness adaptation: a neural mismatch model",
            "venue": "J R Soc Med",
            "year": 1978
        },
        {
            "authors": [
                "KC Santosh"
            ],
            "title": "COVID-19 prediction models and unexploited data",
            "venue": "J Med Syst",
            "year": 2020
        },
        {
            "authors": [
                "V Utkin",
                "A Poznyak",
                "YV Orlov",
                "A Polyakov"
            ],
            "title": "Road map for sliding mode control design, SpringerBriefs in mathematics. Springer, Berlin. https://doi.org/10.1007/978-3-030-41709-3 123 Differential Neural Networks Prediction Using Slow and.",
            "year": 2020
        },
        {
            "authors": [
                "Vora Nishith",
                "Daoutidis Prodromos"
            ],
            "title": "Nonlinear model reduction of chemical reaction systems",
            "venue": "AIChE J",
            "year": 2001
        },
        {
            "authors": [
                "L Wynants",
                "B Van Calster",
                "GS Collins",
                "RD Riley",
                "G Heinze",
                "E Schuit",
                "M van Smeden"
            ],
            "title": "Prediction models for diagnosis and prognosis of covid-19: a systematic review and critical appraisal",
            "venue": "BMJ",
            "year": 2020
        },
        {
            "authors": [
                "A Zeb",
                "E Alzahrani",
                "V Suat Erturk",
                "G Zaman"
            ],
            "title": "Mathematical model for coronavirus disease",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "Keywords Artificial neural networks \u00b7 Hybrid learning \u00b7 Virus evolution \u00b7 COVID"
        },
        {
            "heading": "1 Introduction",
            "text": ""
        },
        {
            "heading": "1.1 Predictions Based on Neural Networks",
            "text": "This section reviews how neural networks may predict the evolution of signal temporal evolution.\nB A. Poznyak apoznyak@ctrl.cinvestav.mx\nI. Chairez isaac.chairez@tec.mx\n1 CINVESTAV IPN, DCA, Cd. de Mexico, Mexico\n2 Tecnologico de Monterrey, Institute of Advanced Materials for Sustainable Manufacturing, Cd. de Guadalajara, Mexico\n3 Institute of Radio Engineering and Electronics, Fryazino Branch, Ran, Fryazino, Russia"
        },
        {
            "heading": "1.1.1 Prediction by Static (Feed-Forward) NN",
            "text": "A set of input values are considered by each neuron. Each one is associated with a weight, which is a varying value that may be determined by supervised or unsupervised training techniques like data clustering, and a bias. The network selects a neuron\u2019s output depending on its weight and bias. All such activities in the context of Classification require labeled datasets. You thus require guided learning. In supervised learning, people verify that the neural network\u2019s predictions are accurate. This aids the neural network in comprehending how labels and data related. Face identification, picture recognition and labeling, voice detection, and speech transcription are a few examples of this. Deep learning can link pixels in a picture and a person\u2019s name via categorization. The act of grouping or clustering is the identification of commonalities. Understand that labels are not always necessary for the deep learning model to detect commonalities. Unsupervised learning is when a system utilizes machine learning to learn on its own when there are no helping human labels from which to draw. This keeps the possibility of creating extremely precise models. Customer churn is a type of clustering.\nAswe are all aware, predictive analytic usesmethods like predictivemodeling andmachine learning to examine historical data and forecast future patterns [7]. Contrary to conventional forecasting techniques, neural networks are unique. In contrast to a neural network, the most popular model, linear regression, is actually a pretty straightforward approach to problem-solving. Because of their hidden layers, neural networks do predictive analytic more effectively. Only input and output nodes are used in linear regression models to generate predictions. The hidden layer is also used by the neural network to improve prediction accuracy. That\u2019s because it learns similarly to how people do. So why isn\u2019t neural network prediction used by everyone? They are prohibitively expensive due to their high computer power requirements. In addition, massive data sets are required to train neural networks, which your company might not have. But as IT technology becomes more affordable, the first obstacle could soon vanish. Soon, there won\u2019t be any more \"unpleasant shocks\" because to technologies like Artificial Neural Networks (ANNs)."
        },
        {
            "heading": "1.1.2 Prediction by Dynamic (with Feedback) NN",
            "text": "ANNs are often regarded as effective instruments for modeling intricate, nonlinear systems using hazy dynamic models. ANNs were first utilized as reliable predictors of various processes with static reliance on input\u2013output data. The time effect should be included in the ANN when it must be used to describe a rough model of time-dependent input\u2013output interactions, which necessitates the creation of a dynamic ANN or DNN [11]. In continuous time modelling we will be refereed to DNN asDifferential Neural Networks. The review [13] lays forth the different recurrent and differential forms ofDynamic Neural Networks (DNN), their mathematical construction, and techniques for adjusting the network weights. The characteristics of DNNs motivate their use to represent the dynamics of decontamination processes. This review details recent findings on the DNN application for the modelling and controlling of treatment systems based on either biological or chemical processes. The modeling application of DNN for common methods used in the treatment of wastewater, contaminated soil, and the atmosphere is described. The major benefits of using the approximate DNN-based model instead of designing the complex mathematical description for each treatment are analyzed to enhance the efficiency of the decontamination treatment. In this paper, we also highlight the remarkable efficiency of DNNs as a keystone tool for modelling of epidemics. [15, 18]."
        },
        {
            "heading": "1.2 OnMathematical Predictions of Epidemics",
            "text": "In the last few years, researchers and government officials have used computer-based models to try to forecast the course of the coronavirus pandemic (see, for example, [2, 8, 10, 19]). To predict the future of the coronavirus disease 2019 (COVID-19) outbreaks globally, several mathematical models have been developed. These forecasts have a significant impact on how soon and forcefully, governments respond to an outbreak. However, rather than producing accurate quantitative predictions regarding the magnitude or duration of illness burdens, the primary and most efficient application of epidemiological models to evaluate the relative efficacy of different treatments in lowering disease burden.\nThere are several studies remarking that models are hardly crystal balls when it comes to making predictions, and according to science journalist Miles O\u2019Brien (PBS News Hour), \"all of them require human assumptions\" [1]. The creation of these models and their eventual goal are more sophisticated than many of us think, according to specific research periodicals. Our world is complex and has more data than knowledge. The Global Epidemic andMobility Model, or GLEAM, is curated by a group of bio-statisticians at Seattle\u2019s Fred Hutchinson Cancer Research Center [3]. They create mathematical models that explain how infections spread chaotically and exponentially. According to the projection from last month, 17,000 to 29,300 additional fatalities would likely be reported in the US solely for the week ending February 13, 2021, totaling 465,000 to 508,000 COVID-19 deaths by this time. The accuracy of mathematical forecasts in battling epidemics is still being worked on. Nevertheless, creating such illness prediction models is a crucial issue for scientific societies worldwide. It necessitates prompt and comprehensive answers, including a potential application for defining new politics and prevention schemes."
        },
        {
            "heading": "1.3 Main Concepts of This Paper",
            "text": "The results presented here are based on three principle concepts:\n\u2022 Although we have hundreds of years of theoretical knowledge on how to create mathematical models of infectious diseases, have any of these models ever been put to the test using all of the data sources at our disposal in real-time?No. Aswe create this automobile and learn more about these models, it is hurtling down the highway. For a more accurate model design, it is really difficult to take into consideration all human aspects (social, informational, climatic, and others) acting during sickness. \u2022 Any recommended model must include the inherent uncertainties associated with the most recent data. Thus, for instance, we lack sufficient statistical data to accept all of the conditions that should be satisfied to use any stochastic prediction models that are accessible (such as the Kalman filter or any of its modifications such as a requirement for noises to have Gaussian distributions with known covariation matrices, local linearity of the model, exact knowledge all participating parameters and so on). We only have one data trajectory (realization), making it complicated to apply statistical concepts like mathematical expectation (mean value), variance, and confidence interval. We can also not repeat the experiment to get at least one other data curve. This indicates that a statistical method for this kind of problem is not applicable! \u2022 Given the previous items, we suppose that the current data-set represents the output of some dynamic system governed by a nonlinear ordinary differential equation and may be modeled by a Differential Neural Network with time-varying weights matrix\nparameters whose dynamics is governed by special Learning laws containing slow and fast components.\nAll results reported below justify nice performances of the suggested approach."
        },
        {
            "heading": "2 DNNModel with Slow and Fast Learning",
            "text": ""
        },
        {
            "heading": "2.1 Ideas of a Prediction Algorithm for Models with Complete Information",
            "text": ""
        },
        {
            "heading": "2.1.1 Non-causal Model",
            "text": "Consider initially an ideal scenario where we know that the following mathematical model produces the scalar output x (t) \u2208 R of any dynamic plant.\nx (n) (t) = f (t, x (t) , x\u0307 (t) , . . . , x (n\u22121) (t))\nx (r) (0) = x (r)0 , r = 0, 1, . . . , n \u2212 1\n\u23ab \u23ac \u23ad (1)\nwhere the nonlinear function f : R+ \u00d7 Rn \u2192 R and initial condition x0 supposed to be known exactly. Defining vector x (t) \u2208 Rn with components\nx1 (t) := x (t) , x2 (t) := x\u03071, . . . , x\u0307n\u22122 := xn\u22121, xn := x\u0307n\u22121, (2) we can represent (1) as\nx\u0307 (t) = F (t, x (t)) = Ax (t) +bv(t)\nA =\n\u239b\n\u239c\u239c\u239c\u239c\u239c \u239d 0 1 0 \u00b7 \u00b7 \u00b7 0 0 0 0 0 1 0 \u00b7 \u00b7 \u00b7 0 0 . . . 0 0 1 0 \u00b7 \u00b7 \u00b7 0 0 0 0 0 1 0 \u00b7 \u00b7 \u00b7 0 0 \u00b7 \u00b7 \u00b7 0 0 0 0\n\u239e\n\u239f\u239f\u239f\u239f\u239f \u23a0 \u2208 Rn\u00d7n,b =\n\u239b\n\u239c\u239c\u239c\u239c\u239c \u239d\n0 0 ... 0 1\n\u239e\n\u239f\u239f\u239f\u239f\u239f \u23a0 \u2208 Rn\u00d71\nv(t) = f (t, x1 (t) , x2 (t) , . . . , xn\u22121 (t)) \u2208 R\n\u23ab \u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23ac\n\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23ad\n(3)\nIn the corresponding integral form the differential model (3) can be rewritten as\nx (t + T ) = x (t) + t+T\u222b\n\u03c4=t F (\u03c4, x (\u03c4 )) d\u03c4 = x (t) + T r (t, T )\nr (t, T ) := 1 T\nt+T\u222b\n\u03c4=t F (\u03c4, x (\u03c4 )) d\u03c4\n(4)\nwhere the variable r (t, T ) represents the \"averaged rate\" of changing the considered output variable x (t) on the time-interval [t, t + T ]. Considering the data set {x (\u03c4 )}\u03c4\u2208[0,t] as the information on the process available up to the moment t we may conclude that r (t, T ) (4) contains the information on nearest future {x (\u03c4 )}\u03c4\u2208[t,t+T ] with the horizon T and hence may be considered as \u201cnon-causal\u201d."
        },
        {
            "heading": "2.1.2 Causal Approximation",
            "text": "Introduce standard operators of delay e\u2212sT and differentiation s acting as\ne\u2212sT f (t) = f (t \u2212 T ) , s f (t) = f \u2032 (t) and using the local approximation\nesT 1 + sT + T 2 2 s2 + T 3 6 s3\nfor the \"forecasting operator\" esT , we can obtain the following approximate relation:\nr (t, T ) = esT e\u2212sT r (t, T ) = esT r (t \u2212 T , T ) ( 1 + sT + T 2\n2 s2 + T\n3\n6 s3\n) r (t \u2212 T , T ) = r (t \u2212 T , T )+\nT r\u0307 (t \u2212 T , T ) + T 2 2 r\u0308 (t \u2212 T , T ) + T 2 2 ... r (t \u2212 T , T ) := rcaus (t, T ) ,\n\u23ab \u23aa\u23aa\u23aa\u23aa\u23ac\n\u23aa\u23aa\u23aa\u23aa\u23ad\n(5)\nwhere\nr (t \u2212 T , T ) = 1 T\nt\u222b\n\u03c4=t\u2212T F (\u03c4, x (\u03c4 )) d\u03c4, (6)\ndepends on available information {x (\u03c4 )}\u03c4\u2208[t\u2212T ,t]. Given that the new variable rcaus (t, T ) (5) can be treated as the \"causal approximation\" of variable r (t, T ) and the integral representation (4) of the considered dynamics (1) can be locally approximated as\nx (t + T ) x (t) + T rcaus (t, T ) . (7) Remark 1 Since the right-hand side of (7) contains only information {x (\u03c4 )}\u03c4\u2208[t\u2212T ,t], available up to time t , we can consider the value x (t + T ) as the \" prediction (or forecasting)\" of the process {x (\u03c4 )}\u03c4\u2208[0,t] ahead on horizon T ."
        },
        {
            "heading": "3 Prediction Algorithm for Models with Incomplete Information: DNN Approach",
            "text": "When the original dynamics F (t, x (t)) in (3) is completely or partially unknown, we suggest applying theDNNapproach [11]which showed nice results being applied to various problems in bio-engineering and the environment science [12, 13]."
        },
        {
            "heading": "3.1 DNN IdentificationModel",
            "text": "Artificial neural networks (ANNs) are thought to be effective modeling tools for non-linear, complicated systems with ambiguous dynamic models. ANNs were first utilized as reliable predictors of various processes with static reliance on input\u2013output data. The time effect must be included in the ANN when it is used to characterize a rough model of time-dependent input\u2013output relationships, which necessitates the reconstruction of a dynamic ANN or the use of Recurrent Neural Networks (RNNs) in discrete time or Differential Neural Networks (DNNs) in continuous time. DNNs sometimes referred to as Auto Associative or Feedback Networks, are a subclass of ANNs in which the connections between the input and the output\nare organized into a directed cycle. As a result, the network develops an internal state that displays dynamic, temporally dependent behavior. DNN allows the signal to go both forward and backward by including loops in the network design or topology. To achieve the required behavior of this DNN, a particular tuning for the time-dependent weight matrix parameters is realized as a result of such a suggestion. In our scenario, define the single layer DNN model following [11], where the measurable output x(t) is a vector, as\nd dt x\u0302 (t) = Ax\u0302 (t)+bx (n)1 (t)+W\u0302 (t) \u03c3 ( x\u0302 (t) ) +L [x1(t)\u2212C x\u0302 (t) ] ,\nx\u0302 (0)=x (0) \u2208 Rn,C = (1, 0, . . . , 0) \u2208 Rn, W\u0302 (t) \u2208 Rn\u00d7p, \u03c3 : Rn \u2192 Rp,\n\u23ab \u23aa\u23ac \u23aa\u23ad (8)\nwhere\n\u2022 \u03c3 (x\u0302) = (\u03c31 ( x\u0302 ) , \u03c32 ( x\u0302 ) , . . . , \u03c3p ( x\u0302 )) is the vector with sigmoidal components\n\u03c3 j ( x\u0302 ) = \u03b1 j\n1 + \u03b2 j e\u2212\u03b3 j x\u0302\n+ \u03b4 j , j = 1, . . . , p\n(\u03b1 j , \u03b2 j and \u03b4 j are positive scalars and \u03b3 j \u2208 Rn is a weighting vector for the component of x\u0302);\n\u2022 W\u0302 (t) is the weight matrix, changing in time according to the Learning Procedure (LP) d\ndt W\u0302 (t) = K\u22121P [x(t) \u2212 x\u0302(t)] \u03c3 (x\u0302 (t))\n0 < K = K \u2208 Rn\u00d7n, 0 < P = P \u2208 Rn\u00d7n\n\u23ab \u23aa\u23ac \u23aa\u23ad (9)\n\u2022 The vector L \u2208 Rn must be selected in such a way that\nL \u2208 Rn\u00d71 : A0(L) = A \u2212 LC is Hurwitz, spectrum ( A0(L) ) \u2208 C\u2212.\nAs it ismentioned in ( [11]), a special selection ofmatrix P wemayguarantee a goodDNNapproximation (identification) x\u0302 (t) x (t) practically for all t \u2265 0. The next subsection explains how the algorithms ( 8) and (9) should be modified to be able to generate a good prediction trajectory x\u0302 (t + T ) using only available information {x\u0302 (\u03c4 ) , }\n\u03c4\u2208[t\u2212T ,t]."
        },
        {
            "heading": "3.2 DNN PredictionModel",
            "text": "The DNN dynamics (8) in the integral causal format (7) may be represented as\nx\u0302 (t + T ) = x\u0302 (t) + T r\u0302caus (t, T ) , (10)\nwhere\n\u2022 the signal x\u0302 (t) is generated by (8), \u2022 the auxiliary vector r\u0302caus (t, T ) is defined as\nr\u0302caus (t, T ) :=r\u0302 (t \u2212 T , T ) + T d dt r\u0302 (t \u2212 T , T ) +T 2\n2\nd2\ndt2 r\u0302 (t \u2212 T , T ) + T\n2\n2\nd3 dt3 r\u0302 (t \u2212 T , T )\n(11)\nwith\nr\u0302 (t \u2212 T , T ) := 1 T\nt\u222b\n\u03c4=t\u2212T F\u0302 (\u03c4, x (\u03c4 )) d\u03c4,\nF\u0302 (\u03c4, x (\u03c4 )) := Ax\u0302 (\u03c4 ) +bx (n)1 (\u03c4 ) + W\u0302 (\u03c4 ) \u03c3 ( x\u0302 (\u03c4 ) ) + L [x1(\u03c4 ) \u2212 C x\u0302 (\u03c4 ) ] ,\n\u23ab \u23aa\u23aa\u23aa\u23aa\u23ac\n\u23aa\u23aa\u23aa\u23aa\u23ad\n(12)\n\u2022 the derivatives x (m)1 (t) , (m = 1, . . . , n) and dk\ndtk r\u0302 (t \u2212 T , T ) , (k = 1, 2, 3) are calcu-\nlated recurrently based on \"super-twist algorithm\" ( [9]), [16]. To differentiate time function f (t), the super-twisting controller is designed to reduce the error s(t) (s = x\u2212 f ) between its input f (t) and output x(t) to zero:\nx\u0307(t) = \u2212\u03b1\u221a|s(t)|sign(s(t)) + y(t), y\u0307(t) = \u2212M sign(s(t))\u2223\u2223 f\u0308 \u2223\u2223 < F0, M > F0 \u23ab \u23ac \u23ad (13)\nThe error s(t) is reduced to zero after a finite time interval t0 and state component y(t) is equal to the first-time derivative of a function f (t) , namely, y(t) = ddt f (t) for all t \u2265 t0. If f (t) is corrupted by bounded noise |s(t)| \u2264 = const, then an upper bound of the differentiation, the error is estimated by inequality\n\u2223\u2223\u2223\u2223y(t) \u2212 d\ndt f (t)\n\u2223\u2223\u2223\u2223 \u2264 \u03b11 + \u03b12 \u221a ,\u03b11, \u03b12 - positive constants."
        },
        {
            "heading": "3.3 DNN Predictor with Slow and Fast Components",
            "text": "There are several systems whose trajectories can be understood as the overlapping of signals formed with the combination of slow and fast components. Such systems are also known as multi-rate system that appears naturally in mobile robotics [4], chemical [17] and biochemical [6] reactions, evolution of medical sicknesses [14], evolution of ecosystems animal populations [5] andmany others. The same type of combined dynamics is valid for describing the evolution of both infected and deceased persons suffering of the Covid-19 sickness.\nThe developed DNN structure with mixed (slow and fast) learning scheme could be useful to represent the dynamics of COVID-19. Such a fact can be justified considering that the evolution of infected and deceased persons can be represented as the combination of a slow dynamics defined by the seasonal variations and a fast evolution which corresponds to the daily evolution. Onemay notice that suchmulti-rate dynamics has not been considered before in the design of non-parametric identifiers based on differential neural networks, which is indeed a contribution of this study."
        },
        {
            "heading": "3.3.1 Slow Predictive Component",
            "text": "Based on available data {x (\u03c4 )}\u03c4\u2208[0,t] let us reconstruct a \"slow\" trajectory {xslow (\u03c4)}\u03c4\u2208[0,t] defined as the best least squares polynomial approximation of a given order N , that is,\nxslow (t) = N\u2211\ni=0 c\u0304i t i ,\nc\u0304 = arg min c\u2208RN+1\nt\u222b\n\u03c4=0\n(\nx (\u03c4 ) \u2212 N\u2211\ni=0 ci\u03c4\ni\n)2\nd\u03c4\n= \u239b \u239d t\u222b\n\u03c4=0 x (\u03c4 ) \u00f8 (\u03c4 ) \u00f8 (\u03c4 ) d\u03c4\n\u239e\n\u23a0\n\u22121 t\u222b\n\u03c4=0 x (\u03c4 ) \u00f8 (\u03c4 ) d\u03c4,\nc\u0304 := (c\u03040, . . . , c\u0304N ) \u2208 RN+1, \u00f8 (\u03c4 ) := ( 1, \u03c4, \u03c4 2, . . . , \u03c4 N ) \u2208 RN+1\n\u23ab \u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23ac\n\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23ad\n(14)\nThe behavior of trajectory xslow (t) is shown in Fig. 1 for COVID-19 case. Then, as in (10), (11) and (12), define x\u0302slow (t + T ) as\nx\u0302slow (t + T ) = x\u0302slow (t) + T r\u0302slowcaus (t, T ) , (15) where\n\u2022 x\u0302slow (t) is generated by the following DNN model: d\ndt x\u0302slow (t)= Ax\u0302slow (t)+bx (n)1,slow (t) +\nW\u0302slow (t) \u03c3 ( x\u0302slow (t) ) + L [x1,slow(t) \u2212 C x\u0302slow (t) ] ,\nd dt W\u0302slow (t) = K\u22121P ( xslow(t) \u2212 x\u0302slow(t) ) \u03c3 ( x\u0302slow (t) )\nx\u0302slow (0) = x (0) \u2208 Rn,C = (1, 0, . . . , 0) \u2208 Rn, W\u0302slow (t) \u2208 Rn\u00d7p, \u03c3 : Rn \u2192 Rp,\n\u23ab \u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23ac\n\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23ad\n(16)\n\u2022 the auxiliary vector r\u0302slowcaus (t, T ) is defined as\nr\u0302slowcaus (t, T ) := r\u0302slow (t \u2212 T , T ) + T d\ndt r\u0302slow (t \u2212 T , T ) +\nT 2\n2\nd2\ndt2 r\u0302slow (t \u2212 T , T ) + T\n2\n2\nd3 dt3 r\u0302slow (t \u2212 T , T ) ,\n\u23ab \u23aa\u23ac \u23aa\u23ad (17)\nwith\nr\u0302slow (t \u2212 T , T ) := 1 T\nt\u222b\n\u03c4=t\u2212T F\u0302slow (\u03c4, xslow (\u03c4)) d\u03c4,\nF\u0302slow (\u03c4, xslow (\u03c4)) := Ax\u0302slow (\u03c4) +bx (n)1,slow (\u03c4) + W\u0302 (\u03c4 ) \u03c3 ( x\u0302slow (\u03c4) ) + L [x1,slow(\u03c4) \u2212 C x\u0302slow (\u03c4) ] ,\n\u23ab \u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23ac\n\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23ad\n(18)"
        },
        {
            "heading": "3.3.2 Fast Predictive Component",
            "text": "Define x f ast (t) as\nx f ast (t) := x (t) \u2212 xslow (t) (19) The behavior of trajectory x f ast (t) is shown in Fig. 2 for COVID-19 case. Then, as in (16), (17) and (18), define x\u0302 f ast (t + T ) as\nx\u0302 f ast (t + T ) = x\u0302 f ast (t) + T r\u0302 f astcaus (t, T ) , (20) where\n\u2022 x\u0302 f ast (t) is generated by the following DNN model:\nd dt x\u0302 f ast (t)= Ax\u0302 f ast (t) +bx (n)1, f ast (t) +\nW\u0302 f ast (t) \u03c3 ( x\u0302 f ast (t) ) + L [x1, f ast (t) \u2212 C x\u0302 f ast (t) ] ,\nd dt W\u0302 f ast (t) = K\u22121P ( x f ast (t) \u2212 x\u0302 f ast (t) ) \u03c3 ( x\u0302 f ast (t) )\nx\u0302 f ast (0) = x (0) \u2208 Rn,C = (1, 0, . . . , 0) \u2208 Rn, W\u0302 f ast (t) \u2208 Rn\u00d7p, \u03c3 : Rn \u2192 Rp,\n\u23ab \u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23ac\n\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23ad\n(21)\n\u2022 the auxiliary vector r\u0302 f astcaus (t, T ) is defined as\nr\u0302 f astcaus (t, T ) := r\u0302 f ast (t \u2212 T , T ) + T ddt r\u0302 f ast (t \u2212 T , T )+\nT 2\n2\nd2\ndt2 r\u0302 f ast (t \u2212 T , T ) + T\n2\n2\nd3 dt3 r\u0302 f ast (t \u2212 T , T ) ,\n\u23ab \u23aa\u23ac \u23aa\u23ad (22)\nwith\nr\u0302 f ast (t \u2212 T , T ) := 1 T\nt\u222b\n\u03c4=t\u2212T F\u0302 f ast\n( \u03c4, x f ast (\u03c4 ) ) d\u03c4,\nF\u0302 f ast ( \u03c4, x f ast (\u03c4 ) ) := Ax\u0302 f ast (\u03c4 ) +bx (n)1, f ast (\u03c4 ) + W\u0302 (\u03c4 ) \u03c3 ( x\u0302 f ast (\u03c4 ) ) + L [x1, f ast (\u03c4 ) \u2212 C x\u0302 f ast (\u03c4 ) ] .\n\u23ab \u23aa\u23aa\u23aa\u23aa\u23aa\u23ac\n\u23aa\u23aa\u23aa\u23aa\u23aa\u23ad\n(23)"
        },
        {
            "heading": "3.4 Joint Slow and Fast Predictor",
            "text": "In this paper, we use more advance predictor, consisting in two components: slow x\u0302slow (t + T ) and fast x\u0302 f ast (t + T ) , namely,\nx\u0302 (t + T ) = x\u0302slow (t + T ) + x\u0302 f ast (t + T ) . (24)"
        },
        {
            "heading": "4 Structure of Numerical Procedure",
            "text": "The suggested predictive numerical structure consists of the following steps:\n1. Based on given discrete-time data {x (k)}k\u2208[0,1,...K], where x (k) corresponds to the data value at day k, and applying a spline approximation (herein examples, we use the spline of 15-th order) we construct the continuous-time curve {x (\u03c4 )}\u03c4\u2208[0,t] where t = K ( is time interval between discrete data). 2. Then using (14) and (19), based on the obtained curve {x (\u03c4 )}\u03c4\u2208[0,t] we need to construct the slow xslow (t) and fast x f ast (t) trajectories. 3. Applying the procedures (16), (17) and (18) we obtain the slow predictive curve x\u0302slow (t + T ) (15). 4. Then applying the procedures (20), (22 ) and (23) we obtain the fast predictive curve x\u0302 f ast (t + T ) (15). 5. The last step is to construct the final predictive curve x\u0302 (t + T ) (24) for desired T (for example, taking T = 60, 90, 120 days on the COVID-19 prediction).\nThe corresponding block scheme is shown in Fig. 3.\nFig. 3 Flow diagram describing how the proposed forecasting evolution is derived using the tools included in the toolbox\nThe developed algorithm was implemented accordingly to the following pseudocode.\n1. Load information corresponding to infected or deceased patients suffered from COVID19 sickness 2. Interpolate the loaded information using a third order spline strategy 3. Implement a p-th order low-pass filter with a finite-impulse response strategy using a cut-\noff frequency of 0.5 Hz. This frequency was determined using the collected information. The value of p-th order is fixed to 7 considering the evolution of Covid information. 4. Filter the loaded information separating the slow and fast components of the infected or deceased datasets, according to the selected cut-off frequency. 5. Develop the slow learning algorithm in the first differential neural network implemented as an non-parametric identifier. 6. Develop the fast learning algorithm in the first differential neural network implemented as an non-parametric identifier. 7. Divide the information considering the training period and the complementary validation period. 8. Evaluate both the slow and fast identifiers to reproduce the information corresponding to the information considered in the training period. 9. Repeat the identification task until the least mean square error of the identification error for both the slow and the fast learning is smaller than a given threshold value \u03b5. 10. Once the expected quality of training is expected, recover the values of the weights produced during this part of the process for both the slow and fast evaluations. 11. Develop the numerical simulation of the differential neural network working as the predictor using two models using the recovered weights from the slow and the fast evolution of the training algorithms. 12. Add the results of the slow and fast predictors to reconstruct the information during the prediction period. 13. Compare if possible, the information obtained from the Covid statistics during the prediction period with respect to the obtained data during the evaluation of the added identifier. 14. Determine the LeastMean Square Error and theMaximumError for the predicted period, if possible to characterize the quality of the prediction task."
        },
        {
            "heading": "5 Seventy Days Prediction of Infections and Deaths for Different Countries",
            "text": "This research uses a publicly available dataset \u201c2019 Novel Coronavirus Data Repositorypublished by Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE) available at: https://github.com/CSSEGISandData/COVID-19. Models achieved and the code used in their generation are available in a repository located at: https://github.com/ RitehAIandRobot/COVID-19-MLP. This information is proposed in COVID-19MLP, Riteh AI and Robotics Group, 2020, https://github.com/RitehAIandRobot/COVID-19-MLP.\nThe presented set of numerical simulations considered a temporal horizon of 70days. All the selected parameters were obtained using the Hurwitz conditions for A \u2212 LC . The values for the parameters considered in the activation functions were obtained with a uniform distribution for the exponential term, unitary gain with a fixed offset to 0.5. Hence, the parameters used for solving the numerical simulation for this the study was the following:\nA = 1.0 \u00b7 10\u22122 \u00b7\n\u23a1\n\u23a2 \u23a2 \u23a2\u23a2 \u23a3 \u221225 0 0 0 \u22128 \u221232 0 0 \u221212 \u221211 \u221244 0 \u221213 \u221213 \u221212 \u221252\n\u23a4\n\u23a5 \u23a5 \u23a5\u23a5 \u23a6\n(25)\nThe number of sigmoidal functions (artificial neurons in the DNN) used for the identification process was 9600. The parameters in the sigmoidal functions were \u03b1 j = 1 for all j = 1, . . . , 9600. The parameter in the denominator are \u03b2 j = 0.05 and \u03b2 j = 0.08 for all j = 1, . . . , 9600. The period T was fixed to 10 days. All the initial conditions were fixed as random values between 0 and 1. These selections were obtained using a trial and testing method that effectively estimated the number of infected and deceased persons with SarsCov2 virus. These estimations were evaluated using the collected information reported by the World Health Organization.\nThe values of matrices K , P , and L are as follows:\nK =\n\u23a1\n\u23a2\u23a2 \u23a3\n25.0 1.2 \u22121.5 2.3 1.2 32.0 \u22121.1 \u22121.3 \u22121.5 \u22121.1 44 1.2 2.3 \u22121.3 1.2 52\n\u23a4\n\u23a5\u23a5 \u23a6 ,\nP =\n\u23a1\n\u23a2\u23a2 \u23a3\n2.5 0.8 \u22121.2 \u22120.3 0.8 5.2 \u22121.1 \u22120.1 \u22121.2 \u22121.1 6.4 \u22121.2 \u22120.3 \u22120.1 \u22121.2 9.2\n\u23a4\n\u23a5\u23a5 \u23a6 ,\nL = [\u221215 \u221228 \u221232 \u221253 ] .\n\u23ab \u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23ac\n\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23ad\n(26)"
        },
        {
            "heading": "5.1 Turkey",
            "text": "Figure4 shows the comparison of estimated data evolution for infected people in Turkey. The comparison of trajectories confirms at first glance the effectiveness of the proposed DNN-based forecasting considering a period of estimation of 70 days. Moreover, it shows the effective estimation of the forecast information.\nFigure5 depicts the evolution of the predicted data for deceased people in Turkey using the proposed multi-rate identifier. In this case, there is a comparison considering the estimated data and the one corresponding to the actual data."
        },
        {
            "heading": "5.2 USA",
            "text": "Figure6 shows the comparison of estimated data evolution for infected people in the United States of America. The comparison of trajectories confirms at first glance the effectiveness of the proposed DNN-based forecasting considering a period of 70 days.\nFigure5 depicts the evolution of the predicted data for deceased people in the United States of America using the proposed multi-rate identifier. In this case, there is a comparison considering the estimated data and the one corresponding to the actual data.\nAll the previous results confirm that the proposed forecaster is based on the dual configuration of DNN. Moreover, the proposed technique can be easily implemented in different forecast problems taking advantage of the generalized formulation presented here.\nFor both studied cases, we included here some methods used for comparison including a traditional recurrent neural network (RNN), a Long-Short termmemory (LSTM), and a gated network unit (GNU). These networks were considered for comparison taking into account the significant outcomes shown before as potential predictors of complex time dependent information. We have presented two tables (one per infected and one per deceased persons) comparing some quality measurements, including the least mean square evaluation for the signals corresponding to the evolution of infected and deceased persons during the COVID outbreak (Tables 1 and 2). With the aim of introducing a fair comparison, the number of flops used for each of the prediction tasks was also estimated. These results confirm the\nTable 2 Comparison results for deceased persons showing the prediction outcomes for the infected persons and the flops used to perform the task\nMethod LMSE MEPP Flops\nRNN 45.5 48.0 5.4 \u00d7 107 LSTM 65.3 67.0 4.3 \u00d7 107 GNU 76.4 78.0 1.9 \u00d7 107 Slow-fast method 34.3 24.0 3.0 \u00d7 108\nadvances generated by applying the proposed predictor based on the combined learning method introduced in this study.\nThe proposed outcomes shown in the previous tables confirm the benefits of the proposed methodology, including the prediction quality, as well as the convergence conditions (noticing the maximum error value). However, the augmented number of flops required by the methodology considered in this study still requires some work to improve the prediction abilities. Moreover, showing the better least mean square errors obtained with the proposed methodology highlights the benefit of introducing the mixed learning with slow and fast dynamics."
        },
        {
            "heading": "6 Conclusions",
            "text": "\u2022 In this paper, it is shown that time-series datamay be effectivelymodeled by aDifferential Neural Network (DNN) with time-varying weights matrix parameters whose dynamics are governed by special Learning laws containing slow and fast components; \u2022 This study also demonstrates oneof the possible applications of the suggested technique to COVID-19 epidemic prediction, where we suppose that the current data set represents the output of some dynamic system, governed by a nonlinear ordinary differential equation; this method has been evaluated for two nations\u2019 databases (Turkey and the USA) and has demonstrated great performances (70 days of forecast).\nAcknowledgements The paperwas prepared under the financial support of theAutomatic Control Department at CINVESTAV-IPN, Mexico.\nData availibility The datasets generated during and/or analyzed during the current study are available from the corresponding author upon reasonable request.\nDeclarations\nConflict of interest All the authors declare no conflict of interest."
        }
    ],
    "title": "Differential Neural Networks Prediction Using Slow and Fast Hybrid Learning: Application to Prognosis of Infectionsand Deaths of COVID-19 Dynamics",
    "year": 2023
}