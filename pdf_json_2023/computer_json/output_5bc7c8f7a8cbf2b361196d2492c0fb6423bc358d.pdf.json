{
    "abstractText": "Keyword spotting (KWS) refers to the task of identifying a set of predefined words in audio streams. With the advances seen recently with deep neural networks, it has become a popular technology to activate and control small devices, such as voice assistants. Relying on such models for edge devices, however, can be challenging due to hardware constraints. Moreover, as adversarial attacks have increased against voice-based technologies, developing solutions robust to such attacks has become crucial. In this work, we propose VIC-KD, a robust distillation recipe for model compression and adversarial robustness. Using self-supervised speech representations, we show that imposing geometric priors to the latent representations of both Teacher and Student models leads to more robust target models. Experiments on the Google Speech Commands datasets show that the proposed methodology improves upon current state-of-the-art robust distillation methods, such as ARD and RSLAD, by 12% and 8% in robust accuracy, respectively.",
    "authors": [
        {
            "affiliations": [],
            "name": "Heitor R. Guimar\u00e3es"
        },
        {
            "affiliations": [],
            "name": "Arthur Pimentel"
        },
        {
            "affiliations": [],
            "name": "Anderson Avila"
        },
        {
            "affiliations": [],
            "name": "Tiago H. Falk"
        }
    ],
    "id": "SP:b1f48b7d4342f431c7d655c0a4535f2ec21413b5",
    "references": [
        {
            "authors": [
                "G. Chen",
                "C. Parada",
                "G. Heigold"
            ],
            "title": "Small-footprint keyword spotting using deep neural networks",
            "venue": "2014 IEEE international conference on acoustics, speech and signal processing (ICASSP), 2014, pp. 4087\u20134091.",
            "year": 2014
        },
        {
            "authors": [
                "D. O\u2019Shaughnessy"
            ],
            "title": "Trends and developments in automatic speech recognition research",
            "venue": "Computer Speech & Language, vol. 83, pp. 101538, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "A.R. Avila",
                "J. Monteiro",
                "D. O\u2019Shaughneussy",
                "T.H. Falk"
            ],
            "title": "Speech emotion recognition on mobile devices based on modulation spectral feature pooling and deep neural networks",
            "venue": "2017 IEEE International Symposium on Signal Processing and Information Technology (ISSPIT), 2017, pp. 360\u2013365.",
            "year": 2017
        },
        {
            "authors": [
                "A. Baevski",
                "Y. Zhou",
                "A. Mohamed"
            ],
            "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
            "venue": "Advances in Neural Information Processing Systems, vol. 33, pp. 12449\u201312460, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "S. Chen",
                "C. Wang",
                "Z. Chen"
            ],
            "title": "Wavlm: Largescale self-supervised pre-training for full stack speech processing",
            "venue": "IEEE Journal of Selected Topics in Signal Processing, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "G. Hinton",
                "O. Vinyals",
                "J. Dean"
            ],
            "title": "Distilling the knowledge in a neural network",
            "venue": "NIPS Deep Learning and Representation Learning Workshop, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "H.R. Guimar\u00e3es",
                "Y. Zhu",
                "O. Mengara"
            ],
            "title": "Assessing the vulnerability of self-supervised speech representations for keyword spotting under white-box adversarial attacks",
            "venue": "2023 IEEE International Conference on Systems, Man, and Cybernetics (SMC), 2023.",
            "year": 2023
        },
        {
            "authors": [
                "C. Zhao",
                "Z. Li",
                "H. Ding"
            ],
            "title": "Utio: Universal, targeted, imperceptible and over-the-air audio adversarial example",
            "venue": "2022 IEEE 28th International Conference on Parallel and Distributed Systems, 2023, pp. 346\u2013353.",
            "year": 2022
        },
        {
            "authors": [
                "A. Madry",
                "A. Makelov",
                "L. Schmidt"
            ],
            "title": "Towards deep learning models resistant to adversarial attacks",
            "venue": "International Conference on Learning Representations, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "H. Zhang",
                "Y. Yu",
                "J. Jiao",
                "E. Xing"
            ],
            "title": "Theoretically principled trade-off between robustness and accuracy",
            "venue": "International conference on machine learning. PMLR, 2019, pp. 7472\u20137482.",
            "year": 2019
        },
        {
            "authors": [
                "M. Goldblum",
                "L. Fowl",
                "S. Feizi"
            ],
            "title": "Adversarially robust distillation",
            "venue": "Proc. of the AAAI Conference on Artificial Intelligence, 2020, vol. 34, pp. 3996\u20134003.",
            "year": 2020
        },
        {
            "authors": [
                "B. Zi",
                "S. Zhao",
                "X. Ma"
            ],
            "title": "Revisiting adversarial robustness distillation: Robust soft labels make student better",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 16443\u201316452.",
            "year": 2021
        },
        {
            "authors": [
                "K.P. Huang",
                "Y.K. Fu",
                "T.Y. Hsu"
            ],
            "title": "Improving generalizability of distilled self-supervised speech processing models under distorted settings",
            "venue": "2022 IEEE Spoken Language Technology Workshop, 2023, pp. 1112\u20131119.",
            "year": 2022
        },
        {
            "authors": [
                "H.R. Guimar\u00e3es",
                "A. Pimentel",
                "A.R. Avila"
            ],
            "title": "Robustdistiller: Compressing universal speech representations for enhanced environment robustness",
            "venue": "2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2023, pp. 1\u20135.",
            "year": 2023
        },
        {
            "authors": [
                "J.B. Grill",
                "F. Strub",
                "F. Altch\u00e9"
            ],
            "title": "Bootstrap your own latent-a new approach to self-supervised learning",
            "venue": "Advances in neural information processing systems, vol. 33, pp. 21271\u201321284, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "J. Zbontar",
                "Li Jing",
                "I. Misra"
            ],
            "title": "Barlow twins: Selfsupervised learning via redundancy reduction",
            "venue": "International Conference on Machine Learning. PMLR, 2021, pp. 12310\u201312320.",
            "year": 2021
        },
        {
            "authors": [
                "A. Bardes",
                "J. Ponce",
                "Y. LeCun"
            ],
            "title": "VICReg: Varianceinvariance-covariance regularization for self-supervised learning",
            "venue": "International Conference on Learning Representations, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "P. Warden"
            ],
            "title": "Speech commands: A dataset for limited-vocabulary speech recognition",
            "venue": "arXiv preprint arXiv:1804.03209, 2018.",
            "year": 1804
        },
        {
            "authors": [
                "M. Ravanelli",
                "T. Parcollet",
                "P. Plantinga"
            ],
            "title": "Speech- Brain: A general-purpose speech toolkit",
            "venue": "2021, arXiv:2106.04624.",
            "year": 2021
        },
        {
            "authors": [
                "S. Choi",
                "S. Seo",
                "B. Shin"
            ],
            "title": "Temporal Convolution for Real-Time Keyword Spotting on Mobile Devices",
            "venue": "Proc. Interspeech 2019, 2019, pp. 3372\u20133376.",
            "year": 2019
        },
        {
            "authors": [
                "D. Snyder",
                "D. Garcia-Romero",
                "G. Sell"
            ],
            "title": "Xvectors: Robust dnn embeddings for speaker recognition",
            "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018, pp. 5329\u20135333.",
            "year": 2018
        },
        {
            "authors": [
                "F. Croce",
                "M. Hein"
            ],
            "title": "Reliable evaluation of adversarial robustness with an ensemble of diverse parameterfree attacks",
            "venue": "International conference on machine learning. PMLR, 2020, pp. 2206\u20132216.",
            "year": 2020
        },
        {
            "authors": [
                "F. Croce",
                "M. Hein"
            ],
            "title": "Minimally distorted adversarial examples with a fast adaptive boundary attack",
            "venue": "International Conference on Machine Learning. PMLR, 2020, pp. 2196\u20132205.",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014 Keyword Spotting, Adversarial Robustness, Knowledge Distillation, Robust Distillation, VICReg.\n1. INTRODUCTION\nRecent advances in the deep learning field have profoundly impacted the speech-processing community. As such, it has resulted in cutting-edge systems for tasks such as keyword spotting (KWS) [1] and speech/emotion recognition [2, 3], to name a few. In particular, KWS systems are usually the first layer of virtual assistants, where such models attempt to identify preset words in an utterance to activate the device (e.g., \u201cHey Siri\u201d). On-device speech processing, such as Google Now and Alexa, is becoming ubiquitous in our daily lives; however, this can impose challenges on energy efficiency, real-time processing, and on user privacy.\nFor KWS, the use of self-supervised speech representation learning (S3RL) has become a popular tool. S3RL models, such as Wav2Vec 2.0 [4] and WavLM [5], are designed to learn combined acoustic and language models for continuous audio inputs. Notwithstanding, when working with edge devices, it is crucial to take into account model size due to hard-\nware constraints. To this end, knowledge distillation (KD) [6] has emerged as a powerful tool to transfer knowledge from a larger (Teacher) model to a smaller (Student) one, resulting in comparable generalization capabilities.\nMoreover, it is known that edge processing can provide an extra layer of security protection for the user, ensuring that private data is processed locally on the device and not sent over the cloud to a third-party server. However, recent research has shown that KWS systems based on self-supervised representations can be vulnerable to so-called adversarial attacks during inference time [7]. Adversarial attacks aim to design a small perturbation \u03b4 that when added to the test signal will force the system to fail. Over-the-air adversarial attacks, for example, have shown that imperceptible noise can be added to the user\u2019s voice resulting in misclassifications [8].\nAdversarial training (AT) [9] has emerged as a potential defense technique against adversarial attacks. AT is a robust optimization formulation that, in practice, can be seen as a data augmentation technique that generates adversarial versions of a natural sample to be used during training of Teacher models. The TRADES (TRadeoff-inspired Adversarial DEfense via Surrogate-loss minimization) method, for example, has generated inspiring results [10]. Efficiently designing small (Student) models, however, is a challenging task. The Adversarial Robust Distillation (ARD) [11] method combines adversarial training with KD, emphasizing the importance of minimizing the Kullback-Leibler divergence between Student and Teacher logits to enhance Student robustness. In contrast, the Robust Soft Label Adversarial Distillation (RSLAD) [12] method uses soft labels from the Teacher model (in lieu of hard labels) for guidance during distillation, yielding better results than ARD empirically. Notwithstanding, neither of these methods can consistently surpass the accuracy of a smaller model trained with AT, thus suggesting that further innovations are needed if large self-supervised speech representations are to be used in edge devices.\nIn this work, we propose VIC-KD, a novel distillation recipe that compresses the S3RL model size while increasing its robustness against adversarial attacks. In particular, we show that multi-view inputs and geometric constraints on the latent space of the Student model are essential to achieve these advantages. We consider Student models with fewer\nar X\niv :2\n30 9.\n12 91\n4v 1\n[ ee\nss .A\nS] 2\n2 Se\np 20\n23\nthan 96K parameters and 3 MMACs, and knowledge distillation is performed from fine-tuned versions of Wav2Vec 2.0 and WavLM. Results show that our distillation recipe can achieve better robustness than traditional defense techniques, such as TRADES, and improve upon robust distillation methods, such as ARD and RSLAD.\n2. VARIANCE-INVARIANCE-COVARIANCE KNOWLEDGE DISTILLATION\nHerein, instead of using the Teacher model to guide how the Student logits should behave, we induce some geometric properties of the latent space of the Student via the varianceinvariance-covariance regularization and the usage of multiview inputs to each model. Although the usage of multi-view inputs for KD has already been explored in the literature to increase the environmental robustness of Student models [13, 14], its effect on robust distillation still needs to be determined. Figure 1 depicts a block diagram of the proposed VIC-KD distillation recipe.\nIn the self-supervised learning literature, joint embedding architectures (JEA) are becoming popular due to their effectiveness in learning latent factors from data [15, 16]. VICReg [17] is one such method that is based on preserving the information of the embeddings while avoiding representational collapse. Here, we expand those ideas to a robust distillation method. First, we sample an utterance from x \u223c Dtrain and two random transformations {t, t\u2032 \u223c T |t \u0338= t\u2032}. As depicted in the upper branch of Figure 1, the Teacher model T\u03b8 is responsible for extracting representations from the speech input. More precisely, representation Z is extracted, which is the weighted sum of the intermediate representations from all Transformer layers, and aggregated over the time dimension.\nThe Student branch, on the other hand, receives the other utterance view generated by t\u2032 with an added adversary perturbation. First, for both inputs, we extract the latent representations of the Student model, denoted by H \u2032. This representation is then fed to a classification head, generating the model logits Y \u2032 that attempt to predict the spoken commands. In parallel, H \u2032 is fed to a projection head responsible for generating Z \u2032, which has the same dimensionality as the Teacher\nlatent representation. Note that, at test time, only the Student encoder S\u03b8\u2032 and the classification head are used.\nIn addition to the mechanism described above, we now describe the contribution of each term that composes the threefactor VIC-KD loss function, presented in equation 1: (1) variance, (2) invariance, and (3) covariance terms. Following our previous notation, we define Z = [z1, z2, ..., zn] and Z \u2032 = [z\u20321, z \u2032 2, ..., z \u2032 n] as the d-dimensional latent representations generated by the Teacher and Student models for natural and adversarially perturbed input, respectively, for a batch of n utterances. First, the invariance term consists of the mean squared error between the two latent representations, Z and Z \u2032. Next, the variance term induces the learned latent embedding to vary within the other batch elements, thus avoiding collapse on the same vector. Lastly, the covariance term prevents the Student model, which already has limited capacity, from encoding similar features. In our experiments, we consider an equal contribution of each term in the final VICReg loss. The interested reader is referred to [17] for more details about the loss implementation.\nLVICReg = Var(Z \u2032) + Inv(Z,Z \u2032) + Cov(Z \u2032) (1)\nFinally, the VIC-KD loss is computed as the convex combination between the TRADES [10] and VICReg losses, controlled through a hyperparameter \u03b1 as described in Figure 1.\n3. EXPERIMENTAL SETUP\nTo train the KWS system, we use the Google Speech Commands v0.02 (GSC) dataset [18]. This dataset has about 100k 1-second utterances spread across 35 commands and sampled at 16 kHz. We consider two versions of the dataset, one with all commands and another with only 12 classes that include labels such as {yes, no, up, down, left, right, on, off, stop, go, unknown, and silence}. The silence class is made up of background noises with no speech, while the unknown label includes utterances uniformly sampled from unused classes. We follow the SpeechBrain [19] recipe, where the dataset is split into train, validation, and test sets in the ratio of 80%, 10%, and 10%, respectively.\nIn this study, we examine two Teacher models, Wav2Vec 2.0 and WavLM, along with two Student models, TC-ResNet-\n8 [20] and a custom XVector [21], with a reduced number of TDNN layers and smaller kernel sizes to fit our resource constraints. Furthermore, as summarized in Table 1, we show the results with conventional training, as well as AT via TRADES. Moreover, we report accuracy for clean speech files, as well as speech files corrupted by the AutoAttack method [22]. Experiments herein rely on these reference values for comparison.\nBaseline models are trained for 100 epochs, with a batch size of 32 samples, using an Adam optimizer, and learning of 10\u22123 that linearly decays to 10\u22124. Conversely, we fine-tune the Teacher model for the KWS task for 10 epochs, and the learning rate is scheduled from 5 \u00d7 10\u22124 to 5 \u00d7 10\u22125. The Teacher model consists of the S3RL encoder with a weighted sum and aggregation mechanism described previously and a simple linear layer that classifies the input speech signal into the desired number of classes. Note that, for VIC-KD, we can discard the Teacher\u2019s linear layer since we are interested in the latent representation. For adversarial attacks, we rely on AutoAttack [22], an ensemble method comprised of APGD [22], APGD-T [22], and FAB [23] attacks. We consider \u2113\u221e bounded attacks with an \u03f5 = 1.5\u00d7 10\u22123.\nThree distillation methods, namely KD, ARD, and RSLAD, are used to benchmark the proposed technique. Training is performed for 250 epochs with parameters matching the small baseline models, as specified above. VIC-KD relies on a 10- step PGD-like attack, also with an \u03f5 = 1.5\u00d7 10\u22123 and a step size of 3 \u00d7 10\u22124, to generate the adversarial perturbation for training. Additionally, multi-view inputs are used for Student and Teacher models, incorporating clean data, noise, reverberation, noise-plus-reverberation, wave chunk dropping, and speed perturbation into the transformations T .\n4. EXPERIMENTAL RESULTS AND DISCUSSION"
        },
        {
            "heading": "4.1. Classification accuracy",
            "text": "Table 2 presents the main experimental results exploring two scenarios: standard vs. robust Teachers as guides. We report clean and robust accuracies for each robust distillation method, with differences relative to the respective baseline in parentheses. In fact, given the low robust accuracy and that KD is not a robust method, we do not compute its delta against the robust baseline. KD with standard Teacher outper-"
        },
        {
            "heading": "KD [6]",
            "text": "ARD [11]\nWav2Vec 2.0 / TC-ResNet 94.54 76.41 (-2.8%) 94.92 74.16 (-5.1%) Wav2Vec 2.0 / XVector 95.37 79.74 (-0.6%) 95.71 79.94 (-0.4%) WavLM / TC-ResNet 95.17 74.13 (-5.1%) 95.30 76.87 (-2.4%)\nWavLM / XVector 95.27 78.33 (-2.0%) 95.29 74.03 (-6.3%)\nRSLAD [12]\nWav2Vec 2.0 / TC-ResNet 94.65 78.06 (-1.2%) 94.59 76.27 (-3.0%) Wav2Vec 2.0 / XVector 95.37 80.79 (+0.5%) 95.45 81.98 (+1.7%) WavLM / TC-ResNet 95.11 77.17 (-2.1%) 94.11 77.99 (-1.3%)\nWavLM / XVector 95.34 81.30 (+1.0%) 95.69 81.90 (+1.6%)\nVIC-KD (Ours)\nWav2Vec 2.0 / TC-ResNet 95.31 83.75 (+4.5%) 95.48 83.38 (+4.1%) Wav2Vec 2.0 / XVector 96.50 86.12 (+5.8%) 96.29 85.33 (+5.0%) WavLM / TC-ResNet 95.39 83.30 (+4.1%) 95.37 83.73 (+4.5%)\nWavLM / XVector 95.92 86.08 (+5.8%) 96.39 86.31 (+6.0%)\nforms baselines and other robust methods in clean accuracy. However, despite some improvement, the robust accuracy is still below random guess. ARD, on the other hand, significantly enhances overall robust accuracy, but still falls short of the baseline performance under the robust condition.\nRegardless, we observe an improvement in the overall scenario for RSLAD and VIC-KD, which are built upon TRADES. For the Student model with bigger capacity, e.g., the XVector, we observe the first results where the model can improve upon the baseline for robust accuracy. The TCResNet on the RSLAD distillation has inline results with the baseline, but improves upon the ARD method. On the other hand, VIC-KD for both TC-ResNet and XVector can substantially improve upon both robust distillation recipes and the baselines for the clean and under-attack scenarios, thus showing the benefits of the proposed methodology. In fact, VIC-KD with the Wav2Vec 2.0 / XVector pair outperforms the baseline by a relative percentual difference of 7.2% on robust accuracy. Similarly, the proposed model outperforms ARD\nand RSLAD on attacked scenarios by 8.0% and 6.6%, respectively. On the downside, the VIC-KD recipe training time is four times slower than RSLAD. Note that, at the inference stage, the time is the same for all distillation recipes since it depends solely on the architecture of the student model.\nFinally, some conclusions can be drawn from the distillation of robust Teachers. First, from the KD experiment, it is crucial to notice that the Student model does not necessarily inherit robustness from the Teacher; hence, specific techniques are needed if the goal is to design adversarially robust models. However, a tradeoff between clean and robust accuracy needs to be decided. For the robust distillation methods, we observe that using robust Teachers, in general, does not improve the final performance of the Student model; thus, for our specific use case on the GSC dataset, unless one already has off-the-shelf robust Teacher models, it is not worth the extra computation of transforming a standard Teacher into a robust one first and then perform the robust distillation."
        },
        {
            "heading": "4.2. The effects of multi-view inputs",
            "text": "Next, we investigate the effect of multi-view (MV) inputs on robust distillation, as shown in Fig.2. Here, we investigate the WavLM/TC-ResNet pair as our Teacher and Student models, respectively. For comparison, a red dashed line shows the baseline robust accuracy of TC-ResNet trained with TRADES without any guidance from Teacher models. As observed in the figure, all distillation methods benefit from multi-view. For instance, ARD and RSLAD, which had a robust accuracy below the baseline, can surpass this landmark after MV. Our proposed VIC-KD method can outperform the baseline with or without MV. However, MV helps us to achieve the best overall robust accuracy.\nWe hypothesize that the reason behind the performance gain by using MV is two-fold. First, MV induces the student model to learn perturbation-invariant features from speech signals and to better disentangle noisy factors from speech\nfeatures, thus improving generalization. Lastly, other works already have discussed that self-supervised learning methods that employ MV are forcing the model to maximize the mutual information of representations between the two views [17]. We suppose a similar conclusion can be drawn for the KD case, but more studies are still needed."
        },
        {
            "heading": "4.3. Expanding the distillation to more classes",
            "text": "Lastly, we further stress the distillation recipes to a more significant number of output classes by using all 35 classes from the GSC dataset. In Fig. 3, we show the accuracy results for both clean and AutoAttack-based methods using the WavLM / XVector models as Teacher and Student respectively. The blue and green dashed lines denote the clean and robust accuracy of the supervised baseline, XVector, trained via TRADES. Similarly to previous findings, KD can surpass the clean accuracy of the baseline, but the robust accuracy is not satisfactory. Among the robust distillation methods, VICKD exhibits the overall best performance by achieving the best accuracy and showing the smallest gap between clean and robust accuracies.\n5. CONCLUSIONS\nHere, we propose VIC-KD, a methodology to improve the adversarial robustness of distilled models. Our recipe is twofold: (1) use multi-view inputs to induce the Student to learn perturbation-invariant features and (2) apply the varianceinvariance-covariance regularization (VICReg) to the latent representation of the Teacher/Student model. Experiments on the Google Speech Command dataset, with 12 and 35 classes, show the proposed methodology outperforming state-of-theart robust distillation recipes on \u2113\u221e-bounded ensemble of attack (AutoAttack). Overall, VIC-KD canbetter balance the tradeoff between clean and robust accuracy, making this technique a strong candidate for developing and deploying trustworthy speech applications on the edge.\n6. REFERENCES\n[1] G. Chen, C. Parada, and G. Heigold, \u201cSmall-footprint keyword spotting using deep neural networks,\u201d in 2014 IEEE international conference on acoustics, speech and signal processing (ICASSP), 2014, pp. 4087\u20134091.\n[2] D. O\u2019Shaughnessy, \u201cTrends and developments in automatic speech recognition research,\u201d Computer Speech & Language, vol. 83, pp. 101538, 2023.\n[3] A. R. Avila, J. Monteiro, D. O\u2019Shaughneussy, and T. H. Falk, \u201cSpeech emotion recognition on mobile devices based on modulation spectral feature pooling and deep neural networks,\u201d in 2017 IEEE International Symposium on Signal Processing and Information Technology (ISSPIT), 2017, pp. 360\u2013365.\n[4] A. Baevski, Y. Zhou, A. Mohamed, et al., \u201cwav2vec 2.0: A framework for self-supervised learning of speech representations,\u201d Advances in Neural Information Processing Systems, vol. 33, pp. 12449\u201312460, 2020.\n[5] S. Chen, C. Wang, Z. Chen, et al., \u201cWavlm: Largescale self-supervised pre-training for full stack speech processing,\u201d IEEE Journal of Selected Topics in Signal Processing, 2022.\n[6] G. Hinton, O. Vinyals, and J. Dean, \u201cDistilling the knowledge in a neural network,\u201d in NIPS Deep Learning and Representation Learning Workshop, 2015.\n[7] H. R. Guimara\u0303es, Y. Zhu, O. Mengara, et al., \u201cAssessing the vulnerability of self-supervised speech representations for keyword spotting under white-box adversarial attacks,\u201d in 2023 IEEE International Conference on Systems, Man, and Cybernetics (SMC), 2023.\n[8] C. Zhao, Z. Li, H. Ding, et al., \u201cUtio: Universal, targeted, imperceptible and over-the-air audio adversarial example,\u201d in 2022 IEEE 28th International Conference on Parallel and Distributed Systems, 2023, pp. 346\u2013353.\n[9] A. Madry, A. Makelov, L. Schmidt, et al., \u201cTowards deep learning models resistant to adversarial attacks,\u201d in International Conference on Learning Representations, 2018.\n[10] H. Zhang, Y. Yu, J. Jiao, E. Xing, et al., \u201cTheoretically principled trade-off between robustness and accuracy,\u201d in International conference on machine learning. PMLR, 2019, pp. 7472\u20137482.\n[11] M. Goldblum, L. Fowl, S. Feizi, et al., \u201cAdversarially robust distillation,\u201d in Proc. of the AAAI Conference on Artificial Intelligence, 2020, vol. 34, pp. 3996\u20134003.\n[12] B. Zi, S. Zhao, X. Ma, et al., \u201cRevisiting adversarial robustness distillation: Robust soft labels make student better,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 16443\u201316452.\n[13] K.P. Huang, Y.K. Fu, T.Y. Hsu, et al., \u201cImproving generalizability of distilled self-supervised speech processing models under distorted settings,\u201d in 2022 IEEE Spoken Language Technology Workshop, 2023, pp. 1112\u20131119.\n[14] H. R. Guimara\u0303es, A. Pimentel, A. R. Avila, et al., \u201cRobustdistiller: Compressing universal speech representations for enhanced environment robustness,\u201d in 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2023, pp. 1\u20135.\n[15] J.B. Grill, F. Strub, F. Altche\u0301, et al., \u201cBootstrap your own latent-a new approach to self-supervised learning,\u201d Advances in neural information processing systems, vol. 33, pp. 21271\u201321284, 2020.\n[16] J. Zbontar, Li Jing, I. Misra, et al., \u201cBarlow twins: Selfsupervised learning via redundancy reduction,\u201d in International Conference on Machine Learning. PMLR, 2021, pp. 12310\u201312320.\n[17] A. Bardes, J. Ponce, and Y. LeCun, \u201cVICReg: Varianceinvariance-covariance regularization for self-supervised learning,\u201d in International Conference on Learning Representations, 2022.\n[18] P. Warden, \u201cSpeech commands: A dataset for limited-vocabulary speech recognition,\u201d arXiv preprint arXiv:1804.03209, 2018.\n[19] M. Ravanelli, T. Parcollet, P. Plantinga, et al., \u201cSpeechBrain: A general-purpose speech toolkit,\u201d 2021, arXiv:2106.04624.\n[20] S. Choi, S. Seo, B. Shin, et al., \u201cTemporal Convolution for Real-Time Keyword Spotting on Mobile Devices,\u201d in Proc. Interspeech 2019, 2019, pp. 3372\u20133376.\n[21] D. Snyder, D. Garcia-Romero, G. Sell, et al., \u201cXvectors: Robust dnn embeddings for speaker recognition,\u201d in 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018, pp. 5329\u20135333.\n[22] F. Croce and M. Hein, \u201cReliable evaluation of adversarial robustness with an ensemble of diverse parameterfree attacks,\u201d in International conference on machine learning. PMLR, 2020, pp. 2206\u20132216.\n[23] F. Croce and M. Hein, \u201cMinimally distorted adversarial examples with a fast adaptive boundary attack,\u201d in International Conference on Machine Learning. PMLR, 2020, pp. 2196\u20132205."
        }
    ],
    "title": "VIC-KD: VARIANCE-INVARIANCE-COVARIANCE KNOWLEDGE DISTILLATION TO MAKE KEYWORD SPOTTING MORE ROBUST AGAINST ADVERSARIAL ATTACKS",
    "year": 2023
}