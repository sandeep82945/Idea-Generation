{
    "abstractText": "Observational studies of recurrent event rates are common in biomedical statistics. Broadly, the goal is to estimate differences in event rates under two treatments within a defined target population over a specified followup window. Estimation with observational claims data is challenging because while membership in the target population is defined in terms of eligibility criteria, treatment is rarely assigned exactly at the time of eligibility. Ad-hoc solutions to this timing misalignment, such as assigning treatment at eligibility based on subsequent assignment, incorrectly attribute prior event rates to treatment resulting in immortal risk bias. Even if eligibility and treatment are aligned, a terminal event process (e.g. death) often stops the recurrent event process of interest. Both processes are also censored so that events are not observed over the entire followup window. Our approach addresses misalignment by casting it as a treatment switching problem: some patients are on treatment at eligibility while others are off treatment but may switch to treatment at a specified time if they survive long enough. We define and identify an average causal effect of switching under specified causal assumptions. Estimation is done using a g-computation framework with a joint semiparametric Bayesian model for the death and recurrent event processes. Computing the estimand for various switching times allows us to assess the impact of treatment timing. We apply the method to contrast hospitalization rates under different opioid treatment strategies among patients with chronic back pain using Medicare claims data. \u2217Email: arman oganisian@brown.edu 1 ar X iv :2 30 4. 03 24 7v 1 [ st at .M E ] 6 A pr 2 02 3",
    "authors": [
        {
            "affiliations": [],
            "name": "Arman Oganisian"
        },
        {
            "affiliations": [],
            "name": "Anthony Girard"
        },
        {
            "affiliations": [],
            "name": "Jon A. Steingrimsson"
        }
    ],
    "id": "SP:ea8c9fb3421af097a1308faac68bd5131b45f993",
    "references": [
        {
            "authors": [
                "Chien-Lin Su",
                "Robert W Platt",
                "Jean-Fran\u00e7ois Plante"
            ],
            "title": "Causal inference for recurrent event data using pseudo-observations",
            "venue": "Biostatistics, 23(1):189\u2013206,",
            "year": 2020
        },
        {
            "authors": [
                "Douglas E. Schaubel",
                "Min Zhang"
            ],
            "title": "Estimating treatment effects on the marginal recurrent event mean in the presence of a terminating event",
            "venue": "Lifetime Data Analysis,",
            "year": 2010
        },
        {
            "authors": [
                "Chien-Lin Su",
                "Russell Steele",
                "Ian Shrier"
            ],
            "title": "Doubly robust estimation and causal inference for recurrent event data",
            "venue": "Statistics in Medicine,",
            "year": 2020
        },
        {
            "authors": [
                "Tianmeng Lyu",
                "Bj\u00f6rn Bornkamp",
                "Guenther Mueller-Velten",
                "Heinz Schmidli"
            ],
            "title": "Bayesian inference for a principal stratum estimand on recurrent events truncated by death",
            "year": 2022
        },
        {
            "authors": [
                "Matias Janvin",
                "Jessica G. Young",
                "P\u030aal C. Ryalen",
                "Mats J. Stensrud"
            ],
            "title": "Causal inference with recurrent and competing events. arXiv, 2022",
            "venue": "doi: 10.48550/ARXIV.2202.08500",
            "year": 2022
        },
        {
            "authors": [
                "James Robins"
            ],
            "title": "A new approach to causal inference in mortality studies with a sustained exposure period\u2014application to control of the healthy worker survivor effect",
            "venue": "Mathematical Modelling,",
            "year": 1986
        },
        {
            "authors": [
                "Arman Oganisian",
                "Jason A. Roy"
            ],
            "title": "A practical introduction to bayesian estimation of causal effects: Parametric and nonparametric approaches",
            "venue": "Statistics in Medicine,",
            "year": 2021
        },
        {
            "authors": [
                "Jessica G. Young",
                "Mats J. Stensrud",
                "Eric J. Tchetgen Tchetgen",
                "Miguel A. Hern\u00e1n"
            ],
            "title": "A causal framework for classical statistical estimands in failure-time settings with competing events",
            "venue": "Statistics in Medicine,",
            "year": 2020
        },
        {
            "authors": [
                "Miguel A Hern\u00e1n",
                "Babette Brumback",
                "James M Robins"
            ],
            "title": "Marginal structural models to estimate the joint causal effect of nonrandomized treatments",
            "venue": "Journal of the American Statistical Association,",
            "year": 2001
        },
        {
            "authors": [
                "W.A. Thompson"
            ],
            "title": "On the treatment of grouped observations in life studies",
            "venue": "Biometrics, 33(3):463\u2013470,",
            "year": 1977
        },
        {
            "authors": [
                "Donald B. Rubin"
            ],
            "title": "The Bayesian Bootstrap",
            "venue": "The Annals of Statistics,",
            "year": 1981
        },
        {
            "authors": [
                "James Dahlhamer",
                "Jacqueline Lucas",
                "Carla Zelaya",
                "Richard Nahin",
                "Sean Mackey",
                "Lynn DeBar",
                "Robert Kerns",
                "Michael Von Korff",
                "Linda Porter",
                "Charles Helmick"
            ],
            "title": "Prevalence of chronic pain and highimpact chronic pain among adults\u2014united states, 2016",
            "venue": "Morbidity and Mortality Weekly Report,",
            "year": 2018
        },
        {
            "authors": [
                "Kevin E Vowles",
                "Mindy L McEntee",
                "Peter Siyahhan Julnes",
                "Tessa Frohe",
                "John P Ney",
                "David N Van Der Goes"
            ],
            "title": "Rates of opioid misuse, abuse, and addiction in chronic pain: a systematic review and data synthesis",
            "venue": "Pain, 156(4):569\u2013576,",
            "year": 2015
        },
        {
            "authors": [
                "Bob Carpenter",
                "Andrew Gelman",
                "Matthew D Hoffman",
                "Daniel Lee",
                "Ben Goodrich",
                "Michael Betancourt",
                "Marcus Brubaker",
                "Jiqiang Guo",
                "Peter Li",
                "Allen Riddell"
            ],
            "title": "Stan: A probabilistic programming language",
            "venue": "Journal of statistical software,",
            "year": 2017
        },
        {
            "authors": [
                "John D Kalbfleisch",
                "Ross L Prentice"
            ],
            "title": "The statistical analysis of failure time data",
            "year": 2011
        }
    ],
    "sections": [
        {
            "text": "\u2217Email: arman oganisian@brown.edu\nar X\niv :2\n30 4.\n03 24\n7v 1\n[ st\nat .M\nE ]\n6 A\npr 2\n02 3"
        },
        {
            "heading": "1 Introduction",
            "text": "The increasing availability of large claims data bases and electronic health records has popularized observational studies of recurrent event outcomes. The goal of these analyses is generally to contrast event rates within a target population over a given followup window under hypothetical treatment interventions (hereafter, treated and untreated). Causal recurrent event modeling using observational data poses two unique sets of challenges. The first set is related to causal modeling. While many recurrent events can be observed for a given subject, the recurrent event process is often terminated by some other \u201cterminal\u201d event such as death. Additionally, a censoring process coarsens the data up to a lower bound on both survival time and the accumulated number of events within the followup window. Moreover, event rates are inherently composite outcomes with survival time in the denominator and event counts in the numerator. Since both are potential outcomes of treatment, they must be modeled jointly over time as a function of treatment. Ad-hoc approaches such as including death as an event or censoring patients upon death are not appropriate.\nA second set of difficulties is in the design of the study - specifically, when to begin followup. Principles of target trial emulation [Herna\u0301n and Robins, 2016] suggest that followup start when 1) eligibility criteria are met, 2) treatment strategies are assigned, and 3) the recurrent/terminal events begin to be counted. The eligibility criteria define membership into the target population, which is crucial for interpretation. The design issue arises when patients rarely receive treatment exactly at the time of eligibility. This timing misalignment leads to an array of ad-hoc strategies of determining treatment assignment that may induce bias. These include looking past eligibility to check whether a patient was ever treated and, if so, classifying them as being in the treatment group. A comparison of post-eligibility event rates between groups will in general be subject to immortal risk bias: in the constructed treated group, survival time (the immortal time) and events (the risk) accrued between eligibility and treatment initiation are misattributed to treatment. Alternatively, analyzing events starting from time of treatment initiation may induce survivor bias as it shifts the target population to the subpopulation who survived long enough to start treatment. If survival is differential across treatments, this could lead to bias in the treatment effect estimate.\nOur contributions advance existing work along several important fronts. Immortal time bias resulting from misalignment of study and treatment starts is well-studied with survival outcomes and various methods such as grace periods and landmarking have been proposed [Duchesneau et al., 2022, Wang et al., 2022, Karim et al., 2016]. While precise causal framing of immortal time bias, grace periods [Wanis et al., 2022], and solutions such as cloning [Herna\u0301n, 2018] have been formulated, direct application to recurrent event data is not appropriate. As we show, recurrent event analysis under such misalignment requires a separate set of identification assumptions, estimands, and models.\nOn the other hand, the literature on recurrent events has led to development of important classes of models such as proportional mean models, frailty models, and discrete-time transition models [Amorim and Cai, 2014]. Recently, interest has grown in developing a more precise causal framing of these settings. For instance, one proposal [Su et al., 2020a] uses a set of estimators of causal effects on cumulative event rate over a specified time. However, unlike our approach, it does does not consider terminal events such as death within followup. Inverse-probability weighted estimators [Schaubel and Zhang, 2010] with doubly-robust extensions [Su et al., 2020b] have also been developed. However, these approaches are developed in the point-treatment setting where there is no timing misalignment. Thus they need not contend with causal identification and estimation in our setting with a time-varying treatment. Other work Lyu et al. [2022] develops Bayesian estimation for a difference in average potential number of events within a principal stratum of survivors. However, a drawback of principal stratification is that stratum membership is not identifiable, leading to difficulty in interpretation. In contrast, our work\ntargets a marginal estimand with a clean interpretation. Recent work [Janvin et al., 2022] reviews causal estimands in recurrent events and provides a discrete-time causal framework but in the point-treatment setting with no timing misalignment.\nIn this manuscript we develop an all-in-one Bayesian approach that simultaneously 1) handles timing misalignment, 2) is valid under covariate-dependent right-censoring and 3) accounts for terminal processes while modeling counts. Specifically, we define a monotone time-varying treatment indicator that is zero before starting on treatment and one on and after the treatment switch. The causal target estimand of interest is defined as the difference in average potential incidence rates had subjects switched to treatment at time s versus s\u2032. This emulates a trial in which patients are enrolled at eligibility and randomized to switch to treatment at some specified time. Under assumptions on the joint treatment-censoring mechanism, we derive a g-formula [Robins, 1986] that nonparametically identifies this estimand in terms of the joint distribution of survival and event counts. The modeling challenge is addressed by a sequence of two semiparametric Bayesian models: a discrete-time survival model and a model for the count of events at a time interval given survival up to that interval. Tailored priors are developed to regularize the models when the at-risk count is low. In addition, computing the estimand across various s allows us to assess the impact of the treatment\u2019s timing. We apply this method to an analysis of opioid treatments using Medicare claims data. The goal of the analysis is to do inference for the causal effect of opioid treatment on hospitalization rates among patients with chronic back pain who suffer from opioid use disorder. We apply our method to appropriately 1) adjust for confounding, 2) avoid biases due to opioid initiation timing, and 3) jointly model recurrent and terminal events in the presence of censoring.\nMoreover, we contribute to the literature on Bayesian causal inference [Oganisian and Roy, 2021]. We are unaware of any Bayesian methods to date for causal analysis of recurrent event data to accommodate timing misalignment. A Bayesian approach here is valuable as discrete-time models, though common in causal inference, are difficult to fit with standard frequentist methods. For instance, Young et al. [2020] use such models to estimate causal effects on time-to-event outcomes in the presence of competing risks. They document having to simplify the models as \u201cthe construction of confidence intervals failed to converge under the more flexible model\u201d in the required bootstrap procedures. This is unsurprising as the bootstrap is fundamentally non-smooth, leading to pathological iterations where the models do not converge even if the analysis on the full data set does converge. This is all too common in finite samples with rare events and as the at-risk counts decline in the followup. The post-selection inference issues of iterating between many different models until convergence is attained may compromise the validity of frequentist inference. Discarding the pathological bootstrap iterations is also not proper. In contrast, Bayesian estimation with posterior g-computation is smooth and we show in simulations that this smoothness endows it with desirable frequentist properties."
        },
        {
            "heading": "2 Observed Longitudinal Data Structure",
            "text": "We consider a general setting where the aim is to estimate the effect of treatment on the incidence of a recurrent events within a target population over a specified time window. In observational studies, membership in the target population is defined in terms of eligibility criteria. While general in nature, we motivate the problem setup within the context of our data application for concreteness. In our data application, the treatment of interest is opioid use and the target population consists of treatmentnaive patients newly diagnosed with chronic pain. Eligibility is defined using two diagnosis codes for chronic pain and no claims for opioids in the year prior to diagnosis. Upon the first instance of meeting all the eligibility criteria, the patient is considered to be eligible for treatment. From a target trial emulation perspective, it is helpful to view this patient as being enrolled into a hypothetical trial with\nthe following setup: At the time of eligibility (time zero), we record a set of covariate information, denoted by p\u2212dimensional vector L \u2208 L, and follow patients for a maximum of \u03c4 time points (measured in weeks). Though any covariate type is allowable, we assume all covariates are discrete to avoid measure-theoretic notation in the presentation. In this hypothetical trial, patients are conditionally randomized to either switch to treatment at some time s (which could be exactly at enrollment or any time afterwards) or never switch within trial followup (s > \u03c4).\nIn the observed claims data, patients with chronic pain switch to opioid within the followup period at time 0 \u2264 W < \u03c4 . For patients who switch to opioids at eligibility, W = 0, while for patients who are never assigned opioids in the followup period W > \u03c4 . The goal is to contrast hospitalization rate in the followup period had everyone in the target population switched to treatment immediately at eligibility versus delayed switching by some time - an estimand that will be precisely defined in the next section. Following eligibility, patients may die at time T or be censored at time C\u0303 = min(C, \u03c4), where C is some subject-level censoring time. In our claims data, C is the first of either the date of the end of the data cut or date at which medicare eligibility is lost - both relative to a patient\u2019s eligibility date. The value \u03c4 is end of the followup window measured from patient\u2019s eligibility date. Let T\u0303 = min(T, C\u0303) be the observed time on study with death indicator \u03b4T = I(T < C\u0303). Additionally, we observe event occurrence history up to this time. Specifically, we observe J events with the jth event occurring at time TY j < T\u0303 . Let Y (T\u0303 ) = {TY j : TY j < T\u0303 \u2200j}Jj=1 denote the observed events in the followup period. In practical settings patients may die or be censored before we ever observe treatment switch. Let W\u0303 = min(W, T\u0303 ) be the potentially censored time of treatment with indicator A = I(W < T\u0303 ). Note that the treatment assignment mechanism is coupled with the timing such that P (A = 1 | L = l) = P (W \u2264 T\u0303 | L = l). This type of assignment mechanism characterizes a wide range of applications. For instance, in transplant studies, a patient is assigned to receive transplant whenever an organ becomes available. If they die before that time, a transplant will not occur. In observational studies treatment timing is not randomized but may instead be confounded by L, motivating the need for formal causal adjustment procedures.\nThe observed data consists of D = { W\u0303i, Ai, Li, T\u0303i, \u03b4Ti, Yi(T\u0303i), }n i=1 . Let P (D) denote the joint distribution of the observed data. Though often observed continuously, we can discretize the observed data as follows. We partition the followup window [0, \u03c4) into K > 1 intervals Ik = [\u03c4k\u22121, \u03c4k) for k = 1, 2, . . . ,K where \u03c40 = 0 and \u03c4K = \u03c4 . We assume the intervals have equal size d\u03c4k = \u03c4k \u2212 \u03c4k\u22121 = d\u03c4 for all k. We define survival status at the beginning of interval k as Tk = I(T\u0303 \u2264 \u03c4k\u22121, \u03b4T = 1). This indicator process is monotone - taking values Tk = 0 up to the interval in which death occurs, and then takes values Tk = 1 afterward. Similarly, let Ck = I(T\u0303 \u2264 \u03c4k\u22121, \u03b4T = 0) be censoring status at the beginning of interval k, which is also monotone zero until the interval in which censoring occurs, and then one afterward. We define a time-varying indicator Ak = I(W\u0303 \u2264 \u03c4k, A = 1) of whether treatment has been assigned by the start of interval k - the indicator is zero for all intervals before treatment and is one for all k on and after treatment. Finally we define the number of hospitalization events in interval\nk as Yk = \u2211J j=1 I(TY j \u2208 Ik). By convention T1 = C1 = 0 - that is, patients enter the first interval alive and uncensored."
        },
        {
            "heading": "3 Potential Outcomes and Target Estimand",
            "text": "In this section we define the causal incidence rate in terms of potential outcomes and discuss issues due to immortal risk bias. Throughout, we use overbar notation to denote process history, e.g. X\u0304k = (X1, X2, . . . , Xk) and underbar notation to denote process futures, e.g. Xk = (Xk, Xk+1, . . . XK). We define a treatment sequence A\u0304K = (A1, A2, . . . , AK). Recall that the treatment process A\u0304K is monotone - being zero for all k before the interval of treatment and taking on values of 1 for all subsequent\npoints. Suppose a patient switches just ahead of interval s and let A(s) = (A\u0304s\u22121 = 0, As = 1) denote the corresponding treatment sequence that is zero up through the interval in which the switch occurs and one thereafter. Those who were assigned treatment immediately at eligibility (s = 1) followed observed sequence A(1) = (1, 1, . . . , 1). Those who never switched (s = K + 1) followed sequence A(K) = (0, 0, . . . , 0). As is standard in causal inference with censoring, we treat censoring as an additional intervention considered jointly with treatment, A(s) [Herna\u0301n et al., 2001]. Specifically, at each interval we define Zk = (Ck, Ak) to be the joint treatment-censoring status. Let z(s) = (c\u0304K = 0, a(s)) denote the joint intervention of setting switching time to s and eliminating censoring (e.g. by a hypothetical decision to follow all patients through \u03c4). Above, 0 denotes the length K zero vector but we will avoid indexing vectors by length (e.g. 0K) where there is no ambiguity.\nWe let T z(s) k denote potential death indicator had a subject, possibly counter to the fact, been following sequence z(s) up to k. It is a slight abuse of notation since the index does not contain all components of z(s), but just the components of z(s) up to k - e.g. if s = 2, then T z(s) 2 = T (0,0),(0,1) 2 while T z(s) 3 = T (0,0,0),(0,1,1) 3 . Note that strictly, T1 := 0, is not a potential outcome since it is definitionally zero for patients who entered the study. Similarly, we define potential number of events at interval k, Y z(s) k . Since death is a terminal event it is clear that T z(s) j = 1 with probability 1 for all j > k if T z(s) k = 1. Similarly, events cannot occur in an interval if a patient is dead entering that interval, i.e. Y z(s) j = 0 with probability 1 for all j \u2265 k if T z(s)k = 1. The random vector T\u0304 z(s) K can take on values t(k), which live in the space of binary, monotone, length K vectors. There are K such vectors, which we denote by t(k) = (t\u0304k\u22121 = 0, tk). Similarly Y\u0304 z(s) k can take on values y(k) = (y\u0304k\u22121, yk = 0) - length K vectors with non-negative integer values up to and include entry k \u2212 1 and zero afterwards. In this paper, we are concerned with estimates of population-level contrasts of the form E[g(Y\u0304\nz(s) K , T\u0304 z(s) K )],\nwhere g is some function of the potential outcomes. Our motivating example will be the contrast of potential incidence rate g(Y\u0304 z(s) K , T\u0304 z(s) K ) = \u2211K k=1 Y z(s) k K\u2212 \u2211K\nk=1 T z(s) k\nhad everyone in the target populations switched at\ntime s versus switched at time s\u2032\n\u03a8(s, s\u2032) = E [ \u2211K k=1 Y z(s) k\nK \u2212 \u2211K k=1 T z(s) k\n] \u2212 E [ \u2211K k=1 Y z(s\u2032) k\nK \u2212 \u2211K k=1 T z(s\u2032) k\n] (1)\nIn each term, the numerator is the potential number of hospitalization events in the followup. The denominator is the potential number of intervals at risk (i.e. alive) for hospitalization. The ratio is then the potential incidence rate under a hypothetical switching strategy. Note that since T1 := 0, for all subjects, the denominator cannot be zero. The estimand has some interesting special cases: 1) \u03a8(1,K+ 1) is the difference in event rates had everyone switched to treatment immediately versus never switched. 2) \u03a8(s, 1) is the estimated incidence rate had everyone switched at time s versus switched immediately. 3) \u03a8(s,K + 1) is the difference in rates had everyone switched at s versus never switched. The structure of the estimand captures the important fact that in causal recurrent event analysis both the time at risk - terminated by death - and recurrent event count processes are potential outcomes of treatment and so must be considered jointly. In contrast, common approaches may censor patients at death or condition on observed time as an offset in the counting (e.g. Poisson) process model. Both of these approaches are invalid in general. In the first case, death does not coarsen the recurrent event process, it terminates it. We know for certain that the event count after death is zero. In the second case, since survival status is downstream of treatment, it would be incorrect to condition on it. Doing so does not capture the fact that two switching strategies may have the same number of potential events, but very different survival times - leading to different rates. That is, a particular switching strategy s may have lower number of total events relative to s\u2032, not because the average total number of events is lower, but because switching strategy s causes earlier death and, therefore, less time over which to accrue\nevents. A contrast of rates is essentially a composite estimand of at-risk time and events. Moreover, indexing the estimand by s makes it clear that the effect of the intervention may vary depending on the switching time and thus can help us assess the effect of timing."
        },
        {
            "heading": "3.1 Causal Identification under Right-Censoring",
            "text": "For some s, E[g(Y\u0304 z(s) K , T\u0304 z(s) K )] is an average over the joint probability mass function (pmf) of the potential outcomes, P \u2217(Y\u0304 z(s) K = y(k), T\u0304 z(s) K = t(k)) - this includes each term of (1). Since this is a distribution of unobserved counterfactuals, estimation requires assumptions about the causal mechanism so that P \u2217 can be expressed in terms of the observed data distribution, P . In this case, we require extensions of standard causal identification assumptions that are used in time-varying treatment settings. Recall that Zk = (Ak, Ck) is the joint treatment and censoring indicator. Then P \u2217 is identified under the following assumptions\n1. Sequential Ignorability: Among those at risk for an event at interval k, treatment and censoring\nat that interval are unrelated to future potential outcomes conditional on available history. At each k,\nY z(s) k , T z(s) k \u22a5 Zk | A\u0304k\u22121, L, Y\u0304k\u22121, Ck\u22121 = Tk\u22121 = 0\nThat is, Hk = (A\u0304k\u22121, L, Y\u0304k\u22121) is sufficient to control for confounding. This rules out situations in which patients who switch at interval k are those who would have systematically had higher outcome incidence relative to those who did not, even after adjusting for A\u0304k\u22121, L, Y\u0304k\u22121. It also rules out situations in which patients censored at interval k would have gone on to have different event incidence relative to those who are uncensored even after adjusting for available history. Crucially, in full generality, hospitalization history is a time-varying confounder of treatment switch/censoring at each week and so must be conditioned on to satisfy ignorability.\n2. Switching Positivity: To identify outcomes under a hypothetical switch at interval s, it must be\npossible to remain uncensored, alive, and untreated until interval s\u2212 1 and then switch at interval s.\n\u03bbAs (l, y\u0304s\u22121)(1\u2212 \u03bbCs (l, y\u0304s\u22121)) s\u22121\u220f k=1 (1\u2212 \u03bbAk (l, y\u0304k\u22121))(1\u2212 \u03bbCk (l, y\u0304k\u22121)) > 0\nwhere \u03bbAk (l, y\u0304k\u22121) = P (Ak = 1 | A\u0304k\u22121 = Tk\u22121 = Ck\u22121 = 0, l, y\u0304k\u22121) is the discete-time causespecific hazard of treatment and \u03bbCk (l, y\u0304k) = P (Ck = 1 | A\u0304k\u22121 = Tk\u22121 = Ck\u22121 = 0, l, y\u0304k\u22121) is the discrete-time cause-specific hazard of censoring. Additionally, it must be possible to remain uncensored through the end of follow-up. That is, for each k = s + 1, s + 2, . . . ,K, we must have that (1\u2212 \u03bbCk (l, y\u0304k)) > 0 for each k. In the case of a violation of the first condition, there would be some subgroup of the target population for whom the potential outcome under z(s) is undefined (a so-called structural violation). In the second case, there would be some subgroup would always be censored before \u03c4 - making it impossible to learn the incidence rate over the desired followup window for that subgroup. Finally, note that at k = 1 there is no previous censoring/death indicator so they are not in the conditioning set of the distributions above.\nSUTVA is also required at each interval to connect observed and potential outcomes: at each interval k, Yk = \u2211 s Y z(s) k I(A\u0304k = a(s), C\u0304k = 0k) and Tk = \u2211 s T z(s) k I(A\u0304k = a(s), C\u0304k = 0k). Under these assumptions we can identify P \u2217 via the g-formula\nP \u2217 ( Y\u0304 z(s) K = y(k), T\u0304 z(s) K = t(k) ) = \u2211 l\u2208L \u03bbk(a s k, y\u0304k\u22121, l) k\u22121\u220f j=1 f(yj | asj , y\u0304j\u22121, l) ( 1\u2212 \u03bbj(asj , y\u0304j\u22121, l) ) fL(l) (2)\nwith support in k = 1, 2, . . . ,K+1 and switching indicator being set to ask = I(k > s). That is treatment is zero up until interval s and then one afterward. A proof is provided in the supplement. There are three unknowns in this expression which can be learned from observed data: 1) A model for the terminal death process,\n\u03bbk(ak, y\u0304k\u22121, l) = P (Tk = 1 | Tk\u22121 = Ck\u22121 = 0, ak, y\u0304k\u22121, l)\nconditional on treatment history, event history, and confounders. This is also called the discrete-time death hazard (probability of death in interval k conditional on survival up through interval k\u2212 1); 2) A model for the recurrent event process. Specifically, the distribution of the number of events in interval k, given survival up to that interval\nf(yk | ak, y\u0304k\u22121, l) = P (Yk = yk | Tk = Ck = 0, ak, y\u0304k\u22121, l)\nconditional on treatment, event history, and confounders; 3) a model for the joint distribution of the confounders fL(l). This model casts recurrent events as a secondary process whose evolution is modeled jointly with the terminal death process. Bayesian inference follows from obtaining a posterior distribution over the parameters governing these models - which induces a posterior on the joint, P \u2217, and functionals of it such as \u03a8(s, s\u2032). The required averaging will be done by simulation in a g-computation procedure."
        },
        {
            "heading": "3.2 Immortal Risk, Grace Periods, and Positivity Violations",
            "text": "Here we provide a formal causal framing of various naive methods, the conditions under which they are used, and the issues that arise. Issues of immortal risk arise when patients would like to estimate treatment effects but relatively few subjects may be treated exactly at eligibility. From a causal perspective, this is a random violation of the sequential positivity assumption in the previous section. If very few subjects are treated in the first interval, then P (A1 = 1 | l, y0) \u2248 0. In order to circumvent the issue, it is common to look past time zero to see if any switch occurred during the followup. If a switch occurred in, say, interval s then we classify that subject as \u201ctreated\u201d, retroactively set Ak = 1 for all k = 1, 2, . . . , s, and proceed with an \u201cever-versus-never\u201d. With survival outcomes, this is well known to induce immortal time bias: survival through intervals k = 1, 2, . . . , s \u2212 1 are misattributed to treatment A = 1, making treatment A = 1 look better. With recurrent event outcomes, the event rates in these intervals are misattributed to treatment A = 1 - leading to what we call immortal risk. In general, the direction of the bias depends on if the event rate is lower or higher in these intervals relative to post-switching intervals.\nA common alternative is to allow for a certain grace period after eligibility. If treatment is assigned by 6 months, the patient is flagged as treated and if it is not the patient is flagged as untreated. The approach then proceeds to analyze outcomes after 10 weeks. However, this moves estimand away from the target population and towards the subpopulation of that target who survive and remain uncensored 10 weeks - which may not be a relevant subpopulation. Moreover it is unclear what causal recurrent event contrast this approach targets. In contrast, the switching approach outlined here both targets welldefined causal estimands and correctly attributes recurrent events and survival to each treatment status in a time-varying fashion. \u03a8(s,K + 1) has a direct interpretation as the difference in average incidence rates had everyone in the target population switched at week s versus never switched. Similarly, we can compute \u03a8(s, 1) for several values of s to gauge the effect of treatment delay of s weeks. When choosing s, we must restrict ourselves to those values that are supported by the data (i.e. for which positivity holds). As we illustrate in the data analysis, this is checkable since we can assess when switches tend to occur in the data and select s accordingly."
        },
        {
            "heading": "4 Bayesian Semiparametric Model with Shrinkage Priors",
            "text": "In this section we outline Bayesian models for the terminal event probability, \u03bbk(ak, y\u0304k\u22121, l), as well as the distribution of recurrent events f(yk | ak, y\u0304k\u22121, l) at each k. Across k, these two models characterize the joint evolution of the recurrent event process and the terminating process as a function of treatment and confounders. We model the processes via the following two mean functions\n\u03bbk(ak, y\u0304k\u22121, l;\u03b2) = expit ( \u03b20k + \u03b2L(l, yk\u22121) + \u03b2A(l, yk\u22121)ak ) \u00b5k(ak, yk\u22121, l; \u03b8) = exp ( \u03b80k + \u03b8L(l, y\u0304k\u22121) + \u03b8A(l, yk\u22121)ak\n) (3) Here, \u03bbk governs hazard of death. While we use inverse logit link, other appropriate link functions can be considered as well but this choice does not materially effect causal effect estimates. The function \u00b5k is the conditional mean function of f(yk | ak, y\u0304k\u22121, l), which can be any distribution with appropriate support for count outcomes such as Poisson, Negative Binomial, or zero-inflated versions of the two. As K increases the partition gets finer (i.e. d\u03c4 \u2192 0) the baseline hazard in the model \u03bbk, controlled by {\u03b20k}Kk=1, gets more flexible and the model limits to Cox\u2019s semiparametric proportional hazard model [Thompson, 1977]. Similarly, \u00b5k is a proportional mean model for the recurrent events at interval k, with a flexible baseline intensity/event rate controlled by {\u03b80k}Kk=1. In the logistic model, \u03b2L(\u00b7) is a function representing the main effects of covariates l and recurrent event history y\u0304k\u22121. The function \u03b2A(\u00b7) represents the conditional treatment effects, which itself can vary with covariates and recurrent event history. The functions \u03b8L(\u00b7) and \u03b8A(\u00b7) have analogous interpretations in the recurrent event model. In full generality, we should index these functions by k and allow for dependence on the entire recurrent event history, e.g. \u03b2Lk(l, y\u0304k\u22121), but in practice it is infeasible to estimate separate models for each k. Additionally, a Markov assumptions in frequently invoked so that \u03b2Lk(l, y\u0304k\u22121) = \u03b2L(l, yk\u22121), which models dependence only as a function of the first-order lag. Bayesian nonparametric models such as Gaussian Processes can be used to flexibly model functions like \u03b2L(l, yk\u22121), but it is more common in practice to have linear-additive specifications such, say, \u03b2L(l, yk\u22121) = l \u2032\u03b21 + yk\u22121\u03b22. Similarly, a model with no covariate/event history interactions with treatment corresponds to \u03b2A(l, yk\u22121) = \u03b23. Similar functional form assumptions can be made in the recurrent event model. All of these modeling choices are application-specific so we proceed with a more general presentation in (3). Finally, while we index the models for k = 1, 2, . . . ,K note that for k = 1 there is no death model since T1 = 1 for everyone by convention. Additionally, the model for Y1 does not condition on history since there is no history at the first interval.\nThe chief benefit of employing Bayesian methods here is that frequentist estimation of (3) is challenging, especially for fine partitions. For instance, in some intervals (especially the later ones) there may be fewer patients at risk. Estimation of baseline hazards and event rates via time-specific \u03b20j and \u03b80j , respectively, may be infeasible. This often forces ad-hoc simplifications of the model - perhaps assuming a constant intercept \u03b20j = \u03b20 for each j - in practice. In contrast, we fit the above models with a smoothing prior process on the time-varying coefficients, {\u03b20k}Kk=1 and {\u03b80k}Kk=1. We say a sequence of random variables, {Xk}Kk=1, follows a Gaussian first-order autoregressive process if X1 = \u03b7+\u03c3 1 and for k = 2, . . . ,K,\nXk = \u03b7(1\u2212 \u03c1) + \u03c1Xk\u22121 + \u03c3 k (4)\nHere, k iid\u223c N(0, 1) and \u03b7, \u03c1, and \u03c3 are the parameters of the process. We denote this as {Xk}Kk=1 \u223c gAR1(\u03b7, \u03c1, \u03c3). The mean of the process is given by E[Xk] = \u03b7. The variance of this process is given by V [Xk] = \u03c32 1\u2212\u03c12 . The parameter \u22121 < \u03c1 < 1 regulates the state dependence of the process. Specifically,\nCorr(Xv, Xv\u2212u) = \u03c1 u is the correlation between Xv and the state of the process u periods in the past, with correlation declining as we go further back in time.\nTo induce smoothness in the time-varying coefficients, we specify priors {\u03b20k}Kk=1 \u223c gAR1(\u03b20, \u03c1\u03b2 , \u03c3\u03b2) and {\u03b80k}Kk=1 \u223c gAR1(\u03b80, \u03c1\u03b8, \u03c3\u03b8), which induces dependence between the baseline death/event rates across intervals. This is a sensible prior to shrink towards as it reflects the prior belief that - in the absence of data in interval k - the baseline hazard rate shouldn\u2019t be too different from the hazard rate in the previous interval, providing useful regularization at later intervals with fewer patients at risk. Figure 1 illustrates the smoothing effect of the prior in a synthetic data set.\nWe note that the usual frequentist approaches correspond to strongly informative settings of this prior: estimation of separate intercepts in the terminal event model at each interval corresponds to a strong prior belief that \u03c1\u03b2 \u223c \u03b40, where \u03b40 is the point-mass distribution at zero with an improper prior f(\u03b20, \u03c3\u03b2) \u221d 1 on the hyperparameters. Fitting a model with a single intercept across time points corresponds to both setting \u03c1\u03b2 \u223c \u03b40 and additionally that \u03c3 \u223c \u03b40, implying that \u03b20k = \u03b20 for all k. Our approach is a compromise between these extremes that places weakly informative priors over the hyperparameters, \u03c1\u03b2 , \u03c1\u03b8 \u223c U(\u22121, 1), \u03b20, \u03b80 \u223c N(0, 1), with flat f(\u03c3\u03b2) \u221d 1 and f(\u03c3\u03b8) \u221d 1. The priors on \u03b20 and \u03b80 are quite flat on the outcome scales, putting significant prior mass on hazards in the range expit(\u00b11.96) \u2248 (.12, .88) and baseline event rates in the range of exp(\u00b11.96) \u2248 (.14, 7.1). It would be quite rare to have more than seven hospitalizations per week.\nWe will denote the hazard and event rate parameters as \u03b2 = ({\u03b20k}Kk=1, \u03b2L, \u03b2A) and \u03b8 = ({\u03b80k}Kk=1, \u03b8L, \u03b8A), respectively. Lastly, for the confounder distribution model, we use the Bayesian bootstrap [Rubin, 1981]\n- i.e. a point-mass distribution fL(l;\u03c0) = \u2211n i=1 \u03c0i\u03b4Li(l) that puts weight \u03c0i on each unique confounder\nvector Li in the data. Each \u03c0i \u2208 (0, 1) and \u2211 i \u03c0i = 1. Note the empirical distribution is a special\ncase where \u03c0i = 1/n. To account for uncertainty in the confounder distribution, we place a conjugate Dirichlet prior on the weight vector \u03c0 = (\u03c01, \u03c02, . . . , \u03c0n), \u03c0 \u223c Dir(0n) where 0n denotes the length n zero vector. The resulting posterior is \u03c0 \u223c Dir(1n). In addition to regularization, another advantage of our Bayesian approach is uncertainty estimation via the posterior with no need for bootstrapping. Bootstrapping is inherently non-smooth such that in small samples pathological bootstrap datasets may be drawn for which the models cannot be estimated (say, due to perfect separation in the logistic models). Posterior sampling is smooth as long as the posterior is proper and so avoids such issues."
        },
        {
            "heading": "5 Posterior Sampling and Bayesian G-computation",
            "text": "Bayesian inference for \u03a8(s, s\u2032) requires the posterior distribution of the parameters governing the three unknown models in the g-formula in (2), which we denote as \u03c9 = (\u03b8, \u03b2, \u03c0). While not available in closed form, standard Markov Chain Monte Carlo (MCMC) methods can be used to obtain, say, M draws from the posterior, indexed by m = 1, 2, . . . ,M . We provide the likelihood construction and joint posterior up to a proportionality constant in the Supplement. This unnormalized posterior can be specified in standard software such as PROC MCMC in SAS or Rstan or rjags in R. The idea is to simulate the terminal and counting process jointly under both interventions for each posterior draw. The difference in the average incidence rates can then be computed, yielding a draw from the posterior of \u03a8(s, s\u2032). This process is known as g-compuation in the causal literature. The simulation to be described shortly can also be done in standard software such as PROC IML in SAS or the generated quantities block in an Rstan program. Specifically, given a posterior draw of the parameters \u03c9(m) = (\u03b8(m), \u03b2(m), \u03c0(m)), the observed set of covariate vectors {li}ni=1, and a desired switching intervention, s, we do the following:\n1. Starting at subject i = 1. Do the following b = 1, 2, . . . , B times using Li: simulate death and\nevent counts jointly as follows. Let Ask = I(k \u2265 s). Starting with k = 1 recall that T1 := 0 simulate hospitalization and death under treatment switch at s. First, simulate the number of events in interval k in the interval from\nY (b),(m) ki \u223c f(yk | ak, Y\u0304 (b),(m) k\u22121i , li;\u00b5k(A s k, Y\u0304 (b),(m) k\u22121i , li; \u03b8 (m)))\nFor example, if a Poisson model was used, then f(yk|\u2212) is the Poisson pmf with mean function \u00b5k(A s k, Y\u0304 (b),(m) k\u22121i , li; \u03b8 (m)). Note that at k = 1 there is no simulated history, Y\u0304 (b),(m) k\u22121i , yet.\n2. Now, set k = 2 and compute Ask = I(k \u2265 s). simulate death by the next interval by first computing \u03bbk(A s k, Y\u0304 (b),(m) k\u22121i , li;\u03b2 (m)) then drawing a uniform random variate U (b),(m) ki \u223c Unif(0, 1). Then set\nT (b),(m) ki = I\n( U\n(b),(m) ki \u2264 \u03bbk(A s k, Y\u0304 (b),(m) k\u22121i , li;\u03b2\n(m)) )\n3. Now if T (b),(m) ki = 1, then the subject is dead entering the interval. Set T (b),(m) ji = 1 for all\nj = k + 1, k + 2, . . . ,K. Record Y (b),(m) ki and set Y (b),(m) ji = 0 for j = k, k + 1, . . . ,K. Otherwise, T (b),(m) ki = 0 and the subject is alive entering interval k, so simulate for them a Y (b),(m) ki as before. Move to interval k \u2192 k + 1 and repeat until a person dies or reaches interval K. This yields incidence rate for subject i under switching intervention s,\u2211K\nk=1 Y (b),(m) ki K \u2212 \u2211K k=1 T (b),(m) ki\n4. Doing the above B times and averaging yields a Monte Carlo approximation to the conditional (on\nL = li) expected incidence rate\nE(m) [ \u2211K k=1 Y z(s) k\nK \u2212 \u2211K k=1 T z(s) k | L = li ] \u2248 1 B B\u2211 b=1 \u2211K k=1 Y (b),(m) ki K \u2212 \u2211K k=1 T (b),(m) ki\nRepeat this until we have the above for each i = 1, 2, . . . , n.\n5. Repeat Steps 1-3 under the second intervention s\u2032.\n6. Using a draw of the Bayesian bootstrap weights, \u03c0(m) = (\u03c0 (m) 1 , \u03c0 (m) 2 , . . . , \u03c0 (m) n ) \u223c Dir(1n) we inte-\ngrate the difference in conditional expectations over the confounder distribution via the weighted average to arrive at a difference in marginal expectations,\n\u03a8(s, s\u2032)(m) \u2248 n\u2211 i=1 { E(m) [ \u2211K k=1 Y z(s) k K \u2212 \u2211K k=1 T z(s) k | L = li ] \u2212 E(m) [ \u2211K k=1 Y z(s\u2032) k K \u2212 \u2211K k=1 T z(s\u2032) k | L = li ]} \u03c0 (m) i\nDoing this for each posterior draw of the parameters, yields a set of M posterior draws from the causal contrast of interest, {\u03a8(s, s\u2032)(m)}Mm=1. The posterior can be summarized in all of the usual ways. For instance, the posterior mean can be approximated by taking the average of the draws. An equal-tailed (1 \u2212 \u03b1)100% credible interval can be formed by taking the 2.5th and 97.5th percentiles of these draws. Inference in the frequentist paradigm follows the same procedure as above, but m = 1, 2, . . . ,M will index parameters estimating using the mth bootstrap re-sample of the full data. In large samples, estimators will perform similarly, but the smoothness of the Bayesian approach is desirable in sparse settings when we encounter convergence issues in the frequentist bootstrap."
        },
        {
            "heading": "6 Simulation Studies Assessing Finite-Sample Performance",
            "text": "In principle, the prior shrinkage on the baseline hazard and event rates of the proposed method should allow for more desirable bias-variance tradeoff and, therefore, lead to posterior causal estimates with good frequentist properties especially in sparse conditions. However, finite-sample properties of the proposed models have not been previously assessed. Thus, we designed a simulation study in which 1,000 data sets were generated with n = 500 subjects each. Discrete-time longitudinal survival and recurrent events were simulated for K = 12 intervals. Survival at each interval was simulated from a logistic model conditional on treatment and a set of five baseline covariates (two binary and three continuous). The number of events in each interval was simulated from a Poisson distribution with conditional mean dependent on treatment, baseline covariates, and number of events in the previous interval. We simulate data in two settings. In Setting 1, censoring is relatively light about 190/500 subjects were censored before the last interval, on average across the 1,000 simulated datasets. About 180 patients either died while the remaining survived uncensored through the end of the followup - leaving us with a decent amount of of subjects with complete survival/event rate data within the followup. In Setting 2, censoring rates were higher with 335/500 patients being censored in a typical data set. There were only about 85/500 deaths and only about 80 subjects who survived through the end of the followup uncensored within a typical simulated dataset. Relative to Setting 1, much fewer subjects have complete event/survival information throughout the followup. In both setting, censoring status in each interval was simulated conditional on baseline covariates. More details on the simulation study are presented in the Supplement.\nFor each simulated data set we apply several methods to estimate the causal effect of switching by interval 6, \u03a8(6, 13). The results of the simulation are presented in Table 1. We use the Bayesian models with the gAR1 prior described in the previous section (labeled \u201cBayes gAR1\u201d) and the frequentist\nanalogue (labeled \u201cFreq GLM\u201d). For the Bayesian models, we use uninformative priors described in the supplement and in the previous sections. In order to isolate the effect of the shrinkage on performance, both models are correctly specified. In Setting 1, both models have relatively low bias as a percentage of the true effect - 3% for Bayes gAR1 versus 1% for Freq GLM. While average finite-sample bias of the posterior mean estimator is slightly higher for the Bayesian model, the variance is lower due to the regularizing gAR1 prior - this leads to an overall lower mean squared error (MSE). This aligns with usual bias-variance trade off that comes with Bayesian and frequentist estimators. In terms of interval estimation, the Bayesian method yields a slightly narrower interval with closer to nominal coverage.\nAs comparators we also include two common, yet naive, methods that are often applied in practice. This is mainly to highlight the importance of proper joint modeling of recurrent events and the deficiencies of ad-hoc approaches. The first is the Grace Period method, in this approach, we assign subjects to treatment based on whether or not they have switched to treatment by interval six. Event rates are then estimated using data after interval six using a Poisson regression with observed time on study as an offset and the total number of events as the outcome. Predictions from these Poisson models are obtained for each subject under both treatment assignments and the difference is averaged to obtain a point estimate of the incidence rate difference. A bootstrap procedure with 500 resamples is used to compute interval estimates. In Setting 1, this approach yields a biased estimate with average relative bias of around 12%. This is due in part to the exclusion of subjects who died or censored before reaching interval 6. Variability is also relative high (about 2.17 times higher than the Bayesian posterior mean) because the Poisson regression does not account for the correlation between the survival and event count processes. The other naive method is the \u201cever-never\u201d method which simply classifies patients as treated (A = 1) if they ever switched to treatment in the follow up and untreated (A = 0) if they never switched. A Poisson regression was used to estimate event rates with the total number of events in the followup period as the outcome and the total observed followup time included as an offset. This approach is severely biased due to the induced immortal risk bias. In Setting 2, the data are much more sparse due to heavier censoring. This is where the induced shrinkage of the Bayesian gAR1 method provides a significant advantage. While both the frequentist and Bayesian methods have close to nominal coverage, the Bayesian method yields a narrower interval. Morover, finite-sample bias, variance, and MSE are all lower for the Bayesian method. This highlights the benefit of shrinkage in sparse data settings. The relative performance of the two naive methods worsens as censoring rate increases."
        },
        {
            "heading": "7 Analysis of Opioid use among Patients with Chronic Pain",
            "text": "According to 2016 estimates, 20% [Dahlhamer et al., 2018] of adults in the United States suffer from chronic pain. While opioids are commonly used to manage pain, approximately 8%-12% [Vowles et al., 2015] of patients with chronic pain also have opioid use disorder (OUD). OUD is diagnosed based on unsuccessful attempts by the patient at controlling opioid use. The benefit (or harm) of prescribing opioids in this subpopulation is therefore an active research question. In this analysis we are specifically interested in whether opioids lead to increased acute and critical care hospitalizations among patients with chronic back pain who have a history of OUD at the time of chronic pain diagnosis. On the one hand, prescribing opioids may potentially exacerbate OUD and lead to increased hospitalization rate. On the other hand, failing to prescribe opioids may cause patients to subsequently seek illicit opioids which could lead to hospitalizations.\nWe analyze data from a nationally representative 20% random sample of Medicare claims data from January 1, 2016 to December 31, 2019, which includes Part D prescription data used to identify opioid assignment. Even though data on clinical features is limited, claims data are popular in opioid studies due to relatively complete prescription information. A key eligibility criterion is a diagnosis of chronic back pain, defined by two claims diagnosis codes for back pain separated at least three months apart. Additionally, we require that patients have been enrolled in Medicare for 12 months before diagnosis and have had no opioid prescriptions over that time. Followup (time zero) begins at the first instance that a patient meets these eligibility criteria. Since we are interested in evaluating effects among patients with OUD, we subset to those who have a history of OUD at time zero. There are a total of N = 1391 patients with OUD who meet these criteria. At time zero, we record values of baseline characteristics summarized in Table 2, which we collectively denote as L. Of particular importance is the Gagne Score which is a composite score of comorbidities like cancer and liver disease which may be related to both opioids and hospitalization risk. Higher score values indicate greater comorbidity and mortality risk. We also record whether they are prescribed opioids at the time of diagnosis. We count outcomes from time zero for 1 year (the followup window), death, or censoring - whichever comes first. About 24% of patients were censored within the 1-year followup. This was mostly administrative due to end of data cut at December 31, 2019 (about 65% of censored patients). The remaining patients were censored due to to loss of Medicare eligibility. We partition the followup period into K = 52 weekly intervals index by k. In each interval, we record whether a patient is alive or dead entering that interval (Tk), if Tk = 0 then we record the number of hospitalizations in that interval, Yk. We also record whether a patient is censored upon entering interval k, Ck. Additionally we record whether a patient has received an opioid treatment by week k, Ak. Only 25 patients (\u2248 2% of the sample) switched to opioid exactly at time zero while 303 subjects (\u2248 22%) switched within one year. The remaining 1063 subjects (\u2248 76% of the sample) never switches to opioid in the followup.\nThe target estimand is the average difference in event rates had everyone switched to opioids s weeks after diagnosis versus immediately switched to opioids denoted by \u03a8(s, 1). Negative values indicate lower hospitalization risk due to a delay of s weeks while positive values indicate higher hospitalization risk. Recall that a key assumption for identification is switching positivity. This is the only testable assumption as we can directly estimate the cause-specific hazards of treatment and censoring and check that they are bounded. Assessment will also help determine for which s we can reliably estimate \u03a8(s, 1) and for which s our estimates would be driven by model extrapolation. To assess positivity, we model the discrete-time hazards via logistic regressions and use maximum likelihoood estimation (MLE) to obtain estimated hazards \u03bb\u0302Ak (li, y\u0304k\u22121,i) and \u03bb\u0302 C k (li, y\u0304k\u22121,i) for each subject. We then compute the estimated\nprobability of subject i switching to opioid at each k as\nP\u0302 (Ai = a(k)) = \u03bb\u0302 A s (li, y\u0304k\u22121,i) k\u22121\u220f j=1 (1\u2212 \u03bb\u0302Aj (li, y\u0304k\u22121,i))\nTo assess where positivity holds for \u03a8(s, 1), we can check whether the distribution P\u0302 (Ai = a(s)) across i is centered above zero. The left panel of Figure 2 depicts the distribution for each week. Notice that the probability of switching to opioid at week 34 is nearly zero. Meaning that estimates of \u03a8(34, 1) is not identifiable with observed data, but only with model extrapolation. Relatively few patients switched to treatment exactly at any particular week, meaning that some smoothing will be in order.\nSimilarly, recall that in order to estimate \u03a8(s, 1) there must be some patients who survive through the end of the follow up. That is, there should not be subgroups for whom 1 \u2212 \u03bbCk (l, y\u0304k\u22121) \u2248 1 for any k. Accordingly, we empirically we verified that the distribution of 1\u2212 \u03bb\u0302Ck (li, y\u0304k\u22121,i) across i is close to 1.\nFinally, in terms of assessing ignorability, recall that we require both treatment and censoring be conditionally independent from potential outcomes at each time point. This is untestable, but we can be somewhat confident that at least censoring is independent of potential outcomes since it is largely administrative in these data. With these assumptions in mind, we specify the following models for death and hospitalization risk\n\u03bbk(ak, y\u0304k\u22121, l;\u03b2) = expit ( \u03b20k + \u03b2Akak + \u03b2 \u2032 Ll + \u03b2Y I(yk\u22121 > 0) ) \u00b5k(ak, yk\u22121, l; \u03b8) = exp ( \u03b80k + \u03b8Akak + \u03b8 \u2032 Ll + \u03b8Y I(yk\u22121 > 0)\n) (5) Notice that we allow the conditional treatment effects, \u03b2Ak and \u03b8Ak, to vary by week along with the intercepts. However, due to the sparsity of patients switching to opioid at a particular week we require smoothing these coefficient estimates in addition to the intercepts. We specify separate gAR1 priors on coefficients \u03b20k, \u03b2Ak, \u03b80k and \u03b8Ak. Additionally, we allow for dependence between time intervals by modeling outcomes at each week as a function of occurrence of any hospitalizations in the previous week.\nWe use Hamiltonian Monte Carlo Markov Chain (MCMC) as implemented in Stan [Carpenter et al., 2017] to obtain M = 1000 posterior draws after discarding 1000 burn-in draws. Then for each s ranging from 2 to 50 in 2 week increments, we use these draws to perform g-computation with B = 20 as describe in Section 5. This yielded M = 1000 draws of each \u03a8(s, 1) which we use to form posterior point and credible interval estimates.\nThe right panel of Figure 2 depicts posterior point and interval estimates of \u03a8(s, 1). The scale is number of hospitalizations per year and we see that, generally, opioid initiation delay leads to a reduction in hospitalization risk. For instance, switching to opioids 26 weeks after diagnosis versus immediately leads to about .05 hospitalizations per year with a posterior 95% interval that excludes zero. This is a relatively small difference and in fact the absolute risk of hospitalization is low marginally. The smoothness of the curve in the right panel of Figure 2 is an artifact of the parametric models used in the analysis, which is necessary because relatively few patients switched to opioid at any given week. In a hypothetical randomized trial this would not be an issue since, at time zero, we could randomize patients to different opioid initiation times. In a large enough sample this would guarantee that sufficient patients would follow that particular initiation strategy."
        },
        {
            "heading": "8 Discussion",
            "text": "We developed a causally principled, all-in-one method for analyzing recurrent event outcomes with observational data that accounts for several unique complexities at once: treatment misalignment, rightcensoring, and termination due to death. From a causal perspective we showed that both time at risk and event counts are downstream of treatments and so must be treated as potential outcomes of treatment itself. We formulate a tailored causal estimand that is broadly applicable in recurrent event analysis and show that, in terms of identification, recurrent event history essentially acts as a time-varying confounder of treatment switching. We construct Bayesian models with smoothing priors and demonstrate in simulations that the resulting causal effect estimators have desirable frequentist properties.\nAside from hospitalization history, the method focuses on adjusting for time-constant covariates. Adjustment for time-varying covariates can be easily incorporated into the g-computation framework but requires modeling the evolution of the these covariates over time along with death and event counts. Moreover, we note that the discrete-time approach used here is quite popular in causal inference since the models can be fit using standard software with data transformed from wide to long (i.e. each row being a person-interval) form. However, it does come at the cost of computational expense and loss of information. Computationally, with just 52 intervals the long-form dataset can contain millions of rows even with a moderate number of patients. Finally, we view discretization choice as essentially an implicit smoothing prior. A fine discretization yields a more flexible fit at the cost of higher variability. An extremely coarse discretization such as, say, just a single interval favors reduction in variance over bias - it assumes the death and event count risk is constant across the entire followup. The Bayesian approach here is a principled way of addressing this: we choose a fine partition but with a smoothing prior. Future work, however, should investigate the utility of continuous-time models that avoid such discretization altogether."
        },
        {
            "heading": "Acknowledgments",
            "text": "This work is partially funded by National Institute on Drug Abuse grant R03DA051778."
        }
    ],
    "title": "A Bayesian Framework for Causal Analysis of Recurrent Events in Presence of Immortal Risk",
    "year": 2023
}