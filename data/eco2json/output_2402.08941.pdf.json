{
    "abstractText": "We introduce a multivariate local-linear estimator for multivariate regression discontinuity designs in which treatment is assigned by crossing a boundary in the space of running variables. The dominant approach uses the Euclidean distance from a boundary point as the scalar running variable; hence, multivariate designs are handled as uni-variate designs. However, the distance running variable is incompatible with the assumption for asymptotic validity. We handle multivariate designs as multivariate. In this study, we develop a novel asymptotic normality for multivariate local-polynomial estimators. Our estimator is asymptotically valid and can capture heterogeneous treatment effects over the boundary. We demonstrate the effectiveness of our estimator through numerical simulations. Our empirical illustration of a Colombian scholarship study reveals a richer heterogeneity (including its absence) of the treatment effect that is hidden in the original estimates.",
    "authors": [
        {
            "affiliations": [],
            "name": "Masayuki Sawada"
        },
        {
            "affiliations": [],
            "name": "Takuya Ishihara"
        },
        {
            "affiliations": [],
            "name": "Daisuke Kurisu"
        },
        {
            "affiliations": [],
            "name": "Yasumasa Matsuda"
        },
        {
            "affiliations": [],
            "name": "Yoichi Arai"
        },
        {
            "affiliations": [],
            "name": "Hidehiko Ichimura"
        },
        {
            "affiliations": [],
            "name": "Hiroaki Kaido"
        },
        {
            "affiliations": [],
            "name": "Toru Kitagawa"
        }
    ],
    "id": "SP:a6f1d0597be9692057b3e27637505096d65dc5b5",
    "references": [
        {
            "authors": [
                "Y. Arai",
                "H. Ichimura"
            ],
            "title": "Simultaneous selection of optimal bandwidths for the sharp regression discontinuity estimator",
            "venue": "Quantitative Economics,",
            "year": 2018
        },
        {
            "authors": [
                "Y. Arai",
                "T. Otsu",
                "M.H. Seo"
            ],
            "title": "Regression Discontinuity Design with Potentially Many Covariates",
            "year": 2021
        },
        {
            "authors": [
                "S.E. Black"
            ],
            "title": "Do Better Schools Matter? Parental Valuation of Elementary Education",
            "venue": "The Quarterly Journal of Economics,",
            "year": 1999
        },
        {
            "authors": [
                "S. Calonico",
                "M.D. Cattaneo",
                "M.H. Farrell",
                "R. Titiunik"
            ],
            "title": "Rdrobust: Software for Regression-discontinuity Designs",
            "year": 2017
        },
        {
            "authors": [
                "S. Calonico",
                "M.D. Cattaneo",
                "M.H. Farrell",
                "R. Titiunik"
            ],
            "title": "Regression Discontinuity Designs Using Covariates",
            "venue": "The Review of Economics and Statistics,",
            "year": 2019
        },
        {
            "authors": [
                "S. Calonico",
                "M.D. Cattaneo",
                "R. Titiunik"
            ],
            "title": "Robust Data-Driven Inference in the Regression-Discontinuity Design",
            "year": 2014
        },
        {
            "authors": [
                "S. Calonico",
                "M.D. Cattaneo",
                "R. Titiunik"
            ],
            "title": "Robust Nonparametric Confidence Intervals for Regression-Discontinuity Designs",
            "year": 2014
        },
        {
            "authors": [
                "M.D. Cattaneo",
                "B.R. Frandsen",
                "R. Titiunik"
            ],
            "title": "Randomization Inference in the Regression Discontinuity Design: An Application to Party Advantages in the U.S. Senate",
            "venue": "Journal of Causal Inference,",
            "year": 2015
        },
        {
            "authors": [
                "M.D. Cattaneo",
                "N. Idrobo",
                "R. Titiunik"
            ],
            "title": "A Practical Introduction to Regression Discontinuity Designs: Foundations",
            "year": 2019
        },
        {
            "authors": [
                "M.D. Cattaneo",
                "N. Idrobo",
                "R. Titiunik"
            ],
            "title": "A Practical Introduction to Regression Discontinuity Designs: Extensions",
            "year": 2023
        },
        {
            "authors": [
                "M.D. Cattaneo",
                "L. Keele",
                "R. Titiunik",
                "G. Vazquez-Bare"
            ],
            "title": "Interpreting Regression Discontinuity Designs with Multiple Cutoffs",
            "venue": "The Journal of Politics,",
            "year": 2016
        },
        {
            "authors": [
                "M.D. Cattaneo",
                "L. Keele",
                "R. Titiunik",
                "G. Vazquez-Bare"
            ],
            "title": "Extrapolating Treatment Effects in Multi-Cutoff Regression Discontinuity Designs",
            "venue": "Journal of the American Statistical Association,",
            "year": 2021
        },
        {
            "authors": [
                "M.D. Cattaneo",
                "R. Titiunik",
                "G. Vazquez-Bare"
            ],
            "title": "Inference in Regression Discontinuity Designs under Local Randomization",
            "year": 2016
        },
        {
            "authors": [
                "M.D. Cattaneo",
                "R. Titiunik",
                "G. Vazquez-Bare"
            ],
            "title": "Comparing Inference Approaches for RD Designs: A Reexamination of the Effect of Head Start on Child Mortality",
            "venue": "Journal of Policy Analysis and Management,",
            "year": 2017
        },
        {
            "authors": [
                "M.D. Cattaneo",
                "R. Titiunik",
                "G. Vazquez-Bare"
            ],
            "title": "Analysis of regressiondiscontinuity designs with multiple cutoffs or multiple scores. The Stata Journal: Promoting communications on statistics and Stata, 20(4):866\u2013891",
            "year": 2020
        },
        {
            "authors": [
                "J. DiNardo",
                "D.S. Lee"
            ],
            "title": "Chapter 5 - Program Evaluation and Research Designs",
            "venue": "Handbook of Labor Economics,",
            "year": 2011
        },
        {
            "authors": [
                "M. Fr\u00f6lich",
                "M. Huber"
            ],
            "title": "Including Covariates in the Regression Discontinuity Design",
            "venue": "Journal of Business & Economic Statistics,",
            "year": 2019
        },
        {
            "authors": [
                "J. Hahn",
                "P. Todd",
                "W.V. der Klaauw"
            ],
            "title": "Identification and Estimation of Treatment Effects with a Regression-Discontinuity Design",
            "year": 2001
        },
        {
            "authors": [
                "G. Imbens",
                "K. Kalyanaraman"
            ],
            "title": "Optimal Bandwidth Choice for the Regression Discontinuity Estimator",
            "venue": "The Review of Economic Studies,",
            "year": 2012
        },
        {
            "authors": [
                "G. Imbens",
                "S. Wager"
            ],
            "title": "Optimized Regression Discontinuity Designs",
            "venue": "The Review of Economics and Statistics,",
            "year": 2019
        },
        {
            "authors": [
                "G.W. Imbens",
                "T. Lemieux"
            ],
            "title": "Regression discontinuity designs: A guide to practice",
            "venue": "Journal of Econometrics,",
            "year": 2008
        },
        {
            "authors": [
                "L. Keele",
                "R. Titiunik",
                "J.R. Zubizarreta"
            ],
            "title": "Enhancing a geographic regression discontinuity design through matching to estimate the effect of ballot initiatives on voter turnout",
            "venue": "Journal of the Royal Statistical Society: Series A (Statistics in Society),",
            "year": 2015
        },
        {
            "authors": [
                "L.J. Keele",
                "R. Titiunik"
            ],
            "title": "Geographic Boundaries as Regression Discontinuities",
            "venue": "Political Analysis,",
            "year": 2015
        },
        {
            "authors": [
                "A. Krei\u00df",
                "C. Rothe"
            ],
            "title": "Inference in Regression Discontinuity Designs with HighDimensional Covariates",
            "year": 2021
        },
        {
            "authors": [
                "D. Kurisu",
                "Y. Matsuda"
            ],
            "title": "Local polynomial regression for spatial data on R. Bernoulli",
            "year": 2023
        },
        {
            "authors": [
                "K. Kwon",
                "S. Kwon"
            ],
            "title": "Adaptive Inference in Multivariate Nonparametric Regression Models Under Monotonicity",
            "year": 2020
        },
        {
            "authors": [
                "D.S. Lee",
                "T. Lemieux"
            ],
            "title": "Regression Discontinuity Designs in Economics",
            "venue": "Journal of Economic Literature,",
            "year": 2010
        },
        {
            "authors": [
                "J. Londo\u00f1o-V\u00e9lez",
                "C. Rod\u0155\u0131guez",
                "F. S\u00e1nchez"
            ],
            "title": "Replication package for: Upstream and downstream impacts of college merit-based financial aid for low-income",
            "year": 2020
        },
        {
            "authors": [
                "J. 2024-01-23. Londo\u00f1o-V\u00e9lez",
                "C. Rod\u0155\u0131guez",
                "F. S\u00e1nchez"
            ],
            "title": "Upstream and Downstream",
            "year": 2020
        },
        {
            "authors": [
                "C. Noack",
                "T. Olma",
                "C. Rothe"
            ],
            "title": "Flexible Covariate Adjustments in Regression",
            "year": 2021
        },
        {
            "authors": [
                "econ",
                "D. stat]. Ruppert",
                "M.P. Wand"
            ],
            "title": "Multivariate Locally Weighted Least Squares",
            "venue": "Discontinuity Designs",
            "year": 1994
        }
    ],
    "sections": [
        {
            "text": "Keywords: Causal Inference, Multiple Running Variables, Asymptotic Normality\n\u2217We thank Yoichi Arai, Hidehiko Ichimura, Hiroaki Kaido, Toru Kitagawa, and the participants of the third Tohoku-ISM-UUlm workshop at Tohoku University, Summer Econometrics Forum at the University of Tokyo, and the seminar at Hitotsubashi University and Kobe University for their valuable comments. This work was supported by the JSPS KAKENHI Grant Number 22K13373 (ISHIHARA), the JSPS KAKENHI Grant Number 23K12456 (KURISU), the Grant-in-Aid for Scientific Research (B) 21H03400 (MATSUDA) and the JSPS KAKENHI Grant Number 21K13269 (SAWADA).\nar X\niv :2\n40 2.\n08 94\n1v 1"
        },
        {
            "heading": "1 Introduction",
            "text": "The regression discontinuity (RD) design takes advantage of a particular treatment assignment mechanism that is set by the running variables. 1 An example of such a mechanism is a scholarship that is awarded to applicants whose scores are above a threshold. The eligibility sometimes involves an additional requirement. For example, the applicants\u2019 poverty scores must be below another threshold to be eligible. These RD designs are multivariate in their running variables because a student must exceed a policy boundary in the space of multivariate running variables to be treated.\nThe multivariate RD design may exhibit heterogeneous treatment effects that are aggregated in the standard design. When the design is multivariate, students at different points on the boundary have different incomes and test scores; hence, their treatment effects may be heterogeneous over the boundary. Policymakers may consider modifying the current scholarship program when the effect declines as income or test scores increase. Heterogeneity across boundary points is critical for improved policy guidance.\nNevertheless, such a heterogeneity is often dismissed. One popular approach aggregates observations over the boundary to handle multivariate RD designs. For example, Matsudaira (2008) considers the participation in a program based on either a failure in language or math exams. Matsudaira (2008) reduces the multivariate design by aggregating the languagepassing students who are at the boundary of the math exam. This aggregation approach is valid, but loses heterogeneous treatment effects and is not applicable to a non-rectangular boundary.\nAnother popular approach handles heterogeneous treatment effects by constructing a running variable as the Euclidean distance from a boundary point. For example, Black\n1See Imbens and Lemieux (2008), Lee and Lemieux (2010), DiNardo and Lee (2011), and Cattaneo et al. (2019,2023) for extensive surveys of RD literature\n(1999) computes the closest boundary point for each unit and compares units of the same closest boundary point to achieve the mean effect across the boundary. Keele and Titiunik (2015) propose another approach with the Euclidean distance from a particular boundary point. The distance approach can capture heterogeneous effects at each boundary point, is straightforward to implement, available as Stata and R packages, rdmulti (Cattaneo et al., 2020), and is applicable to a wider range of designs.\nHowever, we find that Assumption 1 (a) of Calonico et al. (2014b) is violated when the running variable is the Euclidean distance from a boundary point. Specifically, the value of its density function shrinks to zero as the running variable approaches the boundary point. 2 Consequently, neither approach maintains both flexible interpretation and asymptotic validity simultaneously.\nIn this study, we obtain an asymptotically valid estimator with flexible interpretation.\nWe take multivariate RD designs as multivariate. For this purpose, we develop a novel asymptotic theory of the multivariate local-polynomial estimator with dimension-specific bandwidths. In the simulation studies, our estimator demonstrates favorable performance against the current practices. Furthermore, we apply our estimates to the data of London\u0303oVe\u0301lez et al. (2020b) who study the impact of a Colombian scholarship program on the college attendance rate. In this application, our estimates reveal a new finding regarding the heterogeneity of treatment effects hidden in the original estimates. Specifically, the impact of the tuition program is homogeneous across different poverty levels with the same test scores, however, the impact sharply declines among the poor students with particularly high test scores. Consequently, policymakers may consider expanding the eligibility of the\n2The distance approach may remain valid with a different distance measure. For example, an analysis with \u201cthe driving or walking distance, or the distance along paved roads\u201d (Cattaneo et al., 2023, p.132) can remain valid. Nevertheless, the natural distance measure is not necessarily available for the other designs such as those in non-geographical studies.\nprogram by loosening the income requirement.\nWe contribute to two different strands of literature. First, we contribute to the literature on the local-polynomial estimation. Ruppert and Wand (1994) show the consistency of the multivariate local-polynomial estimator; Masry (1996) later shows the asymptotic normality of the estimator; however, Masry (1996) imposes that the bandwidths are common across dimensions. Fro\u0308lich and Huber (2019) is a closely related study that considers the multivariate local-linear estimation for covariates adjustment; however, their focus is on the aggregated scalar parameter, and the optimal bandwidth selection for each dimension is not provided. In our simulation results, allowing for heterogeneous bandwidths is critical for the bias correction procedure in the RD estimation. Therefore, our first contribution is theoretically and practically relevant.\nSecond and more importantly, we contribute to the literature on the estimation of treatment effects for RD designs. For a scalar running variable, the local-linear estimation of Calonico et al. (2014b) is the first choice for estimating treatment effects. Its statistical package, rdrobust (Calonico et al., 2014a, Calonico et al., 2017), is the dominant and reliable package for a uni-variate RD design with a large sample. Local-linear estimation is the first choice for the RD estimator for several reasons. However, existing local-linear estimators cannot handle a multivariate running variable as multivariate. Multivariate estimations are available only in a non-kernel procedure such as Imbens and Wager (2019) and Kwon and Kwon (2020) with tuning parameters of the worst-case second derivative instead of the bandwidth. Our local-linear estimator is as intuitive as a scalar-variable RD design, applicable to a variety of designs, and capable of revealing a rich heterogeneity in treatment effects as demonstrated in our empirical illustration.\nThe remainder of the paper is organized as follows. We introduce and motivate our\nestimator in Section 2. In Section 3, we evaluate the proposed estimator in a Monte Carlo simulation. We demonstrate the added value of our estimator in an empirical study by London\u0303o-Ve\u0301lez et al. (2020b) in Section 4. Finally, we conclude the paper and discuss future challenges in Section 5."
        },
        {
            "heading": "2 Methods",
            "text": "Consider a multivariate RD design for a student with a pair of test scores (R1, R2). For example, we consider a program that accepts students whose scores exceed their corresponding thresholds (c1, c2). In this program, the eligibility is set by a treatment region T = {(R1, R2) \u2208 R2 : R1 \u2265 c1, R2 \u2265 c2} (Figure 2 (a)). For another example, consider a program that accepts students whose total score exceeds a single threshold c1 + c2. The eligibility is set by another region T = {(R1, R2) \u2208 R2 : R1 +R2 \u2265 c1 + c2} (Figure 2 (b)). In general, we consider a binary treatment D \u2208 {0, 1} and associated pair of potential outcomes {Y (1), Y (0)} such that Y = DY (1)+(1\u2212D)Y (0) for an observed outcome Y \u2208 R. We consider a sharp RD design with a vector of running variables R \u2208 R \u2286 Rd for some integer d \u2265 1. Specifically, let T be the treatment region, which is an open subset of the support, R. Let T C be the complement of the closure of T . This T C is the control region, and both T and T C have non-zero Lebesgue measures, and D = 1{R \u2208 T }.\nLet (Yi, Di, Ri)i\u2208{1,...,n} where Ri = (Ri,1, Ri,2) be the i.i.d. sample of (Y,D,R) where R = (R1, R2). Let c be a particular point on the boundary of the closure of T . Our target parameter is \u03b8(c) := limr\u2192c,r\u2208T E[Y (1)\u2212 Y (0)|R = r]\u2212 limr\u2192c,r\u2208T C E[Y (1)\u2212 Y (0)|R = r]. In the following section, we focus on the issues in estimating the given identified parameter \u03b8(c). Under the following assumptions (Hahn et al., 2001; Keele and Titiunik, 2015), \u03b8(c) is the average treatment effect (ATE) at each point of the boundary c:\nProposition 2.1. (Keele and Titiunik, 2015, Proposition 1) If E[Y (1)|R = r] and E[Y (0)|R = r] are continuous in r at all points c of the boundary of the closure of T ; P (Di = 1) = 1 for all i such that Ri \u2208 T ; P (Di = 1) = 0 for all i such that Ri \u2208 T C , then, \u03b8(c) = E[Y (1)\u2212 Y (0)|R = c] for all c in the boundary."
        },
        {
            "heading": "2.1 Issues in the Conventional Estimators",
            "text": "As described in the Introduction, two estimation strategies are popular for multivariate RD designs. The former aggregation strategy such as Matsudaira (2008) is a single-variate RD design that aggregates students who satisfy all but one requirement for treatment. The aggregation strategy is valid, but has two shortcomings. First, the strategy is limited to a particular assignment mechanism. Second, the aggregation strategy dismisses the treatment effect heterogeneity, which is the important merit of the multivariate designs. We demonstrate this critical merit of our strategy in discovering the heterogeneity in Section 4 with the London\u0303o-Ve\u0301lez et al. (2020b) data.\nIn the latter distance strategy, a multivariate running variable is explicitly reduced to a scalar distance measure. A frequent choice is the Euclidean distance from a point or the closest boundary (Keele and Titiunik, 2015). The distance strategy can be easily implemented in most designs. However, its inference is not theoretically guaranteed because Assumption 1 (a) of Calonico et al. (2014b) is violated. Specifically, the density of the distance running variable shrinks to zero as it approaches the boundary when the distance d\u0303 bounds the Euclidean distance with some constant:\nProposition 2.2. Let d\u0303(\u00b7, \u00b7) be a distance on Rd such that c1\u2225a \u2212 b\u2225 \u2264 d\u0303(a, b) for any a, b \u2208 Rd and some constant c1 > 0. Here \u2225a \u2212 b\u2225 is the Euclidean distance between a = (a1, . . . , ad) \u2032 and b = (b1, . . . , bd) \u2032. Define Zi = d\u0303(Ri, c) with c = (0, . . . , 0) \u2032 and assume that Ri and Zi have density functions f and fZ , respectively.\nAssume that f and fZ are continuous. Then we have fZ(z) \u2192 0 as z \u2192 0.\nProof. By construction of Zi, for z > 0,\n\u222b z 0 fZ(r)dr =P (Zi \u2264 z) = P (d\u0303(Ri, 0) \u2264 z) \u2264 P (c1\u2225Ri\u2225 \u2264 z) = P (\u2225Ri\u2225 \u2264 z/c1)\n= \u222b z/c1 0 t (\u222b 2\u03c0 0 f(t cos \u03b8, t sin \u03b8)d\u03b8 ) dt =\n\u222b z 0 c\u221221 r (\u222b 2\u03c0 0 f(c\u221211 r cos \u03b8, c \u22121 1 r sin \u03b8)d\u03b8 ) dr\nwhere the last equality uses the change of variable r = c1t. If f is continuous, then we can show that fZ(0) = 0 by using the above inequality. Since fZ is continuous, the statement follows.\nRemark 2.1. The boundary point value c is set to zero for illustration. The same argument applies in general by normalizing the running variables with respect to the boundary point. The distance d\u0303 includes the Euclidean norm \u2225a\u2212b\u2225, \u2113\u221e-norm \u2225a\u2212b\u2225\u221e = max1\u2264j\u2264d |aj\u2212bj| \u2265\n(1/d)\u2225a\u2212 b\u2225, and \u21131-norm \u2225a\u2212 b\u22251 = \u2211 j=1d |aj \u2212 bj| \u2265 \u2225a\u2212 b\u2225.\nTo illustrate the proposition in an example, consider Ri = (R1i, R2i) where R1i and R2i independent each other, and R1i \u223c U [\u22121, 1] and R2i \u223c U [0, 1]. The distribution function of Zi = \u2225Ri\u2225 is P (Zi \u2264 z) = P (R21i +R22i \u2264 z2) = (\u03c0/4)z2. The half-circle area shrinks to zero at the order of z2 as z approaches the value 0 at the boundary point (0, 0).\nThis property of fZ(z) violates the critical assumption of the valid inference of a scalar RD estimate. Calonico et al. (2014b) assumes that the density fZ(z) is continuous and bounded away from zero (Assumption 1 (a)). Hence, we cannot guarantee the asymptotic normality of the local-linear estimation with Zi.\nIn Online Appendix C, we further show that the kernel density estimation of the distance running variable diminishes to 0 as the bandwidth h \u2192 0. Hence, this issue can be severe with a direct density estimation as in Imbens and Kalyanaraman (2012). Using the rdrobust package avoids the direct density estimation. Nevertheless, the same concern applies to its asymptotic validity."
        },
        {
            "heading": "2.2 Our Estimator",
            "text": "The two dominant approaches suffer from limitations, and we resolve these limitations using a new estimator. Our estimator can capture the heterogeneous treatment effect over the boundary unlike the aggregation strategy; our estimator avoids the issues in its inference unlike the distance strategy.\nWe demonstrate our estimator in a special case of two-dimensional running variables.\nConsider the following local-linear estimator \u03b2\u0302+(c) = (\u03b2\u0302+0 (c), \u03b2\u0302 + 1 (c), \u03b2\u0302 + 2 (c)) \u2032\n\u03b2\u0302+(c) = arg min (\u03b20,\u03b21,\u03b22)\u2032\u2208R3 n\u2211 i=1 (Yi \u2212 \u03b20 \u2212 \u03b21(Ri,1 \u2212 c1)\u2212 \u03b22(Ri,2 \u2212 c2))2Kh (Ri \u2212 c) 1{Ri \u2208 T }\nwhere Kh(Ri \u2212 c) = K ((Ri,1 \u2212 c1)/h1, (Ri,2 \u2212 c2)/h2) and each hj is a sequence of positive bandwidths such that hj \u2192 0 as n \u2192 \u221e. Unlike Masry (1996), we allow h1 \u0338= h2 for the asymptotic normality. Later in Section 3, we demonstrate the importance of allowing heterogeneous bandwidths. Similarly, let \u03b2\u0302\u2212(c) be the estimator using 1{Ri \u2208 T c} subsample. Our multivariate RD estimator at c is \u03b2\u0302+0 (c)\u2212 \u03b2\u0302\u22120 (c).\nWe consider a random sample. The treated sample is independent of the control sample\nand we consider the following nonparametric regression models for each sample:\nYi =m+(Ri) + \u03b5+,i, E[\u03b5+,i|Ri] = 0, i \u2208 {1, . . . , n : Ri \u2208 T } and Yi =m\u2212(Ri) + \u03b5\u2212,i, E[\u03b5\u2212,i|Ri] = 0, i \u2208 {1, . . . , n : Ri \u2208 T C}.\nFor the asymptotic normality, we impose the following regularity conditions that are standard in kernel regression estimations. We provide the conditions under its general possible form. In Online Appendix A, we present the general results for pth order localpolynomial estimation with d-dimensional running variables. The general results in the\nOnline Appendix are the basis of the bias correction procedure of our estimator.\nIn Assumption 2.1, we assume the existence of a continuous density function for the running variable R. Assumption 2.2 is the regularity conditions for a kernel function. We select a particular set of kernel functions for our subsequent analysis. Assumption 2.3 imposes a set of smoothness conditions for the conditional mean functions m and for the conditional moments of residuals \u03b5i. Assumption 2.4 specifies the rate of convergence of the vector of bandwidths {h1, . . . , hd} relative to the sample size n.\nAssumption 2.1. Let Ur be a neighborhood of r = (r1, . . . , rd) \u2032 \u2208 R.\n(a) The vector of random variables Ri has a probability density function f .\n(b) The density function f is continuous on Ur and f(r) > 0.\nAssumption 2.2. Let K : Rd \u2192 R be a kernel function such that\n(a) \u222b K(z)dz = 1.\n(b) The kernel function K is bounded and there exists a constant CK > 0 such that K is\nsupported on [\u2212CK , CK ]d.\n(c) Define \u03ba (v) 0 :=\n\u222b Kv(z)dz, \u03ba\n(v) j1,...,jM\n:= \u222b \u220fM\n\u2113=1 zj\u2113K v(z)dz for integer v, and\nz\u030c := (1, (z)\u20321, . . . , (z) \u2032 p) \u2032, (z)L =\n( L\u220f\n\u2113=1\nzj\u2113 )\u2032 1\u2264j1\u2264\u00b7\u00b7\u00b7\u2264jL\u2264d , 1 \u2264 L \u2264 p.\nThe matrix S = \u222b K(z)  1 z\u030c  (1 z\u030c\u2032)dz is non-singular. Assumption 2.3. Let Ur be a neighborhood of r \u2208 R.\n(a) The mean function m is (p+ 1)-times continuously partial differentiable on Ur and\ndefine \u2202j1...jLm(r) := \u2202m(r) \u2202rj1 ...rjL , 1 \u2264 j1, . . . , jL \u2264 d, 0 \u2264 L \u2264 p+ 1. When L = 0, we set \u2202j1...jLm(r) = \u2202j0m(r) = m(r).\n(b) The variance function \u03c32(z) = E[\u03b52i |Ri = z] is continuous at r.\n(c) There exists a constant \u03b4 > 0 such that supz\u2208Ur E[|\u03b51|2+\u03b4|R1 = z] \u2264 U(r) < \u221e.\nAssumption 2.4. As n \u2192 \u221e,\n(a) hj \u2192 0 for 1 \u2264 j \u2264 d,\n(b) nh1 \u00b7 \u00b7 \u00b7hd \u00d7 h2j1 . . . h 2 jp \u2192 \u221e for 1 \u2264 j1 \u2264 \u00b7 \u00b7 \u00b7 \u2264 jp \u2264 d,\n(c) nh1 \u00b7 \u00b7 \u00b7hd \u00d7 h2j1 . . . h 2 jph 2 jp+1 \u2192 cj1...jp+1 \u2208 [0,\u221e) for 1 \u2264 j1 \u2264 \u00b7 \u00b7 \u00b7 \u2264 jp+1 \u2264 d.\nUnder these assumptions, we establish the asymptotic normality of \u03b2\u0302+.\nTheorem 2.1 (Asymptotic normality of local-linear estimators). Under Assumptions 2.1, 2.2, 2.3 and 2.4 for r = c, the mean function m+ with d = 2 and p = 1, the conditional mean residual \u03b5+,i, and the variance function \u03c3 2 +(z) = E[\u03b5 2 +,i|Ri = z], as n \u2192 \u221e, we have\n\u221a nh1h2 ( H ll ( \u03b2\u0302+(c)\u2212M+(c) ) \u2212 S\u22121B(2,1)M (2,1)+,n (c) ) d\u2192 N ( 0, \u03c32+(c)\nf(c) S\u22121KS\u22121\n) ,\nwhere\nH ll = diag(1, h1, h2) \u2208 R3\u00d73,\nM+(c) = (m+(c), \u22021m+(c), \u22022m+(c)) \u2032 ,\nM (2,1) +,n (r) =\n( \u220211m+(c)\n2 h21, \u220212m+(c)h1h2,\n\u220222m+(c)\n2 h22\n)\u2032 , and\nB(2,1) = \u222b  1 z\u030c  (z)\u20322dz, K = \u222b K2(z)  1 z\u030c  (1 z\u030c\u2032)dz.\nThe result for \u03b2\u0302\u2212(c) holds under the parallel restrictions.\nFrom Theorem 2.1, we derive the following asymptotic expansion of the mean-squared\nerror (MSE) of m\u0302+(c): for e1 = (1, 0, 0) \u2032,\ne1S\u22121B(2,1)  \u220211m+(c) h21 2 \u220212m+(c)h1h2\n\u220222m+(c) h22 2\n  2\n\ufe38 \ufe37\ufe37 \ufe38 Bias term\n+ \u03c32+(c)\nnh1h2f(c) e1S \u22121KS\u22121e\u20321\ufe38 \ufe37\ufe37 \ufe38 Variance term .\nWe follow the standard bandwidth selection procedure in RD designs to find the pair of (h1, h2) that minimizes the above asymptotic MSE.\nHowever, all three coefficients of the three partial derivatives \u220211m+(c), \u220212m+(c) and \u220222m+(c) in the bias term may be non-zero. This general expression is too complex to obtain an analytical formula for the optimal bandwidths. The above expression can be simplified by choosing the kernels as follows:\n\u03ba (1,1) 1 = \u03ba (1,1,1) 1,2 = \u03ba (1,2) 1 = \u03ba (1,1,2) 1,2 = \u03ba (1,2,1) 1,2 = 0. (2.1)\nAmong the product kernels of the form K(z1, z2) = K1(z1)K2(z2), the above restriction amounts to rotate the space so that the boundary becomes either the x or y-axis. Among\nthe product kernels, the following kernels satisfy the above restrictions:\nK1(z) =  (1\u2212 |z|)1{|z|\u22641} (two-sided triangular kernel),\n3 4 (1\u2212 z2)1{|z|\u22641} (Epanechnikov kernel),\nK2(z) = 2(1\u2212 |z|)1{0\u2264z\u22641} (one-sided triangular kernel).\nThe same restriction is possible without a product kernel. For example, a cone kernel\nK(z1, z2) = 6\n\u03c0\n( 1\u2212 \u221a z21 + z 2 2 ) 1{z21+z22\u22641,z2\u22650} = 6 \u03c0 (1\u2212 \u2225z\u2225) 1{\u2225z\u2225\u22641,z2\u22650}.\nwhere z = (z1, z2) and \u2225z\u2225 = \u221a z21 + z 2 2 satisfy (2.1).\nIn the subsequent analysis, we assume that K1 is the two-sided triangular kernel and K2 is the one-sided triangular kernel. For example, the design with T = {(R1, R2) \u2208 R2 : R1 \u2265 c1, R2 \u2265 c2} satisfies the restriction (2.1) as is or with a 90 degrees rotation; the design with T = {(R1, R2) \u2208 R2 : R1 + R2 \u2265 c1 + c2} satisfies the restriction (2.1) with a 45 degrees rotation.\nUnder (2.1), MSE(m\u0302+(c)), is simplified as follows\n{h21 2 \u220211m+(c) ( s\u03031\u03ba (2,1) 1 + s\u03033\u03ba (2,1,1) 1,2 ) + h22 2 \u220222m+(c) ( s\u03031\u03ba (2,1) 2 + s\u03033\u03ba (3,1) 2 )}2 + \u03c32+(c)\nf(c)nh1h2 \u00d7 \u03ba (2) 0 ( \u03ba (2,1) 1 \u03ba (2,1) 2 )2 \u2212 2\u03ba(1,2)2 ( \u03ba (2,1) 1 )2 \u03ba (2,1) 2 \u03ba (1,1) 2 + \u03ba (2,2) 1 ( \u03ba (2,1) 1 \u03ba (1,1) 2 )2 ( \u03ba (1) 0 \u03ba (2,1) 1 \u03ba (2,1) 2 \u2212 ( \u03ba (1,1) 2 )2 \u03ba (2,1) 2 )2\nwhere\n s\u03031 s\u03032\ns\u03033\n := 1 \u03ba (1) 0 \u03ba (2,1) 1 \u03ba (2,1) 2 \u2212 ( \u03ba (1,1) 2 )2 \u03ba (2,1) 2  \u03ba (2,1) 1 \u03ba (2,1) 2 0\n\u2212\u03ba(2,1)1 \u03ba (1,1) 2\n = S\u22121e1.\nConsequently, the MSE of the estimator m\u0302+(c)\u2212 m\u0302+(c) is\n{ h21 2 (\u220211m+(c)\u2212 \u220211m\u2212(c)) ( s\u03031\u03ba (2,1) 1 + s\u03033\u03ba (2,1,1) 1,2 ) +\nh22 2 (\u220222m+(c)\u2212 \u220222m\u2212(c))\n( s\u03031\u03ba (2,1) 2 + s\u03033\u03ba (3,1) 2\n)}2\n+ (\u03c32+(c) + \u03c3 2 \u2212(c))\nf(c)nh1h2 e1S\n\u22121KS\u22121e\u20321\nwhen the same kernels are used for both the treatment and control sides.\nWe consider the optimal pair of bandwidths (h1, h2) that minimizes the above asymptotic MSE. There are two remaining issues in minimizing asymptotic MSE. First, the bias term may disappear when the second derivatives of the treatment and control mean functions are equal. Nevertheless, the second derivatives match exactly only in an extreme scenario. Following (Imbens and Kalyanaraman, 2012), we assume the second derivatives \u220211m+(c) and \u220211m\u2212(c) as well as \u220222m+(c) and \u220222m\u2212(c) are different.\nSecond, we can choose a pair (h1, h2) such that the bias term equals zero when the signs differ across \u220211m+(c)\u2212 \u220211m\u2212(c) and \u220222m+(c)\u2212 \u220222m\u2212(c). We first consider the optimal bandwidths when both issues are absent, and then consider a remedy when the latter issue arises. When both issues are absent under the following restrictions\n\u220211m+(c) \u0338= \u220211m\u2212(c), \u220222m+(c) \u0338= \u220222m\u2212(c), and\nsgn { (\u220211m+(c)\u2212 \u220211m\u2212(c)) ( s\u03031\u03ba (2,1) 1 + s\u03033\u03ba (2,1,1) 1,2 )} =sgn { (\u220222m+(c)\u2212 \u220222m\u2212(c)) ( s\u03031\u03ba (2,1) 2 + s\u03033\u03ba (3,1) 2 )} ,\nwe attain the unique pair of optimal bandwidths as\nh1 h2 =\n\u221a B2(c)\nB1(c) and h61 =\n(\u03c32+(c) + \u03c3 2 \u2212(c))\n2n e1S\n\u22121KS\u22121e\u20321(B \u22125/2 1 (c)B 1/2 2 (c))\nwhere\nB1(c) =(\u220211m+(c)\u2212 \u220211m\u2212(c)) ( s\u03031\u03ba (2,1) 1 + s\u03033\u03ba (2,1,1) 1,2 ) , and\nB2(c) =(\u220222m+(c)\u2212 \u220222m\u2212(c)) ( s\u03031\u03ba (2,1) 2 + s\u03033\u03ba (3,1) 2 ) .\nThe above bandwidths are not optimal when the signs of the bias terms differ. A similar issue arises in the single-variable RD estimation with heterogeneous bandwidths with the treatment and control mean functions (Imbens and Kalyanaraman, 2012). Arai and Ichimura (2018) derive the higher-order expansion of the bias terms for the single-variable RD estimation. In Online Appendix A.2.1, we derive the higher-order expansion of the bias terms. Nevertheless, we do not follow Arai and Ichimura (2018)\u2019s approach because estimating higher-order bias correction terms is unreliable for multivariate RD estimations. As shown in Online Appendix A.2.1, a higher-order bias correction procedure requires a reliable local estimation of a cubic polynomial with 10 coefficients. Instead, we follow Imbens and Kalyanaraman (2012) to rely on regularization. In particular, we take the absolute values of the bias terms B1(c) and B2(c) as\nh1 h2 =\n( B2(c) 2\nB1(c)2\n)1/4 and h1 = [ (\u03c32+(c) + \u03c3 2 \u2212(c))\n2n e1S\n\u22121KS\u22121e\u20321(|B1(c)|\u22125/2|B2(c)|1/2) ]1/6 ,\nand add regularization terms to B1(c) and B2(c) to prevent the bandwidths from blowing up when the bias terms are zero or close to zero.\nWe note that the optimal ratio h1/h2 is common across the inner and corner solutions of the minimization problem. Given the same bandwidth ratio, we choose h1 from the above formula when the realized signs of the estimated bias terms are the same. If they differ, the bandwidths are determined by regularization, assuming that the bias term disappears. Furthermore, we follow Calonico et al. (2014b) for a bias correction to obtain appropriate inference. We propose a plug-in bias correction with a multivariate local-quadratic estimation. See Online Appendix B for these implementation details."
        },
        {
            "heading": "3 Simulation Results",
            "text": "We demonstrate the numerical properties of our estimator in the following Monte Carlo simulations with four different designs, partially taken from Arai and Ichimura (2018), Calonico et al. (2014b), and Imbens and Kalyanaraman (2012). Specifically, we take four designs of Arai and Ichimura (2018) as the base specifications for the mean function shapes for one of the two dimensions, R2. The four specifications are repeatedly used in other RD studies such as Calonico et al. (2014b) and Imbens and Kalyanaraman (2012) to evaluate their numerical performances. Based on the shapes for R2, we model the two-dimensional mean functions by multiplying a cosine function of the other dimension R1 with the mean functions for R2. We chose the cosine function among trigonometric functions so that their second derivatives in R1 and R2 are nonzero.\nFigures 2 show the contour plots of the mean functions. Among the four designs, Design 1 has a relatively moderate shape compared to the other specifications. Design 2 has a massive jump on the boundary with similar shapes on both sides; Design 3 is extremely flat\non the treatment side; Design 4 has a complex shape in the treatment side.\nFor each draw of a simulation sample, we draw a random sample of two-dimensional running variables as R1 \u223c U [\u22121, 1] and R2 \u223c 2\u00d7Beta(2, 4)\u2212 1 independent of each other, and generate the outcome variable as m(Ri1, Ri2) + \u03f5i where \u03f5i \u223c N(0, 0.12952).\nWe compare the quality of our estimator, rd2dim, relative to the distance estimation\nusing rdrobust. Figure 3 shows histograms of realized estimates of 3, 000 times replications. The dark-colored histograms of rd2dim have thinner shapes than the light-colored histograms\nof distance estimation using rdrobust. Nevertheless, for some specifications, the distance approach achieved better bias corrections than ours. The light-colored histograms are better centered around the vertical line of the true effect than the dark-colored histogram.\nA close examination of the performance comparisons is presented in Table 1. Our first observation is that estimations with heterogeneous bandwidths h1 \u0338= h2 matter. The common estimator is a version of rd2dim that imposes h1 = h2. For all designs, the 95% coverage rate of the true effect size is much worse for common compared to rd2dim, apparently due to better bias correction with heterogeneous bandwidth selection.\nWhen we compare rd2dim with distance, the RMSE of rd2dim is less than half of the distance for Designs 1, 2, and 3, and the RSMEs are similar for the two estimators for Design 4. We conjecture that the reason for the massively superior performances in Design 1 and 2 is their sufficient and smooth variations in the mean functions over both axes of R1 and R2. The other designs are less natural as two-dimensional designs than Designs 1 or 2, and have extremely flat or extremely dipping shapes. Specifically, Design 3 is challenging for both approaches as the mean functions are flat. Our rd2dim fails its coverage while distance results in an exploding confidence interval length; both approaches report more than 20% of estimates failed.\nOur rd2dim is equally favorable in terms of the 95% coverage rate compared with distance, except for Design 3. Given that the assumption for the valid inference is violated (see Section 2.1 for details), the bias-correcting property of rdrobust is surprisingly good for maintaining coverage. Nevertheless, the consequence of the violation appears in much larger confidence intervals for distance relative to our rd2dim for most specifications."
        },
        {
            "heading": "4 Application",
            "text": "We illustrate our estimator through an empirical application of a Colombian scholarship, London\u0303o-Ve\u0301lez et al. (2020b) (London\u0303o-Ve\u0301lez et al., 2020a). The scholarship of interest is primarily determined by two thresholds: merit-based and need-based. Consequently, a policy\nboundary exists instead of a single cutoff. Our estimator is particularly relevant to their study because of their interest in the heterogeneity over the policy boundary. The outcome of interest is enrollment in any college; hence, the policy impact may be heterogeneous according to their poverty level and their level of academic ability.\nFrom 2014 to 2018, the Colombian government operated a large-scale scholarship program called Ser Pilo Paga (SPP). The scholarship loan covers \u201cthe full tuition cost of attending any four-year or five-year undergraduate program in any government-certified \u2018high-quality\u2019 university in Colombia.\u201d (London\u0303o-Ve\u0301lez et al. (2020b), pp.194). The scholarship takes the form of a loan, but the loan is forgiven if the recipient graduates from the university appropriately. The eligibility of the SPP program is three-fold: first, students must have their scores from a high school exit exam exceeding a threshold; second, the students must be from a welfare recipient household; and third, the students must be admitted to an eligible university. The first threshold is merit-based, determined by the nationally standardized high school graduation exam, SABER 11. In 2014 of London\u0303o-Ve\u0301lez et al. (2020b)\u2019s study period, the cutoff was the top 9% of the score distribution. The second threshold is need-based, and is determined by the eligibility of the social welfare program, SISBEN. SISBEN-eligible families are roughly the poorest 50 percent. When students\u2019 scores exceed two thresholds, they must be accepted by an eligible college in Colombia to receive the scholarship. Hence, the impact of exceeding both thresholds is not the impact of the program itself owing to noncompliance. The estimand is the impact of the program eligibility, which is the intention-to-treat effect.\nThe aggregation approach is the empirical strategy of London\u0303o-Ve\u0301lez et al. (2020b).\nThey run two separate local regressions for the merit-based cutoff among need-eligible students and for the need-based cutoff among merit-eligible students. Figure 4 is a scatter\nplot in the space of the need-based criterion (SISBEN) for the x-axis and the merit-based criterion (SAVER11) for the y-axis. Their strategy is to estimate the effect of exceeding the SISBEN threshold for those around the SISBEN score near 0 and of exceeding the SABER11 threshold among those around the SABER11 score near 0.\nFor each aggregated subsample, they run rdrobust based on Calonico et al. (2014b). London\u0303o-Ve\u0301lez et al. (2020b) prefer this approach because \u201cthe discontinuities represent different populations, and the heterogeneity in estimated impacts across these frontiers is informative\u201d (pp.205). They report the effect of exceeding the merit-based (SABER11) threshold on enrollment in any eligible college is 0.32 with a standard error of 0.012 for the need-based (SISBEN) eligible subsample, and the effect of exceeding the need-based (SISBEN) threshold on enrollment in any eligible college is 0.274 with a standard error of 0.027 for the merit-based (SABER11) eligible subsample. Students with the need eligibility in the x-axis boundary of Figure 4 have a slightly higher effect than students with the merit eligibility in the y-axis boundary of Figure 4. Indeed, their strategy captures certain heterogeneity in the sub-populations, albeit with richer heterogeneity within. The SISBEN threshold students are heterogeneous in their SABER11 scores; the SABER11 threshold students are heterogeneous in their SISBEN scores.\nWe estimate the heterogeneous effects over the entire boundary. We summarize our results in Figure 5. The dark-colored intervals are the pointwise 95% confidence intervals from our rd2dim estimates at each boundary point value, and the light-colored intervals are\nthe pointwise 95% confidence intervals from the distance estimates. For most points, the two estimates show similar patterns across the boundary points with a notable difference in the length of the confidence intervals. For most of the need-based eligible students (point 3 through 15), our confidence intervals are shorter than the distance-based ones. Furthermore, for the points that the effective sample sizes are much smaller (points 1, 2, 29, and 30), our confidence intervals reflect its noisier estimates.\nBoth estimates suggest substantial heterogeneity in the effects among the merit-eligible students (16 \u223c 30) but not among the need-eligible students (1 \u223c 15). Specifically, the\nprogram has similar effects among the majority of students, but has no impact or even negative impact on extremely capable students (points 25 through 30). The null effect for extremely capable students is reasonable because they would have received other scholarships to attend college anyway. Consequently, the program could have benefited from accepting a larger number of students with higher household incomes because their impact is expected to be similar.\nOne notable difference between the two estimates is the negative impact on the most capable students (points 29 and 30). This negative impact may be consistent with the definition of the dependent variable. The dataset is constructed from the administrative SABER11 and SISBEN scores dataset which is merged with the dataset from the Ministry of Education of Colombia, which tracks students of the postsecondary education system. Hence, the dependent variable may not capture the outside options such as enrolling in the selected US schools. The distance estimation does not capture this heterogeneity and takes the opposite sign from the other estimates. We conjecture that the distance estimation picks the outliers that are away from the boundary because students of the same distance from the point are compared equally. This sign-flipping pattern disappears in the distance estimates when the relative scale of two axes is adjusted by the absolute maximum values of each axis (Appendix Figures 6 and 7). An appropriate relative scaling of the two axes is hardly known, and our rd2dim is free from such a difficult re-scaling task. Hence, our approach is superior in handling the relative scaling of the two-dimensional data as is."
        },
        {
            "heading": "5 Conclusion",
            "text": "We provide an alternative estimator for RD designs with multivariate running variables. Specifically, our estimator does not convert a multivariate RD estimation problem into a\nscalar RD estimation problem. We estimate the multivariate conditional mean functions as is. For the purpose of RD estimations, we develop a new asymptotic result for the multivariate local-polynomial regression with dimension specific bandwidths. In numerical simulations, we demonstrate the favorable performance of our estimator against a frequently used procedure of a distance measure as the scalar running variable. We apply our estimator to the study of London\u0303o-Ve\u0301lez et al. (2020b) who study the impact of a scholarship program that has two eligibility requirements. In this application, our estimates are consistent with the original estimates and reveal a richer heterogeneity in the program impacts over the policy boundary than the original estimates.\nOur contributions can be summarized as follows. First, we demonstrate the issues in the current practices of multivariate RD designs and offer a remedy for them. The distance approach (Black, 1999, Keele and Titiunik, 2015, for example) of converting a multivariate running variable with the Euclidean distance from a point violates the inference assumption of Calonico et al. (2014b); the aggregation approach (Matsudaira, 2008, for example) of aggregating students with eligibility for all but one requirement has limited applicability and capability to capture heterogeneous effects. We provide a strategy for estimating heterogeneous effects without dimension reduction.\nSecond, our asymptotic results complete the theory of multivariate local-polynomial estimates. After Masry (1996) has shown the asymptotic normality of multivariate localpolynomial estimates with common bandwidths between dimensions, no studies have achieved the asymptotic theory with dimension-specific bandwidths. As demonstrated in our simulation results, allowing different bandwidths for each dimension matters substantially for the bias correction procedure, which results in the improved coverage rate of our preferred estimates.\nHowever, some theoretical and practical issues remain. First, our consideration is limited to a random sample; hence, spatial RD designs are excluded from our consideration. We defer our focus to spatial design because of its theoretical and conceptual complexity. Nevertheless, we aim to propose a spatial RD estimation based on newly developed asymptotic results of Kurisu and Matsuda (2023) in a separated study. Second, our theoretical results can be applied to any finite-dimensional RD design; however, the practical performance of estimators with more than two dimensions is limited. Although most RD designs have at most two dimensions, the practical implementation of a higher-dimensional RD estimation is an open question. Similarly, we provide the higher-order bias expressions for our multivariate local-polynomial estimates; however, estimating the derived bias expressions is challenging. Therefore, a new approach to exploiting these expressions is desirable. Third, our approach requires a sufficiently large sample over the boundary, and its performance with an extremely small sample size is limited. For a smaller sample, an explicit randomization approach is a compelling alternative. Cattaneo et al. (2015), Cattaneo et al. (2016b) and Cattaneo et al. (2017) propose the concepts and a randomization inference. Their approach requires a substantially stronger assumption but is applicable to a geographical RD design (Keele et al., 2015) as well. Nonetheless, these two approaches are complementary. On the one hand, if one assumes a stronger homogeneous treatment effect assumption around the units at a certain distance from the boundary, then the randomization approach can also be used for a small sample. However, we can relax this strong assumption by using our estimator with a sufficiently large sample. Fourth, covariates are often incorporated in the estimation procedures in RD designs. For the efficiency gain, Fro\u0308lich and Huber (2019) propose a method with a multi-dimensional non-parametric estimation; Calonico et al. (2019) develop an easy-to-implement augmentation; and recently Noack et al. (2021) considers flexible and\nefficient estimation including machine-learning devices and several studies such as Krei\u00df and Rothe (2021) and Arai et al. (2021) explore augmentation with high-dimensional covariates. We defer these analyses to theoretical and conceptual complications for a companion study for a geographic RD design. Fifth, we provided the optimal bandwidths for multivariate RD estimation; however, the optimal kernel for this class of estimators is unknown. Exploring the optimal kernel for a multivariate estimator is a topic for future research. Finally, we do not provide any procedure to aggregate heterogeneous estimates over the set of boundary points. Averaging over boundary points is a preferred feature. For example, a major feature of the rdmulti package, Cattaneo et al. (2020), is averaging over multiple boundary points. In considering the pooling parameter, Cattaneo et al. (2016a) offers the target parameter. Furthermore, Cattaneo et al. (2021) uses a different policy in Columbia with multiple cutoffs to extrapolate the missing part of the support. These ideas can be a good benchmark to consider averaging and extrapolation when the support has holes in the boundary. The holes in the boundary is a typical feature in a geographical RD design, and we will explore this issue in a future study."
        },
        {
            "heading": "A Additional figures",
            "text": "SUPPLEMENTARY MATERIAL\nOnline Appendices: Online Appendix A provides the general local polynomial regression\nresults with their proofs; Online Appendix B provides the implementation details of the proposed estimator; Online Appendix C provides a statement that the property of the kernel density estimation with the Euclidean distance score. (.pdf file)\nR-package rd2dim : R-package rd2dim containing code to perform the estimation and\ninference of two-dimensional regression discontinuity designs. (.zip file)"
        },
        {
            "heading": "A Asymptotic Theory for multivariate",
            "text": "Local-Polynomial Regressions\nA.1 Local-polynomial estimator\nConsider the following nonparametric regression model:\nYi = m(Ri) + \u03b5i, E[\u03b5i|Ri] = 0, i = 1, . . . , n,\nwhere {(Yi, Ri)}ni=1 is a sequence of i.i.d. random vectors such that Yi \u2208 R, Ri = (Ri,1, . . . , Ri,d) \u2032 \u2208 Rd. Define\nD = #{(j1, . . . , jL) : 1 \u2264 j1 \u2264 \u00b7 \u00b7 \u00b7 \u2264 jL \u2264 d, 0 \u2264 L \u2264 p}, D\u0304 = #{(j1, . . . , jp+1) : 1 \u2264 j1 \u2264 \u00b7 \u00b7 \u00b7 \u2264 jp+1 \u2264 d},\nand (sj1...jL1, . . . , sj1...jLd) \u2208 Zd\u22650 such that sj1...jLk = #{j\u2113 : j\u2113 = k, 1 \u2264 \u2113 \u2264 L}. Further, define sj1...jL ! = sj1...jL1! . . . sj1...jLd!. When L = 0, we set (j1, . . . , jL) = j0 = 0, sj1...jL ! = 1.\nNote that \u2211d\nj=1 sj1...jL\u2113 = L. The local-polynomial estimator\n\u03b2\u0302(r) = (\u03b2\u0302j1,...jL(r)) \u2032 1\u2264j1\u2264\u00b7\u00b7\u00b7\u2264jL\u2264d,0\u2264L\u2264p\n:= (\u03b2\u03020(r), \u03b2\u03021(r), . . . , \u03b2\u0302d(r), \u03b2\u030211(r), . . . \u03b2\u0302dd(r), . . . , \u03b2\u03021...1(r), . . . , \u03b2\u0302d...d(r)) \u2032.\nof\nM(r) =\n( 1\nsj1...jL ! \u2202j1,...jLm(r) )\u2032 1\u2264j1\u2264\u00b7\u00b7\u00b7\u2264jL\u2264d,0\u2264L\u2264p\n:= ( m(r), \u22021m(r), . . . , \u2202dm(r), \u220211m(r)\n2! , \u220212m(r) 1!1! , . . . , \u2202ddm(r) 2! ,\n. . . , \u22021...1m(r) p! , \u22021...2m(r) (p\u2212 1)!1! . . . , \u2202d...dm(r) p!\n)\u2032\nis given as a solution of the following problem:\n\u03b2\u0302(r) = arg min \u03b2\u2208RD n\u2211 i=1\n( Yi \u2212\np\u2211 L=0 \u2211 1\u2264j1\u2264\u00b7\u00b7\u00b7\u2264jL\u2264d \u03b2j1...jL L\u220f \u2113=1 (Ri,j\u2113 \u2212 rj\u2113)\n)2 Kh (Ri \u2212 r) (A.1)\nwhere \u03b2 = (\u03b2j1...jL) \u2032 1\u2264j1\u2264\u00b7\u00b7\u00b7\u2264jL\u2264d,0\u2264L\u2264p,\nKh(Ri \u2212 r) = K ( Ri,1 \u2212 ri\nh1 , . . . , Ri,d \u2212 rd hd\n)\nand each hj is a sequence of positive constants (bandwidths) such that hj \u2192 0 as n \u2192 \u221e.\nFor notational convenience, we interpret \u2211 1\u2264j1\u2264\u00b7\u00b7\u00b7\u2264jL\u2264d \u03b2j1...jL \u220fL \u2113=1(Ri,j\u2113 \u2212 rj\u2113) = \u03b20 when\nL = 0. We introduce some notations:\nY :=  Y1 ...\nYn\n , W := diag (Kh (R1 \u2212 r) , . . . , Kh (Rn \u2212 r)) ,\nR := (R1, . . . ,Rn) = \n1 \u00b7 \u00b7 \u00b7 1\n(R1 \u2212 r)1 \u00b7 \u00b7 \u00b7 (Rn \u2212 r)1 ... . . . ... (R1 \u2212 r)p \u00b7 \u00b7 \u00b7 (Rn \u2212 r)p\n =  1 . . . 1 R\u030c1 . . . R\u030cn  ,\nwhere\n(Ri \u2212 r)L =\n( L\u220f\n\u2113=1\n(Ri,j\u2113 \u2212 rj\u2113) )\u2032 1\u2264j1\u2264\u00b7\u00b7\u00b7\u2264jL\u2264d .\nThe minimization problem (A.1) can be rewritten as\n\u03b2\u0302(r) = arg min \u03b2\u2208RD (Y \u2212R\u2032\u03b2)\u2032W (Y \u2212R\u2032\u03b2) = arg min \u03b2\u2208RD Qn(\u03b2).\nThen the first order condition of the problem (A.1) is given by\n\u2202\n\u2202\u03b2 Qn(\u03b2) = \u22122RWY + 2RWR\u2032\u03b2 = 0.\nHence the solution of the problem (A.1) is given by\n\u03b2\u0302(r) = (RWR\u2032)\u22121RWY\n=\n[ n\u2211\ni=1\nKh (Ri \u2212 r)RiR\u2032i ]\u22121 n\u2211 i=1 Kh (Ri \u2212 r)RiYi.\nDefine\nH := diag(1, h1, . . . , hd, h 2 1, h1h2, . . . , h 2 d, . . . , h p 1, h p\u22121 1 h2, . . . , h p d) \u2208 R D\u00d7D.\nTheorem A.1 (Asymptotic normality of local-polynomial estimators). Under Assumptions 2.1, 2.2, 2.3 and 2.4, as n \u2192 \u221e, we have\n\u221a nh1 \u00b7 \u00b7 \u00b7hd ( H ( \u03b2\u0302(r)\u2212M(r) ) \u2212 S\u22121B(d,p)M (d,p)n (r) )\nd\u2192 N   0 ...\n0\n , \u03c32(r) f(r) S\u22121KS\u22121  ,\nwhere\nM (d,p)n (r) =\n( \u2202j1...jp+1m(r)\nsj1...jp+1 ! p+1\u220f \u2113=1 hj\u2113 )\u2032 1\u2264j1\u2264\u00b7\u00b7\u00b7\u2264jp+1\u2264d\n=\n( \u22021...1m(r)\n(p+ 1)! hp+11 ,\n\u22021...2m(r)\np! hp1h2, . . . ,\n\u2202d...dm(r)\n(p+ 1)! hp+1d\n)\u2032 \u2208 RD\u0304,\nB(d,p) = \u222b  1 z\u030c  (z)\u2032p+1dz \u2208 RD\u00d7D\u0304, K = \u222b K2(z)  1 z\u030c  (1 z\u030c\u2032)dz.\nProof. Define h := (h1, . . . , hd) \u2032 and for r, y \u2208 Rd, let r \u25e6 y = (r1y1, \u00b7 \u00b7 \u00b7 , rdyd)\u2032 be the Hadamard product. Considering Taylor\u2019s expansion of m(r) around r = (r1, . . . , rd) \u2032,\nm(Ri) = (1, R\u030c \u2032 i)M(r) +\n1\n(p+ 1)! \u2211 1\u2264j1\u2264\u00b7\u00b7\u00b7\u2264jp+1\u2264d (p+ 1)! sj1...jp+1 ! \u2202j1,...,jp+1m(R\u0303i)\n\u00d7 p+1\u220f \u2113=1 (Ri,j\u2113 \u2212 rj\u2113),\nwhere R\u0303i = r + \u03b8i(Ri \u2212 r) for some \u03b8i \u2208 [0, 1). Then we have\n\u03b2\u0302(r)\u2212M(r)\n= (RWR\u2032)\u22121RW (Y \u2212R\u2032M(r))\n=  n\u2211 i=1 Kh (Ri \u2212 r)  1 R\u030ci  (1 R\u030c\u2032i)  \u22121 n\u2211 i=1 Kh (Ri \u2212 r)  1 R\u030ci  \u00d7\n\u03b5i + \u2211 1\u2264j1\u2264\u00b7\u00b7\u00b7\u2264jp+1\u2264d\n1\nsj1...jp+1 ! \u2202j1,...,jp+1m(R\u0303i) p+1\u220f \u2113=1 (Ri,j\u2113 \u2212 rj\u2113)  . This yields\n\u221a nh1 \u00b7 \u00b7 \u00b7hdH(\u03b2\u0302(r)\u2212M(r)) = S\u22121n (Vn(r) +Bn(r)),\nwhere\nSn(r) = 1\nnh1 \u00b7 \u00b7 \u00b7hd n\u2211 i=1 Kh (Ri \u2212 r)H\u22121  1 R\u030ci  (1 R\u030c\u2032i)H\u22121,\nVn(r) = 1\u221a\nnh1 \u00b7 \u00b7 \u00b7hd n\u2211 i=1 Kh (Ri \u2212 r)H\u22121  1 R\u030ci  \u03b5i =: (Vn,j1...jL(r)) \u2032 1\u2264j1\u2264\u00b7\u00b7\u00b7\u2264jL\u2264d,0\u2264L\u2264p,\nBn(r) = 1\u221a\nnh1 \u00b7 \u00b7 \u00b7hd n\u2211 i=1 Kh (Ri \u2212 r)H\u22121  1 R\u030ci  \u00d7\n\u2211 1\u2264j1\u2264\u00b7\u00b7\u00b7\u2264jp+1\u2264d\n1\nsj1...jp+1 ! \u2202j1,...,jp+1m(R\u0303i) p+1\u220f \u2113=1 (Ri,j\u2113 \u2212 rj\u2113)\n=: (Bn,j1...jL(R\u0303i)) \u2032 1\u2264j1\u2264\u00b7\u00b7\u00b7\u2264jL\u2264d,0\u2264L\u2264p.\n(Step 1) Now we evaluate Sn(r). For 1 \u2264 j1,1 \u2264 \u00b7 \u00b7 \u00b7 \u2264 j1,L1 , j2,1, . . . , j2,L2 \u2264 d, 0 \u2264 L1, L2 \u2264 p, we define\nIn,j1,1...j1,L1 ,j2,1...j2,L2\n:= 1\nnh1 \u00b7 \u00b7 \u00b7hd n\u2211 i=1 Kh (Ri \u2212 r) L1\u220f\n\u21131=1\n( Ri,j\u21131 \u2212 rj\u21131\nhj\u21131\n) L2\u220f\n\u21132=1\n( Ri,j\u21132 \u2212 rj\u21132\nhj\u21132\n) .\nObserve that\nE [ In,j1,1...j1,L1 ,j2,1...j2,L2 ] = 1\nh1 \u00b7 \u00b7 \u00b7hd E\n[ Kh (Ri \u2212 r)\nL1\u220f \u21131=1\n( Ri,j\u21131 \u2212 rj\u21131\nhj\u21131\n) L2\u220f\n\u21132=1\n( Ri,j\u21132 \u2212 rj\u21132\nhj\u21132\n)]\n= \u222b ( L1\u220f \u21131=1 zj\u21131 )( L2\u220f \u21132=1 zj\u21132 ) K(z)f(r + h \u25e6 z)dz = f(r)\u03ba (1) j1,1...j1,L1j2,1...j2,L2 + o(1).\nFor the last equation, we used the dominated convergence theorem.\nVar(In,j1,1...j1,L1 ,j2,1...j2,L2 )\n= 1\nn(h1 \u00b7 \u00b7 \u00b7hd)2 Var\n( Kh (R1 \u2212 r)\nL1\u220f \u21131=1\n( Ri,j\u21131 \u2212 rj\u21131\nhj\u21131\n) L2\u220f\n\u21132=1\n( Ri,j\u21132 \u2212 rj\u21132\nhj\u21132\n))\n= 1\nnh1 \u00b7 \u00b7 \u00b7hd\n \u222b L1\u220f\n\u21131=1\n( Ri,j\u21131 \u2212 rj\u21131\nhj\u21131\n)2 L2\u220f \u21132=1 ( Ri,j\u21132 \u2212 rj\u21132 hj\u21132 )2 K2(z)f(r + h \u25e6 z)dz\n\u2212h1 \u00b7 \u00b7 \u00b7hd (\u222b L1\u220f \u21131=1 ( Ri,j\u21131 \u2212 rj\u21131 hj\u21131 ) L2\u220f \u21132=1 ( Ri,j\u21132 \u2212 rj\u21132 hj\u21132 ) K(z)f(r + h \u25e6 z)dz )2 = 1\nnh1 \u00b7 \u00b7 \u00b7hd\n( f(r)\u03ba\n(2) j1,1...j1,L1j2,1...j2,L2j1,1...j1,L1j2,1...j2,L2\n+ o(1) )\n\u2212 1 n (f(r)\u03ba (1) j1,1...j1,L1j2,1...j2,L2 + o(1))2\n= f(r)\u03ba\n(2) j1,1...j1,L1j2,1...j2,L2j1,1...j1,L1j2,1...j2,L2\nnh1 \u00b7 \u00b7 \u00b7hd + o\n( 1\nnh1 \u00b7 \u00b7 \u00b7hd\n) .\nFor the third equation, we used the dominated convergence theorem. Then for any \u03c1 > 0,\nP ( |In,j1,1...j1,L1 ,j2,1...j2,L2 \u2212 f(r)\u03ba (1) j1,1...j1,L1j2,1...j2,L2 | > \u03c1 )\n\u2264 \u03c1\u22121 { Var(In,j1,1...j1,L1 ,j2,1...j2,L2 ) + ( E[In,j1,1...j1,L1 ,j2,1...j2,L2 ]\u2212 f(r)\u03ba (1) j1,1...j1,L1j2,1...j2,L2 )2} = O ( 1\nnh1 \u00b7 \u00b7 \u00b7hd\n) + o(1) = o(1).\nThis yields In,j1,1...j1,L1 ,j2,1...j2,L2 p\u2192 f(r)\u03ba(1)j1,1...j1,L1j2,1...j2,L2 . Hence we have Sn(r) p\u2192 f(r)S. (Step 2) Now we evaluate Vn(r). For any t = (t0, t1, . . . , td, t11, . . . , tdd, . . . , t1...1, . . . , td...d) \u2032 \u2208 RD, we define\nRn,i,j1...jL := 1\u221a\nnh1 \u00b7 \u00b7 \u00b7hd Kh (Ri \u2212 r) L\u220f \u2113=1 ( Ri,j\u2113 \u2212 rj\u2113 hj\u2113 ) \u03b5i, 1 \u2264 j1, . . . , jL \u2264 d,\nZn,i := p\u2211 L=0 \u2211 1\u2264j1\u2264\u00b7\u00b7\u00b7\u2264jL\u2264d tj1...jLRn,i,j1...jL .\nObserve that\n\u03c32n,j1...jL := Var\n( n\u2211\ni=1\nRn,i,j1...jL\n) =\n1\nh1 \u00b7 \u00b7 \u00b7hd E\n[ \u03b52iK 2 h (R1 \u2212 r) L\u220f \u2113=1 ( R1,j\u2113 \u2212 rj\u2113 hj\u2113 )2]\n= 1\nh1 \u00b7 \u00b7 \u00b7hd E\n[ \u03c32(Ri)K 2 h (R1 \u2212 r) L\u220f \u2113=1 ( R1,j\u2113 \u2212 rj\u2113 hj\u2113 )2]\n= \u222b \u03c32(r + h \u25e6 z) ( L\u220f\n\u2113=1\nz2j\u2113\n) K2(z)f(r + h \u25e6 z)dz\n= \u03c32(r)f(r)\u03ba (2) j1...jLj1...jL + o(1).\nFor the last equation, we used the dominated convergence theorem. Moreover, for 1 \u2264 j1,1 \u2264 \u00b7 \u00b7 \u00b7 \u2264 j1,L1 \u2264 d and 1 \u2264 j2,1 \u2264 \u00b7 \u00b7 \u00b7 \u2264 j2,L2 \u2264 d, we have\nCov(Vn,j1,1...j1,L1 (r), Vn,j2,1...j2,L2 (r)) = 1\nh1 \u00b7 \u00b7 \u00b7hd E\n[ \u03c32(Ri)K 2 h (Ri \u2212 r) L1\u220f \u21131=1 ( Ri,j1,\u21131 \u2212 rj1,\u21131 hj1,\u21131 ) L2\u220f \u21132=1 ( Ri,j2,\u21132 \u2212 rj2,\u21132 hj2,\u21132 )]\n= \u222b \u03c32(r + h \u25e6 z) ( L1\u220f\n\u21131=1\nzj1,\u21131\n)( L2\u220f\n\u21132=1\nzj2,\u21132\n) K2(z)f(r + h \u25e6 z)dz\n= \u03c32(r)f(r)\u03ba (2) j1,1...j1,L1j2,1...j2,L2 + o(1).\nFor the last equation, we used the dominated convergence theorem. For sufficiently large n, we have\nn\u2211 i=1 E[|Zn,i|2+\u03b4] = 1\nn\u03b4/2(h1 \u00b7 \u00b7 \u00b7hd)1+\u03b4/2 E [ |\u03b5i|2+\u03b4 |Kh (Ri \u2212 r)|2+\u03b4\n\u00d7 \u2223\u2223\u2223\u2223\u2223 p\u2211\nL=0 \u2211 1\u2264j1\u2264\u00b7\u00b7\u00b7\u2264jL\u2264d tj1...jL L\u220f \u2113=1 ( Ri,j\u2113 \u2212 rj\u2113 hj\u2113 )\u2223\u2223\u2223\u2223\u2223 2+\u03b4 \n\u2264 U(r) (nh1 \u00b7 \u00b7 \u00b7hd)\u03b4/2 \u222b \u2223\u2223\u2223\u2223\u2223 p\u2211\nL=0 \u2211 1\u2264j1\u2264\u00b7\u00b7\u00b7\u2264jL\u2264d tj1...jL L\u220f \u2113=1 zj\u2113 \u2223\u2223\u2223\u2223\u2223 2+\u03b4 |K(z)|2+\u03b4f(r + h \u25e6 z)dz\n= U(r)f(r)\n(nh1 \u00b7 \u00b7 \u00b7hd)\u03b4/2\n\u222b \u2223\u2223\u2223\u2223\u2223 p\u2211\nL=0 \u2211 1\u2264j1\u2264\u00b7\u00b7\u00b7\u2264jL\u2264d tj1...jL L\u220f \u2113=1 zj\u2113 \u2223\u2223\u2223\u2223\u2223 2+\u03b4 |K(z)|2+\u03b4dz + o(1)\n= o(1).\nFor the second equation, we used the dominated convergence theorem. Thus, Lyapounov\u2019s\ncondition is satisfied for \u2211n\ni=1 Zn,i. Therefore, by Crame\u0301r-Wold device, we have\nVn(r) d\u2192 N   0 ...\n0\n , \u03c32(r)f(r)K  .\n(Step 3) Now we evaluate Bn(r). Decompose\nBn,j1...jL(R\u0303i) = { Bn,j1...jL(R\u0303i)\u2212Bn,j1...jL(r)\u2212 E [ Bn,j1...jL(R\u0303i)\u2212Bn,j1...jL(r) ]} + E [ Bn,j1...jL(R\u0303i)\u2212Bn,j1...jL(r)\n] + {Bn,j1...jL(r)\u2212 E [Bn,j1...jL(r)]} + E [Bn,j1...jL(r)]\n=: 4\u2211\n\u2113=1\nBn,j1...jL\u2113.\nDefine Nr(h) := \u220fd j=1[rj \u2212 CKhj, rj + CKhj]. For Bn,j1...jL1,\nVar(Bn,j1...jL1) \u2264 1 {(p+ 1)!}2h1 \u00b7 \u00b7 \u00b7hd E [ K2h (Ri \u2212 r) L\u220f \u2113=1 ( Ri,j\u2113 \u2212 rj\u2113 hj\u2113 )2 \u00d7\n\u2211 1\u2264j1,1\u2264\u00b7\u00b7\u00b7\u2264j1,p+1\u2264d,1\u2264j2,1\u2264\u00b7\u00b7\u00b7\u2264j2,p+1\u2264d\n1\nsj1,1...j1,p+1 !\n1\nsj2,1...j2,p+1 !\n\u00d7(\u2202j1,1...j1,p+1m(R\u0303i)\u2212 \u2202j1,1...j1,p+1m(r))(\u2202j2,1...j2,p+1m(R\u0303i)\u2212 \u2202j2,1...j2,p+1m(r)) \u00d7 p+1\u220f \u21131=1 (Ri,j1,\u21131 \u2212 rj1,\u21131 ) p+1\u220f \u21132=1 (Ri,j2,\u21132 \u2212 rj2,\u21132 ) ]\n\u2264 1 {(p+ 1)!}2 max 1\u2264j1\u2264\u00b7\u00b7\u00b7\u2264jp+1\u2264d sup y\u2208Nr(h) |\u2202j1...jp+1m(y)\u2212 \u2202j1...jp+1m(r)|2\n\u00d7 \u2211\n1\u2264j1,1\u2264\u00b7\u00b7\u00b7\u2264j1,p+1\u2264d,1\u2264j2,1\u2264\u00b7\u00b7\u00b7\u2264j2,p+1\u2264d p+1\u220f \u21131=1 hj1,\u21131 p+1\u220f \u21132=1 hj2,\u21132\n\u00d7 \u222b ( L\u220f\n\u2113=1 |zj\u2113 | p+1\u220f \u21131=1 |zj1,\u21131 | p+1\u220f \u21132=1 |zj2,\u21132 |\n) K2(z)f(r + h \u25e6 z)dz\n= o  \u2211 1\u2264j1,1\u2264\u00b7\u00b7\u00b7\u2264j1,p+1\u2264d,1\u2264j2,1\u2264\u00b7\u00b7\u00b7\u2264j2,p+1\u2264d p+1\u220f \u21131=1 hj1,\u21131 p+1\u220f \u21132=1 hj2,\u21132  . (A.2) Then we have Bn,j1...jL1 = op(1). For Bn,j1...jL2,\n|Bn,j1...jL2|\n\u2264 1 (p+ 1)! max 1\u2264j1,...,jp+1\u2264d sup y\u2208Nr(h) |\u2202j1...jp+1m(y)\u2212 \u2202j1...jp+1m(r)|\n\u00d7 \u221a nh1 \u00b7 \u00b7 \u00b7hd \u2211 1\u2264j1,1\u2264\u00b7\u00b7\u00b7\u2264j1,p+1\u2264d p+1\u220f \u21131=1 hj1,\u21131 \u222b ( L\u220f \u2113=1 |zj\u2113| p+1\u220f \u21131=1 |zj1,\u21131 | ) |K(z)|f(r + h \u25e6 z)dz\n= o(1). (A.3)\nFor Bn,j1...jL3,\nVar(Bn,j1...jL3) \u2264 1 {(p+ 1)!}2 \u2211 1\u2264j1,1\u2264\u00b7\u00b7\u00b7\u2264j1,p+1\u2264d,1\u2264j2,1\u2264\u00b7\u00b7\u00b7\u2264j2,p+1\u2264d \u2202j1,1...j1,p+1m(r)\u2202j2,1...j2,p+1m(r)\n\u00d7 p+1\u220f \u21131=1 hj1,\u21131 p+1\u220f \u21132=1 hj2,\u21132 \u222b ( L\u220f \u2113=1 z2j\u2113 p+1\u220f \u21131 |zj1,\u21131 | p+1\u220f \u21132=1 |zj2,\u21132 | ) K2(z)f(r + h \u25e6 z)dz\n= o(1). (A.4)\nThen we have Bn,j1...jL3 = op(1). For Bn,j1...jL4,\nBn,j1...jL4\n= \u221a nh1 \u00b7 \u00b7 \u00b7hd \u2211 1\u2264j1,1\u2264\u00b7\u00b7\u00b7\u2264j1,p+1\u2264d \u2202j1,1...j1,p+1m(r) sj1,1...j1,p+1 !\n\u00d7 p+1\u220f \u21131=1 hj1,\u21131 \u222b ( L\u220f \u2113=1 zj\u2113 p+1\u220f \u21131=1 zj1,\u21131 ) K(z)f(r + h \u25e6 z)dz\n= f(r) \u221a nh1 \u00b7 \u00b7 \u00b7hd \u2211 1\u2264j1,1\u2264\u00b7\u00b7\u00b7\u2264j1,p+1\u2264d \u2202j1,1...j1,p+1m(r) sj1,1...j1,p+1 ! p+1\u220f \u21131=1 hj1,\u21131\u03ba (1) j1...jLj1,1...j1,p+1 + o(1). (A.5)\nCombining (A.2)-(A.5),\nBn,j1...jL(R\u0303i) = f(r) \u221a nh1 \u00b7 \u00b7 \u00b7hd \u2211\n1\u2264j1,1\u2264\u00b7\u00b7\u00b7\u2264j1,p+1\u2264d\n\u2202j1,1...j1,p+1m(r)\nsj1,1...j1,p+1 !\n\u00d7 p+1\u220f \u21131=1 hj1,\u21131\u03ba (1) j1...jLj1,1...j1,p+1 + op(1).\n(Step 4) Combining the results in Steps1-3, we have\nAn(r) := Vn(r) + ( Bn(r)\u2212 f(r) \u221a nh1 \u00b7 \u00b7 \u00b7hd (bn,j1...jL(r)) \u2032 1\u2264j1\u2264\u00b7\u00b7\u00b7\u2264jL\u2264d,0\u2264L\u2264p )\nd\u2192 N   0 ...\n0\n , \u03c32(r)f(r)K  .\nThis yields the desired result. Remark A.1 (General form of the MSE of \u0302\u2202j1...jLm(r)). Define\nb(d,p)n (r) := B (d,p)M (d,p)n (r)\n= (bn,0(r), bn,1(r), . . . , bn,d(r),\nbn,11(r), bn,12(r), . . . , bn,dd(r), . . . , bn,1...,1(r), bn,1...2(r), . . . , bn,d...d(r)) \u2032\nand let ej1...jL = (0, . . . , 0, 1, 0, . . . , 0) \u2032 be a D-dimensional vector such that\ne\u2032j1...jLb (d,p) n (r) = bj1...jL(r). Theorem A.1 yields that\nbn,j1,...,jL(r) := \u2211\n1\u2264j1,1\u2264\u00b7\u00b7\u00b7\u2264j1,p+1\u2264d\n\u2202j1,1...j1,p+1m(r)\nsj1,1...j1,p+1 ! p+1\u220f \u21131=1 hj1,\u21131\u03ba (1) j1...jLj1,1...j1,p+1 ,\nfor 1 \u2264 j1 \u2264 \u00b7 \u00b7 \u00b7 \u2264 jL \u2264 d, 0 \u2264 L \u2264 p and\nMSE( \u0302\u2202j1...jLm(r))\n= { sj1...jL ! (S\u22121ej1...jL) \u2032B(d,p)M (d,p) n (r)\u220fL\n\u2113=1 hj\u2113\n}2\n+ (sj1...jL !) 2 \u03c3\n2(r) nh1 \u00b7 \u00b7 \u00b7hd \u00d7 (\u220fL \u2113=1 hj\u2113 )2 f(r) e\u2032j1...jLS \u22121KS\u22121ej1...jL .\nA.2 Higher-order bias\nIn this section, we derive higher-order biases of local-polynomial estimators. Suppose that Assumptions 2.1, 2.2, 2.3 and 2.4 hold. Further, we assume that\n\u2022 the density function f is continuously differentiable on Ur.\n\u2022 the mean function m is (p+ 2)-times continuously differentiable on Ur.\nRecall that\n\u221a nh1 \u00b7 \u00b7 \u00b7hdH(\u03b2\u0302(r)\u2212M(r)) = S\u22121n (Vn(r) +Bn(r)),\nwhere\nSn(r) = 1\nnh1 \u00b7 \u00b7 \u00b7hd n\u2211 i=1 Kh (Ri \u2212 r)H\u22121  1 R\u030ci  (1 R\u030c\u2032i)H\u22121,\nVn(r) = 1\u221a\nnh1 \u00b7 \u00b7 \u00b7hd n\u2211 i=1 Kh (Ri \u2212 r)H\u22121  1 R\u030ci  \u03b5i =: (Vn,j1...jL(r))\u20321\u2264j1\u2264\u00b7\u00b7\u00b7\u2264jL\u2264d,0\u2264L\u2264p,\nBn(r) = 1\u221a\nnh1 \u00b7 \u00b7 \u00b7hd n\u2211 i=1 Kh (Ri \u2212 r)H\u22121  1 R\u030ci  \u00d7\n \u2211 1\u2264j1\u2264\u00b7\u00b7\u00b7\u2264jp+1\u2264d\n1\nsj1...jp+1 ! \u2202j1,...,jp+1m(r) p+1\u220f \u2113=1 (Ri,j\u2113 \u2212 rj\u2113)\n+ \u2211\n1\u2264j1\u2264\u00b7\u00b7\u00b7\u2264jp+2\u2264d\n1\nsj1...jp+2 ! \u2202j1,...,jp+2m(R\u0303i) p+2\u220f \u2113=1 (Ri,j\u2113 \u2212 rj\u2113)  =: (Bn,j1...jL(R\u0303)) \u2032 1\u2264j1\u2264\u00b7\u00b7\u00b7\u2264jL\u2264d,0\u2264L\u2264p.\nNow we focus on Bn,j1...jL(R\u0303).\nBn,j1...jL(R\u0303)\n= 1\u221a\nnh1 \u00b7 \u00b7 \u00b7hd n\u2211 i=1 Kh (Ri \u2212 r)\n( L\u220f\n\u2113=1\nRi,j\u2113 \u2212 rj\u2113 hj\u2113\n)\n\u00d7  \u2211 1\u2264j1,1\u2264\u00b7\u00b7\u00b7\u2264j1,p+1\u2264d\n1\nsj1,1...j1,p+1 ! \u2202j1,1,...,j1,p+1m(r) p+1\u220f \u21131=1 (Ri,j1,\u21131 \u2212 rj1,\u21131 )\n+ \u2211\n1\u2264j1,1\u2264\u00b7\u00b7\u00b7\u2264j1,p+2\u2264d\n1\nsj1,1...j1,p+2 ! \u2202j1,1,...,j1,p+2m(R\u0303i) p+2\u220f \u21131=1 (Ri,j1,\u21131 \u2212 rj1,\u21131 )  =: Bn,1(r) + Bn,2(R\u0303).\nFor Bn,1(r),\nE[Bn,1(r)] = \u221a\nn\nh1 \u00b7 \u00b7 \u00b7hd E\n[ Kh(R1 \u2212 r) ( L\u220f\n\u2113=1\nR1,j\u2113 \u2212 rj\u2113 hj\u2113\n)\n\u00d7 \u2211\n1\u2264j1,1\u2264\u00b7\u00b7\u00b7\u2264j1,p+1\u2264d\n1\nsj1,1...j1,p+1 ! \u2202j1,1,...,j1,p+1m(r) p+1\u220f \u21131=1 (R1,j1,\u21131 \u2212 rj1,\u21131 )\n\n= \u221a nh1 \u00b7 \u00b7 \u00b7hd \u2211\n1\u2264j1,1\u2264\u00b7\u00b7\u00b7\u2264j1,p+1\u2264d\n1\nsj1,1...j1,p+1 ! \u2202j1,1,...,j1,p+1m(r) p+1\u220f \u21131=1 hj1,\u21131\n\u00d7 \u222b L\u220f\n\u2113=1\nzj\u2113 p+1\u220f \u21131=1 zj1,\u21131K(z)f(r + h \u25e6 z)dz\n= \u221a nh1 \u00b7 \u00b7 \u00b7hd \u2211\n1\u2264j1,1\u2264\u00b7\u00b7\u00b7\u2264j1,p+1\u2264d\n1\nsj1,1...j1,p+1 ! \u2202j1,1,...,j1,p+1m(r) p+1\u220f \u21131=1 hj1,\u21131\n\u00d7 ( f(r) \u222b L\u220f \u2113=1 zj\u2113 p+1\u220f \u21131=1 zj1,\u21131K(z)dz\n+ d\u2211\nk=1\n\u2202kf(r)hk \u222b zk L\u220f \u2113=1 zj\u2113 p+1\u220f \u21131=1 zj1,\u21131K(z)dz ) (1 + o(1)). (A.6)\nVar(Bn,1(r))\n\u2264 \u2211\n1\u2264j1,1\u2264\u00b7\u00b7\u00b7\u2264j1,p+1\u2264d,1\u2264j2,1\u2264\u00b7\u00b7\u00b7\u2264j2,p+1\u2264d\n\u2202j1,1...j1,p+1m(r)\u2202j2,1...j2,p+1m(r)\n\u00d7 p+1\u220f \u21131=1 hj1,\u21131 p+1\u220f \u21132=1 hj2,\u21132 \u222b ( L\u220f \u2113=1 z2j\u2113 p+1\u220f \u21131=1 |zj1,\u21131 | p+1\u220f \u21132=1 |zj2,\u21132 | ) K2(z)f(r + h \u25e6 z)dz\n= O  \u2211 1\u2264j1\u2264\u00b7\u00b7\u00b7\u2264jp+1\u2264d p+1\u220f \u2113=1 hj\u2113 2 . (A.7) For Bn,2(R\u0303),\nBn,2(R\u0303) = { Bn,2(R\u0303)\u2212 Bn,2(r)\u2212 E[Bn,2(R\u0303)\u2212 Bn,2(r)] } + E[Bn,2(R\u0303)\u2212 Bn,2(r)]\n+ Bn,2(r)\u2212 E[Bn,2(r)] + E[Bn,2(r)]\n=: 4\u2211\n\u2113=1\nBn,2\u2113.\nDefine Nr(h) := \u220fd j=1[rj \u2212 CKhj, rj + CKhj]. For Bn,21,\nVar(Bn,21) \u2264 1 h1 \u00b7 \u00b7 \u00b7hd E [ K2h (Ri \u2212 r) L\u220f \u2113=1 ( Ri,j\u2113 \u2212 rj\u2113 hj\u2113 )2 \u00d7\n\u2211 1\u2264j1,1\u2264\u00b7\u00b7\u00b7\u2264j1,p+2\u2264d,1\u2264j2,1\u2264\u00b7\u00b7\u00b7\u2264j2,p+2\u2264d\n1\nsj1,1...j1,p+2 !\n1\nsj2,1...j2,p+2 !\n\u00d7(\u2202j1,1...j1,p+2m(R\u0303i)\u2212 \u2202j1,1...j1,p+2m(r))(\u2202j2,1...j2,p+2m(R\u0303i)\u2212 \u2202j2,1...j2,p+2m(r)) \u00d7 p+2\u220f \u21131=1 (Ri,j1,\u21131 \u2212 rj1\u21131 ) p+2\u220f \u21132=1 (Ri,j2,\u21132 \u2212 rj2\u21132 ) ]\n\u2264 max 1\u2264j1\u2264\u00b7\u00b7\u00b7\u2264jp+2\u2264d sup y\u2208Nr(h) |\u2202j1...jp+2m(y)\u2212 \u2202j1...jp+2m(r)|2\n\u00d7 \u2211\n1\u2264j1,1\u2264\u00b7\u00b7\u00b7\u2264j1,p+2\u2264d,1\u2264j2,1\u2264\u00b7\u00b7\u00b7\u2264j2,p+2\u2264d p+2\u220f \u21131=1 hj1,\u21131 p+2\u220f \u21132=1 hj2,\u21132\n\u00d7 \u222b ( L\u220f\n\u2113=1 |zj\u2113 | p+2\u220f \u21131=1 |zj1,\u21131 | p+2\u220f \u21132=1 |zj2,\u21132 |\n) K2(z)f(r + h \u25e6 z)dz\n= o  \u2211 1\u2264j1\u2264\u00b7\u00b7\u00b7\u2264jp+2\u2264d p+2\u220f \u2113=1 hj\u2113 2 . (A.8) For Bn,22,\n|Bn,22|\n\u2264 max 1\u2264j1,...,jp+2\u2264d sup y\u2208Nr(h) |\u2202j1...jp+2m(y)\u2212 \u2202j1...jp+2m(r)|\n\u00d7 \u221a nh1 \u00b7 \u00b7 \u00b7hd \u2211 1\u2264j1,1\u2264\u00b7\u00b7\u00b7\u2264j1,p+2\u2264d p+2\u220f \u21131=1 hj1,\u21131 \u222b ( L\u220f \u2113=1 |zj\u2113| p+2\u220f \u21131=1 |zj1,\u21131 | ) |K(z)|f(r + h \u25e6 z)dz\n= o \u221anh1 \u00b7 \u00b7 \u00b7hd \u2211 1\u2264j1,1\u2264\u00b7\u00b7\u00b7\u2264j1,p+2\u2264d p+2\u220f \u21131=1 hj1,\u21131  . (A.9)\nFor Bn,23,\nVar(Bn,23)\n\u2264 \u2211\n1\u2264j1,1\u2264\u00b7\u00b7\u00b7\u2264j1,p+2\u2264d,1\u2264j2,1\u2264\u00b7\u00b7\u00b7\u2264j2,p+2\u2264d\n\u2202j1,1...j1,p+2m(r)\u2202j2,1...j2,p+2m(r)\n\u00d7 p+2\u220f \u21131=1 hj1,\u21131 p+2\u220f \u21132=1 hj2,\u21132 \u222b ( L\u220f \u2113=1 z2j\u2113 p+2\u220f \u21131 |zj1,\u21131 | p+2\u220f \u21132=1 |zj2,\u21132 | ) K2(z)f(r + h \u25e6 z)dz\n= O  \u2211 1\u2264j1\u2264\u00b7\u00b7\u00b7\u2264jp+2\u2264d p+2\u220f \u2113=1 hj\u2113 2 . (A.10) For Bn,24, Bn,24 = \u221a nh1 \u00b7 \u00b7 \u00b7hd\n\u2211 1\u2264j1,1\u2264\u00b7\u00b7\u00b7\u2264j1,p+2\u2264d \u2202j1,1...j1,p+2m(r) sj1,1...j1,p+2 !\n\u00d7 p+2\u220f \u21131=1 hj1,\u21131 \u222b ( L\u220f \u2113=1 zj\u2113 p+2\u220f \u21131=1 zj1,\u21131 ) K(z)f(r + h \u25e6 z)dz\n= f(r) \u221a nh1 \u00b7 \u00b7 \u00b7hd\n\u00d7  \u2211 1\u2264j1,1\u2264\u00b7\u00b7\u00b7\u2264j1,p+2\u2264d \u2202j1,1...j1,p+2m(r) sj1,1...j1,p+2 ! p+2\u220f \u21131=1 hj1,\u21131 \u222b ( L\u220f \u2113=1 zj\u2113 p+2\u220f \u21131=1 zj1,\u21131 ) K(z)dz  (1 + o(1)). (A.11)\nCombining (A.6)-(A.11),\nBn,j1...jL(R\u0303)\n= \u221a nh1 \u00b7 \u00b7 \u00b7hd \u2211\n1\u2264j1,1\u2264\u00b7\u00b7\u00b7\u2264j1,p+1\u2264d\n1\nsj1,1...j1,p+1 ! \u2202j1,1,...,j1,p+1m(r) p+1\u220f \u21131=1 hj1,\u21131\n\u00d7 ( f(r) \u222b L\u220f \u2113=1 zj\u2113 p+1\u220f \u21131=1 zj1,\u21131K(z)dz + d\u2211 k=1 \u2202kf(r)hk \u222b ( zk L\u220f \u2113=1 zj\u2113 p+1\u220f \u21131=1 zj1,\u21131 ) K(z)dz ) (1 + o(1)).\n+ \u221a nh1 \u00b7 \u00b7 \u00b7hd\n\u00d7 f(r) \u2211 1\u2264j1,1\u2264\u00b7\u00b7\u00b7\u2264j1,p+2\u2264d \u2202j1,1...j1,p+2m(r) sj1,1...j1,p+2 ! p+2\u220f \u21131=1 hj1,\u21131 \u222b ( L\u220f \u2113=1 zj\u2113 p+2\u220f \u21131=1 zj1,\u21131 ) K(z)dz  (1 + o(1)). A.2.1 Higher-order bias of the local-linear estimator\nFor local-linear estimators (i.e., d = 2, p = 1), we have\nbn,0 = f(r)\n2 2\u2211 j,k=1 \u2202jkm(r)hjhk \u222b zkzjK(z)dz\n+ 2\u2211\n\u2113=1\n\u2202\u2113f(r)\n2\n2\u2211 j,k=1 \u2202jkm(r)hjhkh\u2113 \u222b zjzkz\u2113K(z)dz\n+ f(r)\n6 2\u2211 j,k,\u2113=1 \u2202jk\u2113m(r)hjhkh\u2113 \u222b zjzkz\u2113K(z)dz,\nbn,1 = f(r)\n2 2\u2211 j,k=1 \u2202jkm(r)hjhk \u222b z1zkzjK(z)dz\n+ 2\u2211\n\u2113=1\n\u2202\u2113f(r)\n2\n2\u2211 j,k=1 \u2202jkm(r)hjhkh\u2113 \u222b z1zjzkz\u2113K(z)dz\n+ f(r)\n6 2\u2211 j,k,\u2113=1 \u2202jk\u2113m(r)hjhkh\u2113 \u222b z1zjzkz\u2113K(z)dz,\nbn,2 = f(r)\n2 2\u2211 j,k=1 \u2202jkm(r)hjhk \u222b z2zkzjK(z)dz\n+ 2\u2211\n\u2113=1\n\u2202\u2113f(r)\n2\n2\u2211 j,k=1 \u2202jkm(r)hjhkh\u2113 \u222b z2zjzkz\u2113K(z)dz\n+ f(r)\n6 2\u2211 j,k,\u2113=1 \u2202jk\u2113m(r)hjhkh\u2113 \u222b z2zjzkz\u2113K(z)dz.\nWhen K(z) = K1(z1)K2(z2) where K1(z1) = (1\u2212 |z1|)1{|z1|\u22641} and K2(z2) = 2(1\u2212 z2)1{0\u2264z2\u22641}, we have\nbn,0 = f(r)\n2\n{ h21\u220211m(r)\u03ba (2,1) 1 + h 2 2\u220222m(r)\u03ba (2,1) 2 } + \u22021f(r)\n2\n( 2h21h2\u220212m(r)\u03ba (2,1,1) 1,2 ) + \u22022f(r)\n2\n( h21h2\u220211m(r)\u03ba (2,1,1) 1,2 + h 3 2\u220222m(r)\u03ba (3,1) 2 ) + f(r)\n6\n( 3h21h2\u2202112m(r)\u03ba (2,1,1) 1,2 + h 3 2\u2202222m(r)\u03ba (3,1) 2 ) ,\nbn,1 = f(r)\n2\n( 2h1h2\u220212m(r)\u03ba (2,1,1) 1,2 ) + \u22021f(r)\n2\n( h32\u220211m(r)\u03ba (4,1) 1 + h 2 1h2\u220222m(r)\u03ba (2,2,1) 1,2 ) + \u22022f(r)\n2\n( 2h1h 2 2\u220212m(r)\u03ba (2,2,1) 1,2 ) + f(r)\n6\n( h31\u2202111m(r)\u03ba (4,1) 1 + 3h1h 2 2\u2202122m(r)\u03ba (2,2,1) 1,2 ) ,\nbn,2 = f(r)\n2\n( h21\u220211m(r)\u03ba (2,1,1) 1,2 + h 2 2\u220222m(r)\u03ba (3,1) 2 ) + \u22021f(r)\n2\n( 2h21h2\u220212m(r)\u03ba (2,2,1) 1,2 ) + \u22022f(r)\n2\n( h21h2\u220211m(r)\u03ba (2,2,1) 1,2 + h 3 2\u220222m(r)\u03ba (4,1) 2 ) + f(r)\n6\n( 3h21h2\u2202112m(r)\u03ba (2,2,1) 1,2 + h 3 2\u2202222m(r)\u03ba (4,1) 2 ) .\nTherefore,\nBias(m\u0302(r))\n= s\u03031bn,0 + s\u03033bn,2\n= { h21 2 \u220211m(r)(s\u03031\u03ba (2,1) 1 + s\u03033\u03ba (2,1,1) 1,2 ) + h22 2 \u220222m(r)(s\u03031\u03ba (2,1) 2 + s\u03033\u03ba (3,1) 2 ) } + h21h2 ( \u220211m(r)\n2\n\u22022f(r)\nf(r) + \u220212m(r)\n\u22021f(r)\nf(r) +\n\u2202112m(r)\n2\n) (s\u03031\u03ba (2,1,1) 1,2 + s\u03033\u03ba (2,2,1) 1,2 )\n+ h32\n( 1\n2 \u220222m(r)\n\u22022f(r)\nf(r) +\n1 6 \u2202222m(r)\n) (s\u03031\u03ba (3,1) 2 + s\u03033\u03ba (4,1) 2 ).\nB Implementation details\nIn section 2.2, we propose our optimal bandwidth selection from the following formula:\nh1 h2 =\n( B2(c) 2\nB1(c)2\n)1/4\nand\nh1 =\n[ (\u03c32+(c) + \u03c3 2 \u2212(c))\n2n e1S\n\u22121KS\u22121e\u20321|B1(c)|\u22125/2|B2(c)|\u22121/2) ]1/6\nand our RD estimate prior to the bias correction is \u03b2\u0302+0 (c)\u2212 \u03b2\u0302\u22120 (c) where these intercept terms of the local-polynomial estimates {\u03b2\u0302+0 (c), \u03b2\u0302\u22120 (c)} are computed with the bandwidths specified above. Nevertheless, to compute the optimal bandwidth, we need to estimate the bias terms B1(c) and B2(c) as well as the residual variances {\u03c32+(c), \u03c32\u2212(c)}. We follow Calonico et al., 2014b, Section 5) in estimation of the residual variances at the boundary point c. For the bias terms, as in Calonico et al. (2014b), we set a pair of pilot bandwidths with the local-quadratic regression. The key complication of our study is that the local-quadratic regression is also multivariate. The expression of the bias terms involve a pair of partial derivatives (\u220211m+(c), \u220222m+(c)) for the treated and (\u220211m\u2212(c), \u220222m\u2212(c)) for the control. Given a pair of pilot bandwidths\nb+ and b\u2212 for the treated and the control, we run the local-quadratic estimation\n\u03b3\u0302+(c) = arg min (\u03b30,...,\u03b35)\u2032\u2208R6 n\u2211 i=1 (Yi \u2212 \u03b30 \u2212 \u03b31(Ri,1 \u2212 c1)\n\u2212 \u03b32(Ri,2 \u2212 c2)\u2212 \u03b33(Ri,1 \u2212 c2)2 \u2212 \u03b34(Ri,1 \u2212 c1)(Ri,2 \u2212 c2) \u2212 \u03b35(Ri,2 \u2212 c2)2)2Kb (Ri \u2212 c) 1{Ri \u2208 T }\nand\n\u03b3\u0302\u2212(c) = arg min (\u03b30,...,\u03b35)\u2032\u2208R6 n\u2211 i=1 (Yi \u2212 \u03b30 \u2212 \u03b31(Ri,1 \u2212 c1)\n\u2212 \u03b32(Ri,2 \u2212 c2)\u2212 \u03b33(Ri,1 \u2212 c2)2 \u2212 \u03b34(Ri,1 \u2212 c1)(Ri,2 \u2212 c2) \u2212 \u03b35(Ri,2 \u2212 c2)2)2Kb (Ri \u2212 c) 1{Ri \u2208 T C}\nwhere Kb(Ri \u2212 c) = K ( Ri,1\u2212c1 b , Ri,2\u2212c2 b ) to obtain these partial derivatives. These pilot bandwidths (b+, b\u2212) are chosen from minimizing the mean squared error of estimating the bias term, which involves the local cubic regression. 3 Given the pilot bandwidths, we estimate the bias terms B1(c) and B2(c). Let B\u03021(c) and B\u03022(c) be their estimates. In the optimal bandwidth selection, we follow Imbens and Kalyanaraman (2012) to regularize the bias term which appears in the denominator. Specifically, we employ their result that the inverse of bias term estimation error is approximated by 3 times their variance. If the estimated signs of the bias terms are the\n3Furthermore, we choose the preliminary bandwidth for the local cubic regression from minimizing the mean squared error of estimating the bias term for the pilot bandwidth. This preliminary bandwidth selection involves the global 4th order polynomial regressions.\nsame, sgn(B\u03021(c)B\u03022(c)) \u2265 0, then the optimal bandwidths should be chosen from the first-order condition: we set\nh1 = (\u03c3\u03022+(c) + \u03c3\u03022\u2212(c)) 2n e1S \u22121KS\u22121e\u20321(B\u03021(c)2 + 3V\u0302 (B\u03021(c))\u22121 ( B\u03022(c) 2\nB\u03021(c)2 + 3V\u0302 (B\u03021(c))\n)1/41/6\nand\nh2 = (\u03c3\u03022+(c) + \u03c3\u03022\u2212(c)) 2n e1S \u22121KS\u22121e\u20321(B\u03022(c)2 + 3V\u0302 (B\u03022(c))\u22121 ( B\u03021(c) 2\nB\u03022(c)2 + 3V\u0302 (B\u03022(c))\n)1/41/6\nseparately for each subsample of the treated and control, where V\u0302 (B\u03021(c)) and V\u0302 (B\u03022(c)) are variance estimates from the bias estimation with the pilot bandwidths. If the estimated signs of the bias terms are different, sgn(B\u03021(c)B\u03022(c)) < 0, then we use the same bandwidth ratio h1/h2, but the first-order bias can be eliminated. Hence, we set\nh1 = (\u03c3\u03022+(c) + \u03c3\u03022\u2212(c)) 2n e1S \u22121KS\u22121e\u20321(3V\u0302 (B\u03021(c)))/2)\u22121 ( B\u03022(c) 2\nB\u03021(c)2 + 3V\u0302 (B\u03021(c))\n)1/41/6\nand\nh2 = (\u03c3\u03022+(c) + \u03c3\u03022\u2212(c)) 2n e1S \u22121KS\u22121e\u20321(3V\u0302 (B\u03022(c))/2)\u22121 ( B\u03021(c) 2\nB\u03022(c)2 + 3V\u0302 (B\u03022(c)))\n)1/41/6\nwhere the bias terms are replaced with the regularization terms."
        },
        {
            "heading": "C Consequence of converting two-dimensional data to",
            "text": "one dimension.\nLet Zi = \u2225Ri\u2225 and K1(r) = 2(1\u2212 r)1{0\u2264r\u22641}. Define\nf\u030c(0) = 1\nn\u030ch n\u2211 i=1 K1(Zi/h)1{Ri,2\u22650}, n\u030c = n\u2211 i=1 1{Ri,2\u22650}.\nNote that n\u030c n = P (R1,2 \u2265 0) +Op(n\u22121/2) and\nf\u030c(0) =\n( 1\n(n\u030c/n) \u2212 1 P (R1,2 \u2265 0) +\n1\nP (R1,2 \u2265 0)\n) 1\nnh n\u2211 i=1 K1(Zi/h)1{Ri,2\u22650}\n= 1 P (R1,2 \u2265 0) 1 nh n\u2211 i=1 K1(Zi/h)1{Ri,2\u22650} +Op(n \u22121/2) =: 1\nP (R1,2 \u2265 0) f\u0303(0) +Op(n\n\u22121/2).\nFurther,\nE[f\u0303(0)] = 2\nh E[K1(Z1/h)1{R1,2\u22650}]\n= 2\nh\n\u222b (1\u2212 \u2225(r1/h, r2/h)\u2225)1{\u2225(r1/h, r2/h)\u2225 \u2264 1}1{r2\u22650}f(r)dr\n= 2\nh\n\u222b (1\u2212 \u2225(r1/h, r2/h)\u2225)1{\u2225(r1/h, r2/h)\u2225 \u2264 1}1{r2/h\u22650}f(r)dr\n= 2h \u222b (1\u2212 \u2225z\u2225)1{\u2225z\u2225\u22641,z2\u22650}f(hz1, hz2)dz\n= 2h ( f(0) \u222b (1\u2212 \u2225z\u2225)1{\u2225z\u2225\u22641,z2\u22650}dz + o(1) ) = 2h ( f(0)\n\u222b 1 0 (1\u2212 r)rdr \u222b \u03c0 0 d\u03b8 + o(1) ) = 2h (\u03c0 6 f(0) + o(1) )\nwhere we used the dominated convergence theorem for the fifth equation, and\nVar(f\u0303(0)) \u2264 1 nh2\nE [ K21(Z1/h)1{R1,2\u22650} ] = 4\nn\n\u222b (1\u2212 \u2225z\u2225)21{\u2225z\u2225\u22640,z2\u22650}f(hz1, hz2)dz\n= 4\nn\n( f(0) \u222b (1\u2212 \u2225z\u2225)21{\u2225z\u2225\u22641,z2\u22650}dz + o(1) ) = 4\nn\n( f(0) \u222b 1 0 (1\u2212 r)2rdr \u222b \u03c0 0 d\u03b8 + o(1) ) = 4\nn ( \u03c0 12 f(0) + o(1) )\nwhere we used the dominated convergence theorem for the second equation. Then we have\nf\u030c(0) = \u03c0h\n3P (R1,2 \u2265 0) f(0) + o(h) +Op(n\n\u22121/2)."
        }
    ],
    "title": "Local-Polynomial Estimation for Multivariate Regression Discontinuity Designs",
    "year": 2024
}