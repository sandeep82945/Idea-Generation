{
    "abstractText": "When inferring reward functions from human behavior (be it demonstrations, comparisons, physical corrections, or estops), it has proven useful to model the human as making noisy-rational choices, with a \u201crationality coefficient\u201d capturing how much noise or entropy we expect to see in the human behavior. Prior work typically sets the rationality level to a constant value, regardless of the type, or quality, of human feedback. However, in many settings, giving one type of feedback (e.g. a demonstration) may be much more difficult than a different type of feedback (e.g. answering a comparison query). Thus, we expect to see more or less noise depending on the type of human feedback. In this work, we advocate that grounding the rationality coefficient in real data for each feedback type, rather than assuming a default value, has a significant positive effect on reward learning. We test this in both simulated experiments and in a user study with real human feedback. We find that overestimating human rationality can have dire effects on reward learning accuracy and regret. We also find that fitting the rationality coefficient to human data enables better reward learning, even when the human deviates significantly from the noisy-rational choice model due to systematic biases. Further, we find that the rationality level affects the informativeness of each feedback type: surprisingly, demonstrations are not always the most informative\u2014when the human acts very suboptimally, comparisons actually become more informative, even when the rationality level is the same for both. Ultimately, our results emphasize the importance and advantage of paying attention to the assumed human-rationality-level, especially when agents actively learn from multiple types of human feedback.",
    "authors": [
        {
            "affiliations": [],
            "name": "Gaurav R. Ghosal"
        },
        {
            "affiliations": [],
            "name": "Matthew Zurek"
        },
        {
            "affiliations": [],
            "name": "Daniel S. Brown"
        },
        {
            "affiliations": [],
            "name": "Anca D. Dragan"
        }
    ],
    "id": "SP:d6a24520dc27d5f64588df47fc0870085be5ae2f",
    "references": [
        {
            "authors": [
                "A. Alanqary",
                "G.Z. Lin",
                "J. Le",
                "T. Zhi-Xuan",
                "V.K. Mansinghka",
                "J.B. Tenenbaum"
            ],
            "title": "Modeling the Mistakes of Boundedly Rational Agents Within a Bayesian Theory of Mind",
            "year": 2021
        },
        {
            "authors": [
                "A. Bajcsy",
                "D.P. Losey",
                "M.K. O\u2019Malley",
                "A.D. Dragan"
            ],
            "title": "Learning robot objectives from physical human interaction",
            "venue": "In Conference on Robot Learning,",
            "year": 2017
        },
        {
            "authors": [
                "C.L. Baker",
                "J.B. Tenenbaum",
                "R.R. Saxe"
            ],
            "title": "Goal inference as inverse planning",
            "venue": "Proceedings of the Annual Meeting of the Cognitive Science Society, volume 29.",
            "year": 2007
        },
        {
            "authors": [
                "E. B\u0131y\u0131k",
                "D.P. Losey",
                "M. Palan",
                "N.C. Landolfi",
                "G. Shevchuk",
                "D. Sadigh"
            ],
            "title": "Learning reward functions from diverse sources of human feedback: Optimally integrating demonstrations and preferences",
            "venue": "arXiv preprint arXiv:2006.14091.",
            "year": 2020
        },
        {
            "authors": [
                "R.A. Bradley",
                "M.E. Terry"
            ],
            "title": "Rank analysis of incomplete block designs: I",
            "venue": "The method of paired comparisons. Biometrika, 39(3/4): 324\u2013345.",
            "year": 1952
        },
        {
            "authors": [
                "D. Brown",
                "R. Coleman",
                "R. Srinivasan",
                "S. Niekum"
            ],
            "title": "Safe imitation learning via fast bayesian reward inference from preferences",
            "venue": "International Conference on Machine Learning, 1165\u20131177. PMLR.",
            "year": 2020
        },
        {
            "authors": [
                "D. Brown",
                "S. Niekum"
            ],
            "title": "Efficient probabilistic performance bounds for inverse reinforcement learning",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 32.",
            "year": 2018
        },
        {
            "authors": [
                "K. Chaloner",
                "I. Verdinelli"
            ],
            "title": "Bayesian experimental design: A review",
            "venue": "Statistical Science, 273\u2013304.",
            "year": 1995
        },
        {
            "authors": [
                "L. Chan",
                "A. Critch",
                "A. Dragan"
            ],
            "title": "Human irrationality: both bad and good for reward inference",
            "venue": "arXiv preprint arXiv:2111.06956.",
            "year": 2021
        },
        {
            "authors": [
                "P.F. Christiano",
                "J. Leike",
                "T. Brown",
                "M. Martic",
                "S. Legg",
                "D. Amodei"
            ],
            "title": "Deep reinforcement learning from human preferences",
            "venue": "Advances in neural information processing systems, 30.",
            "year": 2017
        },
        {
            "authors": [
                "Y. Cui",
                "S. Niekum"
            ],
            "title": "Active reward learning from critiques",
            "venue": "2018 IEEE international conference on robotics and automation (ICRA), 6907\u20136914. IEEE.",
            "year": 2018
        },
        {
            "authors": [
                "A.M. Do",
                "A.V. Rupert",
                "G. Wolford"
            ],
            "title": "Evaluations of pleasurable experiences: The peak-end rule",
            "venue": "Psychonomic bulletin & review, 15(1): 96\u201398.",
            "year": 2008
        },
        {
            "authors": [
                "O. Evans",
                "A. Stuhlm\u00fcller",
                "N.D. Goodman"
            ],
            "title": "Learning the Preferences of Ignorant, Inconsistent Agents",
            "venue": "Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, AAAI\u201916, 323\u2013329. AAAI Press.",
            "year": 2016
        },
        {
            "authors": [
                "C. Finn",
                "S. Levine",
                "P. Abbeel"
            ],
            "title": "Guided cost learning: Deep inverse optimal control via policy optimization",
            "venue": "International conference on machine learning, 49\u2013",
            "year": 2016
        },
        {
            "authors": [
                "T. Gr\u00fcne-Yanoff"
            ],
            "title": "Models of temporal discounting 1937\u20132000: An interdisciplinary exchange between economics and psychology",
            "venue": "Science in context, 28(4): 675\u2013713.",
            "year": 2015
        },
        {
            "authors": [
                "S. Guan",
                "L. Cheng",
                "Y. Fan",
                "X. Li"
            ],
            "title": "Myopic decisions under negative emotions correlate with altered time perception",
            "venue": "Frontiers in Psychology, 6: 468.",
            "year": 2015
        },
        {
            "authors": [
                "T. Haarnoja",
                "H. Tang",
                "P. Abbeel",
                "S. Levine"
            ],
            "title": "Reinforcement learning with deep energy-based policies",
            "venue": "International Conference on Machine Learning, 1352\u20131361. PMLR.",
            "year": 2017
        },
        {
            "authors": [
                "D. Hadfield-Menell",
                "A. Dragan",
                "P. Abbeel",
                "S. Russell"
            ],
            "title": "The off-switch game",
            "venue": "Workshops at the ThirtyFirst AAAI Conference on Artificial Intelligence.",
            "year": 2017
        },
        {
            "authors": [
                "D. Hadfield-Menell",
                "S. Milli",
                "P. Abbeel",
                "S.J. Russell",
                "A. Dragan"
            ],
            "title": "Inverse reward design",
            "venue": "Advances in neural information processing systems, 30.",
            "year": 2017
        },
        {
            "authors": [
                "P. Henry",
                "C. Vollmer",
                "B. Ferris",
                "D. Fox"
            ],
            "title": "Learning to navigate through crowded environments",
            "venue": "2010 IEEE International Conference on Robotics and Automation, 981\u2013986. IEEE.",
            "year": 2010
        },
        {
            "authors": [
                "B. Ibarz",
                "J. Leike",
                "T. Pohlen",
                "G. Irving",
                "S. Legg",
                "D. Amodei"
            ],
            "title": "Reward learning from human preferences and demonstrations in Atari",
            "venue": "arXiv preprint arXiv:1811.06521.",
            "year": 2018
        },
        {
            "authors": [
                "H.J. Jeon",
                "S. Milli",
                "A.D. Dragan"
            ],
            "title": "Rewardrational (implicit) choice: A unifying formalism for reward learning",
            "venue": "arXiv preprint arXiv:2002.04833.",
            "year": 2020
        },
        {
            "authors": [
                "A. Jonnavittula",
                "D.P. Losey"
            ],
            "title": "I know what you meant: Learning human objectives by (under) estimating their choice set",
            "venue": "2021 IEEE International Conference on Robotics and Automation (ICRA), 2747\u20132753. IEEE.",
            "year": 2021
        },
        {
            "authors": [
                "R.E. Kalman"
            ],
            "title": "When is a linear control system optimal? Transactions ASME",
            "venue": "Journal Basic Engineering.,",
            "year": 1964
        },
        {
            "authors": [
                "K.M. Kitani",
                "B.D. Ziebart",
                "J.A. Bagnell",
                "M. Hebert"
            ],
            "title": "Activity forecasting",
            "venue": "European conference on computer vision, 201\u2013214. Springer.",
            "year": 2012
        },
        {
            "authors": [
                "W.B. Knox",
                "P. Stone"
            ],
            "title": "Interactively shaping agents via human reinforcement: The TAMER framework",
            "venue": "Proceedings of the fifth international conference on Knowledge capture, 9\u201316.",
            "year": 2009
        },
        {
            "authors": [
                "H. Kretzschmar",
                "M. Spies",
                "C. Sprunk",
                "W. Burgard"
            ],
            "title": "Socially compliant mobile robot navigation via inverse reinforcement learning",
            "venue": "The International Journal of Robotics Research, 35(11): 1289\u20131307.",
            "year": 2016
        },
        {
            "authors": [
                "Y. Liu",
                "G. Datta",
                "E. Novoseller",
                "D.S. Brown"
            ],
            "title": "Efficient Preference-Based Reinforcement Learning Using Learned Dynamics Models",
            "venue": "International Conference on Robotics and Automation (ICRA). IEEE.",
            "year": 2023
        },
        {
            "authors": [
                "J. Mainprice",
                "R. Hayne",
                "D. Berenson"
            ],
            "title": "Predicting human reaching motion in collaborative tasks using inverse optimal control and iterative re-planning",
            "venue": "2015 IEEE International Conference on Robotics and Automation (ICRA), 885\u2013892. IEEE.",
            "year": 2015
        },
        {
            "authors": [
                "C. Matuszek",
                "N. FitzGerald",
                "L. Zettlemoyer",
                "L. Bo",
                "D. Fox"
            ],
            "title": "A joint model of language and perception for grounded attribute learning",
            "venue": "arXiv preprint arXiv:1206.6423.",
            "year": 2012
        },
        {
            "authors": [
                "O. Morgenstern",
                "J. Von Neumann"
            ],
            "title": "Theory of games and economic behavior",
            "venue": "Princeton university press.",
            "year": 1953
        },
        {
            "authors": [
                "A.Y. Ng",
                "S. J Russell"
            ],
            "title": "Algorithms for inverse reinforcement learning",
            "venue": "In Icml,",
            "year": 2000
        },
        {
            "authors": [
                "M. Palan",
                "G. Shevchuk",
                "N. Charles Landolfi",
                "D. Sadigh"
            ],
            "title": "Learning Reward Functions by Integrating Human Demonstrations and Preferences",
            "venue": "Robotics: Science and Systems.",
            "year": 2019
        },
        {
            "authors": [
                "D. Ramachandran",
                "E. Amir"
            ],
            "title": "Bayesian Inverse Reinforcement Learning",
            "venue": "IJCAI, volume 7, 2586\u20132591.",
            "year": 2007
        },
        {
            "authors": [
                "N.D. Ratliff",
                "J.A. Bagnell",
                "M.A. Zinkevich"
            ],
            "title": "Maximum margin planning",
            "venue": "Proceedings of the 23rd international conference on Machine learning, 729\u2013736.",
            "year": 2006
        },
        {
            "authors": [
                "M.L. Schrum",
                "E. Hedlund-Botti",
                "N. Moorman",
                "M.C. Gombolay"
            ],
            "title": "Mind meld: Personalized metalearning for robot-centric imitation learning",
            "venue": "2022 17th ACM/IEEE International Conference on Human-Robot Interaction (HRI), 157\u2013165. IEEE.",
            "year": 2022
        },
        {
            "authors": [
                "R. Shah",
                "N. Gundotra",
                "P. Abbeel",
                "A. Dragan"
            ],
            "title": "On the feasibility of learning, rather than assuming, human biases for reward inference",
            "venue": "International Conference on Machine Learning, 5670\u20135679. PMLR.",
            "year": 2019
        },
        {
            "authors": [
                "T. Sharot",
                "A.M. Riccardi",
                "C.M. Raio",
                "E.A. Phelps"
            ],
            "title": "Neural mechanisms mediating optimism bias",
            "venue": "Nature, 450(7166): 102\u2013105.",
            "year": 2007
        },
        {
            "authors": [
                "S.C. Thompson"
            ],
            "title": "Illusions of control: How we overestimate our personal influence",
            "venue": "Current Directions in Psychological Science, 8(6): 187\u2013190.",
            "year": 1999
        },
        {
            "authors": [
                "D. Vasquez",
                "B. Okal",
                "K.O. Arras"
            ],
            "title": "Inverse reinforcement learning algorithms and features for robot navigation in crowds: an experimental comparison",
            "venue": "2014 IEEE/RSJ International Conference on Intelligent Robots and Systems, 1341\u20131346. IEEE.",
            "year": 2014
        },
        {
            "authors": [
                "C. Wirth",
                "R. Akrour",
                "G. Neumann",
                "J F\u00fcrnkranz"
            ],
            "title": "A survey of preference-based reinforcement learning methods",
            "venue": "Journal of Machine Learning Research",
            "year": 2017
        },
        {
            "authors": [
                "M. Wulfmeier",
                "P. Ondruska",
                "I. Posner"
            ],
            "title": "Maximum entropy deep inverse reinforcement learning",
            "venue": "arXiv preprint arXiv:1507.04888.",
            "year": 2015
        },
        {
            "authors": [
                "B.D. Ziebart",
                "A.L. Maas",
                "J.A. Bagnell",
                "A. K Dey"
            ],
            "title": "Maximum entropy inverse reinforcement learning",
            "venue": "In Aaai,",
            "year": 2008
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Reward learning started from the inverse optimal control idea that we can recover the underlying objective when observing optimal behavior (Kalman 1964), and transitioned into AI with the introduction of inverse reinforcement learning (Ng, Russell et al. 2000). While initial research assumed optimal demonstrators (Ng, Russell et al. 2000; Ratliff, Bagnell, and Zinkevich 2006), the field quickly moved to the noisy-rational human model (Morgenstern and Von Neumann 1953): a number of simultaneous works, with different motivations, converged on a Bolzmann (maximum\n*These authors contributed equally. Copyright \u00a9 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nentropy) distribution, where the human actions are exponentially more probable the higher value they are (Baker, Tenenbaum, and Saxe 2007; Ramachandran and Amir 2007; Ziebart et al. 2008; Henry et al. 2010; Vasquez, Okal, and Arras 2014; Kretzschmar et al. 2016; Kitani et al. 2012; Wulfmeier, Ondruska, and Posner 2015; Brown and Niekum 2018; Christiano et al. 2017; Finn, Levine, and Abbeel 2016; Mainprice, Hayne, and Berenson 2015). Often this model would have a \u201drationality\u201d coefficient \u03b21 meant to capture how good of an optimizer the human is\u2014setting \u03b2 to 0 would yield the uniform distribution capturing a random human, while \u03b2 \u2192 \u221e would put all the probability mass on optimal actions.\nInspired by the way economists look at preferences, the field then started looking beyond learning from demonstrations to learning from comparisons (Wirth et al. 2017; Christiano et al. 2017; B\u0131y\u0131k et al. 2020). The model was similar: still a Boltzmann distribution, but over two trajectories/actions, instead of over all possible trajectories/actions. Other researchers started looking at a deluge of feedback types: comparisons (Wirth et al. 2017), language (Matuszek et al. 2012), demonstrations (Ng, Russell et al. 2000), trajectory rankings (Brown et al. 2020), corrections (Bajcsy et al. 2017), critiques (Cui and Niekum 2018), e-stops (HadfieldMenell et al. 2017a), binary feedback (Knox and Stone 2009), and proxy rewards (Hadfield-Menell et al. 2017b). Recently, it was shown that all of these can be interpreted as noisy-rational (Boltzmann) choices (Jeon, Milli, and Dragan 2020), opening the door to learning from all of these feedback types in combination, and even enabling robots to actively select feedback types.\nBoltzmann-rationality\u2019s ability to unify different feedback types is useful, but the model comes with this one parameter, \u03b2, which begs the question: what should we set that to? Prior work often either omits \u03b2 (implicitly setting it to 1) (Finn, Levine, and Abbeel 2016; Christiano et al. 2017; Ibarz et al. 2018) or sets it to a fixed, often heuristic, value across all feedback types (Ramachandran and Amir 2007; Shah et al. 2019; B\u0131y\u0131k et al. 2020; Jeon, Milli, and Dragan 2020). But demonstrations are sometimes easier or harder to give, depending on the task and the interface, suggesting that \u03b2 should be adapted to the domain. And comparisons\n1Sometimes denoted by \u03b1 and sometimes as the inverse (1/\u03b2).\nar X\niv :2\n20 8.\n10 68\n7v 2\n[ cs\n.L G\n] 9\nM ar\n2 02\n3\nmight be much easier to answer than demonstrations, suggesting we should be using a higher \u03b2 for the former. Our goal in this work is to answer the question: does this matter? Are there real benefits to grounding \u03b2 in real data for each feedback type, or is it safe to stick to a default value?\nWe analyze this in both simulation and a user study. Our investigation begins in the single-feedback regime, where we isolate the key role played by \u03b2 in interpreting feedback. Through empirical and theoretical analysis, we conclusively demonstrate that overestimating \u03b2 is particularly harmful for reward inference and that inferring \u03b2 benefits reward learning by avoiding this. Importantly, our results generalize across varying forms of biased and noisy behavior, beyond Boltzmann rationality. We find that many known systematic biases can be approximated by a noisy-rational model with a learned \u03b2, enabling better reward inference. We demonstrate this in simulation with specified biases as well as in a user study, where feedback contains arbitrary human biases.\nWe next study the importance of \u03b2 when an agent actively learns from multiple feedback types. In this setting, we observe a new role played by \u03b2: in addition to controlling the reward inference, the estimate of \u03b2 also affects the selection of which type of feedback should be queried. In particular, we see that \u03b2 strongly affects the informativeness of different feedback types. Surprisingly, this is true even when there is a shared rationality level across feedback types: at low \u03b2 values, we show that comparisons are more informative than demonstrations, while demonstrations gain an advantage at higher \u03b2 values. As a result of \u03b2\u2019s role in both query selection and feedback interpretation, setting it correctly has a significant impact on performance and different settings of beta significantly change the queries selected by active learning. Notably, our findings show the insufficiency of relying on popular heuristics such as starting with demonstra-\ntions and fine-tuning with comparisons (Ibarz et al. 2018; Palan et al. 2019; B\u0131y\u0131k et al. 2020; Liu et al. 2023). Rather, we demonstrate that accounting for rationality is essential for active learning to uncover the feedback query that is truly most informative.\nOverall, we contribute an analysis of the effects of estimating a human\u2019s rationality level on the quality of reward learning and demonstrate the importance of using the \u03b2 parameter in a principled way over heuristic approaches. In particular, we show that setting \u03b2 appropriately becomes increasingly crucial as we develop agents that actively learn from multiple types of human feedback. Our analysis can be summarized by the following practical findings:\n1. Modeling human behavior with Boltzmann rationality provides benefits even in the face of harder to model systematic biases.\n2. When accurate estimates of \u03b2\u0302 cannot be found, one should err on the side of underestimation.\n3. The success of active learning over multiple feedback types depends strongly on an accurate estimate of \u03b2\u0302.\n4. The most informative feedback type varies as a function of the human\u2019s rationality, even when the feedback types share a common rationality level.\n5. It is possible to obtain good estimates of \u03b2\u0302 by obtaining a small amount of calibration feedback (where the human optimizes a known reward)."
        },
        {
            "heading": "2 Formulation",
            "text": "Preliminaries and Notation. We model the environment as a finite horizon Markov decision process (MDP) with states s \u2208 S, actions a \u2208 A, and transition dynamics P (s\u2032 | s, a). The reward function r : (S \u00d7 A) \u2192 R is initially unknown\nto the robot but is communicated by a human through multiple forms of feedback, such as demonstrations of desired behavior, preference comparisons between trajectories, and corrective interventions.\nFollowing prior work (Jeon, Milli, and Dragan 2020), we interpret these varying forms of feedback as a rewardrational choice over a (potentially implicit) choice set C. In this work, we study a robot that actively chooses from multiple feedback types. To facilitate this, we model a query for human feedback as a Bayesian experimental design problem (Chaloner and Verdinelli 1995). We define feedback types as functions from designs to (choice set, grounding) pairs and active learning as the optimization over designs for information gain. For each possible feedback type, the robot has a choice over different possible experimental designs, X , where the experiment design must be specified before the human can provide data. Given an experiment design x \u2208 X and a human choice c \u2208 C(x), we use the grounding function \u03d5(x, c) to ground the human feedback into the space of trajectories, \u039e. The grounded trajectory is interpreted to be Boltzmann rational under the human\u2019s reward function. A trajectory, \u03be, is defined as a sequence of state-action pairs. We use \u03bei:j to denote the sub-trajectory starting with the ith state-action pair and ending with the jth state-action pair."
        },
        {
            "heading": "2.1 Human Feedback Types",
            "text": "In this work, we study the three feedback types below. We refer the reader to Jeon et al. (Jeon, Milli, and Dragan 2020) for a discussion of how many other feedback types can be formalized similarly. Demonstrations can be viewed as a sequence of explicit choices over actions conditioned on states.2\nThe design is all starting states: X = S and the grounding function is identity. For a demonstration \u03be starting from state s0 we have the following observation model.\nP (\u03be | r, \u03b2) = \u220f\n(st,at)\u2208\u03be\n\u03c0\u03b2(at | st)\n= \u220f\n(st,at)\u2208\u03be\nexp(\u03b2Qsoftt (st, at | r))\u2211 b\u2208A exp(\u03b2Q soft t (st, b | r)) (1)\nwhere Qsoftt (s, a | r) = r(s, a) + \u03b3Es\u2032 [V softt+1 (s\u2032)], and V softt (s) = Ea\u223c\u03c0\u03b2 [Qsoftt (s, a) \u2212 log \u03c0\u03b2(a | s)] are the soft Q-function, and Value function, respectively (Kitani et al. 2012; Haarnoja et al. 2017), and \u03c0\u03b2 is the corresponding (time-dependent) policy. Comparisons are a choice between two trajectories. Thus, the possible designs are X = \u039e2, all pairs of trajectories and the grounding function maps to the preferred trajectory. The likelihood that the human prefers trajectory A over B\n2Jeon et al. (Jeon, Milli, and Dragan 2020) model human demonstrations as choices over all possible trajectories; however, with stochastic dynamics, human actions are conditioned on observed state transitions and the human cannot pre-select a specific trajectory.\nis given by the Bradley-Luce-Shepherd rule (Bradley and Terry 1952; Christiano et al. 2017):\nP (\u03beA | r, \u03b2) = exp (\u03b2 \u00b7 r(\u03beA))\nexp (\u03b2 \u00b7 r(\u03beA)) + exp (\u03b2 \u00b7 r(\u03beB)) (2)\nE-stops represent the intervention of a human telling the robot to stop rather than continue its trajectory. We assume that the human is able to observe the robot\u2019s planned trajectory and then selects a desired stopping point t at which point the episode terminates. Thus, the space of possible designs is X = \u039e, all trajectories. The choice set is the stopping time t, and the grounding function is the sub-trajectory \u03be0:t. Given a robot trajectory \u03be, we have the following likelihood function for the human\u2019s choice ch = t:\nP (t | \u03be, r, \u03b2) = exp(\u03b2 \u00b7 r(\u03be0:t))\u2211T k=0 exp(\u03b2 \u00b7 r(\u03be0:k)) . (3)"
        },
        {
            "heading": "2.2 Estimating a Human\u2019s Rationality Level from Data",
            "text": "Rather that assuming a known or constant value for the rationality coefficient, we study the effect of learning an estimate of the human\u2019s rationality level, \u03b2\u0302, from human data. As a vehicle for our analysis, we consider access to a separate calibration phase where we present the human with a known, calibration reward function, r\u2032, and then ask them to provide feedback (e.g., demonstrations, comparisons, e-stops) with respect to this reward function. The benefit of this calibration is that given human feedback that corresponds to a known reward function, we can find \u03b2\u0302 that maximizes the log-likelihood of Eq. (3):\n\u03b2\u0302 = arg max \u03b2 \u03b2 \u00b7 E\u03be\u223c\u03d5(x,ch)[r(\u03be)]\n\u2212 log \u2211 c\u2208C exp ( \u03b2 \u00b7 E\u03be\u223c\u03d5(x,c)[r(\u03be)] ) ,\n(4)\nsince ch is collected from the human during calibration and r\u2032, x, C, \u03d5 are constant and known to the robot. This approach intentionally favors simplicity over real-world practicality\u2014our focus is on assessing the importance of having a good model of the human\u2019s rationality level\u2014in practice, one could also fit \u03b2\u0302 on human data optimizing an unknown r by marginalizing over r."
        },
        {
            "heading": "2.3 Active Learning over Feedback Types",
            "text": "We consider the scenario in which the robot can actively query the most informative feedback given its current belief over a parameterized reward function. We can cast this as the problem of selecting a design X which optimizes the expected information gain over the possible human feedback induced by X . Concretely, this can be written as the following optimization problem\nmax x\u2208X\nEch\u223cP (ch|x) [ DKL ( P (\u03b8|ch, x)\u2016P (\u03b8) )] , (5)\nin which we consider P (\u03b8) to be our prior distribution over the reward function, r\u03b8, parameterized by \u03b8."
        },
        {
            "heading": "3 Effect of \u03b2\u0302 when Learning from a Single Feedback Type",
            "text": "In this section, we test the following hypothesis:\nH1: Reward inference with a fitted beta will perform better than reward inference with a default beta across different feedback types and different forms of human irrationality.\nWe test hypothesis H1 over different forms of simulated feedback: first Boltzmann-rational and then systematically biased. In Section 5, we test H1 on real human data.\nMetrics. We measure reward inference performance via the normalized regret from optimizing the posterior mean reward i.e., the difference measured in the ground truth reward between optimizing the ground truth reward vs optimizing the posterior mean inferred reward. In Appendix C, we evaluate other reward inference metrics, which show similar trends.\nExperimental Design. We follow the same experimental design when fitting \u03b2\u0302 and performing reward inference for both simulated and real human feedback. When fitting \u03b2\u0302, we use 4 randomly chosen calibration reward functions and query the human for feedback 5 times for each calibration reward. When performing reward inference we query the human for feedback 5 times and then perform reward inference using the previously calibrated \u03b2\u0302. When analyzing learning from individual feedback types, we randomly sample designs. In Sec. 4 we examine actively selecting designs.\nExperimental Domains. Our simulation experiments take place in a suite of discrete gridworld navigation environments. Reward functions, r\u03b8, are parameterized by a linear combination of features that indicate the color of each gridcell (see Appendix B for more details). In order to perform exact Bayesian inference, we discretize the reward space by 1000 points. Additionally, in Appendix K, we provide evidence of similar results in a self-driving car domain."
        },
        {
            "heading": "3.1 Learning from Simulated Boltzmann-Rational Feedback",
            "text": "Results. We assess the importance of beta fitting for demonstrations, comparisons, and e-stops by running reward inference on simulated Boltzmann-rational feedback generated with different \u03b2 values. We compare reward inference using the fitted \u03b2\u0302 (Fitted) to a default method that sets \u03b2\u0302 = 1 (Default) and an oracle method that has access to the ground truth \u03b2 value (Oracle). The results in Fig. 2 demonstrate that when feedback is highly sub-optimal, Fitted results in significantly better inference than Default, and performs comparably to Oracle. These observations support H1\u2014using a learned value for \u03b2\u0302 improves performance, especially in cases where the human acts more noisily. Remark: Under-estimating \u03b2 is better than overestimating it. In Fig. 2, we observe an asymmetry between the settings of over- and under-estimating \u03b2. We see that while over-estimation results in poorer performance, underestimation does not harm reward inference performance as much. In what follows, we present some intuition for this phenomenon. In particular, we show that using a lower \u03b2\u0302 is risk-averse when the human is suboptimal but is still a good choice even when the person is optimal, whereas a high value of \u03b2\u0302 leads to poor reward inference when the human is suboptimal. To simplify notation, we define r(c) , E\u03be\u223c\u03d5(x,c)[r(\u03be)] for c \u2208 C. Proposition 1. If the human is optimal, then r\u2217 is an MLE estimate for any value of \u03b2\u0302 \u2208 [0,\u221e).\nProof. An optimal human (i.e., \u03b2 = \u221e) never makes mistakes. Thus, r\u2217(ch) \u2265 r\u2217(c),\u2200c \u2208 C and\nr\u2217 \u2208 arg max r er(ch) = arg max r e\u03b2\u0302\u00b7r(ch)\n= arg max r P (ch|r, \u03b2\u0302).\n(6)\nThus, r\u2217 is an MLE estimate given ch, \u2200\u03b2\u0302 \u2208 [0,\u221e).\nEven though the MLE reward may not change, the shape of the posterior distribution over r is strongly influenced by the choice of \u03b2\u0302. When the human is suboptimal, we want to have robots that hedge their bets, rather than becoming overly confident in their estimate of the true reward function. Prior work proposes the Shannon entropy over the robot\u2019s belief P (r | ch) as a quantitative measure of the robot\u2019s confidence (Jonnavittula and Losey 2021). Using this definition, we present the following result.\nProposition 2. The robot becomes more (over-)confident as \u03b2\u0302 increases.\nProof. If \u03b2\u0302 = 0 and we have a uniform prior, then we have a uniform belief distribution over r, resulting in maximum entropy and risk-averse behavior. As \u03b2\u0302 increases, the posterior distribution concentrates on only a small number of reward functions, resulting in lower entropy and less risk-aversion. To see this note that\nP (r | ch, \u03b2\u0302) \u221d exp(\u03b2\u0302 \u00b7 r(ch))\u2211 c\u2208C exp(\u03b2\u0302 \u00b7 r(c)) P (r)\n= 1 1 + \u2211 c 6=ch exp(\u03b2\u0302 \u00b7 (r(c)\u2212 r(ch))) P (r)\n(7)\nAs \u03b2\u0302 \u2192\u221e, we have exp(\u03b2\u0302 \u00b7 (r(c)\u2212 r(ch)))\u2192 0 if r(c) < r(ch) and exp(\u03b2\u0302 \u00b7 (r(c)\u2212 r(ch))) \u2192 \u221e if r(c) > r(ch). Thus, P (r | ch, \u03b2\u0302)\u2192 0 as \u03b2\u0302 \u2192\u221e if ch does not maximize r(c) and P (r | ch, \u03b2\u0302) \u221d P (r) if ch uniquely maximizes r(c). If ch is a non-unique maximizer of r(\u03c6(x, c)), then we have P (r | ch, \u03b2\u0302) \u221d P (r)/|{c : r(c) = r(ch)}|.\nThe above result shows that, as \u03b2\u0302 increases, the Shannon entropy decreases and the robot places high probability on a smaller set of reward functions, thereby behaving very confidently about its estimate of the reward. Finally, we have the following result for when the human makes a sub-optimal feedback choice.\nProposition 3. If the human makes a suboptimal feedback choice, the likelihood of the true reward, r\u2217, decreases exponentially as \u03b2\u0302 increases.\nProof. For a suboptimal choice ch, \u2203c\u2217 such that r\u2217(ch) < r\u2217(c\u2217) and\nP (ch|r\u2217, \u03b2\u0302) = exp (\u03b2\u0302r\u2217(ch))\u2211 c exp (\u03b2\u0302r \u2217(c))\n\u2264 exp (\u03b2\u0302r \u2217(ch))\nexp (\u03b2\u0302r\u2217(c\u2217)) = exp (\u03b2\u0302(r\u2217(ch)\u2212 r\u2217(c\u2217)))\n(8)\nBy assumption r\u2217(ch) \u2212 r\u2217(c\u2217) < 0. Therefore, the likelihood decreases exponentially as \u03b2\u0302 increases.\nThis analysis supports our empirical findings and shows overestimating \u03b2\u0302 can have negative consequences since it\nmakes the robot overconfident in the human data, potentially overfitting to mistakes. On the other hand, using a lower \u03b2\u0302 leads to more risk-averse behavior (beneficial when the human is suboptimal), while still being optimal (under a uniform prior) when learning from perfectly-rational humans."
        },
        {
            "heading": "3.2 Learning from Simulated Biased Feedback",
            "text": "The results in Section 3.1 demonstrate the importance of modeling a human\u2019s rationality level when the human actually is Boltzmann-rational. But of course, human behavior can suffer from systematic biases and irrationalities, not just Boltzmann (ir)rationality (Evans, Stuhlmu\u0308ller, and Goodman 2016; Alanqary et al. 2021). In this section, we study whether we can use the model of Boltzmann rationality, with a learned rationality level, to improve reward inference in the presence of biases commonly exhibited in human behavior (Guan et al. 2015; Do, Rupert, and Wolford 2008; Sharot et al. 2007; Thompson 1999). H1 hypothesizes that inferring beta can help compensate for these unmodelled aspects of human behavior and therefore lead to better reward inference, in particular when the impact of the bias is not consistent across feedback types. We first evaluate this hypothesis by generating simulated feedback with various biases unknown at reward-inference time. Later we evaluate this hypothesis with a user study. Types of Simulated Bias. We study several different models of human biases. Following (Chan, Critch, and Dragan 2021), we formalize each bias as a particular modification to the standard Bellman update, resulting in a modified value function which we use to determine the resulting policy and simulate choices from a biased human. We assume that the person is Boltzmann rational under their biased value function. Thus, \u03b2 remains a parameter, and we set it to 1 in all cases; however, the presence of the bias means that the human is actually not Boltzmann \u03b2-rational for any \u03b2 > 0. Myopia Bias: Humans sometimes demonstrate myopic behavior, concentrating on immediate rewards without evaluating the longer-term impacts of their actions (Guan et al. 2015; Gru\u0308ne-Yanoff 2015). We simulate myopic human feedback by changing the discount factor \u03b3 \u2208 [0, 1] and then providing Boltzmann-rational feedback with respect to the value function computed using this discount factor. Extremal Bias: Humans sometimes pay attention to highintensity aspects of an experience, at the exclusion of lowerintensity events (Do, Rupert, and Wolford 2008). We model this behavior using a modified Bellman update\nVi+1(s) = \u2211 s\u2032\u2208S P (s\u2032|s, a) max(r(s, a), (1\u2212\u03b1)r(s, a)+\u03b1Vi(s\u2032)),\n(9) where \u03b1 \u2208 [0, 1]. As \u03b1 \u2192 1 the human seeks to maximize\nthe maximum reward obtained at any point within a trajectory. As \u03b1\u2192 0, the human maximizes immediate reward. Optimism/Pessimism Bias: Humans can sometimes over- or under-estimate the likelihood of experiencing a good or bad event (Sharot et al. 2007). We simulate this bias by changing the transition function that the biased human uses for planning to reflect the fact that the human believes that the\nlikelihood of an outcome depends on its value: P\u0303 (s\u2032 | s, a) \u221d P (s\u2032 | s, a) \u00b7 exp ( \u03c4 \u00b7 (r(s, a) + \u03b3Vi(s\u2032)) ) , (10) where \u03c4 \u2208 R. As \u03c4 increases (decreases) the human becomes more optimistic (pessimistic). Results. We study three variants of reward inference: (1) Default assumes \u03b2\u0302 = 1 and performs inference under the corresponding Boltzmann-rational model of feedback, (2) Fitted first fits the rationality parameter \u03b2\u0302 and then performs reward inference using the \u03b2\u0302-Boltzmann rational model, and (3) Oracle performs reward inference using the true model of the bias. We present the results for demonstrations in Fig. 3 and refer the reader to Appendix D for results on comparisons and e-stops.\nOur findings in Fig. 3 provide evidence for H1: using a learned value for \u03b2\u0302 overall results in lower regret than using a default value. However, we find that there is often a large gap between the performance of Fitted and Oracle. This demonstrates that while there is utility in estimating the rationality level of the human, it is not always possible to accurately model systematically biased behavior using a tuned Boltzmann rationality model. In particular, for the Optimism bias, we find that both Fitted and Default perform similarly, and that as the \u03c4 parameter diverges from 0 (diverging from Boltzmann rationality) regret increases. Understanding the Success and Failure of Beta Fitting for Biased Human Feedback. To understand when \u03b2-fitting improves reward inference, we study \u03b2\u0302-generalization and quality of fit on the Myopia and Optimism biases. We consider the infinite data limit of maximum likelihood estimation, which, as shown in Appendix F, can be calculated from the biased demonstration policy via an adapted policy evaluation technique.\nIn Fig. 4 (a) and (c), we show the variance of the MLE \u03b2\u0302 over different reward functions at various levels of Myopia and Optimism bias, respectively. A lower variance in\nthis experiment implies that \u03b2\u0302 generalizes well across different reward functions for that bias setting. The uniformly low variances for the myopia bias suggest that the fitted \u03b2\u0302 remains consistent over different reward functions, while the higher variances for the optimism bias suggest that the fitted rationality coefficient, \u03b2\u0302, is less transferable across different reward functions.\nIn Fig. 4 (b) and (d), we fixed the ground truth reward \u03b8 and show scatter plots of the KL-Divergence between the biased policy on \u03b8 and the soft-optimal policies for a large sample of other rewards, \u03b8\u2032. We generate these scatter plots for different bias settings. We observe that the Myopia bias has some rewards with low magnitude KL-Divergences to the biased policy, indicating that the biased policy can be fit well by Boltzmann rationality. On the other hand, for some settings of optimism bias (such as \u03c4 = \u00b140), the magnitude of the KL-Divergence is uniformly high, indicating that Boltzmann rationality cannot fit the biased policy well with any reward function. Interestingly, neither a low beta variance nor a low KL-Divergence ensures that \u03b2-fitting can recover the true reward. For example, when \u03b3 = 0.1 both \u03b2 variance and KL-divergence are close to 0, yet fitted performs worse than oracle.\nIn Fig. 4, we see that under some bias settings, such as \u03c4 = \u00b140 or \u03b3 = 0.1, all the soft-optimal policies for the sampled rewards (\u03b8\u2032) are roughly equidistant from the biased policy (forming a tight cluster in the scatter plot). In these settings, \u03b2-fitting fails since the true reward cannot be uniquely identified\u2014all rewards appear to model the biased behavior equally well. Comparing Fig. 4 (b) and (d), we see this situation can arise both when the biased behavior can (in the case of myopia \u03b3 = 0.1) and cannot (in the case of optimism \u03c4 = \u00b140) be modeled well by the Boltzmann distribution, as measured by the scale of the KL-Divergences. We leave further analysis of which biases preserve reward identifiability to future work."
        },
        {
            "heading": "4 Effect of \u03b2\u0302 when Actively Learning over Multiple Feedback Types",
            "text": "In the previous section, we observed fitting \u03b2 is beneficial when learning passively from a single feedback type. In this section, we consider allowing the robot to actively select the feedback it receives from a set of multiple feedback types, taking into account its current belief over the reward function. When performing this selection, an additional notion of feedback informativeness plays a key role. We first investigate the interaction between rationality and feedback informativeness, followed by an analysis of the overall reward inference performance in the active learning setting. Ultimately, we test the following hypothesis: H2: Active learning that decides what feedback to ask for will perform better with a fitted beta for each type than with a default beta. Rationality and Feedback Informativeness. We first examine the information gain provided by the different feedback types when the human is equally rational across alltypes. In this case, it may appear intuitive that demonstrations would uniformly provide the most information gain because they represent an implicit choice over all possible trajectories. However, our results in Fig. 5a reveal that the most valuable human feedback type is a function of the common rationality parameter \u03b2. While demonstrations do provide the most information when the human is highly rational, comparisons gain an advantage when querying a more irrational human.\nIn Appendix E we further explore this surprising finding in a toy environment. We construct a toy reward inference environment with a finite set of reward functions and choices. The structure of the reward function and the choice set of this environment means that, given a ground truth reward, only a subset of the choices will be sensitive to correctly identifying this reward. Intuitively, this means that there will be many \u201cuninformative\u201d choices in the choice set. An uninformative choice will result in poor information gain, as the posterior reward distribution will remain largely\nunchanged. We model demonstrations as a feedback query where the user may choose any choice from the entire choice set. As \u03b2 \u2192 0, demonstrations become increasingly noisy and the human converges to choosing uniformly from all choices, yielding a higher probability of making an uninformative choice and reducing the expected information gain. On the other hand, comparisons restrict the choice set to two elements. Thus, it is possible to construct a comparison query which eliminates uninformative choices and therefore has a higher expected information gain. This analysis confirms the trend we see in Fig. 5a, where the rationality coefficient \u03b2 has a strong influence on the informativeness of different feedback types.\nImportance of Beta Fitting for Active Reward Learning. In practice, a human is likely to have have varying degrees of rationality across feedback types. Intuitively, \u03b2 plays two roles in this setting: it affects the kinds of queries that are selected as well as the interpretation of the response. In order to gain a more complete understanding of the impact of \u03b2 in this regime, we seek to disentangle the relative importance of these roles. We study the reward inference performance of four variants of active learning, where each of the 2 steps (query selection and reward inference) has either the correct or default \u03b2. In Fig. 5, we show these results for one choice of relative rationalities (\u03b2demo = 0.1, \u03b2comp = 10, \u03b2estop = 1) and in Appendix I, we consider the case where demonstrations are the most rational but all feedback rationalities are overestimated by default. We observe that the relative importance of having the correct \u03b2 for query selection vs. reward inference varies significantly between these cases. When comparisons have a much higher rationality than demonstrations (shown in Fig. 5), the value of \u03b2 used for active learning plays a significant role in quality of reward inference, because when comparisons are selected, the default \u03b2 underestimates rationality. On the other hand, Appendix I shows that when the default values of \u03b2 are all overestimates and demonstrations are the most rational, then the \u03b2 used for reward inference has a significant effect on performance. Ultimately, our results reveal an in-\ntricate dependency of the optimal active learning strategy on the human rationality and underscores the insufficiency of generic heuristics and the importance of calibrating to individual biases and noise levels."
        },
        {
            "heading": "5 User Study",
            "text": "We conducted a small-scale user study ofN = 7 users (aged 21-58, mean = 28) in order to test the effect of conducting \u03b2fitting on real-world human irrationality. The user study took place in the same grid-world navigation setup as the simulated experiments and each user provided a set of 5 comparisons and 5 demonstrations for each of 5 reward functions. In our setting, the interface for demonstrations was more challenging than that for comparisons, due to the presence of slippery dynamics in the demonstrations control interface (see Appendix J for more details). For each reward function, we tested the reward inference by using the data corresponding to the 4 other reward functions to fit \u03b2\u0302 and then running reward inference using the individual feedback types, as well as active selection from both. The comparison of performance between using \u03b2 = 1 (Default) and \u03b2 = \u03b2\u0302 (Fitted) are shown in Figure 4(c). We observe that the results validate both H1 and H2. \u03b2-fitting on demonstrations results in better performance than using the default \u03b2 and we observe a particularly large benefit from fitting beta in the active learning setting. For comparisons, we observed that the users were able to perform close-to-optimally, which lessened the importance of modeling the rationality level."
        },
        {
            "heading": "6 Discussion",
            "text": "Summary: In this work, we examine the importance of modeling the level of human rationality when learning from multiple kinds of human feedback. We demonstrate the importance of utilizing the correct rationality coefficient in cases where the human is Boltzmann-rational (with an unknown rationality level), as well in cases where the human\nis not Boltzmann-rational, but is systematically biased. Finally, we demonstrate that \u03b2-fitting is especially important when performing active learning: in a user study we find that active queries based on learned rationality levels significantly outperform an active learning baseline that uses a uniform, default level of rationality across feedback types. Limitations and Future Work: Our contribution is studying the importance of having an estimate of \u03b2 (the human\u2019s level of rationality), but how exactly to get that remains an open question\u2014our experiments use calibration data, assuming that we can \u201cincept\u201d a calibration reward function into a human\u2019s head and then ask them to provide feedback. We note that this type of calibration approach has been shown to work well in some settings, such as humans interacting with a driving simulator (Schrum et al. 2022); however, in other settings, this type of calibration may be difficult, and future work includes studying \u03b2-fitting techniques that do not require providing the human with an explicit calibration reward function. Furthermore, while we study the benefits of \u03b2-fitting on actively learning from multiple feedback types, we have only modeled the rationality level of each feedback type, ignoring the query cost in terms of cognitive burden and time required. Future work includes learning models of the cognitive burden per user and per feedback type, incorporating cognitive and feedback-time costs into active learning, and analyzing \u03b2-fitting in more domains."
        },
        {
            "heading": "Ethics Statement",
            "text": "Our work seeks to approximate potentially biased behavior with noisily rational behavior. This could have negative societal impacts if it leads a robot to incorrectly infer human intent, especially in safety critical settings. While our results show that \u03b2-fitting is useful, we caution against simply forcing robots to view all human behavior as \u03b2-rational\u2014using more nuanced and sophisticated models of human bias and irrationality is an important area of future work."
        },
        {
            "heading": "Acknowledgments",
            "text": "We thank the members of the InterACT lab for helpful discussion and advice. This work was supported by the ONR Young Investigator Program (YIP)."
        },
        {
            "heading": "B.1 Environment Details",
            "text": "Our user study and simulation experiments take place in a 10 by 10 colored grid-world environment, with each tile having one of four colors and the bottom-right corner being a terminal goal state. An example is shown in Figure 6. We consider a space of\nreward functions that assign each tile color to a reward earned for landing on that color. For simplicity, we consider the space of\nnormalized reward functions; the reward functions have unit `2 norm when considered as four-dimensional vectors  RredRblueRgreen Ryellow ."
        },
        {
            "heading": "B.2 Reward Inference Performance Metrics",
            "text": "In this work, we consider two metrics for the success of reward inference metric, as described below: Here Rinferred refers to the posterior mean reward and \u03b8inferred refers to the posterior mean reward vector.\nReward Regret The final goal of reward inference is often to train an agent to maximize the human\u2019s reward function. In this setting, the actual values of the reward parameters are irrelevant so long as they induce a similar optimal behavior to the ground-truth reward function. We measure the normalized regret-the amount of reward the agent misses by following the inferred reward- as:\nRegret = 1\u2212 Rinferrd \u2212Rrandom Rtrue \u2212Rrandom\n(11)\nwhere Rinferred is the reward attained by the optimal policy of the inferred reward under the ground truth reward, Rrandom is the reward attained by a random policy under the ground-truth reward, and RTrue is the reward attained by the ground-truth reward under the ground-truth reward.\nReward MSE Alternatively, we might be interested in how closely the inferred reward parameters match those of the groundtruth. In order to measure this, we computed a reward mean squared error by treating the reward functions as vectors in \u03b8 \u2208 R4,\n\u03b8 =  R\u03b8red R\u03b8blue R\u03b8green R\u03b8yellow . Then, we can compute reward mean-squared-error as: MSE = ||\u03b8inferred \u2212 \u03b8ground-truth||22 (12)\nIn the main text of the paper, we focused on the normalized regret metric, but as we show in the Appendix, our observations correlate well to the reward MSE in general.\nError Bars All error bars and ribbons corresponded to the standard error of the mean (SEM). The simulation results have the average and standard error of the mean computed over 10 random reward functions and 2 seeds. The user study results have the average and standard error of the mean computed over the 35 trials performed over 7 subjects."
        },
        {
            "heading": "B.3 Hyperparameters",
            "text": "For all simulated experiments, we used the same hyper-parameter settings. All trajectories took place on a 10-by-10 Grid-world with a 4-dimensional reward vector, all lying on the unit sphere. There was a 0.1 transition noise present in the environment. For the purposes of maintaining distributions over reward functions, we discretized this unit sphere by a grid of 1000 points. All experiments were averaged over 10 random seeds.\nC Reward Inference for Boltzmann Rational Human Feedback In the main text of the paper, we measured the performance of reward inference using the normalized reward regret. However, another metric we can measure reward inference performance with is the mean-squared error (MSE) when considering the reward functions as four-dimensional vectors. In Figure 7, we provide the results of beta-fitting using simulated Boltzmannrational feedback using this MSE metric. We observe that the mean-squared error metric follows the same general trends as the regret seen in the main text. One important exception, however, is that in the case where we underestimate \u03b2, the MSE results show default having a higher MSE, whereas the regret continues to perform comparably to the fitted and oracle methods. We attribute this to the fact that shifting a reward function by a constant can result in the same eventual behavior, but a reward function whose entries are very different from the ground-truth.\nWe also tested the accuracy of the beta fitting procedure across the different feedback types and show the results in Table 2. These results demonstrate that our beta-fitting procedure is highly accurate, and this accuracy explains the benefits seen by fitting beta in our paper.\nD Reward Inference for Simulated Biased Human Feedback In the main text of the paper, we have explored the performance of reward inference on simulated-biased demonstrations. Here, we examine the case of a simulated bias for comparisons and e-stops. In the discussion that follows, it is important to observe that different feedback types are varyingly impacted by different biases. One illustrative case of this is biases which affect the perception of the transition dynamics. Such biases would clearly be expected to impact demonstrations because demonstrations rely upon planning a trajectory through the environment. On the other hand, when confronted with the task of comparing two trajectories having already taken place, transition dynamics play no role in computing the Boltzmann rational choice. Notably, this observation plays an important role in motivating the importance of modelling irrationality. If it is possible for a robot to identify that a human\u2019s bias has no impact on a particular feedback type, it has the opportunity to select for that feedback type. In this case, we specifically analyze the case of myopia bias and we leave further analysis of the systematic biases affecting other feedback types to future work.\nMultiple Biases We tested the effect of fitting \u03b2 when there are multiple biases simultaneously for Demonstrations. In particular, we examined the case of a simultaneous myopia and extremal bias with parameters (0.5, 0.5). We found that found that Default achieved regret 0.37\u00b10.12, Fitted achieved 0.11\u00b10.08, and Oracle achieved 0.05\u00b10.10."
        },
        {
            "heading": "D.1 Comparisons",
            "text": "Recall that the observation model for Boltzmann-rational comparisons as shown in Equation 2, involves the quantity r(\u03be) =\u2211\n(s,a)\u2208\u03be r(s, a), the cumulative reward achieved during the trajectory. In order to simulate Myopia bias for comparisons, we introduce a discount factor \u03b3 into this cumulative reward calculation and simulate the comparison choices to be Boltzmannrational with respect to the discounted cumulative rewards. In Figure 8 (a), we display the impact of \u03b2-fitting for these biased comparisons. We find that there exists a narrower range of \u03b3 under which the reward can be recovered well even by the oracle method. Nevertheless, on this range we find the \u03b2-fitting results in substantial improvements to the reward inference."
        },
        {
            "heading": "D.2 E-Stops",
            "text": "In order to simulate myopic e-stops, we multiplied a discount factor to the accumulated reward used in the observation model for Boltzmann-rational E-Stops. In Figure 8 (b), we examine the impact of \u03b2-fitting on myopic e-stops, finding that \u03b2-fitting provides a slight, but not significant improvement over the default method.\nE Information Gain Crossover Example Here is a simple environment determined by a few parameters where different settings of the parameters change whether demonstrations or comparisons are more informative, lending some insight into aspects of the environment which favor different feedback types. The set of possible reward parameters is \u0398 = {\u03b8+1 , \u03b8 \u2212 1 , . . . , \u03b8 + N , \u03b8\n\u2212 N}, the set of possible choices is C =\u22c3N\ni=1{c + i , c \u2212 i , c p1 i , . . . , c pK i , c n1 i , . . . , c nK i }, and the rewards of each choice depending on the value of the reward parameter are\nr(csi , \u03b8 s\u2032 j ) =  R1 i = j, s = s \u2032 \u2212R1 i = j, (s = +, s\u2032 = \u2212) or (s = \u2212, s\u2032 = +) R2 i = j, (s = p`, s \u2032 = +) or (s = n`, s\u2032 = \u2212) for some ` \u2212R2 i = j, (s = p`, s\u2032 = \u2212) or (s = n`, s\u2032 = +) for some `\nR3 i 6= j. We will set R1 > R2 > R3 > 0. Intuitively, the reward has N different \u201ddirections,\u201d and along a direction i there are two options \u03b8+i and \u03b8 \u2212 i . Supposing that one of \u03b8 + i or \u03b8 \u2212 i are the true reward parameter, then there are 2(K + 1) choices which are \u201dsensitive\u201d to whether the reward is \u03b8+i versus \u03b8 \u2212 i , while the remaining (N \u2212 1)2(K + 1) choices all have equal value either way. Among the 2(K + 1) choices which are sensitive to \u03b8+i versus \u03b8 \u2212 i , there are two extreme options c + i , c \u2212 i which give large reward R1 if they \u201dmatch\u201d the reward parameter, and very large cost \u2212R1 if they do not match. There are also a larger number 2K of \u201dconservative\u201d options which give smaller reward R2 if they match the reward parameter but also less severe cost \u2212R2 if they do not match the reward parameter.\nNow we analyze the expected information gains from human demonstrations and comparisons in this environment. Demonstrations will allow the human to make any choice, while comparisons will present the human with only two choices. We will only analyze comparisons of the form c+i vs c \u2212 i (for some i), but this is hardly a limitation because the resulting information gain provides a lower bound on the information gain of the best comparison. We assume a uniform prior over reward parameters in \u0398, and for computational convenience we note that maximizing expected information gain is equivalent to minimizing expected posterior entropy, so we can compute expected posterior entropies for demonstrations and comparisons.\nFirst we compute the expected posterior entropy of a demonstration. After observing a choice csi , the posterior mass on \u03b8 s\u2032 j is\nP (\u03b8s \u2032 j | csi ) \u221d P (csi | \u03b8s \u2032 j )\n\u221d  exp\u03b2R1 C1 exp\u2212\u03b2R1 C2 exp\u03b2R2 C3 exp\u2212\u03b2R2 C4 exp\u03b2R3 i 6= j.\nC1 : i = j, s = s \u2032 C2 : i = j, (s = +, s \u2032 = \u2212) or (s = \u2212, s\u2032 = +) C3 : i = j, (s = p`, s \u2032 = +) or (s = n`, s\u2032 = \u2212) for some ` C4 : i = j, (s = p`, s \u2032 = \u2212) or (s = n`, s\u2032 = +) for some `\nFor 2(N \u2212 1) values of \u03b8s\u2032j , we will be in the bottom case, and there will only be two values of \u03b8s \u2032\nj for which we are in any of the top four cases. If csi was an extreme choice c + i or c \u2212 i then we will reach the top two cases each once, otherwise we will reach cases three and four each once. Therefore if one of the extreme options c+i or c \u2212 i was taken, using H(p1, . . . , pk) to denote the entropy of a distribution proportional to (p1, . . . , pk), we will obtain posterior entropy\nH ( exp(\u03b2R1), exp(\u2212\u03b2R1), exp(\u03b2R3), . . . , exp(\u03b2R3)\ufe38 \ufe37\ufe37 \ufe38\n2(N\u22121)\n) .\nIf csi was a conservative choice in c p1 i , . . . , c pK i , c n1 i , . . . , c nK i then the posterior entropy will be H ( exp(\u03b2R2), exp(\u2212\u03b2R2), exp(\u03b2R3), . . . , exp(\u03b2R3)\ufe38 \ufe37\ufe37 \ufe38\n2(N\u22121)\n) .\nThe probability of the human choosing an extreme choice is proportional to exp(\u03b2R1) + exp(\u2212\u03b2R1) + 2(N \u2212 1) exp(\u03b2R3) and the probability of choosing a conservative choice is proportional toK exp(\u03b2R2)+K exp(\u2212\u03b2R2)+2K(N\u22121) exp(\u03b2R3). (Adding these two quantities provides the normalizing constant.) These calculations combine to yield the expected entropy of the human posterior after providing a demonstration.\nNow we compute the expected posterior entropy after a comparison. Again, instead of computing the expected posterior entropies of all comparisons and using the best, we only consider comparisons of the form c+i vs c \u2212 i , where i is fixed. WLOG we can assume that c+i is chosen (since the posterior in the case that c \u2212 i is chosen is the same up to permutation). The posterior mass on \u03b8sj is\nP (\u03b8sj | c+i ) \u221d P (c + i | \u03b8 s j )\n\u221d  exp(\u03b2R1)/Z i = j, s = +\nexp(\u2212\u03b2R1)/Z i = j, s = \u2212 1/2 i 6= j\nwhere we define Z = exp(\u03b2R1) + exp(\u2212\u03b2R1). Therefore the posterior entropy is H ( exp(\u03b2R1)/Z, exp(\u2212\u03b2R1)/Z, 1/2, . . . , 1/2\ufe38 \ufe37\ufe37 \ufe38\n2(N\u22121)\n) .\nWe now provide more intuition into these calculations. When an \u201dextreme\u201d choice is taken by the human for a demonstration, we obtain the posterior with the lowest entropy. However, as K increases, it becomes more likely that the human will instead take one of the (less rewarding but more numerous) \u201dconservative\u201d demonstration choices, which result in a worse posterior. This provides an opportunity for comparisons to have a lower expected posterior entropy since the comparison is between some two extreme options c+i and c \u2212 i , thus eliminating the conservative and less informative choices. However, for comparisons to be more informative, we need N to be sufficiently low (depending on the values of the other parameters, including \u03b2). This can be verified by examining the entropies calculated above, but an intuitive explanation is that a comparison can only present the two options c+i and c \u2212 i for one particular value of i, but the correct value of i (matching the j such that \u03b8 s j is the human\u2019s reward parameter) to ask for is not known and thus there is only a 1/N chance that the robot guesses correctly, and the rest of the time the robot must be presenting the human two options towards which the human is completely indifferent. The robot knows this and thus must temper the formed posterior. When N = 1 and K > 0, comparisons are actually strictly superior to demonstrations (for all \u03b2), since the comparison is removing the conservative options and thus forcing the human to make a more informative choice, without being penalized for possibly missing the important direction i. When N > 1, keeping all other parameters fixed, whether demonstrations or comparisons are superior may depend on \u03b2, since it depends on which effect is more harmful: high \u201ddimensionality\u201d causing a large probability of a comparison asking about an irrelevant reward direction, or large K and low \u03b2 causing a large probability of a provided demonstration making a choice which is suboptimal and thus results in a higher-entropy posterior.\nF Exact Beta Inference"
        },
        {
            "heading": "F.1 Demonstrations",
            "text": "When there is access given to the underlying demonstration policy, it is possible to conduct exact inference of the human\u2019s rationality. Because we have access to the entire policy (for a given \u03b8) \u03c0 describing the simulated biased demonstrators, we can circumvent forming \u03b2 estimates by sampling from \u03c0 by instead performing an exact M-projection of \u03c0 onto the family of softoptimal policies {\u03c0\u03b2,\u03b8 : \u03b2 \u2208 (0,\u221e)} for the particular reward \u03b8:\n\u03b2\u0302 = arg min \u03b2\u2208(0,\u221e)\nDKL(\u03c0 || \u03c0\u03b2,\u03b8)\n= arg min \u03b2\u2208(0,\u221e) \u2211 \u03be \u03c0(\u03be) log ( \u03c0(\u03be) \u03c0\u03b2,\u03b8(\u03be) )\n= arg min \u03b2\u2208(0,\u221e) \u2211 \u03be \u03c0(\u03be) ( T\u22121\u2211 t=0 log ( \u03c0(at | st) \u03c0\u03b2,\u03b8(at | st) ))\nand the term inside the arg min can be evaluated efficiently because it is equivalent to policy evaluation of the policy \u03c0 on the non-stationary reward rt(s, a) = log(\n\u03c0(at|st) \u03c0\u03b2,\u03b8(at|st) ). This is an optimization problem over a single scalar variable so it is easy to\nsolve."
        },
        {
            "heading": "F.2 Comparisons and E-Stops",
            "text": "For comparisons and E-stops we take the same approach of performing exact M-projection, but in these cases this is simpler to compute due to the much smaller number of possible choices. We describe this in slightly more generality and then specialize to comparisons and E-stops.\nGiven m choices c1, . . . , cm with rewards r1,\u03b8, . . . , rm,\u03b8, we can define the \u03b2-rational policy (for a particular \u03b8) \u03c0\u03b2,\u03b8 to be the policy which makes choice ci with probability exp(\u03b2ri,\u03b8)/ \u2211m j=1 exp(\u03b2rj,\u03b8). A general human can be described by the policy \u03c0 which specifies the probabilities of taking each choice. To compute the M-projection we evaluate DKL(\u03c0 || \u03c0\u03b2,\u03b8) by simply expanding its definition. This applies to comparisons by taking m = 2 and to E-stops by having the choices as c1, . . . , cm = \u03be0:0, . . . , \u03be0,T for some trajectory \u03be. As described this only accommodates a single design (ex. one pair of trajectories to compare/one trajectory to stop along) but the extension to multiple designs is immediate since the choice taken in each design is independent and thus the KL divergence between the true model and a \u03b2-rational model for multiple choices in multiple designs is simply the sum of the individual KL-divergences for each design.\nG Exact Beta Inference Empirical Results In the main text of the paper, we presented some analysis of the generalization of \u03b2 and the reward identifiability for the myopia and optimism biases. Here we provide a similar analysis for the extremal bias. In Figure 10 (a), we show KL Divergence between the extremal-biased policy and Boltzmann-rational policies for every other candidate reward function. We find that at extremal bias levels where the KL Divergences are spread out, our fitted method performs comparably with the oracle method. On the other hand, at \u03b1 values where all reward functions have roughly the same KL-Divergence, we find that the fitted method performs worse than oracle. This gives further evidence for the concept of reward identifiability presented in the paper. Additionally, in Figure 10 (b), we presented the variance of \u03b2\u0302 over all the reward functions at at different levels of extremal bias. We observe that the magnitude of \u03b2 variance is uniformly low. However, we observe the slightly counter-intuitive result that at the parameter values where the \u03b2 variance is lowest, our method does not perform as well as Oracle, showing that the generalization ability of \u03b2 across different reward functions is insufficient to explain the success or failure of \u03b2 fitting.\nH Impact of Assumed \u03b2 on Learning From a Biased Human\nIn Figure 11, we examine the effect of assuming different default \u03b2 values when learning from myopic demonstrations. In one case, we decreased the assumed \u03b2 to 0.5 and found that this caused the default method to perform more comparably to fitted and oracle. In another case, we increased the assumed beta to 5. We found that this caused the default method to perform even more poorly relative to fitted and oracle. These results give further evidence for the theory presented in Section 3: Reducing the assumed \u03b2 causes default to overestimate less, leading to improved reward inference. On the other hand, when we increase the assumed \u03b2, default overestimates the true \u03b2, leading to worse reward inference.\nI Additional Active Learning Ablations In addition to the active learning setting provided in the main text, we also analyzed the case where (a) the default \u03b2 overestimates the all true rationalities and (b) the relative rationalities of different feedback types are changed. In order to conduct this study, we set the assumed \u03b2=10 and the true \u03b2demo = 1, \u03b2comp = 0.5, \u03b2estop = 0.1. The results are shown in Figure 10. In\nthis case, we find that having the correct \u03b2 for reward inference plays a larger role in determining the performance of the active reward inference. We attribute this to the fact that when the default rationality is used for reward inference, demonstrations are primarily still selected. However, the \u03b2 = 10 in reward inference severely overestimates the true rationality which causes reward inference to be poor. In Figure 11 (b), we consider the case where the default over-estimates all feedback rationalities and the most informative feedback type is comparisons. In this case, having the correct active learning beta is very important, as this is necessary to correctly choose comparisons queries. However, we see that not having the correct reward inference beta also leads larger to reductions in performance than seen in Figure 4(b). This can be explained by the fact that using the default reward inference rationality in this case causes an overestimation of the feedback rationality, which as shown by our theory can be harmful.\nJ User Study Details The user study was performed according to IRB approval and consent was obtained by having the participant review and ask questions about a consent form prior to the study taking place. All data collected from each participant was anonymized. Participants were compensated for their time at a rate of 20 dollars an hour. The user study was designed to take approximately 45 minutes and participants received no less than 15 dollars regardless of the amount of time it took them to complete the study. In total, $105 was spent on participant compensation.\nIn our user study, we presented 5 calibration rewards to each user (using the interface shown below) and asked them to provide 10 instances of feedback per reward (5 comparisons and 5 demonstrations). In Figure 13, we provide examples of the feedback collection interfaces for each feedback type. For both feedback types, the reward function was displayed in the top-left hand corner, as a mapping between tile colors and the rewards earned for stepping on the relevant tiles. In addition, there was a completion reward present in the environment of 250 reward and users were advised of this during the instructions phase of the study. The completion reward was chosen to ensure that the optimal path ended in the goal state, while also ensuring that the shortest path from start to goal was not always optimal.\nDemonstration Interface Figure 13 (a) shows the demonstration collection interface. In addition to the reward function, participants were provided with real-time access to the current reward score achieved by the demonstration, as well as the current time step. The agent\u2019s position was depicted by the black circle and the participant was able to control its movement by using the arrow keys. Participants experienced time pressure when giving the demonstration, as the interface ran at 6 timesteps per second and failure to provide control input at a given time-step would result in the agent continuing to slide in its last direction of movement. The participant was given unlimited time to review the reward function and starting state prior to beginning the demonstration and would press the A key to start the demonstration.\nComparisons Interface The participant was presented with two trajectories and asked to compare which was superior under the given reward function. To assist them in making their decision, they were provided with environment maps depicting a trace of the agent\u2019s movement. Moreover, participants were provided with time-step counts of how long the agent spent in each color tile. Participants were allowed unlimited time and could indicate their selection by pressing 1 to select the left trajectory and 2 for the right.\nPrior to beginning, participants were provided with the following instructions: In this user study, you will be providing two different feedback types to help an artificial intelligence agent infer a reward function. You will be providing demonstrations and comparisons. You will be providing feedback in a 10x10 gridworld environment, where the tiles have four possible colors. You will be controlling the orange circle when you give your demonstrations. Each reward function will be a mapping between tile colors and a reward you get for every time step you spend on the tile color. The environment will have a horizon length of 25. This means that the maximum number of steps in a trajectory is 25.\nThe black square in the bottom right corner of the environment is a goal state. Provided you reach the goal state by the end of the horizon, you will receive +250 reward. If you arrive at the goal before the horizon ends, you will receive +250 reward at the point you reach the goal and +0 reward for all future time steps. You will provide demonstrations using an interface that looks like the above. You will control the movement of the agent using the arrow keys (up key moves up, down key moves down, left key moves left, right key moves right). You will experience time pressure when giving demonstrations. The interface will run at 6 time-steps per second and if you don\u2019t provide input at a given step, the agent will slide in the direction it was last moving. Score: The score field on the left-hand side will tell you the current score you have accumulated Step: The step field on the right-hand side will tell you how many time-steps have elapsed since the beginning of the episode.\nYou are encouraged to review the reward function carefully before starting the demonstration. When you are ready to begin, you should press the \u201ca\u201d key and then begin controlling the robot. You will also be providing preference comparisons between two trajectories that the robot itself has generated.A trace of the trajectory is shown on the screen. We have also provided information about how long the robot spends on each color to help you decide which trajectory better represents the reward.\nFollowing the presentation of the instructions, the participant was given unlimited practice attempts on each feedback type. They were also given a chance to ask any questions they ha about the interface or the strategy for giving the feedback. After this, the participants began the formal study.\nParticipants first provided 25 demonstrations, followed by 25 comparisons. The feedback of each type was provided in 5 blocks of 5, where each block consisted consisted of the same reward function. In between blocks, the interface alerted the participants to the fact that the reward function had changed. Since participants had unlimited time to review the reward function before providing a demonstration or indicating a comparison, they were able to pace themselves and take breaks as necessary.\nWe used a hold-one-out method for analyzing the user study data. In this setting, we would test reward inference on each reward function by fitting beta on the data provided for the other 4 reward functions and then testing the reward inference on the held-out reward function. We completed this for each reward function, resulting in 5 trials per user.\nStatistical Tests We also conducted a two-tailed paired T-Test between the fitted and default reward inference results computed in the User Study. The results are shown in Table 3 and demonstrate that, while there was a statistically significant difference for both active learning and demonstrations, we did not observe a statistically significant difference between the methods for comparisons.\nK Car Driving Simulator In addition to our results in the gridworld navigation environment, we tested the importance of fitting \u03b2 on a car driving simulator environment we developed. In our simulator, the agent car drives along a three-lane road in the presence of other vehicles. The reward features considered are a car crash indicator, in addition to a one-hot encoded indicator of the cars lane (allowing for a lane preference in the reward function). In Figure 14, we show that learning a reward function from biased feedback can be improved by modeling rationality in the way we propose.\nL Compute Resources Used The majority of experiments, as well as the user study for this paper were run on a Mid-2015 MacBook-Pro with an Intel Quad-Core i7 Processor. Certain experiments were run on a lab Intel 8-core i7 Processor."
        }
    ],
    "title": "The Effect of Modeling Human Rationality Level on Learning Rewards from Multiple Feedback Types",
    "year": 2023
}