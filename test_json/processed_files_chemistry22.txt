The peer-review process is currently under stress due to the increasingly large number of submissions to top-tier venues, especially in Artificial Intelligence (AI) and Machine Learning (ML). Consequently, the quality of peer reviews is under question, and dissatisfaction among authors is not uncommon but rather prominent. In this work, we propose \"ReviVal\" (expanded as \"REVIew eVALuation\"), a system to automatically grade a peer-review report for its informativeness. We define review informativeness in terms of its Exhaustiveness and Strength, where Exhaustiveness signifies how exhaustively the review covers the different sections and qualitative aspects1 of the paper and Strength signifies how sure the reviewer is of their evaluation. We train ReviVal, a multitask deep network for review informativeness prediction on the publicly available peer reviews, which we curate from the openreview2 platform. We annotate the review sentence(s) with labels for (a) which sections and (b) what quality aspects of the paper those refer. We automatically annotate our data with the reviewer\u2019s sentiment intensity to capture the reviewer\u2019s conviction. Our approach significantly outperforms several intuitive baselines for this novel task. To the best of our knowledge, our work is a first-of-its-kind to automatically estimate the informativeness of a peer review report.

Peer review is the central system of scientific research validation. However, it is not undebated that the system is without flaws. Several studies highlight the problems like bias [38], inconsistencies [28, 37], arbitrariness [9] of the process, thus, degrading the integrity and trust on the central system of research validation. Hence, it is not uncommon that because of these issues, sometimes good research gets ignored, and sub-par papers find a place. One such unfortunate example is the retractions of referred COVID19 articles [30]. Area chairs of conferences or editors of journals are responsible for mitigating these issues by assigning papers to relevant, expert reviewers and then evaluating those reviewers\u2019 comments to make informed decisions. However, the exponential rise in paper submissions has put the peer-review system under severe stress in recent years, leading to a dearth of experienced reviewers. Sometimes, the chairs or editors are left with no other option than to delegate this critical yet voluntary job to inexperienced reviewers to the extent of even to graduate students3. Many reviewers evaluate the submitted research based on some poor indicators, e.g., whether the work is SOTA or not [35]. Fortunately, the Natural Language Processing (NLP) community has proactively paid attention to these problems. They have been arguing for changes to the current reviewing practices (e.g., the introduction of ACL Rolling Review to enforce good reviewing practises 4, EMNLP 2020/2023 releasing strict rubrics for research evaluation [12]), organizing tutorials [11] for researchers to write high-quality reviews [33]. However, peer review suffers another major problem: reviewers\u2019 tendency to invest less time in this critical voluntary job [17]. Sometimes, the reviewers themselves are unsure of their judgment on the merit of the work, which becomes evident from their poor reviews [8] [29]. Thus, these reform programs, which are primarily targeted\n3https://www.the-scientist.com/critic-at-large/opinion--exorcising-ghostwritingfrom-peer-review-66940 4https://www.aclweb.org/adminwiki/index.php?title=ACL_Rolling_Review_ Proposal\ntowards improving the reviewing practices of human reviewers, might not be sufficient.\nAn important direction towards re-establishing trust in the peerreview process should be an attempt to establish confidence in the peer-review reports. Therefore, while promoting good quality and constructive reviews, a means to detect and penalize inferior reviews should also exist. Given the ever-increasing submission load and the importance of this problem, an automated system to judge reviews for quality would largely help. In this work, we propose a mechanism to detect the surface-level indicators of good and bad reviews. Our approach takes a review and grades it for informativeness. Defining peer-review quality is not straightforward [36]; it will vary across domains. In this work, we assert that: a good review should comment on the critical sections (e.g., Problem Statement, Methodology, Experiments, Results/Analysis, etc.) and address the critical aspects of the paper (e.g., Novelty, Theoretical Soundness, etc.) while clearly bringing out the reviewer\u2019s stand on the work. As we mostly fixate our attention towards estimating the inclusiveness of the review (in terms of section, aspect, sentiment), instead of quality, we instead call our investigation a means to determine the informativeness of the review. Here, our contributions are two-fold:\n\u2022 We define informativeness via two components: Exhaustiveness and Strength. Our deep neural architecture takes as input the full-paper text and the full review text, and we train in a multi-task setting. We employ reviewer confidence score prediction and review recommendation score as scaffolds for Exhaustiveness and Strength, respectively. \u2022 We propose a scoring mechanism to manifest exhaustiveness of a review via its section and aspect coverage of the paper.
2 RELATEDWORK
Although we did not find any prior work addressing the exact task, we do find literature on AI-support for the peer review system [15, 19, 32, 34]. Additionally, [36] enlists some major points to address in a peer review. There have been numerous discussions on the implications of AI in the peer review pipeline. Authors [39] study the effect of revealing the identity of the reviewer in the peer review process and if it can enhance the quality of peer reviews whereas [18] study the effect of blind review model. However, all these are studies on different peer review models to improve review quality. In another relevant work [44], authors predicts the helpfulness of a peer-review using manual features. Their objective is somewhat similar to ours in term of Exhaustive component.\nOur work in this direction is in line with the recent efforts to incorporate Artificial Intelligence (AI) in the peer review pipeline [13, 15, 32, 34] till generation of decision-awaremeta-reviews [6, 22\u2013 24, 27]. In our earlier work, we try to model the harshness [41], constructiveness [3], informativeness [2], uncertainty [14] of peer reviews and confidence of reviewers [5] from a computational perspective. We also investigated aspect extraction [26, 42] and decision/recommendation score prediction [4, 7, 25] from peer reviews, which served as subtasks in our current work.\nPublishers and allied stakeholders are already considering AIassistants in the peer-review cycle [16]. An AI that could grade the reviews based on some quality standards would help regulating\na thrust on poor reviews. It would also help human reviewers to take up their job seriously. Editors would be able to discard trivially written reviews and ask emergency reviewers to step in. It could also be a step towards building a reviewer profile data of their review quality, thereby increasing trust in reviewers.
3 PEER REVIEW INFORMATIVENESS
As we discussed earlier, quality of a review is a somewhat subjective term and may encompass many other aspects. However, our definition of informativeness builds upon the expectations from a good review in general AI/ML venues, which are probably similar across venues from other domains. As per the rubrics defined in [36], we expect a good review should demonstrate the reviewer\u2019s understanding of the work with a concise summary, an evaluation of the writing quality (clarity), comments on the novelty of the proposal, and a fair evaluation of the experiments/results, and all the other major sections of the paper. To this end, we define review informativeness in terms of its Exhaustiveness and implicit Strength. We view Exhaustiveness as to how detailed the review is, and Strength as how opinionated it is in reflecting the reviewer\u2019s perspective. Eventually, we want to distinguish reviews written by experienced reviewers vs. those written by novice/non-expert/slack reviewers. A good review should be detailed and also high on the opinion. It should be comprehensive enough to discuss the merit of the work based on aspects like Impact, Empirical Soundness, etc., and opinionated enough to bring out the reviewer\u2019s stand on the work. However, the review\u2019s exhaustiveness and opinionatedness can textually reflect in varied proportions across different reviews. For example, when the work is excellent, we do not expect to see many comments about the work in general, but the opinion will be strongly positive. Our objective is to detect non-informative reviews, which are unfortunately very common in academia. To this end, our definition of Exhaustiveness has two components. Review Exhaustiveness component serves as a simple heuristic for whether the reviewer has read/understood the paper. If the reviewer has understood the paper, we assert that the review would be more detailed with comments on several parts of the paper, be it the problem statement, the proposed methodology, or the proposed results etc. This hypothesis works against the kind of reviews illustrated in R1 below.\nR1: This paper analyzes the expressiveness and loss surface of deep CNN. I think the paper is clearly written, and has some interesting insights.\nICLR 2018, id=BJjquybCW\nAnother essential role of the review is to assess the impact of the work in a broader academic context. Reviewers assess the proposed work for its novelty, soundness of the research methodology, etc. Aspect Exhaustiveness score puts thrust on this critical component. A review should have information that not only discusses the research but also validates it. Hence, we split the Exhaustiveness of the review into two components: Section Exhaustiveness and Aspect Exhaustiveness. We define the third component, Review Strength, as a measure of the degree of review opinionatedness and hence\nquantify the intensity of the review. With intensity, we do not emphasize the review\u2019s positive or negative sentiment but how strong a particular sentiment is.
3.1 Section Exhaustiveness
Wedefine set= {Abstract(ABS), Introduction(INT), RelatedWorks(RWK), ProblemDefinition/Idea(PDI), Data/Datasets(DAT), Methodology(MET), Experiments(EXP), Results(RES), Tables and Figures(TNF), Analysis(ANA), Future Work(FWK), Overall(OAL), Bibliography(BIB), External(EXT)}. Here, we consider the general components in a typical AI/ML paper with obvious exceptions. Considering , we score section exhaustiveness of review  (1) (2)\nWe consider relative importance in our scoring function to give more importance to certain sections, like Methodology, Experiments, etc., discussed in almost all the reviews. This is intuitive as ICLR is an empirical venue; it is imperative for the reviewers to talk about methodology, experiments, related work for comparison, etc. Hence, we agree that the proposed scoring depends on the venue (whether theoretical or empirical)."
3.2 Aspect Exhaustiveness
For Aspect Exhaustiveness, we define 46 = {Appropriateness (APR), Originality/Novelty (NOV), Significance/Impact (IMP), Meaningful Comparison (CMP), Presentation and Formatting (PNF), Recommendation (REC), Empirical/Theoretical Soundness (EMP), Substance (SUB), Clarity(CLA)} as the set containing this reviewer subjectivity aspects. We again score a review r for aspect exhaustiveness as: (3)\nSymbols carry the same meaning as before.3.3 
Review Strength






