{
    "abstractText": "Quantum state tomography (QST) aims at estimating a quantum state from averaged quantum measurements made on copies of the state. Most quantum algorithms rely on QST at some point and it is a well explored topic in the literature, mostly for mixed states. In this paper we focus on the QST of a pure quantum state using parallel unentangled measurements. Pure states are a small but useful subset of all quantum states, their tomography requires fewer measurements and is essentially a phase recovery problem. Parallel unentangled measurements are easy to implement in practice because they allow the user to measure each qubit individually. We propose two sets of quantum measurements that one can make on a pure state as well as the algorithms that use the measurements outcomes in order to identify the state. We also discuss how those estimates can be fined tuned by finding the state that maximizes the likelihood of the measurements with different variants of the likelihood. The performances of the proposed three types of QST methods are validated by means of detailed numerical tests.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yannick Deville"
        }
    ],
    "id": "SP:61421f5e92ac819acd3d4b41b7789e7a12cb276b",
    "references": [
        {
            "authors": [
                "M.A. Nielsen",
                "I.L. Chuang"
            ],
            "title": "Quantum computation and quantum information",
            "year": 2000
        },
        {
            "authors": [
                "A. Kalev",
                "R.L. Kosut",
                "I.H. Deutsch"
            ],
            "title": "npj Quantum Information 1",
            "venue": "10.1038/npjqi.2015.18 ",
            "year": 2015
        },
        {
            "authors": [
                "A. Smith",
                "C.A. Riof\u0155\u0131o",
                "B.E. Anderson",
                "H. Sosa- Martinez",
                "I.H. Deutsch",
                "P.S. Jessen"
            ],
            "title": "Physical Review A 87",
            "venue": "10.1103/physreva.87.030102 ",
            "year": 2013
        },
        {
            "authors": [
                "D. Gross",
                "Y.-K. Liu",
                "S.T. Flammia",
                "S. Becker",
                "J. Eisert"
            ],
            "title": "Physical Review Letters 105",
            "venue": "10.1103/physrevlett.105.150401 ",
            "year": 2010
        },
        {
            "authors": [
                "X. Ma",
                "T. Jackson",
                "H. Zhou",
                "J. Chen",
                "D. Lu"
            ],
            "title": "M",
            "venue": "D. 18 Mazurek, K. A. G. Fisher, X. Peng, D. Kribs, K. J. Resch, Z. Ji, B. Zeng, and R. Laflamme, Physical Review A 93, 10.1103/physreva.93.032140 ",
            "year": 2016
        },
        {
            "authors": [
                "T. Cai",
                "D. Kim",
                "Y. Wang",
                "M. Yuan",
                "H.H. Zhou"
            ],
            "title": "The Annals of Statistics 44",
            "venue": "10.1214/15-aos1382 ",
            "year": 2016
        },
        {
            "authors": [
                "Y. Wang"
            ],
            "title": "The Annals of Statistics 41",
            "venue": "10.1214/13-aos1156 ",
            "year": 2013
        },
        {
            "authors": [
                "C.H. Baldwin",
                "I.H. Deutsch",
                "A. Kalev"
            ],
            "title": "Physical Review A 93",
            "venue": "10.1103/physreva.93.052105 ",
            "year": 2016
        },
        {
            "authors": [
                "C. Ferrie"
            ],
            "title": "Physical Review Letters 113",
            "venue": "10.1103/physrevlett.113.190404 ",
            "year": 2014
        },
        {
            "authors": [
                "R.J. Chapman",
                "C. Ferrie",
                "A. Peruzzo"
            ],
            "title": "Physical Review Letters 117",
            "venue": "10.1103/physrevlett.117.040402 ",
            "year": 2016
        },
        {
            "authors": [
                "S.T. Ahmad",
                "A. Farooq",
                "H. Shin"
            ],
            "title": "Scientific Reports 12",
            "venue": "10.1038/s41598-022-09143-7 ",
            "year": 2022
        },
        {
            "authors": [
                "J. Cotler",
                "F. Wilczek"
            ],
            "title": "Physical Review Letters 124",
            "venue": "10.1103/physrevlett.124.100401 ",
            "year": 2020
        },
        {
            "authors": [
                "J. Finkelstein"
            ],
            "title": "Physical Review A 70",
            "venue": "10.1103/physreva.70.052107 ",
            "year": 2004
        },
        {
            "authors": [
                "T. Heinosaari",
                "L. Mazzarella",
                "M.M. Wolf"
            ],
            "title": "Communications in Mathematical Physics 318",
            "venue": "355 ",
            "year": 2013
        },
        {
            "authors": [
                "D. Goyeneche",
                "G. Ca\u00f1as",
                "S. Etcheverry",
                "E. G\u00f3mez",
                "G. Xavier",
                "G. Lima",
                "A. Delgado"
            ],
            "title": "Physical Review Letters 115",
            "venue": "10.1103/physrevlett.115.090401 ",
            "year": 2015
        },
        {
            "authors": [
                "R. Balan",
                "P. Casazza",
                "D. Edidin"
            ],
            "title": "Applied and Computational Harmonic Analysis 20",
            "venue": "345 ",
            "year": 2006
        },
        {
            "authors": [
                "I. Waldspurger"
            ],
            "title": "A",
            "venue": "d\u2019Aspremont, and S. Mallat, Mathematical Programming 149, 47 ",
            "year": 2013
        },
        {
            "authors": [
                "A.S. Bandeira",
                "J. Cahill",
                "D.G. Mixon",
                "A.A. Nelson"
            ],
            "title": "Applied and Computational Harmonic Analysis 37",
            "venue": "106 ",
            "year": 2014
        },
        {
            "authors": [
                "N.Z. Shor"
            ],
            "title": "Soviet Journal of Computer and Systems Sciences 25",
            "venue": "1 ",
            "year": 1987
        },
        {
            "authors": [
                "C.G. Broyden"
            ],
            "title": "IMA Journal of Applied Mathematics 6",
            "venue": "76 ",
            "year": 1970
        },
        {
            "authors": [
                "Z. Hradil",
                "J. \u0158eh\u00e1\u010dek",
                "J. Fiur\u00e1\u0161ek"
            ],
            "title": "and M",
            "venue": "Je\u017eek, in Quantum State Estimation ",
            "year": 2004
        },
        {
            "authors": [
                "F. Verdeil",
                "Y. Deville"
            ],
            "title": "and A",
            "venue": "Deville, in 2021 IEEE Statistical Signal Processing Workshop (SSP) ",
            "year": 2021
        },
        {
            "authors": [
                "R. Blume-Kohout"
            ],
            "title": "Physical Review Letters 105",
            "venue": "200504 ",
            "year": 2010
        }
    ],
    "sections": [
        {
            "text": "ar X\niv :2\n20 8.\n04 01\n3v 1\n[ qu\nan t-\nph ]\n8 A\nug 2\n02 2\nPure State Tomography with Parallel Unentangled Measurements\nFranc\u0327ois Verdeil and Yannick Deville Universite\u0301 de Toulouse, UPS, CNRS, CNES, OMP, IRAP, Toulouse, France\n(Dated: August 9, 2022)\nQuantum state tomography (QST) aims at estimating a quantum state from averaged quantum measurements made on copies of the state. Most quantum algorithms rely on QST at some point and it is a well explored topic in the literature, mostly for mixed states. In this paper we focus on the QST of a pure quantum state using parallel unentangled measurements. Pure states are a small but useful subset of all quantum states, their tomography requires fewer measurements and is essentially a phase recovery problem. Parallel unentangled measurements are easy to implement in practice because they allow the user to measure each qubit individually. We propose two sets of quantum measurements that one can make on a pure state as well as the algorithms that use the measurements outcomes in order to identify the state. We also discuss how those estimates can be fined tuned by finding the state that maximizes the likelihood of the measurements with different variants of the likelihood. The performances of the proposed three types of QST methods are validated by means of detailed numerical tests."
        },
        {
            "heading": "I. PRIOR WORK AND PROBLEM STATEMENT",
            "text": "Quantum state tomography (QST) aims at estimating a quantum state from averaged quantum measurements made on copies of the state. It often is a necessary step in quantum computation [1], it has been extensively studied for mixed states. The most basic version is detailed in [1] at the beginning of Section 8.4.2, it uses measurements defined by Pauli operators, often called Pauli measurements ([2], [3], [4], [5], [6] and [7]). This version is simple and very robust but requires computing the averages of 4nqb\u22121 different types of 2-outcome measurements where nqb is the number of qubits of the state. This scales really badly with the number of qubits but requiring so many types of measurements is not surprising because an arbitrary state is represented by a d \u00d7 d Hermitian density matrix with 4nqb real parameters (where d = 2nqb is the dimension of the Hilbert space in which the considered state evolves). In order to perform QST with fewer types of measurements, one can focus on a subset of all states. The most popular assumption is that the density matrix \u03c1 representing the state has a low rank. [4] introduced a compressed sensing approach that requires the averages ofO(rd log(d)2) 2-outcome measurements to estimate the state where r is the rank of \u03c1. [3], [2], later built upon this idea of QST via compressed sensing. More recently bounded rank QST was introduced [8]. It assumes that the rank r is known and allows the explicit reconstruction of \u03c1 using predetermined measurements (contrary to the compressed sensing approach of [4] that does not specify the measurements to be used and finds \u03c1 by minimizing the nuclear norm of \u03c1 under constraints).\nOther approaches do not make any assumption on \u03c1. In 2014 Self-Guided Quantum Tomography (SGQT) was introduced [9] and further studied in [10], [11]. It makes no assumption on \u03c1, and the number of measurements scales reasonably with the number of qubits. The drawback of SGQT is that the measurements that need to be performed on the state are not known beforehand and are generally entangled measurements. Entangled measurements correspond to multiqubit operators that cannot be expressed as a tensor product of single-qubit measurement operators i.e. they cannot be performed by measuring each qubit independently. In 2020 [12] introduced a method to partially identify large quantum systems (more than 100 qubits) with entangled states, for which the total state cannot even be stored on a classical computer. It relies on unentangled measurements which are easier to perform than entangled measurements in practice.\nThe present paper focuses on the tomography of pure states using unentangled measurements. This has been studied in [5] which tried to find the minimal number of Pauli measurements for 2 and 3 qubits (Pauli measurements are unentangled). Our addition to that article is that we will address the generic case with any nqb. Furthermore, we will use parallel measurements like in [12] where it is shown that all 4nqb averaged Pauli measurements can be computed from the averages of 3nqb parallel unentangled measurements. A parallel measurement has d outcomes and provides more information on the system than a Pauli measurement that only has two outcomes.\nIn [13] Finkelstein describes a setup able to distinguish almost all pure states, with only nprob = 2d probabilities. [13] does not beat the lower bound of [14], detailled below, because there is a negligi-\n2 ble (zero measure) set of pure states that the setup of [13] cannot recover up to a global phase, it is called the failure set. In addition to the failure set, the main problem of [13] is that the measurements are not practical, they are entangled and cannot be performed in parallel (as the matrix A associated with the measurements cannot be written as the vertical concatenation of unitary matrices). In [15] Goyeneche et al. introduced a set of nprob = 4d probabilities that also has a negligible failure set. Technically [15] introduces 5 measurements that yield 5d probabilities (obtained from averaging the results of 5 different kinds of d-outcome measurements), but only the 4 measurements defined by its Equation (2) are needed to achieve QST. The measurements of [15] are more realistic as they are performed on 4 orthonormal bases. Two of them are unentangled but the other two are entangled. Goyeneche et al. acknowledge that this is a problem and point out the fact that the two entangled bases can be mapped into the two unentangled ones by applying the quantum Fourier transform twice. In practice this would introduce additional errors, as there are no error-free circuits able to perform the quantum Fourier transform, and one would need to perform quantum process tomography (which generally relies on QST) in order to quantify the errors and improve the Fouriertransform circuit. This is a common issue with entangled measurements, the easiest way to perform them with the current version of quantum computers is to transform them into measurements in an unentangled basis, by means of a corresponding quantum gate.\nThe applied mathematics community also dealt with an equivent version of the QST problem for pure states: The phase retrieval problem (see [14], [16], [17], [18], [13]). A pure state |\u03d5\u3009 of an nqbqubit system is represented by a complex unit-norm vector v with d elements. Pure state tomography aims at estimating v from measurements. The theoretical probabilities of all outcomes of the considered types of measurements are contained in the vector |Av|2 where A is an nprob \u00d7 d matrix (nprob is the total number of probabilities) determined by the types of measurements performed and |.|2 stands for component-wise squared modulus. Recovering v (up to a global phase) from |Av|2 (generally it is |Av| instead of |Av|2 but both problems are essentially the same) is called phase retrieval. The first question asked in phase recovery is injectivity: how can one choose A in order to make sure that |Av|2 contains enough information to recover v up to a global phase? Proving that a given A guarantees injectivity is a difficult question. [14] gave a minimal number of measurements below which injectivity is impossible.\nIn our case this condition is nprob > 4d\u2212 3\u2212 c(d)nqb rows for some c(d) \u2208 [1, 2]. [16] showed that for a generic A, having 4d\u2212 2 rows or more is a sufficient condition for injectivity.\nBeyond injectivity, finding a solution to the phase recovery problem (whether it is unique up to a global phase or not) is the main difficulty of pure state tomography. Both [13] and [15] give their own closedform algorithms to recover the phases which are adapted to their versions of A. [17] focuses on this particular problem with a generic A.\nOur contributions in the present paper are as follows. Section II describes the quantum state to be identified and the measurements made. In particular, we formalize the definition of a parallel unentangled measurement.\nSection III describes a method to achieve QST with nprob = 4d using an optimization algorithm of [17] on a number of probabilities consistent with the lower bound of [14]. The probabilities can be obtained by averaging the results of 4 types of parallel unentangled measurements.\nSection IV describes an original method with nprob = (2nqb + 1)d probabilities for which phase recovery can be achieved with a closed-form recursive algorithm. Those probabilities are obtained by averaging the results of 2nqb + 1 different kinds of measurements.\nSection V describes a more precise fine tuning method that works with all types of measurements, it requires an initial estimate from one of the algorithms of Sections III or IV which it uses in order to maximize the likelihood of the measurements.\nFinally, in Section VI we evaluate the performance of the proposed algorithms with simulated data."
        },
        {
            "heading": "II. STATE AND MEASUREMENTS",
            "text": ""
        },
        {
            "heading": "A. Considered state",
            "text": "An nqb-qubit pure state |\u03d5\u3009 can be decomposed in the canonical basis |0...0\u3009, ..., |1...1\u3009. The components of |\u03d5\u3009 in the basis can be stored in a d-element vector (d = 2nqb) v = [ v1 ... vd ]T where T stands for transpose. The components vj are complex and\u2211d j=1 |vj |2 = 1. The global phase of |\u03d5\u3009 has no physical meaning, so we can assume that v1 is a real nonnegative number.\n3"
        },
        {
            "heading": "B. Projective measurement",
            "text": "According to Section 2.2.5 in [1] a projective measurement is defined by a Hermitian matrix B whose distinct eigenvalues mk are the possible outcomes of the measurement. The probability of getting mk when measuring a pure state represented by v is \u033ak(v): the squared norm of the projection of v on the eigenspace associated with mk. A projective measurement can have the following properties:\n\u2022 d-outcome: If B has d distincts eigenvalues. The eigenstates are then 1-dimensional spaces, and \u033ak(v) = |e\u2217kv|2 where ek is a d-element unit-norm vector belonging to the k-th eigenspace of B (\u2217 is the transconjugate). By performing the measurement several times on copies of the state, we can therefore estimate {|e\u2217kv|2}1\u2264k\u2264d where ek spans an orthonormal basis.\n\u2022 Unentangled: If the measurement can be performed with simultaneous local measurements on each qubit. For example if the matrix B can be written as a tensor product of nqb matrices with dimension 2 \u00d7 2: B = B1 \u2297 ... \u2297 Bnqb . Then the measurement represented by B can be performed by measuring simultaneously each qubit with the measurements represented byB1, ...,Bnqb and then computing the product of all the outcomes. Depending on how the 2 eigenvalues of each Bk are chosen, computing the product of the outcomes can result in a loss of information (i.e. the outcomes of each Bk cannot be retrieved from their product knowing their respective 2 possible values). If this is the case B will have fewer than d distinct eigenvalues (see e.g. Pauli measurements)."
        },
        {
            "heading": "C. Parallel unentangled measurement",
            "text": "If a quantum measurement represented by B is unentangled and has d outcomes, we call it a parallel unentangled measurement. It can be performed on single qubits in parallel using the following setup:\n...\nM1\nM2\nMnqb\nq1\nq2\nqnqb\n2 outcomes\n2 outcomes ...\n2 outcomes\nd outcomes\nEach qubit q1, ..., qnqb composing the system is measured with a one-qubit measurement which has two distinct outcomes. The eigenvalues of B are not important in the present paper. Changing them without changing the eigenvectors would change B but (as long as the eigenvalues remain distinct) the resulting measurement would be equivalent in the sense that it would give the same information; the possible measured values would then depend on B but the associated probabilities would be the same up to a permutation, and this is what matters in the present paper. Since the parallel unentangled measurements we consider have d outcomes (by definition), \u033ak(v) = |e\u2217kv|2 where ek is a d-element unit-norm vector belonging to the k-th eigenspace. For a given parallel unentangled measurement M let us define the eigenvector matrix: EM =[ e1 . . . ed ] . The vector pM(v) = |E\u2217Mv|2 contains the d probabilities of the possible outcomes. By performing several measurements on copies of the state represented by v, we compute the frequencies of occurrence of each outcome, we get p\u0302M, which we use as an approximation of |E\u2217Mv|2. We call p\u0302M the averaged measurements or sample probabilities. The sum of the elements of pM(v) is one (it is the sum of the probabilities of all possible outcomes), so no information is lost by removing one element. We define EM the non-redundant eigenvector matrix as composed of the first d \u2212 1 columns of EM. Then pM(v) = |E\u2217Mv|2 is redundant but pM(v) = |E\u2217Mv|2 is not. E\n\u2217 M, E \u2217 M, pM(v) and pM(v) will all be used at different points of this article with M replaced by the actual measurements we will perform."
        },
        {
            "heading": "D. Considered types of measurements",
            "text": "We perform measurements for all qubits in parallel, with one measurement direction per qubit. For one qubit, we choose to perform measurements that are equivalent (up to a factor 1/2 on the outcomes) to the 3 non-trivial Pauli measurements. The measurement matrices B associated with the directions X, Y and Z are the last three Pauli matrices defined in Section 2.1.3 of [1] and the corresponding eigenvector matrices may be shown to read:\nEX = 1\u221a 2 ( 1 1 1 \u22121 ) EY = 1\u221a 2 ( 1 1 i \u2212i ) EZ = ( 1 0 0 1 ) . (1) If the qubit represents the spin of an electron, those eigenvector matrices represent the measurement of the spin component along 3 orthogonal directions.\n4 There is a factor 1/2 between the outcome of the spin measurements and the Pauli measurements but it does not affect the eigenvectors. For two or more qubits, the different qubits can be measured along X, Y or Z. It can be shown that the resulting eigenvector matrix is the tensor product of the 2-dimensional matrices of (1). For example for 2 qubits, measuring the first one along Z and the second one along X has the following eigenvector matrix\nEZX = EZ \u2297EX = 1\u221a2   1 1 0 0 1 \u22121 0 0 0 0 1 1 0 0 1 \u22121  .\nIn this example, if the qubits represent the spins of 2 electrons, then the measurement we perform is equivalent to measuring the first spin component along Z and the second along X . The spin measurement has 4 possible outcomes (+ 12 ,+ 1 2 ), (+ 1 2 ,\u2212 12 ), (\u2212 12 ,+ 12 ) and (\u2212 12 ,\u2212 12 ) and if v represents the considered state, the probabilities of each outcome are in the vector |E\u2217ZXv|2. This measurement is not equivalent to a two-qubit Pauli measurement (even if we forget the factor 1/2), as such a Pauli measurement only has two outcomes. In fact, the Pauli measurement along ZX would return +1 for spins measurement outcomes (+ 12 ,+ 1 2 ) and (\u2212 12 ,\u2212 12 ) and \u22121 for spins measurement outcomes (+ 12 ,\u2212 12 ) and (\u2212 12 ,+ 12 ). This is inefficient as half the information is wasted. For nqb qubits there are 3\nnqb different measurements of this type. Both of the QST methods of Section III and IV as well as the fine tuning algorithms of Section V use a specific subset of all possible measurements."
        },
        {
            "heading": "E. Justification",
            "text": "We think that performing QST using a kind of measurement that is not parallel unentangled (i.e. has fewer than d outcomes or is entangled) should not be recommended in practice with the current state of quantum computers for the following reasons:\n\u2022 Performing a quantum measurement that has fewer than d outcomes is suboptimal. Indeed, instead of considering a j-outcome measurement Mj (j < d) we can use a d-outcome measurement Md that has the same eigenvectors and d distinct eigenvalues. With this definition it is strictly better to use Md than Mj in all situations, as the outcomes of Md can be mapped injectively onto the outcomes of Mj but the reverse is not true. Therefore Md brings us strictly more information on the system\nthan Mj and performing either of them should be as difficult (a copy of the state is used up).\n\u2022 Performing an entangled measurement requires the use of a quantum gate. This gate itself is never going to act exactly as expected and will introduce errors. In order to see if the gate works as expected, we would need to perform quantum process tomography which generally relies on QST.\nBut the literature on QST is full of theoretical papers that consider measurements that fall within the two types that we do not recommend. Here are some examples:\n\u2022 [9] uses successive 2-outcome projective measurements on non-orthogonal entangled eigenstates. And each iteration of the algorithm would require a new type of measurement (that depends on what has been measured before and is most likely going to be entangled) and therefore a new quantum gate has to be built on the fly.\n\u2022 [13] considers projective 2-outcome measurements on 1-dimentional spaces. Half of those measurements can be performed using a single parallel unentangled measurement (with the identity matrix as eigenvector matrix); but the other half cannot.\n\u2022 [15] considers 2 parallel unentangled measurements (called local measurements in [15]) and 2 d-outcome entangled measurements that can be mapped on the other two using a gate that performs the Fourier transform. This setup is way more reasonable than the others as it requires a single known standard gate.\n\u2022 [2], [3], [4], [5], [6] and [7] all use multiqubit Pauli measurements. Multiqubit Pauli measurements have the advantage of being unentangled and also simplify the calculation for the QST of mixed states (see the beginning of Section 8.4.2 in [1], (8.149) only works for orthogonal sets of matrices with respect to the Hilbert\u2013Schmidt inner product, like Pauli matrices). They have the disadvantage of being 2-outcome measurements returning either +1 or -1. There are sets of Pauli measurements whose expected values can be deduced from the outcomes of parallel unentangled measurements without loss of information ([12] explains how it can be done for two qubits). But that is not the case for any set of Pauli measurements.\nIn contrast to those articles we here make a point to only use unentangled parallel measurements. We could have chosen other matrices than (1). We chose those matrices in order to be closer to the Pauli measurements widely used in the literature.\n5"
        },
        {
            "heading": "III. TOMOGRAPHY WITH MINIMAL NUMBER OF MEASUREMENT TYPES",
            "text": "The current section describes our first QST setup, Section III A describes the 4 types of parallel unentangled measurements that are performed, Section III B explains why it is reasonable to think that they are injective up to a global phase and Section III C describes a first algorithm to recover the phases."
        },
        {
            "heading": "A. Types of measurements",
            "text": "In the QST method described here, we perform 4 types of measurements on the considered ddimensional state: The first measurement measures all the qubits along Z, its eigenvector matrix, EZ...Z is the identity matrix, the second measurement measures all the qubits along Y, the third along X, and the fourth measures every odd-numbered qubit along X and every even-numbered qubit along Y. After performing the measurements several times on copies of the state, we compute the sample probabilities p\u0302M for M spanning the 4 types of measurements. We then have an ns = 4d dimensional vector with ns = 4(d \u2212 1) degrees of freedom. We call it p\u0302s. The associated theoretical probability vector is ps = |Asv|2, where s stands for \u201csmall\u201d because the corresponding matrix in Section IV has more rows. As is the concatenation of the transconjugates of the eigenvector matrices of the measurements we perform, As is defined similarly\nAs =   E \u2217 Z...Z E \u2217 Y...Y\nE \u2217 X...X\nE \u2217 XYXY...\n  and As =   E \u2217 Z...Z E \u2217 Y...Y\nE \u2217 X...X\nE \u2217 XYXY...\n  . (2)\nLet us define ps = |Asv|2. Since the norm of v is 1, ps and ps contain the same information (see Section II C). In Section III B we will consider As, ns and ps in order to see if the measurements are injective because we do not want to introduce redundancy when counting the measurements. But, for the sake of simplicity, we will considerAs, ns and ps in Section III C in order to recover the state from the measurements. We want to use all the measurements from p\u0302s whether they are redundant or not.\nB. Injectivity\nAs is an ns \u00d7 d matrix and v has unit norm. We want to know whether the measurements we chose\nare sufficient to recover any v from |Asv|2 up to a global phase. In the rest of the paper this property will be called injectivity. It is a bit of an exaggeration because v \u2192 |Asv|2 is never truly injective as changing the global phase of v will not change |Asv|2. This issue of injectivity was studied before in [14], [16], [18] in a slightly different setup: the considered measurements are |Asv| instead of |Asv|2 , this does not change anything for the injectivity, also v is not assumed to have unit norm, and this is important. In order to reconcile the two setups we can relax the unit-norm hypothesis for v and insert the row [0, ..., 0, 1] between the (d \u2212 1)-th row and the d-th row of As. This ensures that the norm of v is constrained: its square is the sum of the first d constrained measurements, because the first d rows of As are the identity matrix. With this change As has 4d\u2212 3 rows. According to [14] the minimal number of rows for As below which injectivity is impossible is 4d \u2212 3 \u2212 c(d)nqb rows for some c(d) \u2208 [1, 2]. Since we have 4d\u22123 rows, this necessary condition is satisfied. However there is no simple sufficient condition on As that ensures injectivity, and proving it for a given As is a known hard problem. The closest result we found to a sufficient condition is in [16] where it is shown that for a generic As, having 4d \u2212 2 or more rows ensures injectivity. As must be generic in the sense that it is part of a specific open dense set with full measure. We cannot identify this set and check that As would be in it (although it probably would because the set is of full measure), but this is moot because we are one row short of satisfying the 4d\u2212 2 condition anyway. However [18] explained why it is natural to think that 4d\u22124 is the actual lower bound. It remains a conjecture though. We can be sure that 3 measurement types would not be enough to achieve injectivity with nqb > 2 as the bound of [14] would not be fulfilled: we would have 3d \u2212 2 independent rows (3d \u2212 3 plus the unitnorm constraint). This is always strictly smaller than 4d\u2212 3 \u2212 2nqb for nqb > 2. 4 is the lowest number of measurement types for which we can hope to always achieve injectivity."
        },
        {
            "heading": "C. A first quantum pure state tomography method",
            "text": "In the current section, we show how the method proposed in [17] can be used in our framework to recover v from the sample probabilities p\u0302s, an estimate of ps = |Asv|2 (we only consider As from now on, As was only useful to discuss the injectivity). The optimization problem considered in [17] is the follow-\n6 ing:\nmin v\n\u2225\u2225\u2225|Asv| \u2212 \u221a p\u0302s \u2225\u2225\u2225 (3)\nwhere \u221a\np\u0302s is the element-wise square root of p\u0302s and ||.|| is the L2 norm. [17] does not include the unitnorm constraint on v but, since we use As, this constraint is implicit in the criterion to be minimized. In fact, the sum of the first d elements of |Asv|2 is the squared norm of v and the sum of the first d elements of p\u0302s is one, therefore if |Asv| is close to \u221a p\u0302s, their squared norms will also be close, and therefore the squared norm of v will be close to 1. In [17], it is shown that (3) is equivalent to the following optimization problem (originally it came from [19]):\nmin U s.t. C tr(UM) (4)\nwhere M = diag(p\u0302s)(I \u2212 AsA\u2020s)diag(p\u0302s), \u2020 is the pseudo-inverse, diag(p\u0302s) is the diagonal matrix whose diagonal is p\u0302s and C represents the following condition on the ns \u00d7 ns matrix U:\n\u2203u \u2208 Cnssuch that |u| = [1, ..., 1]T and U = uu\u2217. (5)\n[17] shows that if U is a solution of (4), then the associated u of (5) is an approximation of the phase of Asv, and the resulting estimate of v defined as:\nv\u03020 = A \u2020 s(u \u2217 \u221a p\u0302s) (6)\n(\u2217 is the element-wise product) is the solution of (3) proposed in [17]. (4) is almost a convex optimization problem. In fact if C is reformulated in an equivalent way: Ui,i = 1 \u2200i \u2208 [1, ns],U 0, Rank(U) = 1 (U 0 means that U is both Hermitian and non-negative definite), according to [17] the criterion tr(UM) is convex and the only constraint that makes the problem nonconvex in C is Rank(U) = 1. By relaxing it we have a convex problem that can be solved without the need for a good initialization:\nmin U s.t. Ui,i=1\u2200i,U 0 tr(UM). (7)\nOnce (7) is solved using the PhaseCut algorithm of [17], the eigenvectors and eigenvalues of the solution U are computed. In order to get an estimate of u, [17] then computes u\u0302, the eigenvector associated with the largest eigenvalue. From u\u0302, we get the estimate of v defined in (6):\nv\u0302pc = A \u2020 s(u\u0302 \u2217 \u221a p\u0302s). (8)\nIn [17] this method is tested with A matrices which represent usual use-cases in the signal/image processing community (oversampled Fourier transform, multiple random illumination filters, wavelet transform) for which PhaseCut works well. However forA = As, PhaseCut is a good initial point but needs the fine tuning that we will detail in Section V."
        },
        {
            "heading": "D. Comparison with the literature",
            "text": "Let us sum up the main features of our first QST algorithm:\n\u2022 It uses 4d probabilities that can be obtained by averaging the results of 4 parallel unentangled measurements.\n\u2022 It is reasonable to think that the chosen measurements are injective (the failure set is most likely empty).\n\u2022 The algorithm that reconstructs the state is not explicit (optimization).\nGoyeneche el al. [15] uses the same number of measurement types, has a known failure space of zero measure and provides an explicit reconstruction algorithm. The main advantage our approach based on PhaseCut has compared to [15] is that we do not use unentangled measurements. The more general compressed sensing approach of [4] requires O(rd log(d)2) probabilities to estimate the state where r, the rank of the density matrix, is 1 in the case of a pure state. Those probabilities could be obtained by averaging the results of O(log(d)2) different unentangled measurements. Our method is more efficient since we use 4 = O(1) different unentangled measurements. Both methods have no theoretical guarantee of injectivity or closed-form solution. The validity of the solution can only be shown in simulations."
        },
        {
            "heading": "IV. CLOSED-FORM STATE TOMOGRAPHY ALGORITHM",
            "text": ""
        },
        {
            "heading": "A. Alternative types of measurements",
            "text": "In the alternative QST method described here, we perform the following measurements{\nZ...Z\ufe38 \ufe37\ufe37 \ufe38 nqb times\n, {\nZ...Z\ufe38 \ufe37\ufe37 \ufe38 nqb \u2212 i times S X...X\ufe38 \ufe37\ufe37 \ufe38 i \u2212 1 times\n, 1 \u2264 i \u2264 nqb S \u2208 {X,Y }\n}}\nThe number of types of measurements is 2nqb + 1. The resultingAt (t stands for \u201ctall\u201d) matrix has nr =\n7 d(2nqb + 1) rows:\nAt =   E \u2217 Z...Z E \u2217 Z...ZX E \u2217 Z...ZY ...\nE \u2217 X...X\nE \u2217 YX...X\n  . (9)\nEach measurement is performed several times and we compute the sample probabilities p\u0302t which are estimates of the theoretical probabilities pt = |Atv|2. 2nqb + 1 sounds like a lot compared to the 4 measurement types of Section III but it is a small fraction of the 3nqb possible types of measurements defined in Section IID. This setup also has the advantage of coming with an attractive way to recover the state from the measurements, as will be explained in Section IVB."
        },
        {
            "heading": "B. A recursive pure quantum state tomography method",
            "text": "Let us show how a vector v can be recovered up to a global phase from |Atv|2 by induction on the number of qubits. At depends on nqb, in the rest of the current section this dependence will not be omitted and At will be called At(nqb). We first show how to solve the problem (recover v from |Atv|2) with nqb = 1. We then explain how solving the problem for nqb \u2212 1 qubits yields the solution for nqb qubits. From there a recursive algorithm can be implemented.\nnqb = 1: At(1) =\n  E \u2217 Z\nE \u2217 X E \u2217 Y\n , with the EZ ,EX ,EY\nof (1). The state vector is v =\n( |v1| |v2|ei\u03b8 ) . Basic\ncalculations show:\n|At(1)v|2 =  \n|v1|2 |v2|2\n1 2 ( |v1|2 + |v2|2 + 2|v1||v2| cos(\u03b8) ) 1 2 ( |v1|2 + |v2|2 \u2212 2|v1||v2| cos(\u03b8)\n) 1 2 ( |v1|2 + |v2|2 + 2|v1||v2| sin(\u03b8)\n) 1 2 ( |v1|2 + |v2|2 \u2212 2|v1||v2| sin(\u03b8) )\n  .\n(10) Therefore, |At(1)v|2 gives |v1|2, |v2|2, |v1||v2| cos(\u03b8) and |v1||v2| sin(\u03b8). From there, we have two cases: \u2022 If |v1| = 0 or |v2| = 0, then knowing |v1| and |v2| is enough because\n( |v1| |v2| ) is the same as v up to a\nglobal phase. Thus, there is no need to compute \u03b8.\n\u2022 If |v1||v2| > 0 then we can derive cos(\u03b8) and sin(\u03b8) from the above-defined quantities and get \u03b8. Thus we know all parameters of v.\nLet us now assume that the state recovery is possible for nqb \u2212 1 qubits, i.e. there is a function fnqb\u22121 such that for a vector w with 2 nqb\u22121 elements\nfnqb\u22121 ( |At(nqb \u2212 1)w|2 ) is equal to w up to a global phase. Let v be a d = 2nqb element vector (it does not have to be unit-norm). We split v into two 2nqb\u22121\nelement vectors w1 and w2: v = [ w1 w2 ] . Let us show how v can be recovered up to a global phase from |At(nqb)v|2 using the fact that w1 and w2 can be recovered form |At(nqb \u2212 1)w1|2 and |At(nqb \u2212 1)w2|2 up to global phases using fnqb\u22121. We start by comparing At(nqb \u2212 1) to At(nqb):\nAt(nqb \u2212 1) =   E \u2217 s1 ...\nE \u2217 s2nqb\u22121\n \nwith (9) giving the values of the strings s1, ..., s2nqb\u22121. We can also notice that:\nAt(nqb) =   E \u2217 Zs1 ... E \u2217 Zs2nqb\u22121\nE \u2217 X...X\nE \u2217 YX...X\n \n(11)\nwhere Zsk is the string made up of Z followed by s1. Using the definition of E in Section IID, we have:\nE \u2217 Zsk = E \u2217 Z \u2297E\u2217sk =\n[ E\n\u2217 sk 0\n0 E \u2217 sk\n] \u2200k. (12)\nLet k be an integer ranging from 1 to 2nqb \u2212 1, from (11) and (12), we have:\n|At(nqb)v|2ik = \u2223\u2223\u2223\u2223 [ E \u2217 sk 0\n0 E \u2217 sk ] [ w1 w2 ] \u2223\u2223\u2223\u2223 2 = [ |E\u2217skw1|2 |E\u2217skw2|2 ]\n(13) where |At(nqb)v|2ik is the vector that contains the elements of |At(nqb)v|2 indexed between (k\u22121)d+1 and kd. And using the same notation for |At(nqb\u22121)wl|2 with l being either 1 or 2, we have\n|At(nqb \u2212 1)wl|2ik = |E \u2217 skwl| 2. (14)\nFrom (14) and (13), we see that all the elements of |At(nqb \u2212 1)wl|2ik are in |At(nqb)v|2ik \u2200k \u2208 {1, ..., 2nqb \u2212 1}. Since |At(nqb \u2212 1)wl|2ik \u2200k \u2208 {1, ..., 2nqb \u2212 1} spans all the vector |At(nqb \u2212 1)wl|2\n8 we have shown that |At(nqb \u2212 1)wl|2 is known from part of the measurements (|At(nqb)v|2) for l = 1 and l = 2. Using the induction hypothesis we can apply fnqb\u22121 to the known quantities |At(nqb \u2212 1)w1|2 and |At(nqb \u2212 1)w2|2 in order to get w1 and w2 up to global phases. Let us call our estimates w\u03021 and w\u03022, w1 = e i\u03b81w\u03021 and w2 = e i\u03b82w\u03022. We now only need to know \u03b82 \u2212 \u03b81 in order to know v up to a global phase. Let us get \u03b82 \u2212 \u03b81 from the last 2d elements of |At(nqb)v|2. We define Lm as the column vector containing those last 2d elements\nLm = \u2223\u2223\u2223\u2223 [ E \u2217 XX...X\nE \u2217 YX...X ] [ w1 w2 ] \u2223\u2223\u2223\u2223 2 = \u2223\u2223\u2223\u2223 [ E \u2217 X \u2297E\u2217X...X E \u2217 Y \u2297E\u2217X...X ] [ w1 w2 ] \u2223\u2223\u2223\u2223 2\nwhere on the left-hand side the strings XX...X , Y X...X have nqb characters and on the right-hand side X...X have nqb \u2212 1 characters. By replacing EX and EY by their values of Section IID and calculating the tensor products, we get\nLm =\n\u2223\u2223\u2223\u2223\u2223 1\u221a 2   E \u2217 X...Xw1 +E \u2217 X...Xw2 E \u2217 X...Xw1 \u2212E\u2217X...Xw2 E \u2217 X...Xw1 \u2212 iE\u2217X...Xw2\nE \u2217 X...Xw1 + iE \u2217 X...Xw2\n  \u2223\u2223\u2223\u2223\u2223 2\n= 12 \u2223\u2223\u2223\u2223\u2223   E \u2217 X...Xw\u03021e i\u03b81 + E\u2217X...Xw\u03022e i\u03b82 E \u2217 X...Xw\u03021e i\u03b81 \u2212 E\u2217X...Xw\u03022ei\u03b82 E \u2217 X...Xw\u03021e\ni\u03b81 \u2212 iE\u2217X...Xw\u03022ei\u03b82 E\n\u2217 X...Xw\u03021e i\u03b81 + iE\u2217X...Xw\u03022e i\u03b82\n  \u2223\u2223\u2223\u2223\u2223 2 .\nLet us introduce the following notations\nm = 1\n2 |E\u2217X...Xw\u03021|2 +\n1 2 |E\u2217X...Xw\u03022|2\ndc = E\u2217X...Xw\u03021 \u2217E\u2217X...Xw\u03022 d(\u03b8) = cos(\u03b8)Re(dc)\u2212 sin(\u03b8)Im(dc)\n(15)\nwhere \u2217 again represents the element-wise product between two vectors and .\u0304 is the conjugate. w\u03021 and w\u03022 are known quantities (from |At(nqb)v|2) som and dc are known and d(\u03b8) can be computed for any \u03b8 \u2208 [0, 2\u03c0]. Let us rewrite Lm as a function of (\u03b82 \u2212 \u03b81) using those quantities\nLm(\u03b82 \u2212 \u03b81) =   m+ d(\u03b82 \u2212 \u03b81) m\u2212 d(\u03b82 \u2212 \u03b81)\nm+ d(\u03b82 \u2212 \u03b81 \u2212 \u03c0/2) m\u2212 d(\u03b82 \u2212 \u03b81 \u2212 \u03c0/2)\n  . (16)\nWe aim at deriving \u03b82 \u2212 \u03b81 from Lm (which is known from the measurements). We first notice from the definition of d(\u03b8) in (15) that if dc is 0 on every component then d(\u03b82 \u2212 \u03b81) is also 0 on every component (which means it does not depend on \u03b82\u2212\u03b81) and Lm is simply m repeated 4 times (see (16)). Therefore recovering \u03b82\u2212\u03b81 (and v) from Lm is impossible. However, we hereafter show that this is the only case\nwhen \u03b82 \u2212 \u03b81 cannot be recovered from Lm. And the ensemble of v which make this occur has zero measure. Let us assume that at least a single element of dc is not zero, let us call k its index, dk the corresponding non-zero element (we take the element which has the highest modulus), and call dk(\u03b82 \u2212 \u03b81) and mk the k-th elements of d(\u03b82\u2212\u03b81) and m respectively. Then all we need is the k-th and (k + d)-th elements of Lm whose expressions are mk+cos(\u03b82\u2212\u03b81)Re(dk)\u2212 sin(\u03b82 \u2212 \u03b81)Im(dk) and mk + sin(\u03b82 \u2212 \u03b81)Re(dk) + cos(\u03b82 \u2212 \u03b81)Im(dk). Those known elements can be put in a column vector and re-written as:\n( Re(dk) \u2212Im(dk) Im(dk) Re(dk) )( cos(\u03b82 \u2212 \u03b81) sin(\u03b82 \u2212 \u03b81) ) . (17)\nThe 2 \u00d7 2 matrix on the left-hand side is known (since dk is known) and invertible (since its determinant is |dk|2 > 0). Therefore \u03b82\u2212\u03b81 can be recovered (because we have its sine and cosine) from 2 elements of Lm (so two probabilities). We could stop there and get an estimate \u03b8d of \u03b82 \u2212 \u03b81 that is computed using two elements of Lm. But, in practice the sample probabilities give an im-\nperfect estimate of Lm which we call L\u0302m. In order to be robust to the errors, we aim to find the angle \u03b8\u03022 \u2212 \u03b81 that minimizes ||Lm(\u03b82 \u2212 \u03b81) \u2212 L\u0302m||, this way we use all sample probabilities and not just two. We use a quasi-Newton BFGS algorithm [20] (implemented with fminunc in the Matlab numerical software) initialized at \u03b8d, the optimization stops when the step is smaller than 10\u221230. Technically with this optimization, the algorithm is no longer closedform but, since it involves a single parameter, it is really fast, and improves the performances quite significantly so we choose to perform it anyway. If the readers want a real closed-form algorithm, they can use \u03b8d instead of computing \u03b8\u03022 \u2212 \u03b81, or use a closedform optimization algorithm with a fixed number of steps to compute \u03b8\u03022 \u2212 \u03b81. Let us now take a step back and summarize what we have proved in this section:\n\u2022 Recovering the state (up to a global phase) from the measurements is possible for nqb = 1.\n\u2022 Assuming it is possible for nqb \u2212 1 we showed it is also possible for nqb unless the state is in an ensemble of zero measure.\nUsing those previous two results, we can construct a recursive algorithm that recovers v from the measurements. It will work except on the union of a finite number of failure sets of zero measure which would\n9 also be of zero measure. The estimate given by this recursive algorithm will be called v\u0302rec."
        },
        {
            "heading": "C. Discussion about the number of probabilities used",
            "text": "The recursive algorithm of the previous section calls itself twice for each reduction of the number of qubits by 1. This means that for nqb, it is called once with nqb qubits, twice with nqb \u2212 1 qubits, ..., 2nqb\u22121 times with 1 qubit. For 1 qubit, the state is recovered using (10) which involve 6 probabilities, among which only 4 are required (we could obtain the same result without using the fourth and sixth elements of |At(1)v|2). For q > 1 qubit before calling the recursive function with one fewer qubit, we compute \u03b82 \u2212 \u03b81 using (16). This involves 2 \u00d7 2q probabilities among which only 2 are strictly required for the first estimate \u03b8d. The minimum number of needed probabilities is 4\u00d7 2nqb\u22121+2 \u2211nqb\nq=2 2 nqb\u2212q = 2d+2(2nqb\u22121\u22121) = 3d\u22122.\nFurthermore, if we take into account the fact that v has unit norm, then one of the probabilities along the Z axis (which are all used) becomes redundant, and this number becomes 3(d\u2212 1). In practice all probabilities are used in order to minimize the impact of the statistical error on the probabilities. But if we wanted to remove rows from At in (9) and only keep 3(d\u22121) of them, we could still achieve QST. However, this is a bad idea because we would no longer have a concatenation of d-outcome parallel measurements. And in practice the final estimate of the state would be less robust to the errors on the sample probabilities and the quantum setup would not be any easier to put in place, as the estimation of the 3(d \u2212 1) probabilities to be kept requires all 2nqb + 1 measurements to be performed anyway."
        },
        {
            "heading": "D. Comparison with the literature",
            "text": "Let us sum up the main features of our second QST algorithm:\n\u2022 It uses (2nqb+1)d probabilities that can be obtained by averaging the results of 2nqb +1 parallel unentangled measurements.\n\u2022 The measurements are injective outside a known failure set with zero measure.\n\u2022 The algorithm that reconstructs the state is explicit.\nThose features are very similar to those of Goyeneche et al. [15]. The advantage of our method it that the measurements it uses are unentangled. Its drawback is that it requires 2nqb + 1 measurements which is more than 4 (except for the trivial case nqb = 1). That is the price to pay for using only unentangled measurements, we could not find a simple closed-form algorithm that works with fewer types of unentangled measurements. The more general compressed sensing approach of [4] requires O(rd log(d)2) probabilities to estimate the state where r, the rank of the density matrix, is 1 in the case of a pure state. Those probabilities could be obtained by averaging the results of O(log(d)2) different unentangled measurements. We do better here since we only use 2nqb+1 = O(log(d)) measurements. We also have the advantage of providing a closed-form algorithm contrary to the method of [4] which is very general (works for mixed states and any kind of measurement), but uses an optimization algorithm and provides no proof of injectivity."
        },
        {
            "heading": "V. LIKELIHOOD MAXIMIZATION",
            "text": ""
        },
        {
            "heading": "A. Main idea",
            "text": "Sections III and IV give us estimates of the state v, denoted as v\u0302pc and v\u0302rec respectively. v\u0302pc is the solution of the QST problem with one constraint (rank(U) = 1) relaxed, so it can be inaccurate even in the absence of errors in the sample probabilities. The algorithm of Section IVB that computes v\u0302rec is also imperfect. It relies heavily on the measurements along Z...Z, Z..ZX and Z..ZY (used 2nqb\u22121 times for one qubit at the end of the recursive tree to compute all the moduli and half the phases differences) and it almost does not use the measurements along X...X and Y X...X (used only once to compute one phase difference (\u03b82 \u2212 \u03b81) with (16)). Each of those last two measurements contains as much information on v as the measurements along Z...Z, but the former are barely used. Therefore the estimation methods of Section III and IV are hereafter supplemented by a final tuning to make them more precise. To this end, we take a maximum likelihood (ML) approach:\n(x\u0302, y\u0302) = argmin x,y s.t. ||x||2+||y||2<1 L(x,y)(p\u0302) (18)\nwhere p\u0302 is the vector that contains sample probabilities and L(x,y)(p\u0302) is to be understood as the negative log-likelihood of measuring the sample probabilities p\u0302 if the true state is v(x,y), with x and y defined hereafter. In the whole paper, whenever we write\n10\n\u201cnegative log-likelihood\u201d (or L ) we mean \u201copposite of the log-likelihood up to additive and positive multiplicative constants\u201d. These constants will not matter as the negative log-likelihood will be minimized. The vector v(x,y) with respect to which L will be minimized is defined as: v(x,y) = [ \u221a 1\u2212 ||x||22 \u2212 ||y||22, x1 + iy1, ..., xd\u22121 + iyd\u22121]T .\nx and y are d \u2212 1 element vectors representing the real and imaginary parts of the last elements of v. The constraint in (18) is r2 < 1 (with r =\u221a ||x||22 + ||y||22) and not r2 \u2264 1 because optimization is easier on an open set). We mitigate the effect of this imperfect constraint by permuting the first component of v and the component of v with the highest modulus at the initial point of the optimization. Thus, we ensure that r2 is not going to be close to 1 unless the initial point was way off. The sample probabilities and the columns of A are permuted in the same way. Those change are limited to the optimization algorithm. Since the optimization set is open we can change the variables in order to remove the constraint altogether:\nx\u2032 = tan(\u03c0 2 r) r x and x = 2 \u03c0 atan(r\u2032) r\u2032 x \u2032 y\u2032 = tan(\u03c0 2 r)\nr y and y = 2 \u03c0 atan(r\u2032) r\u2032 y \u2032\n(with r\u2032 = \u221a ||x\u2032||22 + ||y\u2032||22). The new optimization problem on x\u2032and y\u2032 does not have any constraint, as when r\u2032 spans the whole space r remains strictly smaller than 1. Eq. (18) is therefore replaced by\n(x\u0302\u2032, y\u0302\u2032) = argmin x\u2032,y\u2032 L(x\u2032,y\u2032)(p\u0302). (19)\nIn order to solve (19) we again use the BFGS algorithm where the analytical expressions of the gradients are provided. The algorithm stops when the norm of the optimization step is smaller than 10\u221230. Like in most non-convex optimization methods, we need a good initialization point, we use either v\u0302pc or v\u0302rec. The most likely v is v\u0302ml = v(x\u0302\u2032, y\u0302\u2032), with x\u0302\u2032, y\u0302\u2032 defined in (19). All that remains now is to define the expression of the negative log-likelihood L with respect to v. In the following 2 subsections we will give 2 expressions for the normalized log-likelihood: L exact(x\u2032,y\u2032)(p\u0302) and L gauss(x\u2032,y\u2032)(p\u0302)."
        },
        {
            "heading": "B. Exact likelihood",
            "text": "In [21] the formula for the likelihood of a multioutput quantum measurement is given (albeit for a\nmixed state represented by \u03c1 which we would have to replace by vv\u2217). It boils down to:\nL exact (x\u2032,y\u2032)(p\u0302) = \u2212\nnprob\u2211\nk=1\nnklog ( (|Av(x\u2032,y\u2032)|2)k ) . (20)\n( |Av(x\u2032,y\u2032)|2 ) k is the k-th element of |Av(x\u2032,y\u2032)|2, A is the measurement matrix, either As or At, nk is the number of times the k-th outcome occurred i.e. the k-th element of p\u0302 (either p\u0302s or p\u0302t) multiplied by the number of times the measurement is repeated, and nprob is the number of rows of A. In order to get to this result we must consider the measurement counts as the realizations of a multinomial random variable. This is not an approximation, this is why we call this likelihood \u201cexact\u201d."
        },
        {
            "heading": "C. Gaussian approximation",
            "text": "In this subsection, we use the central limit theorem to approximate the scaled sample probabilities as the realization of a multivariate normal distribution. It is appropriate as the vector p\u0302 whose likelihood we want to compute is the average of independent realizations of the same random variable. Its expected value is the vector of theoretical probabilities p(x\u2032,y\u2032) that depends on the state. Let us define \u03b5(p\u0302,x\u2032,y\u2032) = p\u0302 \u2212 p(x\u2032,y\u2032) and \u03b5(p\u0302,x\u2032,y\u2032) is \u03b5(p\u0302,x\u2032,y\u2032) with the last element removed (no information is lost as the sum of the elements of \u03b5(p\u0302,x\u2032,y\u2032) is 0). In Appendix A, we show that if N is the number of times the measurements have been averaged, then \u221a N\u03b5(p\u0302,x\u2032,y\u2032) asymptotically (N \u2192 +\u221e) follows a zero-mean multivariate normal distribution. Its covariance matrix \u03a3 is computed in Appendix A. \u03a3 depends on the theoretical probabilities, we need to remove this dependency. With that in mind, we get to the following approximation for the negative log-likelihood:\nL gauss (x\u2032,y\u2032)(p\u0302) = N\u03b5(p\u0302,x \u2032,y\u2032)T \u03a3\u0303\u22121\u03b5(p\u0302,x\u2032,y\u2032) (21)\nwhere \u03a3\u0303\u22121 is an approximation of the covariance matrix that uses p\u0303 = p\u0302+ 5 N\n1+ 5d N\nas a regularized approxima-\ntion of p, this is justified in Appendix A. Appendix A also shows that this equation boils down to\nL gauss (x\u2032,y\u2032)(p\u0302) = N\nd\u2211\nk=1\n\u03b5k(p\u0302,x \u2032,y\u2032)2\np\u0303k . (22)\nThis log-likelihood is the result of two approximations that are true only when N \u2192 +\u221e: we approximated \u03b5(p\u0302,x\u2032,y\u2032) as the realization of a Gaussian random vector and we used an approximation\n11\nfor \u03a3. In practice, the resulting approximation is smoother and easier to minimize than L exact(x\u2032,y\u2032)(p\u0302) if the initialization point is not good enough (as will be shown in Section VIC). However, with a good initialization, the state that minimizes L exact(x\u2032,y\u2032)(p\u0302) should be closer to the true state than the one that minimizes L gauss(x\u2032,y\u2032)(p\u0302). The smaller N , the starker the difference. This will be shown in Section VI B."
        },
        {
            "heading": "D. Mixed minimization",
            "text": "As stated above L gauss(x\u2032,y\u2032) is supposed to be easier to minimize but the minimum of L exact(x\u2032,y\u2032) is supposed to be a better estimate. A good way to combine the two advantages is to start the optimization process by minimizing L gauss(x\u2032,y\u2032) and finish it by minimizing L exact(x\u2032,y\u2032). In practice, we here again run the BFGS algorithm on L gauss(x\u2032,y\u2032) for 100 iterations starting from the initialization point of Sections III or IV, this yields v\u0302inter . And then we run the BFGS algorithm on L exact(x\u2032,y\u2032) starting from v\u0302inter and stopping only once a local (hopefully global) minimum has been found."
        },
        {
            "heading": "VI. NUMERICAL RESULTS",
            "text": ""
        },
        {
            "heading": "A. Performances of the two initialization algorithms",
            "text": "Sections III and IV detail two methods to perform QST which are used for initialization of ML algorithms. The current section aims at estimating the precision of those methods and comparing them whenever possible. The recursive algorithm of Section IV only works for a specific set of measurement types but is explicit and does not require an undefined number of iterations to converge contrary to PhaseCut defined in Section III. We only explained PhaseCut for the setup with 4 different measurement types described in Section IIIA, but it can be applied to any types of measurements. In particular we could apply it to the setup with 2nqb + 1 measurement types of Section IVA. In the current section, we test both PhaseCut and the recursive algorithm on 50 randomly generated 7-qubit pure states. The two sets of measurement types of Sections III A and IVA are considered. They contain respectively 4 and 2\u00d7 7 + 1 = 15 measurement types. We test those algorithms with 2 different fixed numbers of total measurements NC : 5 000 and 500 000. Thus each one of the 4 measurement types of the setup of Section III A\nis performed either NC = 1250 or NC = 125000 times and each one of the 15 measurement types of the setup of Section III A is performed either NC = 333 or NC = 33333 times. The metric used in order to quantify the proximity of v\u0302 to the actual vector v up to a phase factor is \u00b5 = ||v \u2212 v\u0302.e\u2212i\u03be||2 with \u03be the angle that minimizes our metric: ei\u03be = v \u2217 v\u0302\n|v\u2217v\u0302| . We call \u00b5 this error in the\nrest of the paper. \u00b5 is maximal for orthogonal states (it is then \u221a 2), and minimal for states that differ by a global phase (is is then 0). A more widely used metric in the literature is the fidelity (see Section 9.2.2 in [1]) f = |v\u2217v\u0302|. It can be shown that f = (1 \u2212 \u00b522 ). We do not use the fidelity because it can push some interesting values too close to 1. Fig. 1 shows the error of v\u0302pc obtained by using PhaseCut with 100 to 100 000 iterations for the two setups (4 and 15 measurement types). With 15 (and not with 4) measurement types, the recursive algorithm can be implemented. We display biggest and smallest errors of v\u0302rec obtained with the recursive algorithm with horizontal bold green and red lines respectively. The recursive algorithm is performed in a fixed number of steps, this is why we plot the error on horizontal lines and not on a curve with respect to a number of iterations. The aim of this simulation is to see how many iterations of PhaseCut are required to get a good estimate of the state and to compare the performances of the recursive algorithm with those of the more versatile PhaseCut.\n12\nWith enough iterations (\u223c 104 for Nc = 5000 and \u223c 105 for Nc = 500000) PhaseCut is more precise than the recursive algorithm on the setups on which they can both be implemented, but it takes way more time. Each iteration of PhaseCut is costly, because we are working on an nprob\u00d7nprob matrix. With Matlab, on a 2.11 GHz 4-core processor with 32 Go RAM, each iteration of PhaseCut takes around 4 ms for the setup with 4 types of measurements and around 45 ms for the setup with 15 types of measurements. In that same 15 measurement type setup, the recursive algorithm takes 200 ms. This is way faster than PhaseCut which runs in minutes, as it requires thousands of iterations."
        },
        {
            "heading": "B. Likelihood estimator comparison",
            "text": "In Section V we defined two likelihood estimators, based on the likelihood maximization. The first one minimizes the true negative log-likelihood L exact(x\u2032,y\u2032) and the other minimizes a version of the negative loglikelihood that is supposed to be smoother, namely L\ngauss (x\u2032,y\u2032). We know that L gauss (x\u2032,y\u2032) is an approximation of the likelihood that is accurate only if the number of measurements per measurement type is high enough. Therefore we expect the global minimum of L gauss(x\u2032,y\u2032) to be a worse estimator than the global minimum of L exact(x\u2032,y\u2032) for a limited number of measurements. In order to check whether this is true and quantify the difference, we compute the errors on both estimators when they are initialized at the true state v. Doing this ignores the error on the initialization point (to which the regularized Gaussian estimate is supposed to be robust). We also compute the error for the mixed algorithm which starts by minimizing L gauss(x\u2032,y\u2032) and then minimizes L exact(x\u2032,y\u2032). These 3 types of errors are computed with 1000 random initial states on the four setups described in Section VIA with 4 or 15 measurement types and 5000 or 500 000 total measurements. For each of the four setups, the empirical cumulative density function (empirical cdf) is computed on the 1000 errors associated with the initial states, those cdf are shown in Fig. 2.\nAs predicted the error is larger with the Gaussian estimate of the likelihood, and the difference decreases when the number of measurements per measurement type increases.\nThe performance of the mixed minimization algorithm is very close to that of the estimator that minimizes L exact(x\u2032,y\u2032). There can be small differences however. Its turns out that they sometimes converge toward close but different minima. This is due to the\nfact that the small error made by the first 100 iterations of the mixed algorithm (during which L gauss(x\u2032,y\u2032) is minimized) can be enough to affect the final result. The differences between the 3 estimators are only noticeable for Nc = 5000 with 15 and 4 different measurements (so 333 or 1250 measurements per measurement type)."
        },
        {
            "heading": "C. Convergence of the likelihood estimators",
            "text": "In the current section, we intend to see what precision on the initial state is required to make sure that the likelihood optimization algorithm converges towards a reasonable solution, and compare the robustness of the three ML estimates. We compare the rates of divergence (denoted as \u03b4 and defined below) of the algorithms that minimize L exact(x\u2032,y\u2032) and L gauss (x\u2032,y\u2032) as well as the mixed algorithm. 1000 random states v to be estimated are considered with 1000 associated initial states of ML algorithms that have an initialization error \u00b5 linearly varied from 0 to \u221a 2 (as stated above \u221a 2 is the highest possible value for \u00b5, it is reached if the two states are orthogonal). Let us denote as {\u00b5i}i\u2208{1,...,1000} the 1000 values of this initial error on states v and de-\nfine { balgoi , i \u2264 1000, algo \u2208 {exact,Gauss,mixed} } where balgoi is \u22121 if the algo algorithm converges towards the same minimum with the \u00b5i initialization error and with no error and +1 if it converges toward a different minimum. We say that those two minima are the same if the error \u00b5 between the two is smaller than one percent of the error between the first one\n13\n(initialized without error) and the true state vector. For each of the 3 algorithms, we then define the rate of divergence \u03b4algo(\u00b5) associated with a given error \u00b5. It takes all the balgoi into account but gives more weight to those for which the associated \u00b5i is close to \u00b5:\n\u03b4algo(\u00b5) = 1 2\n( 1 + \u2211 1000 i=1 balgo i e \u2212( \u00b5\u2212\u00b5i\u03b1 ) 2\n\u2211 1000 i=1 e \u2212( \u00b5\u2212\u00b5i\u03b1 ) 2\n) .\nSimply put, if the majority of \u00b5i in the vicinity of \u00b5 are associated with balgoi equal to \u22121 (i.e. the algorithm converges towards the proper minimum with initialization errors around \u00b5) then, \u03b4algo(\u00b5) will be close to 0. If the associated balgoi are 1 (i.e. the algorithm does not converge towards the proper minimum) then, \u03b4algo(\u00b5) will be close to 1. The parameter \u03b1 quantifies how far away from \u00b5 we look for results, we picked \u03b1 = 0.1. Fig. 3 shows the rates of divergence of the 3 algorithms in the four setups described in Section VIA with 4 or 15 measurement types and 5000 or 500 000 total measurements.\nThe two plots on the right are of limited interest to us as the rate of divergence is always very low (\u2264 10\u22124) for errors lower than 0.75. We are mostly interested in the rates of divergence for initialization errors \u00b5 smaller than 0.75 because according to Fig. 1, the recursive algorithm always yields an estimate that corresponds to an error lower than 0.75 and PhaseCut also does so quite quickly (for more than 5000 iterations) for every setup. For those errors (on the two plots on the left), the best algorithm seems to be the minimization of L gauss(x\u2032,y\u2032), indeed increased robustness to the initialization error is the whole reason why we introduced L gauss(x\u2032,y\u2032). The mixed algo-\nrithm does not quite reach the same robustness but it is certainly an improvement over the algorithm that minimizes L exact(x\u2032,y\u2032) which has the worst performances for the relevant initialization errors. We should note that the name given to \u03b4: \u201crate of divergence\u201d is a bit severe as the likelihood algorithms never diverge in practice, they simply converge toward a false local minimum that is sometimes close to the real global minimum. \u03b4 is not useless however, and Fig. 3 shows us that, generally, with either the mixed algorithm or the algorithm that minimizes L gauss(x\u2032,y\u2032), an initialization error lower 0.75 leads to proper convergence towards the real minimum. According to Fig. 1, 5000 iterations of PhaseCut as well as the recursive algorithm generally yield an error smaller than 0.75. Therefore we choose to use the recursive algorithm when it is possible i.e. with the setup of Section IV with 15 types of measurements for 7 qubits (because it is faster than PhaseCut) and when PhaseCut has to be used (so with 4 measurement types) we only perform 5 000 iterations. We could let PhaseCut run longer but our implementation of the ML algorithm is faster."
        },
        {
            "heading": "D. Global performances",
            "text": "This section aims to test the algorithms of Sections III and IV, fine tuned with the 3 algorithms of Section V on nqb = 7 qubits, with the four setups described in Section VIA. For each setup, and for each version of the ML algorithm, 4 estimates of v are computed:\n\u2022 The initial estimate, so v\u0302pc for the setup with 4 measurement types or v\u0302rec for the setup with 15 measurement types. It does not depend on the choice of the ML algorithm.\n\u2022 v\u0302ml which is the result of the likelihood optimization (minimizing either L exact(x\u2032,y\u2032) or L gauss (x\u2032,y\u2032) or both\nsuccessively) initialized at the initial estimate.\n\u2022 v\u0302ref which is the result of the likelihood optimization initialized at the true v (not available in practice, it should be the global maximum likelihood; if v\u0302ml = v\u0302ref then the initial estimate was good enough). We call v\u0302ref the reference, it has already been defined (but not named) in Section VIB and represented in Fig. 2.\n\u2022 And v\u0302rnd which is the result of the likelihood optimization initialized at a random normalized vector (if v\u0302rnd is not worse than v\u0302ml, then the initial estimate was unnecessary and one can only use the maximum likelihood algorithm initialized randomly).\n14\nFor each setup, 1000 tests are performed with 1000 randomly generated v. We compute the estimates of each v with the different algorithms and display the empirical cumulative density function (cdf) of the errors in Fig. 4 to Fig. 6.\nThe performances of the three ML algorithms are quite similar (when excluding the random initialization), but some differences can be noted:\n\u2022 The algorithm that minimizes L exact(x\u2032,y\u2032) is supposed\nto be less robust to the initialization error than the others. It is only apparent for the setup with 4 measurements and Nc = 5000. v\u0302ml is not quite as precise as v\u0302ref .\n\u2022 The algorithm that minimizes L gauss(x\u2032,y\u2032) does not\nhave that problem, v\u0302ml and v\u0302ref are always indistinguishable. However the version of v\u0302ref computed by minimizing L gauss(x\u2032,y\u2032) is not as precise as the version that minimizes L exact(x\u2032,y\u2032). This can be seen by comparing Fig. 4 and Fig. 5 but it is more visible on Fig. 2 that represents the performances of the 3 references on a single graph.\n\u2022 The mixed algorithm seems to combine the advantages of those based on L gauss(x\u2032,y\u2032) and L exact (x\u2032,y\u2032).\nv\u0302ml is almost equal to v\u0302ref , and v\u0302ref is almost as good with this mixed algorithm as with L exact(x\u2032,y\u2032) (see Fig. 2 for a clearer comparison of the two values of v\u0302ref ).\nThe performances of v\u0302rnd, the maximum likelihood estimators initialized at a random point, are interesting. With the 4 measurement type setup, it is always a much worse estimate than v\u0302ml. But with 15 measurement types it is (almost) as good as the maximum likelihood estimators initialized at v\u0302rec (unless we use the L gauss(x\u2032,y\u2032) minimization). This could make us question the relevance of the recursive algorithm defined in Section IV. It would seem that the structure of the measurement matrix At is such that the gradient descent algorithm naturally converges towards the global minimum from any initial point. However the recursive algorithm is still useful because it is very fast and speeds up the likelihood maximization (see\nTable 2). We can also compare the performances of the two\ninitialization algorithms v\u0302pc or v\u0302rec (blue curve) with v\u0302ml (dashed red curve). The error on v\u0302ml is at least 3 times smaller (or way less for v\u0302pc and Nc = 500000) than that of the initialization algorithms. This shows that the fine tuning with ML is very useful to reduce the error. Comparing the precision of the initialization algorithm with v\u0302rnd is unwise because v\u0302pc and v\u0302rec can be improved with the ML algorithm whereas v\u0302rnd cannot as it is a local minimum of the likelihood. Furthermore, with Nc = 5000, v\u0302pc and v\u0302rec have a similar accuracy (respectively on 4 and 15 measurement types). And with Nc = 500000, v\u0302rec is a way better estimate than v\u0302pc because the PhaseCut algorithm is limited to 5000 iterations (allowing it enough iterations to converge properly would be way slower and not as accurate as the likelihood maximization).\nAfter likelihood optimization the performances of v\u0302ml with 15 and 4 measurement types are comparable (with the mixed algorithm, the 15 measurement setup is slightly better). Also the final error is roughly 10 times smaller when the number of measurements is multiplied by 100. This means that for more than 5000 measurements one can extrapolate the error (and therefore its cdf), as the error is proportional to N \u22121/2 c .\nThe fact that the recursive algorithm used to compute v\u0302rec has a zero measure failure set on which phase recovery is impossible (see Section IV) turns out to be a non-issue. We could have expected to see some outliers on the error of v\u0302rec, and the v\u0302ml computed from it, if the randomly generated v was close enough to the failure set. It is not the case, each one of the 1000 initial states has been successfully recovered with a reasonable error. The same is true when using PhaseCut with the 4 measurement type setup. Even though we were not able to prove the injectivity, the QST goes well in practice and there are no outliers in the error if the proper algorithms are used.\nTable 1 and Table 2 give the median execution time of all the algorithms on an Intel Xeon Gold 6226R 2.9 GHz core, all the scripts ran on 1 thread on Matlab. There are no significant differences between the 3 ML algorithms when they are not initialized at random. The random initialization is never relevant, as for the 4 measurement type setup it is relatively fast (as it spares us the initialization step with PhaseCut) but inaccurate; and for the 15 measurement type setup it is always slower (sometimes way slower) that the likelihood maximization with proper initialization.\nIn conclusion, we recommend using the mixed algorithm for the likelihood, it is a good compromise between the L gauss(x\u2032,y\u2032) minimization and the L exact (x\u2032,y\u2032) minimization. The choice between the setup with 4 types of measurements and the setup with 2nqb + 1 types of measurements is less obvious. The first one is obviously simpler for the operator and the likelihood optimization is faster (see Table 1 and Table 2) but:\n\u2022 It yields a slightly less precise result. The median error with the mixed algorithm and Nc = 5000 is 0.22 against 0.19 with 15 measurement types.\n\u2022 We have no closed-form algorithm that retrieves the state from the measurements. We must rely on PhaseCut which is unprecise. PhaseCut is also slow but the time gained during the mixed ML algorithm more than makes up for it (see Table 1 and\nTable 2).\n\u2022 We explained (in Section III B) why we think the measurements are injective, and in practice all 1000 tested states were recovered, but we were unable to prove the injectivity so far.\n16"
        },
        {
            "heading": "VII. CONCLUSION AND FUTURE WORK",
            "text": "In this paper we first showed how some of the work made in the applied mathematics community in the field of phase recovery can be used to define a set of four types of d-outcome measurements that should be enough to achieve QST for any pure state using the PhaseCut optimization algorithm. We also proposed a set of (2nqb+1) types of d-outcome measurements as well as a recursive algorithm which allows explicit reconstruction of the state (nqb is the number of qubits, d = 2nqb). Experimentally, they both give similar performances when the total number of measurements is the same (slight advantage for the second set of measurements); the first set is easier to set up and the second set is more theoretically sound. The initial estimates of the considered state are then fined tuned with the maximum likelihood approach that is widely used in the quantum information processing literature. We introduced some refinements which make it more robust by considering a smooth an easy way to maximize an approximation of the likelihood. We intend to use those QST methods to perform quantum process tomography (QPT) like in [22]. In [22] we introduced a QPT method that relies on measuring the state of the system after different time delays. At each time delay, we have to perform QST."
        },
        {
            "heading": "Appendix A: Covariance matrix and likelihood of the error on the sample probabilities",
            "text": ""
        },
        {
            "heading": "1. Covariance matrix",
            "text": "Appendix A aims at computing the asymptotic law of \u221a N\u03b5 = \u221a N(p\u0302 \u2212 p) defined in Section VC and at simplifying the expression of the likelihood of \u03b5. We consider that p contains the probabilities of a single type of d-outcome measurement. The generalization is straightforward as the errors on different measurements are independent (see Section A3). The only random vector in \u03b5 is p\u0302 defined as the vector that contains the sample probabilities of each of the d outcomes. So p\u0302 = 1Nn where each component ni of n contains the number of times the i-th outcome occurred. By definition n follows a multinomial distribution characterized by the number of trials N and the theoretical probabilities of each outcome contained in p. The expected value and covariance matrix of the multinomial distribution are known: E(n) = Np and Cov(n) = N(diag(p)\u2212 ppT ). We want to use the central limit theorem so let us write n as a sum: n = \u2211N\nk=1 \u03b4k where the {\u03b4k}k\nare independent and have the same distribution for different k. \u03b4k contains d \u2212 1 zeros and one 1 at a random index ik \u2208 {1, ..., N} whose density function is j \u2212\u2192 pj (i.e. the probability that ik takes the value j \u2208 {1, ..., N} is pj, the j-th element of p). \u03b4k follows a multinomial distribution with N = 1 trial. Its expected value is therefore p and its covariance matrix is diag(p) \u2212 ppT . Therefore \u03b5 is the difference between the empirical average of \u03b4k with N realizations and its expected value. According to the central limit theorem, when N \u2192 +\u221e, the distribution of \u221a N\u03b5 tends to a centered multivariate normal distribution, and its covariance matrix is \u03a3full = diag(p)\u2212 ppT . \u03a3\u0302full is an estimate of \u03a3full, it uses p\u0302 as we do not want to depend on the unknown vector p: \u03a3\u0302full = diag(p\u0302)\u2212 p\u0302p\u0302T ."
        },
        {
            "heading": "2. Likelihood",
            "text": "The easiest way to compute the likelihood of a vector that follows a multivariate normal distribution requires us to invert the covariance matrix [23]. If the covariance matrix is not invertible, then it is not of full rank, this means that at least one component of the random vector is linearly dependent on the others and therefore it is not needed to compute the likelihood. Those components can be removed and the likelihood of the smaller vector is the same as the likelihood of the original vector. In our case, the components of \u221a N\u03b5 sum to zero, therefore its covariance matrix is not invertible and any component can be removed without loosing any information that could be used to compute the likelihood. Let us consider\u221a N\u03b5, it is the same vector as \u221a N\u03b5 with the last component removed, and thus, its covariance matrix is the same with the last row and column removed: \u03a3 = diag(p) \u2212 ppT (p is p with the last element removed). It can be estimated with the sample prob-\nabilities p\u0302 =   p\u03021 ...\np\u0302d\u22121\n  instead of p. The resulting\nmatrix is \u03a3\u0302 = diag(p\u0302)\u2212 p\u0302p\u0302T . Straightforward calcu-\nlations show that if no element of p\u0302 =   p\u03021 ...\np\u0302d\n  (with\np\u0302d = 1\u2212 \u2211d\u22121 k=1 p\u0302k) is zero, then, \u03a3\u0302 is invertible and\n\u03a3\u0302 \u22121 =\n1\np\u0302d 1+ diag(1/p\u0302). (A1)\nis its inverse. 1/p\u0302 is the element-wise inverse of p\u0302\n17\nand 1 is the d\u2212 1\u00d7 d\u2212 1 matrix with only ones. In practice, elements of p\u0302 can be zeros, it would make the matrix singular. In order to overcome this difficulty and avoid giving too much importance to the errors on the scarcely observed outcomes, we modify the sample probability and create a new vector p\u0303:\np\u0303 = p\u0302+ 5N 1 + 5dN . (A2)\nThis means that we consider that each outcome has been observed 5 more times than it actually was, and the total number of observations changes from N to N + 5d (the choice of 5 is arbitrary). This is a standard method to make a criterion smoother (see [24]). The resulting estimate of the inverse of the covariance matrix is\n\u03a3\u0303 \u22121 =\n1\np\u0303d 1+ diag(1/p\u0303) (A3)\nWith the inverse of \u03a3\u0303 and knowing that the distribution is normal and centered, we can compute the negative log-likelihood of the vector (see [23]):\nL gauss (x\u2032,y\u2032)(p\u0302) = N\u03b5(p\u0302,x \u2032,y\u2032)T \u03a3\u0303\u22121\u03b5(p\u0302,x\u2032,y\u2032). (A4)\nWe use p\u0302 and not p\u0303 to compute \u03b5 otherwise estimator that minimizes the criterion would become biased (as the minimum of L gauss(x\u2032,y\u2032) would fit p\u0303 which does not contains the actual sample probabilities) and the criterion would not be smoother. Let us simplify this expression using (A3) and the fact that \u2211 k \u03b5k = 0 \u21d2 \u03b5d = \u2212 \u2211d\u22121 k=1 \u03b5k:\nN\u03b5T \u03a3\u0303\u22121\u03b5 = N\u03b5T   1 p\u0303d \u2211d\u22121 k=1 \u03b5k + \u03b51 p\u03031\n... 1 p\u0303d \u2211d\u22121 k=1 \u03b5k + \u03b5d\u22121 p\u0303d\u22121\n \n= N\u03b5T   \u03b51 p\u03031 \u2212 \u03b5dp\u0303d ...\n\u03b5d\u22121 p\u0303d\u22121 \u2212 \u03b5dp\u0303d\n \n= N (\u2211d\u22121\nk=1 \u03b52k p\u0303k \u2212 \u03b5dp\u0303d \u2211d\u22121 k=1 \u03b5k )\n= N \u2211d\nk=1 \u03b52k p\u0303k .\nTherefore, the expression of the negative log-\nlikelihood is:\nL gauss (x\u2032,y\u2032)(p\u0302) = N\nd\u2211\nk=1\n\u03b5k(p\u0302,x \u2032,y\u2032)2\np\u0303k . (A5)\n3. Extension to several d-outcome\nmeasurements\nSince the beginning of the appendix we assumed that only one type of measurement with d outcomes was performed. In practice the methods we describe require either 4 (in Section III) or 2nqb + 1 (in Section IV) types of measurements. The errors between the empirical and theoretical probabilities of different measurements are independent. Therefore if \u03b5(p\u0302,x\u2032,y\u2032) contains nt > 1 types of measurements and dnt real components, then, its covariance matrix is a block diagonal matrix with the covariance matrix of each measurement type on the diagonal (because the measurement errors on two different measurement types are independent.). And the same goes for the inverse of its regularized covariance matrix:\n\u03a3\u0303 \u22121 =   \u03a3\u0303 \u22121 1 . . .\n\u03a3\u0303 \u22121 nt\n  . (A6)\nEach \u03a3\u0303\u22121k is the regularized inverse of the covariance matrix for one measurement type defined in (A3).\nThe negative log-likelihood of \u03b5(p\u0302,x\u2032,y\u2032) containing nprob = ntd measurements errors on nt types of measurements is the sum of the nt negative loglikelihoods of the error vectors of each measurement type\nL gauss (x\u2032,y\u2032)(p\u0302) = N\nnprob\u2211\nk=1\n\u03b5k(p\u0302,x \u2032,y\u2032)2\np\u0303k . (A7)\n[1] M. A. Nielsen and I. L. Chuang, Quantum computation and quantum information (Cambridge University Press, 2000). [2] A. Kalev, R. L. Kosut, and I. H. Deutsch, npj Quantum Information 1, 10.1038/npjqi.2015.18 (2015). [3] A. Smith, C. A. Riofr\u0301\u0131o, B. E. Anderson, H. Sosa-\nMartinez, I. H. Deutsch, and P. S. Jessen, Physical Review A 87, 10.1103/physreva.87.030102 (2013). [4] D. Gross, Y.-K. Liu, S. T. Flammia, S. Becker, and J. Eisert, Physical Review Letters 105, 10.1103/physrevlett.105.150401 (2010). [5] X. Ma, T. Jackson, H. Zhou, J. Chen, D. Lu, M. D.\n18\nMazurek, K. A. G. Fisher, X. Peng, D. Kribs, K. J. Resch, Z. Ji, B. Zeng, and R. Laflamme, Physical Review A 93, 10.1103/physreva.93.032140 (2016). [6] T. Cai, D. Kim, Y. Wang, M. Yuan, and H. H. Zhou, The Annals of Statistics 44, 10.1214/15-aos1382 (2016). [7] Y. Wang, The Annals of Statistics 41, 10.1214/13-aos1156 (2013). [8] C. H. Baldwin, I. H. Deutsch, and A. Kalev, Physical Review A 93, 10.1103/physreva.93.052105 (2016). [9] C. Ferrie, Physical Review Letters 113, 10.1103/physrevlett.113.190404 (2014). [10] R. J. Chapman, C. Ferrie, and A. Peruzzo, Physical Review Letters 117, 10.1103/physrevlett.117.040402 (2016). [11] S. T. Ahmad, A. Farooq, and H. Shin, Scientific Reports 12, 10.1038/s41598-022-09143-7 (2022). [12] J. Cotler and F. Wilczek, Physical Review Letters 124, 10.1103/physrevlett.124.100401 (2020). [13] J. Finkelstein, Physical Review A 70, 10.1103/physreva.70.052107 (2004). [14] T. Heinosaari, L. Mazzarella, and M. M. Wolf, Communications in Mathematical Physics 318, 355 (2013). [15] D. Goyeneche, G. Can\u0303as, S. Etcheverry, E. Go\u0301mez, G. Xavier, G. Lima, and A. Delgado, Physical Review Letters 115, 10.1103/physrevlett.115.090401 (2015). [16] R. Balan, P. Casazza, and D. Edidin, Applied and Computational Harmonic Analysis 20, 345 (2006). [17] I. Waldspurger, A. d\u2019Aspremont, and S. Mallat, Mathematical Programming 149, 47 (2013). [18] A. S. Bandeira, J. Cahill, D. G. Mixon, and A. A. Nelson, Applied and Computational Harmonic Analysis 37, 106 (2014). [19] N. Z. Shor, Soviet Journal of Computer and Systems Sciences 25, 1 (1987). [20] C. G. Broyden, IMA Journal of Applied Mathematics 6, 76 (1970). [21] Z. Hradil, J. R\u030ceha\u0301c\u030cek, J. Fiura\u0301s\u030cek, and M. Jez\u030cek, in Quantum State Estimation (Springer Berlin Heidelberg, 2004) pp. 59\u2013112. [22] F. Verdeil, Y. Deville, and A. Deville, in 2021 IEEE Statistical Signal Processing Workshop (SSP) (IEEE, Rio de Janeiro, Brazil, 2021) pp. 161\u2013165. [23] A. Gut, An Intermediate Course in Probability (Springer, 2009). [24] R. Blume-Kohout, Physical Review Letters 105, 200504 (2010)."
        }
    ],
    "year": 2022
}