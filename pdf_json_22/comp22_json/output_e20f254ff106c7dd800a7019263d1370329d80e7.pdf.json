{
    "abstractText": "Deep reinforcement learning (RL) is a promising approach to solving complex robotics problems. However, the process of learning through trial-and-error interactions is often highly time-consuming, despite recent advancements in RL algorithms. Additionally, the success of RL is critically dependent on how well the reward-shaping function suits the task, which is also time-consuming to design. As agents trained on a variety of robotics problems continue to proliferate, the ability to reuse their valuable learning for new domains becomes increasingly significant. In this paper, we propose a post-hoc technique for policy fusion using Optimal Transport theory as a robust means of consolidating the knowledge of multiple agents that have been trained on distinct scenarios. We further demonstrate that this provides an improved weights initialisation of the neural network policy for learning new tasks, requiring less time and computational resources than either retraining the parent policies or training a new policy from scratch. Ultimately, our results on diverse agents commonly used in deep RL show that specialised knowledge can be unified into a \u201cRenaissance agent\u201d, allowing for quicker learning of new skills.",
    "authors": [
        {
            "affiliations": [],
            "name": "Julia Tan"
        },
        {
            "affiliations": [],
            "name": "Ransalu Senanayake"
        },
        {
            "affiliations": [],
            "name": "Fabio Ramos"
        }
    ],
    "id": "SP:11510a24375af64be04e19e3d7c4d0daec8de33e",
    "references": [
        {
            "authors": [
                "T. Inoue",
                "G.D. Magistris",
                "A. Munawar",
                "T. Yokoya",
                "R. Tachibana"
            ],
            "title": "Deep reinforcement learning for high precision assembly tasks",
            "venue": "2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 819\u2013825, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "P.M. Scheikl",
                "B. Gyenes",
                "T. Davitashvili",
                "R. Younis",
                "A. Schulze",
                "B.P. M\u00fcller-Stich",
                "G. Neumann",
                "M. Wagner",
                "F. Mathis-Ullrich"
            ],
            "title": "Cooperative assistance in robotic surgery through multi-agent reinforcement learning",
            "venue": "2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 1859\u20131864, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "F. Yandun",
                "T. Parhar",
                "A. Silwal",
                "D. Clifford",
                "Z. Yuan",
                "G. Levine",
                "S. Yaroshenko",
                "G. Kantor"
            ],
            "title": "Reaching pruning locations in a vine using a deep reinforcement learning policy",
            "venue": "2021 IEEE International Conference on Robotics and Automation (ICRA), pp. 2400\u20132406, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "G. Kahn",
                "A. Villaflor",
                "B. Ding",
                "P. Abbeel",
                "S. Levine"
            ],
            "title": "Selfsupervised deep reinforcement learning with generalized computation graphs for robot navigation",
            "venue": "2018 IEEE International Conference on Robotics and Automation (ICRA), pp. 5129\u20135136, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "H.F. Durrant-Whyte"
            ],
            "title": "Sensor models and multisensor integration",
            "venue": "Autonomous robot vehicles, pp. 73\u201389, Springer, 1990.",
            "year": 1990
        },
        {
            "authors": [
                "P.K. Sharma",
                "R. Fernandez",
                "E. Zaroukian",
                "M. Dorothy",
                "A. Basak",
                "D.E. Asher"
            ],
            "title": "Survey of recent multi-agent reinforcement learning algorithms utilizing centralized training",
            "venue": "Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications III, vol. 11746, p. 117462K, International Society for Optics and Photonics, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "M.B. Alatise",
                "G.P. Hancke"
            ],
            "title": "A review on challenges of autonomous mobile robot and sensor fusion methods",
            "venue": "IEEE Access, vol. 8, pp. 39830\u201339846, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "D. Novak",
                "R. Riener"
            ],
            "title": "A survey of sensor fusion methods in wearable robotics",
            "venue": "Robotics and Autonomous Systems, vol. 73, pp. 155\u2013 170, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "M. Guo",
                "M.M. Zavlanos"
            ],
            "title": "Multirobot data gathering under buffer constraints and intermittent communication",
            "venue": "IEEE transactions on robotics, vol. 34, no. 4, pp. 1082\u20131097, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "W. Zhi",
                "L. Ott",
                "R. Senanayake",
                "F. Ramos"
            ],
            "title": "Continuous occupancy map fusion with fast bayesian hilbert maps",
            "venue": "2019 International Conference on Robotics and Automation (ICRA), pp. 4111\u20134117, IEEE, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "D. Wolpert"
            ],
            "title": "Stacked generalization",
            "venue": "Neural Networks, vol. 5, 12 1992.",
            "year": 1992
        },
        {
            "authors": [
                "R.E. Schapire"
            ],
            "title": "A brief introduction to boosting",
            "venue": "1999 International Joint Conference on Artificial Intelligence (IJCAI), Morgan Kaufmann Publishers Inc., 1999.",
            "year": 1999
        },
        {
            "authors": [
                "J. Schmidhuber"
            ],
            "title": "Learning complex, extended sequences using the principle of history compression",
            "venue": "Neural Computation, vol. 4, no. 2, pp. 234\u2013242, 1992.",
            "year": 1992
        },
        {
            "authors": [
                "J. Utans"
            ],
            "title": "Weight averaging for neural networks and local resampling schemes",
            "venue": "Proc. AAAI-96 Workshop on Integrating Multiple Learned Models, AAAI Press, 1996.",
            "year": 1996
        },
        {
            "authors": [
                "M. Yurochkin",
                "M. Agarwal",
                "S.S. Ghosh",
                "K.H. Greenewald",
                "T.N. Hoang",
                "Y. Khazaeni"
            ],
            "title": "Bayesian nonparametric federated learning of neural networks",
            "venue": "2019 International Conference on Machine Learning (ICML), 2019.",
            "year": 2019
        },
        {
            "authors": [
                "S.P. Singh",
                "M. Jaggi"
            ],
            "title": "Model fusion via optimal transport",
            "venue": "2020 Conference on Neural Information Processing Systems (NeurIPS), 2020.",
            "year": 2020
        },
        {
            "authors": [
                "V.M. Dax",
                "M. Kochenderfer",
                "R. Senanayake",
                "U. Ibrahim"
            ],
            "title": "Infrastructure-enabled autonomy: An attention mechanism for occlusion handling",
            "venue": "International Conference on Robotics and Automation (ICRA), 2022.",
            "year": 2022
        },
        {
            "authors": [
                "K. Rana",
                "V. Dasagi",
                "J. Haviland",
                "B. Talbot",
                "M. Milford",
                "N. S\u00fcnderhauf"
            ],
            "title": "Bayesian controller fusion: Leveraging control priors in deep reinforcement learning for robotics",
            "venue": "arXiv preprint arXiv:2107.09822, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "J. Schulman",
                "F. Wolski",
                "P. Dhariwal",
                "A. Radford",
                "O. Klimov"
            ],
            "title": "Proximal policy optimization algorithms",
            "venue": "Computing Research Repository (CoRR), vol. abs/1707.06347, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "D. Silver",
                "G. Lever",
                "N. Heess",
                "T. Degris",
                "D. Wierstra",
                "M. Riedmiller"
            ],
            "title": "Deterministic policy gradient algorithms",
            "venue": "Proceedings of the 31st International Conference on International Conference on Machine Learning - Volume 32, ICML, p. I\u2013387\u2013I\u2013395, JMLR.org, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "A. Tompkins",
                "R. Senanayake",
                "F. Ramos"
            ],
            "title": "Online domain adaptation for occupancy mapping",
            "venue": "Robotics: Science and Systems (RSS), 2020.",
            "year": 2020
        },
        {
            "authors": [
                "G. Peyr\u00e9",
                "M. Cuturi"
            ],
            "title": "Computational optimal transport: With applications to data science",
            "venue": "Foundations and Trends in Machine Learning, vol. 11, no. 5-6, pp. 355\u2013607, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "N. Bonneel",
                "G. Peyr\u00e9",
                "M. Cuturi"
            ],
            "title": "Wasserstein barycentric coordinates: Histogram regression using optimal transport",
            "venue": "ACM Trans. Graph., vol. 35, July 2016.",
            "year": 2016
        },
        {
            "authors": [
                "S.T. Rachev"
            ],
            "title": "The monge\u2013kantorovich mass transference problem and its stochastic applications",
            "venue": "Theory of Probability & Its Applications, vol. 29, no. 4, pp. 647\u2013676, 1985.",
            "year": 1985
        },
        {
            "authors": [
                "W. Li",
                "S. Osher",
                "W. Gangbo"
            ],
            "title": "A fast algorithm for earth mover\u2019s distance based on optimal transport and l1 type regularization",
            "venue": "2016.",
            "year": 2016
        },
        {
            "authors": [
                "Y. LeCun",
                "C. Cortes"
            ],
            "title": "MNIST handwritten digit database",
            "venue": "2010.",
            "year": 2010
        },
        {
            "authors": [
                "G. Brockman",
                "V. Cheung",
                "L. Pettersson",
                "J. Schneider",
                "J. Schulman",
                "J. Tang",
                "W. Zaremba"
            ],
            "title": "Openai gym",
            "venue": "2016. cite arxiv:1606.01540.",
            "year": 2016
        },
        {
            "authors": [
                "B. Ellenberger"
            ],
            "title": "Pybullet gymperium.",
            "venue": "https://github.com/ benelot/pybullet-gym,",
            "year": 2018
        },
        {
            "authors": [
                "E. Todorov",
                "T. Erez",
                "Y. Tassa"
            ],
            "title": "Mujoco: A physics engine for model-based control",
            "venue": "2012 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 5026\u20135033, IEEE, 2012.",
            "year": 2012
        }
    ],
    "sections": [
        {
            "text": "I. INTRODUCTION\nIntelligent systems are increasingly penetrating into realworld applications, incurring the need for sophisticated models that can perform complex and adaptive tasks. Deep neural networks are already ubiquitous in solving classification problems in many fields of interest. Recent advancements in reinforcement learning (RL) have extended the capacity of neural networks into the physical domain, allowing for intelligent control of robotic agents in a dynamic environment. Accordingly, RL in robotics has the potential to advance key industries, from manufacturing [1] to healthcare [2], agriculture [3] or transportation [4].\nWhile RL is progressively being used in a range of robotics problems, most real-world applications require numerous specialised capabilities. The computational resource constraints associated with sample-inefficient training, combined with the complexity of re-designing the appropriate reward structures for each capability, make it difficult to achieve solutions for varying goals in a reasonable timeframe. Traditional methods of transfer learning are also inadequate, without a robust way to determine which parts of a policy are still relevant in the new context. Even with targeted policy reuse, the knowledge of only one agent can be leveraged. This is likely not sufficient for most cases, since small differences in the\n*This work was not supported by any organization 1Julia Tan is with the School of Computer Science, University of Sydney, Sydney, Australia jtan5838@uni.sydney.edu.au 2Ransalu Senanayake is with the Dept. of Computer Science, Stanford University, Stanford, CA 94305, USA ransalu@cs.stanford.edu 3Fabio Ramos is with NVIDIA, USA and School of Computer Science, University of Sydney, Australia ftozetoramos@nvidia.com\nenvironment or the kinematics of the robotic agent itself can drastically change the rewards of previously familiar actions. Consequently, we require a minimal dependency fusion method that can effectively consolidate the learned behaviours of multiple parent policies.\nIn this paper, we present a technique to fuse RL policies represented as neural networks using Optimal Transport (OT), and propose the fused policy as a superior initialisation of weights for faster learning of diverse skills. With reference to the Renaissance man, we call this fusion result a \u201cRenaissance agent\u201d.\nIn particular, our contributions are:\n1) A method based on OT for efficiently combining RL policies post-hoc, requiring no awareness of the original data used to train them;\n2) A demonstration that this OT fusion approach generalises the prior knowledge of parent policies over different environments as well as different robot kinematics, and\n3) Validation that the Renaissance agent learns new skills more effectively and within a shorter time period than either retraining the parent agents or training a new agent from scratch.\nar X\niv :2\n20 7.\n00 97\n8v 1\n[ cs\n.L G\n] 3\nJ ul\n2 02\n2"
        },
        {
            "heading": "II. RELATED WORK",
            "text": "Information fusion is a widely studied topic in robotics for decades [5]. As opposed to the common paradigm of controlling multiple robots using a centralised controller [6] or fusing data from various sources [7], our objective is to gather data from individually operating robots each with a different skill to train a single robot that possesses diverse skills appropriately learned from all other robots by fusing knowledge. Roboticists have attempted to perform knowledge fusion at the perception stage or decision-making stage of the robot autonomy stack. Most research studies in the perception level focuses on either fusing data from multiple sensors or at the perception algorithm level. The former, sensor fusion, is a well-studied topic in robotics [5], [7] and has applications ranging from field robotics [5], [7] to wearable sensors [8]. There are also studies on aggregating data from multiple robots [9] as the simplest fusion technique. Some of the main limitations of such aggregation methods include the requirement of saving the ever growing dataset and training the model online.\nWhile there are many techniques for data fusion, fewer exist for fusing learned perception models. This is because it is more difficult to determine how and what to fuse for neural networks. Zhi et al. [10] has proposed a technique to fuse Bayesian Hilbert maps from data gathered from various agents. However, it is not clear how the proposed occupancy map fusion technique can be used when the underlying model is a neural network.\nResearchers have studied model fusion techniques for various applications beyond robotics. The most common technique, ensembling [11], [12] requires all models to be running concurrently, which is computationally expensive. Moreover, it does not actually aggregate each model\u2019s distinct capabilities, since decision making occurs separately. Hence, it is not suitable for adapting to customised tasks, especially when the learned behaviours of the parent models differ significantly. In contrast, there are models that result in a single model by transferring knowledge from a teacher network [13]. Whilst there are also techniques [14] that yield a combined model without the need for retraining, they also do not perform empirically better than naive averaging. More recently, in federated learning settings, aligning neural networks has been studied [15]. In such frameworks, the number of parameters required grows with the size of the data, so it is infeasible for fusing large data models ubiquitous in robotics. More recently, others have used OT theory to fuse neural network models [16], although this has not yet been applied to RL or decision-making. When fusing robots, it is ideal if we can make them as agnostic as possible to sensory settings because the ultimate goal of a robot is to maneuver safely and efficiently.\nOn the other end of the fusion spectrum, it is possible to combine decision-making modules [17] or their outputs, though the latter, by construction, is ill-informed about data and typically produces less accurate results. Researchers have investigated how to combine the outputs of a traditional hand-\ncrafted controller with a deep RL controller using Bayesian inference [18]. However, it is not clear how to combine two or more neural network controllers, especially at the decisionmaking stage rather than once the outputs are produced. In our work, we fuse architecturally similar deep RL controllers that possess different skills. Though Dax et al. [17] also discusses sensor selection and decision fusion, their proposed model also does not represent policies using neural networks."
        },
        {
            "heading": "III. OPTIMAL TRANSPORT POLICY FUSION",
            "text": ""
        },
        {
            "heading": "A. Policy Networks in RL",
            "text": "In RL, an agent is informed by a policy \u03c0 : A\u00d7S \u2192 [0, 1] that determines what actions A to perform given the current states S . A policy \u03c0E\u03b8 , trained in domain E , can be represented as a neural network parameterized by \u03b8. When training a policy, an RL algorithm such as Proximal Policy Optimisation (PPO) [19] or Deep Deterministic Policy Gradient (DDPG) [20] is used to update the weights of the neural network so that the cumulative reward is maximised over time. Depending on the task configuration during training, these learned weights will differ between agents. However, for policy networks that have the same input features, similar weight vectors at each individual layer may represent common learned patterns. Therefore, we wish to find an efficient way to consolidate the common patterns while diminishing any over-optimised patterns, so that we can generate a Renaissance agent that is capable of leveraging prior knowledge for new domains.\nFor two policy networks, \u03c0E1\u03b81 , \u03c0 E2 \u03b82\n, trained independently in domains, E1, E2, respectively, our objective is to determine the best fusion operation c : \u03a0E1 \u00d7 \u03a0E2 \u2192 \u03a0E\u2217 for the policies \u03c0E \u2208 \u03a0E and new domain E\u2217 such that empirically r(\u03c0E\u2217\u03b8\u2217 ) > r(\u03c0 E\u2217 \u03b81 ), r(\u03c0E\u2217\u03b82 ), where r(\u03c0 E) is the reward when policy \u03c0 is deployed in domain E . For instance, naive policy averaging c(\u03c0E1\u03b81 , \u03c0 E2 \u03b82 ) = (\u03c0E1\u03b81 + \u03c0 E2 \u03b82\n)/2 is a simple fusion operation. However, it is ineffective since it does not find the optimal alignment between the parameter vectors of the policies. In this paper, we propose that OT theory is apt for finding an alignment map between the weights of two policy networks. This enables us to match the weights that are most likely to represent common patterns."
        },
        {
            "heading": "B. Optimal Transport Theory",
            "text": "1) Monge-Kantorovich Problem: OT theory has previously been applied to mapping problems in order to allow robots to operate in new environments [21]. However, in this work, we utilise OT for policy fusion so that robotic agents can consolidate learned behaviours for new domains, including new environments as well as new kinematics. Typically, OT is used to transform a single source distribution S to a target distribution T [22]. When applied to the incoming weight vectors of a policy network, OT represents them as distributions of Dirac \u201cmasses\u201d [23]. The \u201cdistance\u201d between any two distributions is then quantified by the least costly way of transporting each mass from S to T .\nAccording to the Monge-Kantorovich Problem [24], for the datasets \u00b5(S) and \u00b5(T ) of size NS and size NT on independent metric spaces \u2126(S) and \u2126(T ), there exists an\noptimal probabilistic coupling P\u2217 \u2208 RN (S)\u00d7N(T ) between the two datasets. This optimal coupling always exists for a distance function D : \u2126(S) \u00d7 \u2126(T ) \u2192 [0,\u221e), and is defined by,\nP\u2217 = arginf P\u2208\u0393 \u222b \u2126(S)\u00d7\u2126(T ) D(\u00b5(S),\u00b5(T ))dP (\u00b5(S),\u00b5(T )). (1)\n2) Earth Mover\u2019s Distance: The minimum cost of transporting each mass from S to T is based on the Wasserstein metric, also known as the Earth Mover\u2019s Distance [25],\nEMD(S, T ) = \u03a3mi=1\u03a3 n j=1P\u2217i,jdi,j\n\u03a3mi=1\u03a3 n j=1P\u2217i,j\n, (2)\nwhere P\u2217 is the optimal coupling and di,j is the ground distance between distributions Si and Tj . We can formulate OT as an alignment problem by considering \u201csimilarity\u201d as inversely proportional to the Earth Mover\u2019s Distance. By computing the optimal coupling P\u2217 between two sets of weight vectors (each representing neurons from a different policy network), we are able to match the second set as the source distribution S to the first set as the target distribution T by taking the dot product,\nSaligned = P\u2217 \u00b7 T . (3)"
        },
        {
            "heading": "C. Neural Matching and Weights Fusion",
            "text": "In order to fuse two parent policies parameterised by neural networks, we first align their neurons layer-wise so that the associated weight vectors are matched. Given that the input layers of each model receive the exact same format of input vectors, we begin neural matching from the incoming weights to the first hidden layer. As we do not know explicitly what each neuron has been trained to identify, we can pair them together based only on the \u201csimilarity\u201d of the incoming weights.\nA one-to-one similarity is computed by minimising the Earth Mover\u2019s Distance between the weight vectors in one model and the weight vectors in the other model as probability distributions. Hence, the optimal coupling P\u2217 represents the alignment matrix for re-positioning the weight vectors in the second model (i.e. the source distribution S) according to the weight vectors in the first model (i.e. the target distribution T ). Taking the dot product of P\u2217 and T yields the aligned S.\nTo perform neural matching in the subsequent layer, we must start with rearranging the elements of each weight vector to correspond with the new positions of the neurons in the previous layer (after alignment). For the current layer, we then compute the OT map as before and align the weight vectors in the second model to the weight vectors in the first model. Algorithm 1 outlines this process. Once every layer has been aligned, we then fuse the matched weights across the two policies by averaging the values. This keeps the weights with similar matches close to the same value, while changing those that are dissimilar and therefore unlikely to represent common patterns. Thus, we build the fused policy layer-by-layer, as shown in Figure 2.\nAlgorithm 1 align neurons Input: weights of N -layer parent policies for i = 2 to N do S \u2190 incoming weights to i-th layer of second policy T \u2190 incoming weights to i-th layer of first policy if i > 2 then\nRearrange S to match positions of previous layer alignment end if Compute OT map P\u2217, according to Equation 1 Align source weights to target weights using P\u2217, according to Equation 3\nend for Output: aligned weights of N -layer parent policies"
        },
        {
            "heading": "IV. EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "A. Proof of Concept: Image Classification Task",
            "text": "1) Experimental Setup: To validate the theoretical concept behind the proposed OT fusion method, we first test it on a image classification task before extending to robotics tasks in RL. The task is constructed using the MNIST handwritten digits dataset [26]. We independently train two parent models to be binary classifiers on the MNIST dataset. The first model classifies 0s versus all other digits, while the second model classifies 1s versus all other digits. Hence, we modify two distinct training datasets, where the class labels in each correspond to either \u201cyes\u201d for the digit to be identified or \u201cno\u201d for all other digits.\nThere are originally 60,000 images in the training dataset. However, since the original MNIST classification problem is to identify each digit from 0 to 9, we need to rebalance the dataset in order to achieve good training outcomes for the binary classifiers. Otherwise, the models will be biased towards the class \u201cno\u201d, since the majority of training examples are not the digit to be identified. We use a random undersampler to rebalance the dataset so that the majority class (i.e. \u201cno\u201d) will have twice as many samples as the minority class (i.e. \u201cyes\u201d).\nEach parent model will have \u201cfully-connected\u201d layers according to the neural network architecture shown in Table I. Both models will be trained for 10 epochs on batch sizes of 128 with a 10% validation split. Evaluating on the testing dataset of 10,000 examples (with the labels similarly modified), we get the test accuracy and test loss of each model shown in Table II.\nWe now reformulate the problem domain so that neither one of the parent models are able to perform well without retraining. The new task that we aim to solve is classifying both 0s and 1s versus all other digits. We fuse the parent models by applying the OT technique explained in subsection III-C.\n2) Results: Without additional retraining, the fused model is immediately able to classify some digits correctly but not all. An example subset of the fused model\u2019s predictions is shown in Figure 3.\nThe fused model performs significantly better after retraining, correctly classifying most examples. This is evident from the confusion matrix in Figure 4, showing that the fused model mainly predicts true positives and true negatives.\nTo evaluate the effectiveness of the fused model, we compare it with retraining the parent models as well as training a new model from scratch. The training accuracy and validation accuracy for each of these models over 10 training epochs are shown in Figure 6. It is evident that the fused model reaches high consistent performance much faster than the other models, surpassing a training accuracy of 95% and a validation accuracy of 90% after only 2 epochs. In contrast, it takes at least 4 epochs for the next best model to converge to similar performance.\nMoreover, the final performance of the fused model remains visibly higher than the other models after all 10 epochs have finished. From Table III, we can see that the fused model exhibits the highest final test accuracy as well as the lowest final test loss, as evaluated on the testing dataset of 10,000 examples."
        },
        {
            "heading": "B. Experimental Setup: RL Tasks",
            "text": "To test the effectiveness of the OT technique for policy fusion, we construct four separate RL robotics tasks using OpenAI Gym [27]: Reacher, Walker2d, HalfCheetah and FetchReach. The first three tasks are simulated using PyBullet [28] while the last task is simulated using Mujoco [29]. Example renders are shown in Figure 5. For each task, the policy of the robotic agent is represented as a Multilayer Perceptron (MLP) with 2 hidden layers of 64 nodes each. The size of the input and output layers differ across the tasks according to Table I, since the environments have varying observation and action spaces. All policies will be trained using the PPO algorithm.\nTo test each task, we will first independently train two robotic agents on different configurations of the task. Both agents will be trained for 500,000 timesteps. The mean episode rewards, as evaluated on 10 separate and randomly generated environments, are included in Table IV. These agents will act as the \u201cparents\u201d of the Renaissance agent, which we will generate via the OT policy fusion method. Then, we will refine the Renaissance agent on a new domain, and compare its performance against retraining the parent agents and training a brand new agent over the same time period. To provide an additional benchmark for OT, we will also compare it against naive averaging as an alternative method of fusing policy weights."
        },
        {
            "heading": "C. Generalising over Changes in the Environment",
            "text": "1) Experimental Setup: The learned behaviour of a robotic agent is highly sensitive to changes in the environment, particularly when such changes impact how rewards are earned. To demonstrate that the OT fusion technique is able to generalise the knowledge of two agents trained in differing environment configurations, we construct a simple RL task using the Gym environment Reacher. This task involves a two-link robotic arm as the agent, which operates within a two-dimensional workspace. The goal is for the end-effector\nto reach to a randomly located target position, with the agent gaining 0.1 rewards for each timestep that it does so. The input layer of the agent\u2019s policy network takes in the angular positions and velocities of the robotic arm, and the output layer defines the motor torque of the central joint and the elbow joint.\nThe first policy learns to reach to targets randomly located in the first quadrant of the workspace, while the second policy learns to reach to targets randomly located in the second quadrant of the workspace. We now reformulate the problem scope to targets that are placed randomly anywhere within the first and second quadrants. Since the required kinematics of the two-link robotic arm reaching to targets in each quadrant differ significantly, the parent policies have limited capabilities in this new domain. We fuse the parent policies using the OT technique to produce an initialisation of weights for the Renaissance agent.\n2) Results: After retraining on the new domain for 500,000 timesteps, the fused agent exhibits consistent performance. Renders of the fused agent over a random evaluation episode are shown in Figure 7.\nTABLE IV: Mean episode rewards for each trained parent policy over 10 evaluation episodes.\nParent Policy Reacher Task Walker2d Task HalfCheetah Task FetchReach Task\nFirst 20.10 \u00b1 6.80 660.80 \u00b1 420.96 2258.43 \u00b1 642.28 -2.69 \u00b1 0.19 Second 19.57 \u00b1 10.06 728.28 \u00b1 340.60 2013.14 \u00b1 8.02 -1.48 \u00b1 0.07\nThe mean episode rewards for the fused agent, compared with the other benchmark agents, are shown in Figure 8. It is evident that the fused agent achieves high consistent performance much faster than the rest, surpassing 15 rewards per episode after 200,000 timesteps. In contrast, it takes approximately 350,000 timesteps for the next best agent to converge to similar performance. Upon completion of the retraining period, we can see from Table V that the fused agent exhibits the highest mean reward as well as the lowest standard deviation, as tested over 10 random evaluation episodes."
        },
        {
            "heading": "D. Generalising over Changes in Robot Kinematics",
            "text": "1) Experimental Setup: Agents with different robot kinematics will learn different behaviours to reach the desired goal, even if the environment is the same. To demonstrate that the Renaissance agent generalises over different robot kinematics, we construct two RL tasks using the Gym environments Walker2d and HalfCheetah.\n2) Walker2d: The Walker2d task involves a bipedal robot as the agent, which operates within a 2D plane (i.e. can only move forwards or backwards). The goal is for the agent to walk forward as fast as possible, with the agent gaining rewards as a function of the current forward velocity, plus a constant bonus for not falling over. The input layer of the agent\u2019s policy network takes in the angular positions and velocities of the joints, and the output layer defines the signals to move the torso-connected foot and the left foot.\nThe first agent is trained with a more powerful torsoconnected foot (coefficient = 40) compared to its left foot (coefficient = 10), while the second agent is trained with a more powerful left foot (coefficient = 25) compared to its torso-connected foot (coefficient = 20). We now fuse the parent policies and reformulate the target domain to learning the Walker2d task on an agent with equally powerful torsoconnected and left feet (coefficient = 30).\nAfter retraining on the new domain for 500,000 timesteps, the fused agent is able to walk forward in a natural-looking manner. Renders of the agent over a random evaluation episode are shown in Figure 9.\nFig. 9: Renders of fused agent after retraining on new Walker2d task.\nThe mean episode rewards for the fused agent, compared with the other benchmark agents, are shown in Figure 10. We can see that the fused agent is the only one that achieves high consistent performance, surpassing 1000 rewards per episode by the end of the retraining period. From Table V, it is evident that the fused agent exhibits the highest minimum, maximum and mean reward, as tested over 10 random evaluation episodes.\n3) HalfCheetah: We further demonstrate the ability of OT policy fusion to generalise over different robot kinematics by extending to a more complex agent. The HalfCheetah task involves a quadruped robot as the agent, which also moves only forwards or backwards within a 2D plane. Similar to the Walker2d task, the goal is for the agent to run forward as fast as possible. The agent\u2019s policy network determines how much torque to apply on its thighs, shins and feet.\nWe train the first parent policy on an agent with greater thigh and shin power, but reduced foot power. This results in more rigid foot joints. We then train the second parent policy on an agent with the opposite kinematics, resulting in more rigid thigh and shin joints. Similar to the Walker2d task, we now fuse the policies and change the target domain to learning the HalfCheetah task on an agent with neutral power for all joints. This should allow the agent to learn joint movements that are less rigid.\nAfter retraining on the new domain for 500,000 timesteps, the fused agent is able to run forward without any of the joint\nrigidity displayed by the parent agents. Renders of the agent over a random evaluation episode are shown in Figure 11.\nThe mean episode rewards for the fused agent, compared with the other benchmark agents, are shown in Figure 12. It is evident that the fused agent learns much faster than the new agent, and overtakes the parent agents by the end of the retraining period. From Table V, we can further observe that the fused agent exhibits the highest mean reward and lowest standard deviation, as tested over 10 random evaluation episodes.\nAdditionally, the retrained parent agents still exhibit very similar kinematics to what they learned from their original training domain. In contrast, both the fused agent and the new agent display the desired kinematics of the new domain, which has less joint rigidity. This demonstrates how the OT fusion technique is useful for consolidating the common knowledge of parent models which have been over-optimised to a previous task."
        },
        {
            "heading": "E. Evaluating on Robotic Arm Task",
            "text": "1) Experimental Setup: The \u201cFetch\u201d Gym environments are based on the 7-DoF Fetch Robotics arm. This provides a more complex agent, observation and action space compared to the Reacher task shown in subsection IV-C. We extend that task by using the Gym environment FetchReach.\nThe FetchReach task involves the Fetch arm as the agent, which operates within a 3D space. The goal is for the agent to move its end-effector to target positions randomly located above a workspace. Rewards are binary: at each timestep, the agent obtains a reward of 0 if its end-effector is at the target position, and -1 otherwise. The agent\u2019s policy network takes in the Cartesian positions and linear velocities of the arm links and gripper as observations, and outputs the desired movements in Cartesian coordinates as actions.\nIn the previous 2-link Reacher task, we set the target locations to be within the first or second quadrants of the workspace. However, since the Fetch robot stands above the workspace and moves in 3D, these existing problem formulations are not different enough to ensure that the parent policies will learn distinct behaviours. Instead, the first policy learns to move the end-effector to target positions located over the bottom left of the workspace, while the second policy learns for the top right of the workspace. We now fuse the policies and change the problem scope to target positions that are located randomly anywhere over the workspace, excluding the corners.\n2) Results: After retraining on the new domain for 200,000 timesteps, the fused agent is able to quickly and consistently\nmove its end-effector to target positions located anywhere over the central area of the workspace. Renders of the agent over a random evaluation episode are shown in Figure 13.\nThe mean episode rewards for the fused agent, compared with the other benchmark agents, are shown in Figure 14. It is evident that the fused agent achieves high consistent performance much faster than the rest, surpassing -2.5 rewards per episode by 25,000 timesteps. In contrast, it takes approximately 100,000 timesteps for the next best agent to converge to similar performance. From Table V, we can see that the fused agent achieves the highest mean reward as well as the lowest standard deviation, as tested over 10 random evaluation episodes."
        },
        {
            "heading": "V. CONCLUSION",
            "text": "In this work, we developed policy fusion utilising OT theory to create Renaissance agents. We demonstrated that with minimal retraining, the Renaissance agents can quickly learn a new domain by leveraging generalised prior knowledge. In contrast, the parent agents are often over-optimised to their original tasks, and thus less adept at learning diverse skills in a variety of common RL robotics problems. Ultimately, the fused agents were empirically demonstrated to learn new domains significantly faster and achieve superior sustained performance compared with other benchmarks. This is especially valuable for RL in robotics since training is highly time-consuming and resource expensive. In light of the simulation-to-simulation examples discussed in this work, applying the proposed technique to simulation-to-real policy fusion is a promising future direction."
        },
        {
            "heading": "ACKNOWLEDGMENT",
            "text": "We thank Anthony Tompkins for early discussions and\nideas, especially on Optimal Transport theory."
        }
    ],
    "title": "Renaissance Robot: Optimal Transport Policy Fusion for Learning Diverse Skills",
    "year": 2022
}