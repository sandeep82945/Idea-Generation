{
    "abstractText": "In this paper, we discuss the statistical properties of the lq optimization methods (0 < q \u2264 1), including the lq minimization method and the lq regularization method, for estimating a sparse parameter from noisy observations in high-dimensional linear regression with either a deterministic or random design. For this purpose, we introduce a general q-restricted eigenvalue condition (REC) and provide its sufficient conditions in terms of several widely-used regularity conditions such as sparse eigenvalue condition, restricted isometry property, and mutual incoherence property. By virtue of the q-REC, we exhibit the stable recovery property of the lq optimization methods for either deterministic or random designs by showing that the l2 recovery bound O(\u03b5 ) for the lq minimization method and the oracle inequality and l2 recovery bound O(\u03bb 2 2\u2212q s) for the lq regularization method hold respectively with high probability. The results in this paper are nonasymptotic. The numerical results verify the established statistical property and demonstrate the advantages of the lq regularization method over the classical Lasso.",
    "authors": [
        {
            "affiliations": [],
            "name": "Xin Li"
        },
        {
            "affiliations": [],
            "name": "Yaohua Hu"
        },
        {
            "affiliations": [],
            "name": "Chong Li"
        },
        {
            "affiliations": [],
            "name": "Xiaoqi Yang"
        },
        {
            "affiliations": [],
            "name": "Tianzi Jiang"
        }
    ],
    "id": "SP:e786157ec94d771e26b8e4ac756d820491ec2af1",
    "references": [
        {
            "authors": [
                "A. Agarwal",
                "S. Negahban",
                "M.J. Wainwright"
            ],
            "title": "Fast global convergence of gradient methods for high-dimensional statistical recovery",
            "venue": "Annals of Statistics,",
            "year": 2012
        },
        {
            "authors": [
                "S. Aronoff"
            ],
            "title": "Remote Sensing for GIS Managers",
            "venue": "Environmental Systems Research, Redlands,",
            "year": 2004
        },
        {
            "authors": [
                "A. Beck",
                "M. Teboulle"
            ],
            "title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems",
            "venue": "SIAM Journal on Imaging Sciences,",
            "year": 2009
        },
        {
            "authors": [
                "P.J. Bickel",
                "Y. Ritov",
                "A.B. Tsybakov"
            ],
            "title": "Simultaneous analysis of Lasso and Dantzig selector",
            "venue": "Annals of Statistics,",
            "year": 2009
        },
        {
            "authors": [
                "T. Blumensath",
                "M.E. Davies"
            ],
            "title": "Iterative thresholding for sparse approximations",
            "venue": "Journal of Fourier Analysis and Applications,",
            "year": 2008
        },
        {
            "authors": [
                "F. Bunea",
                "A. Tsybakov",
                "M. Wegkamp"
            ],
            "title": "Sparsity oracle inequalities for the Lasso",
            "venue": "Electronic Journal of Statistics,",
            "year": 2007
        },
        {
            "authors": [
                "T.T. Cai",
                "G.W. Xu",
                "J. Zhang"
            ],
            "title": "On recovery of sparse signals via l1 minimization",
            "venue": "IEEE Transactions on Information Theory,",
            "year": 2009
        },
        {
            "authors": [
                "E.J. Cand\u00e8s",
                "J.K. Romberg",
                "T. Tao"
            ],
            "title": "Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information",
            "venue": "IEEE Transactions on Information Theory,",
            "year": 2006
        },
        {
            "authors": [
                "E.J. Cand\u00e8s",
                "J.K. Romberg",
                "T. Tao"
            ],
            "title": "Stable signal recovery from incomplete and inaccurate measurements",
            "venue": "Communications on Pure and Applied Mathematics,",
            "year": 2006
        },
        {
            "authors": [
                "E.J. Cand\u00e8s",
                "T. Tao"
            ],
            "title": "Decoding by linear programming",
            "venue": "IEEE Transactions on Information Theory,",
            "year": 2005
        },
        {
            "authors": [
                "E.J. Cand\u00e8s",
                "M.B. Wakin",
                "S.P. Boyd"
            ],
            "title": "Enhancing sparsity by reweighted l1 minimization",
            "venue": "Journal of Fourier Analysis and Applications,",
            "year": 2008
        },
        {
            "authors": [
                "R. Chartrand"
            ],
            "title": "Exact reconstruction of sparse signals via nonconvex minimization",
            "venue": "IEEE Signal Processing Letters,",
            "year": 2007
        },
        {
            "authors": [
                "R. Chartrand",
                "W.T. Yin"
            ],
            "title": "Iteratively reweighted algorithms for compressive sensing",
            "venue": "In IEEE International Conference on Acoustics,",
            "year": 2008
        },
        {
            "authors": [
                "S.S. Chen",
                "D.L. Donoho",
                "M.A. Saunders"
            ],
            "title": "Atomic decomposition by basis pursuit",
            "venue": "SIAM Review,",
            "year": 2001
        },
        {
            "authors": [
                "I. Daubechies",
                "R. Devore",
                "M. Fornasier"
            ],
            "title": "Iteratively reweighted least squares minimization for sparse recovery",
            "venue": "Communications on Pure and Applied Mathematics,",
            "year": 2010
        },
        {
            "authors": [
                "Z.L. Dong",
                "X.Q. Yang",
                "Y.H. Dai"
            ],
            "title": "A unified recovery bound estimation for noise-aware lq optimization model in compressed sensing",
            "venue": "arXiv preprint arXiv:1609.01531,",
            "year": 2016
        },
        {
            "authors": [
                "D.L. Donoho"
            ],
            "title": "For most large underdetermined systems of linear equations the minimal l1-norm solution is also the sparsest solution",
            "venue": "Communications on Pure and Applied Mathematics,",
            "year": 2006
        },
        {
            "authors": [
                "D.L. Donoho",
                "M. Elad",
                "V.N. Temlyakov"
            ],
            "title": "Stable recovery of sparse overcomplete representations in the presence of noise",
            "venue": "IEEE Transactions on Information Theory,",
            "year": 2006
        },
        {
            "authors": [
                "D. L Donoho",
                "X.M. Huo"
            ],
            "title": "Uncertainty principles and ideal atomic decomposition",
            "venue": "IEEE Transactions on Information Theory,",
            "year": 2001
        },
        {
            "authors": [
                "J.Q. Fan",
                "R.Z. Li"
            ],
            "title": "Variable selection via nonconcave penalized likelihood and its oracle properties",
            "venue": "Journal of the American statistical Association,",
            "year": 2001
        },
        {
            "authors": [
                "M. A T Figueiredo",
                "R.D. Nowak",
                "S.J. Wright"
            ],
            "title": "Gradient projection for sparse reconstruction: Application to compressed sensing and other inverse problems",
            "venue": "IEEE Journal of Selected Topics in Signal Processing,",
            "year": 2007
        },
        {
            "authors": [
                "S. Foucart",
                "M.-J. Lai"
            ],
            "title": "Sparsest solutions of underdetermined linear systems via lq-minimization for 0 < q < 1",
            "venue": "Applied and Computational Harmonic Analysis,",
            "year": 2009
        },
        {
            "authors": [
                "J. Herman",
                "R. Kucera",
                "J. Simsa"
            ],
            "title": "Equations and Inequalities: Elementary Problems and Theorems in Algebra and Number Theory",
            "year": 2000
        },
        {
            "authors": [
                "Y.H. Hu",
                "C. Li",
                "K.W. Meng",
                "J. Qin",
                "X.Q. Yang"
            ],
            "title": "Group sparse optimization via lp,q regularization",
            "venue": "Journal of Machine Learning Research,",
            "year": 2017
        },
        {
            "authors": [
                "H.C. Liu",
                "T. Yao",
                "R.Z. Li",
                "Y.Y. Ye"
            ],
            "title": "Folded concave penalized sparse linear regression: sparsity, statistical performance, and algorithmic theory for local solutions",
            "venue": "Mathematical Programming,",
            "year": 2017
        },
        {
            "authors": [
                "P.-L. Loh",
                "M.J. Wainwright"
            ],
            "title": "High-dimensional regression with noisy and missing data: Provable guarantees with nonconvexity",
            "venue": "Annals of Statistics,",
            "year": 2012
        },
        {
            "authors": [
                "P.-L. Loh",
                "M.J. Wainwright"
            ],
            "title": "Regularized M-estimators with nonconvexity: Statistical and algorithmic theory for local optima",
            "venue": "Journal of Machine Learning Research,",
            "year": 2015
        },
        {
            "authors": [
                "B.K. Natarajan"
            ],
            "title": "Sparse approximate solutions to linear systems",
            "venue": "SIAM Journal on Computing,",
            "year": 1995
        },
        {
            "authors": [
                "S. Negahban",
                "P. Ravikumar",
                "M. J Wainwright",
                "B. Yu"
            ],
            "title": "A unified framework for high-dimensional analysis of M-estimators with decomposable regularizers",
            "venue": "Statistical Science,",
            "year": 2012
        },
        {
            "authors": [
                "J. Qin",
                "Y.H. Hu",
                "F. Xu",
                "H.K. Yalamanchili",
                "J.W. Wang"
            ],
            "title": "Inferring gene regulatory networks by integrating ChIP-seq/chip and transcriptome data via LASSO-type regularization methods",
            "year": 2014
        },
        {
            "authors": [
                "C.R. Rao",
                "M. Statistiker"
            ],
            "title": "Linear Statistical Inference and Its Applications",
            "year": 1973
        },
        {
            "authors": [
                "G. Raskutti",
                "M.J. Wainwright",
                "B. Yu"
            ],
            "title": "Restricted eigenvalue properties for correlated gaussian designs",
            "venue": "Journal of Machine Learning Research,",
            "year": 2010
        },
        {
            "authors": [
                "G. Raskutti",
                "M.J. Wainwright",
                "B. Yu"
            ],
            "title": "Minimax rates of estimation for high-dimensional linear regression over lq-balls",
            "venue": "IEEE Transactions on Information Theory,",
            "year": 2011
        },
        {
            "authors": [
                "B. Recht",
                "M. Fazel",
                "P.A. Parrilo"
            ],
            "title": "Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization",
            "venue": "SIAM Review,",
            "year": 2010
        },
        {
            "authors": [
                "S. Ross"
            ],
            "title": "A First Course in Probability",
            "year": 2009
        },
        {
            "authors": [
                "C.B. Song",
                "S.T. Xia"
            ],
            "title": "Sparse signal recovery by lq minimization under restricted isometry property",
            "venue": "IEEE Signal Processing Letters,",
            "year": 2014
        },
        {
            "authors": [
                "R. Tibshirani"
            ],
            "title": "Regression shrinkage and selection via the Lasso",
            "venue": "Journal of the Royal Statistical Society, Series B,",
            "year": 1996
        },
        {
            "authors": [
                "S.A. van de Geer"
            ],
            "title": "High-dimensional generalized linear models and the Lasso",
            "venue": "Annals of Statistics,",
            "year": 2008
        },
        {
            "authors": [
                "S.A. van de Geer",
                "P. B\u00fchlmann"
            ],
            "title": "On the conditions used to prove oracle results for the Lasso",
            "venue": "Electronic Journal of Statistics,",
            "year": 2009
        },
        {
            "authors": [
                "Z.B. Xu",
                "X.Y. Chang",
                "F.M. Xu",
                "H. Zhang"
            ],
            "title": "L1/2 regularization: A thresholding representation theory and a fast solver",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems,",
            "year": 2012
        },
        {
            "authors": [
                "C.-H. Zhang"
            ],
            "title": "Nearly unbiased variable selection under minimax concave penalty",
            "venue": "Annals of Statistics,",
            "year": 2010
        },
        {
            "authors": [
                "C.H. Zhang",
                "J. Huang"
            ],
            "title": "The sparsity and bias of the Lasso selection in high-dimensional linear regression",
            "venue": "Annals of Statistics,",
            "year": 2008
        },
        {
            "authors": [
                "C.-H. Zhang",
                "T. Zhang"
            ],
            "title": "A general theory of concave regularization for high-dimensional sparse estimation problems",
            "venue": "Statistical Science,",
            "year": 2012
        },
        {
            "authors": [
                "T. Zhang"
            ],
            "title": "Some sharp performance bounds for least squares regression with l1 regularization",
            "venue": "Annals of Statistics,",
            "year": 2009
        },
        {
            "authors": [
                "T. Zhang"
            ],
            "title": "Analysis of multi-stage convex relaxation for sparse regularization",
            "venue": "Journal of Machine Learning Research,",
            "year": 2010
        },
        {
            "authors": [
                "S.H. Zhou"
            ],
            "title": "Restricted eigenvalue conditions on subgaussian random matrices",
            "venue": "arXiv preprint arXiv:0912.4045,",
            "year": 2009
        }
    ],
    "sections": [
        {
            "text": "2) for the \u2113q minimization method and the oracle inequality and \u21132 recovery bound O(\u03bb 2\n2\u2212q s) for the \u2113q regularization method hold respectively with high probability. The results in this paper are nonasymptotic. The numerical results verify the established statistical property and demonstrate the advantages of the \u2113q regularization method over the classical Lasso.\nKeywords: sparse estimation, lower-order optimization method, restricted eigenvalue condition, \u21132 recovery bound, oracle property\n\u2217School of Mathematical Sciences, Zhejiang University, Hangzhou 310027, P. R. China (11435017@zju.edu.cn).\n\u2020College of Mathematics and Statistics, Shenzhen University, Shenzhen 518060, P. R. China (mayhhu@szu.edu.cn).\n\u2021School of Mathematical Sciences, Zhejiang University, Hangzhou 310027, P. R. China (cli@zju.edu.cn).\n\u00a7Department of Applied Mathematics, The Hong Kong Polytechnic University, Kowloon, Hong Kong (mayangxq@polyu.edu.hk).\n\u00b6Brainnetome Center, Institute of Automation, Chinese Academy of Sciences, Beijing 100190, P.R. China (jiangtz@nlpr.ia.ac.cn)."
        },
        {
            "heading": "1 Introduction",
            "text": "In various areas of applied sciences and engineering, a fundamental problem is to estimate an unknown parameter \u03b2\u2217 \u2208 Rn of a linear regression model\ny = X\u03b2\u2217 + e, (1)\nwhere X \u2208 Rm\u00d7n is a design matrix, e \u2208 Rm is a vector containing random measurement noise, and thus y \u2208 Rm is the corresponding vector of the noisy observations. According to the context of practical applications, the design matrix could be either deterministic or random.\nThe curse of dimensionality always occurs in the high-dimensional regime of many application fields. For example, in magnetic resonance imaging [9], remote sensing [2], systems biology [30], one is typically only able to collect far fewer samples than the number of variables due to physical or economical constraints, i.e., m \u226a n. Under the high-dimensional scenario, estimating the true underlying parameter of model (1) is a vital challenge in contemporary statistics, whereas the classical ordinary least squares (OLS) does not work well in this scenario because the corresponding linear system is seriously ill-conditioned."
        },
        {
            "heading": "1.1 \u21131 Optimization Problems",
            "text": "Fortunately, in practical applications, a wide class of problems usually have certain special structures, employing which could eliminate the nonidentifiability of model (1) and enhance the predictability. One of the most popular structures is the sparsity structure, that is, the underlying parameter \u03b2\u2217 in the high-dimensional space is sparse. One common way to measure the degree of sparsity is the \u2113q norm, which for 0 < q \u2264 1 is defined as\n\u2225\u03b2\u2225q :=\n( n\u2211\ni=1\n|\u03b2i|q )1/q ,\nwhile \u2225\u03b2\u22250 is defined as the number of nonzero entries of \u03b2. We first review the literature of sparse estimation for the case when the design matrix X is deterministic. Considering a bounded noise (i.e., \u2225e\u22252 \u2264 \u03f5), it is natural to solve the following (constrained) \u21130 minimization problem\n(CP0,\u03f5) min \u2225\u03b2\u22250 s.t. \u2225y \u2212X\u03b2\u22252 \u2264 \u03f5.\nUnfortunately, it is NP-hard to compute its global solution due to the nonconvex and combinational natures [28].\nTo deal with this obstacle, a common technique is to use the (convex) \u21131 norm to approach the \u21130 norm:\n(CP1,\u03f5) min \u2225\u03b2\u22251 s.t. \u2225y \u2212X\u03b2\u22252 \u2264 \u03f5,\nwhich can be efficiently solved by several standard methods; see [14, 21] and references therein. The stable statistical properties of (CP1,\u03f5) have been explored under the regularity conditions. One of the most important stable statistical properties is the \u21132 recovery bound property, which is to estimate the upper bound of the error between the optimal solution of the optimization problem and the true underlying parameter in terms of the noise level \u03f5. More specifically, let s \u226a n and \u03b2\u2217 be an s-sparse parameter (i.e., \u2225\u03b2\u2217\u22250 \u2264 s) satisfying the linear regression model (1). The \u21132 recovery bound for (CP1,\u03f5) was provided in [18] and [9] under the mutual incoherence property (MIP) or the restricted isometry property (RIP)1, respectively:\n\u2225\u03b2\u03041,\u03f5 \u2212 \u03b2\u2217\u22252 = O(\u03f5),\nwhere \u03b2\u03041,\u03f5 stands for the optimal solution of (CP1,\u03f5). Motivated by the computational issue, another popular technique for sparse estimation is to solve the (unconstrained) \u21131 regularization problem:\n(RP1,\u03bb) min 1\n2m \u2225y \u2212X\u03b2\u222522 + \u03bb\u2225\u03b2\u22251,\nwhere \u03bb > 0 is the regularization parameter, providing a tradeoff between data fidelity and sparsity. The \u21131 regularization model, also named the Lasso estimator [37], has attracted a great deal of attention in parameter estimation in the high-dimensional scenario, because its convexity structure is beneficial in designing exclusive and efficient algorithms and gaining wide applications; see [3, 15] and references therein. For the noise-free case, the \u21132 recovery bound for (RP1,\u03bb) was provided in [39] under the RIP or the restricted eigenvalue condition (REC)2:\n\u2225\u03b2\u03021,\u03bb \u2212 \u03b2\u2217\u222522 = O(\u03bb2s),\nwhere \u03b2\u03021,\u03bb denotes the optimal solution of (RP1,\u03bb). Furthermore, assuming that the noise in model (1) is normally distributed e \u223c N (0, \u03c32Im), it\n1It was claimed in [7] that the RIP [10] is implied by the MIP [19], while the restricted isometry constant (RIC) is more difficult to be calculated than the mutual incoherence constant (MIC).\n2It was reported in [4] that the REC is implied by the RIP, and in [32] that a broad class of correlated Gaussian design matrices satisfy the REC but violate the RIP with high probability.\nwas established in [4, 6, 44] that the following \u21132 recovery bound with high probability \u2225\u03b2\u03021,\u03bb \u2212 \u03b2\u2217\u222522 = O ( \u03c32s log n\nm\n) ,\nwhen the regularization parameter is chosen as \u03bb = \u03c3 \u221a\nlogn m and under\nthe RIP, REC or other regularity conditions, respectively. However, the \u21131 minimization and regularization problems suffer several dissatisfactions in both theoretical and practical applications. In particular, it was reported by extensive theoretical and empirical studies that the \u21131 minimization and regularization problems suffer from significant estimation bias when parameters have large absolute values; the induced solutions are much less sparse than the true parameter, they cannot recover a sparse signal with the least samples when applied to compressed sensing, and that they often result in sub-optimal sparsity in practice; see, e.g., [12, 20, 41, 40, 45]. Therefore, there is a great demand for developing the alternative sparse estimation technique that enjoys nice statistical theory and successful applications.\nTo address the bias and the sub-optimal issues induced by the \u21131 norm, several nonconvex regularizers have been proposed such as the smoothly clipped absolute deviation (SCAD) [20], minimax concave penalty (MCP) [41], \u21130 norm [43], \u2113q norm (0 < q < 1) [22], and capped \u21131 norm [27]; specifically, the SCAD and MCP fall into the category of folded concave penalized (FCP) methods. It was studied in [43] that the global solution of the FCP sparse linear regression enjoys the oracle property under the sparse eigenvalue condition; see Remark 4(iii) for details.\nIt is worth noting that the \u2113q norm regularizer (0 < q < 1) has been recognized as an important technique for sparse optimization and gained successful applications in various applied science fields; see, e.g., [12, 30, 40]. In the present paper, we focus on the statistical property of the \u2113q optimization method, which is beyond the category of the FCP. Throughout the whole paper, we always assume that 0 < q \u2264 1 unless otherwise specified."
        },
        {
            "heading": "1.2 \u2113q Optimization Problems",
            "text": "Due to the fact that limq\u21920+ \u2225\u03b2\u2225 q q = \u2225\u03b2\u22250, the \u2113q norm has also been adopted as another alternative sparsity promoting penalty function of the \u21130 and \u21131 norms. The following \u2113q optimization problems have attracted a great amount of attention and gained successful applications in a wide range of fields (see [12, 30, 40] and references therein):\n(CPq,\u03f5) min \u2225\u03b2\u2225q s.t. \u2225y \u2212X\u03b2\u22252 \u2264 \u03f5,\nand\n(RPq,\u03bb) min 1\n2m \u2225y \u2212X\u03b2\u222522 + \u03bb\u2225\u03b2\u2225qq.\nIn particular, [12] and [40] showed that the \u2113q minimization and the \u2113 1 2 regularization admit a significantly stronger sparsity promoting capability than the \u21131 minimization and the \u21131 regularization, respectively; that is, they allow to obtain a more sparse solution from a smaller amount of samplings. [30] revealed that the \u2113 1\n2 regularization achieved a more reliable biological\nsolution than the \u21131 regularization in the field of systems biology. The advantage of the lower-order optimization problem has also been shown in theory that it requires a weaker regularity condition to guarantee the stable statistical property than the classical \u21131 optimization problem. In particular, let \u03b2\u0304q,\u03f5 and \u03b2\u0302q,\u03bb denote the optimal solution of (CPq,\u03f5) and (RPq,\u03bb), respectively. The \u21132 recovery bound for (CPq,\u03f5) was established in [16] and [36] under MIP and RIP respectively:\n\u2225\u03b2\u0304q,\u03f5 \u2212 \u03b2\u2217\u22252 = O(\u03f5), (2)\nwhere the MIP or RIP is weaker than the one used in the study of (CP1,\u03f5). [24] established an \u21132 recovery bound for (RPq,\u03bb) in the noise-free case:\n\u2225\u03b2\u0302q,\u03bb \u2212 \u03b2\u2217\u222522 = O(\u03bb 2 2\u2212q s) (3)\nunder the introduced q-REC, which is strictly weaker than the classical REC. However, the theoretical study of the \u2113q optimization problem is still limited; particularly, there is still no paper devoted to establishing the statistical property of the \u2113q minimization problem when the noise is randomly distributed, and that of the \u2113q regularization problem in the noise-aware case."
        },
        {
            "heading": "1.3 Contributions of This Paper",
            "text": "The main contribution of the present paper is the establishment of the statistical properties for the \u2113q optimization problems, including (CPq,\u03f5) and (RPq,\u03bb), in the noise-aware case; specifically, in the case when the linear regression model (1) involves a Gaussian noise e \u223c N (0, \u03c32Im). For this purpose, we extend the q-REC [24] to a more general one, which is one of the weakest regularity conditions for estimating the \u21132 recovery bounds of sparse estimation models, and provide some sufficient conditions for guaranteeing the general q-REC in terms of REC, RIP, MIP (with a less restrictive\nconstant); see Propositions 1 and 2. Under the general q-REC, we show that the \u21132 recovery bound (2) holds for (CPq,\u03f5) with high probability, and that\n\u2225\u03b2\u0302q,\u03bb \u2212 \u03b2\u2217\u222522 = O\n(( \u03c32 log n\nm\n) 1 2\u2212q\ns ) ,\nas well as the estimation of prediction loss and the oracle property, hold for (RPq,\u03bb) with high probability; see Theorems 1 and 2, respectively. These results provide a unified framework of the statistical properties of the \u2113q optimization problems, and improve the ones of the \u2113q minimization problem [16, 36] and the \u21131 regularization problem [4, 6, 44] under the q-REC; see Remark 4. They are not only of independent interest in establishing statistical properties for the lower-order optimization problems with randomly noisy data, but also provide a useful tool for the study of the case when the design matrix X is random.\nAnother contribution of the present paper is to explore the \u21132 recovery bounds for the \u2113q optimization problems with a random design matrix X and random noise e, which is more realistic in the real-world applications; e.g., compressed sensing [8], signal processing [9], statistical learning [1]. As reported in [32], the key issue for studying the statistical properties of a sparse estimation model with a random design matrix is to provide suitable conditions on the population covariance matrix \u03a3 of X, which can guarantee the regularity conditions with high probability; see, e.g., [9, 32]. Motivated by the real-world applications, we consider the standard case when X is a Gaussian random design with i.i.d. N (0,\u03a3) rows and the linear regression model (1) involves a Gaussian noise, explore a sufficient condition for ensuring the q-REC of X with high probability in terms of the q-REC of \u03a3, and apply the preceding results to establish the \u21132 recovery bounds (2) for (CPq,\u03f5), and (3), as well as the predication loss and the oracle inequality, for (RPq,\u03bb), respectively; see Theorems 3 and 4. These results provide a unified framework of the statistical properties of the \u2113q optimization problems with a Gaussian random design under the q-REC, which cover the ones of the \u21131 optimization problems (see [46, Theorem 3.1]) as special cases; see Corollaries 3 and 4. To the best of our knowledge, most results presented in this paper are new, either for the deterministic or random design matrix.\nWe also carry out the numerical experiments on the standard simulated data. The preliminary numerical results verify the established statistical properties and show that the \u2113q optimization methods possess better recovery performance than the \u21131 optimization method, which coincides with\nexisting numerical studies [24, 40] on the \u2113q regularization problem. The remainder of this paper is organized as follows. In section 2, we introduce the lower-order REC and discuss its sufficient conditions. In section 3, we establish the \u21132 recovery bounds for (CPq,\u03f5) and (RPq,\u03bb) with a deterministic design matrix. The extension to the linear regression model with a Gaussian random design and preliminary numerical results are presented in sections 4 and 5, respectively.\nWe end this section by presenting the notations adopted in this paper. We use Greek lowercase letters \u03b1, \u03b2, \u03b4 to denote the vectors, capital letters J , T to denote the index sets, and script captical letters A , B, C to denote the random events. For \u03b2 \u2208 Rn and J \u2286 {1, 2, . . . , n}, we use \u03b2J to denote the vector in Rn that (\u03b2J)i = \u03b2i for i \u2208 J and zero elsewhere, |J | to denote the cardinality of J , Jc := {1, 2, . . . , n} \\ J to denote the complement of J , and supp(\u03b2) to denote the support of \u03b2, i.e., the index set of nonzero entries of \u03b2. Particularly, Im stands for the identity matrix in Rm, and P(A ) and P(A |B) denote the probability that event A happens and the conditional probability that event A happens given that event B happens, respectively."
        },
        {
            "heading": "2 Restricted Eigenvalue Conditions",
            "text": "This section aims to discuss some regularity conditions imposed on the design matrix X that are needed to guarantee the stable statistical properties of (CPq,\u03f5) and (RPq,\u03bb).\nIn statistics, the ordinary least squares (OLS) is a classical technique for estimating the unknown parameters in a linear regression model and has favourable properties if some regularity conditions are satisfied; see, e.g., [31]. For example, the OLS always requires the positive definiteness of the Gram matrix \u0393(X) := X\u22a4X, that is,\nmin \u03b2\u2208Rn:\u03b2 \u0338=0\n(\u03b2\u22a4\u0393(X)\u03b2)1/2\n\u2225\u03b2\u22252 = min \u03b2\u2208Rn:\u03b2 \u0338=0 \u2225X\u03b2\u22252 \u2225\u03b2\u22252 > 0. (4)\nHowever, in the high-dimensional setting, the OLS does not work well; in fact, the matrix \u0393(X) is seriously degenerate, i.e.,\nmin \u03b2\u2208Rn:\u03b2 \u0338=0 \u2225X\u03b2\u22252 \u2225\u03b2\u22252 = 0.\nTo deal with the challenges caused by the high-dimensional data, the Lasso (least absolute shrinkage and selection operator) estimator was introduced by [37]. Since then the Lasso estimator has gained a great success in the\nsparse representation and machine learning of high-dimensional data; see, e.g., [4, 38, 44] and references therein. It was pointed out that Lasso requires a weak condition, called the restricted eigenvalue condition (REC) [4], to ensure the nice statistical properties; see, e.g., [39, 26, 29]. In the definition of REC, the minimum in (4) is replaced by a minimum over a restricted set of vectors measured by an \u21131 norm inequality, and the norm \u2225\u03b2\u22252 in the denominator is replaced by the \u21132 norm of only a part of \u03b2. The notion of REC was extended to the group-wised lower-order REC in [24], which was used there to explore the oracle property and \u21132 recovery bound of the \u2113p,q regularization problem in a noise-free case.\nInspired by the ideas in [4, 24], we here introduce a lower-order REC for the \u2113q optimization problems, similar to but more general than the one in [24], where the minimum is taken over a restricted set of vectors measured by an \u2113q norm inequality. To proceed, we shall introduce some useful notations. For the remainder of this paper, let a > 0 and (s, t) be a pair of integers such that\n1 \u2264 s \u2264 t \u2264 n and s+ t \u2264 n. (5)\nFor \u03b4 \u2208 Rn and J \u2286 {1, 2, . . . , n}, we define by J(\u03b4; t) the index set corresponding to the first t largest coordinates in absolute value of \u03b4 in Jc. For X \u2208 Rm\u00d7n, its q-restricted eigenvalue modulus relative to (s, t, a) is defined by\n\u03d5q(s, t, a,X) := min\n{ \u2225X\u03b4\u22252\n\u2225\u03b4J\u222aJ(\u03b4;t)\u22252 : |J | \u2264 s, \u2225\u03b4Jc\u2225qq \u2264 a\u2225\u03b4J\u2225qq\n} . (6)\nThe lower-order REC is defined as follows.\nDefinition 1. Let 0 \u2264 q \u2264 1 and X \u2208 Rm\u00d7n. X is said to satisfy the q-restricted eigenvalue condition relative to (s, t, a) (q-REC(s, t, a) in short) if\n\u03d5q(s, t, a,X) > 0.\nRemark 1. (i) Clearly, the q-REC(s, t, a) provides a unified framework of the REC-type conditions, e.g., it includes the classical REC in [4] (when q = 1) and the q-REC(s, t) in [24] (when a = 1) as special cases.\n(ii) The restricted eigenvalue modulus (with q = 1) defined in (6) is slightly different from the one for the classical REC in [4], in which the factor\u221a m appears in the denominator there. For example, if the matrix X has i.i.d. Gaussian entries, the restricted eigenvalue modulus in [4] scales as a constant independent of s, m, and n and thus \u03d5q(s, t, a,X) given by (6) scales as \u221a m, whenever sm log n is bounded.\nIt is natural to study the relationships between the q-RECs and other types of regularity conditions. To this end, we first recall some basic properties of the \u2113q norm in the following lemmas; particularly, Lemma 1 is taken from [23, Section 8.12] and [24, Lemmas 1 and 2].\nLemma 1. Let \u03b1, \u03b2 \u2208 Rn. Then the following relations are true:\n\u2225\u03b2\u2225q2 \u2264 \u2225\u03b2\u2225q1 \u2264 n 1 q1 \u2212 1 q2 \u2225\u03b2\u2225q2 for any 0 < q1 \u2264 q2 < +\u221e, (7)\n\u2225\u03b1\u2225qq \u2212 \u2225\u03b2\u2225qq \u2264 \u2225\u03b1+ \u03b2\u2225qq \u2264 \u2225\u03b1\u2225qq + \u2225\u03b2\u2225qq for any 0 < q \u2264 1. (8) Lemma 2. Let p \u2265 1, n1, n2 \u2208 N, \u03b1 \u2208 Rn1+ , \u03b2 \u2208 R n2 + and c > 0 be such that\nmax 1\u2264i\u2264n1 \u03b1i \u2264 min 1\u2264j\u2264n2 \u03b2j and n1\u2211 i=1 \u03b1i \u2264 c n2\u2211 j=1 \u03b2j . (9)\nThen n1\u2211 i=1 \u03b1pi \u2264 c n2\u2211 j=1 \u03b2pj . (10)\nProof. Let \u03b1max := max 1\u2264i\u2264n1 \u03b1i and \u03b2min := min 1\u2264j\u2264n2 \u03b2j . Then it holds that\n\u03b1max n1\u2211 i=1 \u03b1pi \u2264 \u03b1 p max n1\u2211 i=1 \u03b1i and \u03b2 p min n2\u2211 j=1 \u03b2j \u2264 \u03b2min n2\u2211 j=1 \u03b2pj . (11)\nWithout loss of generality, we assume that \u03b1max > 0; otherwise, (10) holds automatically. Thus, by the first inequality of (9) and noting p \u2265 1, we have that\n0 < \u03b1pmax\u03b2min \u2264 \u03b1max\u03b2 p min. (12)\nMultiplying the inequalities in (11) by \u03b2min n2\u2211 j=1 \u03b2j and \u03b1max n1\u2211 i=1 \u03b1i respectively, we obtain that\n\u03b1max\u03b2min n1\u2211 i=1 \u03b1pi n2\u2211 j=1 \u03b2j \u2264 \u03b1pmax\u03b2min n1\u2211 i=1 \u03b1i n2\u2211 j=1 \u03b2j\n\u2264 \u03b1max\u03b2pmin n1\u2211 i=1 \u03b1i n2\u2211 j=1 \u03b2j\n\u2264 \u03b1max\u03b2min n1\u2211 i=1 \u03b1i n2\u2211 j=1 \u03b2pj ,\nwhere the second inequality follows from (12). This, together with the second inequality of (9), yields (10). The proof is complete.\nExtending [24, Proposition 5] to the general q-REC, the following proposition validates the relationship between the q-RECs: the lower the q, the weaker the q-REC. However, the inverse of this implication is not true; see [24, Example 1] for a counter example. We provide the proof so as to make this paper self-contained, although the idea is similar to that of [24, Proposition 5].\nProposition 1. Let X \u2208 Rm\u00d7n, a > 0, and (s, t) be a pair of integers satisfying (5). Suppose that 0 < q1 \u2264 q2 \u2264 1 and that X satisfies the q2-REC(s, t, a). Then X satisfies the q1-REC(s, t, a).\nProof. Associated with the q-REC(s, t, a), we define the feasible set\nCq(s, a) := {\u03b4 \u2208 Rn : \u2225\u03b4Jc\u2225qq \u2264 a\u2225\u03b4J\u2225qq for some |J | \u2264 s}. (13)\nBy Definition 1, it remains to show that Cq1(s, a) \u2286 Cq2(s, a). To this end, let \u03b4 \u2208 Cq1(s, a), and let J0 denote the index set of the first s largest coordinates in absolute value of \u03b4. By the assumption that \u03b4 \u2208 Cq1(s, a) and by the construction of J0, one has \u2225\u03b4Jc0\u2225 q1 q1 \u2264 a\u2225\u03b4J0\u2225 q1 q1 . Then we obtain by Lemma 2 (with q2/q1 in place of p) that \u2225\u03b4Jc0\u2225 q2 q2 \u2264 a\u2225\u03b4J0\u2225 q2 q2 ; consequently, \u03b4 \u2208 Cq2(s, a). Hence, it follows that Cq1(s, a) \u2286 Cq2(s, a), and the proof is complete.\nIt is revealed from Proposition 1 that the classical REC is a sufficient condition of the lower-order REC. In the sequel, we will further discuss some other types of regularity conditions: the sparse eigenvalues condition (SEC), the restricted isometry property (RIP), and the mutual incoherence property (MIP), which have been widely used in the literature of statistics and engineering, for ensuring the lower-order REC.\nThe SEC is a popular regularity condition required to guarantee the nice properties of sparse representation; see [4, 17, 43] and references therein. For \u2206 \u2208 Rn\u00d7n and s \u2208 N, the s-sparse minimal eigenvalue and s-sparse maximal eigenvalue of \u2206 are respectively defined by\n\u03c3min(s,\u2206) := min \u03b2\u2208Rn:1\u2264\u2225\u03b2\u22250\u2264s\n\u03b2\u22a4\u2206\u03b2\n\u03b2\u22a4\u03b2 , \u03c3max(s,\u2206) := max\n\u03b2\u2208Rn:1\u2264\u2225\u03b2\u22250\u2264s\n\u03b2\u22a4\u2206\u03b2\n\u03b2\u22a4\u03b2 .\n(14) The SEC was first introduced in [17] to show that the optimal solution of (CP1,\u03f5) well approximates that of (CP0,\u03f5) whenever \u03c3min(2s,\u0393(X)) > 0.\nThe RIP is another well-known regularity condition in the scenario of sparse learning, which was introduced by [10] and has been widely used\nin the study of the oracle property and \u21132 recovery bound for the highdimensional regression model; see [4, 9, 34] and references therein. Below, we recall the RIP-type notions from [10].\nDefinition 2. Let X \u2208 Rm\u00d7n and let s, t \u2208 N be such that s+ t \u2264 n.\n(i) The s-restricted isometry constant of X, denoted by \u03b7s(X), is defined to be the smallest quantity such that, for any \u03b2 \u2208 Rn and J \u2286 {1, . . . , n} with |J | \u2264 s,\n(1\u2212 \u03b7s(X))\u2225\u03b2J\u222522 \u2264 \u2225X\u03b2J\u222522 \u2264 (1 + \u03b7s(X))\u2225\u03b2J\u222522. (15)\n(ii) The (s, t)-restricted orthogonality constant of X, denoted by \u03b8s,t(X), is defined to be the smallest quantity such that, for any \u03b2 \u2208 Rn and J, T \u2286 {1, . . . , n} with |J | \u2264 s, |T | \u2264 t and J \u2229 T = \u2205,\n|\u27e8X\u03b2J , X\u03b2T \u27e9| \u2264 \u03b8s,t(X)\u2225\u03b2J\u22252\u2225\u03b2T \u22252. (16)\nThe MIP is also a well-known regularity condition in the scenario of sparse learning, which was introduced by [19] and has been used in [4, 7, 17, 18] and references therein. In the case when each diagonal element of the Gram matrix \u0393(X) is 1, \u03b81,1(X) coincides with the mutual incoherence constant; see [19].\nThe following lemmas are useful for establishing the relationship between the q-REC and other types of regularity conditions; in particular, Lemmas 3 and 4 are taken from [10, Lemma 1.1] and [39, Lemma 3.1], respectively.\nLemma 3. Let X \u2208 Rm\u00d7n and s, t \u2208 N be such that s+ t \u2264 n. Then\n\u03b8s,t(X) \u2264 \u03b7s+t(X) \u2264 \u03b8s,t(X) + max{\u03b7s(X), \u03b7t(X)}.\nLemma 4. Let \u03b1, \u03b2 \u2208 Rn and 0 < \u03c4 < 1 be such that \u2212\u27e8\u03b1, \u03b2\u27e9 \u2264 \u03c4\u2225\u03b1\u222522. Then (1\u2212 \u03c4)\u2225\u03b1\u22252 \u2264 \u2225\u03b1+ \u03b2\u22252.\nFor the sake of simplicity, a partition structure and some notations are presented. For a vector \u03b4 \u2208 Rn and an index set J \u2286 {1, 2, . . . , n}, we use rank(\u03b4i; J c) to denote the rank of the absolute value of \u03b4i in J c (in a decreasing order) and Jk(\u03b4; t) to denote the index set of the k-th batch of the first t largest coordinates in absolute value of \u03b4 in Jc. That is,\nJk(\u03b4; t) := {i \u2208 Jc : rank(\u03b4i; Jc) \u2208 {kt+ 1, . . . , (k + 1)t}} for each k \u2208 N. (17)\nLemma 5. Let X \u2208 Rm\u00d7n, 0 < q \u2264 1, a > 0, and (s, t) be a pair of integers satisfying (5). Then the following relations are true:\n\u03d5q(s, t, a,X) \u2265 \u221a \u03c3min(s+ t,\u0393(X))\u2212 a 1 q (s t ) 1 q \u2212 1 2 \u221a \u03c3max(t,\u0393(X)), (18) \u03d5q(s, t, a,X) \u2264 \u221a \u03c3max(s+ t,\u0393(X)) + a 1 q (s t ) 1 q \u2212 1 2 \u221a \u03c3max(t,\u0393(X)). (19)\nProof. Fix \u03b4 \u2208 Cq(s, a), as defined by (13). Then there exists J \u2286 {1, 2, . . . , n} such that\n|J | \u2264 s and \u2225\u03b4Jc\u2225qq \u2264 a\u2225\u03b4J\u2225qq. (20)\nWrite r := \u2308n\u2212st \u2309 (where \u2308u\u2309 denotes the largest integer not greater than u), Jk := Jk(\u03b4; t) (defined by (17)) for each k \u2208 N and J\u2217 := J \u222a J0. Then it follows from [24, Lemma 7] and (20) that\nr\u2211 k=1 \u2225\u03b4Jk\u22252 \u2264 t 1 2 \u2212 1 q \u2225\u03b4Jc\u2225q \u2264 a 1 q t 1 2 \u2212 1 q \u2225\u03b4J\u2225q \u2264 a 1 q (s t ) 1 q \u2212 1 2 \u2225\u03b4J\u22252 (21)\n(due to (7)). Noting by (17) and (20) that |J\u2217| \u2264 s+ t and |Jk| \u2264 t for each k \u2208 N, one has by (14) that\u221a\n\u03c3min(s+ t,\u0393(X))\u2225\u03b4J\u2217\u22252 \u2264 \u2225X\u03b4J\u2217\u22252 \u2264 \u221a \u03c3max(s+ t,\u0393(X))\u2225\u03b4J\u2217\u22252,\n\u2225X\u03b4Jk\u22252 \u2264 \u221a \u03c3max(t,\u0393(X))\u2225\u03b4Jk\u22252 for each k \u2208 N.\nThese, together with (21), imply that\n\u2225X\u03b4\u22252 \u2265 \u2225X\u03b4J\u2217\u22252 \u2212 r\u2211\nk=1\n\u2225X\u03b4Jk\u22252\n\u2265 (\u221a \u03c3min(s+ t,\u0393(X))\u2212 a 1 q (s t ) 1 q \u2212 1 2 \u221a \u03c3max(t,\u0393(X)) ) \u2225\u03b4J\u2217\u22252.\nSince \u03b4 and J satisfying (20) are arbitrary, (18) is shown to hold by (6) and the fact that J\u2217 = J \u222aJ(\u03b4; t). One can prove (19) in a similar way, and thus, the details are omitted.\nThe following proposition provides the sufficient conditions for the q-REC in terms of the SEC, RIP and MIP; see (a), (b) and (c) below respectively.\nProposition 2. Let X \u2208 Rm\u00d7n, 0 < q \u2264 1, a > 0, and (s, t) be a pair of integers satisfying (5). Then X satisfies the q-REC(s, t, a) provided that one of the following conditions:\n(a) \u03c3min(s+ t,\u0393(X)) > a ( as t ) 2 q \u22121 \u03c3max(t,\u0393(X)).\n(b) \u03b7t(X) + \u03b8s,t(X) + a 1 2 ( as t ) 1 q \u2212 1 2 \u03b8t,s+t(X) < 1.\n(c) each diagonal element of \u0393(X) is 1 and \u03b81,1(X) < (( 1 + 2a ( as t ) 1 q \u22121 ) (s+ t) )\u22121 .\nProof. It directly follows from Lemma 5 (cf. (18)) that X satisfies the qREC(s, t, a) provided that condition (a) holds. Fix \u03b4 \u2208 Cq(s, a), and let J , r, Jk (for each k \u2208 N) and J\u2217 be defined, respectively, as in the beginning of the proof of Lemma 5. Then (21) follows directly and it follows from [24, Lemma 7] and (17) that\n\u2225\u03b4Jc\u2217\u22251 = r\u2211\nk=1\n\u2225\u03b4Jk\u22251 \u2264 t 1\u2212 1 q \u2225\u03b4Jc\u2225q \u2264 a 1 q t 1\u2212 1 q \u2225\u03b4J\u2225q \u2264 a 1 q (s t ) 1 q \u22121 \u2225\u03b4J\u22251.\n(22) Suppose that condition (b) is satisfied. By Definition 2 (cf. (16)), one has\nthat\n|\u27e8X\u03b4J\u2217 , X\u03b4Jc\u2217\u27e9| \u2264 r\u2211\nk=1\n|\u27e8X\u03b4J\u2217 , X\u03b4Jk\u27e9| \u2264 \u03b8t,s+t(X)\u2225\u03b4J\u2217\u22252 r\u2211\nk=1\n\u2225\u03b4Jk\u22252.\nThen it follows from (21) that\n|\u27e8X\u03b4J\u2217 , X\u03b4Jc\u2217\u27e9| \u2264 a 1 q (s t ) 1 q \u2212 1 2 \u03b8t,s+t(X)\u2225\u03b4J\u2217\u22252\u2225\u03b4J\u22252\n\u2264 a\n1 q ( s t ) 1 q \u2212 1 2 \u03b8t,s+t(X)\n1\u2212 \u03b7s+t(X) \u2225X\u03b4J\u2217\u222522 (23)\n(by (15)). Since s \u2264 t (by (5)), one has by Definition 2(i) that \u03b7s(X) \u2264 \u03b7t(X), and then by Lemma 3 that \u03b7s+t(X) \u2264 \u03b8s,t(X) + \u03b7t(X). Then it follows from (b) that\n0 < a\n1 q ( s t ) 1 q \u2212 1 2 \u03b8t,s+t(X)\n1\u2212 \u03b7s+t(X) \u2264\na 1 q ( s t ) 1 q \u2212 1 2 \u03b8t,s+t(X)\n1\u2212 (\u03b7t(X) + \u03b8s,t(X)) < 1. (24)\nThis, together with (23), shows that Lemma 4 is applicable (with X\u03b4J\u2217 ,\nX\u03b4Jc\u2217 , a 1 q ( st ) 1 q\u2212 1 2 \u03b8t,s+t(X) 1\u2212\u03b7s+t(X) in place of \u03b1, \u03b2, \u03c4) to concluding that\n\u2225X\u03b4\u222522 \u2265 1\u2212 a 1q ( st ) 1q\u2212 12 \u03b8t,s+t(X) 1\u2212 \u03b7s+t(X) 2 \u2225X\u03b4J\u2217\u222522 \u2265 (1\u2212 \u03b7s+t(X)) 1\u2212 a 1q ( st ) 1q\u2212 12 \u03b8t,s+t(X) 1\u2212 \u03b7s+t(X)\n2 \u2225\u03b4J\u2217\u222522 (due to (15)). Since \u03b4 and J satisfying (20) are arbitrary, we derive by (6) and (24) that\n\u03d5q(s, t, a,X) \u2265 \u221a 1\u2212 \u03b7s+t(X) 1\u2212 a 1q ( st ) 1q\u2212 12 \u03b8t,s+t(X) 1\u2212 \u03b7s+t(X)  > 0; consequently, X satisfies the q-REC(s, t, a).\nSuppose that (c) is satisfied. Then we have by (22) and Definition 2 (cf. (16)) that\n\u2225X\u03b4\u222522 = \u2225X\u03b4J\u2217\u222522 + 2\u27e8X\u03b4J\u2217 , X\u03b4Jc\u2217\u27e9+ \u2225X\u03b4Jc\u2217\u2225 2 2\n\u2265 \u2225X\u03b4J\u2217\u222522 \u2212 2|\u27e8X\u03b4J\u2217 , X\u03b4Jc\u2217\u27e9| \u2265 \u2225X\u03b4J\u2217\u222522 \u2212 2\u03b81,1(X)\u2225\u03b4J\u2217\u22251\u2225\u03b4Jc\u2217\u22251\n\u2265 \u2225X\u03b4J\u2217\u222522 \u2212 2a 1 q (s t ) 1 q \u22121 \u03b81,1(X)\u2225\u03b4J\u2217\u222521.\n(25)\nSeparating the diagonal and off-diagonal terms of the quadratic form \u03b4TJ\u2217X TX\u03b4J\u2217 , one has by (7) and (c) that\n\u2225X\u03b4J\u2217\u222522 = n\u2211\ni=1\n(XTX)i,i(\u03b4J\u2217)i(\u03b4J\u2217)i + n\u2211\nj \u0338=k (XTX)j,k(\u03b4J\u2217)j(\u03b4J\u2217)k\n= \u2225\u03b4J\u2217\u222522 + n\u2211\nj \u0338=k \u27e8X\u00b7j(\u03b4J\u2217)j , X\u00b7k(\u03b4J\u2217)k\u27e9\n\u2265 \u2225\u03b4J\u2217\u222522 \u2212 \u03b81,1(X)\u2225\u03b4J\u2217\u222521 \u2265 (1\u2212 (s+ t)\u03b81,1(X))\u2225\u03b4J\u2217\u222522.\nCombining this inequality with (25), we get that \u2225X\u03b4\u222522 \u2265 ( 1\u2212 ( 1 + 2a 1 q (s t ) 1 q \u22121 ) (s+ t)\u03b81,1(X) ) \u2225\u03b4J\u2217\u222522.\nSince \u03b4 and J satisfying (20) are arbitrary, we derive by (6) and (c) that \u03d5q(s, t, a,X) \u2265 1\u2212 ( 1 + 2a 1 q (s t ) 1 q \u22121 ) (s+ t)\u03b81,1(X) > 0;\nconsequently, X satisfies the q-REC(s, t, a). The proof is complete.\nRemark 2. It was established in [4, Lemma 4.1(ii)], [39, Corollary 7.1 and 3.1] and [4, Assumption 5] that X satisfies the classical REC under one of the following conditions:\n(a\u2019) \u03c3min(s+ t,\u0393(X)) > s ta 2\u03c3max(t,\u0393(X)).\n(b\u2019) \u03b7t(X) + \u03b8s,t(X) + ( s t ) 1 2 a\u03b8t,s+t(X) < 1.\n(c\u2019) each diagonal element of \u0393(X) is 1 and \u03b81,1(X) < ((1 + 2a)(s+ t)) \u22121.\nProposition 2 extends the existing results to the general case when 0 < q \u2264 1 and partially improves them; in particular, each of conditions (a)-(c) in Proposition 2 required for the q-REC is less restrictive than the corresponding one of conditions (a\u2019)-(c\u2019) required for the classical REC in the situation when t > as, which usually occurs in the high-dimensional scenario (see, e.g., [4, 9, 46]). Moreover, by Propositions 1 and 2, we achieve that the q-REC(s, t, a) is satisfied provided that one of the following conditions:\n(a\u25e6) \u03c3min(s+ t,\u0393(X)) > min { 1, ( as t ) 2 q \u22122 } s ta 2\u03c3max(t,\u0393(X)).\n(b\u25e6) \u03b7t(X) + \u03b8s,t(X) + min { 1, ( as t ) 1 q \u22121 }( s t ) 1 2 a\u03b8t,s+t(X) < 1.\n(c\u25e6) each diagonal element of \u0393(X) is 1 and \u03b81,1(X) < (( 1 + 2amin { 1, ( as t ) 1 q \u22121 }) (s+ t) )\u22121 ."
        },
        {
            "heading": "3 Recovery Bounds for Deterministic Design",
            "text": "This section is devoted to establishing the \u21132 recovery bounds for (CPq,\u03f5) and (RPq,\u03bb) in the case that X is deterministic. Throughout this paper, we assume that the linear regression model (1) involves a Gaussian noise, i.e., e \u223c N (0, \u03c32Im), and adopt the following notations:\nlet \u03b2\u2217 be a solution of (1), J := supp(\u03b2\u2217), s := |J |, and let t \u2208 N satisfy (5).\nThe \u21132 recovery bound of the \u21131 regularization problem (i.e., Lasso estimator) was established in [4] under the assumption of the classical REC.\nThe deduction of the \u21132 recovery bound is based on an important property of the optimal solution. More precisely, let \u03b2\u03041,\u03f5 and \u03b2\u03021,\u03bb be the solutions of the \u21131 minimization and the \u21131 regularization problems, respectively. It was reported in [9, Eq. (2.2)] and [4, Corollary B.2] that the corresponding residuals satisfy the following dominant properties, with high probability,\n\u2225(\u03b2\u03041,\u03f5 \u2212 \u03b2\u2217)Jc\u22251 \u2264 \u2225(\u03b2\u03041,\u03f5 \u2212 \u03b2\u2217)J\u22251\nand \u2225(\u03b2\u03021,\u03bb \u2212 \u03b2\u2217)Jc\u22251 \u2264 3\u2225(\u03b2\u03021,\u03bb \u2212 \u03b2\u2217)J\u22251\nfor the \u21131 minimization and the \u21131 regularization problems, respectively. In the study of the \u2113q minimization and the \u2113q regularization problems, a natural question arises whether the residuals of solutions of (CPq,\u03f5) or (RPq,\u03bb) satisfy such a dominant property on the support of the true underlying parameter of linear regression (1) with high probability. Below, we provide a positive answer for this question in Propositions 3 and 4. To this end, we present some preliminary lemmas to measure the probabilities of random events related to the linear regression model (1), in which Lemma 6 is taken from [46, Lemma C.1].\nLemma 6. Let 0 \u2264 \u03b8 < 1 and b \u2265 0. Suppose that\nmax 1\u2264j\u2264n\n\u2225X\u00b7j\u22252 \u2264 (1 + \u03b8) \u221a m. (26)\nThen\nP\n( \u2225X\u22a4e\u2225\u221e\nm \u2265 \u03c3(1 + \u03b8)\n\u221a 2(1 + b) log n\nm\n) \u2264 ( nb \u221a \u03c0 log n )\u22121 .\nLemma 7. Let d \u2265 5. Then\nP ( \u2225e\u222522 \u2265 dm\u03c32 ) \u2264 exp ( \u2212d\u2212 1\n4 m\n) .\nProof. Recall that e = (e1, . . . , em) \u22a4 \u223c N (0, \u03c32Im). Let ui := 1\u03c3ei for i = 1, . . . ,m. Then one has that u1, . . . , um are i.i.d. Gaussian variables with ui \u223c N (0, 1) for i = 1, . . . ,m. Let u := (u1, . . . , um)\u22a4. Clearly, \u2225u\u222522 = 1\u03c32 \u2225e\u2225 2 2 is a chi-square random variable with m degrees of freedom (see, e.g., [35, Section 5.6]). Then it follows from standard tail bounds of chi-square random variable (see, e.g., [33, Appendix I]) that\nP ( \u2225u\u222522 \u2212m\nm \u2265 d\u2212 1\n) \u2264 exp ( \u2212d\u2212 1\n4 m\n)\n(as d \u2265 5). Consequently, we obtain that\nP ( \u2225e\u222522 \u2265 dm\u03c32 ) = P ( \u2225u\u222522 \u2265 dm ) \u2264 exp ( \u2212d\u2212 1\n4 m\n) .\nThe proof is complete.\nRecall that \u03b2\u2217 satisfies the linear regression model (1).\nLemma 8. Let \u03b2\u0302q,\u03bb be an optimal solution of (RPq,\u03bb). Then\n1\n2m \u2225X\u03b2\u2217 \u2212X\u03b2\u0302q,\u03bb\u222522 \u2264 \u03bb\u2225\u03b2\u2217\u2225qq \u2212 \u03bb\u2225\u03b2\u0302q,\u03bb\u2225qq +\n1\nm \u2225\u03b2\u0302q,\u03bb \u2212 \u03b2\u2217\u22251\u2225X\u22a4e\u2225\u221e.\nProof. Since \u03b2\u0302q,\u03bb is an optimal solution of (RPq,\u03bb), it follows that\n1\n2m \u2225y \u2212X\u03b2\u0302q,\u03bb\u222522 + \u03bb\u2225\u03b2\u0302q,\u03bb\u2225qq \u2264\n1\n2m \u2225y \u2212X\u03b2\u2217\u222522 + \u03bb\u2225\u03b2\u2217\u2225qq.\nThis, together with (1), yields that\n\u03bb\u2225\u03b2\u0302q,\u03bb\u2225qq \u2212 \u03bb\u2225\u03b2\u2217\u2225qq \u2264 1 2m \u2225y \u2212X\u03b2\u2217\u222522 \u2212 1 2m \u2225y \u2212X\u03b2\u0302q,\u03bb\u222522\n= 1\nm\n\u27e8 X(\u03b2\u0302q,\u03bb \u2212 \u03b2\u2217), e \u27e9 \u2212 1\n2m \u2225X\u03b2\u2217 \u2212X\u03b2\u0302q,\u03bb\u222522\n\u2264 1 m \u2225\u03b2\u0302q,\u03bb \u2212 \u03b2\u2217\u22251\u2225X\u22a4e\u2225\u221e \u2212 1 2m \u2225X\u03b2\u2217 \u2212X\u03b2\u0302q,\u03bb\u222522.\nThe proof is complete.\nBelow, we present some notations that are useful for the following discussion of the \u21132 recovery bounds. Recall that \u03b2\n\u2217 is a solution of (1). Throughout the remainder of this paper, let\na > 1, 0 \u2264 \u03b8 < 1, b \u2265 0, (27)\nunless otherwise specified, and let r > 0 be such that\nr \u2265 \u2225\u03b2\u2217\u2225q. (28)\nLet\n\u03f5 := \u03c3 \u221a 5m and \u03c1 :=\n( 5\u03c32\n2\u03bb + rq\n)1/q , (29)\nand select the regularization parameter in (RPq,\u03bb) as\n\u03bb := max\n{ a+ 1\na\u2212 1 \u03c3(1 + \u03b8)21\u2212q(1 + rq)\n1\u2212q q\n\u221a 2(1 + b) log n\nm , 5 2 \u03c32\n} . (30)\nDefine the following two random events relative to linear regression model (1) by\nA := {e : \u2225e\u22252 \u2264 \u03f5} (31)\nand\nB := { e : a+ 1\n(a\u2212 1)m (2\u03c1)1\u2212q\u2225X\u22a4e\u2225\u221e \u2264 \u03bb\n} . (32)\nThe following lemma estimates the probabilities of events A and B.\nLemma 9. The probability of event A satisfies\nP(A ) \u2265 1\u2212 exp(\u2212m). (33)\nMoreover, suppose that (26) is satisfied. Then P(B) \u2265 1\u2212 ( nb \u221a \u03c0 log n )\u22121 , (34)\nP(A \u2229 B) \u2265 1\u2212 exp(\u2212m)\u2212 ( nb \u221a \u03c0 log n )\u22121 . (35)\nProof. By (29) and (31), Lemma 7 is applicable (with d = 5) to showing that P(A c) \u2264 exp(\u2212m), that is, (33) is proved. Then it remains to show (34) and (35). For this purpose, we have by (30) that \u03bb \u2265 52\u03c3\n2, and noting that 0 < q \u2264 1,\n\u03bb \u2265 a+ 1 a\u2212 1\n\u03c3(1 + \u03b8)21\u2212q ( 5\u03c32\n2\u03bb + rq\n) 1\u2212q q \u221a 2(1 + b) log n\nm\n= a+ 1\na\u2212 1 \u03c3(1 + \u03b8)(2\u03c1)1\u2212q\n\u221a 2(1 + b) log n\nm\n(due to (29)). Then one has by (32) that\nP(Bc) \u2264 P\n( a+ 1\n(a\u2212 1)m (2\u03c1)1\u2212q\u2225X\u22a4e\u2225\u221e \u2265\na+ 1 a\u2212 1 \u03c3(1 + \u03b8)(2\u03c1)1\u2212q\n\u221a 2(1 + b) log n\nm\n)\n= P\n( \u2225X\u22a4e\u2225\u221e\nm \u2265 \u03c3(1 + \u03b8)\n\u221a 2(1 + b) log n\nm\n) .\nHence, by assumption (26), Lemma 6 is applicable to ensuring (34). Moreover, it follows from the elementary probability theory that\nP(A \u2229 B) \u2265 P(A )\u2212 P(Bc) \u2265 1\u2212 exp(\u2212m)\u2212 ( nb \u221a \u03c0 log n )\u22121 .\nThe proof is complete.\nWe show in the following two propositions that the optimal solution \u03b2\u0302 of the \u2113q minimization problem (CPq,\u03f5) or the \u2113q regularization problem (RPq,\u03bb) satisfies the following dominant property on the support of the true underlying parameter of (1) with high probability:\n\u2225(\u03b2\u0302 \u2212 \u03b2\u2217)Jc\u2225qq \u2264 c\u2225(\u03b2\u0302 \u2212 \u03b2\u2217)J\u2225qq (36)\nwith c = 1 or c = a, respectively.\nProposition 3. Let \u03b2\u0304q,\u03f5 be an optimal solution of (CPq,\u03f5) with \u03f5 given by (29). Then it holds under the event A that\n\u2225(\u03b2\u0304q,\u03f5 \u2212 \u03b2\u2217)Jc\u2225q \u2264 \u2225(\u03b2\u0304q,\u03f5 \u2212 \u03b2\u2217)J\u2225q. (37)\nProof. Let e \u2208 A . Recall that \u03b2\u2217 satisfies the linear regression model (1), one has that \u2225y \u2212X\u03b2\u2217\u22252 = \u2225e\u22252 \u2264 \u03f5 (under the event A ), and so, \u03b2\u2217 is a feasible vector of (CPq,\u03f5). Consequently, by the optimality of \u03b2\u0304q,\u03f5 for (CPq,\u03f5), it follows that \u2225\u03b2\u0304q,\u03f5\u2225q \u2264 \u2225\u03b2\u2217\u2225q. Write \u03b4 := \u03b2\u0304q,\u03f5 \u2212 \u03b2\u2217. Then we obtain that\n\u2225\u03b2\u2217\u2225qq \u2265 \u2225\u03b2\u2217 + \u03b4\u2225qq = \u2225\u03b2\u2217 + \u03b4J + \u03b4Jc\u2225qq = \u2225\u03b2\u2217 + \u03b4J\u2225qq + \u2225\u03b4Jc\u2225qq, (38)\nwhere the last equality holds because \u03b2\u2217Jc = 0. On the other hand, one has by (8) that \u2225\u03b2\u2217 + \u03b4J\u2225qq \u2265 \u2225\u03b2\u2217\u2225qq \u2212 \u2225\u03b4J\u2225qq. This, together with (38), implies (37). The proof is complete.\nProposition 4. Let \u03b2\u0302q,\u03bb be an optimal solution of (RPq,\u03bb) with \u03bb given by (30). Suppose that (26) is satisfied. Then\n\u2225\u03b2\u0302q,\u03bb \u2212 \u03b2\u2217\u22251 \u2264 (2\u03c1)1\u2212q\u2225\u03b2\u0302q,\u03bb \u2212 \u03b2\u2217\u2225qq (39)\nunder the event A , and\n\u2225(\u03b2\u0302q,\u03bb \u2212 \u03b2\u2217)Jc\u2225qq \u2264 a\u2225(\u03b2\u0302q,\u03bb \u2212 \u03b2\u2217)J\u2225qq (40)\nunder the event A \u2229 B.\nProof. Let e \u2208 A . Since \u03b2\u0302q,\u03bb is an optimal solution of (RPq,\u03bb), one has that\n1\n2m \u2225y \u2212X\u03b2\u0302q,\u03bb\u222522 + \u03bb\u2225\u03b2\u0302q,\u03bb\u2225qq \u2264\n1\n2m \u2225y \u2212X\u03b2\u2217\u222522 + \u03bb\u2225\u03b2\u2217\u2225qq.\nThen, by (1) and (28), it follows that\n\u2225\u03b2\u0302q,\u03bb\u2225qq \u2264 1 2m\u03bb \u2225y \u2212X\u03b2\u2217\u222522 + \u2225\u03b2\u2217\u2225qq \u2264 1 2m\u03bb \u2225e\u222522 + rq \u2264 \u03c1q\n(due to (29) and (31)). Write \u03b4 := \u03b2\u0302q,\u03bb \u2212 \u03b2\u2217. Then, we obtain by (7) and (28) that\n\u2225\u03b4\u22251 \u2264 \u2225\u03b2\u0302q,\u03bb\u22251 + \u2225\u03b2\u2217\u22251 \u2264 \u2225\u03b2\u0302q,\u03bb\u2225q + \u2225\u03b2\u2217\u2225q \u2264 \u03c1+ r < 2\u03c1. Consequently, noting that 0 < q \u2264 1, one sees that \u2225\u03b4\u222512\u03c1 \u2264 ( \u2225\u03b4\u22251 2\u03c1 )q , and then, by (7) that\n\u2225\u03b4\u22251 \u2264 (2\u03c1)1\u2212q\u2225\u03b4\u2225q1 \u2264 (2\u03c1) 1\u2212q\u2225\u03b4\u2225qq. (41)\nThis shows that (39) is proved. Then it remains to claim (40). To this end, noting that \u03b2\u2217Jc = 0, we derive by Lemma 8 that\n\u2212 1 m \u2225\u03b4\u22251\u2225X\u22a4e\u2225\u221e \u2264 \u03bb\u2225\u03b2\u2217\u2225qq \u2212 \u03bb\u2225\u03b2\u2217 + \u03b4\u2225qq\n= \u03bb\u2225\u03b2\u2217J\u2225qq \u2212 \u03bb\u2225\u03b2\u2217J + \u03b4J\u2225qq \u2212 \u03bb\u2225\u03b4Jc\u2225qq \u2264 \u03bb ( \u2225\u03b4J\u2225qq \u2212 \u2225\u03b4Jc\u2225qq ) (by (8)). This, together with (41), yields that\n\u03bb ( \u2225\u03b4J\u2225qq \u2212 \u2225\u03b4Jc\u2225qq ) \u2265 \u2212 1\nm (2\u03c1)1\u2212q\u2225\u03b4\u2225qq\u2225X\u22a4e\u2225\u221e.\nThen, under the event A \u2229 B, we obtain by (32) that (a+ 1) ( \u2225\u03b4J\u2225qq \u2212 \u2225\u03b4Jc\u2225qq ) \u2265 \u2212(a\u2212 1)\u2225\u03b4\u2225qq = \u2212(a\u2212 1)(\u2225\u03b4J\u2225qq + \u2225\u03b4Jc\u2225qq),\nwhich yields (40). The proof is complete.\nRemark 3. By Lemma 9, Propositions 3 and 4 show that (37) holds with probability at least 1 \u2212 exp(\u2212m), and (40) holds with probability at least 1\u2212 exp(\u2212m)\u2212 ( nb \u221a \u03c0 log n )\u22121 if (26) is satisfied, respectively.\nBy virtue of Lemma 9 and Proposition 3, one of the main theorems of this section is as follows, in which we establish the \u21132 recovery bound for the \u2113q minimization problem (CPq,\u03f5) under the q-REC. This theorem provides a unified framework to show that one can stably recover the underlying parameter with high probability via solving the \u2113q minimization problem when the design matrix satisfies the weak q-REC.\nTheorem 1. Let \u03b2\u0304q,\u03f5 be an optimal solution of (CPq,\u03f5) with \u03f5 given by (29). Suppose that X satisfies the q-REC(s, t, 1). Then, with probability at least 1\u2212 exp(\u2212m), we have that\n\u2225\u03b2\u0304q,\u03f5 \u2212 \u03b2\u2217\u222522 \u2264 1 +\n( s t ) 2 q \u22121\n\u03d52q(s, t, 1, X) 4\u03f52. (42)\nProof. Write \u03b4 := \u03b2\u0304q,\u03f5 \u2212 \u03b2\u2217, and let J\u2217 := J \u222a J0(\u03b4; t) (defined by (17)). Fix e \u2208 A . Then it follows from [24, Lemma 7] and Proposition 3 that\n\u2225\u03b4Jc\u2217\u2225 2 2 \u2264 t 1\u2212 2 q \u2225\u03b4Jc\u22252q \u2264 t 1\u2212 2 q \u2225\u03b4J\u22252q \u2264 (s t ) 2 q \u22121 \u2225\u03b4J\u222522 \u2264 (s t ) 2 q \u22121 \u2225\u03b4J\u2217\u222522\n(by (7)), and so\n\u2225\u03b4\u222522 = \u2225\u03b4J\u2217\u222522 + \u2225\u03b4Jc\u2217\u2225 2 2 \u2264\n( 1 + (s t ) 2 q \u22121 ) \u2225\u03b4J\u2217\u222522. (43)\nRecalling that \u03b2\u2217 satisfies the linear regression model (1), we have that \u2225y \u2212X\u03b2\u2217\u22252 = \u2225e\u22252 \u2264 \u03f5 (by (31)), and then\n\u2225X\u03b4\u22252 = \u2225X\u03b2\u0304q,\u03f5 \u2212X\u03b2\u2217\u22252 \u2264 \u2225X\u03b2\u0304q,\u03f5 \u2212 y\u22252 + \u2225X\u03b2\u2217 \u2212 y\u22252 \u2264 2\u03f5. (44)\nOn the other hand, Proposition 3 is applicable to concluding that (37) holds, which shows \u03b4 \u2208 Cq(s, 1) (cf. (13)). Consequently, we obtain by the assumption of the q-REC(s, t, 1) that\n\u2225\u03b4J\u2217\u22252 \u2264 \u2225X\u03b4\u22252\n\u03d5q(s, t, 1, X) .\nThis, together with (43) and (44), implies that (42) holds under the event A . Noting from Lemma 9 that P(A ) \u2265 1\u2212 exp(\u2212m), we obtain the conclusion. The proof is complete.\nIn the special case when the underlying data is noise-free, Theorem 1 shows that (CPq,\u03f5) can exactly predict the parameter for the deterministic linear regression with high probability under the lower-order REC. For the realistic scenario where the measurements are noisy-aware, Theorem 1 illustrates the stable recovery capability of (CPq,\u03f5) in the sense that its solution approaches to the true sparse parameter within a tolerance proportional to the noise level with high probability. Moreover, Theorem 1 establishes the \u21132 recovery bound \u2225\u03b2\u0304q,\u03f5 \u2212 \u03b2\u2217\u22252 = O(\u03f5) under a weaker assumption than the RIP-type or MIP-type condition used in [16, 36], respectively.\nAs a special case of Theorem 1 when q = 1, the following corollary presents the \u21132 recovery bound of the \u21131 minimization problem (CP1,\u03f5) as\n\u2225\u03b2\u03041,\u03f5 \u2212 \u03b2\u2217\u22252 = O(\u03f5) (45)\nunder the classical REC. This result improves the ones in [7, 9] under a weaker assumption, in which the \u21132 recovery bound (45) was obtained under the RIP-type conditions.\nCorollary 1. Let \u03b2\u03041,\u03f5 be an optimal solution of (CP1,\u03f5) with \u03f5 given by (29). Suppose that X satisfies the 1-REC(s, t, 1). Then, with probability at least 1\u2212 exp(\u2212m), we have that\n\u2225\u03b2\u03041,\u03f5 \u2212 \u03b2\u2217\u222522 \u2264 1 + st\n\u03d521(s, t, 1, X) 4\u03f52.\nThe other main theorem of this section is as follows, in which we exploit the statistical properties of the \u2113q regularization problem (RPq,\u03bb) under the q-REC. The results include the estimation of prediction loss and recovery bound of parameter approximation, and also the oracle property, which provides an upper bound on the prediction loss plus the violation of false parameter estimation.\nTheorem 2. Let \u03b2\u0302q,\u03bb be an optimal solution of (RPq,\u03bb) with \u03bb given by (30). Suppose that X satisfies the q-REC(s, t, a) and that (26) is satisfied.\nThen, with probability at least 1\u2212 exp(\u2212m)\u2212 ( nb \u221a \u03c0 log n )\u22121 , we have that\n1\nm \u2225X\u03b2\u0302q,\u03bb \u2212X\u03b2\u2217\u222522 \u2264\n( 2a\u03bb\n(\u03d5q(s, t, a,X)/ \u221a m)q\n) 2 2\u2212q\ns, (46)\n1\n2m \u2225X\u03b2\u0302q,\u03bb \u2212X\u03b2\u2217\u222522 + \u03bb\u2225(\u03b2\u0302q,\u03bb)Jc\u2225qq \u2264\n( 2 q 2a\u03bb\n(\u03d5q(s, t, a,X)/ \u221a m)q\n) 2 2\u2212q\ns, (47)\n\u2225\u03b2\u0302q,\u03bb \u2212 \u03b2\u2217\u222522 \u2264 ( 1 + a 2 q (s t ) 2 q \u22121 )(\n2a\u03bb\n(\u03d5q(s, t, a,X)/ \u221a m)2\n) 2 2\u2212q\ns. (48)\nProof. Write \u03b4 := \u03b2\u0302q,\u03bb \u2212 \u03b2\u2217 and fix e \u2208 A \u2229 B. Note by (39) and (32) that\n1\nm \u2225\u03b4\u22251\u2225X\u22a4e\u2225\u221e \u2264 a\u2212 1 a+ 1 \u03bb\u2225\u03b4\u2225qq.\nThis, together with Lemma 8, implies that\n1\n2m \u2225X\u03b2\u0302q,\u03bb \u2212X\u03b2\u2217\u222522 \u2264 \u03bb\u2225\u03b2\u2217\u2225qq \u2212 \u03bb\u2225\u03b2\u0302q,\u03bb\u2225qq + a\u2212 1 a+ 1 \u03bb\u2225\u03b4\u2225qq\n\u2264 \u03bb\u2225\u03b4J\u2225qq \u2212 \u03bb\u2225(\u03b2\u0302q,\u03bb)Jc\u2225qq + a\u2212 1 a+ 1 \u03bb\u2225\u03b4\u2225qq (49)\n(noting that \u03b2\u2217Jc = 0 and by (8)). Let J\u2217 := J \u222a J0(\u03b4; t). One has by (40) and (7) that\n\u03bb\u2225\u03b4J\u2225qq + a\u2212 1 a+ 1 \u03bb\u2225\u03b4\u2225qq \u2264 a\u03bb\u2225\u03b4J\u2225qq \u2264 a\u03bbs1\u2212 q 2 \u2225\u03b4J\u2225q2,\nand by the assumption of the q-REC(s, t, a) that\n\u2225\u03b4J\u22252 \u2264 \u2225\u03b4J\u2217\u22252 \u2264 \u2225X\u03b4\u22252\n\u03d5q(s, t, a,X) .\nThese two inequalities, together with (49), imply that\n1\n2m \u2225X\u03b2\u0302q,\u03bb \u2212X\u03b2\u2217\u222522 + \u03bb\u2225(\u03b2\u0302q,\u03bb)Jc\u2225qq \u2264\na\u03bbs1\u2212 q 2\n\u03d5qq(s, t, a,X) \u2225X\u03b2\u0302q,\u03bb \u2212X\u03b2\u2217\u2225q2.\nThis yields that\n(46) and (47) hold under the event A \u2229 B. (50)\nFurthermore, it follows from [24, Lemma 7] that\n\u2225\u03b4Jc\u2217\u2225 2 2 \u2264 t 1\u2212 2 q \u2225\u03b4Jc\u22252q \u2264 a 2 q t 1\u2212 2 q \u2225\u03b4J\u22252q \u2264 a 2 q (s t ) 2 q \u22121 \u2225\u03b4J\u222522.\n(by (40) and (7)). By the assumption of the q-REC(s, t, a), one has by (46) that\n\u2225\u03b4J\u2217\u222522 \u2264 \u2225X\u03b4\u222522 \u03d52q(s, t, a,X) \u2264 (\n2a\u03bb\n(\u03d5q(s, t, a,X)/ \u221a m)2\n) 2 2\u2212q\ns.\nHence we obtain that\n\u2225\u03b2\u0302q,\u03bb \u2212 \u03b2\u2217\u222522 = \u2225\u03b4J\u2217\u222522 + \u2225\u03b4Jc\u2217\u2225 2 2 \u2264\n( 1 + a 2 q (s t ) 2 q \u22121 ) \u2225\u03b4J\u2217\u222522\n\u2264 ( 1 + a 2 q (s t ) 2 q \u22121 )(\n2a\u03bb\n(\u03d5q(s, t, a,X)/ \u221a m)2\n) 2 2\u2212q\ns.\nThis shows that\n(48) holds under the event A \u2229 B. (51)\nBy assumption (26), Lemma 9 is applicable to concluding that\nP(A \u2229 B) \u2265 1\u2212 exp(\u2212m)\u2212 ( nb \u221a \u03c0 log n )\u22121 .\nThis, together with (50) and (51), yields that (46)-(48) hold with probability at least 1\u2212 exp(\u2212m)\u2212 ( nb \u221a \u03c0 log n )\u22121 . The proof is complete.\nRemark 4. (i) It is worth noting that each of the estimations provided in Theorem 2 (cf. (46)-(48)) involves the term \u03d5q(s, t, a,X)/ \u221a m in the denominator, which scales as a constant if X has i.i.d. Gaussian entries; see Remark 1(ii).\n(ii) Theorem 2 provides a unified framework of the statistical properties of the \u2113q regularization problem under the weak q-REC that is one of the weakest regularity conditions in the literature, in which each of the obtained estimations depends on the noise amplitude and sample size. In particular,\nfor the regularization parameter scaling as \u03bb \u224d max ( \u03c3 \u221a\nlogn m , \u03c3 2\n) (cf.\n(30)), Theorem 2 indicates the prediction loss and the \u21132 recovery bound for (RPq,\u03bb) scale as\n1\nm \u2225X\u03b2\u0302q,\u03bb \u2212X\u03b2\u2217\u222522 = O\n(( \u03c32 log n\nm\n) 1 2\u2212q\ns ) ,\nand\n\u2225\u03b2\u0302q,\u03bb \u2212 \u03b2\u2217\u222522 = O\n(( \u03c32 log n\nm\n) 1 2\u2212q\ns ) . (52)\n(iii) It was shown in [43] that the global solution of the FCP sparse linear regression, including the SCAD and MCP as special cases, has an \u21132 recovery bound O(\u03bb2s) under the SEC. Though the recovery bounds are slightly better than (52), the condition required is substantially stronger than the q-REC. In [43], the authors also established the oracle property for the \u21130 regularization method under the SEC; while its \u21132 recovery bound cannot be guaranteed in their work. We shall see in section 5 that the \u2113q regularization method performs better in parameter estimation than either the SCAD/MCP or the \u21130 regularization method via several numerical experiments.\nRemark 5. Recently, some works concerned the statistical property for the local minimum of some nonconvex regularization problems; see [25, 27].\n(i) Loh and Wainwright [27] studied the \u21132 recovery bound for the local minimum of a general regularization problem:\nmin Lm(\u03b2;X) + n\u2211\nj=1\n\u03c1\u03bb(\u03b2j), (53)\nwhere Lm : Rn \u00d7 Rm\u00d7n \u2192 R is the loss function, and \u03c1\u03bb : R \u2192 R is the (possibly nonconvex) penalty function. In [27], the penalty function \u03c1\u03bb is assumed to satisfy the following assumptions:\n(a) \u03c1\u03bb(0) = 0 and is symmetric around zero; (b) \u03c1\u03bb is nondecreasing on R+; (c) For t > 0, the function t 7\u2192 \u03c1\u03bb(t)t is nonincreasing in t; (d) \u03c1\u03bb is differentiable for each t \u0338= 0 and subdifferentiable at t = 0, with\nlim t\u21920+\n\u03c1 \u2032 \u03bb(t) = \u03bbL;\n(e) There exists \u00b5 > 0 such that \u03c1\u03bb,\u00b5(t) := \u03c1\u03bb(t) + \u00b5 2 t 2 is convex. Loh and Wainwright established in [27, Theorem 1] the \u21132 recovery bound for the critical point satisfying the first-order necessary condition of (53) under the restricted strong convex condition, which is a variant of the classical REC.\nThe \u2113q norm can be reformulated as the penalty function \u03c1\u03bb(\u03b2j) := \u03bb|\u03b2j |q, however, it does not satisfy assumptions (d) or (e); in particular, assumption (e) plays a key role in the establishment of oracle property and \u21132 recovery bound for the local minimum. Therefore, the result in [27] cannot be directly applied to the \u2113q regularization problem, and the oracle property for the general local minimum of the \u2113q regularization problem is still unsolved at this moment.\n(ii) Liu et al. [25] studied the statistical property of the FCP sparse linear regression and presented the oracle property and \u21132 recovery bound for the certain local minimum, which satisfies a subspace second-order necessary condition and lies in the level set of the FCP regularized function at the true solution, under the SEC. Although the \u2113q regularizer is beyond the FCP, our established Theorem 2 provides a theoretical result similar to [25] in the sense that the oracle property and \u21132 recovery bound are shown for the local minimum within the level set of the \u2113q regularized function at the true solution.\nAs an application of Theorem 2 to the case when q = 1, the following corollary presents the statistical properties of the \u21131 regularization problem under the classical REC, which covers [4, Theorem 7.2] as a special case when a = 3, \u03b8 = 0 and b = 0. The same \u21132 recovery bound rate O(\u03c3\n2s log n/m) was reported in [42] under the sparse Riesz condition, which is comparable with the classical REC; while the same oracle inequality rate O(\u03c32s log n/m) was established in [39] under the compatibility condition, which is slightly weaker than the classical REC but cannot guarantee the \u21132 recovery bound.\nCorollary 2. Let \u03b2\u03021,\u03bb be an optimal solution of (RP1,\u03bb) with\n\u03bb = 2\u03c3(1 + \u03b8)\n\u221a 2(1 + b) log n\nm .\nSuppose that X satisfies the 1-REC(s, t, 3) and that (26) is satisfied. Then, with probability at least 1\u2212 ( nb \u221a \u03c0 log n )\u22121 , we have that\n1\nm \u2225X\u03b2\u03021,\u03bb \u2212X\u03b2\u2217\u222522 \u2264\n288(1 + b)(1 + \u03b8)2\n\u03d521(s, t, 3, X)/m \u03c32s\nlog n\nm ,\n1\n2m \u2225X\u03b2\u03021,\u03bb \u2212X\u03b2\u2217\u222522 + \u03bb\u2225(\u03b2\u03021,\u03bb)Jc\u22251 \u2264\n144(1 + b)(1 + \u03b8)2\n\u03d521(s, t, 3, X)/m \u03c32s\nlogn\nm ,\n\u2225\u03b2\u03021,\u03bb \u2212 \u03b2\u2217\u222522 \u2264 288(1 + b)(1 + \u03b8)2\n( 1 + 9 st ) \u03d541(s, t, 3, X)/m 2 \u03c32s log n m ."
        },
        {
            "heading": "4 Recovery Bounds for Random Design",
            "text": "In practical applications, it is a more realistic scenario that the design matrix X is random. In this section, we consider this situation and present the \u21132 recovery bounds for (CPq,\u03f5) and (RPq,\u03bb) by virtue of the results obtained in the preceding section. In particular, throughout this section, we shall assume that the linear regression model (1) involves a Gaussian noise, i.e., e \u223c N (0, \u03c32Im), and\nX \u2208 Rm\u00d7n is a Gaussian random design with i.i.d. N (0,\u03a3) rows,\nthat is, X1\u00b7, . . . , Xm\u00b7 are i.i.d. random vectors with each Xi\u00b7 \u223c N (0,\u03a3). Recall that a, \u03b8, and b are given by (27), and let (s, t) be a pair of integers satisfying (5).\nTo study the statistical properties of (CPq,\u03f5) and (RPq,\u03bb) with a random design X, we first provide some sufficient condition for the q-REC of X in terms of the population covariance matrix \u03a3. For this purpose, we use \u03a3 1 2 to denote the square root of \u03a3 and \u03b6(\u03a3) := max1\u2264j\u2264n\u03a3j,j to denote the maximal variance. Let a > 0, and two random events related to the linear regression model (1) with X being a Gaussian random design are defined as follows\nCa := { \u03d5q(s, t, a,X) > \u221a m\n2 \u03d5q(s, t, a,\u03a3\n1 2 ) } , (54)\nand\nD := { max 1\u2264j\u2264n \u2225X\u00b7j\u22252 \u2264 (1 + \u03b8) \u221a m } . (55)\nThe following lemma is taken from [1, Supplementary, Lemma 6], which is useful for providing a sufficient condition for the q-REC of X.\nLemma 10. There exist universal positive constants (c1, c2) (independent of m,n,\u03a3) such that it holds with probability at least 1 \u2212 exp(\u2212c2m) that, for each \u03b4 \u2208 Rn\n\u2225X\u03b4\u222522 m \u2265 1 2 \u2225\u03a3 1 2 \u03b4\u222522 \u2212 c1\u03b6(\u03a3) logn m \u2225\u03b4\u222521. (56)\nThe following lemma calculates the probabilities of events Cc and D , which is crucial for establishing the \u21132 recovery bounds of (CPq,\u03f5) and (RPq,\u03bb) with a random design X. In particular, part (i) of this lemma shows that the Gaussian random design X satisfies the q-REC with high probability as long as the sample size m is sufficiently large and the square root of its population covariance matrix \u03a3 1 2 satisfies the q-REC; part (ii) of this lemma presents that each column of the Gaussian random design X has an Euclidean norm scaling as \u221a m with an overwhelming probability.\nLemma 11. (i) Let a > 0. Suppose that \u03a3 1 2 satisfies the q-REC(s, t, a). Then, there exist universal positive constants (c1, c2) (independent of m,n, q, s, t, a,\u03a3) such that, if\nm > c1\u03b6(\u03a3)\n\u03d52q(s, t, a,\u03a3 1 2 )\n(\u221a s+ t+ a \u221a s (as\nt\n) 1 q \u22121 )2\nlog n, (57)\nthen P(Ca) \u2265 1\u2212 exp(\u2212c2m). (58)\n(ii) Suppose that \u03a3j,j = 1 for all j = 1, . . . , n. Then, there exist universal positive constants (c3, c4) and \u03c4 \u2265 1 (independent of m,n, \u03b8,\u03a3) such that, if\nm > c3\u03c4\n4\n\u03b82 log n, (59)\nthen P(D) \u2265 1\u2212 2 exp(\u2212c4\u03b82m/\u03c44). (60)\nProof. (i) We first claim that\n\u03d5q(s, t, a,X) >\n\u221a m\n2 \u03d5q(s, t, a,\u03a3\n1 2 ), (61)\nwhenever (56) holds for each \u03b4 \u2208 Rn. To this end, we suppose that (56) is satisfied for each \u03b4 \u2208 Rn. Fix \u03b4 \u2208 Cq(s, a), and let J , r, Jk (for each k \u2208 N)\nand J\u2217 be defined, respectively, as in the beginning of the proof of Lemma 5. Then (22) follows directly, and one has that\n\u2225\u03b4\u22251 = \u2225\u03b4J\u2217\u22251 + \u2225\u03b4Jc\u2217\u22251\n\u2264 \u221a s+ t\u2225\u03b4J\u2217\u22252 + a \u221a s (as\nt\n) 1 q \u22121\n\u2225\u03b4J\u22252 \u2264 (\u221a s+ t+ a \u221a s (as\nt\n) 1 q \u22121 ) \u2225\u03b4J\u2217\u22252.\n(62)\nBy the assumption that \u03a3 1 2 satisfies the q-REC(s, t, a), it follows that\n\u2225\u03a3 1 2 \u03b4\u222522 \u2265 \u03d52q(s, t, a,\u03a3 1 2 )\u2225\u03b4J\u2217\u222522.\nSubstituting this inequality and (62) into (56) yields\n\u2225X\u03b4\u222522 m \u2265\n( 1\n2 \u03d52q(s, t, a,\u03a3 1 2 )\u2212 c1\u03b6(\u03a3)\n(\u221a s+ t+ a \u221a s (as\nt\n) 1 q \u22121 )2 log n\nm\n) \u2225\u03b4J\u2217\u222522.\nThis, together with (57), shows that\n\u2225X\u03b4\u222522 m \u2265 1 4 \u03d52q(s, t, a,\u03a3 1 2 )\u2225\u03b4J\u2217\u222522.\nSince \u03b4 and J satisfying (20) are arbitrary, we derive by (6) that (61) holds, as desired. Then, Lemma 10 is applicable to concluding (58). (ii) Noting by the assumption that \u03a3j,j = 1 for all j = 1, . . . , n, [46, Theorem 1.6] is applicable to showing that there exist universal positive constants (c1, c2) and \u03c4 \u2265 1 such that\nP ( \u2229nj=1 { (1\u2212 \u03b8) \u221a m \u2264 \u2225X\u00b7j\u22252 \u2264 (1 + \u03b8) \u221a m }) \u2265 1\u2212 2 exp(\u2212c2\u03b82m/\u03c44),\nwhenever m satisfies (59). Then it immediately follows from (55) that\nP(D) = P(\u2229nj=1{\u2225X\u00b7j\u22252 \u2264 (1 + \u03b8) \u221a m})\n\u2265 1\u2212 2 exp(\u2212c2\u03b82m/\u03c44),\nthat is, (60) is proved.\nRemark 6. (i) As a direct application of Lemma 11(i), the classical REC is satisfied by X with high probability if \u03a3 1 2 satisfies the classical REC(s,t,a) and\nm > c1\u03b6(\u03a3)\n\u03d521(s, t, a,\u03a3 1 2 )\n(\u221a s+ t+ a \u221a s )2 log n,\nwhich covers [32, Corollary 1] as a special case when t = 0. (ii) Recall from Remark 1 that \u03d5q(s, t, a,X) given by (6) usually scales as \u221a m independent of s and n for the Gaussian random design X. Then Lemma 11(i) is applicable to indicating that \u03d5q(s, t, a,\u03a3 1 2 ) usually scales as a constant independent of s, m and n.\nBelow, we consider the dominant property (36) in the situation when X is a Gaussian random design. For the \u2113q minimization problem (CPq,\u03f5), Proposition 3 is still applicable for the case when X is a Gaussian random design since it does not rely on the assumption of X, and thus, (37) holds with the same probability for the random design scenario; see Remark 3. In the following proposition, we show the dominant property (40) for the \u2113q regularization problem (RPq,\u03bb) with a random design by virtue of Proposition 4. Recall that \u03f5, \u03bb, \u03c1 and the events A and B are given in the preceding section; see (29)-(32) for details.\nProposition 5. Let \u03b2\u0302q,\u03bb be an optimal solution of (RPq,\u03bb) with \u03bb given by (30). Suppose that \u03a3j,j = 1 for all j = 1, . . . , n. Then, there exist universal positive constants (c1, c2) and \u03c4 \u2265 1 (independent of m,n, q, a, \u03b8, b, \u03f5, r, \u03bb,\u03a3) such that, if\nm > c1\u03c4\n4\n\u03b82 log n, (63)\nthen (40) holds with probability at least (1\u2212 ( nb \u221a \u03c0 log n )\u22121 )(1\u22122 exp(\u2212c2\u03b82m/\u03c44))\u2212 exp(\u2212m).\nProof. By (55), one sees by Proposition 4 that (40) holds under the event A \u2229B \u2229D . Then it remains to estimate P(A \u2229B \u2229D). By Lemma 11(ii), there exist universal positive constants (c1, c2) and \u03c4 \u2265 1 such that\nP(D) \u2265 1\u2212 2 exp(\u2212c2\u03b82m/\u03c44),\nwhenever m satisfies (63). From Lemma 9 (cf. (34)), we have also by (55) that P(B|D) \u2265 1\u2212 (nb \u221a \u03c0 log n)\u22121.\nThen, it follows that\nP(B \u2229 D) = P(B|D)P(D) \u2265 (1\u2212 (nb \u221a \u03c0 log n)\u22121)(1\u2212 2 exp(\u2212c2\u03b82m/\u03c44)),\nand then by the elementary probability theory and (33) that,\nP(A \u2229 B \u2229 D) = P(B \u2229 D)\u2212 P(B \u2229 D \u2229 A c) \u2265 P(B \u2229 D) + P(A )\u2212 1\n\u2265 ( 1\u2212 ( nb \u221a \u03c0 log n )\u22121) (1\u2212 2 exp(\u2212c2\u03b82m/\u03c44))\u2212 exp(\u2212m),\nwhenever m satisfies (63). The proof is complete.\nNow we are ready to present the main theorems of this section, in which we establish the \u21132 recovery bounds for (CPq,\u03f5) and (RPq,\u03bb) when X is a Gaussian random design. The first theorem illustrates the stable recovery capability of the \u2113q minimization problem (CPq,\u03f5) (within a tolerance proportional to the noise) with high probability when the design matrix is random as long as the vector \u03b2\u2217 is sufficiently sparse and the sample size m is sufficiently large.\nTheorem 3. Let \u03b2\u0304q,\u03f5 be an optimal solution of (CPq,\u03f5) with \u03f5 given by (29). Suppose that \u03a3 1 2 satisfies the q-REC(s, t, 1). Then, there exist universal positive constants (c1, c2) (independent of m,n, q, s, t, \u03f5,\u03a3) such that, if (57) is satisfied, then it holds with probability at least (1\u2212exp(\u2212m))(1\u2212exp(\u2212c2m)) that\n\u2225\u03b2\u0304q,\u03f5 \u2212 \u03b2\u2217\u222522 \u2264 16(1 +\n( s t ) 2 q \u22121 )\nm\u03d52q(s, t, 1,\u03a3 1 2 ) \u03f52. (64)\nProof. To simplify the proof, corresponding to inequalities (42) and (64), we define the following two events\nE1 := \u2225\u03b2\u0304q,\u03f5 \u2212 \u03b2\u2217\u222522 \u2264 4(1 + ( s t ) 2 q \u22121 ) \u03d52q(s, t, 1, X) \u03f52  , E2 := \u2225\u03b2\u0304q,\u03f5 \u2212 \u03b2\u2217\u222522 \u2264 16(1 + ( s t ) 2 q \u22121 )\nm\u03d52q(s, t, 1,\u03a3 1 2 ) \u03f52  . Then, by the definition of C1 (54), we have that C1 \u2229 E1 \u2286 E2 and thus\nP(E2) \u2265 P(E1 \u2229 C1) = P(E1|C1)P(C1). (65)\nNote by Theorem 1 that\nP(E1|C1) \u2265 1\u2212 exp(\u2212m). (66)\nBy Lemma 11(i) (with a = 1), there exist universal positive constants (c1, c2) such that (57) ensures (58). Then we obtain by (65) and (66) that\nP(E2) \u2265 (1\u2212 exp(\u2212m))(1\u2212 exp(\u2212c2m)),\nwhenever m satisfies (57). The proof is complete.\nAs a direct application of Theorem 3 to the special case when q = 1, the following corollary presents the \u21132 recovery bound of the \u21131 minimization problem (CP1,\u03f5) with a Gaussian random design as\n\u2225\u03b2\u03041,\u03f5 \u2212 \u03b2\u2217\u22252 = O(\u03f5)\nunder the classical REC.\nCorollary 3. Let \u03b2\u03041,\u03f5 be an optimal solution of (CP1,\u03f5) with \u03f5 given by (29). Suppose that \u03a3 1 2 satisfies the 1-REC(s, t, 1). Then, there exist universal positive constants (c1, c2) (independent of m,n, q, s, t, \u03f5,\u03a3) such that, if\nm > c1\u03b6(\u03a3) \u03d521(s, t, 1,\u03a3 1 2 ) ( \u221a s+ t+ \u221a s)2 log n,\nthen it holds with probability at least (1\u2212 exp(\u2212m))(1\u2212 exp(\u2212c2m)) that\n\u2225\u03b2\u03041,\u03f5 \u2212 \u03b2\u2217\u222522 \u2264 16(1 + st )\nm\u03d521(s, t, 1,\u03a3 1 2 ) \u03f52.\nThe other main theorem of this section is as follows, in which we exploit the estimation of prediction loss, the oracle property and the \u21132 recovery bound of parameter approximation of the \u2113q regularization problem (RPq,\u03bb) with a Gaussian random design under the q-REC of the square root of its population covariance matrix.\nTheorem 4. Let \u03b2\u0302q,\u03bb be an optimal solution of (RPq,\u03bb) with \u03bb given by (30). Suppose that \u03a3j,j = 1 for all j = 1, . . . , n and \u03a3 1 2 satisfies the qREC(s, t, a). Then, there exist universal positive constants (c1, c2, c3, c4) and \u03c4 \u2265 1 (independent of m,n, q, s, t, a, \u03b8, b, \u03f5, r, \u03bb,\u03a3) such that, if\nm > max c1( \u221a s+ t+ a 1 q \u221a s ( s t ) 1 q \u22121 )2\n\u03d52q(s, t, a,\u03a3 1 2 )\nlog n, c3\u03c4\n4\n\u03b82 log n  , (67)\nthen it holds with probability at least ( 1\u2212 exp(\u2212m)\u2212 ( nb \u221a \u03c0 log n )\u22121) (1\u2212\nexp(\u2212c2m)\u2212 2 exp(\u2212c4\u03b82m/\u03c44)) that\n1\nm \u2225X\u03b2\u0302q,\u03bb \u2212X\u03b2\u2217\u222522 \u2264\n( 2q+1a\u03bb\n\u03d5qq(s, t, a,\u03a3 1 2 )\n) 2 2\u2212q\ns, (68)\n1\n2m \u2225X\u03b2\u0302q,\u03bb \u2212X\u03b2\u2217\u222522 + \u03bb\u2225(\u03b2\u0302q,\u03bb)Jc\u2225qq \u2264\n( 8 q 2a\u03bb\n\u03d5qq(s, t, a,\u03a3 1 2 )\n) 2 2\u2212q\ns, (69)\n\u2225\u03b2\u0302q,\u03bb \u2212 \u03b2\u2217\u222522 \u2264 ( 1 + a 2 q (s t ) 2 q \u22121 )(\n8a\u03bb\n\u03d52q(s, t, a,\u03a3 1 2 )\n) 2 2\u2212q\ns. (70)\nProof. To simplify the proof, we define the following six events\nF1 = {(46) happens} , F2 = {(47) happens} , F3 = {(48) happens} , G1 = {(68) happens} , G2 = {(69) happens} , G3 = {(70) happens} .\nFix i \u2208 {1, 2, 3}. Then, we have by (54) that Ca \u2229 Fi \u2286 Gi and thus\nP(Gi) \u2265 P(Ca \u2229 Fi). (71)\nBy Lemma 11, there exist universal positive constants (c1, c2, c3, c4) and \u03c4 \u2265 1 such that, (67) ensures (58) and (60). Then it follows from (58) and (60) that\nP(Ca\u2229D) \u2265 P(Ca)+P (D)\u22121 \u2265 1\u2212exp(\u2212c2m)\u22122 exp(\u2212c4\u03b82m/\u03c44), (72)\nwhenever m satisfies (67). Recall from Theorem 2 that P(Fi|Ca \u2229 D) \u2265 1\u2212 exp(\u2212m)\u2212 ( nb \u221a \u03c0 log n )\u22121 .\nThis, together with (72), implies that\nP(Ca \u2229 Fi) \u2265 P(Fi|Ca \u2229 D)P(Ca \u2229 D) \u2265 ( 1\u2212 exp(\u2212m)\u2212 ( nb \u221a \u03c0 logn )\u22121) (1\u2212 exp(\u2212c2m)\u2212 2 exp(\u2212c4\u03b82m/\u03c44)).\nThen, one has by (71) that P(Gi) \u2265 ( 1\u2212 exp(\u2212m)\u2212 ( nb \u221a \u03c0 log n )\u22121) (1\u2212exp(\u2212c2m)\u22122 exp(\u2212c4\u03b82m/\u03c44)),\nwhenever m satisfies (67). The proof is complete.\nAs an application of Theorem 4 to the special case when q = 1 and a = 3, the following corollary presents the statistical properties of the \u21131 regularization problem with a Gaussian random design under the classical REC. A similar \u21132 recovery bound was shown in [46, Theorem 3.1] by using a different analytic technique.\nCorollary 4. Let \u03b2\u03021,\u03bb be an optimal solution of (RP1,\u03bb) with\n\u03bb = 2\u03c3(1 + \u03b8)\n\u221a 2(1 + b) log n\nm .\nSuppose that \u03a3j,j = 1 for all j = 1, . . . , n and \u03a3 1 2 satisfies the 1-REC(s, t, 3). Then, there exist universal positive constants (c1, c2, c3, c4) and \u03c4 \u2265 1 (independent of m,n, s, t, \u03b8, b,\u03a3) such that, if\nm > max\n{ c1( \u221a s+ t+ 3 \u221a s)2\n\u03d521(s, t, 3,\u03a3 1 2 )\nlog n, c3\u03c4\n4\n\u03b82 log n\n} ,\nthen it holds with probability at least (1 \u2212 exp(\u2212m) \u2212 ( nb \u221a \u03c0 log n )\u22121 )(1 \u2212 exp(\u2212c2m)\u2212 2 exp(\u2212c4\u03b82m/\u03c44)) that\n1\nm \u2225X\u03b2\u03021,\u03bb \u2212X\u03b2\u2217\u222522 \u2264\n1152(1 + b)(1 + \u03b8)2\n\u03d521(s, t, 3,\u03a3 1 2 )\ns log n\nm \u03c32,\n1\n2m \u2225X\u03b2\u03021,\u03bb \u2212X\u03b2\u2217\u222522 + \u03bb\u2225(\u03b2\u03021,\u03bb)Jc\u22251 \u2264\n576(1 + b)(1 + \u03b8)2\n\u03d521(s, t, 3,\u03a3 1 2 )\ns log n\nm \u03c32,\n\u2225\u03b2\u03021,\u03bb \u2212 \u03b2\u2217\u222522 \u2264 4608(1 + b)(1 + \u03b8)2(1 + 9 st )\n\u03d541(s, t, 3,\u03a3 1 2 )\ns log n\nm \u03c32."
        },
        {
            "heading": "5 Numerical Experiments",
            "text": "The purpose of this section is to carry out the numerical experiments to illustrate the stability of the \u2113q optimization methods, verify the established theory of the \u21132 recovery bounds in the preceding sections and compare the numerical performance of the \u2113q regularization methods with another two widely used nonconvex regularization methods, namely the SCAD and MCP. In particular, we are concerned with the cases when q = 0, q = 1/2, 2/3 and 1. To solve the \u2113q minimization problems, we will apply the iterative reweighted algorithm [11, 13] . To solve the \u2113q regularization problems,\nwe will apply the iterative hard thresholding algorithm [5] for q = 0, the proximal gradient algorithm [24] for q = 1/2 and 2/3, and FISTA [3] for q = 1, respectively. The proximal gradient algorithm proposed in [27] will be used to solve the SCAD and MCP. All numerical experiments are performed in MATLAB R2014b and executed on a personal desktop (Intel Core i7-4790, 3.60 GHz, 8.00 GB of RAM).\nThe simulated data are generated via a standard process; see, e.g., [1, 24]. Specifically, we randomly generate an i.i.d. Gaussian ensemble X \u2208 Rm\u00d7n and a sparse vector \u03b2\u2217 \u2208 Rn with the sparsity being equal to s. The observation y is then generated by the MATLAB script\ny = X \u2217 \u03b2\u2217 + sigma \u2217 randn(m, 1),\nwhere sigma is the noise level, that is the standard deviation of Gaussian noise. In the numerical experiments, the dimension of variables and the noise level are set as n = 1024 and sigma = 0.01, respectively.\nFor each sparsity level (e.g., s/n = 5%, 10%, 15%, 20%), we randomly generate the data X, \u03b2\u2217, y for 100 times and run the algorithms mentioned above to solve the \u2113q optimization problems for q = 0, 1/2, 2/3 and 1 as well as the SCAD and MCP. The parameter \u03f5 in the \u2113q minimization methods\n(CP)q,\u03f5 is set as \u03f5 = sigma \u2217 \u221a m+ 2 \u221a 2m in order to guarantee that \u2225e\u222522 is no more than \u03f52 with overwhelming probability [9, 11]. The parameter \u03bb in the \u2113q regularization methods (RPq,\u03bb) is chosen by 10-fold cross validation. To simplify the notations, the solution of different problems will all be denoted as \u03b2\u0302. In order to reveal the dependence of \u21132 recovery bounds on sample size and inspired by the established theorems in the preceding sections (e.g., (67)), we report the numerical results for a range of sample sizes of the form m = \u2126(s log n).\nThe first experiment is conduct to show the performance on parameter estimation of the \u2113q minimization methods. Figure 1 plots the estimated error \u2225\u03b2\u0302 \u2212 \u03b2\u2217\u22252 along with different sample size m. From Figure 1, we can see that the estimated error of each minimization method decreases consistently as the sample size increases. In addition, we find that the lower the q, the better the corresponding minimization method to achieve a more accurate solution.\nThe second experiment is carried out to show the performance on parameter estimation of the \u2113q regularization methods and compare the performance with the SCAD and MCP. The corresponding result is displayed in Figure 2, which plots the estimated error \u2225\u03b2\u0302 \u2212 \u03b2\u2217\u22252 along with the sample size m. As shown by Figure 2, the estimated error of each regularization method\ndecreases consistently as the sample size increases, and that the lower-order regularization method (e.g., when q = 1/2, 2/3) outperforms the \u21130/\u21131 regularization method in the sense that its estimated error decreases faster when the sample size increases and achieves a more accurate solution than the \u21130/\u21131 regularization method. This is due to the fact that the q-REC is satisfied when the sample size is larger than a certain level (see Lemma 11(i)) and the lower-order regularization method requires a weaker q-REC to guarantee its nice statistical property (see Theorems 2 and 4). This result is consistent with the existing empirical studies on the \u2113q regularization methods as in [40, 24]. In addition, it is obvious that the lower-order regularization methods perform much better than the SCAD and MCP to achieve an accurate solution no matter whether the sparsity level is high or low.\nThe third experiment is implemented to study the performance on variable selection of the \u2113q regularization methods as well as the SCAD and MCP. We use two criteria to characterize the capability of variable selection, namely the sensitivity = true positivetrue positive+false negative and the specificity = true negative true negative+false positive , which respectively measures the proportion of positives and negatives that are correctly identified. The larger values of both sensitivity and specificity mean the higher capability of variable selection. The results are illustrated by averaging over the 100 random trials. Tables 1 and 2 respectively chart the sensitivity and specificity of these methods at a sparsity level 10% corresponding to Figure 2(b). It is illustrated that the sensitivity and specificity of all these methods increase as the sample size grows, except for the specificity of Lasso, which is resulted from the fact that there are many small nonzero coefficients estimated by Lasso. We also note that the lower-order regularization method (e.g., when q = 1/2, 2/3) outperforms the other regularization methods in the sense that it can almost completely select the true model from a small number of samples."
        }
    ],
    "title": "Sparse estimation via lq optimization method in high-dimensional linear regression",
    "year": 2019
}