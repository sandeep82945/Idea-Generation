{
    "abstractText": "The advent of personalized news recommendation has given rise to increasingly complex recommender architectures. Most neural news recommenders rely on user click behavior and typically introduce dedicated user encoders that aggregate the content of clicked news into user embeddings (early fusion). These models are predominantly trained with standard point-wise classification objectives. The existing body of work exhibits two main shortcomings: (1) despite general design homogeneity, direct comparisons between models are hindered by varying evaluation datasets and protocols; (2) it leaves alternative model designs and training objectives vastly unexplored. In this work, we present a unified framework for news recommendation, allowing for a systematic and fair comparison of news recommenders across several crucial design dimensions: (i) candidate-awareness in user modeling, (ii) click behavior fusion, and (iii) training objectives. Our findings challenge the status quo in neural news recommendation. We show that replacing sizable user encoders with parameter-efficient dot products between candidate and clicked news embeddings (late fusion) often yields substantial performance gains. Moreover, our results render contrastive training a viable alternative to point-wise classification objectives.",
    "authors": [
        {
            "affiliations": [],
            "name": "Andreea Iana"
        },
        {
            "affiliations": [],
            "name": "Goran Glava\u0161"
        },
        {
            "affiliations": [],
            "name": "Heiko Paulheim"
        }
    ],
    "id": "SP:8864110b455e124ccb4d9b9e88b6c9623bba0dac",
    "references": [
        {
            "authors": [
                "Mingxiao An",
                "Fangzhao Wu",
                "Chuhan Wu",
                "Kun Zhang",
                "Zheng Liu",
                "Xing Xie"
            ],
            "title": "Neural News Recommendation with Long- and Short-term User Representations",
            "venue": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
            "year": 2019
        },
        {
            "authors": [
                "Dzmitry Bahdanau",
                "Kyunghyun Cho",
                "Yoshua Bengio"
            ],
            "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
            "year": 2014
        },
        {
            "authors": [
                "Antoine Bordes",
                "Nicolas Usunier",
                "Alberto Garcia-Duran",
                "Jason Weston",
                "Oksana Yakhnenko"
            ],
            "title": "Translating Embeddings for Modeling Multi-relational Data",
            "venue": "In Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume",
            "year": 2013
        },
        {
            "authors": [
                "Kyunghyun Cho",
                "Bart van Merri\u00ebnboer",
                "Caglar Gulcehre",
                "Dzmitry Bahdanau",
                "Fethi Bougares",
                "Holger Schwenk",
                "Yoshua Bengio"
            ],
            "title": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation",
            "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
            "year": 2014
        },
        {
            "authors": [
                "Shansan Gong",
                "Kenny Q Zhu"
            ],
            "title": "Positive, Negative and Neutral: Modeling Implicit Feedback in Session-based News Recommendation",
            "venue": "In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval",
            "year": 2022
        },
        {
            "authors": [
                "Jon Atle Gulla",
                "Lemei Zhang",
                "Peng Liu",
                "\u00d6zlem \u00d6zg\u00f6bek",
                "Xiaomeng Su"
            ],
            "title": "The Adressa Dataset for News Recommendation",
            "venue": "In Proceedings of the International Conference on Web Intelligence",
            "year": 2017
        },
        {
            "authors": [
                "Po-Sen Huang",
                "Xiaodong He",
                "Jianfeng Gao",
                "Li Deng",
                "Alex Acero",
                "Larry Heck"
            ],
            "title": "Learning Deep Structured Semantic Models for Web Search using Clickthrough Data",
            "venue": "In Proceedings of the 22nd ACM international conference on Information & Knowledge Management",
            "year": 2013
        },
        {
            "authors": [
                "Andreea Iana",
                "Mehwish Alam",
                "Heiko Paulheim"
            ],
            "title": "A Survey on Knowledge-aware News Recommender Systems",
            "venue": "Semantic Web Preprint",
            "year": 2022
        },
        {
            "authors": [
                "Guoliang Ji",
                "Shizhu He",
                "Liheng Xu",
                "Kang Liu",
                "Jun Zhao"
            ],
            "title": "Knowledge Graph Embedding via DynamicMappingMatrix",
            "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume",
            "year": 2015
        },
        {
            "authors": [
                "Prannay Khosla",
                "Piotr Teterwak",
                "ChenWang",
                "Aaron Sarna",
                "Yonglong Tian",
                "Phillip Isola",
                "Aaron Maschinot",
                "Ce Liu",
                "Dilip Krishnan"
            ],
            "title": "Supervised Contrastive Learning",
            "venue": "In Proceedings of the 34th International Conference on Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Yoon Kim"
            ],
            "title": "Convolutional Neural Networks for Sentence Classification",
            "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics,",
            "year": 2014
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A Method for Stochastic Optimization",
            "venue": "ICLR",
            "year": 2014
        },
        {
            "authors": [
                "Miaomiao Li",
                "Licheng Wang"
            ],
            "title": "A Survey on Personalized News Recommendation Technology",
            "venue": "IEEE Access",
            "year": 2019
        },
        {
            "authors": [
                "Yizhi Li",
                "Zhenghao Liu",
                "Chenyan Xiong",
                "Zhiyuan Liu"
            ],
            "title": "More Robust Dense Retrieval with Contrastive Dual Learning",
            "venue": "In Proceedings of the 2021 ACM SIGIR International Conference on Theory of Information Retrieval",
            "year": 2021
        },
        {
            "authors": [
                "Shumpei Okura",
                "Yukihiro Tagami",
                "Shingo Ono",
                "Akira Tajima"
            ],
            "title": "Embedding-based News Recommendation for Millions of Users",
            "venue": "In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data",
            "year": 2017
        },
        {
            "authors": [
                "Aaron van den Oord",
                "Yazhe Li",
                "Oriol Vinyals"
            ],
            "title": "Representation Learning with Contrastive Predictive Coding",
            "venue": "arXiv preprint arXiv:1807.03748",
            "year": 2018
        },
        {
            "authors": [
                "Jeffrey Pennington",
                "Richard Socher",
                "Christopher D Manning"
            ],
            "title": "Glove: Global Vectors for Word Representation",
            "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
            "year": 2014
        },
        {
            "authors": [
                "Tao Qi",
                "Fangzhao Wu",
                "Chuhan Wu",
                "Yongfeng Huang"
            ],
            "title": "Personalized News Recommendation with Knowledge-aware Interactive Matching",
            "venue": "In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval",
            "year": 2021
        },
        {
            "authors": [
                "Tao Qi",
                "Fangzhao Wu",
                "Chuhan Wu",
                "Yongfeng Huang"
            ],
            "title": "PP-Rec: News Recommendation with Personalized User Interest and Time-aware News Popularity",
            "venue": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume",
            "year": 2021
        },
        {
            "authors": [
                "Tao Qi",
                "Fangzhao Wu",
                "Chuhan Wu",
                "Yongfeng Huang"
            ],
            "title": "News Recommendation with Candidate-aware User Modeling",
            "venue": "In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval",
            "year": 2022
        },
        {
            "authors": [
                "Tao Qi",
                "FangzhaoWu",
                "ChuhanWu",
                "Yongfeng Huang",
                "Xing Xie"
            ],
            "title": "Privacy- Preserving News Recommendation Model Learning",
            "venue": "In Findings of the Association for Computational Linguistics:",
            "year": 2020
        },
        {
            "authors": [
                "Tao Qi",
                "Fangzhao Wu",
                "Chuhan Wu",
                "Peiru Yang",
                "Yang Yu",
                "Xing Xie",
                "Yongfeng Huang"
            ],
            "title": "HieRec: Hierarchical User Interest Modeling for Personalized News Recommendation",
            "venue": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume",
            "year": 2021
        },
        {
            "authors": [
                "Shaina Raza",
                "Chen Ding"
            ],
            "title": "News Recommender System: A Review of Recent Progress, Challenges, and Opportunities",
            "venue": "Artificial Intelligence Review",
            "year": 2022
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention Is All You Need",
            "venue": "Proceedings of the 31st International Conference on Neural Information Processing Systems",
            "year": 2017
        },
        {
            "authors": [
                "Heyuan Wang",
                "Fangzhao Wu",
                "Zheng Liu",
                "Xing Xie"
            ],
            "title": "Fine-grained Interest Matching for Neural News Recommendation",
            "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
            "year": 2020
        },
        {
            "authors": [
                "Hongwei Wang",
                "Fuzheng Zhang",
                "Xing Xie",
                "Minyi Guo"
            ],
            "title": "DKN: Deep Knowledge-aware Network for News Recommendation",
            "venue": "In Proceedings of the 2018 World Wide Web Conference",
            "year": 2018
        },
        {
            "authors": [
                "Rongyao Wang",
                "Shoujin Wang",
                "Wenpeng Lu",
                "Xueping Peng"
            ],
            "title": "News Recommendation via Multi-interest News Sequence Modelling",
            "venue": "In ICASSP 2022-",
            "year": 2022
        },
        {
            "authors": [
                "Yinwei Wei",
                "Xiang Wang",
                "Qi Li",
                "Liqiang Nie",
                "Yan Li",
                "Xuanping Li",
                "Tat-Seng Chua"
            ],
            "title": "Contrastive Learning for Cold-start Recommendation",
            "venue": "In Proceedings of the 29th ACM International Conference on Multimedia",
            "year": 2021
        },
        {
            "authors": [
                "Chuhan Wu",
                "Fangzhao Wu",
                "Mingxiao An",
                "Jianqiang Huang",
                "Yongfeng Huang",
                "Xing Xie"
            ],
            "title": "Neural News Recommendation with Attentive Multi-view Learning",
            "venue": "In Proceedings of the 28th International Joint Conference on Artificial Intelligence",
            "year": 2019
        },
        {
            "authors": [
                "Chuhan Wu",
                "Fangzhao Wu",
                "Mingxiao An",
                "Jianqiang Huang",
                "Yongfeng Huang",
                "Xing Xie"
            ],
            "title": "NPA: Neural News Recommendation with Personalized Attention",
            "venue": "In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining",
            "year": 2019
        },
        {
            "authors": [
                "Chuhan Wu",
                "Fangzhao Wu",
                "Mingxiao An",
                "Yongfeng Huang",
                "Xing Xie"
            ],
            "title": "Neural News Recommendation with Topic-aware News Representation",
            "venue": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
            "year": 2019
        },
        {
            "authors": [
                "ChuhanWu",
                "FangzhaoWu",
                "Suyu Ge",
                "Tao Qi",
                "Yongfeng Huang",
                "Xing Xie"
            ],
            "title": "Neural News Recommendation with Multi-head Self-attention",
            "venue": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP- IJCNLP)",
            "year": 2019
        },
        {
            "authors": [
                "Chuhan Wu",
                "Fangzhao Wu",
                "Yongfeng Huang"
            ],
            "title": "Rethinking InfoNCE: How Many Negative Samples Do You Need",
            "venue": "In Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Chuhan Wu",
                "Fangzhao Wu",
                "Yongfeng Huang",
                "Xing Xie"
            ],
            "title": "Personalized News Recommendation: Methods and Challenges",
            "venue": "ACM Transactions on Information Systems 41,",
            "year": 2023
        },
        {
            "authors": [
                "Chuhan Wu",
                "Fangzhao Wu",
                "Tao Qi",
                "Yongfeng Huang"
            ],
            "title": "Sentirec: Sentiment Diversity-aware Neural News Recommendation",
            "venue": "In Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing",
            "year": 2020
        },
        {
            "authors": [
                "Chuhan Wu",
                "Fangzhao Wu",
                "Tao Qi",
                "Yongfeng Huang"
            ],
            "title": "Empowering News Recommendation with Pre-trained Language Models",
            "venue": "In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval",
            "year": 2021
        },
        {
            "authors": [
                "Chuhan Wu",
                "Fangzhao Wu",
                "Tao Qi",
                "Chenliang Li",
                "Yongfeng Huang"
            ],
            "title": "Is News Recommendation a Sequential Recommendation Task",
            "venue": "In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval",
            "year": 2022
        },
        {
            "authors": [
                "Fangzhao Wu",
                "Ying Qiao",
                "Jiun-Hung Chen",
                "Chuhan Wu",
                "Tao Qi",
                "Jianxun Lian",
                "Danyang Liu",
                "Xing Xie",
                "Jianfeng Gao",
                "Winnie Wu",
                "Ming Zhou"
            ],
            "title": "Mind: A Large-scale Dataset for News Recommendation",
            "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
            "year": 2020
        },
        {
            "authors": [
                "Xu Xie",
                "Fei Sun",
                "Zhaoyang Liu",
                "Shiwen Wu",
                "Jinyang Gao",
                "Jiandong Zhang",
                "Bolin Ding",
                "Bin Cui"
            ],
            "title": "Contrastive Learning for Sequential Recommendation",
            "venue": "IEEE 38th International Conference on Data Engineering (ICDE)",
            "year": 2022
        },
        {
            "authors": [
                "Chun Yang",
                "Jianxiao Zou",
                "JianHua Wu",
                "Hongbing Xu",
                "Shicai Fan"
            ],
            "title": "Supervised Contrastive Learning for Recommendation",
            "venue": "Knowledge-Based Systems",
            "year": 2022
        },
        {
            "authors": [
                "Yang Yu",
                "Fangzhao Wu",
                "ChuhanWu",
                "Jingwei Yi",
                "Qi Liu"
            ],
            "title": "Tiny-NewsRec: Effective and Efficient PLM-based News Recommendation",
            "venue": "In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
            "year": 2022
        },
        {
            "authors": [
                "Qi Zhang",
                "Qinglin Jia",
                "Chuyuan Wang",
                "Jingjie Li",
                "Zhaowei Wang",
                "Xiuqiang He"
            ],
            "title": "Amm: Attentive Multi-field Matching for News Recommendation",
            "venue": "In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "CCS CONCEPTS \u2022 Information systems\u2192 Recommender systems.\nKEYWORDS neural news recommendation, user modeling, late fusion, training objectives, contrastive learning, evaluation ACM Reference Format: Andreea Iana, GoranGlava\u0161, andHeiko Paulheim. 2023. Simplifying ContentBased Neural News Recommendation: On User Modeling and Training Objectives. In Proceedings of (SIGIR \u201923). ACM, New York, NY, USA, 5 pages. https://doi.org/XXXXXXX.XXXXXXX"
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "In recent years, content-based news recommendation has seen increasingly complex neural recommender architectures that aim to customize suggestions to users\u2019 interests [13, 34]. Most neural news\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan \u00a9 2023 Association for Computing Machinery. ACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00 https://doi.org/XXXXXXX.XXXXXXX\nrecommendation (NNR) models commonly comprise (i) a dedicated news encoder (NE) and (ii) a user encoder (UE) [34, 36]. NEs \u2013 instantiated as a convolutional network [26, 29, 31], self-attention network [19, 32, 35], graph attention network [18], or, most recently, as a pretrained transformer network [36, 41] \u2013 convert input features (e.g. titles, categories, entities) into the news embedding. UEs aggregate embeddings of clicked news into a user-level representation by means of sequential [1, 21, 27] or attentive [26, 29, 32] encoders that contextualize embeddings of clicked news based on patterns in clicking behavior [1, 15, 37]. We dub this predominant paradigm early fusion (EF) because it aggregates representations of clicked news (i.e., builds user representation) before comparison with the recommendation candidate.\nMost NNR models encode users and candidate news separately, in a candidate-agnostic manner [1, 29, 32]. Candidate-aware models [20, 22, 25, 42], in contrast, acknowledge that not all clicked news are equally informative w.r.t. the relevance of the candidate (e.g., a candidate is often representative of only a subset of a user\u2019s preferences), and contextualize representations of clicked news with the embedding of the candidate in user-level aggregation with UE. Finally, the candidate\u2019s embedding (output of NE) is compared against the user embedding (output of UE): the candidate\u2019s recommendation score is computed directly as the dot product of the two embeddings [29] or with a feed-forward scorer [26]. NNR models are predominantly trained via standard classification objectives [26, 29, 32, 36] with negative sampling [7, 30].\nThe existing body of work has two main shortcomings. First, despite general design homogeneity, direct comparisons between recent NNRs are hindered by lack of transparency and adoption of ad-hoc evaluation protocols [8, 23]. In particular, a vast majority of personalized news recommenders are evaluated on proprietary datasets (e.g. MSN News [29, 32], Bing News [26], NewsApp [20]). Even the few models evaluated using the publicly available datasets such as Adressa [6] or MIND [38] cannot be directly compared due to different dataset splits and evaluation protocols (e.g., model selection strategy) [5, 27, 36, 42]. Secondly, simpler and arguably more intuitive design alternatives have largely been left unexplored. First, the existing work adopts EF as default architecture, proposing increasingly complex user encoding components [1, 20], often with little empirical justification for added complexity. Second, only a small fraction of NNRs leverage contrastive learning objectives [33, 41], despite such training criteria being proven highly effective in closely related retrieval and recommendation tasks [14, 28, 39, 40].\nIn this work, we remedy the above shortcomings of current NNRs and shed new light on user modeling and training objectives.1 1)\n1Disclaimer: In this work we focus exclusively on NNR models that do not resort to graph-based modeling of relations between users.\nar X\niv :2\n30 4.\n03 11\n2v 1\n[ cs\n.I R\n] 6\nA pr\n2 02\n3\nConcretely, we introduce a unified framework for neural news recommendation, facilitating systematic and fair comparison of NNR models across three crucial design dimensions: (i) candidateawareness in user modeling, (ii) click behavior fusion, and (iii) training objectives. 2) We propose to replace user modeling with complex user encoders (i.e., early fusion) with simple pooling of dot-product scores between candidate and clicked news embeddings (i.e. late fusion). We show that, despite conceptual simplicity, LF brings substantial performance gains over EF-based NNR, rendering complex UEs empirically unjustified. 3) Finally, we demonstrate the benefits of supervised contrastive training as a viable alternative to pointwise classification. Our work fundamentally challenges the status quo of NNR by introducing simpler and more effective alternatives to the established paradigm based on complex user modeling."
        },
        {
            "heading": "2 METHODOLOGY",
            "text": "Figure 1 depicts our unified evaluation framework for NNR, focusing on three critical dimensions of news recommendation. Given input data, comprising news and user behaviors, we analyze (i) candidateagnostic (C-AG) vs. candidate-aware (C-AW) user modeling under (ii) two click behavior fusion strategies, namely EF and LF, where each model can be (iii) trained by minimizing either the standard cross-entropy loss (CE) or a supervised contrastive objective (SCL). Next, we describe the models selected for evaluation and formalize the concrete design choices."
        },
        {
            "heading": "2.1 User Modeling",
            "text": "Candidate-Agnostic (C-AG) Models. For these models, the UE produces the user embedding from embeddings of clicked news without contextualization against the candidate. We evaluate the following C-AG models, mutually differing in their NE component (i.e., how they embed the clicked news): (1) NPA [30] uses a personalized attention module to aggregate the representations of the users\u2019 clicked news, with projected embeddings of the users IDs as attention queries; (2) NAML [29] uses additive attention [2] to encode users\u2019 preferences; (3) NRMS [32] learns user representations with a two-layer encoder that consists of multi-head self-attention [24] and additive attention; (4) LSTUR [38] learns user representations with recurrent networks: a short-term user embedding is produced from the clicked news with a GRU [4], and combined with a long-term embedding, consisting of a randomly initialized and fine-tuned part; the final user embedding is then obtained either (i) as the final hidden state of the short-term GRU, initialized with the long-term embedding (LSTURini), or (ii) by simply concatenating the short- and long-term user embeddings (LSTURcon); (5) CenNewsRec [21] adopts a similar UE architecture as LSTUR, but learns long-term user vectors from clicked news using a sequence of multihead self-attention and attentive pooling networks, as opposed to storing an explicit embedding per user; (6)MINS [27] encodes users through a combination of multi-head self-attention, multi-channel GRU-based recurrent network, and additive attention.\nCandidate-Aware (C-AW)Models.UEs in candidate-agnosticmodels produce the same user embedding, regardless of the content of the candidate news. In contrast, UEs of candidate-aware models, two of which we include in our empirical analysis, produce user\nembeddings dependent on the candidate. (7) DKN [26] computes candidate-aware representations of users as the weighted sum of their clicked news embeddings, with weights being produced by an attention network that takes as input the embeddings of the candidate and of the clicked news, as produced by the NE. More recently, (8) CAUM [20] combines (i) a candidate-aware self-attention network to model long-range dependencies between clicked news, conditioned on the candidate, and (ii) a candidate-aware convolutional network (CNN) to capture short-term user interests from adjacent clicks, again conditioned with the candidate\u2019s content; the candidate-aware user embedding is finally obtained by attending over the long-range and short-term representations.\nNews Encoders. The NNR models included in our evaluation primarily use news titles as input, which they typically embed via pretrained word embeddings [17]. NAML, LSTUR, MINS, and CAUM additionally leverage category information, with categories embedded with a linear layer. CAUM additionally encodes title entities and DKN exploits knowledge graph embeddings [9]. The shallow word and entity embeddings are contextualized either using a combination of multi-head self-attention (in NRMS, MINS, CAUM), or a sequence of CNN [11] and additive attention networks (in NAML, LSTUR). NPA [30] also utilizes a CNN to contextualize word embeddings, followed by a personalized attention module, analogous to the one used in its user encoder, whereas DKN employs a word-entity-aligned knowledge-aware CNN [26]. CenNewsRec [21] combines the CNN network with multi-head self-attention and additive attention modules. Models with multiple feature vectors produce final news embeddings by simply concatenating them (LSTUR, CAUM), or by attending over them (NAML, MINS)."
        },
        {
            "heading": "2.2 Click Behavior Fusion",
            "text": "We question whether the design and computational complexity of early fusion (EF), i.e., existence of dedicated user encoders in stateof-the-art NNRmodels, is justified. To this end, we propose, as a lightweight alternative, the late fusion (LF) approach that replaces the elaborate user encoders with mean-pooling of dot-product scores between the embedding of the candidate \ud835\udc5b\ud835\udc50 and the embeddings of the clicked news \ud835\udc5b\ud835\udc62\n\ud835\udc56 . Given a candidate news \ud835\udc5b\ud835\udc50 and a sequence of\nnews clicked by the user \ud835\udc3b = \ud835\udc5b\ud835\udc621 , ..., \ud835\udc5b \ud835\udc62 \ud835\udc41 , we compute the relevance score of the candidate news with regards to the user \ud835\udc62\u2019s history as \ud835\udc60 (n\ud835\udc50 , \ud835\udc62) = 1\n\ud835\udc41 \u2211\ud835\udc41 \ud835\udc56=1 n \ud835\udc50 \u00b7 n\ud835\udc62 \ud835\udc56 , where n denotes the embedding of a\nnews learned by the news encoder and \ud835\udc41 the history length. Although LF suggests that explicitly encoding user behavior may not be necessary for click prediction, user embeddings are still needed in collaborative-filtering models [13]. Note that the LF formulation above is equivalent to the dot product between the candidate embedding n\ud835\udc50 and the mean of embeddings of the user\u2019s clicked news n\ud835\udc62\n\ud835\udc56 , \ud835\udc60 (n\ud835\udc50 , \ud835\udc62) = n\ud835\udc50 \u00b7 ( 1 \ud835\udc41 \u2211\ud835\udc41 \ud835\udc56=1 n \ud835\udc62 \ud835\udc56 ) . This means\nthat LF can also seamlessly provide user embeddings (simply as averages of clicked news embeddings) if needed. LF can thus been seen as a parameterless user encoder, i.e., a computationally efficient alternative to complex parameterized UEs in existing EF models. Because (i) we produce embeddings of candidate and clicked news independently, and (ii) yield user embeddings as averages of clicked news embeddings, LF models are candidate agnostic (C-AG)."
        },
        {
            "heading": "2.3 Training Objectives",
            "text": "The vast majority of existing NNR work, regardless of the concrete user modeling architecture, tunes the parameters by minimizing the arguably most straightforward classification objective, crossentropy loss (with negative sampling; see Figure 1), and largely fails to explore effective alternatives, foremost contrastive objectives [16, 33]. This prevents understanding of models effectiveness under different training regimes. We address this limitation by training all models (see \u00a72.1) not only with (1) common cross-entropy loss (with negative sampling), but also via (2) a contrastive learning objective, in particular supervised contrastive loss [10]."
        },
        {
            "heading": "3 EXPERIMENTAL SETUP",
            "text": "Data. We conduct experiments on the MINDsmall and MINDlarge datasets, introduced by Wu et al. [38]. Table 1 summarizes their main statistics. Since Wu et al. [38] do not release test set labels, we use the respective validation portions for testing, and split the respective training sets into temporally disjoint training (first four days of data) and validation portions (the last day).\nImplementation andOptimizationDetails.Weuse 300-dimensional pretrained Glove embeddings [17] and 100-dimensional TransE embeddings [3] pretrained onWikidata to initialize respectively the word and entity embeddings of the NNR models under comparison. We set the maximum history length to 50. Following Wu et al. [33], our negative sampling creates four negatives per positive example. We find the optimal temperature for SCL using the validation performance, sweeping the interval [0.08, 0.3] with a 0.02 step. We train with batch size of 512 for all C-AG models, 256 for DKN and only 64 for CAUM (due to computational limitations). We set all other\nmodel-specific hyperparameters, to optimal values reported in the respective papers. We train all models with mixed precision, under a fixed computational budget: for 25 epochs on MINDsmall and 10 epochs on MINDlarge. We optimize with the Adam algorithm [12], with the learning rate set to 1e-4. We repeat each experiment five times (with different random seeds) and report averages (and std. deviation) for common metrics: AUC, MRR, nDCG@5, and nDCG@10. Each model is trained on a single NVIDIA Tesla V100 GPU with 32GB memory. Our implementation is publicly available.2"
        },
        {
            "heading": "4 RESULTS AND DISCUSSION",
            "text": "Table 2 shows the performance on MINDsmall and MINDlarge for both C-AG (NPA, NAML, NRMS, LSTUR, CenNewsRec, and MINS) and C-AWmodels (DKN, CAUM), under four different configurations of our comparative evaluation framework: (i) user modeling with EF vs. LF, combined with (2) training with CE vs. SCL objective. We next dissect the results along the three axes of our framework (\u00a72): user modeling, click behavior fusion, and training objectives.\nCandidate-Agnostic vs. Candidate-Aware NNRs.Weanalyze C-AG vs. C-AW models under their default EF configuration, since with LF all models become candidate-agnostic. CAUM, with the most complex and candidate-aware UE, generally outperforms all other models under both training regimes (CE and SCL) and for most evaluation metrics. The gaps are particularly prominent on the large training dataset, MINDlarge, w.r.t. the AUCmetric. This result alone could mislead to a conclusion that more complex, candidate-aware user modeling is necessary for better recommendation. The fact that (1) DKN, as the other C-AWmodel in our evaluation \u2013 generally performs much worse than C-AG models, as well as that (2) our LF models variants with trivial, parameterless UEs match or surpass the performance of CAUM with EF, undermine this conclusion. With the exception of DKN, all other models exhibit better performance when trained on the larger MINDlarge dataset. NAML and MINS are, however, competitive (except w.r.t. AUC metric) on MINDsmall, but fall behind CAUM on MINDlarge, suggesting that CAUM\u2019s elaborate UE benefits the most from more training data.\nOne confounding factor that we do not control for, however, and which warrants a mindful comparison of the results, is that models differ not just in UE, but also in NE components, i.e., w.r.t. how they encode news and which features they use as input. For example, NAML and MINS, with an identical NE, achieve similar performance on MINDsmall. On MINDlarge, however, the more complex UE of MINS brings substantial gains over the simpler UE of NAML (but only under standard EF fusion and CE training).\n2Code available at: https://github.com/andreeaiana/simplifying_nnr\nTable 2: Recommendation performance of the comparedmodels under combinations of click behavior fusion (CBF), and training objectives. We report averages and standard deviations across five different runs.\nMINDsmall MINDlarge\nAUC MRR nDCG@5 nDCG@10 AUC MRR nDCG@5 nDCG@10\nModel CBF CE SCL CE SCL CE SCL CE SCL CE SCL CE SCL CE SCL CE SCL EF 54.7\u00b10.6 56.5\u00b10.7 29.0\u00b10.7 28.4\u00b10.6 26.9\u00b10.8 26.6\u00b10.6 33.2\u00b10.8 32.6\u00b10.4 56.8\u00b10.2 58.1\u00b10.8 31.4\u00b10.5 30.0\u00b10.6 29.5\u00b10.4 27.7\u00b10.6 35.9\u00b10.4 34.3\u00b10.6NPA LF 55.1\u00b10.9 57.3\u00b11.2 28.6\u00b10.3 27.5\u00b11.0 26.4\u00b10.4 25.5\u00b11.1 32.9\u00b10.4 31.8\u00b10.9 61.2\u00b10.5 58.7\u00b10.8 32.1\u00b10.6 28.3\u00b10.5 30.2\u00b10.7 26.0\u00b10.7 36.6\u00b10.6 32.7\u00b10.7 EF 50.1\u00b10.0 57.1\u00b11.1 33.6\u00b10.5 32.2\u00b10.7 31.6\u00b10.6 30.4\u00b10.7 38.0\u00b10.5 36.8\u00b10.7 50.1\u00b10.0 60.4\u00b10.6 33.2\u00b10.4 34.2\u00b10.4 31.3\u00b10.5 32.5\u00b10.3 37.9\u00b10.4 38.9\u00b10.3NAML LF 50.0\u00b10.0 62.7\u00b10.5 33.7\u00b10.8 32.0\u00b10.7 31.8\u00b10.9 30.3\u00b10.7 38.1\u00b10.8 36.6\u00b10.6 50.0\u00b10.0 65.4\u00b10.5 32.7\u00b10.5 33.5\u00b10.5 31.0\u00b10.5 31.7\u00b10.5 37.6\u00b10.4 38.2\u00b10.4 EF 52.6\u00b11.3 59.9\u00b10.6 27.6\u00b10.8 29.2\u00b10.7 25.7\u00b10.5 27.2\u00b10.9 32.3\u00b10.5 33.7\u00b10.7 54.6\u00b11.4 62.8\u00b10.7 31.9\u00b11.0 32.4\u00b10.5 30.0\u00b11.1 30.5\u00b10.7 36.6\u00b11.0 36.9\u00b10.6NRMS LF 58.9\u00b11.0 60.2\u00b11.1 31.8\u00b10.7 30.7\u00b10.6 29.9\u00b10.7 28.7\u00b10.6 36.3\u00b10.6 35.1\u00b10.6 56.1\u00b12.1 63.6\u00b11.1 32.9\u00b10.7 32.4\u00b10.6 31.7\u00b11.1 30.6\u00b10.8 37.8\u00b10.4 37.1\u00b10.7 EFini 53.5\u00b11.2 55.4\u00b10.5 29.6\u00b10.5 28.1\u00b10.7 28.0\u00b10.5 26.5\u00b10.7 34.4\u00b10.4 32.9\u00b10.6 50.0\u00b10.1 56.9\u00b11.5 32.5\u00b12.4 31.3\u00b11.5 30.9\u00b12.4 29.8\u00b11.8 37.4\u00b12.4 36.2\u00b11.6 EFcon 50.2\u00b10.0 59.8\u00b11.4 31.8\u00b10.7 31.3\u00b10.8 30.1\u00b10.8 30.3\u00b11.3 36.4\u00b10.7 36.2\u00b10.7 51.4\u00b10.4 54.3\u00b10.4 27.7\u00b10.4 26.5\u00b10.2 25.9\u00b10.5 24.6\u00b10.2 32.3\u00b10.5 31.1\u00b10.2LSTUR LF 50.0\u00b10.0 50.0\u00b10.0 33.8\u00b10.6 33.9\u00b10.6 31.9\u00b10.7 32.0\u00b10.7 38.0\u00b10.6 38.1\u00b10.6 50.0\u00b10.0 50.0\u00b10.0 34.7\u00b10.6 33.1\u00b10.2 33.2\u00b10.6 31.6\u00b10.4 39.2\u00b10.5 37.7\u00b10.3 EF 54.0\u00b10.8 60.0\u00b10.4 28.3\u00b10.5 30.6\u00b10.8 26.5\u00b10.4 28.5\u00b10.8 32.9\u00b10.3 34.8\u00b10.7 56.4\u00b10.8 64.5\u00b10.4 33.7\u00b10.3 33.5\u00b10.4 31.9\u00b10.3 31.8\u00b10.5 38.3\u00b10.2 38.1\u00b10.4CenNewsRec LF 59.3\u00b10.6 61.9\u00b10.7 32.8\u00b10.8 32.2\u00b10.8 30.9\u00b10.8 30.4\u00b10.8 37.1\u00b10.6 36.6\u00b10.7 53.3\u00b10.7 64.2\u00b10.6 33.2\u00b10.4 33.3\u00b10.4 31.4\u00b10.5 31.7\u00b10.4 37.9\u00b10.4 38.1\u00b10.4 EF 50.6\u00b10.3 62.9\u00b11.7 33.7\u00b11.0 32.4\u00b10.3 31.9\u00b11.1 30.7\u00b10.4 38.3\u00b10.9 37.1\u00b10.3 51.7\u00b10.2 65.8\u00b10.5 34.3\u00b10.2 34.4\u00b10.5 32.5\u00b10.4 32.6\u00b10.5 39.1\u00b10.4 39.1\u00b10.5MINS LF 59.1\u00b11.2 64.2\u00b10.7 35.0\u00b10.5 34.1\u00b10.6 33.2\u00b10.6 32.3\u00b10.7 39.4\u00b10.6 38.5\u00b10.6 53.8\u00b10.6 66.7\u00b10.8 34.9\u00b10.2 34.8\u00b10.7 33.0\u00b10.2 33.1\u00b10.7 39.5\u00b10.2 39.6\u00b10.6 EF 50.0\u00b10.0 51.0\u00b12.3 26.4\u00b10.6 25.9\u00b10.9 24.4\u00b10.7 23.9\u00b11.1 31.0\u00b10.6 30.5\u00b10.9 50.0\u00b10.0 50.0\u00b10.0 25.2\u00b10.4 24.8\u00b10.3 23.4\u00b10.7 22.6\u00b10.3 30.0\u00b10.5 29.1\u00b10.3DKN LF 50.0\u00b10.0 50.0\u00b10.0 27.5\u00b10.6 26.4\u00b10.8 25.0\u00b10.5 24.0\u00b10.8 31.7\u00b10.6 30.8\u00b10.8 50.0\u00b10.0 50.0\u00b10.0 29.1\u00b10.4 27.8\u00b11.1 26.3\u00b10.3 25.4\u00b11.0 33.2\u00b10.4 32.1\u00b11.0 EF 61.4\u00b11.0 63.2\u00b10.9 33.8\u00b10.6 33.7\u00b10.8 32.0\u00b10.6 31.8\u00b10.9 38.4\u00b10.5 38.2\u00b10.8 67.1\u00b10.8 66.4\u00b10.9 35.3\u00b10.5 35.1\u00b10.5 33.6\u00b10.6 33.4\u00b10.6 40.1\u00b10.5 39.9\u00b10.5CAUM LF 62.4\u00b10.8 63.5\u00b10.8 33.7\u00b10.6 33.7\u00b10.7 31.8\u00b10.5 31.8\u00b10.8 38.2\u00b10.5 38.0\u00b10.7 53.1\u00b10.3 65.9\u00b10.2 34.5\u00b10.4 34.5\u00b10.1 32.6\u00b10.3 32.8\u00b10.1 39.2\u00b10.3 39.3\u00b10.1\nEarly vs. Late Click Behavior Fusion. Replacing complex EFbased UEs with the simple parameterless LF that we propose brings substantial performance gains across the board. Averaged across all models and both training objectives, LF brings massive gains of 5.58 and 4.63 MRR points on MINDsmall and MINDlarge, respectively. Equally importantly, with LF \u2013 i.e., with the same parameterless UE \u2013 models exhibit mutually much more similar performance than under EF, with other models generally closing the gap to CAUM. This suggests that LFmakes differences in NE architectures across models less consequential, thus not only simplifying UE with parameterless averaging of clicked news embeddings, but also allowing for simpler news encoders.\nCross-Entropy vs. Supervised Contrastive Loss. Overall, we find SCL to be a viable alternative to the common cross-entropy based classification with negative sampling (compare columns CE and SCL across evaluation metrics in Table 2). SCL brings large gains over CE in terms of AUC (+8.26 points on MINDsmall and +12.14 points on MINDlarge, averaged across all models, in both EF and LF variants). This suggests that, SCL leads to better separation of clicked and not clicked news in the representation space. In contrast, SCL falls slightly behind CE according to ranking measures, MRR and nDCG (-1.54 and -1.78 MRR points on MINDsmall and MINDlarge, respectively). We hypothesize that this is because of hard negatives \u2013 news not clicked by the user that resemble user\u2019s clicked news \u2013 for which CEmore directly signals irrelevance: these likely become highly-ranked false positives for SCL-trained models.\nModel Size. Finally, we quantify the reduction in model parameters that LF brings w.r.t. EF. Figure 2 shows the number of trainable parameters in original EF configurations, on MINDsmall.3 While the NE accounts for the majority of parameters in most models, the plot shows that the proportion of UE parameters is non-negligible for several models, and largest by a wide margin for LSTUR. With a parameterless UE, along with performance gains, LF brings a relative reduction of model size of 14.7%, 18.1%, and massive 82.3% for CenNewsRec, CAUM, and LSTURini, respectively.\n3For some models, e.g., LSTUR with its user embedding matrix, the number of parameters depends on the size of the training data."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "Rapid development of personalized neural news recommenders hinders fair comparative model evaluations and systematic analyses of design choices. In this work we introduce a unified framework for neural news recommendation focusing on three crucial design dimensions of NNR: (i) candidate-awareness in user modeling, (ii) click behavior fusion, and (iii) training objectives. Extensive evaluation of a wide range of recent state-of-the-art models reveals that NNR can be drastically simplified: replacing complex user encoders with parameterless aggregation of clicked news embeddings brings substantial performance gains across the board, reducing at the same time model complexity. Further, we show that contrastive learning can be a viable alternative to standard classification-based (cross-entropy) loss. We hope that our findings will inspire more transparent NNR evaluation, including systematic model ablations to uncover components that drive the performance."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "The authors acknowledge support by the state of Baden-W\u00fcrttemberg through bwHPC. Andreea Iana was supported by the ReNewRS project grant, which is funded by the Baden-W\u00fcrttemberg Stiftung in the Responsible Artificial Intelligence program."
        }
    ],
    "title": "Simplifying Content-Based Neural News Recommendation:  On User Modeling and Training Objectives",
    "year": 2023
}