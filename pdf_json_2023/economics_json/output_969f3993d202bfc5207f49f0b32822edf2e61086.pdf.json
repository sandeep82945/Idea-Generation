{
    "abstractText": "We advance a recently flourishing line of work at the intersection of learning theory and computational economics by studying the learnability of two classes of mechanisms prominent in economics, namely menus of lotteries and two-part tariffs. The former is a family of randomized mechanisms designed for selling multiple items, known to achieve revenue beyond deterministic mechanisms, while the latter is designed for selling multiple units (copies) of a single item with applications in real-world scenarios such as car or bike-sharing services. We focus on learning high-revenue mechanisms of this form from buyer valuation data in both distributional settings, where we have access to buyers\u2019 valuation samples up-front, and the more challenging and less-studied online settings, where buyers arrive one-at-a-time and no distributional assumption is made about their values. Our main contribution is proposing the first online learning algorithms for menus of lotteries and two-part tariffs with strong regret-bound guarantees. In the general case, we provide a reduction to a finite number of experts, and in the limited buyer type case, we show a reduction to online linear optimization, which allows us to obtain no regret guarantees by presenting buyers with menus that correspond to a barycentric spanner. In addition, we provide algorithms with improved running times over prior work for the distributional settings. The key difficulty when deriving learning algorithms for these settings is that the relevant revenue functions have sharp transition boundaries. In stark contrast with the recent literature on learning such unstructured functions, we show that simple discretization-based techniques are sufficient for learning in these settings.",
    "authors": [
        {
            "affiliations": [],
            "name": "Maria-Florina Balcan"
        },
        {
            "affiliations": [],
            "name": "Hedyeh Beyhaghi"
        }
    ],
    "id": "SP:43d17ee266cf8028fe59ebd86242b410898853cf",
    "references": [
        {
            "authors": [
                "Noga Alon",
                "Nicol\u00f2 Cesa-Bianchi",
                "Claudio Gentile",
                "Shie Mannor",
                "Yishay Mansour",
                "Ohad Shamir"
            ],
            "title": "Nonstochastic multi-armed bandits with graph-structured feedback",
            "venue": "SIAM J. Comput.,",
            "year": 2017
        },
        {
            "authors": [
                "Peter Auer",
                "Nicolo Cesa-Bianchi",
                "Yoav Freund",
                "Robert E Schapire"
            ],
            "title": "Gambling in a rigged casino: The adversarial multi-armed bandit problem",
            "venue": "In Proceedings of IEEE 36th annual foundations of computer science,",
            "year": 1995
        },
        {
            "authors": [
                "Baruch Awerbuch",
                "Robert Kleinberg"
            ],
            "title": "Online linear optimization and adaptive routing",
            "venue": "J. Comput. Syst. Sci.,",
            "year": 2008
        },
        {
            "authors": [
                "Baruch Awerbuch",
                "Yishay Mansour"
            ],
            "title": "Adapting to a reliable network path",
            "venue": "Proceedings of the Twenty-Second ACM Symposium on Principles of Distributed Computing,",
            "year": 2003
        },
        {
            "authors": [
                "Maria-Florina Balcan",
                "Avrim Blum"
            ],
            "title": "Approximation algorithms and online mechanisms for item pricing",
            "venue": "Proceedings 7th ACM Conference on Electronic Commerce (EC-2006),",
            "year": 2006
        },
        {
            "authors": [
                "Maria-Florina Balcan",
                "Dravyansh Sharma"
            ],
            "title": "Data driven semi-supervised learning",
            "venue": "Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems",
            "year": 2021
        },
        {
            "authors": [
                "Maria-Florina Balcan",
                "Avrim Blum",
                "Jason D Hartline",
                "Yishay Mansour"
            ],
            "title": "Reducing mechanism design to algorithm design via machine learning",
            "venue": "Journal of Computer and System Sciences,",
            "year": 2008
        },
        {
            "authors": [
                "Maria-Florina Balcan",
                "Avrim Blum",
                "Nika Haghtalab",
                "Ariel D. Procaccia"
            ],
            "title": "Commitment without regrets: Online learning in stackelberg security games",
            "venue": "Proceedings of the Sixteenth ACM Conference on Economics and Computation, EC \u201915,",
            "year": 2015
        },
        {
            "authors": [
                "Maria-Florina Balcan",
                "Tuomas Sandholm",
                "Ellen"
            ],
            "title": "Vitercik. Sample complexity of automated mechanism design",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2016
        },
        {
            "authors": [
                "Maria-Florina Balcan",
                "Vaishnavh Nagarajan",
                "Ellen Vitercik",
                "Colin White"
            ],
            "title": "Learningtheoretic foundations of algorithm configuration for combinatorial partitioning problems",
            "venue": "In Conference on Learning Theory,",
            "year": 2017
        },
        {
            "authors": [
                "Maria-Florina Balcan",
                "Travis Dick",
                "Tuomas Sandholm",
                "Ellen Vitercik"
            ],
            "title": "Learning to branch",
            "venue": "In International conference on machine learning,",
            "year": 2018
        },
        {
            "authors": [
                "Maria-Florina Balcan",
                "Travis Dick",
                "Ellen Vitercik"
            ],
            "title": "Dispersion for data-driven algorithm design, online learning, and private optimization",
            "venue": "IEEE 59th Annual Symposium on Foundations of Computer Science (FOCS),",
            "year": 2018
        },
        {
            "authors": [
                "Maria-Florina Balcan",
                "Tuomas Sandholm",
                "Ellen Vitercik"
            ],
            "title": "A general theory of sample complexity for multi-item profit maximization",
            "venue": "In Proceedings of the 2018 ACM Conference on Economics and Computation,",
            "year": 2018
        },
        {
            "authors": [
                "Maria-Florina Balcan",
                "Travis Dick",
                "Wesley Pegden"
            ],
            "title": "Semi-bandit optimization in the dispersed setting",
            "venue": "In Conference on Uncertainty in Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Maria-Florina Balcan",
                "Siddharth Prasad",
                "Tuomas Sandholm"
            ],
            "title": "Efficient algorithms for learning revenue-maximizing two-part tariffs",
            "venue": "In Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence,{IJCAI-20},",
            "year": 2020
        },
        {
            "authors": [
                "Maria-Florina Balcan",
                "Dan DeBlasio",
                "Travis Dick",
                "Carl Kingsford",
                "Tuomas Sandholm",
                "Ellen Vitercik"
            ],
            "title": "How much data is sufficient to learn high-performing algorithms? generalization guarantees for data-driven algorithm design",
            "venue": "In Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing,",
            "year": 2021
        },
        {
            "authors": [
                "Maria-Florina Balcan",
                "Mikhail Khodak",
                "Dravyansh Sharma",
                "Ameet Talwalkar"
            ],
            "title": "Learning-to-learn non-convex piecewise-lipschitz functions",
            "venue": "Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems",
            "year": 2021
        },
        {
            "authors": [
                "Maria-Florina Balcan",
                "Christopher Seiler",
                "Dravyansh Sharma"
            ],
            "title": "Faster algorithms for learning to link, align sequences, and price two-part tariffs",
            "venue": "arXiv preprint arXiv:2204.03569,",
            "year": 2022
        },
        {
            "authors": [
                "Maria-Florina F. Balcan",
                "Misha Khodak",
                "Dravyansh Sharma",
                "Ameet Talwalkar"
            ],
            "title": "Provably tuning the elasticnet across instances",
            "venue": "In NeurIPS, 2022b. URL http://papers.nips.cc/paper_files/paper/2022/hash/b21a34c4e8dba253f05f4a5adc68ba73-Abstract-Conference.html",
            "year": 2022
        },
        {
            "authors": [
                "Avrim Blum",
                "Jason D. Hartline"
            ],
            "title": "Near-optimal online auctions",
            "venue": "In Proceedings of the Sixteenth Annual ACM-SIAM Symposium on Discrete Algorithms,",
            "year": 2005
        },
        {
            "authors": [
                "Avrim Blum",
                "Vijay Kumar",
                "Atri Rudra",
                "Felix Wu"
            ],
            "title": "Online learning in online auctions",
            "venue": "Theoretical Computer Science,",
            "year": 2004
        },
        {
            "authors": [
                "Patrick Briest",
                "Shuchi Chawla",
                "Robert Kleinberg",
                "S Matthew Weinberg"
            ],
            "title": "Pricing randomized allocations",
            "venue": "In Proceedings of the twenty-first annual ACM-SIAM symposium on Discrete Algorithms,",
            "year": 2010
        },
        {
            "authors": [
                "S\u00e9bastien Bubeck",
                "Nikhil R. Devanur",
                "Zhiyi Huang",
                "Rad Niazadeh"
            ],
            "title": "Online auctions and multi-scale online learning",
            "venue": "Proceedings of the 2017 ACM Conference on Economics and Computation, EC \u201917,",
            "year": 2017
        },
        {
            "authors": [
                "Nicolo Cesa-Bianchi",
                "Claudio Gentile",
                "Yishay Mansour"
            ],
            "title": "Regret minimization for reserve prices in second-price auctions",
            "venue": "IEEE Transactions on Information Theory,",
            "year": 2014
        },
        {
            "authors": [
                "Partha Dasgupta",
                "Peter Hammond",
                "Eric Maskin"
            ],
            "title": "The implementation of social choice rules: Some general results on incentive compatibility",
            "venue": "The Review of Economic Studies,",
            "year": 1979
        },
        {
            "authors": [
                "Constantinos Daskalakis",
                "Alan Deckelbaum",
                "Christos Tzamos"
            ],
            "title": "The complexity of optimal mechanism design",
            "venue": "In Proceedings of the twenty-fifth annual ACM-SIAM symposium on Discrete algorithms,",
            "year": 2014
        },
        {
            "authors": [
                "Shaddin Dughmi",
                "Li Han",
                "Noam Nisan"
            ],
            "title": "Sampling and representation complexity of revenue maximization",
            "venue": "In International Conference on Web and Internet Economics,",
            "year": 2014
        },
        {
            "authors": [
                "Paul D\u00fctting",
                "Zhe Feng",
                "Harikrishna Narasimhan",
                "David Parkes",
                "Sai Srivatsa Ravindranath"
            ],
            "title": "Optimal auctions through deep learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Yannai A Gonczarowski",
                "S Matthew Weinberg"
            ],
            "title": "The sample complexity of up-to-\u03b5 multidimensional revenue maximization",
            "venue": "Journal of the ACM (JACM),",
            "year": 2021
        },
        {
            "authors": [
                "Sergiu Hart",
                "Noam Nisan"
            ],
            "title": "Selling multiple correlated goods: Revenue maximization and menu-size complexity",
            "venue": "Journal of Economic Theory,",
            "year": 2019
        },
        {
            "authors": [
                "Robert D. Kleinberg",
                "Frank Thomson Leighton"
            ],
            "title": "The value of knowing a demand curve: Bounds on regret for online posted-price auctions",
            "venue": "In 44th Symposium on Foundations of Computer Science (FOCS 2003),",
            "year": 2003
        },
        {
            "authors": [
                "W Arthur Lewis"
            ],
            "title": "The two-part tariff",
            "venue": "Economica, 8(31):249\u2013270,",
            "year": 1941
        },
        {
            "authors": [
                "L\u00e1szl\u00f3 Lov\u00e1sz",
                "Santosh S. Vempala"
            ],
            "title": "Fast algorithms for logconcave functions: Sampling, rounding, integration and optimization",
            "venue": "Annual IEEE Symposium on Foundations of Computer Science (FOCS 2006),",
            "year": 2006
        },
        {
            "authors": [
                "Mehryar Mohri",
                "Andr\u00e9s Munoz Medina"
            ],
            "title": "Learning algorithms for second-price auctions with reserve",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2016
        },
        {
            "authors": [
                "Jamie Morgenstern",
                "Tim Roughgarden"
            ],
            "title": "Learning simple auctions",
            "venue": "In Conference on Learning Theory,",
            "year": 2016
        },
        {
            "authors": [
                "Jamie H Morgenstern",
                "Tim Roughgarden"
            ],
            "title": "On the pseudo-dimension of nearly optimal auctions",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2015
        },
        {
            "authors": [
                "Walter Y Oi"
            ],
            "title": "A disneyland dilemma: Two-part tariffs for a mickey mouse monopoly",
            "venue": "The Quarterly Journal of Economics,",
            "year": 1971
        },
        {
            "authors": [
                "Tim Roughgarden",
                "Joshua R. Wang"
            ],
            "title": "Minimizing regret with multiple reserves",
            "venue": "Proceedings of the 2016 ACM Conference on Economics and Computation, EC \u201916,",
            "year": 2016
        },
        {
            "authors": [
                "Vasilis Syrgkanis"
            ],
            "title": "A sample complexity measure with applications to learning optimal auctions",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Leslie G Valiant"
            ],
            "title": "A theory of the learnable",
            "venue": "Communications of the ACM,",
            "year": 1984
        },
        {
            "authors": [
                "Asample by Bassily"
            ],
            "title": "Algorithm 6. These implementations satisfy the conditions in Definition 34. The first runs in time poly(d",
            "year": 2014
        },
        {
            "authors": [
                "Balcan"
            ],
            "title": "2018b], gives the statement\u2019s regret bound and running time",
            "year": 2018
        },
        {
            "authors": [
                "Balcan"
            ],
            "title": "Proposition 36 determines dispersion for two-part tariff menus with probability 1\u2212\u03b6",
            "year": 2018
        },
        {
            "authors": [
                "Balcan"
            ],
            "title": "2020a]. We adapt it to our setting and consider an efficient",
            "year": 2020
        },
        {
            "authors": [
                "Alon"
            ],
            "title": "At each time step, the algorithm learns the revenue function (only) inside the region P(t) \u220b \u03c1t that the presented menu belongs to and updates the menu weights for the next round accordingly",
            "year": 2017
        },
        {
            "authors": [],
            "title": "2020a], stating that if the loss functions are Lipschitz functions satisfying \u03b2-point-dispersion, running Algorithm 7 has expected regret bounded by \u00d5",
            "year": 2020
        },
        {
            "authors": [],
            "title": "the approximate integration and sampling operations performed in the algorithm succeed and the density function of the approximate distribution used for sampling is always within (1 \u2212 \u03b7) fraction of the exact distribution. Using these parameters together with Theorem 1 in [Balcan et al., 2018b] conclude that the same regret bound is achievable",
            "year": 2018
        },
        {
            "authors": [
                "Balcan"
            ],
            "title": "2018c] introduce delineability as a condition to upper bound the pseudodimension and therefore, the sample complexity. They show the class of lotteries is (l(m+ 1), (l+ 1)2 +ml)-delineable. Also, ifM is a mechanism class that is (d, t)-delineable, then the pseudo dimension of M is at most 9d log(4dt)",
            "year": 2018
        }
    ],
    "sections": [
        {
            "text": "ar X\niv :2\n30 2.\n11 70\n0v 2\n[ cs\n.G T\n] 3\n0 Ju\nOur main contribution is proposing the first online learning algorithms for menus of lotteries and two-part tariffs with strong regret-bound guarantees. In the general case, we provide a reduction to a finite number of experts, and in the limited buyer type case, we show a reduction to online linear optimization, which allows us to obtain no regret guarantees by presenting buyers with menus that correspond to a barycentric spanner. In addition, we provide algorithms with improved running times over prior work for the distributional settings. The key difficulty when deriving learning algorithms for these settings is that the relevant revenue functions have sharp transition boundaries. In stark contrast with the recent literature on learning such unstructured functions, we show that simple discretization-based techniques are sufficient for learning in these settings."
        },
        {
            "heading": "1 Introduction",
            "text": "In recent years, a growing body of work has emerged in the field of machine learning for pricing and mechanism design problems. These problems involve selling items to buyers with the objective of maximizing revenue. The majority of the existing research has primarily concentrated on distributional settings, i.e., when the buyers\u2019 values for the items are drawn from an unknown distribution. Less attention has been paid to the more challenging case of online setting, where buyers arrive one-by-one and no distributional assumption about\nbuyers\u2019 values is considered. In this case, the previous literature has mostly focused on simple mechanisms such as posted pricing or, more generally, mechanisms that sell the items separately [Blum et al., 2004, Kleinberg and Leighton, 2003, Blum and Hartline, 2005, Balcan and Blum, 2006, Bubeck et al., 2017, Cesa-Bianchi et al., 2014, Balcan et al., 2018b, 2020a]. We advance this line of work by studying the learnability of two prominent classes of mechanisms, both represented as menus providing the buyers a list of allocation and payment options to choose from, namely menus of two-part tariffs and lotteries. These mechanisms go beyond selling the items separately, resulting in potentially higher revenue guarantees with applications to modern real-world scenarios. We provide the first online learning guarantees for these scenarios and improved guarantees for distributional learning. In the process, we discover the power of data-independent discretization for data-driven mechanism design and algorithm design more generally.\nThe first class we study is menus of two-part tariffs [Lewis, 1941], used for selling multiple units (i.e., copies) of a single item. In this family of mechanisms, the buyer is presented with a list (menu) of two-part tariffs, where tariff i is a pair consisting of an up-front fee, p (i) 1 , and a per-unit fee, p (i) 2 . If the buyer wishes to buy k \u2265 1 units of tariff i, she pays in total p (i) 1 + kp (i) 2 , and if she does not want to buy anything, she does not pay anything. The buyer has the freedom to select any of the tariffs. In particular, the cost for purchasing k \u2265 1 units is the minimum cost among all the tariffs, i.e., mini(p (i) 1 + kp (i) 2 ). Various products in the real world are sold via menus of two-part tariffs; for example, car or bike-sharing services and delivery service subscriptions.\nThe second class we study is the menus of lotteries for selling multiple items. In this context, the buyer is presented with a list (menu) of lotteries, where lottery i is defined as a pair consisting of a vector of probabilities for allocating each item, \u03c6(i), and a price, p(i). If the buyer wishes to choose lottery i, she receives each item j with probability \u03c6(i)[j] and pays p(i). Menus of lotteries are a crucial family of mechanisms because (1) this family captures all possible mechanisms, including the optimal one [Dasgupta et al., 1979, Guesnerie and Oddou, 1981], and (2) menus of lotteries achieve revenue beyond other well-studied families of mechanisms such as posted pricing and, more generally, any deterministic mechanism [Briest et al., 2010, Hart and Nisan, 2019].\nWe study menus of two-part tariffs and lotteries in the context of parameter optimization, where the objective function (revenue) depends on parameter vectors. In menus of two-part tariffs, the parameters determining the mechanisms are the up-front fees and per-unit fees for each tariff, while for menus of lotteries, the allocation probability vectors and the prices for the lotteries determine the mechanism. In the parameter space, each point corresponds to a mechanism. A common approach in learning algorithms involves considering the objective function for a fixed buyer\u2019s valuation [Balcan et al., 2017, 2018c,b]. In our context, the mechanism designer faces a utility-maximizing buyer, who, given the parameters determining the menu, chooses the entry, i.e., a lottery or a two-part tariff, in the menu that maximizes her utility. Therefore, the revenue function at any parameter vector is equal to the payment corresponding to the entry selected by the buyer."
        },
        {
            "heading": "1.1 Our Contributions",
            "text": "We study the learnability of menus of two-part tariffs and lotteries in both online and distributional settings. We advance the state-of-the-art in several aspects.\nTechnical challenges. Discretization is a natural technique in data-driven algorithm design. In this approach, a finite set of parameter vectors, each representing a menu in the parameter space, are selected, and the algorithms optimize over that set. The smaller the set, the better the generalization guarantees will be in the distributional setting, and the better the regret guarantees will be in the online setting, with respect to the best menu in the set. In our setting, a proper data-independent discretization scheme would guarantee that independent of the buyer\u2019s valuation, this set always contains a nearly optimal menu. More specifically, for any arbitrary parameter vector representing a menu, a menu in the set should generate almost as much revenue, independent of the buyer\u2019s valuation. However, due to sharp discontinuities of revenue in the parameter space, devising such a discretization can be challenging. For instance, consider a menu with two high-utility entries for a buyer such that these entries have similar utility for the buyer but very different prices (e.g., one with high allocation and high price, the other with low allocation and low price). Minor changes in the parameters of these entries; e.g., rounding the parameters down to multiples of \u01eb, may alter their utility order, causing the buyer to switch between them, resulting in arbitrary loss in revenue.\nStructural Properties and a Revenue Preserving Cover. By extracting structural properties for menus of two-part tariffs, we develop a novel discretization method that identifies a finite set of menus that approximate the revenue of any arbitrary limited-length menu (Theorem 1). In menus of lotteries, we extend the discretization of menus of lotteries developed by [Dughmi et al., 2014] (Theorem 16). Our extension is three-fold: we remove the lower bound assumption on value distribution, support additive valuations, and provide improved regret bounds and running times when the size of the menu is limited. In both settings (two-part tariffs lotteries), our discretization is data-independent; e.g., the set of discretized menus consists of all menus with parameters that are multiples of \u03b5 or powers of (1 \u2212 \u03b5). The novelty of the result, however, lies in the analysis, which illustrates despite the challenges discussed above, for each arbitrary menu and valuation, this set contains a corresponding approximately revenue-preserving menu. For finding corresponding menus, rounding is nontrivial in the sense that entries with higher prices need to experience a larger decrease in price and a smaller decrease in allocation so that no buyer switches from a high-price to a low-price entry.\nOnline Learning (adversarial inputs and smooth distributional assumptions). For menus of two-part tariffs, we provide the first no-regret online learning algorithms under adversarial inputs and also smooth distributional assumptions. For the full information setting, both settings lead to similar regret terms; however, the comparison of their running time depends on the support of the distribution and the maximum number of units available (Theorems 2 and 5). In the bandit setting, again, the regret of both settings are similar. However, the comparison between the efficiencies of the algorithms depends on the\nsmoothness factor of the distributions (Theorems 3 and 6). Furthermore, we provide the first no-regret algorithm for a semibandit-setting (Theorem 7) with a polynomial running time in the number of discontinuities in the parameter space. This setting lies between the full-information and bandit settings, and the learner observes the revenue function for a set of menus containing the menu used. For menus of lotteries, we provide the first no-regret online learning algorithms under adversarial inputs (Theorems 17 to 19). In addition, we provide evidence that menus of lotteries may not satisfy dispersion\u2014a sufficient condition to provide a no-regret algorithm under smooth distributional assumption\u2014without assuming extra structures about the optimal solution (Theorem 58). Menus of lotteries are the first family of mechanisms for which there is evidence of a potential failure of the dispersion property.\nDistributional Learning. We also provide novel distributional learning algorithms for menus of two-part tariffs and lotteries. Our algorithms choose several menus in a dataindependent way (via data-independent discretization) and then select the best of them based on the data. In the context of two-part tariffs, our algorithm is much simpler than prior ones for the same problem, yet it enjoys improved worst-case runtime guarantees compared to them [Balcan et al., 2018c, 2020b] when the length of the menu is more than one (Theorem 15). We note that for other data-driven algorithm design problems, such as datadriven clustering and data-driven learning to branch, it was proven that algorithms that use data-independent discretization could perform very poorly [Balcan et al., 2017, 2018a]. Thus, by contrast, our work shows the power of data-independent discretization for datadriven mechanism design and algorithm design more generally. In the context of lotteries, compared to the previous distributional learning results for fixed-length menus [Balcan et al., 2018c], our algorithm requires similar sample complexity; however, it has an efficient implementation (Theorems 22 and 56).\nLimited Buyer Types. For limited buyer types, we provide improved regret bounds for both the full-information and partial-information (bandit) settings for both menus of two-part tariffs and lotteries (Theorems 13, 14, 20 and 21). The high-level idea is as follows. Consider the revenue function in the parameter space for a fixed buyer. The parameter space is partitioned into regions where, within each region, the buyer selects the same option in the menu, e.g., the same lottery, resulting in a continuous revenue function. Discontinuity occurs across regions. For limited-type buyers, by superimposing the revenue functions for all types, the parameter space divides into more (albeit still a limited number of) regions. Regardless of the buyer type at hand, the revenue function is continuous within each region, and in our case, linear. Therefore, it is sufficient to only consider the corner points as potential parameter vectors that maximize the revenue. We show that in the full information case, running the weighted majority algorithm on the set of menus corresponding to the regions\u2019 corner points results in sublinear regret.\nIn the partial information setting, in each round, we only observe the revenue of the current menu. To estimate the revenue from all the menus efficiently, or in other words, to find an unbiased estimator with a bounded range, we employ the notion of barycentric spanners in online learning introduced by Awerbuch and Kleinberg [2008]. By utilizing this\nconcept, we provide algorithms with a regret bound that is sublinear in the number of timesteps and polynomial in other parameters. This is the first time that barycentric spanner notion has been applied to an auction design setting. Similar contributions have been made in security games by [Balcan et al., 2015]."
        },
        {
            "heading": "1.2 Related Work",
            "text": "Studying learnability of classes of mechanisms for the revenue maximization objective has been of great interest in recent years. These mechanisms have been studied mostly in a distributional setting, where buyers\u2019 values are drawn from an unknown distribution, and the online setting, where there is no distributional assumption on the buyers\u2019 values, has been explored less.\u2217 In the distributional setting, various mechanism classes, including posted-price mechanisms, second-price auctions with reserves, menus of two-part tariffs, and menus of lotteries, are known to be learnable [Morgenstern and Roughgarden, 2015, 2016, Balcan et al., 2016, 2018c, 2021a, Dughmi et al., 2014, Gonczarowski and Weinberg, 2021, Mohri and Medina, 2016, Syrgkanis, 2017, Du\u0308tting et al., 2019]. In the online setting, under adversarial input [Blum et al., 2004, Kleinberg and Leighton, 2003, Blum and Hartline, 2005, Balcan and Blum, 2006, Roughgarden and Wang, 2016, Bubeck et al., 2017], and also under stochastic input [Cesa-Bianchi et al., 2014, Balcan et al., 2018b, 2020a] mostly simple mechanisms such as posted pricing and second-price auction are considered where both mechanisms sell the items separately. An exception is Roughgarden and Wang [2016] who study Vickrey-Clarke-Groves (VCG) mechanism with multiple reserves; however, the algorithms provided are not no-regret in the classic sense but are bounded-regret compared to a constant approximation of the optimal solution.\nTwo of the prominent approaches used for developing distributional results are pseudodimension-based and discretization-based. In the first approach, despite the discontinuity present in the utility of buyers as a function of the parameters used in the mechanism, it is shown that the pseudo-dimension of the family is bounded by using smoothness assumptions on the distribution. This approach applies to all the mechanisms mentioned above. In the discretization approach, a finite set of parameters are identified such that limiting the search space to this set is approximately optimal. This approach has been used for a limited number of mechanisms, such as item-pricing for combinatorial auctions for unrestricted supply Balcan et al. [2008] and menus of lotteries in a limited setting [Dughmi et al., 2014]. In the online setting, Balcan et al. [2018b] and Balcan et al. [2020a] introduce dispersion as a sufficient condition for online learnability of families of mechanisms. They show several classes of mechanisms, such as posted-price mechanisms and second-price auctions with reserves, satisfy dispersion and, therefore, establish strong regret bounds for online learning. Discretization-based techniques in online learning scenarios have been used for the simple cases of item-pricing [Blum et al., 2004] and the second-price auctions [Cesa-Bianchi et al., 2014].\n\u2217Some online learning algorithms, including those proved via the dispersion method, explained later, still make distributional assumptions; however, unlike the distributional learning setting, the draws are not necessarily from identical distributions.\nTwo-Part Tariffs. Two-part tariff pricing schemes were first introduced by Lewis [1941] and later analyzed by Oi [1971]. Menus of two-part tariffs have been studied recently in the context of distributional learning [Balcan et al., 2018c, 2020b, 2022a]. A recent work [Balcan et al., 2022a] provides improved running time bounds over [Balcan et al., 2020b] for distributional learning of two-part tariffs in the case where the number of pieces with continuous sum of utility functions u(xi, \u00b7) across all problem instances is small (as defined in Section 3.2.2 the utility function u(xi, .) measures the performance of our two-part tariff mechanisms on a fixed problem instance xi as a function of its parameters). However, for the case where the menu length is strictly greater than 1, [Balcan et al., 2022a] approach does not lead to improved running time over [Balcan et al., 2020b] for worst-case instances. So for worst-case instances and menu-length > 1, our approach for distributional learning improves over previously best known results.\nMenus of Lotteries. Menus of lotteries capture all possible mechanisms, including the optimal one, for selling items to buyers. The Taxation Principle [Dasgupta et al., 1979, Guesnerie and Oddou, 1981] asserts that any mechanism for a single buyer can be represented as a menu of lotteries, where the buyer selects their favorite lottery (that is, the one that maximizes the buyer\u2019s expected value for the randomized allocation minus the price paid). Furthermore, menus of lotteries achieve revenue beyond other well-studied families of mechanisms such as posted pricing and, more generally, any deterministic mechanism. For a correlated buyer (a buyer whose values for items are correlated), even in the simple cases where the buyer is additive (their value for a bundle of items is the sum of the value for individualized items) or unit-demand (their value for a bundle of items is the maximum value for an item in the bundle), the gap between optimal randomized mechanism (lotteries) and item-pricing is infinite [Briest et al., 2010, Hart and Nisan, 2019]. Daskalakis et al. [2014] show that even for an independent additive buyer (the values for the items are independent), lotteries (randomized mechanisms) are necessary and provide strictly more revenue compared to any deterministic mechanism, including pricing mechanisms.\nFailure of data-independent discretization-based learning. Discretization is a natural approach for designing algorithms to tune parameters (e.g., prices for menus of two-part tariffs and allocation probabilities and prices for menus of lotteries) and is commonly used in applied fields such as applied machine learning. However, recent work has shown that in tuning parameters of algorithms for solving discrete combinatorial problems, discretization in the context of data-driven algorithm design does not always work if discretization is done in a data-independent way. For the case of tuning parameters for linkage-based algorithms, Balcan et al. [2017] showed that for several natural parameterized families of clustering procedures, for any data-independent discretization, there exists an infinite family of clustering instances such that any of the discrete parameters will output a clustering that is an \u2126(n) factor worse than the optimal parameter, where n is the input size. Here, the quality of clustering can be defined according to several well-known objectives, including kmedian, k-means, and k-center. Balcan et al. [2018a] show that for the data-driven problem of learning to branch for solving mixed integer linear programs (MILPs), data-independent discretization will not work either. More specifically, for any discretization of the parame-\nter space [0, 1], there exists an infinite family of distributions over MILP problem instances such that for any parameter in the discretization, the expected tree size is exponential in the input parameter. Yet, there exists an infinite number of parameters such that the tree size is just a constant (with probability 1). Remarkably, we show that in our context, even data-independent discretization works.\nDispersion and Online Data-Driven Algorithm Design. Dispersion is a recentlydeveloped notion for families of algorithmic and mechanism design problems and serves as a sufficient condition for the existence of bounded-regret online learning algorithms [Balcan et al., 2018b, 2020a] and differentially private distributional learning algorithms [Balcan et al., 2018b]. Generally speaking, this condition bounds the concentration of discontinuities of the objective function in any small regions in the parameter space. Dispersion-based techniques have been established successfully for a variety of algorithms [Balcan and Sharma, 2021, Balcan et al., 2021b, 2022b], among which is tuning parameters in combinatorial problems, such as clustering problems discussed above [Balcan et al., 2018b]. For menus of two-part tariffs, we show dispersion condition is satisfied, immediately implying no-regret online learning algorithms and differentially-private algorithms for distributional learning. Surprisingly, we present evidence that dispersion might not apply to menus of lotteries. In particular, we show in menus of lotteries the objective function might have sharp discontinuities concentrated in a small region. This structural property is in stark contrast with menus of two-part tariffs and other mechanism and algorithm families satisfying dispersion. Despite this evidence, we show that a simple discretization-based approach leads to no-regret online learning algorithms for menus of lotteries.\nSample Complexity for Menus of Lotteries. The sample complexity for menus of lotteries has been studied under two different assumptions: independence of valuation across items, as studied by Gonczarowski and Weinberg [2021] and correlated valuation across items, as studied by Dughmi et al. [2014]. By assuming independence simultaneously among the buyers and the items, a significant improvement over the sample complexity is possible [Gonczarowski and Weinberg, 2021]. However, when the value for the items are possibly correlated, Dughmi et al. show a lower bound on the sample complexity verifying an exponential gap on the dependence in the number of buyers compared to Gonczarowski and Weinberg. Similar to Dughmi et al. and in contrast with Gonczarowski and Weinberg, we do not assume independence across items and only assume independence among the buyers."
        },
        {
            "heading": "2 Model and Preliminaries",
            "text": "We consider selling items to a single buyer for the revenue objective through parameterized families of mechanisms. In this paper the family of mechanisms is either the set of menus of two-part tariffs or lotteries. To put our notations in context, in this section we focus on menus of two-part tariffs as our running example.\nMenus of two-part tariffs are used for selling multiple units (i.e., copies) of a single item through a list of up-front and per-unit fee pairs that the buyer can choose from. Menu\nM = {(\np (1) 1 , p (1) 2\n) , . . . , (\np (\u2113) 1 , p (\u2113) 2\n)}\n\u2286 R2\u2113, is a length-\u2113 menu of two-part tariffs. Each menu M is parameterized by \u03c1 which in this case is 2\u2113-dimensional and contains all p\n(j) 1 and p (j) 2\nwhere all p (j) 1 , p (j) 2 \u2208 [0, H ]. p (j) 1 and p (j) 2 are called the up-front fee (price) and per-unit fee (price) of tariff j, respectively. We denote a buyer\u2019s valuations for all 1, 2, . . . , K units by v = (v(1), . . . , v(K)), where the values are nonnegative, monotonically increasing, belong to [0, H ], and v(0) = 0. Under the tariff j denoted by (\np (j) 1 , p (j) 2\n)\nand the number of units\nk \u2208 {1, . . . , K} that the buyer selects, she receives k units of the item and pays p(j)1 + kp (j) 2 . The buyer\u2019s utility is her value for the number of units bought v(k) less the payment. Each buyer has the option of buying their utility-maximizing tariff and number of units. In other words, the buyer will buy k units using tariff j that maximizes v(k) \u2212 p(j)1 \u2212 kp (j) 2 or does not buy and does not pay anything. Let M be an infinite set of mechanisms parameterized by a set C \u2286 Rd. In this paper, M is either the set of two-part tariff menus or lottery menus. Consider the case whereM is the set of two-part tariff menus for selling multiple units of a single item to a buyer with value v while the menu corresponds to parameter \u03c1 \u2208 C. Next, let \u03a0 be a set of problem instances forM, such as a set of buyer valuations v, and let u : \u03a0\u00d7 C \u2192 [0, H ] be a utility function where u(x,\u03c1) measures the performance of the mechanism with parameters \u03c1 on problem instance x \u2208 \u03a0. In our case, u(x,\u03c1) is the revenue of the mechanism (a menu of two-part tariffs or lotteries) with parameters \u03c1 on input x. For example, for the menus of two-part tariffs, M is the set of possible menus M and since each menu is 2\u2113-dimensional with each dimension in [0, H ], C = [0, H ]2\u2113 \u2286 R2\u2113. \u03a0 is the set of buyer valuations v and u : \u03a0 \u00d7 C \u2192 [0, H ] be a utility function where u(v,\u03c1) measures the revenue of the menu with parameters \u03c1 on buyer valuations v \u2208 \u03a0.\nOnline Setting In this setting, a sequence of functions u1, . . . , uT : C \u2192 [0, H ] arrive one by one. Unlike u, ut only takes parameter \u03c1t as the input and is defined as ut(\u03c1t) := u(\u03c1t, xt), where xt is the problem instance at timestep t. At the time t, the no-regret learning algorithm chooses a parameter vector \u03c1t and then either observes the function ut in the full information setting, the scalar ut(\u03c1t) in the bandit setting, or ut(\u03c1t) for a set of \u03c1 in the semibandit setting. The goal is to minimize the expected regret, E[max\u03c1\u2208C \u2211\nut(\u03c1)\u2212 ut(\u03c1t)]. We study the online setting both under adversarial input, where ut() are selected adversarially, and under smoothed distribution inputs which assume more structure. The expectation in the regret formula is taken over the randomness of the algorithm in the adversarial setting and over the randomness of the algorithm and distribution of buyers in the smoothed distributional setting.\nDistributional Setting In the distributional setting, the algorithm receives samples from an unknown distribution D over problem instances \u03a0. The goal is to find a parameter vector \u03c1\u0302 that nearly maximizes the expected utility, i.e., max\u03c1\u2208C Ex\u223cD[u(x,\u03c1)] similar to statistical learning theory [Vapnik, 1998] or PAC learning Valiant [1984]."
        },
        {
            "heading": "3 Menus of Two-Part Tariffs",
            "text": "In this section, we consider M = {(\np (1) 1 , p (1) 2\n) , . . . , (\np (\u2113) 1 , p (\u2113) 2\n)}\n\u2286 R2\u2113 as a length-\u2113 menu of two-part tariffs. See Section 2 for a detailed description."
        },
        {
            "heading": "3.1 Discretization Procedure",
            "text": "This section shows a discretization procedure for the menus of two-part tariffs. Given any menu and value 0 < \u03b1 < 1, we provide an alternate menu such that all the price elements, p (i) 1 and p (i) 2 for all i, are multiples of \u03b1 and the alternate menu provides nearly as much revenue as the given menu up to a term that depends on \u03b1. The main result of this section is summarized in the following statement.\nTheorem 1. Given a menu of two-part tariffs M and parameter 0 < \u03b1 < 1, Algorithm 4 outputs menu M \u2032 whose revenue is at least the revenue of M less 2K\u03b1\u2113, for any buyer\u2019s valuation. Furthermore, for all i, all p\n(i) 1 and p (i) 2 are multiples of \u03b1. The set of potential\noutcomes constitutes a space with at most min{(H/\u03b1)2\u2113, 2H2/\u03b12} menus, where H is the maximum value for any number of units.\nAlgorithm 1: (Almost) revenue preserving rounding for menus of two-part tariffs\nInput: Menu M , discretization parameter \u03b1. 1: Let M \u2032 be the menu of Pareto frontier tariffs in M , derived by one by one deleting\ntariffs i for which there exists tariff j 6= i such that p(i)1 \u2265 p (j) 1 and p (i) 2 \u2265 p (j) 2 .\n2: Reindex the tariffs in M \u2032 in increasing order of p1 (and hence, decreasing order of p2). 3: For each tariff i, decrease p (i) 1 and p (i) 2 by (i\u2212 1)\u03b1. \u22b2 The revenue preserving step. 4: Round down all p (i) 1 and p (i) 2 to the closest multiple of \u03b1.\n5: Remove the duplicate tariffs. Output: Menu M \u2032.\nProof idea of Theorem 1 and intuition behind Algorithm 1. The main structural ideas deriving the algorithm and the proof of the revenue guarantee are as follows: (i) for a fixed number of units k to be purchased, the utility-maximizing tariff is the same across all the buyer\u2019s valuations; namely, the tariff that has the smallest overall price (upfront price plus k times per-unit price), and (ii) as the number of units to be purchased increases, the per-unit price of the utility-maximizing tariff decreases. The main idea of the rounding algorithm is decreasing the corresponding prices of tariffs with lower per-unit fees by a larger amount (Line 3). By doing so, for each buyer, the total price of buying more units decreases more than the total price of buying fewer units. This step ensures that the buyer does not switch from purchasing more units to fewer units after the rounding. This property is sufficient for the revenue guarantees. The other steps of the algorithm delete redundant tariffs (Lines 1 and 5) and ensure the final prices are multiples of \u03b1 (Line 4). The theorem provides two upper bounds for the size of the discretized space. By Line 4, all the prices are multiples of \u03b1. Therefore, the 2\u2113 price components in a length-\u2113 menu each have H/\u03b1 options, which\ngives the first bound. On the other hand, if we consider a single tariff, each of the up-front fee and the per-unit fee has H/\u03b1 possibilities, therefore, the total number of possible unique tariffs are H2/\u03b12. Each of these possible tariffs may or may not be on the menu, giving the second bound. The full proof is provided in Appendix A."
        },
        {
            "heading": "3.2 Online Learning",
            "text": "We provide bounded-regret online learning algorithms in full and partial information settings. Sections 3.2.1 to 3.2.3 provide online algorithms under adversarial input, under smooth distributions, and for limited type buyers, respectively."
        },
        {
            "heading": "3.2.1 Online Learning Under Adversarial Inputs",
            "text": "The main statements are Theorems 2 and 3 which provide regret guarantees for the fullinformation case and partial-information case, respectively. Using the discretization in Section 3.1, we show a reduction to finite number of experts and run standard learning algorithms (weighted majority and Exp3) over the menus in the discretized set. Similar ideas were used in previous papers, for example [Blum et al., 2004, Balcan et al., 2018b].\nFull Information In the full information setting, the seller sees the revenue generated for all the possible menus. To design an online algorithm in this case, we use a variant of the weighted majority algorithm by [Auer et al., 1995]. The experts in our case are the discretized menus from the previous section, denoted in the algorithm by set X = m1, . . . , mn. Furthermore vt is the valuation of the buyer are time t and Revk(v1, . . . , vt) is the cumulative revenue of menu mk for the buyers until time step t.\nAlgorithm 2: Full-information (Weighted majority on discretized menus)\nInput: Set of menus (experts) X = m1, . . . , mn, learning rate \u03b2 \u2208 (0, 1]. 1: Initialize: For each menu mk, initialize Revk() = 0, wk(0) = 1 2: for buyer t = 1, . . . , T do\nSelect menu at time t to be mk with probability \u03c0k[t] = wk(t\u22121)\u2211n j=1 wj(t\u22121) ; Observe valuation of buyer t as vt ; For each menu mk, update Revk(v1, . . . , vt) and\nwk(t) = (1 + \u03b2) Revk(v1,v2,...,vt)/H ;\nTheorem 2. In the full information case for length-\u2113 menus of two-part tariffs, running Algorithm 2 over discretized set of menus specified in Theorem 1 for \u03b1 = \u03b2 = 1/ \u221a T has regret bounded by O\u0303 ( \u2113(K +H lnH) \u221a T ) , and running time O(T\u2113Kmin{H2\u2113T \u2113, 2H2T}).\nThe proof follows by combining the guarantees of the discretization procedure (Theorem 1) and previously known results (specifically [Auer et al., 1995], Theorem 3.2) and is deferred to Appendix A.\nPartial Information (Bandit Setting) In the partial information setting, the seller does not see the outcome for all the possible menus and only observes the outcome of the menu used (the tariff and number of units chosen by the buyer). To design an online algorithm in this case, we use a version of the Exp3 algorithm in [Auer et al., 1995]. This variant of the Exp3 algorithm contains the weighted majority algorithm (Algorithm 2) a subroutine. At each step, we mix the probability distribution \u03c0, used by the weighted majority algorithm, with the uniform distribution to obtain a modified probability distribution \u03c0, which is then used to select a menu from our discretized set. Following the tariff and the number of units chosen by buyer t, we use the price paid (the gain from the chosen menu) to formulate a simulated gain vector, which is then used to update the weights maintained by the weighted majority algorithm.\nAlgorithm 3: Partial-information (Exp3 on discretized menus)\nInput: Set of menus (experts) X = m1, . . . , mn, learning rate \u03b2 \u2208 (0, 1], parameter \u03b3 \u2208 (0, 1].\n1: Initialize: For each menu mk, initialize Revk() = 0, wk(0) = 1 2: for buyer t = 1, . . . , T do\nSelect menu at time t to be mk with probability \u03c0k(t) = (1\u2212 \u03b3)\u03c0k(t) + \u03b3/n where \u03c0k[t] =\nwk(t\u22121)\u2211n j=1 wj(t\u22121) ;\nFor the selected menu k\u2217, set gk\u2217(t) to be the price paid by buyer t. Set\ngk\u2217(t) = \u03b3 n gk\u2217(t) \u03c0k\u2217(t) ; For all other menus k, set gk(t) = 0; For all menus k, update Revk(t) = Revk(t\u2212 1) + gk(t) and wk(t) = (1 + \u03b2)Revk(t)/H ;\nTheorem 3. In the partial information case for length-\u2113 menus of two-part tariffs, running Algorithm 3 over discretized set of menus in Theorem 1 for \u03b1 = T\u22121/(2(1+\u2113)), \u03b2 = \u03b3 = T\u22121/(4(1+\u2113)) has regret bound O\u0303 ( T 1\u2212 1 2(1+\u2113) \u2113(K +H2\u2113+1) )\n, and running time O(T min{min{H2\u2113T \u2113, 2H\n2T}, 2H2T}). The proof follows by combining the guarantees of the discretization procedure (Theorem 1)\nand previously known results (specifically [Auer et al., 1995], Theorem 4.1) and is deferred to Appendix A."
        },
        {
            "heading": "3.2.2 Online Learning Under Smooth Distributions",
            "text": "Recent papers studying online learning of mechanisms studied the problem in the restricted setting, where at each point in time, the buyers\u2019 valuations come from \u03ba-bounded distributions, where the density function is bounded at all points by \u03ba. This assumption has proved to be sufficient for a few classes of mechanisms, including posted-pricing and second-price mechanisms, to establish dispersion. At a high-level, dispersion ensures that the number of discontinuities in a small ball in the parameter space is limited with high probability and is a sufficient condition for bounded-regret online algorithms. We prove that menus of two-part tariffs satisfy dispersion and use it to derive bounded-regret algorithms for fullinformation, bandit, and semi-bandit settings. The main difference between the algorithms\nused in this suction compared to the adversarial input setting in Section 3.2.1 is that we previously needed to go through a careful data-independent discretization step (Section 3.1) to reduce the problem to a finite number of experts. However, under smooth distributions, the assumed properties of the distribution influence the set of experts chosen.\nWe provide the main results in this setting, followed by a discussion of the key ideas behind the algorithms and proofs. After establishing the dispersion constraint for menus of two-part tariffs, it is sufficient to employ previously known algorithms designed for dispersed settings to achieve no-regret guarantees. The primary purpose of this section is to compare the regret guarantees from the recently developed online learning technique of dispersion and the discretization approach discussed in the previous section. The formal definition of dispersion and technical descriptions of the algorithms and proofs are deferred to the appendix. The main results are as follows\u2020:\nDefinition 4. [\u03ba-bounded] A density function f : R\u2192 R corresponds to a \u03ba-bounded distribution if max{f(x)} \u2264 \u03ba.\nTheorem 5. Let u1, . . . , uT : C \u2192 [0, H ] be the revenue functions of two-part tariff menus such that ut(\u03c1) denotes the revenue of a mechanism associated with menu parameters \u03c1 for the buyer arriving at time t. Let the samples of buyers\u2019 values be drawn from S \u223c D(1)\u00d7\u00b7 \u00b7 \u00b7\u00d7D(T ). Suppose v(k) \u2208 [0, H ] for any number of units k \u2208 [K]. Also, suppose that for each distribution D(t), and every pair of number of units k and k\u2032, v(k) and v(k\u2032) have a \u03ba-bounded joint distribution. An efficient implementation of the exponentially weighted forecaster with \u03bb = \u221a 2\u2113 ln(2H2\u03ba \u221a T )/T/H (Algorithm 5) has expected regret bounded by O\u0303((H\u21132K2 \u221a log \u03ba+ 1/(H\u03ba)) \u221a T ) and runs in time O\u0303((T + 1)poly(\u2113,K)poly(\u2113, \u221a T ) +KT \u221a T ).\nTheorem 6. Let u1, . . . , uT : C \u2192 [0, H ] be the revenue functions of two-part tariff menus such that ut(\u03c1) denotes the revenue of a mechanism associated with menu parameters \u03c1 for the buyer arriving at time t. Let the samples of buyers\u2019 values be drawn from S \u223c D(1) \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 D(T ). Suppose v(k) \u2208 [0, H ] for any number of units k \u2208 [K]. Also, suppose that for each distribution D(t), and every pair of number of units k and k\u2032, v(k) and v(k\u2032) have a \u03ba-bounded joint distribution. There is a bandit-feedback online optimization algorithm with expected regret O\u0303 ( T (2\u2113+1)/(2\u2113+2) ( H2K \u221a \u2113\u03bad/2 \u221a log \u03ba ) + 1/H\u03ba +H\u21132K2 ) . The per-round\nrunning time is O(H4\u2113\u03ba2\u2113T \u2113).\nTheorem 7. Suppose the buyers\u2019 values are drawn from D(1)\u00d7\u00b7 \u00b7 \u00b7\u00d7D(T ), where each D(t) is \u03ba-bounded for \u03ba = o\u0303(T ). Then, running the continuous Exp3-SET algorithm (Algorithm 7) for menus of two-part tariffs under semi-bandit feedback has expected regret bounded by O\u0303(H \u221a \u2113T ). An efficient implementation has the same regret bound and running time O\u0303((T +\n1)poly(\u2113,K)poly(\u2113, \u221a T ) +KT \u221a T ).\n\u2020The regret term in the semi-bandit algorithm (Theorem 7) is better than the full-information algorithm (Theorem 5) since different notions of dispersion are used. Also, the stated running time of both algorithms are the same; however, this is in the worst case, and the semi-bandit algorithm potentially performs fewer computations.\nPartitioning of parameter space to convex regions with linear utilities [Balcan et al., 2018c] Consider the sequence of buyers valuations b. At each time step, a buyer is presented a menu, and based on the menu and their valuation, they select the tariff index and number of units that maximizes their utility. Formally, given menu \u03c1, buyer i with valuation bi selects option (j, k), where j is the tariff index and k is the number of units if this option produces more utility for the buyer than any other options. Concretely,\nbi(k)\u2212I{k \u2265 1} ( p (j) 1 (\u03c1) + kp (j) 2 (\u03c1) ) \u2265 bi(k\u2032)\u2212I{k\u2032 \u2265 1} ( p (j\u2032) 1 (\u03c1) + k \u2032p (j\u2032) 2 (\u03c1) ) \u2200j\u2032, k\u2032 (1)\nwhere p (j) 1 (\u03c1) and p (j) 2 (\u03c1) are the up-front fee and per-unit fee of tariff j in menu \u03c1. The above inequalities identify a convex polytope of parameter vectors (menus \u03c1) with hyperplane boundaries. Since the tariff index and the number of units that bi selects are fixed in the region, the revenue, I{k \u2265 1} (\np (j) 1 (\u03c1) + kp (j) 2 (\u03c1)\n)\n, is continuous and more specifically linear\nin the region (formally proved in Lemma 39). Following the same argument for the buyers in the sequence, the parameter space for each buyer is partitioned into convex polytopes where the revenue for the buyer\u2019s valuation is linear inside the polytopes. By superimposing these partitionings, since the intersections of convex regions are also convex, and the sum of linear functions (here revenues) is linear, the parameter space, C is partitioned into convex regions such that the cumulative revenue for the sequence is linear in each region. Inside each region, the utility-maximizing choice of each buyer is fixed; therefore, each region is associated with a mapping from buyer valuations to their corresponding utility-maximizing tariff index and number of units. We may use the mapping, formally defined in Section 3.2.3, to denote the region, e.g., region P\u00b5 corresponding to mapping \u00b5, or simply use cardinal indices for the regions P1,P2, . . ..\nDispersion for menus of two-part tariffs We provide intuition why menus of two-part tariffs for bounded density distributions satisfy dispersion; that is, the discontinuities in the revenue function do not concentrate with high probability. To prove this, we focus on Equation (1) for fixed values of j, k, j\u2032, k\u2032, i.e., pairs of tariffs and units, and for all bi \u2208 b. The equalities for all of these equations are met at parallel hyperplanes because, for each \u03c1 and fixed pairs of tariffs and units, other parameters, i.e., k, k\u2032, p (j) 1 , p (j) 2 , p (j\u2032) 1 , p (j\u2032) 2 are fixed, and the equations are only different in bi. Assuming independence of distributions among buyers and \u03ba-bounded joint distributions over bi(k) and bi(k\n\u2032), with high probability the intersection of multisets of parallel hyperplanes, defined by Equation (1) do not concentrate, implying dispersion. A concrete definition of dispersion and a formal dispersion proof are presented in the appendix.\nOverview of Algorithms We provide high-level ideas for the full-information, banditsetting, and semibandit-setting algorithms used for Theorems 5 to 7, respectively. Generic forms of these algorithms were devised by Balcan et al. [2018b, 2020a] for dispersed families of algorithms. The full information algorithm considers the cumulative revenue function up until the time t \u2212 1 over the parameter space and samples the menu to present at time t proportional to an exponential function of its cumulative revenue. In order to have an efficient implementation, they use techniques from high-dimensional geometry and approximately sample menu \u03c1t. Let P1, . . . ,Pn be the partition of C until time t. The algorithm picks Pi with probability approximately proportional to the region\u2019s cumulative weight and outputs a sample from the conditional distribution of menus in Pi. The bandit-setting algorithm considers a grid over the parameter space, whose granularity depends on the dispersion parameters, and runs the Exp3 algorithm over menus corresponding to the grid. The semibandit-setting algorithm is a continuous version of the Exp3-SET algorithm of Alon et al. [2017]. At each time step, the algorithm learns the revenue function (only) inside the region Pi that the presented menu belongs to and updates the menu weights for the next round accordingly.\nComparison to the results in Section 3.2.1 Although the discretization-based algorithms work under adversarial inputs and are more general, they provide similar regret bounds and even improved running times in some cases. In the full information case, the dependence on the regret bound in parameter T is similar in both algorithms. In running time, the discretization-based algorithm suffers worse dependence in H , but enjoys better dependence in T and K (the maximum number of units) compared to the dispersion-based algorithm. In the bandit setting, similarly, the regret bounds are similar in their dependence on T , while the running-time comparison depends on the value of \u03ba (maximum density under smoothness assumption) such that lower-density distributions may result in better running times."
        },
        {
            "heading": "3.2.3 Limited Buyer Types",
            "text": "In this section, we assume that there are a finite known number of buyer types. This information provides extra structures compared to the general setting considered previously. In particular, now the mechanism designer is aware of where the potential discontinuities\nhappen as a function of the parameter space. We provide algorithms with bounded regrets both for the full information and partial information settings specific to limited types. These algorithms improve the regret bounds significantly when the number of buyer types is small. This section is inspired by Balcan et al. [2015] and includes similar algorithms and notations.\nBalcan et al. [2015] study a security games setting, in which at each time step, the defender has a mixed strategy (a probability distribution) for protecting the attack targets. Knowing this mixed strategy, the attacker selects a target to attack, which maximizes the attacker\u2019s utility (depending on the attacker\u2019s type). Considering the target selected by each attacker type as a function of the defender\u2019s mixed strategy, the mixed strategy space is partitioned into regions where the action of each attacker type is fixed throughout each region. This is very similar to our setting, where the parameter space is partitioned into regions, where inside each region, each buyer type selects a fixed tariff index and the number of units (see the discussion on partitioning the parameter space in Section 3.2.2). Balcan et al. use the linear structure of utility inside each region to develop a no-regret full-information algorithm. In the partial information setting, other than the linearity of utility functions, they use the dependence of an agent\u2019s (in their case, attacker, and in our case, buyer) actions across different regions and identify a limited number of mixed strategies (corresponding to menus in our case) such that observing the agent\u2019s response to them suffice to estimate the utility of other strategies. We use similar machinery in both the full and partial information settings. However, the source of linearity of the utility is different across the two settings. In the security games context, the attackers\u2019 actions are fixed inside regions, and the cumulative utility is a weighted sum of the utility of those actions where the weights are the parameter space coordinates. In our setting, we utilize the specific structure of menus and show the cumulative utility is a linear function of coordinates. For completeness and making the paper self-contained, we include a full description of the algorithms and techniques adapted to our setting and using our terminology.\nIn this setting, we utilize the knowledge of the potential buyer types to design a limited number of menus and optimize over this set. In contrast to the previous section, where the valuations were realized after the arrival of the buyers, here, we have access to all potential buyer types up-front, but similarly, as discussed in Section 3.2.2, the piecewise linear structure of the utility for the buyers partition the parameter space such that each part has linear cumulative utility [Balcan et al., 2018c]. This partitioning is equivalent to dividing the parameter space into convex regions such that in each region, there is a fixed mapping from the buyer types to the menu options that each buyer selects. We show that in each region, we need to consider only a limited number of menus, namely the extreme points.\nConsider v1, . . . , vV as the set of all potential buyer valuations. V denotes the number of buyer types. In order to define the behavior of buyers in each region, we need to define a concept called menu options, which determines the choices of the buyers.\nDefinition 8 (menu option for menus of two-part tariffs, O). A pair (j, k), where j is the tariff index 1, . . . , \u2113, and k is the number of units 0, 1, . . . , K is a menu option. We denote the set of all menu options as O. This set identifies all potential actions of a buyer when presented with a menu.\nDefinition 9 (mapping \u00b5, feasible mappings, P\u00b5). A mapping \u00b5 is a function from buyer types, v1, . . . , vV to menu options (j, k), where j and k are the tariff index and the number of units assigned to the buyer type respectively. Mapping \u00b5 is feasible if there is a menu corresponding to the mapping, i.e., a menu that if presented to the buyers, each buyer selects their corresponding option in the mapping as their utility maximizing option. P\u00b5 denotes the region of the parameter space corresponding to \u00b5, i.e., the set of menus inducing mapping \u00b5.\nUsing the discussion in Section 3.2.2 and as formally defined in Lemmas 38 and 39, the parameter space is partitioned to convex polytopes, each with a linear utility function for any sequence of buyer types. Therefore, for optimization purposes, it seems enough to only consider menus corresponding to the extreme points. This intuition is accurate conditioned on a small tweak. Depending on the tie-breaking rule of buyers among menu options producing the same utility, the polytopes P\u00b5 may not be closed. Therefore depending on the tie-breaking rule, we consider a menu in proximity to the extreme point but inside the polytope.\nDefinition 10 (E , extended set of extreme points [Balcan et al., 2015]). For a given \u03b5 > 0, set E is the set of menus as follows: for any \u00b5 and any \u03c1 that is an extreme point of the closure of P\u00b5, if \u03c1 \u2208 P\u00b5, then \u03c1 \u2208 E , otherwise, there exists \u03c1\u2032 \u2208 E such that \u03c1\u2032 \u2208 P\u00b5 and ||\u03c1\u2212 \u03c1\u2032||1 \u2264 \u03b5. From now on, we may refer to E as the extreme points. Lemma 11. The number of extreme points, |E| is at most (V \u21132K2/4)2\u2113. Proof. Length-\u2113 menus of two-part tariffs occupy a 2\u2113-dimensional parameter space. In each d-dimensional space, an extreme point is the intersection of d linearly independent hyperplanes. The total number of hyperplanes defining the regions is H = V (\n\u2113 2\n)(\nK 2\n)\n, where for each buyer type compares the utility of any pair of options, i.e., number of units 0, . . . , K and tariff indices 1, . . . , \u2113. Out of these hyperplanes, we need 2\u2113 of them to intersect to for an extreme point. Therefore, the number of extreme points is at most\n(H 2\u2113 )\n, implying the statement.\nThe following lemma bounds the loss in utility where the set of menus is limited to the extreme points E . The proof is similar to Balcan et al. [2015], however, the loss depends on the problem-specific utility functions.\nLemma 12. Let E be as defined in Definition 10, then for any sequence of buyer valuations b = b1, . . . , bT , and \u03c1 \u2217 as the optimal menu in the hindsight:\nmax\u03c1\u2208E\nT \u2211\nt=1\nu(bt,\u03c1) \u2265 T \u2211\nt=1\nu(bt,\u03c1 \u2217)\u2212 2K\u03b5T.\nProof. The proof consists of a few simple steps: (i) since the mappings partition the space into regions with a fixed mapping, there exists a mapping \u00b5 such that \u03c1\u2217 \u2208 P\u00b5, (ii) the revenue of the buyer valuation sequence is linear in P\u00b5 as shown in Lemma 39, (iii) the closure of P\u00b5 is a convex polytope whose extreme points contain the maximizers of the linear function \u2211\nbi\u2208b u(bi,\u03c1), (iv) one of the maximizers has cumulative utility at least as \u03c1 \u2217 (v)\nthe parameter vectors in \u03b5 proximity of the extreme point inside P\u00b5 approximately preserve the revenue of the extreme points (vi) since by definition of E the L1 distance of each member to an extreme point is at most \u03b5, there is at most \u03b5 distance in the upfront fee and per-unit fee for any tariffs, resulting in the bound in the statement.\nFull Information We first provide an algorithm for the full information case specific to the finite number of buyers. The main result of this section is provided below. The algorithm to achieve this regret guarantee is a weighted majority algorithm (Algorithm 2) on the set of menus corresponding to the extreme points E .\nTheorem 13. In the full information case for length-\u2113 menus of two-part tariffs, when there are V types of buyers, running Algorithm 2 over the set of menus corresponding to set E for \u03b2 = 1/ \u221a T has regret bounded by O\u0303(H\u2113 \u221a T ln(V \u2113K)).\nThe proof follows from Lemma 12 and the guarantee of weighted majority algorithm and is deferred to the appendix.\nPartial Information (bandit) In the partial information setting, in each time step t, we present the arriving buyer a menu and only observe the option selected by the buyer (e.g., the tariff and the number of units) in the presented menu. A natural approach in this setting is running the EXP3 algorithm and using the weighted majority algorithm for the full information case as a subroutine. However, this approach leads to a regret bound that is exponential in the size of the menu (this result is presented formally in Appendix A). An alternative to this approach is estimating the revenue of other menus, more technically finding an unbiased estimator with bounded range for the revenue of all the menus, and then running the full information algorithm with the estimates, as introduced by [Awerbuch and Mansour, 2003]. We take the latter approach and find the estimates by employing the notion of barycentric spanners [Awerbuch and Kleinberg, 2008]. A barycentric spanner is a basis in a vector space such that any vector can be represented as a linear combination of basis vectors with bounded coefficients. By utilizing this concept, we provide algorithms with a regret bound that is sublinear in the number of timesteps and polynomial in other parameters. Similar ideas were employed in [Balcan et al., 2015].\nThere are two main ideas deriving our bounded-regret algorithm. The first is a reduction from the partial information case to the full information case assuming Oracle access to proper estimates of utilities for all the menus, and the second is deriving these estimates. The first idea was introduced by Awerbuch and Mansour [2003], and we directly use an inspired theorem by Balcan et al. [2015] that suits our setting more accurately. For the second, we also use similar machinery to Balcan et al. [2015].\nWe first show how to estimate the utility of any menu by only using the response of the buyers to a limited number of menus. In doing so, we take advantage of the dependence between responses of the buyers for different menus to obtain estimates for unused menus. In order to estimate the expected revenue of each menu over a time interval, it is sufficient to estimate the probability of selection of each option in the menu (tariff index and number of units) by the buyers. Since the price of each option is determined by the menu, we can infer the expected revenue using these probabilities. Note that the option that each buyer type selects is fixed throughout each region. Balcan et al. [2015] use the dependence between these probabilities across regions to find a limited set of menus that infer the estimates. An analogous argument to theirs in our setting is as follows. Let I be the set of length-V indicator vectors that, for each region P\u00b5 and each option (j, k), indicate the (maximal set of) buyer types that select the option (j, k) given menus in P\u00b5. The algorithm presents the\nmenus corresponding to the barycentric spanner of I to buyers at random times and records whether the buyer selects the corresponding option. We show the utility of each menu can be represented as a linear function of its corresponding vectors in I and, therefore, a linear function of the barycentric spanner vectors of I . This is enough to derive the estimates.\nNow, we describe the overall structure of the algorithm. The algorithm operates in time blocks, with each block consisting of exploitation and exploration time steps. The exploration time steps are selected uniformly at random within the block and are limited in number. In an exploitation step, the menu used is the output of the full information algorithm, employing unbiased estimators from the previous time block. These menus are always the extreme points E . During exploration time steps, the menus corresponding to the barycentric spanner are used. At the end of each time block, the algorithm refines the unbiased estimators of all corner points using the information gathered in the exploration phases. A detailed description and proof of the theorem are provided in the appendix.\nTheorem 14. In the partial information (bandit) case for length-\u2113 menus of two-part tariffs, when there are V different types of buyers, there is an algorithm with regret bound of O\u0303(T 2/3\u2113(HKV )1/3 log1/3(V \u2113K))."
        },
        {
            "heading": "3.3 Distributional Learning for Two-Part Tariffs",
            "text": "We present distributional learning results for menus of two-part tariffs. The learning algorithm simply considers all menus in the discretized set specified by Theorem 1 and outputs the empirical revenue-maximizing menu given the samples. More specifically, for each menu in the discretized set, the algorithm computes the cumulative revenue achieved from the samples and outputs the menu with the maximum cumulative revenue. The revenue from each sample (buyer) for a fixed menu is the total payment corresponding to the buyer\u2019s utility maximizing option (tariff index and the number of units). This approach has a major difference with the previous line of work, e.g., [Balcan et al., 2018c, 2020b, 2022a], that did not use a discretization and optimized over the infinite parameter space.\nTheorem 15. In the distributional setting, for length-\u2113 menus of two-part tariffs, there exists a learning algorithm with sample complexity H 2\n2\u03b52 (2\u2113 ln (2KH\u2113 \u03b5 ) + ln (2/\u03b4)), and running time\nH2 2\u03b52 ( 2\u2113 ln ( 2KH\u2113 \u03b5 ) + ln (2/\u03b4) ) K\u2113 ( 2HK\u2113 \u03b5 )2\u2113 .\nRemark. For menus of length larger than one, i.e., \u2113 > 1, The running time from Theorem 15 is roughly the square root of the running time of the previous result [Balcan et al., 2020b, 2022a] in the worst case in terms of parameters H , K, and 1/\u03b5. Under extra structural assumptions, [Balcan et al., 2022a] may result in better running times. See Appendix B for more details."
        },
        {
            "heading": "4 Menus of Lotteries",
            "text": "Consider selling m items to a buyer. A set M = {( \u03c6(0), p(0) ) , ( \u03c6(1), p(1) ) , . . . , ( \u03c6(\u2113), p(\u2113) )} \u2286 R m \u00d7 R, where \u03c6(0) = 0 and p(0) = 0 is a length-\u2113 menu of lotteries. Each \u03c6(j) is a vector\nof length m. Under the lottery ( \u03c6(j), p(j) ) , a buyer receives each item i with probability \u03c6(j)[i] and pays a price of p(j). The buyer\u2019s expected utility for the lottery ( \u03c6(j), p(j) )\nis their expected value for the lottery less their payment. We consider additive and unit-demand buyers. For additive buyers, their value for lottery j is\n\u2211m i=1 v(ei) \u00b7 \u03c6(j)[i], where v(ei) is\ntheir value for item i. The buyer\u2019s expected utility is \u2211m i=1 v(ei) \u00b7 \u03c6(j)[i]\u2212 p(j). Note that for additive buyers, due to linearity of expectation, it does not matter whether the allocation of the items in a lottery, are independent or correlated. For unit-demand buyers, without loss of generality, we only consider lotteries such that\n\u2211m i=1 \u03c6 (j)[i] \u2264 1. Under this constraint, for each lottery j, the allocation of the items are dependent, and the buyer never receives more than one item. In this case, the utility for lottery j has the same expression as for additive buyers. Presented with a menu of lotteries, the buyer selects a utility-maximizing lottery (\n\u03c6(j \u2217), p(j\n\u2217) )\nand the mechanism achieves revenue p(j \u2217).\nPutting the problem formulation in the context of Section 2, M is the set of all menus of lotteries, each parameterized by \u03c1 which in this case contains all \u03c6(j) and p(j), where each \u03c6(j)[i] \u2208 [0, 1] and p(j) is \u2208 [0, mH ] for the additive setting (and \u2208 [0, H ] for the unit-demand setting). \u03a0 is the set of buyer valuations and u : \u03a0\u00d7C \u2192 [0, mH ] be a utility function where u(v,\u03c1) measures the revenue of the menu with parameters \u03c1 on buyer valuations v \u2208 \u03a0."
        },
        {
            "heading": "4.1 Discretization procedure",
            "text": "In this section, we introduce a rounding procedure for menus of lotteries. In this procedure, given any vector of parameters (representing a menu) with arbitrary coordinates, we find a transformation to another vector that has two properties; first, the revenue of the output is nearly as high as the original menu for any valuation; secondly, the coordinates corresponding to allocation probabilities and prices belong to a finite set of values. This rounding procedure performed on all possible menus results in a final set of outcomes. We perform the learning algorithms over this finite set.\nTheorem 16. Given a menu of lotteries M and parameters 0 < \u03b1 < 1, 0 < \u03b4 < 1, and K, an arbitrary natural number, Algorithm 4 outputs menu M \u2032 such that Rev(M \u2032) \u2265 Rev(M)(1 \u2212 \u03b4)(1 \u2212 \u03b1)K \u2212 (2K + 1)\u03b1 \u2212 mH(1 \u2212 \u03b4)K. The set of possible allocation probabilities is {0, (1 \u2212 \u03b1)K \u2032, (1 \u2212 \u03b1)K \u2032\u22121, . . . (1 \u2212 \u03b1)0 = 1}, where K \u2032 = \u230a1/\u03b1 ln (Hm/\u03b1)\u230b and the set of possible prices is {0, Hm\u03b1, 2Hm\u03b1, . . .Hm}. This constitutes a space with at most O ( (1/\u03b1\u2113m+\u2113) (ln (Hm/\u03b1))lm ) discrete points, when limiting to length-\u2113 menus and O ( 2(1/\u03b1 m+1)(ln (Hm/\u03b1))m ) discrete points for arbitrary-length menus.\nOverview of Algorithm 4. The algorithm consists of three main steps and its logic is similar to that of Dughmi et al. [2014]. In step 1, we divide the lotteries in the menu exceeding a minimum price into K levels based on their price (and remove the ones below the minimum). The division in prices is proportional to powers of (1 \u2212 \u03b4) with a higher level k having a higher price, compared to a lower level k\u2032 < k. Step 2 rounds down the allocation probability coordinates to a finite set. By multiplying \u03c6 by (1 \u2212 \u03b1)K\u2212k and then rounding to integer powers of (1\u2212 \u03b1), the allocation probabilities of lower-price levels decrease by a larger factor, making lower-price levels less desirable. Step 3 rounds down the prices, first by multiplying all prices by the same factor, (1\u2212 \u03b1)K , then by rounding to\nmultiples of \u03b1 and finally by subtracting 2k\u03b1, which results in more subtraction of price for originally higher-price entries. The main insight behind nearly preserving the revenue of the original menu (and circumventing the issue with simple rounding) is that prices of the more expensive lotteries (higher-price level) are decreased more than the lower-price ones, while their allocation decreases by a lower factor. This ensures that no buyer with any valuation, switches from a higher-price level to a lower-price, after the rounding.\nAlgorithm 4: (Almost) revenue preserving rounding for menus of lotteries\nInput: Menu of lotteries M with entries of pairs (\u03c6, p), K \u2208 N, and \u03b1 such that 0 < \u03b1 < 1. Step 1: Partition the entries (\u03c6, p) of the menu M into levels, where each level k, for k = 1, . . . , K, contains all entries whose price is in the range mH(1\u2212 \u03b4)K\u2212k+1 < p \u2264 mH(1\u2212 \u03b4)K\u2212k. For every entry (\u03c6, p) in level k, put an entry (\u03c6\u2032, p\u2032) in M \u2032 where \u03c6\u2032 is the outcome of step 2 and p\u2032 is obtained by step 3. Step 2: multiply \u03c6 by (1\u2212 \u03b1)K\u2212k, and round down all allocation probabilities to the set of zero and all integer powers of (1\u2212 \u03b1) in the range [ \u03b1\nHm , 1].\nStep 3: First, multiplying p by a factor of (1\u2212 \u03b1)K , then rounding p down to an integer multiple of \u03b1, and then subtracting 2k\u03b1. Output: M \u2032: the modified menu."
        },
        {
            "heading": "4.2 Online Learning",
            "text": "We provide bounded-regret online learning algorithms in full and partial information settings for fixed and arbitrary-length menus of lotteries. The setting considered is as follows. In each round, a new buyer arrives, and a length-\u2113 lottery menu is presented to the buyer. The buyer selects her utility-maximizing lottery j and pays p(j). The mechanism achieves revenue p(j). Missing proofs and explicit description of the algorithms are deferred to Appendix B.\nIn the full information setting, the seller sees the revenue generated for all the possible menus. Similar to the previous section, we run Algorithm 2 (a weighted majority algorithm) over the discretized set as the outcome of Algorithm 4 and derive the following results for the length-\u2113 menus and arbitrary length menus.\nTheorem 17. In the full information case for length-\u2113 menus of lotteries, running Algorithm 2 over the discretized set of menus specified in Theorem 16 for \u03b1 = T\u22121, \u03b2 = T\u22120.5, K = T 0.5, and \u03b4 = T\u22120.5 has regret O\u0303(m2H\u2113 \u221a T ).\nTheorem 18. In the full information case for arbitrary length menus of lotteries, running Algorithm 2 on menus specified in Theorem 16 for \u03b1 = T\u22121/(2m+2), \u03b2 = T\u22121/(m+1), K = T 1/(m+1), and \u03b4 = T\u22121/(m+1) has regret O\u0303(mHT 1\u22121/(2m+4) lnm (mHT )).\nIn the partial information setting, the seller only observes the revenue generated for the menu at hand. Similar to the previous section, we run Algorithm 3 (EXP3 algorithm) over the discretized set as the outcome of Algorithm 4 and derive the following result for length \u2113 menus.\nTheorem 19. In the partial information case for length-\u2113 menus of lotteries, running Algorithm 3 over discretized set of menus in Theorem 16 for \u03b1 = T\u22121/(\u2113m+2), \u03b2 = \u03b3 = T\u22121/(4\u2113m+8), K = T 1/(2\u2113m+4), and \u03b4 = T\u22121/(2\u2113m+4) has regret O\u0303(m2H\u2113T 1\u22121/(2\u2113m+4) ln\u2113m+1 (mHT )).\nFor the case with V buyer types, we use similar machinery to Section 3.2.3 to derive bounded regret algorithms in the full and partial information settings. The discussion of how to adapt to the lotteries setting is deferred to the appendix. the partial information case.\nTheorem 20. In the full information case for length-\u2113 menus of lotteries, when there are V types of buyers, there is an algorithm with regret bound of O(m2H\u2113 \u221a T ln (V \u2113)).\nTheorem 21. In the partial information (bandit) case for length-\u2113 menus of lotteries, when there are V different types of buyers, there is an algorithm with regret bound of O(T 2/3(\u2113m)4/3(HV )1/3 log1/3(V \u2113)).\nRemark The above results hold under adversarial input. Unlike menus of two-part tariffs (and many other families of algorithms and mechanisms discussed in Balcan et al. [2018b, 2020a]), for menus of lotteries, we provide evidence that dispersion, a sufficient condition for online learning under smooth distributions, may not hold. A formal result is stated as Theorem 58."
        },
        {
            "heading": "4.3 Distributional Learning",
            "text": "In the distributional setting, we have sample access to buyers\u2019 valuations. The value of the buyer for item i is drawn from distribution Di with support [0, H ]\nm; we do not assume independence among items. Similar to the distributional learning algorithm for menus of two-part tariffs, the algorithm simply considers all menus in the discretized set specified by Theorem 16 and outputs the empirical revenue-maximizing menu given the samples. The revenue from each sample (buyer) for a fixed menu is the payment corresponding to the buyer\u2019s utility-maximizing lottery in the menu.\nTheorem 22. For length-\u2113 menus of lotteries, there is a discretization-based distributional learning algorithm with sample complexity O\u0303 (m2H2/\u03b52(\u2113m+ ln (2/\u03b4))), and running time O\u0303 ( (2m2H2/\u03b52) \u2113m+\u2113+1 \u2113(\u2113m+ ln (2/\u03b4)) ln\u2113m (mH/\u03b5 ln (mH/\u03b5)) ) .\nRemark For the limited menu length, the sample complexity of Theorem 22 is roughly the same as [Balcan et al., 2018c], but the advantage is that we provide an efficient algorithm when m and \u2113 are constant. The analysis for arbitrary-length menus is provided in the appendix as Theorem 56. The sample complexity and running time provided are similar to that of [Dughmi et al., 2014], however, Theorem 56 works for a more general setting."
        },
        {
            "heading": "5 Discussion",
            "text": "This paper contributes to both learning theory and mechanism design by studying prominent families of mechanisms from a learning perspective. Our work is focused on learning menu mechanisms that go beyond selling the items separately. Menus of lotteries provide a list of randomized allocations and their corresponding prices to the buyers and are specifically advantageous for selling multiple items. Menus of two-part tariffs, on the other hand, are employed for selling multiple units (copies) of an item by presenting a list of up-front fees and per-unit fees to the buyer.\nDiscretization versus Dispersion The majority of the paper focuses on online learning of these families of mechanisms. Two of the commonly used techniques for this setting are (the more traditional) discretization-based and (the recently-developed) dispersion-based techniques. Menus of lotteries and two-part tariffs are examples of parametric algorithm or mechanism design, where the objective function, here revenue, has sharp discontinuities in the parameter space, and the standard procedures, such as rounding down the parameters to multiples of \u03b5, may result in arbitrary revenue loss. A discretization scheme means that there exists a grid in the parameter space such that for any arbitrary parameter vector, there is a corresponding parameter vector in proximity over the grid generating similar revenue. However, finding the corresponding parameter vector (the direction to move from the original parameter vector in the space) needs taking extra care, and moving in arbitrary direction may cause a large revenue loss. In contrast to the discretization scheme, another method developed for proving online learnability of parameterized algorithms, called dispersion Balcan et al. [2018b, 2020a], asserts that under smoothness assumptions moving in a small ball of parameter vectors, does not face sharp discontinuities with high probability. This means that with high probability, moving in any direction preserves similar revenue. Nevertheless, we show evidence that the dispersion may not hold for menus of lotteries Theorem 58 and while dispersion holds for menus of two-part tariffs Propositions 33 and 36, it heavily uses the smoothness assumption. In conclusion, although a small but arbitrary modification may change the revenue drastically when starting from a parameter vector, in designing our discretization scheme, we show a specific direction such that small modification along that direction preserves the revenue. See Theorems 1 and 16.\nLimitations While we present strong regret-bound guarantees both in the general case and limited buyer types, our algorithms are not always computationally efficient. Designing corresponding computationally efficient algorithms is an open direction."
        },
        {
            "heading": "6 Acknowledgement",
            "text": "The authors would like to thank Avrim Blum, Misha Khodak, Rattana Pukdee and anonymous reviewers for helpful feedback and comments. This material is based on work supported in part by the National Science Foundation under grant CCF-1910321 and a Simons Investigator Award."
        },
        {
            "heading": "A Missing Proofs of Section 3",
            "text": ""
        },
        {
            "heading": "A.1 Discretization Procedure",
            "text": "Before providing the proof of the discretization procedure, we provide intuition why discretization is a nontrivial procedure for menus of two-part tariffs. For this family of mechanisms, standard procedures, such as rounding down the prices to multiples of \u03b1, may result in arbitrary revenue loss because the price parameters of each tariff decrease by different amounts affecting unpredictable changes in utilities of selecting each tariff and number of units. It would be possible that the utility-maximizing choice for a buyer switches from a higher-price tariff and more units (that originally has slightly higher utility for the buyer) to a low-price tariff and fewer units (that originally has slightly lower utility for the buyer) after a simple rounding.\nNow, we provide structural results that enable us to design a discretization procedure. Given a menu of two-part tariffs, the following definition deletes the dominated tariffs (independent of the valuation).\nDefinition 23 (Pareto frontier tariffs). Given menu M with distinct tariffs, the Pareto frontier of M \u2032 is derived by deleting all tariffs i for which there exists a tariff j 6= i such that p (j) 1 \u2264 p (i) 1 and p (j) 2 \u2264 p (i) 2 .\nLemma 24. Given a menu of tariffs, a user only selects a tariff in the Pareto frontier.\nLemma 25. Sorting the tariffs in the Pareto frontier in increasing order of p1 is equivalent to sorting them in decreasing order of p2.\nLemma 26. For any fixed number of units k, the highest utility tariff in M is argmin p (i) 1 + kp (i) 2 . This is independent of the buyers\u2019 values.\nThe following lemma states that as we increase the number of units the utility-maximizing tariff has higher p1 and lower p2.\nLemma 27. Let M \u2032 be the menu of Pareto frontier tariffs derived from menu M . Suppose the tariffs in M \u2032 are reindexed in increasing order of p1. Consider the index of the utilitymaximizing tariff for each number of units. This index is increasing as a function of the number of units.\nTheorem 1. Given a menu of two-part tariffs M and parameter 0 < \u03b1 < 1, Algorithm 4 outputs menu M \u2032 whose revenue is at least the revenue of M less 2K\u03b1\u2113, for any buyer\u2019s valuation. Furthermore, for all i, all p\n(i) 1 and p (i) 2 are multiples of \u03b1. The set of potential\noutcomes constitutes a space with at most min{(H/\u03b1)2\u2113, 2H2/\u03b12} menus, where H is the maximum value for any number of units.\nProof. First, we reason about the length of the outcome menu. Let \u2113 and \u2113\u2032 be the length of the original menu and outcome menu, respectively. First, note that \u2113\u2032 is also the length of the menu after rounding down p\n(i) 1 and p (i) 2 to their closest multiples of \u03b1. Observe that \u2113 \u2032\nis at most \u2113 (because we never add extra tariffs) and also at most H2/\u03b12 because there are H/\u03b1 distinct options for each p1 and p2. Therefore, \u2113 \u2032 \u2264 min{\u2113,H2/\u03b12}.\nThen, we reason about the maximum loss in revenue. First, note that for any fixed tariff and number of units, the total price decreases by at most 2K\u2113\u2032\u03b1. We only need to show that the buyer does not switch from buying more units to fewer. Switching in the opposite order does not decrease the revenue more than 2K\u2113\u2032\u03b1. The reason is that the total price of each tariff is an increasing function as the number of units. Therefore, the minimum total price is increasing as a function of the number of units. Next, we prove that a buyer never switches from buying more units to less. We show two cases: switching between tariffs and staying with the same tariff. In the first case, by Lemma 27, this means that that a buyer never switches from a tariff with higher p1 (lower p2) to a lower p1 (higher p2). Since in the discretization procedure, the price of tariffs with higher p1 decreases more than lower p1, the lower p1 tariffs do not become utility-maximizing if they were not before. In the second case, by the rounding procedure, the total price of more units in the same tariff always decreases more; therefore, the lower number of units never becomes utility maximizing. Therefore, we conclude the payment of each tariff and therefore the revenue decreases at most by 2K\u2113\u2032\u03b1. Thus, Rev(M \u2032) \u2265 Rev(M)\u2212 2K\u03b1\u2113\nFinally, we find the total number of possible menus. Also, after the discretization all p (i) 1 and p (i) 2 are multiples of \u03b1. Therefore, when restricted to length-\u2113 menus, there are H/\u03b1 choices for each 2\u2113 parameter of the menu, making an upper bound of (H/\u03b1)2\u2113. On the other hand, there are at most H2/\u03b12 possible tariffs, and each one of them may appear or not in the menu. Therefore, the number of menus is also bounded by 2H 2/\u03b12 ."
        },
        {
            "heading": "A.2 Online Learning",
            "text": ""
        },
        {
            "heading": "A.2.1 Online Learning Under Adversarial Inputs",
            "text": "Full Information\nProposition 28 ([Auer et al., 1995], Theorem 3.2). For any sequence of valuations v\u0304,\nRevWM (v\u0304) \u2265 OPTX (v\u0304)\u2212 \u03b2\n2 OPTX (v\u0304)\u2212\nH lnn\n\u03b2 ,\nwhere X = m1, . . . , mn are the set of experts (two-part tariff menus), RevWM(v\u0304) is the expected revenue outcome of Algorithm 2, and OPTX (v\u0304) is the revenue of the optimal menu in X.\nTheorem 2. In the full information case for length-\u2113 menus of two-part tariffs, running Algorithm 2 over discretized set of menus specified in Theorem 1 for \u03b1 = \u03b2 = 1/ \u221a T has regret bounded by O\u0303 ( \u2113(K +H lnH) \u221a T ) , and running time O(T\u2113Kmin{H2\u2113T \u2113, 2H2T}).\nProof. Let n be the number of menus resulting from the discretization procedure in Section 3.1. Let vi be the valuation of the buyer at step i, and v\u0304 be the vector of valuation of all buyers in rounds 1 through T . We denote RevM \u2032() as the maximum revenue obtained in the set of menus resulting from the discretization procedure, OPT() as the optimal revenue, and RevWM() as the revenue obtained from the weighted majority algorithm discussed above on\nthe set of outcome menus of the discretization procedure. Then,\nn = (H/\u03b1)2\u2113,\nRevWM (v\u0304) \u2265 Rev(M \u2032) (v\u0304)\u2212 \u03b2 2 Rev(M \u2032) (v\u0304)\u2212 H lnn \u03b2 ,\nRevM \u2032 (v\u0304) = T \u2211\ni=1\nRevM \u2032 (vi) ,\nRevM \u2032 (vi) \u2265 OPT (vi)\u2212 2K\u2113\u03b1; where the first expression is a result of the discretization procedure, the second expression uses Proposition 28, the third expands the revenue over T terms, and the last uses Theorem 1. Rearranging the terms, we have:\nRevM \u2032 (vi) \u2265 OPT (vi)\u2212 2K\u2113\u03b1 RevM \u2032 (v\u0304) \u2265 OPT (v\u0304)\u2212 2K\u2113\u03b1T RevWM (v\u0304) \u2265 OPT (v\u0304)\u2212 2K\u2113\u03b1T \u2212 \u03b2HT\n2 \u2212 H lnn \u03b2\nRevWM (v\u0304) \u2265 OPT (v\u0304)\u2212 2K\u2113\u03b1T \u2212 \u03b2HT 2 \u2212 2H\u2113 (ln (H/\u03b1)) \u03b2\nWe set variables \u03b1 and \u03b2 to minimize the exponent of T in the regret. By substituting n, the regret is upper bounded by\n2K\u2113\u03b1T + \u03b2HT\n2 + 2H\u2113 (lnH \u2212 ln\u03b1) \u03b2 .\nBy setting \u03b1 = \u03b2 = 1\u221a T , The regret will be O\u0303\n( \u2113(K +H lnH) \u221a T ) . Based on the parameters\nchosen, the number of menus is O(min{H2\u2113T \u2113, 2H2T}). The algorithm needs to maintain the weights for these menus and update them based on the revenue at each time step. The revenue of each menu can be calculated in O(K\u2113) given the buyer\u2019s valuation, resulting in the stated running time.\nPartial Information\nProposition 29 ([Auer et al., 1995], Theorem 4.1). For any sequence of valuations v\u0304,\nRevExp3 (v\u0304) \u2265 OPTX \u2212 ( \u03b3 + \u03b2\n2\n)\nOPTX \u2212 Hn lnn\n\u03b2\u03b3 ,\nwhere X = m1, . . . , mn are the set of experts (two-part tariff menus), RevExp3(v\u0304) is the expected revenue outcome of Algorithm 3, and OPTX (v\u0304) is the revenue of the optimal menu in X.\nTheorem 3. In the partial information case for length-\u2113 menus of two-part tariffs, running Algorithm 3 over discretized set of menus in Theorem 1 for \u03b1 = T\u22121/(2(1+\u2113)), \u03b2 = \u03b3 = T\u22121/(4(1+\u2113)) has regret bound O\u0303 ( T 1\u2212 1 2(1+\u2113) \u2113(K +H2\u2113+1) )\n, and running time O(T min{min{H2\u2113T \u2113, 2H 2T}, 2H2T}).\nProof. The proof follows the same logic as that of Theorem 2. We denote RevExp3() as the revenue obtained from Exp3 algorithm described above on the set of outcome menus of the discretization procedure. Similar to the proof of Theorem 2, in what follows n denotes the number of menus resulting from the discretization procedure in Section 3.1. vi is the valuation of the buyer at step i, and v\u0304 is the sequence of valuation of all buyers in rounds 1 through T . RevM \u2032() is the maximum revenue obtained in the set of menus resulting from the discretization procedure and OPT() is the optimal revenue.\nn = (H/\u03b1)2\u2113,\nRevExp3 (v\u0304) \u2265 Rev(M \u2032) (v\u0304)\u2212 ( \u03b3 + \u03b2\n2\n)\nRev(M \u2032) (v\u0304)\u2212 Hn lnn \u03b2\u03b3 ,\nRevM \u2032 (v\u0304) =\nT \u2211\ni=1\nRevM \u2032 (vi) ,\nRevM \u2032 (vi) \u2265 OPT (vi)\u2212 2K\u2113\u03b1;\nwhere the first expression is a result of Theorem 1, the second expression uses Proposition 29, the third expands the revenue over T terms, and the last uses Theorem 1. Rearranging the terms gives:\nRevM \u2032 (v\u0304) \u2265 OPT (v\u0304)\u2212 2K\u2113\u03b1T\nRevExp3 (v\u0304) \u2265 OPT (v\u0304)\u2212 2K\u2113\u03b1T \u2212 ( \u03b3 + \u03b2\n2\n)\nHT \u2212 Hn lnn \u03b2\u03b3\nRevExp3 (v\u0304) \u2265 OPT (v\u0304)\u2212 2K\u2113\u03b1T \u2212 ( \u03b3 + \u03b2\n2\n)\nHT \u2212 2H(H/\u03b1) 2\u2113\u2113 (lnH \u2212 ln\u03b1) \u03b2\u03b3\nWe set variables \u03b1 and \u03b2 as a function of T to minimize the exponent of T in the regret. By\nsetting \u03b1 = T\u22121/(2(1+\u2113)), \u03b2 = \u03b3 = T\u22121/(4(1+\u2113)), the regret isO ( T 1\u2212 1 2(1+\u2113) ln (T )\u2113(K +H2\u2113+1 lnH) ) .\nThe algorithm involves maintaining weights for all the menus in the discretized set at each time step, therefore the running time at each time step is proportional to the number of the menus that is derived based on parameter \u03b1."
        },
        {
            "heading": "A.2.2 Online Learning Under Smooth Distributions",
            "text": "Smoothed Distributional Assumptions. In an online setting under smoothed distributions, the algorithm receives samples S \u223c DT , where D is an arbitrary distribution over problem instances \u03a0 (which in our case is the buyer valuations). The goal is to find \u03c1\u0302 that nearly maximizes \u2211\nv\u2208S u(v,\u03c1). In this setting, the goal is to find a value \u03c1 that is nearly optimal in hindsight over a stream v1, . . . , vT of instances, or equivalently, over a stream u1 = u(v1, \u00b7), . . . , uT = u(vT , \u00b7) of functions. Each vt is drawn from a distribution D(t), which may be adversarial. Therefore, {v1, . . . , vT} \u223c D(1) \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 D(T ).\nDispersion. Let u1, . . . , uT be a set of functions mapping a set C \u2286 Rd to [0, H ]. In this paper, we study the mechanism selection setting, given a collection of problem instances v1, . . . , vT \u2208 \u03a0 and a utility function u : \u03a0 \u00d7 C \u2192 [0, H ], each function ui(\u00b7) might equal\nthe function u(vi, \u00b7), measuring a mechanism\u2019s performance on a fixed problem instance as a function of its parameters. Informally, dispersion is a constraint on the functions u1, . . . , uT that guarantees although each function ui may have discontinuities, they do not concentrate in a small region of space. We study two definitions of dispersion previously introduced in algorithm and mechanism selection problems. We show that menus of two-part tariffs satisfy both definitions; (k, w)-dispersion (Definition 32) and \u03b2-dispersion (Definition 35). Then, we use the first to establish online learning results for full-information and bandit settings and the second for the semi-bandit setting.\nIn order to prove menus of two-part tariffs satisfy dispersion under smoothed assumptions, we show this family of mechanisms satisfies certain structural properties. Balcan et al. [2018c] show in two-part tariff menus, for each function ui, the parameter space C is partitioned into sets P1, . . . ,Pn such that ui is L-Lipschitz on each piece, but ui may have discontinuities at the boundaries between pieces.\u2021 We refine this structural property and show that multi-sets of parallel hyperplanes, corresponding to the stream of buyer valuations, partition the parameter space C into convex polytopes with bounded-degree linear utility functions inside each polytope. Later, we show this property is sufficient for proving dispersion and employing the related algorithms.\nLemma 30. Consider the sequence of buyer valuations v arrived until time t. For menus of two-part tariffs, the parameter space C is partitioned into convex polytopes, P1, . . . ,Pn by multisets of parallel hyperplanes, such that the utility function at each time step inside each region Pj is a linear function satisfying (K + 1)-Lipschitz continuity.\nProof. Part of the proof that identifies the regions with linear utilities has been shown previously in Balcan et al. [2018c], Lemma 3.15. We reiterate that part for completeness and also prove the extra structural properties. Consider the set of menus for which the buyer with valuation v(i) arriving at time i selects the tariff index j and the number of units k. The buyer selects this option for menu \u03c1 if it produces more utility for the buyer than any other option. Formally,\nv(i)(k)\u2212 I{k \u2265 1} (\np (j) 1 (\u03c1) + kp (j) 2 (\u03c1)\n) \u2265 v(i)(k\u2032)\u2212 I{k\u2032 \u2265 1} (\np (j\u2032) 1 (\u03c1) + k \u2032p (j\u2032) 2 (\u03c1)\n)\n. \u2200j\u2032, k\u2032\n(2)\nThe above inequalities identify a convex polytope of parameter vectors (menus \u03c1) with hyperplane boundaries. Considering all the possible selections (j, k) (the tariff index and the number of units), the parameter space for v(i) is partitioned into convex polytopes where inside each polytope the payment of v(i) is linear; i.e., I{k \u2265 1} (\np (j) 1 (\u03c1) + kp (j) 2 (\u03c1)\n)\n.\nConsidering the same analysis for all the buyers\u2019 valuations in the sequence, for each buyer, the parameter space is partitioned into convex polytopes where inside each polytope, the revenue function is linear and (K + 1)-Lipschitz. Since convex polytopes are closed under intersection, superimposing the partitions for i = 1, . . . , t results in polytopes with the properties in the statement.\n\u2021This previously-known structural result suffices for the techniques used in the setting with the limited number of buyers (Section 3.2.3 and appendix A.2.3); however, we need a refined statement for proving dispersion.\nFor a fixed valuation vector v(i), the discontinuities in the utility function are defined\nby at most \u21132K2 hyperplanes: v(i)(k) \u2212 I{k \u2265 1} (\np (j) 1 (\u03c1) + kp (j) 2 (\u03c1)\n)\n= v(i)(k\u2032) \u2212 I{k\u2032 \u2265\n1} (\np (j\u2032) 1 (\u03c1) + k \u2032p (j\u2032) 2 (\u03c1)\n)\n. Let \u03a8v be the multi-set union of all these hyperplanes. Consider\na set S = { v(1), . . . , v(t) }\nwith corresponding multi-sets \u03a8v(1), . . . ,\u03a8v(t) of hyperplanes. We now partition the multi-set union of \u03a8v(1), . . . ,\u03a8v(t) into at most \u2113\n2K2 multi-sets Bj,k,j\u2032,k\u2032 for all j, j\u2032 \u2208 [\u2113] and k, k\u2032 \u2208 [K] and i \u2208 [t] such that for each Bj,k,j\u2032,k\u2032, the hyperplanes in Bj,k,j\u2032,k\u2032 are parallel with probability 1 over the draw of S. To this end, define a single multi-set Bj,k,j\u2032,k\u2032 to consist of the hyperplanes\n{v(1) (k)\u2212 I{k \u2265 1} (\np (j) 1 (\u03c1) + kp (j) 2 (\u03c1)\n) = v(1) (k\u2032)\u2212 I{k\u2032 \u2265 1} (\np (j\u2032) 1 (\u03c1) + k \u2032p (j\u2032) 2 (\u03c1)\n)\n,\nv(2) (k)\u2212 I{k \u2265 1} (\np (j) 1 (\u03c1) + kp (j) 2 (\u03c1)\n) = v(2) (k\u2032)\u2212 I{k\u2032 \u2265 1} (\np (j\u2032) 1 (\u03c1) + k \u2032p (j\u2032) 2 (\u03c1)\n)\n,\n. . . , v(t) (k)\u2212 I{k \u2265 1} (\np (j) 1 (\u03c1) + kp (j) 2 (\u03c1)\n) = v(t) (k\u2032)\u2212 I{k\u2032 \u2265 1} (\np (j\u2032) 1 (\u03c1) + k \u2032p (j\u2032) 2 (\u03c1)\n)\n};\nwhere the only variables are coordinates of \u03c1. The hyperplanes inside each multi-set are parallel and the utility of the regions defined by the hyperplanes are linear and K + 1- Lipschitz.\u00a7\nNext, we establish an upper bound on the number of regions with continuous (linear) regions.\nLemma 31. The partitioning of the parameter space for menus of two-part tariffs explained in Lemma 30 after T rounds results in O((T +1)\u2113\n2K2) regions, with linear cumulative utility function inside each region.\nProof. Lemma 30 identifies multi-sets Bj,k,j\u2032,k\u2032 of size T for each j, k, j\u2032, k\u2032 such that the hyperplanes inside the multi-sets are parallel. Therefore, each multi-set divides the parameter space into T + 1 parts. Thus, each region with continuous utility can be defined as the intersection at most \u21132K2 parts, where each part corresponds to a distinct multi-set. This results in at most O((T + 1)\u2113 2K2) such regions.\nIn order to prove dispersion, we need to use an assumption on the distributions called \u03ba-boundedness.\nDefinition 4. [\u03ba-bounded] A density function f : R\u2192 R corresponds to a \u03ba-bounded distribution if max{f(x)} \u2264 \u03ba.\nWe first provide the definition of (w, k)-dispersion. Recall that \u03a0 is a set of instances, C \u2282 Rd is a parameter space, and u is an abstract utility function. We use the l2 distance and let B(\u03c1, r) = {\u03c1\u2032 \u2208 Rd : \u2016\u03c1\u2212 \u03c1\u2032\u20162 \u2264 r} denote a ball of radius r centered at \u03c1. We use this notion of dispersion to derive our full-information and bandit setting results.\n\u00a7Partitioning of the parameter space by parallel multisets of hyperplanes has been established before for other families of mechanism design such as posted pricing [Balcan et al., 2018b]. We extend this idea to the more complicated case of two-part tariffs.\nDefinition 32 ([Balcan et al., 2018b], (w, k)-dispersion). Let u1, . . . , uT : C \u2192 [0, H ] be a collection of functions where ui is piecewise Lipschitz over a partition Pi of C. We say that Pi splits a set A if A intersects with at least two sets in Pi. The collection of functions is (w, k)-dispersed if every ball of radius w is split by at most k of the partitions P1, . . . ,PT . More generally, the functions are (w, k)-dispersed at a maximizer if there exists a point \u03c1\u2217 \u2208 argmax\u03c1\u2208C \u2211T i=1 ui(\u03c1) such that the ball B(\u03c1\n\u2217, w) is split by at most k of the partitions P1, . . . ,PT .\nWe now prove menus of two-part tariffs satisfy (w, k) dispersion, and use it to derive no-regret online learning results for full-information and bandit settings.\nProposition 33. Suppose that u(v,\u03c1) is the revenue of the two-part tariff menu mechanism with prices \u03c1 and buyer\u2019s values v. With probability at least 1 \u2212 \u03b6 over the draw S \u223c D(1) \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 D(T ) for any \u03b1 \u2265 1/2 the following statement holds:\nSuppose v(k) \u2208 [0, H ] for any number of units k \u2208 [K]. Also, suppose that for each distribution D(t), and every pair of number of units k and k\u2032, v(k) and v(k\u2032) have a \u03babounded joint distribution. Then u is\n(\n1\n2H\u03baT 1\u2212\u03b1 , O\n(\n\u21132K2T \u03b1\n\u221a\nln \u2113K\n\u03b6\n))\n-dispersed\nwith respect to S.\nProof. Lemma 30 gives multisets of parallel hyperplanes that partition the parameter space into regions withK+1-Lipschitz continuous utility functions. Since the samples are drawn independently from \u03ba-bounded distributions with support [0, H ], the offsets of the hyperplanes in each multiset Bj,k,j\u2032,k\u2032 are independent random variables with H\u03ba-bounded distributions. Furthermore, the number of multisets is at most \u21132K2. Using these properties, Theorem 32 of Balcan et al. [2018b] gives the statement.\nAfter establishing dispersion and showing that the parameter space is partitioned into convex regions with cumulative linear utility inside each region, the no-regret guarantees and their performances are implied by prior results.\nFull Information For completeness we include previously established algorithms for the full information setting, under dispersion condition, adapted to our setting.\nOverview of Algorithms 5 and 6, related to Theorem 5. Algorithm 5 [Balcan et al., 2018b] is an efficient algorithm for online learning in the full-information setting under smoothed distributional assumptions that uses Algorithm 6 [Balcan et al., 2018b] as a subroutine. The algorithm considers the cumulative revenue function up until the time t \u2212 1 over the parameter space,\n\u2211t\u22121 0 us, and samples the menu to be presented at time t approx-\nimately proportional to an exponential function of its cumulative revenue, i.e., eg(\u03c1t), where g = \u03bb\n\u2211t\u22121 0 us. In order to have an efficient implementation for sampling menu \u03c1t approx-\nimately from distribution \u00b5 with density f\u00b5(\u03c1) \u221d eg(\u03c1t), techniques from high-dimensional\nAlgorithm 5: Full-information two-part tariffs under smoothed distributional assumptions (Adapted to two-part tariffs from [Balcan et al., 2018b], Algorithm 4)\nInput: \u03bb \u2208 (0, 1/H ], \u03b7, \u03b6 \u2208 (0, 1). 1: Set u0(\u00b7) = 0 (to be the constant 0 function over C). 2: for buyer t = 1, 2, . . . , T do\nPresent menu \u03c1t obtained from Algorithm 6 with g = \u03bb \u2211t\u22121\ns=0 us, approximation parameter \u03b7/4, and confidence parameter \u03b6/T to the buyer. (Menu \u03c1t is sampled with probability that is approximately proportional to eg(\u03c1t).); Observe the revenue for all the potential menus as function ut(\u00b7). Receive payment ut(\u03c1t) = I{k \u2265 1}(p(i)1 (\u03c1t) + kp (i) 2 (\u03c1t)), where i and k are the tariff index and the number of units chosen by buyer t respectively given menu \u03c1t.\nAlgorithm 6: Multi-dimensional sampling algorithm ([Balcan et al., 2018b], Algorithm 2)\nInput: Function g, partition with regions P1, . . . ,Pn, approximation parameter \u03b7, confidence parameter \u03b6 .\n1: Define \u03b1 = \u03b2 = \u03b7/3. 2: Let h(\u03c1) = exp(g(\u03c1)) and hi(\u03c1) = I{\u03c1 \u2208 Pi}h(\u03c1) be h restricted to Pi. 3: For each i \u2208 [n], let Z\u0302i = Aintegrate(hi, \u03b1, \u03b6/(2n)). 4: Choose random partition index I = i with probability Z\u0302i/ \u2211\nj Z\u0302j. 5: Let \u03c1\u0302 be the sample output by Asample(hI , \u03b2, \u03b6/2).\nOutput: \u03c1\ngeometry are used in Algorithm 6. This algorithm is used when g is piecewise concave (in our case, linear), and each piece is a convex set (in our case, convex polytopes where each buyer already in the sequence selects a fixed tariff index and the number of units) as shown in Lemma 30. Let P1, . . . ,Pn be the partition of C until time t. The algorithm first picks Pi with probability proportional to the integral of f\u00b5 on that region and then outputs a sample from the conditional distribution of menus in Pi. The algorithm assumes access to two procedures for approximate integration and sampling, namely Aintegrate(h, \u03b1, \u03b6) and Asample(h, \u03b2, \u03b6). Aintegrate(hi, \u03b1, \u03b6) is a polynomial running-time procedure that takes the approximate integral of any logconcave function hi restricted to region Pi with accuracy parameter \u03b1 and failure probability \u03b6 . Asample(hi, \u03b2, \u03b6) is a polynomial procedure that approximately samples a menu with probability distribution according to hi in the region Pi with accuracy parameter \u03b2 and failure probability \u03b6 .\nDefinition 34 (Aintegrate(h, \u03b1, \u03b6) and Asample(h, \u03b2, \u03b6) [Balcan et al., 2018b]). For any logconcave function h : Rd \u2192 R, any accuracy parameter \u03b1 > 0, and any failure probability \u03b6 > 0, Aintegrate(h, \u03b1, \u03b6) outputs a number Z that with probability at least 1 \u2212 \u03b6 satisfies e\u2212\u03b1 \u222b h \u2264 Z \u2264 e\u03b1 \u222b\nh. For any logconcave function h : Rd \u2192 R, any accuracy parameter \u03b2 > 0, and any failure probability \u03b6 > 0, Asample(h, \u03b2, \u03b6) outputs a sample X drawn from a distribution u\u0302h that with probability at least 1 \u2212 \u03b6, D\u221e(\u00b5, \u00b5\u0302) \u2264 \u03b2, where D\u221e(\u00b5, \u00b5\u0302) is the relative (multiplicative) distance between probability measures \u00b5 and \u00b5\u0302. Formally, D\u221e(\u00b5, \u00b5\u0302) = sup\u03c1 | log d\u00b5d\u00b5\u0302 |, where d\u00b5 d\u00b5\u0302 denotes the Radon-Nikodym derivative.\nSimilar to [Balcan et al., 2018b], we use the implementation ofAintegrate by Lova\u0301sz and Vempala [2006] and Asample by Bassily et al. [2014], Algorithm 6. These implementations satisfy the conditions in Definition 34. The first runs in time poly(d, 1\n\u03b1 , log 1 \u03b6 , log R r ), where the domain\nof function h is a subset of a ball of radius R and its level set of probability mass 1/8 is a superset of a ball with radius r. The second succeeds with probability 1 and runs in time poly(d, L, 1\n\u03b2 , log R r ).\nTheorem 5. Let u1, . . . , uT : C \u2192 [0, H ] be the revenue functions of two-part tariff menus such that ut(\u03c1) denotes the revenue of a mechanism associated with menu parameters \u03c1 for the buyer arriving at time t. Let the samples of buyers\u2019 values be drawn from S \u223c D(1)\u00d7\u00b7 \u00b7 \u00b7\u00d7D(T ). Suppose v(k) \u2208 [0, H ] for any number of units k \u2208 [K]. Also, suppose that for each distribution D(t), and every pair of number of units k and k\u2032, v(k) and v(k\u2032) have a \u03ba-bounded joint distribution. An efficient implementation of the exponentially weighted forecaster with \u03bb = \u221a 2\u2113 ln(2H2\u03ba \u221a T )/T/H (Algorithm 5) has expected regret bounded by O\u0303((H\u21132K2 \u221a log \u03ba+ 1/(H\u03ba)) \u221a T ) and runs in time O\u0303((T + 1)poly(\u2113,K)poly(\u2113, \u221a T ) +KT \u221a T ).\nProof. Proposition 33 determines the dispersion for two-part tariff menus with probability 1\u2212\u03b6 . Theorem 1 in Balcan et al. [2018b] relates dispersion to a regret bound for full information online learning algorithms. It states if a sequence of piecewise L-Lipschitz functions in d dimensions is (w, k)-dispersed, there is an exponentially weighted forecaster with expected regret O(H( \u221a\nTd logR/w + k) + TLw). Since dispersion holds with probability 1 \u2212 \u03b6 , the final regret bound is O((1\u2212 \u03b6)(H( \u221a Td logR/w + k) + TLw)) + \u03b6H . Substituting w and k\nby dispersion found in Proposition 33 gives:\nO\n(\nH\n(\n\u221a\n2T\u2113 log(2H2\u03baT 1\u2212\u03b1) + \u21132K2T \u03b1\n\u221a\nln \u2113K\n\u03b6\n)\n+ T \u03b1\n2H\u03ba + \u03b6HT\n)\n.\nFor all rounds, t \u2208 [T ], the sum of utilities is linear over at most (T + 1)\u21132K2 pieces, and all the pieces are convex. In this case, we may use Algorithm 6 as a subroutine to Algorithm 5 for a more efficient but approximate implementation. Setting dispersion parameters \u03b6 = 1/ \u221a T and \u03b1 = 0.5 and approximation parameters \u03b7 = \u03b6 = 1/ \u221a T and using Theorem 1 in Balcan et al. [2018b], gives the statement\u2019s regret bound and running time."
        },
        {
            "heading": "Bandit Setting",
            "text": "The bandit-setting algorithm considers a grid over the parameter space, whose granularity depends on the dispersion parameters, and runs the Exp3 algorithm over menus corresponding to the grid.\nTheorem 6. Let u1, . . . , uT : C \u2192 [0, H ] be the revenue functions of two-part tariff menus such that ut(\u03c1) denotes the revenue of a mechanism associated with menu parameters \u03c1 for the buyer arriving at time t. Let the samples of buyers\u2019 values be drawn from S \u223c D(1) \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 D(T ). Suppose v(k) \u2208 [0, H ] for any number of units k \u2208 [K]. Also, suppose that for each distribution D(t), and every pair of number of units k and k\u2032, v(k) and v(k\u2032) have a \u03ba-bounded joint distribution. There is a bandit-feedback online optimization algorithm with expected regret O\u0303 ( T (2\u2113+1)/(2\u2113+2) ( H2K \u221a \u2113\u03bad/2 \u221a log \u03ba ) + 1/H\u03ba +H\u21132K2 ) . The per-round\nrunning time is O(H4\u2113\u03ba2\u2113T \u2113).\nProof. Proposition 36 determines dispersion for two-part tariff menus with probability 1\u2212\u03b6 . Theorem 3 in Balcan et al. [2018b] relates dispersion to a regret bound for the bandit setting. It states if a sequence of piecewise L-Lipschitz functions that are (w, k)-dispersed and when the parameter space is contained in a ball of radius R, running Exp3 algorithm has regret\nO\n\nH\n\u221a\nTd\n(\n3R\nw\n)d\nlog R\nw + TLw +Hk\n\n .\nThe per-round running time is O((3R/w)d). Note that dispersion holds only with probability 1 \u2212 \u03b6 and with probability \u03b6 , regret is bounded by HT . In our case, L = K + 1, R = H and d = 2\u2113. Substituting these terms along with w and k, and setting \u03b1 = 2\u2113+1/2\u2113+2 and \u03b6 = 1/ \u221a T gives the regret bound and running time in the theorem statement.\nSemi-Bandit Setting For the semi-bandit setting, we need to invoke a more recent definition of dispersion.\nDefinition 35 ([Balcan et al., 2020a], \u03b2-point-dispersion). The sequence of loss functions l1, l2, . . . is \u03b2-point-dispersed for the Lipschitz constant L if for all T and for all \u03b5 \u2265 T\u2212\u03b2, we have that, in expectation, the maximum number of functions among l1, . . . , lT that fail the LLipschitz condition for any pair of points at distance \u03b5 in C is at most O\u0303(\u03b5T ). That is, for all\nT and for all \u03b5 \u2265 T\u2212\u03b2, we have E [ max\u03c1,\u03c1\u2032 \u2223 \u2223{t \u2208 [T ] : |lt(\u03c1)\u2212 lt(\u03c1\u2032)| > L\u2016\u03c1\u2212\u03c1\u2032\u20162} \u2223 \u2223 ]\n= O\u0303(\u03b5T ). where the max is taken over all \u03c1, \u03c1\u2032 \u2208 C : \u2016\u03c1\u2212 \u03c1\u2032\u20162 \u2264 \u03b5.\nProposition 36. Suppose lt(\u03c1) = H \u2212 ut(\u03c1), where ut(\u03c1) is the revenue of the two-part tariff menu mechanism with prices \u03c1 and buyer\u2019s values vt at time t, where buyers\u2019 values are drawn from D(1) \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 D(T ). If D(i) are \u03ba-bounded, where \u03ba = o\u0303(T ), and K and \u2113, the maximum number of units and the number of tariffs, are polynomial in T , these loss functions are \u03b2-point-dispersed for \u03b2 = 1/2.\nProof. We use the following statement from Balcan and Sharma [2021], theorem 7.\nProposition 37. [Balcan and Sharma, 2021] Let l1, . . . , lT : R d \u2192 R be independent piecewise L-Lipschitz functions, each having discontinuities specified by a collection of at most K \u2032 algebraic hypersurfaces of bounded degree. Let P denote the set of axis-aligned paths between pairs of points in Rd, and for each s \u2208 P define D(T, s) = |{1 \u2264 t \u2264 T | lt has a discontinuity along s}|. Then we have E[sups\u2208P D(T, s)] \u2264 sups\u2208P E[D(T, s)] + O( \u221a T log(TK \u2032)).\nThe number of hyperplanes, defined as K \u2032 in the theorem, is at most T\u21132K2 and lts are piecewise (K + 1)-Lipschitz function (by Lemma 51); where T is the number of buyers (rounds), \u2113 is the number of tariffs, and K is the maximum number of units. Note that, as shown in Lemma 30. The independence of lts comes from the assumptions of this setting, where the buyer valuations for each round are drawn independently.\nDefinition 35 counts the number of times (in T time intervals) that the difference in utility of the pair violates the L-Lipschitz condition, and finds the worst pair for this property. Proposition 37, counts the number of times that in an axis-aligned path, the utility function has discontinuities. Therefore, sups\u2208P E[D(T, s)] + O( \u221a\nT log(TK \u2032)) is an upper bound on E [ max\u03c1,\u03c1\u2032 \u2223 \u2223{t \u2208 [T ] : |ut(\u03c1)\u2212 ut(\u03c1\u2032)| > L\u2016\u03c1\u2212 \u03c1\u2032\u20162} \u2223 \u2223 ]\n. To find the dispersion we need to find sups\u2208P E[D(T, s)].\nRecall from the proof of Proposition 33 that the discontinuities can be partitioned into \u21132K2 multisets of parallel hyperplanes, such that multiset Bj,k,j\u2032,k\u2032 corresponds to pairs of tariffs and the number of units (j, k) and (j\u2032, k\u2032). In addition, since we assume the buyers\u2019 valuations are in the range [0, H ] and are drawn from pairwise \u03ba-bounded joint distributions, the offsets of the hyperplanes are independent draws from a H\u03ba-bounded distribution. The number of multi-sets is \u21132K2, and the size of each multi-set is T . The hyperplanes within each multi-set are well-dispersed. For a multi-set Bj,k,j\u2032,k\u2032, let \u0398j,k,j\u2032,k\u2032 be the multi-set of the hyperplanes\u2019 offsets. By assumption, the elements of \u0398j,k,j\u2032,k\u2032 are independently drawn from H\u03ba-bounded distributions. Since the offsets are H\u03ba-bounded, the probability that it falls in any interval of length \u03b5 is O(H\u03ba\u03b5). The expected number of hyperplanes crossed from each multiset in distance \u03b5 along each axis is at most H\u03ba\u03b5|Bj,k,j\u2032,k\u2032|, and since there are 2\u2113 dimensions, the total expected number of crossings is 2\u2113H\u03ba\u03b5|Bj,k,j\u2032,k\u2032|. Using the upper bound on |Bj,k,j\u2032,k\u2032|, in total, for any pair of points at distance \u03b5, sups\u2208P E[D(T, s)] = O(\u21133K2H\u03ba\u03b5T ). By Proposition 37, E[sups\u2208P D(T, s)] \u2264 sups\u2208P E[D(T, s)] +O( \u221a T log(TK\u2113)), which in our case is upper bounded by: O(\u21133K2H\u03ba\u03b5T + \u221a T log(TK\u2113)). For \u03ba = o\u0303(T ), K = O(poly(T ))\nand \u2113 = O(poly(T )), E[sups\u2208P D(T, s)] = O\u0303(\u03b5T ). Therefore, these loss functions are \u03b2-point dispersed for \u03b2 = 1/2, satisfying the statement.\nOverview of Algorithm 7 The generic algorithm for the semi-bandit case was previously developed in Balcan et al. [2020a]. We adapt it to our setting and consider an efficient implementation using the approximate integration and sampling from Balcan et al. [2018b] discussed in Definition 34. The semi-bandit-setting algorithm is a continuous version of the Exp3-SET algorithm of Alon et al. [2017]. At each time step, the algorithm learns the revenue function (only) inside the region P(t) \u220b \u03c1t that the presented menu belongs to and updates the menu weights for the next round accordingly.\nAlgorithm 7: Semi-bandit two-part tariff under smoothed distributional assumptions (Adapted from [Balcan et al., 2020a], Algorithm 1 for two-part tariffs)\nInput: Step size \u03bb \u2208 [0, 1] 1: Let w1(\u03c1) = 1 for all \u03c1 \u2208 C 2: for buyer t = 1, . . . , T do\nLet pt(\u03c1) = wt(\u03c1) Wt , where Wt = \u222b\nC wt(\u03c1) d\u03c1; Sample \u03c1t from pt, present it to buyer t, observe the tariff index j and the\nnumber of units k selected by the buyer and region P(t) for which the buyer takes this action; the revenue inside P(t) is ut(\u03c1) = I{k \u2265 1}(p(i)1 (\u03c1) + kp (i) 2 (\u03c1)) and the normalized loss is lt(\u03c1) = H\u2212ut(\u03c1) H for all \u03c1 \u2208 P(t);\nLet l\u0302t(\u03c1) = I{\u03c1\u2208P(t)} pt(P(t)) lt(\u03c1), where we define pt(P (t)) = \u222b P(t) pt(\u03c1) d\u03c1; Let wt+1(\u03c1) = wt(\u03c1) exp(\u2212\u03bbl\u0302t(\u03c1)) for all \u03c1.\nTheorem 7. Suppose the buyers\u2019 values are drawn from D(1)\u00d7\u00b7 \u00b7 \u00b7\u00d7D(T ), where each D(t) is \u03ba-bounded for \u03ba = o\u0303(T ). Then, running the continuous Exp3-SET algorithm (Algorithm 7) for menus of two-part tariffs under semi-bandit feedback has expected regret bounded by O\u0303(H \u221a \u2113T ). An efficient implementation has the same regret bound and running time O\u0303((T +\n1)poly(\u2113,K)poly(\u2113, \u221a T ) +KT \u221a T ).\nProof. For the regret bound, we invoke Theorem 2 of Balcan et al. [2020a], stating that if the loss functions are Lipschitz functions satisfying \u03b2-point-dispersion, running Algorithm 7 has expected regret bounded by O\u0303( \u221a dT + T 1\u2212\u03b2), when the loss function is in [0, 1]. In our case, d, the number of dimensions is 2\u2113, the dispersion parameter \u03b2 = 1/2, and the loss function is in [0, H ]. This implies the regret bound.\nNow, we discuss the running time of the algorithm. At each time t, using the buyer\u2019s valuation vector, the tariff j and the number of units k selected by the buyer, we can determine the region P(t), where the buyer makes the same selection and whose utility function is linear by solving a linear program (the inequalities in Equation (2)). This computation is done in time poly(\u2113,K). Next, for the integration procedures inside the algorithm, we use the approximate version introduced in Definition 34 and for sampling, we use the efficient implementation demonstrated in Algorithm 6. In particular, we consider \u03b7 = \u03b6 = 1/(3 \u221a T ). For \u222b\nC wt(\u03c1) d\u03c1, we use lines 1 through 3 of Algorithm 6 and take the sum of the integra-\ntion outcomes of line 3, for \u03b7\u2032 = \u03b7/4 and \u03b6 \u2032 = \u03b6/T . For pt(P(t)) = \u222b\nP(t) pt(\u03c1) d\u03c1 we do the same, except that now we do the integration operations in line 3 only for the regions inside P(t). For sampling \u03c1t from pt, we use the complete procedure Algorithm 6 that takes\nthe regions with linear cumulative utility, \u03bb = \u221a 2\u2113 ln(2H2\u03ba \u221a T )/T/H , g = \u03bb\n\u2211t\u22121 s=0 us and\n\u03b7 = \u03b6 = 1/(3 \u221a T ). Note that since the loss is only updated for P(t), for any regions outside this part, we do not need to repeat the integration operations in Algorithm 6. This may result in potentially better running time for semi-bandit compared to full-information; however, we do not quantify the improvement. Using union bound, with probability at least 1 \u2212 1/ \u221a T , all the approximate integration and sampling operations performed in the algorithm succeed and the density function of the approximate distribution used for sampling is always within (1 \u2212 \u03b7) fraction of the exact distribution. Using these parameters together with Theorem 1 in [Balcan et al., 2018b] conclude that the same regret bound is achievable from the approximate operations and give the running time in the statement."
        },
        {
            "heading": "A.2.3 Limited Buyer Types",
            "text": "We reiterate the results of partitioning the parameter space into convex regions with linear cumulative utility functions where the statements are adapted to the limited buyer type setting and corresponding notations.\nLemma 38. [Adapted from Balcan et al. [2018c], Lemma 3.15] For each feasible mapping \u00b5, as defined in Definition 9, P\u00b5 is a convex polytope with hyperplane boundaries.\nProof. For a fixed buyer type i and option (j, k), let P(i)(j,k) be the set of all parameter vectors \u03c1 corresponding to the length-\u2113 menus that buyer type i selects option (j, k). The buyer selects option (j, k) for menu \u03c1 if this option produces more utility for the buyer than any other option. Formally,\nvi(k)\u2212 I{k \u2265 1} ( p (j) 1 (\u03c1) + kp (j) 2 (\u03c1) ) \u2265 vi(k\u2032)\u2212 I{k\u2032 \u2265 1} ( p (j\u2032) 1 (\u03c1) + k \u2032p (j\u2032) 2 (\u03c1) ) . \u2200j\u2032, k\u2032\nThe above inequalities identify a convex polytope of parameter vectors (menus \u03c1) with hyperplane boundaries. P\u00b5 is the intersection of P(i)\u00b5(i) for i = 1, . . . , V . Therefore, P\u00b5 is also a convex region with hyperplane boundaries.\nLemma 39. [Adapted from Balcan et al. [2018c], Lemma 3.15] For each feasible mapping \u00b5 and any sequence of buyer valuations b the cumulative utility, \u2211\ni u(bi,\u03c1), is linear in P\u00b5. Proof. We show that for any buyer valuation vi in the sequence, u(vi, \u03c1) is linear in the region. Proving this claim is sufficient for concluding the statement. Let (j, k) = \u00b5(vi), i.e., j is the tariff index and k is number of units that buyer valuation vi selects under \u00b5. Therefore, the utility for this buyer for menu \u03c1 \u2208 P\u00b5 is vi(k)\u2212I{k \u2265 1} ( p (j) 1 (\u03c1) + kp (j) 2 (\u03c1) ) . Both p (j) 1 (\u03c1) and p (j) 2 (\u03c1) grow linearly as a function of \u03c1. Therefore, since the option that each buyer valuation selects (the tariff index and the number of units) is fixed inside P\u00b5, the utility is also linear.\nFull Information Setting\nTheorem 13. In the full information case for length-\u2113 menus of two-part tariffs, when there are V types of buyers, running Algorithm 2 over the set of menus corresponding to set E for \u03b2 = 1/ \u221a T has regret bounded by O\u0303(H\u2113 \u221a T ln(V \u2113K)).\nProof. We run the weighted majority algorithm Algorithm 2 with parameter \u03b2 = 1/ \u221a T on the set E as the set of menus (experts). The proof directly follows from Lemma 12 and Proposition 28. Let n = |E|. Let bi be the valuation of the buyer at step i, and b\u0304 be the vector of valuation of all buyers in rounds 1 through T . We denote RevE() as the maximum revenue obtained in the set of E , OPT() as the optimal revenue, and RevWM() as the revenue obtained from Algorithm 2 on the set of experts X = E . Then,\nn \u2264 (V \u21132K2/4)2\u2113,\nRevWM ( b\u0304 ) \u2265 Rev(E) ( b\u0304 ) \u2212 \u03b2 2 Rev(E) ( b\u0304 ) \u2212 H lnn \u03b2 ,\nRevE ( b\u0304 ) =\nT \u2211\ni=1\nRevE (bi) ,\nRevE (bi) \u2265 OPT (bi)\u2212 2K\u03b5;\nwhere the first expression uses the size of E in Lemma 11, the second expression uses Proposition 28, the third expands the revenue over T terms, and the last uses Lemma 12. Rearranging the terms, we have:\nRevE (bi) \u2265 OPT (bi)\u2212 2K\u03b5 RevE ( b\u0304 ) \u2265 OPT ( b\u0304 ) \u2212 2K\u03b5T\nRevWM ( b\u0304 ) \u2265 OPT ( b\u0304 ) \u2212 2K\u03b5T \u2212 \u03b2HT 2 \u2212 H lnn \u03b2\nRevWM ( b\u0304 ) \u2265 OPT ( b\u0304 ) \u2212 2K\u03b5T \u2212 \u03b2HT 2 \u2212 2\u2113H (ln (V \u2113K)) \u03b2\nWe set variables \u03b5 and \u03b2 to minimize the exponent of T in the regret. By setting \u03b2 = 1\u221a T\nand \u03b5 = 1/(K \u221a T ), The regret will be O(H\u2113 \u221a T ln (V \u2113K)).\nPartial Information Setting We first show how to estimate the utility of any menu by only using the response of the buyer to a limited number of menus. In doing so, we take advantage of the interdependence of the buyers\u2019 responses for different menus to obtain estimates for unused menus. In particular, using barycentric spanner concept from [Awerbuch and Kleinberg, 2008], we devise a basis for the menus such that observing buyers\u2019 responses to them is sufficient for estimating the revenue of other menus.\nLet I be a set of length-V indicator vectors, such that for each feasible mapping \u00b5 and option to select (j, k), which is the tariff index and the number of units, there is a vector in I . This vector indicates the (maximal) set of buyer types that select this option in mapping \u00b5. As an example, if in mapping \u00b5, {v2, v3} is the exact set of valuation types that select the same option (j, k), vector (0, 1, 1, 0, . . .) belongs to I . For I \u2208 I , \u00b5I and (j, k)I denote the corresponding mapping and option to I, respectively. Similarly, I\u00b5,(j,k) is the vector in I , corresponding to mapping \u00b5 and option (j, k). Using principles from linear algebra, since the vectors are V -dimensional, there is a set of at most V vectors in I such that any other vector in I is a linear combination of the vectors in this set. Awerbuch and Kleinberg make this property stronger and show that there is a set of V vectors in I , called the barycentric\nspanner or spanner for short, we denote it by S, such that any member of I can be written as a linear combination of vectors in S with coefficients in [\u22121, 1].\nLemma 40. There exists set S in I such that, for all I \u2208 I , there exists coefficients \u03bb1, . . . , \u03bbV \u2208 [\u22121, 1], so that I = \u2211V j=1 \u03bbisj.\nProof. The statement is a direct corollary of [Awerbuch and Kleinberg, 2008] Proposition 2.2.\nHere is the main idea on how to find estimates for the utility of all the menus by only presenting the menus corresponding to the spanner S to the buyers. First, similar to Balcan et al. [2015], we define function f\u03c4 (\u00b7) for the vectors in I that will be instrumental in computing the utility for all the menus based on the spanner. Recall that each vector I in I corresponds to a mapping \u00b5I and an option (j, k)I . Let f\u03c4 (I) be the number of times during a time block \u03c4 that given a menu in P\u00b5 the arriving buyer selects option (j, k). First, we show how the quantity of this function on inputs from the spanner is sufficient for finding the revenue of arbitrary menus and then show how to estimate it.\nLemma 41. For each menu \u03c1 and any time block \u03c4 : t + 1, . . . , t + \u03c4\u2113, let u\u03c4 (\u03c1) represent the average utility of \u03c1 for buyer types in \u03c4 . Then,\nu\u03c4 (\u03c1) = 1\n\u2113\u03c4\n\u2211\n(j,k)\u2208O I{k \u2265 1}\n(\np (j) 1 (\u03c1) + kp (j) 2 (\u03c1)\n) V \u2211\ni=1\n\u03bbi(I\u00b5\u03c1,(j,k))f\u03c4 (si)\nProof. By definition, u\u03c4 (\u03c1) is the average utility of menu \u03c1 for buyers arriving in \u03c4 . Menu \u03c1, corresponds to a feasible mapping \u00b5\u03c1. By definition, the buyers in time block \u03c4 select option (j, k) equal to f\u03c4 (I\u00b5\u03c1,(j,k)) number of times. By Lemma 40, I\u00b5\u03c1,(j,k) can be written as a linear combination of the vectors in the spanner. Furthermore, f\u03c4 (.) is a linear function as it is equivalent to the dot product of a vector indicating the frequency, i.e., the number of arrivals, of each buyer type during \u03c4 and the function input. Therefore,\nu\u03c4(\u03c1) = 1\n\u2113\u03c4\n\u2211\n(j,k)\u2208O I{k \u2265 1}\n(\np (j) 1 (\u03c1) + kp (j) 2 (\u03c1)\n)\nf\u03c4 (I\u00b5\u03c1,(j,k))\n= 1\n\u2113\u03c4\n\u2211\n(j,k)\u2208O I{k \u2265 1}\n(\np (j) 1 (\u03c1) + kp (j) 2 (\u03c1)\n) V \u2211\ni=1\n\u03bbi(I\u00b5\u03c1,(j,k))f\u03c4 (si).\nLet f\u0302\u03c4 (si) be the estimator to f\u03c4 (si)/\u2113\u03c4 for the spanner vectors. Let \u00b5si be the corresponding mapping to si. Recall that f\u03c4 (si) is the number of times during \u03c4 that given a menu in P\u00b5si , the arriving buyer, selects option (j, k)si . In order to estimate this quantity we present a corresponding menu to si, i.e., a menu in P\u00b5si , once uniformly at random during the time block \u03c4 . If the buyer selects option (j, k)si , we let f\u0302\u03c4 (si) equal to 1 and otherwise set it to 0. The next lemma shows that f\u0302\u03c4 (si) has the same expected value and has range [0, 1]. Intuitively, the reason is that due to uniform random selection of the time step, the estimator has the same expected value.\nLemma 42 (Adapted from Balcan et al. [2015] Lemma 6.3). For any s \u2208 S, E[f\u0302\u03c4 (s)]\u2113\u03c4 = f\u03c4 (s).\nProof. Note that f\u0302\u03c4 (s) = 1 if and only if at the time step that menu \u03c1s was presented, (j, k)s was selected. Since \u03c1s is presented once uniformly at random over the time steps and is independent of the sequence of buyers, the buyer presented with \u03c1s is also picked uniformly at random over the time steps. Therefore, E[f\u0302\u03c4 (s)] is the probability that a randomly chosen buyer from time block \u03c4 selects (j, k)s.\nNow, we prove that the expected value of the utility estimator for each menu is equal to the utility of that manu, i.e., the estimator is an unbiased, and moreover, has a bounded range. The utility estimator is defined as follows, where f\u03c4 (si)/\u2113\u03c4 in the utility formula is replaced by its estimator f\u0302\u03c4 (si).\nu\u0302\u03c4 (\u03c1) = \u2211\n(j,k)\u2208O I{k \u2265 1}\n(\np (j) 1 (\u03c1) + kp (j) 2 (\u03c1)\n) V \u2211\ni=1\n\u03bbi(I\u00b5\u03c1,(j,k))f\u0302\u03c4 (si)\nLemma 43. For any menu \u03c1, E[u\u0302\u03c4 (\u03c1)] = u\u03c4 (\u03c1) and u\u0302\u03c4(\u03c1) \u2208 [\u2212\u2113KV H, \u2113KV H ].\nProof. The proof of the equality of the expectation simply follows from u\u0302\u03c4 (\u03c1) and u\u03c4(\u03c1) definitions and Lemma 42. Now, we prove the range of the estimator. Since S is a barycentric spanner, for any I \u2208 I , \u03bbi(I) \u2208 [\u22121, 1]. Also, f\u0302\u03c4 (.) belongs to {0, 1}. Also, the utility of the buyer selecting each option in the menu, e.g., p\n(j) 1 (\u03c1)+kp (j) 2 (\u03c1), is always in [0, H ]. Therefore,\nusing the formula of the estimator, it is bounded by H times the number of options times the number of buyer types.\nWe use the algorithm below along with the weighted majority algorithm in the fullinformation (similar to Algorithm 2) that uses the utility (revenue) estimates. We use E as the set of experts (menus) and obtain distribution q over set E as the weight vector.\nOverview of Algorithm 8 First, we provide a high-level structure of the algorithm and then discuss the details. The algorithm operates in time blocks, with each block consisting of exploitation and exploration time steps. The exploration time steps are selected uniformly at random within the block and are limited in number. In an exploitation step, the menu used is the output of the full information algorithm, employing the utility estimators from the previous time block. These menus are always the extreme points of the continuity regions, as discussed at the beginning of the section. During exploration time steps, the corresponding menu to a vector in the spanner is used. At the end of each time block, the algorithm refines the unbiased estimators of the utility of all extreme points using the information gathered in the exploration phases.\nZ is the number of time blocks, with each time block consisting of T/Z time steps. The algorithm uniformly at random picks time steps t1, . . . , tV and their permutation \u03c0 in the current time block. Whenever the time step is equal to ti, the algorithm runs an exploration step; otherwise, the algorithm runs an exploitation step. In the exploration step at time step ti, a menu corresponding to si, \u03c1s\u03c0(i) , is presented to the arriving buyer and the estimator f\u0302\u03c4 (s\u03c0(i)) will be assigned as 1 if the buyer selects (j, k)s\u03c0(i) and will be assigned as\nAlgorithm 8: Partial-Information Algorithm for Limited Buyer Types (adapted from [Balcan et al., 2015] Algorithm 1)\nInput: V : the number of buyer types, O : the set of menu options (|O| = \u2113(K + 1)) 1: Z \u2190 (T 2|O|2V log(|O|V ))1/3 \u22b2 the number of time blocks 2: Create set I = {I\u00b5,(j,k)| for all options (j, k) and feasible mappings \u00b5} such that the ith\ncomponent of I\u00b5,(j,k) is 1 iff vi selects (j, k) in \u00b5 and is 0 otherwise. 3: Find a barycentric spanner S = {s1, ..., sV } for I . For every s \u2208 S, let \u00b5s be the\ncorresponding mapping, (j, k)s, the corresponding option, and \u03c1s a menu in P\u00b5s . 4: for all I \u2208 I do\nlet \u03bb(I) be the representation of I in spanner S. That is \u2211V\ni=1 \u03bbi(I)si = I. 5: Let q1 be the uniform distribution over E . \u22b2 initial weight vector over menus in E 6: for \u03c4 = 1, ..., Z do \u22b2 time blocks\nChoose a random permutation \u03c0 over [V ] and t1, . . . , tV from [T/Z].; for t = (\u03c4 \u2212 1)(T/Z) + 1, ..., \u03c4(T/Z), do \u22b2 time steps in a time block\nif t = ti for some i \u2208 [V ], then \u22b2 exploration time step \u03c1t \u2190 \u03c1s\u03c0(j) ; If (j, k)s\u03c0(j) is selected, then f\u0302\u03c4 (s\u03c0(j))\u2190 1, otherwise f\u0302\u03c4 (s\u03c0(j))\u2190 0; else \u22b2 exploitation time step draw \u03c1t at random from distribution q\u03c4 ;\nfor all \u03c1 \u2208 E , for \u00b5 such that \u03c1 \u2208 P\u00b5, do u\u0302\u03c4(\u03c1) = \u2211 (j,k)\u2208O I{k \u2265 1} ( p (j) 1 (\u03c1) + kp (j) 2 (\u03c1) ) \u2211V i=1 \u03bbi(I\u00b5\u03c1,(j,k))f\u0302\u03c4 (si).; Call Algorithm 2 for experts E and (u\u0302\u03c4) as their revenue function; And receive q\u03c4+1 as a distribution over all mixed strategies in E .\n0, otherwise. At the end of the time block, we update the estimates of the revenue of the menus corresponding to the extreme points.\nLemma 44. [[Balcan et al., 2015] Lemma 6.2] Let M be the set of all actions. For any time block (set of consecutive time steps) T \u2032 and action j \u2208 M , let cT \u2032(j) be the average loss of action j over T \u2032. Assume that S \u2286 M is such that by sampling all actions in S, we can compute c\u0302T \u2032(j) for all j \u2208 M with the following properties: E[c\u0302T \u2032(j)] = cT \u2032(j) and c\u0302T \u2032(j) \u2208 [\u2212\u03ba, \u03ba]. Then there is an algorithm with loss Lalg \u2264 Lmin+O ( T 2 3 |S| 13\u03ba 13 log 13 (|M |) ) ,\nwhere Lmin is the loss of the best action in hindsight.\nWe are now ready to prove the main result of this section.\nTheorem 14. In the partial information (bandit) case for length-\u2113 menus of two-part tariffs, when there are V different types of buyers, there is an algorithm with regret bound of O\u0303(T 2/3\u2113(HKV )1/3 log1/3(V \u2113K)).\nProof. In Lemma 44, |S| is the number of dimensions (barycentric spanner set), \u03ba is the maximum revenue times the number of buyer types times the number of their options (entries in the menu), |M | is the number of discrete points. In our case, |S| = 2\u2113, \u03ba = H\u2113KV , and |M | \u2264 (V \u21132K2/4)2\u2113. By Lemma 43, the expected value of the estimated utility is equal to the exact value of utility with range [\u2212H\u2113KV,H\u2113KV ].\nUsing Lemma 44, the regret for menus of two-part tariffs is bounded by\nO(T 2/3\u21131/3(H\u2113KV )1/3\u21131/3 log1/3(V \u2113K)) \u2208 O(T 2/3\u2113(HKV )1/3 log1/3(V \u2113K)).\nThe following quantifies the regret of simply running the Exp3 algorithm on the set of extreme points.\nProposition 45. In the partial information case for length-\u2113 menus of two-part tariffs when there are V buyer types, running Algorithm 3 over menus corresponding to E for \u03b2 = \u03b3 = T\u22121/3 has regret bound O ( T 2/3\u2113H(V \u21132K2/4)2\u2113 ln (V \u2113K) ) .\nProof. The proof is similar to that of Theorem 6. We denote RevExp3() as the revenue obtained from Exp3 algorithm as presented in Algorithm 3 on the set of menus corresponding to E . Let n denote the number of such menus. bi is the valuation of the buyer at step i, and b\u0304 is the sequence of valuation of all buyers in rounds 1 through T . RevE() is the maximum revenue obtained in the set E and OPT() is the optimal revenue.\nn \u2264 (V \u21132K2/4)2\u2113,\nRevExp3 ( b\u0304 ) \u2265 Rev(E) ( b\u0304 ) \u2212 ( \u03b3 + \u03b2\n2\n)\nRev(E) ( b\u0304 ) \u2212 Hn lnn \u03b2\u03b3 ,\nRevE ( b\u0304 ) = T \u2211\ni=1\nRevE (bi) ,\nRevE (bi) \u2265 OPT (bi)\u2212 2K\u03b5;\nwhere the first expression uses the size of E in Lemma 11, the second expression uses Proposition 29, the third expands the revenue over T terms, and the last uses Lemma 12.Rearranging the terms, we have:\nRevE (bi) \u2265 OPT (bi)\u2212 2K\u03b5 RevE ( b\u0304 ) \u2265 OPT ( b\u0304 ) \u2212 2K\u03b5T\nRevExp3 ( b\u0304 ) \u2265 OPT ( b\u0304 ) \u2212 2K\u03b5T \u2212 ( \u03b3 + \u03b2\n2\n)\nHT \u2212 Hn lnn \u03b2\u03b3\nRevExp3 ( b\u0304 ) \u2265 OPT ( b\u0304 ) \u2212 2K\u03b5T \u2212 ( \u03b3 + \u03b2\n2\n)\nHT \u2212 2\u2113H(V \u2113 2K2/4)2\u2113 (ln (V \u2113K))\n\u03b2\u03b3\nWe set variables \u03b5 in E and \u03b2 = \u03b3 as a function of T to minimize the exponent of T in the regret. By setting \u03b2 = \u03b3 = T\u22121/3 and \u03b5 = T\u22121/2, the regret isO ( T 2/3\u2113H(V \u21132K2/4)2\u2113 ln (V \u2113K) ) .\nRemark. The standard technique for the partial information algorithm of running the Exp3 algorithm on the extreme points leads to a regret bound that is exponential in the size of the menu as stated in Proposition 45; however, Algorithm 8 has regret bound polynomial in the size of the menus. Therefore, the new technique results in a significant improvement."
        },
        {
            "heading": "A.3 Distributional Learning",
            "text": "Theorem 15. In the distributional setting, for length-\u2113 menus of two-part tariffs, there exists a learning algorithm with sample complexity H 2\n2\u03b52 (2\u2113 ln (2KH\u2113 \u03b5 ) + ln (2/\u03b4)), and running time\nH2 2\u03b52 ( 2\u2113 ln ( 2KH\u2113 \u03b5 ) + ln (2/\u03b4) ) K\u2113 ( 2HK\u2113 \u03b5 )2\u2113 .\nProof. We need to find the number of samples such that with probability 1\u2212\u03b4, the difference between the expected revenue of our algorithm and the optimal revenue is at most \u03b5. Note that since our algorithm uses discretization of possible menus, we face two types of errors: the discretization error, and the usual empirical error in a PAC learning setting. We find the sample complexity and discretization parameters such that the total error is bounded by \u03b5.\nThe possible number of menus after discretization using parameter N is computed by the following formula.\n|H| = (H/\u03b1)2\u2113.\nUsing uniform convergence in the PAC learning setting, the sample complexity for empirical error \u03b5\u2032 is as follows.\n|S| \u2265 H 2\n2\u03b5\u20322 (ln |H|+ ln (2/\u03b4)) .\nReplacing lnH we have,\n|S| \u2265 H 2\n2\u03b5\u20322 (2\u2113 ln (H/\u03b1) + ln (2/\u03b4)) .\nAlso, the revenue loss compared to the optimum for arbitrary buyer i with valuation vi is:\nRevM \u2032 (vi) \u2265 OPT (vi)\u2212 2K\u2113\u03b1.\nThe total error (from discretization and empirical error), when the empirical error is set to \u03b5\u2032, is\n2K\u2113\u03b1 + \u03b5\u2032.\nBy setting 2K\u2113\u03b1 = \u03b5\u2032, we have\n\u03b1 = \u03b5\u2032\n2K\u2113 ,\nReplacing \u03b1 gives the following sample complexity:\n|S| \u2265 H 2\n2\u03b5\u20322 (2\u2113 ln (H/\u03b1) + ln (2/\u03b4))\n\u2265 H 2\n2\u03b5\u20322 (2\u2113 ln (2K\u2113H/\u03b5\u2032) + ln (2/\u03b4))\nwhich by replacing \u03b5\u2032 with \u03b5/2 results in \u03b5 total error. The computational complexity of finding the empirical optimal menu for |S| buyers and menu of size \u2113 is:\nO(|S|K\u2113|H|) = |S|K\u2113 ( 2HK\u2113\n\u03b5\n)2\u2113\n.\nThis implies the efficiency of the algorithm.\nLemma 46. The running time of distributional learning algorithm for two-part tariffs in [Balcan et al., 2020b] is at least\n(\nc\n(\nH\n\u03b5\n)2(\n18\u2113 log (82K2\u21133) + log 1\n\u03b4\n)\n)2\u2113+1\nK4\u2113+2(2\u2113)2+1/18.\nProof. The algorithm involves computingN2\u2113K4\u2113 regions, whereN is c(H/\u03b5)2(18\u2113 log (8K2\u21133)+ log 1\n\u03b4 ), and solving a linear program for each region with 2\u2113 variables and NK2 constraints,\nwhich takes O\u0303((2\u2113)2+1/18NK2).\nComparison with previous results. The sample complexity using the pseudo-dimension method of [Balcan et al., 2018c] is O(H2/\u03b52(\u2113 log (K\u2113)+ log (1/\u03b4))) and the best previouslyknown running time [Balcan et al., 2022a] is O ( R2(2\u2113)2\u2113+1KH2/\u03b52(\u2113 log (K\u2113) + log (1/\u03b4)) )\n, where R the number of discontinuity regions is bounded by O([H2/\u03b52(\u2113 log (K\u2113)+log (1/\u03b4))]3K), resulting in the worst case running time ofO ( (H2/\u03b52(\u2113 log (K\u2113) + log (1/\u03b4))) 2\u2113+1 K4\u2113+2(2\u2113)2+1/18 )\ndue to [Balcan et al., 2020b, 2022a] (See Lemma 46)."
        },
        {
            "heading": "B Missing Proofs of Section 4",
            "text": ""
        },
        {
            "heading": "B.1 Missing Proofs for the Discretization Procedure",
            "text": "Before providing the proof of the discretization step, we note that this procedure for menus of lotteries needs extra care and the common rounding of the parameters may result in arbitrarily lower revenue. For example, if there are two lotteries with a similar utility for\nthe buyer but a large difference in prices, minor changes in the probability of allocations or the prices may make the user switch from the high-price lottery to the low-price one. What follows is a concrete example of why standard rounding procedures fail.\nExample 1. Consider a menu of three lotteries.\nalloc. prob. price utility 0 0 0\n0.26 0.24 -0.084 0.95 0.52 0.05\nalloc. prob. price utility 0 0 0\n0.25 0.125 0.025 0.5 0.5 -0.2\nalloc. prob. price utility 0 0 0 0.5 0.125 0.175 1 0.5 0.1\nalloc. prob. price utility 0 0 0 0.5 0.25 0.05 1 1 -0.4\nConsider the buyer that has value 0.6 for the item. The first table shows the original menu. With this menu the buyer\u2019s highest utility option is the last lottery that causes the highest revenue, i.e., Rev = 0.52. The following tables show the new menus after rounding down the allocation probabilities and prices, rounding up allocation probabilities and rounding down prices, and rounding up allocation probabilities and prices (all to powers of 1/2), respectively. All these transformations result in the highest utility lottery changing to the middle lottery which causes smaller revenue.\nTheorem 16. Given a menu of lotteries M and parameters 0 < \u03b1 < 1, 0 < \u03b4 < 1, and K, an arbitrary natural number, Algorithm 4 outputs menu M \u2032 such that Rev(M \u2032) \u2265 Rev(M)(1 \u2212 \u03b4)(1 \u2212 \u03b1)K \u2212 (2K + 1)\u03b1 \u2212 mH(1 \u2212 \u03b4)K. The set of possible allocation probabilities is {0, (1 \u2212 \u03b1)K \u2032, (1 \u2212 \u03b1)K \u2032\u22121, . . . (1 \u2212 \u03b1)0 = 1}, where K \u2032 = \u230a1/\u03b1 ln (Hm/\u03b1)\u230b and the set of possible prices is {0, Hm\u03b1, 2Hm\u03b1, . . .Hm}. This constitutes a space with at most O ( (1/\u03b1\u2113m+\u2113) (ln (Hm/\u03b1))lm ) discrete points, when limiting to length-\u2113 menus and O ( 2(1/\u03b1 m+1)(ln (Hm/\u03b1))m ) discrete points for arbitrary-length menus.\nProof. Most of this proof is identical to that of Dughmi et al. [2014]. Note that in the algorithm, the original entries in a menu are divided into levels k = 1, . . . , K such that k = 1 is the lowest-price level and k = K is the highest price one. First, we show that if a buyer\u2019s utility-maximizing lottery is in level k given M , their utility-maximizing lottery in M \u2032 is never in a lower-price level k\u2032 < k. Intuitively, the reason is that the lotteries with lower-level prices have their allocation reduced more and their prices reduced less than the ones in higher levels. More formally, let (x, p) be at level k and (y, q) at level k\u2032 < k. Also, let (x\u2032, p\u2032) and (y\u2032, q\u2032) be the transformed lotteries in the output of the algorithm. Than, p\u2032 \u2212 q\u2032 < ((1\u2212 \u03b1)Kp\u2212 2k\u03b1)\u2212 ((1\u2212 \u03b1)Kq \u2212 2k\u2032\u03b1\u2212 \u03b1) \u2264 (1\u2212 \u03b1)K(p\u2212 q)\u2212 \u03b1, and for every valuation v, x\u2032\u00b7v\u2212y\u2032\u00b7v > ((1\u2212\u03b1)K\u2212k+1x\u00b7v\u2212\u03b1)\u2212(1\u2212\u03b1)K\u2212k\u2032y\u00b7v \u2265 (1\u2212\u03b1)K(x\u00b7v\u2212y\u00b7v)\u2212\u03b1. Now, consider an arbitrary valuation v that has higher utility choosing (x, p) than y, q. Therefore x \u00b7 v \u2212 p \u2265 y \u00b7 v \u2212 q, and therefore p\u2212 q \u2264 x \u00b7 v \u2212 y \u00b7 v. Combining this inequality with the ones above implies x\u2032 \u00b7 v \u2212 p\u2032 \u2265 y\u2032 \u00b7 v \u2212 q\u2032.\nSecondly, we compute an upper bound on the loss incurred. Suppose the original utilitymaximizing lottery was (x, p) in M . Also, suppose in M \u2032, the utility-maximizing lottery is\n(y\u2032, q\u2032) which is the transformation of (y, q). The first scenario is when p \u2265 mH(1 \u2212 \u03b4)K . Note that in this case, q may be smaller by a factor (1 \u2212 \u03b4) than p, then to obtain q\u2032 we first lost a multiplicative factor of (1 \u2212 \u03b1)K and then an additive factor of at most (2K + 1)\u03b1 (including the rounding). Thus q\u2032 \u2265 (1 \u2212 \u03b4)(1 \u2212 \u03b1)Kp \u2212 (2K + 1)\u03b1. In the second case where p < mH(1\u2212 \u03b4)K , the loss is at most mH(1\u2212 \u03b4)K . Therefore, in any case, q\u2032 \u2265 (1\u2212 \u03b4)(1\u2212 \u03b1)Kp\u2212 (2K + 1)\u03b1\u2212mH(1\u2212 \u03b4)K .\nThirdly, the set of possible prices is {0, Hm\u03b1, 2Hm\u03b1, . . .Hm} which is of size 1/\u03b1 and the set of possible allocation probabilities is {0, (1\u2212 \u03b1)K \u2032, (1\u2212 \u03b1)K \u2032\u22121, . . . (1\u2212 \u03b1)0 = 1}, for K \u2032 = \u230a1/\u03b1 ln (Hm/\u03b1)\u230b which is of size 1/\u03b1 ln(Hm/\u03b1). In the \u2113-length menus, there are \u2113 prices and m\u2113 allocation probabilities in total. In the unlimited-length menus, we consider the possibility that each potential lottery (each distinct vector of parameters) belongs to the lottery or not. This analysis gives us the final size of the discrete points."
        },
        {
            "heading": "B.2 Online Learning",
            "text": "Similar to the section on two-part tariffs, using the outcome of the discretization summarized in Theorem 16, we show a reduction to a finite number of experts and run standard learning algorithms (weighted majority and Exp3) over the menus in the discretized set."
        },
        {
            "heading": "B.2.1 Full Information",
            "text": "In the full information setting, the seller sees the revenue generated for all the possible menus. To design an online algorithm in this case, we use a variant of the weighted majority algorithm by [Auer et al., 1995]. The experts in our case are the discretized menus from the previous section, denoted in the algorithm by set X = m1, . . . , mn. Furthermore, vt is the valuation of the buyer are time t and Revk(v1, . . . , vt) is the cumulative revenue of menu mk for the buyers until time step t.\nSimilar to two-part tariffs, we use Algorithm 2 for the full information case. The only difference is that since the maximum revenue in lotteries is mH , as opposed to two-part tariffs where it is H , in the algorithm we need to replace H with mH .\nProposition 47 ([Auer et al., 1995], Theorem 3.2). For any sequence of valuations v\u0304,\nRevWM (v\u0304) \u2265 ( 1\u2212 \u03b2 2 ) OPTX (v\u0304)\u2212 mH lnn \u03b2 ,\nwhere X = m1, . . . , mn are the set of experts (lottery menus), RevWM(v\u0304) is the expected revenue outcome of Algorithm 2 where H is replaced with mH, and OPTX (v\u0304) is the revenue of the optimal menu in X.\nTheorem 17. In the full information case for length-\u2113 menus of lotteries, running Algorithm 2 over the discretized set of menus specified in Theorem 16 for \u03b1 = T\u22121, \u03b2 = T\u22120.5, K = T 0.5, and \u03b4 = T\u22120.5 has regret O\u0303(m2H\u2113 \u221a T ).\nProof. Let n be the number of menus resulting from Algorithm 4. Let vi be the valuation of the buyer at step i, and v\u0304 be the vector of valuation of all buyers in rounds 1 through T .\nWe denote RevM \u2032() as the maximum revenue obtained in the set of menus resulting from Algorithm 4, OPT() as the optimal revenue, and RevWM() as the revenue obtained from the weighted majority algorithm discussed above on the set of outcome menus of Algorithm 4. We have\nn = (1/\u03b1\u2113m+\u2113) (ln (Hm/\u03b1))lm ,\nRevWM (v\u0304) \u2265 RevM \u2032 (v\u0304)\u2212 \u03b2 2 Rev(M \u2032) (v\u0304)\u2212 mH lnn \u03b2 ,\nRevM \u2032 (v\u0304) =\nT \u2211\ni=1\nRevM \u2032 (vi) ,\nRevM \u2032 (vi) \u2265 OPT (vi) (1\u2212 \u03b4)(1\u2212 \u03b1)K \u2212 (2K + 1)\u03b1\u2212mH(1\u2212 \u03b4)K ; where the first expression is a result of Algorithm 4, the second expression uses Proposition 47, the third expands the revenue over T terms, and the last uses Theorem 16. Rearranging the terms, we have:\nRevM \u2032 (vi) \u2265 OPT (vi) (1\u2212 \u03b4)(1\u2212 \u03b1)K \u2212 (2K + 1)\u03b1\u2212mH(1\u2212 \u03b4)K\n\u2265 OPT (vi)\u2212OPT (vi) ( 1\u2212 (1\u2212 \u03b4)(1\u2212 \u03b1)K ) \u2212 (2K + 1)\u03b1\u2212mH(1\u2212 \u03b4)K \u2265 OPT (vi)\u2212mH ( 1\u2212 (1\u2212 \u03b4)(1\u2212 \u03b1)K ) \u2212 (2K + 1)\u03b1\u2212mH(1\u2212 \u03b4)K\nRevM \u2032 (v\u0304) \u2265 OPT (v\u0304)\u2212mHT ( 1\u2212 (1\u2212 \u03b4)(1\u2212 \u03b1)K ) \u2212 T (2K + 1)\u03b1\u2212mHT (1\u2212 \u03b4)K\nRevWM (v\u0304) \u2265 OPT (v\u0304)\u2212mHT ( 1\u2212 (1\u2212 \u03b4)(1\u2212 \u03b1)K ) \u2212 T (2K + 1)\u03b1\n\u2212mHT (1\u2212 \u03b4)K \u2212 \u03b2mHT 2 \u2212 mH lnn \u03b2\nWe set variables K, \u03b1, \u03b4, and \u03b2 as a function of T to minimize the exponent of T in the regret. The regret is upper bounded by\nmHT ( 1\u2212 (1\u2212 \u03b4)(1\u2212 \u03b1)K ) + T (2K + 1)\u03b1 +mHT (1\u2212 \u03b4)K + \u03b2mHT 2 + mH lnn \u03b2 ,\n\u2264mHT ( 1\u2212 (1\u2212 \u03b4)(1\u2212 \u03b1)K ) + T (2K + 1)\u03b1 +mHT (1\u2212 \u03b4)K + \u03b2mHT 2 + mHO (\u2113m ln (Hm/\u03b1)) \u03b2 ;\nwhere the inequality follows by upper bounding n. By setting \u03b1 = T\u22121, \u03b2 = T\u22120.5, K = T 0.5, and \u03b4 = T\u22120.5 the regret is bounded by O\u0303(m2H\u2113 \u221a T ).\nTheorem 18. In the full information case for arbitrary length menus of lotteries, running Algorithm 2 on menus specified in Theorem 16 for \u03b1 = T\u22121/(2m+2), \u03b2 = T\u22121/(m+1), K = T 1/(m+1), and \u03b4 = T\u22121/(m+1) has regret O\u0303(mHT 1\u22121/(2m+4) lnm (mHT )).\nProof. The proof follows the same argument as Theorem 17. The only difference in the parameters is n, the number of experts, which in this case is n = 2(1/\u03b1\nm+1)(ln (Hm/\u03b1))m . We set variables K, \u03b1, \u03b4, and \u03b2 as a function of T to minimize the exponent of T in the regret. The regret is upper bounded by the formula below after substituting n\nmHT ( 1\u2212 (1\u2212 \u03b4)(1\u2212 \u03b1)K ) + T (2K + 1)\u03b1+mHT (1\u2212 \u03b4)K\n+ \u03b2mHT\n2 +\nmH(1/\u03b1m+1)(ln (Hm/\u03b1))mln2\n\u03b2\nBy setting \u03b1 = T\u22121/(2m+2), \u03b2 = T\u22121/(m+1), K = T 1/(m+1), and \u03b4 = T\u22121/(m+1), the regret is bounded by O\u0303(mHT 1\u22121/(2m+4) lnm (mHT ))."
        },
        {
            "heading": "B.2.2 Bandit Setting",
            "text": "In the partial information setting, the seller does not see the outcome for all the possible menus and only observes the outcome of the menu used (the lottery chosen by the buyer). Similar to the two-part tariffs results, to design an online algorithm in this case, we use a version of the Exp3 algorithm in [Auer et al., 1995]. This variant of the Exp3 algorithm contains the weighted majority algorithm (Algorithm 2) a subroutine. At each step, we mix the probability distribution \u03c0, used by the weighted majority algorithm, with the uniform distribution to obtain a modified probability distribution \u03c0, which is then used to select a menu from our discretized set. Following the lottery chosen by buyer t, we use the price paid (the gain from the chosen menu) to formulate a simulated gain vector, which is then used to update the weights maintained by the weighted majority algorithm.\nSimilar to two-part tariffs, we use Algorithm 3 for the bandit case. The only difference is that since the maximum revenue in lotteries is mH , as opposed to two-part tariffs where it is H , in the algorithm we need to replace H with mH .\nProposition 48 ([Auer et al., 1995], Theorem 4.1). For any sequence of valuations v\u0304,\nRevExp3 (v\u0304) \u2265 OPTX \u2212 ( \u03b3 + \u03b2\n2\n)\nOPTX \u2212 mHn lnn\n\u03b2\u03b3 ,\nwhere X = m1, . . . , mn are the set of experts (lottery menus), RevExp3(v\u0304) is the expected revenue outcome of Algorithm 3 where H is replaced with mH, and OPTX (v\u0304) is the revenue of the optimal menu in X.\nTheorem 19. In the partial information case for length-\u2113 menus of lotteries, running Algorithm 3 over discretized set of menus in Theorem 16 for \u03b1 = T\u22121/(\u2113m+2), \u03b2 = \u03b3 = T\u22121/(4\u2113m+8), K = T 1/(2\u2113m+4), and \u03b4 = T\u22121/(2\u2113m+4) has regret O\u0303(m2H\u2113T 1\u22121/(2\u2113m+4) ln\u2113m+1 (mHT )).\nProof. The proof follows the same logic as that of Theorem 17. We denote RevExp3() as the revenue obtained from Exp3 algorithm described above on the set of outcome menus of Algorithm 4. Similar to the proof of Theorem 17, in what follows n denotes the number of menus resulted from the procedure Algorithm 4. vi is the valuation of the buyer at step i, and v\u0304 is the vector of valuation of all buyers in rounds 1 through T . RevM \u2032() is the maximum revenue obtained in the set of menus resulted from Algorithm 4 and OPT() as the optimal revenue.\nn = (1/\u03b1\u2113m+\u2113) (ln (Hm/\u03b1))lm ,\nRevExp3 (v\u0304) \u2265 RevM \u2032 \u2212 ( \u03b3 + \u03b2\n2\n)\nRevM \u2032 \u2212 mHn lnn\n\u03b2\u03b3 ,\nRevM \u2032 (v\u0304) =\nT \u2211\ni=1\nRevM \u2032 (vi) ,\nRevM \u2032 (vi) \u2265 OPT (vi) (1\u2212 \u03b4)(1\u2212 \u03b1)K \u2212 (2K + 1)\u03b1\u2212mH(1\u2212 \u03b4)K ;\nwhere the first expression is a result of Algorithm 4, the second expression uses Proposition 48, the third expands the revenue over T terms, and the last uses Theorem 16. Rearranging the terms, we have:\nRevExp3 (v\u0304) \u2265 RevM \u2032 (v\u0304)\u2212 ( \u03b3 + \u03b2\n2\n)\nRevM \u2032 (v\u0304)\u2212 mHn lnn\n\u03b2\u03b3\n\u2265 RevM \u2032 (v\u0304)\u2212 ( \u03b3 + \u03b2\n2\n)\nmHT \u2212 mHn lnn \u03b2\u03b3\n\u2265 OPT (v\u0304)\u2212mHT ( 1\u2212 (1\u2212 \u03b4)(1\u2212 \u03b1)K ) \u2212 T (2K + 1)\u03b1\u2212mHT (1\u2212 \u03b4)K\n\u2212 ( \u03b3 + \u03b2\n2\n)\nmHT \u2212 mHn lnn \u03b2\u03b3\nWe set variables K, \u03b1, \u03b4, \u03b2, and \u03b3 as a function of T to minimize the exponent of T in the regret. After substituting n, the regret is upper bounded by\nmHT ( 1\u2212 (1\u2212 \u03b4)(1\u2212 \u03b1)K ) + T (2K + 1)\u03b1+mHT (1\u2212 \u03b4)K + ( \u03b3 + \u03b2\n2\n)\nmHT\n+ 2\u2113m2H(1/\u03b1\u2113m+\u2113) (ln (Hm/\u03b1))\u2113m+1\n\u03b2\u03b3\nBy setting \u03b1 = T\u22121/(\u2113m+2), \u03b2 = \u03b3 = T\u22121/(4\u2113m+8), K = T 1/(2\u2113m+4), and \u03b4 = T\u22121/(2\u2113m+4), the regret is bounded by O\u0303(m2H\u2113T 1\u22121/(2\u2113m+4) ln\u2113m+1 (mHT ))."
        },
        {
            "heading": "B.3 Limited Buyer Types",
            "text": "The ideas for designing a specific algorithm specific to the limited buyer types in the menus of lotteries are similar to those for menus of two-part tariffs. There are a few changes that we overview here.\nOne of the main differences is the menu options O. Unlike two-part tariffs that given a menu, the buyer needed to select a tariff and number of units that maximized the buyer\u2019s utility; for menus of lotteries, the options are exactly aligned with menu entries, and |O| = \u2113+1 for length-\u2113 lotteries. The mechanism designer\u2019s utility (revenue) given menu \u03c1 is equal to p(j)(\u03c1) if the buyer selects entry j. The buyer selects entry j, if this entry results in higher utility than any other entry in menu \u03c1. These inequalities identify regions P\u00b5, where the buyer\u2019s utility maximizing option is aligned with \u00b5.\nDefinition 49 (menu option for menus of lotteries, O). Index j such that 0 \u2264 j \u2264 \u2113 indicating a lottery index in the menu is a menu option. We denote the set of all menu options as O. This set identifies all potential actions of a buyer when presented with a menu.\nDefinition 50 (mapping \u00b5, feasible mappings, P\u00b5). A mapping \u00b5 is a function from buyer types, v1, . . . , vV to menu options j = 0, 1, . . . , \u2113, where j is the lottery index assigned to the buyer type. Mapping \u00b5 is feasible if there is a menu corresponding to the mapping, i.e.,\na menu that if presented to the buyers, each buyer selects their corresponding option in the mapping as their utility maximizing option. P\u00b5 denotes the region of the parameter space corresponding to \u00b5, i.e., the set of menus inducing mapping \u00b5.\nLemma 51. For each feasible mapping \u00b5, as defined in Definition 50, P\u00b5 is a convex polytope with hyperplane boundaries. Proof. For a fixed buyer type i and option j = 0, . . . , \u2113, let P(i)j be the set of all parameter vectors \u03c1 corresponding to the length-\u2113 menus that buyer type i selects option j. The buyer selects option j for menu \u03c1 if this option produces more utility for the buyer than any other option. Formally,\nm \u2211\nk=1\nv(ek)\u03c6 (j)[k](\u03c1)\u2212 p(j)(\u03c1) \u2265\nm \u2211\nk=1\nv(ek)\u03c6 (j\u2032)[k](\u03c1)\u2212 p(j\u2032)(\u03c1); \u2200j\u2032.\nThe above inequalities identify a convex polytope of parameter vectors (menus \u03c1) with hyperplane boundaries. P\u00b5 is the intersection of P(i)\u00b5(i) for i = 1, . . . , V . Therefore, P\u00b5 is also a convex region with hyperplane boundaries.\nLemma 52. For each feasible mapping \u00b5 and any sequence of buyer valuations b the cumulative utility, \u2211\ni u(bi,\u03c1), is linear in P\u00b5. Proof. We show that for any buyer valuation vi in the sequence, u(vi, \u03c1) is linear in the region. Proving this claim is sufficient for concluding the statement. Let j = \u00b5(vi), i.e., j is the lottery index that buyer valuation vi selects under \u00b5. Therefore, the utility for this buyer for menu \u03c1 \u2208 P\u00b5 is \u2211m k=1 v(ek)\u03c6\n(j)[k](\u03c1) \u2212 p(j)(\u03c1). Note that \u03c6(j)[k](\u03c1) is a coordinate of \u03c1 and therefore, has a linear dependence on \u03c1. Therefore, since the option that each buyer valuation selects is fixed inside P\u00b5, the utility is also linear. Lemma 53. The number of extreme points for menus of lotteries, |E|, is at most (V \u21132)m(\u2113+1). Proof. Length-\u2113menus of lotteries occupy a \u2113(m+1)-dimensional parameter space. In each ddimensional space, an extreme point is the intersection of d linearly independent hyperplanes. The total number of hyperplanes defining the regions is H = V (\n\u2113 2\n)\n, where for each buyer type compares the utility of two menu entries. Out of these hyperplanes, we need \u2113(m+ 1) of them to intersect for an extreme point. Therefore, the number of extreme points is at most\n( H \u2113(m+1) ) , implying the statement.\nThe following lemma bounds the loss in utility where the set of menus is limited to the extreme points E . The proof is similar to Balcan et al. [2015]; however, the loss depends on the problem-specific utility functions.\nLemma 54. Let E be as defined in Definition 10, then for any sequence of buyer valuations b = b1, . . . , bT , and \u03c1 \u2217 as the optimal menu in the hindsight:\nmax\u03c1\u2208E\nT \u2211\nt=1\nu(bt,\u03c1) \u2265 T \u2211\nt=1\nu(bt,\u03c1 \u2217)\u2212 \u03b5T.\nProof. The proof is similar to that of Lemma 12. The only difference is in step (vi) which computes the loss in revenue between menus that are at \u03b5 L1 distance. In menus of lotteries this distance implies a price difference of at most \u03b5 in any of the lotteries in the menu, and therefore causes \u03b5 total loss per time step.\nFull Information Setting\nTheorem 20. In the full information case for length-\u2113 menus of lotteries, when there are V types of buyers, there is an algorithm with regret bound of O(m2H\u2113 \u221a T ln (V \u2113)).\nProof. The proof follows the same logic as of theorem 13. We run the weighted majority algorithm (Algorithm 2, where H is replaced bymH) with parameter \u03b2 = 1/ \u221a T on the set E as the set of menus (experts). The proof directly follows from Lemma 54 and Proposition 47. Let n = |E|. Let bi be the valuation of the buyer at step i, and b\u0304 be the vector of valuation of all buyers in rounds 1 through T . We denote RevE() as the maximum revenue obtained in the set of E , OPT() as the optimal revenue, and RevWM() as the revenue obtained from Algorithm 2 on the set of experts X = E . Then,\nn \u2264 (V \u21132)m(\u2113+1),\nRevWM ( b\u0304 ) \u2265 Rev(E) ( b\u0304 ) \u2212 \u03b2 2 Rev(E) ( b\u0304 ) \u2212 mH lnn \u03b2 ,\nRevE ( b\u0304 ) =\nT \u2211\ni=1\nRevE (bi) ,\nRevE (bi) \u2265 OPT (bi)\u2212 \u03b5;\nwhere the first expression uses the size of E in Lemma 53, the second expression uses Proposition 47, the third expands the revenue over T terms, and the last uses Lemma 54. Rearranging the terms, we have:\nRevE (bi) \u2265 OPT (bi)\u2212 \u03b5 RevE ( b\u0304 ) \u2265 OPT ( b\u0304 ) \u2212 \u03b5T\nRevWM ( b\u0304 ) \u2265 OPT ( b\u0304 ) \u2212 \u03b5T \u2212 \u03b2mHT 2 \u2212 mH lnn \u03b2\nRevWM ( b\u0304 ) \u2265 OPT ( b\u0304 ) \u2212 \u03b5T \u2212 \u03b2mHT 2 \u2212 m 2(\u2113+ 1)H (ln (V \u2113)) \u03b2\nWe set variables \u03b5 and \u03b2 to minimize the exponent of T in the regret. By setting \u03b2 = 1\u221a T\nand \u03b5 = 1/( \u221a T ), The regret will be O(m2H\u2113 \u221a T ln (V \u2113)).\nPartial Information (Bandit) Setting In the partial information setting, the change in the menu options also affects the definition of set I that consists of indicator vectors over the buyer types that select the same menu entry j in a mapping \u00b5. The changes that need to be made in Algorithm 8 to work for menus of lotteries include changing |O| to \u2113+1, using option (menu entry) j instead of (j, k), and changing utility from I{k \u2265 1} (\np (j) 1 (\u03c1) + kp (j) 2 (\u03c1)\n)\nto\np(j)(\u03c1). After making these changes, we can perform the modified algorithm to achieve a bounded regret.\nLemma 55. For any menu \u03c1, E[u\u0302\u03c4 (\u03c1)] = u\u03c4(\u03c1) and u\u0302\u03c4 (\u03c1) \u2208 [\u2212mH(\u2113+ 1)V,mH(\u2113+ 1)V ].\nProof. The proof is similar to Lemma 55. The proof of the equality of the expectation simply follows from u\u0302\u03c4 (\u03c1) and u\u03c4(\u03c1) definitions and Lemma 42. Now, we prove the range of the estimator. Since S is a barycentric spanner, for any I \u2208 I , \u03bbi(I) \u2208 [\u22121, 1]. Also, f\u0302\u03c4 (.) belongs to {0, 1}. Additionally, the utility of the buyer selecting each option in the menu, e.g., p(j)(\u03c1), is always in [0, mH ]. Therefore, using the formula of the estimator, it is bounded by mH times the number of options times the number of buyer types.\nTheorem 21. In the partial information (bandit) case for length-\u2113 menus of lotteries, when there are V different types of buyers, there is an algorithm with regret bound of O(T 2/3(\u2113m)4/3(HV )1/3 log1/3(V \u2113)).\nProof. The proof follows the same logic as of theorem 14. In Lemma 44, |S| is the number of dimensions (barycentric spanner set), \u03ba is the maximum revenue times the number of buyer types times the number of their options (entries in the menu), |M | is the number of discrete points. In our case, |S| = \u2113(m+1), \u03ba = mHV (\u2113+1), and |M | \u2264 (V \u21132)m(\u2113+1). By Lemma 55, the expected value of the estimated utility is equal to the exact value of utility with range [\u2212mH(\u2113+ 1)V,mH(\u2113+ 1)V ].\nUsing Lemma 44, the regret for menus of lotteries is bounded by\nO(T 2/3(\u2113m)4/3(HV )1/3 log1/3(V \u2113))."
        },
        {
            "heading": "B.4 Distributional Learning",
            "text": "Theorem 22. For length-\u2113 menus of lotteries, there is a discretization-based distributional learning algorithm with sample complexity O\u0303 (m2H2/\u03b52(\u2113m+ ln (2/\u03b4))), and running time O\u0303 ( (2m2H2/\u03b52) \u2113m+\u2113+1 \u2113(\u2113m+ ln (2/\u03b4)) ln\u2113m (mH/\u03b5 ln (mH/\u03b5)) ) .\nProof. We need to find the number of samples such that with probability 1\u2212\u03b4, the difference between the expected revenue of our algorithm and the optimal revenue is at most \u03b5. Note that since our algorithm uses discretization of possible menus, we face two types of errors: the discretization error, and the usual empirical error in a PAC learning setting. We find the sample complexity and discretization parameters such that the total error is bounded by \u03b5.\nThe possible number of menus after discretization using Algorithm 4 with parameter \u03b1 is computed by the following formula.\n|H| = (1/\u03b1\u2113m+\u2113) (ln (Hm/\u03b1))\u2113m\nUsing uniform convergence in the PAC learning setting, the sample complexity for empirical error \u03b5\u2032 is as follows.\n|S| \u2265 m 2H2\n2\u03b5\u20322 (ln |H|+ ln (2/\u03b4))\nReplacing lnH we have,\n|S| \u2265 m 2H2\n2\u03b5\u20322 (\u2113m(ln(1/\u03b1) + ln ln (mH) + ln (2/\u03b4))\nAlso, the revenue loss compared to the optimum for arbitrary buyer i with valuation vi when using Algorithm 4 with parameters \u03b1, K, and d (we use d instead of \u03b4 in Algorithm 4 and reserve \u03b4 for (\u03b5, \u03b4)-learning) is computed by the following formula.\nRevM \u2032 (vi) \u2265 OPT(vi)(1\u2212 d)(1\u2212 \u03b1)K \u2212 (2K + 1)\u03b1\u2212mH(1\u2212 d)K\nThe total error (from discretization and empirical error), when the empirical error is set to \u03b5\u2032, is\nmH [1\u2212 (1\u2212 d)(1\u2212 \u03b1)K ] + (2K + 1)\u03b1 +mH(1\u2212 d)K + \u03b5\u2032\nBy setting d = \u03b5\u2032/(2mH), K = 2mH/\u03b5\u2032 ln(mH/\u03b5\u2032), and \u03b1 = \u03b5\u2032/(2m2H2 ln(mH/\u03b5\u2032)), the total mistake is less than 4\u03b5\u2032.\nReplacing these parameters and substituting \u03b5\u2032 with \u03b5/4 to satify total error \u03b5, we have the following sample complexity:\n|S| \u2265 m 2H2\n2\u03b5\u20322 (\u2113m(ln(1/\u03b1) + ln ln (mH) + ln (2/\u03b4))\n= O\u0303\n(\nm2H2\n\u03b52 (\u2113m+ ln (2/\u03b4))\n)\nAlso, replacing the parameters we have:\n|H| = O ( ( 2m2H2\n\u03b52\n)\u2113m+\u2113\nln\u2113m (mH/\u03b5 ln (mH/\u03b5))\n)\nThe computational complexity of finding the empirical optimal menu for |S| buyers and menu of size \u2113 is:\n|S|\u2113|H| = O\u0303 ( ( 2m2H2\n\u03b52\n)\u2113m+\u2113+1\n\u2113(\u2113m+ ln (2/\u03b4)) ln\u2113m (mH/\u03b5 ln (mH/\u03b5))\n)\nThis implies the computational complexity of the algorithm.\nTheorem 56. For arbitrary-length menus of lotteries, there is a discretization-based distributional learning algorithm with sample complexity\nO\n(\nm2H2\n\u03b52 ( (32m2H2/\u03b52)m+1 lnm (mH/\u03b5 ln(mH/\u03b5)) lnm+1 (mH/\u03b5) + ln (1/\u03b4) )\n)\n,\nand running time\nO ( 2(32m 2H2/\u03b52)m+1 lnm (mH/\u03b5 ln(mH/\u03b5)) lnm+1 (mH/\u03b5) ) .\nProof. This proof follows the same line as the proof of Theorem 22. We need to find the number of samples such that with probability 1 \u2212 \u03b4, the difference between the expected revenue of our algorithm and the optimal revenue is at most \u03b5. Note that since our algorithm\nuses discretization of possible menus, we face two types of errors: the discretization error, and the usual empirical error in a PAC learning setting. We find the sample complexity and discretization parameters such that the total error is bounded by \u03b5.\nThe possible number of menus after discretization using Algorithm 4 with parameter \u03b1 is computed by the following formula.\n|H| = O ( 2(1/\u03b1 m+1)(ln (Hm/\u03b1))m )\nUsing uniform convergence in the PAC learning setting, the sample complexity for empirical error \u03b5\u2032 is as follows.\n|S| \u2265 m 2H2\n2\u03b5\u20322 (ln |H|+ ln (2/\u03b4))\nReplacing lnH we have,\n|S| \u2265 m 2H2\n2\u03b5\u20322\n(\nlnm (Hm/\u03b1)\n\u03b1m+1 + ln (2/\u03b4)\n)\nAlso, the revenue loss compared to the optimum for arbitrary buyer i with valuation vi when using Algorithm 4 with parameters \u03b1, K, and d (we use d instead of \u03b4 in Algorithm 4 and reserve \u03b4 for (\u03b5, \u03b4)-learning) is computed by the following formula.\nRevM \u2032 (vi) \u2265 OPT(vi)(1\u2212 d)(1\u2212 \u03b1)K \u2212 (2K + 1)\u03b1\u2212mH(1\u2212 d)K\nThe total error (from discretization and empirical error) when the empirical error is set to \u03b5\u2032 is\nmH [1\u2212 (1\u2212 d)(1\u2212 \u03b1)K ] + (2K + 1)\u03b1 +mH(1\u2212 d)K + \u03b5\u2032\nBy setting d = \u03b5\u2032/(2mH), K = 2mH/\u03b5\u2032 ln(mH/\u03b5\u2032), and \u03b1 = \u03b5\u2032/(2m2H2 ln(mH/\u03b5\u2032)), the total mistake is less than 4\u03b5\u2032.\nReplacing these parameters and substituting \u03b5\u2032 with \u03b5/4 to satify total error \u03b5, we have the following sample complexity:\n|S| \u2265 m 2H2\n2\u03b5\u20322\n(\nlnm (mH/\u03b1)\n\u03b1m+1 + ln (2/\u03b4)\n)\n= O\n(\nm2H2\n\u03b52 ( (32m2H2/\u03b52)m+1 lnm (mH/\u03b5 ln(mH/\u03b5)) lnm+1 (mH/\u03b5) + ln (1/\u03b4) )\n)\nAlso, replacing the parameters we have:\n|H| = O ( 2(1/\u03b1 m+1) lnm (Hm/\u03b1) )\n= O ( 2(32m 2H2/\u03b52)m+1 lnm (mH/\u03b5 ln(mH/\u03b5)) lnm+1 (mH/\u03b5) )\nThe computational complexity of finding the empirical optimal menu for |S| buyers is the number of potential menus |H| times |S| times the maximum size of a menu which is O(ln(H)).\nLemma 57. The sample complexity of length \u2113 menus of lotteries using the techniques in [Balcan et al., 2018b] is bounded by\nc\n(\nH\n\u03b5\n)2(\n9\u2113(m+ 1) log ( 4\u2113(m+ 1) ( (\u2113+ 1)2 +m\u2113 )) + log 1\n\u03b4\n)\n.\nProof. Balcan et al. [2018c] introduce delineability as a condition to upper bound the pseudodimension and therefore, the sample complexity. They show the class of lotteries is (\u2113(m+ 1), (\u2113+ 1)2 +m\u2113)-delineable. Also, ifM is a mechanism class that is (d, t)-delineable, then the pseudo dimension of M is at most 9d log(4dt). Therefore, the pseudo-dimension for menus of lotteries is bounded by 9\u2113(m + 1) log (4\u2113(m+ 1) ((\u2113+ 1)2 +m\u2113)). Furthermore, the sample complexity is at most c(H/\u03b5)2 (Pdim(H) + log (1/\u03b4)), which by replacing pseudo dimension for this class of mechanism completes the proof."
        },
        {
            "heading": "C Failure of Dispersion for menus of lotteries",
            "text": "In this section, we prove that without making extra assumptions about optimal menus of lotteries, both definitions of dispersion (Definitions 32 and 35) fail. In particular, we show that the failure of both conditions happens if the optimal menu (maximizer) has two lotteries close to each other (similar coordinates) and satisfies some other properties. Example 2 illustrates a setting where there are lotteries with arbitrarily close coordinates in the optimal menu.\nTheorem 58. Let the maximizer \u03c1\u2217 have the following properties, where \u03c6 (1) \u03c1\u2217 , p (1) \u03c1\u2217 , \u03c6 (2) \u03c1\u2217 , p (2) \u03c1\u2217 are the coordinates of \u03c1\u2217, respectively illustrating the probability of allocating item one in lottery 1, the price of lottery 1, the probability of allocating item one in lottery 2, the price of lottery 2, and the allocation probability for other items are the same across these lotteries.\n1. p (1) \u03c1\u2217 \u2212 p (2) \u03c1\u2217 = (L+ 1/2)\u03b5, where L is the Lipschitz parameter.\n2. \u03c6 (1) \u03c1\u2217 \u2212 \u03c6(2)\u03c1\u2217 = (L+ 1)\u03b5/c+ \u03b5/2.\n3. c is a constant such that c \u2264 H.\nIn this case, for every \u03ba-bounded distribution whose density is also lower-bounded by 1/\u03ba, the conditions of Definitions 32 and 35, are violated. In particular, in Definition 32, the probability of a hyperplane crossing the \u03b5-radius ball centered at the maximizer is a constant depending on c; and in Definition 35, there exists a pair of points such that the expected number of times that their loss function difference violates the Lipschitz condition for any Lipschitz constant L\u2032 = L/2 is a constant depending on c.\nProof. We first show why Definition 32 fails. Consider a ball of radius \u03b5 centered at the maximizer \u03c1\u2217. Let this ball be B. We show that the probability of a hyperplane crossing B is constant. Consider a point \u03c1 \u2208 B. We first find the probability density of hyperplanes going through \u03c1. Then, we integrate to find the probability of crossing the ball. The following equation shows for what value of v (the value for the item), the hyperplane goes through \u03c1.\nv\u03c6(1)\u03c1 \u2212 p(1)\u03c1 = v\u03c6(2)\u03c1 \u2212 p(2)\u03c1\nv = p (1) \u03c1 \u2212 p(2)\u03c1\n\u03c6 (1) \u03c1 \u2212 \u03c6(2)\u03c1\nLet vminB and v max B be the minimum value of v for which the hyperplane crosses the ball (i.e., there is \u03c1 \u2208 B such that vminB = p (1) \u03c1 \u2212 p(2)\u03c1\n\u03c6 (1) \u03c1 \u2212 \u03c6(2)\u03c1\n), and the maximum value respectively. The\nprobability that the hyperplane crosses the ball is \u222b vmaxB vmin B f(v)dv, where f(v) is the density function of the value for the item. We consider the following points. These points are all in \u03b5 proximity of \u03c1\u2217, therefore, fall in a ball of radius \u03b5 centered at \u03c1\u2217. Consider points with p(2) = p (2) \u03c1\u2217 and \u03c6 (2) = \u03c6 (2) \u03c1\u2217 . Let p (1) be in [p (2) \u03c1\u2217 + L\u03b5, p (2) \u03c1\u2217 + (L+ 1)\u03b5]. Let \u03c6 (1) be in [\u03c6 (2) \u03c1\u2217 + (L+ 1)\u03b5/c, \u03c6 (2) \u03c1\u2217 + (L+ 1)\u03b5/c+ \u03b5].\nWith the above construction, the numerator ranges from L\u03b5 to (L+1)\u03b5, and the denominator ranges from (L+ 1)\u03b5/c to (L+ 1)\u03b5/c+ \u03b5. Therefore, vminB = Lc L+c+1 and vmaxB = c. For \u03ba-bounded distribution with support [0, 1], \u222b vmaxB vmin B f(v)dv is at least\nc\u2212 Lc L+c+1\n\u03ba =\nc(c+1) L+c+1\n\u03ba ;\nwhich is constant for a constant c. Now, we show that Definition 35 fails. To do so, we still consider pair of points \u03c1 and \u03c1\u2032 which correspond to vminB and v max B , respectively. If we consider the line segment connecting \u03c1 and \u03c1\u2032, the probability of the hyperplane crossing these two points is still \u222b vmin B\nvmin B\nf(v)dv which\nagain for \u03ba-bounded distribution with support [0, 1] whose density is also lower-bounded by 1/\u03ba, \u222b vmax B\nvmin B\nf(v)dv is at least\nc\u2212 Lc L+c+1\n\u03ba =\nc(c+1) L+c+1\n\u03ba ;\nwhich is constant for a constant c. Note that |p(1)\u03c1 \u2212 p(2)\u03c1\u2032 | \u2265 L\u03b5 and |p (2) \u03c1 \u2212 p(1)\u03c1\u2032 | \u2265 L\u03b5 which implies anytime the hyperplane crosses between \u03c1 and \u03c1\u2032, the difference in the loss, |\u2113t(\u03c1)\u2212 \u2113t(\u03c1\u2032)| is at least L\u03b5. Also, the Euclidean distance between \u03c1 and \u03c1\u2032 is less than 2\u03b5. Therefore, the Lipschitz condition for constant L\u2032 = L/2 is violated a constant fraction of times in expectation.\nThe following example shows that in the optimal menu of lotteries, lottery pairs can be arbitrarily close to each other.\nExample 2 ([Daskalakis et al., 2014]). Consider the case of two items, when the buyer\u2019s value for each item is drawn i.i.d. from the distribution supported on [0, 1] with density function f(x) = 2(1 \u2212 x). Daskalakis et al. prove for this example that the unique (up to differences of measure zero) optimal mechanism has uncountable menu complexity. That is,\nthe number of distinct options available for the buyer to purchase is uncountable. They show that the optimal mechanism contains the following four kinds of options: (a) the buyer can receive item one with probability 1, and item two with probability 2\n(4\u22125x)2 paying the price 2\u22123x 4\u22125x + 2x (4\u22125x)2 , for any x \u2208 [0,\u2248 .0618), (b) the buyer can receive item two with probability 1, and item one with probability 2 (4\u22125x)2 paying the price 2\u22123x 4\u22125x + 2x (4\u22125x)2 , for any x \u2208 [0,\u2248 .0618), (c) the buyer can receive both items and pay \u2248 .5535, and (d) the buyer can receive neither item and pay nothing."
        }
    ],
    "title": "Learning Revenue Maximizing Menus of Lotteries and Two-Part Tariffs",
    "year": 2023
}