{
    "abstractText": "This paper presents a novel approach for exploiting the global context for the task of word sense disambiguation (WSD). This is done by using topic features constructed using the latent dirichlet allocation (LDA) algorithm on unlabeled data. The features are incorporated into a modified na \u0131\u0308ve Bayes network alongside other features such as part-of-speech of neighboring words, single words in the surrounding context, local collocations, and syntactic patterns. In both the English all-words task and the English lexical sample task, the method achieved significant improvement over the simple na \u0131\u0308ve Bayes classifier and higher accuracy than the best official scores on Senseval-3 for both task.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jun Fu Cai"
        },
        {
            "affiliations": [],
            "name": "Wee Sun Lee"
        },
        {
            "affiliations": [],
            "name": "Yee Whye Teh"
        }
    ],
    "id": "SP:4f94381a38a58288bf5abbfa5809b59d50f7500a",
    "references": [
        {
            "authors": [
                "Y.K. Lee",
                "H.T. Ng."
            ],
            "title": "An Empirical Evaluation of Knowledge Sources and Learning Algorithms for Word Sense Disambiguation",
            "venue": "In",
            "year": 2002
        },
        {
            "authors": [
                "U.S. Kohomban",
                "W.S. Lee"
            ],
            "title": "Learning Semantic Classes for Word Sense Disambiguation",
            "year": 2005
        },
        {
            "authors": [
                "Proc. of ACL.R.K. Ando."
            ],
            "title": "Applying Alternating Structure Optimization to Word Sense Disambiguation",
            "venue": "In",
            "year": 2006
        },
        {
            "authors": [
                "Proc. of CoNLL.Y.S. Chan",
                "H.T. Ng"
            ],
            "title": "Scaling Up Word Sense Disambiguation via Parallel Texts",
            "venue": "In",
            "year": 2005
        },
        {
            "authors": [
                "R.K. Ando",
                "T. Zhang"
            ],
            "title": "A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data",
            "year": 2005
        },
        {
            "authors": [
                "R.K. Ando",
                "T. Zhang."
            ],
            "title": "A High-Performance Semi-Supervised Learning Method for Text Chunking",
            "venue": "Proc. of ACL.",
            "year": 2005
        },
        {
            "authors": [
                "P. Resnik",
                "D. Yarowsky."
            ],
            "title": "A Perspective on Word Sense Disambiguation Methods and Their Evaluation",
            "venue": "InProc. of ACL.",
            "year": 1997
        },
        {
            "authors": [
                "D.M. Blei",
                "A.Y. Ng",
                "M.I. Jordan."
            ],
            "title": "Latent Dirichlet Allocation",
            "venue": "Journal of Machine Learning Research",
            "year": 2003
        },
        {
            "authors": [
                "A. Ratnaparkhi"
            ],
            "title": "A Maximum Entropy Model for",
            "year": 1996
        },
        {
            "authors": [
                "E. Charniak"
            ],
            "title": "A Maximum-Entropy-Inspired Parser",
            "venue": "InProc. of the 1st Meeting of the North American Chapter of the Association for Computational Linguistics.",
            "year": 2000
        },
        {
            "authors": [
                "V.N. Vapnik"
            ],
            "title": "The Nature of Statistical Learning Theory",
            "venue": "Springer-Verlag, New York.",
            "year": 1995
        },
        {
            "authors": [
                "R. Florian",
                "D. Yarowsky"
            ],
            "title": "Modeling consensus: Classifier Combination for Word Sense Disambiguation",
            "venue": "Proc. of EMNLP.",
            "year": 2002
        },
        {
            "authors": [
                "D. Wu",
                "W. Su",
                "M. Carpuat."
            ],
            "title": "A Kernel PCA Method for Superior Word Sense Disambiguation",
            "venue": "Proc. of ACL.",
            "year": 2004
        },
        {
            "authors": [
                "C. Strapparava",
                "A. Gliozzo",
                "C. Giuliano"
            ],
            "title": "Pattern Abstraction and Term Similarity for Word Sense Disambiguation: IRST at Senseval-3",
            "venue": "In",
            "year": 2004
        },
        {
            "authors": [
                "C. Grozea"
            ],
            "title": "Finding Optimal Parameter Settings for High Performance Word Sense Disambiguation",
            "venue": "In Proc. of Senseval-3",
            "year": 2004
        },
        {
            "authors": [
                "R. Mihalcea"
            ],
            "title": "Bootstrapping Large Sense Tagged Corpora",
            "venue": "InProc. of the 3rd International Conference on Languages Resources and Evaluations",
            "year": 2002
        },
        {
            "authors": [
                "V. Hoste",
                "A. Kool",
                "W. Daelmans"
            ],
            "title": "Classifier Optimization and Combination in English All Words Task",
            "venue": "InProc. of Senseval-2",
            "year": 2001
        },
        {
            "authors": [
                "B. Decadt",
                "V. Hoste",
                "W. Daelmans"
            ],
            "title": "GAMBL, Genetic Algorithm Optimization of Memory-Based WSD. InProc",
            "year": 2004
        },
        {
            "authors": [
                "R. Mihalcea",
                "E. Faruque"
            ],
            "title": "Sense-learner: Minimally Supervised Word Sense Disambiguation for All Words in Open Text. InProc",
            "year": 2004
        }
    ],
    "sections": [
        {
            "text": "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp. 1015\u20131023, Prague, June 2007. c\u00a92007 Association for Computational Linguistics\nThis paper presents a novel approach for exploiting the global context for the task of word sense disambiguation (WSD). This is done by using topic features constructed using the latent dirichlet allocation (LDA) algorithm on unlabeled data. The features are incorporated into a modified na\u0131\u0308ve Bayes network alongside other features such as part-of-speech of neighboring words, single words in the surrounding context, local collocations, and syntactic patterns. In both the English all-words task and the English lexical sample task, the method achieved significant improvement over the simple na\u0131\u0308ve Bayes classifier and higher accuracy than the best official scores on Senseval-3 for both task."
        },
        {
            "heading": "1 Introduction",
            "text": "Natural language tends to be ambiguous. A word often has more than one meanings depending on the context. Word sense disambiguation (WSD) is a natural language processing (NLP) task in which the correct meaning (sense) of a word in a given context is to be determined.\nSupervised corpus-based approach has been the most successful in WSD to date. In such an approach, a corpus in which ambiguous words have been annotated with correct senses is first collected. Knowledge sources, or features, from the context of the annotated word are extracted to form the training data. A learning algorithm, like the support vector\nmachine (SVM) or na\u0308\u0131ve Bayes, is then applied on the training data to learn the model. Finally, in testing, the learnt model is applied on the test data to assign the correct sense to any ambiguous word.\nThe features used in these systems usually include local features, such as part-of-speech (POS) of neighboring words, local collocations , syntactic patterns and global features such as single words in the surrounding context (bag-of-words) (Lee and Ng, 2002). However, due to the data scarcity problem, these features are usually very sparse in the training data. There are, on average, 11 and 28 training cases per sense in Senseval 2 and 3 lexical sample task respectively, and 6.5 training cases per sense in the SemCor corpus. This problem is especially prominent for the bag-of-words feature; more than hundreds of bag-of-words are usually extracted for each training instance and each feature could be drawn from any English word. A direct consequence is that the global context information, which the bag-of-words feature is supposed to capture, may be poorly represented.\nOur approach tries to address this problem by clustering features to relieve the scarcity problem, specifically on the bag-of-words feature. In the process, we construct topic features, trained using the latent dirichlet allocation (LDA) algorithm. We train\ntopic model (Blei et al., 2003) on unlabeled data, clustering the words occurring in the corpus to a predefined number of topics. We then use the resulting topic model to tag the bag-of-words in the labeled corpus with topic distributions. We incorporate the distributions, called the topic features, using a simple Bayesian network, modified from na\u0131\u0308ve Bayes\n1015\nmodel, alongside other features and train the model on the labeled corpus. The approach gives good performance on both the lexical sample and all-words tasks on Senseval data.\nThe paper makes mainly two contributions. First, we are able to show that a feature that efficiently captures the global context information using LDA algorithm can significantly improve the WSD accuracy. Second, we are able to obtain this feature from unlabeled data, which spares us from any manual labeling work. We also showcase the potential strength of Bayesian network in the WSD task, obtaining performance that rivals state-of-arts methods."
        },
        {
            "heading": "2 Related Work",
            "text": "Many WSD systems try to tackle the data scarcity problem. Unsupervised learning is introduced primarily to deal with the problem, but with limited success (Snyder and Palmer, 2004). In another approach, the learning algorithm borrows training instances from other senses and effectively increases the training data size. In (Kohomban and Lee, 2005), the classifier is trained using grouped senses for verbs and nouns according to WordNet top-level synsets and thus effectively pooling training cases across senses within the same synset. Similarly, (Ando, 2006) exploits data from related tasks, using all labeled examples irrespective of target words for learning each sense using the Alternating Structure Optimization (ASO) algorithm (Ando and Zhang, 2005a; Ando and Zhang, 2005b). Parallel texts is proposed in (Resnik and Yarowsky, 1997) as potential training data and (Chan and Ng, 2005) has shown that using automatically gathered parallel texts for nouns could significantly increase WSD accuracy, when tested on Senseval-2 English all-words task.\nOur approach is somewhat similar to that of using generic language features such as POS tags; the words are tagged with its semantic topic that may be trained from other corpuses."
        },
        {
            "heading": "3 Feature Construction",
            "text": "We first present the latent dirichlet allocation algorithm and its inference procedures, adapted from the original paper (Blei et al., 2003)."
        },
        {
            "heading": "3.1 Latent Dirichlet Allocation",
            "text": "LDA is a probabilistic model for collections of discrete data and has been used in document modeling and text classification. It can be represented as a three level hierarchical Bayesian model, shown graphically in Figure 1. Given a corpus consisting of M documents, LDA models each document using a mixture overK topics, which are in turn characterized as distributions over words.\nIn the generative process of LDA, for each documentd we first draw the mixing proportion over topics\u03b8d from a Dirichlet prior with parameters\u03b1. Next, for each of theNd wordswdn in documentd, a topic zdn is first drawn from a multinomial distribution with parameters\u03b8d. Finally wdn is drawn from the topic specific distribution over words. The probability of a word tokenw taking on valuei given that topicz = j was chosen is parameterized using a matrix\u03b2 with \u03b2ij = p(w = i|z = j). Integrating out \u03b8d\u2019s andzdn\u2019s, the probabilityp(D|\u03b1, \u03b2) of the corpus is thus:\nM\u220f d=1 \u222b p(\u03b8d|\u03b1) ( Nd\u220f n=1 \u2211 zdn p(zdn|\u03b8d)p(wdn|zdn, \u03b2) ) d\u03b8d"
        },
        {
            "heading": "3.1.1 Inference",
            "text": "Unfortunately, it is intractable to directly solve the posterior distribution of the hidden variables given a document, namelyp(\u03b8, z|w, \u03b1, \u03b2). However, (Blei et al., 2003) has shown that by introducing a set of variational parameters,\u03b3 and\u03c6, a tight lower bound on the log likelihood of the probability can be found using the following optimization procedure:\n(\u03b3\u2217, \u03c6\u2217) = arg min \u03b3,\u03c6 D(q(\u03b8, z|\u03b3, \u03c6)\u2016p(\u03b8, z|w, \u03b1, \u03b2))\nwhere\nq(\u03b8, z|\u03b3, \u03c6) = q(\u03b8|\u03b3) N\u220f\nn=1\nq(zn|\u03c6n),\n\u03b3 is the Dirichlet parameter for\u03c6 and the multinomial parameters(\u03c61 \u00b7 \u00b7 \u00b7\u03c6N ) are the free variational parameters. Note here\u03b3 is document specific instead of corpus specific like\u03b1. Graphically, it is represented as Figure 2. The optimizing values of\u03b3 and \u03c6 can be found by minimizing the Kullback-Leibler (KL) divergence between the variational distribution and the true posterior."
        },
        {
            "heading": "3.2 Baseline Features",
            "text": "For both the lexical sample and all-words tasks, we use the following standardbaseline featuresfor comparison.\nPOS Tags For each training or testing word,w, we include POS tags forP words prior to as well as afterw within the same sentence boundary. We also include the POS tag ofw. If there are fewer than P words prior or afterw in the same sentence, we denote the corresponding feature as NIL.\nLocal Collocations CollocationCi,j refers to the ordered sequence of tokens (words or punctuations) surroundingw. The starting and ending position of the sequence are denotedi andj respectively, where a negative value refers to the token position prior to w. We adopt the same 11 collocation features as (Lee and Ng, 2002), namelyC\u22121,\u22121, C1,1, C\u22122,\u22122, C2,2, C\u22122,\u22121, C\u22121,1, C1,2, C\u22123,\u22121, C\u22122,1, C\u22121,2, andC1,3.\nBag-of-Words For each training or testing word, w, we getG words prior to as well as afterw, within the same document. These features are position insensitive. The words we extract are converted back to their morphological root forms.\nSyntactic Relations We adopt the same syntactic relations as (Lee and Ng, 2002). For easy reference, we summarize the features into Table 1.\nThe exact values ofP andG for each task are set according to cross validation result."
        },
        {
            "heading": "3.3 Topic Features",
            "text": "We first select an unlabeled corpus, such as 20 Newsgroups, and extract individual words from it (excluding stopwords). We choose the number of topics,K, for the unlabeled corpus and we apply the LDA algorithm to obtain the\u03b2 parameters, where \u03b2 represents the probability of a wordwi given a topic zj , p(wi|zj) = \u03b2ij . The model essentially clusters words that occurred in the unlabeled corpus according toK topics. The conditional probability p(wi|zj) = \u03b2ij is later used to tag the words in the unseen test example with the probability of each topic.\nFor some variants of the classifiers that we construct, we also use the\u03b3 parameter, which is document specific. For these classifiers, we may need to run the inference algorithm on the labeled corpus and possibly on the test documents. The\u03b3 parameter provides an approximation to the probability of\nselecting topici in the document:\np(zi|\u03b3) = \u03b3i\u2211 K \u03b3k . (1)"
        },
        {
            "heading": "4 Classifier Construction",
            "text": ""
        },
        {
            "heading": "4.1 Bayesian Network",
            "text": "We construct a variant of the na\u0131\u0308ve Bayes network as shown in Figure 3. Here,w refers to the word. s refers to the sense of the word. In training,s is observed while in testing, it is not. The featuresf1 to fn are baseline features mentioned in Section 3.2 (including bag-of-words) whilez refers to the latent topic that we set for clustering unlabeled corpus. The bag-of-wordsb are extracted from the neighbours ofw and there areL of them. Note thatL can be different fromG, which is the number of bag-ofwords in baseline features. Both will be determined by the validation result.\nThe log-likelihood of an instance,\u0300(w, s, F, b) whereF denotes the set of baseline features, can be written as\n= logp(w) + logp(s|w) + \u2211 F log(p(f |s))\n+ \u2211 L log (\u2211 K p(zk|s)p(bl|zk) ) .\nThe log p(w) term is constant and thus can be ignored. The first portion is normal na\u0131\u0308ve Bayes. And second portion represents the additional LDA plate.\nWe decouple the training process into three separate stages. We first extract baseline features from the task training data, and estimate, using normal na\u0131\u0308ve Bayes,p(s|w) andp(f |s) for all w, s andf . The parameters associated withp(b|z) are estimated using LDA from unlabeled data. Finally we estimate the parameters associated withp(z|s). We experimented with three different ways of both doing the estimation as well as using the resulting model and chose one which performed best empirically."
        },
        {
            "heading": "4.1.1 Expectation Maximization Approach",
            "text": "For p(z|s), a reasonable estimation method is to use maximum likelihood estimation. This can be done using the expectation maximization (EM) algorithm. In classification, we just choose\u2217 that maximizes the log-likelihood of the test instance, where:\ns\u2217 = arg max s `(w, s, F, b)\nIn this approach,\u03b3 is never used which means the LDA inference procedure is not used on any labeled data at all."
        },
        {
            "heading": "4.1.2 Soft Tagging Approach",
            "text": "Classification in this approach is done using the full Bayesian network just as in the EM approach. However we do the estimation ofp(z|s) differently. Essentially, we perform LDA inference on the training corpus in order to obtain\u03b3 for each document. We then use the\u03b3 and\u03b2 to obtainp(z|b) for each word using\np(zi|bl, \u03b3) = p(bl|zi)p(zi|\u03b3)\u2211 K p(bl|zk)p(zk|\u03b3) ,\nwhere equation [1] is used for estimation ofp(zi|\u03b3). This effectively transformsb to a topical distribution which we call a soft tag where each soft tag is probability distributiont1, . . . , tK on topics. We then use this topical distribution for estimating p(z|s). Let si be the observed sense of instancei and tij1 , . . . , t ij K be the soft tag of thej-th bag-ofword feature of instancei. We estimatep(z|s) as\np(zjk|s) = \u2211 si=s t ij k\u2211\nsi=s \u2211 k\u2032 t ij k\u2032\n(2)\nThis approach requires us to do LDA inference on the corpus formed by the labeled training data, but\nnot the testing data. This is because we need\u03b3 to get transformed topical distribution in order to learn p(z|s) in the training. In the testing, we only apply the learnt parameters to the model."
        },
        {
            "heading": "4.1.3 Hard Tagging Approach",
            "text": "Hard tagging approach no longer assumes thatz is latent. Afterp(z|b) is obtained using the same procedure in Section 4.1.2, the topiczi with the highestp(zi|b) among allK topics is picked to represent z. In this way,b is transformed into a single most \u201cprominent\u201d topic. This topic label is used in the same way as baseline features for both training and testing in a simple na\u0131\u0308ve Bayes model.\nThis approach requires us to perform the transformation both on the training as well as testing data, sincez becomes an observed variable. LDA inference is done on two corpora, one formed by the training data and the other by testing data, in order to get the respective values of\u03b3."
        },
        {
            "heading": "4.2 Support Vector Machine Approach",
            "text": "In the SVM (Vapnik, 1995) approach, we first form a\ntraining and a testing file using all standard features for each sense following (Lee and Ng, 2002) (one classifier per sense). To incorporate LDA feature, we use the same approach as Section 4.1.2 to transform b into soft tags,p(z|b). As SVM deals with only observed features, we need to transformb both in the training data and in the testing data. Compared to (Lee and Ng, 2002), the only difference is that for each training and testing case, we have additional L \u2217K LDA features, since there areL bag-of-words and each has a topic distribution represented byK values."
        },
        {
            "heading": "5 Experimental Setup",
            "text": "We describe here the experimental setup on the English lexical sample task and all-words task.\nWe use MXPOST tagger (Adwait, 1996) for POS tagging, Charniak parser (Charniak, 2000) for extracting syntactic relations, SVMlight1 for SVM classifier and David Blei\u2019s version of LDA2 for LDA training and inference. All default parameters are used unless mentioned otherwise. For all standard\n1http://svmlight.joachims.org 2http://www.cs.princeton.edu/\u02dcblei/lda-c/\nbaseline features, we use Laplace smoothing but for the soft tag (equation [2]), we use a smoothing parameter value of 2."
        },
        {
            "heading": "5.1 Development Process",
            "text": ""
        },
        {
            "heading": "5.1.1 Lexical Sample Task",
            "text": "We use the Senseval-2 lexical sample task for preliminary investigation of different algorithms, datasets and other parameters. As the dataset is used extensively for this purpose, only the Senseval-3 lexical sample task is used for evaluation.\nSelecting Bayesian Network The best achievable result, using the three different Bayesian network approaches, when validating on Senseval-2 test data is shown in Table 2. The parameters that are used areP = 3 andG = 3.\nEM 68.0 Hard Tagging 65.6 Soft Tagging 68.9\nTable 2: Results on Senseval-2 English lexical sample using different Bayesian network approaches.\nFrom the results, it appears that both the EM and the Hard Tagging approaches did not yield as good results as the Soft Tagging approach did. The EM approach ignores the LDA inference result,\u03b3, which we use to get our topic prior. This information is document specific and can be regarded as global context information. The Hard Tagging approach also uses less information, as the original topic distribution is now represented only by the topic with the highest probability of occurring. Therefore, both methods have information loss and are disadvantaged against the Soft Tagging approach. We use the Soft Tagging approach for the Senseval-3 lexical sample and the all-words tasks.\nUnlabeled Corpus Selection The unlabeled corpus we choose to train LDA include 20 Newsgroups, Reuters, SemCor, Senseval-2 lexical sample data and Senseval-3 lexical sample data. Although the last three are labeled corpora, we only need the words from these corpora and thus they can be regarded as unlabeled too. For Senseval-2 and Senseval-3 data, we define the whole passage for each training and testing instance as one document.\nThe relative effect using different corpus and combinations of them is shown in Table 3, when validating on Senseval-2 test data using the Soft Tagging approach.\nTable 3: Effect of using different corpus for LDA training, |w| represents the corpus size in terms of the number of words in the corpus\nThe 20 Newsgroups corpus yields the best result if used individually. It has a relatively larger corpus size at 1.7 million words in total and also a well balanced topic distribution among its documents, ranging across politics, finance, science, computing, etc. The Reuters corpus, on the other hand, focuses heavily on finance related articles and has a rather skewed topic distribution. This probably contributed to its inferior result. However, we found that the best result comes from combining all the corpora together with K = 60 andL = 40.\nResults for Optimized Configuration As baseline for the Bayesian network approaches, we use na\u0308\u0131ve Bayes with all baseline features. For the baseline SVM approach, we chooseP = 3 and include all the words occurring in the training and testing passage as bag-of-words feature.\nThe F-measure result we achieve on Senseval-2 test data is shown in Table 4. Our four systems are listed as the top four entries in the table. Soft Tag refers to the soft tagging Bayesian network approach. Note that we used the Senseval-2 test data for optimizing the configuration (as is done in the ASO result). Hence, the result should not be taken as reliable. Nevertheless, it is worth noting that the improvement of Bayesian network approach over its baseline is very significant (+5.5%). On the other hand, SVM with topic features shows limited improvement over its baseline (+0.8%).\nBayes (Soft Tag) 68.9 SVM-Topic 66.0 SVM baseline 65.2 NB baseline 63.4 ASO(best configuration)(Ando, 2006)68.1 Classifier Combination(Florian, 2002)66.5 Polynomial KPCA(Wu et al., 2004) 65.8 SVM(Lee and Ng, 2002) 65.4 Senseval-2 Best System 64.2"
        },
        {
            "heading": "5.1.2 All-words Task",
            "text": "In the all-words task, no official training data is provided with Senseval. We follow the common practice of using the SemCor corpus as our training data. However, we did not use SVM approach in this task as there are too few training instances per sense for SVM to achieve a reasonably good accuracy.\nAs there are more training instances in SemCor, 230, 000 in total, we obtain the optimal configuration using 10 fold cross validation on the SemCor training data. With the optimal configuration, we test our system on both Senseval-2 and Senseval-3 official test data.\nFor baseline features, we setP = 3 andB = 1. We choose a LDA training corpus comprising 20 Newsgroups and SemCor data, with number of topicsK = 40 and number of LDA bag-of-wordsL = 14."
        },
        {
            "heading": "6 Results",
            "text": "We now present the results on both English lexical sample task and all-words task."
        },
        {
            "heading": "6.1 Lexical Sample Task",
            "text": "With the optimal configurations from Senseval-2, we tested the systems on Senseval-3 data. Table 5 shows our F-measure result compared to some of the best reported systems. Although SVM with topic features shows limited success with only a0.6% improvement, the Bayesian network approach has again demonstrated a good improvement of3.8% over its baseline and is better than previous reported best systems except ASO(Ando, 2006).\nBayes (Soft Tag) 73.6 SVM-topic 73.0 SVM baseline 72.4 NB baseline 69.8 ASO(Ando, 2006) 74.1 SVM-LSA (Strapparava et al., 2004) 73.3 Senseval-3 Best System(Grozea, 2004)72.9"
        },
        {
            "heading": "6.2 All-words Task",
            "text": "The F-measure micro-averaged result for our systems as well as previous best systems for Senseval-2 and Senseval-3 all-words task are shown in Table 6 and Table 7 respectively. Bayesian network with soft tagging achieved2.6% improvement over its baseline in Senseval-2 and1.7% in Senseval-3. The results also rival some previous best systems, except for SMUaw (Mihalcea, 2002) which used additional labeled data.\nBayes (Soft Tag) 66.3 NB baseline 63.7 SMUaw (Mihalcea, 2002) 69.0 Simil-Prime (Kohomban and Lee, 2005)66.4 Senseval-2 Best System 63.6 (CNTS-Antwerp (Hoste et al., 2001))\nBayes (Soft Tag) 66.1 NB baseline 64.6 Simil-Prime (Kohomban and Lee, 2005) 66.1 Senseval-3 Best System 65.2 (GAMBL-AW-S(Decadt et al., 2004)) Senseval-32nd Best System (SenseLearner64.6 (Mihalcea and Faruque, 2004))"
        },
        {
            "heading": "6.3 Significance of Results",
            "text": "We perform the\u03c72-test, using the Bayesian network and its na\u0308\u0131ve Bayes baseline (NB baseline) as pairs,\nto verify the significance of these results. The result is reported in Table 8. The results are significant at 90% confidence level, except for the Senseval-3 allwords task.\nTable 8: P value for\u03c72-test significance levels of results."
        },
        {
            "heading": "6.4 SVM with Topic Features",
            "text": "The results on lexical sample task show that SVM benefits less from the topic feature than the Bayesian approach. One possible reason is that SVM baseline is able to use all bag-of-words from surrounding context while na\u0308\u0131ve Bayes baseline can only use very few without decreasing its accuracy, due to the sparse representation. In this sense, SVM baseline already captures some of the topical information, leaving a smaller room for improvement. In fact, if we exclude the bag-of-words feature from the SVM baseline and add in the topic features, we are able to achieve almost the same accuracy as we did with both features included, as shown in Table 9. This further shows that the topic feature is a better representation of global context than the bag-of-words feature.\nTable 9: Results on Senseval-3 English lexical sample task"
        },
        {
            "heading": "6.5 Results on Different Parts-of-Speech",
            "text": "We analyse the result obtained on Senseval-3 English lexical sample task (using Senseval-2 optimal\nconfiguration) according to the test instance\u2019s part-\nof-speech, which includes noun, verb and adjec-\ntive, compared to the na\u0131\u0308ve Bayes baseline. Table 10 shows the relative improvement on each partof-speech. The second column shows the number of testing instances belonging to the particular partof-speech. The third and fourth column shows the\naccuracy achieved by na\u0131\u0308ve Bayes baseline and the Bayesian network. Adjectives show no improvement while verbs show a moderate+2.2% improvement. Nouns clearly benefit from topical information much more than the other two parts-of-speech, obtaining a+5.7% increase over its baseline."
        },
        {
            "heading": "6.6 Sensitivity to L and K",
            "text": "We tested on Senseval-2 all-words task using different L and K. Figure 4 is the result."
        },
        {
            "heading": "6.7 Results on SemEval-1",
            "text": "We participated in SemEval-1 English coarsegrained all-words task (task 7), English fine-grained all-words task (task 17, subtask 3) and English coarse-grained lexical sample task (task 17, subtask 1), using the method described in this paper. For all-words task, we use Senseval-2 and Senseval-3\nall-words task data as our validation set to fine tune the parameters. For lexical sample task, we use the training data provided as the validation set.\nWe achieved 88.7%, 81.6% and 57.6% for coarsegrained lexical sample task, coarse-grained allwords task and fine-grained all-words task respectively. The results ranked first, second and fourth in the three tasks respectively."
        },
        {
            "heading": "7 Conclusion and Future Work",
            "text": "In this paper, we showed that by using LDA algorithm on bag-of-words feature, one can utilise more topical information and boost the classifiers accuracy on both English lexical sample and all-words task. Only unlabeled data is needed for this improvement. It would be interesting to see how the feature can help on WSD of other languages and other natural language processing tasks such as named-entity recognition."
        }
    ],
    "title": "Improving Word Sense Disambiguation Using Topic Features",
    "year": 2007
}