{
    "abstractText": "Modeling and simulations of pandemic dynamics play an essential role in understanding and addressing the spreading of highly infectious diseases such as COVID-19. In this work, we propose a novel deep learning architecture named Attention-based Multiresolution Graph Neural Networks (ATMGNN) that learns to combine the spatial graph information, i.e. geographical data, with the temporal information, i.e. timeseries data of number of COVID-19 cases, to predict the future dynamics of the pandemic. The key innovation is that our method can capture the multiscale structures of the spatial graph via a learning to cluster algorithm in a data-driven manner. This allows our architecture to learn to pick up either local or global signals of a pandemic, and model both the long-range spatial and temporal dependencies. Importantly, we collected and assembled a new dataset for New Zealand. We established a comprehensive benchmark of statistical methods, temporal architectures, graph neural networks along with our spatio-temporal model. We also incorporated socioeconomic cross-sectional data to further enhance our prediction. Our proposed model have shown highly robust predictions and outperformed all other baselines in various metrics for our new dataset of New Zealand along with existing datasets of England, France, Italy and Spain. For a future work, we plan to extend our work for real-time prediction and global scale. Our data and source code are publicly available at https://github.com/HySonLab/pandemic_tgnn.",
    "authors": [
        {
            "affiliations": [],
            "name": "Viet Bach Nguyen"
        },
        {
            "affiliations": [],
            "name": "Truong Son Hy"
        },
        {
            "affiliations": [],
            "name": "Long Tran-Thanh"
        },
        {
            "affiliations": [],
            "name": "Nhung Nghiem"
        }
    ],
    "id": "SP:c0a2b8a26c17c52047aaabba8411b62d8f39a18f",
    "references": [
        {
            "authors": [
                "Wei",
                "Y. Y"
            ],
            "title": "Fitting and forecasting the trend of COVID-19 by SEIR(+CAQ) dynamic model",
            "venue": "Za Zhi 41,",
            "year": 2020
        },
        {
            "authors": [
                "Poonia",
                "R. C"
            ],
            "title": "An enhanced SEIR model for prediction of COVID-19 with vaccination effect",
            "venue": "Life (Basel) 12,",
            "year": 2022
        },
        {
            "authors": [
                "S Hendy"
            ],
            "title": "Mathematical modelling to inform new zealand\u2019s covid-19 response",
            "venue": "J. Royal Soc. New Zealand 51,",
            "year": 2021
        },
        {
            "authors": [
                "W.C. Roda",
                "M.B. Varughese",
                "D. Han",
                "M.Y. Li"
            ],
            "title": "Why is it difficult to accurately predict the COVID-19 epidemic? Infect",
            "venue": "Dis. Model",
            "year": 2020
        },
        {
            "authors": [
                "T. Kufel"
            ],
            "title": "Arima-based forecasting of the dynamics of confirmed covid-19 cases for selected european countries",
            "venue": "Equilibrium. Q. J. Econ. Econ. Policy 15,",
            "year": 2020
        },
        {
            "authors": [
                "ArunKumar",
                "K.E. et al. Forecasting the dynamics of cumulative COVID-19 cases (confirmed",
                "recovered",
                "deaths) for top-16 countries using statistical machine learning models"
            ],
            "title": "Auto-Regressive integrated moving average (ARIMA) and seasonal Auto-Regressive integrated moving average (SARIMA)",
            "venue": "Appl. Soft Comput. 103, 107161",
            "year": 2021
        },
        {
            "authors": [
                "N. Kumar",
                "S. Susan"
            ],
            "title": "COVID-19 pandemic prediction using time series forecasting models",
            "venue": "In 2020 11th International Conference on Computing, Communication and Networking Technologies (ICCCNT),",
            "year": 2020
        },
        {
            "authors": [
                "R.C. Staudemeyer",
                "E.R. Morris"
            ],
            "title": "Understanding lstm \u2013 a tutorial into long short-term memory recurrent neural networks, DOI: 10.48550/ARXIV.1909.09586 (2019)",
            "year": 2019
        },
        {
            "authors": [
                "R. Chandra",
                "A. Jain",
                "D. Singh Chauhan"
            ],
            "title": "Deep learning via lstm models for covid-19 infection forecasting in india",
            "venue": "PLOS ONE 17,",
            "year": 2022
        },
        {
            "authors": [
                "B. Lucas",
                "B. Vahedi",
                "M. Karimzadeh"
            ],
            "title": "A spatiotemporal machine learning approach to forecasting covid-19 incidence at the county level in the usa",
            "venue": "Int. J. Data Sci. Anal. DOI:",
            "year": 2022
        },
        {
            "authors": [
                "Zhou",
                "J. et al. Graph neural networks"
            ],
            "title": "A review of methods and applications",
            "venue": "AI Open 1, 57\u201381, DOI: https://doi.org/10. 1016/j.aiopen.2021.01.001",
            "year": 2020
        },
        {
            "authors": [
                "G. Panagopoulos",
                "G. Nikolentzos",
                "M. Vazirgiannis"
            ],
            "title": "Transfer graph neural networks for pandemic forecasting",
            "venue": "Proc. AAAI Conf. on Artif. Intell",
            "year": 2021
        },
        {
            "authors": [
                "T.S. Hy",
                "R. Kondor"
            ],
            "title": "Multiresolution equivariant graph variational autoencoder, DOI: 10.48550/ARXIV.2106.00967 (2021)",
            "year": 2021
        },
        {
            "authors": [
                "T.S. Hy",
                "V.B. Nguyen",
                "L. Tran-Thanh",
                "R. Kondor"
            ],
            "title": "Temporal multiresolution graph neural networks for epidemic prediction",
            "venue": "Proceedings of the 1st Workshop on Healthcare AI and COVID-19, ICML 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Cheng",
                "C. et al. The incubation period of covid-19"
            ],
            "title": "a global meta-analysis of 53 studies and a chinese observation study of 11 545 patients",
            "venue": "Infect. Dis. Poverty 10, 119, DOI: 10.1186/s40249-021-00901-9",
            "year": 2021
        },
        {
            "authors": [
                "J. Wolfe"
            ],
            "title": "From \u2018zero\u2019 to surge",
            "venue": "The New York Times",
            "year": 2022
        },
        {
            "authors": [
                "G. Kaur",
                "P. Kaur",
                "N. Kaur"
            ],
            "title": "Forecasting prediction of covid-19 outbreak using linear regression",
            "venue": "Data Intelligence and Cognitive Informatics,",
            "year": 2023
        },
        {
            "authors": [
                "S. Ketu",
                "P.K. Mishra"
            ],
            "title": "Enhanced gaussian process regression-based forecasting model for COVID-19 outbreak and significance of IoT for its detection",
            "venue": "Appl. Intell",
            "year": 2021
        },
        {
            "authors": [
                "J. Galasso",
                "D.M. Cao",
                "R. Hochberg"
            ],
            "title": "A random forest model for forecasting regional COVID-19 cases utilizing reproduction number estimates and demographic data",
            "venue": "Chaos Solitons Fractals",
            "year": 2022
        },
        {
            "authors": [
                "Fang",
                "Z.-G.",
                "Yang",
                "S.-Q.",
                "Lv",
                "C.-X.",
                "An",
                "S.-Y.",
                "Wu",
                "W. Application of a data-driven XGBoost model for the prediction of COVID-19 in the USA"
            ],
            "title": "a time-series study",
            "venue": "BMJ Open 12, e056685",
            "year": 2022
        },
        {
            "authors": [
                "Mahmud",
                "S. Bangladesh covid-19 daily cases time series analysis using facebook prophet model. PSN"
            ],
            "title": "Dis",
            "venue": "& Illn. (Topic)",
            "year": 2020
        },
        {
            "authors": [
                "J. Gilmer",
                "S.S. Schoenholz",
                "P.F. Riley",
                "O. Vinyals",
                "G.E. Dahl"
            ],
            "title": "Neural message passing for quantum chemistry",
            "venue": "Proceedings of the 34th International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "T.S. Hy",
                "R. Kondor"
            ],
            "title": "Multiresolution equivariant graph variational autoencoder",
            "venue": "Mach. Learn. Sci. Technol",
            "year": 2023
        },
        {
            "authors": [
                "Paszke",
                "A. et al. Pytorch"
            ],
            "title": "An imperative style, high-performance deep learning library",
            "venue": "In Wallach, H. et al. (eds.) Advances in Neural Information Processing Systems, vol. 32",
            "year": 2019
        },
        {
            "authors": [
                "M. Fey",
                "J.E. Lenssen"
            ],
            "title": "Fast graph representation learning with PyTorch Geometric",
            "venue": "In ICLR Workshop on Representation Learning on Graphs and Manifolds",
            "year": 2019
        },
        {
            "authors": [
                "Xia",
                "J. et al. Mole-BERT"
            ],
            "title": "Rethinking pre-training graph neural networks for molecules",
            "venue": "In The Eleventh International Conference on Learning Representations",
            "year": 2023
        },
        {
            "authors": [
                "Y. Li",
                "J. Zhou",
                "S. Verma",
                "F. Chen"
            ],
            "title": "A survey of explainable graph neural networks: Taxonomy and evaluation metrics",
            "year": 2022
        },
        {
            "authors": [
                "L Cuesta-Herrera"
            ],
            "title": "Analysis of seir-type models used at the beginning of covid-19 pandemic reported in high-impact journals",
            "venue": "Medwave",
            "year": 2022
        },
        {
            "authors": [
                "M. Yousaf",
                "S. Zahir",
                "M. Riaz",
                "S.M. Hussain",
                "K. Shah"
            ],
            "title": "Statistical analysis of forecasting covid-19 for upcoming month in pakistan",
            "year": 2020
        },
        {
            "authors": [
                "R. Rodiah",
                "E. Patriya",
                "D.T. Susetianingtias",
                "E. Sutanty"
            ],
            "title": "Implementation of the prophet model in covid-19 cases forecast",
            "venue": "ILKOM J. Ilmiah",
            "year": 2022
        },
        {
            "authors": [
                "G. Battineni",
                "N. Chintalapudi",
                "F. Amenta"
            ],
            "title": "Forecasting of covid-19 epidemic size in four high hitting nations (usa, brazil, india and russia) by fb-prophet machine learning model",
            "venue": "Appl. Comput. Informatics",
            "year": 2020
        },
        {
            "authors": [
                "A.K. Gupta",
                "V. Singh",
                "P. Mathur",
                "C.M. Travieso-Gonzalez"
            ],
            "title": "Prediction of covid-19 pandemic measuring criteria using support vector machine, prophet and linear regression models in indian scenario",
            "venue": "J. Interdiscip. Math. 24,",
            "year": 2021
        },
        {
            "authors": [
                "S. Wang",
                "C. Li",
                "A. Lim"
            ],
            "title": "Why are the arima and sarima not sufficient, DOI: 10.48550/ARXIV.1904.07632 (2019)",
            "year": 2019
        },
        {
            "authors": [
                "R.J. Hyndman"
            ],
            "title": "A brief history of forecasting competitions",
            "venue": "Int. J. Forecast",
            "year": 2020
        },
        {
            "authors": [
                "K. Hornik",
                "M.B. Stinchcombe",
                "H.L. White"
            ],
            "title": "Multilayer feedforward networks are universal approximators",
            "venue": "Neural Networks",
            "year": 1989
        },
        {
            "authors": [
                "H.R. Niazkar",
                "M. Niazkar"
            ],
            "title": "Application of artificial neural networks to predict the covid-19 outbreak",
            "venue": "Glob. Heal. Res. Policy",
            "year": 2020
        },
        {
            "authors": [
                "H.I. Fawaz",
                "G. Forestier",
                "J. Weber",
                "L. Idoumghar",
                "Muller",
                "P.-A. Deep learning for time series classification"
            ],
            "title": "a review",
            "venue": "Data Min. Knowl. Discov. 33, 917\u2013963, DOI: 10.1007/s10618-019-00619-1",
            "year": 2019
        },
        {
            "authors": [
                "Y. Yu",
                "X. Si",
                "C. Hu",
                "Zhang",
                "J. A review of recurrent neural networks"
            ],
            "title": "Lstm cells and network architectures",
            "venue": "Neural Comput. 31, 1235\u20131270, DOI: 10.1162/neco_a_01199",
            "year": 2019
        },
        {
            "authors": [
                "V.K.R. Chimmula",
                "L. Zhang"
            ],
            "title": "Time series forecasting of covid-19 transmission in canada using lstm networks",
            "year": 2020
        },
        {
            "authors": [
                "B. Nikparvar",
                "M.M. Rahman",
                "F. Hatami",
                "Thill",
                "J.-C. Spatio-temporal prediction of the covid-19 pandemic in us counties"
            ],
            "title": "modeling with a deep lstm neural network",
            "venue": "Sci. Reports 11",
            "year": 2021
        },
        {
            "authors": [
                "J. Luo",
                "Z. Zhang",
                "Y. Fu",
                "F. Rao"
            ],
            "title": "Time series prediction of covid-19 transmission in america using lstm and xgboost algorithms",
            "venue": "Results Phys",
            "year": 2021
        },
        {
            "authors": [
                "S. Shastri",
                "K. Singh",
                "S. Kumar",
                "P. Kour",
                "Mansotra",
                "V. Time series forecasting of covid-19 using deep learning models"
            ],
            "title": "India-usa comparative case study",
            "venue": "Chaos, Solitons, Fractals 140, 110227 \u2013 110227",
            "year": 2020
        },
        {
            "authors": [
                "M.M. Organero",
                "P. Queipo-\u00c1lvarez"
            ],
            "title": "Deep spatiotemporal model for covid-19 forecasting",
            "venue": "Sensors (Basel, Switzerland)",
            "year": 2022
        },
        {
            "authors": [
                "F. Scarselli",
                "M. Gori",
                "A.C. Tsoi",
                "M. Hagenbuchner",
                "G. Monfardini"
            ],
            "title": "The graph neural network model",
            "venue": "IEEE Transactions on Neural Networks",
            "year": 2009
        },
        {
            "authors": [
                "M. Niepert",
                "M. Ahmed",
                "K. Kutzkov"
            ],
            "title": "Learning convolutional neural networks for graphs",
            "venue": "In Proceedings of The 33rd International Conference on Machine Learning,",
            "year": 2014
        },
        {
            "authors": [
                "Y. Li",
                "R. Zemel",
                "M. Brockschmidt",
                "D. Tarlow"
            ],
            "title": "Gated graph sequence neural networks",
            "venue": "In Proceedings of ICLR\u201916",
            "year": 2016
        },
        {
            "authors": [
                "P. Battaglia",
                "R. Pascanu",
                "M. Lai",
                "D. Jimenez Rezende",
                "kavukcuoglu"
            ],
            "title": "Interaction networks for learning about objects, relations and physics",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2016
        },
        {
            "authors": [
                "Duvenaud",
                "D. K"
            ],
            "title": "Convolutional networks on graphs for learning molecular fingerprints",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2015
        },
        {
            "authors": [
                "S. Kearnes",
                "K. McCloskey",
                "M. Berndl",
                "V. Pande",
                "Riley",
                "P. Molecular graph convolutions"
            ],
            "title": "moving beyond fingerprints",
            "venue": "J. Comput. Mol. Des. 30, 595\u2013608, DOI: 10.1007/s10822-016-9938-8",
            "year": 2016
        },
        {
            "authors": [
                "Hy",
                "T. S"
            ],
            "title": "Predicting molecular properties with covariant compositional networks",
            "venue": "The J. Chem. Phys",
            "year": 2018
        },
        {
            "authors": [
                "T.S. Hy",
                "S. Trivedi",
                "H. Pan",
                "B.M. Anderson",
                "R. Kondor"
            ],
            "title": "Covariant compositional networks for learning graphs",
            "venue": "In Proc. International Workshop on Mining and Learning with Graphs (MLG)",
            "year": 2019
        },
        {
            "authors": [
                "A. Fout",
                "J. Byrd",
                "B. Shariat",
                "A. Ben-Hur"
            ],
            "title": "Protein interface prediction using graph convolutional networks",
            "venue": "In Proceedings of the 31st International Conference on Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "E Rossi"
            ],
            "title": "Temporal graph networks for deep learning on dynamic graphs",
            "venue": "arXiv preprint arXiv:2006.10637",
            "year": 2020
        },
        {
            "authors": [
                "Z Diao"
            ],
            "title": "Dynamic spatial-temporal graph convolutional neural networks for traffic forecasting",
            "venue": "Proc. AAAI Conf. on Artif. Intell",
            "year": 2019
        },
        {
            "authors": [
                "Y. Li",
                "R. Yu",
                "C. Shahabi",
                "Liu",
                "Y. Diffusion convolutional recurrent neural network"
            ],
            "title": "Data-driven traffic forecasting",
            "venue": "In International Conference on Learning Representations",
            "year": 2018
        },
        {
            "authors": [
                "D.T. Nguyen",
                "M.D.T. Nguyen",
                "T.S. Hy",
                "R. Kondor"
            ],
            "title": "Fast temporal wavelet graph neural networks",
            "venue": "arXiv preprint",
            "year": 2023
        },
        {
            "authors": [
                "S. Hochreiter",
                "J. Schmidhuber"
            ],
            "title": "Long short-term memory",
            "venue": "Neural Comput",
            "year": 1997
        },
        {
            "authors": [
                "K Cho"
            ],
            "title": "Learning phrase representations using RNN encoder\u2013decoder for statistical machine translation",
            "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2014
        },
        {
            "authors": [
                "J. Devlin",
                "Chang",
                "M.-W.",
                "K. Lee",
                "Toutanova",
                "K. BERT"
            ],
            "title": "Pre-training of deep bidirectional transformers for language understanding",
            "venue": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 4171\u20134186, DOI: 10.18653/v1/N19-1423",
            "year": 2019
        },
        {
            "authors": [
                "Dosovitskiy",
                "A. et al. An image is worth 16x16 words"
            ],
            "title": "Transformers for image recognition at scale",
            "venue": "In International Conference on Learning Representations",
            "year": 2021
        },
        {
            "authors": [
                "A Vaswani"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "T.N. Kipf",
                "M. Welling"
            ],
            "title": "Semi-supervised classification with graph convolutional networks",
            "venue": "In International Conference on Learning Representations",
            "year": 2017
        },
        {
            "authors": [
                "Gumbel",
                "E.J. Statistical theory of extreme values",
                "some practical applications"
            ],
            "title": "a series of lectures",
            "venue": "US Govt. Print. Off. Number 33",
            "year": 1954
        },
        {
            "authors": [
                "E. Jang",
                "S. Gu",
                "B. Poole"
            ],
            "title": "Categorical reparameterization with gumbel-softmax",
            "venue": "ICLR",
            "year": 2017
        },
        {
            "authors": [
                "Wes McKinney"
            ],
            "title": "Data Structures for Statistical Computing in Python",
            "venue": "Proceedings of the 9th Python in Science Conference,",
            "year": 2010
        },
        {
            "authors": [
                "Harris",
                "C. R"
            ],
            "title": "Array programming with NumPy",
            "venue": "Nature 585,",
            "year": 2020
        },
        {
            "authors": [
                "A.A. Hagberg",
                "D.A. Schult",
                "P.J. Swart"
            ],
            "title": "Exploring network structure, dynamics, and function using networkx",
            "venue": "Proceedings of the 7th Python in Science Conference,",
            "year": 2008
        }
    ],
    "sections": [
        {
            "text": "Modeling and simulations of pandemic dynamics play an essential role in understanding and addressing the spreading of highly infectious diseases such as COVID-19. In this work, we propose a novel deep learning architecture named Attention-based Multiresolution Graph Neural Networks (ATMGNN) that learns to combine the spatial graph information, i.e. geographical data, with the temporal information, i.e. timeseries data of number of COVID-19 cases, to predict the future dynamics of the pandemic. The key innovation is that our method can capture the multiscale structures of the spatial graph via a learning to cluster algorithm in a data-driven manner. This allows our architecture to learn to pick up either local or global signals of a pandemic, and model both the long-range spatial and temporal dependencies. Importantly, we collected and assembled a new dataset for New Zealand. We established a comprehensive benchmark of statistical methods, temporal architectures, graph neural networks along with our spatio-temporal model. We also incorporated socioeconomic cross-sectional data to further enhance our prediction. Our proposed model have shown highly robust predictions and outperformed all other baselines in various metrics for our new dataset of New Zealand along with existing datasets of England, France, Italy and Spain. For a future work, we plan to extend our work for real-time prediction and global scale. Our data and source code are publicly available at https://github.com/HySonLab/pandemic_tgnn."
        },
        {
            "heading": "1 Introduction",
            "text": "The Coronavirus Disease started in 2019 (COVID-19) has been and currently is a major global pandemic, challenging every country\u2019s population and public health systems. As a fairly water-isolated island country, New Zealand mostly contained the spread of COVID-19 until early 2022, when infection cases surged to more than 2 million confirmed cases by the end of the year (WHO data, https://covid19.who.int/region/wpro/country/nz). While New Zealand responded promptly, contained and effectively vaccinated the population to keep the case number low, the sudden rise in infections posed certain challenges to the healthcare system.\nIn the wake of the spread of COVID-19, many epidemiological modeling and prediction models emerged, seeking to project the progression of the pandemic and inform public health authorities to take measures when appropriate. Traditional dynamics case prediction models, including the family of Susceptible, Infectious, or Recovered models (SEIR) and their variants, have been widely applied to simulate the trajectories of the pandemic1\u20133. SEIR models are hard to use due to difficulty in estimating parameters and underlying nonlinear systems of ordinary differential equations, alongside embedding restricted assumptions4. Other statistical prediction methods have been used, among which are the autoregressive integrated moving average (ARIMA) models5, 6 and the time-series prediction Prophet model7. However, linear statistical models cannot capture the non-linear nature of disease infection progression, especially in the case of New Zealand infection growth patterns. To model non-linear disease growth functions, artificial neural networks and deep learning models have been developed and trained to predict the infection case time series of each health area. The most common types of deep learning models for epidemic modeling are Long Short-Term Memory (LSTM)-based models, in which the architecture is specially designed to learn and represent historical or temporal information8. LSTM-based forecasting models can more accurately predict the number of cases and capture non-linear non-monotonous patterns in case data9, but otherwise cannot exploit crucial spatial or geographical information for predicting the spread of COVID-19 over multiple areas.\nIt is shown that incorporating geospatial information, including but not limited to movement and connectedness information, helps with the forecasting performance of LSTM-based and deep learning model10. One of the classes of deep learning models that can seamlessly embed geospatial information is Graph Neural Networks (GNNs), neural network deep learning models\nar X\niv :2\n30 5.\n07 73\n1v 1\n[ cs\n.L G\n] 1\n2 M\nay 2\nthat can capture topological information in graph- and network-based data11. Following in the footstep of previous efforts at COVID-19 forecasting with GNNs and spatial disease features12, we propose improved spatiotemporal graph neural network models that can accurately learn and forecast COVID-19 case progression in New Zealand. To this end, we gathered and reformated New Zealand COVID-19 case data, and constructed day-to-day disease graphs based on geographical information; graph disease representations are then fed to a hierarchical, multi-resolution temporal graph neural network model that can automatically group multiple disease areas to learn large-scale disease properties13, 14. The Results section 2 contains a demonstration of the graph-based New Zealand COVID-19 data format, various disease spread progression information, a performance comparison between different types of forecasting models, and an evaluation of the models\u2019 predictive capabilities. The Discussion section 3 discusses the implications of the results on practical applications, as well as potential improvements and precautions. The Related works section 4 presents a literature summary of the most relevant and up-to-date prior works related to our current work.The Methods section 5 provides details on the construction and mechanisms of the experimental epidemiological forecasting models."
        },
        {
            "heading": "2 Results",
            "text": ""
        },
        {
            "heading": "2.1 Descriptive data analysis",
            "text": "The primary component of the dataset is the number of new cases in each region of the 20 district health boards in New Zealand (confirmed localized cases or general quarantine/unknown cases). Preliminary data analysis shows that there are significant variations between the different district health boards in terms of the volume of new cases quantified by mean and standard deviation, but the progression patterns are similar between different boards (Figure 3). Spikes in new case counts are typically 5-7 days apart, indicative of either the disease incubation period16 or spaced window flaw in data collection. Moreover, spikes between different regions generally coincide (Figure 3), with the highest spikes and surges across the entire sampled time range occurring in March 2022. We confirmed that the case surges demonstrated by the dataset are consistent with existing pandemic reports during the same time frames17."
        },
        {
            "heading": "2.2 Experiment task description",
            "text": "We comprehensively evaluate the forecasting effectiveness of the models in short-, mid-, and long-term prediction windows. The models are trained and assessed on their predictions 3, 7, 14, and 21 days from the input data. Data from day 1 to day T is used to train one model at a time, and then predictions are obtained from the model from day T +1 to day T +d, where\n2/21\n3/21\nd is the prediction window size and 0 < d < 22. Note that each model within a single class of models is trained separately and specifically for a single fixed time window. In other words, two different models are trained to predict days T +a and T +b, where a,b > 0 and a 6= b. The size of the training set gradually increases as time progresses, and for each value of T the best model is identified via a validation set with no overlapping day with the test set. We trained and validated the models on the time series data from March 4th, 2022 to September 4th, 2022, and performed further model evaluations to examine generalization performance on an out-of-distribution starting from September 4th, 2022 to November 4th, 2022. Additionally, we ran experiments on all previous EU datasets 1-to-1 as mentioned in a previous study12 to further test the effectiveness of baseline models as well as more succinctly comparing graph spatiotemporal models to alternatives in a variety of disease settings."
        },
        {
            "heading": "2.3 Baselines and Comparisons",
            "text": "Several state-of-the-art COVID-19 forecasting models can be implemented as baseline models to measure the relative performance of our proposal models. Therefore, we compare the different spatio-temporal models with traditional statistical prediction and neural network-based regression models that have been recently applied to the problem of COVID-19 forecasting. Note that models that work with recovery, deaths, and policies such as SEIR are omitted since the dataset only provides the number of confirmed cases.\nSimple statistical models The class of most rudimentary statistical models for forecasting. The models examined include (1) AVG: The average number of cases for one region up to the time of the test day (e.g., the prediction for day 13 is based on\n4/21\nthe average number of cases of the last 12 days); (2) AVG_WINDOW: The average number of cases in the past d-day window for one region (e.g., for d = 7, the prediction for day 13 is based on the average number of cases of the last 7 days, from day 6 onwards); and (3) LAST_DAY: The prediction for the next day in one region is the same as the number of cases on the previous day.\nTraditional machine learning models The input format for all models in this class is the case history up to the prediction date of each district health board. The models examined include (4) LIN_REG18: Ordinary least squares Linear Regression, which fit a line onto seen training samples to predict the number of future cases (linear approximation); (5) GP_REG19: Gaussian Process Regressor, a non-parametric based regression model commonly used in predictive analysis that implements Gaussian processes; (6) RAND_FOREST20: A random forest regression model that produces case predictions using decision trees, with multiple trees built based on training case data to best average the final results; and (7) XGBOOST21: An improved version of the random forest regression model using gradient boosting.\nParameterized regression time-series forecasting models The class of linear regression models with specific components represented as parameters. The models examined include (8) PROPHET22: A forecasting model for various types of time series that has also seen extensive use in forecasting COVID-19 where the input is the entire time-series historical number of cases of one region up to before the testing day; (9) ARIMA5: A simple autoregressive moving average model, which the input is similar to PROPHET; and (10) LSTM9: A two-layer bidirectional LSTM model that takes as input the sequence of new cases in a region for the last 7 days, popular for the forecasting task capable of state-of-the-art performance.\nGraph neural network-based models The proposal graph models to be compared to the previous baseline models, with and without temporal components. The models examined include (11) MPNN23: Message-passing neural network model with separate layers for each day in the case time series; (12) MGNN14, 24: Message-passing neural network model similar to MPNN, but with multiple graph resolutions and learned clustering of different regions; (13) MPNN + LSTM: Message-passing neural network model with long-short term memory neural time series model; and (14) ATMGNN: Multiresolution graph model based on the MGNN model combined with Transformers for modeling time series. All models in this category are described in detail in Section 5."
        },
        {
            "heading": "2.4 Experimental setup",
            "text": "We detail the hyperparameters setup of the deep learning prediction models in our experiments. For all graph-based models (MPNN, MPNN+LSTM, ATMGNN), training lasts for a maximum of 300 epochs with early stopping patience of 50 epochs, and early stopping is only enabled from epoch 100th onward. Models are trained with the Adam optimizer (lr = 10\u22123), batch size 128. For the neighborhood aggregation layers of the graph models, batch normalization is applied to the output of all layers with dropout applied to 0.5 times the total number of nodes. The LSTM component of the MPNN+LSTM model is\n5/21\nimplemented with a hidden state size of 64. The multiresolution component of the ATMGNN is configured for two additional coarsening layers with 10- and 5-node clusters, respectively; self-attention is configured with a single head for all regions. The models with the lowest validation loss at each prediction day shift are saved as parameter checkpoints for the sake of further evaluation, and validation information is outputted for further examination. All models are implemented with PyTorch25 and PyTorch geometric26."
        },
        {
            "heading": "2.5 Evaluation metrics",
            "text": "We measured the performance of all models with three evaluation metrics: Mean absolute error (MAE), root mean squared error (RMSE), and the coefficient of determination (R2-score).\nMean Absolute Error (MAE) The metric measures the sum of the absolute differences between the predicted case count and the actual case count. MAE cannot indicate the degree of under-/over-prediction since all variations have equal weight. The formula to calculate MAE is as follows:\nMAE = 1 N\nN\n\u2211 i=1 |y\u0302i\u2212 yi|\nwhere y\u0302i represents the forecast case count, yi represents the ground truth case count, N represents the total number of days in the case count time series, and i represents the case count statistic of a single day in the time series.\nRoot Mean Squared Error (RMSE) The metric measures the square root of the average squared deviation between the forecast and the actual case count. RMSE is a good measure of prediction accuracy, mostly used when the error is highly nonlinear. The formula to calculate RMSE is as follows:\nRMSE =\n\u221a N\n\u2211 i=1 (y\u0302i\u2212 yi)2 N\nCoefficient of Determination (R2) The metric represents the proportion of variance of the case count that has been explained by the independent variables in the model. R2 indicates goodness of fit and measures how well unseen samples are likely to be predicted by the model (through the proportion of explained variance). The formula to calculate R2-score is as follows:\nR2 = 1\u2212 \u2211 N i=1(y\u0302\u2212 yi)2\n\u2211Ni=1(y\u0304\u2212 yi)2\nwhere y\u0304 = 1N \u2211 N i=1 yi, or the average of the ground truth time series. The best possible score is 1.0, and the score can be negative (i.e., the model can arbitrarily badly fit the case count time series). A constant model that always predicts the average number of cases over the entire periods with no respect to the inputs would get an R2 score of 0.0.\n6/21"
        },
        {
            "heading": "2.6 Observations",
            "text": "Performance measurement Table 1 details the performances of all experimental models in the benchmark study. Across the board, MPNN+LSTM is the highest-performing model, with relatively low mean error and root means square error, alongside accurate trend prediction at R2-score consistently over 0.8. Other baseline methods performed inconsistently across different time ranges, with massive fluctuations in heuristic statistical methods (AVG, AVG_WINDOW, and especially LAST_DAY), owning to these baselines simply forecasting based on rudimentary statistics of the data. However, in near-future time prediction windows (from 1-7 days), simple statistical methods can be competitive compared to more complex models; nevertheless, our goal is to eliminate or mitigate performance decay during long-term predictions. The class of traditional machine learning models performed reasonably well, with tree-based methods RAND_FOREST and XGBOOST outperforming simple statistical methods and parameterized models aside from LSTM. In New Zealand from 14 days prediction length onwards, graph-based temporal models on average see a 20.31% and 25.37% relative reduction in MAE and RMSE, respectively; while correlation metric R2 relatively improving 9.43%. Similar results are obtained from cross-examining Italy and England COVID datasets, with graph-based temporal models (e.g., MPNN+LSTM, ATMGNN) generally outperforming other baseline models and LSTM models coming second on all metrics. The exception to performance patterns across countries is the overall performance of traditional machine learning models, which perform inconsistently across different countries. France and Spain\u2019s tables are included in the Appendix.\nPerformance decay over long forecasting windows Across all models, the AVG model performed the worst when it comes to performance decay relative to the length of forecasting windows, concerning both absolute error and correlation metrics. On the other hand, the two other heuristic statistical methods, AVG_WINDOW and LAST_DAY, outperformed regression-based methods ARIMA and PROPHET with respect to the rate of decay and error increment over longer forecasting windows (Figure 5 and Table 2). We observed anomalies in the results of our LSTM implementation: both the model\u2019s absolute error and correlation score R2 do not decay over time, but rather improved over the long run (Figure 5); this phenomenon may be explained considering LSTM-based models performed better in certain long time ranges, or the models have certain \"sweet spots\". Traditional machine learning models that implement tree-based learning, including RAND_FOREST and XGBOOST, performed reasonably well in terms of performance decay, with lower error increase rate and R2 decrease rate than most other models aside from graph-based temporal models and ARIMA. Graph- and temporal-hybrid models MPNN+LSTM and ATMGNN maintained a stable performance decay profile with a low decay rate on both error and correlation compared to every other model aside from the LSTM exception, alongside lower values in both metrics across the board. Both temporal graph models started with relatively high performance in terms of all metrics compared to other baselines and mostly maintained the same performance when predicting longer time ranges with minimal decay, resulting in them outperforming all other baseline models. We specifically demonstrated the relative metric and decay stability of the two graph-based temporal models by averaging over several runs and computing the deviation as in Figure 6, showing the performance and decay similarities between these models over time.\n7/21\n8/21\nOut-of-distribution forecasting We examined the out-of-distribution performances of two of our best-performing models, the MPNN+LSTM and ATMGNN. The evaluation is done on the number of new cases between September 4th, 2022 and November 4th, 2022, with no overlapping between the evaluation set and the train/validation sets. All models are evaluated as autoregressive models, meaning for the 30-day prediction window the models use the prediction output of the previous day as an input feature for predicting the current day. As demonstrated in Figure 7, ATMGNN outperformed MPNN+LSTM when it comes to prediction error and emulating the spiking dynamics of the number of new cases. The predictions retrieved from the outputs of ATMGNN showed that the model can fairly precisely simulate the case spiking dynamics even when tested on a dataset completely unrelated and separate from the training dataset, demonstrating good generalization performance of the model. All other baseline models are not included in the evaluation after extensive testing showing that their performances are not remotely comparable to the two models demonstrated above. Further testing with different information windows with ground truth case information feed into the model ranging between 3 and 9 days before the target day to predict showed that both models maintained similar forecasting patterns with ATMGNN better conforming to the ground truth and the models maintaining relatively stable predictions given different case information levels.\nEconomic graph features To further analyze the role of auxiliary features in graph-based models, we integrated economic features into the two best-performing graph-based forecasting models. Details on the source and construction of these features within the dataset are discussed in Section 5.3. As shown in Figure 6 and Table 5, the two models that include economic features (ECON models) slightly underperformed compared to baseline graph-based temporal models, even with all economic features normalized. The decrease in performance is likely due to these economic features remaining constant throughout all prediction periods, indicating that relative economic allocation between different district health boards do not necessarily add any helpful information. Because of either low baseline performance or incompatibility, such as the economic zones and DHBs were not strictly overlapped as per Figure 1, these economic features were not added to other baseline models. Our sensitivity analysis that removed DHBs with inconsistent economic zones suggested there was a small improvement in model performance as expected (see the Appendix).\nDemographic graph features We additionally tested different modalities of the original graph models, particularly enhancing the input to the model with separate age group data from the original data source, as well as augmenting the output of the models to predict the number of new cases for each age group of each district health board in New Zealand. Through testing, however, outputting each age group separately, adding age group data to the models\u2019 inputs, as well as adding custom demographic\n9/21\nweighting to each age group during training did not improve the models\u2019 performance. For reference, the results of testing demographic-enhanced models are presented in the Appendix."
        },
        {
            "heading": "3 Discussion",
            "text": "Interpretation of the main results We provided a comprehensive evaluation of four classes of COVID-19 forecasting models, with a detailed analysis of the models\u2019 performance, decay over time, out-of-distribution forecasting, and economic features addition. Generally, graph neural network-based models, specifically the temporal variant of graph-based models outperformed every other baseline model in terms of performance metrics and performance decay over time. This trend is not shared by non-temporal graph-based models, indicating the importance of temporal mechanisms in forecasting models, whether it is attention-based on recurrent network-based (i.e., LSTM). Additionally, for far less computation cost, traditional machine learning models outperformed statistical and parameterized regression time-series forecasting models, even remaining competitive with the neural network-based LSTM model. The results suggest that the spatiotemporal approach to modeling the spread of COVID-19 based on the number of new cases is effective compared to other traditional modeling methods. Intuitively, graph-based models can accurately simulate the change in the number of new cases in one region when given that region\u2019s traffic connectivity with its neighbors. Since the spread of COVID-19 in every country, not only in New Zealand, is movement-based in nature, by modeling such geographical connectivity we can find latent information by accounting for human contacts with graph-based models. Moreover, the out-of-distribution performance of multiresolution temporal graph models also demonstrates the utility of modeling the problem of COVID-19 forecasting as a hierarchical system, with spreads localized in adjacent regions that have significant traffic volume.\nStrength Graph-based temporal models can embody the hidden correlations between different district health board regions when forecasting COVID-19. The approach is straightforward with versatility in modeling various connected systems and structures, not only with the task of forecasting disease spread trajectories but also in the task of chemical molecule construction and discrimination27. As indicated by the performance metrics, temporal models perform well compared to other traditional models, predicting with relatively low errors and remaining accurate even when predicting further into the future. The multiresolution setting of graph-based models can also accurately model the disease\u2019s case dynamics to a reasonable degree when facing completely new and separate data from the training set. Additionally, the models\u2019 current implementation relies on comparatively low computational resources, with training and inferencing solely based on an online connection to an instance of Google Colab. Our COVID-19 data were captured during the Omicron waves in New Zealand with a highly vaccinated population. Our data also reflected a unique social experimental event that the New Zealand border was opened in May 2022 after strictly closing for two years (since 25 March 2020).\nLimitations While certain metrics of the proposed models are satisfactory, we have identified several weaknesses of the models that were tested. Graph-based models, while powerful, still require a certain amount of computational resources and adequate time for the process of training the models. Data inputs also have to be well-structured and preprocessed carefully to suit the formatting of the models, though this is less of a concern given the availability and accuracy of case datasets such as the New Zealand COVID-19 public dataset. Furthermore, data features can be further enriched with more detailed movement data between regions, traffic density information for all traveling modalities (e.g., land, sea, or air travel), and local movement details within each region. Most importantly, graph-based networks and deep learning models in general are black-boxes, offering little insight into the precise mechanisms of forecasting and modeling disease dynamics for the sake of studying the exact nature of epidemic spread.\nPolicy implications The out-of-distribution performance and time range availability of up to 30 days show that graph-based multiresolution temporal models can be effectively used in aiding public health policies. With appropriate data processing and extension, the models can be adapted to predicting new cases on coarser temporal resolutions (i.e., weeks or months), potentially becoming a useful tool for epidemic predictive modeling and local or country-level intervention measures simulations.\nFuture research Future directions of research are aplenty, from additional data features and enrichment features readily incorporable into the model as node features (e.g., more fine-grained socioeconomic features) or as edge features (e.g., mobility data), to interpretation methods designed for graph neural networks28 for the sake of understanding the inner workings of such prediction models. Another interesting direction is to comparatively examine the spatial modeling of deep learning models with other dynamical forecasting models, and the correlation influence of each disease feature/parameter on the final prediction output of each type of model.\nConclusions Our study suggested that graph neural network-based models outperformed every other baseline model in terms of performance metrics and performance decay over time. Furthermore, our graph neural network-based models can effectively predict the mumber of COVID-19 cases upto 30 days, and therefore can assist with public health policies planning in order to\n10/21\ncontrol the COVID-19 outbreaks. Finally, our results in terms of model structures and frameworks can be generalized to other countries with similar settings."
        },
        {
            "heading": "4 Related works",
            "text": ""
        },
        {
            "heading": "4.1 Linear and statistical forecasting models",
            "text": "Various traditional statistical and linear models have been employed to forecast the spread of COVID-19 cases. Among these traditional models are the Susceptible, Infectious, or Recovered (SEIR) models, where the dynamics of disease spread is modeled as a function of various population and the interactions between them1\u20133. While mathematical models such as SEIR can estimate the effect of control measures even before the start of the pandemic, these models cannot make accurate predictions due to a lack of data and their inherent assumptions restricting the class of available learnable disease functions4, 29.\nSome other prevalent classes of forecasting methods are the Autoregressive Integrated Moving Average models (ARIMA) and the time-series prediction Prophet model. ARIMA models are well-known for being able to forecast future points in time series data, especially when the mean of the data is non-stationary; evidently, this family of models has been applied numerous times to forecast COVID-19 cases in several countries5, 6, 30. On the other hand, the Meta-developed Prophet model and its variants have also been utilized to tackle the task of predicting the progression of COVID-19 cases with some success, namely in forecasting the number of cases in India31 and generally for any country using day level case information7, 32. Several combinations of the traditional and statistical linear models have been examined33, effectively achieving better performance using compositions of successful statistical time-series models.\nWhile statistical models are proficient at capturing certain COVID-19 case dynamics and are similarly motivated by repeated case patterns shown in pandemic data, these models are linear in nature and incapable of modeling spatial information as well as higher-level disease functions. Among the above models, ARIMA-based models have been proven to be insufficient even in their specialty of modeling time-series data given the complexities of such type of data in certain dimensions, while also underperformed compared to simple empirical methods34, 35."
        },
        {
            "heading": "4.2 Neural networks-based time-series forecasting models",
            "text": "Neural networks are a type of machine learning method that is capable of learning arbitrary non-linear functions underlying the data, making them one of the most powerful general learning algorithms for a wide range of tasks36. The vanilla neural networks without any additional component have been tested as predictors of COVID-19 outbreaks across several countries, owing to their high capacity modeling of disease patterns and functions when certain assumptions (e.g., disease incubation period) are encoded37.\nEven among powerful Artificial Neural Networks (ANNs), there arises a need to explicitly model the temporal nature of certain types of data (i.e., the time-dependent trends of disease outbreaks)38. The most common methods for modeling time-series data, particularly pandemic forecasting input data, are Recurrent Neural Network (RNN) models. RNNs are widely adopted in areas with sequential data; intuitively, RNNs are capable of \"memorizing\" the nature of patterns within time-series data. Most popular within the class of RNNs are Long-Short Term Memory (LSTM) models, a variant of the traditional RNNs that is capable of modeling long-term dependencies, allowing the models to learn information and patterns from distant past39. LSTMs and their variants have been used extensively to forecast COVID-19 case progression, notably in Canada, where the model was tested and modified to accommodate disease-specific information, accurately predicting an exponential surge in the number of cases40. LSTM-based models were also used to simulate and forecast the COVID-19 pandemic in several other countries, either independently or in conjunction with various distinct statistical models incorporating spatial features41\u201343.\nWhile time-series models are effective at modeling the evolution of the disease over time in a single specific geographical region, LSTM/RNN-based models inherently cannot incorporate spatial features without employing heuristics that either explicitly change the architecture or interpolate spatial information into the input data itself44. To address the shifting topology of the natural geographical map and represent the causal relationship between pandemic regions, a more graphical and hierarchical approach is needed."
        },
        {
            "heading": "4.3 Temporal graph neural networks forecasting models",
            "text": "Graph neural networks (GNNs) utilizing various ways of generalizing the concept of convolution to graphs45\u201347 have been widely applied to many learning tasks, including modeling physical systems48, finding molecular representations to estimate quantum chemical computation23, 49\u201352, and protein interface prediction53. One of the most popular types of GNNs is message passing neural nets (MPNNs)23 that are constructed based on the message passing scheme in which each node propagates and aggregates information, encoded by vectorized messages, to and from its local neighborhood. In order to capture the dynamic nature of evolving features or connectivity over time, temporal graph neural networks (TGNN) have been proposed by14, 54 as a generic, efficient deep learning framework that combines graph encoding (e.g., MPNNs) with time-series encoding architectures (e.g., LSTM, Tranformers, etc.). Applications of TGNN include traffic prediction55\u201357 and learning on brain networks57, etc.\n11/21\n5 Methods"
        },
        {
            "heading": "5.1 Temporal architectures",
            "text": ""
        },
        {
            "heading": "5.1.1 Long Short-Term Memory",
            "text": "Long Short-Term Memory (LSTM), first proposed by58, is a special kind of Recurrent Neural Networks that was designed for learning sequential and time-series data. LSTM has been widely applied into many current state-of-the-art Deep Learning models in various aspects of Machine Learning including Natural Language Processing, Speech Recognition and Computer Vision. One successful variant of LSTM is the Gated Recurrent Unit (GRU) introduced by59 as a simplification with less computational cost in the context of sequential modeling. The forward pass of an LSTM cell with a forget gate can be described as follows:\nft = \u03c3g(Wf xt +U f ht\u22121 +b f ) \u2208 (0,1)h is the forget gate\u2019s activation vector it = \u03c3g(Wixt +Uiht\u22121 +bi) \u2208 (0,1)h is the input/update gate\u2019s activation vector ot = \u03c3g(Woxt +Uoht\u22121 +bo) \u2208 (0,1)h is the output gate\u2019s activation vector c\u0303t = \u03c3c(Wcxt +Ucht\u22121 +bc) \u2208 (\u22121,1)h is the cell input activation vector ct = ft ct\u22121 + it c\u0303t \u2208 Rh is the cell state vector ht = ot \u03c3h(ct) \u2208 (\u22121,1)h is the hidden state (output) vector of the LSTM unit\nwhere the initial values are c0 = 0 and h0 = 0; the subscript t indexes the time step; d and h refer to the number of input features and number of hidden units, respectively; xt \u2208 Rd is the input vector to the LSTM unit; W \u2208 Rh\u00d7d , U \u2208 Rh\u00d7h, and b \u2208 Rh are learnable weight matrices and bias vector; the operator denotes the element-wise (i.e. Hadamard) product; \u03c3g(\u00b7) is the sigmoid function; \u03c3c(\u00b7) and \u03c3h(\u00b7) are the hyperbolic tangent function."
        },
        {
            "heading": "5.1.2 Transformers",
            "text": "Recently, Transformers have achieved superior performances in various deep learning tasks60\u201362. Among multiple advantages of Transformers, the ability to capture long-range dependencies and interactions is particularly important for time series modeling. The backbone of Transformers is the self-attention mechanism62, also called scaled dot-product attention or softmax attention. Self-attention transforms the input sequence X = [x1, ..,xL]T \u2208 RL\u00d7d of length L into the output sequence H = [h1, ..,hL]T \u2208 RL\u00d7h in the following two steps:\n12/21\n1. The input sequence X is projected into the query matrix Q = [q1, ..,qL]T , the key matrix K = [k1, ..,kL]T and the value matrix V = [v1, ..,vL]T via three linear transformations:\nQ = XW TQ , K = XW T K , V = XW T V ,\nwhere WQ,WK ,WV \u2208 Rh\u00d7d are learnable weight matrices.\n2. The output sequence H is then computed as follows: H = Attention(Q,K,V ) = softmax (\nQKT\u221a h\n) V = AV, (1)\nwhere the softmax function is applied to each row of the matrix QKT , and A\u2208RL\u00d7L is the attention matrix of ai j attention scores. Equation 1 can be rewritten as:\nhi = L\n\u2211 j=1\nsoftmax (\nqTi k j\u221a h\n) = L\n\u2211 j=1 ai jv j.\nEach output sequence H forms an attention head. Let n be the number of heads and WO \u2208 Rnh\u00d7nh be the projection matrix for the output. In multi-head attention, multiple heads are concatenated to compute the final output defined as follows:\nMultihead({Q,K,V}ni=1) = Concat(H1, ..,Hn)WO."
        },
        {
            "heading": "5.2 Graph Neural Networks",
            "text": ""
        },
        {
            "heading": "5.2.1 Graph construction",
            "text": "We process the input disease data as graphs, a form of non-Euclidean irregular data that is permutation invariant in nature (i.e., changing the ordering of the nodes in a graph does not change the data that the graph represents). To represent the New Zealand pandemic data as graphs, the entirety of the country is formatted as a single graph G = (V,E), where n = |V | is the number of nodes, and each node represents a single district health board in New Zealand. We create a series of graphs G(1),G(2), ...,G(T ) corresponding to each day in the case dataset of New Zealand, where the current day t is within the available day case data for every district health board. The topology (i.e., connecting edges and adjacency matrix) of the graphs remains constant over all time steps. The adjacency matrix A represents the connection between edges in the disease graph; we constructed the connections between nodes based on geographical adjacency between any two district health boards. Between any two district health boards u and v, the edge (u,v) from u to v is Au,v = 2 if two district health boards share any border length, and Au,v = 1 otherwise. For each node or district health board, we denote the features, or the number of cases in the last d days in the region u, as the vector x(t)u = (c (t\u2212d) u , ...,c (t) u ) > \u2208 Rd . The number of cases over multiple previous days is used to account for irregular case reporting and the length of the incubation period."
        },
        {
            "heading": "5.2.2 Message-passing neural networks",
            "text": "We model the spatial and geographical spread of COVID-19 in New Zealand using a well-known family of GNNs known as message-passing neural networks (MPNNs)23. Vector messages are exchanged between nodes and updated using neural networks; intuitively, the model takes into account the interaction between neighbors in the network to model the spread of the disease. District health boards with shared borders are more connected and see more traffic between them compared to two boards in remote regions. The goal of the model is to generate a vectorized representation for each node in the disease graph.\nWith hidden embedding h(k)u representing each node/district health board u \u2208 V , we define the GNN message-passing mechanism based on the Graph Convolutional Network63 as\nh(k)u = \u03c3 ( W (k) \u2211\nv\u2208N (u)\u222a{u}\nhv\u221a |N (u)||N (u)|\n) (2)\nwhere W (k) is the trainable parameter matrix of layer k, and \u03c3 is an element-wise nonlinearity (i.e., ReLU or tanh function). N (u) denotes the set of neighboring nodes to node u; in this case, the set represents all district health boards adjacent to board u. Note that the term hv\u221a\n|N (u)||N (u)| represents neighborhood normalization based on the degrees of nodes (i.e., number of\nshared borders) for the purpose of increasing computational stability. The number of message-passing layers k represents the number of aggregation operations, as one layer integrates each node with the information from its adjacent neighbors, while two layers add neighbors two steps away from the target node, etc.\n13/21\nAcross multiple layers and to account for the vectorization of the node embeddings, we define the neighborhood aggregation scheme as\nH(k) = \u03c3(A\u0303H(k\u22121)W (k)) (3)\nwhere H(k\u22121) is a matrix containing the generated node embeddings from the previous layer, H(k) = (h(k)1 ,h (k) 2 , ...,h (k) n ) > denotes the matrix arrangement of the node embeddings of all nodes in the graph (H(0) = X), and A\u0303 denotes the aforementioned normalized graph Laplacian. Note that Equation 3 represents the matrix vectorization of the MPNN described in Equation 2, and thus the two equations are equivalent representations of the same MPNN. For the sake of brevity, the time index is omitted from both equations; the model is in fact applied to all input graphs G(1),G(2), ...,G(T ) in the time series separately. Since the connectivity and adjacency of the disease graphs are constant over time, the matrix A\u0303 is shared across all temporal graphs alongside the weight matrices W (1), ...,W (K) for K message-passing layers, while the node embeddings H0, ...,HK are unique for each disease day graph in the time series."
        },
        {
            "heading": "5.2.3 Multiresolution Graph Neural Networks",
            "text": "In field of graph learning, it is important to build a graph neural network that can capture the multiscale and hierarchical structures of graphs. Multiresolution Graph Neural Networks (MGNN) was originally proposed by24 as a graph encoder in the context of graph generation via variational autoencoder, and adopted by14 in combination with a temporal architecture to learn and predict the dynamics of an epidemic or a pandemic. Instead of a fixed coarse-graining process, MGNN introduces a learnable clustering algorithm that iteratively constructs a hierarchy of coarsening graphs, also called multiresolution or multiple levels of resolutions (see Def. 2):\n1. Based on the node embeddings, we cluster a graph into multiple partitions. Each partition is coarsened into a single node, and all the edges connecting between two partitions are aggregated into a single edge (see Def. 1). This process results into a smaller coarsened graph.\n2. We continue to apply message passing on the coarsened graph to produce its node embeddings, and then cluster it further. On each resolution, all the node embeddings are pooled into a single graph-level vectorized representation, i.e. latent. The hierarchy of latents allows us to capture both local information (in the lower levels) and global information (in the higher levels) of a graph.\n14/21\nDefinition 1 A k-cluster partition on a graph G = (V,E) partitions its set of nodes into k disjoint sets {V1,V2, ..,Vk}. A coarsening of G is a graph G\u0303 = (V\u0303 , E\u0303) of k nodes in which node v\u0303i \u2208 V\u0303 corresponds to a induced subgraph of G on Vi. The weighted adjacency matrix A\u0303 \u2208 Nk\u00d7k of G\u0303 is defined as:\nA\u0303i j = { 1 2 \u2211u,v\u2208Vi Auv, if i = j, \u2211u\u2208Vi,v\u2208V j Auv, if i 6= j,\nwhere the diagonal of A\u0303 denotes the number of edges inside each cluster, while the off-diagonal denotes the number of edges between two clusters.\nDefinition 2 An L-level of resolutions, i.e. multiresolution, of a graph G is a series of L graphs G\u03031, .., G\u0303L in which: (i) G\u0303L is G itself; and (ii) For 1\u2264 `\u2264 L\u22121, G\u0303` is a coarsening graph of G\u0303`+1 as defined in Def. 1. The number of nodes in G\u0303` is equal to the number of clusters in G\u0303`+1. The top level coarsening G\u03031 is a graph consisting of a single node.\nThe key innovation of MGNN is how the model can learn to cluster graph G\u0303`+1 into G\u0303` in a data-driven manner. Without the loss of generality, we suppose that the number of nodes in G\u0303` is K, i.e. |V\u0303`|= K, meaning that we cluster G\u0303`+1 into K partitions. First, we employ a GNN to produce a K-channel node embedding for each node of G\u0303`+1. Then, we apply a softmax over the node embedding to compute the probability of assigning each node to one of the K clusters. However, we want each node to be in a single cluster, i.e. hard clustering, thus we employ the Gumbel-max trick64\u201366 to sample/select the cluster based on the assignment probability while maintaining differentiability for back-propagation. This results into an assignment matrix P \u2208 {0,1}|V\u0303`+1|\u00d7K . The adjacency matrix of G\u0303` can be computed as A\u0303` = PT A\u0303`+1P. We repeat this clustering process iteratively in order to build multiple resolutions of coarsening graphs."
        },
        {
            "heading": "5.2.4 Spatio-temporal graph neural networks",
            "text": "In this section, we build our spatio-temporal GNNs by combining all the previously defined modules. Suppose that we are given a historical data of T timesteps which can be modeled by T input graphs G(1),G(2), ..,G(T ). The simplest combination is MPNN+LSTM in which we employ MPNN (see Section 5.2.2) to encode each G(t) into a graph-level vectorized representation and then feed it into an LSTM backbone (see Section 5.1.1). Furthermore, we want to capture the multiscale information, i.e. local to global, that is essential in modeling the long-range spatial and temporal dependencies. Thus, instead of MPNN, we apply MGNN (see Section 5.2.3) to construct a hierarchy of latents (i.e. each latent is a graph-level representation for a resolution) for each graph G(t). At the t-th timestep, a Transformer (see Section 5.1.2) is applied to encode the hierarchy of latents into a single vector that will be fed further into a temporal architecture. Finally, another Transformer is used, instead of LSTM, as the temporal backbone. We call this novel architecture as Attention-based Multiresolution Graph Neural Networks or ATMGNN."
        },
        {
            "heading": "5.3 Data preprocessing",
            "text": "Our dataset primarily focuses on COVID-19 in New Zealand. The number of cases in different district health boards or regions of New Zealand was gathered from government official open data and later reprocessed into the format suitable for input into the forecasting models. The reprocessed data is available on our GitHub page for the project https://github.com/ HySonLab/pandemic_tgnn.\nNew Zealand daily new cases with graphs Official data originally obtained is in tabular form with information regarding the sex, age group, district health board location, case status and travel of each COVID-19 infected patient. All cases are filtered so that only cases in 2022 and cases that are confirmed are included in the dataset. For each district health board, on each day, all confirmed cases regardless of sex or age group are aggregated and counted toward the daily new cases count. From the geographical map of the district health boards (Figure 1), an adjacency matrix that represents the topology of the disease graph is generated by connecting each board to itself with a unit weighted edge, and each board to every other board that shares any part of its border with edges weighted as 2. Original data is imported and transformed using the Python packages Pandas and NumPy67, 68, while disease graphs are built with the included code and the NetworkX69 package. All data that was preprocessed and converted to graph form between March 4th, 2022 and November 4th, 2022 is available on GitHub.\nNew Zealand economic features Official categorical GDP data is obtained from NZStats70, with the original data containing GDP information in terms of NZ dollars for each predefined administrative region and for every 22 available economic industry categories. At the time of collection, only GDP information until the end of 2020 is finalized and available for all applicable industries. Thus, based on the assumption that categorical GDP from 2020 and 2022 does not change significantly year-to-year, GDP data from 2020 is incorporated as additional features into all time steps of the forecasting model. Due to differences between the administrative region map and the district health board map, all regions between the two maps are matched\n15/21\nappropriately, with the merged GDP being the sum or the average between matched regions accordingly. The raw GDP number of each industry/category of each region is concatenated to a common vector of that region without labels to be added to the inputs as a single feature vector. All economic feature vectors are normalized (via mean and standard deviation) to allow the models to learn properly and mitigate exploding/vanishing gradients."
        },
        {
            "heading": "6 Appendix",
            "text": "20/21\n21/21"
        }
    ],
    "title": "Predicting COVID-19 pandemic by spatio-temporal graph neural networks: A New Zealand\u2019s study",
    "year": 2023
}