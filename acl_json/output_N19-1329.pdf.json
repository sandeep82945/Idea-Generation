{
    "abstractText": "Research has shown that neural models implicitly encode linguistic features, but there has been no research showing how these encodings arise as the models are trained. We present the first study on the learning dynamics of neural language models, using a simple and flexible analysis method called Singular Vector Canonical Correlation Analysis (SVCCA), which enables us to compare learned representations across time and across models, without the need to evaluate directly on annotated data. We probe the evolution of syntactic, semantic, and topic representations and find that part-of-speech is learned earlier than topic; that recurrent layers become more similar to those of a tagger during training; and embedding layers less similar. Our results and methods could inform better learning algorithms for NLP models, possibly to incorporate linguistic information more effectively.",
    "authors": [
        {
            "affiliations": [],
            "name": "Naomi Saphra"
        },
        {
            "affiliations": [],
            "name": "Adam Lopez"
        }
    ],
    "id": "SP:897846aa4adfe0c18d7ed9498349e37b1b6f9fdf",
    "references": [
        {
            "authors": [
                "Afra Alishahi",
                "Marie Barking",
                "Grzegorz Chrupa\u0142a."
            ],
            "title": "Encoding of phonology in a recurrent neural model of grounded speech",
            "venue": "Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 368\u2013378.",
            "year": 2017
        },
        {
            "authors": [
                "Yonatan Belinkov",
                "Nadir Durrani",
                "Fahim Dalvi",
                "Hassan Sajjad",
                "James Glass"
            ],
            "title": "What do Neural Machine Translation Models Learn about Morphology? In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Vol",
            "year": 2017
        },
        {
            "authors": [
                "Yonatan Belinkov",
                "Llu\u00eds M\u00e0rquez",
                "Hassan Sajjad",
                "Nadir Durrani",
                "Fahim Dalvi",
                "James R. Glass."
            ],
            "title": "Evaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks",
            "venue": "CoRR, abs/1801.07772.",
            "year": 2018
        },
        {
            "authors": [
                "Johannes Bjerva",
                "Barbara Plank",
                "Johan Bos."
            ],
            "title": "Semantic Tagging with Deep Residual Networks",
            "venue": "COLING.",
            "year": 2016
        },
        {
            "authors": [
                "Terra Blevins",
                "Omer Levy",
                "Luke Zettlemoyer."
            ],
            "title": "Deep RNNs Encode Soft Hierarchical Syntax",
            "venue": "arXiv:1805.04218 [cs]. ArXiv: 1805.04218.",
            "year": 2018
        },
        {
            "authors": [
                "Johan Bos",
                "Valerio Basile",
                "Kilian Evang",
                "Noortje J Venhuizen",
                "Johannes Bjerva."
            ],
            "title": "The groningen meaning bank",
            "venue": "Handbook of linguistic annotation, pages 463\u2013496. Springer.",
            "year": 2017
        },
        {
            "authors": [
                "Ond\u0159ej C\u00edfka",
                "Ond\u0159ej Bojar"
            ],
            "title": "Are bleu and meaning representation in opposition? arXiv preprint arXiv:1805.06536",
            "year": 2018
        },
        {
            "authors": [
                "Remi Tachet des Combes",
                "Mohammad Pezeshki",
                "Samira Shabanian",
                "Aaron Courville",
                "Yoshua Bengio."
            ],
            "title": "On the Learning Dynamics of Deep Neural Networks",
            "venue": "arXiv:1809.06848 [cs, stat]. ArXiv: 1809.06848.",
            "year": 2018
        },
        {
            "authors": [
                "Alexis Conneau",
                "German Kruszewski",
                "Guillaume Lample",
                "Lo\u00efc Barrault",
                "Marco Baroni."
            ],
            "title": "What you can cram into a single vector: Probing sentence embeddings for linguistic properties",
            "venue": "arXiv:1805.01070 [cs]. ArXiv: 1805.01070.",
            "year": 2018
        },
        {
            "authors": [
                "Jonathan Frankle",
                "Michael Carbin."
            ],
            "title": "The Lottery Ticket Hypothesis: Training Pruned Neural Networks",
            "venue": "arXiv:1803.03635 [cs]. ArXiv: 1803.03635.",
            "year": 2018
        },
        {
            "authors": [
                "Mario Giulianelli",
                "Jack Harding",
                "Florian Mohnert",
                "Dieuwke Hupkes",
                "Willem Zuidema."
            ],
            "title": "Under the Hood: Using Diagnostic Classifiers to Investigate and Improve how Language Models Track Agreement Information",
            "venue": "arXiv:1808.08079 [cs].",
            "year": 2018
        },
        {
            "authors": [
                "Dieuwke Hupkes",
                "Sara Veldhoen",
                "Willem Zuidema."
            ],
            "title": "Visualisation and\u2019diagnostic classifiers\u2019 reveal how recurrent and recursive neural networks process hierarchical structure",
            "venue": "Journal of Artificial Intelligence Research, 61:907\u2013926.",
            "year": 2018
        },
        {
            "authors": [
                "Yova Kementchedjhieva",
                "Adam Lopez."
            ],
            "title": "Indicatements that character language models learn English morpho-syntactic units and regularities",
            "venue": "Proc. of Workshop on Analyzing and interpreting neural networks for NLP.",
            "year": 2018
        },
        {
            "authors": [
                "Urvashi Khandelwal",
                "He He",
                "Peng Qi",
                "Dan Jurafsky."
            ],
            "title": "Sharp Nearby, Fuzzy Far Away: How Neural Language Models Use Context",
            "venue": "arXiv:1805.04623 [cs]. ArXiv: 1805.04623.",
            "year": 2018
        },
        {
            "authors": [
                "Christo Kirov",
                "Ryan Cotterell."
            ],
            "title": "Recurrent neural networks in linguistic theory: Revisiting pinker and prince (1988) and the past tense debate",
            "venue": "arXiv preprint arXiv:1807.04783.",
            "year": 2018
        },
        {
            "authors": [
                "Tal Linzen",
                "Emmanuel Dupoux",
                "Yoav Goldberg."
            ],
            "title": "Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies",
            "venue": "arXiv:1611.01368",
            "year": 2016
        },
        {
            "authors": [
                "Tomas Mikolov",
                "Ilya Sutskever",
                "Kai Chen",
                "Greg S Corrado",
                "Jeff Dean."
            ],
            "title": "Distributed Representations of Words and Phrases and their Compositionality",
            "venue": "Advances in Neural Information Processing Systems 26, pages 3111\u20133119. Curran Associates,",
            "year": 2013
        },
        {
            "authors": [
                "Ari S. Morcos",
                "Maithra Raghu",
                "Samy Bengio."
            ],
            "title": "Insights on representational similarity in neural networks with canonical correlation",
            "venue": "arXiv:1806.05759 [cs, stat]. ArXiv: 1806.05759.",
            "year": 2018
        },
        {
            "authors": [
                "Morteza Noshad",
                "Alfred O. Hero III."
            ],
            "title": "Scalable Mutual Information Estimation using Dependence Graphs",
            "venue": "arXiv preprint arXiv:1801.09125.",
            "year": 2018
        },
        {
            "authors": [
                "Matthew E Peters",
                "Mark Neumann",
                "Mohit Iyyer",
                "Matt Gardner",
                "Christopher Clark",
                "Kenton Lee",
                "Luke Zettlemoyer."
            ],
            "title": "Deep contextualized word representations",
            "venue": "arXiv preprint arXiv:1802.05365.",
            "year": 2018
        },
        {
            "authors": [
                "Maithra Raghu",
                "Justin Gilmer",
                "Jason Yosinski",
                "Jascha Sohl-Dickstein."
            ],
            "title": "SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability",
            "venue": "arXiv:1706.05806 [cs, stat]. ArXiv: 1706.05806.",
            "year": 2017
        },
        {
            "authors": [
                "Ravid Shwartz-Ziv",
                "Naftali Tishby."
            ],
            "title": "Opening the Black Box of Deep Neural Networks via Information",
            "venue": "arXiv:1703.00810 [cs]. ArXiv: 1703.00810.",
            "year": 2017
        },
        {
            "authors": [
                "Eva Vanmassenhove",
                "Jinhua Du",
                "Andy Way."
            ],
            "title": "Investigating \u2018aspect\u2019in nmt and smt: Translating the english simple past and present perfect",
            "venue": "Computational Linguistics in the Netherlands Journal, 7:109\u2013128.",
            "year": 2017
        },
        {
            "authors": [
                "John Wieting",
                "Douwe Kiela."
            ],
            "title": "No training required: Exploring random encoders for sentence classification",
            "venue": "arXiv preprint arXiv:1901.10444.",
            "year": 2019
        },
        {
            "authors": [
                "Chiyuan Zhang",
                "Samy Bengio",
                "Moritz Hardt",
                "Benjamin Recht",
                "Oriol Vinyals."
            ],
            "title": "Understanding deep learning requires rethinking generalization",
            "venue": "arXiv preprint arXiv:1611.03530.",
            "year": 2016
        },
        {
            "authors": [
                "Kelly W. Zhang",
                "Samuel R. Bowman."
            ],
            "title": "Language Modeling Teaches You More Syntax than Translation Does: Lessons Learned Through Auxiliary Task Analysis",
            "venue": "arXiv:1809.10040 [cs]. ArXiv: 1809.10040.",
            "year": 2018
        }
    ],
    "sections": [
        {
            "text": "Proceedings of NAACL-HLT 2019, pages 3257\u20133267 Minneapolis, Minnesota, June 2 - June 7, 2019. c\u00a92019 Association for Computational Linguistics\n3257"
        },
        {
            "heading": "1 Introduction",
            "text": "Large neural networks have a notorious capacity to memorize training data (Zhang et al., 2016), but their high accuracy on many NLP tasks shows that they nonetheless generalize. One apparent explanation for their performance is that they learn linguistic generalizations even without explicit supervision for those generalizations\u2014for example, that subject and verb number agree in English (Linzen et al., 2016); that derivational suffixes attach to only specific parts of speech (Kementchedjhieva and Lopez, 2018); and that short segments of speech form natural clusters corresponding to phonemes (Alishahi et al., 2017). These studies tell us that neural models learn to implicitly represent linguistic categories and their interactions. But how do they learn these representations?\nOne clue comes from the inspection of multilayer models, which seem to encode lexical cate-\ngories in lower layers, and more contextual categories in higher layers. For example, Blevins et al. (2018) found that a word\u2019s part of speech (POS) is encoded by lower layers, and the POS of its syntactic parent is encoded by higher layers; while Belinkov et al. (2018) found that POS is encoded by lower layers and semantic category is encoded by higher layers. More generally, the most useful layer for an arbitrary NLP task seems to depend on how \u201chigh-level\u201d the task is (Peters et al., 2018). Since we know that lower layers in a multi-layer model converge to their final representations more quickly than higher layers (Raghu et al., 2017), it is likely that models learn local lexical categories like POS earlier than they learn higher-level linguistic categories like semantic class.\nHow and when do neural representations come to encode specific linguistic categories? Answers could explain why neural models work and help us improve learning algorithms. We investigate how representations of linguistic structure are learned over time in neural language models (LMs), which are central to NLP: on their own, they are used to produce contextual representations of words for many tasks (e.g. Peters et al., 2018); while conditional LMs power machine translation, speech recognition, and dialogue systems. We use a simple and flexible method, Singular Vector Canonical Correlation Analysis (SVCCA; Raghu et al., 2017), which allows us to compare representations from our LM at each epoch of training with representations of other models trained to predict specific linguistic categories. We discover that lower layers initially discover features shared by all predictive models, but lose these features as the LM explores more specific clusters. We demonstrate that different aspects of linguistic structure are learned at different rates within a single recurrent layer, acquiring POS tags early but continuing to learn global topic information later in training."
        },
        {
            "heading": "2 Methods",
            "text": "Our experiments require a LM, tagging models, and a method to inspect the models: SVCCA."
        },
        {
            "heading": "2.1 Language model",
            "text": "We model the probability distribution over a sequence of tokens x1 . . . x|x| with a conventional two-layer LSTM LM. The pipeline from input xt at time step t to a distribution over xt+1 is described in Formulae (1)\u2013(4). At time step t, input word xt is embedded as (1) het , which is input to a two-layer LSTM, producing outputs (2) h1t and (3) h2t at these layers, along with cell states c 1 t and c2t . A softmax layer converts h 2 t to a distribution from which (4) x\u0302t+1 is sampled.\nhet = embedding(xt) (1)\nh1t , c 1 t = LSTM1(h e t , h 1 t\u22121, c 1 t\u22121) (2) h2t , c 2 t = LSTM2(h 1 t , h 2 t\u22121, c 2 t\u22121) (3)\nx\u0302t+1 \u223c softmax(h2t ) (4)\nEach function can be thought of as a representation or embedding of its discrete input; hence het is a representation of xt, and\u2014due to the recursion in (2)\u2014h1t is a representation of x1 . . . xt."
        },
        {
            "heading": "2.2 Tagging models",
            "text": "To inspect our language model for learned linguistic categories, we will use a collection of tagging models, designed to mimic the behavior of our language model but predicting the next tag rather than the next word. Given x1 . . . x|x|, we model a corresponding sequence of tags y1 . . . y|x| using a onelayer LSTM. (Our limited labeled data made this more accurate on topic tagging than another twolayer LSTM, so this architecture does not directly parallel the LM.)\nhet \u2032 = embedding\u2032(xt) (5)\nh1t \u2032, c1t \u2032 = LSTM\u2032(het \u2032, h1t\u22121 \u2032, c1t\u22121 \u2032) (6)\ny\u0302t+1 \u223c softmax\u2032(h1t \u2032) (7)\nWe will also discuss input taggers, which share this architecture but instead sample yt, the tag of the most recently observed word."
        },
        {
            "heading": "2.3 SVCCA",
            "text": "SVCCA is a general method to compare the correlation of two vector representations. Let dA and dB be their dimensions. ForN data points we have two distinct views, given by matricesA \u2208 RN\u00d7dA\nand B \u2208 RN\u00d7dB . We project these views onto a shared subspace in two steps:\n1. Use Singular Value Decomposition (SVD) to reduce matrices A and B to lower dimensional matrices A\u2032 and B\u2032, respectively. This is necessary because many dimensions in the representations are noisy, and in fact cancel each other out (Frankle and Carbin, 2018). SVD removes dimensions that were likely to be less important in the original representations from A and B, and in keeping with Raghu et al. (2017), we retain enough dimensions to keep 99% of the variance in the data.\n2. Use Canonical Correlation Analysis (CCA) to project A\u2032 and B\u2032 onto a shared subspace, maximizing the correlation of the projections. Formally, CCA identifies vectors w, v to maximize \u03c1 = <w\n>A\u2032,v>B\u2032> \u2016w>A\u2032\u2016\u2016v>B\u2032\u2016 . We\ntreat these w, v as new basis vectors, computing the top dC (a hyperparameter) such basis vectors to form projection matrices W \u2208 RdC\u00d7dA\u2032 , V \u2208 RdC\u00d7dA\u2032 . The resulting projections WA\u2032 and V B\u2032 map onto a shared subspace where the representations of each datapoint from A\u2032 and B\u2032 are maximally correlated.\nIntuitively, the correlation \u03c1 will be high if both representations encode the same information, and low if they encode unrelated information. Figure 1 illustrates how we use SVCCA to compare representation h2t of our language model with the recurrent representation of a tagger, h1t\n\u2032. In practice, we run over all time steps in a test corpus, rather than a single time step as illustrated."
        },
        {
            "heading": "3 Experimental Setup",
            "text": "We trained our LM on a corpus of tokenized, lowercased English Wikipedia (70/10/20 train/dev/test split). To reduce the number of unique words in the corpus, we excluded any sentence with a word type appearing fewer than 100 times. Words appearing fewer than 100 times in the resulting training set are replaced with an unknown token. The resulting training set has over 227 million tokens of 20K types.\nWe train for 50 epochs to maximize crossentropy, using a batch size of 40, dropout ratio of 0.2, and sequence length of 35. The optimizer is standard SGD with clipped gradients at 0.25,\nLanguage model\nwith the learning rate quartered when validation loss increases. The result of training is shown in Figure 2, which illustrates the dips in loss when learning rate changes. All experiments on the LM throughout training are conducted by running the model at the end of each epoch in inference mode over the test corpus."
        },
        {
            "heading": "3.1 Taggers",
            "text": "To understand the representations learned by our LM, we compare them with the internal representations of tagging models, using SVCCA. Where possible, we use coarse-grained and fine-grained tagsets to account for effects from the size of the tagset. Table 1 illustrates our tagsets.\nPOS tagging For syntactic categories, we use POS tags, as in Belinkov et al. (2017). As a coarse-grained tagset, we use silver Universal Dependency Parse (UDP) POS tags automatically added to our Wikipedia corpus with spacy.1 We also use a corpus of fine-grained human annotated Penn Treebank POS tags from the Groningen Meaning Bank (GMB; Bos et al., 2017).\nSemantic tagging We follow Belinkov et al. (2018) in representing word-level semantic information with silver SEM tags (Bjerva et al., 2016). SEM tags disambiguate POS tags in ways that are relevant to multilingual settings. For example, the comma is not assigned a single tag as punctuation, but has distinct tags according to its function: conjunction, disjunction, or apposition. The 66 fine-grained SEM tag classes fall under 13 coarsegrained tags, and an \u2018unknown\u2019 tag.\nGlobal topic For topic, we classify each word of the sequence by its source Wikipedia article; for example, every word in the wikipedia article on Trains is labeled \u201cTrains\u201d. This task assesses whether the network encodes the global topic of the sentence.\n1https://spacy.io/\nUDP silver POS and topic information use the same corpus, taken from the 100 longest articles in Wikipedia randomly partitioned in a 70/10/20 train/dev/test split. Each token is tagged with POS and with the ID of the source article. The corpus is taken from the LM training data, which may increase the similarity between the tag model and LM. Because both tag predictors are trained and tested on the same domain as the LM, they can be easily compared in terms of their similarity to the LM representation. Though the SEM corpus and the PTB corpus are different domains from the Wikipedia training data, we compare their activations on the same 191K-token 100-article test corpus.\nTable 2 describes the training and validation corpus statistics for each tagging task. Note that topic and UDP POS both apply to the same enwikipedia corpus, but PTB POS and SEM use two different unaligned sets from the GMB corpus."
        },
        {
            "heading": "4 Experiments, Results, and Analysis",
            "text": "A benefit of SVCCA is its flexibility: it can compute the correlation of a hidden representation to any other vector. Raghu et al. (2017) used it to understand learning dynamics by comparing a learned representation to snapshots of the same representation at different epochs during training. We use a similar experiment to establish the basic learning dynamics of our model. In our shallow 2-level model, activations at h1 converge slightly after h2 (Figure 3). This differs from the results of Raghu et al. (2017), who found that a 5-layer stacked LSTM LM exhibits faster convergence at lower layers, but this difference may be attributed to our much larger training data, which gives our model sufficient training data at early epochs.\nEmpirical upper bounds. Our main experiments will test the rate at which different linguistic categories are learned by different layers, but to interpret the results, we need to understand the behaviour of SVCCA for these models. In theory, SVCCA scores can vary from 0 for no correlation to 1 for perfect correlation. But in practice, these extreme cases will not occur. To establish an empirical upper bound on correlation, we compared the similarity at each epoch of training to the frozen final state of a LM with identical architecture but different initialization, trained on the same data (Figure 4).2 The correlations increase over time as expected, but to a maximum near 0.64; we don\u2019t expect correlations between our LM and other models to exceed this value. We explore corresponding lower bounds in our main experiments below.\nCorrelations between different layers. Next we examine the correlation between different layers of the same model over time (Figure 5). We observe that, while over time correlation increases, in general closer layers are more similar, and they are less correlated than they are with the same layer of a differently initialized model. This supports the idea that we should compare recurrent layers with recurrent layers because their representations play similar roles within their respective architectures.\nSVCCA vs. Diagnostic classifiers A popular method to analyze learned representations is to use a diagnostic classifier (Belinkov et al., 2017; Hupkes et al., 2018) or probe (Conneau et al., 2018), a separate model that is trained to predict a linguistic category of interest, yt, from an arbitrary hidden layer ht. Diagnostic classifiers are widely used (Belinkov et al., 2018; Giulianelli et al., 2018).\n2This experiment is similar to the comparisons of randomly initialized models by Morcos et al. (2018).\nBut if a diagnostic classifier is trained on enough examples, then random embeddings as input representations often outperform any pretrained intermediate representation (Wieting and Kiela, 2019; Zhang and Bowman, 2018). This suggests that diagnostic classifiers may work simply by memorizing the association between an embedding and the most frequent output category associated with that embedding; since for many words their category is (empirically) unambiguous, this may give an inflated view of just how much a model \u201cunderstands\u201d about that category.\nOur use of SVCCA below will differ from the use of diagnostic classifiers in an important way. Diagnostic classifiers use the intermediate representations of the LM as inputs to a tagger. A representation is claimed to encode, for example, POS if the classifier accurately predicts it\u2014in other words, whether it can decode it from the representation. We will instead evaluate the similarity between the representations in an LM and in an independently-trained tagger. The intuition behind this is that, if the representation of our LM encodes a particular category, then it must be similar to the representation of model that is specifically trained to predict that category. A benefit of the approach is that similarity can be evaluated on any dataset, not only one that has been labeled with the linguis-\ntic categories of interest. Another distinction from the typical use of diagnostic classifiers is that probes are usually used to decode tag information about the context or most recent input from the hidden state at the current step. Because the hidden representation at time t is meant to encode predictive information about the target word at time t+1, we treat it as encoding a prediction about the tag of the target word.\nTo understand the empirical strengths and weaknesses of these approaches, we compare the use of SVCCA and diagnostic classifiers in understanding learning dynamics. In other words, we ask: is our first conceptual shift (to SVCCA) necessary? To test this, we use the same model as Belinkov et al. (2017), which classifies an arbitrary representation using a ReLU followed by a softmax layer. To be consistent with Belinkov et al. (2017), we use yt as their target label. We repeat their method in this manner (Figure 6) as well as applying our second modification, in which we instead target the label yt+1 (Figure 7).\nWe found the correlations to be relatively stable over the course of training. This is at odds with\nthe results in Figures 2 and 3, which suggest that representations change substantially during training in ways that materially affect the accuracy of the LM. This suggests that diagnostic classifiers are not illustrating improvements in word representations throughout training, and we conclude that they are ineffective for understanding learning dynamics. Our remaining experiments use only SVCCA."
        },
        {
            "heading": "4.1 SVCCA on Output Tag Prediction",
            "text": "We applied SVCCA to each layer of our LM with the corresponding layer of each tag predictor in order to find the correlation between the LM representation and the tag model representation at each level (Figure 8). To establish empirical lower bounds on correlation, we also trained our taggers on the same data with randomly shuffled labels, as in Zhang et al. (2016). These latter experiments, denoted by the dotted lines of Figure 8, show how much of the similarity between models is caused by their ability to memorize arbitrary associations. Note that the resulting scores are nonzero, likely because the linguistic structure of the input shapes representations even when the output is random, due to the memorization phase of training (Shwartz-Ziv and Tishby, 2017).\nThe strongest similarity at recurrent layers belongs to the most local property, the UDP POS tag. Both coarse- and fine-grained semantic tags, which rely on longer range dependencies, fall below UDP POS consistently. Topic, which is global to an entire document, is the least captured and the slowest to stabilize. Indeed, correlation with true\ntopic falls consistently below the score for a model trained on randomized topic tags, implying that early in training the model has removed the context necessary to identify topic (below even the inadequate contextual information memorized by a model with random labels), which depends on the general vocabulary in a sentence rather than a local sequence. Over time correlation rises, possibly because the model permits more long-distance context to be encoded. Khandelwal et al. (2018) found that LSTMs remember content words like nouns for more time steps than they remember function words like prepositions and articles. We hypothesize that the LM\u2019s slower stabilization on topic is related to this phenomenon, since it must depend on content words, and its ability to remember them increases throughout training.\nThe encoder layer exhibits very different patterns. Because the representation produced by the encoder layer is local to the word, the nuances that determine how a word is tagged in context cannot be learned. The encoder layers are all highly similar to each other, which suggests that the unigram representations produced by the encoder are less dependent on the particular end task of the neural network. Similarity between the encoders declines over time as they become more specialized towards the language modeling task. This decline points to some simple patterns which are learned for all language tasks, but which are gradually replaced by representations more useful for language modeling. This process may even be considered a naturally occurring analog to the common practice of initializing the encoder layer as word embeddings pretrained an unrelated task such as skipgram or CBOW (Mikolov et al., 2013). It seems that the \u2018easy\u2019 word properties, which immediately improve performance, are similar regardless of the particular language task.\nAt h1, the correlation shows a clear initial decline in similarity for all tasks. This seems to point to an initial representation that relies on simple shared properties, which in the first stage of training is gradually dissolved before the layer begins to converge on a structure shared with each tag predictor. It may also be linked to the information bottleneck learning phases explored by ShwartzZiv and Tishby (2017). They suggest that neural networks learn by first maximizing the mutual information between the input and internal representation, then minimizing the mutual informa-\ntion between the internal representation and output. The network thus initially learns to effectively represent the input, then compresses this representation, keeping only the elements relevant to the output.3 If the LM begins by maximizing mutual information with input, because the input is identical for the LM and tag models it may lead to these similar initial representations, followed by a decline in similarity as the compression narrows to properties specific to each task."
        },
        {
            "heading": "4.2 SVCCA on Input Tagging",
            "text": "Our second conceptual shift is to focus on output tag prediction\u2014asking what a representation encodes about the next output word, rather than what it has encoded about words it has already observed in the input. What effect does this have? Since we already studied output tags in the previous set of experiments, here we consider input tags, in the style of most diagnostic classifier analysis (Figure 9). The learning dynamics are similar to those for tag prediction, but the UDP POS tagger decreases dramatically in all correlations while the GMB-trained taggers4 often increase slightly. While the shapes of the lines are similar, UDP POS no longer consistently dominates the other tasks in recurrent layer correlation. Instead, we find the more granular PTB POS tags lead to the most similar representations."
        },
        {
            "heading": "5 Discussion and Conclusions",
            "text": "We find clear patterns in the encoding of linguistic structure with SVCCA, in contrast to the weaker results from a less responsive diagnostic classifier. Because SVCCA proves so much more sensitive than the diagnostic classifiers currently in use, we believe that future work on measuring the encoding of linguistic structure should use the similarity of individual modules from independently trained tag predictors rather than the performance of tag predictors trained on a particular representation.\nThis system should also be of interest because it is efficient. To train a diagnostic classifier, we must run a forward pass of the LM for each forward pass of the auxiliary model, while SVCCA only requires the LM to run on the test set. A fur-\n3This memorization\u2013compression learning pattern parallels the memorization\u2013generalization of the first half of the U-shaped curve exhibited by human children learning irregular word forms. Kirov and Cotterell (2018) observe similar patterns when artificially modeling inflection.\n4PTB POS, SEM (fine), and SEM (coarse)\nther efficiency gain is particular to studying learning dynamics: we train only one tagger and compare it to different versions of the LM over training, but for standard probing, we must train a new version of each layer\u2019s tagger at each epoch. Our SVCCA experiments in Figure 8 ran in hours, while the diagnostic classifier experiments in Figure 7 ran for days.\nOur method holds another, more subtle advantage. Our analysis provides an alternative view of what it means for a model to encode some linguistic trait. The literature on analyzing neural networks includes a broad spectrum of interpretations about what it means to encode a property. At one end of the spectrum lies the purely informational view (e.g., mutual information; Noshad and Hero III, 2018). Mutual information is a very flexible view, but it requires us to compute theoretical information content, which in practice can only be estimated. Furthermore, information can be represented without being used, as shown by Vanmassenhove et al. (2017), who found that NMT systems often predicted tense according to a diagnostic classifier but did not produce the correct tense as output. The other end of the spectrum is focused on the structure of the representation space (e.g., the features and the property in question are linearly similar; Alishahi et al., 2017). Analyzing structural similarity should remedy the shortcomings of the informational view, but most intermediate representations are not targeted to extract the property in question through a linear transformation, and failing to be interpretable through such simple extraction should not be equated to a failure to encode that property.\nMost of the literature on analyzing representations, by probing with a more complex architecture, seeks the flexibility of mutual information with the concreteness and tractability of the structural view \u2013 but instead obscures the strict information view without offering interpretable information about the structure, because the architecture of a diagnostic classifier affects its performance. It should not be surprising that representational quality as measured by such systems is a poor indicator of translation quality (C\u00edfka and Bojar, 2018). SVCCA, in contrast, is a structural view that does not directly compare an activation that targets word prediction with a particular tag, but instead compares that activation with one targeting the prediction of the tag.\nLet us consider a specific common probing method. What do we learn about the LM when a feedforward network cannot extract tag information directly from the embedding layer, but can from a recurrent layer? It may be tempting to conclude that tag information relies heavily on context, but consider some alternative explanations. If the embedding encodes the tag to be interpreted by a recurrent layer, a feedforward network may not be capable of representing the function to extract that tag because it does not have access to a context vector for aiding interpretation of the hidden layer. Perhaps its activation functions cover a different range of outputs. By directly comparing LSTM layers to LSTM layers and embedding layers to embedding layers, we respect the shape of their outputs and the role of each module within the network in our analysis.\nThe results of our analysis imply that early in training, representing part of speech is the natural way to get initial high performance. However, as training progresses, it increasingly benefits the model to represent categories with longer-range dependencies, such as topic."
        },
        {
            "heading": "6 Future Work",
            "text": "One direction for future work is exploring how generalization interacts with the correlations between LMs and tag predictors. It may be that a faithful encoding of a property like POS tag indicates that the LM is relying more on linguistic structure than on memorizing specific phrases, and therefore is associated with a more general model.\nIf these measurements of structure encoding are associated with more general models, we might introduce regularizers or other modifications that explicitly encourage correlation with a tagging task.\nCombes et al. (2018) identified the phenomenon of gradient starvation, meaning that while frequent and unambiguous features are learned quickly in training, they slow down the learning of rarer features. For example, artificially brightening images according to their class leads to a delay in learning to represent the more complex natural class features. Although it is tempting to claim that semantic structure is learned using syntactic structure as natural scaffolding, it is possible that the simple predictive power of POS is acting as an attractor and starving semantic features that are rarer and more ambiguous. A possible direction for future work would be to explore which of\nthese explanations is true, possibly by decorrelating particular aspects of linguistic structure from language modeling representations.\nThe techniques in this paper could be applied to better understand the high performance of a system like ELMo (Peters et al., 2018). Different layers in such a system are useful for different tasks, and this effect could be understood in terms of the gradual divergence between the layers and their respective convergence to representations geared toward a single task."
        },
        {
            "heading": "Acknowledgements",
            "text": "We thank Denis Emelin, Sameer Bansal, Toms Bergmanis, Maria Corkery, Sharon Goldwater, Sorcha Gilroy, Aibek Makazhanov, Yevgen Matusevych, Kate McCurdy, Janie Sinclair, Ida Szubert, Nikolay Bogoychev, Clara Vania, and the anonymous reviewers for helpful discussion and comments on drafts. We thank Matthew Summers for assistance with visualisations."
        },
        {
            "heading": "A Performance Out Of Domain",
            "text": "Because SEM tags and PTB POS tags were both trained on the GMB corpus, we present the SVCCA similarities on an in-domain GMB test corpus as well as the Wikipedia test corpus used elsewhere in the paper. The results are in Figures 10-11. In general correlations are higher using the original tagging domain, but not enough to contradict our earlier analysis. The shapes of the curves remain similar."
        }
    ],
    "title": "Understanding Learning Dynamics Of Language Models with SVCCA",
    "year": 2019
}