{
    "abstractText": "Zeros of rational transfer function matrices R(\u03bb) are the eigenvalues of associated polynomial system matrices P (\u03bb), under minimality conditions. In this paper we define a structured condition number for a simple eigenvalue \u03bb0 of a (locally) minimal polynomial system matrix P (\u03bb), which in turn is a simple zero \u03bb0 of its transfer function matrix R(\u03bb). Since any rational matrix can be written as the transfer function of a polynomial system matrix, our analysis yields a structured perturbation theory for simple zeros of rational matrices R(\u03bb). To capture all the zeros of R(\u03bb), regardless of whether they are poles or not, we consider the notion of root vectors. As corollaries of the main results, we pay particular attention to the special case of \u03bb0 being not a pole of R(\u03bb) since in this case the results get simpler and can be useful in practice. We also compare our structured condition number with Tisseur\u2019s unstructured condition number for eigenvalues of matrix polynomials, and show that the latter can be unboundedly larger. Finally, we corroborate our analysis by numerical experiments.",
    "authors": [
        {
            "affiliations": [],
            "name": "VANNI NOFERINI"
        },
        {
            "affiliations": [],
            "name": "LAURI NYMAN"
        },
        {
            "affiliations": [],
            "name": "MAR\u00cdA C. QUINTANA"
        }
    ],
    "id": "SP:8d689f9068ceeed4c7e8d32badc2ab6104bbcbba",
    "references": [
        {
            "authors": [
                "A.L. Andrew",
                "K.W. Eric Chu",
                "P. Lancaster"
            ],
            "title": "Derivatives of eigenvalues and eigenvectors of matrix functions",
            "venue": "SIAM J. Matrix Anal. Appl. 14 (4) ",
            "year": 1993
        },
        {
            "authors": [
                "T. Betcke",
                "N.J. Higham",
                "V. Mehrmann",
                "C. Schroeder",
                "F. Tisseur"
            ],
            "title": "NLEVP: A Collection of Nonlinear Eigenvalue Problems",
            "venue": "ACM Trans. Math. Soft. 39(2) ",
            "year": 2013
        },
        {
            "authors": [
                "M. Dahleh",
                "M.A. Dahleh",
                "G. Verghese"
            ],
            "title": "Lectures on Dynamic Systems and Control",
            "venue": "Massachusetts Institute of Technology",
            "year": 2011
        },
        {
            "authors": [
                "F.M. Dopico",
                "S. Marcaida",
                "M.C. Quintana"
            ],
            "title": "Strong linearizations of rational matrices with polynomial part expressed in an orthogonal basis",
            "venue": "Linear Algebra Appl. 570 ",
            "year": 2019
        },
        {
            "authors": [
                "F.M. Dopico",
                "S. Marcaida",
                "M.C. Quintana",
                "P. Van Dooren"
            ],
            "title": "Local linearizations of rational matrices with application to rational approximations of nonlinear eigenvalue problems",
            "venue": "Linear Algebra Appl. 604 ",
            "year": 2020
        },
        {
            "authors": [
                "F.M. Dopico",
                "S. Marcaida",
                "M.C. Quintana",
                "P. Van Dooren"
            ],
            "title": "Block full rank linearizations of rational matrices",
            "venue": "Linear and Multilinear Algebra ",
            "year": 2022
        },
        {
            "authors": [
                "F.M. Dopico",
                "V. Noferini"
            ],
            "title": "Root polynomials and their role in the theory of matrix polynomials",
            "venue": "Linear Algebra Appl. 584 ",
            "year": 2020
        },
        {
            "authors": [
                "I. Gohberg",
                "P. Lancaster",
                "L. Rodman"
            ],
            "title": "Matrix Polynomials",
            "venue": "SIAM, Philadelphia, PA, USA, 2009 ",
            "year": 1982
        },
        {
            "authors": [
                "S. G\u00fcttel",
                "L. Tisseur"
            ],
            "title": "The Nonlinear Eigenvalue Problem",
            "venue": "Acta Numer. 26 ",
            "year": 2017
        },
        {
            "authors": [
                "S. G\u00fcttel",
                "R. Van Beeumen",
                "K. Meerbergen",
                "W. Michiels"
            ],
            "title": "NLEIGS: A class of fully rational Krylov methods for nonlinear eigenvalue problems",
            "venue": "SIAM J. Sci. Comput. 36(6) ",
            "year": 2014
        },
        {
            "authors": [
                "N.J. Higham"
            ],
            "title": "Accuracy and stability of numerical algorithms",
            "venue": "SIAM, Philadelphia (PA)",
            "year": 1996
        },
        {
            "authors": [
                "T. Kailath"
            ],
            "title": "Linear Systems",
            "venue": "Prentice Hall, New Jersey",
            "year": 1980
        },
        {
            "authors": [
                "T. Kato"
            ],
            "title": "Perturbation Theory for Linear Operators",
            "venue": "Springer",
            "year": 1980
        },
        {
            "authors": [
                "P. Lietaert",
                "J. P\u00e9rez",
                "B. Vandereycken",
                "K. Meerbergen"
            ],
            "title": "Automatic rational approximation and linearization of nonlinear eigenvalue problems",
            "venue": "IMA J. Numer. Anal. 42(2) ",
            "year": 2022
        },
        {
            "authors": [
                "B. McMillan"
            ],
            "title": "Introduction to formal realizability theory II",
            "venue": "Bell System Tech. J. 31 ",
            "year": 1952
        },
        {
            "authors": [
                "V. Merhmann",
                "H. Voss"
            ],
            "title": "Nonlinear eigenvalue problems: a challenge for modern eigenvalue methods",
            "venue": "GAMM Mitteilungen 27(2) ",
            "year": 2005
        },
        {
            "authors": [
                "J. P\u00e9rez",
                "M.C. Quintana"
            ],
            "title": "Linearizations of rational matrices from general representations",
            "venue": "Linear Algebra Appl. 647 ",
            "year": 2022
        },
        {
            "authors": [
                "J.R. Rice"
            ],
            "title": "A theory of condition",
            "venue": "SIAM J. Numer. Anal. 3 ",
            "year": 1966
        },
        {
            "authors": [
                "H.H. Rosenbrock"
            ],
            "title": "State-space and Multivariable Theory",
            "venue": "Thomas Nelson and Sons, London",
            "year": 1970
        },
        {
            "authors": [
                "F. Tisseur"
            ],
            "title": "Backward error and condition of polynomial eigenvalue problems",
            "venue": "Linear Algebra Appl. 309 ",
            "year": 2000
        },
        {
            "authors": [
                "A.I.G. Vardulakis"
            ],
            "title": "Linear Multivariable Control",
            "venue": "John Wiley and Sons, New York",
            "year": 1991
        }
    ],
    "sections": [
        {
            "text": "Key words. rational matrix, transfer function matrix, polynomial system matrix, rational eigenvalue problem, zeros, poles, root vectors, condition number\nAMS subject classifications. 65F15, 15A18, 15A54, 93B20, 93B60\n1. Introduction. Given a rational matrix R(\u03bb), the Rational Eigenvalue Problem (REP) is often defined as the problem of finding scalars \u03bb0 such that R(\u03bb0) has finite entries (that is, \u03bb0 is not a pole of R(\u03bb)) and that there exist nonzero constant vectors x and y (called eigenvectors) satisfying R(\u03bb0)x = 0 and y\nTR(\u03bb0) = 0, under the assumptions that R(\u03bb) is regular, i.e., R(\u03bb) is square and its determinant is not identically equal to zero. However, zeros of rational matrices can also be poles: a situation not uncommon in certain applications such as control theory [13, 18, 21]. Thus, in this paper we will define the REP regardless of whether a zero is also a pole or not. For that, we extend the definition of eigenvectors by using the more general notion of root vectors [7, 10, 18]. That is, we define the REP as the problem of finding scalars \u03bb0 and rational vectors x(\u03bb) \u2208 C(\u03bb)m and y(\u03bb) \u2208 C(\u03bb)m with x(\u03bb0) 6= 0 and y(\u03bb0) 6= 0 such that lim\u03bb\u2192\u03bb0 R(\u03bb)x(\u03bb) = 0 and lim\u03bb\u2192\u03bb0 y(\u03bb)TR(\u03bb) = 0. The above definition captures all the zeros of regular rational matrices R(\u03bb).\nIt is known [21] that any rational matrix R(\u03bb) can be written (or is directly given) as the transfer function matrix of a polynomial system matrix. That is, of the form\n(1.1) R(\u03bb) = D(\u03bb) + C(\u03bb)A(\u03bb)\u22121B(\u03bb),\nwhere A(\u03bb), B(\u03bb), C(\u03bb) and D(\u03bb) are arbitrary polynomial matrices [9], with A(\u03bb) regular. Clearly, the expression in (1.1) is not uniquely determined by R(\u03bb). Then, under minimality conditions [5, 21], the zeros of R(\u03bb) are the eigenvalues of the\n\u2217Aalto University, Department of Mathematics and Systems Analysis, P.O. Box 11100, FI-00076, Aalto, Finland. (vanni.noferini@aalto.fi) Supported by an Academy of Finland grant (Suomen Akatemian pa\u0308a\u0308to\u0308s 331230). \u2020Aalto University, Department of Mathematics and Systems Analysis, P.O. Box 11100, FI-00076, Aalto, Finland. (lauri.s.nyman@aalto.fi). \u2021University of Montana, Department of Mathematical Science, 32 Campus Dr, Missuola, MT 59812, United States. (javier.perez-alvaro@mso.umt.edu) \u00a7Corresponding author. Aalto University, Department of Mathematics and Systems Analysis, P.O. Box 11100, FI-00076, Aalto, Finland. (maria.quintanaponce@aalto.fi) Supported by an Academy of Finland grant (Suomen Akatemian pa\u0308a\u0308to\u0308s 331230) and by the Agencia Estatal de Investigacio\u0301n of Spain through grant PID2019-106362GB-I00 MCIN/ AEI/10.13039/501100011033/.\n1\nar X\niv :2\n20 7.\n06 79\n1v 2\n[ m\nat h.\nN A\n] 1\n5 Ju\nl 2 02\n2\n2 associated polynomial matrix\n(1.2) P (\u03bb) = [ \u2212A(\u03bb) B(\u03bb) C(\u03bb) D(\u03bb) ] ,\nwhich is said to be a polynomial system matrix of R(\u03bb). In this paper, we will study the structured conditioning of simple zeros of (1.2), allowing perturbation of each block; if (1.2) is (locally) minimal, the analysis thus provides a structured perturbation theory of simple zeros of rational matrices written as in (1.1).\nRational matrices appear directly from applications, as in linear systems and control theory, or as approximations to nonlinear eigenvalue problems (NLEPs) [11, 15]. An example of rational matrix arising from application [17] is the following: (1.3) R(\u03bb) = \u2212K + \u03bbM + \u03bb2 k\u2211 i=1 1 \u03c9i \u2212 \u03bb Ci,\nwhich can be easily written as the transfer function of a polynomial system matrix, for example, as follows:\n(1.4) R(\u03bb) = \u2212K + \u03bbM + [ \u03bbI \u00b7 \u00b7 \u00b7\u03bbI ] (w1 \u2212 \u03bb)I . . . (wk \u2212 \u03bb)I  \u22121 \u03bbC1... \u03bbCk  . Another example is the problem loaded string from [2]:\n(1.5) R(\u03bb) = A\u2212 \u03bbB + k\u03bb \u03bb\u2212 k/m ene T n ,\nwhere A,B are certain n \u00d7 n matrices, k,m are parameters, and en denotes the nth column of the n \u00d7 n identity matrix. The rational matrix (1.5) is also easy to write as a transfer function, for instance:\n(1.6) R(\u03bb) = A\u2212 \u03bbB + \u03bben [ \u03bb\u2212 k/m ]\u22121 keTn .\n1.1. Condition numbers. The condition number of a function measures how much the output of the function can change for a small perturbation in the input. To establish a general framework, consider two normed vector spaces (U , \u2016 \u00b7 \u2016U ) and (V, \u2016 \u00b7 \u2016V) and a function f : U \u2192 V, u 7\u2192 f(u). The (worst-case) absolute condition number of f at u is defined as [20]\n(1.7) \u03baf (u) = lim \u21920 sup \u2016e\u2016U\u22641\n\u2016f(u+ e)\u2212 f(u)\u2016V\n.\nIf f is Fre\u0301chet differentiable at u, then this definition implies that \u03ba is the operator norm of the Fre\u0301chet derivative of f at u. However, (1.7) is valid more generally. The (worst-case) relative condition number of f at u is defined analogously as\n(1.8) \u03baf,rel(u) = lim \u21920 sup \u2016e\u2016U \u2016u\u2016U \u22641\n\u2016f(u+ e)\u2212 f(u)\u2016V \u2016f(u)\u2016V = \u03baf (u) \u00b7 \u2016u\u2016U \u2016f(u)\u2016V .\nCondition numbers are geometric invariants, meaning that (once an arbitrary, but fixed, choice of norms is made) they are an intrinsic property of the function f and\n3 the point u; in other words, they do not depend on the algorithm used in practice to compute f(u). In numerical analysis, their use is popular to measure how difficult it is to accurately compute f in finite precision arithmetic. This has been effectively summarized by N. Higham [12, Sec. 1.6] with the \u201crule of thumb\u201d\nforward error . \u03ba \u00b7 backward error,\nwhich comes from a first order expansion and therefore is technically true only for finite condition numbers and small enough values of the backward error. In practice, it is often still valid more generally as an approximate inequality. For a backward stable algorithm, the backward error is guaranteed to be small, but whether this implies that the output of the computation is accurate (that is, whether the forward error is small) also depends on the condition number \u03ba. Finally, structured condition numbers can be defined analogously, but with the further restriction that the set of allowed perturbations e must satisfy certain properties. In other words, instead of taking the supremum over the unit ball \u2016e\u2016U \u2264 1 as in (1.7), for defining a structured condition number one takes the supremum over a certain subset of the unit ball. For example, if u is a Hermitian matrix and U = Cn\u00d7n, then one may impose that e is such that u+ e is also a Hermitian matrix. Note that, by the properties of the supremum, this immediately implies that a structured condition number is less than or equal to the corresponding unstructured condition number; equality is however generally possible.\nIn this paper, we specialize the general theory to condition numbers of simple zeros (also called eigenvalues) of either rational or polynomial matrices, always assumed to be regular. Suppose that \u03bb0 is a simple zero for a rational matrix R(\u03bb), associated with left and right root vectors [10, 18] y(\u03bb) and x(\u03bb) respectively. Then, as we prove in Lemma 3.3, for any sufficiently small > 0 and any perturbation \u2206R(\u03bb, ) = \u2206R(\u03bb)+ o( ) satisfying certain technical assumptions (corresponding to perturbing the blocks of a polynomial system matrix), the perturbed rational matrix R(\u03bb) + \u2206R(\u03bb, ) has\na finite zero \u03bb\u03020( ) such that\n|\u03bb\u03020( )\u2212 \u03bb0| = lim \u03bb\u2192\u03bb0 \u2223\u2223\u2223\u2223y(\u03bb)T\u2206R(\u03bb)x(\u03bb)y(\u03bb)TR\u2032(\u03bb)x(\u03bb) \u2223\u2223\u2223\u2223+ o( ).\nFrom this result, after giving some preliminaries on rational matrices and polynomial system matrices in Section 2, we define in Section 3 a structured condition number \u03baS for a locally minimal [5] polynomial system matrix P (\u03bb) of a rational matrix R(\u03bb), where the structured perturbation \u2206P (\u03bb) preserves the degrees of the blocks of P (\u03bb) in (1.2). For that, we fix the function f to be the input-output function\nthat maps P (\u03bb) to \u03bb0 and P (\u03bb) + \u2206P (\u03bb) to \u03bb\u03020( ). In Section 4, we derive an expression for \u03baS . Then, in Section 5, we show that this structured condition number for polynomial system matrices is never larger, and can be much smaller, than Tisseur\u2019s unstructured condition number for matrix polynomials [22]. If we consider the case in which the rational matrixR(\u03bb) is expressed in the form R(\u03bb) = D(\u03bb)+C(\u03bbIn\u2212A)\u22121B, where D(\u03bb) is a polynomial matrix and C(\u03bbIn \u2212A)\u22121B is a minimal state-space realization, it was shown in [8] that there are algorithms to compute the zeros of R(\u03bb), via linearization, that guarantee a small (global) backward error, preserving precisely the structure of R(\u03bb). Note that this is a particular case in the representation (1.1) where the matrix A(\u03bb) is linear and the matrices B(\u03bb) and C(\u03bb) are constant, but expressing R(\u03bb) in that form is valid for any rational matrix and is used in applications [21]. Therefore, in problems where the structured condition number is much smaller than the unstructured one, structured algorithms can thus eventually lead to\n4 significant increase in accuracy. We also include some numerical experiments on this comparison in Section 6. Finally, in Section 7 we give some conclusions and related open problems."
        },
        {
            "heading": "2. Preliminaries.",
            "text": "2.1. Rational matrices and related subspaces. A rational matrix R(\u03bb) \u2208 C(\u03bb)p\u00d7m is a matrix whose entries are scalar rational functions in the variable \u03bb. Given \u03bb0 \u2208 C, R(\u03bb) is said to be defined at \u03bb0 if R(\u03bb0) \u2208 Cp\u00d7m and is said to be invertible at \u03bb0 if, in addition, detR(\u03bb0) 6= 0. Rational matrices can have zeros and poles, which can be defined through the notion of the local Smith\u2013McMillan form. That is, for any rational matrix R(\u03bb) \u2208 C(\u03bb)p\u00d7m, with normal rank r, and \u03bb0 \u2208 C there exist rational matrices G1(\u03bb) and G2(\u03bb), that are invertible at \u03bb0, such that\n(2.1) G1(\u03bb)R(\u03bb)G2(\u03bb) =\n[ diag ((\u03bb\u2212 \u03bb0)\u03bd1 , . . . , (\u03bb\u2212 \u03bb0)\u03bdr ) 0\n0 0(p\u2212r)\u00d7(m\u2212r)\n] ,\nwhere \u03bd1 \u2264 \u00b7 \u00b7 \u00b7 \u2264 \u03bdr are integers. The integers \u03bd1, . . . , \u03bdr are uniquely determined by R(\u03bb) and \u03bb0 and are called the invariant orders at \u03bb0 of R(\u03bb). The matrix in (2.1) is called the local Smith\u2013McMillan form of R(\u03bb) at \u03bb0.\nIf \u03bdi > 0, for some i = 1, . . . , r, then \u03bb0 is a zero (or eigenvalue) of R(\u03bb) with partial multiplicity \u03bdi, and (\u03bb\u2212\u03bb0)\u03bdi is said to be a zero elementary divisor of R(\u03bb) at \u03bb0. If \u03bdi < 0, for some i = 1, . . . , r, then \u03bb0 is a pole of R(\u03bb) with partial multiplicity \u2212\u03bdi, and (\u03bb\u2212 \u03bb0)\u2212\u03bdi is said to be a pole elementary divisor of R(\u03bb) at \u03bb0 [13, 23]. A zero of R(\u03bb) is said to be simple if it only has one positive invariant order and it is equal to 1 (but a simple zero can also be a pole with any pole partial multiplicities). The (global) definition of the Smith\u2013McMillan form of a rational matrix can be found in [13, 21, 23] and was first given by B. McMillan in [16].\nIf R(\u03bb) is a polynomial matrix then the nonzero integers \u03bdi 6= 0 are all positive and are called partial multiplicities of R(\u03bb) at \u03bb0 [9]. In addition, in this case, the diagonal matrix in (2.1) is simply called the local Smith form of R(\u03bb) at \u03bb0 and the polynomials (\u03bb\u2212 \u03bb0)\u03bdi with \u03bdi 6= 0 are called the elementary divisors of R(\u03bb) at \u03bb0. The finite zeros of polynomial matrices are also called eigenvalues.\nFor rational matrices having full column normal rank, zeros can be determined by the following equivalent definition [3, Chapter 27].\nDefinition 2.1 (Zero location). A rational matrix R(\u03bb) \u2208 C(\u03bb)p\u00d7m with full column normal rank has a zero at \u03bb0 if there exists a rational vector x(\u03bb) \u2208 C(\u03bb)m defined at \u03bb0 such that x(\u03bb0) 6= 0 and\n(2.2) lim \u03bb\u2192\u03bb0 R(\u03bb)x(\u03bb) = 0.\nIt follows that, in the regular case, i.e., p = m and detR(\u03bb) 6\u2261 0, R(\u03bb) has a zero at \u03bb0 if there exist rational vectors x(\u03bb) \u2208 C(\u03bb)m and y(\u03bb) \u2208 C(\u03bb)m defined at \u03bb0 such that\n(2.3) lim \u03bb\u2192\u03bb0 R(\u03bb)x(\u03bb) = 0 and lim \u03bb\u2192\u03bb0\ny(\u03bb)TR(\u03bb) = 0,\nwith x(\u03bb0) 6= 0 and y(\u03bb0) 6= 0. Then x(\u03bb) and y(\u03bb) are said to be right and left root vectors of R(\u03bb) associated with \u03bb0, respectively [18]. Their evaluation at \u03bb0, that is, x(\u03bb0) and y(\u03bb0), are called right and left eigenvectors of R(\u03bb) associated with \u03bb0.\n5 Remark 2.2. The REP is often defined in the literature as the problem of finding scalars \u03bb0 \u2208 C and nonzero vectors x \u2208 Cm and y \u2208 Cm such that R(\u03bb0)x = 0 and yTR(\u03bb0) = 0. But this definition assumes that \u03bb0 is a zero of R(\u03bb) but not a pole, as R(\u03bb) would be defined at \u03bb0. In this paper we consider the more general definition in (2.3) to define the REP in order to capture all the zeros of R(\u03bb).\nRemark 2.3. The rational vectors x(\u03bb) and y(\u03bb) appearing in (2.3) are root vectors of R(\u03bb) for the special case of a regular rational matrix R(\u03bb), see [18, Definition 3.4] for a more general definition of root vectors for rational matrices, not necessarily regular or square, and over arbitray fields. At least in the regular case, root vectors had been studied in the more general context of nonlinear eigenvalue problems over the complex field: see [10] and the references therein; for the (more involved) nonregular case, a thorough analysis has recently been made for polynomial [7] and rational [18] matrices over an arbitrary field.\nIn particular, the theory of root vectors for rational functions can be refined [18] so that they not only identify zeros, but also their partial multiplicities (via the concept of a maximal set). As in this paper we will restrict to simple zeros, these details are not important. We refer the reader who would like to know more about maximal sets of root vectors to [7, 10, 18] and the references therein.\nExample 2.4. Consider the rational matrix R(\u03bb) = [ 1 0 1\n\u03bb\u22121 1\n] . It is clear that\nR(\u03bb) has a pole at \u03bb0 = 1, as R(\u03bb) is not defined at 1, but it is less clear that R(\u03bb) has also a zero at 1. If we consider x(\u03bb) = [ \u03bb\u22121 \u22121 ]\nwe have that lim \u03bb\u21921 R(\u03bb)x(\u03bb) = 0 and\nx(1) 6= 0, i.e., x(\u03bb) is a right root vector of R(\u03bb) associated with \u03bb0 = 1. Thus \u03bb0 = 1 is also a zero of R(\u03bb). Indeed, the Smith\u2013McMillan form of R(\u03bb) is\n[ 1\n\u03bb\u22121 0\n0 \u03bb\u22121\n] , which\nshows that R(\u03bb) has a zero and a pole at \u03bb0 = 1.\n2.2. Null spaces. For any rational matrix R(\u03bb) (regular or singular), we define the following rational vector spaces over C(\u03bb):\nkerR(\u03bb) = {x(\u03bb) \u2208 C(\u03bb)m\u00d71 : R(\u03bb)x(\u03bb) = 0}, and cokerR(\u03bb) = {y(\u03bb)T \u2208 C(\u03bb)1\u00d7p : y(\u03bb)TR(\u03bb) = 0},\nwhich are called the right and left null spaces over C(\u03bb) of R(\u03bb), respectively. For a finite \u03bb0 \u2208 C that is not a pole of R(\u03bb), kerR(\u03bb0) and cokerR(\u03bb0) denote the right and left null spaces over C of the constant matrix R(\u03bb0), respectively. Namely,\nkerR(\u03bb0) = {x \u2208 Cm\u00d71 : R(\u03bb0)x = 0}, and cokerR(\u03bb0) = {yT \u2208 C1\u00d7p : yTR(\u03bb0) = 0}.\nThe vector spaces kerR(\u03bb0) and cokerR(\u03bb0) are called the right and left null spaces of R(\u03bb) at \u03bb0, respectively.\nThe definition of kerR(\u03bb0) and cokerR(\u03bb0) can be generalized to the case where \u03bb0 is possibly a pole of R(\u03bb) as follows:\nkerR(\u03bb0) := {x \u2208 Cm\u00d71 : there exists x(\u03bb) \u2208 C(\u03bb)m\u00d71 with lim \u03bb\u2192\u03bb0 R(\u03bb)x(\u03bb) = 0 and x(\u03bb0) = x}, and\ncokerR(\u03bb0) := {yT \u2208 C1\u00d7p : there exists y(\u03bb)T \u2208 C(\u03bb)1\u00d7p with lim \u03bb\u2192\u03bb0 y(\u03bb)TR(\u03bb) = 0 and y(\u03bb0) = y}.\n6 For regular rational matrices R(\u03bb), if \u03bb0 is a zero of R(\u03bb) then kerR(\u03bb0) and cokerR(\u03bb0) are non trivial and contain the right and left eigenvectors of R(\u03bb) associated with \u03bb0, respectively. In [18, Definition 3.8 and Remark 3.9] one can find a more general definition for kerR(\u03bb0) and cokerR(\u03bb0) valid over an arbitrary field F and that specializes to the one above when F = C."
        },
        {
            "heading": "2.3. Polynomial system matrices and transfer function matrices. It is",
            "text": "known that any rational matrix R(\u03bb) \u2208 C(\u03bb)p\u00d7m can be written with an expression of the form\n(2.4) R(\u03bb) = D(\u03bb) + C(\u03bb)A(\u03bb)\u22121B(\u03bb),\nwhere A(\u03bb), B(\u03bb), C(\u03bb) and D(\u03bb) are matrix polynomials, with A(\u03bb) \u2208 C[\u03bb]n\u00d7n regular. In addition, the representation (2.4) is not unique. The associated polynomial matrix\n(2.5) P (\u03bb) = [ \u2212A(\u03bb) B(\u03bb) C(\u03bb) D(\u03bb) ] is said to be a polynomial system matrix of the rational matrix R(\u03bb) in (2.4), and the rational matrix R(\u03bb) is called the transfer function matrix of P (\u03bb) (see [21]). Note that R(\u03bb) is the Schur complement of A(\u03bb) in P (\u03bb). The polynomial system matrix P (\u03bb) in (2.5), with n > 0, is said to be minimal if the polynomial matrices\n(2.6)\n[ \u2212A(\u03bb) C(\u03bb) ] and [ A(\u03bb) B(\u03bb) ] have no eigenvalues in C [21]. An important consequence of minimality is that, if a polynomial system matrix P (\u03bb) is minimal, the poles of its transfer function matrix R(\u03bb) are the zeros of A(\u03bb), and the zeros of R(\u03bb) are the zeros of P (\u03bb), together with their partial multiplicities [21, Chapter 3, Theorem 4.1]. The notion of minimality was extended in a local sense in [5]. For that, if the matrices in (2.6) have no eigenvalues in a nonempty subset \u2126 \u2286 C, P (\u03bb) is said to be minimal in \u2126 [5, Definition 3.1]. Then, if a polynomial system matrix is minimal in a set \u2126, the poles of R(\u03bb) in \u2126 are the zeros of A(\u03bb) in \u2126, and the zeros of R(\u03bb) in \u2126 are the zeros of P (\u03bb) in \u2126, together with their partial multiplicities [5, Theorem 3.5]. If P (\u03bb) is minimal in \u2126 := {\u03bb0}, with \u03bb0 \u2208 C, then P (\u03bb) is said to be minimal at \u03bb0.\nGiven a polynomial system matrix P (\u03bb) minimal at \u03bb0, the following Proposition 2.5 establishes the relation between the right and left null spaces of P (\u03bb) and its transfer function matrix R(\u03bb) at \u03bb0, and it is valid for arbitrary rational matrices R(\u03bb) (regular or singular). This result was stated in [6, Lemma 2.1] in the (easier) special case when \u03bb0 is not a pole of R(\u03bb), which follows from [4, Proposition 5.1] and [4, Proposition 5.2]. Here, we give a proof of the general case.\nProposition 2.5. Let\nP (\u03bb) = [ \u2212A(\u03bb) B(\u03bb) C(\u03bb) D(\u03bb) ] \u2208 C[\u03bb](n+p)\u00d7(n+m)\nbe a polynomial system matrix, with A(\u03bb) \u2208 C[\u03bb]n\u00d7n regular, and transfer function matrix R(\u03bb) = D(\u03bb) +C(\u03bb)A(\u03bb)\u22121B(\u03bb) \u2208 C(\u03bb)p\u00d7m. Let \u03bb0 \u2208 C. If P (\u03bb) is minimal at \u03bb0 then the following statements hold:\n7 (a) The linear map\nLr : kerR(\u03bb0) \u2212\u2192 kerP (\u03bb0), x0 7\u2212\u2192\n[ lim \u03bb\u2192\u03bb0 A(\u03bb)\u22121B(\u03bb)x(\u03bb)\nx0 ] is a bijection, where x(\u03bb) \u2208 C(\u03bb)m is such that lim\n\u03bb\u2192\u03bb0 R(\u03bb)x(\u03bb) = 0 and\nx(\u03bb0) = x0. (b) The linear map\nL` : cokerR(\u03bb0) \u2212\u2192 cokerP (\u03bb0), yT0 7\u2212\u2192 [\nlim \u03bb\u2192\u03bb0\ny(\u03bb)TC(\u03bb)A(\u03bb)\u22121 yT0 ]\nis a bijection, where y(\u03bb) \u2208 C(\u03bb)p is such that lim \u03bb\u2192\u03bb0 y(\u03bb)TR(\u03bb) = 0 and y(\u03bb0) = y0.\nProof. We only prove part (a) since part (b) is analogous. First, we prove that Lr is well-defined. Let x0 \u2208 kerR(\u03bb0). Then there exists\na rational vector x(\u03bb) \u2208 C(\u03bb)m such that lim \u03bb\u2192\u03bb0 R(\u03bb)x(\u03bb) = 0 and x(\u03bb0) = x0, by definition. Since lim \u03bb\u2192\u03bb0 R(\u03bb)x(\u03bb) = 0 we have that\n(2.7) lim \u03bb\u2192\u03bb0 [ \u2212A(\u03bb) B(\u03bb) C(\u03bb) D(\u03bb) ] [ A(\u03bb)\u22121B(\u03bb)x(\u03bb) x(\u03bb) ] = 0.\nThen, note that (2.7) can be rewritten as\n(2.8) lim \u03bb\u2192\u03bb0 ([ \u2212A(\u03bb) C(\u03bb) ] A(\u03bb)\u22121B(\u03bb)x(\u03bb) + [ B(\u03bb) D(\u03bb) ] x(\u03bb) ) = 0.\nSince P (\u03bb) is minimal at \u03bb0, we know that [ \u2212A(\u03bb) C(\u03bb) ] has a rational left inverse that is defined at \u03bb0. In other words, there exists a rational matrix H(\u03bb) such that \u03bb0 is\nnot a pole of H(\u03bb) and H(\u03bb) [ \u2212A(\u03bb) C(\u03bb) ] = I (the claim follows immediately from the\nobservation that the local Smith form of [ \u2212A(\u03bb) C(\u03bb) ] at \u03bb0 is [ I0 ]). Taking into account (2.8), we obtain that the limit\nlim \u03bb\u2192\u03bb0 A(\u03bb)\u22121B(\u03bb)x(\u03bb) = lim \u03bb\u2192\u03bb0 H(\u03bb) [ \u2212A(\u03bb) C(\u03bb) ] A(\u03bb)\u22121B(\u03bb)x(\u03bb)\n= \u2212 lim \u03bb\u2192\u03bb0 H(\u03bb) [ B(\u03bb) D(\u03bb) ] x(\u03bb) = H(\u03bb0) [ B(\u03bb0) D(\u03bb0) ] x\nis defined. Thus, lim \u03bb\u2192\u03bb0\n[ A(\u03bb)\u22121B(\u03bb)x(\u03bb)\nx(\u03bb)\n] \u2208 kerP (\u03bb0). This proves that Lr is well-\ndefined. Next, note that if [ lim\u03bb\u2192\u03bb0 A(\u03bb)\n\u22121B(\u03bb)x(\u03bb) x0\n] = 0 then x0 = 0, and this es-\ntablishes the injectivity of Lr. Finally, to see that Lr is a bijection, we prove that dim kerR(\u03bb0) = dim kerP (\u03bb0). Denote by s the number of positive invariant orders of R(\u03bb) at \u03bb0; observe that s is also equal to the number of partial multiplicities of P (\u03bb) at \u03bb0, by [5, Theorem 3.5], since P (\u03bb) is minimal at \u03bb0. By [18, Theorem 3.10],\ndim kerR(\u03bb0) = dim kerR(\u03bb) + s, dim kerP (\u03bb0) = dim kerP (\u03bb) + s.\nOn the other hand, by [19, Lemma 2.1], dim kerP (\u03bb) = dim kerR(\u03bb), and hence dim kerP (\u03bb0) = dim kerR(\u03bb0).\n8 Remark 2.6 (Notation). If \u03bb0 is a pole of R(\u03bb), then A(\u03bb) \u22121 is not defined at \u03bb0. However, as we proved in Proposition 2.5, if P (\u03bb) is minimal at \u03bb0 the limit lim \u03bb\u2192\u03bb0 A(\u03bb)\u22121B(\u03bb)x(\u03bb) is defined at \u03bb0, even if \u03bb0 is a pole of R(\u03bb). Thus, in what follows, for a given x(\u03bb) \u2208 C(\u03bb)m such that lim \u03bb\u2192\u03bb0 R(\u03bb)x(\u03bb) = 0, we will slightly abuse notation and write A(\u03bb0) \u22121B(\u03bb0)x(\u03bb0) as a shorthand for lim\n\u03bb\u2192\u03bb0 A(\u03bb)\u22121B(\u03bb)x(\u03bb); this\nshould be interpreted as the rational vector A(\u03bb)\u22121B(\u03bb)x(\u03bb) evaluated at \u03bb0 (this evualuation is guaranteed to yield a complex vector by Proposition 2.5). The same notation will be used for the limit lim\n\u03bb\u2192\u03bb0 y(\u03bb)TC(\u03bb)A(\u03bb)\u22121 for any y(\u03bb) \u2208 C(\u03bb)p such\nthat lim \u03bb\u2192\u03bb0\ny(\u03bb)TR(\u03bb) = 0, i.e., y(\u03bb0) TC(\u03bb0)A(\u03bb0) \u22121.\nWe know that if \u03bb0 is a zero of a rational matrix R(\u03bb) then \u03bb0 is also a zero (an eigenvalue) of any polynomial system matrix P (\u03bb) of R(\u03bb), with same partial multiplicities, whenever P (\u03bb) is minimal at \u03bb0 [5, Theorem 3.5]. Then Proposition 2.5 shows a corresponding relation between the eigenvectors, even if \u03bb0 is not only a zero but also a pole of R(\u03bb). Moreover, since the maps in Proposition 2.5 are bijections, they preserve linear independence. Thus, one can recover a basis of kerP (\u03bb0) (resp. cokerP (\u03bb0)) from a basis of kerR(\u03bb0) (resp. cokerR(\u03bb0)), and conversely.\nExample 2.7. Consider the rational matrix R(\u03bb) in Example 2.4, and the right root vector x(\u03bb) of R(\u03bb) associated with \u03bb0 = 1. Define the polynomial system matrix\nP (\u03bb) :=  \u03bb\u2212 1 1 00 1 0 \u22121 0 1  =: [\u2212A(\u03bb) B(\u03bb) C(\u03bb) D(\u03bb) ] ,\nwhose transfer function matrix is R(\u03bb) and is minimal at \u03bb0 = 1. Then \u03bb0 = 1 is also a zero of P (\u03bb) and v = [ A(\u03bb0)\n\u22121B(\u03bb0)x(\u03bb0) x(\u03bb0)\n] = [ \u22121 0 \u22121 ]T is a right eigenvector of"
        },
        {
            "heading": "P (\u03bb) associated with \u03bb0 = 1.",
            "text": "3. Perturbation expansion for simple zeros. Let R(\u03bb) be the transfer function matrix of a polynomial system matrix P (\u03bb). In this section, we assume that R(\u03bb) is regular (so that P (\u03bb) is also regular) and that \u03bb0 is a simple finite zero of R(\u03bb). If P (\u03bb) is minimal at \u03bb0, \u03bb0 is also a simple eigenvalue of P (\u03bb) [5, Theorem 3.5]. We first study how analytic perturbations of P (\u03bb) affect to simple eigenvalues and local minimality.\nTheorem 3.1. Let P (\u03bb) = [ \u2212A(\u03bb) B(\u03bb) C(\u03bb) D(\u03bb) ] be a regular polynomial system matrix, with A(\u03bb) regular. Let \u03bb0 \u2208 C be a finite simple eigenvalue of P (\u03bb), with right (resp. left) eigenvector v (resp. w), and assume that P (\u03bb) is minimal at \u03bb0. Let\nP\u0302 (\u03bb, ) = [ \u2212A\u0302(\u03bb, ) B\u0302(\u03bb, ) C\u0302(\u03bb, ) D\u0302(\u03bb, ) ]\nbe a matrix-valued function that can be written as P\u0302 (\u03bb, ) = \u2211d i=0 Pi( )\u03bb\ni where each Pi( ) is an analytic matrix function of in some open ball centered at 0. Assume that P\u0302 (\u03bb, 0) = P (\u03bb) Then there exists a constant r > 0 such that the following statements hold:\n(a) There exist functions \u03bb\u03020( ), v\u0302( ) and w\u0302( ) such that (i) \u03bb\u03020( ), v\u0302( ) and w\u0302( )\nare analytic in for all | | < r; and (ii) \u03bb\u03020(0) = \u03bb0, w\u0302(0) = w and v\u0302(0) = v.\n9 (b) P\u0302 (\u03bb, ) has a simple zero at \u03bb\u03020( ), for all | | < r, with associated right (resp. left) eigenvector v\u0302( ) (resp. w\u0302( )).\n(c) P\u0302 (\u03bb, ), viewed as a polynomial matrix in \u03bb, is a polynomial system matrix\nminimal at both \u03bb0 and \u03bb\u03020( ) for all | | < r. (d) The transfer function matrix R\u0302(\u03bb, ) of P\u0302 (\u03bb, ) has a simple zero at \u03bb\u03020( ) for\nall | | < r. Proof. Items (a) and (b) are classical results, see for example [1, 14].\nFor (c), we only prove that for sufficiently small , P\u0302 (\u03bb, ) must be minimal at \u03bb\u03020( ). Indeed, minimality is equivalent to [ \u2212A\u0302(\u03bb\u03020( ), ) B\u0302(\u03bb\u03020( ), ) ] having full row\nrank and [ \u2212A\u0302(\u03bb\u03020( ), ) C\u0302(\u03bb\u03020( ), ) ] having full column rank. On the other hand, P\u0302 (\u03bb\u03020( ), ) is analytic in ; hence, so is any of its submatrices and the statement follows by the continuity of singular values [14].\nFinally, (d) follows from (b) and (c). That is, since P\u0302 (\u03bb, ) is minimal at \u03bb\u03020( ) and has a simple zero at \u03bb\u03020( ), by [5, Theorem 3.5], it implies that \u03bb\u03020( ) is also a simple zero of R\u0302(\u03bb, ).\nRemark 3.2. For a given rational matrix R(\u03bb), not every small perturbation of\nR(\u03bb) has a polynomial system matrix P\u0302 (\u03bb, ) satisfying the assumptions in Theorem 3.1. For instance, consider the rational matrix\nR(\u03bb) =\n[ 1 \u03bb 1 \u03bb3\n0 1\u03bb\n] .\nThe invariant orders of R(\u03bb) at 0 are \u22123, 1, and there are not any other finite zeros or poles. For any > 0, let\nR(\u03bb) + \u2206R(\u03bb) = 1\n\u03bb3\n[ \u03bb2 + 1\n0 \u03bb2 \u2212\n] , where \u2206R(\u03bb) = [ 1 \u03bb3 0 0 \u2212 1\u03bb3 ] .\nThe invariant orders of R(\u03bb) + \u2206R(\u03bb) at 0 are \u22123,\u22123. Then, 0 is a pole. Moreover, the zeros of R(\u03bb) + \u2206R(\u03bb) are not analytic in : indeed, they can be shown to be equal to \u00b1 1/2 and \u00b1i 1/2. Thus, the statement of Theorem 3.1(d) is not satisfied for this particular perturbation. However, this is not a counterexample to the theorem because the assumptions are also false: suppose indeed that P (\u03bb) = P\u0302 (\u03bb, 0) such\nthat P (\u03bb) and P\u0302 (\u03bb, ) are polynomial system matrices of R(\u03bb) and R(\u03bb) + \u2206R(\u03bb), respectively, both minimal at 0. Then, by [5, Theorem 3.5], the matrices A(\u03bb) and A\u0302(\u03bb, ) must have determinant, respectively, p(\u03bb)\u03bb3 and f(\u03bb, )\u03bb6 where p(\u03bb) is a polynomial with p(0) 6= 0 and f(\u03bb, ) is a function polynomial in \u03bb, analytic in with f(0, ) 6= 0. But then, if P (\u03bb) = P\u0302 (\u03bb, 0), by the continuity of the determinant we would have that f(\u03bb, 0)\u03bb6 = p(\u03bb)\u03bb3, which is impossible. Although every rational matrix admits minimal polynomial system matrices, the crux here is the requirement P\u0302 (\u03bb, 0) = P (\u03bb). For example,\nP (\u03bb) =  \u03bb3 \u03bb2 11 0 0 \u03bb2 \u2212\u03bb 0  and Q(\u03bb, ) =  \u03bb3 0 0 0 \u03bb3 \u03bb2 1 0 1 0 0 \u03bb2 \u2212\u03bb 0  are minimal polynomial system matrices of R(\u03bb) and R(\u03bb) + \u2206R(\u03bb), respectively, but clearly Q(\u03bb, 0) 6= P (\u03bb) (as the sizes are different).\n10\nNevertheless, in this paper we consider perturbed rational matrices having polynomial system matrices that always satisfy the assumptions in Theorem 3.1. More precisely, we consider rational matrices R(\u03bb) expressed as in (2.4), i.e., R(\u03bb) = D(\u03bb) + C(\u03bb)A(\u03bb)\u22121B(\u03bb), for arbitrary polynomial matrices A(\u03bb), B(\u03bb), C(\u03bb) and\nD(\u03bb), and its associated polynomial system matrix P (\u03bb) = [ \u2212A(\u03bb) B(\u03bb) C(\u03bb) D(\u03bb) ] . Then, we consider a perturbation of P (\u03bb) of the form:\n(3.1) P (\u03bb) + \u2206P (\u03bb) := [ \u2212A(\u03bb) B(\u03bb) C(\u03bb) D(\u03bb) ] + [ \u2212\u2206A(\u03bb) \u2206B(\u03bb) \u2206C(\u03bb) \u2206D(\u03bb) ] ,\nwhere \u2206A(\u03bb), \u2206B(\u03bb), \u2206C(\u03bb) and \u2206D(\u03bb) are polynomial matrices. It is clear that\nP\u0302 (\u03bb, ) := P (\u03bb) + \u2206P (\u03bb) satisfies the assumptions in Theorem 3.1 as it is a linear perturbation in of P (\u03bb). In order to study perturbations of simple eigenvalues, we will use Lemma 3.3, which is known for general polynomial matrices [22, Proof of Theorem 5] but that we state here for the particular case of polynomial system matrices.\nLemma 3.3. Let \u03bb0 \u2208 C be a simple eigenvalue of a regular polynomial system matrix P (\u03bb); and let v and w be, respectively, right and left eigenvectors of P (\u03bb) associated with \u03bb0. Consider the perturbed polynomial system matrix matrix P (\u03bb) + \u2206P (\u03bb). Then, for > 0 small enough, there exist \u2206\u03bb0 and \u2206v satisfying\n[P (\u03bb0 + \u2206\u03bb0) + \u2206P (\u03bb0 + \u2206\u03bb0)] [v + \u2206v] = 0.\nThat is, \u03bb0 +\u2206\u03bb0 is an eigenvalue for the perturbed polynomial system matrix P (\u03bb)+ \u2206P (\u03bb). In addition,\n(3.2) |\u2206\u03bb0| = \u2223\u2223\u2223\u2223wT\u2206P (\u03bb0)vwTP \u2032(\u03bb0)v \u2223\u2223\u2223\u2223+ o( ). Now, we consider the perturbed transfer function matrix of the perturbed polynomial system matrix in (3.1). Namely,\nR(\u03bb) + \u2206R(\u03bb, ) := D(\u03bb) + \u2206D(\u03bb)+\n(C(\u03bb) + \u2206C(\u03bb))(A(\u03bb) + \u2206A(\u03bb))\u22121(B(\u03bb) + \u2206B(\u03bb)).\n(3.3)\nThen, we set\n\u2206R(\u03bb) :=\n[ \u2202\u2206R(\u03bb, )\n\u2202\n] =0 = \u2206D(\u03bb) + \u2206C(\u03bb)A(\u03bb)\u22121B(\u03bb) + C(\u03bb)A(\u03bb)\u22121\u2206B(\u03bb)\n\u2212 C(\u03bb)A(\u03bb)\u22121\u2206A(\u03bb)A(\u03bb)\u22121B(\u03bb).\n(3.4)\nNote that (3.4) implies the formal 1 expansion \u2206R(\u03bb, ) = \u2206R(\u03bb) + o( ). In Corollary 3.6, we write (3.2) in terms of R(\u03bb) and \u2206R(\u03bb) by using root vectors [18]. For that, we need Lemma 3.4.\n1While one can always write such a formal expansion, when \u03bb \u2192 \u03bb0 and \u03bb0 is a pole of R(\u03bb) then the radius of convergence (in ) of such an expansion may tend to 0. For example, consider\nR(\u03bb) + \u2206R(\u03bb, ) = 1\n\u03bb\u2212 =\n1\n\u03bb \u221e\u2211 k=0 k \u03bbk , where R(\u03bb) = 1 \u03bb and \u2206R(\u03bb, ) = \u03bb2 + o( ).\nFor any fixed \u03bb 6= 0, the radius of convergence of the power series in is |\u03bb|. Hence, there is no uniform radius of convergence for \u03bb in a neighbourhood of the pole \u03bb0 = 0.\n11\nLemma 3.4. Let P (\u03bb) \u2208 C[\u03bb](n+p)\u00d7(n+p) be a polynomial system matrix as in (2.5), with A(\u03bb) \u2208 C[\u03bb]n\u00d7n regular, and transfer function matrix R(\u03bb) = D(\u03bb) + C(\u03bb)A(\u03bb)\u22121B(\u03bb) \u2208 C(\u03bb)p\u00d7p. Assume that P (\u03bb) is minimal at \u03bb0. Let v = [ v1v2 ] \u2208 kerP (\u03bb0) and w T = [ wT1 w T 2 ] \u2208 cokerP (\u03bb0), partitioned conformably to the blocks of P (\u03bb), with v 6= 0 and w 6= 0. Then, there exist right and left root vectors x(\u03bb) \u2208 C(\u03bb)p and y(\u03bb) \u2208 C(\u03bb)p of R(\u03bb), both associated with \u03bb0, such that\n(3.5) wTP \u2032(\u03bb0)v = lim \u03bb\u2192\u03bb0 y(\u03bb)TR\u2032(\u03bb)x(\u03bb),\nwith x(\u03bb0) = v2 and y(\u03bb0) = w2, and that\n(3.6) wT\u2206P (\u03bb0)v = lim \u03bb\u2192\u03bb0 y(\u03bb)T\u2206R(\u03bb)x(\u03bb),\nwhere \u2206R(\u03bb) is defined as in (3.4).\nProof. By Proposition 2.5, we know that if v \u2208 kerP (\u03bb0) and w \u2208 cokerP (\u03bb0)\nthen v = [ lim \u03bb\u2192\u03bb0 A(\u03bb)\u22121B(\u03bb)x(\u03bb)\nx(\u03bb0)\n] and wT = [ lim \u03bb\u2192\u03bb0 y(\u03bb)TC(\u03bb)A(\u03bb)\u22121 y(\u03bb0) T ] for\nsome x(\u03bb) \u2208 C(\u03bb)p and y(\u03bb) \u2208 C(\u03bb)p with lim \u03bb\u2192\u03bb0 R(\u03bb)x(\u03bb) = 0 and lim \u03bb\u2192\u03bb0 y(\u03bb)TR(\u03bb) = 0, respectively. Then\nwTP \u2032(\u03bb0)v\n= [\nlim \u03bb\u2192\u03bb0\ny(\u03bb)TC(\u03bb)A(\u03bb)\u22121 y(\u03bb0) T ] [\u2212A\u2032(\u03bb0) B\u2032(\u03bb0)\nC\u2032(\u03bb0) D \u2032(\u03bb0)\n][ lim \u03bb\u2192\u03bb0 A(\u03bb)\u22121B(\u03bb)x(\u03bb)\nx(\u03bb0) ] = lim \u03bb\u2192\u03bb0 y(\u03bb)T (D\u2032(\u03bb) + C\u2032(\u03bb)A(\u03bb)\u22121B(\u03bb) + C(\u03bb)A(\u03bb)\u22121B\u2032(\u03bb)\n\u2212 C(\u03bb)A(\u03bb)\u22121A\u2032(\u03bb)A(\u03bb)\u22121B(\u03bb))x(\u03bb)\n= lim \u03bb\u2192\u03bb0\ny(\u03bb)TR\u2032(\u03bb)x(\u03bb)\nand\nwT\u2206P (\u03bb0)v\n= [\nlim \u03bb\u2192\u03bb0\ny(\u03bb)TC(\u03bb)A(\u03bb)\u22121 y(\u03bb0) T ] [\u2212\u2206A(\u03bb0) \u2206B(\u03bb0)\n\u2206C(\u03bb0) \u2206D(\u03bb0)\n] [ lim \u03bb\u2192\u03bb0 A(\u03bb)\u22121B(\u03bb)x(\u03bb)\nx(\u03bb0) ] = lim \u03bb\u2192\u03bb0 y(\u03bb)T (\u2206D(\u03bb) + \u2206C(\u03bb)A(\u03bb)\u22121B(\u03bb) + C(\u03bb)A(\u03bb)\u22121\u2206B(\u03bb)\n\u2212 C(\u03bb)A(\u03bb)\u22121\u2206A(\u03bb)A(\u03bb)\u22121B(\u03bb))x(\u03bb)\n= lim \u03bb\u2192\u03bb0\ny(\u03bb)T\u2206R(\u03bb)x(\u03bb),\nwith x(\u03bb0) = v2 and y(\u03bb0) = w2. We still have to prove that x(\u03bb0) 6= 0 and y(\u03bb0) 6= 0. We only prove that x(\u03bb0) 6= 0, since y(\u03bb0) 6= 0 can be proved analogously. Since lim \u03bb\u2192\u03bb0 R(\u03bb)x(\u03bb) = 0 we have that\nlim \u03bb\u2192\u03bb0 [ \u2212A(\u03bb) C(\u03bb) ] A(\u03bb)\u22121B(\u03bb)x(\u03bb) + [ B(\u03bb) D(\u03bb) ] x(\u03bb) = 0.\nBy contradiction, if we assume that x(\u03bb0) = 0, we get that lim \u03bb\u2192\u03bb0\n[ B(\u03bb) D(\u03bb) ] x(\u03bb) = 0\nand, therefore, lim \u03bb\u2192\u03bb0\n[ \u2212A(\u03bb) C(\u03bb) ] A(\u03bb)\u22121B(\u03bb)x(\u03bb) = 0. Taking into account that P (\u03bb)\n12\nis minimal at \u03bb0, this is, that lim \u03bb\u2192\u03bb0\n[ \u2212A(\u03bb) C(\u03bb) ] = [ \u2212A(\u03bb0) C(\u03bb0) ] has full column rank, we\nobtain that lim \u03bb\u2192\u03bb0\nA(\u03bb)\u22121B(\u03bb)x(\u03bb) = 0, which is a contradiction since this implies\nthat v = 0.\nRemark 3.5. We remark that wTP \u2032(\u03bb0)v 6= 0 by [1, Theorem 3.2], for right and left eigenvectors v and w of P (\u03bb), respectively, associated with a simple eigenvalue \u03bb0. Taking into account Lemma 3.4, this also implies that lim\n\u03bb\u2192\u03bb0 y(\u03bb)TR\u2032(\u03bb)x(\u03bb) 6= 0,\nfor right and left root vectors x(\u03bb) and y(\u03bb) of R(\u03bb), respectively, associated with a simple zero \u03bb0 of R(\u03bb).\nCorollary 3.6. Assume that the polynomial system matrix P (\u03bb) in Lemma 3.3 is minimal at \u03bb0, and let R(\u03bb) be the transfer function matrix of P (\u03bb). Let R(\u03bb) + \u2206R(\u03bb, ) be the transfer function matrix of the perturbed polynomial system matrix P (\u03bb) + \u2206P (\u03bb) in (3.1). Define \u2206R(\u03bb) as in (3.4). Then, for > 0 small enough, \u03bb0 + \u2206\u03bb0 is also a zero for R(\u03bb) + \u2206R(\u03bb, ). In addition, there exist right and left root vectors x(\u03bb) \u2208 C(\u03bb)p and y(\u03bb) \u2208 C(\u03bb)p of R(\u03bb) associated with \u03bb0, respectively, such that\n(3.7) |\u2206\u03bb0| = lim \u03bb\u2192\u03bb0 \u2223\u2223\u2223\u2223y(\u03bb)T\u2206R(\u03bb)x(\u03bb)y(\u03bb)TR\u2032(\u03bb)x(\u03bb) \u2223\u2223\u2223\u2223+ o( ).\n4. Condition number formulae. In this section, we define absolute condition numbers for simple zeros of minimal polynomial system matrices as in (1.2). For that, we express the block polynomial matrices A(\u03bb), B(\u03bb), C(\u03bb) and D(\u03bb) in terms of the monomial basis. Namely,\nD(\u03bb) = kD\u2211 i=0 Di\u03bb i \u2208 C[\u03bb]p\u00d7p, C(\u03bb) = kC\u2211 i=0 Ci\u03bb i \u2208 C[\u03bb]p\u00d7n,\nA(\u03bb) = kA\u2211 i=0 Ai\u03bb i \u2208 C[\u03bb]n\u00d7n, and B(\u03bb) = kB\u2211 i=0 Bi\u03bb i \u2208 C[\u03bb]n\u00d7p.\nThen, we perturb the matrix coefficients of each matrix polynomial, and the perturbed rational matrix we get is (3.3) with\n\u2206D(\u03bb) = kD\u2211 i=0 \u2206Di\u03bb i \u2208 C[\u03bb]p\u00d7p, \u2206C(\u03bb) = kC\u2211 i=0 \u2206Ci\u03bb i \u2208 C[\u03bb]p\u00d7n,\n\u2206A(\u03bb) = kA\u2211 i=0 \u2206Ai\u03bb i \u2208 C[\u03bb]n\u00d7n, and \u2206B(\u03bb) = kB\u2211 i=0 \u2206Bi\u03bb i \u2208 C[\u03bb]n\u00d7p.\nWith the above representation, the goal is to study how (simple) zeros of P (\u03bb) change to first order in . Recall that the perturbed rational matrix (3.3) is the transfer function matrix of the perturbed polynomial system matrix (3.1) and, by the discussion in Section 3, it is clear that it is equivalent to define such a structured condition number for the PEP referring to simple eigenvalues of (structured perturbations of) P (\u03bb) or simple zeros of the corresponding perturbations of its transfer function R(\u03bb).\nDefinition 4.1 (Absolute structured condition number of a simple zero). Let R(\u03bb) be a regular rational matrix as in (2.4), and let P (\u03bb) be a regular polynomial system matrix for R(\u03bb) as in (2.5). Assume that \u03bb0 \u2208 C is a simple finite eigenvalue\n13\nof R(\u03bb) and that P (\u03bb) is minimal at \u03bb0. Then, we define the structured absolute condition number of the eigenvalue \u03bb0 as follows:\n\u03baS(\u03bb0) := lim \u21920 sup\n{ |\u2206\u03bb0|\n: [P (\u03bb0 + \u2206\u03bb0) + \u2206P (\u03bb0 + \u2206\u03bb0)][v + \u2206v] = 0 with \u2016\u2206Di\u20162 \u2264 di, \u2016\u2206Ai\u20162 \u2264 ai , \u2016\u2206Bi\u20162 \u2264 bi , \u2016\u2206Ci\u20162 \u2264 ci } ,\nwhere the ai, bi, ci, di are nonnegative parameters, and v is a right eigenvector of P (\u03bb) associated with \u03bb0. Equivalently, \u03baS(\u03bb0) can be defined by using the transfer function matrix R(\u03bb) of P (\u03bb) as follows:\n\u03baS(\u03bb0) := lim \u21920 sup\n{ |\u2206\u03bb0|\n: \u03bb0 + \u2206\u03bb0 is a zero of R(\u03bb) + \u2206R(\u03bb, ) with \u2016\u2206Di\u20162 \u2264 di, \u2016\u2206Ai\u20162 \u2264 ai , \u2016\u2206Bi\u20162 \u2264 bi , \u2016\u2206Ci\u20162 \u2264 ci } .\nThe nonnegative parameters ai, bi, ci, di will be called weights and provide some freedom in how perturbations to the rational matrix R(\u03bb) are measured. For example, two natural choices are either ai = bi = ci = di = 1 (the coefficients are allowed to be perturbed by perturbations of uniform absolute magnitude) or ai = \u2016Ai\u2016, bi = \u2016Bi\u2016, ci = \u2016Ci\u2016, di = \u2016Di\u2016 (the coefficients are allowed to be perturbed by perturbations of uniform relative magnitude). We prefer to keep the weights as free parameters in our analysis as, in principle, a particular problem may suggest more peculiar choices. In Theorem 4.4 we derive a formula for \u03baS(\u03bb0).\nRemark 4.2. For any choice of positive weights, Definition 4.1 yields an absolute structured condition number (1.7) for the eigenvalue-computing function f : P (\u03bb) 7\u2192 \u03bb0 defined in Section 1.1. Here, U is the vector space of polynomial system matrices of the form (1.2) such that A(\u03bb), B(\u03bb), C(\u03bb) and D(\u03bb) are matrix polynomials of degree at most, resp., kA, kB , kC and kD. In particular, Definition 4.1 is the condition number induced by the choice of the norm\n(4.1) P (\u03bb) 7\u2192 \u2016P (\u03bb)\u2016 = max {\nkA max i=0 \u2016Ai\u2016 ai , kB max i=0 \u2016Bi\u2016 bi , kC max i=0 \u2016Ci\u2016 ci , kD max i=0 \u2016Di\u2016 di\n} ,\nwith the convention that if, e.g., B(\u03bb) = 0 then kB = \u2212\u221e and maxkBi=0 \u2016Bi\u2016 bi\n= 0. We leave as an exercise to the reader the straightforward verification that (4.1) is indeed a norm, for any choice of the parameters.\nRemark 4.3. A subtler problem is to assess whether Definition 4.1 yields a condition number for the eigenvalue-computing function f : R(\u03bb) 7\u2192 \u03bb0: indeed, (4.1) is clearly not a norm on the vector space of rational functions. On the other hand, it is worth observing that the set of allowed perturbation is highly structured with respect to the original REP, i.e., not every rational small perturbation of R(\u03bb) can be written in this form (in fact, in a precise geometric sense, only very few can). Indeed, note that the tangent space at = 0 to the set of perturbations that we consider is\n{\u2206D(\u03bb)+\u2206C(\u03bb)A(\u03bb)\u22121B(\u03bb)+C(\u03bb)A(\u03bb)\u22121\u2206B(\u03bb)\u2212C(\u03bb)A(\u03bb)\u22121\u2206A(\u03bb)A(\u03bb)\u22121B(\u03bb}\nwhich is a finite dimensional vector space over C. Indeed, even neglecting the issue of the potential non-unicity of the representation, its dimension is readily verified\n14\nto be \u2264 pm(\u03b7A + \u03b7B + \u03b7C + \u03b7D + 4) where \u03b7D = kD if D(\u03bb) 6= 0 or \u03b7D = \u22121 otherwise, and similarly for kA, kB , kC . In contrast, the vector space of p\u00d7m rational matrices has (uncountably) infinite dimension over C. Nevertheless, we believe that our definition provide a sensible practical tool and that restricting to this set of perturbation is justified when working with the representation of rational matrices that we are considering in this paper.\nNow, we derive a formula for the condition number introduced in Definition 4.1.\nTheorem 4.4 (Condition number formula). Let P (\u03bb) \u2208 C[\u03bb](n+p)\u00d7(n+p) be a regular polynomial system matrix as in (2.5), with A(\u03bb) \u2208 C[\u03bb]n\u00d7n regular, and transfer function matrix R(\u03bb). Assume that \u03bb0 \u2208 C is a simple eigenvalue of R(\u03bb) and that P (\u03bb) is minimal at \u03bb0. Then the structured absolute condition number \u03baS(\u03bb0) in Definition 4.1 is given by\n\u03baS(\u03bb0) = 1\nK\n[ \u2016y(\u03bb0)TC(\u03bb0)A(\u03bb0)\u22121\u20162 \u2016y(\u03bb0)\u20162 ] S(\u03bb0)\n[ \u2016A(\u03bb0)\u22121B(\u03bb0)x(\u03bb0)\u20162\n\u2016x(\u03bb0)\u20162\n] ,\nby using the notation in Remark 2.6, where x(\u03bb) and y(\u03bb) are, respectively, right and left root vectors of R(\u03bb) associated with \u03bb0,\nK := lim \u03bb\u2192\u03bb0\n|y(\u03bb)TR\u2032(\u03bb)x(\u03bb)| and S(\u03bb) :=\n[\u2211kA i=0 ai|\u03bb|i \u2211kB i=0 bi|\u03bb|i\u2211kC\ni=0 ci|\u03bb|i \u2211kD i=0 di|\u03bb|i\n] .\nProof. By using Lemma 3.3 and Proposition 2.5, and ignoring o( ) terms, we get\n|\u2206\u03bb0| \u2264\nK\n( \u2016x(\u03bb0)\u20162\u2016y(\u03bb0)\u20162\nkD\u2211 i=0 di|\u03bb0|i + \u2016A(\u03bb0)\u22121B(\u03bb0)x(\u03bb0)\u20162\u2016y(\u03bb0)\u20162 kC\u2211 i=0 ci|\u03bb0|i+\n\u2016x(\u03bb0)\u20162\u2016y(\u03bb0)TC(\u03bb0)A(\u03bb0)\u22121\u20162 kB\u2211 i=0 bi|\u03bb0|i+\n\u2016y(\u03bb0)TC(\u03bb0)A(\u03bb0)\u22121\u20162\u2016A(\u03bb0)\u22121B(\u03bb0)x(\u03bb0)\u20162 kA\u2211 i=0\nai|\u03bb0|i ) =\nK\n[ \u2016y(\u03bb0)TC(\u03bb0)A(\u03bb0)\u22121\u20162 \u2016y(\u03bb0)\u20162 ] S(\u03bb0)\n[ \u2016A(\u03bb0)\u22121B(\u03bb0)x(\u03bb0)\u20162\n\u2016x(\u03bb0)\u20162\n] .\nThis provides an upper bound for the condition number \u03baS(\u03bb0). We need to show that this upper bound is sharp, i.e., there is a perturbation of the form that we consider and that achieves the bound. Let us define\n\u00b5 = { 0 if \u03bb0 = 0; \u03bb0 |\u03bb0| otherwise,\nand let us consider the following rank-1 matrix perturbations: \u2022\n\u2206Di = di\u00b5 i y(\u03bb0)x(\u03bb0)\nT\n\u2016x(\u03bb0)\u20162\u2016y(\u03bb0)\u20162 ,\nfor i = 0, 1, . . . , kD. \u2022\n\u2206Ci = ci\u00b5 i y(\u03bb0)(A(\u03bb0)\n\u22121B(\u03bb0)x(\u03bb0)) T\n\u2016y(\u03bb0)\u20162\u2016A(\u03bb0)\u22121B(\u03bb0)x(\u03bb0)\u20162 ,\nif A(\u03bb0) \u22121B(\u03bb0)x(\u03bb0) 6= 0 or \u2206Ci = 0 otherwise, for i = 0, 1, . . . , kC .\n15\n\u2022 \u2206Bi = bi\u00b5 i (y(\u03bb0) TC(\u03bb0)A(\u03bb0) \u22121)Tx(\u03bb0) T\n\u2016x(\u03bb0)\u20162\u2016y(\u03bb0)TC(\u03bb0)A(\u03bb0)\u22121\u20162 ,\nif y(\u03bb0) TC(\u03bb0)A(\u03bb0) \u22121 6= 0 or \u2206Bi = 0 otherwise, for i = 0, 1, . . . , kB , and \u2022\n\u2206Ai = \u2212ai\u00b5i (y(\u03bb0)\nTC(\u03bb0)A(\u03bb0) \u22121)T (A(\u03bb0) \u22121B(\u03bb0)x(\u03bb0)) T\n\u2016y(\u03bb0)TC(\u03bb0)A(\u03bb0)\u22121\u20162\u2016A(\u03bb0)\u22121B(\u03bb0)x(\u03bb0)\u20162 ,\nif A(\u03bb0) \u22121B(\u03bb0)x(\u03bb0) 6= 0 and y(\u03bb0)TC(\u03bb0)A(\u03bb0)\u22121 6= 0 or \u2206Ai = 0 other-\nwise, for i = 0, 1, . . . , kA. It is clear that these matrix perturbations satisfy \u2016\u2206Di\u2016 \u2264 di, \u2016\u2206Bi\u2016 \u2264 bi,\n\u2016\u2206Ci\u2016 \u2264 ci and \u2016\u2206Ai\u2016 \u2264 ai. Then we have\ny(\u03bb0) T\n(\nkD\u2211 i=0 \u03bbi0\u2206Di\n) x(\u03bb0) = \u2016x(\u03bb0)\u20162\u2016y(\u03bb0)\u20162 ( kD\u2211 i=0 |\u03bb0|idi ) ,\ny(\u03bb0) T\n(\nkC\u2211 i=0 \u03bbi0\u2206Ci\n) (A(\u03bb0)\n\u22121B(\u03bb0)x(\u03bb0)) = \u2016A(\u03bb0)\u22121B(\u03bb0)x(\u03bb0)\u20162\u2016y(\u03bb0)\u20162 kC\u2211 i=0 ci|\u03bb0|i,\n(y(\u03bb0) TC(\u03bb0)A(\u03bb0) \u22121)\n(\nkB\u2211 i=0 \u03bbi0\u2206Bi\n) x(\u03bb0) = \u2016x(\u03bb0)\u20162\u2016y(\u03bb0)TC(\u03bb0)A(\u03bb0)\u22121\u20162\nkB\u2211 i=0 bi|\u03bb0|i,\nand\n(y(\u03bb0) TC(\u03bb0)A(\u03bb0) \u22121)\n(\nkA\u2211 i=0 \u03bbi0\u2206Ai\n) (A(\u03bb0) \u22121B(\u03bb0)x(\u03bb0)) =\n\u2212 \u2016y(\u03bb0)TC(\u03bb0)A(\u03bb0)\u22121\u20162\u2016A(\u03bb0)\u22121B(\u03bb0)x(\u03bb0)\u20162 kA\u2211 i=0 ai|\u03bb0|i.\nHence, for these perturbations all the norm inequalities are satisfied as equalities and, so, the upper bound is tight."
        },
        {
            "heading": "4.1. Condition number formula for zeros of R(\u03bb) that are not poles.",
            "text": "Theorem 4.4 can be simplified for those zeros \u03bb0 such that detA(\u03bb0) 6= 0. Note that, if detA(\u03bb0) 6= 0, the polynomial system matrix P (\u03bb) is minimal at \u03bb0. It is worth emphasizing that if detA(\u03bb0) = 0 not necessarily \u03bb0 is a pole of A(\u03bb), if we do not assume minimality on P (\u03bb) at \u03bb0, but all the poles of R(\u03bb) satisfy that detA(\u03bb0) = 0. So if detA(\u03bb0) 6= 0 then \u03bb0 is not a pole of R(\u03bb), and the root vectors in Theorem 4.4 can be considered to be constant.\nCorollary 4.5 (Condition number formula for a zero \u03bb0 with detA(\u03bb0) 6= 0). Let R(\u03bb) be a regular rational matrix as in (2.4), and assume that \u03bb0 \u2208 C is a simple eigenvalue of R(\u03bb) with detA(\u03bb0) 6= 0. Then the structured absolute condition number \u03baS(\u03bb0) in Definition 4.1 is given by\n\u03baS(\u03bb0) = 1 |yTR\u2032(\u03bb0)x| [ \u2016yTC(\u03bb0)A(\u03bb0)\u22121\u20162 \u2016y\u20162 ] S(\u03bb0)\n[ \u2016A(\u03bb0)\u22121B(\u03bb0)x\u20162\n\u2016x\u20162\n] ,\nwhere x and y are, respectively, right and left eigenvectors of R(\u03bb) associated with \u03bb0 and\nS(\u03bb) :=\n[\u2211kA i=0 ai|\u03bb|i \u2211kB i=0 bi|\u03bb|i\u2211kC\ni=0 ci|\u03bb|i \u2211kD i=0 di|\u03bb|i\n] .\n16\n5. Comparison with Tisseur\u2019s unstructured condition number. As we explain in Remark 4.2, Definition 4.1 is a structured absolute condition number for the eigenvalue computing function f : P (\u03bb) 7\u2192 \u03bb0, where P (\u03bb) is a polynomial system matrix as in (2.5) for a rational transfer function matrix R(\u03bb). That is, due to the fact that we perturb each block A(\u03bb), B(\u03bb), C(\u03bb) and D(\u03bb) of P (\u03bb) and the allowed perturbations respect, separately, the degrees of each block: the approach of studying this set of structured perturbation is natural in the context of the representation of R(\u03bb) that we assume to be given. In contrast, the condition number for matrix polynomials defined by F. Tisseur in [22] considers perturbations that do not respect the degrees of particular blocks of the corresponding polynomial matrix P (\u03bb) but the overall degree of P (\u03bb). In other words, the more general theory developed in [22] corresponds to studying unstructured condition numbers, as opposed to our structured condition numbers.\nIn this section, we compare our structured condition number with the unstructured condition number for P (\u03bb) obtained from [22]. Let us first recall Tisseur\u2019s result. For that, we expand P (\u03bb) on the monomial basis, i.e., P (\u03bb) = Pd\u03bb\nd + \u00b7 \u00b7 \u00b7+P1\u03bb+P0, where d = max{kA, kB , kC , kD}. Let us now perturb each matrix coefficient Pi of P (\u03bb) with a perturbation \u2206Pi such that \u2016\u2206Pi\u20162 \u2264 pi, then the condition number formula in [22, Theorem 5] for a simple eigenvalue \u03bb0 of P (\u03bb) is\n(5.1) \u03baU (\u03bb0) =\n\u2016w\u20162\u2016v\u20162 ( d\u2211 i=0 |\u03bb0|ipi ) |wTP \u2032(\u03bb0)v| ,\nwhere w and v are right and left eigenvectors of P (\u03bb) associated with \u03bb0, respectively.\nRemark 5.1. In [22, Theorem 5] the definition of condition number, and hence its formula, is scaled by dividing by |\u03bb0|. Dividing by |\u03bb0| is more suitable if one wants to obtain a relative condition number, as done in [22]. However, we prefer to present our results for absolute condition numbers as this simplifies the exposition. Note that a relative condition number can always be obtained from an absolute one via (1.8). We do not divide by |\u03bb0| in (5.1) so to obtain an absolute condition number, for the purpose of a fair comparison with Definition 4.1.\nWe now note that if each block polynomial matrix of P (\u03bb) is perturbed so that \u2016\u2206Di\u20162 \u2264 di, \u2016\u2206Ai\u20162 \u2264 ai , \u2016\u2206Bi\u20162 \u2264 bi and \u2016\u2206Ci\u20162 \u2264 ci then the sharp upper bound for \u2016\u2206Pi\u2016 is \u2016\u2206Pi\u20162 \u2264 maxv2+w2=1(aiv+ biw)2 +(civ+diw)2. In other words, when we compare the structured and unstructured condition numbers, we should set\npi :=\n( max\nv2+w2=1 (aiv + biw)\n2 + (civ + diw) 2 )1/2 (5.2)\nas this yields the smallest set of unstructured perturbations that includes the set of structured perturbations. Moreover, we can see that\nmax{ai, bi, ci, di}2 \u2264 max v2+w2=1 (aiv + biw) 2 + (civ + diw) 2 \u2264 4 max{ai, bi, ci, di}2,\nwhere the lower and upper bounds are reached for some values of ai, bi, ci, di. A special case of particular interest is when ai = bi = ci = di = 1; in this case, (5.2) yields pi = 2. If we bound \u2016\u2206Pi\u20162 by pi as given in (5.2), we then expect that the unstructured absolute condition number \u03baU (\u03bb0) in (5.1) is larger than the structured\n17\nabsolute condition number \u03baS(\u03bb0) in Definition 4.1 or Theorem 4.4. This indeed follows by definition of supremum, because the set of allowed perturbations for \u03baS is a subset of the set of allowed perturbations for \u03baU .\nTheorem 5.2. Let P (\u03bb) be a polynomial system matrix as in (2.5) with transfer function matrix R(\u03bb). Assume that \u03bb0 \u2208 C is a simple zero of R(\u03bb) and that P (\u03bb) is minimal at \u03bb0. Assume that \u2016\u2206Di\u20162 \u2264 di, \u2016\u2206Ai\u20162 \u2264 ai , \u2016\u2206Bi\u20162 \u2264 bi , \u2016\u2206Ci\u20162 \u2264 ci and let pi be defined as in 5.2. Then \u03baS(\u03bb0) \u2264 \u03baU (\u03bb0).\nIn the following example, we see that \u03baU can be unboundedly larger than \u03baS . Then, in some applications restricting to structured perturbations can make a very significant difference.\nExample 5.3. Consider the rational matrix\nR(\u03bb;\u03b1, \u03b2, k) :=\n[ \u03bb\u2212 \u03b1 0\n0 1 ] [ \u03bbk \u03b2 \u03b2 \u03bbk ]\u22121 for some parameters \u03b1, \u03b2 and k such that \u03b1 6= 0 and k > 0. We can set A(\u03bb;\u03b2, k) :=[ \u03bbk \u03b2 \u03b2 \u03bbk ] , C(\u03bb;\u03b1) := [ \u03bb\u2212 \u03b1 0 0 1 ] , B := I2 and D := 02 such that\nP (\u03bb;\u03b1, \u03b2, k) := [ \u2212A(\u03bb;\u03b2, k) B C(\u03bb;\u03b1) D ] is a polynomial system matrix of R(\u03bb;\u03b1, \u03b2, k). Note that R(\u03bb;\u03b1, \u03b2, k) has a zero at \u03b1, which is also a zero of P (\u03bb;\u03b1, \u03b2, k). A right eigenvector for P associated with \u03b1 is\nv = [ 1 0 \u03b1k \u03b2 ]T and an associated left eigenvector is w = [ 0 0 1 0 ]T . Then, we compute the condition numbers \u03baU (\u03b1) and \u03baS(\u03b1) given in (5.1) and Theorem 4.4, respectively. We remark that, although the expression for \u03baS in Theorem 4.4 involves the inverse of A and right and left root vectors of R, \u03baS can be computed by using right and left eigenvectors of P taking into account Proposition 2.5 and (3.5). For \u03baS(\u03b1), we choose the weights d0 = 0 and ai = bi = ci = 1 for all i \u2264 kA, kB , kC , respectively, and we obtain \u03baS(\u03b1) = |\u03b1| + 1. For \u03baU (\u03b1), we choose the weights pi according to (5.2) for all i \u2264 k. This yields p0 = (1 + \u221a 5)/2, p1 = \u221a 2 and pi = 1\nfor 1 < i \u2264 k if k > 1. We obtain \u03baU (\u03b1) = \u2211k i=0 pi|\u03b1|i \u221a 1 + |\u03b1|2k + |\u03b2|2. Therefore, when |\u03b1| > 1, \u03baU (\u03b1) \u2192 \u221e as k \u2192 \u221e or \u03b2 \u2192 \u221e, while \u03baS(\u03b1) keeps constant. Moreover, \u03baU (\u03b1)/\u03baS(\u03b1) \u2192 \u221e as \u03b1 \u2192 \u221e. Finally, note that \u03b1 is both a zero and a pole when \u03b2 = \u00b1\u03b1k.\n6. Numerical experiments. In this section, we explore the relationship between the unstructured and structured condition numbers \u03baU and \u03baS , respectively, via numerical experiments. We will show that, also in actual applications, \u03baU may be much larger than \u03baS . Moreover, the choice of the representation of the rational matrix can influence the ratio between \u03baU and \u03baS .\nExperiment 1. We first consider a REP from the real-life application (1.3). One natural choice for the weights for \u03baS is to choose ai = bi = ci = di = 1 for all i \u2264 kA, kB , kC , kD, respectively, and otherwise the weights are set to 0. We choose the weights pi for \u03baU according to (5.2) for all i \u2264 d. More realistically, we might want to perturb only the actual data. In the representation given in (1.4), this corresponds to setting c0 = c1 = b0 = a1 = 0 and d0 = d1 = b1 = a0 = 1. If we use the representation in (1.4), the ratio of the condition numbers can be bounded above by 2 when using\n18\nthe former way of choosing the weights. In our numerical experiments, the ratio is bounded by a relatively low number also in the case when only the data is perturbed.\nIt is also possible to represent (1.3) in different ways. For example, we can move one factor \u03bb from B(\u03bb) to C(\u03bb), or, in other words, consider the representation\n(6.1) R(\u03bb) = \u2212K + \u03bbM + [ \u03bb2I \u00b7 \u00b7 \u00b7\u03bb2I ] (w1 \u2212 \u03bb)I . . . (wk \u2212 \u03bb)I  \u22121 C1... Ck  rather than (1.4). The structured condition number \u03baS is unchanged by this change of representation. However, the corresponding unstructured condition number \u03baU is not. In the representation (6.1), perturbing only the real data correpsonds to setting c0 = c1 = c2 = a1 = 0 and d0 = d1 = b0 = a0 = 1. This yields p0 = (1 + \u221a 5)/2, p1 = 1, and p2 = 0. Choosing ai = bi = ci = di = 1 for all i \u2264 kA, kB , kC , kD yields p0 = 2, p1 = (1 + \u221a 5)/2, and p2 = 1. For this representation, we compute the condition numbers \u03baU and \u03baS given in (5.1) and in Theorem 4.4, respectively, for the largest finite eigenvalue in magnitude. We set K,M,Ci \u2208 C200\u00d7200 and k = 2, where we generate the entries of the matrices as well as the poles \u03c9i randomly from a normal distribution of mean 0 and variance 1. We generate 103 realizations in the described way and compute the ratio for the condition numbers. The results are shown in Figure 6.1. We can see that, in both cases, the ratio can grow large although higher ratios are increasingly less probable.\nExperiment 2. Let us next consider the real-life application (1.5). In particular, we consider two different ways to represent this rational matrix. The first representation is (1.6) while the second representation is (6.2) R(\u03bb) = A\u2212 \u03bbB + k\u03bben [ \u03bb\u2212 k/m ]\u22121 eTn .\nFor perturbations, we set ai = bi = ci = di = 1 for all i \u2264 kA, kB , kC , kD, respectively, and otherwise we set the weights to 0. Again, we choose the weights pi for \u03baU according to (5.2). Let us fix n = 10 and m = 1, and let k vary. We compute the\n19\ncondition numbers \u03baU and \u03baS for the largest finite eigenvalue in magnitude for both representations. The results are in Figures 6.2 and 6.3.\nIn Figure 6.2, both \u03baU and \u03baS are linear in k. However, in Figure 6.3 the slope for \u03baU in the loglog-plot is close to 2, which implies that \u03baU is quadratic in k; whereas the slope for \u03baS is close to 1, which implies that \u03baS is linear in k. As a result, the ratio \u03baU/\u03baS diverges as k approaches infinity. In addition, we have that the structured condition number \u03baS is the same for the two representations (1.6) and (6.2), whereas the corresponding unstructured condition number \u03baU does depend on this choice.\n7. Conclusions and open problems. We defined a structured condition number \u03baS for eigenvalues \u03bb0 of a (locally) minimal polynomial system matrix, which are the zeros of the corresponding transfer function R(\u03bb). We then derived a computable expression for \u03baS , that is valid even in the case of \u03bb0 being also a pole of R(\u03bb). For that, we used the notion of root vectors. We finally compared our structured condition number \u03baS with the standard condition number for polynomial matrices defined by F. Tisseur in [22], showing that \u03baS is never larger and that may actually be much smaller in some applications, taking into account different representations of R(\u03bb).\n20\nAn open problem is to study which representations of rational matrices have favourable properties with respect to conditioning. In addition, nowadays the standard approach to solve the REP is via linearizations. That is, transforming the REP into a generalized eigenvalue problem in such a way that the spectral information is preserved. It is also an open to study which linearizations have favourable properties with respect to conditioning. Another related problem is to derive a computable expression for local backward errors for pairs of approximate eigenvalues and eigenvectors of rational matrices.\nAcknowledgements. VN and MQ thank Froila\u0301n Dopico for enlighetning comments on the topic of this paper and for chupitos, both offered during a research visit at Universidad Carlos III de Madrid."
        }
    ],
    "title": "PERTURBATION THEORY OF TRANSFER FUNCTION MATRICES",
    "year": 2022
}