{
    "abstractText": "Representing visual signals by implicit neural representation (INR) has prevailed among many vision tasks. Its potential for editing/processing given signals remains less explored. This work explores a new intriguing direction: training a stylized implicit representation, using a generalized approach that can apply to various 2D and 3D scenarios. We conduct a pilot study on a variety of INRs, including 2D coordinate-based representation, signed distance function, and neural radiance field. Our solution is a Unified Implicit Neural Stylization framework, dubbed INS. In contrary to vanilla INR, INS decouples the ordinary implicit function into a style implicit module and a content implicit module, in order to separately encode the representations from the style image and input scenes. An amalgamation module is then applied to aggregate these information and synthesize the stylized output. To regularize the geometry in 3D scenes, we propose a novel self-distillation geometry consistency loss which preserves the geometry fidelity of the stylized scenes. Comprehensive experiments are conducted on multiple task settings, including fitting images using MLPs, stylization for implicit surfaces and sylized novel view synthesis using neural radiance. We further demonstrate that the learned representation is continuous not only spatially but also style-wise, leading to effortlessly interpolating between different styles and generating images with new mixed styles. Please refer to the video on our project page for more view synthesis results: https://zhiwenfan.github.io/INS.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zhiwen Fan"
        },
        {
            "affiliations": [],
            "name": "Yifan Jiang"
        },
        {
            "affiliations": [],
            "name": "Peihao Wang"
        },
        {
            "affiliations": [],
            "name": "Xinyu Gong"
        },
        {
            "affiliations": [],
            "name": "Dejia Xu"
        },
        {
            "affiliations": [],
            "name": "Zhangyang Wang"
        }
    ],
    "id": "SP:5bff2beda6536542af61208bd0880340f449d702",
    "references": [
        {
            "authors": [
                "H. Aan\u00e6s",
                "R.R. Jensen",
                "G. Vogiatzis",
                "E. Tola",
                "A.B. Dahl"
            ],
            "title": "Large-scale data for multiple-view stereopsis",
            "venue": "International Journal of Computer Vision 120(2), 153\u2013 168",
            "year": 2016
        },
        {
            "authors": [
                "K.A. Aliev",
                "A. Sevastopolsky",
                "M. Kolos",
                "D. Ulyanov",
                "V. Lempitsky"
            ],
            "title": "Neural point-based graphics",
            "venue": "European Conference on Computer Vision. pp. 696\u2013712. Springer",
            "year": 2020
        },
        {
            "authors": [
                "B. Attal",
                "E. Laidlaw",
                "A. Gokaslan",
                "C. Kim",
                "C. Richardt",
                "J. Tompkin",
                "M. O\u2019Toole"
            ],
            "title": "T\u00f6rf: Time-of-flight radiance fields for dynamic scene view synthesis",
            "venue": "Advances in neural information processing systems 34",
            "year": 2021
        },
        {
            "authors": [
                "M. Atzmon",
                "N. Haim",
                "L. Yariv",
                "O. Israelov",
                "H. Maron",
                "Y. Lipman"
            ],
            "title": "Controlling neural level sets",
            "venue": "Advances in Neural Information Processing Systems 32",
            "year": 2019
        },
        {
            "authors": [
                "M. Atzmon",
                "Y. Lipman"
            ],
            "title": "Sal: Sign agnostic learning of shapes from raw data",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 2565\u20132574",
            "year": 2020
        },
        {
            "authors": [
                "J.T. Barron",
                "B. Mildenhall",
                "M. Tancik",
                "P. Hedman",
                "R. Martin-Brualla",
                "P.P. Srinivasan"
            ],
            "title": "Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 5855\u20135864",
            "year": 2021
        },
        {
            "authors": [
                "J.T. Barron",
                "B. Mildenhall",
                "D. Verbin",
                "P.P. Srinivasan",
                "P. Hedman"
            ],
            "title": "Mip-nerf 360: Unbounded anti-aliased neural radiance fields",
            "venue": "arXiv preprint arXiv:2111.12077",
            "year": 2021
        },
        {
            "authors": [
                "A. Bergman",
                "P. Kellnhofer",
                "G. Wetzstein"
            ],
            "title": "Fast training of neural lumigraph representations using meta learning",
            "venue": "Advances in Neural Information Processing Systems 34",
            "year": 2021
        },
        {
            "authors": [
                "E.R. Chan",
                "C.Z. Lin",
                "M.A. Chan",
                "K. Nagano",
                "B. Pan",
                "S. De Mello",
                "O. Gallo",
                "L. Guibas",
                "J. Tremblay",
                "S Khamis"
            ],
            "title": "Efficient geometry-aware 3d generative adversarial networks",
            "venue": "arXiv preprint arXiv:2112.07945",
            "year": 2021
        },
        {
            "authors": [
                "E.R. Chan",
                "M. Monteiro",
                "P. Kellnhofer",
                "J. Wu",
                "G. Wetzstein"
            ],
            "title": "pi-gan: Periodic implicit generative adversarial networks for 3d-aware image synthesis",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 5799\u20135809",
            "year": 2021
        },
        {
            "authors": [
                "D. Chen",
                "J. Liao",
                "L. Yuan",
                "N. Yu",
                "G. Hua"
            ],
            "title": "Coherent online video style transfer",
            "venue": "Proceedings of the IEEE International Conference on Computer Vision. pp. 1105\u20131114",
            "year": 2017
        },
        {
            "authors": [
                "H. Chen",
                "B. He",
                "H. Wang",
                "Y. Ren",
                "S.N. Lim",
                "A. Shrivastava"
            ],
            "title": "Nerv: Neural representations for videos",
            "venue": "Advances in Neural Information Processing Systems 34",
            "year": 2021
        },
        {
            "authors": [
                "T. Chen",
                "P. Wang",
                "Z. Fan",
                "Z. Wang"
            ],
            "title": "Aug-nerf: Training stronger neural radiance fields with triple-level physically-grounded augmentations",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 15191\u2013 15202",
            "year": 2022
        },
        {
            "authors": [
                "X. Chen",
                "Y. Zhang",
                "Y. Wang",
                "H. Shu",
                "C. Xu",
                "C. Xu"
            ],
            "title": "Optical flow distillation: Towards efficient and stable video style transfer",
            "venue": "European Conference on Computer Vision. pp. 614\u2013630. Springer",
            "year": 2020
        },
        {
            "authors": [
                "X. Chen",
                "Q. Zhang",
                "X. Li",
                "Y. Chen",
                "Y. Feng",
                "X. Wang",
                "J. Wang"
            ],
            "title": "Hallucinated neural radiance fields in the wild",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 12943\u201312952",
            "year": 2022
        },
        {
            "authors": [
                "Y. Chen",
                "S. Liu",
                "X. Wang"
            ],
            "title": "Learning continuous image representation with local implicit image function",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 8628\u20138638",
            "year": 2021
        },
        {
            "authors": [
                "Y. Chen",
                "S. Liu",
                "X. Wang"
            ],
            "title": "Learning continuous image representation with local implicit image function",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 8628\u20138638",
            "year": 2021
        },
        {
            "authors": [
                "Z. Chen",
                "H. Zhang"
            ],
            "title": "Learning implicit fields for generative shape modeling",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 5939\u20135948",
            "year": 2019
        },
        {
            "authors": [
                "P.Z. Chiang",
                "M.S. Tsai",
                "H.Y. Tseng",
                "W.S. Lai",
                "W.C. Chiu"
            ],
            "title": "Stylizing 3d scene via implicit representation and hypernetwork",
            "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. pp. 1475\u20131484",
            "year": 2022
        },
        {
            "authors": [
                "B. Curless",
                "M. Levoy"
            ],
            "title": "A volumetric method for building complex models from range images",
            "venue": "Proceedings of the 23rd annual conference on Computer graphics and interactive techniques. pp. 303\u2013312",
            "year": 1996
        },
        {
            "authors": [
                "T. DeVries",
                "M.A. Bautista",
                "N. Srivastava",
                "G.W. Taylor",
                "J.M. Susskind"
            ],
            "title": "Unconstrained scene generation with locally conditioned radiance fields",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 14304\u201314313",
            "year": 2021
        },
        {
            "authors": [
                "R.A. Drebin",
                "L. Carpenter",
                "P. Hanrahan"
            ],
            "title": "Volume rendering",
            "venue": "ACM Siggraph Computer Graphics 22(4), 65\u201374",
            "year": 1988
        },
        {
            "authors": [
                "E. Dupont",
                "A. Goli\u0144ski",
                "M. Alizadeh",
                "Y.W. Teh",
                "A. Doucet"
            ],
            "title": "Coin: Compression with implicit neural representations",
            "venue": "arXiv preprint arXiv:2103.03123",
            "year": 2021
        },
        {
            "authors": [
                "C. Gao",
                "Y. Shih",
                "W.S. Lai",
                "C.K. Liang",
                "J.B. Huang"
            ],
            "title": "Portrait neural radiance fields from a single image",
            "venue": "arXiv preprint arXiv:2012.05903",
            "year": 2020
        },
        {
            "authors": [
                "L. Gatys",
                "A.S. Ecker",
                "M. Bethge"
            ],
            "title": "Texture synthesis using convolutional neural networks",
            "venue": "Advances in neural information processing systems 28",
            "year": 2015
        },
        {
            "authors": [
                "L.A. Gatys",
                "A.S. Ecker",
                "M. Bethge"
            ],
            "title": "A neural algorithm of artistic style",
            "venue": "arXiv preprint arXiv:1508.06576",
            "year": 2015
        },
        {
            "authors": [
                "K. Genova",
                "F. Cole",
                "D. Vlasic",
                "A. Sarna",
                "W.T. Freeman",
                "T. Funkhouser"
            ],
            "title": "Learning shape templates with structured implicit functions",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 7154\u20137164",
            "year": 2019
        },
        {
            "authors": [
                "X. Gong",
                "H. Huang",
                "L. Ma",
                "F. Shen",
                "W. Liu",
                "T. Zhang"
            ],
            "title": "Neural stereoscopic image style transfer",
            "venue": "Proceedings of the European Conference on Computer Vision (ECCV)",
            "year": 2018
        },
        {
            "authors": [
                "A. Gropp",
                "L. Yariv",
                "N. Haim",
                "M. Atzmon",
                "Y. Lipman"
            ],
            "title": "Implicit geometric regularization for learning shapes",
            "venue": "arXiv preprint arXiv:2002.10099",
            "year": 2020
        },
        {
            "authors": [
                "J. Gu",
                "L. Liu",
                "P. Wang",
                "C. Theobalt"
            ],
            "title": "Stylenerf: A style-based 3d-aware generator for high-resolution image synthesis",
            "venue": "arXiv preprint arXiv:2110.08985",
            "year": 2021
        },
        {
            "authors": [
                "X. Gu",
                "Z. Fan",
                "S. Zhu",
                "Z. Dai",
                "F. Tan",
                "P. Tan"
            ],
            "title": "Cascade cost volume for high-resolution multi-view stereo and stereo matching",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 2495\u2013 2504",
            "year": 2020
        },
        {
            "authors": [
                "J. Han",
                "A. Jentzen",
                "E. Weinan"
            ],
            "title": "Solving high-dimensional partial differential equations using deep learning",
            "venue": "Proceedings of the National Academy of Sciences 115(34), 8505\u20138510",
            "year": 2018
        },
        {
            "authors": [
                "Z. Hao",
                "A. Mallya",
                "S. Belongie",
                "M.Y. Liu"
            ],
            "title": "Gancraft: Unsupervised 3d neural rendering of minecraft worlds",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 14072\u201314082",
            "year": 2021
        },
        {
            "authors": [
                "P. Hedman",
                "P.P. Srinivasan",
                "B. Mildenhall",
                "J.T. Barron",
                "P. Debevec"
            ],
            "title": "Baking neural radiance fields for real-time view synthesis",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 5875\u20135884",
            "year": 2021
        },
        {
            "authors": [
                "L. H\u00f6llein",
                "J. Johnson",
                "M. Nie\u00dfner"
            ],
            "title": "Stylemesh: Style transfer for indoor 3d scene reconstructions",
            "venue": "arXiv preprint arXiv:2112.01530",
            "year": 2021
        },
        {
            "authors": [
                "H.P. Huang",
                "H.Y. Tseng",
                "S. Saini",
                "M. Singh",
                "M.H. Yang"
            ],
            "title": "Learning to stylize novel views",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 13869\u201313878",
            "year": 2021
        },
        {
            "authors": [
                "X. Huang",
                "S. Belongie"
            ],
            "title": "Arbitrary style transfer in real-time with adaptive instance normalization",
            "venue": "Proceedings of the IEEE international conference on computer vision. pp. 1501\u20131510",
            "year": 2017
        },
        {
            "authors": [
                "Y.H. Huang",
                "Y. He",
                "Y.J. Yuan",
                "Y.K. Lai",
                "L. Gao"
            ],
            "title": "Stylizednerf: consistent 3d scene stylization as stylized nerf via 2d-3d mutual learning",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 18342\u2013 18352",
            "year": 2022
        },
        {
            "authors": [
                "S. Iizuka",
                "E. Simo-Serra",
                "H. Ishikawa"
            ],
            "title": "Let there be color! joint end-to-end learning of global and local image priors for automatic image colorization with simultaneous classification",
            "venue": "ACM Transactions on Graphics (ToG) 35(4), 1\u201311",
            "year": 2016
        },
        {
            "authors": [
                "C. Jiang",
                "A. Sud",
                "A. Makadia",
                "J. Huang",
                "M. Nie\u00dfner",
                "T Funkhouser"
            ],
            "title": "Local implicit grid representations for 3d scenes",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 6001\u20136010",
            "year": 2020
        },
        {
            "authors": [
                "Y. Jiang",
                "D. Ji",
                "Z. Han",
                "M. Zwicker"
            ],
            "title": "Sdfdiff: Differentiable rendering of signed distance fields for 3d shape optimization",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 1251\u20131261",
            "year": 2020
        },
        {
            "authors": [
                "J. Johnson",
                "A. Alahi",
                "L. Fei-Fei"
            ],
            "title": "Perceptual losses for real-time style transfer and super-resolution",
            "venue": "European conference on computer vision. pp. 694\u2013711. Springer",
            "year": 2016
        },
        {
            "authors": [
                "H. Kato",
                "Y. Ushiku",
                "T. Harada"
            ],
            "title": "Neural 3d mesh renderer",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 3907\u20133916",
            "year": 2018
        },
        {
            "authors": [
                "K.N. Kutulakos",
                "S.M. Seitz"
            ],
            "title": "A theory of shape by space carving",
            "venue": "International journal of computer vision 38(3), 199\u2013218",
            "year": 2000
        },
        {
            "authors": [
                "Z. Li",
                "N. Kovachki",
                "K. Azizzadenesheli",
                "B. Liu",
                "K. Bhattacharya",
                "A. Stuart",
                "A. Anandkumar"
            ],
            "title": "Fourier neural operator for parametric partial differential equations",
            "venue": "arXiv preprint arXiv:2010.08895",
            "year": 2020
        },
        {
            "authors": [
                "J. Lorraine",
                "D. Duvenaud"
            ],
            "title": "Stochastic hyperparameter optimization through hypernetworks",
            "venue": "arXiv preprint arXiv:1802.09419",
            "year": 2018
        },
        {
            "authors": [
                "R. Martin-Brualla",
                "N. Radwan",
                "M.S. Sajjadi",
                "J.T. Barron",
                "A. Dosovitskiy",
                "D. Duckworth"
            ],
            "title": "Nerf in the wild: Neural radiance fields for unconstrained photo collections",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 7210\u20137219",
            "year": 2021
        },
        {
            "authors": [
                "Q. Meng",
                "A. Chen",
                "H. Luo",
                "M. Wu",
                "H. Su",
                "L. Xu",
                "X. He",
                "J. Yu"
            ],
            "title": "Gnerf: Gan-based neural radiance field without posed camera",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 6351\u20136361",
            "year": 2021
        },
        {
            "authors": [
                "L. Mescheder",
                "M. Oechsle",
                "M. Niemeyer",
                "S. Nowozin",
                "A. Geiger"
            ],
            "title": "Occupancy networks: Learning 3d reconstruction in function space",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4460\u2013 4470",
            "year": 2019
        },
        {
            "authors": [
                "M. Michalkiewicz",
                "J.K. Pontes",
                "D. Jack",
                "M. Baktashmotlagh",
                "A. Eriksson"
            ],
            "title": "Implicit surface representations as layers in neural networks",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 4743\u20134752",
            "year": 2019
        },
        {
            "authors": [
                "B. Mildenhall",
                "P. Hedman",
                "R. Martin-Brualla",
                "P. Srinivasan",
                "J.T. Barron"
            ],
            "title": "Nerf in the dark: High dynamic range view synthesis from noisy raw images",
            "venue": "arXiv preprint arXiv:2111.13679",
            "year": 2021
        },
        {
            "authors": [
                "B. Mildenhall",
                "P.P. Srinivasan",
                "R. Ortiz-Cayon",
                "N.K. Kalantari",
                "R. Ramamoorthi",
                "R. Ng",
                "A. Kar"
            ],
            "title": "Local light field fusion: Practical view synthesis with prescriptive sampling guidelines",
            "venue": "ACM Transactions on Graphics (TOG) 38(4), 1\u201314",
            "year": 2019
        },
        {
            "authors": [
                "B. Mildenhall",
                "P.P. Srinivasan",
                "M. Tancik",
                "J.T. Barron",
                "R. Ramamoorthi",
                "R. Ng"
            ],
            "title": "Nerf: Representing scenes as neural radiance fields for view synthesis",
            "venue": "European conference on computer vision. pp. 405\u2013421. Springer",
            "year": 2020
        },
        {
            "authors": [
                "F. Mu",
                "J. Wang",
                "Y. Wu",
                "Y. Li"
            ],
            "title": "3d photo stylization: Learning to generate stylized novel views from a single image",
            "venue": "arXiv preprint arXiv:2112.00169",
            "year": 2021
        },
        {
            "authors": [
                "M. Niemeyer",
                "A. Geiger"
            ],
            "title": "Giraffe: Representing scenes as compositional generative neural feature fields",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 11453\u201311464",
            "year": 2021
        },
        {
            "authors": [
                "M. Niemeyer",
                "L. Mescheder",
                "M. Oechsle",
                "A. Geiger"
            ],
            "title": "Occupancy flow: 4d reconstruction by learning particle dynamics",
            "venue": "Proceedings of the IEEE/CVF international conference on computer vision. pp. 5379\u20135389",
            "year": 2019
        },
        {
            "authors": [
                "M. Oechsle",
                "L. Mescheder",
                "M. Niemeyer",
                "T. Strauss",
                "A. Geiger"
            ],
            "title": "Texture fields: Learning texture representations in function space",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 4531\u20134540",
            "year": 2019
        },
        {
            "authors": [
                "J.J. Park",
                "P. Florence",
                "J. Straub",
                "R. Newcombe",
                "S. Lovegrove"
            ],
            "title": "Deepsdf: Learning continuous signed distance functions for shape representation",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 165\u2013 174",
            "year": 2019
        },
        {
            "authors": [
                "J.J. Park",
                "P. Florence",
                "J. Straub",
                "R. Newcombe",
                "S. Lovegrove"
            ],
            "title": "Deepsdf: Learning continuous signed distance functions for shape representation",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 165\u2013 174",
            "year": 2019
        },
        {
            "authors": [
                "K. Park",
                "U. Sinha",
                "J.T. Barron",
                "S. Bouaziz",
                "D.B. Goldman",
                "S.M. Seitz",
                "R. MartinBrualla"
            ],
            "title": "Nerfies: Deformable neural radiance fields",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 5865\u20135874",
            "year": 2021
        },
        {
            "authors": [
                "K. Park",
                "U. Sinha",
                "P. Hedman",
                "J.T. Barron",
                "S. Bouaziz",
                "D.B. Goldman",
                "R. MartinBrualla",
                "S.M. Seitz"
            ],
            "title": "Hypernerf: A higher-dimensional representation for topologically varying neural radiance fields",
            "venue": "arXiv preprint arXiv:2106.13228",
            "year": 2021
        },
        {
            "authors": [
                "S. Peng",
                "M. Niemeyer",
                "L. Mescheder",
                "M. Pollefeys",
                "A. Geiger"
            ],
            "title": "Convolutional occupancy networks",
            "venue": "European Conference on Computer Vision. pp. 523\u2013540. Springer",
            "year": 2020
        },
        {
            "authors": [
                "M. Ruder",
                "A. Dosovitskiy",
                "T. Brox"
            ],
            "title": "Artistic style transfer for videos",
            "venue": "German conference on pattern recognition. pp. 26\u201336. Springer",
            "year": 2016
        },
        {
            "authors": [
                "S. Saito",
                "Z. Huang",
                "R. Natsume",
                "S. Morishima",
                "A. Kanazawa",
                "H. Li"
            ],
            "title": "Pifu: Pixel-aligned implicit function for high-resolution clothed human digitization",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 2304\u20132314",
            "year": 2019
        },
        {
            "authors": [
                "K. Schwarz",
                "Y. Liao",
                "M. Niemeyer",
                "A. Geiger"
            ],
            "title": "Graf: Generative radiance fields for 3d-aware image synthesis",
            "venue": "Advances in Neural Information Processing Systems 33, 20154\u201320166",
            "year": 2020
        },
        {
            "authors": [
                "S. Shen",
                "Z. Wang",
                "P. Liu",
                "Z. Pan",
                "R. Li",
                "T. Gao",
                "S. Li",
                "J. Yu"
            ],
            "title": "Non-line-ofsight imaging via neural transient fields",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "year": 2021
        },
        {
            "authors": [
                "R. Shrestha",
                "Z. Fan",
                "Q. Su",
                "Z. Dai",
                "S. Zhu",
                "P. Tan"
            ],
            "title": "Meshmvs: Multi-view stereo guided mesh reconstruction",
            "venue": "2021 International Conference on 3D Vision (3DV). pp. 1290\u20131300. IEEE",
            "year": 2021
        },
        {
            "authors": [
                "K. Simonyan",
                "A. Zisserman"
            ],
            "title": "Very deep convolutional networks for large-scale image recognition",
            "venue": "arXiv preprint arXiv:1409.1556",
            "year": 2014
        },
        {
            "authors": [
                "V. Sitzmann",
                "E. Chan",
                "R. Tucker",
                "N. Snavely",
                "G. Wetzstein"
            ],
            "title": "Metasdf: Metalearning signed distance functions",
            "venue": "Advances in Neural Information Processing Systems 33, 10136\u201310147",
            "year": 2020
        },
        {
            "authors": [
                "V. Sitzmann",
                "J. Martel",
                "A. Bergman",
                "D. Lindell",
                "G. Wetzstein"
            ],
            "title": "Implicit neural representations with periodic activation functions",
            "venue": "Advances in Neural Information Processing Systems 33, 7462\u20137473",
            "year": 2020
        },
        {
            "authors": [
                "V. Sitzmann",
                "J. Martel",
                "A. Bergman",
                "D. Lindell",
                "G. Wetzstein"
            ],
            "title": "Implicit neural representations with periodic activation functions",
            "venue": "Advances in Neural Information Processing Systems 33, 7462\u20137473",
            "year": 2020
        },
        {
            "authors": [
                "V. Sitzmann",
                "S. Rezchikov",
                "W.T. Freeman",
                "J.B. Tenenbaum",
                "F. Durand"
            ],
            "title": "Light field networks: Neural scene representations with single-evaluation rendering",
            "venue": "arXiv preprint arXiv:2106.02634",
            "year": 2021
        },
        {
            "authors": [
                "V. Sitzmann",
                "M. Zollh\u00f6fer",
                "G. Wetzstein"
            ],
            "title": "Scene representation networks: Continuous 3d-structure-aware neural scene representations",
            "venue": "Advances in Neural Information Processing Systems 32",
            "year": 2019
        },
        {
            "authors": [
                "Y. Sun",
                "J. Liu",
                "M. Xie",
                "B. Wohlberg",
                "U.S. Kamilov"
            ],
            "title": "Coil: Coordinate-based internal learning for imaging inverse problems",
            "venue": "arXiv preprint arXiv:2102.05181",
            "year": 2021
        },
        {
            "authors": [
                "M. Tancik",
                "P. Srinivasan",
                "B. Mildenhall",
                "S. Fridovich-Keil",
                "N. Raghavan",
                "U. Singhal",
                "R. Ramamoorthi",
                "J. Barron",
                "R. Ng"
            ],
            "title": "Fourier features let networks learn high frequency functions in low dimensional domains",
            "venue": "Advances in Neural Information Processing Systems 33, 7537\u20137547",
            "year": 2020
        },
        {
            "authors": [
                "D. Ulyanov",
                "V. Lebedev",
                "A. Vedaldi",
                "V.S. Lempitsky"
            ],
            "title": "Texture networks: Feedforward synthesis of textures and stylized images",
            "venue": "ICML. vol. 1, p. 4",
            "year": 2016
        },
        {
            "authors": [
                "P. Wang",
                "Z. Fan",
                "T. Chen",
                "Z. Wang"
            ],
            "title": "Neural implicit dictionary learning via mixture-of-expert training",
            "venue": "International Conference on Machine Learning. pp. 22613\u201322624. PMLR",
            "year": 2022
        },
        {
            "authors": [
                "D. Xu",
                "Y. Jiang",
                "P. Wang",
                "Z. Fan",
                "H. Shi",
                "Z. Wang"
            ],
            "title": "Sinnerf: Training neural radiance fields on complex scenes from a single image",
            "venue": "arXiv preprint arXiv:2204.00928",
            "year": 2022
        },
        {
            "authors": [
                "Y. Xu",
                "X. Qiu",
                "L. Zhou",
                "X. Huang"
            ],
            "title": "Improving bert fine-tuning via self-ensemble and self-distillation",
            "venue": "arXiv preprint arXiv:2002.10345",
            "year": 2020
        },
        {
            "authors": [
                "K. Yanai",
                "R. Tanno"
            ],
            "title": "Conditional fast style transfer network",
            "venue": "Proceedings of the 2017 ACM on International Conference on Multimedia Retrieval. pp. 434\u2013437",
            "year": 2017
        },
        {
            "authors": [
                "Y. Yao",
                "Z. Luo",
                "S. Li",
                "T. Fang",
                "L. Quan"
            ],
            "title": "Mvsnet: Depth inference for unstructured multi-view stereo",
            "venue": "Proceedings of the European conference on computer vision (ECCV). pp. 767\u2013783",
            "year": 2018
        },
        {
            "authors": [
                "L. Yariv",
                "Y. Kasten",
                "D. Moran",
                "M. Galun",
                "M. Atzmon",
                "B. Ronen",
                "Y. Lipman"
            ],
            "title": "Multiview neural surface reconstruction by disentangling geometry and appearance",
            "venue": "Advances in Neural Information Processing Systems 33, 2492\u20132502",
            "year": 2020
        },
        {
            "authors": [
                "L. Yariv",
                "Y. Kasten",
                "D. Moran",
                "M. Galun",
                "M. Atzmon",
                "B. Ronen",
                "Y. Lipman"
            ],
            "title": "Multiview neural surface reconstruction by disentangling geometry and appearance",
            "venue": "Advances in Neural Information Processing Systems 33, 2492\u20132502",
            "year": 2020
        },
        {
            "authors": [
                "A. Yu",
                "V. Ye",
                "M. Tancik",
                "A. Kanazawa"
            ],
            "title": "pixelnerf: Neural radiance fields from one or few images",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4578\u20134587",
            "year": 2021
        },
        {
            "authors": [
                "K. Zeng",
                "M. Zhao",
                "C. Xiong",
                "S.C. Zhu"
            ],
            "title": "From image parsing to painterly rendering",
            "venue": "ACM Trans. Graph. 29(1), 2\u20131",
            "year": 2009
        },
        {
            "authors": [
                "K. Zhang",
                "G. Riegler",
                "N. Snavely",
                "V. Koltun"
            ],
            "title": "Nerf++: Analyzing and improving neural radiance fields",
            "venue": "arXiv preprint arXiv:2010.07492",
            "year": 2020
        },
        {
            "authors": [
                "Y. Zhang",
                "T. van Rozendaal",
                "J. Brehmer",
                "M. Nagel",
                "T. Cohen"
            ],
            "title": "Implicit neural video compression",
            "venue": "arXiv preprint arXiv:2112.11312",
            "year": 2021
        },
        {
            "authors": [
                "M. Zhao",
                "S.C. Zhu"
            ],
            "title": "Customizing painterly rendering styles using stroke processes",
            "venue": "Proceedings of the ACM SIGGRAPH/Eurographics Symposium on nonphotorealistic animation and rendering. pp. 137\u2013146",
            "year": 2011
        },
        {
            "authors": [
                "E.D. Zhong",
                "T. Bepler",
                "B. Berger",
                "J.H. Davis"
            ],
            "title": "Cryodrgn: reconstruction of heterogeneous cryo-em structures using neural networks",
            "venue": "Nature Methods 18(2), 176\u2013 185",
            "year": 2021
        },
        {
            "authors": [
                "P. Zhou",
                "L. Xie",
                "B. Ni",
                "Q. Tian"
            ],
            "title": "Cips-3d: A 3d-aware generator of gans based on conditionally-independent pixel synthesis",
            "venue": "arXiv preprint arXiv:2110.09788",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Implicit Neural Representation (INR) has gained remarkable popularity in representing concise signal representation in computer vision and computer graphics [70,53,58,49,82]. As an alternative to discrete grid-based signal representation, implicit representation is able to parameterize modern signals as samples of a continuous manifold, using multi-layer perceptions (MLP) to map between coordinates and signal values. Several seminal works [53,70,83] have verified the effectiveness of INR in representing image, video, and audio. Followups further apply INR to more challenging tasks including novel-view synthesis [53,6,7,86], 3D-aware generative model [90,10,9], and inverse problem [16,74]. While implicit\n\u22c6 Equal contribution\nar X\niv :2\n20 4.\n01 94\n3v 3\n[ cs\n.C V\n] 2\n1 A\nug 2\nWe show stylized results on three different types of implicit representation, including 2D coordinate-based mapping function (SIREN [70]), Signed Distance Function(SDF [58]), and Neural Radiance Field (NeRF [53]).\nneural representation reveals multiple advantages compared to conventional discrete signals, a general question of curiosity might be: which and how modern visual signal processing approaches/tasks designed for discrete signals can also be applied to continuous representations? Research pursuing this answer has been conducted on implicit neural representation since its origin. Chen et al. [17] apply a local implicit function to image super-resolution and they observe that INR can surpass bilinear and nearest upsampling. Sun et al. [74] demonstrate the effectiveness of INR in the context of sparse-view X-ray CT. Dupont et al. [23] propose to store the weights of a neural implicit function instead of pixel values, which surprisingly outperforms JPEG compression format. [87] further demonstrates superior video compression using similar ideas.\nWe investigate a novel setting: to yield visually pleasing stylized examples under various 2D and 3D scenarios, using a generalized approach leveraging implicit neural representations. Note that, training a stylized implicit neural representation still faces many hurdles. On one hand, the aforementioned works mostly have the access to dense measurements or at least sparse clean data, which enables training an implicit neural network under the supervision of target signal. In contrast to those tasks/approaches, current image stylization mechanisms are mostly conducted in an unsupervised manner, due to the absence of stylized ground truth data. Consequently, it is still unknown whether coordinate-based MLP can be optimized without accessing corresponding ground truth signals. On the other hand, existing style images are mostly based on 2D scenes, which raises obstacles when being considered as the appearance of 3D implicit representation. Prior art [19] attempted on marrying stylization with one specific type of Implicit Neural Representation, the neural radiance field (NeRF) [53]. Nevertheless, it still captures the statistics of style information by a series of pre-trained convolution-based hypernetwork to generate model weights, rather than a direct implicitly encoding stylization. As indicated by\nrecent literature [46], training a robust hypernetwork requires a large amount of training samples, while novel-view synthesis tasks commonly hold no more than hundreds of views, potentially jeopardizing the synthesized visual quality.\nTo conquer the aforementioned fragility, we propose a Unified ImplicitNeural Stylization framework, coined as INS. Different from the vanilla implicit function which is built upon a single MLP network, the proposed framework divides an ordinary implicit neural representation to multiple individual components. Concretely speaking, we introduce a Style Implicit Module to the ordinary implicit representation, and coin the later one as Content Implicit Module in our framework. During the training process, the stylized information and content scene are encoded as one continuous representation, and then fused by another Amalgamation Module. To further regularize the geometry of given scenes, we utilize an additional self-distilled geometry consistency loss on top of the rendered density, for the stylization of NeRF. Eventually, INS is able to render view-consistent stylized scenes from novel views, with visually impressive texture details: a few examples are shown in Figure. 1.\nOur contributions are outlined below:\n\u2013 We propose INS, a unified implicit neural stylization framework, consists of a style implicit module, a content implicit module, and an amalgamation module, which enables us to synthesize promising stylized scenes under multiple 2D and 3D implicit representations. \u2013 We conduct comprehensive experiments on several popular implicit representation frameworks in this novel stylization setting, including 2D coordinatebased framework (SIREN [70]), Neural Radiance Field (NeRF [53]), and Signed Distance Functions (SDF [58]). The rendering results are found to be more consistent, in both shape and style details, from different views. \u2013 We further demonstrate that INS is able to learn representations that are continuous not only with regard to spatial placements (including views), but also in the style space. This leads to effortlessly interpolating between different styles and generating images rendered by the new mixed styles."
        },
        {
            "heading": "2 Related Works",
            "text": ""
        },
        {
            "heading": "2.1 Implicit Function",
            "text": "Recent research has exhibited the potential of Implicit Neural Representation (INR) to replace traditional discrete signals with continuous functions parameterized by multilayer perceptrons (MLP), in computer vision and graphics [75,71]. The coordinate-based neural representations [18,49,50] have become a popular representation for various tasks such as representing image/video [70,23,87], 3D reconstruction [27,4,57,18,29,49,56,58,62,64], and 3D-aware generative modelling [10,21,30,33,48,55,65,90]. Analogously, as this representation is differentiable, prior works apply coordinate-based MLPs to many inverse problems in computational photography [16,12,72,3,66,77] and scientific computing [45,32,89]."
        },
        {
            "heading": "2.2 Implicit 3D Scene Representation",
            "text": "Traditional 3D reconstruction methods utilizes discrete representations such as point cloud [2], meshes [43,67], multi-plane images [52], depth maps [81,31] and voxel grids [44]. Recently, INR has also prevailed among 3D scene representation tasks, which simply adopt an MLP that maps from any continuous input 3D coordinate to the geometry of the scene, including signed distance function (SDF) [58,41,5,50,29,73,40], 3D occupancy network [49,18], and so on. In addition to representing shape, INR has also been extended to encode object appearance. Among them, Neural Radiance Field (NeRF [53]) is one of the most effective coordinate-based neural representations for photo-realistic view synthesis that represents a scene as a field of particles. Draw inspiration from the preliminary success made by NeRF, a lot of following works further improve and extend it to wider application [6,7,8,24,47,84,51,61,13,34,6,7,61,60,78]. Different from the grid-based approaches, training a stylized implicit representation can not access ground truth signals, which further amplifies the difficulty of optimizing the implicit neural representation."
        },
        {
            "heading": "2.3 Stylization",
            "text": "Traditionally, image stylization is formulated as a painterly rendering process through stroke prediction [88,85]. The first neural style transfer method, proposed by Gatys et al. [26], builds an iterative framework to optimize the input image in order to minimize the content and style loss defined by a pretrained VGG network. Due to the frustratingly large cost of training time, a number of follow-ups further explore how to design a feed-forward deep neural networks [42,76], which obtain real-time performance without sacrificing too much style information. Recently, several works extend it to video stylization [63,37,11,14] and 3D environment [54,36,19,28,35].\nHa-NeRF [15] is proposed for recovering a realistic NeRF at a different time of day from a group of tourism images, with a CNN to encode the appearance latent code. The most related NeRF-based stylization works: Style3D [19] and StylizedNeRF [38] still require a CNN-based hypernetwork or decoder to generate the stylized parameters for neural radiance field. In comparison, our proposed INS framework can work for more general implicit representations beyond neural radiance field, and can also be extended to encoding multiple styles. Experiments demonstrate that INS generate more faithful stylization on NeRF compared with Style3D [19]."
        },
        {
            "heading": "3 Preliminary",
            "text": "This section introduces the relevant background on several implicit representations and volumetric radiance representations, including image fitting [70], neural radiance field [53] and signed distance function [58].\nImplicit Image Fitting: The most prototypical example of neural implicit representation is image regression [70,75]. Consider fitting a function f : R2 \u2192 R3 that encodes the pixel array of a given image I into a continuous representation. Function f(x) takes pixel coordinates x \u2208 R2 as the inputs, and outputs the corresponding RGB colors c \u2208 R3. Parameterizing f with a multi-layer perception networks (MLPs), it can be optimized by the mean-squared error (MSE) loss function L = Ex\u223cP(I)\u2225f(x)\u2212 c\u222522, where P(I) is a probability measure support in image lattice I.\nNeural Radiance Field: In contrast to point-wisely regression of implicit fields, NeRF [53] proposes to reconstruct a radiance field by inversing a differentiable rendering equation from captured images. Specifically, NeRF learns an MLP f : (x,\u03b8) 7\u2192 (c, \u03c3) with parameters \u0398, where x is the spatial coordinate in 3D space and \u03b8 represents the view directions \u2208 [\u2212\u03c0, \u03c0]2. The output c \u2208 R3 indicates the predicted color of the sampled point, \u03c3 \u2208 R+ signifies its density value. The pixel color intensity can be obtained using volume rendering [22] by ray tracing, integrating the predicted color and density along the ray. To render a pixel on the image plane, NeRF casts a ray r = (o,d,\u03b8) through the pixel and accumulate the color and density of K point samples along the view direction in the 3D space. The pixel color intensity can be estimated:\nC(r|\u0398) = K\u2211\nk=1\nTk(1\u2212 exp(\u2212\u03c3k\u2206tk))ck, (1)\nwhere (ck, \u03c3k) = f(xk,\u03b8), xk = o+tkd, tk are the marching distance of sampled points, and Tk = exp(\u2212 \u2211k\u22121 l=1 \u03c3l\u2206tl) is known as the transmittance to model\nocclusion. \u2206tk = tk+1 \u2212 tk indicates the distance of sampled point in 3D space. With this approximated rendering pipeline, the model weights are optimized by minimizing the L2 distance between rendered ray colors C(r) and captured pixel colors C\u0302(r) as follows:\n\u0398\u2217 = argmin \u0398\nEr\u223cP(R) \u2225\u2225\u2225C(r|\u0398)\u2212 C\u0302(r)\u2225\u2225\u22252\n2 , (2)\nwhere R is a collection of rays cast from all pixels in the training set, and P(R) defines a distribution over it.\nImplicit Surface Representation: Signed Distance Function (SDF) [20] f : R3 \u2192 R is an implicit representation of 3D geometries. SDF specifies each spatial point with the signed distance to the implicit iso-surface, where the sign indicates whether the point is inside or outside the object. Recent works of [59,69] propose to employ MLPs to represent this continuous field via direct supervision using point clouds. To optimize a textured SDF from multi-view images like NeRF [51], Yariv et al.[83] proposes a neural rendering pipeline, named IDR, which enables rendering images from an SDF. With this framework, one can indirectly supervise SDF using its multi-view projections. Suppose given a camera pose, we can cast rays r = (o,v) through each pixel to trace an intersected point with the surface:\nx\u0302 = o+ t0v \u2212 v\n\u2207f0 \u00b7 v0 f(o+ t0v), (3)\nwhere t0, v0 and f0 are initial states when performing ray tracing (see [83]). After obtaining the intersection x\u0302 of ray and surface, IDR also lets the SDF network f output an appearance embedding \u03b3\u0302, and computes the normal n\u0302 = \u2207f(x\u0302). Then the ray color can be rendered by another rendering MLP conditioned on both point coordinate x\u0302 and normal n\u0302:\nC(r|\u0398) = r(x\u0302, n\u0302,d, \u03b3\u0302). (4)\nSimilar to NeRF [51], f and r are simultaneously optimized by photometric loss between captured image pixels and rendered rays (see Equation 2)."
        },
        {
            "heading": "4 Method",
            "text": "We next illustrate the main pipeline of Unified ImplicitNeural Stylization (INS). INS consists of a Style Implicit Module (SIM) to transform the input style embedding into implicit style representations, a Content Implicit Module (CIM) to map the input coordinates into implicit scene representations, and an Amalgamate Module (AM) which amalgamates the two representation to predict RGB intensity. To preserve the geometry fidelity while generating the stylized texture of rendered views, a self-distilled geometry consistency regularization is applied upon the INS framework."
        },
        {
            "heading": "4.1 Implicit Style and Content Representation",
            "text": "Generating the stylized images Y can be formulated as an energy minimization problem [26]. It consists of a content loss and a style loss, defined under a pretrained VGG network [68]. We build upon the prior work and thus propose our implicit stylization framework for SIREN, SDF and NeRF.\nContent Representation The content loss in 2D images stylization pipeline [26] Lcontent is defined as:\nLcontent(C,Y ) = 1\nCi,jWi,jHi,j \u2211 (i,j)\u2208J \u2225Fi,j(C)\u2212 Fi,j(Y )\u22252F , (5)\nwhereC denotes the content ground truth image and Y denotes synthesized output, Fi,j denotes the feature map extracted from a VGG-16 model pre-trained on ImageNet, i represents its i-th max pooling, and j represents its j-th convolutional layer after i-th max pooling layer. Ci,j , Wi,j and Hi,j are the dimensions of the extracted feature maps. We adapt the content loss to the intermediate layer of INS pipeline to preserve the content of the predicted color image patch, we choose i = 2, j = 2 by default.\nStyle Representation To extract representation of the stylized information, [68] introduces a different feature space to capture texture information [25]. Similar to the content loss, the feature space is built upon the filter response in multiple layers of a pre-trained VGG network. By capturing the correlations of the filter responses expressed by the Gram matrix Gi,j \u2208 RCi,j\u00d7Ci,j between the style image S and the synthesized image Y , multi-scale representations can be obtained to capture the texture information from the style image and endow such texture on the stylized image. Here, we define our style loss Lstyle using the same layers of VGG-16 with [26] on the top of the prediction of implicit neural representations and the given style image:\nLstyle(S,Y ) = \u2211\n(i,j)\u2208J\n\u2225Gi,j(S)\u2212Gi,j(Y )\u22252F , (6)\nwhere [Gi,j ]c,c\u2032(Y ) = 1\nCi,jWi,jHi,j H\u00d7W\u2211 k=1 Fi,j(Y )c,kFi,j(Y )c\u2032,k, (7)\nwhere J are the indices of selected feature maps. In practice, we choose J = {(1, 2), (2, 2), (3, 3), (4, 3)} in our experiments.\nConditional INS Stylization Conditional encoding has been widely applied in convolutional networks [39,80]. Similarly, we propose the conditional implicit representation by input with style conditioned embeddings using and extracting style-dependent features to render stylized color and density, which is shown in Figure 2. In training, we prepare n style images with an n dimensional one-hot style-condition vector. A mini-batch is constructed with the combinations of one\ncontent training patch and all candidate style images. The one-hot vector is fed into SIM to extract the w-dimensional style features, they are then concatenated with the implicit representations output from CIM. The following layers of AM take the two features, aggregate them to render the pixel intensity and scene geometry along the rays. A pre-trained VGG [68] is appended on the top of the INS pipeline to apply implicit style and content constraints during training. During the inference stage, we discard the VGG network, the INS framework becomes a pure MLPs-based network."
        },
        {
            "heading": "4.2 Geometry Consistency for Neural Radiance Field",
            "text": "NeRF [53] learns reasonable 3D geometry inherently due to the particular design of implicit radiance field and the supervision from multiple views. However, the INS framework is expected to integrate style statistics from 2D image into 3D radiance field, where no multi-view style images accessible during training process. To specialize INS for neural radiance fields, we propose to regularize INS with proper geometry constraint to produce faithful shape and appearance. As the ground truth of target geometry is unavailable in most novel view synthesis benchmarks, we seek help from the self-distillation framework [79]. Concretely speaking, we first train the content implicit module (CIM) only to obtain a clean geome-\ntry \u03c31, following the vanilla NeRF training pipeline. After that, we copy the trained weight of CIM and keep that fixed (as shown in the grey block in Figure 3). In the next, we turn to optimize the whole INS framework (SIM, CIM and AM) with implicit stylization constrains. Meanwhile, a self-distilled geometry constraint between the original geometry \u03c31 produced by the fixed CIM weight and the final stylized geometry \u03c32 reconstructed by the implicit neural stylization framework. The objective of self-distilled geometry consistency loss is formulated as Lgeo = |\u03c31 \u2212 \u03c32|, where we adopt the mean-absolute error for the densities of each sampled point.\nSampling-Stride Ray Sampling Neural Radiance Field casts a number of rays (typically not adjacent) from camera origin, intersecting the pixel, into the volume and accumulating the color based on density along the ray.\nWhile our model input with rays intersected with an image patch of size P \u2208 K \u00d7 K, predicting the stylized patch P \u2032 \u2208 K \u00d7 K with its texture closed\nto the given style images. 2D style transfer methods [11,37] typically crop the patch larger than 256\u00d7256. However, it is too expensive for the neural radiance field as it queries the MLPs more than 256\u00d7 256\u00d7N times of the MLP for each step [53], where N indicates sampled points number along each ray.\nSimilar to [65,48], we adopt a Sampling-Stride Ray Sampling strategy to enlarge the receptive field of the sampled patch to capture a more global context. The illustration of the ray sampling can be found in our supplementary materials, where a sampling stride larger than 1 result in a large receptive field while keeping computational cost fixed."
        },
        {
            "heading": "4.3 Optimization",
            "text": "Let F (T ) denote an INS model parameterized by \u0398, which synthesizes an image (patch) from the view specified by the camera pose T \u2208 R4\u00d74 by marching and rendering the rays for all the pixels on the image (patch). Given multi-view images and the corresponding camera parameters T = {Ci,T i}Ni=1, as well as a set of style images S = {Si}Mi=1, we train the INS using a combination of losses including reconstruction loss Lrecon, geometry consistency loss Lgeo, content loss Lcontent and style loss Lstyle:\nLtotal(\u0398|T ,S,\u0398vgg) = E(C,T )\u223cP(T ) [\u03bb0Lrecon(F (T ),C) + \u03bb1Lgeo(F (T ))] (8) + ES\u223cP(S) E(C,T )\u223cP(T ) [\u03bb2Lcontent(F (T ),C|\u0398vgg) + \u03bb3Lstyle(F (T ),S|\u0398vgg)] ,\nwhere \u03bb0, \u03bb1, \u03bb2, \u03bb3 control the strength of each loss term, \u0398vgg denotes the parameters of the VGG network, and P(\u00b7) defines a distribution over a support."
        },
        {
            "heading": "5 Experiments",
            "text": ""
        },
        {
            "heading": "5.1 Stylization on SIREN MLPs",
            "text": "As one representative example, we apply INS on fitting an image via SIREN [71] MLPs. We reuse the original SIREN framework [70] as CIM and follow its training recipes to fit images of 512\u00d7512 pixels. Besides that, we also incorporate the SIM and AM on SIREN. A pre-trained VGG-16 network is appended on the output to provide style and content supervisions during training. As seen in Figure 4, the proposed framework successfully in representing the images with the given style statistics in an implicit way."
        },
        {
            "heading": "5.2 Novel View Synthesis with NeRF",
            "text": "Experimental Settings We train our INS framework on NeRF-Synthetic [53] dataset and Local Light Field Fusion(LLFF) [52] dataset. NeRF-Synthetic consists of complex scenes with 360-degree views, where each scene has a central object with 100 inward-facing cameras distributed randomly on the upper hemisphere. Both rendered images and ground truth meshes are provided in NeRFSynthetic dataset. LLFF dataset consists of forward-facing scenes, with fewer images. We implement INS on the same architecture and training strategy with the\noriginal NeRF [53]. \u03bb1, \u03bb2 and \u03bb3 are set as zero in the first 150k iterations and then set as 1e6, 1 and 1e8 in the following 50k iterations. The self-distilled density supervision depicted in Figure 3 is generated from the CIM with 150k iterations pre-training. Adam optimizer is adopted with learning rates of 0.0005. Hyperparameters are carefully tuned via grid searches and the best configuration is applied to all experiments. All experiments are trained on one NVIDIA RTX A6000 GPU. We retrain Style3D [19] in NeRF-Synthetic and LLFF datasets using their provided code and setting. We train all methods using the same number of style images for fair comparisons.\nResults In Figure 5, we can see INS generates faithful and view-consistent results for new viewpoints, with rich textures across scenes and styles. We further compare INS with three state-of-the art methods, including 3D neural stylization [19], image-based stylization methods [37,42]. As is shown in Figure 6, We can see that stylizations from image-based methods produce noisy and view-inconsistent stylization as they transfer styles based on a single image. Style3D [19] generates blur results as it still relies on convolution networks (a.k.a. hypernetwork) to generate the MLP weights for the subsequent volume rendering. Our proposed implicit neural stylization method is trained to preserve correct scene geometry as well as capture global context, generating better view-consistent stylizations."
        },
        {
            "heading": "5.3 Stylization on Signed Distance Function",
            "text": "Experimental Settings DeepSDF [59] only learns the 3D geometry from given inputs. Later work IDR [83] extend it to reconstruct both 3D surface and appearance. We follow IDR [83] to implement the implicit neural stylization framework.\nTo encode style statistic onto IDR, we project the learned textured SDF into multi-view images and implement our style loss on the rendered results. In the experiments, we picked 2 scenes from the DTU dataset [1], where each scene consists of 50 to 100 images and object masks captured from different angles. Similar to NeRF, we pre-train the IDR model for chosen scenes by minimizing the loss between the ground truth image and the rendered result. Then both the SDF network and rendering network are jointly optimized the proposed framework from projected views. Note that due to IDR\u2019s architectural design, we are no longer able to impose self-distilled geometry consistency loss. Instead, we employ content/style loss in the masked region, which is similar to [83]. Besides, we observe that the SDF representation is more sensitive to parameter variations. To maintain intact geometries, we adjust the learning rate for the SDF network to 10\u221211 times smaller than the rendering network.\nResults As are shown in Figure 8, the visualizations of two-view SDF representation demonstrate that both the learned appearance and geometry have deformed to fit the given style statistics."
        },
        {
            "heading": "5.4 Conditional Style Interpolation",
            "text": "Training with style-conditioned one-hot embedding, we can interpolate between style images to mix multiple styles with arbitrary weights. Specifically, we train INS on NeRF with two style images along with a two-dimensional one-hot vector as conditional code. After training, we mix the two style statistics by using a weighted two-dimensional vector. As is shown in Figure 9, the synthesized results can smoothly transfer from the first style to the second style when we linearly mix the two style embeddings at inference time."
        },
        {
            "heading": "5.5 Ablation Study",
            "text": "Effect of Updating NeRF\u2019s Geometry Field The geometry modification in the density branch enables a more flexible stylization, by stylizing shape tweaks on the object surface. As shown in Figure. 11, only updating the color branch\n(a). w/ updating density (b). w/o updating density (c). w/ updating density (d).w/o updating density\nFig. 11. Ablation study of optimizing style-conditioned density and color. By updating the density, it can render richer textures than only updating the color branch.\neasily collapses to the low-level color transformation instead of the painterly texture transformation, violating our original goal. This phenomenon is also addressed in the previous 3D mesh stylization [43], where they explicitly model the 3D shape by deforming the mesh vertexes.\nEffect of Self-distilled Geometry Consistency To evaluate the effectiveness of the proposed geometry consistency regularizer, we visualize the front and back viewpoints of the synthesized color images and depth maps. As is shown in Figure 10, the proposed self-distilled geometry consistency learns a good tradeoff between stylization and clean geometry.\nShould INS Learned with Larger Receptive Field? To investigate the effect by using the Sampling Stride (SS) Ray Sampling strategy, we conduct a comparison of ray sampling with and without sampling stride for NeRF stylization. For a fair comparison, we set the ray number as 64\u00d764 in both settings. INS with sampling stride covers the content resolution of (64\u00d7s) \u00d7 (64\u00d7s) where s indicates sampling strides depicted in Section 4.2 and here we set s=4. Figure 7 shows that INS with SS achieves significantly better visual results, as it results in a higher receptive field in perceiving content statistics."
        },
        {
            "heading": "6 Conclusions",
            "text": "In this work, we present a Unified Implicit Neural Stylization framework (INS) to stylize complex 2D/3D scenes using implicit function. We conduct a pilot study on different types of implicit representations, including 2D coordinatebased mapping function, Neural Radiance Field, and Signed Distance Function. Comprehensive experiments demonstrate that the proposed method yields photo-realistic images/videos with visually consistent stylized textures. One limitation of our work lies in the training efficiency issue, similar to most implicit representation, rendering a style scene requires several hours of training, precluding on-device training. Addressing this issue could become a future direction."
        }
    ],
    "title": "Unified Implicit Neural Stylization",
    "year": 2022
}