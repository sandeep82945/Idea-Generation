{
    "abstractText": "Residential demand response programs aim to activate demand flexibility at the household level. In recent years, reinforcement learning (RL) has gained significant attention for these type of applications. A major challenge of RL algorithms is data efficiency. New RL algorithms, such as proximal policy optimisation (PPO), have tried to increase data efficiency. Additionally, combining RL with transfer learning has been proposed in an effort to mitigate this challenge. In this work, we further improve upon state-of-the-art transfer learning performance by incorporating demand response domain knowledge into the learning pipeline. We evaluate our approach on a demand response use case where peak shaving and selfconsumption is incentivised by means of a capacity tariff. We show our adapted version of PPO, combined with transfer learning, reduces cost by 14.51% compared to a regular hysteresis controller and by 6.68% compared to traditional PPO.",
    "authors": [
        {
            "affiliations": [],
            "name": "Thijs Peirelincka"
        },
        {
            "affiliations": [],
            "name": "Chris Hermans"
        },
        {
            "affiliations": [],
            "name": "Fred Spiessens"
        },
        {
            "affiliations": [],
            "name": "Geert Deconinck"
        }
    ],
    "id": "SP:0e2bd1a23efe0dec6da04e173b8a3605a52ede5a",
    "references": [
        {
            "authors": [
                "Donald Azuatalam",
                "Wee Lih Lee",
                "Frits de Nijs",
                "Ariel Liebman"
            ],
            "title": "Reinforcement learning for wholebuilding HVAC control and demand response",
            "venue": "Energy and AI,",
            "year": 2020
        },
        {
            "authors": [
                "Ruben Baetens",
                "Roel De Coninck",
                "Filip Jorissen",
                "Damien Picard",
                "Lieve Helsen",
                "Dirk Saelens"
            ],
            "title": "Openideas-an open framework for integrated district energy simulations",
            "venue": "In Proceedings of Building Simulation",
            "year": 2015
        },
        {
            "authors": [
                "Shahab Bahrami",
                "Yu Christine Chen",
                "Vincent W.S. Wong"
            ],
            "title": "Deep Reinforcement Learning for Demand Response in Distribution Networks",
            "venue": "IEEE Transactions on Smart Grid,",
            "year": 2021
        },
        {
            "authors": [
                "Oscar De Somer",
                "Ana Soares",
                "Tristan Kuijpers",
                "Koen Vossen",
                "Koen Vanthournout",
                "Fred Spiessens"
            ],
            "title": "Using Reinforcement Learning for Demand Response of Domestic Hot Water Buffers: a Real-Life Demonstration",
            "venue": "IEEE PES Innovative Smart Grid Technologies Europe (ISGT Europe),",
            "year": 2017
        },
        {
            "authors": [
                "Skai Edwards",
                "Ian Beausoleil-Morrison",
                "Andr\u00e9 Laperri\u00e8re"
            ],
            "title": "Representative hot water draw profiles at high temporal resolution for simulating the performance of solar thermal systems",
            "venue": "Solar Energy,",
            "year": 2015
        },
        {
            "authors": [
                "Hussain Kazmi",
                "Fahad Mehmood",
                "Stefan Lodeweyckx",
                "Johan Driesen"
            ],
            "title": "Gigawatt-hour scale savings on a budget of zero: Deep reinforcement learning based optimal control of hot water",
            "venue": "systems. Energy,",
            "year": 2018
        },
        {
            "authors": [
                "Vijay R Konda",
                "John N Tsitsiklis"
            ],
            "title": "Actor-Critic Algorithms",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 1999
        },
        {
            "authors": [
                "Mingxi Liu",
                "Stef Peeters",
                "Duncan S. Callaway",
                "Bert J. Claessens"
            ],
            "title": "Trajectory Tracking With an Aggregation of Domestic Hot Water Heaters: Combining Model-Based and Model-Free Control in a Commercial Deployment",
            "venue": "IEEE Transactions on Smart Grid,",
            "year": 2019
        },
        {
            "authors": [
                "Brida V. Mbuwir",
                "Fred Spiessens",
                "Geert Deconinck"
            ],
            "title": "Distributed optimization for scheduling energy flows in community microgrids",
            "venue": "Electric Power Systems Research,",
            "year": 2020
        },
        {
            "authors": [
                "Volodymyr Mnih",
                "Koray Kavukcuoglu",
                "David Silver",
                "Alex Graves",
                "Ioannis Antonoglou",
                "Daan Wierstra",
                "Martin Riedmiller"
            ],
            "title": "Playing Atari with Deep Reinforcement Learning",
            "venue": "In NIPS Deep Learning Workshop",
            "year": 2013
        },
        {
            "authors": [
                "Kingsley Nweye",
                "Bo Liu",
                "Peter Stone",
                "Zoltan Nagy"
            ],
            "title": "Real-world challenges for multi-agent reinforcement learning in grid-interactive buildings",
            "venue": "Energy and AI,",
            "year": 2022
        },
        {
            "authors": [
                "Adam Paszke",
                "Sam Gross",
                "Francisco Massa",
                "Adam Lerer",
                "James Bradbury Google",
                "Gregory Chanan",
                "Trevor Killeen",
                "Zeming Lin",
                "Natalia Gimelshein",
                "Luca Antiga",
                "Alban Desmaison",
                "Andreas K\u00f6pf Xamla",
                "Edward Yang",
                "Zach Devito",
                "Martin Raison Nabla",
                "Alykhan Tejani",
                "Sasank Chilamkurthy",
                "Qure Ai",
                "Benoit Steiner",
                "Lu Fang Facebook",
                "Junjie Bai Facebook",
                "Soumith Chintala"
            ],
            "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2019
        },
        {
            "authors": [
                "Christophe Patyn",
                "Thijs Peirelinck",
                "Geert Deconinck"
            ],
            "title": "Intelligent Electric Water Heater Control with Varying State Information",
            "venue": "IEEE International Conference on Communications, Control, and Computing Technologies for Smart Grids (SmartGridComm),",
            "year": 2018
        },
        {
            "authors": [
                "Christophe Patyn",
                "Frederik Ruelens",
                "Geert Deconinck"
            ],
            "title": "Comparing neural architectures for demand response through model-free reinforcement learning for heat pump control",
            "venue": "IEEE International Energy Conference,",
            "year": 2018
        },
        {
            "authors": [
                "Thijs Peirelinck",
                "Chris Hermans",
                "Fred Spiessens",
                "Geert Deconinck"
            ],
            "title": "Domain Randomization for Demand Response of an Electric Water Heater",
            "venue": "IEEE Transactions on Smart Grid,",
            "year": 2020
        },
        {
            "authors": [
                "Thijs Peirelinck",
                "Hussain Kazmi",
                "Brida V Mbuwir",
                "Chris Hermans",
                "Fred Spiessens",
                "Johan Suykens",
                "Geert Deconinck"
            ],
            "title": "Transfer learning in demand response: A review of algorithms for data-efficient modelling and control",
            "venue": "Energy and AI,",
            "year": 2022
        },
        {
            "authors": [
                "Thijs Peirelinck",
                "Frederik Ruelens",
                "Geert Deconinck"
            ],
            "title": "Using reinforcement learning for optimizing heat pump control in a building model in Modelica",
            "venue": "IEEE International Energy Conference (ENERGYCON),",
            "year": 2018
        },
        {
            "authors": [
                "Thijs Peirelinck",
                "Fred Spiessens",
                "Chris Hermans",
                "Geert Deconinck"
            ],
            "title": "Double Q-learning for Demand Response of an Electric Water Heater",
            "venue": "IEEE PES Innovative Smart Grid Technologies Europe (ISGT Europe),",
            "year": 2019
        },
        {
            "authors": [
                "Stefan Pfenninger",
                "Iain Staffell"
            ],
            "title": "Long-term patterns of European PV output using 30 years of validated hourly reanalysis and satellite data",
            "venue": "Energy, 114:1251\u20131265,",
            "year": 2016
        },
        {
            "authors": [
                "F Ruelens",
                "B J Claessens",
                "S Vandael",
                "B De Schutter",
                "R Babu\u0161ka",
                "R Belmans"
            ],
            "title": "Residential Demand Response of Thermostatically Controlled Loads Using Batch Reinforcement Learning",
            "venue": "IEEE Transactions on Smart Grid,",
            "year": 2017
        },
        {
            "authors": [
                "John Schulman",
                "Philipp Moritz",
                "Sergey Levine",
                "Michael I. Jordan",
                "Pieter Abbeel"
            ],
            "title": "High-dimensional continuous control using generalized advantage estimation",
            "venue": "In 4th International Conference on Learning Representations,",
            "year": 2016
        },
        {
            "authors": [
                "John Schulman",
                "Filip Wolski",
                "Prafulla Dhariwal",
                "Alec Radford",
                "Oleg Klimov"
            ],
            "title": "Proximal Policy Optimization Algorithms. jul 2017",
            "year": 2017
        },
        {
            "authors": [
                "Ana Soares",
                "Oscar De Somer",
                "Dominic Ectors",
                "Filip Aben",
                "Jan Goyvaerts",
                "Milo Broekmans",
                "Fred Spiessens",
                "Dennis Van Goch",
                "Koen Vanthournout"
            ],
            "title": "Distributed Optimization Algorithm for Residential Flexibility Activation-Results from a Field Test",
            "venue": "IEEE Transactions on Power Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Ana Soares",
                "Davy Geysen",
                "Fred Spiessens",
                "Dominic Ectors",
                "Oscar De Somer",
                "Koen Vanthournout"
            ],
            "title": "Using reinforcement learning for maximizing residential self-consumption \u2013 Results from a field test",
            "venue": "Energy and Buildings,",
            "year": 2020
        },
        {
            "authors": [
                "Richard S Sutton",
                "David Mcallester",
                "Satinder Singh",
                "Yishay Mansour"
            ],
            "title": "Policy Gradient Methods for Reinforcement Learning with Function Approximation",
            "venue": "In Advances in Neural Information Processing Systems (NIPS 1999),",
            "year": 2000
        },
        {
            "authors": [
                "Dennis van Goch",
                "Marc Eulen",
                "Stefan Lodeweyckx",
                "Chris Caerts"
            ],
            "title": "Rennovates, Flexibility Activated Zero Energy Districts, H2020",
            "venue": "Technical report,",
            "year": 2017
        },
        {
            "authors": [
                "Jos\u00e9 R. V\u00e1zquez-Canteli",
                "Zolt\u00e1n Nagy"
            ],
            "title": "Reinforcement learning for demand response: A review of algorithms and modeling techniques",
            "venue": "Applied Energy,",
            "year": 2019
        },
        {
            "authors": [
                "Xiangyu Zhang",
                "Yue Chen",
                "Andrey Bernstein",
                "Rohit Chintala",
                "Peter Graf",
                "Xin Jin",
                "David Biagioni"
            ],
            "title": "Two-Stage Reinforcement Learning Policy Search for Grid-Interactive Building Control",
            "venue": "IEEE Transactions on Smart Grid, 13(3):1976\u20131987,",
            "year": 1987
        }
    ],
    "sections": [
        {
            "text": "Residential demand response programs aim to activate demand flexibility at the household level. In recent years, reinforcement learning (RL) has gained significant attention for these type of applications. A major challenge of RL algorithms is data efficiency. New RL algorithms, such as proximal policy optimisation (PPO), have tried to increase data efficiency. Additionally, combining RL with transfer learning has been proposed in an effort to mitigate this challenge. In this work, we further improve upon state-of-the-art transfer learning performance by incorporating demand response domain knowledge into the learning pipeline. We evaluate our approach on a demand response use case where peak shaving and selfconsumption is incentivised by means of a capacity tariff. We show our adapted version of PPO, combined with transfer learning, reduces cost by 14.51% compared to a regular hysteresis controller and by 6.68% compared to traditional PPO."
        },
        {
            "heading": "1. Introduction",
            "text": "Renewable Energy Sources (RES) are reshaping the energy sector\u2019s landscape. Due to their decentralised and intermittent nature, market and tariff designs are challenged. In the Flemish region of Belgium the energy regulator (VREG) has recently announced a change of distribution fee design [28]. Previously, Flemish residential electricity distribution fees have been energy-based. The rise of residential photovoltaic (PV) installations and netmetering meant a reduction in income for the Distribution Network Operator (DNO). With the introduction of digital metering, the regulator takes the opportunity to introduce a capacity tariff, starting from 2023 [28]. The regulator motivates its decision by arguing that the DNO\u2019s main costs are capacity-based rather than energy-based [28]. As a result, a more cost-reflective price signal is provided to consumers. At the same time, by tying the distribution grid fees to power rather than energy consumption, the VREG encourages consumers, aiming to minizime cost, to reduce their peak power consumption. This causes an interesting application for Demand Response (DR), which we will consider in this work.\n\u2217Corresponding author\nPreprint submitted to Elsevier\nar X\niv :2\n21 1.\n14 83\n1v 1\n[ ee\nss .S\nY ]\n2 7\nN ov\n2 02\nThermostatically Controlled Loads (TCLs) are considered excellent appliances for DR, as they have an inherent energy buffer [17, 14]. In other DR applications with TCLs, Reinforcement Learning (RL) has shown promising results. For instance, Ruelens et al . [20] showed that Fitted Q-Iteration (FQI) can reduce energy consumption cost of a heat pump by 19 %, compared to a default controller, in an energy arbitrage scenario. And Mbuwir et al . [9] apply the same algorithm for local optimisation in their transactive control framework. Their methodology manages to reduce grid congestion using flexibility available in the microgrid\u2019s heat pumps.\nAdditionally, multiple examples exist of successful Electric Water Heater (EWH) control with RL [20, 13, 17, 6, 15, 3, 27]. Reducing peak power consumption can (in part) be accomplished by local PV self-consumption. Self-consumption is a DR application in itself, and earlier work has applied RL for maximising residential self-consumption. Soares et al . [23, 24] use a model-based RL algorithm to control residential batteries and heat pumps. In their field-test, they achieve a 68 % average self-consumption rate. This means, on average, 68 % of heat pump energy is covered by local PV generation. In a similar field-test, using the same model-based RL approach, but only scheduling the EWH heat cycle, De Somer et al . [4] manage to increase PV self-consumption by 20 %.\nThe capacity tariff DR application, which is considered here, differs from Soares et al . [23, 24] and De Somer et al . [4] as the goal is not to maximise self-consumption. Rather, it is to minimise peak power consumption. In the past, we have already touched upon a capacity tariff scenario [18]. However, the capacity tariff treated here is different, and is as proposed by the Flemish regulator in Belgium, i.e., VREG [28]. The DR use case of peak power reduction is of interest in other regions as well [1].\nState-of-the-art RL has improved over time. Since its introduction, policy gradient methods [25] have gradually gained interest. Compared to value iteration methods, the policy gradient approach uses a function approximator that explicitly represents the policy [25]. To further improve the policy gradient, which is used to update the approximator of the policy, an estimate of the expected future reward can be used [7]. This is achieved by the introduction of a critic [7]. These actor-critic methods thus combine the advantages of value iteration and policy iteration [7]. Proximal Policy Iteration (PPO) [22] is the latest introduced family of actor-critic methods and is now widely used in RL research. It has been introduced in an effort to mitigate two of the main drawbacks of RL: (1) data (in)effiency and (2) the (dis)ability to perform well in a variety of domains [22]. Earlier research [20, 13, 8, 27, 11] has shown the benefits of model-free RL in residential DR applications. In short, due to the hetergenuous nature of the appliances and users, the problem quickly becomes intractable. Additionally, the two mentioned properties of PPO are of particular interest for DR. The first property (data effiency) is important because the end user prefers to have a well performing agent as soon as possible. The second property (perform well in a variety of domains) is important because the agent needs to be able to quickly adapt its policy to the current season, as hot water demand and PV output varies throughout the seasons. Consequently, we have opted to use PPO in this work. Furthermore, because earlier work [16, 29] has shown transfer learning can further improve data effiency of RL algorithms, we combine PPO with transfer learning.\nThus, in this work, PPO has been adapted for automated DR when a consumer with an EWH is billed, at least partly, based on quarter hourly peak power consumption. The RL controller\u2019s aim is to minimise final energy cost by turning the EWH on or off. Cost can be minimised by avoiding energy consumption when other (inflexible) loads are already using power or by self-consuming locally generated PV power. To achieve this goal, we use transfer learning to pre-train the agent based on readily available consumption [5, 2] and production [19] data. This paper\u2019s main contribution is the presentation of a data-efficient modelfree residental DR algorithm and learning pipeline. It does so by proposing a method to incorporate expert knowledge and transfer learning in the learning process, and by showing the benefits of a state-space design that explicitly takes into account transfer learning. Additionally, this paper contributes to design of RL algorithms which can be applied in dynamically changing environments by showing that the proposed RL-agent adapts to the seasonality of the control problem.\nWe test our approach on data from real households, obtained from a field-test in the Netherlands [4, 26]. Although the use case considers the Flemish tariff design, the method is more generally applicable. Moreover, reducing peak power demand is a widespread DR setting.\nThis paper has been divided in four sections. Section 2 gives a more detailed formulation of the capacity tariff design, formulates the Markov Decision Process (MDP) and lays out the RL algorithm and its modifications. The paper then goes on to Section 3, presenting the experiments and discussing their results. Finally, Section 4 concludes this work and gives future work directions."
        },
        {
            "heading": "2. Problem Formulation & Algorithm",
            "text": "The first part of this section elaborates on the capacity tariff, as designed by the Flemish electricity and gas regulator (VREG). The second part defines the Sequential Decision making Problem (SDP), and formulates it as an MDP. The final part presents the algorithm used to solve the MDP."
        },
        {
            "heading": "2.1. Tariff Design",
            "text": "A current Flemish residential electricity bill approximately consists of three parts; one part the energy cost [e/kWh], a second part the distribution costs [e/kWh] and a third part taxes and levies (only partly dependent on energy or power consumption). In the remainder of this work, we assume this simplified decoupling of the Flemish electricity bill. Traditionally, residential consumers only have a Ferraris meter installed, which is limited to net metering of energy consumption. The introduction of residential PV installations meant the DNO saw its income reduce, as so-called prosumers have relatively low net energy consumption and, as mentioned, distribution costs are energy-based. To compensate for this loss, a prosumer-tariff was introduced. This tariff is charged based on the power inverter capacity of the PV installation [e/kWinverter] [28].\nWith the introduction of digital metering comes the ability to measure electricity consumption and production separately, and have finer grained measurement points. Together\nwith the observation that distribution grid investment cost is mainly tied to grid capacity (and not energy transported), the regulator opted for a capacity-based distribution fee, from 2023 onwards. This approximately results in the second part of the earlier mentioned electricity bill being capacity-based [e/kW] [28]. As such, a capacity fee provides a more cost-reflective price signal to end-consumers.\nThe peak power to calculate the bill is based on the quarter-hourly measurements of the digital meter. The capacity fee of a residential consumer will be calculated based on the running Mean Month Peak (MMP) of the past 12 months. It only takes into account net off-take, i.e., there is no capacity fee based on grid injection. Thus, assuming Pmt is the quarter-hourly power consumption time-series of the current month m in kW, i.e., the digital meter output, and \u03bbP is the price per kW in euro, the capacity fee F of a residential consumer is calculated by Eq. (2).\nMMP =\n\u2211m i=m\u221212 max(2.5,max(P i t ))\n12 (1)\nF = \u03bbP \u00b7MMP (2)\nEq. (1) implies a minimal capacity fee based on a monthly peak power consumption of 2.5 kW, as in the tariff design [28]. The main aim of this work is to minimise the final energy bill, i.e., including the parts related to energy consumption and taxes. The total energy bill is calculated by Eq. (3).\nCcapacity = \u03bbE \u00b7 E + \u03bbP \u00b7MMP + \u03bbEtax \u00b7 E + \u03bbtax (3)\nwith \u03bbE the energy price, E the total energy consumption of the considered year, \u03bb E tax the taxes charged based on energy consumption and \u03bbtax the fixed taxes payable per year. Eq. (3) has been used to judge agent performance.\nIn RL, the reward function can be used to direct the agent to optimal parts of the solution space, based on expert knowledge. For example, here we know self-consumption of locally generated PV will be beneficial for both reducing energy consumption cost and reducing the MMP. Furthermore, \u03bbtax is independent of MMP and E. As a consequence, neither equation (2) nor equation (3) is used as the MDP\u2019s reward-function. The following part of this section formulates the control problem as MDP by presenting the state-space, action-space and reward-function."
        },
        {
            "heading": "2.2. Markov Decision Problem",
            "text": "The problem is formulated as a discrete-time MDP with time steps of length \u2206t = 15 minutes. The MDP consists of state-space X , action-space U , reward-function r : (X ,U)\u2192 R and state-transition probabilities p(\u00b7|x, u), as modeled by the EWH model. The agent is unaware of these transition probabilities. The agent\u2019s goal is to learn a policy \u03c0 : X \u2192 U which maximises cumulative reward.\nWith the setting as mentioned in the previous section, the main objective of this work is to reduce peak power consumption. Intuitively, reducing peak power consumption goes hand in hand with increasing self-consumption.\nOur reward-function aims to formalise this intuition. Following the MDP framework, this objective is translated to reward-function (4).\nr(xt, ut) = { min(P c \u2212 P nett , 0) + P sct PEWHt 6= 0 0 PEWHt = 0\n(4)\nWhere P c is 2.5 kW, PEWHt is the electrical power consumed by the EWH, P net t is the net power consumption and P sct is the self-consumed EWH power, at quarter t. Given PDt is the household\u2019s other inflexible electrical energy demand and P PV t is the electrical power produced by the PV installation, the net power consumption P nett and the EWH self-consumption P sct are calculated by equations (5) and (6), respectively.\nP nett = P EWH t + P D t \u2212 P PVt (5)\nP sct = min(max(0, P PV t \u2212 PDt ), PEWHt ) (6)\nAn additional challenge considered here is the aim of reducing the need for extensive local PV and demand forecasts. On top of that, the learned control policy will be applied to different residential buildings and households. Therefore, to facilitate policy transfer, the state-space is designed to be independent of environment parameters. For instance, the learned policy should preferably be independent of inverter capacity, as different households will have different PV installations. Preference for policy independence on environment parameters can be illustrated by imagining a simple policy that turns the EWH on whenever PV power production is above 2 kW. When this policy would be transferred to a different household, this absolute number does not apply anymore. Moreover, even within one single household this number would have to change between seasons.\nAt time-step t \u2208 {0, . . . , 95}, state xt \u2208 X is defined by (7), with \u00b5Tt the mean temperature inside the buffer at time-step t, \u2206T tb\u2212Tmin the difference between the sensor measurement and the minimal allowed water temperature, tcos and tsin the projection of the time-step on a circle [15]. Based on other work and to restore the Markov property we have opted to use a history of three time steps for the mean temperature [20, 10].\nxt = {\u00b5Tt , \u00b5Tt\u22121, . . . , \u00b5Tt\u22123,\u2206T bt \u2212Tmin , F E PV , P PVt F PPV , tcos, tsin, t} (7)\nThere are two state features dependent on a forecast of the local PV production: FEPV and F PPV . They are defined by equations (8) and (9), respectively. F E PV is the forecast of the current day\u2019s energy consumption, scaled with a rough estimate of the maximally possible energy production given the inverter power Pinv. And, F P PV is a forecast of the peak power production of that same day. The agent has no (explicit) information on local power demand.\nFEPV = bt/96c+96\u2211 i=bt/96c EPVi 96/2 \u00b7\u2206t \u00b7 Pinv\n(8)\nF PPV = bt/96c+96\nmax i=bt/96c ( EPVi \u2206t ) (9)\nThis work considers a binary action-space. Every quarter t, the agent chooses an action ut \u2208 U = {0, 1}, turning the EWH on or off. When temperature constraints are violated, the controller overrules the agent\u2019s action according to Eq. (10), with T bt the temperature at the backup sensor location (halfway up the buffer\u2019s height), Tmin = 45 \u25e6C and Tmax = 55 \u25e6C.\nuphys = H(T b t , ut) =  1 T bt \u2264 Tmin ut T b t > Tmin and T b t < Tmax\n0 T bt \u2265 Tmax (10)\nA two-layer EWH model, similar to earlier work [4, 23, 24, 15] is used as a virtual testbed. The model is based on the heat balance equations. A more detailed presentation is given in [15]."
        },
        {
            "heading": "2.3. Algorithm",
            "text": "We use PPO [22] to obtain a stochastic policy, maximising expected total reward, given reward-function (4). It is an actor-critic algorithm. The family of actor-critic algorithms has been introduced in effort to combine the advantages of both policy-iteration and valueiteration algorithms. The actor acts, i.e., it decides which action to take, and is trained based on a policy-iteration approach. The critic informs the actor about the state value and how it should update its policy in the right direction. The critic is trained with a value-iteration algorithm. Both actor and critic are parametrised using a (separate) Neural Network (NN), with parameters \u03b8actor and \u03b8critic, respectively.\nAt every iteration k, the actor\u2019s objective Lactork (11) and the critic\u2019s objective L critic k (12)\nare (approximately) minimised. Lactork = E\u0302k [ min ( gk(\u03b8actor)A\u0302k,clip(gk(\u03b8actor),1\u2212 ,1+ )A\u0302k) )] (11)\nLcritick = E\u0302k [( V\u03b8critick (xk)\u2212 V targ k )2] (12)\nBy the use of these update rules, an estimate of the value-function V (x) and policy \u03c0(x) is obtained by the critic and actor, respectively. In Eq. (11), rk(\u03b8) denotes the probability ratio and is defined by (13) [22]. A\u0302k denotes an advantage estimate and is defined by (14) [21], with \u03b3 and \u03bb hyper-parameters and \u03b4Vt given by (15). The clipping in update rule (11) avoids destructively large policy updates [22]. We use a clipping value of = 0.2, and \u03b3 = \u03bb = 0.99.\ngk(\u03b8) = \u03c0\u03b8(uk|xk) \u03c0\u03b8old(uk|xk)\n(13)\nA\u0302k = \u221e\u2211 l=0 (\u03b3\u03bb)(\u03b4Vt+l) (14) \u03b4Vt = rt + \u03b3V (xt + 1)\u2212 V (xt) (15)\nTo improve convergence speed and policy transfer capabilities the actor-NN is tailored to the task at hand, i.e., expert knowledge is incorporated into the actor NN\u2019s design. The domain knowledge is included in such a way that it does not restrict the applicability to one household or one type of TCL. The actor\u2019s NN has been designed based on three main observations:\n\u2022 Time-steps close to each other have a similar policy.\n\u2022 If the current net consumption is close to the forecasted maximum PV output of the day, its quite likely beneficial to heat water.\n\u2022 The backup controller affects the policy and can be incorporated into the actor.\nMore specifically, the actor is divided in 24 subnetworks, i.e., one for every hour of the day. Each of these networks has 3 layers, of which the first is a feature extraction layer. The subnetwork architecture is shown in Fig. 1. The output of subnetwork T \u2208 {0, . . . , 23}, with parameters \u03b8actorT , equals the probability of choosing action ut = 1. The final neuron of each subnetwork implements Eq. (10), assuring temperature stays within comfort bounds. The actor thus uses only part of the observable state and, so does the critic. The full observable state is given in (7). The part used by the actor and critic is defined by (16) and (17), respectively. The time-step is not omitted in xactort , as it is needed to determine if it is better to turn the EWH on at night or not.\nxactort = {T bt , \u00b5Tt , FEPV , P PVt F PPV , t} (16)\nxcritict = {\u00b5Tt , \u00b5Tt\u22121, . . . , \u00b5Tt\u22123,\u2206T bt \u2212Tmin , F E PV , P PVt F PPV , tcos, tsin} (17)\nThe critic\u2019s NN has a conventional fully-connected architecture, with two layers of 28 neurons. Apart from the neuron representing the backup controller, every neuron uses a ReLu activation function. All NNs have been implemented in PyTorch [12].\nTable 1: General metrics of training- and test data, for 1 year.\nDHW cons. [l/day]\n\u2211\nED [kWh]\n\u2211\nEPV [kWh]\nTraining 188.93 3781.55 3766.76 House 1 57.7 2929.57 7894.09 House 2 116.06 5966.85 8269.54 House 3 33.03 3627.04 7467.20 House 4 29.05 3761.71 8296.90 House 5 185.98 5335.82 7920.31"
        },
        {
            "heading": "3. Simulations & Results",
            "text": ""
        },
        {
            "heading": "3.1. Experiment Set-up",
            "text": "In earlier work we have shown transfer learning increases performance for agents applied in a DR setting [15]. Hence, the task is separated in a pre-training and test phase. The pre-training phase only uses readily available data. Three data streams are needed. First, simulated PV production data is taken from the ninja tool developed by Pfenninger and Staffell [19]. Second, electrical load data is generated using Strobe [2]. Third, training phase simulations use Domestic Hot Water (DHW) consumption data from Edwards et al . [5]. These three sets contain data of one year and pre-training lasts 15 simulation-years. After the initial pre-training phase, the obtained parameters \u03b8actor and \u03b8critic are used as initial values for the test phase. During the test-phase, which lasts one simulation-year, we use real residential PV, load and DHW data, from five houses, obtained from a field-test in the Netherlands [4, 26]. Data is available starting from the first of October. Each experiment has been conducted 10 times to account for variability in both phases. Table 1 gives some general metrics of the training- and test data. Clearly, a variety of households has been considered.\nThe presented approach has been compared with three other control approaches: Hysteresis Control (HC), Rule-Based Control (RBC) and a non-expert version of RL (PPO). The hystersis (or, bang-bang) controller assures user comfort and turns the EWH on or off according to Eq. (10). Like RL, RBC adds an additional layer on top of this hystersis controller. The implemented rule-based controller turns the EWH on for four hours, at a fixed time tRBC. While the choice of tRBC may affect control performance, its optimal value is unknown beforehand. Therefore, all RBC simulations have been run four times, with tRBC \u2208 {10, 11, 12, 13} hour. The non-expert version of RL also uses PPO as a training algorithm. However, the actor has a more traditional fully connected NN with two layers of 10 neurons each. This allows to confirm if the tailor made actor increases performance.\nIn the next section we show different result metrics, such as the final energy bill of the considered household, calculated by (3). The Flemish regulator has published capacity tariff values \u03bbP for different DNOs. We have chosen the average value \u03bbP = 47.78e/kW."
        },
        {
            "heading": "3.2. Results",
            "text": "This part presents the results of the simulations. We start with a visualisation of the three main control approaches (HC, RBC, (expert) RL). Thereafter, we compare their performance\ndifferences in a more thorough manner. For the sake of simplicity, only expert RL has been considered at the start. It has been referred to as RL from now on. Only at the final detailed comparison of the main metrics, non-expert RL has been included.\nFig. 2 shows several test-phase days for the three considered control approaches. In each subfigure, the grey area depicts net uncontrollable load, i.e., inflexible load PDt minus PV production P PVt . The EWH\u2019s power demand P EWH t for each control approach is depicted by different line-styles. As explained earlier, simulations have been conducted with several values for tRBC. In all subfigures of Fig. 2, results of the tRBC value which resulted in the best final RBC performance for the considered house has been shown.\nFig. 2a shows the three days immediately succeeding pre-training. At first sight, RBC seems to be a good initial control approach, consuming power when local PV production is high. This is, however, as expected, as the choice for RBC is the result of expert domain knowledge. These three shown days further suggest RL has resulted in similar behaviour as RBC. This confirms the observation that RBC is a rather good initial approach.\nFig. 2b shows three winter days, which illustrate how RL further improves upon RBC. The second and third day, the RL agent turns the EWH on at night, this avoids a heating cycle in the afternoon (during RBC hours), when PV production is low and consumption high. Clearly, HC performs worse than the other approaches, as it does not take into account net consumption whatsoever.\nFinally, Fig. 2c presents three spring days. All have quite a lot of PV production and, therefore, clear negative consumption in the afternoon. However, PV production is intermittent and has certain drops during the day. While the RL agent has no forecast of PV production in the next quarter, it has current net consumption as an input. As a result, and contrary to RBC, it temporarily interrupts heating when PV output suddenly drops. This is illustrated by the third day (day 242) of Fig. 2c.\nFig. 3 gives a more extensive overview of the results. It presents an entire test-phase simulation year for house 3. The bottom graph shows each month\u2019s total PV energy production \u2211 EPV , total inflexible load \u2211 ED and average DHW consumption. These measures are useful for interpretation of control performance. Second, the middle graph shows selfconsumption ratio of each month. For RBC and RL it shows mean and standard deviation of all simulation runs. The self-consumption ratio is the share of total EWH power consumption which has been locally produced by the PV installation. Intuitively, it is clear that this has to be maximised. The graph shows RL performs slightly better than RBC in this regard, for all months. But, especially in months in which PV production is low, RL manages to capture more of the scarce local renewable energy for own consumption. Finally, the top figure shows Pmax, i.e., the month peak, for each month and for all three control approaches, with mean and standard deviation for RBC and RL. Remembering our final goal, Pmax should be minimised. RL outperforms the other control approaches for all months, except November, in this household.\nIn the end, the main goal is reducing the final yearly energy bills of households in Flanders, prone to a capacity tariff. Fig. 4 shows all interesting metrics for the whole testphase year and for all houses. The top figure shows the MMP, calculated by (1). (Expert) RL outperforms RBC, HC and non-expert RL for each household. More precisely, on average\nover all houses, RL reduces the MMP by 16.85 % compared to HC, and by 6.84 % compared to RBC. In absolute numbers, this is a reduction of 0.90 kW and 0.33 kW, respectively.\nAs in Fig. 3, the middle graph of Fig. 4 shows the EWH\u2019s self-consumption ratio. This figure shows that, for each household, RL manages to shift EWH power consumption better to quarters in which local energy production is available. Compared to HC and RBC, expert RL captures 192.34 % and 9.93 % more PV production, respectively. This means that, over the year on average, 495.21 kWh more EWH energy consumption is locally produced, compared to HC, or 57.24 kWh compared to RBC. Of further interest is that expert RL manages to greatly reduce variability of the final performance, compared to non-expert RL.\nThe bottom bar chart shows the final (electrical) energy bill of each household, defined by (3). (Expert) RL is 14.51 % cheaper than HC and 4.59 % cheaper than RBC. For these households and the considered capacity cost, this thus results in an average reduction in cost of e78.07 and e22.13 compared to HC and RBC, respectively. Moreover, by incorporating domain knowledge into the RL algorithm, we have managed to reduce costs with 6.68 % or e32.96. Additionally, the performance\u2019s variability has decreased.\nThe training-phase is an important step in the design and implementation of this RL set-up for DR. RL is known to be data inefficient [22]. A pre-training-phase mitigates this drawbacks as data is less scarce in this phase. Fig. 2a has illustrated that, because of the pretraining-phase, the RL agent performs its task well immediately at the start of the test-phase. Fig. 5 aims to inspect if final pre-training performance affects test-phase performance. This\nfigure shows that, although pre-training\u2019s final year mean reward varies between simulation runs, test-phase mean reward is always rather similar. This result suggests that, while it is important to pre-train the agent, it might not be necessary to somehow find the best pre-trained agent. The average Pearson correlation coefficient between the mean reward of the final year of pre-training and the mean test-phase reward is 0.23."
        },
        {
            "heading": "4. Conclusions & Future Work",
            "text": "We adapted state-of-art RL, based on PPO, to the DR setting and have applied it for EWH control in a capacity tariff use case. The considered setting is highly relevant in Belgium, as the (Flemish) regulator has decided to introduce a capacity tariff for residential consumers as of 2022. In this scenario, the goal was identified to be two-fold; reduce the EWH\u2019s peak power consumption and increase self-consumption of local rooftop PV production. In our test-phase, we have used real-life data from five houses, all with different consumption patterns, but equipped with the same EWH. Beforehand, the agent had been trained with readily available data. The setting is particularly challenging as the agent has no forecast of residential load and limited knowledge on future PV production. Furthermore, we proposed an extension of PPO where domain knowledge is incorporated into the actor\u2019s design. The results indicate that, using this approach, above human-level control performance is achieved.\nIn our experiments, we compared the RL controller with RBC and HC. The experiments showed, for all considered houses, expert RL outperforms both these two other control approaches. Self-consumption ratio was increased by 192.34 % compared to HC, and by 9.93 % compared to RBC. This resulted in a final energy bill reduction of 14.51 % and 4.59 % compared to HC and RBC, respectively. In absolute numbers, this translates to an average reduction of e78.07 and e22.13, respectively, in the yearly energy bill of the considered households. One should note that, while RBC performs relatively well, it is the result of expert knowledge and the best choice of tRBC differs for each house. Tuning this parameter needs expert knowledge, which comes at a cost. In contrary, once deployed, expert RL automatically adapts to its environment. This has been shown in our experiments by the low correlation coefficient between pre-training and final mean reward.\nOur experiments indicate that data which has been generated/collected for other puposes, such as district level simulations or feasability studies, can be used to improve RL performance. This has the potential to facilitate RL-based control uptake. Furthermore, while model-free learning has proven to be promising in DR settings, it has also shown some drawbacks. Our experiments show that by including expert knowledge into the learning pipeline it is possible to mitigate some of these drawbacks, while still being generally applicable. This is in line with the current observations in physics-informed machine learning research.\nWe believe the introduction of a capacity tariff is an opportunity for Flemish residential consumers to adopt smart control approaches for EWHs (and other TCLs). Aggregators can potentially provide the necessary technology. Therefore, our future work is directed towards incorporating local control for reducing the MMP within an aggregator framework."
        },
        {
            "heading": "5. Acknowledgments",
            "text": "Thijs Peirelinck, Chris Hermans and Fred Spiessens acknowledge support from the Flemish Institute for Technological Research (VITO) in the preparation of this manuscript."
        }
    ],
    "title": "Combined Peak Reduction and Self-Consumption  Using Proximal Policy Optimization",
    "year": 2022
}