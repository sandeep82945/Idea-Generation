{
    "abstractText": "This paper presents our proposed approach that won the first prize at the ICLR competition \u201dHardware Aware Efficient Training\u201d. The challenge is to achieve the highest possible accuracy in an image classification task in less than 10 minutes. The training is done on a small dataset of 5000 images picked randomly from CIFAR-10 dataset. The evaluation is performed by the competition organizers on a secret dataset with 1000 images of the same size. Our approach includes applying a series of technique for improving the generalization of ResNet-9 including: sharpness aware optimization, label smoothing, gradient centralization, input patch whitening as well as meta-learning based training. Our experiments show that the ResNet-9 can achieve the accuracy of 88% while trained only on a 10% subset of CIFAR-10 dataset in less than 10 minuets.",
    "authors": [
        {
            "affiliations": [],
            "name": "Omar Mohamed Awad"
        },
        {
            "affiliations": [],
            "name": "Habib Hajimolahoseini"
        },
        {
            "affiliations": [],
            "name": "Michael Lim"
        },
        {
            "affiliations": [],
            "name": "Gurpreet Gosal"
        },
        {
            "affiliations": [],
            "name": "Walid Ahmed"
        },
        {
            "affiliations": [],
            "name": "Yang Liu"
        },
        {
            "affiliations": [],
            "name": "Gordon Deng"
        }
    ],
    "id": "SP:fdc9e8b0c945f3e67b514ad6d9a9b11e2a873341",
    "references": [
        {
            "authors": [
                "Walid Ahmed",
                "Habib Hajimolahoseini",
                "Austin Wen",
                "Yang Liu"
            ],
            "title": "Speeding up resnet architecture with layers targeted low rank decomposition",
            "venue": "In Edge Intelligence Workshop,",
            "year": 2022
        },
        {
            "authors": [
                "Jonathan T Barron"
            ],
            "title": "Continuously differentiable exponential linear units",
            "venue": "arXiv preprint arXiv:1704.07483,",
            "year": 2017
        },
        {
            "authors": [
                "Cody Coleman",
                "Deepak Narayanan",
                "Daniel Kang",
                "Tian Zhao",
                "Jian Zhang",
                "Luigi Nardi",
                "Peter Bailis",
                "Kunle Olukotun",
                "Chris R\u00e9",
                "Matei Zaharia"
            ],
            "title": "Dawnbench: An end-to-end deep learning benchmark and competition",
            "year": 2017
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "IEEE conference on computer vision and pattern recognition,",
            "year": 2009
        },
        {
            "authors": [
                "Pierre Foret",
                "Ariel Kleiner",
                "Hossein Mobahi",
                "Behnam Neyshabur"
            ],
            "title": "Sharpness-aware minimization for efficiently improving generalization",
            "venue": "arXiv preprint arXiv:2010.01412,",
            "year": 2020
        },
        {
            "authors": [
                "Habib Hajimolahoseini",
                "Javad Hashemi",
                "Damian Redfearn"
            ],
            "title": "Ecg delineation for qt interval analysis using an unsupervised learning method",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2018
        },
        {
            "authors": [
                "Habib Hajimolahoseini",
                "Walid Ahmed",
                "Mehdi Rezagholizadeh",
                "Vahid Partovinia",
                "Yang Liu"
            ],
            "title": "Strategies for applying low rank decomposition to transformer-based models",
            "venue": "In 36th Conference on Neural Information Processing Systems",
            "year": 2022
        },
        {
            "authors": [
                "Habib Hajimolahoseini",
                "Kaushal Kumar",
                "DENG Gordon"
            ],
            "title": "Methods, systems, and media for computer vision using 2d convolution of 4d video data tensors, April 20 2023",
            "venue": "US Patent App",
            "year": 2023
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Herv\u00e9 J\u00e9gou",
                "Ond\u0159ej Chum"
            ],
            "title": "Negative evidences and co-occurences in image retrieval: The benefit of pca and whitening",
            "venue": "In European conference on computer vision,",
            "year": 2012
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Geoffrey Hinton"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "year": 2009
        },
        {
            "authors": [
                "Tianda Li",
                "Yassir El Mesbahi",
                "Ivan Kobyzev",
                "Ahmad Rashid",
                "Atif Mahmud",
                "Nithin Anchuri",
                "Habib Hajimolahoseini",
                "Yang Liu",
                "Mehdi Rezagholizadeh"
            ],
            "title": "A short study on compressing decoderbased language models",
            "venue": "arXiv preprint arXiv:2110.08460,",
            "year": 2021
        },
        {
            "authors": [
                "Gabriel Pereyra",
                "George Tucker",
                "Jan Chorowski",
                "\u0141ukasz Kaiser",
                "Geoffrey Hinton"
            ],
            "title": "Regularizing neural networks by penalizing confident output distributions",
            "venue": "arXiv preprint arXiv:1701.06548,",
            "year": 2017
        },
        {
            "authors": [
                "Yang Liu. Walid Ahmed",
                "Habib Hajimolahoseini"
            ],
            "title": "Training acceleration of low-rank decomposed networks using sequential freezing and rank quantization",
            "venue": "In EIW 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Hongwei Yong",
                "Jianqiang Huang",
                "Xiansheng Hua",
                "Lei Zhang"
            ],
            "title": "Gradient centralization: A new optimization technique for deep neural networks, 2020",
            "year": 2020
        },
        {
            "authors": [
                "Xiangyu Zhang",
                "Xinyu Zhou",
                "Mengxiao Lin",
                "Jian Sun"
            ],
            "title": "Shufflenet: An extremely efficient convolutional neural network for mobile devices",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Deep learning models have become larger and larger over the past few years which leads to models with huge numbers of training parameters. Training such a huge model requires a large amount of memory and computational power Hajimolahoseini et al. (2022); Hajimolahoseini et al.; Walid Ahmed (2022); Ahmed et al. (2022) as well as big datasets. However, for some applications, especially on the edge devices with online learning capabilities, the memory and training time could be limited and a large dataset may not be available too Li et al. (2021); Hajimolahoseini et al. (2023; 2019). In cases like this, the model is highly prone to the issue of overfitting and may also not be able to generalize well. Hence, having a model which is able to learn fast on a small dataset and generalize well is highly beneficial Hajimolahoseini et al. (2018).\nThere are some techniques proposed in the literature which try to improve the training efficiency using different approaches during training Yong et al. (2020); Foret et al. (2020); Zhang et al. (2018). For a detailed review of related work the reader is referred to Coleman et al. (2017). In this competition, the goal is to reach to the highest possible accuracy in an image classification task in less than 10 minuets. The training is performed on a 10% subset of CIFAR10 dataset including 10 classes, each with 500 training samples and 100 test samples of 32 \u00d7 32 pixels RGB images Krizhevsky et al. (2009). The evaluation is performed on mini-ImageNet dataset (hidden at development time) of size similar to the development dataset Deng et al. (2009). No pre-trained weights are allowed so the models are trained from scratch. More details about the proposed methodology is presented in the following sections."
        },
        {
            "heading": "2 METHODOLOGY",
            "text": ""
        },
        {
            "heading": "2.1 BASELINE MODEL",
            "text": "Inspired by most of the recent approaches in the similar competitions Coleman et al. (2017), we adopt the well-known ResNet-9 architecture He et al. (2016) as our baseline. Due to its relatively\n\u2217Corresponding author \u2020Same contribution as the first author in writing the paper\nar X\niv :2\n30 9.\n03 96\n5v 1\n[ cs\n.L G\n] 7\nS ep\n2 02\n3\nsmall size, it could be beneficial in preventing the over-fitting issue. However, for training in such a short time on a small dataset, some strategies need to be applied in order to increase the accuracy of the model and its generalization ability."
        },
        {
            "heading": "2.2 OPTIMIZER",
            "text": "During the training of the baseline model, we observed that the generalization of first-order optimizers such as SGD was sub-par, since the test accuracy of ResNet-9 trained on the 10% of CIFAR10 dataet could not reach higher than 76.65%. Therefore, a Sharpness Aware Minimization (SAM) technique is employed to improve the generalization of the model Foret et al. (2020).\nFor models with a very large capacity, the value of training loss does not necessarily guarantee the ability of the model to generalize well Foret et al. (2020). In contrast, SAM optimizer minimizes both the value and sharpness of the loss function at the same time. In other words, it looks for parameters in the surrounding area that have a uniformly low loss. This method converts the minimization problem into a min-max optimization on which SGD could be performed more efficiently. Instead of looking for parameter values that show a low training loss, SAM searches for those parameter values whose entire surrounding area has a uniformly low training loss. It means that the neighborhoods that have both low loss and low curvature Foret et al. (2020)."
        },
        {
            "heading": "2.3 GRADIENT CENTRALIZATION",
            "text": "Another technique that can help improving the generalization of small models is Gradient Centralization (GC) Yong et al. (2020). It centralizes the gradients so that they have zero mean. This is done by creating a projected gradient descent method with a constrained loss function. By regularizing the solution space of model parameters, GC helps to reduce the possibility of overfitting on training data and improving generalization of trained models, especially for small datasets."
        },
        {
            "heading": "2.4 IMPROVED PREPROCESSING",
            "text": "We use a set of techniques called Improved Preprocessing (IP). The methods are described in the following sections:"
        },
        {
            "heading": "2.4.1 LABEL SMOOTHING",
            "text": "In order to improve our training strategy, we use label smoothing technique Pereyra et al. (2017). It blends the one-hot target probabilities with a uniform distribution over class labels inside the cross entropy loss as follow:\nyls = (1\u2212 \u03b1)\u00d7 yhot + \u03b1\nK (1)\nwhere yls and yhot represent the label smoothed and one-hot probabilities and K and \u03b1 are the number of classes (K = 10 for our case) and smoothing factor, respectively. In our experiments, we set the smoothing factor \u03b1 = 0.1 and number of classes are K = 10. This helps to stabilize the output distribution and prevents the network from making overconfident predictions which might inhibit further training."
        },
        {
            "heading": "2.4.2 WEIGHT DECAY",
            "text": "Weight decay is another regularization method that we use which keeps the model weights as small as possible:\nLnew(w) = Lorg(w) + \u03bbw Tw (2)\nwhere Lnew(w) and Lorg(w) are the new and original loss functions and \u03bb is the decay factor which we set to \u03bb = 0.0005.. The small values of weights guarantees that the network behaviour will not change much if we change a few random inputs which in turn makes it difficult for the regularized network to learn local noise in the data."
        },
        {
            "heading": "2.4.3 ACTIVATION FUNCTION",
            "text": "We replaced the RELU activation function of ResNet-9 with CELU Barron (2017) which helps improving the generalization since smoothed functions lead to a less expressive function class:\nCELU(x, \u03b1) = {\nx x \u2265 0 \u03b1(exp( x\u03b1 )\u2212 1) otherwise\n} (3)\nAccording to 3, CELU converges to ReLU as \u03b1 \u2192 0. In our experiments we set \u03b1 = 0.3."
        },
        {
            "heading": "2.4.4 INPUT PATCH WHITENING",
            "text": "In order to reduce the covariance between channels and pixels we apply PCA whitening to 3\u00d73 patches of inputs as an initial 3\u00d73 convolution with fixed (non-trainable) weights followed by a trainable 1\u00d71 convolution Je\u0301gou & Chum (2012)."
        },
        {
            "heading": "2.5 META-LEARNING BASED TRAINING",
            "text": "As a mechanism for generalizing the knowledge learned over many few-shot learning tasks, metalearning is a promising training approach for few-shot learning problems. A collection of different (but similar) few-shot learning tasks are learned in parallel to learn representations that are common to all tasks (e.g. Omniglot dataset).\nThis meta-learning approach has been reframed as a single-task algorithm for training on small dataset (10 classes of Mini-ImageNet) \u2013 named Meta-Learning based Training Procedure (MLTP). MLTP was applied in hopes to get good generalization from a small dataset, and ultimately higher classification accuracy. In this approach, the Mini-ImageNet dataset is broken up into 2 tasks (each containing 250 samples of each class), and applied as a 2-task MLTP problem. The base architecture used is ResNet9. This process is depicted in Fig.1."
        },
        {
            "heading": "3 EVALUATION",
            "text": "The hardware used in this competition is an Nvidia V100 GPU which has a memory of 32GB, running with an Intel(R) Xeon(R) Gold 6230 CPU @ 2.10GHz processor, with a RAM of 12GB. We randomly pick 5000 images from the CIFAR-10 dataset (500 images from each of the 10 classes) for training.\nThe experimental results are shown in Table 1. According to this table, although each of the SAM, GC and meta-learning techniques can improve the accuracy of the baseline model individually, however, combining the SAM optimizer and the IP techniques (SAM+IP) can boost the model accuracy by 9.95% and 7.7% on Mini-CIFAR10 and Mini-ImageNet datasets, respectively when comparing to the baseline model."
        },
        {
            "heading": "4 CONCLUSION",
            "text": "In this paper, we adopted some techniques for improving the generalization of ResNet-9 model when training on a small dataset in a very short time. Experimental results reveal that thechniques such as SAM, GC, IP and meta learning can boost the performance of the baseline model especially when they are combined together. It is also worth mentioning that the proposed methods are orthogonal to each other and therefore they can be combined together. Due to time limitation of the competition, we did not experiment with all different combinations of the proposed methods. This is left for future work."
        }
    ],
    "title": "IMPROVING RESNET-9 GENERALIZATION TRAINED ON SMALL DATASETS",
    "year": 2023
}