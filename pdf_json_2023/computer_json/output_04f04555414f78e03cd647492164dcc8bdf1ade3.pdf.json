{
    "abstractText": "Argument mining is to analyze argument structure and extract important argument information from unstructured text. An argument mining system can help people automatically gain causal and logical information behind the text. As argumentative corpus gradually increases, like more people begin to argue and debate on social media, argument mining from them is becoming increasingly critical. However, argument mining is still a big challenge in natural language tasks due to its difficulty, and relative techniques are not mature. For example, research on non-tree argument mining needs to be done more. Most works just focus on extracting tree structure argument information. Moreover, current methods cannot accurately describe and capture argument relations and do not predict their types. In this paper, we propose a novel neural model called AutoAM to solve these problems. We first introduce the argument component attention mechanism in our model. It can capture the relevant information between argument components, so our model can better perform argument mining. Our model is a universal end-to-end framework, which can analyze argument structure without constraints like tree structure and complete three subtasks of argument mining in one model. The experiment results show that our model outperforms the existing works on several metrics in two public datasets.",
    "authors": [
        {
            "affiliations": [],
            "name": "Lang Cao"
        }
    ],
    "id": "SP:fdda17cf08b9893123bf98b234a7b21329d19e26",
    "references": [
        {
            "authors": [
                "S. Afantenos",
                "A. Peldszus",
                "M. Stede"
            ],
            "title": "Comparing decoding mechanisms for parsing argumentative structures",
            "venue": "Argument & Computation 9, 1\u201316",
            "year": 2018
        },
        {
            "authors": [
                "J. Bao",
                "C. Fan",
                "J. Wu",
                "Y. Dang",
                "J. Du",
                "R. Xu"
            ],
            "title": "A neural transition-based model for argumentation mining",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). pp. 6354\u20136364. Association for Computational Linguistics, Online",
            "year": 2021
        },
        {
            "authors": [
                "T. Chakrabarty",
                "C. Hidey",
                "S. Muresan",
                "K. McKeown",
                "A. Hwang"
            ],
            "title": "AMPERSAND: Argument mining for PERSuAsive oNline discussions",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 2933\u20132943. Association for Computational Linguistics, Hong Kong, China",
            "year": 2019
        },
        {
            "authors": [
                "D. Chen",
                "C. Manning"
            ],
            "title": "A fast and accurate dependency parser using neural networks",
            "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 740\u2013750. Association for Computational Linguistics, Doha, Qatar",
            "year": 2014
        },
        {
            "authors": [
                "T. Dozat",
                "C.D. Manning"
            ],
            "title": "Simpler but more accurate semantic dependency parsing",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). pp. 484\u2013490. Association for Computational Linguistics, Melbourne, Australia",
            "year": 2018
        },
        {
            "authors": [
                "Eemeren",
                "F.H.v.",
                "R. Grootendorst"
            ],
            "title": "A Systematic Theory of Argumentation: The pragma-dialectical approach",
            "venue": "Cambridge University Press",
            "year": 2003
        },
        {
            "authors": [
                "S. Eger",
                "J. Daxenberger",
                "I. Gurevych"
            ],
            "title": "Neural end-to-end learning for computational argumentation mining",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). pp. 11\u2013 22. Association for Computational Linguistics, Vancouver, Canada",
            "year": 2017
        },
        {
            "authors": [
                "A. Galassi",
                "M. Lippi",
                "P. Torroni"
            ],
            "title": "Argumentative link prediction using residual networks and multi-objective learning",
            "venue": "Proceedings of the 5th Workshop on Argument Mining. pp. 1\u201310. Association for Computational Linguistics, Brussels, Belgium",
            "year": 2018
        },
        {
            "authors": [
                "C. G\u00f3mez-Rod\u0155\u0131guez",
                "T. Shi",
                "L. Lee"
            ],
            "title": "Global transition-based non-projective dependency parsing",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). pp. 2664\u2013 2675. Association for Computational Linguistics, Melbourne, Australia",
            "year": 2018
        },
        {
            "authors": [
                "T. Kuribayashi",
                "H. Ouchi",
                "N. Inoue",
                "P. Reisert",
                "T. Miyoshi",
                "J. Suzuki",
                "K. Inui"
            ],
            "title": "An empirical study of span representations in argumentation structure parsing",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 4691\u20134698. Association for Computational Linguistics, Florence, Italy",
            "year": 2019
        },
        {
            "authors": [
                "N. Kwon",
                "L. Zhou",
                "E. Hovy",
                "S.W. Shulman"
            ],
            "title": "Identifying and classifying subjective claims",
            "venue": "p. 76\u201381. dg.o \u201907, Digital Government Society of North America",
            "year": 2007
        },
        {
            "authors": [
                "J. Lawrence",
                "C. Reed"
            ],
            "title": "Argument Mining: A Survey",
            "venue": "Computational Linguistics 45(4), 765\u2013818",
            "year": 2020
        },
        {
            "authors": [
                "Y. Liu",
                "M. Ott",
                "N. Goyal",
                "J. Du",
                "M. Joshi",
                "D. Chen",
                "O. Levy",
                "M. Lewis",
                "L. Zettlemoyer",
                "Stoyanov",
                "V.: Roberta"
            ],
            "title": "A robustly optimized BERT pretraining approach",
            "venue": "CoRR abs/1907.11692",
            "year": 1907
        },
        {
            "authors": [
                "I. Loshchilov",
                "F. Hutter"
            ],
            "title": "Fixing weight decay regularization in adam",
            "year": 2017
        },
        {
            "authors": [
                "R. Mochales",
                "M.F. Moens"
            ],
            "title": "Argumentation mining: The detection, classification and structure of arguments in text",
            "venue": "pp. 98\u2013107",
            "year": 2009
        },
        {
            "authors": [
                "M.F. Moens",
                "E. Boiy",
                "R. Mochales",
                "C. Reed"
            ],
            "title": "Automatic detection of arguments in legal texts",
            "venue": "pp. 225\u2013230",
            "year": 2007
        },
        {
            "authors": [
                "G. Morio",
                "H. Ozaki",
                "T. Morishita",
                "Y. Koreeda",
                "K. Yanai"
            ],
            "title": "Towards better nontree argument mining: Proposition-level biaffine parsing with task-specific parameterization",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 3259\u20133266. Association for Computational Linguistics, Online",
            "year": 2020
        },
        {
            "authors": [
                "V. Niculae",
                "J. Park",
                "C. Cardie"
            ],
            "title": "Argument mining with structured SVMs and RNNs",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). pp. 985\u2013995. Association for Computational Linguistics, Vancouver, Canada",
            "year": 2017
        },
        {
            "authors": [
                "J. Park",
                "C. Cardie"
            ],
            "title": "A Corpus of eRulemaking User Comments for Measuring Evaluability of Arguments",
            "venue": "chair), N.C.C., Choukri, K., Cieri, C., Declerck, T., Goggi, S., Hasida, K., Isahara, H., Maegaard, B., Mariani, J., Mazo, H., Moreno, A., Odijk, J., Piperidis, S., Tokunaga, T. (eds.) Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018). European Language Resources Association (ELRA), Miyazaki, Japan",
            "year": 2018
        },
        {
            "authors": [
                "A. Paszke",
                "S. Gross",
                "F. Massa",
                "A. Lerer",
                "J. Bradbury",
                "G. Chanan",
                "T. Killeen",
                "Z. Lin",
                "N. Gimelshein",
                "L. Antiga",
                "A. Desmaison",
                "A. K\u00f6pf",
                "E. Yang",
                "Z. DeVito",
                "M. Raison",
                "A. Tejani",
                "S. Chilamkurthy",
                "B. Steiner",
                "L. Fang",
                "J. Bai",
                "S. Chintala"
            ],
            "title": "Pytorch: An imperative style, high-performance deep learning library",
            "year": 2019
        },
        {
            "authors": [
                "A. Peldszus"
            ],
            "title": "Towards segment-based recognition of argumentation structure in short texts",
            "venue": "Proceedings of the First Workshop on Argumentation Mining. pp. 88\u201397. Association for Computational Linguistics, Baltimore, Maryland",
            "year": 2014
        },
        {
            "authors": [
                "A. Peldszus",
                "M. Stede"
            ],
            "title": "Joint prediction in MST-style discourse parsing for argumentation mining",
            "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. pp. 938\u2013948. Association for Computational Linguistics, Lisbon, Portugal",
            "year": 2015
        },
        {
            "authors": [
                "I. Persing",
                "V. Ng"
            ],
            "title": "End-to-end argumentation mining in student essays",
            "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 1384\u2013 1394. Association for Computational Linguistics, San Diego, California",
            "year": 2016
        },
        {
            "authors": [
                "M.E. Peters",
                "M. Neumann",
                "M. Iyyer",
                "M. Gardner",
                "C. Clark",
                "K. Lee",
                "L. Zettlemoyer"
            ],
            "title": "Deep contextualized word representations",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers). pp. 2227\u20132237. Association for Computational Linguistics, New Orleans, Louisiana",
            "year": 2018
        },
        {
            "authors": [
                "P. Potash",
                "A. Romanov",
                "A. Rumshisky"
            ],
            "title": "Here\u2019s my point: Joint pointer architecture for argument mining",
            "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. pp. 1364\u20131373. Association for Computational Linguistics, Copenhagen, Denmark",
            "year": 2017
        },
        {
            "authors": [
                "N. Srivastava",
                "G. Hinton",
                "A. Krizhevsky",
                "I. Sutskever",
                "R. Salakhutdinov"
            ],
            "title": "Dropout: A simple way to prevent neural networks from overfitting",
            "venue": "Journal of Machine Learning Research",
            "year": 1929
        },
        {
            "authors": [
                "C. Stab",
                "I. Gurevych"
            ],
            "title": "Annotating argument components and relations in persuasive essays",
            "venue": "Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers. pp. 1501\u20131510",
            "year": 2014
        },
        {
            "authors": [
                "C. Stab",
                "I. Gurevych"
            ],
            "title": "Parsing argumentation structures in persuasive essays",
            "venue": "Computational Linguistics 43(3), 619\u2013659",
            "year": 2017
        },
        {
            "authors": [
                "A. Vaswani",
                "N. Shazeer",
                "N. Parmar",
                "J. Uszkoreit",
                "L. Jones",
                "A.N. Gomez",
                "Kaiser",
                "L.u",
                "I. Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2017
        },
        {
            "authors": [
                "V.R. Walker",
                "D. Foerster",
                "J.M. Ponce",
                "M. Rosen"
            ],
            "title": "Evidence types, credibility factors, and patterns or soft rules for weighing conflicting evidence: Argument mining in the context of legal rules governing evidence assessment",
            "venue": "Proceedings of the 5th Workshop on Argument Mining. pp. 68\u201378. Association for Computational Linguistics, Brussels, Belgium",
            "year": 2018
        },
        {
            "authors": [
                "T. Wolf",
                "L. Debut",
                "V. Sanh",
                "J. Chaumond",
                "C. Delangue",
                "A. Moi",
                "P. Cistac",
                "T. Rault",
                "R. Louf",
                "M. Funtowicz",
                "J. Davison",
                "S. Shleifer",
                "P. von Platen",
                "C. Ma",
                "Y. Jernite",
                "J. Plu",
                "C. Xu",
                "T.L. Scao",
                "S. Gugger",
                "M. Drame",
                "Q. Lhoest",
                "A.M. Rush"
            ],
            "title": "Huggingface\u2019s transformers: State-of-the-art natural language processing",
            "venue": "https://arxiv",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "Keywords: Argument Mining \u00b7 Information Extraction \u00b7 Natural Language Processing"
        },
        {
            "heading": "1 Introduction",
            "text": "Argument mining (AM) is a technique for analyzing argument structure and extracting important argument information from unstructured text, which has gained popularity in recent years [12]. An argument mining system can help people automatically gain causal and logical information behind the text. The argument mining techniques benefit plenty of many fields, like legal [31], public opinions [19], finance, etc. Argument mining is beneficial to human society, but there is still much room for development. Argument mining consists of several tasks and has a variety of different paradigms [12]. In this paper, we focus on the most common argument structure of monologue. It is an argumentative text from one side, not an argument from two sides. The microscopic structure of\nar X\niv :2\n30 9.\n09 30\n0v 1\n[ cs\n.C L\n] 1\n7 Se\np 20\nargumentation is the primary emphasis of the monologue argument structure, which primarily draws out the internal relations of reasoning.\nIn this setting, an argumentative paragraph can be viewed as an argument graph. An argument graph can efficiently describe and reflect logical information and reasoning paths behind the text. An example of AM result after extraction is shown in Figure 1. The two important elements in an argument graph are the argument component (AC) and the argument relation (AR). ACs are nodes in this graph, and ARs are edges. The goal of an AM system is to construct this argument graph from unstructured text automatically. The process of the AM system definition we use is as following steps:\n1. Argument Component Identification (ACI): Given an argumentative paragraph, AM systems will detect ACs from it and separate this text. 2. Argument Component Type Classification (ACTC): AM systems will determine the types of these ACs. 3. Argument Relation Identification (ARI): AM systems will identify the existence of a relationship between any ACs. 4. Argument Relation Type Classification (ARTC): AM systems will determine the type of ARs, which are the existing relations between ACs.\nSubtask 1) is a token classification task, which is also a named entity recognition task. This task has a large amount of research work on it. Most of the previous argument mining works [25] [10] [3] assume that the subtask 1) argument component identification has been completed, which is the argument component has been identified and can be obtained from the argumentative text. Therefore, the emphasis of argument mining research is placed on other subtasks. Following previous works, we also make such assumptions in this paper. On this basis, we design an end-to-end model to complete ACTC, ARI, and ARTC subtasks simultaneously.\nARI and ARTC are the hardest parts of the whole argument mining process. An AR is represented by two ACs. It is difficult to represent AR precisely and\ncapture this relation. Most ACs pairs do not have a relationship at all, which leads to a serious sample imbalance problem. Among the whole process, ARI and ARTC are parts of ACI and ACTC, so the performance of these tasks will be influenced. Due to these reasons, many previous works give up and ignore the classification of ARs. Besides, much research imposes some argument constraints to do argument mining. In most normal cases, they assume argument information is a tree structure, and they can use the characteristic of the tree to extract information. Tree structure argument information is common in an argumentative essay. However, argument information with no constraints is more normal in the real world, like a huge amount of corpus on social media. This information is just like the general argument graphs mentioned before and needs to be extracted in good quality.\nIn this paper, we solve the above problems with a novel model called AutoAM (the abbreviation of Automactic and UniversalArgumentMining Model). This is an efficient and accurate argument mining model to complete the entire argument mining process. This model does not rely on domain-specific corpus and does not need to formulate special syntactic constraints, etc., to construct argument graphs from argumentative text. To improve the performance of nontree structured argument mining, we first introduce the argument component attention mechanism (ArguAtten) in this model, which can better capture the relevant information of argument components in an argumentative paragraph. It benefits the overall performance of argument mining. We use a distance matrix to add the key distance feature to represent ARs. A stratified learning rate is also a critical strategy in the model to balance multi-task learning. To the best of our knowledge, we are the first to propose an end-to-end universal AM model without structure constraints to complete argument mining. Meanwhile, we combine our novelty and some successful experience to achieve the state of the art in two public datasets.\nIn summary, our contributions are as follows:\n\u2013 We propose a novel model AutoAM for argument mining which can efficiently solve argument mining in all kinds of the argumentative corpus. \u2013 We introduce ArguAtten (argument component attention mechanism) to better capture the relation between argument components and improve overall argument mining performance. \u2013 We conduct extensive experiments on two public datasets and demonstrate that our method substantially outperforms the existing works. The experiment results show that the model proposed in this paper achieves the best results to date in several metrics. Especially, there is a great improvement over the previous studies in the tasks of ARI (argument relation identification) and ARTC (argument relation type classification)."
        },
        {
            "heading": "2 Related Work",
            "text": "Since argument mining was first proposed [16], much research has been conducted on it. At first, people used rule-based or some traditional machine learning\nmethods. With the help of deep learning, people begin to get good performance on several tasks and start to focus on non-tree structured argument mining. We discuss related work following the development of AM."
        },
        {
            "heading": "2.1 Early Argument Mining",
            "text": "The assumption that the argument structure could be seen as a tree or forest structure was made in the majority of earlier work, which made it simpler to tackle the problem because various tree-based methods with structural restrictions could be used. In the early stage of the development of argument mining, people usually use rule-based structural constraints and traditional machine learning methods to conduct argumentative mining. In 2007, Moens et al. [16] conducted the first argument mining research on legal texts in the legal field, while Kwon et al. [11] also conducted relevant research on commentary texts in another field. However, the former only identified the content of the argument and did not classify the argument components. Although the latter one further completed the classification of argument components, it still did not extract the relationship between argument components, and could not explore the argument structure in the text. It only completed part of the process of argument mining."
        },
        {
            "heading": "2.2 Tree Structured Argument Mining with Machine Learning",
            "text": "According to the argumentation paradigm theory of Van Eemeren et al. [6], Palau and Moens [15] modeled the argument information in legal texts as a tree structure and used the hand-made Context-Free Grammar (CFG) to parse and identify the argument structure of the tree structure. This method is less general and requires different context-free grammars to be formulated for different structural constraints of argument. By the Stab and Gurevych [27] [28] tree structure of persuasive Essay (Persuasive Essay, PE) dataset has been in argument mining has been applied in many studies and practices. In this dataset, Persing and Ng [23] and Stab and Gurevych [28] used the Integer Linear Programming (ILP) framework to jointly predict the types of argument components and argument relations. Several structural constraints are defined to ensure a tree structure. The arg-micro text (MT) dataset created by Peldszus [21] is another tree-structured dataset. In studies using this dataset, decoding techniques based on tree structure are frequently used, such as Minimum Spanning tree (MST) [22], and ILP [1]."
        },
        {
            "heading": "2.3 Neural Network Model in Argument Mining",
            "text": "With the popularity of deep learning, neural network models have been applied to various natural language processing tasks. For deep learning methods based on neural networks, Eger et al. [7] studied argument mining as a sequence labeling problem that relies on parsing multiple neural networks. Potash et al. [25] used sequence-to-sequence pointer network [30] in the field of argument mining\nand identified the different types of argument components and the presence of argument relations using the output of the encoder and decoder, respectively. Kuribayashi et al. [10] developed a span representation-based argumentation structure parsing model that employed ELMo [24] to derive representations for ACs."
        },
        {
            "heading": "2.4 Recent Non-Tree Structured Argument Mining",
            "text": "Recently, more works have focused on the argument mining of non-tree structures. The US Consumer Debt Collection Practices (CDCP) dataset [18] [19] greatly promotes the development of non-tree structured argument mining. The argument structures contained in this dataset are non-tree structures. On this dataset, Niculae et al. [18] carry out a structured learning method based on a factor graph. This method can also handle the tree structure of datasets. It can also be used in the PE dataset, but the factor diagram needs a specific design according to the different types of the argument structure. Galassi et al. [8] used the residual network on the CDCP dataset. Mor IO et al. [17] developed an argument mining model, which uses a task-specific parameterized module to encode argument. In this model, there is also a bi-affine attention module [5] to capture the argument. Recently, Jianzhu Bao et al. [2]tried to solve both tree structure argument and non-tree structure argument by introducing the transformation-based dependency analysis method [4] [9]. This work gained relatively good performance on the CDCP dataset but did not complete the ARTC task in one model and did not show the experiment results of ARTC.\nHowever, these methods either do not cover the argument mining process with a good performance or impose a variety of argument constraints. There is no end-to-end model for automatic and universal argument mining before. Thus, we solve all the problems above in this paper."
        },
        {
            "heading": "3 Methodology",
            "text": "As shown in Figure 2, we propose a new model called AutoAM. This model adopts the joint learning approach. It uses one model to simultaneously learn the ACTC, ARI, and ARTC three subtasks in argument mining. For the argument component extraction, the main task is to classify the argument component type, and the argument component identification task has been completed by default on both the PE and the CDCP datasets. For argument relation extraction, the model regards ARI and ARTC as one task. The model classifies the relationship between the argument components by a classifier and then gives different prediction results for two tasks by post-processing prediction labels."
        },
        {
            "heading": "3.1 Task Formulation",
            "text": "The input data contains two parts: a) A set of n argumentative text T = {T1, T2, ..., Tn}, b) for the ith argumentative text, there are m argument component spans S = {S1, S2, ..., Sm}, where every span marks the start and end scope"
        },
        {
            "heading": "AR Representation",
            "text": "of each AC Si = (starti, endi). Our aim is to train an argument mining model and use it to get output data: a) types of m ACs provided in the input data ACs = {AC1, AC2, ..., ACm}, b) k existing ARs ARs = {AR1, AR2, ..., ARk} and their types, where ARi = (ACa \u2192 ACb)."
        },
        {
            "heading": "3.2 Argument Component Extraction",
            "text": "By default, the argument component identification task has been completed. The input of the whole model is an argumentative text and a list of positional spans corresponding to each argument component Si = (starti, endi).\nWe input argumentative text T into pre-trained language models (PLMs) to get contextualized representations H \u2208 Rm\u00d7db , where db is the dimension of the last hidden state from PLMs. Therefore, we represent argumentative text asH = (h1, h2, ..., hm), where hi denotes the ith token contextualized representation.\nWe separate argument components from the paragraph using argument component spans S. In the PE dataset, the argument components do not appear continuously. We use mean pooling to get the representation of each argument component. Specifically, the i argument component can be represented as:\nACi = 1\nendi \u2212 starti + 1 endi\u2211 j=starti hi, (1)\nwhere ACi \u2208 Rdb . Therefore, all argument components in the argumentative text can be represented as ACs = (AC1, AC2, ..., ACn). For each argument component, we input it into AC Type Classifier MLPa in order. This classifier contains\na multi-layer perceptron. A Softmax layer is after it. The probability of every type of argument component can be get by:\np(yi|ACi) = Softmax(MLPa(ACi)), (2)\nwhere yi represent the predicted labels of the ith argument component. We get the final predicted label of its argument component as:\ny\u0302i = Argmax(p(yi|ACi)). (3)"
        },
        {
            "heading": "3.3 Argument Relation Extraction",
            "text": "This model views ARI and ARTC as having the same task and distinguish them by post-processing predictions. We classify every argument component pair (ACi \u2192 ACj). Argument component pairs are different of (ACi \u2192 ACj) and (ACj \u2192 ACi). We add a label, \u2018none\u2019 here. \u2018none\u2019 represents that there is no relation of ACi \u2192 ACj .\nIn the argument relation extraction part, we use the enumeration method. We utilize output results from the ACTC step. We combine two argument components and input them into AR Type Classifier to get the predicted output.\nFirst, the model uses ArguAtten (Argument Component Attention mechanism) to enhance the semantic representation of argument components. The self-attention mechanism is first proposed in the Transformer [29]. The core of this mechanism is the ability to capture how each element in a sequence relates to the other elements, i.e., how much attention each of the other elements pays to that element. When the self-attention mechanism is applied to natural language processing tasks, it can often capture the interrelationship of all lexical elements in a sentence and better strengthen the contextual semantic representation. In the task of argument mining, all argument components in an argumentative text also meet this characteristic. The basic task of argument mining is to construct an argument graph containing nodes and edges, where nodes are argument components and edges are argument relations. Before the argument relation extraction task, the self-attention mechanism of argument components can be used to capture the mutual attention of argument components. It means that it can better consider and capture the argument information of the full text. This mechanism is conducive to argument relations extraction and the construction of an argument graph. We define ArguAtten as:\nArguAtten(Q,K, V ) = Softmax( QKT\u221a\ndk )\u00d7 V, (4)\nwhere Q, K, V are got by multiplying ACs with WQ, WK , WV . They are three parameter matrices WQ,WK ,WV \u2208 Rdb\u00d7dk , and dk is the dimension of attention layer. Besides, we also use ResNet and layer normalization (LN) after the attention layer to avoid gradient explosion:\nResNetOut = LN(ACs+ArgutAtten(ACs)). (5)\nThrough the self-attention of argument components, we obtain a better contextualized representation of argument components and then begin to construct argument pairs to perform argument relation extraction.\nWe consider that the relative distance between two argument components has a decisive influence on the type of argument relations between them. By observing the dataset, we can find that there is usually no argument relation between the two argument components, which are relatively far apart. It can significantly help the model to classify the argument relation types. Therefore, we incorporate this feature into the representation of argument relations. At first, the distance vector is introduced, and the specific definition is shown as:\nVdist = (i\u2212 j)\u00d7Wdist, (6)\nwhere (i\u2212j) represents a relative distance, it can be positive or negative. Wdist \u2208 R1\u00d7ddist is a distance transformation matrix, and it can transform a distance scalar to a distance vector. ddist is the length of the distance vector.\nFor each argument relation, it comes from the source argument component (Src AC), the target argument component (Trg AC), and the distance vector (Dist Vec). We concatenate them to get the representation of an argument relation as:\nARi,j = [ACi, ACj , Vdist], (7)\nwhere ARi,j \u2208 Rdb\u00d72+ddist , ddist is the length of distance vector. Therefore, argument relations in an argumentative text can be represented as ARs = (AC1,2, AC1,3, ..., ACn,n\u22121, ), contains n\u00d7 (n\u2212 1) potential argument relations in total. We do not consider self-relation like AR = (ACi \u2192 ACi).\nFor each potential argument relation, we separately and sequentially input them into the AR Type Classifier MLPb. The classifier uses a Multi-Layer Perceptron (MLP) containing a hidden layer of 512 dimensions. The output of the last layer of the Multi-layer Perceptron is followed by a Softmax layer to obtain the probability of an argument relationship in each possible type label, as shown in:\np(yi,j |ARi,j) = Softmax(MLPb(ARi,j)), (8)\nwhere yi,j denotes the predicted label of the argument relation from the ith argument component to the jth argument component. The final predicted labels are:\ny\u0302i,j = Argmax(p(yi,j |ARi,j)). (9)\nTo get the predicted labels of ARI and ARTC, we post-processed the prediction of the model. The existence of an argument relation in the ARI task is defined as:\ny\u0302ARI =\n{ 0 if y\u0302AR = 0\n1 if y\u0302AR \u0338= 0 (10)\nwhere y\u0302AR is the predicted label from the model output.\nWhen we gain the type of an existing argument relation in the ARTC task, we assign the probability of \u2018none\u2019 to zero and select the other label with the higher probability. They are represented as:\ny\u0302ARTC = Argmax(p(yAR|ARi,j)), ynone = 0, (11)\nwhere ynone is the model output of the label \u2018none\u2019."
        },
        {
            "heading": "3.4 Loss Function Design",
            "text": "This model jointly learns the argument component extraction and the argument relation extraction. By combining these two tasks, the training objective and loss function of the final model is obtained as:\nL(\u03b8) = \u2211 i log(p(yi|ACi)) + \u2211 i,j p(yi,j |ARi,j) + \u03bb 2 ||\u03b8||2, (12)\nwhere \u03b8 represents all the parameters in the model, and \u03bb represents the coefficient of L2 regularization. According to the loss function, the parameters in the model are updated repeatedly until the model achieves better performance results to complete the model training."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Datasets",
            "text": "We evaluate our proposed model on two public datasets: Persuasive Essays (PE) [28] and Consumer Debt Collection Practices (CDCP) [18].\nThe PE dataset only has tree structure argument information. It has three types of ACs: Major-Claim, Claim, and Premise, and two types of AR: support and attack.\nThe CDCP dataset has general structure argument information, not limited to a tree structure. It is different from the PE dataset and is more difficult. The argument information in this dataset is more similar to the real world. There are five types of ACs (propositions): Reference, Fact, Testimony, Value, and Policy. Between these ACs, there are two types of ARs: reason and evidence.\nWe both use the original train-test split of two datasets to conduct experiments."
        },
        {
            "heading": "4.2 Setups",
            "text": "In the model training, roberta-base [13] was used to fine-tune, and AdamW optimizer [14] was used to optimize the parameters of the model during the training. We apply a stratified learning rate to obtain a better semantic representation of BERT context and downstream task effect. The stratified learning rate is important in this task because this multi-task learning is complex and have three\nsubtasks. The ARI and ARTC need a relatively bigger learning rate to learn the data better. The initial learning rate of the BERT layer is set as 2e-5. The learning rate of the AC extraction module and the AR extraction module is set as 2e-4 and 2e-3, respectively. After BERT output, the Dropout Rate [26] is set to 0.2. The maximum sequence length of a single piece of data is 512. We cut off ACs and ARs in the over-length text. The batch size in each training step is set to 16 in the CDCP dataset and 2 in the PE dataset. The reason is that there are more ACs in one argumentative text from the PE dataset than in the CDCP dataset.\nIn training, we set an early stop strategy with 5 epochs. We set the minimum training epochs as 15 to wait for the model to become stable. We use MacroF1ARI as monitoring indicators in our early stop strategy. That is because AR extraction is our main improvement direction. Furthermore, the ARI is between the ACTC and the ARTC, so we can better balance the three tasks\u2019 performance in the multi-task learning scenario.\nThe code implementation of our model is mainly written using PyTorch [20] library, and the pre-trained model is loaded using Transformers [32] library. In addition, model training and testing were conducted on one NVIDIA GeForce RTX 3090."
        },
        {
            "heading": "4.3 Compared Methods",
            "text": "We compare our model with several baselines to evaluate the performance:\n\u2013 Joint-ILP [28] uses Integer Linear Programming (ILP) to extract ACs and ARs. We compare our model with it in the PE dataset. \u2013 St-SVM-full [18] uses full factor graph and structured SVM to do argument mining. We compare our model with it in both the PE and the CDCP datasets. \u2013 Joint-PN [25] employs a Pointer Network with an attention mechanism to extract argument information. We compare our model with it in the PE dataset. \u2013 Span-LSTM [10] use LSTM-based span representation with ELMo to perform argument mining. We compare our model with it in the PE dataset. \u2013 Deep-Res-LG [8] uses Residual Neural Network on AM tasks. We compare our model with it in the CDCP dataset. \u2013 TSP-PLBA [17] introduces task-specific parameterization and bi-affine attention to AM tasks. We compare our model with it in the CDCP dataset. \u2013 BERT-Trans [2] use transformation-based dependency analysis method to solve AM problems. We compare our model with it in both the PE and the CDCP datasets. It is also the state of the art on two datasets."
        },
        {
            "heading": "4.4 Performance Comparison",
            "text": "The evaluation results are summarized in Table 1 and Table 2. In both tables, \u2018-\u2019 indicates that the original paper does not measure the performance of this\nmetric for its model. The best results are in bold, and the second-best results are in italics.\nOn the CDCP dataset, we can see our model achieves the best performance on all metrics in ACTC, ARI, and ARTC tasks. We are the first to complete all the tasks and get ideal results on the CDCP dataset. Our model outperforms the state of the art with an improvement of 2.1 in ACTC and 0.6 in ARI. The method BERT-Trans does not perform ARTC with other tasks at the same time, and it does not report results of ARTC, maybe due to unsatisfactory performance. In particular, compared with the previous work, we have greatly improved the task performance of ARTC and achieved ideal results.\nOn the PE dataset, our model also gets ideal performance. However, we get the second-best scores in several metrics. The first reason is that the PE dataset is tree-structured, so many previous work impose some structure constraints. Their models incorporate more information, and our model assumes they are general argument graphs in contrast. Another reason is that the models BERTTrans, Span-LSTM, and Joint-PN combine extra features to represent ACs, like paragraph types, BoW, position embedding, etc. This information will change in the different corpus, and we want to build an end-to-end universal model. For example, there is no paragraph type information in the CDCP dataset. Therefore, we do not use them in our model. Even if our model does not take these factors into account, we achieve similar results to the state of the art."
        },
        {
            "heading": "4.5 Ablation Study",
            "text": "The ablation study results are summarized in Table 3. We conduct an ablation study on the CDCP dataset to see the impact of key modules in our model. It can be observed that the stratified learning rate is the most critical in this model. It verifies the viewpoint that multi-task learning is complex in this model and ARs extraction module needs a bigger learning rate to perform well. We can see ArguAtten improve the ACTC and ARTC performance by 1.7 and 13.6. However, the ARI matric decreases a little bit. Even though the numbers are small, we think that the reason is the interrelationship between ACs has little impact on the prediction of ARs\u2019 existence. ArguAtten mainly plays an effect in predicting the type of ARs. From this table, we can also find that the distance matrix brings the important distance feature to AR representation with an overall improvement of 6.5."
        },
        {
            "heading": "5 Conclusion and Future Work",
            "text": "In this paper, we propose a novel method for argument mining and first introduce the argument component attention mechanism. This is the first end-to-end argument mining model that can extract argument information without any structured constraints and get argument relations of good quality. In the model, ArguAtten can better capture the correlation information of argument components in an argumentative paragraph so as to better explore the argumentative relationship. Our experiment results show that our method achieves the state of the art. In the future, we will continue to explore designing a better model to describe and capture elements and relationships in argument graphs."
        }
    ],
    "title": "AutoAM: An End-To-End Neural Model for Automatic and Universal Argument Mining",
    "year": 2023
}