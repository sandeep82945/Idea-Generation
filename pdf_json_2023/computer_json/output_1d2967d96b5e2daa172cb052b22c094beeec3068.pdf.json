{
    "abstractText": "We train language models (LMs) with federated learning (FL) and differential privacy (DP) in the Google Keyboard (Gboard). We apply the DP-Follow-the-Regularized-Leader (DPFTRL) [Kairouz et al., 2021b] algorithm to achieve meaningfully formal DP guarantees without requiring uniform sampling of client devices. To provide favorable privacy-utility trade-offs, we introduce a new client participation criterion and discuss the implication of its configuration in large scale systems. We show how quantile-based clip estimation [Andrew et al., 2021] can be combined with DP-FTRL to adaptively choose the clip norm during training or reduce the hyperparameter tuning in preparation for training. With the help of pretraining on public data, we train and deploy more than twenty Gboard LMs that achieve high utility and \u03c1\u2212zCDP privacy guarantees with \u03c1 \u2208 (0.2, 2), with two models additionally trained with secure aggregation [Bonawitz et al., 2017]. We are happy to announce that all the next word prediction neural network LMs in Gboard now have DP guarantees, and all future launches of Gboard neural network LMs will require DP guarantees. We summarize our experience and provide concrete suggestions on DP training for practitioners.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zheng Xu"
        },
        {
            "affiliations": [],
            "name": "Yanxiang Zhang"
        },
        {
            "affiliations": [],
            "name": "Peter Kairouz"
        },
        {
            "affiliations": [],
            "name": "H. Brendan McMahan"
        },
        {
            "affiliations": [],
            "name": "Jesse Rosenstock"
        },
        {
            "affiliations": [],
            "name": "Yuanbo Zhang"
        }
    ],
    "id": "SP:64a5b0fdb9e3d64981cef477125ee39662b89f82",
    "references": [
        {
            "authors": [
                "Martin Abadi",
                "Andy Chu",
                "Ian Goodfellow",
                "H Brendan McMahan",
                "Ilya Mironov",
                "Kunal Talwar",
                "Li Zhang"
            ],
            "title": "Deep learning with differential privacy",
            "venue": "In Proceedings of the 2016 ACM SIGSAC conference on computer and communications security,",
            "year": 2016
        },
        {
            "authors": [
                "Naman Agarwal",
                "Peter Kairouz",
                "Ziyu Liu"
            ],
            "title": "The skellam mechanism for differentially private federated learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Galen Andrew",
                "Om Thakkar",
                "H Brendan McMahan",
                "Swaroop Ramaswamy"
            ],
            "title": "Differentially private learning with adaptive clipping",
            "venue": "Conference on Neural Information Processing Systems (NeurIPS),",
            "year": 2021
        },
        {
            "authors": [
                "Borja Balle",
                "Peter Kairouz",
                "Brendan McMahan",
                "Om Thakkar",
                "Abhradeep Guha Thakurta"
            ],
            "title": "Privacy amplification via random check-ins",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "James Henry Bell",
                "Kallista A Bonawitz",
                "Adri\u00e0 Gasc\u00f3n",
                "Tancr\u00e8de Lepoint",
                "Mariana Raykova"
            ],
            "title": "Secure single-server aggregation with (poly) logarithmic overhead",
            "venue": "In Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security,",
            "year": 2020
        },
        {
            "authors": [
                "Kallista Bonawitz",
                "Peter Kairouz",
                "Brendan McMahan",
                "Daniel Ramage"
            ],
            "title": "Federated learning and privacy: Building privacy-preserving systems for machine learning and data science on decentralized data",
            "year": 2021
        },
        {
            "authors": [
                "Keith Bonawitz",
                "Vladimir Ivanov",
                "Ben Kreuter",
                "Antonio Marcedone",
                "H Brendan McMahan",
                "Sarvar Patel",
                "Daniel Ramage",
                "Aaron Segal",
                "Karn Seth"
            ],
            "title": "Practical secure aggregation for privacypreserving machine learning",
            "venue": "In proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security,",
            "year": 2017
        },
        {
            "authors": [
                "Keith Bonawitz",
                "Hubert Eichner",
                "Wolfgang Grieskamp",
                "Dzmitry Huba",
                "Alex Ingerman",
                "Vladimir Ivanov",
                "Chloe Kiddon",
                "Jakub Kone\u010dn\u1ef3",
                "Stefano Mazzocchi",
                "Brendan McMahan"
            ],
            "title": "Towards federated learning at scale: System design",
            "venue": "Proceedings of machine learning and systems,",
            "year": 2019
        },
        {
            "authors": [
                "Mark Bun",
                "Thomas Steinke"
            ],
            "title": "Concentrated differential privacy: Simplifications, extensions, and lower bounds",
            "venue": "In Theory of Cryptography Conference,",
            "year": 2016
        },
        {
            "authors": [
                "Nicholas Carlini",
                "Chang Liu",
                "\u00dalfar Erlingsson",
                "Jernej Kos",
                "Dawn Song"
            ],
            "title": "The secret sharer: Evaluating and testing unintended memorization in neural networks",
            "venue": "In 28th USENIX Security Symposium (USENIX Security",
            "year": 2019
        },
        {
            "authors": [
                "Wei-Ning Chen",
                "Christopher A Choquette Choo",
                "Peter Kairouz",
                "Ananda Theertha Suresh"
            ],
            "title": "The fundamental price of secure aggregation in differentially private federated learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Yae Jee Cho",
                "Pranay Sharma",
                "Gauri Joshi",
                "Zheng Xu",
                "Satyen Kale",
                "Tong Zhang"
            ],
            "title": "On the convergence of federated averaging with cyclic client participation",
            "venue": "arXiv preprint arXiv:2302.03109,",
            "year": 2023
        },
        {
            "authors": [
                "Christopher A Choquette-Choo",
                "H Brendan McMahan",
                "Keith Rush",
                "Abhradeep Thakurta"
            ],
            "title": "Multi-epoch matrix factorization mechanisms for private machine learning",
            "venue": "arXiv preprint arXiv:2211.06530,",
            "year": 2022
        },
        {
            "authors": [
                "Soham De",
                "Leonard Berrada",
                "Jamie Hayes",
                "Samuel L Smith",
                "Borja Balle"
            ],
            "title": "Unlocking highaccuracy differentially private image classification through scale",
            "venue": "arXiv preprint arXiv:2204.13650,",
            "year": 2022
        },
        {
            "authors": [
                "Cynthia Dwork",
                "Aaron Roth"
            ],
            "title": "The algorithmic foundations of differential privacy",
            "venue": "Foundations and Trends\u00ae in Theoretical Computer Science,",
            "year": 2014
        },
        {
            "authors": [
                "Robin C Geyer",
                "Tassilo Klein",
                "Moin Nabi"
            ],
            "title": "Differentially private federated learning: A client level perspective",
            "venue": "arXiv preprint arXiv:1712.07557,",
            "year": 2017
        },
        {
            "authors": [
                "Andrew Hard",
                "Kanishka Rao",
                "Rajiv Mathews",
                "Swaroop Ramaswamy",
                "Fran\u00e7oise Beaufays",
                "Sean Augenstein",
                "Hubert Eichner",
                "Chlo\u00e9 Kiddon",
                "Daniel Ramage"
            ],
            "title": "Federated learning for mobile keyboard prediction",
            "venue": "arXiv preprint arXiv:1811.03604,",
            "year": 2018
        },
        {
            "authors": [
                "Peter Kairouz",
                "Ziyu Liu",
                "Thomas Steinke"
            ],
            "title": "The distributed discrete gaussian mechanism for federated learning with secure aggregation",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Peter Kairouz",
                "Brendan Mcmahan",
                "Shuang Song",
                "Om Thakkar",
                "Abhradeep Thakurta",
                "Zheng Xu"
            ],
            "title": "Practical and private (deep) learning without sampling or shuffling",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2021
        },
        {
            "authors": [
                "Xuechen Li",
                "Florian Tramer",
                "Percy Liang",
                "Tatsunori Hashimoto"
            ],
            "title": "Large language models can be strong differentially private learners",
            "venue": "arXiv preprint arXiv:2110.05679,",
            "year": 2021
        },
        {
            "authors": [
                "Brendan McMahan",
                "Abhradeep Thakurta"
            ],
            "title": "Federated learning with formal differential privacy guarantees, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Brendan McMahan",
                "Eider Moore",
                "Daniel Ramage",
                "Seth Hampson",
                "Blaise Aguera y Arcas"
            ],
            "title": "Communication-efficient learning of deep networks from decentralized data",
            "venue": "In AISTATS,",
            "year": 2017
        },
        {
            "authors": [
                "Brendan McMahan",
                "Daniel Ramage",
                "Kunal Talwar",
                "Li Zhang"
            ],
            "title": "Learning differentially private recurrent language models",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2018
        },
        {
            "authors": [
                "Natalia Ponomareva",
                "Hussein Hazimeh",
                "Alex Kurakin",
                "Zheng Xu",
                "Carson Denison",
                "H. Brendan McMahan",
                "Sergei Vassilvitskii",
                "Steve Chien",
                "Abhradeep Thakurta"
            ],
            "title": "How to dp-fy ml: A practical guide to machine learning with differential privacy, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu"
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "Journal of Machine Learning Research,",
            "year": 2019
        },
        {
            "authors": [
                "Swaroop Ramaswamy",
                "Om Thakkar",
                "Rajiv Mathews",
                "Galen Andrew",
                "H Brendan McMahan",
                "Fran\u00e7oise Beaufays"
            ],
            "title": "Training production language models without memorizing user data",
            "venue": "arXiv preprint arXiv:2009.10031,",
            "year": 2020
        },
        {
            "authors": [
                "Raphael Shu",
                "Hideki Nakayama"
            ],
            "title": "Compressing word embeddings via deep compositional code learning",
            "venue": "arXiv preprint arXiv:1711.01068,",
            "year": 2017
        },
        {
            "authors": [
                "Boxin Wang",
                "Yibo Jacky Zhang",
                "Yuan Cao",
                "Bo Li",
                "H Brendan McMahan",
                "Sewoong Oh",
                "Zheng Xu",
                "Manzil Zaheer"
            ],
            "title": "Can public large language models help private cross-device federated learning",
            "venue": "arXiv preprint arXiv:2305.12132,",
            "year": 2023
        },
        {
            "authors": [
                "Jianyu Wang",
                "Zachary Charles",
                "Zheng Xu",
                "Gauri Joshi",
                "H Brendan McMahan",
                "Blaise Aguera y Arcas",
                "Maruan Al-Shedivat",
                "Galen Andrew",
                "Salman Avestimehr",
                "Katharine Daly"
            ],
            "title": "A field guide to federated optimization",
            "year": 2021
        },
        {
            "authors": [
                "Zheng Xu",
                "Maxwell Collins",
                "Yuxiao Wang",
                "Liviu Panait",
                "Sewoong Oh",
                "Sean Augenstein",
                "Ting Liu",
                "Florian Schroff",
                "H Brendan McMahan"
            ],
            "title": "Learning to generate image embeddings with user-level differential privacy",
            "venue": "arXiv preprint arXiv:2211.10844,",
            "year": 2022
        },
        {
            "authors": [
                "Linting Xue",
                "Noah Constant",
                "Adam Roberts",
                "Mihir Kale",
                "Rami Al-Rfou",
                "Aditya Siddhant",
                "Aditya Barua",
                "Colin Raffel"
            ],
            "title": "mt5: A massively multilingual pre-trained text-to-text transformer",
            "year": 2010
        },
        {
            "authors": [
                "Timothy Yang",
                "Galen Andrew",
                "Hubert Eichner",
                "Haicheng Sun",
                "Wei Li",
                "Nicholas Kong",
                "Daniel Ramage",
                "Fran\u00e7oise Beaufays"
            ],
            "title": "Applied federated learning: Improving google keyboard query suggestions",
            "venue": "arXiv preprint arXiv:1812.02903,",
            "year": 2018
        },
        {
            "authors": [
                "Da Yu",
                "Saurabh Naik",
                "Arturs Backurs",
                "Sivakanth Gopi",
                "Huseyin A Inan",
                "Gautam Kamath",
                "Janardhan Kulkarni",
                "Yin Tat Lee",
                "Andre Manoel",
                "Lukas Wutschitz"
            ],
            "title": "Differentially private fine-tuning of language models",
            "venue": "arXiv preprint arXiv:2110.06500,",
            "year": 2021
        },
        {
            "authors": [
                "Chen Zhu",
                "Zheng Xu",
                "Mingqing Chen",
                "Jakub Kone\u010dn\u1ef3",
                "Andrew Hard",
                "Tom Goldstein"
            ],
            "title": "Diurnal or nocturnal? federated learning of multi-branch networks from periodically shifting distributions",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "FL and Gboard LMs. In cross-device federated learning (FL), client devices collaboratively train a model without directly exchanging their local data [Kairouz et al., 2019]. Google Keyboard (Gboard) was an early adopter of FL to train models that improve the user experience, following data minimization principles [Bonawitz et al., 2021] to protect users\u2019 privacy from some risks. Language models (LMs) are trained with FL to support various features in Gboard, including Next Word Prediction (NWP), Smart Compose (SC), and On-The-Fly rescoring (OTF). As illustrated in Fig. 1, NWP [Hard et al., 2018] uses an LM to suggest a word, which is triggered after a previous word is committed; SC provides longer inline suggestions to accelerate typing, which can be triggered per character when the confidence is high; OTF is used to re-rank the candidate words generated during typing before a word is committed.\nModels, metrics and tasks. We train LMs with the same neural network (NN) architecture described in [Hard et al., 2018]: a one-layer LSTM/CIFG of 670 hidden neurons, with input and output word-based embeddings of dimension 96. OTF LMs use a larger vocabulary (\u223c 30K words) compared to NWP LMs (\u223c 10\u201320K words); the number of parameters for models with a 10K/20K/30K vocabulary is 2.4M/4.4M/6.4M, respectively. SC is a downstream task that reuses NWP LMs without any retraining from data. We train NWP LMs and OTF LMs from populations of devices\n\u2217Equal contribution, alphabetical order. Correspondence to {xuzheng,zhangyx}@google.com.\nar X\niv :2\n30 5.\n18 46\n5v 2\n[ cs\n.L G\n] 1\n7 Ju\ncategorized by language and location. For example, en-US NWP denotes the task of training NWP model on data generated by devices using English in the United States.\nFederated Averaging (FedAvg) [McMahan et al., 2017] and variants [Wang et al., 2021] are popular FL training algorithms in practice. In each communication round, the server will orchestrate a small subset of client devices for training and aggregate the resulting model deltas to update the global model. In a successful round, the system guarantees the number of clients participating in training is at least as large as the configured report goal [Bonawitz et al., 2019]. A model is typically tested and deployed after training for several thousands of rounds. Top-1 in-vocab accuracy is used to track the utility during training and additional metrics for A/B testing are introduced in Sec. 3.\nDP and DP-FTRL. Differential privacy (DP) can be combined with FL to provide a formal guarantee that the trained model will not memorize specific users\u2019 data, which provides stronger privacy protection by executing data anonymization principles [Bonawitz et al., 2021; Wang et al., 2021]. Ramaswamy et al. [2020] applied DP-FedAvg [Geyer et al., 2017; McMahan et al., 2018], a variant of DP-SGD [Abadi et al., 2016] for user/client-level DP, to train production LMs in FL. Ramaswamy et al. [2020] demonstrated anonymization via empirical auditing techniques by Carlini et al. [2019] but did not provide a formal DP guarantee. Achieving a strong formal DP guarantee for DP-FedAvg would require privacy amplification-by-sampling, which necessitates sampling clients uniformly at random on each round. However, a cross-device FL system has limited control over client sampling as devices have to satisfy local criteria such as being charging and connected to an unmetered network to be eligible for participation [Balle et al., 2020; Bonawitz et al., 2019]. In contrast, we deploy a recent algorithm, DP-FTRL [Kairouz et al., 2021b], allowing us to achieve strong privacy and utility for production models without uniform sampling assumptions.\nContributions. We discuss our strategy and experience of training Gboard LMs with FL and DP. We introduce an algorithm that enables adaptive clipping [Andrew et al., 2021] in DP-FTRL [Kairouz et al., 2021b] (Sec. 2.1), which can reliably estimate the clip norm to reduce hyperparameter tuning. We discuss the impact of scaling up computation and limiting client participation (Sec. 2.2), and identify the algorithm and system configurations for the regime of strong privacy and utility. We also successfully apply pre-training (Sec. 2.3) to improve privacy and utility, which is (to the best of our knowledge) the first time pretraining is applied to training a DP model directly from users\u2019 data.\nWe combine DP-FTRL with secure aggregation (SecAgg) to further strengthen the data minimization properties of our approach (Sec. 2.4). Fig. 2 provides a system overview of the techniques for training Gboard language models with federated learning and differential privacy. Finally, we summarize concrete suggestions for practitioners training differentially private models to deploy in production in (Sec. 2.5), and present and analyze twenty Gboard LMs trained with formal DP guarantees (Sec. 3). We are happy to announce that all the next word prediction neural network LMs in Gboard now have DP guarantees, and all future launches of Gboard neural network LMs will require DP guarantees."
        },
        {
            "heading": "2 DP FL in Practice",
            "text": ""
        },
        {
            "heading": "2.1 DP-FTRL and adaptive clipping",
            "text": "As described in Alg. 1, we apply DP-FTRL in FL by modifying the FedAvg algorithm: clip the model update \u2206, and add noise when updating the global model. Two additional hyperparameters are introduced for DP: the clip norm C, which bounds the norm of \u2206, and the noise multiplier z, which determines the standard deviation zC for the added Gaussian noise. We discuss clip norm in this section and defer the discussion of noise multiplier and other privacy related hyperparameters to Sec. 2.2.\nAndrew et al. [2021] introduced an adaptive clipping method that automatically adjusts the clip norm each round by privately estimating the norm of the model delta at a targeted quantile. However, adaptive clipping cannot be directly applied to DP-FTRL as the tree-based noise addition in DP-FTRL assumes a fixed clip norm across rounds. We integrate adaptive clipping in DP-FTRL through restarts, where the quantile estimate Ct is continually tracked but only becomes an active clip norm C\u03b8 upon tree restarting. As both the aggregated model delta \u2206\u0303\nt and the quantile b\u0303t use tree-based noise, we can directly use the privacy accounting in [Kairouz et al., 2021b] by applying the noise transformation in Thm. 1 .\nTheorem 1 (Privacy Accounting for Adaptive Clipping [Andrew et al., 2021]). One step of DPFTRL with adaptive clipping using \u03c3b noise standard deviation on the clipped counts \u2211 bti and z\u2206\nAlgorithm 1 Federated DP-FTRL with adaptive clipping\ninput : report goal m, learning rate for model weights on client \u03b7c and on server \u03b7s, momentum \u03b2 = 0.9, noise multiplier for model delta z\u2206, total number of rounds T , restart rounds\nR = {128 + 1024i, i = 0, 1, . . .}, quantile based norm estimation C0 , target quantile \u03b3 = 0.5 , learning rate for norm \u03b7\u03b3 = 0.2 , noise stddev for clip estimation \u03c3b = m/20\nInitialize model \u03b80, momentum buffer \u2206\u03040 = 0, clip norm C\u03b8 = C 0 Initialize tree T\u03b8 with z\u2206, C\u03b8, and Tb with \u03c3b for each round t = 0, 1, 2, . . . , T do Qt \u2190 (at least m users for this round) for each user i \u2208 Qt in parallel do (\u2206ti, b t i)\u2190 ClientUpdate(i, \u03b8t, C\u03b8, Ct)\n//Update model weights with noise addition \u2206\u0303t = 1mPrivateSum ( T\u03b8, \u2211 i\u2208Qk \u2206 k i , k \u2208 [0, t] ) \u2206\u0304t = \u03b2\u2206\u0304t\u22121 + \u2206\u0303t, \u03b8t+1 \u2190 \u03b80 + \u03b7s\u2206\u0304t //Estimate quantile-based norm\nb\u0303t = 1mPrivateSum ( Tb, \u2211 i\u2208Qk b k i , k \u2208 [0, t] ) Ct+1 \u2190 C0 \u00b7 exp ( \u2212\u03b7\u03b3(b\u0303t \u2212 t\u03b3)\n) //Restart and adjust clip norm\nif t \u2208 R then C\u03b8 \u2190 Ct+1\nRestart tree T\u03b8 and Tb with updated C\u03b8\nfunction ClientUpdate(i, \u03b80, C\u03b8, C) \u03b8 \u2190 \u03b80 G \u2190 (user i\u2019s local data split into batches) for batch g \u2208 G do\n\u03b8 \u2190 \u03b8 \u2212 \u03b7c\u2207\u2113(\u03b8; g) \u2206\u2190 \u03b8 \u2212 \u03b80 b\u2190 I||\u2206||\u2264C\n\u2206\u2032 \u2190 \u2206 \u00b7min ( 1, C\u03b8||\u2206|| ) //Clipping return (\u2206\u2032, b)\nnoise multiplier on the vector sums \u2211\n\u2206ti is equivalent to one step of non-adaptive DP-FTRL with noise multiplier z if we set z\u2206 = ( z\u22122 \u2212 (2\u03c3b)\u22122 )\u22121/2 .\nIn practice, Alg. 1 slightly inflates the noise for the model from zC to z\u2206C and requires restarts that complicate the privacy accounting for DP-FTRL. Moreover, we find that a fixed clip norm can achieve comparable or slightly better model utility, and is more robust in experiments with large report goal. For example, adaptive clipping for the de-DE NWP model experiences catastrophic failure and makes no progress in the first 1000 rounds.\nNevertheless, adaptive clipping can reduce hyperparameter tuning for many tasks when privacy budget allows. Fig. 3 shows the evaluation accuracy and corresponding clip norm for DP training the en-GB NWP model with report goal 6500 and noise multiplier 7. The adaptive clip curve starts from a small initial clip norm to avoid catastrophic failure due to large initial noise and eventually catches up on accuracy. The estimated clip norm (quantile \u03b3 = 0.5) stabilizes and we can fix the clip norm to 5 based on the estimated value. The clip norm is relatively insensitive, especially when tuning together with the server learning rate. However, clip norm can have a wide tuning range across tasks and models, and quantile-based estimation is still useful for estimating a clip norm to be fixed."
        },
        {
            "heading": "2.2 DP parameters and system configuration",
            "text": "The privacy guarantees of DP-FTRL [Kairouz et al., 2021b] are affected by several factors: noise multiplier z, number of total rounds T , max participation (MaxP) of a client, and min separation (MinS) of rounds between the participation of the same client. The noise multiplier is a conventional parameter for controlling privacy-utility trade-off: large noise achieves strong privacy guarantees but can potentially hurt the utility. Achieving the same utility with smaller rounds T can significantly\nimprove the privacy guarantees. Next, we discuss the effect of MaxP and MinS, and the privacyutility-computation trade-off for system configuration.\nClient participation. DP-FTRL achieves strong privacy if each client only participates once during training, or the number of client participation is limited when a client can participate multiple times. Two parameters are introduced to characterize client participation for DP-FTRL: the maximum participations (MaxP) of a client in all training rounds and the minimum round separation (MinS) between any single client\u2019s two participations. MaxP and MinS are correlated as MaxP is upper bounded by rounds T divided by MinS. In general, for fixed rounds T , decreasing MaxP and increasing MinS can lead to stronger privacy guarantees without changing utility. In addition, Cho et al. [2023] suggests potential advantage of increasing MinS for utility.\nWhen using the worst-case MaxP estimated by rounds T divided by MinS, Fig. 4c shows increasing MinS can achieve stronger privacy measured by smaller zCDP values. However, the maximum MinS is limited by the population size divided by the number of clients per round lower bounded by the report goal. For example, when the report goal is 6500 for small population of around 106,\nMinS has to be smaller than 153 rounds, so strong privacy guarantees are difficult to achieve when training for 3000 rounds. While we cannot measure the precise population size in the FL system due to client dynamics, we estimate the population size of various Gboard tasks as ranging from 0.8 million to 16.6 million in Tab. 1.\nReport goal. We study report goal for privacy-computation trade-off based on a hypothesis used in [Kairouz et al., 2021b; McMahan et al., 2018; Xu et al., 2022]: for sufficiently large data, the utility is approximately non-decreasing if the noise multiplier and clients per round (lower bounded by report goal) proportionally increase. We provide empirical justification to this hypothesis by comparing the evaluation accuracy of two training runs: one with a report goal of 500 and noise multiplier of 0.54, versus another of report goal 6500 and noise multiplier 7. On more than three Gboard language tasks, we observed that the final utility remains similar, or slightly better for larger report goals. Moreover, using a larger report goal speeds up learning at the beginning of training. Based on the hypothesis, we plot Figs. 4a and 4b by linearly increasing report goal and noise multiplier, and assuming the MinS is set to the maximum possible value (population divided by report goal) for strong privacy. Though a large report goal can limit the MinS, it generally leads to stronger privacy guarantees for reasonable population size and total rounds.\nSystem configuration. According to Figs. 4a and 4b, we choose a large report goal of 6500 supported by the large scale FL systems and aim for maximum MinS for DP-FTRL. To control MinS in practice, a timer is introduced on clients in the FL system so that a client will only become eligible to participate in training (again) after a certain period of time has passed. McMahan and Thakurta [2022] used a timer period of 24 hours to train the es-ES NWP model, which led to an observed MinS of 313. The MinS of es-ES is upper bounded by 4.21M/6500 \u223c 647 and can be potentially improved by increasing the timer period. We increase the timer period in the unit of 24 hours due to the uneven diurnal participation pattern [Yang et al., 2018; Zhu et al., 2022], and generally observe that MinS can proportionally increase with the timer period before reaching the possible maximum. However, there are many factors in the FL system that may affect the wall clock training speed, which makes it challenging to optimize the timer period to maximize MinS."
        },
        {
            "heading": "2.3 Public pretraining",
            "text": "We explore pretraining on public data for production models, which were shown to substantially improve model utility in DP simulations [De et al., 2022; Li et al., 2021; Wang et al., 2023; Xu et al., 2022; Yu et al., 2021]. We pretrain a model for each Gboard language task using the multi-lingual C4 dataset [Raffel et al., 2019; Xue et al., 2020] collected from public web pages. Fig. 3a shows that pretraining can reduce \u223c 1000 rounds to reach a given utility threshold under the same noise multiplier, which can significantly improve the privacy guarantees as shown in Fig. 4.\nWe additionally observe that: (1) it is challenging to fine-tune from a pretrained model when the word embeddings are shared for input and output to reduce the parameter size of LMs for on-device deployment; (2) the accuracy may decrease in the first a few rounds of fine-tuning; (3) pretraining helps with diminishing marginal returns: at some point further pretraining does not necessarily improve the final performance. Therefore, we use models with separate input and output embeddings and pretrain with half of the C4 dataset for Gboard LMs."
        },
        {
            "heading": "2.4 Combining with secure aggregation",
            "text": "Secure aggregation (SecAgg) Bonawitz et al. [2017] is a cryptographic multiparty computation protocol ensures that the central server can only access the aggregated update from a large set of clients, preventing inspection of individual client updates. We combine SecAgg and DP-FTRL to provide strong data minimization and anonymization protection [Bonawitz et al., 2021]. This work considers\ncentral DP and honest-but-curious server, and not the setting where the DP mechanism is applied distributively (i.e. on the client) as in Agarwal et al. [2021]; Kairouz et al. [2021a]. We describe the algorithm in Alg. 2, and provide detailed discussion tackling the main challenge: how we can properly calibrate the sensitivity for DP when using SecAgg.\nAlgorithm 2 Federated DP-FTRL with SecAgg\ninput : report goal m, learning rate for model weights on client \u03b7c and on server \u03b7s, momentum \u03b2 = 0.9, noise multiplier for model delta z\u2206, total number of rounds T , fixed clip norm C , and\na large scaling parameter s .\nInitialize model \u03b80, momentum buffer \u2206\u03040 = 0 Initialize tree T with z\u2206 and C d\u2190 dimension of \u03b80\nC\u221e \u2190 ceil ( 2s \u00b7 C log(d)/ \u221a d )\nM \u2190 2 \u00b7 C\u221e \u00b7m+ 1 //SecAgg\u2019s modulus \u03b1\u2190 exp (\u22120.5)\nH\u0303d \u2190 1\u221adHd //Normalized Hadmard matrix for each round t = 0, 1, 2, . . . , T do Qt \u2190 (m users for this round) \u03be \u2190 uniform random sign vector \u2208 {\u22121,+1}d\nD\u03be \u2190 diagonal matrix with \u03be on the diagonal for each user i \u2208 Qt in parallel do \u2206ti \u2190 ClientUpdate(i, \u03b8t, \u03be)\n//Securely aggregate the model updates \u2206t = \u2211 i\u2208Qk \u2206 t i mod M\n//Unshift, unscale, and unrotate\n\u2206t \u2190 (1/s) \u00b7D\u03be \u00b7 H\u0303Td \u00b7 (\u2206t \u2212 C\u221e) //Update model weights with noise addition \u2206\u0303t = 1mPrivateSum (T , \u2206t, k \u2208 [0, t])\n\u2206\u0304t = \u03b2\u2206\u0304t\u22121 + \u2206\u0303t, \u03b8t+1 \u2190 \u03b80 + \u03b7s\u2206\u0304t\nfunction ClientUpdate(i, \u03b80, \u03be) \u03b8 \u2190 \u03b80 D\u03be \u2190 diagonal matrix with \u03be on the diagonal G \u2190 (user i\u2019s local data split into batches) for batch g \u2208 G do\n\u03b8 \u2190 \u03b8 \u2212 \u03b7c\u2207\u2113(\u03b8; g) \u2206\u2190 \u03b8 \u2212 \u03b80 \u2206\u2190 \u2206 \u00b7 s \u00b7min ( 1, C||\u2206||2 ) //Scale and \u21132 clip\n\u2206\u2190 H\u0303d \u00b7D\u03be \u00b7\u2206 //Randomly rotate \u2206\u2190 \u2206 \u00b7min ( 1, C\u221e||\u2206||\u221e ) //Clip to bound \u2113\u221e\n//Conditional stochastic rounding repeat\n\u2206\u2032 \u2190 stochastically round the coordinates of \u2206\nuntil \u2225\u2206\u2032\u222522\u2264s 2C2+d/4+ \u221a 2 log(1/\u03b1)\u00b7(sC+ \u221a d/2)\n//Shift to center around C\u221e and apply modulo return \u2206\u2032 + C\u221e mod M\nSecAgg overview. We start by overviewing how SecAgg works at a high level. For an in depth treatment of the cryptographic protocol, we refer the reader to Bell et al. [2020]; Bonawitz et al. [2017]. We will treat SecAgg as a \u201cblack box\u201d which is guaranteed to faithfully compute the modular sum of integer vectors in a finite group, while not revealing any information about any individual vector beyond what can be learned from the sum; our methods do not depend on the specifics of the implementation of SecAgg. Concretely, for a vector \u2206i \u2208 ZdM representing the client\u2019s model update after all its elements being appropriately represented in a finite group of size M (i.e., \u2206i is a d-dimensional vector with integer entries in the finite set {0, \u00b7 \u00b7 \u00b7 ,M}), using SecAgg as a \u201cblack box\u201d, the server obtains:\n\u2206 := m\u2211 i \u2206i mod M, (1)\nand applies the DP-FTRL protocol to \u2206. SecAgg\u2019s finite group, M , dictates the number of bits needed per parameter, and d log2 M bits are needed per model update vector.\nSensitivity analysis. Our goal is to show that if the model updates are clipped to an \u21132 norm of C prior to being represented as integer vectors, the sensitivity of \u2206 can be bounded and quantified, and then we can rely on the DP analysis of DP-FTRL to do privacy accounting. Before diving into the sensitivity analysis, we first overview the steps needed to transform a model update vector in Rd to one in ZdM , which is what the SecAgg protocol requires. These steps are performed on the client device; they are shown in Algorithm 2 inside of the ClientUpdate function (and highlighted in pink).\nThe first step is to clip the model update to bound its \u21132 norm to C and then scaling it by a factor s (a large constant). Notice that this clipping operation is required for DP even if we did not want to use SecAgg\u2014only the post-clipping scaling is new. As we will see later in the discretization step, scaling by s will prove crucial in terms of minimizing the inflation in sensitivity. Next, we apply a uniform random rotation, which helps us go from an \u21132 geometry to an \u2113\u221e geometry. Precisely, a random rotation preserves the \u21132 norm and provides a \u201dhigh probability\u201d bound on the \u2113\u221e norm: for x \u2208 Rd, the coordinates of Urotatex are sub-Gaussians with \u03c32 = ||x||22/d (see Lemma 28 of Kairouz et al. [2021a]). For an efficient implementation, we randomly rotate x by first randomizing the signs of its coordinates and then multiplying it by an appropriately scaled Hadamard matrix (see Lemma 29 of Kairouz et al. [2021a]). After this step, each coordinate of the clipped, scaled, and randomly rotated model update vector is in the range [\u22122sC/ \u221a d, 2sC/ \u221a d] with high probability1. Applying a union bound, the \u2113\u221e norm of this vector is in the range [\u22122sC log(d)/ \u221a d, 2sC log(d)/ \u221a d] with high probability. We therefore apply another clipping operation to bound the \u2113\u221e of the vector to C\u221e = ceil(2sC log(d)/ \u221a d). This bounds the range of every coordinate in the vector while guaranteeing that: (a) its \u21132 norm is not inflated; and (b) the signal is preserved since this clipping operation is expected to be a \u201cno op\u201d with high probability. We are finally ready to move from Rd to Zd. To this end, we leverage the conditional stochastic rounding procedure introduced in Kairouz et al. [2021a] to obtain a bounded norm on the scaled and rounded client vector.\nLet x\u0303 be a stochastic rounding of vector x \u2208 Rd to the integer grid Zd. Then, for \u03b1 \u2208 (0, 1), we have P [ \u2225x\u0303\u222522 \u2264 \u2225x\u222522 + d/4 + \u221a 2 log(1/\u03b1) \u00b7 ( \u2225x\u22252 + \u221a d/2 )] \u2265 1\u2212 \u03b1. (2)\nConditional stochastic rounding is thus defined as retrying the stochastic rounding on x until \u2225x\u0303\u222522 is within the probabilistic bound above. We show, in Algorithm 2, how we can specialize this approach to our context, yielding discretized model update vectors in {\u2212C\u221e, \u00b7 \u00b7 \u00b7 , C\u221e}d with an \u211322 norm smaller than s2C2+ d/4+ \u221a 2 log(1/\u03b1) \u00b7 ( sC + \u221a d/2 ) . We used \u03b1 = exp(\u22120.5) in production\ntraining. We finally shift the vectors so that they have coordinates centered around C\u221e, i.e. in {0, \u00b7 \u00b7 \u00b7 ,+C\u221e, \u00b7 \u00b7 \u00b7 , 2C\u221e}d, and apply modulo M so that the vector entries are in the right format that is expected by SecAgg.\nAfter applying the SecAgg protocol, we unshift, unscale, and unrotate the securely aggregated client outputs. It\u2019s important to notice that M = 2C\u221e \u00b7m + 1 was deliberately chosen so that no modulo wrap-arounds (overflows) happen during the secure aggregation step2. Upon unscaling by 1/s, the \u211322 norm of the vectors securely summed becomes C 2 inflated = C 2 + d/(4s2) + \u221a\n2 log(1/\u03b1) \u00b7( C/s+ \u221a d/(2s2) ) . Thus, the larger the s is, the closer the inflated \u21132 is to C (the original clip norm applied in Rd). In reality, we cannot choose s to be arbitrarily large because M , which dictates 1The factor of 2 is to ensure that the range we consider contains 4\u03c3. We could have used a larger factor, but it would require increasing M , the modulus of SecAgg. In practice, we observed that 2 performs well. 2We did not explicitly use clipping to bound the \u2113\u221e of client vectors in the current version of training. This means that modulo wrap-arounds could have happened, inflating the \u21132 sensitivity by up to dM . However, our choice of M was relatively large, and the probability of a modulo wrap-around is less than 2\u00d7 10\u221227.\nthe communication and computational costs of SecAgg, scales linearly with s. For DP accounting purposes, we have used the DP-FTRL accounting code with Cinflated instead of C to account for SecAgg\u2019s role.\nChallenges and future work. The large report goal requirement for strong DP guarantees is challenging for SecAgg in practice, which requires a slightly different system configuration. The SecAgg training speeds we observe are still notably slower, and we leave for future work potential improvements such as compression for communication efficiency [Chen et al., 2022], new DP methods to reduce report goal [Choquette-Choo et al., 2022], and embedding compression to reduce round time [Shu and Nakayama, 2017]."
        },
        {
            "heading": "2.5 Recommended strategies and practices",
            "text": "We summarize our strategy for training Gboard LMs with DP. (1) Pre-train the model on public datasets if possible. (2) Choose the maximum noise multiplier that meets the utility target based on small report goal simulation experiments on public datasets that is similar to the production task. (3) Based on the target number of rounds and estimated population, linearly increase the report goal and noise multiplier to meet the privacy target, and choose a large report goal supported by the system. If the privacy target is unachievable, fix the report goal to maximum, and increase the noise multiplier to target on a model with suboptimal utility. (4) Estimate the possible maximum MinS based on chosen report goal and estimated population, and configure the timer period to approach the MinS; use previous experience of model training speed if applicable. (5) If the hyperparameters (e.g., learning rates) are known from previous experiments or simulation on public datasets, apply DP-FTRL with adaptive clipping (Alg. 1) without manual tuning to try meet the privacy and utility goals. Note that Alg. 1 needs to account the noise inflation and restart for privacy guarantees. (6) If Alg. 1 fails or stronger privacy and utility are desirable, we can run a few small report goal experiments with Alg. 1 that tune quantile \u03b3 and server learning rate \u03b7s, select the best learning rate, and fix the clip norm based on the estimation; and run DP-FTRL with large report goals. (7) SecAgg can be used for all experiments, and precise MaxP and MinS are computed by post-processing for privacy accounting."
        },
        {
            "heading": "3 Deploying DP LMs",
            "text": "A/B test metrics. We introduce metrics in A/B test to measure the utility of Gboard LMs. (1) Picked Rate (PRate): the ratio of picked candidates among the NWP predictions; or SC predictions when it is triggered. (2) Accuracy (Acc): the ratio of candidates matching the final committed words among the NWP model predictions. (3) Trigger Rate: the ratio of words with SC triggered among all committed words, which is an important metric when PRate is fixed. (4) Word Modified Ratio (WMR): the ratio of words being modified during typing or after committed; improvement is shown by reduction. (5) Word Per Minute (WPM): the number of committed words per minute.\nPrivacy guarantees. Same as [McMahan and Thakurta, 2022], the zero-one device neighboring relationship ([Kairouz et al., 2021b, definition 1.1]) is adopted for DP. For user\u2019s with a single device, device-level DP corresponds directly to user-level DP. Our privacy guarantee holds for all well-behaved clients during training, and we do not account for privacy cost of modest amount of hyperparameter tuning. DP is measured by the zero-Concentrated DP (zCDP) [Bun and Steinke, 2016] guarantee that has been used by US census bureau [US Census Bureau, 2021], and can be easily converted to (\u03f5, \u03b4)-DP. We use the privacy accounting in [Kairouz et al., 2021b, appendix D] implemented in Tensorflow Privacy [TFP Authors, 2022], and follow the guidelines outlined in [Ponomareva et al., 2023, Sec. 5.3] to report detailed narratives of privacy guarantees in App. A.\nImplementation. We open sourced implementation of DP-FTRL in Tensorflow Privacy [TFP Authors, 2022] integrated with Tensorflow Federated [TFF Authors, 2022b] as a DP aggregator for federated learning. Conceptually, DP-FTRL adds noise to the summation of updates across rounds, i.e., PrivateSum in Alg. 1. Instead of tracking the noise and summation separately, PrivateSum is implemented to only track the noise and updates \u03b8\u0303t\u22121 by adding the residual of noise between round t and round t\u2212 1. This design makes it easy to integrate with various optimizer choices, for example, the use of momentum, which is important for utility. It also allows ephemeral access of model deltas without directly storing unnoised states.\nExperimental setup. We apply the strategy in Sec. 2.5 to train Gboard LMs with DP. We present NWP results in Tab. 1, and OTF results in Tab. 2. As Smart Compose (SC) reuses NWP LMs, SC has the same DP guarantees as NWP models by the post-processing property [Dwork et al., 2014]. Following es-ES NWP model in [McMahan and Thakurta, 2022], we choose noise multiplier 7\nand report goal 6500 based on simulation in [Kairouz et al., 2021b] on public StackOverflow dataset [TFF Authors, 2022a]. We pretrain the models on public datasets and configure the timer period to control client participation, separately for different tasks. We use DP-FTRL with adaptive clipping and small report goal 500 to tune server learning rate and estimate the clip norm. Interestingly, we observe the learning rate and clip norm to be consistent for various Gboard LMs, and tuning seems to be unnecessary. DP-FTRL with fixed clip and large report goal is used to run the final model for deployment.\nResult analysis. All NWP and OTF models in Tabs. 1 and 2 are trained with stronger guarantees (smaller zCDP) compared to zCDP > 2.6 used by US Census Bureau [US Census Bureau, 2021]. For five NWP models in Europe (DE, GB, FR, IT, PT), the DP NN models significantly improve the utility compared to previous N-gram models. On en-US, pt-BR and en-IN, DP NN models also achieve comparable, or slightly better utility compared to their non-private versions as the strong models. SecAgg is successfully applied to en-US and es-ES, and can achieve good privacy-utility trade-off with a smaller number of rounds, likely due to the system configuration that results in more clients per round. However, SecAgg is also notably slower. There is a general positive correlation between the estimated population size and privacy guarantees.\nHowever, only a few tasks approach the possible maximum MinS for strong privacy guarantees, which highlights the challenge of both estimating population and controlling client participation. Longer training rounds are often used for NWP (compared to OTF) as the non-private NN baselines are strong, and to improve the downstream SC performance. As an example, we train es-ES NWP for 1900 rounds with a pretrained model, while the previous models [McMahan and Thakurta, 2022] is trained for 2000 rounds without pretraining. Our es-ES NWP model slightly improves the utility measured by PRate and Acc, and improves the zCDP bound from 0.81 to 0.35 due to the larger MinS by timer configuration. We highlight that our es-ES model at round 1240 already achieves similar NWP utility and a strong privacy guarantee, but the utility of SC keeps improving with training. Compared to the previous model in [McMahan and Thakurta, 2022], our model improves the SC trigger rate by 4.23% at round 1240, and 9.51% at round 1900."
        },
        {
            "heading": "4 Concluding remarks",
            "text": "We discuss our experience and summarize our strategy for training production Gboard LMs with FL and DP. We propose an algorithm applying adaptive clipping [Andrew et al., 2021] in DP-FTRL [Kairouz et al., 2021b] to reduce the hyperparamter tuning. We discuss the impact on privacy and utility of several important factors: the clip norm, report goal, client participation, and pre-training. Our study highlights the importance of system and algorithm co-design for differential privacy in practice, the challenges of tuning in FL systems, and opportunities to improve the scalability and stability of FL with DP and/or SecAgg. More than twenty LMs with formal DP guarantees are trained and launched to support Gboard NWP, SC, and OTF features, including en-US and es-ES NWP models additionally trained with SecAgg. Our experience demonstrates the possibility of training DP models for practical applications when a large scale system is available for large scale data. Therefore, Gboard is introducing and enforcing a new policy: DP has to be applied in all future training and launching of Gboard LMs."
        },
        {
            "heading": "Acknowledgement",
            "text": "The authors would like to thank Stanislav Chiknavaryan, Adria Gascon, Zachary Garrett, and Timon Van Overveldt for infrastructure configuration support; Swaroop Ramaswamy, Om Thakkar, Abhradeep Thakurta for early discussion on models and algorithms; Jeremy Gillula for internal review process; Xu Liu, Shumin Zhai, and Daniel Ramage for leadership support."
        },
        {
            "heading": "A Reporting privacy guarantees",
            "text": "This section clarifies the nuances of the reported DP guarantees following the guidelines outlined in [Ponomareva et al., 2023, Sec. 5.3]\n1. DP setting. This a central DP guarantee where the service provider is trusted to correctly implement the mechanism. 2. Instantiating the DP Definition\n(a) Data accesses covered : The DP guarantee applies to all well-behaved clients3 in a single training run. We do not account for hyperparameter tuning, or the selection of the final model checkpoint using evaluation metrics or A/B testing in our guarantees. Public multilingual C4 data [Raffel et al., 2019; Xue et al., 2020] is used for pre-training.\n(b) Final mechanism output : Only the final model checkpoint is released for production launches, however the mechanism\u2019s output is technically the full sequence of privatized gradients, and so the guarantee also applies at this level, and hence all intermediate models are protected (including those sent to devices participating in federated learning).\n(c) Unit of privacy. Device-level DP is considered, i.e., the notion of adjacency is with respect to arbitrary training datasets on each client device, and the device might have an arbitrarily large local dataset containing arbitrary training examples. For user\u2019s with a single device, this corresponds directly to user-level DP; for devices shared with multiple users, this provides a stronger notion of DP than user-level; for a user with multiple devices that happen to both participate in training the model, the notion is weaker, but group privacy can be used to obtain a user-level guarantee.\n(d) Adjacency definition for \u201cneigbouring\u201d datasets: We use the zero-out definition [Kairouz et al., 2021b]. This is a a special form of the add-or-remove definition, where neighboring data sets differ by addition/removal of a single client. In the absence of a client at any training step, we assume that the client\u2019s model update gets replaced with the all zeros vector. This assumption enforces a subtle modification to the traditional definition of the add/remove notion of DP which allows neighboring data sets to have the same number of records.\n3. Privacy accounting details\n(a) Type of accounting used : Both \u03c1\u2212zCDP [Bun and Steinke, 2016] accounting, and PLD accounting [DP Team, 2022] for (\u03f5, \u03b4)\u2212DP are used.\n(b) Accounting assumptions : Each client only participates limited times during the training, and there are at least a min-separation number of rounds between two consecutive participation of a client, i.e., MaxP and MinS as discussed in Sec. 2.2. Client participation is enforced by a timer on clients in the cross-device FL system.\n(c) The formal DP statement : The launched Gboard LMs have \u03c1\u2212zCDP range in (0.2, 2). We also transform zCDP to (\u03f5, \u03b4)\u2212DP by PLD accounting [DP Team, 2022]: given \u03b4 = 10\u221210, the smallest zCDP \u03c1 = 0.25 corresponds to DP \u03f5 = 4.49; the largest zCDP \u03c1 = 1.86 corresponds to DP \u03f5 = 13.69.\n(d) Transparency and verifiability : We open sourced our core implementation code in TensorFlow Federated and Tensorflow Privacy. Key portions of the cross-device FL system are also open sourced.\n3Clients that faithfully follow the algorithm including participation limits. Due to the design of the algorithm, a mis-behaved client does not adversely affect the DP guarantee of any well-behaved clients."
        }
    ],
    "title": "Federated Learning of Gboard Language Models with Differential Privacy",
    "year": 2023
}