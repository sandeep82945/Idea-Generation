{
    "abstractText": "Detecting structures at the particle scale within plastically deformed crystalline materials allows a better understanding of the occurring phenomena. While previous approaches mostly relied on applying hand-chosen criteria on different local parameters, these approaches could only detect already known structures. We introduce an unsupervised learning algorithm to automatically detect structures within a crystal under plastic deformation. This approach is based on a study developed for structural detection on colloidal materials. This algorithm has the advantage of being computationally fast and easy to implement. We show that by using local parameters based on bond-angle distributions, we are able to detect more structures and with a higher degree of precision than traditional hand-made criteria.",
    "authors": [
        {
            "affiliations": [],
            "name": "Armand Barbot"
        },
        {
            "affiliations": [],
            "name": "Riccardo Gatti"
        }
    ],
    "id": "SP:234ab8016af2b4f5f3bca19956537399b9413f2c",
    "references": [
        {
            "authors": [
                "G.J. Ackland",
                "A.P. Jones"
            ],
            "title": "Applications of local crystal structure measures in experiment and simulation",
            "venue": "Physical Review B",
            "year": 2006
        },
        {
            "authors": [
                "J.B. Adams",
                "S.M. Foiles",
                "W.G. Wolfer"
            ],
            "title": "Self-diffusion and impurity diffusion of fcc metals using the five-frequency model and the Embedded Atom Method",
            "venue": "Journal of Materials Research",
            "year": 1989
        },
        {
            "authors": [
                "J. Amodeo",
                "C. Begau",
                "E. Bitzek"
            ],
            "title": "Atomistic Simulations of Compression Tests on Ni 3 Al Nanocubes",
            "venue": "Materials Research Letters",
            "year": 2014
        },
        {
            "authors": [
                "J. Amodeo",
                "K. Lizoul"
            ],
            "title": "Mechanical properties and dislocation nucleation in nanocrystals with blunt edges",
            "venue": "Materials & Design 135,",
            "year": 2017
        },
        {
            "authors": [
                "A. Barbot"
            ],
            "title": "Characterization of shear bands and plasticity in model glasses at the atomic scale",
            "venue": "These de doctorat. Sorbonne universite\u0301",
            "year": 2020
        },
        {
            "authors": [
                "A. Barbot",
                "M. Lerbinger",
                "A. Lem\u00e2\u0131tre",
                "D. Vandembroucq",
                "S. Patinet"
            ],
            "title": "Rejuvenation and shear banding in model amorphous solids",
            "venue": "Physical Review E",
            "year": 2020
        },
        {
            "authors": [
                "E. Boattini",
                "M. Dijkstra",
                "L. Filion"
            ],
            "title": "Unsupervised learning for local structure detection in colloidal systems",
            "venue": "The Journal of Chemical Physics",
            "year": 2019
        },
        {
            "authors": [
                "E. Boattini",
                "M. Ram",
                "F. Smallenburg",
                "L. Filion"
            ],
            "title": "Neuralnetwork-based order parameters for classification of binary hard-sphere crystal structures",
            "venue": "Molecular Physics 116,",
            "year": 2018
        },
        {
            "authors": [
                "W. DEXA.1999.795279. Chen",
                "A.R. Tan",
                "A.L. Ferguson"
            ],
            "title": "Collective variable discovery",
            "year": 2018
        },
        {
            "authors": [
                "P. Geiger",
                "C. Dellago"
            ],
            "title": "Data Mining, AAAI Press, Portland, Oregon",
            "year": 2013
        },
        {
            "authors": [
                "M. Physics. Gevrey",
                "I. Dimopoulos",
                "S. Lek"
            ],
            "title": "Review and comparison",
            "year": 2003
        },
        {
            "authors": [
                "J.D. Honeycutt",
                "H.C. Andersen"
            ],
            "title": "Molecular dynamics study of melting and freezing of small Lennard-Jones clusters",
            "venue": "The Journal of Physical Chemistry",
            "year": 1987
        },
        {
            "authors": [
                "W.G. Hoover"
            ],
            "title": "Canonical dynamics: Equilibrium phase-space distributions. Physical Review A 31, 1695\u20131697",
            "venue": "URL: https://link.aps.org/ doi/10.1103/PhysRevA.31.1695,",
            "year": 1985
        },
        {
            "authors": [
                "D. Hull",
                "D.J. Bacon"
            ],
            "title": "Chapter 5 - Dislocations in Face-centered Cubic Metals",
            "venue": "Introduction to Dislocations (Fifth Edition). Butterworth-Heinemann,",
            "year": 2011
        },
        {
            "authors": [
                "W. Lechner",
                "C. Dellago"
            ],
            "title": "Accurate determination of crystal structures based on averaged local bond order parameters",
            "venue": "The Journal of Chemical Physics",
            "year": 2008
        },
        {
            "authors": [
                "S. Lloyd"
            ],
            "title": "Least squares quantization in PCM",
            "venue": "IEEE Transactions on Information Theory 28,",
            "year": 1982
        },
        {
            "authors": [
                "J. MacQueen"
            ],
            "title": "Some methods for classification and analysis of multivariate observations",
            "venue": "Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Statistics",
            "year": 1967
        },
        {
            "authors": [
                "S. Menon",
                "G.D. Leines",
                "J. Rogal"
            ],
            "title": "pyscal: A python module for structural analysis of atomic environments",
            "venue": "Journal of Open Source Software",
            "year": 2019
        },
        {
            "authors": [
                "Z. Noori",
                "M. Panjepour",
                "M. Ahmadian"
            ],
            "title": "Study of the effect of grain size on melting temperature of Al nanocrystals by molecular dynamics simulation",
            "venue": "Journal of Materials Research",
            "year": 2015
        },
        {
            "authors": [
                "S. Nose"
            ],
            "title": "A unified formulation of the constant temperature molecular dynamics methods",
            "venue": "The Journal of Chemical Physics",
            "year": 1984
        },
        {
            "authors": [
                "J.D. Olden",
                "M.K. Joy",
                "R.G. Death"
            ],
            "title": "An accurate comparison of methods for quantifying variable importance in artificial neural networks using simulated data",
            "venue": "Ecological Modelling",
            "year": 2004
        },
        {
            "authors": [
                "F. Pedregosa",
                "G. Varoquaux",
                "A. Gramfort",
                "V. Michel",
                "B. Thirion",
                "O. Grisel",
                "M. Blondel",
                "P. Prettenhofer",
                "R. Weiss",
                "V. Dubourg",
                "J. Vanderplas",
                "A. Passos",
                "D. Cournapeau",
                "M. Brucher",
                "M. Perrot",
                "E. Duchesnay"
            ],
            "title": "Scikit-learn: Machine Learning in Python",
            "venue": "Journal of Machine Learning Research",
            "year": 2011
        },
        {
            "authors": [
                "D.E. Rumelhart",
                "G.E. Hinton",
                "R.J. Williams"
            ],
            "title": "Learning internal representations by error propagation, in: Parallel distributed processing: explorations in the microstructure of cognition, vol. 1: foundations",
            "year": 1986
        },
        {
            "authors": [
                "S. Salvador",
                "P. Chan"
            ],
            "title": "Determining the number of clusters/segments in hierarchical clustering/segmentation algorithms",
            "venue": "IEEE International Conference on Tools with Artificial Intelligence, IEEE Comput. Soc, Boca Raton, FL,",
            "year": 2004
        },
        {
            "authors": [
                "E. Sanz",
                "C. Valeriani",
                "T. Vissers",
                "A. Fortini",
                "M.E. Leunissen",
                "Blaaderen",
                "A.v",
                "D. Frenkel",
                "M. Dijkstra"
            ],
            "title": "Out-of-equilibrium processes in suspensions of oppositely charged colloids: liquid-to-crystal nucleation and gel formation",
            "venue": "Journal of Physics: Condensed Matter",
            "year": 2008
        },
        {
            "authors": [
                "M. Scardi",
                "L.W. Harding"
            ],
            "title": "Developing an empirical model of phy",
            "year": 1999
        },
        {
            "authors": [
                "A. Stukowski"
            ],
            "title": "Structure identification methods for atomistic",
            "year": 2012
        },
        {
            "authors": [
                "V.V. Bulatov",
                "A. Arsenlis"
            ],
            "title": "Automated identification",
            "year": 2012
        },
        {
            "authors": [
                "J. Yao",
                "N. Teng",
                "H.L. Poh",
                "C.L. Tan"
            ],
            "title": "Forecasting and analysis",
            "year": 1998
        }
    ],
    "sections": [
        {
            "text": "Detecting structures at the particle scale within plastically deformed crystalline materials allows a better understanding of the occurring phenomena. While previous approaches mostly relied on applying hand-chosen criteria on different local parameters, these approaches could only detect already known structures.\nWe introduce an unsupervised learning algorithm to automatically detect structures within a crystal under plastic deformation. This approach is based on a study developed for structural detection on colloidal materials. This algorithm has the advantage of being computationally fast and easy to implement. We show that by using local parameters based on bond-angle distributions, we are able to detect more structures and with a higher degree of precision than traditional hand-made criteria.\nKeywords: Structure Detection, Unsupervised learning, Molecular Dynamics, Crystal, Plastic deformation PACS: 0000, 1111 2000 MSC: 0000, 1111"
        },
        {
            "heading": "1. Introduction",
            "text": "Molecular dynamics simulation is a powerful method allowing to simulate at the particle scale different materials such as colloidal systems (Boattini et al., 2019), glassy materials (Barbot et al., 2020; Barbot, 2020) or metallic nanocrystals (Amodeo and Lizoul, 2017). To help in interpreting the simulations results, being able to determine the local structure at the particle-scale is essential. To do so, several approaches were developed, mainly relying\nPreprint submitted to January 2, 2023\nar X\niv :2\n21 2.\n14 81\n3v 1\n[ co\nnd -m\nat .m\ntr l-\nsc i]\non local order parameters to describe the surrounding environment of each particle (number of neighbours, angles formed with the neighbours, ...) to detect underlying substructures in the simulated atomistic sample. Among these methods, we can cite the Bond Orientational Order (BOO) parameter (Steinhardt et al., 1983), the Common Neighbours Analysis (CNA) (Honeycutt and Andersen, 1987), or the Bond-Angle Distribution (BAD) (Ackland and Jones, 2006). Such methods were applied with success to study several phenomena such as crystal nucleation (Sanz et al., 2008), melting (Noori et al., 2015) or plasticity (Ackland and Jones, 2006; Stukowski, 2012; Amodeo et al., 2014). However, they are mostly relying on hand chosen criteria and thus only works for already known structures.\nRecently, different approaches using machine learning to detect substructures from local order parameters were developed. The first models, which relied on supervised learning (Geiger and Dellago, 2013; Dietz et al., 2017; Boattini et al., 2018), suffered from the same problem as the hand-chosen techniques: they could only be trained to find an expected structure in the studied systems. However, a recent work from Boattini et al (Boattini et al., 2019) successfully applied an unsupervised learning method using the BOO parameter to automatically detect different structures within a colloidal material without relying on a priori knowledge of the underlying structures. This method also had the advantage of being easy to implement and computationally fast.\nWhile the BOO parameter is able to discriminate between the different structures in crystalline materials, it does not perform well when the simulated system is under deformation (Lechner and Dellago, 2008). Being able to automatically detect substructures within crystal under plastic deformation, including some previously undetected and unexpected case, would open the way to a better understanding of crystal plasticity at the atomic scale. This requires to find a local order parameter suitable to discriminate substructures at the atomistic level for elastically and plastically deformed systems.\nIn this study, we present a method inspired by the paper from Boattini et al (Boattini et al., 2019) to automatically study and detect the different substructures within a crystal under plastic deformation appearing at the atomistic scale. This approach relies on the BAD parameters designed for single crystals to describe the environment around each atom, as highlighted in (Ackland and Jones, 2006). This local parameter was already used in previous structure detection approaches relying on bond-angle distributions, showing the ability to discriminate structures associated with crystal\nplasticity (Ackland and Jones, 2006; Amodeo et al., 2014). Our method consists on extracting the most pertinent BAD parameters using an autoencoder neural networks (Rumelhart et al., 1986; Dietz et al., 2017) before applying two clustering models: K-means (Lloyd, 1982; MacQueen, 1967) and DBSCAN (Ester et al., 1996). It has the advantage of being computationally fast and easy to implement. We finally test our approach on a FCC single crystal of Nickel under plastic deformation."
        },
        {
            "heading": "2. Methods",
            "text": "This section is divided in three main parts. We first describe the system on which we apply our structure detection method. We then depict the BAD parameters we use to capture the local environment of each particle. The last section focuses on the algorithms we use to detect the local atomistic substructures in the system from the BAD parameters."
        },
        {
            "heading": "2.1. System",
            "text": "In this study, we perform Molecular Dynamics (MD) simulations using LAMMPS (Thompson et al., 2022) to plastically deform a Ni FCC defect free single crystal. These simulations are then used as benchmark to apply our method, based on unsupervised learning, developed to detect the different structures emerging in the system during plastic deformation.\nWe first create a cubic sample with edge lengths equal to 20nm and free surfaces oriented along the \u3008100\u3009 directions as shown on Fig 1 (a). The interatomic interaction linking the atoms is the EAM potential for Ni (Adams et al., 1989). The system is then equilibrated for 5 ps at 5K using NoseHoover thermostat (Nose, 1984; Hoover, 1985). A deformation is imposed using uniaxial compression with a flat indenter along the [100] direction, with a strain rate of 108s\u22121. The flat indenter is modeled as an infinite plane exerting a repulsive force on atoms defined by\nF (r) = \u2212K(r \u2212R)2, (1)\nwith r being the atom position, R the plane position and K the force constant. Here we choose K=1000 eV/A\u030a3. The temperature is maintained at 5K during the compression through the Nose-Hoover thermostat.\nTo determine when the first plastic event occurs during the deformation, we first measure on the fly the compressive stress applied on the system by\nthe indenter using a method similar to the one used in (Amodeo et al., 2014) and looking for the first stress drop corresponding to the first plastic event. The stress-strain curve obtained from the compression is shown on Fig 1 (b), with the first plastic event represented by a vertical red line.\nIn our simulations, we observe the nucleation of Shockley head partial dislocations with burger vectors: {1\n6 [\u22121 1 2], 1 6 [1 \u22121 2], 1 6 [\u22121 \u22121 2], 1 6 [\u22121 \u2212\n1 \u22122]}. The dislocation lines are thus accompanied by a stacking fault in the HCP structure (Hull and Bacon, 2011)."
        },
        {
            "heading": "2.2. Bond Angle Distribution Parameter",
            "text": "To describe the local structural environment around each particle, we measure the Bond-Angle Distribution (BAD) parameters for each of them, based on the method described in (Ackland and Jones, 2006). For each atom i, we first extract its nearest neighbors Nb(i) using the adaptive cutoff method as described in (Stukowski, 2012). This method has the advantage of being parameter-free, so can be directly applied to any structure, while being just slightly slower than using a fixed cutoff (Stukowski, 2012).\nWe define the bond-angle \u03b8jik with j, k \u2208 {1..Nb(i)} being two nearest neighbors of i. We then estimate {cos(\u03b8jik)} for all the Nb(i)(Nb(i) \u2212 1)/2 neighbor pairs of atom i. From it, we describe nine ranges of {cos(\u03b8jik)} over which the BAD parameters {\u03c7l}l\u2208{0..8} are estimated as shown in Table 1. These ranges are based on those from (Ackland and Jones, 2006) which\nwere optimised to differentiate crystal structures. The idea here is to count the number of bond-angle cosines in each ranges to obtain the value of the corresponding {\u03c7l}l\u2208{0..8}.\nFor instance, let us consider a case where a atom i has six bond-angle cosines. If two of them are in the range [\u22121.0,\u22120.945] and four of them are in the range [\u22120.945,\u22120.915], then for this atom, we will have \u03c70 = 2, \u03c71 = 4 and {\u03c7l}l\u2208{2..8} = 0.\nNote that in this paper, the BAD parameters were calculated using the package Pyscal (Menon et al., 2019)."
        },
        {
            "heading": "2.3. Unsupervised learning",
            "text": "From the knowledge of the BAD parameters for each atom, the aim of our method is to determine automatically the different structures present in the system. This part is done through unsupervised learning and can be decomposed on two main tasks. First we use an autoencoder to determine the good number of parameters to describe the local structures in our system. Then, we apply a perturbative method to the autoencoder to determine which parameters are the most effective. From this, we focus on the most effective parameters applying clustering and classification methods to extract the different substructures of our system.\nNote that as the surface atoms can be easily filtered from their number of nearest neighbours as in (Amodeo et al., 2014). , i.e. atoms for which Nb < 11, we filter out the surface atoms using this criteria and focus on the structures within the inner atoms."
        },
        {
            "heading": "2.3.1. Autoencoder",
            "text": "As explained above, the aim of this section is to extract the number of pertinent parameters to determine the different structures of our system. Indeed, out of the nine BAD parameters, some could be irrelevant or redundant to describe the local structural environment around each atom. This task is here performed using an autoencoder based neural network to perform dimensionality reduction (Goodfellow et al., 2016; Chen et al., 2018; Boattini et al., 2019).\nThe autoencoder network can be divided in two parts: the encoder part and the decoder part as shown on Fig 2. The encoder aims at encoding the input into a lower dimension: the bottleneck. Then the decoder will try to reconstruct the input from the bottleneck. The output of the network will then be the reconstruction of the input done by the decoder from the bottleneck.\nIn practice, the input of the encoder is \u03c7(i) = {\u03c7l}l\u2208{0..8}(i) \u2208 N9, with i \u2208 [1..N ], N being the number of atoms over which the training in performed.\nWe train the encoder to reduce the difference between the input and the output for a bottleneck dimensions going from 1 to 9.\nTo determine the pertinent dimension of input, we measure for each bottleneck Nbneck dimension the mean square error between the input \u03c7 and the output \u03c7\u0302 as\nENbneck = 1\nN N\u2211 i=1 \u2016\u03c7(i)\u2212 \u03c7\u0302(i)\u2016. (2)\nWe then look from which dimension we have the minimum difference between the input and the output.\nFor more details about the implementation of the autoencoder, please refer to the paper (Boattini et al., 2019) which served as a reference for this section."
        },
        {
            "heading": "2.3.2. Relevant parameters identification",
            "text": "After determining the relevant number of parameters to study the substructures in our system, we want to go beyond the black box of the neural network and determine which parameters are most pertinent for our study. To do so, we use the input perturbation method (Yao et al., 1998; Scardi and Harding, 1999; Gevrey et al., 2003; Olden et al., 2004).\nThe principle of this method is to take the trained autoencoder neural network with the optimal bottleneck dimension, and to perturbate one of the input parameters by increasing its value with a fixed amount. In this work, we increase the chosen input value by 10% which was shown to give satisfactory results (Boattini et al., 2019).\nAfter perturbing one of the inputs, we look at how much this perturbation influenced the output by calculating:\nEpertk = 1\nN N\u2211 i=1 \u2016\u03c7(i)\u2212 \u03c7\u0302pertk (i)\u2016, (3)\nwith k \u2208 [0..8] being the index of the perturbed input, \u03c7\u0302pertk the output after the perturbation and \u03c7 the input without perturbation.\nIf the perturbed input k was considered during the training as carrying no pertinent information to reconstruct the input at the output, the perturbation will not propagate to the output. This will lead to a very small value of Epertk . On the contrary, if the perturbed input k is necessary to obtain an\noutput similar to the input, its perturbation will lead to a higher value of Epertk .\nFrom this, we determine the relative importance RIk of the input k as:\nRIk = \u2206Ek\u22118 j=0 \u2206Ej , (4)\nwith \u2206Ek = E pert k \u2212 E being the difference between the autoencoder mean square error after perturbation of the input k and without perturbation. From this relative importance index, we can determine the importance of each input during the autoencoder training."
        },
        {
            "heading": "2.3.3. Clustering and classification",
            "text": "In the last section, we described how to isolate the most relevant BAD parameters to study the local structure. In our system most of the atoms will be in two structures, FCC and HCP (in the stacking fault), which corresponds to close values of BAD parameters (Ackland and Jones, 2006). While most structure detection approaches focus on the already known main crystalline structures, i.e. FCC, BCC and HCP (Stukowski, 2012), the goal of our method is also to be able to automatically detect substructures without having an a priori knowledge about them.\nTo this end, we chose to apply two clustering methods on the relevant BAD parameters space, one to detect the main structures containing most of the atoms, and the second one to focus on the substructures containing few atoms but located far from each other in the parameter space. The clusters extracted by these two clustering approaches will then be combined to identify the different substructures of the studied system.\nIn the next sections, first we introduce the K-means clustering method (MacQueen, 1967; Lloyd, 1982) we use to detect the main structures. Then, we detail the Density-Based Spatial Clustering of Applications with Noise (DBSCAN) (Ester et al., 1996) which allows to detect efficiently more isolated clusters.\nWhile K-means is able to efficiently determine the cluster of a new data point without having to re-apply the clustering over the whole data set, it is not the case for DBSCAN. We thus need to apply a classification algorithm over the clusters detected by DBSCAN. We thus present in the last part the logistic regression classifier we use to perform this task."
        },
        {
            "heading": "2.3.3.1 Kmeans",
            "text": "In order to detect the main substructures present in the system, we use the Kmeans clustering algorithm (MacQueen, 1967; Lloyd, 1982) as implemented in sci-kit learn (Pedregosa et al., 2011). The principle of the method is the following: let us consider a set of N data points in the parameter space that we want to separate into K clusters. We name {xi}i\u2208[1..N ] the position of the data point in the parameter space and {\u00b5j}j\u2208[1..K] the center of each cluster, commonly called \u201dcentroid\u201d.\nThe aim of the K-means algorithm is to choose optimal centroids in order to minimize the inertia defined by\nIN(K) = N\u2211 i=1 minj\u2208[1..K](\u2016xi \u2212 \u00b5j\u20162). (5)\nIf the points are located in clusters, the inertia will be minimized by placing the centroids at the center of each cluster.\nWhile the K-means requires the number of clusters as an input, the optimal number of cluster can be obtained with the following procedure: we first apply K-means to our data set for varying number of clusters K and calculate the corresponding inertia IN(K). In this paper, we did it for K \u2208 [1..10]. Then we apply the elbow method to the function IN(K) following the protocol described in (Salvador and Chan, 2004). From this method, we can obtain the value of K from which the inertia stop decreasing with K, thus corresponding to the optimal number of clusters for K-means clustering.\nAfter training the K-means algorithm over the data set for the optimal cluster number K, we obtain the centroids locations {\u00b5j}j\u2208[1..K] of the different clusters. From this information, we can determine in which cluster is a new data point by looking at its closest centroid. This allows to classify new data depending on their cluster without having to retrain the algorithm each time.\nThe K-means method being only looking at the distance between the data points and the centroid, it does not perform well for elongated clusters orfor clusters with irregular shapes. However, it performs very well if most of the data points are located in the same position in space. As the main crystalline structures are each associated with a specific position in the BAD parameter space, the K-means algorithm is well suited to detect the clusters associated with these structures."
        },
        {
            "heading": "2.3.3.2 DB Scan",
            "text": "To be able to detect clusters with smaller number of points and non-isotripic shape, we apply another clustering method: the Density-Based Spatial Clustering of Applications with Noise (DBSCAN) (Ester et al., 1996) as implemented in sci-kit learn (Pedregosa et al., 2011).\nThe principle of the algorithm is the following: we input two parameters: (1) the maximum distance between two points lmax and (2) the minimum number of points per cluster Nmin. In this method, two data points located at a distance l < lmax are considered as neighbors. From it, the algorithm will class the data points into three categories:\n(i) if a data point has at leastNmin\u22121 neighbors, it will then be considered as a \u201dcore point\u201d.\n(ii) if a data point has less than Nmin \u2212 1 neighbors but at least one of them is a core point, it will be considered as a \u201dborder point\u201d.\n(iii) the remaining data points will be considered as \u201dnoise\u201d. The core points and the border points are then regrouped together to form clusters, located at a distance lcluster > lmax from each other. In our system, many atoms can be associated with the same values of BAD parameters. As the DBSCAN only look at the distance between points, having many points located at the same spatial position will only slow down the clustering without improving the result. We thus apply the DBSCAN over the list of the unique spatial positions of our data set. Finally, the BAD parameters being integers, we use for the distance parameter the value lmax = 1. We also determined that Nmin = 5 gives satisfactory results for our data set.\nAs this method is not influenced by the number of point at a given spatial position but only by the distance between the unique spatial positions in the data set, it would be able detect clusters associated with structure containing fewer atoms but isolated from the clusters corresponding to the main crystalline structures."
        },
        {
            "heading": "2.3.3.3 Logistic regression classifier",
            "text": "In the opposite of K-means, if we want to know from DBSCAN in which cluster is a new data point, we would need to re-perform the clustering over the whole data set, plus the new data point. But this solution would be very inefficient and time consuming. One solution to solve this problem is to train a classification algorithm over the clusters detected by DBSCAN.\nThe classification algorithm takes as an input the training data set and as the output the cluster found by DBSCAN. It will then decompose the parameter space into region; each associated with a cluster. When a new data point is added, it will be attributed a cluster depending on its location in the parameter space.\nIn this study, we use the logistic regression classifier (Brzezinski and Knafl, 1999) implemented in scikit-learn as it simple, fast and gives satisfactory result when applied to the clusters which detected by DBSCAN in this system. More details about this classifier can be found in sci-kit learn (Pedregosa et al., 2011)."
        },
        {
            "heading": "2.3.3.4 Cluster combination",
            "text": "In the previous parts, we described two clustering methods. One to detect the main structures in the system: the K-means algorithm, the other one to detect the substructures containing few points, more isolated in the parameter space: the DBSCAN. Then, both K-means and the association of DBSCAN with the logistic regression classifier will separate the parameter space into regions corresponding to detected clusters. Thus, each data point will be associated with two clusters: the one detected by K-means, the other detected by DBSCAN.\nIn order to have a precise detection of the structures present in the system, we combine the results obtained from the two clustering methods. Finally, we obtain NKmeans \u00b7 NDBSCAN clusters, with NKmeans and NDBSCAN corresponding to the number of clusters found by K-means and DBSCAN, respectively."
        },
        {
            "heading": "3. Results and discussions",
            "text": "In this section, we present the results obtained by following the procedure described in the previous sections. First we analyse the data obtained we get from the autoencoder: the relevant number of the BAD order parameter necessary to detect the substructures in our system as well as which of the \u03c7(i) = {\u03c7l}l\u2208{0..8}(i) \u2208 N9 parameters are the most significant. We then apply the clustering methods on the extracted parameter, later analysing their performances. Finally, we examine the different substructures detected by this method, comparing the outcomes with other structure detection methods."
        },
        {
            "heading": "3.1. Autoencoder output analysis",
            "text": "After compressing our Ni crystal until the first plastic event occurs, as described in Section 2.1, we record at a regular time step interval the atomic positions of the simulated nano-object. We then measure BAD parameters for each atoms as detailed in Section 2.2, filtering out the surface atoms as explained in Section 2.3. It turned out that computing the BAD parameters of a single time step is sufficient to train the autoencoder and to observe the appearance of different clusters. Here, the time step used to train our algorithm corresponds to an applied strain of \u2206 = 0.05% after the first plastic event.\nFig 3 shows the mean square error between the autoencoder input and output for different bottleneck dimensions. We can observe that the error reaches a plateau for a bottleneck dimension of N = 3. We thus consider here that the pertinent number of BAD parameters to study the structure in our system is therefore three.\nOur next step is to determine which are the three most pertinent BAD parameters out of nine we computed for each atom. To this end, we calculate the relative importance (RI) of each parameter applying the perturbation method described in Section 2.3.2. The results are shown in Fig 4. From this figure, we can clearly observe the most significant three parameters:\nthe triplet {\u03c74, \u03c75, \u03c77}. From here on in this study, we will only focus on these three BAD parameters to study the substructures within the studied Ni nano-object."
        },
        {
            "heading": "3.2. Clustering methods combination",
            "text": "We determined that the most pertinent BAD parameters to study the emerging substructures in our numerical sample are {\u03c74, \u03c75, \u03c77}. Here, we apply two clustering algorithms on these parameters, K-means and DBSCAN, and then we combine the outcome of the obtained clusters. The dataset used for the clustering processes is the same that we used for extract BAD parameters using the autoencoder (we also verified that using different atomistic configurations extracted after the onset of plasticity gives similar results) .\nOn Fig 5 (a), we show a scatter plot representing for each atom their position in the {\u03c74, \u03c75, \u03c77} space, as well as the result of the K-means clustering. Note that as the BAD parameters are integers, one position in the {\u03c74, \u03c75, \u03c77} space can correspond to many atoms. Each point represented here corresponds to at least one atom. On this figure, we can see that the algorithm detected two clusters: one in dark blue and the other in yellow.\nAs detailed in Section 2.3.3.1, the aim of using the K-means clustering is to detect the main crystalline structures, located close to each other in\nthe BAD parameter space (Ackland and Jones, 2006). While the Fig 6 (a) clearly shows three blocks, the two detected clusters seems to correspond to half of the central block. To understand what was detected by the K-means algorithm, we look on Fig 5 the distribution of \u03c74 (b), \u03c75 (c) and \u03c77 (d). From this figure, we can see that the large majority of the atoms are concentrated on two close positions in the {\u03c74, \u03c75, \u03c77} space: {21, 13, 24} and {24, 12, 24}. On (Ackland and Jones, 2006), these two configurations were defined as corresponding to HCP and FCC, respectively. As HCP corresponds to the stacking fault structure in a FCC crystal (Hull and Bacon, 2011), we can expect that most of the atoms will be in these two structures. From what we can see, K-means seems to have only succeeded in separating these two configurations located within the central block. This will be confirmed in the next part focusing on analysing the structures detected by the clustering algorithms.\nWhile the K-means algorithm successfully separated the two locations in\nthe {\u03c74, \u03c75, \u03c77} space, we can see on Fig 5 (a) that it totally fails in identifying two clusters, corresponding to different substructures, located further away from the central block. To detect them, we use another clustering method: the DBSCAN combined with the logistic regression classifier detailed in 2.3.3.2 and 2.3.3.3, respectively. We show its results on Fig 6 (b). From this figure, we can see that the DBSCAN successfully managed to separate into three clusters the three blocks observed in the scatter plot. After measuring the proportion of data points contained in each cluster, it appears that the central cluster (in purple) contains 99.8% of the points, while the two remaining clusters contains 0.1% each. This confirms that most of the atoms are located in the main structures, corresponding to the central block, making it difficult for the K-means clustering to detect the two smaller blocks.\nFinally, to detect the main crystalline structures while being able to discern other structures containing fewer atoms in the system, we combine the results obtained by the two clustering methods (K-means shown on Fig 6 (a), DBSCAN shown on Fig 6 (b)) as highlighted on Fig 6 (c). From this figure, we can see that we managed to detect a total of six different clusters: the dark blue and yellow ones from the central structures from the central block, containing the 99.8% of the atoms, corresponding to the main crystalline structures. The red and green cluster, corresponding to the two other aggregation of points clearly visible in the scatter plot of Fig 6 (c). Finally, the sky blue and the orange clusters, which only corresponds to few atoms, and detected only in few time steps."
        },
        {
            "heading": "3.3. Analysis of detected structures",
            "text": "On the previous section, by combining two clustering methods, we managed to extract six different clusters from our dataset by a projection in the {\u03c74, \u03c75, \u03c77} space. We now focus on analyzing the structures associated with each cluster. To this end, we show on Fig 7 (a) a snapshot made with OVITO (Stukowski, 2009) of the Ni nano-cube deformed plastically, with an applied strain of \u2206 = 0.02% beyond the yield point. On this snapshot, the atoms are colored corresponding to their clusters. For instance, the atoms contained in the yellow cluster on Fig 6 (c) will be colored in yellow on Fig 7. On Fig 7 (a), we can see that most of the atoms are contained in the yellow cluster which we can then associate with the FCC structure as our system is a Ni FCC nano-crystal.\nAfter filtering out the atoms in the FCC structure, we can now see on Fig 7 (b) atoms in three different structures. Fig 7 (c) corresponds to a zoom within Fig 7 (b) to highlight more clearly the detected substructures. From this figure, the atoms colored in green and red follows what looks like a dislocation line, while the atoms in dark blue seems to correspond to the stacking fault created after the glide of a partial dislocation. By using the algorithm DXA (Stukowski et al., 2012) implemented in OVITO (Stukowski, 2009), we could confirm our prediction: the green and the red atoms are\nsurrounding a partial dislocation line. The relative position of the green and red atoms depends on the burger vector as well as on the direction of the dislocation line (see the appendix Appendix A for more details). From this, we can deduce that the dark blue atoms correspond to HCP stacking fault structures generated by the gliding of a partial dislocations (as mentioned above in FCC crystals stacking faults have a HCP structure).\nThe two remaining clusters detected in in Fig 6 c), the orange and the sky blue one, are very rarely observed during the time steps recorded during the deformation. The orange structure is observed when two dislocation lines are located close to each other while the sky blue one is detected during the interaction between a dislocation line and a stacking fault, as shown later.\nOn Fig 8, we compare our structure detection method, shown in panel (a), with three other existing methods: (i) hand-chosen criterion using the BAD parameters from (Ackland and Jones, 2006) (Fig 8 (b)), (ii) the CNA method (Honeycutt and Andersen, 1987; Stukowski, 2012) implemented in OVITO ,which is one of the most commonly used method for structure detection (Fig 8 (c)) and which consists of using hand-chosen criterion on a radial-distribution based local order parameter, and (iii) the autoencoder approach using the BOO parameters from (Boattini et al., 2019) which served as an inspiration for this paper (Fig 8 (d)). For this comparison, we filtered beforehand the surface atoms following the method explained in section 2.3.\nOn Fig 8, we only focus on one dislocation line and the associated stacking fault. From this figure, we can see that our method is able to capture\nwith much more details the different structures present within a FCC crystal plastically deformed. To further emphasize the efficiency of our method, on Fig 9 we show a snapshot of the system with two dislocation lines about to interact and on Fig 10 we show the interaction between a dislocation line and a stacking fault.\nOut of the three methods used for comparison, two were mostly designed to detect the main crystalline structures (FCC, HCP or BCC): the handchosen criterion using the BAD parameters and CNA. We can still remark that CNA, which also relies on hand-chosen criterion, is able to capture the location of the dislocation line but with less accuracy than our method: the number of atoms close to the line defect is much larger than in our method. Also, this method is not able to capture sub-structures associated with plasticity. The atoms outside of the main crystalline structures, such as those in the line defect, are labelled as \u201dother\u201d (in red in Fig 8 (c)). The hand chosen method using the BAD also categorizes from time to time atoms in the category \u201dother\u201d, like in Fig 9 (b), but it is not able to capture the line defect.\nThe last method, the autoencoder applied to the BOO parameters, is also designed to detect automatically the different structures present within the system. However, when applying the method to our system, only two structures were detected: one associated with FCC and the other associated with HCP (the stacking fault). Also, only two structures were visible on the lower dimensional subspace over which the clustering was applied when following the method (see appendix Appendix B). We thus conclude that\nthe BOO parameters are not the most pertinent parameters to detect the structures within a plastically deformed crystal.\nOverall, these comparisons shows that our method is able to detect more structures than the other presented approaches. It is also able to capture the\ncontours of the dislocation lines with a better precision than CNA as we can see on Fig 10."
        },
        {
            "heading": "4. Conclusion",
            "text": "In conclusion, we improved a method initially developed to detect automatically structures in colloidal materials. Through this method, we were able to finely detect different structure present in a Ni FCC crystal under plastic deformation. This algorithm uses the BAD parameters to describe the local environment of the atoms. Thanks to an autoencoder associated with a perturbation method, the most relevant BAD parameters were extracted. Then, by applying on these selected parameters two clustering methods, Kmeans and DBscan as well as the logistic regression classifier, different local structures have been successfully extracted. This procedure was able to detect the local structures present within the dislocation lines with a higher degree of precision compared to the commonly used CNA method. Our procedure is also more efficient than a method relying on hand-chosen criteria applied to BAD parameters or than combining autoencoder with BOO parameters.\nOverall, this study shows that unsupervised learning applied to the BAD parameters is a promising approach to obtain more precise structure detection within crystalline materials. As the method detailed in this paper is based on parameters using hand-chosen ranges in its definition, it is not directly applicable to other crystalline system, especially alloys. While modified BAD parameters adapted for specific systems exist (Amodeo et al., 2014), developing a method to automatically find the optimal ranges to define the BAD parameters for any materials would allow to achieve an universal method for structural detection for crystalline materials."
        },
        {
            "heading": "Appendix A. Linking detected structures and dislocation lines",
            "text": "On Fig A.11 we show a comparison between the structures detected with our method (a) and the dislocation lines obtained from DXA (Stukowski, 2012) (b). On Fig A.11 (b) the black arrows correspond to the burger vector of each dislocation and the green arrows show the dislocation lines directions."
        },
        {
            "heading": "Appendix B. Autoencoder and clustering applied to BOO parameters",
            "text": "To compare more precisly the BOO and BAD parameters to detect local structure, we reproduced the method from (Boattini et al., 2019), which served as an inspiration for our method, and applied it to our data set. The method consist basically on calculation the BOO parameters for each atoms and training the autoencoder to find the optimal bottleneck dimension. Then, our method diverge from ours as they apply gaussian clustering directly to the output of the bottleneck.\nBy applying this approach, we obtain an optimal bottleneck dimension equal to two. We then show on Fig B.12 a scatter plot showing for each atom their position in the space of the two bottleneck dimensions that we name {btneck1, btneck2}. By following the clustering method detailed in (Boattini et al., 2019), we extract two clusters: one in yellow corresponding to the FCC structure and one in blue corresponding to the HCP structure (in the stacking faults). We can see that no other clusters that could correspond to the structures within the dislocation lines are visible on Fig B.12. From this, we deduce that the BOO parameter is not pertinent to separate the dislocation lines from the other structures in a crystal under plastic deformation."
        }
    ],
    "title": "Unsupervised learning for structure detection in plastically deformed crystals",
    "year": 2023
}