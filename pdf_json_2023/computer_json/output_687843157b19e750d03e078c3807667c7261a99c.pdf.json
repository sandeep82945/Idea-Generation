{
    "abstractText": "Video captioning (VC) is a fast moving, cross-disciplinary area of research that bridges work in the fields of computer vision, natural language processing (NLP), linguistics and human-computer interaction. In essence, VC involves understanding a video and describing it with language. Captioning is used in a host of applications from creating more accessible interfaces (e.g., low-vision navigation) to video question answering (V-QA), video retrieval and content generation. This survey covers deep learning-based VC, including but, not limited to, attention-based architectures, graph networks, reinforcement learning, adversarial networks, dense video captioning (DVC), and more. We discuss the datasets and evaluation metrics used in the field, and limitations, applications, challenges, and future directions for VC.",
    "authors": [
        {
            "affiliations": [],
            "name": "Moloud Abdar"
        },
        {
            "affiliations": [],
            "name": "Meenakshi Kollati"
        },
        {
            "affiliations": [],
            "name": "Swaraja Kuraparthi"
        },
        {
            "affiliations": [],
            "name": "Farhad Pourpanah"
        },
        {
            "affiliations": [],
            "name": "Daniel McDuff"
        },
        {
            "affiliations": [],
            "name": "Mohammad Ghavamzadeh"
        },
        {
            "affiliations": [],
            "name": "Shuicheng Yan"
        },
        {
            "affiliations": [],
            "name": "Abduallah Mohamed"
        },
        {
            "affiliations": [],
            "name": "Abbas Khosravi"
        }
    ],
    "id": "SP:ada095fba83e4fe1974b008a325ebc3a7e39dda8",
    "references": [
        {
            "authors": [
                "O. Vinyals",
                "A. Toshev",
                "S. Bengio",
                "D. Erhan"
            ],
            "title": "Show and tell: Lessons learned from the 2015 mscoco image captioning challenge",
            "venue": "IEEE transactions on pattern analysis and machine intelligence, vol. 39, no. 4, pp. 652\u2013663, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "Q. You",
                "H. Jin",
                "Z. Wang",
                "C. Fang",
                "J. Luo"
            ],
            "title": "Image captioning with semantic attention",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 4651\u20134659.",
            "year": 2016
        },
        {
            "authors": [
                "P. Sharma",
                "N. Ding",
                "S. Goodman",
                "R. Soricut"
            ],
            "title": "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2018, pp. 2556\u20132565.",
            "year": 2018
        },
        {
            "authors": [
                "J. Gu",
                "J. Bradbury",
                "C. Xiong",
                "V.O. Li",
                "R. Socher"
            ],
            "title": "Nonautoregressive neural machine translation",
            "venue": "arXiv preprint arXiv:1711.02281, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "L. Guo",
                "J. Liu",
                "X. Zhu",
                "X. He",
                "J. Jiang",
                "H. Lu"
            ],
            "title": "Nonautoregressive image captioning with counterfactuals-critical multi-agent learning",
            "venue": "arXiv preprint arXiv:2005.04690, 2020.",
            "year": 2005
        },
        {
            "authors": [
                "A. Gandhi",
                "K. Adhvaryu",
                "S. Poria",
                "E. Cambria",
                "A. Hussain"
            ],
            "title": "Multimodal sentiment analysis: A systematic review of history, datasets, multimodal fusion methods, applications, challenges and future directions",
            "venue": "Information Fusion, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "X. Xiao",
                "Y. Zhang",
                "R. Feng",
                "T. Zhang",
                "S. Gao",
                "W. Fan"
            ],
            "title": "Video captioning with temporal and region graph convolution network",
            "venue": "2020 IEEE International Conference on Multimedia and Expo (ICME). IEEE, 2020, pp. 1\u20136.",
            "year": 2020
        },
        {
            "authors": [
                "G. Zhao",
                "M. Zhang",
                "Y. Li",
                "J. Liu",
                "B. Zhang",
                "J.-R. Wen"
            ],
            "title": "Pyramid regional graph representation learning for contentbased video retrieval",
            "venue": "Information Processing & Management, vol. 58, no. 3, p. 102488, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "J. Lei",
                "L. Wang",
                "Y. Shen",
                "D. Yu",
                "T.L. Berg",
                "M. Bansal"
            ],
            "title": "Mart: Memory-augmented recurrent transformer for coherent video paragraph captioning",
            "venue": "arXiv preprint arXiv:2005.05402, 2020.",
            "year": 2005
        },
        {
            "authors": [
                "Y. Song",
                "S. Chen",
                "Q. Jin"
            ],
            "title": "Towards diverse paragraph captioning for untrimmed videos",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 11 245\u201311 254.",
            "year": 2021
        },
        {
            "authors": [
                "Y. LeCun",
                "Y. Bengio",
                "G. Hinton"
            ],
            "title": "Deep learning",
            "venue": "nature, vol. 521, no. 7553, pp. 436\u2013444, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "X. Wang",
                "Y. Zhao",
                "F. Pourpanah"
            ],
            "title": "Recent advances in deep learning",
            "venue": "International Journal of Machine Learning and Cybernetics, vol. 11, no. 4, pp. 747\u2013750, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "F. Pourpanah",
                "M. Abdar",
                "Y. Luo",
                "X. Zhou",
                "R. Wang",
                "C.P. Lim",
                "X.-Z. Wang",
                "Q.J. Wu"
            ],
            "title": "A review of generalized zero-shot learning methods",
            "venue": "IEEE transactions on pattern analysis and machine intelligence, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "K.-H. Zeng",
                "T.-H. Chen",
                "C.-Y. Chuang",
                "Y.-H. Liao",
                "J.C. Niebles",
                "M. Sun"
            ],
            "title": "Leveraging video descriptions to learn video question answering",
            "venue": "Thirty-First AAAI Conference on Artificial Intelligence, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "J. Lei",
                "L. Yu",
                "M. Bansal",
                "T.L. Berg"
            ],
            "title": "Tvqa: Localized, compositional video question answering",
            "venue": "arXiv preprint arXiv:1809.01696, 2018.",
            "year": 1809
        },
        {
            "authors": [
                "S. Chen",
                "X. Zhong",
                "S. Wu",
                "Z. Sun",
                "W. Liu",
                "X. Jia",
                "H. Xia"
            ],
            "title": "Memory-attended semantic context-aware network for video captioning",
            "venue": "Soft Computing, pp. 1\u201313, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "J. Song",
                "H. Zhang",
                "X. Li",
                "L. Gao",
                "M. Wang",
                "R. Hong"
            ],
            "title": "Self-supervised video hashing with hierarchical binary autoencoder",
            "venue": "IEEE Transactions on Image Processing, vol. 27, no. 7, pp. 3210\u20133221, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "Y. Xiong",
                "B. Dai",
                "D. Lin"
            ],
            "title": "Move forward and tell: A progressive generator of video descriptions",
            "venue": "Proceedings of the European Conference on Computer Vision (ECCV), 2018, pp. 468\u2013483.",
            "year": 2018
        },
        {
            "authors": [
                "T.-H. Chen",
                "K.-H. Zeng",
                "W.-T. Hsu",
                "M. Sun"
            ],
            "title": "Video captioning via sentence augmentation and spatio-temporal attention",
            "venue": "Asian Conference on Computer Vision. Springer, 2016, pp. 269\u2013286.",
            "year": 2016
        },
        {
            "authors": [
                "N. Xu",
                "A.-A. Liu",
                "Y. Wong",
                "Y. Zhang",
                "W. Nie",
                "Y. Su",
                "M. Kankanhalli"
            ],
            "title": "Dual-stream recurrent neural network for video captioning",
            "venue": "IEEE Transactions on Circuits and Systems for Video Technology, vol. 29, no. 8, pp. 2482\u20132493, 2018. 33",
            "year": 2018
        },
        {
            "authors": [
                "A. Kojima",
                "T. Tamura",
                "K. Fukunaga"
            ],
            "title": "Natural language description of human activities from video images based on concept hierarchy of actions",
            "venue": "International Journal of Computer Vision, vol. 50, no. 2, pp. 171\u2013184, 2002.",
            "year": 2002
        },
        {
            "authors": [
                "R.J. Babariya",
                "T. Tamaki"
            ],
            "title": "Meaning guided video captioning",
            "venue": "Asian Conference on Pattern Recognition. Springer, 2019, pp. 478\u2013488.",
            "year": 2019
        },
        {
            "authors": [
                "S. Guadarrama",
                "N. Krishnamoorthy",
                "G. Malkarnenkar",
                "S. Venugopalan",
                "R. Mooney",
                "T. Darrell",
                "K. Saenko"
            ],
            "title": "Youtube2text: Recognizing and describing arbitrary activities using semantic hierarchies and zero-shot recognition",
            "venue": "Proceedings of the IEEE international conference on computer vision, 2013, pp. 2712\u20132719.",
            "year": 2013
        },
        {
            "authors": [
                "N. Krishnamoorthy",
                "G. Malkarnenkar",
                "R. Mooney",
                "K. Saenko",
                "S. Guadarrama"
            ],
            "title": "Generating natural-language video descriptions using text-mined knowledge",
            "venue": "Twenty-Seventh AAAI Conference on Artificial Intelligence, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "J. Thomason",
                "S. Venugopalan",
                "S. Guadarrama",
                "K. Saenko",
                "R. Mooney"
            ],
            "title": "Integrating language and vision to generate natural language descriptions of videos in the wild",
            "venue": "University of Texas at Austin Austin United States, Tech. Rep., 2014.",
            "year": 2014
        },
        {
            "authors": [
                "M. Rohrbach",
                "W. Qiu",
                "I. Titov",
                "S. Thater",
                "M. Pinkal",
                "B. Schiele"
            ],
            "title": "Translating video content to natural language descriptions",
            "venue": "Proceedings of the IEEE international conference on computer vision, 2013, pp. 433\u2013440.",
            "year": 2013
        },
        {
            "authors": [
                "K. Khurana",
                "U. Deshpande"
            ],
            "title": "Video question-answering techniques, benchmark datasets and evaluation metrics leveraging video captioning: A comprehensive survey.",
            "venue": "IEEE Access,",
            "year": 2021
        },
        {
            "authors": [
                "L. Gao",
                "X. Wang",
                "J. Song",
                "Y. Liu"
            ],
            "title": "Fused gru with semantic-temporal attention for video captioning",
            "venue": "Neurocomputing, vol. 395, pp. 222\u2013228, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "S. Venugopalan",
                "H. Xu",
                "J. Donahue",
                "M. Rohrbach",
                "R. Mooney",
                "K. Saenko"
            ],
            "title": "Translating videos to natural language using deep recurrent neural networks",
            "venue": "arXiv preprint arXiv:1412.4729, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "N. Aafaq",
                "N. Akhtar",
                "W. Liu",
                "A. Mian"
            ],
            "title": "Empirical autopsy of deep video captioning encoder-decoder architecture",
            "venue": "Array, vol. 9, p. 100052, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "S. Olivastri",
                "G. Singh",
                "F. Cuzzolin"
            ],
            "title": "End-to-end video captioning",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops, 2019, pp. 0\u20130.",
            "year": 2019
        },
        {
            "authors": [
                "P. Goyal",
                "S. Pandey",
                "K. Jain"
            ],
            "title": "Deep learning for natural language processing",
            "venue": "New York: Apress, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "C. Deng",
                "S. Chen",
                "D. Chen",
                "Y. He",
                "Q. Wu"
            ],
            "title": "Sketch, ground, and refine: Top-down dense video captioning",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 234\u2013243.",
            "year": 2021
        },
        {
            "authors": [
                "Z. Chang",
                "D. Zhao",
                "H. Chen",
                "J. Li",
                "P. Liu"
            ],
            "title": "Event-centric multi-modal fusion method for dense video captioning",
            "venue": "Neural Networks, vol. 146, pp. 120\u2013129, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "N. Aafaq",
                "A.S. Mian",
                "N. Akhtar",
                "W. Liu",
                "M. Shah"
            ],
            "title": "Dense video captioning with early linguistic information fusion",
            "venue": "IEEE Transactions on Multimedia, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "S. Pu",
                "Y. He",
                "Z. Li",
                "M. Zheng"
            ],
            "title": "Multimodal topic learning for video recommendation",
            "venue": "arXiv preprint arXiv:2010.13373, 2020.",
            "year": 2010
        },
        {
            "authors": [
                "S. Islam",
                "A. Dash",
                "A. Seum",
                "A.H. Raj",
                "T. Hossain",
                "F.M. Shah"
            ],
            "title": "Exploring video captioning techniques: A comprehensive survey on deep learning methods",
            "venue": "SN Computer Science, vol. 2, no. 2, pp. 1\u201328, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "V. Jain",
                "F. Al-Turjman",
                "G. Chaudhary",
                "D. Nayar",
                "V. Gupta",
                "A. Kumar"
            ],
            "title": "Video captioning: a review of theory, techniques and practices",
            "venue": "Multimedia Tools and Applications, pp. 1\u201335, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "D. Moctezuma",
                "T. Ram\u0131\u0301rez-delReal",
                "G. Ruiz",
                "O. Gonz\u00e1lez- Ch\u00e1vez"
            ],
            "title": "Video captioning: a comparative review of where we are and which could be the route",
            "venue": "arXiv preprint arXiv:2204.05976, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "S. Li",
                "Z. Tao",
                "K. Li",
                "Y. Fu"
            ],
            "title": "Visual to text: Survey of image and video captioning",
            "venue": "IEEE Transactions on Emerging Topics in Computational Intelligence, vol. 3, no. 4, pp. 297\u2013312, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "N. Aafaq",
                "A. Mian",
                "W. Liu",
                "S.Z. Gilani",
                "M. Shah"
            ],
            "title": "Video description: A survey of methods, datasets, and evaluation metrics",
            "venue": "ACM Computing Surveys (CSUR), vol. 52, no. 6, pp. 1\u201337, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "J. Wang",
                "J. Jiao",
                "L. Bao",
                "S. He",
                "Y. Liu",
                "W. Liu"
            ],
            "title": "Selfsupervised spatio-temporal representation learning for videos by predicting motion and appearance statistics",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 4006\u20134015.",
            "year": 2019
        },
        {
            "authors": [
                "M. Caron",
                "P. Bojanowski",
                "J. Mairal",
                "A. Joulin"
            ],
            "title": "Unsupervised pre-training of image features on non-curated data",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 2959\u20132968.",
            "year": 2019
        },
        {
            "authors": [
                "J. Jaworek-Korjakowska",
                "P. Kleczek",
                "M. Gorgon"
            ],
            "title": "Melanoma thickness prediction based on convolutional neural network with vgg-19 model transfer learning",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, 2019, pp. 0\u20130.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Xie",
                "D. Richmond"
            ],
            "title": "Pre-training on grayscale imagenet improves medical image classification",
            "venue": "Proceedings of the European conference on computer vision (ECCV) workshops, 2018, pp. 0\u20130.",
            "year": 2018
        },
        {
            "authors": [
                "C. Szegedy",
                "S. Ioffe",
                "V. Vanhoucke",
                "A. Alemi"
            ],
            "title": "Inceptionv4, inception-resnet and the impact of residual connections on learning",
            "venue": "Proceedings of the AAAI conference on artificial intelligence, vol. 31, no. 1, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "Y. Xu",
                "J. Yang",
                "K. Mao"
            ],
            "title": "Semantic-filtered soft-split-aware video captioning with audio-augmented feature",
            "venue": "Neurocomputing, vol. 357, pp. 24\u201335, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "L. Deng",
                "Y. Liu"
            ],
            "title": "Deep learning in natural language processing",
            "year": 2018
        },
        {
            "authors": [
                "T. Jin",
                "S. Huang",
                "Y. Li",
                "Z. Zhang"
            ],
            "title": "Low-rank hoca: Efficient high-order cross-modal attention for video captioning",
            "venue": "arXiv preprint arXiv:1911.00212, 2019.",
            "year": 1911
        },
        {
            "authors": [
                "K. Nakamura",
                "H. Ohashi",
                "M. Okada"
            ],
            "title": "Sensor-augmented egocentric-video captioning with dynamic modal attention",
            "venue": "Proceedings of the 29th ACM International Conference on Multimedia, 2021, pp. 4220\u20134229.",
            "year": 2021
        },
        {
            "authors": [
                "P.P. Mohanta"
            ],
            "title": "Boosting video captioning with dynamic loss network",
            "venue": "arXiv preprint arXiv:2107.11707, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "S. Hochreiter",
                "J. Schmidhuber"
            ],
            "title": "Long short-term memory",
            "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.",
            "year": 1997
        },
        {
            "authors": [
                "X. Shi",
                "J. Cai",
                "J. Gu",
                "S. Joty"
            ],
            "title": "Video captioning with boundary-aware hierarchical language decoding and joint video prediction",
            "venue": "Neurocomputing, vol. 417, pp. 347\u2013356, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "P. Ramachandran",
                "P.J. Liu",
                "Q.V. Le"
            ],
            "title": "Unsupervised pretraining for sequence to sequence learning",
            "venue": "arXiv preprint arXiv:1611.02683, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "A. Dosovitskiy",
                "L. Beyer",
                "A. Kolesnikov",
                "D. Weissenborn",
                "X. Zhai",
                "T. Unterthiner",
                "M. Dehghani",
                "M. Minderer",
                "G. Heigold",
                "S. Gelly"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "arXiv preprint arXiv:2010.11929, 2020.",
            "year": 2010
        },
        {
            "authors": [
                "H. Touvron",
                "M. Cord",
                "M. Douze",
                "F. Massa",
                "A. Sablayrolles",
                "H. J\u00e9gou"
            ],
            "title": "Training data-efficient image transformers & distillation through attention",
            "venue": "International Conference on Machine Learning. PMLR, 2021, pp. 10 347\u201310 357.",
            "year": 2021
        },
        {
            "authors": [
                "F. Yang",
                "H. Yang",
                "J. Fu",
                "H. Lu",
                "B. Guo"
            ],
            "title": "Learning texture transformer network for image super-resolution",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp. 5791\u20135800.",
            "year": 2020
        },
        {
            "authors": [
                "A. Vaswani",
                "N. Shazeer",
                "N. Parmar",
                "J. Uszkoreit",
                "L. Jones",
                "A.N. Gomez",
                "L. Kaiser",
                "I. Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems, vol. 30, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "T. Jin",
                "S. Huang",
                "M. Chen",
                "Y. Li",
                "Z. Zhang"
            ],
            "title": "Sbat: Video captioning with sparse boundary-aware transformer",
            "venue": "arXiv preprint arXiv:2007.11888, 2020.",
            "year": 2007
        },
        {
            "authors": [
                "Z. Yu",
                "N. Han"
            ],
            "title": "Accelerated masked transformer for dense video captioning",
            "venue": "Neurocomputing, vol. 445, pp. 72\u201380, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "P. Xu",
                "X. Chang",
                "L. Guo",
                "P.-Y. Huang",
                "X. Chen",
                "A.G. Hauptmann"
            ],
            "title": "A survey of scene graph: Generation and application",
            "venue": "EasyChair Preprint, no. 3385, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "D. Chen",
                "W.B. Dolan"
            ],
            "title": "Collecting highly parallel data for paraphrase evaluation",
            "venue": "Proceedings of the 49th annual meeting of the association for computational linguistics: human language technologies, 2011, pp. 190\u2013200.",
            "year": 2011
        },
        {
            "authors": [
                "F. Caba Heilbron",
                "V. Escorcia",
                "B. Ghanem",
                "J. Carlos Niebles"
            ],
            "title": "Activitynet: A large-scale video benchmark for human activity understanding",
            "venue": "Proceedings of the ieee con- 34 ference on computer vision and pattern recognition, 2015, pp. 961\u2013970.",
            "year": 2015
        },
        {
            "authors": [
                "C.-Y. Ma",
                "A. Kadav",
                "I. Melvin",
                "Z. Kira",
                "G. AlRegib",
                "H.P. Graf"
            ],
            "title": "Grounded objects and interactions for video captioning",
            "venue": "arXiv preprint arXiv:1711.06354, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "V. Iashin",
                "E. Rahtu"
            ],
            "title": "A better use of audio-visual cues: Dense video captioning with bi-modal transformer",
            "venue": "arXiv preprint arXiv:2005.08271, 2020.",
            "year": 2005
        },
        {
            "authors": [
                "L. Zhou",
                "C. Xu",
                "J.J. Corso"
            ],
            "title": "Towards automatic learning of procedures from web instructional videos",
            "venue": "Thirty-Second AAAI Conference on Artificial Intelligence, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "S. Amirian",
                "K. Rasheed",
                "T.R. Taha",
                "H.R. Arabnia"
            ],
            "title": "Automatic generation of descriptive titles for video clips using deep learning",
            "venue": "Advances in Artificial Intelligence and Applied Cognitive Computing. Springer, 2021, pp. 17\u201328.",
            "year": 2021
        },
        {
            "authors": [
                "T. Wang",
                "R. Zhang",
                "Z. Lu",
                "F. Zheng",
                "R. Cheng",
                "P. Luo"
            ],
            "title": "End-to-end dense video captioning with parallel decoding",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 6847\u20136857.",
            "year": 2021
        },
        {
            "authors": [
                "H. Chen",
                "J. Li",
                "X. Hu"
            ],
            "title": "Delving deeper into the decoder for video captioning",
            "venue": "arXiv preprint arXiv:2001.05614, 2020.",
            "year": 2001
        },
        {
            "authors": [
                "M. Regneri",
                "M. Rohrbach",
                "D. Wetzel",
                "S. Thater",
                "B. Schiele",
                "M. Pinkal"
            ],
            "title": "Grounding action descriptions in videos",
            "venue": "Transactions of the Association for Computational Linguistics, vol. 1, pp. 25\u201336, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "M. Sun",
                "A. Farhadi",
                "S. Seitz"
            ],
            "title": "Ranking domain-specific highlights by analyzing edited videos",
            "venue": "European conference on computer vision. Springer, 2014, pp. 787\u2013802.",
            "year": 2014
        },
        {
            "authors": [
                "P. Das",
                "C. Xu",
                "R.F. Doell",
                "J.J. Corso"
            ],
            "title": "A thousand frames in just a few words: Lingual description of videos through latent topics and sparse object stitching",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, 2013, pp. 2634\u20132641.",
            "year": 2013
        },
        {
            "authors": [
                "A. Torabi",
                "C. Pal",
                "H. Larochelle",
                "A. Courville"
            ],
            "title": "Using descriptive video services to create a large data source for video annotation research",
            "venue": "arXiv preprint arXiv:1503.01070, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "J. Xu",
                "T. Mei",
                "T. Yao",
                "Y. Rui"
            ],
            "title": "Msr-vtt: A large video description dataset for bridging video and language",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 5288\u20135296.",
            "year": 2016
        },
        {
            "authors": [
                "K.-H. Zeng",
                "T.-H. Chen",
                "J.C. Niebles",
                "M. Sun"
            ],
            "title": "Generation for user generated videos",
            "venue": "European conference on computer vision. Springer, 2016, pp. 609\u2013625.",
            "year": 2016
        },
        {
            "authors": [
                "Y. Li",
                "Y. Song",
                "L. Cao",
                "J. Tetreault",
                "L. Goldberg",
                "A. Jaimes",
                "J. Luo"
            ],
            "title": "Tgif: A new dataset and benchmark on animated gif description",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 4641\u2013 4650.",
            "year": 2016
        },
        {
            "authors": [
                "X. Wang",
                "J. Wu",
                "J. Chen",
                "L. Li",
                "Y.-F. Wang",
                "W.Y. Wang"
            ],
            "title": "Vatex: A large-scale, high-quality multilingual dataset for video-and-language research",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 4581\u2013 4591.",
            "year": 2019
        },
        {
            "authors": [
                "W. Kay",
                "J. Carreira",
                "K. Simonyan",
                "B. Zhang",
                "C. Hillier",
                "S. Vijayanarasimhan",
                "F. Viola",
                "T. Green",
                "T. Back",
                "P. Natsev"
            ],
            "title": "The kinetics human action video dataset",
            "venue": "arXiv preprint arXiv:1705.06950, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "R. Goyal",
                "S. Ebrahimi Kahou",
                "V. Michalski",
                "J. Materzynska",
                "S. Westphal",
                "H. Kim",
                "V. Haenel",
                "I. Fruend",
                "P. Yianilos",
                "M. Mueller-Freitag"
            ],
            "title": "The\u201d something something\u201d video database for learning and evaluating visual common sense",
            "venue": "Proceedings of the IEEE international conference on computer vision, 2017, pp. 5842\u20135850.",
            "year": 2017
        },
        {
            "authors": [
                "R. Wei",
                "L. Mi",
                "Y. Hu",
                "Z. Chen"
            ],
            "title": "Exploiting the local temporal information for video captioning",
            "venue": "Journal of Visual Communication and Image Representation, vol. 67, p. 102751, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "G.A. Sigurdsson",
                "G. Varol",
                "X. Wang",
                "A. Farhadi",
                "I. Laptev",
                "A. Gupta"
            ],
            "title": "Hollywood in homes: Crowdsourcing data collection for activity understanding",
            "venue": "European Conference on Computer Vision. Springer, 2016, pp. 510\u2013526.",
            "year": 2016
        },
        {
            "authors": [
                "G.A. Sigurdsson",
                "A. Gupta",
                "C. Schmid",
                "A. Farhadi",
                "K. Alahari"
            ],
            "title": "Charades-ego: A large-scale dataset of paired third and first person videos",
            "venue": "arXiv preprint arXiv:1804.09626, 2018.",
            "year": 1804
        },
        {
            "authors": [
                "R. Krishna",
                "K. Hata",
                "F. Ren",
                "L. Fei-Fei",
                "J. Carlos Niebles"
            ],
            "title": "Dense-captioning events in videos",
            "venue": "Proceedings of the IEEE international conference on computer vision, 2017, pp. 706\u2013 715.",
            "year": 2017
        },
        {
            "authors": [
                "M. Monfort",
                "A. Andonian",
                "B. Zhou",
                "K. Ramakrishnan",
                "S.A. Bargal",
                "T. Yan",
                "L. Brown",
                "Q. Fan",
                "D. Gutfreund",
                "C. Vondrick"
            ],
            "title": "Moments in time dataset: one million videos for event understanding",
            "venue": "IEEE transactions on pattern analysis and machine intelligence, vol. 42, no. 2, pp. 502\u2013508, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "A. Miech",
                "J.-B. Alayrac",
                "L. Smaira",
                "I. Laptev",
                "J. Sivic",
                "A. Zisserman"
            ],
            "title": "End-to-end learning of visual representations from uncurated instructional videos",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 9879\u20139889.",
            "year": 2020
        },
        {
            "authors": [
                "A. Piergiovanni",
                "M. Ryoo"
            ],
            "title": "Avid dataset: Anonymized videos from diverse countries",
            "venue": "Advances in Neural Information Processing Systems, vol. 33, pp. 16 711\u201316 721, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "M. Monfort",
                "S. Jin",
                "A. Liu",
                "D. Harwath",
                "R. Feris",
                "J. Glass",
                "A. Oliva"
            ],
            "title": "Spoken moments: Learning joint audio-visual representations from video descriptions",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 14 871\u201314 881.",
            "year": 2021
        },
        {
            "authors": [
                "A. Rohrbach",
                "M. Rohrbach",
                "N. Tandon",
                "B. Schiele"
            ],
            "title": "A dataset for movie description",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, 2015, pp. 3202\u20133212.",
            "year": 2015
        },
        {
            "authors": [
                "A. Rohrbach",
                "A. Torabi",
                "M. Rohrbach",
                "N. Tandon",
                "C. Pal",
                "H. Larochelle",
                "A. Courville",
                "B. Schiele"
            ],
            "title": "Movie description",
            "venue": "International Journal of Computer Vision, vol. 123, no. 1, pp. 94\u2013120, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "R. Pasunuru",
                "M. Bansal"
            ],
            "title": "Multi-task video captioning with video and entailment generation",
            "venue": "arXiv preprint arXiv:1704.07489, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "F. Bolelli",
                "L. Baraldi",
                "F. Pollastri",
                "C. Grana"
            ],
            "title": "A hierarchical quasi-recurrent approach to video captioning",
            "venue": "2018 IEEE International Conference on Image Processing, Applications and Systems (IPAS). IEEE, 2018, pp. 162\u2013167.",
            "year": 2018
        },
        {
            "authors": [
                "A. Shin",
                "K. Ohnishi",
                "T. Harada"
            ],
            "title": "Beyond caption to narrative: Video captioning with multiple sentences",
            "venue": "2016 IEEE International Conference on Image Processing (ICIP). IEEE, 2016, pp. 3364\u20133368.",
            "year": 2016
        },
        {
            "authors": [
                "G. Tan",
                "D. Liu",
                "M. Wang",
                "Z.-J. Zha"
            ],
            "title": "Learning to discretely compose reasoning module networks for video captioning",
            "venue": "arXiv preprint arXiv:2007.09049, 2020.",
            "year": 2007
        },
        {
            "authors": [
                "J. Wang",
                "W. Wang",
                "Y. Huang",
                "L. Wang",
                "T. Tan"
            ],
            "title": "Hierarchical memory modelling for video captioning",
            "venue": "Proceedings of the 26th ACM international conference on Multimedia, 2018, pp. 63\u201371.",
            "year": 2018
        },
        {
            "authors": [
                "A. Rohrbach",
                "M. Rohrbach",
                "W. Qiu",
                "A. Friedrich",
                "M. Pinkal",
                "B. Schiele"
            ],
            "title": "Coherent multi-sentence video description with variable level of detail",
            "venue": "German conference on pattern recognition. Springer, 2014, pp. 184\u2013195.",
            "year": 2014
        },
        {
            "authors": [
                "K. Papineni",
                "S. Roukos",
                "T. Ward",
                "W.-J. Zhu"
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th annual meeting of the Association for Computational Linguistics, 2002, pp. 311\u2013318.",
            "year": 2002
        },
        {
            "authors": [
                "M. Denkowski",
                "A. Lavie"
            ],
            "title": "Meteor universal: Language specific translation evaluation for any target language",
            "venue": "Proceedings of the ninth workshop on statistical machine translation, 2014, pp. 376\u2013380.",
            "year": 2014
        },
        {
            "authors": [
                "C.-Y. Lin"
            ],
            "title": "Rouge: A package for automatic evaluation of summaries",
            "venue": "Text summarization branches out, 2004, pp. 74\u2013 81.",
            "year": 2004
        },
        {
            "authors": [
                "R. Vedantam",
                "C. Lawrence Zitnick",
                "D. Parikh"
            ],
            "title": "Cider: Consensus-based image description evaluation",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, 2015, pp. 4566\u20134575.",
            "year": 2015
        },
        {
            "authors": [
                "L. Gao",
                "Y. Lei",
                "P. Zeng",
                "J. Song",
                "M. Wang",
                "H.T. Shen"
            ],
            "title": "Hierarchical representation network with auxiliary tasks for video captioning and video question answering",
            "venue": "IEEE Transactions on Image Processing, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Rubner",
                "C. Tomasi",
                "L.J. Guibas"
            ],
            "title": "The earth mover\u2019s distance as a metric for image retrieval",
            "venue": "International journal of computer vision, vol. 40, no. 2, pp. 99\u2013121, 2000.",
            "year": 2000
        },
        {
            "authors": [
                "M. Kusner",
                "Y. Sun",
                "N. Kolkin",
                "K. Weinberger"
            ],
            "title": "From word embeddings to document distances",
            "venue": "International conference on machine learning. PMLR, 2015, pp. 957\u2013966. 35",
            "year": 2015
        },
        {
            "authors": [
                "P. Anderson",
                "B. Fernando",
                "M. Johnson",
                "S. Gould"
            ],
            "title": "Spice: Semantic propositional image caption evaluation",
            "venue": "European conference on computer vision. Springer, 2016, pp. 382\u2013398.",
            "year": 2016
        },
        {
            "authors": [
                "S. Banerjee",
                "A. Lavie"
            ],
            "title": "Meteor: An automatic metric for mt evaluation with improved correlation with human judgments",
            "venue": "Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, 2005, pp. 65\u201372.",
            "year": 2005
        },
        {
            "authors": [
                "H. Christian",
                "M.P. Agus",
                "D. Suhartono"
            ],
            "title": "Single document automatic text summarization using term frequency-inverse document frequency (tf-idf)",
            "venue": "ComTech: Computer, Mathematics and Engineering Applications, vol. 7, no. 4, pp. 285\u2013294, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "W. Yang",
                "H. Zhang",
                "J. Lin"
            ],
            "title": "Simple applications of bert for ad hoc document retrieval",
            "venue": "arXiv preprint arXiv:1903.10972, 2019.",
            "year": 1903
        },
        {
            "authors": [
                "M. Nabati",
                "A. Behrad"
            ],
            "title": "Multi-sentence video captioning using content-oriented beam searching and multi-stage refining algorithm",
            "venue": "Information Processing & Management, vol. 57, no. 6, p. 102302, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "H. Xiao",
                "J. Shi"
            ],
            "title": "Diverse video captioning through latent variable expansion",
            "venue": "arXiv preprint arXiv:1910.12019, 2019.",
            "year": 1910
        },
        {
            "authors": [
                "S. Deerwester",
                "S.T. Dumais",
                "G.W. Furnas",
                "T.K. Landauer",
                "R. Harshman"
            ],
            "title": "Indexing by latent semantic analysis",
            "venue": "Journal of the American society for information science, vol. 41, no. 6, pp. 391\u2013407, 1990.",
            "year": 1990
        },
        {
            "authors": [
                "Z. Shen",
                "J. Li",
                "Z. Su",
                "M. Li",
                "Y. Chen",
                "Y.-G. Jiang",
                "X. Xue"
            ],
            "title": "Weakly supervised dense video captioning",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 1916\u20131924.",
            "year": 2017
        },
        {
            "authors": [
                "R. Real",
                "J.M. Vargas"
            ],
            "title": "The probabilistic basis of jaccard\u2019s index of similarity",
            "venue": "Systematic biology, vol. 45, no. 3, pp. 380\u2013 385, 1996.",
            "year": 1996
        },
        {
            "authors": [
                "H. Ryu",
                "S. Kang",
                "H. Kang",
                "C.D. Yoo"
            ],
            "title": "Semantic grouping network for video captioning",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, vol. 35, no. 3, 2021, pp. 2514\u20132522.",
            "year": 2021
        },
        {
            "authors": [
                "K. Lin",
                "Z. Gan",
                "L. Wang"
            ],
            "title": "Augmented partial mutual learning with frame masking for video captioning",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, vol. 35, no. 3, 2021, pp. 2047\u20132055.",
            "year": 2021
        },
        {
            "authors": [
                "G. Hinton",
                "O. Vinyals",
                "J. Dean"
            ],
            "title": "Distilling the knowledge in a neural network",
            "venue": "arXiv preprint arXiv:1503.02531, vol. 2, no. 7, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "H. Chen",
                "K. Lin",
                "A. Maye",
                "J. Li",
                "X. Hu"
            ],
            "title": "A semanticsassisted video captioning model trained with scheduled sampling",
            "venue": "Frontiers in Robotics and AI, p. 129, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "P. Cao",
                "Z. Yang",
                "L. Sun",
                "Y. Liang",
                "M.Q. Yang",
                "R. Guan"
            ],
            "title": "Image captioning with bidirectional semantic attention-based guiding of long short-term memory",
            "venue": "Neural Processing Letters, vol. 50, no. 1, pp. 103\u2013119, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "H. Lu",
                "R. Yang",
                "Z. Deng",
                "Y. Zhang",
                "G. Gao",
                "R. Lan"
            ],
            "title": "Chinese image captioning via fuzzy attention-based densenetbilstm",
            "venue": "ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM), vol. 17, no. 1s, pp. 1\u201318, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "D. Bahdanau",
                "K. Cho",
                "Y. Bengio"
            ],
            "title": "Neural machine translation by jointly learning to align and translate",
            "venue": "arXiv preprint arXiv:1409.0473, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "G.-H. Wang",
                "J.-X. Du",
                "H.-B. Zhang"
            ],
            "title": "Multi-feature fusion refine network for video captioning",
            "venue": "Journal of Experimental & Theoretical Artificial Intelligence, pp. 1\u201315, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "A. Singh",
                "T.D. Singh",
                "S. Bandyopadhyay"
            ],
            "title": "Attention based video captioning framework for hindi",
            "venue": "Multimedia Systems, pp. 1\u201313, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "L. Shang",
                "Z. Lu",
                "H. Li"
            ],
            "title": "Neural responding machine for short-text conversation",
            "venue": "arXiv preprint arXiv:1503.02364, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "M. Seo",
                "A. Kembhavi",
                "A. Farhadi",
                "H. Hajishirzi"
            ],
            "title": "Bidirectional attention flow for machine comprehension",
            "venue": "arXiv preprint arXiv:1611.01603, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "Z. Li",
                "Y. Zhang",
                "Y. Wei",
                "Y. Wu",
                "Q. Yang"
            ],
            "title": "End-toend adversarial memory network for cross-domain sentiment classification.",
            "venue": "in IJCAI,",
            "year": 2017
        },
        {
            "authors": [
                "H. Xiao",
                "J. Shi"
            ],
            "title": "Video captioning with text-based dynamic attention and step-by-step learning",
            "venue": "Pattern Recognition Letters, vol. 133, pp. 305\u2013312, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "B. Chen",
                "Z. Zhang",
                "Y. Li",
                "G. Lu",
                "D. Zhang"
            ],
            "title": "Multi-label chest x-ray image classification via semantic similarity graph embedding",
            "venue": "IEEE Transactions on Circuits and Systems for Video Technology, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "M.M. Rahman",
                "T. Abedin",
                "K.S. Prottoy",
                "A. Moshruba",
                "F.H. Siddiqui"
            ],
            "title": "Video captioning with stacked attention and semantic hard pull",
            "venue": "PeerJ Computer Science, vol. 7, p. e664, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "L. Yao",
                "A. Torabi",
                "K. Cho",
                "N. Ballas",
                "C. Pal",
                "H. Larochelle",
                "A. Courville"
            ],
            "title": "Describing videos by exploiting temporal structure",
            "venue": "Proceedings of the IEEE international conference on computer vision, 2015, pp. 4507\u20134515.",
            "year": 2015
        },
        {
            "authors": [
                "X. Li",
                "B. Zhao",
                "X. Lu"
            ],
            "title": "Mam-rnn: Multi-level attention model based rnn for video captioning.",
            "venue": "in IJCAI,",
            "year": 2017
        },
        {
            "authors": [
                "Y. Yu",
                "J. Choi",
                "Y. Kim",
                "K. Yoo",
                "S.-H. Lee",
                "G. Kim"
            ],
            "title": "Supervising neural attention models for video captioning by human gaze data",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 490\u2013498.",
            "year": 2017
        },
        {
            "authors": [
                "M. Chen",
                "Y. Li",
                "Z. Zhang",
                "S. Huang"
            ],
            "title": "Tvt: Two-view transformer network for video captioning",
            "venue": "Asian Conference on Machine Learning. PMLR, 2018, pp. 847\u2013862.",
            "year": 2018
        },
        {
            "authors": [
                "W. Li",
                "D. Guo",
                "X. Fang"
            ],
            "title": "Multimodal architecture for video captioning with memory networks and an attention mechanism",
            "venue": "Pattern Recognition Letters, vol. 105, pp. 23\u201329, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "C. Yan",
                "Y. Tu",
                "X. Wang",
                "Y. Zhang",
                "X. Hao",
                "Y. Zhang",
                "Q. Dai"
            ],
            "title": "Stat: Spatial-temporal attention mechanism for video captioning",
            "venue": "IEEE transactions on multimedia, vol. 22, no. 1, pp. 229\u2013241, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "J. Dong",
                "K. Gao",
                "X. Chen",
                "J. Guo",
                "J. Cao",
                "Y. Zhang"
            ],
            "title": "Not all words are equal: Video-specific information loss for video captioning",
            "venue": "arXiv preprint arXiv:1901.00097, 2019.",
            "year": 1901
        },
        {
            "authors": [
                "Y. Zhu",
                "S. Jiang"
            ],
            "title": "Attention-based densely connected lstm for video captioning",
            "venue": "Proceedings of the 27th ACM international conference on multimedia, 2019, pp. 802\u2013810.",
            "year": 2019
        },
        {
            "authors": [
                "S. Sah",
                "T. Nguyen",
                "R. Ptucha"
            ],
            "title": "Understanding temporal structure for video captioning",
            "venue": "Pattern Analysis and Applications, vol. 23, no. 1, pp. 147\u2013159, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "C. Sur"
            ],
            "title": "Sact: self-aware multi-space feature composition transformer for multinomial attention for video captioning",
            "venue": "arXiv preprint arXiv:2006.14262, 2020.",
            "year": 2006
        },
        {
            "authors": [
                "C. Hori",
                "T. Hori",
                "J.L. Roux"
            ],
            "title": "Optimizing latency for online video captioning using audio-visual transformers",
            "venue": "arXiv preprint arXiv:2108.02147, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "J. Perez-Martin",
                "B. Bustos",
                "J. P\u00e9rez"
            ],
            "title": "Attentive visual semantic specialized network for video captioning",
            "venue": "2020 25th International Conference on Pattern Recognition (ICPR). IEEE, 2021, pp. 5767\u20135774.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Tu",
                "C. Zhou",
                "J. Guo",
                "S. Gao",
                "Z. Yu"
            ],
            "title": "Enhancing the alignment between target words and corresponding frames for video captioning",
            "venue": "Pattern Recognition, vol. 111, p. 107702, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "K. Lin",
                "L. Li",
                "C.-C. Lin",
                "F. Ahmed",
                "Z. Gan",
                "Z. Liu",
                "Y. Lu",
                "L. Wang"
            ],
            "title": "Swinbert: End-to-end transformers with sparse attention for video captioning",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 17 949\u201317 958.",
            "year": 2022
        },
        {
            "authors": [
                "T. Deb",
                "A. Sadmanee",
                "K.K. Bhaumik",
                "A.A. Ali",
                "M.A. Amin",
                "A. Rahman"
            ],
            "title": "Variational stacked local attention networks for diverse video captioning",
            "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2022, pp. 4070\u20134079.",
            "year": 2022
        },
        {
            "authors": [
                "P. Song",
                "D. Guo",
                "J. Cheng",
                "M. Wang"
            ],
            "title": "Contextual attention network for emotional video captioning",
            "venue": "IEEE Transactions on Multimedia, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "M. Zanfir",
                "E. Marinoiu",
                "C. Sminchisescu"
            ],
            "title": "Spatio-temporal attention models for grounded video captioning",
            "venue": "asian conference on computer vision. Springer, 2016, pp. 104\u2013119.",
            "year": 2016
        },
        {
            "authors": [
                "Z. Lei",
                "Y. Huang"
            ],
            "title": "Video captioning based on channel soft attention and semantic reconstructor",
            "venue": "Future Internet, vol. 13, no. 2, p. 55, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "S. Liu",
                "Z. Ren",
                "J. Yuan"
            ],
            "title": "Sibnet: Sibling convolutional encoder for video captioning",
            "venue": "IEEE transactions on pattern analysis and machine intelligence, vol. 43, no. 9, pp. 3259\u20133272, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "T. Chen",
                "Q. Zhao",
                "J. Song"
            ],
            "title": "Boundary detector encoder and decoder with soft attention for video captioning",
            "venue": "Asia- 36 Pacific Web (APWeb) and Web-Age Information Management (WAIM) Joint International Conference on Web and Big Data. Springer, 2019, pp. 105\u2013115.",
            "year": 2019
        },
        {
            "authors": [
                "K. Xu",
                "J. Ba",
                "R. Kiros",
                "K. Cho",
                "A. Courville",
                "R. Salakhudinov",
                "R. Zemel",
                "Y. Bengio"
            ],
            "title": "Show, attend and tell: Neural image caption generation with visual attention",
            "venue": "International conference on machine learning. PMLR, 2015, pp. 2048\u20132057.",
            "year": 2015
        },
        {
            "authors": [
                "Y. Gui",
                "D. Guo",
                "Y. Zhao"
            ],
            "title": "Semantic enhanced encoderdecoder network (sen) for video captioning",
            "venue": "Proceedings of the 2nd Workshop on Multimedia for Accessible Human Computer Interfaces, 2019, pp. 25\u201332.",
            "year": 2019
        },
        {
            "authors": [
                "T. Jin",
                "Y. Li",
                "Z. Zhang"
            ],
            "title": "Recurrent convolutional video captioning with global and local attention",
            "venue": "Neurocomputing, vol. 370, pp. 118\u2013127, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Peng",
                "C. Wang",
                "Y. Pei",
                "Y. Li"
            ],
            "title": "Video captioning with global and local text attention",
            "venue": "The Visual Computer, pp. 1\u2013 12, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "A.-A. Liu",
                "Y. Qiu",
                "Y. Wong",
                "Y.-T. Su",
                "M. Kankanhalli"
            ],
            "title": "A fine-grained spatial-temporal attention model for video captioning",
            "venue": "IEEE Access, vol. 6, pp. 68 463\u201368 471, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "Y. Chen",
                "S. Wang",
                "W. Zhang",
                "Q. Huang"
            ],
            "title": "Less is more: Picking informative frames for video captioning",
            "venue": "Proceedings of the European conference on computer vision (ECCV), 2018, pp. 358\u2013373.",
            "year": 2018
        },
        {
            "authors": [
                "A. Cherian",
                "J. Wang",
                "C. Hori",
                "T. Marks"
            ],
            "title": "Spatio-temporal ranked-attention networks for video captioning",
            "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2020, pp. 1617\u20131626.",
            "year": 2020
        },
        {
            "authors": [
                "S. Qi",
                "L. Yang"
            ],
            "title": "Video captioning via a symmetric bidirectional decoder",
            "venue": "IET Computer Vision, vol. 15, no. 4, pp. 283\u2013296, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "J. Deng",
                "L. Li",
                "B. Zhang",
                "S. Wang",
                "Z. Zha",
                "Q. Huang"
            ],
            "title": "Syntax-guided hierarchical attention network for video captioning",
            "venue": "IEEE Transactions on Circuits and Systems for Video Technology, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "X. Zhang",
                "K. Gao",
                "Y. Zhang",
                "D. Zhang",
                "J. Li",
                "Q. Tian"
            ],
            "title": "Task-driven dynamic fusion: Reducing ambiguity in video description",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 3713\u20133721.",
            "year": 2017
        },
        {
            "authors": [
                "C. Hori",
                "T. Hori",
                "T.-Y. Lee",
                "Z. Zhang",
                "B. Harsham",
                "J.R. Hershey",
                "T.K. Marks",
                "K. Sumi"
            ],
            "title": "Attention-based multimodal fusion for video description",
            "venue": "Proceedings of the IEEE international conference on computer vision, 2017, pp. 4193\u2013 4202.",
            "year": 2017
        },
        {
            "authors": [
                "X. Wu",
                "G. Li",
                "Q. Cao",
                "Q. Ji",
                "L. Lin"
            ],
            "title": "Interpretable video captioning via trajectory structured localization",
            "venue": "Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, 2018, pp. 6829\u20136837.",
            "year": 2018
        },
        {
            "authors": [
                "C. Wu",
                "Y. Wei",
                "X. Chu",
                "S. Weichen",
                "F. Su",
                "L. Wang"
            ],
            "title": "Hierarchical attention-based multimodal fusion for video captioning",
            "venue": "Neurocomputing, vol. 315, pp. 362\u2013370, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "J. Hou",
                "X. Wu",
                "W. Zhao",
                "J. Luo",
                "Y. Jia"
            ],
            "title": "Joint syntax representation learning and visual cue translation for video captioning",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 8918\u20138927.",
            "year": 2019
        },
        {
            "authors": [
                "S. Chen",
                "Y.-G. Jiang"
            ],
            "title": "Motion guided region message passing for video captioning",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 1543\u2013 1552.",
            "year": 2021
        },
        {
            "authors": [
                "S. Chen"
            ],
            "title": "Towards bridging video and language by caption generation and sentence localization",
            "venue": "Proceedings of the 29th ACM International Conference on Multimedia, 2021, pp. 2964\u2013 2968.",
            "year": 2021
        },
        {
            "authors": [
                "X. Wang",
                "A. Gupta"
            ],
            "title": "Videos as space-time region graphs",
            "venue": "Proceedings of the European conference on computer vision (ECCV), 2018, pp. 399\u2013417.",
            "year": 2018
        },
        {
            "authors": [
                "Y.-H.H. Tsai",
                "S. Divvala",
                "L.-P. Morency",
                "R. Salakhutdinov",
                "A. Farhadi"
            ],
            "title": "Video relationship reasoning using gated spatiotemporal energy graph",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 10 424\u201310 433.",
            "year": 2019
        },
        {
            "authors": [
                "Z. Zhang",
                "P. Cui",
                "W. Zhu"
            ],
            "title": "Deep learning on graphs: A survey",
            "venue": "IEEE Transactions on Knowledge and Data Engineering, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "M. Wang",
                "J. Xing",
                "Y. Liu"
            ],
            "title": "Actionclip: A new paradigm for video action recognition",
            "venue": "arXiv preprint arXiv:2109.08472, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "S. Cao",
                "B. Wang",
                "W. Zhang",
                "L. Ma"
            ],
            "title": "Visual consensus modeling for video-text retrieval",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, vol. 36, no. 1, pp. 167\u2013 175, Jun. 2022.",
            "year": 2022
        },
        {
            "authors": [
                "J. Wu",
                "L. Wang",
                "L. Wang",
                "J. Guo",
                "G. Wu"
            ],
            "title": "Learning actor relation graphs for group activity recognition",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 9964\u20139974.",
            "year": 2019
        },
        {
            "authors": [
                "L. Zhou",
                "Y. Kalantidis",
                "X. Chen",
                "J.J. Corso",
                "M. Rohrbach"
            ],
            "title": "Grounded video description",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 6578\u20136587.",
            "year": 2019
        },
        {
            "authors": [
                "J. Hou",
                "X. Wu",
                "X. Zhang",
                "Y. Qi",
                "Y. Jia",
                "J. Luo"
            ],
            "title": "Joint commonsense and relation reasoning for image and video captioning",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, no. 07, 2020, pp. 10 973\u201310 980.",
            "year": 2020
        },
        {
            "authors": [
                "L. Yang",
                "Y. Fan",
                "N. Xu"
            ],
            "title": "Video instance segmentation",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 5188\u20135197.",
            "year": 2019
        },
        {
            "authors": [
                "J. Zhang",
                "Y. Peng"
            ],
            "title": "Object-aware aggregation with bidirectional temporal graph for video captioning",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 8327\u20138336.",
            "year": 2019
        },
        {
            "authors": [
                "F. Zhu",
                "J.-N. Hwang",
                "Z. Ma",
                "G. Chen",
                "J. Guo"
            ],
            "title": "Ovcnet: Object-oriented video captioning with temporal graph and detail enhancement",
            "venue": "arXiv preprint arXiv:2003.03715, 2020.",
            "year": 2003
        },
        {
            "authors": [
                "J. Zhang",
                "Y. Peng"
            ],
            "title": "Video captioning with object-aware spatio-temporal correlation and aggregation",
            "venue": "IEEE Transactions on Image Processing, vol. 29, pp. 6209\u20136222, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Yan",
                "N. Zhuang",
                "J. Zhang",
                "M. Xu",
                "Q. Zhang",
                "Z. ZHENG",
                "S. Cheng",
                "Q. Tian",
                "X. Yang",
                "W. Zhang"
            ],
            "title": "Fine-grained video captioning via graph-based multi-granularity interaction learning",
            "venue": "IEEE transactions on pattern analysis and machine intelligence, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "P. Li",
                "P. Zhang",
                "X. Xu"
            ],
            "title": "Graph convolutional network meta-learning with multi-granularity pos guidance for video captioning",
            "venue": "Neurocomputing, vol. 472, pp. 294\u2013305, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "F. Wu",
                "A. Souza",
                "T. Zhang",
                "C. Fifty",
                "T. Yu",
                "K. Weinberger"
            ],
            "title": "Simplifying graph convolutional networks",
            "venue": "International conference on machine learning, 2019, pp. 6861\u20136871.",
            "year": 2019
        },
        {
            "authors": [
                "Z. Zhang",
                "D. Xu",
                "W. Ouyang",
                "L. Zhou"
            ],
            "title": "Dense video captioning using graph-based sentence summarization",
            "venue": "IEEE Transactions on Multimedia, vol. 23, pp. 1799\u20131810, 2020.",
            "year": 1810
        },
        {
            "authors": [
                "Z. Zhang",
                "Y. Shi",
                "C. Yuan",
                "B. Li",
                "P. Wang",
                "W. Hu",
                "Z.- J. Zha"
            ],
            "title": "Object relational graph with teacher-recommended learning for video captioning",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp. 13 278\u201313 288.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Bai",
                "J. Wang",
                "Y. Long",
                "B. Hu",
                "Y. Song",
                "M. Pagnucco",
                "Y. Guan"
            ],
            "title": "Discriminative latent semantic graph for video captioning",
            "venue": "Proceedings of the 29th ACM International Conference on Multimedia, 2021, pp. 3556\u20133564.",
            "year": 2021
        },
        {
            "authors": [
                "H. Wang",
                "G. Lin",
                "S.C. Hoi",
                "C. Miao"
            ],
            "title": "Cross-modal graph with meta concepts for video captioning",
            "venue": "arXiv preprint arXiv:2108.06458, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Z. Yan",
                "Y. Chen",
                "J. Song",
                "J. Zhu"
            ],
            "title": "Multimodal feature fusion based on object relation for video captioning",
            "venue": "CAAI Transactions on Intelligence Technology, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "L. Li",
                "X. Gao",
                "J. Deng",
                "Y. Tu",
                "Z.-J. Zha",
                "Q. Huang"
            ],
            "title": "Long short-term relation transformer with global gating for video captioning",
            "venue": "IEEE Transactions on Image Processing, vol. 31, pp. 2726\u20132738, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Wang",
                "G. Huang",
                "L. Yuming",
                "H. Yuan",
                "C.-M. Pun",
                "W.-K. Ling",
                "L. Cheng"
            ],
            "title": "Mivcn: Multimodal interaction video captioning network based on semantic association graph",
            "venue": "Applied Intelligence, vol. 52, no. 5, pp. 5241\u20135260, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "S. Zhang",
                "H. Tong",
                "J. Xu",
                "R. Maciejewski"
            ],
            "title": "Graph convolutional networks: a comprehensive review",
            "venue": "Computational Social Networks, vol. 6, no. 1, pp. 1\u201323, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "K. Xu",
                "W. Hu",
                "J. Leskovec",
                "S. Jegelka"
            ],
            "title": "How powerful are graph neural networks?",
            "venue": "arXiv preprint arXiv:1810.00826,",
            "year": 2018
        },
        {
            "authors": [
                "S. Padakandla"
            ],
            "title": "A survey of reinforcement learning algorithms for dynamically varying environments",
            "venue": "ACM Computing Surveys (CSUR), vol. 54, no. 6, pp. 1\u201325, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "I. Goodfellow"
            ],
            "title": "Deep learning-ian goodfellow, yoshua bengio, aaron courville- google books",
            "venue": "2016. 37",
            "year": 2016
        },
        {
            "authors": [
                "R.S. Sutton",
                "A.G. Barto"
            ],
            "title": "Reinforcement learning: An introduction",
            "venue": "MIT press,",
            "year": 2018
        },
        {
            "authors": [
                "R. Pasunuru",
                "M. Bansal"
            ],
            "title": "Reinforced video captioning with entailment rewards",
            "venue": "arXiv preprint arXiv:1708.02300, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "S. Phan",
                "G.E. Henter",
                "Y. Miyao",
                "S. Satoh"
            ],
            "title": "Consensusbased sequence training for video captioning",
            "venue": "arXiv preprint arXiv:1712.09532, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "X. Wang",
                "W. Chen",
                "J. Wu",
                "Y.-F. Wang",
                "W.Y. Wang"
            ],
            "title": "Video captioning via hierarchical reinforcement learning",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 4213\u20134222.",
            "year": 2018
        },
        {
            "authors": [
                "L. Li",
                "B. Gong"
            ],
            "title": "End-to-end video captioning with multitask reinforcement learning",
            "venue": "2019 IEEE winter conference on applications of computer vision (WACV). IEEE, 2019, pp. 339\u2013348.",
            "year": 2019
        },
        {
            "authors": [
                "W. Zhang",
                "B. Wang",
                "L. Ma",
                "W. Liu"
            ],
            "title": "Reconstruct and represent video contents for captioning via reinforcement learning",
            "venue": "IEEE transactions on pattern analysis and machine intelligence, vol. 42, no. 12, pp. 3088\u20133101, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "C. Dong",
                "X. Chen",
                "A. Chen",
                "F. Hu",
                "Z. Wang",
                "X. Li"
            ],
            "title": "Multilevel visual representation with semantic-reinforced learning for video captioning",
            "venue": "Proceedings of the 29th ACM International Conference on Multimedia, 2021, pp. 4750\u20134754.",
            "year": 2021
        },
        {
            "authors": [
                "P. SV",
                "A. Bharadwaj",
                "H. Raj",
                "J. Dadhania",
                "N. Pareek",
                "S. Prasanna"
            ],
            "title": "Exploration of visual features and their weighted-additive fusion for video captioning",
            "venue": "arXiv preprint arXiv:2101.05806, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "R.J. Williams"
            ],
            "title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning",
            "venue": "Machine learning, vol. 8, no. 3, pp. 229\u2013256, 1992.",
            "year": 1992
        },
        {
            "authors": [
                "M. Ranzato",
                "S. Chopra",
                "M. Auli",
                "W. Zaremba"
            ],
            "title": "Sequence level training with recurrent neural networks",
            "venue": "arXiv preprint arXiv:1511.06732, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "A.P. Parikh",
                "O. T\u00e4ckstr\u00f6m",
                "D. Das",
                "J. Uszkoreit"
            ],
            "title": "A decomposable attention model for natural language inference",
            "venue": "arXiv preprint arXiv:1606.01933, 2016.",
            "year": 1933
        },
        {
            "authors": [
                "X. Shi",
                "J. Cai",
                "S. Joty",
                "J. Gu"
            ],
            "title": "Watch it twice: Video captioning with a refocused video encoder",
            "venue": "Proceedings of the 27th ACM International Conference on Multimedia, 2019, pp. 818\u2013826.",
            "year": 2019
        },
        {
            "authors": [
                "W. Xu",
                "J. Yu",
                "Z. Miao",
                "L. Wan",
                "Y. Tian",
                "Q. Ji"
            ],
            "title": "Deep reinforcement polishing network for video captioning",
            "venue": "IEEE Transactions on Multimedia, vol. 23, pp. 1772\u20131784, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "T. Qian",
                "X. Mei",
                "P. Xu",
                "K. Ge",
                "Z. Qi"
            ],
            "title": "Filtration network: A frame sampling strategy via deep reinforcement learning for video captioning",
            "venue": "Journal of Intelligent & Fuzzy Systems, no. Preprint, pp. 1\u201313, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "I. Goodfellow",
                "J. Pouget-Abadie",
                "M. Mirza",
                "B. Xu",
                "D. Warde- Farley",
                "S. Ozair",
                "A. Courville",
                "Y. Bengio"
            ],
            "title": "Generative adversarial nets",
            "venue": "Advances in neural information processing systems, vol. 27, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "H. Emami",
                "M.M. Aliabadi",
                "M. Dong",
                "R.B. Chinnam"
            ],
            "title": "Spa-gan: Spatial attention gan for image-to-image translation",
            "venue": "IEEE Transactions on Multimedia, vol. 23, pp. 391\u2013401, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "H. Zhang",
                "T. Xu",
                "H. Li",
                "S. Zhang",
                "X. Wang",
                "X. Huang",
                "D.N. Metaxas"
            ],
            "title": "Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks",
            "venue": "Proceedings of the IEEE international conference on computer vision, 2017, pp. 5907\u20135915.",
            "year": 2017
        },
        {
            "authors": [
                "M. Frid-Adar",
                "I. Diamant",
                "E. Klang",
                "M. Amitai",
                "J. Goldberger",
                "H. Greenspan"
            ],
            "title": "Gan-based synthetic medical image augmentation for increased cnn performance in liver lesion classification",
            "venue": "Neurocomputing, vol. 321, pp. 321\u2013331, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "Y. Zhang",
                "S. Song",
                "E. Yumer",
                "M. Savva",
                "J.-Y. Lee",
                "H. Jin",
                "T. Funkhouser"
            ],
            "title": "Physically-based rendering for indoor scene understanding using convolutional neural networks",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 5287\u20135295.",
            "year": 2017
        },
        {
            "authors": [
                "N. Bayat",
                "V.R. Khazaie",
                "Y. Mohsenzadeh"
            ],
            "title": "Inverse mapping of face gans",
            "venue": "arXiv preprint arXiv:2009.05671, 2020.",
            "year": 2009
        },
        {
            "authors": [
                "Z. Hu",
                "J.T. Wang"
            ],
            "title": "Generative adversarial networks for video prediction with action control",
            "venue": "International Joint Conference on Artificial Intelligence. Springer, 2019, pp. 87\u2013 105.",
            "year": 2019
        },
        {
            "authors": [
                "J. Wu",
                "C. Zhang",
                "T. Xue",
                "B. Freeman",
                "J. Tenenbaum"
            ],
            "title": "Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling",
            "venue": "Advances in neural information processing systems, vol. 29, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "M. Mirza",
                "S. Osindero"
            ],
            "title": "Conditional generative adversarial nets",
            "venue": "arXiv preprint arXiv:1411.1784, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "L. Yu",
                "W. Zhang",
                "J. Wang",
                "Y. Yu"
            ],
            "title": "Seqgan: Sequence generative adversarial nets with policy gradient",
            "venue": "Proceedings of the AAAI conference on artificial intelligence, vol. 31, no. 1, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "F. Juefei-Xu",
                "R. Dey",
                "V.N. Boddeti",
                "M. Savvides"
            ],
            "title": "Rankgan: a maximum margin ranking gan for generating faces",
            "venue": "Computer Vision\u2013ACCV 2018: 14th Asian Conference on Computer Vision, Perth, Australia, December 2\u20136, 2018, Revised Selected Papers, Part III 14. Springer, 2019, pp. 3\u201318.",
            "year": 2018
        },
        {
            "authors": [
                "J. Hou",
                "X. Ding",
                "J.D. Deng"
            ],
            "title": "Semi-supervised semantic segmentation of vessel images using leaking perturbations",
            "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2022, pp. 2625\u20132634.",
            "year": 2022
        },
        {
            "authors": [
                "Q. Zhang",
                "Y. Song",
                "Q. Jin"
            ],
            "title": "Unifying event detection and captioning as sequence generation via pre-training",
            "venue": "arXiv preprint arXiv:2207.08625, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "F. Husz\u00e1r"
            ],
            "title": "How (not) to train your generative model: Scheduled sampling, likelihood, adversary?",
            "venue": "arXiv preprint arXiv:1511.05101,",
            "year": 2015
        },
        {
            "authors": [
                "B. Dai",
                "S. Fidler",
                "R. Urtasun",
                "D. Lin"
            ],
            "title": "Towards diverse and natural image descriptions via a conditional gan",
            "venue": "Proceedings of the IEEE international conference on computer vision, 2017, pp. 2970\u20132979.",
            "year": 2017
        },
        {
            "authors": [
                "Y. Yang",
                "J. Zhou",
                "J. Ai",
                "Y. Bin",
                "A. Hanjalic",
                "H.T. Shen",
                "Y. Ji"
            ],
            "title": "Video captioning by adversarial lstm",
            "venue": "IEEE Transactions on Image Processing, vol. 27, no. 11, pp. 5600\u20135611, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "D.C. Luvizon",
                "H. Tabia",
                "D. Picard"
            ],
            "title": "Human pose regression by combining indirect part detection and contextual information",
            "venue": "Computers & Graphics, vol. 85, pp. 15\u201322, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "T. Salimans",
                "I. Goodfellow",
                "W. Zaremba",
                "V. Cheung",
                "A. Radford",
                "X. Chen"
            ],
            "title": "Improved techniques for training gans",
            "venue": "Advances in neural information processing systems, vol. 29, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "Y. Pan",
                "Z. Qiu",
                "T. Yao",
                "H. Li",
                "T. Mei"
            ],
            "title": "To create what you tell: Generating videos from captions",
            "venue": "Proceedings of the 25th ACM international conference on Multimedia, 2017, pp. 1789\u20131798.",
            "year": 2017
        },
        {
            "authors": [
                "F. Zhu",
                "J.-N. Hwang",
                "Z. Ma",
                "G. Chen",
                "J. Guo"
            ],
            "title": "Understanding objects in video: Object-oriented video captioning via structured trajectory and adversarial learning",
            "venue": "IEEE Access, vol. 8, pp. 169 146\u2013169 159, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "W. Yan",
                "L. Qi",
                "Y. Tie",
                "C. Jin"
            ],
            "title": "Video captioning via two-stage attention model and generative adversarial network",
            "venue": "2021 8th International Conference on Computational Science/Intelligence and Applied Informatics (CSII). IEEE, 2021, pp. 6\u201311.",
            "year": 2021
        },
        {
            "authors": [
                "H. Xiao",
                "J. Shi"
            ],
            "title": "Diverse video captioning through latent variable expansion",
            "venue": "Pattern Recognition Letters, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "D. Silver",
                "A. Huang",
                "C.J. Maddison",
                "A. Guez",
                "L. Sifre",
                "G. Van Den Driessche",
                "J. Schrittwieser",
                "I. Antonoglou",
                "V. Panneershelvam",
                "M. Lanctot"
            ],
            "title": "Mastering the game of go with deep neural networks and tree search",
            "venue": "nature, vol. 529, no. 7587, pp. 484\u2013489, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "X. Hua",
                "X. Wang",
                "T. Rui",
                "F. Shao",
                "D. Wang"
            ],
            "title": "Adversarial reinforcement learning with object-scene relational graph for video captioning",
            "venue": "IEEE Transactions on Image Processing, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "H. Munusamy"
            ],
            "title": "Video captioning using semantically contextual generative adversarial network",
            "venue": "Computer Vision and Image Understanding, p. 103453, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "C.C. Shekhar"
            ],
            "title": "Domain-specific semantics guided approach to video captioning",
            "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2020, pp. 1587\u20131596.",
            "year": 2020
        },
        {
            "authors": [
                "T. Mikolov",
                "K. Chen",
                "G. Corrado",
                "J. Dean"
            ],
            "title": "Efficient estimation of word representations in vector space",
            "venue": "arXiv preprint arXiv:1301.3781, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "S. Gella",
                "M. Lewis",
                "M. Rohrbach"
            ],
            "title": "A dataset for telling the stories of social media videos.",
            "venue": "in EMNLP,",
            "year": 2018
        },
        {
            "authors": [
                "J. Kasai",
                "J. Cross",
                "M. Ghazvininejad",
                "J. Gu"
            ],
            "title": "Nonautoregressive machine translation with disentangled context 38 transformer",
            "venue": "International Conference on Machine Learning. PMLR, 2020, pp. 5144\u20135155.",
            "year": 2020
        },
        {
            "authors": [
                "B. Yang",
                "Y. Zou",
                "F. Liu",
                "C. Zhang"
            ],
            "title": "Nonautoregressive coarse-to-fine video captioning",
            "venue": "arXiv preprint arXiv:1911.12018, 2019.",
            "year": 1911
        },
        {
            "authors": [
                "B. Wang",
                "L. Ma",
                "W. Zhang",
                "W. Jiang",
                "J. Wang",
                "W. Liu"
            ],
            "title": "Controllable video captioning with pos sequence guidance based on gated fusion network",
            "venue": "Proceedings of the IEEE/CVF international conference on computer vision, 2019, pp. 2641\u20132650.",
            "year": 2019
        },
        {
            "authors": [
                "C. Shao",
                "J. Zhang",
                "Y. Feng",
                "F. Meng",
                "J. Zhou"
            ],
            "title": "Minimizing the bag-of-ngrams difference for non-autoregressive neural machine translation",
            "venue": "Proceedings of the AAAI conference on artificial intelligence, vol. 34, no. 01, 2020, pp. 198\u2013205.",
            "year": 2020
        },
        {
            "authors": [
                "J. Lee",
                "E. Mansimov",
                "K. Cho"
            ],
            "title": "Deterministic nonautoregressive neural sequence modeling by iterative refinement",
            "venue": "arXiv preprint arXiv:1802.06901, 2018.",
            "year": 1802
        },
        {
            "authors": [
                "M. Ghazvininejad",
                "O. Levy",
                "Y. Liu",
                "L. Zettlemoyer"
            ],
            "title": "Maskpredict: Parallel decoding of conditional masked language models",
            "venue": "arXiv preprint arXiv:1904.09324, 2019.",
            "year": 1904
        },
        {
            "authors": [
                "Y. Tian",
                "C. Guan",
                "J. Goodman",
                "M. Moore",
                "C. Xu"
            ],
            "title": "Audio-visual interpretable and controllable video captioning",
            "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition workshops, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "\u2014\u2014"
            ],
            "title": "An attempt towards interpretable audio-visual video captioning",
            "venue": "arXiv preprint arXiv:1812.02872, 2018.",
            "year": 1812
        },
        {
            "authors": [
                "S. Chen",
                "J. Chen",
                "Q. Jin"
            ],
            "title": "Generating video descriptions with topic guidance",
            "venue": "Proceedings of the 2017 ACM on International Conference on Multimedia Retrieval, 2017, pp. 5\u201313.",
            "year": 2017
        },
        {
            "authors": [
                "S. Chen",
                "J. Chen",
                "Q. Jin",
                "A. Hauptmann"
            ],
            "title": "Video captioning with guidance of multimodal latent topics",
            "venue": "Proceedings of the 25th ACM international conference on Multimedia, 2017, pp. 1838\u20131846.",
            "year": 2017
        },
        {
            "authors": [
                "X. Chen",
                "H. Fang",
                "T.-Y. Lin",
                "R. Vedantam",
                "S. Gupta",
                "P. Doll\u00e1r",
                "C.L. Zitnick"
            ],
            "title": "Microsoft coco captions: Data collection and evaluation server",
            "venue": "arXiv preprint arXiv:1504.00325, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "V. Ramanishka",
                "A. Das",
                "D.H. Park",
                "S. Venugopalan",
                "L.A. Hendricks",
                "M. Rohrbach",
                "K. Saenko"
            ],
            "title": "Multimodal video description",
            "venue": "Proceedings of the 24th ACM international conference on Multimedia, 2016, pp. 1092\u20131096.",
            "year": 2016
        },
        {
            "authors": [
                "Q. Jin",
                "J. Chen",
                "S. Chen",
                "Y. Xiong",
                "A. Hauptmann"
            ],
            "title": "Describing videos using multi-modal fusion",
            "venue": "Proceedings of the 24th ACM international conference on Multimedia, 2016, pp. 1087\u20131091.",
            "year": 2016
        },
        {
            "authors": [
                "P. Joshi",
                "C. Saharia",
                "V. Singh",
                "D. Gautam",
                "G. Ramakrishnan",
                "P. Jyothi"
            ],
            "title": "A tale of two modalities for video captioning",
            "venue": "2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW). IEEE, 2019, pp. 3708\u20133712.",
            "year": 2019
        },
        {
            "authors": [
                "S. Varma",
                "D.P. James"
            ],
            "title": "An efficient deep learning-based video captioning framework using multi-modal features",
            "venue": "Expert Systems, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Zheng",
                "Y. Zhang",
                "R. Feng",
                "T. Zhang",
                "W. Fan"
            ],
            "title": "Stacked multimodal attention network for context-aware video captioning",
            "venue": "IEEE Transactions on Circuits and Systems for Video Technology, vol. 32, no. 1, pp. 31\u201342, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "L. Gao",
                "Z. Guo",
                "H. Zhang",
                "X. Xu",
                "H.T. Shen"
            ],
            "title": "Video captioning with attention-based lstm and semantic consistency",
            "venue": "IEEE Transactions on Multimedia, vol. 19, no. 9, pp. 2045\u2013 2055, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "W. Pei",
                "J. Zhang",
                "X. Wang",
                "L. Ke",
                "X. Shen",
                "Y.-W. Tai"
            ],
            "title": "Memory-attended recurrent network for video captioning",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 8347\u20138356.",
            "year": 2019
        },
        {
            "authors": [
                "M. Qi",
                "Y. Wang",
                "A. Li",
                "J. Luo"
            ],
            "title": "Sports video captioning by attentive motion representation based hierarchical recurrent neural networks",
            "venue": "Proceedings of the 1st International Workshop on Multimedia Content Analysis in Sports, 2018, pp. 77\u2013 85.",
            "year": 2018
        },
        {
            "authors": [
                "H. Xu",
                "B. Li",
                "V. Ramanishka",
                "L. Sigal",
                "K. Saenko"
            ],
            "title": "Joint event detection and description in continuous video streams",
            "venue": "2019 IEEE winter conference on applications of computer vision (WACV). IEEE, 2019, pp. 396\u2013405.",
            "year": 2019
        },
        {
            "authors": [
                "M. Suin",
                "A. Rajagopalan"
            ],
            "title": "An efficient framework for dense video captioning",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, no. 07, 2020, pp. 12 039\u201312 046.",
            "year": 2020
        },
        {
            "authors": [
                "J. Wang",
                "W. Jiang",
                "L. Ma",
                "W. Liu",
                "Y. Xu"
            ],
            "title": "Bidirectional attentive fusion with context gating for dense video captioning",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 7190\u20137198.",
            "year": 2018
        },
        {
            "authors": [
                "L. Zhou",
                "Y. Zhou",
                "J.J. Corso",
                "R. Socher",
                "C. Xiong"
            ],
            "title": "Endto-end dense video captioning with masked transformer",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 8739\u20138748.",
            "year": 2018
        },
        {
            "authors": [
                "C.-H. Lu",
                "G.-Y. Fan"
            ],
            "title": "Environment-aware dense video captioning for iot-enabled edge cameras",
            "venue": "IEEE Internet of Things Journal, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "W. Zhu",
                "B. Pang",
                "A. Thapliyal",
                "W.Y. Wang",
                "R. Soricut"
            ],
            "title": "End-to-end dense video captioning as sequence generation",
            "venue": "arXiv preprint arXiv:2204.08121, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Li",
                "T. Yao",
                "Y. Pan",
                "H. Chao",
                "T. Mei"
            ],
            "title": "Jointly localizing and describing events for dense video captioning",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 7492\u20137500.",
            "year": 2018
        },
        {
            "authors": [
                "T. Wang",
                "H. Zheng",
                "M. Yu",
                "Q. Tian",
                "H. Hu"
            ],
            "title": "Event-centric hierarchical representation for dense video captioning",
            "venue": "IEEE Transactions on Circuits and Systems for Video Technology, vol. 31, no. 5, pp. 1890\u20131900, 2020.",
            "year": 1890
        },
        {
            "authors": [
                "A. Chadha",
                "G. Arora",
                "N. Kaloty"
            ],
            "title": "iperceive: Applying common-sense reasoning to multi-modal dense video captioning and video question answering",
            "venue": "arXiv preprint arXiv:2011.07735, 2020.",
            "year": 2011
        },
        {
            "authors": [
                "B. Wu",
                "G. Niu",
                "J. Yu",
                "X. Xiao",
                "J. Zhang",
                "H. Wu"
            ],
            "title": "Weakly supervised dense video captioning via jointly usage of knowledge distillation and cross-modal matching",
            "venue": "arXiv preprint arXiv:2105.08252, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "S. Lee",
                "I. Kim"
            ],
            "title": "Dvc-net: A deep neural network model for dense video captioning",
            "venue": "IET Computer Vision, vol. 15, no. 1, pp. 12\u201323, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "T. Wang",
                "Z. Liu",
                "F. Zheng",
                "Z. Lu",
                "R. Cheng",
                "P. Luo"
            ],
            "title": "Semantic-aware pretraining for dense video captioning",
            "venue": "arXiv preprint arXiv:2204.07449, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "H. Yu",
                "J. Wang",
                "Z. Huang",
                "Y. Yang",
                "W. Xu"
            ],
            "title": "Video paragraph captioning using hierarchical recurrent neural networks",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 4584\u20134593.",
            "year": 2016
        },
        {
            "authors": [
                "S. Sah",
                "S. Kulhare",
                "A. Gray",
                "S. Venugopalan",
                "E. Prud\u2019Hommeaux",
                "R. Ptucha"
            ],
            "title": "Semantic text summarization of long videos",
            "venue": "2017 IEEE Winter Conference on Applications of Computer Vision (WACV). IEEE, 2017, pp. 989\u2013997.",
            "year": 2017
        },
        {
            "authors": [
                "J.S. Park",
                "M. Rohrbach",
                "T. Darrell",
                "A. Rohrbach"
            ],
            "title": "Adversarial inference for multi-sentence video description",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 6598\u20136608.",
            "year": 2019
        },
        {
            "authors": [
                "H. Liu",
                "X. Wan"
            ],
            "title": "Video paragraph captioning as a text summarization task",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), 2021, pp. 55\u201360.",
            "year": 2021
        },
        {
            "authors": [
                "K. Yamazaki",
                "S. Truong",
                "K. Vo",
                "M. Kidd",
                "C. Rainwater",
                "K. Luu",
                "N. Le"
            ],
            "title": "Vlcap: Vision-language with contrastive learning for coherent video paragraph captioning",
            "venue": "arXiv preprint arXiv:2206.12972, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Q. Li",
                "T. Li",
                "H. Wang",
                "C.W. Chen"
            ],
            "title": "Taking an emotional look at video paragraph captioning",
            "venue": "arXiv preprint arXiv:2203.06356, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "K. Zhang",
                "Y. Li",
                "J. Wang",
                "E. Cambria",
                "X. Li"
            ],
            "title": "Realtime video emotion recognition based on reinforcement learning and domain knowledge",
            "venue": "IEEE Transactions on Circuits and Systems for Video Technology, vol. 32, no. 3, pp. 1034\u20131047, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "B. Huber",
                "D. McDuff",
                "C. Brockett",
                "M. Galley",
                "B. Dolan"
            ],
            "title": "Emotional dialogue generation using image-grounded language models",
            "venue": "Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems, 2018, pp. 1\u201312.",
            "year": 2018
        },
        {
            "authors": [
                "V. Karpukhin",
                "B. O\u011fuz",
                "S. Min",
                "P. Lewis",
                "L. Wu",
                "S. Edunov",
                "D. Chen",
                "W.-t. Yih"
            ],
            "title": "Dense passage retrieval for opendomain question answering",
            "venue": "arXiv preprint arXiv:2004.04906, 2020.",
            "year": 2004
        },
        {
            "authors": [
                "K. Guu",
                "K. Lee",
                "Z. Tung",
                "P. Pasupat",
                "M.-W. Chang"
            ],
            "title": "Realm: Retrieval-augmented language model pre-training",
            "venue": "arXiv preprint arXiv:2002.08909, 2020. 39",
            "year": 2002
        },
        {
            "authors": [
                "P. Lewis",
                "E. Perez",
                "A. Piktus",
                "F. Petroni",
                "V. Karpukhin",
                "N. Goyal",
                "H. K\u00fcttler",
                "M. Lewis",
                "W.-t. Yih",
                "T. Rockt\u00e4schel"
            ],
            "title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "venue": "Advances in Neural Information Processing Systems, vol. 33, pp. 9459\u20139474, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "S. Kim",
                "A. Gholami",
                "Z. Yao",
                "M.W. Mahoney",
                "K. Keutzer"
            ],
            "title": "I-bert: Integer-only bert quantization",
            "venue": "International conference on machine learning. PMLR, 2021, pp. 5506\u20135518.",
            "year": 2021
        },
        {
            "authors": [
                "K. Lee",
                "M.-W. Chang",
                "K. Toutanova"
            ],
            "title": "Latent retrieval for weakly supervised open domain question answering",
            "venue": "arXiv preprint arXiv:1906.00300, 2019.",
            "year": 1906
        },
        {
            "authors": [
                "Z. Yang",
                "Y. Han",
                "Z. Wang"
            ],
            "title": "Catching the temporal regionsof-interest for video captioning",
            "venue": "Proceedings of the 25th ACM international conference on Multimedia, 2017, pp. 146\u2013 153.",
            "year": 2017
        },
        {
            "authors": [
                "P. Tang",
                "H. Wang",
                "H. Wang",
                "K. Xu"
            ],
            "title": "Richer semantic visual and language representation for video captioning",
            "venue": "Proceedings of the 25th ACM international conference on Multimedia, 2017, pp. 1871\u20131876.",
            "year": 2017
        },
        {
            "authors": [
                "J. Song",
                "Z. Guo",
                "L. Gao",
                "W. Liu",
                "D. Zhang",
                "H.T. Shen"
            ],
            "title": "Hierarchical lstm with adjusted temporal attention for video captioning",
            "venue": "arXiv preprint arXiv:1706.01231, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "B. Wang",
                "L. Ma",
                "W. Zhang",
                "W. Liu"
            ],
            "title": "Reconstruction network for video captioning",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 7622\u20137631.",
            "year": 2018
        },
        {
            "authors": [
                "B. Zhao",
                "X. Li",
                "X. Lu"
            ],
            "title": "Video captioning with tube features",
            "venue": "IJCAI, 2018, pp. 1177\u20131183.",
            "year": 2018
        },
        {
            "authors": [
                "E. Daskalakis",
                "M. Tzelepi",
                "A. Tefas"
            ],
            "title": "Learning deep spatiotemporal features for video captioning",
            "venue": "Pattern Recognition Letters, vol. 116, pp. 143\u2013149, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "S. Lee",
                "I. Kim"
            ],
            "title": "Multimodal feature learning for video captioning",
            "venue": "Mathematical Problems in Engineering, vol. 2018, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "J. Mun",
                "L. Yang",
                "Z. Ren",
                "N. Xu",
                "B. Han"
            ],
            "title": "Streamlined dense video captioning",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 6588\u20136597.",
            "year": 2019
        },
        {
            "authors": [
                "P. Tang",
                "H. Wang",
                "Q. Li"
            ],
            "title": "Rich visual and language representation with complementary semantics for video captioning",
            "venue": "ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM), vol. 15, no. 2, pp. 1\u201323, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "S. Chen",
                "Y.-G. Jiang"
            ],
            "title": "Motion guided spatial attention for video captioning",
            "venue": "Proceedings of the AAAI conference on artificial intelligence, vol. 33, no. 01, 2019, pp. 8191\u20138198.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Hu",
                "Z. Chen",
                "Z.-J. Zha",
                "F. Wu"
            ],
            "title": "Hierarchical global-local temporal modeling for video captioning",
            "venue": "Proceedings of the 27th ACM International Conference on Multimedia, 2019, pp. 774\u2013783.",
            "year": 2019
        },
        {
            "authors": [
                "G. Huang",
                "B. Pang",
                "Z. Zhu",
                "C. Rivera",
                "R. Soricut"
            ],
            "title": "Multimodal pretraining for dense video captioning",
            "venue": "arXiv preprint arXiv:2011.11760, 2020.",
            "year": 2011
        },
        {
            "authors": [
                "H. Akbari",
                "H. Palangi",
                "J. Yang",
                "S. Rao",
                "A. Celikyilmaz",
                "R. Fernandez",
                "P. Smolensky",
                "J. Gao",
                "S.-F. Chang"
            ],
            "title": "Neurosymbolic representations for video captioning: A case for leveraging inductive biases for vision and language",
            "venue": "arXiv preprint arXiv:2011.09530, 2020.",
            "year": 2011
        },
        {
            "authors": [
                "B. Shi",
                "L. Ji",
                "Z. Niu",
                "N. Duan",
                "M. Zhou",
                "X. Chen"
            ],
            "title": "Learning semantic concepts and temporal alignment for narrated video procedural captioning",
            "venue": "Proceedings of the 28th ACM International Conference on Multimedia, 2020, pp. 4355\u20134363.",
            "year": 2020
        },
        {
            "authors": [
                "V. Iashin",
                "E. Rahtu"
            ],
            "title": "Multi-modal dense video captioning",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, 2020, pp. 958\u2013959.",
            "year": 2020
        },
        {
            "authors": [
                "K. Lin",
                "Z. Gan",
                "L. Wang"
            ],
            "title": "Semi-supervised learning for video captioning",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, 2020, pp. 1096\u20131106.",
            "year": 2020
        },
        {
            "authors": [
                "H. Xiao",
                "J. Xu",
                "J. Shi"
            ],
            "title": "Exploring diverse and fine-grained caption for video by incorporating convolutional architecture into lstm-based model",
            "venue": "Pattern Recognition Letters, vol. 129, pp. 173\u2013180, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "A. Wu",
                "Y. Han"
            ],
            "title": "Hierarchical memory decoding for video captioning",
            "venue": "arXiv preprint arXiv:2002.11886, 2020.",
            "year": 2002
        },
        {
            "authors": [
                "J. Hou",
                "Y. Jia",
                "Y. Qi"
            ],
            "title": "Video captioning using weak annotation",
            "venue": "arXiv preprint arXiv:2009.01067, 2020.",
            "year": 2009
        },
        {
            "authors": [
                "E. Boran",
                "A. Erdem",
                "N. Ikizler-Cinbis",
                "E. Erdem",
                "P. Madhyastha",
                "L. Specia"
            ],
            "title": "Leveraging auxiliary image descriptions for dense video captioning",
            "venue": "Pattern Recognition Letters, vol. 146, pp. 70\u201376, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "N. Aafaq",
                "A. Mian",
                "W. Liu",
                "N. Akhtar",
                "M. Shah"
            ],
            "title": "Crossdomain modality fusion for dense video captioning",
            "venue": "IEEE Transactions on Artificial Intelligence, vol. 1, no. 01, pp. 1\u20131, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Z. Sun",
                "X. Zhong",
                "S. Chen",
                "L. Li",
                "L. Zhong"
            ],
            "title": "Visual-aware attention dual-stream decoder for video captioning",
            "venue": "arXiv preprint arXiv:2110.08578, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "V. Estevam",
                "R. Laroca",
                "H. Pedrini",
                "D. Menotti"
            ],
            "title": "Dense video captioning using unsupervised semantic information",
            "venue": "arXiv preprint arXiv:2112.08455, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "F. Liu",
                "X. Wu",
                "C. You",
                "S. Ge",
                "Y. Zou",
                "X. Sun"
            ],
            "title": "Aligning source visual and target language domains for unpaired video captioning",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "M. Zhu"
            ],
            "title": "Video captioning in compressed video",
            "venue": "2021 6th International Conference on Image, Vision and Computing (ICIVC). IEEE, 2021, pp. 336\u2013341.",
            "year": 2021
        },
        {
            "authors": [
                "W. Ji",
                "R. Wang"
            ],
            "title": "A multi-instance multi-label dual learning approach for video captioning",
            "venue": "ACM Transactions on Multimidia Computing Communications and Applications, vol. 17, no. 2s, pp. 1\u201318, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "X. Zhang",
                "C. Liu",
                "F. Chang"
            ],
            "title": "Guidance module network for video captioning",
            "venue": "2021 40th Chinese Control Conference (CCC). IEEE, 2021, pp. 7955\u20137959.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Bin",
                "X. Shang",
                "B. Peng",
                "Y. Ding",
                "T.-S. Chua"
            ],
            "title": "Multiperspective video captioning",
            "venue": "Proceedings of the 29th ACM International Conference on Multimedia, 2021, pp. 5110\u20135118.",
            "year": 2021
        },
        {
            "authors": [
                "L. Ji",
                "R. Tu",
                "K. Lin",
                "L. Wang",
                "N. Duan"
            ],
            "title": "Multimodal graph neural network for video procedural captioning",
            "venue": "Neurocomputing, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "W. Ji",
                "R. Wang",
                "Y. Tian",
                "X. Wang"
            ],
            "title": "An attention based dual learning approach for video captioning",
            "venue": "Applied Soft Computing, vol. 117, p. 108332, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "P.H. Seo",
                "A. Nagrani",
                "A. Arnab",
                "C. Schmid"
            ],
            "title": "End-toend generative pretraining for multimodal video captioning",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 17 959\u201317 968.",
            "year": 2022
        },
        {
            "authors": [
                "X. Duan",
                "W. Huang",
                "C. Gan",
                "J. Wang",
                "W. Zhu",
                "J. Huang"
            ],
            "title": "Weakly supervised dense event captioning in videos",
            "venue": "Advances in Neural Information Processing Systems, vol. 31, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "T. Rahman",
                "B. Xu",
                "L. Sigal"
            ],
            "title": "Watch, listen and tell: Multimodal weakly supervised dense event captioning",
            "venue": "Proceedings of the IEEE/CVF international conference on computer vision, 2019, pp. 8908\u20138917.",
            "year": 2019
        },
        {
            "authors": [
                "S. Chen",
                "W. Jiang",
                "W. Liu",
                "Y.-G. Jiang"
            ],
            "title": "Learning modality interaction for temporal sentence localization and event captioning in videos",
            "venue": "European Conference on Computer Vision. Springer, 2020, pp. 333\u2013351.",
            "year": 2020
        },
        {
            "authors": [
                "Z. Li",
                "X. Chang",
                "L. Yao",
                "S. Pan",
                "G. Zongyuan",
                "H. Zhang"
            ],
            "title": "Grounding visual concepts for zero-shot event detection and event captioning",
            "venue": "Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2020, pp. 297\u2013305.",
            "year": 2020
        },
        {
            "authors": [
                "T. Wang",
                "H. Zheng",
                "M. Yu"
            ],
            "title": "Dense-captioning events in videos: Sysu submission to activitynet challenge 2020",
            "venue": "arXiv preprint arXiv:2006.11693, 2020.",
            "year": 2006
        },
        {
            "authors": [
                "L. Ji",
                "X. Guo",
                "H. Huang",
                "X. Chen"
            ],
            "title": "Hierarchical contextaware network for dense video event captioning",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 2021, pp. 2004\u20132013.",
            "year": 2021
        },
        {
            "authors": [
                "J. Redmon",
                "A. Farhadi"
            ],
            "title": "Yolov3: An incremental improvement",
            "venue": "arXiv preprint arXiv:1804.02767, 2018.",
            "year": 1804
        },
        {
            "authors": [
                "D.M. Chan",
                "S. Vijayanarasimhan",
                "D.A. Ross",
                "J.F. Canny"
            ],
            "title": "Active learning for video description with cluster-regularized ensemble ranking",
            "venue": "Proceedings of the Asian Conference on Computer Vision, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "L. Baraldi",
                "C. Grana",
                "R. Cucchiara"
            ],
            "title": "Hierarchical boundary-aware neural encoder for video captioning",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 1657\u20131666.",
            "year": 2017
        },
        {
            "authors": [
                "B. Sun",
                "Y. Wu",
                "Y. Zhao",
                "Z. Hao",
                "L. Yu",
                "J. He"
            ],
            "title": "Crosslanguage multimodal scene semantic guidance and leap sam- 40 pling for video captioning",
            "venue": "The Visual Computer, pp. 1\u201317, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "P. Tang",
                "Y. Tan",
                "W. Luo"
            ],
            "title": "Visual and language semantic hybrid enhancement and complementary for video description",
            "venue": "Neural Computing and Applications, pp. 1\u201319, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "N. Aafaq",
                "N. Akhtar",
                "W. Liu",
                "S.Z. Gilani",
                "A. Mian"
            ],
            "title": "Spatio-temporal dynamics and semantic attribute enriched visual encoding for video captioning",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 12 487\u201312 496.",
            "year": 2019
        },
        {
            "authors": [
                "X. Man",
                "D. Ouyang",
                "X. Li",
                "J. Song",
                "J. Shao"
            ],
            "title": "Scenarioaware recurrent transformer for goal-directed video captioning",
            "venue": "ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM), vol. 18, no. 4, pp. 1\u201317, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "H.-R. Huang",
                "D.-Y. Hong",
                "J.-J. Wu",
                "K.-F. Chen",
                "P. Liu",
                "W.-C. Hsu"
            ],
            "title": "Accelerating video captioning on heterogeneous system architectures",
            "venue": "ACM Transactions on Architecture and Code Optimization (TACO), 2022.",
            "year": 2022
        },
        {
            "authors": [
                "M. Hosseinzadeh",
                "Y. Wang"
            ],
            "title": "Video captioning of future frames",
            "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2021, pp. 980\u2013989.",
            "year": 2021
        },
        {
            "authors": [
                "L. Lebron",
                "Y. Graham",
                "K. McGuinness",
                "K. Kouramas",
                "N.E. O\u2019Connor"
            ],
            "title": "Bertha: Video captioning evaluation via transfer-learned human assessment",
            "venue": "arXiv preprint arXiv:2201.10243, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "A. Podlesnaya",
                "S. Podlesnyy"
            ],
            "title": "Deep learning based semantic video indexing and retrieval",
            "venue": "Proceedings of SAI intelligent systems conference. Springer, 2016, pp. 359\u2013372.",
            "year": 2016
        },
        {
            "authors": [
                "J. Perez-Martin",
                "B. Bustos",
                "J. P\u00e9rez"
            ],
            "title": "Improving video captioning with temporal composition of a visual-syntactic embedding",
            "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2021, pp. 3039\u20133049.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Huang",
                "H. Xue",
                "J. Chen",
                "H. Ma",
                "H. Ma"
            ],
            "title": "Semantic tag augmented xlanv model for video captioning",
            "venue": "Proceedings of the 29th ACM International Conference on Multimedia, 2021, pp. 4818\u20134822.",
            "year": 2021
        },
        {
            "authors": [
                "S. Ilyas",
                "H.U. Rehman"
            ],
            "title": "A deep learning based approach for precise video tagging",
            "venue": "2019 15th International Conference on Emerging Technologies (ICET). IEEE, 2019, pp. 1\u20136.",
            "year": 2019
        },
        {
            "authors": [
                "V. Garg",
                "V.A. Markhedkar",
                "S.S. Lale",
                "T. Raghunandan"
            ],
            "title": "Video tagging and recommender system using deep learning",
            "venue": "Innovations in Computational Intelligence and Computer Vision. Springer, 2021, pp. 302\u2013310.",
            "year": 2021
        },
        {
            "authors": [
                "J. Dong",
                "X. Li",
                "C. Xu",
                "S. Ji",
                "Y. He",
                "G. Yang",
                "X. Wang"
            ],
            "title": "Dual encoding for zero-example video retrieval",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 9346\u20139355.",
            "year": 2019
        },
        {
            "authors": [
                "X. Yang",
                "J. Dong",
                "Y. Cao",
                "X. Wang",
                "M. Wang",
                "T.- S. Chua"
            ],
            "title": "Tree-augmented cross-modal encoding for complexquery video retrieval",
            "venue": "Proceedings of the 43rd international ACM SIGIR conference on research and development in information retrieval, 2020, pp. 1339\u20131348.",
            "year": 2020
        },
        {
            "authors": [
                "T. Prathiba",
                "R. Kumari"
            ],
            "title": "Content based video retrieval system based on multimodal feature grouping by kfcm clustering algorithm to promote human\u2013computer interaction",
            "venue": "Journal of Ambient Intelligence and Humanized Computing, vol. 12, no. 6, pp. 6215\u20136229, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "A. Diba",
                "M. Fayyaz",
                "V. Sharma",
                "M. Paluri",
                "J. Gall",
                "R. Stiefelhagen",
                "L.V. Gool"
            ],
            "title": "Large scale holistic video understanding",
            "venue": "European Conference on Computer Vision. Springer, 2020, pp. 593\u2013610.",
            "year": 2020
        },
        {
            "authors": [
                "A. Arnab",
                "C. Sun",
                "C. Schmid"
            ],
            "title": "Unified graph structured models for video understanding",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 8117\u20138126.",
            "year": 2021
        },
        {
            "authors": [
                "Z. Li",
                "W. Wang",
                "Z. Li",
                "Y. Huang",
                "Y. Sato"
            ],
            "title": "Towards visually explaining video understanding networks with perturbation",
            "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2021, pp. 1120\u20131129.",
            "year": 2021
        },
        {
            "authors": [
                "Z. Huang",
                "S. Zhang",
                "L. Pan",
                "Z. Qing",
                "M. Tang",
                "Z. Liu",
                "M.H. Ang Jr"
            ],
            "title": "Tada: Temporally-adaptive convolutions for video understanding",
            "venue": "arXiv preprint arXiv:2110.06178, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Z. Zeng",
                "D. McDuff",
                "Y. Song"
            ],
            "title": "Contrastive learning of global and local video representations",
            "venue": "Advances in Neural Information Processing Systems, vol. 34, pp. 7025\u20137040, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "J. Brousseau",
                "J.-F. Beaumont",
                "G. Boulianne",
                "P. Cardinal",
                "C. Chapdelaine",
                "M. Comeau",
                "F. Osterrath",
                "P. Ouellet"
            ],
            "title": "Automated closed-captioning of live tv broadcast news in french",
            "venue": "Eighth European Conference on Speech Communication and Technology, 2003.",
            "year": 2003
        },
        {
            "authors": [
                "A. \u00c1lvarez",
                "C. Mendes",
                "M. Raffaelli",
                "T. L\u00fa\u0131s",
                "S. Paulo",
                "N. Piccinini",
                "H. Arzelus",
                "J. Neto",
                "C. Aliprandi",
                "A. Del Pozo"
            ],
            "title": "Automating live and batch subtitling of multimedia contents for several european languages",
            "venue": "Multimedia Tools and Applications, vol. 75, no. 18, pp. 10 823\u201310 853, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "T.H. Soe",
                "F. Guribye",
                "M. Slavkovik"
            ],
            "title": "Evaluating ai assisted subtitling",
            "venue": "ACM International Conference on Interactive Media Experiences, 2021, pp. 96\u2013107.",
            "year": 2021
        },
        {
            "authors": [
                "N.M. Guerreiro",
                "R. Rei",
                "F. Batista"
            ],
            "title": "Towards better subtitles: A multilingual approach for punctuation restoration of speech transcripts",
            "venue": "Expert Systems with Applications, vol. 186, p. 115740, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "P. Pareek",
                "A. Thakkar"
            ],
            "title": "A survey on video-based human action recognition: recent updates, datasets, challenges, and applications",
            "venue": "Artificial Intelligence Review, vol. 54, no. 3, pp. 2259\u20132322, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Zhi",
                "Z. Tong",
                "L. Wang",
                "G. Wu"
            ],
            "title": "Mgsampler: An explainable sampling strategy for video action recognition",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 1513\u20131522.",
            "year": 2021
        },
        {
            "authors": [
                "H. Kwon",
                "M. Kim",
                "S. Kwak",
                "M. Cho"
            ],
            "title": "Learning selfsimilarity in space and time as generalized motion for video action recognition",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 13 065\u2013 13 075.",
            "year": 2021
        },
        {
            "authors": [
                "C. Huang",
                "Z. Wu",
                "J. Wen",
                "Y. Xu",
                "Q. Jiang",
                "Y. Wang"
            ],
            "title": "Abnormal event detection using deep contrastive learning for intelligent video surveillance system",
            "venue": "IEEE Transactions on Industrial Informatics, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "W. Pang",
                "Q. He",
                "Y. Chen",
                "Y. Li"
            ],
            "title": "Fall event detection with global and temporal local information in real-world videos",
            "venue": "Multimedia Tools and Applications, pp. 1\u201314, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "N.A. Giudice",
                "G.E. Legge"
            ],
            "title": "Blind navigation and the role of technology",
            "venue": "The engineering handbook of smart technology for aging, disability, and independence, vol. 8, pp. 479\u2013500, 2008.",
            "year": 2008
        },
        {
            "authors": [
                "K. Lakshan"
            ],
            "title": "Blind navigation in outdoor environments: Head and torso level thin-structure based obstacle detection",
            "venue": "Ph.D. dissertation, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "W. Gunethilake"
            ],
            "title": "Blind navigation using deep learning-based obstacle detection",
            "venue": "Ph.D. dissertation, School of Computing, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Kong",
                "Y. Fu"
            ],
            "title": "Human action recognition and prediction: A survey",
            "venue": "arXiv preprint arXiv:1806.11230, 2018.",
            "year": 1806
        },
        {
            "authors": [
                "S. Cascianelli",
                "G. Costante",
                "T.A. Ciarfuglia",
                "P. Valigi",
                "M.L. Fravolini"
            ],
            "title": "Full-gru natural language video description for service robotics applications",
            "venue": "IEEE Robotics and Automation Letters, vol. 3, no. 2, pp. 841\u2013848, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "S.-H. Kang",
                "J.-H. Han"
            ],
            "title": "Video captioning based on both egocentric and exocentric views of robot vision for human-robot interaction",
            "venue": "International Journal of Social Robotics, pp. 1\u201311, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Zhu",
                "Y. Wu",
                "Y. Yang",
                "Y. Yan"
            ],
            "title": "Saying the unseen: Video descriptions via dialog agents",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "M. Elfeki",
                "L. Wang",
                "A. Borji"
            ],
            "title": "Multi-stream dynamic video summarization",
            "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2022, pp. 339\u2013349.",
            "year": 2022
        },
        {
            "authors": [
                "H. Yang",
                "B. Wang",
                "S. Lin",
                "D. Wipf",
                "M. Guo",
                "B. Guo"
            ],
            "title": "Unsupervised extraction of video highlights via robust recurrent auto-encoders",
            "venue": "Proceedings of the IEEE international conference on computer vision, 2015, pp. 4633\u20134641.",
            "year": 2015
        },
        {
            "authors": [
                "H.J. Zhang",
                "J. Wu",
                "D. Zhong",
                "S.W. Smoliar"
            ],
            "title": "An integrated system for content-based video retrieval and browsing",
            "venue": "Pattern recognition, vol. 30, no. 4, pp. 643\u2013658, 1997.",
            "year": 1997
        },
        {
            "authors": [
                "Y. Song",
                "J. Vallmitjana",
                "A. Stent",
                "A. Jaimes"
            ],
            "title": "Tvsum: Summarizing web videos using titles",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, 2015, pp. 5179\u20135187.",
            "year": 2015
        },
        {
            "authors": [
                "P. Li",
                "Q. Ye",
                "L. Zhang",
                "L. Yuan",
                "X. Xu",
                "L. Shao"
            ],
            "title": "Exploring global diverse attention via pairwise temporal relation for video summarization",
            "venue": "Pattern Recognition, vol. 111, p. 107677, 2021. 41",
            "year": 2021
        },
        {
            "authors": [
                "J. Dong",
                "X. Li",
                "C. Xu",
                "G. Yang",
                "X. Wang"
            ],
            "title": "Feature relearning with data augmentation for content-based video recommendation",
            "venue": "Proceedings of the 26th ACM international conference on Multimedia, 2018, pp. 2058\u20132062.",
            "year": 2018
        },
        {
            "authors": [
                "Y. Liu",
                "Q. Liu",
                "Y. Tian",
                "C. Wang",
                "Y. Niu",
                "Y. Song",
                "C. Li"
            ],
            "title": "Concept-aware denoising graph neural network for micro-video recommendation",
            "venue": "Proceedings of the 30th ACM International Conference on Information & Knowledge Management, 2021, pp. 1099\u20131108.",
            "year": 2021
        },
        {
            "authors": [
                "B. Lika",
                "K. Kolomvatsos",
                "S. Hadjiefthymiades"
            ],
            "title": "Facing the cold start problem in recommender systems",
            "venue": "Expert Systems with Applications, vol. 41, no. 4, pp. 2065\u20132073, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "S. Venugopalan",
                "L.A. Hendricks",
                "R. Mooney",
                "K. Saenko"
            ],
            "title": "Improving lstm-based video description with linguistic knowledge mined from text",
            "venue": "arXiv preprint arXiv:1604.01729, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "Z. Gan",
                "L. Li",
                "C. Li",
                "L. Wang",
                "Z. Liu",
                "J. Gao"
            ],
            "title": "Visionlanguage pre-training: Basics, recent advances, and future trends",
            "venue": "arXiv preprint arXiv:2210.09263, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "M. Abdar",
                "F. Pourpanah",
                "S. Hussain",
                "D. Rezazadegan",
                "L. Liu",
                "M. Ghavamzadeh",
                "P. Fieguth",
                "X. Cao",
                "A. Khosravi",
                "U.R. Acharya"
            ],
            "title": "A review of uncertainty quantification in deep learning: Techniques, applications and challenges",
            "venue": "Information Fusion, vol. 76, pp. 243\u2013297, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "X. Zhou",
                "H. Liu",
                "F. Pourpanah",
                "T. Zeng",
                "X. Wang"
            ],
            "title": "A survey on epistemic (model) uncertainty in supervised learning: Recent advances and applications",
            "venue": "Neurocomputing, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "A.F. Psaros",
                "X. Meng",
                "Z. Zou",
                "L. Guo",
                "G.E. Karniadakis"
            ],
            "title": "Uncertainty quantification in scientific machine learning: Methods, metrics, and comparisons",
            "venue": "arXiv preprint arXiv:2201.07766, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "M. Abdar",
                "A. Khosravi",
                "S.M.S. Islam",
                "U.R. Acharya",
                "A.V. Vasilakos"
            ],
            "title": "The need for quantification of uncertainty in artificial intelligence for clinical data analysis: increasing the level of trust in the decision-making process",
            "venue": "IEEE Systems, Man, and Cybernetics Magazine, vol. 8, no. 3, pp. 28\u201340, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "M. Abdar",
                "M.A. Fahami",
                "S. Chakrabarti",
                "A. Khosravi",
                "P. P lawiak",
                "U.R. Acharya",
                "R. Tadeusiewicz",
                "S. Nahavandi"
            ],
            "title": "Barf: A new direct and cross-based binary residual feature fusion with uncertainty-aware module for medical image classification",
            "venue": "Information Sciences, vol. 577, pp. 353\u2013378, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "M. Abdar",
                "M.A. Fahami",
                "L. Rundo",
                "P. Radeva",
                "A.F. Frangi",
                "U.R. Acharya",
                "A. Khosravi",
                "H.-K. Lam",
                "A. Jung",
                "S. Nahavandi"
            ],
            "title": "Hercules: Deep hierarchical attentive multilevel fusion model with uncertainty quantification for medical image classification",
            "venue": "IEEE Transactions on Industrial Informatics, vol. 19, no. 1, pp. 274\u2013285, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "M. Abdar",
                "S. Salari",
                "S. Qahremani",
                "H.-K. Lam",
                "F. Karray",
                "S. Hussain",
                "A. Khosravi",
                "U.R. Acharya",
                "V. Makarenkov",
                "S. Nahavandi"
            ],
            "title": "Uncertaintyfusenet: robust uncertainty-aware hierarchical feature fusion model with ensemble monte carlo dropout for covid-19 detection",
            "venue": "Information Fusion, vol. 90, pp. 364\u2013381, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "P. Secchi",
                "E. Zio",
                "F. Di Maio"
            ],
            "title": "Quantifying uncertainties in the estimation of safety parameters by using bootstrapped artificial neural networks",
            "venue": "Annals of Nuclear Energy, vol. 35, no. 12, pp. 2338\u20132350, 2008.",
            "year": 2008
        },
        {
            "authors": [
                "X. Wang",
                "Y. Chen",
                "W. Zhu"
            ],
            "title": "A survey on curriculum learning",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "A. Graves",
                "M.G. Bellemare",
                "J. Menick",
                "R. Munos",
                "K. Kavukcuoglu"
            ],
            "title": "Automated curriculum learning for neural networks",
            "venue": "international conference on machine learning. PMLR, 2017, pp. 1311\u20131320.",
            "year": 2017
        },
        {
            "authors": [
                "Y. Kong",
                "L. Liu",
                "J. Wang",
                "D. Tao"
            ],
            "title": "Adaptive curriculum learning",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 5067\u20135076.",
            "year": 2021
        },
        {
            "authors": [
                "R. Zhan",
                "X. Liu",
                "D.F. Wong",
                "L.S. Chao"
            ],
            "title": "Meta-curriculum learning for domain adaptation in neural machine translation",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, vol. 35, no. 16, 2021, pp. 14 310\u201314 318.",
            "year": 2021
        },
        {
            "authors": [
                "T. Doan",
                "S.I. Mirzadeh",
                "J. Pineau",
                "M. Farajtabar"
            ],
            "title": "Efficient continual learning ensembles in neural network subspaces",
            "venue": "arXiv preprint arXiv:2202.09826, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "S.I. Mirzadeh",
                "A. Chaudhry",
                "D. Yin",
                "T. Nguyen",
                "R. Pascanu",
                "D. Gorur",
                "M. Farajtabar"
            ],
            "title": "Architecture matters in continual learning",
            "venue": "arXiv preprint arXiv:2202.00275, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "S.I. Mirzadeh",
                "M. Farajtabar",
                "D. Gorur",
                "R. Pascanu",
                "H. Ghasemzadeh"
            ],
            "title": "Linear mode connectivity in multitask and continual learning",
            "venue": "arXiv preprint arXiv:2010.04495, 2020.",
            "year": 2010
        },
        {
            "authors": [
                "S.I. Mirzadeh",
                "M. Farajtabar",
                "R. Pascanu",
                "H. Ghasemzadeh"
            ],
            "title": "Understanding the role of training regimes in continual learning",
            "venue": "Advances in Neural Information Processing Systems, vol. 33, pp. 7308\u20137320, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "M. Ghassemi",
                "L. Oakden-Rayner",
                "A.L. Beam"
            ],
            "title": "The false hope of current approaches to explainable artificial intelligence in health care",
            "venue": "The Lancet Digital Health, vol. 3, no. 11, pp. e745\u2013e750, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "T. Miller"
            ],
            "title": "Explanation in artificial intelligence: Insights from the social sciences",
            "venue": "Artificial intelligence, vol. 267, pp. 1\u201338, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "D. Gunning",
                "M. Stefik",
                "J. Choi",
                "T. Miller",
                "S. Stumpf",
                "G.-Z. Yang"
            ],
            "title": "Xai\u2014explainable artificial intelligence",
            "venue": "Science Robotics, vol. 4, no. 37, p. eaay7120, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "G. Vilone",
                "L. Longo"
            ],
            "title": "Notions of explainability and evaluation approaches for explainable artificial intelligence",
            "venue": "Information Fusion, vol. 76, pp. 89\u2013106, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "T. Lin",
                "Y. Wang",
                "X. Liu",
                "X. Qiu"
            ],
            "title": "A survey of transformers",
            "venue": "arXiv preprint arXiv:2106.04554, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "K. Yuan",
                "S. Guo",
                "Z. Liu",
                "A. Zhou",
                "F. Yu",
                "W. Wu"
            ],
            "title": "Incorporating convolution designs into visual transformers",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 579\u2013588.",
            "year": 2021
        },
        {
            "authors": [
                "S. Khan",
                "M. Naseer",
                "M. Hayat",
                "S.W. Zamir",
                "F.S. Khan",
                "M. Shah"
            ],
            "title": "Transformers in vision: A survey",
            "venue": "ACM Computing Surveys (CSUR), 2021.",
            "year": 2021
        },
        {
            "authors": [
                "M.M. Naseer",
                "K. Ranasinghe",
                "S.H. Khan",
                "M. Hayat",
                "F. Shahbaz Khan",
                "M.-H. Yang"
            ],
            "title": "Intriguing properties of vision transformers",
            "venue": "Advances in Neural Information Processing Systems, vol. 34, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "J. Gou",
                "B. Yu",
                "S.J. Maybank",
                "D. Tao"
            ],
            "title": "Knowledge distillation: A survey",
            "venue": "International Journal of Computer Vision, vol. 129, no. 6, pp. 1789\u20131819, 2021.",
            "year": 1819
        },
        {
            "authors": [
                "L. Wang",
                "K.-J. Yoon"
            ],
            "title": "Knowledge distillation and studentteacher learning for visual intelligence: A review and new outlooks",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "T. Kobayashi"
            ],
            "title": "Extractive knowledge distillation",
            "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2022, pp. 3511\u20133520.",
            "year": 2022
        },
        {
            "authors": [
                "B. Pan",
                "H. Cai",
                "D.-A. Huang",
                "K.-H. Lee",
                "A. Gaidon",
                "E. Adeli",
                "J.C. Niebles"
            ],
            "title": "Spatio-temporal graph for video captioning with knowledge distillation",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 10 870\u201310 879.",
            "year": 2020
        },
        {
            "authors": [
                "B.-w. Kwak",
                "Y. Kim",
                "Y.J. Kim",
                "S.-w. Hwang",
                "J. Yeo"
            ],
            "title": "Trustal: Trustworthy active learning using knowledge distillation",
            "venue": "arXiv preprint arXiv:2201.11661, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "P. Ren",
                "Y. Xiao",
                "X. Chang",
                "P.-Y. Huang",
                "Z. Li",
                "B.B. Gupta",
                "X. Chen",
                "X. Wang"
            ],
            "title": "A survey of deep active learning",
            "venue": "ACM Computing Surveys (CSUR), vol. 54, no. 9, pp. 1\u201340, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "R. Aljundi",
                "N. Chumerin",
                "D.O. Reino"
            ],
            "title": "Identifying wrongly predicted samples: A method for active learning",
            "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2022, pp. 2290\u20132298.",
            "year": 2022
        },
        {
            "authors": [
                "S. Desai",
                "D. Ghose"
            ],
            "title": "Active learning for improved semisupervised semantic segmentation in satellite images",
            "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2022, pp. 553\u2013563.",
            "year": 2022
        },
        {
            "authors": [
                "S. Budd",
                "E.C. Robinson",
                "B. Kainz"
            ],
            "title": "A survey on active learning and human-in-the-loop deep learning for medical image analysis",
            "venue": "Medical Image Analysis, vol. 71, p. 102062, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Li",
                "M. Min",
                "D. Shen",
                "D. Carlson",
                "L. Carin"
            ],
            "title": "Video generation from text",
            "venue": "Proceedings of the AAAI conference on artificial intelligence, vol. 32, no. 1, 2018. 42",
            "year": 2018
        },
        {
            "authors": [
                "K. Deng",
                "T. Fei",
                "X. Huang",
                "Y. Peng"
            ],
            "title": "Irc-gan: Introspective recurrent convolutional gan for text-to-video generation.",
            "venue": "in IJCAI,",
            "year": 2019
        },
        {
            "authors": [
                "X. Li",
                "C. Xu",
                "G. Yang",
                "Z. Chen",
                "J. Dong"
            ],
            "title": "W2vv++ fully deep learning for ad-hoc video search",
            "venue": "Proceedings of the 27th ACM international conference on multimedia, 2019, pp. 1786\u20131794.",
            "year": 2019
        },
        {
            "authors": [
                "X. Li",
                "J. Dong",
                "C. Xu",
                "J. Cao",
                "X. Wang",
                "G. Yang"
            ],
            "title": "Renmin university of china and zhejiang gongshang university at trecvid 2018: Deep cross-modal embeddings for video-text retrieval.",
            "venue": "TRECVID,",
            "year": 2018
        },
        {
            "authors": [
                "J. Loko\u0107",
                "T. Sou\u0107ek",
                "P. Vesel\u1ef3",
                "F. Mejz\u013a\u0131k",
                "J. Ji",
                "C. Xu",
                "X. Li"
            ],
            "title": "A w2vv++ case study with automated and interactive text-to-video retrieval",
            "venue": "Proceedings of the 28th ACM International Conference on Multimedia, 2020, pp. 2553\u20132561.",
            "year": 2020
        },
        {
            "authors": [
                "S. Chen",
                "Y. Zhao",
                "Q. Jin",
                "Q. Wu"
            ],
            "title": "Fine-grained video-text retrieval with hierarchical graph reasoning",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 10 638\u201310 647.",
            "year": 2020
        },
        {
            "authors": [
                "J. Mun",
                "M. Cho",
                "B. Han"
            ],
            "title": "Local-global video-text interactions for temporal grounding",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 10 810\u201310 819.",
            "year": 2020
        },
        {
            "authors": [
                "F. Hu",
                "A. Chen",
                "Z. Wang",
                "F. Zhou",
                "J. Dong",
                "X. Li"
            ],
            "title": "Lightweight attentional feature fusion: A new baseline for text-to-video retrieval",
            "venue": "European Conference on Computer Vision. Springer, 2022, pp. 444\u2013461.",
            "year": 2022
        },
        {
            "authors": [
                "J. Dong",
                "Y. Wang",
                "X. Chen",
                "X. Qu",
                "X. Li",
                "Y. He",
                "X. Wang"
            ],
            "title": "Reading-strategy inspired visual representation learning for text-to-video retrieval",
            "venue": "IEEE Transactions on Circuits and Systems for Video Technology, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "F. Sener",
                "R. Saraf",
                "A. Yao"
            ],
            "title": "Transferring knowledge from text to video: Zero-shot anticipation for procedural actions",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Z. Shi",
                "X. Chen",
                "X. Qiu",
                "X. Huang"
            ],
            "title": "Toward diverse text generation with inverse reinforcement learning",
            "venue": "arXiv preprint arXiv:1804.11258, 2018.",
            "year": 1804
        },
        {
            "authors": [
                "Z. Shao",
                "M. Huang",
                "J. Wen",
                "W. Xu",
                "X. Zhu"
            ],
            "title": "Long and diverse text generation with planning-based hierarchical variational model",
            "venue": "arXiv preprint arXiv:1908.06605, 2019.",
            "year": 1908
        },
        {
            "authors": [
                "Z. Zhang",
                "L. Schomaker"
            ],
            "title": "Divergan: An efficient and effective single-stage framework for diverse text-to-image generation",
            "venue": "Neurocomputing, vol. 473, pp. 182\u2013198, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "W. Du",
                "J. Zhao",
                "L. Wang",
                "Y. Ji"
            ],
            "title": "Diverse text generation via variational encoder-decoder models with gaussian process priors",
            "venue": "arXiv preprint arXiv:2204.01227, 2022.",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014Deep Learning, Computer Vision, Video Captioning, Dense Video Captioning.\nF"
        },
        {
            "heading": "1 Introduction",
            "text": "Automated descriptions of visual content can be very useful, from creating more accessible content to providing efficient methods of searching and indexing large media corpora to helping generate novel media. Image and video captioning [1], [2], [3], [4], [5] is the task of generating textual descriptions of visual content. While describing what we see is a very natural task for humans, it is not trivial to create artificially intelligent algorithms that do the same. Researchers in the fields of computer vision (CV) and natural language processing (NLP) [6] have not yet been able to completely solve the challenge of converting low-level visual features to higher-level semantic or symbolic abstractions [7]. VC is more complex than image captioning, not only because videos contain many frames, thus carrying significantly more information than a still image, but also because it is necessary to extract information from the\n\u2022 This work was partially supported by the Australian Research Council\u2019s Discovery Projects funding scheme (project DP190102181 and DP210101465). \u2022 M. Abdar and A. Khosravi are with the Institute for Intelligent Systems Research and Innovation (IISRI), Deakin University, Australia (e-mails: m.abdar1987@gmail.com & m.abdar@deakin.edu.au & abbas.khosravi@deakin.edu.au). \u2022 M. Kollati and S. Kuraparthi are with the Department of Electronics and Communication Engineering, Gokaraju Rangaraju Institute of Engineering & Technology, Hyderabad, Telangana, India (e-mails: mkollati@gmail.com & kswarajai@gmail.com). \u2022 F. Pourpanah is with the Department of Electrical and Computer Engineering, and Ingenuity Labs Research Institute, Queen\u2019s University, Kingston, ON, Canada (e-mail: farhad.086@gmail.com). \u2022 D. McDuff is with the University of Washington in Seattle, WA, USA (e-mail: dmcduff@uw.edu). \u2022 M. Ghavamzadeh is with Google Research, Mountain View, CA, USA (e-mail: ghavamza@google.com). \u2022 S. Yan is with the Sea AI Lab., Singapore (e-mail: shuicheng.yan@gmail.com). \u2022 A. Mohamed is with the Meta Reality Labs in Seattle, WA, USA (e-mail: abduallahadel@meta.com). \u2022 E. Cambria is with the School of Computer Science and Engineering, Nanyang Technological University, Singapore (E-mail: cambria@ntu.edu.sg). \u2022 F. Porikli is with Qualcomm AI Research, USA (E-mail: fporikli@qti.qualcomm.com).\ntemporal dimension as well as the spatial dimension. Furthermore, videos are often accompanied by audio, meaning that multimodal understanding might be necessary to interpret the content correctly.\nWith the advent of the internet and online media platforms (e.g., YouTube, Facebook, TikTok), there has been an explosion of content created and shared. Videos are often associated with metadata in the form of simple tags or complete paragraphs. These data provide opportunities for improving automatic descriptions of visual content. The availability of huge datasets (e.g., Activitynet, Max Plank Institute for Informatics Movie Description Dataset (MPII-MD) and Spoken Moments Dataset (S-MiT)) has boosted research in computer vision, including work on Dense Video Captioning (DVC), Video Question Answering (V-QA), Content Based Video Retrieval (CBVR) [8], and paragraph captioning [9], [10]. The qualitative and quantitative performance of VC systems has improved dramatically since the advent of deep learning (DL) [11], [12], [13]. Neural models scale well with data and large parameter models can be successfully trained without overfitting.\nVC has applications across a plethora of fields, from Artificial Intelligence (AI)-assisted medical diagnostics to storytelling through videos to V-QA [14], [15] to lip-reading to scene understanding and autopilot assistance [16]. While VC might appear a purely digital task it has implications for robotic systems that physically interact with humans and comprehend their surroundings [17], [18]. A number of fields have leveraged and contributed to the literature on VC, in this review we cover work in the domains of Human Robot Interaction (HRI), video indexing, video tagging, video description, video recommendation systems, video summarization, visual information retrieval, and accessibility [19].\nEarly VC schemes used template-based approaches [21], [22], [23], [24], [25]. Template methods, extract semantic concepts such as a subject, object, and verb from a set of visual classifiers, and then generate captions through a language model which assigns predicted triplets to predefined sentence templates. Rohrbach et al. [26] proposed a Conditional Ran-\nar X\niv :2\n30 4.\n11 43\n1v 1\n[ cs\n.C V\n] 2\n2 A\npr 2\n02 3\n2\ndom Field (CRF) to learn the relationship between different components in the source video. They found it was possible to generate a rich semantic representation of the visual content, including objects and activity labels. Some of the drawbacks of template-based methods are that they are inflexible and are poor in modeling the diversity and expressiveness of language when the vocabulary is large [27]. Empirical results show they are not good at generalizing outside the training set, and thus, have limited capacity in handling unseen data. Furthermore, as the visual classifiers are not trained with the language model they are not able to learn concepts end-to-end from low-level image features.\nIn deep learning-based VC, encoder-decoder pipelines\nare frequently employed [28]. In these models, the encoder extracts visual features, for example, using a Two dimensional Convolutional Neural Network (2D-CNN) [29] or linear embedding. The feature vector can be further reduced via Mean Pooling (MP), a Temporal Encoding (TE), or via Semantic Attribute Learning (SEM), before being fed to the decoder [30]. Traditionally, a Convolutional Neural Network (CNN) is used in the encoder (video), whereas a RNN is employed as the decoding phase (for language generation) [31]. However, as can be seen from Fig. 3, newly proposed stateof-the-art methods train the encoder and decoder parts separately.\nEncoder-decoder frameworks that employ Three Dimensional Convolutional Neural Network (3D-CNN) layers typically better capture spatio-temporal information than a model that applies a 2D-CNN to each frame separately. Recurrent Neural Networks (RNN) or Dual-Stream RNNs (DS-RNN shown in Fig. 2) can be utilized in the encoding stage and perform a similar function (see [20]). Language decoders utilize pre-trained word embeddings such as Word2Vec, GloVe, and fastText to convert semantically similar words into similar vector representations based on the feature encoding. Since language is sequential an RNN is then used for decoding an output sequence [32]. In recent years, Dynamic Video Captioning (DVC) has emerged to deal with captioning realtime videos. \u201dLive\u201d video captioning typically involves the analysis of longer, unsegmented videos. Videos are often composed of a variety of events with some elements that are irrelevant to the desired caption. To address DVC [33], [34], [35], systems have been designed to describe events using story-telling techniques [33], which are useful in applications such as Content Based Image Retrieval (CBIR) and video recommendation [36].\nRecently, several review papers [37], [38], [39] have covered VC developments in the period between 2020 and 2022. Islam et al. [37] produced a survey on the state-of-the-art deep learning based VC approaches, evaluating the various benchmark datasets and summarizing the evaluation metrics. Jain et al. [38] surveyed VC by compiling, studying, and summarizing the results of the existing VC techniques in the literature. Moctezuma et al. [39] presented another review paper on a subset of VC papers published between 2015 and 2022, and discussed recent evaluation metrics and datasets. Compared to the other review papers [37], [38], [39], our work presents a thorough review of methods in VC, evaluates recent techniques, datasets, and evaluation metrics. We cover relevant papers on VC and related topics from 2015 to 2022, and discuss the main research gaps existing in the field and within related subfields. Table 1 presents a comprehensive comparison of our review paper on VC and the existing review papers in the literature."
        },
        {
            "heading": "1.1 Main Contributions",
            "text": "The major contributions of this review are as follows:\n1) Provide the most comprehensive survey to date of deep learning methods for VC, DVC, Text to Image (T2I), and Text to Video (T2V), incorporating the recent advances in deep learning and highlighting their applications. 2) Provide a thorough comparison of the existing methods on several publicly available datasets, with captioning evaluated based on parameters discussed in Section 2.4.1.\n3 Video Frame\nC N\nN\nE n\nco d\ner\nVideo\nFeatures\nClassification\nLayers\nClassification\nLoss\nRNN Decoder Caption\nLoss\n(a) Disjoint training\nVideo Frame\nC N\nN\nE n\nco d\ner\nVideo\nFeatures RNN Decoder\nCaption\nLoss\n(b) End-to-end training\nFig. 3: Disjoint training vs end-to-end training methods for VC [31].\n3) Unlike other reviews, we cover everything from templatebased, attention-based, graphical, reinforcement learning, adversarial, Dense Video Captioning, and nonautoregressive VC techniques to V-QA, video summarization, paragraph VC, and event captioning from video (see Table 4). 4) For each explicit task, the contemporary landscape of deep learning is summarized, and the technical milestones are highlighted. 5) We conclude our review with discussions on open research challenges and opportunities for future studies.\nThe remainder of this survey has the following structure. We first cover the foundations of deep learning-based VC; including datasets, evaluation metrics, and optimization (e.g., loss) formulations (Section 2). We then categorize VC methods (Section 3) and present a discussion on the applications of VC (Section 4). The main results, research gaps, and future research directions of VC methods are presented towards the end (Section 5). Finally, we conclude the review in Section 6."
        },
        {
            "heading": "2 Deep Learning Video Captioning Foundations",
            "text": "The main goal of VC is to find/generate a sentence (or a sequence of words) for a given video. Assume a video sequence V = {v1, v2, . . . , vn} with n frames, where vi \u2208 Rw\u00d7h\u00d7c represents the ith video, and w, h, and c indicate width, height and the number of channels, respectively. The goal is to learn a model \u03a6 to map the frame sequences into the word vector space, i.e., \u03a6 : V \u2192 Y, where Y = {y1, \u00b7 \u00b7 \u00b7 , ym} consists of\nm words, y \u2208 RN being a one-hot vector and N indicating the size of the word vocabulary. VC can be considered as a sequence-to-sequence learning task, or similarly as an encoding of video features and decoding of text. VC mainly is composed of an encoder and decoder. The encoder deals with the multi-modality of the input data video and audio. The decoder uses the embedding and generate the caption. Below, we explore the different options of both then encoders and decoders."
        },
        {
            "heading": "2.1 Encoders",
            "text": "Encoders in VC often involve extracting features from the input video frames via a pre-trained CNN. The extracted features are then utilized as input to an RNN decoder to generate the caption. The choice of CNNs has a profound impact on the overall performance of the video captioning model. Popular pre-trained model architectures are C3D [42], VGG-16 [43], VGG-19 [44], Inception-v3 [45], and Inception ResNet-v2 [46]. Videos comprise segments with boundaries; each segment may contain multiple scenes, actions, and objects. The segments vary from frame to frame. The segments dictate the underlying structure of the video and knowledge of the structure is important in enhancing the network\u2019s understanding of the video [47]. Most of the prior work has used features solely from video frames. However, multi-modal information such as audio, plays a crucial role in differentiating videos from a mere sequence of images. Extracting multi-modal information, can result in better feature representations, which in-turn help to improve the accuracy of the video captions. Multi-modal\n4\nencoding is not trivial in part due to the challenge of combining latent representations from different modalities (audio, speech, video, image, and text sources) [48]."
        },
        {
            "heading": "2.2 Decoders",
            "text": "The main problem with RNNs used in the decoder is that they suffer from vanishing gradients. In most of the VC algorithms, different types of RNNs are used, such as Long Short-Term Memory (LSTM) [49], [50], [51] networks and Gated Recurrent Unit (GRU). LSTMs help overcome the vanishing gradient problem [52] and have shown promising results in dealing with sequences of frames, learning long-range temporal patterns [32]. GRUs are simple and require fewer parameters for training. They are mainly used for modeling short sequences and are less likely to result over-fitting [53].\nThe accuracy of sequence-sequence VC models can be enhanced using pre-trained weights [54] and by introducing additional LSTM layers. Recently, transformers [55], [56], [57] have been employed in VC to capture long-range dependencies. The use of transformers [58] can accelerate training speed. Given queries q \u2208 Rtq\u00d7dk , keys k \u2208 Rtv\u00d7dk and values v \u2208 Rtv\u00d7dv . The set of queries, keys and values are represented in matrices\u2013q, k and v. In this equation, tq is the length of the queries and tv is the length of the keys and values. dk is the dimension of q and k and dv is the dimension of v and \u221a d is the scaling factor. The attention output is\nf = A (q, k, v) = softmax (\nqkT\u221a d\n) v. (1)\nIn Multi-head attention (MHA) [59] mechanisms, multiple heads are used, where the attention from all the heads are computed in parallel. MHA facilitates the network obtaining attention output from different heads at different positions. For the jth head, the attention output can be written as:\nheadJ = A ( qW qJ , kW k J , vW v J ) , (2)\nMHA (q, k, v) = [head1, head2 \u00b7 \u00b7 \u00b7 headh ] W O , (3)\nwhere W qJ , W k k , W q q are the projection (weight) matrices for the jth head, W O \u2208 Rd\u00d7d , and dh is the dimension of the output features. The attention function in Eq.((1)) can be directly applied on a set of n queries q \u2208 Rn\u00d7d to obtain F \u2208 Rn\u00d7d . The Feed Forward Network (FFN) performs nonlinear operations on the output features of the MHA. If the dimension of the input vector X \u2208 Rn\u00d7d and the output vector is F \u2208 Rn\u00d7d . The output vector F is computed using Eq.((4)) in [60] as:\nF = FFN (X) = max (0,XW1 + b1) W2 + b2. (4)\nTherefore, unlike with an LSTM and GRU which use serial tokens, transformers take parallel tokens. Transformers are better at capturing global dependencies, making them more effective at modeling complex sequences like videos. Captioning accuracy can be further improved in sequencesequence VC models by incorporating several techniques such as attention-based, graph-based, reinforcement-based, adversarial networks, non-autoregressive, DVC, paragraph VC and interactive V-QA as shown in Fig. 1. The chronological sequence of the representative methods is presented in the taxonomy of Fig. 4."
        },
        {
            "heading": "2.3 Datasets",
            "text": "The central goal in VC is to develop models that transform videos into text. The performance of the network models is highly dependent on the knowledge learned from the training data. If these models are trained on one dataset and tested on another dataset from a different domain, the performance of the models is likely to decline dramatically [61]. Therefore, large-scale visual datasets for specific tasks are critical. The datasets used in VC describe open domain videos [62], human activities [63], [64], cooking recipes [65], [66], movie [47], [67],\n5 [68], [69]. Table 2 provides a summary of the most important datasets used in VC."
        },
        {
            "heading": "2.3.1 Open-domain databases",
            "text": "2.3.1.1 Microsoft Research Video Description Corpus (MSVD): The MSVD dataset [47], [67], [68], [69] (otherwise known as YouTube2Text [62]) is a widely used dataset in VC. It comprises 1,970 videos and 80,839 English captions written by Amazon Mechanical Turkers (AMT) workers. Each video clip is associated with 40 descriptions. The clips used in MSVD contain a single activity and are expressed with multi-lingual captions such as English, Turkish, and Chinese. For benchmarking, the training, validation, and testing splits contain 1,200, 100, and 670 videos, respectively [49], [70]. To prevent lexical bias, audio is removed from all the videos. In VC, it is important to be mindful of biases and actively seek to use more neutral or inclusive language. However, a large number of the videos in MSVD are of very poor quality and it presents challenges even for state-of-the-art model architectures [71]. Further, the videos are short and captioned with only a single sentence [71]. 2.3.1.2 YouTube Highlight Dataset: The YouTube Highlight Dataset [72] contains videos from six domains (\u201cskating\u201d, \u201cgymnastics\u201d, \u201cdog\u201d, \u201cparkour\u201d, \u201csurfing\u201d, and \u201cskiing\u201d) and the videos are variable in length. The total duration of the dataset is 1,430 minutes. The data are split in half for training and testing. This dataset poses challenges as (1) it comprises videos captured via handheld devices (2) the start and end of a specific highlight are determined by the subject (3) videos such as interviews and slideshows are included in the dataset. The videos in the test set were evaluated by AMT workers. 2.3.1.3 MSR-VTT: Though there is an increased interest in VC, the existing databases have limited variability and complexity. Often simple and focused on limited tasks such as e.g., cooking [73] they are not applicable to all content. There are few large-scale datasets as videos are difficult to gather, annotate and organize. The MSR-VTT dataset was introduced to address these limitations. Compared to other datasets used in VC such as MSVD [67], YouCook [73], M-VAD [74] TACoS [71], and MPII-MD [26], the MSR-VTT [75] benchmark is the largest in terms of the number of clip-sentence pairs, where each video clip is annotated with multiple sentences. MSR-VTT is particularly valuable for training large parameter models (e.g., RNNs). The database contains videos from 257 popular queries in 20 representative categories. The creators of the database utilized over 3,400 worker hours to gather video-text pairs, annotated and summarized to boost the research in VC. The dataset containd over 40 hours of video content, 10,000 video clips, and 200K clip-sentence pairs in total. The dataset is partitioned into training, testing, and validation sets of percentages of 65%:30%:5%, respectively. 2.3.1.4 Video titles in the wild (VTW): : The VTW dataset [76] was designed for video title generation. The dataset spans 213.2 hours, 10 times longer than the videos in MSVD. The dataset comprises 10k open-domain videos with annotation created by editors rather than AMT workers. 2.3.1.5 TGIF: Tumblr GIF [77] is a large-scale video database of animated GIFs. TGIF comprises 100K animated examples collected from Tumblr, and 120K natural language\nsentences annotated via crowd-sourcing. The crowdsource workers are gathered from only English-spoken countries like Australia, Canada, New Zealand, the UK and the USA. The 100K are split into 90K for the training and 10K for testing. 2.3.1.6 VATEX: The existing databases in VC are heavily biased towards the English language, more multilingual datasets are necessary. VATEX [78] is more diverse compared to the voluminous MSR-VTT [75] dataset. It contains both English and Chinese descriptions at a large scale, which can facilitate research into multilingual captioning. Another important property is that VATEX [78] has the largest number of clip-sentence pairs with each video clip annotated with multiple unique sentences and every caption is unique in the corpus. VATEX contains more complete yet representative video content, covering 600 human activities in total, it is lexically richer and thus can enable more natural and diverse caption generation. It comprises 41,250 videos and 825,000 captions in two languages: English and Chinese. The captions in this dataset have 206,000 English-Chinese parallel translation pairs. To portray a large number of human activities, existing Kinetics-600 videos are reused in the database that contains 600 different human action classes. The official dataset partition is 25,991 for training, 3,000 for validation, and 6,000 for testing."
        },
        {
            "heading": "2.3.2 Activity-based video datasets",
            "text": "Video action classification and captioning have received a lot of research interest though the progress has been limited because of the dearth of large databases. Some databases [79], [80] lack control over pose variations, motion, and other scene properties that might be important for learning fine-grained models. The datasets used for activity-based video captioning mainly focus on actors, objects, and their interactions. 2.3.2.1 Charades: The videos in the Charades dataset [81], [82] contain mundane activities taken indoors by 267 people from three continents. Charades offer 27,847 video descriptions, 66,500 temporally localized intervals for 157 action classes, and 41,104 labels for 46 object classes. Indeed, the scenes and activities captured in the dataset are quite diverse. This diversity provides unique challenges to VC. The Charades dataset contains examples of how people interacte with objects and perform natural action sequences and presents the opportunity to learn common sense and contextual knowledge necessary for high-level reasoning and modeling tasks. 2.3.2.2 Charades-Ego: The Charades-Ego dataset [83] is similar to the Charades dataset and was collected by recruiting crowd workers on the Internet to record the videos in both first and third-person performing a given script of activities. The dataset contains 68,536 activities, 7,860 videos and 68.8 hours of first and thirdperson video. AMT workers were asked to record two videos: one video acting out the script from a third-person perspective; and another collected while they perform the same script in the same way, with a camera fixed to their forehead. The creators of this dataset used a creative approach to capture examples from both first and third-point views concurrently and in a scalable fashion. 2.3.2.3 ActivityNet Captions: The ActivityNet Captions dataset [84] comprises 20k videos and 100,000 sentences. The video caption on average describes 94.6% of the entire\n6 video, thus making the dataset suitable for DVC. To prove that ActivityNet Captions\u2019 captions mark semantically meaningful events, two different temporally annotated paragraphs from different workers were collected for each of the 4926 validation and 5044 test videos. Each pair of annotations was then tested to see how well they temporally corresponded to each other. 2.3.2.4 Something-Something V2: The dataset Something-Something V2 dataset [80] is used for modeling fine-grained action recognition for video captioning tasks. Something-Something V2 contains 220,847 videos of 174 action categories. In the database collection, AMT workers are asked to incorporate the action class and the objects involved. Something-Something V2 is one of the largest video datasets focused on human-object interactions. 2.3.2.5 Moments in Time dataset: : The Moments in Time dataset [85] is a gathering of one million short videos labeled corresponding to an event happening within 3 seconds. The dataset comprises videos labeled from 339 different action classes, to make it possible for the models to understand the dynamics of actions in videos. It is one of the largest human-annotated video datasets capturing visual and audible short events produced by humans, animals, objects, or nature. The most commonly used verbs in the English language were chosen as a way to source the videos as a result there is a significant amount of diversity and intra-class variation. 2.3.2.6 HowTo100m: The HowTo100m dataset [86] contains more than 100 million uncurated instructional videos taken from various databases in the domains of action recognition (HMDB-51, UCF-101, Kinetics-700), textto-video retrieval (YouCook2, MSR-VTT) and action localization (YouTube-8M Segments, Crosstalk). In the absense of manual annotation, the mismatch between narrations and videos is corrected by introducing a loss estimation Multiple Instance Learning (MIL) and Noise Contrastive Estimation (NCE), i.e., MIL-NCE, a multiple instances learning approach derived from noise contrastive estimation. After applying this loss function, the HowTo100M dataset provides solid visual representations that can outperform self-supervised and fullysupervised representations on downstream tasks. 2.3.2.7 Anonymized Videos from Diverse countries (AViD) Dataset: While most datasets are biased and statistically confined to some regions and languages, AViD [87] contains actions from diverse countries obtained using queries in many languages. The authors also consider privacy and licensing, with faces masked and all the videos having a creative commons license. 2.3.2.8 S-MiT: S-MiT is a massive spoken video caption dataset [88], used mainly in video understanding to learn contextual information. It is formed from 3-second clips from the moments in time dataset [85]. In this dataset, spoken captions are aligned with video during the training phase. This is not possible with the other large video caption datasets and allows for spoken caption models to be analyzed with matching video information. The models trained on S-MiT show better generalization in video retrieval problems."
        },
        {
            "heading": "2.3.3 Movie datasets",
            "text": "2.3.3.1 MPII-MD: The MPII-MD dataset [89] consists of videos from 94 Blu-ray Hollywood movies of diverse genres (e.g. drama, comedy, action). It comprises of 68K\nvideo-sentence pairs spanning over 73 hours. The dataset has been used to transcribe and align Audio Descriptions (AD) and scripts. The dataset is freely available on the contributor\u2019s website and the audio data has been preprocessed. Sections of the ADs (which is mixed with the original audio stream) have been semi-automatically segmented and the segments interpreted via a crowd-sourced transcription service.\n2.3.3.2 Large Scale Movie Description Challenge dataset (LSMDC): The LSMDC dataset [90] is a fusion of two benchmark datasets, i.e., Montreal Video Annotation Dataset (M-VAD) and MPII-MD. It consists of video descriptions extracted from professionally generated Descriptive Video Service (DVS) tracks on popular movies. The LSMDC dataset has a rich unique vocabulary tokens, i.e., 23,000. The dataset has been updated over time, LSMDC 2015 dataset comprises 118K sentence-clips pairs and the duration of the video is about 158 hours. The training, validation, blind, and test sets contain 91,908, 6,542, 10,053, and 9,578 video clips, respectively. The LSMDC 2016 contains a total of 128,000 clips, where 101,046 and 7,408 clips are used for training and validation, respectively.\n2.3.3.3 M-VAD: The M-VAD dataset [74] comprises approximately 49,000 video clips from 92 movies. The alignment of descriptions to video clips was performed via an automatic procedure using DVS provided for the movies. It is particularly helpful mainly for the visually impaired. This dataset is the largest DVS-derived movie dataset available requiring less human intervention in the post-processing. The whole dataset is 84.6 hours with 17,609 words. The dataset partition comprises of 38,949, 4,888 and 5,149 video clips for training, validation and testing [91], [92], [93], [94], [95]. Each video is annotated with 20 references by AMT."
        },
        {
            "heading": "2.3.4 Cooking recipe datasets",
            "text": "2.3.4.1 Textually Annotated Cooking ScenesMultilevel (TACoS-Mlevel): The TACoS-Mlevel dataset [96] describes routine cooking activities of 185 long videos, each with an average duration of 6 minutes. The dataset is annotated by multiple AMT workers. Each video is partitioned into temporal intervals and each interval is annotated with a short sentence. The whole dataset is split into 16,145 distinct intervals and comprises 52,478 sentences. Considering a single video, there are about 87 intervals and 284 sentences. The importance of this dataset is that it describes video content with multiple levels of description. It comprises 15 sentence, 3-5 sentence and one sentence descriptions of each video.\n2.3.4.2 YouCook II: The YouCook II dataset [66] contains 2,000 videos with a total number of 14,000 clips, which were gathered from YouTube and include 88 different types of recipes. In this database, 9,600 clips are used for the training and 3,200 clips for validation. The dataset was annotated by AMT workersand contains six different ways of cooking such as making breakfast, making sandwiches and preparing salads. The dataset was recorded from a thirdperson perspective. Three types of annotations are used, i.e., object tracking, actions, and human descriptions. The objects include dairy, condiments and the involved actions including putting objects down, seasoning food and pouring liquids. AMT workers were directed to view the cooking video\n7 multiple times and to describe the video caption in three sentences with a minimum of 15 words."
        },
        {
            "heading": "2.4 Evaluation Metrics",
            "text": "Model performance evaluation by humans is extremely valuable; however, it is also costly, time-consuming, and often subjective. Therefore, automatic evaluation tools are helpful and necessary for VC. The fidelity of evaluation metrics depends on how close the metrics reflect human judgment. The evaluation metrics of VC fall into two main groups: evaluation of correctness and evaluation of diversity."
        },
        {
            "heading": "2.4.1 Evaluation of Correctness",
            "text": "The frequently used evaluation metrics of correctness are Bilingual Evaluation Understudy (BLEU) [B@n] [97], Metric for Evaluation of Translation with Explicit Ordering (METEOR) [M] [98], Recall Oriented Understudy of Gisting Evaluation (ROUGE-L)[R] [99], Consensus based Image Description Evaluation (CIDEr) [C] [100], [101], Word Mover\u2019s Distance (WMD) [102], [103] and Semantic Propositional Image Captioning Evaluation (SPICE) [104]. These metrics are used for many NLP tasks, including text summarization, Image Captioning (IC), text similarity measurement, and the document analysis (see Table 3). A good evaluation metric should give satisfactory results even when there are words replaced with their synonyms, redundant words are add to the text, word order is changed or sentences are abbreviated without changing the meaning [41]. 2.4.1.1 BLEU[B@n]: This metric measures how good a machine translation coincides with a set of human-generated translations (reference translations) by counting the percentage of n-grams in the machine translation overlapping with the references [99].\nBLEU[B@n] is then computed as follows [97]:\nBLEU[B@n] = BF \u00d7 exp( N\u2211\nn=1 wn log pn), (5)\nwhere BF is the brevity penalty factor, pn is the geometric mean of the modified n-gram precision up to length N and wn is the weight of n-gram precision, where the sum of wn terms is equal to 1. Assume cl be the length of the machine translation and rl be the length of the reference translation. The BF in Eq.((6)) is given by:\nBF = { 1, if cl > rl e (1\u2013 rl cl ) if cl \u2264 rl . (6)\nHuman-generated sentence 1: John is middle aged. Human-generated sentence 2: John is thirty five years old. Machine-generated sentence: John has thirty five years. The number of unigrams in the machine-generated sentence: 5. The unigrams covered in human-generated sentences: \u2019John\u2019,\u2019thirty,\u2019 five\u2019, \u2019years\u2019. So, BLUE score is: 4/5 = 0.8.\nThe evaluation metric BLEU[B@n] is based on precision and does not use recall. This metric considers words with synonyms as different words. The metric penalizes even very small variation in words [81]. This shortcoming is addressed\nby METEOR which is, as a result, generally closer to human judgments.\n2.4.1.2 ROUGE-L [R]: ROUGE-L [R] measures the quality of a summary by comparing system-generated summaries with reference summaries. The metric considers ngram matching, word pairs between the machine-generated summary and the ideal summaries framed by human subjects. ROUGE-N is an n-gram recall between a system-generated summary and a set of human-generated summaries. ROUGEN is calculated as follows:\nROUGE \u2013 N =\u2211 s\u2208Reference summaries \u2211 s\u2208n-gram Countmatch(n \u2013 gram)\u2211\ns\u2208n\u2013gram Count(n \u2013 gram) ,\n(7)\nwhere n stands for the length of the n-gram, Count(n \u2013 gram), and Countmatch(n \u2013 gram) is the maximum number of ngram overlaps in a candidate summary and a set of reference summaries. Rouge is a recall-based evaluation metric as the denominator of the equation is the total sum of the number of n-grams occurring on the reference summary side. In Eq.(7), as more reference summaries are included in the metric, the number of matching n-grams in the denominator increases. Every time a reference is added to the pool, the space of distinct summaries is enhanced. ROUGE-N evaluates different aspects of text summarization, by controlling the type of reference added to the reference pool. In Eq.(7), the numerator sums all the reference summaries which assign more weight to match n-grams occurring in multiple references. ROUGEN gives higher scores to a machine-translated summary that has more words that overlap with the reference summaries. There are several variants of ROUGE (see [99]). ROUGE-L is a Longest Common Subsequence (LCS)-based F-Measure (F) to compute the correlation between two text summaries A of length a and B of length b assuming A is a reference summary and B is a model generated summary.\nRLCS = A + B\na , (8)\nPLCS = A + B\nb , (9)\nFLCS = (1 + \u03b32RLCSPLCS )\nRLCS + \u03b32PLCS , (10)\nwhere LCS(A,B) is the length of a LCS of A and B, and \u03b3 = PLCS RLCS .\n8\n2.4.1.3 METEOR: The METEOR metric considers both precision and recall. Precision P is calculated as the ratio of the number of uni-grams in the machine-generated translation that overlap (to uni-grams in the human-generated translation) and the total number of uni-grams in the machinegenerated translation. Recall R is calculated as the ratio of the number of uni-grams in the machine-generated translation that are overlap (to uni-grams in the reference translation) and the total number of uni-grams in the reference translation. The harmonic mean, Hm , of precision and recall is given by [105] as follows:\nHm = 10PR\nR + 9P . (11)\nThe penalty used in METEOR [105] is the form Penalty = 0.5\u00d7 (\n#Chunks #Unigrams matched\n)3 . (12)\nThe METEOR score Ms is computed from Eqs. (11) and (13), as follows:\nMs = Hm \u00d7 (1 \u2013 Penalty) . (13)\n# Computation of METEOR score\nS1 is human-generated sentence and S2 is machinegenerated sentence. S2 is jumbled version of S1. Since all the unigrams are matched. The precision P=6/6=1 and R=6/6=1 The Hm given by (11) is Hm=10\u00d71\u00d711+9\u00d71 =1. Since two subsequent unigrams \u201dThe\u201d, and \u201dman\u201d matched the no of chunks is 2. The penalty in (12) = 0.5\u00d7 (26 )\n3=0.000013 The Meteor score in (13) is 1*(1-0.000013)=0.9997\n2.4.1.4 CIDEr: The CIDEr metric [100] is another evaluation score that was introduced in 2015. It is based on the correlation of a machine-produced caption with a set of ground truth captions written by different human subjects. This metric generally leads to high agreement with human consensus. When using BLEU, analysis is carried out with five descriptions; however, this is sub-optimal when measuring how a \u201cmajority\u201d of humans would designate a video. Therefore, in computing CIDEr, forty to fifty descriptions per video in the database are used. While computing n-gram words, they are first mapped to their stem or root forms. The n-grams common in the all-human references will be given less weight as they are less informative. To encode this, a Term Frequency-Inverse Document Frequency (TF-IDF) [106] weighting for each n-gram is used. TF-IDF is a statistical measure that is used to evaluate how relevant n-gram is to a video in a collection of videos in the database. This is\n9\ndone by multiplying two metrics: how many times an n-gram appears in a video and the inverse document frequency of the n-gram across a set of videos in the database. It operates by increasing proportionally to the number of times an n-gram appears in a video but is offset by the number of videos that contain the n-gram. Thus, words that are common in every VC, such as: this, what, and if, rank lower even though they may appear many times. However, if the n-gram appears many times in one video, while not appearing many times in others, it probably means that it is significant in some way and is given more weight. The frequency of n-gram wk occurring in reference sentence si,j is denoted by hksi,j and hkci,j for candidate sentence. TF-IDF weighting for each n-gram gksi,j is formulated [100], as:\ngk ( si,j ) =\ngk ( si,j )\u2211\nwk inf hl ( si,j ) ln  |I |\u2211 Ip\u2208I min ( 1, \u2211 q hk ( sp,q ))  ,\n(14)\nwhere f is the vocabulary of all n-grams and I is the set of all videos in the dataset. The CIDErn score for n-grams of length n is derived considering the average cosine similarity between the candidate sentence and the reference sentences, which incorporates both precision and recall:\nCIDERn (ci ,Si) = 1 m \u2211\nj\ngn (ci) gn ( Sij )\u2225\u2225gn (Sij)\u2225\u2225\u2225\u2225gn (Sij)\u2225\u2225 . (15)\n2.4.1.5 WMD: The WMD metric is designed for the document retrieval [107] based on document similarity. The widely used techniques for document similarity are the Bag of Words (BoW) and TF-IDF. As shown in Fig. 5, BoW and TF-IDF can not distinguish between two sentences that are semantically similar, but with different words. To solve this WMD was proposed by Kusner et al. [103]. The authors used word2vec embeddings and considered the text documents as a weighted point cloud of embedded words. The distance between two text documents A and B is the minimum cumulative distance that words from document A need to travel to match exactly the point cloud of document B. The computation of WMD involves considering the normalized BoW by removing the stop words using the word2vec embedding. The distance of the two texts is measured as an Earth Mover\u2019s Distance (EMD) [102], which is frequently used in transportation to determine the cost of travel. The WMD metric makes use of the Euclidean distance in the word2vec embedding space\nto take into consideration semantic relatedness between pairs of words. The cost to relocate every word between captions is then used to determine the distance between two texts or captions. A special case of the EMD is used to model the WMD [102] prior to the linear optimization. WMD is less susceptible to the order of words or changing of the synonym than BLUE, ROUGE-L[R], and CIDEr. Moreover, it matches closely the human decisions, like CIDEr and METEOR. 2.4.1.6 SPICE: One of the most recent performance metrics proposed for image and video descriptions is SPICE [104]. BLEU, ROUGE-L[R], and CIDEr are n-gram sensitive and poor when evaluating sentences that have similar meanings and contain different words. Suppose the sentence is \u201ca young girl is standing on the tennis count\u201d, SPICE evaluation metric searches similar words such as (1) there is a girl, (2) girl is young, (3) girl is standing, (4) there is a court, (5) court is for tennis, and (6) girl is standing on the court. If each of the propositions is contained in the caption, then the caption is given more weight in this evaluation metric. SPICE is computed in two stages. In stage one, a dependency parser is constructed considering the syntactic dependencies between the words, and in stage two, dependency trees are transformed to scene graphs. SPICE. With the help of a dependency parse tree, the semantic scene graph preserves objects, respective attributes, and their interconnections. Object classes O(c), relation types R(c), and attribute types A(c) are examples of semantic tokens that make up a scene graph tuple G(c) of a caption C .\nG (C ) = \u3008O (C ) ,R (C ) ,A (C )\u3009 . (16)\nSPICE is calculated from the F1-score seen between tuples of artificially produced descriptions and the real-world data, similar to METEOR, SPICE searches WordNet for synonyms and treats them as close matches. Although the SPICE score plays only a limited role in VC, the precision of the parsing is clearly a potential performance barrier. For example, if the word \u201cswimming\u201d is parsed as an \u201cobject\u201d in the sentence \u201cblack cat swimming through the river,\u201d and the term \u201dcat\u201d is interpreted as an \u201dattribute,\u201d the sentence fails and receives a very low score."
        },
        {
            "heading": "2.4.2 Evaluation of Diversity",
            "text": "Generally, machine-generated sentences are not completely correlated with human-generated sentences because there are many correct answers, as a result diversity-based evaluation metrics such as Vocabulary Size (VS), Percentage of Novel Sentences (PNS), Diversity of Captions (DC) [108] and Diverse Captioning Evaluation (DCE) [109] have been developed. VS provides the number of distinct words in the generated caption. PNS is the percentage of generated captions not taken in the training set. To measure the diversity, mutual overlap and n-gram diversity are considered. Let SX be the group of sentences illustrating the X th test video in the dataset. The n-gram diversity (DIV-n) for the X th test video is calculated as the ratio of the number of distinct n-grams in SX to the number of n-grams in SX . Later, the mean value of n-gram diversities for the test videos gives n-gram diversity or DIV-n value. In the calculation of the mutual overlap metrics, evaluation metrics like BLEU@N, ROUGE-L, METEOR, or CIDEr can be used to calculate the correctness score between\n10\nmachine-generated and human-generated sentences. For the X th test video, the mutual overlap is formulated as:\nMeanCS (SX ) = 1 |SX | \u2211 s\u2208SX CS({s} ,Sx \u2208 s), (17)\nwhere CS(c, r) computes correctness score between a candidate generated sentence c and the human generated sentences r that is measured by a measure such as BLEU@N, METEOR, ROUGE-L or CIDEr. Here |SX | denotes the cardinality of SX set. The mutual overlap metric of the whole dataset is computed with mutual overlap metrics for all the test videos in the dataset. The diversity of generated captions is vital for DVC. The diversity is computed by computing the correlation between pairs of captions or between one caption and a set of other captions. The semantic correlation is captured with Latent Semantic Analysis (LSA).\nThe diversity of generated captions is a key in DVC. The evaluation is based on diversity opposite from the similarity of the captions. The solution to determine the similarity between pairs of captions, or between one caption to a set of other captions. The semantic relatedness of the sentence is measured with LSA [110]. The diversity is computed by finding the cosine similarity of two LSA vectors of sentence [111], as follows:\nDdiv = 1 n \u2211 si ,sj\u2208S ;i 6=j ( 1 \u2013 \u2329 si , sj \u232a) , (18)\nwhere S is sentence length with cardinality n and \u2329 si , sj \u232a is the cosine similarity i between the si and sj . The main bottleneck in [110] is that it does not consider the rationality of the sentence. The two very different sentences get high diversity scores but their descriptions may be wrong. The other bottleneck of [110] is that the proposed LSA method is not able to capture polysemy. To address these problems, [109] proposed a metric called DCE. It is mainly formulated from two angles: diversity between sentences computed at the same time maintained sentences reasonableness. Instead of using the difference between sentences as in [111], Jaccard similarity coefficient [112] is used in determining DCE, which is the best to deal with discrete data in modeling word level correlations. The use of Bidirectional Encoder Representations from Transformers (BERT) also alleviates the problems associated with BoW to produce sentence-level representation. The DCE can be computed as [109]:\nDCE = 1mv mv\u2211 k=1 1 n \u2211 si ,sj\u2208S,i 6=j [M (si)+M (sj)].[\u03b4(1\u2013J (si , sj))\n+ (1 \u2013 \u03b4)(1 \u2013 \u2329 BT (si),BT (sj) \u232a ], (19)\nwhere S is the sentence set with cardinality n (i.e., if each video comprises 10 captions, n maybe 20 because each sentence is used to calculate the similarities with others), mv is the number of videos, \u03b4 is the adjustment coefficient, M (s) is the METEOR score of candidate sentence s, BT (s) is the sentence vector encoded by BERT and \u3008\u3009 represents the Jaccard and cosine similarity."
        },
        {
            "heading": "2.5 Training Loss of Video Captioning",
            "text": "One of the key considerations in machine learning is the computation of the training loss and VC is no exception.\nSuppose V is video and its ground-truth caption y=[y1,y2, \u00b7 \u00b7 \u00b7 ym ] from a dataset on which the model is being trained D, the function for loss is computed as: [113]\nL = LCE + \u03bbCALCA, (20)\nwhere LCE is the cross entropy loss and LCA is the cross attentive loss. The LCE is obtained by taking the log-likelihood which is negative to bring about the correct caption as:\nLCE = \u2211\n(V,y)\u2208D \u2211 t (\u2013 log P (ym |V, y1, y2, \u00b7 \u00b7 \u00b7 ym)) . (21)\nIn order to improve the sentence semantic coherence, in addition to LCE , LCA loss is also used. In order for a semantic group to have a consistent meaning for each of its members, it must have frames that have a high correlation with the relevant phrases. To obtain this, a negative video that does not overlap with the input video\u2019s caption is chosen at random from a collection of videos. The frames of the negative video are provided as erroneous candidates for the semantic aligner. The positive relevance score \u03b1positivei,j,t between a phrase Pi and an input frame Vj , and the relevance score which is negative is computed between a phrase Pi and negative frame Vnegj . Later the obtained positive and negative scores are normalized by implementing the softmax. Let Probca(sei,t)= \u2211m 1 \u03b1 positive i,j,t indicates the probability that the semantic group sei,t will not have any frame which is negative. Probca(sei,t) increases with a positive relevance score compared to the negative relevance score and hence it is named contrastive attentive loss. The contrastive loss is given by Eq.(22).\nLCA = \u2211\n(V,y)\u2208D \u2211 t mt\u2211 i ( log Probca(sei,t ) , (22)\nwhere mt is the surviving phrases after filtering with the phase suppressor. Most VC methods use a single decoder in training. While, Lin et al. [114], for the first time, trained multiple decoders to boost the accuracy by reducing mimicry loss using Augmented Partial Mutual Learning (APML). The mutual learning strategy is used to transfer knowledge between various types of decoders such as LSTM, GRU, and transformers. Unlike the work of [115], which trains two models, i.e., teacher and student, in parallel and the presentation of the student model is enhanced under the teacher\u2019s guidance, in the mutual learning all the decoders act as teachers and try to improve their performance by minimizing the mimicry loss. In addition, Chen et al. [116] improved the accuracy of the VC by minimizing a sentence-length-modulated loss function."
        },
        {
            "heading": "3 Review of Video Captioning Methods",
            "text": "In this section, the main deep learning methods and findings in VC research are discussed (refer to Fig. 1 for the taxonomoy of methods)."
        },
        {
            "heading": "3.1 Attention-based methods",
            "text": "Attention-based models (e.g., transformers) that use deep learning [117], [118] are heavily utilized in NLP applications such as machine language translation [119], [120], [121], dialogue generation [122], machine reading comprehension [123], natural language inference [124], and\n11\nVC [125], [126]. In sequence-to-sequence networks, the context of preceding words and the encoder\u2019s fixed output are used by the decoder to construct the subsequent word. Therefore, it is possible that the overall context of the encoded information is lost, making the output extremely reliant on the most recent hidden cell state. By storing the context from the start to the completion of the sequence, the attention mechanism\u2019s adoption fixes the problem [127]. In view of this, the attention mechanism is used in VC, which learns when and what portions of the video the decoder has to attend to. The incorporation of attention mechanisms helps the model in locating relevant regions of the input. Since not every frame in a video is equally important to the video, the attention mechanism on the decoder side determines a weight distribution for each frame. There are several attention mechanisms used in VC such as Soft-Attention (Soft-Attn), Hard-Attention (Hard-Attn), Global-Attention (Global-Attn), Local-Attention (Local-Attn), Text-Attention (Text-Attn), Temporal-Attention (Temp-Attn), SelfAttention (Self-Attn), MHA, and Stacked-Attention (Stacked-Attn)."
        },
        {
            "heading": "3.1.1 Soft Attention",
            "text": "The attention states used in Soft-Attn are based on the previous hidden state of RNN and visual features [128], [136], [144]. The context vector is computed as the weighted sum of all previously hidden state features and computed using Vt = \u2211N k=1 \u03b1t,ih e i where h e i are previous hidden state of encoder, \u03b1t,i are weights and t is the time step. These weights \u03b1t,i act as alignment mechanism which give higher weights to certain encoders\u2019 hidden states that allow them meet that decoder time step better, and are calculated as:\n\u03b1t,i = exp ( et,i )\u2211N\nj=1 exp ( et,j ) . (23)\nThe unnormalized relevance score is calculated using:\net,i = wT tanh(Wahei + Wbh d t\u20131 + b), (24)\nwhere w, Wa , Wb are learnable parameters. The context vector is given by weighted sum of the encoder hidden states and is given by:\nct = N\u2211\nj=1 \u03b1t,ihei . (25)\nSoft-Attn is fully differentiable and can employ end-to-end backpropagation. The softmax weights non-zero probability to unimportant elements, which will deteriorate the attention given to the few significant elements. A number of VC methods use Soft-Attn [31], [145], [146], [147]."
        },
        {
            "heading": "3.1.2 Hard Attention",
            "text": "Hard-Attn [148] selects a key region in video based on a multinoulli distribution and requires Monte Carlo sampling to train. It relies on the detection of objects and is trained by augmenting the approximate variational lower bound [149]. The Hard-Attn model is based on sequential sampling and it is non-differentiable. It relies on policy gradient reinforcement algorithms to compute attention weights."
        },
        {
            "heading": "3.1.3 Global Attention + Local Attention",
            "text": "A Global-Attn [150] mechanism assesses the attention weights of the temporal features which are global (i.e., not localized to a particular region of the input). Local-Attn [150] mechanisms assess the attention weights of the object features which are constrained to local regions of the input. Jin et al. [150] fused global temporal features and local object-based features in a complementary way to create a multimodal attention mechanism. Peng et al. [151] introduced tags in language decoding by using the global control of the text and local strengthening of it during training. In another study, Deb et al. [142] proposed a variational stacked local attention network (VSLAN) which consists of a local attention network (LAN) and a feature aggregation network (FAN), exploits visual features from assorted pre-trained models for VC. LAN attends related clips of a video using bilinear pooling. Then, FAN aggregates the features in a way that previously learned information is discounted from the preceding LAN. Both LAN and FAN are integrated into the decoder for captioning."
        },
        {
            "heading": "3.1.4 Spatio\u2013temporal attention",
            "text": "This category gives attention to salient regions within the frames in the video. In temporal attention, the attention mechanism focuses on keyframes. In Spatio-temporal attention, both spatial and temporal attention are exploited. Chen et al. [19] and Yao et al. [128] developed a temporal attention method that utilizes attention weights to enable the decoder to concentrate only on a specific group of frames. The authors in [133] proposed Spatial-Temporal Attention Mechanism (STAT) using 2D-CNN, 3D-CNN and RNN as encoder and LSTM as decoder. STAT not only emphasizes on essential frames but also on critical areas within those frames, enabling the decoder to gather sufficient input data and perform precise decoding. In addition, Liu et al. [152] formulated a spatiotemporal attention model by placing emphasis on the objects in the video at the fine-grained region level. A Multi-level Attention (MAM) to encode the video features at the frame and region level is proposed in [129]. It is able to focus on the most correlated visual features in generating the correct caption. Region level attention layer is used to extract the salient regions in the video and the frame level attention layer is used to derive a subset of frames that is correlated to the video caption. In [130], the authors proposed the Gaze Encoding Attention Network that leverages spatial and temporal attention. Spatial attention produces feature pools that are regulated by gaze maps, as well as temporal attention identifies a sample of feature pools for the generation of words by decoder modules. Additionally, in [153], they incorporated a succession of visual features which are given as input into the LSTM encoder, and a GRU decoder assists in producing the sentence. This innovative work\u2019s fundamental idea is the utilization of a CNN and Reinforcement Learning (RL) to decide whether or not a frame has to be encoded.\nIn another study, Chen et al. [131] proposed VC based on Two-View Transformer and fusion blocks. The two fusion blocks used in the work are add-fusion and attentive fusion. There are two benefits to an attentive-fusion block over an add-fusion block. Firstly, the weights assigned to attention vary according to the circumstances of the current position. Secondly, by choosing various modalities like representation of frame, representation of motion, as well as previously\n12\ngenerated words, the decoder is able to select a quality associated context that will jointly direct the description generation methodology. The researchers in [64] emphasized on fine-grained object interaction for video understanding using Attention. LSTM which is called SINet-Caption, as indicated in Fig. 6. The proposed model uses both fine-grained (object) and coarse- (overall image) visual representations for each video frame. Temporal and co-attention are employed for obtaining high-level object interactions in video caption generation. The authors in [134] used hierarchical layers of LSTM and two attention layers. The two attention layers capture the information at the frame level and a novel deep VC architecture which combines a textual memory, a visual memory, and an attribute memory in a hierarchical way to guide attention for efficient video representation extraction and semantic attribute selection [95]. Perez et al. [139] introduced a VC by incorporating two attention models-temporal and adaptive attention. The temporal attention in the VC focuses attention on the keyframes. The adaptive attention mechanism uses two LSTMs as a fusion gate. The fusion gates determine when to provide visual features and when to provide the semantic context information from the bottom and top LSTM layers. Study [140] proposed textual temporalbased attention for improving captioning by introducing pre\u2013 detected visual tags that not only belong to textual modality but also can convey visual information. In another research, hierarchically combines spatial and temporal attention in two different orders: (i) spatiotemporal (ST), and (ii) temporalspatial (TS) attention. ST, first, applies spatial attention and linear pooling on features extracted from each frame and then applies temporal attention. In another study, STaTS [154], which hierarchically integrates spatial and temporal attention in two separate orders: (i) Spatio-Temporal (ST), and (ii) Temporal-Spatial (TS) attention. ST first performs linear pooling and spatial attention to the characteristics that were retrieved from each frame, after which it incorporates temporal attention. In addition, an LSTM-based ranking strategy is formulated to preserve the temporal order. However, all words in the caption may not depend on features that are varying temporarily. A proposed solution, called TS, addresses this problem by first applying temporal attention to identify specific frames to respond, followed by spatial attention to the\n13\nspatial feature representations of selected frames."
        },
        {
            "heading": "3.1.5 Self-Attention and Multi-head attention",
            "text": "The Self-Attn and MHA are widely used in transformers as discussed in section 2. In Eq.(1), If the three values q, k, and v belong to the same feature, then it is called Self-Attn, otherwise it is called cross-attention. Each scaled-dot product module is called a head and in MHA, multiple heads are used to boost the captioning accuracy as given in Eq.(2) and Eq.(3). In [135], self-attention dense LSTM and latest guiding dense LSTM are used to get differential weights in the attention mechanism and to speed up the captioning process [135]. The authors argued that if the attention schemes based on summation and concatenation to fuse previous hidden states, it would result same weight to predict the target word, and therefore the authors employed Self-Attn scheme. Research [155] designed VC based on a symmetric bidirectional decoder. The authors exploited Self-Attn and MHA. MHA is used in the work for past and future predictions. A low latency online VC is proposed based on a transformer with a multi-modal extension of video and audio in the work of [138]. The flaw in MHA is the attention affected by the similarity of the features in multiple frame sequences [137]. The authors pinpointed the loopholes in MHA that they were in a deficit of coherence and often considers unrelated features. The lack of sufficient procedures that can add to the contents of the frames is the cause. For that issue, Self-Aware Composition attention offers a solution."
        },
        {
            "heading": "3.1.6 Other Attention Mechanisms",
            "text": "For multiple data frame-based problems, self-aware composition attention gave importance to classifying the regions of interest, but also on expanding the knowledge regarding the useful frames. This will lessen pollution and might even benefit in improved composition. The loopholes of the present VC such as the existence of gaps in current semantic representations and the generated captions are prevented in [139] by proposing Attentive Visual Semantic Specialized Network (AVSSN). AVSSN can choose to decide when to use visual or semantic information in the language generation process.The authors in [156] formulated a syntax-guided hierarchical attention network, which exploits semantic and syntax cues to bridge the gap between the visual and sentence-context features for captioning. A globally-dependent context encoder is introduced to extract the global sentence-context feature that gives flexibility in generating non-visual words. Later, hierarchical content attention and syntax attention are used\nin captioning. The semantic alignment between the vision and language in captioning is improved by introducing a semantic alignment refiner. This refiner minimizes the distance between the original feature and reproduces one which is expected to be as clear as possible.\nA few studies have recently centered on dynamics fusion by exploiting multi-modal features. In accordance with the status of the model, task-driven dynamic fusion (TDDF) [157] linearly combined heterogeneous data. The authors of [158] suggested an attentional fusion (AF)-based modality-dependent hierarchical fusion network. In [91], a many-to-many multitask learning model with an attention mechanism (attentionbased sequence-to-sequence model for VC) was proposed (see Fig. 7). The authors in [159] proposed a trajectory-structured attentional encoder-decoder (TSA-ED) neural network framework for VC. In the encoder-decoder framework, LSTM is used and sentence description depends on the moving trajectory of objects in the video.\nTemporal Convolutional Block (TCB)\nTemporal Convolutional Block (TCB) ContentSemantic\n14\nIn addition, a variational Part-of-Speech (POS) encoder (VaPEn) has been proposed to enforce diversity in sentences. The proposed method by Hou et al. [161] consists of a video POS tagging and visual cue translation that are jointly trained for VC. It considers both visual perceptions and syntax representations for generating captions. Specifically, POS tags are used for representing syntax structure. It also uses a mixture model to implement the visual perceptions. The mixture model translates visual cues into lexical words conditioned on the extracted syntactic structure. Also, there are different feature fusion with attention mechanism used in VC. For example, HATT [160] is a multi-modal fusion model based on hierarchical attention strategy. It fuses different modalities with attention manner to exploit the complementariness of multi-modal features. HATT consists of three attention layers: (i) low-level attention for dealing with temporal, motion, and audio features, (ii) high-level attention for focusing on semantic labels, and (iii) sequential attention layer for incorporating information obtained by low-level attention with high-level attention. Fig. 10 clearly shows the main differences between the three well-known attention-based multimodal fusion methods, i.e., AAMF, AMF, and HATT.\nHierarchical Representation Network with Auxiliary Tasks (HRNAT) [101] consists of two components: (i) hierarchical representation network (HRN), and (ii) auxiliary tasks. HRN includes a hierarchical encoder module for representing multilevel visual concepts and a description generation module. The former obtains contextual features by employing different multilevel concepts, and the latter generates sequential descriptions. In addition, HRNAT learns three auxiliary tasks to obtain rich semantics-aware, syntax-aware and content-aware cues in a self-supervised manner. Specifically, the semanticsaware cues of hierarchical semantic representation are learned by the encoder from the cross-modality matching task. While the syntax-aware and content-aware cues during a language generation process are obtained by the decoder.\nIn [162], a recurrent region attention module is proposed for extracting multiple regions from each video frame. Then, a motion-guided cross-frame message passing is designed for encoding spatial information for VC. Video representations are compacted flexibly by proposing an adjusted temporal\ngraph decoder. It updates temporal relations between video features. Most methods that use pre-trained 2D CNN for extracting spatial information do not preserve the spatial structure. To alleviate this issue, motion-guided spatial attention (MGSA) [163], which is an adaptive spatial attention mechanism, was proposed. It aggregates the extracted spatial feature map by a spatial attention map. Specifically, motion is used as guidance for spatial attention, and a gated recurrent attention unit is devised for establishing the relation between temporal maps. In addition, a hierarchical visual textual graph is constructed to extract rich semantic information."
        },
        {
            "heading": "3.2 Graph-based Methods",
            "text": "Graphs are data structures consisting of nodes and the nodes are linked by edges. The nodes represent entities such as objects [164] and actions [165], and the edges represent the interaction among the entities. Graphs are used in the real world for depicting objects and their association in various fields such as social networks, e-commerce networks, biology networks, traffic networks, etc., [166]. The objects in the video and the relationships between them have been proposed for achieving the visual tasks of action recognition [167], and video retrieval [168]. Existing VC methods generate captions based on either a scene or object level by operating directly on raw pixels of 2D-CNN or 3D-CNN to capture the higher level of semantic interactions. Their main limitation is that they rely completely on the attributes of objects and ignore the cross-object transformations [169] or object dynamics in temporal framework [169], [170]. While graph-based methods operate on the correlation of higher-level semantic entities and their interactions [171]. MaskTrackRCNN [172] can concurrently perform object detection, instance segmentation, and object tracking. The method is applied to m-frame video to obtain the object trajectories. Graph-based methods employ tracking on VC algorithms to extract the object trajectories in the temporal domain.\nThe following graph-based approaches are used in video captioning. They are given below:\n15"
        },
        {
            "heading": "3.2.1 Temporal graph and Bidirectional Temporal graph based",
            "text": "VC Since the objects in the video across consecutive frames are dynamically changing in shape and location over time, so it is required to \u201ctrace\u201d the objects\u2019 temporal trajectories for capturing the detailed trajectories of intra-frames of the video content. Since the objects do not appear in all the frames of the video, it gives rise to challenges for trajectory capture. To address this issue, object alignment in two directions is required. The bidirectional temporal alignment is used to describe two temporal trajectories for each object if objects are visible only in a few frames of video. Object Aggregation Bidirectional Ttemporal Graph (OA-BTG) [173] is proposed to exploit the salient objects with their detailed temporal dynamics and represent them using discriminating temporal representations by performing object-aware local feature aggregation on detected object regions. The Bidirectional Temporal Graph (BTG) comprises both forward and reverseorder graphs along the temporal order, that capture temporal trajectories of complementary information for each salient object instance. The capture of the detailed temporal dynamics for objects in the global context accurately describes the finegrained captions. Object aware aggregation for each salient object instance by using a learnable Vector of Locally Aggregated Descriptors (VLAD) model on the temporal trajectories to obtain discriminative representation. In object aggregation (OA), two VLAD models are used to learn spatiotemporal correlations of objects at local and global levels to encode discriminative representations. The attention mechanism is a hierarchical structure comprising two attention mechanisms i.e., temporal attention and object attention. The temporal attention attends object regions at different time steps and represents N object VLAD representations into one representation. Object attention is used to describe to discriminate the contribution of different object instances. Zhu et al. [174] designed a video-based object-oriented VC network (OVC)Net via a temporal graph and detail enhancement. They used mask-RCNN for object isolation, segmentation, and tracking. In addition, an object-oriented temporal graph is constructed to understand the ST evolution of the motion trajectory in different frames. They also used a detail enhancement module for capturing the distinct features among the different objects to enhance the accuracy of the VC. This technique is the first scheme that generates holistic descriptions of detailed descriptions for specific objects under small samples. If the scenes and objects in the video are dynamic in a relatively long proposal, the existing methods cannot interpret the scene evolution within the temporal proposal framework."
        },
        {
            "heading": "3.2.2 Spatio-Temporal graph-based VC",
            "text": "Spatio-temporal graphs consider trajectories of objects in inter-frames and intra-frames. Zhang et al. [173] utilized a BTG and object-aware aggregation in which a trainable VLAD is employed for the object feature aggregation. Xiao et al. [7] exploited spatial-temporal information of the videos in the framework of VC by constructing Temporal Graph Network (TGN) and Relation Graph Network (RGN). TGN focuses on the sequential information of frames while RGN is designed to exploit the relationships among salient objects within a given frame. Graph Convolution Network (GCN) is used for encoding frames with their sequential information\nand building a region graph for utilizing object information. A stacked GRU decoder with a coarse-to-fine structure for caption generation. OA-BTG [173] utilized a BTG and objectaware aggregation in VC. Compared with it, Object-aware Spatio-temporal Graph (OSTG) [175] further explores the intra-frame spatial relationships among objects along with a BTG to improve video captioning accuracy. A trainable VLAD is employed for the object feature aggregation."
        },
        {
            "heading": "3.3 Multi-granularity graph-based VC",
            "text": "GLMGIR [176] is a graph-based fine multi-granularity interaction representation technique to model fine-granular actions. The model was designed and evaluated to model team interactions in sports. Three types of interactions are observed, 1) local interactions emphasizing joints of humans, 2) semilocal interactions which are among two or three players; 3) global interactions that reflect a strategic move in offense or defense. GLMGIR [176] develop a systematic interaction representation learning method that encodes different granularity of interactions jointly and well explores their dependency structure. Multi-granular attention Module (MAM) is used to generate attention weights to choose the proper contextual temporal regions for the subsequent sentence generation. On the other hand, it also generates granularity weights that dynamically select the correct/proper granular feature that maps to the specific sentence. GCN Meta-learning with Multigranularity POS (GMMP) [177] is meta learning model based on GCN that uses POS information for VC. It uses GCN to model the temporal dynamics in the video for capturing temporal structure from streaming frames. Specifically, a simple graph convolution (SGC) [178] is used to reduce the complexity of the conventional GCN. In addition, a multigranularity POS attention mechanism is introduced to use POS information in both word and phrase levels for generating high-quality sentences. In order to avoid deteriorating performance during tests, RL is used to optimize sentencelevel metrics by adopting a meta-learning strategy."
        },
        {
            "heading": "3.3.1 Semantic graph-based VC",
            "text": "Semantic graphs are used in VC to produce meaningful captions. This problem is addressed in by Zhang et al. [179] who proposed the Graph-based Partition-and-Summarization (GPaS) framework. In the graph, the semantic words are used as nodes, and the interactions of objects are as edges by coupling a GCN and a LSTM for text summarization. Most video captioning algorithms suffer from the long-tailed problem. In long-tailed problems, content-specific words appear less frequently than common words and function words. Zhang et al. [180] proposed Object Relation Graph (ORG) at the encoder to collect object interaction features to improve the visual representation for VC. They employed a Teacher Recommended Learning (TRL) method to exploit the successful external language model (ELM) to integrate huge linguistic expertise into the caption model. The ELM produces more semantically similar word proposals which extend the ground truth words used for training to curtail the long-tailed problem. Bai et al. [181] proposed video summarization by the latent semantic graph. The conditional graph used in this work is not in the conventional form of a semi-positive indefinite affinity matrix. Rather it takes region-level object\n16\nproposals conditioned on ST information of video frames. In this, a dynamic graph is used instead of a static graph that links enhanced object proposals with randomly initialized nodes. In other words, a large number of enhanced object proposals is summarized into high-level visual knowledge with dynamic graphs. The latent semantic visual words are then fed into the language decoder. A language decoder comprises an attention LSTM network for weighting dynamic visual words and a language LSTM network for producing semantically rich captions. If pre-trained detection models are used in VC, it is possible to lose important semantic concepts, since the model is not trained on the video captioning datasets, it would miss some semantic concepts that are not defined during pre-training. Wang et al. [182] proposed Cross-Modal Graph (CMG) that is constructed with the cross-modal meta details. In Cross-modal meta concepts, a weakly supervised learning method is used to identify the correspondence between visual regions of the given words of target captions. In this model three types of cross-modal graphs are used, one is graphs at video level, the second is the graph at frame level and the third is dynamically formed cross-meta graphs to predict better the object interactions and relations in the VC. Object relation graph and multi-modal feature fusion Object Relation Graph and Multimodal Feature Fusion (ORMF) [183] leverages the relationship between objects in the video and the correlation between multi-modal features. It constructs an object relation features graph using the similarity and Spatiotemporal relationship of objects in video and encodes the object relation using GCN. In addition, a multi-modal feature fusion model is constructed for learning the relationship between features of different models and fusing the features. Li et al. [184] used a long short-term graph (LSTG) to capture short-term semantic relations and long-term transform dependencies. The authors introduced a global gated reasoning module (GGRM) to prevent the relation ambiguity problem in the VC. The Self-Attn block is replaced with GGRM in the transformer block to fully exploit the object\u2019s relations for enhancing the captioning accuracy. In another study, Wang et al. [185] proposed a semantic association graph for the analysis of multi-modal interactions. The semantic association graph has two modules: (i) semantic association graph module (SAGM), and (ii) multimodal attention constraint module (MACM). The SAGM builds the topological graph between semantic attributes and utilizes GRU to learn long-range temporal patterns. MACM is used to capture complementary visual features and filter superfluous visual features. A summary and comparison of graph-based VC methods are presented in the following and Table 5."
        },
        {
            "heading": "3.3.2 Challenges in graph-based VC",
            "text": "\u2022 Increasing the number of convolution layers degrades the\nperformance. So developing deeper structural patterns of convolution layers in GCNs is still an open challenge for the researchers. \u2022 Most graph-based video captioning [186] has relied on static graphs. However, in reality dependancies are often dynamic. To this end, learning static graphs may not provide optimal performance. \u2022 Most of the graph-based VC algorithms are based on the aggregation of objects in inter and intra-frames of video and theoretically modeled with a one-dimensional\nWeisfeiler\u2013Lehman graph isomorphism test. Graph isomorphism networks have reached their limit [187]. \u2022 There is scope for graph neural network-based video captioning algorithms that employ recurrent GNNs, convolutional GNNs, graph autoencoders, and spatial-temporal GNNs. \u2022 In VC object detection-based schemes might result in the generation of unnecessary object bounding boxes or meaningless relationship pairs. Further, these schemes rely on a ranking of probability for outputting relationships, which will result in semantically redundant relationships [61].\nSummary\nWe have comprehensively covered the work on graphbased VC. Most of the research used the MSVD and/or MSR-VTT datasets as benchmarks for evaluation. BLEU, METEOR, ROUGE-L, CIDEr are used for the evaluation of graph-based video captioning algorithms. The graph-based video captioning can be improved by considering inter-frame and intra-frame object interactions. Not only object features, but considering frame features will improve the performance of graph-based VC. Instead of using static graphs, dynamic graphs may depict object interactions well in time and space. The captioning accuracy can be improved by minimizing the loss function. There are several challenges in the graph-based VC."
        },
        {
            "heading": "3.4 Reinforcement Learning",
            "text": "Combining the representational learning power of deep learning with existing RL methods can be helpful in VC applications [188]. The main attraction of RL is that it automatizes the process which requires no human interference [189]. In RL, an agent updates their positions based on rewards or penalties resulting from actions performed in a given environment. The definition of an RL task has four main considerations, the policy, reward, value, and model with which the agents are interacting. The policy defines the agent\u2019s behavior at a given instance of time. It determines the actions to be taken in an environment. The reward function defines the goal of RL. The values are predictions of the reward. Lastly, the model determines a set of actions to be taken to obtain a future course of action before they are actually experienced [190].\nTwo problems in captioning models exist when they are trained to maximize the likelihood of the next the word given the previous ground-truth input words [192]. This approach has two problems, 1) an objective mismatch problem and 2) an exposure bias problem. The objective mismatch occurs due to the objective function optimized at training time being different from the true objective. Mathematically, it is similar to minimizing a weighted squared error in which accuracy on the least probable captions is given higher priority and therefore poorly fit for applications such as VC. The greedy output eventually returned by the system \u2013 is given the lowest priority of all during training which may result in erroneous captions. Another problem is the exposure bias problem [192] in which there is a large disparity between the distribution during training and testing time. During training, the model\n17\nPraveen et al. [197] 2021 WAFTM7 MSVD 34.2 92.43 70.8 50.4 1 CIDEnt-reward RL model; 2 Consensus-based Sequence Training with the Reward-Weighted Cross Entropy; 3 Hierarchical RL; 4 End to End with Beam Search; 5 Reconstruction Network; 6 Semantic-Reinforced Learning; 7 Weighted Additive Fusion Transformer with Memory Augmented Encoders.\n18\nis only exposed to the word sequences from the training data, while at testing time, the model instead only has access to its own predictions; therefore, whenever the model encounters a state it has never been exposed to before, it may behave unpredictably for the rest of the output sequence. To tackle these two problems RL based policy gradient is used in the framework of VC.\n3.4.0.1 VC techniques employing policy\u2013gradient RL: Pasunuru and Bansal [191] proposed RL based on a policy gradient algorithm [198]. In this work, an RNN can be considered as an agent, which interacts with the complex environment (the textual and video context every time step). The parameters of the agent defined a policy, whose implementation marks the agent picking an action. In the sequence generation setting, an action refers to predicting the next word in the sequence at each time step. After taking an action the agent updates its internal state (the hidden units of RNN). Once the agent has reached the terminal condition, it obtains a reward. The authors approximated the gradients via a single sampled word as in [199]. Hence, for training, the authors employed a mixed loss function, which is a weighted combination of the cross-entropy loss (XE) and RL loss. The authors employed a decomposable, intra-sentence attention model of [200]. The reward employed in the work, an entailment score to correct the phrase-matching metric (CIDEr), and the reward is used as a CIDEr-entailment score. The training error is minimized by taking the negative reward function as the objective function, which is given by:\nL(\u03b8) = \u2013Ews \u223c \u03c1\u03b8 [r (ws)] , (26)\nwhere ws is the word to be sampled, \u03b8 is the model parameters, \u03c1\u03b8 is the policy. The gradient of this loss function is defined as:\n\u2207\u03b8L(\u03b8) = \u2013Ews \u223c \u03c1\u03b8 [r (ws)\u2207\u03b8 log (\u03c1\u03b8 (ws))] . (27)\nIn practice, gradients estimated based on (27) are unstable and variance reduction is performed using a baseline b as\n\u2207\u03b8L(\u03b8) = \u2013Ews \u223c \u03c1\u03b8 [r (ws \u2013 b)\u2207\u03b8 log (\u03c1\u03b8 (ws))] . (28)\nPhan et al. [192] formulated a Consensus-based Sequence Training (CST) scheme to produce video captions. It is an alternative to RL but uses the multiple existing training-set captions in a novel way. Firstly, CST performs an RL like pretraining, but with captions from the training data replacing model samples. This alleviates the objective mismatch issue existing in cross-entropy loss estimation. Second, CST applies RL for fine-tuning using average CIDEr among training captions as the baseline b. This fine-tuning additionally removes the exposure bias problem. The two stages of CST allow objective mismatch and exposure bias to be assessed separately and together establish a new state-of-the-art task in VC. Chen et al. [153] proposed a plug-and-play VC (Picknet) based on RL. The reward is given to a subset of frames based on two factors: (i) maximizing the discrimination in visual features, and (ii) minimizing the disparity between the generated caption and the ground truth. The rewards considered in this method are based on the language reward and visual diversity reward. The rewarded agents will be chosen and the corresponding latent representation of the encoder-decoder framework will be updated for future trials.\nThe training is carried out in three phases. In the supervision phase, the encoder and decoder are pre-trained and in the second phase, RL is used to pick key-frames. In the final phase, picknet and encoder-decoder framework are trained with the cross-entropy loss. The results show that the selection of keyframes does not deteriorate the captioning accuracy. End to End learning in video captioning is complex due to input video and output captions being lengthy sequences. The end-toend video sequences are both memory-consuming and datahungry, making it extremely hard to train. Li and Gong [194] proposed to multitask RL approach for training the E2E VC model. The RL is introduced to regulate the search space of the E2E neural network, from which an E2E VC model can be found and generalized to the testing phase. Zhang et al. [195] introduced a reconstructor block in the existing encoder-decoder framework. This block captures latent representations from the encoder to the decoder (forward flow) and similarly from the encoder to a decoder (backward flow). The reconstructor reconstructs the global or local framework of the video using the intermediate hidden state pattern of the decoder as input. Instead of using log-likelihood which is negative to bring about an accurate description sentence, the authors employed a reward function based on policy gradient using RL algorithm. The reconstructor is formed by using LSTMs. The encoder-decoder approach is viewed as an \u201dagent\u201d that communicates with the environment, while the video descriptions and sentences are regarded to form the \u201denvironment\u201d. The policy \u03c0theta predicts a word at each step t for agent LSTMs and then updates \u201dstate,\u201d which refers to the hidden states and cell states of the agent. The authors used CIDEr as the reward.\nRNN encoder used in VC are used to encode a video along the forward direction (from start to end), the backward direction, or in both. In the sequential RNN structure, the current input signal can be attenuated according to the previous hidden state. Thus, in the presence of noisy information at the beginning of a sequence, it has a negative effect on encoding the key information that appears after the noise. To alleviate this problem, Shi et al. [201] proposed an iterative video encoding scheme to predict the key frame and encode a video based on its keyframe, and then integrate it within the captioning model. They combined RL-based training method to jointly train the video refocusing encoder with the captioning model in an end-to-end manner. Gui et al. [149] proposed a semantic-assisted encoder and decoder framework with RL. In this work, the authors employed an attention mechanism at the decoder side which pays more attention to the key frames while producing every word and uses a CIDEr based reward policy gradient RL in training to produce refined captions, Xu et al. [202] proposed a novel deep reinforcement polishing network RL based VC inspired by the proofreading in editing the text by human beings. They employed a reward in deep RL to optimize the global quality of produced sentences. The semantic gap between visual and text is reduced by revising the word errors and grammar errors. by iteratively polishing the generated caption sentences. For better long-sequence generation, the long-term reward in deep RL is adopted to directly optimize the global quality of caption sentences. To reduce the semantic gap between the visual domain and the language domain, the caption candidate is considered an additional cue for VC, which is gradually updated by revising\n19\nthe word errors and grammar errors. Xiao and Shi [109] proposed a model to select discriminant attributes for VC. If attributes are poor, the generated captions may be erroneous. To alleviate the problem, and to enhance the model\u2019s ability to filter redundant attributes, a reward-based RL loss is designed to update the decoding components as:\nLre = \u2013E (RT |S\u03b8) = \u2013 \u2211 w,1\u2208\u03b3 \u03c0\u03b8 ( w,1|S0 ) Q\u03c0\u03b8 ( S0,w , 1 ) , (29)\nwhere S0 is the initial state, Rt is the reward for a complete sequence, w,1 is the produced token, \u03b3 is the vocabulary, \u03c0\u03b8 is the generator policy which influences the action that selects the next token, and Q\u03c0\u03b8 ( S0,w , 1 )\nindicates the action-value function of a sequence.\n3.4.0.2 VC techniques employing Manger\u2013worker\u2013 critic algorithm RL: Wang et al. [193] proposed a hierarchical RL algorithm. This algorithm comprises of three components: a low\u2013level worker, a high-level manager, and an internal critic. The manager sets goals for the worker to accomplish, and the worker generates a word for each time step by following the goal set by the manager. In VC task the manager asks the worker to generate fluent and meaningful sentences, and the worker generates the corresponding words in the next few time steps in order to fulfill the job. The internal critic determines if the worker has accomplished the goal and transmits a binary segment signal to the manager to help it update goals. The whole pipeline terminates once an end-of-sentence token is reached. In this model, both the manager and the worker are equipped with a soft attention module to capture better the temporal dynamics. In training, the manager policy is made deterministic, and the worker policy is stochastic.The manager policy and is denoted by gt=\u00b5\u03b8m(St) and worker policy is denoted by \u03c0\u03b8w(at , st , gt). The worker and manger are trained individually, while training worker goal exploration is ceased. Similarly, when training the manager, the authors consider workers with the Oracle behavior policy. Therefore captions are generated by greedy decoding. Xu et al. [202] formulated a reinforcement polishing network to polish the caption and grammar by introducing word denoise network and grammar checking network in the said model. The main problem in deep learning based RL algorithms is that at encoder, which is impractical to define a trajectory-level reward function which is rich. In general, feedback is minimal and few activities have non-zero returns. Additionally, it is difficult to generate the reward by contrasting the current state with the preferred result. Hence, it is required to exploit sampling strategy to extract key frames in video that can generate meaningful sentences.\nRanzato et al. [199] provided a solution to problems for generative models in generation of text such as exposure bias including loss functions in the sequence model using Mixed Incremental Cross-Entropy Reinforce (MIXER) model. In an incremental learning approach, the random strategy of RL is changed with the cross-entropy trained model\u2019s optimal policy, which involves gradually introducing the model to more and more of its own predictions. Quian et al. [203] proposed a deep RL algorithm based on actor-critic. The fundamental idea behind actor-double-critic is that an agent\u2019s behavior is influenced by both their own personality and their exterior\nsurroundings. The uncertain reward and inadequate feedback in training of conventional RL is avoided and the advantage of actor-double-critic is that it gives steady feedback after every action. Sentences are produced by combining the key frames with the acCCN. The feature combination function in the Codec Network (CCN) provides a merging of visual features that results in an effective semantic modeling for the VC issue.\nZhang et al. [195] introduced reconstructor block in the existing encoder-decoder framework. This block captures latent representations from encoder to decoder (forward flow) and similarly from encoder to decoder (backward flow). The reconstructor reconstructs the global or local framework of the video using the intermediate hidden state pattern of the decoder as input. Instead of using log likelihood which is negative to bring about the accurate description sentence, the authors employed a reward function based on policy gradient using RL algorithm. The reconstructor is formed by using LSTMs. The encoder-decoder approach is viewed as an \u201dagent\u201d that communicates with the environment, while the video descriptions and sentences are regarded to form the \u201denvironment\u201d. The policy \u03c0theta predicts a word at each step t for agent LSTMs and then updates \u201dstate,\u201d which refers to the hidden states and cell states of the agent. The authors used CIDEr as the reward.\nIn addition, Xiao and Shi [109] proposed a model to select discriminant attributes for VC. If attributes are poor, the generated captions may be erroneous. To alleviate the problem, and to enhance the models ability of filtering redundant attributes, a reward-based RL loss is designed to update the decoding components as:\nLre = \u2013E (RT |S\u03b8) = \u2013 \u2211 w,1\u2208\u03b3 \u03c0\u03b8 ( w,1|S0 ) Q\u03c0\u03b8 ( S0,w , 1 ) , (30)\nwhere S0 is the initial state, Rt is the reward for a complete sequence, w,1 is the produced token, \u03b3 is the vocabulary, \u03c0\u03b8 is the generator policy which influences the action that selects the next token, and Q\u03c0\u03b8 ( S0,w , 1 )\nindicates the action-value function of a sequence."
        },
        {
            "heading": "3.5 Adversial Networks",
            "text": "Generative Adversial Network (GAN) [204] models are widely used in computer vision applications such as image-to-image translation [205], text-to-image synthesis [206], medical image generation [207], synthetic image datasets [208], text-to-image translation [209] video prediction [210] and 3D object generation [211]. There are different flavors of GAN such Conditional Generative and Adversial Networks (CGAN) [212], stackGAN [206], SeqGAN [213], rankGAN [214], leaky GAN [215], MASGAN [216] that have been all be employed in various computer vision applications. GANs are unsupervised neural network comprising two networks, i.e., G and D. The G tries to produce real data given input is a random noise variable with the probability distribution of pz (z) and it maps it to pg over x by a mapping function G(z; \u03b8g) where G is a neural network with model parameters \u03b8g . The Discriminator (D), D(x; \u03b8D) outputs a binary signal to represent to distinguish the real data from fake data as accurately as possible. \u03b8D is the model parameters for the D. The D is tried to maximize the possibility of assigning correct labels to both the training\n20\nsamples and the synthetic data coming from G. The G is tried to minimize log(1 \u2013 D(G(z))) and the objective function V (G,D) is\nV (G,D) = min\nG max D Ex\u223cp(g) |logD(x)|\n+ Ez\u223cpz(z) |log(1 \u2013 D(G(z))| (31)\nThe text descriptions in GAN-based VC models comes with two major hurdles due to the special nature of the representation of the sentences. In image generation, the transformation is from random noise to image via a deterministic continuous mapping, whereas in text generation, a sequential sampling procedure is required in which discrete tokens are sampled in each step [217]. This procedure is non-differentiable which makes back-propagation infeasible; therefore, policy-gradient RL is used. The second problem in the conventional GAN setting the generator, G, receives feedback from the evaluator when an entire synthetic image is produced. This procedure leads to several problems in training, such as vanishing gradients and error propagation. To prevent such problems, a mechanism is used that allows the G to get early feedback calculated based on approximated expected future reward through Monte Carlo rollouts [213]. These two hurdles for captioning are solved by Dai et al. [218] in IC. Compared with images, video has rich content and additional temporal dimension, which make it very difficult to extrapolate the algorithms developed for the images for the video captioning problem [219].\nThe VC based video captioning is categorized into three types."
        },
        {
            "heading": "3.5.1 VC using Adversarial GAN",
            "text": "Yang et al. [219] published the first work on video captioning using a GAN. In their approach, an LSTM is used in the caption generation and is expanded by adding a D module which acts as an adversary with respect to sentence generation. The goal is to maximize the conditional probability of an output sequence (y1, y2 \u00b7 \u00b7 \u00b7 ym) given an input sequence (v1, v2. \u00b7 \u00b7 \u00b7 , vn). The conditional probabilities over the sentences can be defined as follows:\np (y|v) = p (y1, y2 \u00b7 \u00b7 \u00b7 ym |v1, v2 \u00b7 \u00b7 \u00b7 vn) . (32)\nThe authors then used the (33) as an objective for faster convergence and are given by\np (y|v) = p (y1, y2 \u00b7 \u00b7 \u00b7 yt1|v1, v2 \u00b7 \u00b7 \u00b7 vt)\n= [ i= 1]t \u220f P(yi |v, y1, y2, \u00b7 \u00b7 \u00b7 ym\u20131), (33)\nwhere t and t1 are the length of the video and the generated text. In G, the authors used LSTM as the encoder and CNN as a decoder. Soft-argmax function [220] is employed in the last layer of the G which multiplies the exponential term in the soft-max function with a very large constant value. The multiplication makes the values either 0 or 1. Then the obtained array values are multiplied with the array index and summed, which provides the index of the array with the maximum value in the array, thus performing the arg-max on the output. The design used in encoding and decoding of [219] overcomes the training difficulties that arise in the works\nof [204], [221]. They employed Soft-Attn in the G network to improve the accuracy of VC. Pan et al. [222] proposed a VC based on Temporal GANs conditioning on Captions (TGANs-C). The inputs to the G are the noise vector plus caption embedding. The authors made a D network that is very effective not only able to discriminate a fake from a real, one but also in testing whether the video correctly matches the caption by incorporating video, motion, and frame discriminators. The authors incorporated three losses namely-video-level and frame-level matching-aware loss to correct the label of real or fake video/frames and align video/frames with the correct caption, respectively, and temporal coherence loss to emphasize temporal consistency. The authors in [109] paid attention to sentence diversity by utilizing the CGAN. study [223] proposed Structured Trajectory Network via Adversarial Learning (STraNet), an object-based VC algorithm using adversarial learning. The STraNet shows the ability to represent precisely by describing concurrent objects and their activities in detail. The STraNet model added an adversarial D to the caption generation to enhance the inter-relationship between the vision to text."
        },
        {
            "heading": "3.5.2 VC using adversarial GANs with policy gradient RL",
            "text": ": As the caption contains discrete tokens, it is difficult to apply the backpropagation directly. To alleviate this, the GAN system is regarded as an adversarial learning system with RL used to update the parameters of Generator (G). Yan et al. [224] developed VC by using conventional GAN. The generator employs a CNN-RNN framework, combined with a hierarchical attention mechanism based on object and frame level. In the D, the first network is used to extract the difference between the real from the fake, and the second network is used to compare whether the generated caption matches the video content, which aims to make the generated captions consistent with the video content. The policy gradient RL algorithm is used to update the parameters of the G. Xiao and shi [225] proposed a diverse captioning module that comprises two parts. To obtain high-quality descriptions the model is trained with Cross entropy (XE) loss with LSTM. Followed by a bidirectional-LSTM for better visual representation Finally, they employed temporal attention and at last, used a hierarchical LSTM to generate descriptions based on visual features and text attention. CGAN is employed and the inputs to the GAN is a latent variable produced by noise vector and the outputs of text and visual attention networks Lt . This captioning scheme receives a reward at the end. similar to Yan et al. [224], the authors used The policy gradient RL algorithm. The objective of the G is to generate a sequence of words y1:T = { y\u20321, y \u2032 2, \u00b7 \u00b7 \u00b7 , y \u2032 n }\nfrom the start state s0 to maximize the expected end result:\nE [RT |s0, \u03b8] = \u2211 y\u2032l\u2208\u03b3 G\u03b8 ( y \u2032 l |s0 ) QG\u03c6 ( s0, y \u2032 l ) (34)\nwhere s0 is the initial state, RT is the reward at the end of a sentence, \u03b3 is the vocabulary, G\u03b8 is the G policy which decides the anticipating word, QG\u03b8(s, a) are the action value pair of the sequence. The D considers the probability of the real as a reward.\nQG\u03b8(s = y1:T\u20131, a = y \u2032 t = D\u03b7(y1:T , v\u0302) (35)\n21\nwhere v\u0302 is the visual features considered. As D provides a reward value for the end of the sequence. For obtaining the long-term reward at each time step, the objective function of previous tokens (prefix) and also the result of future outcomes are considered. It is similar to giving up intermediary results for the final victory [213], [226]. In order to realize this [225] proposed a Monte Carlo rollout for sampling the last T-t tokens to evaluate the action value Q G\u03b8(s = y1:T\u20131, a = y \u2032 t) given by (36).\n{ l l 1K \u2211K n=1 D\u03b7(y n 1:T , v\u0302), (y n 1:T \u2208 MC (y1:T ; K ) if t < T\nD\u03b7(y1:T , v\u0302) if t = T ,\n(36)\nwhere yn1:t and y n t+1:T are sampled on the Monte Carlo Rollout and the current state. To reduce the instability, the pre-trained network and D have been given a warm start-up. The main contribution of authors is that formulated a new metric called DCE. Hua et al. [227] proposed a GAN based VC by exploiting the GCN. Mask RCNN is used to detect, segment, and track objects simultaneously. Later motion features of keyframes are selected. Then, a novel object scene relation graph is exploited to describe spatial and temporal details between the objects and between the objects and environments. To mitigate the sequence mismatch problem, a control gate and self-looping structure were added to GCN to improve the text correctly representing the video content. A trajectory-based feature representation is used in place of the previous data-driven method to extract motion and attribute information, so as to analyze the object\u2019s motion in the time domain and establish the connection between the visual content and language under small datasets. Finally, an adversarial reinforcement learning strategy and a multibranch discriminator are designed to learn the relationship between the visual content and corresponding words so that rich language knowledge is integrated into the model."
        },
        {
            "heading": "3.5.3 VC using Adversarial GAN with Manager-worker policy",
            "text": "RL Hemalatha and Chandrasekher [228] proposed VC based on a semantic contextual GAN. This model utilizes a D which distinguishes the ground truth description from the generated description for the input video, and a G which produces a description for the given video. The two blocks in G are the manager block that sets goals for the worker and the worker block that produces descriptions based on the goal, visual features, and semantic features extracted from a multilayer perceptron Network (MLP) (as in [229]). The G and D parameters are learned through RL. A multi-channel CNN based discriminator is used. Word2vec [230] embedding vector is used to represent words in the description. The semantically weighted word-embedding vectors are mixed with the features from the 2D-CNN and 3d-CNN and are fed to the CNN. The features from the D are leaked and fed to the G. The manager is trained to generate the goal vector gt which will be used by the worker in sentence generation. The discriminator generates the reward only for a complete sentence. Hence for the intermediate steps the expected reward is generated using Monte-Carlo search. The G and the D are initially trained with the maximum likelihood (ML) method. During\nML training, the gradients are computed. While training the G, the parameters of the D are fixed and vice versa. The manager and worker are trained in an interleaved manner during the training.\nSummary\nTo our knowledge there are only five published papers on VC using adversarial networks. In all cases, Maximum likelihood estimation is used for the gradient computation and RL is used as the text is nondifferentiable. The goal-based rewards and semanticbased rewards are used to improve the accuracy of captioning."
        },
        {
            "heading": "3.6 Non\u2013Autoregressive",
            "text": "Autoregressive decoders generate one word at each time step by conditioning on the previously produced words. Most of the works of sequence-sequence based VC used autoregressive decoding [58], [119]. The flaw in autoregressive decoding is that it suffers from latency which is not tolerable in a real-time applications. The latency is further amplified if the description at the decoder is fine-grained [231], [232], [233]. The latency must be as small as possible in real-time situations such as Neural Machine Translation (NMT). As an alternative, researchers focused on Non-Auto regressive decoding techniques to achieve significant inference speedup [234], [235] in which the words are produced in parallel. The speed-up of the scheme is at the expense of performance degradation. The performance degradation is compensated in the works of [4], [236], [237] by iterative refining of the sentences conditioned on parts or whole of the previous outputs rather than producing in one-shot. The loophole in these methods, instead of taking N prior concurrent words, a stochastic random input is given as the decoder input. This is leading to translation errors due to insufficient context which could greatly influence future predictions and thereby generates inefficient captions."
        },
        {
            "heading": "3.7 Multimodal Video Captioning",
            "text": "Multimodal VC concerns the exploitation of latent deep networks pooled from different modalities (audio, speech, video, image, text, source). Tian et al. [238] proposed a multimodal CNN (MMCNN) based audio video framework capable of learning decoupled audio-text and visual-text deep feature hierarchies. Tian et al. [239] designed a multimodal CNN based audio-visual VC framework and introduced a modality\u2013aware module for selecting the modality during the sentence generation. Xu et al. [47] developed a semantic Filtered Soft-split-Aware model to improve VC by fusing semantic concepts with audio-augmented features extracted from the input videos. Moreover, several topic-guided VC methods that jointly create topics and topic-oriented captions using multimodal features in a multi-task framework have been proposed [240], [241], [242]. Incorporating multi-modality information within a sequence-sequence model has been employed to leverage information from both audio and visual modalities for improving the efficiency of captioning [243], [244], [245]. Varma et al. [246] proposed a multi-modal VC model that uses the audio, external knowledge, and attention mechanism to enhance the captioning process. In [247], the historical\n22\ninformation along with motion and object features using the stacked multi-modal attention network are used for VC."
        },
        {
            "heading": "3.8 Dense Video Captioning",
            "text": "One downside of single sentence VC methods [248], [249], [250] is that they generate sentences with very little information. Dense Video Captioning (DVC) [251], [252] has emerged to detect and describe events via a richer storytelling approach [84], which makes it useful in applications such as CBIR and video recommendation [33]. The DVC can be divided into: (i) event proposal blocks, and (ii) caption generation proposal blocks. Indeed, the first DVC proposed by [84] in which the event proposal module spots events with a multi-scale version of deep action proposals for understanding action and denotes them by LSTM hidden states. To incorporate future context along with past context, Wang et al. [253] proposed a novel bidirectional process to encode both past and future context for localizing event proposals. Early DVC methods considered event proposal and caption generation blocks as independent entities. The two blocks are either trained separately or alternatively, which in turn affects the generated descriptions. To address this problem, Zhou et al. [254] proposed an end to end DVC with a masked transformer. Specifically, a differentiable masking scheme is employed to ensure consistency between the two blocks during training. Transformers have been used in DVC to alleviate the limitations of RNNs when modeling long-term dependencies in videos. Despite their superior accuracy, VC techniques employing masked transformers [254] suffers from unsatisfactory run time performance. To directly address this, Yu et. al. [60] proposed an Accelerated Masked Transformer which reduced run time by 2\u00d7 when compared to the reference Masked Transformer model. The Accelerated Masked Transformer [60] introduced a lightweight anchor-free event proposal integrated with a local attention mechanism stage and the single-shot feature masking strategy along with an average attention mechanism in the caption stage. Lu et al. [255] proposed a lightweight DVC model based on the transformer framework to improve execution efficiency for video-caption generation on edge cameras. They used an attention mechanism to hierarchy extract the most relevant visual and text features for generating captions. The attention mechanism consists of an object attention module and event attention module.\nWS-DEC [163], which is a weakly supervised DVC, improved the captioning accuracy where temporal boundary annotations are not available. The conventional DVC considers the event localization proposal and caption generation proposal in a feed forward manner. In contrast, in WS-DEC, there is a mutual exchange of information between the event localization and captioning modules. The event localizer sends two types of information to the caption generation block. The first information is pertaining to the key words learned during video-text alignment to improve the captioning accuracy and the second type of information is by introducing an induced set attention block, the concept features are extracted and thus WS-DEC can obtain richer information compared to the previous works.\nSketch, Ground, and Refine (SGR) [33], first, describes the whole video by generating multi-sentences paragraph, and then improves the quality of the refining stage by refinement\nenhanced training and dual-path cross attention. VisualSemantic Embedding (ViSE) [35] exploits both visual information and linguistic content in the event detection and caption generation process. After extracting n-grams along with their associated weights, the caption embedding is learned and semantic space is constructed by a Sen2Vec model. Then, a visual-semantic joint embedding network (VSJM-Net) is used to conceptualise the visual information and the semantic space. Lu and Han [255] proposed a lightweight DVC method using transformer which is made robust to environmental changes (i.e., input drift). Keeping the advances in pre-trained multimodal networks, Zhu et al. [256] formulated DVC task as a single sequence-to-sequence network using the transformers. Table 7 provide a summary of several studies on DVC task for VC."
        },
        {
            "heading": "3.9 Paragraph Video Captioning",
            "text": "Generating longer descriptions (but not dense captions) has been explored using paragraph captioning. Yu et al. [263] proposed a paragraph VC by stacking sentence generator and paragraph generator. In this method, an RNN is used for modeling pattern changes over time, a multi-modal layer is used to fuse features from different modalities and an attention model based on spatio-temporal features is used to obtain visual elements. Sah et al. [264] evaluated textual summaries of video using recurrent networks, in which the key frames are segregated in the video based on image quality. Then, these key frames are transformed to text annotations based on sequential encoder and decoder design. In another study, Park et al. [265] developed the paragraph VC by employing the ground-truth event segments to produce the coherent paragraphs. A coherent paragraph generation is proposed by Lei et al. [9] using memory augmented transformer. The incorporation of memory module in the transformer architecture produces a highly summarized memory state from the video segments and from the sentence history. Moreover, Song et al. [10] discarded the event detection stage and generate paragraphs for the untrimmed videos. The temporal attention mechanism is augmented with the dynamic video memories to improve the sentence coherence. In language perspective, a diversity driven training strategy is employed. Liu and Wan [266] produced sentence level captions from the video clips and later summarize these captions to produce the final paragraph caption. This scheme does not rely on ground-truth event segments. In another research, Yamazaki et al. [267] proposed a coherant paragraph captioning by minimizing the contrastive learning Visual-Linguistic (VL) loss. Furthermore, Li et al. [268] constructed emotion and logic driven multilingual dataset EMPVC for the paragraph captioning to establish the logical associations between sentences and discover more accurate emotions related to video contents as studies in [269]. Grounding of language models with features associated with emotion (e.g., facial expressions) has been used to successfully produce more emotionally appropriate language associated with visual content [270]."
        },
        {
            "heading": "3.10 Question and Answering",
            "text": "The Open-domain Question Answering [271], [272], [273] requires a deep learning model to answer any questions employing large-scale documents. Open means not providing the\n23\n1 Contextual information past & future events; 2 Lexical fully convolutional neural networks; 3 Dense Video Captioning with ground truth proposals; 4 Dense Video Captioning with learnt proposals; 5 Bidirectional Single Stream Temporal Action Proposals; 6 Graph-based partition-and-summarization; 7 Temporal-linguistic non-maximum suppression; 8 Applying Common-Sense Reasoning to Multi-Modal Dense Video Captioning and Video Question Answering; 9 Weakly supervised Event Caption Generation; 10 Weakly supervised dense event captioning task; 11 End-to-end dense video captioning with parallel decoding; 12 Accelerated Masked Transformer(L=model depth, d=hidden dimensionality);\n24\nmodel with documents comprising the right answers directly but requires the model to retrieve documents related to the question from the massive corpus and then generate the correct answer based on them. Therefore, a novel mechanism is required to handle cross-modal interactions in open-domain and can be readily deployed in VC tasks. The questionanswering task employs unsupervised pre-training models such as BERT and RoBerta [274], in which the knowledge about the world is gained in an abstract way in the model weights \u2014 making it tough to conclude what expertise has been stored and where it is saved in the model. The network\u2019s size also has an impact on the amount of storage available and, consequently, the model\u2019s accuracy. The conventional method is to train ever-larger networks, which can be extremely slow or costlier, in order to gather more global knowledge. REALM [272] adds a knowledge retriever to the language representation model that first gets a different passage of text from a separate document library, like the Wikipedia text corpus.\nThe REALM retrieval system catered for filling the missing words in a given sentence. REALM employs a rewardbased retrieval system that makes good predictions for the missing words are rewarded else it should be discouraged. Just by using question-answer pairings and no information retrieval mechanism, ORQA [275] is the first open-domain question-answering method in which the reader and retriever are simultaneously learned from beginning to end. In this work, the latent variable considered is the evidence retrieval from all of Wikipedia. It is not feasible to train from the scratch, the authors train the retriever using an Inverse Cloze Task (ICT). In ICT, a sentence is considered as a pseudo question, while its context is taken as pseudo evidence. The ICT needs an option for the corresponding pseudo-evidence among the candidates in a batch."
        },
        {
            "heading": "3.11 Event Captioning from Video",
            "text": "Dense event captioning intends to detect and detail all events of interest comprised in a video. This captioning technique tries to improve the inter-task association between event detection and event captioning. Earlier techniques employ dense temporal annotations for event detection which is a time-consuming process. In order to avoid this problem, Duan et al. [307] introduced a Weakly supervised dense Event captioning (WS-DEC) which does not depend upon temporal segmentation while training the model. In WS-DEC [307], [308], the localization and captioning modules can only receive information from each other. The WS-DEC [307] method utilized an iterative technique, where the event captioner as well as sentence localizer in turn feeds results to one another. Its fundamental premise is that the output of the sentence localizer will converge to a location that is ideal for the event captioner by optimizing the reconstruction losses.\nRahman et al. [308] focused on the issue of WS-DEC in videos and demonstrate that, when paired with video, audio may provide performance that is approximately on par with that of a state-of-the-art visual model. Moreover, Chen et al. [309] developed a multimodal interaction process that utilizes a Channel-Gated Modality Interaction method to calculate pairwise modality interactions, which better accomplishes cross-model information in videos. Li et al. [310],\non the other hand, developed a framework for Multi event detection (MED) and Multi event captioning (MEC). Both MED and MEC may use the grounded concepts to detect the zero-shot video event and generate multimedia event captioning. The semantic representation of events is enhanced by combining TF-IDF with semantic word vector representation of keywords from the events captions. Then, they formulated a Statistical Mean-shift outlier model to remove outliers for generating robust grounded visual features.\nAs a part of submission to ActivityNet challenge 2020, Wang et al. [311] proposed a two plug-and-play modules, Temporal Semantic Relation Module (TSRM) and CrossModal gating (CMG). By utilizing the relationships between events in the perspective of both temporal structure and semantic meaning, TSRM enhances the event-level description. CMG is developed to successfully combine linguistic and visual information in hierarchical RNN. Moreover, Zhang et al. [216] also unified event detection and event detection tasks by using the transformer architecture. The authors suggested a unique pre-training task termed Masked Event feature modeling to learn the video-event representation for event detection and used two pre-training tasks, Masked Language Modeling and Masked Video Feature Regression, to learn the video-text representation for event captioning.\nAs these two sub-tasks share the same model architecture and parameters, the authors trained the unified model with the three pre-training tasks to strengthen the cooperation among the two sub-tasks. In another study, Ji et al. [312] proposed a novel hierarchical context-aware model for dense video event captioning to exploit both the local and global context concurrently. The authors applied different mechanisms such as a flat attention module between the source and local context; a cross-attention module for the source to select the global context."
        },
        {
            "heading": "3.12 Other Relevant Studies",
            "text": "As discussed earlier, usually two different methods are used for encoding and decoding the features of video frames. However, Nabati and Behrad [108] proposed a sequencelearning model for multi-sentence VC. Specifically, they used two LSTM modules for encoding and decoding the features of video frames into the sentence words. The first LSTM encodes the visual descriptor of video frames and the second LSTM determines the output sentence by sequentially generating word probabilities. In addition, they proposed a beam search algorithm that constructs a dictionary of objects using YOLOv3 [313] object detector to describe objects in multisense videos. Moreover, a multi-stage RL algorithm is used to remove incorrect sentences. Event-centric Multi-modal fusion approach for dense Video Captioning (EMVC) [34] consists of three modules including multi-modal feature extractor, temporal event proposal and event captioning. EMVC, first, obtains various features from audio and video by applying a multi-head attention module and stores the obtained information using a hierarchical RNN. Then, during the caption generation, the effects of visual, audio, and linguistic features are regulated by applying a balance gating. Instead of collecting features on a frame-by-frame basis, Semantic Grouping Network (SGN) [113] is based on the semantic grouping of people, objects, and actions. It consists of four modules of\n25\n26\nthe visual encoder, phase encoder, semantic grouping filter, and decoder. The visual encoder extracts features from the frames of the video. The phase encoder produces phrases to the immediately generated captions. The semantic grouping filter removes the correlated phrases and the decoder produces the meaningful sentences.\nChan et al. [314] conducted an empirical of several active learning strategies using two models, i.e., transformerbased and LSTM-based models, and showed that traditional uncertainty sampling techniques are not able to significantly outperform random sampling. In addition, they proposed a cluster-based active learning technique, known as clusterregularized ensemble divergence active learning, for VC that is able to increase the diversity of the samples and improve the performance. In [315], a boundary aware LSTM cell is proposed to discover discontinuity points between frames and segments, and enable the encoding layer to enhance the temporal connectivity. To achieve this, it reinitializes the hidden state and memory cell after estimating an action change. In another research, Sun et al. [316] used multiple languages for VC. After extracting features from multiple languages, the high-level sense semantics are learned for each video. In addition, entity words are learned by applying a leap sampling technique to better represent the video content.\nThe VC model in [317] consists of three branches including language, visual semantic, and optimization branches. Firstly, it integrates language and visual semantic branches with a multi-modal feature-based module. Then, a multi-objective training strategy is used to optimize the model. The outcome of these three branches are fused via a weighted average to predict the word for each frame. The proposed visual encoding technique in [318] extracts features enriched with spatiotemporal dynamics of the scene and semantic attributes of the videos to generate rich captions using a gated recurrent unit. Short Fourier transforms are applied hierarchically to the CNN features of the whole video to process the activation of CNN features.\nMost of the VC techniques operate directly on video and they neglect completely additional contextual information. In order to effectively capture this information, Man et al. [319] proposed a VC based on a scenario-aware recurrent transformer. The dependencies in hybrid models of RNN-CNN is less studied in the literature. An attempt is made in [320] to the GPU-capacity-guided pipelining and EdgeTPU-capacity for maximizing GRU and SRAM utilization. The proposed VC method by Hosseinzadeh and Wang [321], first, forecasts the features of future frames in the semantic space of convolutional features, and then merges the contextual information into those features, and applies it to a captioning module that largely improved the efficiency of the VC. Moreover, Lebron et al. [322] provided a new strategy for training a new VC evaluation score. This method learns to correlate systemgenerated captions to human references based on human evaluations. The majority of metrics attempt to compare the system-generated captions to a single or collection of humanannotated captions. This work proposes a new way based on a deep learning model for evaluating these systems. The model relies on BERT, a language model that has proven to be effective in a variety of NLP tasks. The goal of the model is to learn and make a human-like assessment. This is accomplished by analyzing a dataset of human judgments\nof captions generated by the machine. The dataset is made up of human evaluations of captions generated by the algorithm during different years of the TRECVid video-to-text task. To lessen the uncertainty in the training and test data, numerous human judgments per caption are collected as the human decision is often erroneous. MV-GPT [306] is a pre-training framework that learns from unlabeled videos. It uses future utterances as an auxiliary text to solve the lack of captions in unlabeled videos. MV-GPT trains both multi-modal video encoder and sentence decoder jointly, and uses raw pixels and transcribed speech to directly generate a caption. Iashin et al. [290] obtained a temporally aligned textual description of the speech by applying automatic speech recognition (ASR) system. Besides, they formulated the captioning task as a machine translation task and converted the multi-modal input data into textual descriptions via a transformer technique."
        },
        {
            "heading": "4 Applications",
            "text": "VC has applications in video tagging and retrieval, video understanding, video subtitling, video title generation, action recognition, event detection, content-based video retrieval, accessibility, video indexing, human-robot interaction, video summarization and video recommendation. In this section, we review these applications and methods that have been proposed to address them."
        },
        {
            "heading": "4.1 Video Tagging, Indexing and Retrieval",
            "text": "With vast media collections available online there has developed a need for automatic understanding along with efficient retrieval of these data. Commercial search engines like Google, Microsoft Bing, and Yahoo!, offer video search based on indexing textual metadata related to videos. However, the search performance can be unsatisfactory using the limited amount of manually annotated textual metadata. Manual annotations can be inconsistent and incomplete. To reduce human labeling costs, and create more consistent as well as complete textual data, semantic-based indexing has been developed. The deep features may serve as signatures for semantic details of video helpful in many search and retrieval tasks [323]. The graphbased storage structure used in this model for video indexing allows to retrieval the content aptly with complicated spatial and temporal search queries.\nTags are descriptive keywords that are added to videos. Video tagging is used to index and catalog videos based on content making it easier to search. While different from VC, video tagging is nevertheless relevant and has been used in VC systems. Several related innovations have been introduced to build video tagging algorithms, including introducing POS tags to produce syntactically correct text [324], semantic tags to bridge the gap between the vision and text [325], and integrating a video scene ontology with a CNN [326].\nThe strategy used in video retrieval is, given a text query and a pool of candidate videos, to select the video which corresponds to the text query. Garg et al. [327] proposed combining tagging with a neural network to retrieve relevant videos [90]. The LSMDC dataset challenge [90] comprises four video-to-language tasks. The movie retrieval task involves ranking 1,000 movie clips for a given natural language query. Dong et al. [328] proposed zero example video retrieval in which an end user searches for unlabeled videos. Using a\n27\nTree-augmented Cross-modal Encoding technique, Yang et al. [329] presented video retrieval via more complex questions by mutually learning the language structure of queries along with the temporal representation of videos. Cao et al. [168] proposed a two-stage VC approach, in the first stage, video retrieval was used to find sentences related to a given video from the text corpus, and in the second stage, the retrieved captions are used to guide caption generation.\nCBVR systems serve a critical role in improving human\u2013computer interaction. Several modern medical applications, for example, record the health state of patients in realtime and preserve the video for further examination. While tracking things, surveillance agencies mostly depend on video footage. All of the recorded videos are preserved in a database for later investigation, and indeed the database is also relatively large. The ultimate focus of video storage is to examine videos for decision-making as well as comparison\u2019s purposes. Video retrieval is somewhat more important for good data analysis since it aids in the retrieval of relevant videos and improves decision-making. The CBVR system overcomes the challenges of the text query-based video retrieval systems experience, such as increased computational and time demands. A conventional CBVR design comprises of three key phases: query acquisition, video retrieval, and grading. The CBVR system evaluates the query using advanced image processing algorithms and extracts the videos related to the query as soon as the query is sent to it. Finally, the videos are organized into categories based on their degree of importance. Moreover, since it is not essential to process all the frames in a video, only the key frame must be identified and obtained. The essential frame\u2019s features are extracted and used in the video recovery procedure. This strategy thus saves both memory and time while also optimizing work efficiency.\nAs a consequence, the current requirement for an efficient CBVR system is to retrieve relevant videos in a reasonable amount of time with a higher accuracy rate. Taking this as a challenge, Prathiba and Kumari [330] proposed a useful CBVR system that takes into account both audio and video aspects when delivering appropriate videos to the viewer. The work [330] is accomplished through the use of two modules: video and audio. Video frame extraction using shot detection, key frame detection, plus feature extraction are three crucial steps in the video module. Audio denoising as well as feature extraction are part of the audio module. The feature database is created by combining the features gathered from both modules. The attributes are clustered using the kernelized fuzzy C-means (KFCM) technique, which speeds up video retrieval even further. The Euclidean distance seen between the query and the set of features is then calculated. The closer the video is to the question, the more relevant it is. Pyramid regional graph representation learning (PRGRL) [8] takes control of both local and global techniques, and also compensates for their shortcomings by proactively measuring the \u201dimportance\u201d of a video\u2019s sectors in its frames."
        },
        {
            "heading": "4.2 Video Understanding",
            "text": "VC can be thought of as a downstream product of video understanding, with some form of understanding being a prerequisite, in many cases implicitly, for captioning. Video understanding aims to recognize and localize different actions\nor events in a video. In this regard, Diba et al. [331] proposed a spatio-temporal \u201cHolistic Appearance and Temporal Network\u201d (HATNet). The HATNet combines 2D and 3D architectures by joining intermediate representations that capture appearance-based and temporal features. Structured Graph Neural Network (GNN) models have been used to capture spatio-temporal interactions explicitly under the supervision and implicitly as nodes of spatio-temporal graphs [332], as have perturbation-based models [333]. Huang et al. [334] proposed temporally-adaptive convolutions (TAdaConv) arguing that by calibrating the convolution weights for each frame according to their local and global temporal context it combines spatial and temporal modeling abilities. Global-local video features can also be learned contrastively as leveraged by Zeng et al. [335]."
        },
        {
            "heading": "4.3 Video Subtitling",
            "text": "Video subtitling is a direct application of VC that can be used in video production for translating foreign language programs or for making content more accessible. A vast majority of video content is not manually described by humans, therefore machine-generated subtitles [336], [337], while less accurate, can be very useful. Soe et al. [338] showed that using AI in a semi-automated workflow could support but not replace a human annotator. Guerreiro et al. [339] proposed a monolingual model that effectively showed consistent performance in 100 different languages."
        },
        {
            "heading": "4.4 Video Title Generation",
            "text": "Title generation can be thought of as a very condensed form of summarization or subtitling. Zeng et al [76] addressed this by applying two methods for automatic video title generation. The first approach employed a primer with a highlight detector, jointly training a model for title generation and highlight localization. The second approach promoted sentence diversity, such that the generated titles were also distinct and expressive. To achieve sentence diversity, the authors incorporated novel sentence augmentation techniques."
        },
        {
            "heading": "4.5 Action Recognition",
            "text": "Video action recognition is one of the key research areas used for video understanding. It is used for classifying human action categories in videos [340]. One of the main problems with video action recognition models is that they are often limited to detecting only a few classes, whereas captioning as an alternative is more expressive. Wang et al. [167] proposed a video action recognition task using action clips in a multimodal learning framework with semantic supervision. The main advantage of this model is that it enabled zero-shot action recognition without labeled data or parameter tuning. A bottleneck in video action recognition is that most models can only handle a small number of input frames and are therefore unable to capture complex long-term dependencies. Zhi et al. [341] proposed a motion-guided frame sampling, considering the cumulative motion distribution to ensure the sampled frames cover all the important segments and have high motion salience. Equivalent motion saliency cannot be captured well with a simple spatio-temporal convolution due to the requirement for a very large network. In another research, Kwon et al. [342] proposed a method to capture motion\n28\ndynamics based on spatio-temporal self-similarity (STSS). STSS represents each local region as similar to its neighbors in space and time. It can be deployed in any neural architecture and can be trained end-to-end without requiring supervision."
        },
        {
            "heading": "4.6 Event Detection",
            "text": "Event or anomaly detection is the task of identifying abnormal events in the video. Given the nature of rare events, it is very time-consuming to manually annotate content and therefore automated algorithms are attractive, whether it be objected detection or person re-identification. Huang et al. [343] introduced a temporal contrastive network to address the problem of finding rare events in videos. The algorithm was unsupervised, utilizing deep contrastive self-supervised learning to extract higher-level semantic features and anomaly detection with multiple self-supervised tasks. Pan et al. [344] used the real-world fall (RWF) dataset,in which events are captured with mobile devices or cameras. This dataset recorded fall events, with the goal of helping to identify injuries. In their work, temporal local information was extracted via semisupervised learning, they introduced a multiple instance (MI) module to extract small time-scale information. The multiple instance learning determines a segment that may possibly contain falls in a video by minimizing the sum of two losses of the multiple instance learning (MIL) loss and cross-entropy loss."
        },
        {
            "heading": "4.7 Accessibility",
            "text": "VC can be used as an assistive technology in physical environments, for example helping individuals who are blind travel independently [345]. In [346], a blind navigation system was formulated that detected thin structured wires as obstacles. This was realized by taking the video stream of a path taken by a blind individual from a single-lens camera. The system consisted of three blocks an image information extraction block, a tracking block, and a mapping block. In the former block, the difference of Gaussians (DoG) was used to derive keylines. Later, these keylines were used to create an edge map for each frame. In the second stage, the previous edge map was fit into the new edge map by employing a warping function. Each keyline in the new edge map was matched against the ones in the previous map in order to separate real obstacle edges from spurious ones. In [347], an obstacle identification using an SSD mobile net architecture was proposed, with an audio alert to warn of the distance from an obstacle."
        },
        {
            "heading": "4.8 Human-Robot Interaction",
            "text": "HRI [348] is a rapidly developing field of research with applications as broad as search and rescue, mine and bomb detection, scientific exploration, law enforcement, entertainment, and elder care. In many applications, it would be beneficial if a robot could communicate with a human collaborator by providing an explanation of its understanding in natural language. Temporal models have been employed to this end in Natural Language Video Description (NLVD) [349]. The egocentric view of robotic vision and exocentric view of general video are fused in VC for better HRI interaction [350]."
        },
        {
            "heading": "4.9 Video Description and Summarization",
            "text": "Video description has to capture all key aspects of the underlying events in a video and specify with a story-like description with multiple sentences. The generated descriptions must be relevant, coherent and concise. Zhu et al. [351] proposed a video description of videos with occlude information. The authors described the video employing the natural language dialogues between the two agents. The dialogue generation between two agents may be generative, i.e., the agent generates questions along with answers freely, or discriminative, i.e., agents opt for the questions and answers coming out of the candidates. Existing caption datasets for video descriptions are scarce. Hence, Monfort et al. [88] proposed S-MiT, which comprises 500k spoken captions; each ascribed to a unique short video describing a wide range of assorted events. Moreover, Xiong et al. [18] developed a concise and coherent paragraph description of the video using self-critical sequence training of RNN by giving rewards at sentence and paragraph level.\nVideo summarization [352] aims to fuse segments of videos that include key visual information [353]. Video summarization techniques rely on low-level features such as color, and motion [354] or objects [354] and their relationships to select key segments. Meanwhile, others utilize text inputs from user studies to extract the key segments [355]. Video summarization aims to provide this information by generating the gist of a video, benefiting both the users and companies that provide video streaming and search (with increased user engagement). It should be noted that video summarization is an efficient approach to facilitate both video browsing and searching [356]."
        },
        {
            "heading": "4.10 Video Recommendation",
            "text": "The video recommendation system suggests a video to a user. These systems typically feature two blocks, one is a candidate block that extracts videos as candidates from a large-scale video repository, and the second is a block ranking these candidates to select the best match for the user. DL-based video recommender systems exploit multiple features including user demographics and behaviors, video titles, and video tags for suggesting recommendations. Pu et al. [36] formulated a recommender system by integrating semantic features in a multi-modal topic learning algorithm to improve the efficiency of the recommender system. Dong et al. [357] boosted the performance by taking data augmentation at frame level and video level, and later incorporated the features in a multimodal recommender system. Liu et al. [358] proposed conceptaware denoising GNN, which is a micro video recommender system, formed based on a tripartite graph to link user nodes with video nodes, and video nodes with associated concept nodes, retrieved from captions and comments of the videos.\nVideo recommendation is a particularly important tool for online video providers like YouTube and Hulu since it helps viewers discover videos. User interaction data such as clicks, views, comments, or ratings are not available when a video is first published to a service, this is referred to as the cold-start issue. BlerinaLika et al. [359] addressed the cold-start problem by adopting a three-phase approach. The first is the collection of demographic data of new users. The second approach is finding the user\u2019s neighbors based on people with a common\n29\ninterest. The third is classifying new users into a group and based on the classification, computing ratings for new items. The final ratings are modified with a weighting scheme, where developers can pay attention to the specific attributes."
        },
        {
            "heading": "5 Discussion",
            "text": "In this survey, we have provided a comprehensive review of VC methods. We first categorized the VC methods into template and sequence-to-sequence methods, and then, divided each category into sub-categories and presented their corresponding representative methods. Then, we described datasets, and performance metrics, and discussed applications. Table 8 provides a comprehensive comparison of VC methods in the literature. In the following sub-sections, we list the most important research gaps in the domain (see sub-section 5.1) and future research directions (see sub-section 5.2)."
        },
        {
            "heading": "5.1 Research Gaps",
            "text": "Despite the recent and substantial progress in VC research, there are a number of unsolved challenges. Here we discuss these gaps and potential solutions. VC methods are still not very effective at capturing sequences of actions, especially in shorter video clips [91] and many also neglect the multi-modal nature of videos [49]. The proposed encoder-decoder models in [89], [360] do not use high-level video concepts, resulting inpoor results. Sequence-sequence models [41] poorly represent the objects in the video because the encoder is used to generate low-level features and not the objects. In supervised VC algorithms, the generated captions are evaluated with the ground truth caption using a loss function calculated on a word-to-word basis. However, changing a single word in a sentence can have a significant impact on its meaning.\nIn RNNs, a hidden state is computed based on the previous hidden state such that it cannot be parallelized. Sequential computation introduces a high cost, especially for long sequences. The use of pre-trained networks for extracting object interactions in temporal frameworks suffers from several drawbacks. Firstly, they cannot derive discriminant multiobject features. Second, models only trained for recognizing human actions fail to capture the temporal information in other non-human objects in the scene, e.g., animals, and vehicles. Moreover, different visual features that are fed into captioning modules at the same time often represent different time periods, resulting in confusion during training [174]. Third, training a model that captures more subtle fine-grained visual attributes is difficult. Furthermore, object occlusions and unclear boundaries make vision-to-language translation difficult [40]. Finally, the majority of the proposed techniques for VC in the literature mainly focus on English-only video tasks [361]. The lack of video datasets in other languages is one of the reasons for this bias. More attention needs to be paid to faithfully modeling under-represented languages, and evaluating performance in languages other than English."
        },
        {
            "heading": "5.2 Future Research Directions and Outlooks",
            "text": ""
        },
        {
            "heading": "5.2.1 Uncertainty Quantification in Video Captioning",
            "text": "Quantifying the uncertainty of machine learning and deep learning models plays a critical role in producing reliable predictions. In general, the sources of uncertainty can be\ngrouped into aleatoric (data) and epistemic (model) uncertainties [362], [363], [364], [365]. VC has been widely used in many sensitive fields such as medical imaging [366], [367], [368] and other safety-critical scenarios [369]. However, our findings indicate that although quantifying the uncertainty of VC methods is important, to the best of our knowledge, there are no methods that fully characterize uncertainty in an interpretable way. We would suggest the research community continues to direct efforts in this area and develops new uncertainty quantification methods."
        },
        {
            "heading": "5.2.2 Curriculum Learning in Video Captioning",
            "text": "Curriculum Learning (CL) is an effective learning strategy, which is inspired by the human learning ability, that trains a learning algorithm from easier to harder samples of datasets [370], [371]. The key question in developing a good curriculum is how to measure the difficulty of each example [372]. This strategy is able to improve the convergence rate and generalization capability of learning algorithms in tackling various scenarios such as NMT and NLP [373]. Our review shows that there exist only a few studies conducted in CL for VC. Thus, we list this field as potential research for further investigation."
        },
        {
            "heading": "5.2.3 Continual Learning in Video Captioning",
            "text": "Continual Learning is referred to a group of machine learning techniques that attempt to continually learn from a stream of data while its distribution shifts over time [374]. Catastrophic forgetting, e.g., the learning algorithm should able to learn information from new data without forgetting the previously learned information, is the main challenge of this strategy that must be addressed [375], [376], [377]. This is another interesting field that can be used for VC, thus we list it as a research gap that requires further investigations."
        },
        {
            "heading": "5.2.4 Explainability in Video Captioning",
            "text": "Many DL models are black boxes due to their complex structures and a large number of parameters. This makes it difficult for users to interpret their predictions. In addition, it is crucial for many real-world problems such as healthcare and safety-critical industrial problems to interpret the model prediction [378], [379]. In this regard, explainable artificial intelligence (EXI) [380], [381] have been proposed. Our findings in this review paper indicate that interpreting the predictions of the VC methods can improve their performance."
        },
        {
            "heading": "5.2.5 Transformers in Video Captioning",
            "text": "Transformers [382], [383] are prominent types of DL models, originally proposed as for sequence-to-sequence tasks in machine translation. Transformers require minimal inductive biases and they are able to process multiple modalities [384]. In addition, they are robust to domain shifts and perturbations [385]. Raghu et al. [386] showed that transformers extract more uniform features across all the layers. Recently, a few studies have used transformers for VC. For example, the proposed transformer model in [65] generalizes the transformer architecture for a bi-modal input (audio and visual). We believe that transformer-based methods still require more investigations in dealing with the VC problems.\n30"
        },
        {
            "heading": "5.2.6 Knowledge Distillation in Video Captioning",
            "text": "Knowledge Distillation (KD) [387], [388] is a type of compression and acceleration that is able to learn a small student model from a large teacher model in a target domain. In other words, KD is able to solve the problems caused by the lack of labeled samples via transferring knowledge learned from one model to another. Nonetheless, the key challenge of KD is selecting the effective information for transferring [389]. Our review reveals that only a few studies applied KD for VC. For example, Pan et al. [390] used KD mechanism to avoid unstable performance caused by the variable number of objects. Hence, we suggest designing KD-based frameworks for VC."
        },
        {
            "heading": "5.2.7 Active Learning in Video Captioning",
            "text": "Active Learning (AL) methods are effective in dealing with the lack of labeled samples. They label data samples in an iterative manner until achieving sufficient labeled samples with promising outcomes [391], [392], [393]. In other words, AL identifies the most informative samples that should be sued for annotations. It has been adopted in many real-world applications such as satellite image segmentation [394] and medical image analysis [395]. We have found that fewer studies focused on AL-based VC. For example, Chan et al. [314] proposed a cluster-based active learning technique for VC that is able to increase the sample diversity and improve the model performance."
        },
        {
            "heading": "5.2.8 Text-to-Video in Video Captioning",
            "text": "There are billions of (alt-text, image) pairs from HTML pages on the Internet, enabling great research interest in T2I modeling. However, T2V is very complex than T2I due to the additional temporal dimension that makes the generated data high dimensional, and also the generated video must be both photo-realistically diverse and should look natural [396]. T2V is widely used in generating synthetic data for machine learning tasks, domain adaptation, multimedia applications, text-to-video retrieval, and also important research direction to improve the overall effectiveness of interactive video search systems. The two challenges of T2V are that one is the generated video must be natural and temporally coherent and the second is the content of the generated video must match with the input text. The invention of GAN, Variational Auto encoders (VAE), conditional generative models (c-GAN) boosted the research in T2I and T2V. Li et al. [397] proposed T2V based on CGAN. The shortcoming of the scheme is that they used 3D transposed convolution layers in the GAN which produce only a fixed-length video. Second, the videos generated are poor in resolution and thirdly the text-to-video synthesis has shown poor generalization on large datasets. The above problems are rectified in [396] by employing RNN for generating variable length video. The poor resolution of the video is solved by employing RESNET. Further, to strengthen the associations between the conditioned text and the generated video Text-Filter conditioning GAN (TFGAN) is employed. The same problems in [397] are addressed in another way by [398]. Deng et al. [398] have improved the temporal coherence and the visual quality of the video by employing recurrent cells with 2D transconvolutional layers. The 2D transconvolutional layers put more emphasis on the details of each frame than on 3D and improve the videos with\nbetter visual quality. The mutual-information introspection is incorporated in GAN to match the generated video to text semantically. Semantic consistency is measured by introspecting the semantic distance between the generated video and the corresponding text and trying to minimize it to boost semantic consistency. In Text to Video Retrieval (T2VR), the text modality and the video modality need to be represented in a shared common space for T2V similarity matching. For video encoding, 2D-RESNET models are pre-trained on ImageNet to generate visual features. The text is encoded by running multiple text encoders independently with their output concatenated later [399] or by stacking the encoders [400]. Loco et al. [401] deployed a lightweight W2VV++ model in T2VR. The advantage of this scheme is that pipelined architecture of W2VV++ facilitates to the removal of a specific encoder with ease which made the proposed T2VR very interactive in realtime. Chen et al. [402] represented the shared common space for T2V by developing a Hierarchical Graph Reasoning model that decomposes videos and texts into hierarchical semantic levels of events, actions, and entities. Later, they obtained textual embeddings via attention-based graph reasoning and matches the text with videos at different fine-grained levels. Mun et al. [403] proposed T2V scheme is effective in capturing relationships of semantic phrases in the text and video segments by modeling local and global contexts. Hu et al. [404] point out the drawbacks of MHA as it pays more attention to weak features. So they proposed Lightweight Attention Feature Fusion in place of MHA for exploiting diverse, multilevel (off-the-shelf) features for T2VR. Dong et al. [405] formulated the shared common space between the text and video in T2VR problem inspired by the reading strategy of humans. The visual representation learning of videos is analyzed in two stages of the reading, one is previewing and another is intensive reading. The previewing of the video is to analyze the overview and the intensive reading stage is to extract indepth information for obtaining the fine-grained features. In another study, Fadime Sener [406] developed T2V by training on large text corpora and video models are trained on scarce parallel data and can be used for zero-shot queries (data that is unseen prior)."
        },
        {
            "heading": "5.2.9 Diverse Texts in Video Captioning",
            "text": "Generating high-quality texts with high diversity is an important requirement in computer vision. In general, a video may contain complex texts and visual information that is very difficult to represent comprehensively. The existing method employs conventional VC schemes to address this problem, which produces a global caption for the entire scene in a frame. The problems in the existing work are first, it is difficult to know which parts of text in a video to be paraphrased. Second, it is very difficult to exploit the complex relationship between diverse texts in the video. Third, the Generation of multiple captions with diverse content. Shi et al. [407] proposed a novel Auto-captioner method. The authors used the important tokens which are given more attention and take them as anchors. Later, for each identified anchor, its relevant texts are mapped to obtain the Anchor\u2013centered Graph (ACG). Finally, based on different ACGs, the authors produced multi-view caption generation to improve the content diversity of generated captions. The existing neural models are not able to model the input data dynamically during the generation to produce\n31\ndiverse texts. It is due to the lack of capturing inter-sentence coherence The above problem is rectified by [408] using a planning-based hierarchical Variational model to prevent the inter-sentence incoherence problem. The authors planned a hierarchical generation process, which models the process of human writing. To obtain this, the author segments the text generation into a sequence of dependent sentence generation sub-tasks where each sub-task depends specifically on an individual group and the previous context. By this scheme, the input data can be modeled better and thus obtains intersentence coherence. Zhang et al. [409] proposed T2I using Driver GAN. The single-stage schemes used in T2I suffer from the lack of diversity, yielding similar output to diverse texts. To prevent this, the authors introduced a channel-attention module (CAM) and a pixel-attention module (PAM), which give more importance to each word in the given sentence while allowing the network to assign larger weights to the significant channels and pixels semantically aligning with the salient words. The authors achieved text diversity, without harming quality and semantic consistency. Du et al. [410] proposed an improvement over existing works by introducing context-aware variations into the encoder based on Gaussian process priors to produce diverse texts. This improves intersentence coherence which can help to preserve more semantic information from source texts. During generation, the decoder generates diverse outputs conditioning on sampled different context variables. In other words, by learning a stochastic function on top of one deterministic encoder, the proposed approach offers many versions of random context variables for a decoder to generate diverse texts."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this work, a detailed survey on deep learning-based Video Captioning (VC) methods along with datasets and evaluation metrics is presented. Following the detailed survey, the performance of the VC methods using different datasets and evaluation metrics is comprehensively evaluated. This survey covers the wide applicability of VC in other fields such as video tagging, content-based image retrieval, video recommender systems, blind navigation, etc. Some research gaps are highlighted in this to serve as a good starting point for researchers who are interested in VC and shed some light on this ever-growing field."
        },
        {
            "heading": "7 Acknowledgment",
            "text": "We thank Professor Jianfeng Gao at Microsoft Research, Redmond, USA for checking the article and providing valuable feedback and comments.\nAcronyms VC Video Captioning NLP Natural Language Processing CNN Convolutional Neural Network 2D-CNN Two dimensional Convolutional Neural Network GNN Graph Neural Network CBVR Content Based Video Retrieval 3D-CNN Three dimensional Convolutional Neural Network RNN Recurrent Neural Network AI Artificial Intelligence\n3D-CNN Three Dimensional Convolutional Neural Network MP Mean Pooling TE Temporal Encoding SEM Semantic Attribute Learning LSTM Long Short-Term Memory EMVC Event-centric Multi-modal fusion approach for dense Video Captioning SGR Sketch, Ground, and Refine ViSE Visual-Semantic Embedding GRU Gated Recurrent Unit Self-Attn Self-Attention Soft-Attn Soft-Attention Hard-Attn Hard-Attention Text-Attn Text-Attention Local-Attn Local-Attention Global-Attn Global-Attention Stacked-Attn Stacked-Attention Temp-Attn Temporal-Attention FFN Feed Forward Network MAM Multi-level Attention MHA Multi-head attention HATT Hierarchical attention strategy AAMF Attention-based Averaged Multimodal Fusion AMF Attention-based Multimodal Fusion HRNAT Hierarchical Representation Network with Auxiliary Tasks AVSSN Attentive Visual Semantic Specialized Network GPaS Graph-based Partition-and-Summarization GCN Graph Convolutional Network NMT Neural Machine Translation GAN Generative Adversial Network CGAN Conditional Generative Adversial Network DVC Dense Video Captioning TGANs-C Temporal GANs conditioning on Captions CGAN Conditional Generative and Adversial Networks BLEU Bilingual Evaluation Understudy METEOR Metric for Evaluation of Translation with Explicit Ordering ROUGE-L Recall Oriented Understudy of Gisting Evaluation CIDEr Consensus based Image Description Evaluation VS Vocabulary Size PNS Percentage of Novel Sentences DC Diversity of Captions DCE Diverse Captioning Evaluation V-QA Video Question Answering CBIR Content Based Image Retrieval MSVD Microsoft Research Video Description Corpus MSR-VTT Microsoft Research-Video to Text M-VAD Montreal Video Annotation Dataset V2C Video-to-Commonsense ViTT Video Timeline Tags CBVR Content Based Video Retrieval LCS Longest Common Subsequence F F-Measure TF-IDF Term Frequency-Inverse Document Frequency BoW Bag of Words LSA Latent Semantic Analysis BERT Bidirectional Encoder Representations from\nTransformers\n32\nSTraNet Structured Trajectory Network via Adversarial Learning STAT Spatial-Temporal Attention Mechanism POS Part-of-Speech GMMP GCN Meta-learning with Multi-granularity POS MPII-MD Max Plank Institute for Informatics Movie Description Dataset HRI Human Robot Interaction NLVD Natural Language Video Description AMT Amazon Mechanical Turkers DVS Descriptive Video Service TACoS-Mlevel Textually Annotated Cooking Scenes-Multilevel S-MiT Spoken Moments Dataset AD Audio Descriptions LSMDC Large Scale Movie Description Challenge dataset KD Knowledge Distillation OA-BTG Object Aggregation Bidirectional Ttemporal Graph ORG Object Relation Graph TRL Teacher Recommended Learning OSTG Object-aware Spatio-temporal Graph VLAD Vector of Locally Aggregated Descriptors GCN Graph Convolution Network ORMF Object Relation Graph and Multimodal Feature Fusion CL Curriculum Learning AL Active Learning SGN Semantic Grouping Network MIXER Mixed Incremental Cross-Entropy Reinforce RL Reinforcement Learning CCN Codec Network ICT Inverse Cloze Task WS-DEC Weakly supervised dense Event captioning MED Multi event detection MEC Multi event captioning TSRM Temporal Semantic Relation Module CMG CrossModal gating T2I Text to Image T2V Text to Video VAE Variational Auto encoders c-GAN conditional generative models T2VR Text to Video Retrieval ACG Anchor\u2013centered Graph AViD Anonymized Videos from Diverse countries VTW Video titles in the wild ST Spatio-Temporal TS Temporal-Spatial TGN Temporal Graph Network RGN Relation Graph Network BTG Bidirectional Temporal Graph WMD Word Mover\u2019s Distance SPICE Semantic Propositional Image Captioning Evaluation MT Machine Translation IC Image Captioning BoW Bag of Words EMD Earth Mover\u2019s Distance G Generator D Discriminator XE Cross entropy"
        }
    ],
    "title": "A Review of Deep Learning for Video Captioning",
    "year": 2023
}