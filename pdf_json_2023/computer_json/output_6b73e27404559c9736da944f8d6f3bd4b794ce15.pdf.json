{
    "abstractText": "Abstract Many compression techniques have been proposed to reduce the communication overhead of Federated Learning training procedures. However, these are typically designed for compressing model updates, which are expected to decay throughout training. As a result, such methods are inapplicable to downlink (i.e., from the parameter server to clients) compression in the cross-device setting, where heterogeneous clients may appear only once during training and thus must download the model parameters. Accordingly, we propose DoCoFL \u2013 a new framework for downlink compression in the cross-device setting. Importantly, DoCoFL can be seamlessly combined with many uplink compression schemes, rendering it suitable for bi-directional compression. Through extensive evaluation, we show that DoCoFL offers significant bi-directional bandwidth reduction while achieving competitive accuracy to that of a baseline without any compression.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ron Dorfman"
        },
        {
            "affiliations": [],
            "name": "Shay Vargaftik"
        },
        {
            "affiliations": [],
            "name": "Yaniv Ben-Itzhak"
        },
        {
            "affiliations": [],
            "name": "Kfir Y. Levy"
        }
    ],
    "id": "SP:4162102b8c7c203ac2ebcdecdf5ba2cfe6d3e56c",
    "references": [
        {
            "authors": [
                "D. Alistarh",
                "D. Grubic",
                "J. Li",
                "R. Tomioka",
                "M. Vojnovic"
            ],
            "title": "QSGD: Communication-efficient SGD via gradient quantization and encoding",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "D. Alistarh",
                "T. Hoefler",
                "M. Johansson",
                "N. Konstantinov",
                "S. Khirirat",
                "C. Renggli"
            ],
            "title": "The convergence of sparsified gradient methods",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Y. Arjevani",
                "O. Shamir",
                "N. Srebro"
            ],
            "title": "A tight convergence analysis for stochastic gradient descent with delayed updates",
            "venue": "In Algorithmic Learning Theory,",
            "year": 2020
        },
        {
            "authors": [
                "J. Bernstein",
                "Wang",
                "Y.-X",
                "K. Azizzadenesheli",
                "A. Anandkumar"
            ],
            "title": "signSGD: Compressed optimisation for nonconvex problems",
            "venue": "In International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "J. Bernstein",
                "J. Zhao",
                "K. Azizzadenesheli",
                "A. Anandkumar"
            ],
            "title": "signSGD with majority vote is communication efficient and fault tolerant",
            "venue": "arXiv preprint arXiv:1810.05291,",
            "year": 2018
        },
        {
            "authors": [
                "A. Beznosikov",
                "S. Horv\u00e1th",
                "P. Richt\u00e1rik",
                "M. Safaryan"
            ],
            "title": "On biased compression for distributed learning",
            "venue": "arXiv preprint arXiv:2002.12410,",
            "year": 2020
        },
        {
            "authors": [
                "M. Charikar",
                "K. Chen",
                "M. Farach-Colton"
            ],
            "title": "Finding frequent items in data streams",
            "venue": "In Automata, Languages and Programming: 29th International Colloquium,",
            "year": 2002
        },
        {
            "authors": [
                "P.A. Chou",
                "T. Lookabaugh",
                "R.M. Gray"
            ],
            "title": "Entropyconstrained vector quantization",
            "venue": "IEEE Transactions on acoustics, speech, and signal processing,",
            "year": 1989
        },
        {
            "authors": [
                "S. Chraibi",
                "A. Khaled",
                "D. Kovalev",
                "P. Richt\u00e1rik",
                "A. Salim",
                "M. Tak\u00e1\u010d"
            ],
            "title": "Distributed fixed point methods with compressed iterates",
            "year": 1912
        },
        {
            "authors": [
                "A. Cohen",
                "A. Daniely",
                "Y. Drori",
                "T. Koren",
                "M. Schain"
            ],
            "title": "Asynchronous stochastic optimization robust to arbitrary delays",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "G. Cohen",
                "S. Afshar",
                "J. Tapson",
                "A. Van Schaik"
            ],
            "title": "EMNIST: Extending MNIST to handwritten letters",
            "venue": "In 2017 international joint conference on neural networks (IJCNN),",
            "year": 2017
        },
        {
            "authors": [
                "I. Fatkhullin",
                "I. Sokolov",
                "E. Gorbunov",
                "Z. Li",
                "P. Richt\u00e1rik"
            ],
            "title": "EF21 with bells & whistles: Practical algorithmic extensions of modern error feedback",
            "venue": "arXiv preprint arXiv:2110.03294,",
            "year": 2021
        },
        {
            "authors": [
                "N. Giladi",
                "M.S. Nacson",
                "E. Hoffer",
                "D. Soudry"
            ],
            "title": "At Stability\u2019s Edge: How to Adjust Hyperparameters to Preserve Minima Selection in Asynchronous Training of Neural Networks",
            "venue": "In 8th International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "E. Gorbunov",
                "K.P. Burlachenko",
                "Z. Li",
                "Richt\u00e1rik",
                "P. MARINA"
            ],
            "title": "Faster non-convex distributed learning with compression",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "K. Gruntkowska",
                "A. Tyurin",
                "P. Richt\u00e1rik"
            ],
            "title": "EF21-P and Friends: Improved Theoretical Communication Complexity for Distributed Optimization with Bidirectional Compression",
            "venue": "arXiv preprint arXiv:2209.15218,",
            "year": 2022
        },
        {
            "authors": [
                "F. Haddadpour",
                "M.M. Kamani",
                "A. Mokhtari",
                "M. Mahdavi"
            ],
            "title": "Federated learning with compression: Unified analysis and sharp guarantees",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2021
        },
        {
            "authors": [
                "S. Horv\u00e1th",
                "P. Richt\u00e1rik"
            ],
            "title": "A better alternative to error feedback for communication-efficient distributed learning",
            "venue": "arXiv preprint arXiv:2006.11077,",
            "year": 2020
        },
        {
            "authors": [
                "S. Horv\u00f3th",
                "Ho",
                "C.-Y",
                "L. Horvath",
                "A.N. Sahu",
                "M. Canini",
                "P. Richt\u00e1rik"
            ],
            "title": "Natural compression for distributed deep learning",
            "venue": "In Mathematical and Scientific Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "D. Howdle"
            ],
            "title": "Worldwide mobile data pricing",
            "venue": "https://www.cable.co.uk/mobiles/ worldwide-data-pricing/#pricing,",
            "year": 2022
        },
        {
            "authors": [
                "D.A. Huffman"
            ],
            "title": "A method for the construction of minimumredundancy codes",
            "venue": "Proceedings of the IRE,",
            "year": 1952
        },
        {
            "authors": [
                "N. Ivkin",
                "D. Rothchild",
                "E. Ullah",
                "I. Stoica",
                "R Arora"
            ],
            "title": "Communication-efficient distributed SGD with sketching",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "R. Jin",
                "Y. Huang",
                "X. He",
                "H. Dai",
                "T. Wu"
            ],
            "title": "Stochasticsign SGD for federated learning with theoretical guarantees",
            "venue": "arXiv preprint arXiv:2002.10940,",
            "year": 2020
        },
        {
            "authors": [
                "R Cummings"
            ],
            "title": "Advances and open problems in federated learning",
            "venue": "Foundations and Trends\u00ae in Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "S.P. Karimireddy",
                "Q. Rebjock",
                "S. Stich",
                "M. Jaggi"
            ],
            "title": "Error feedback fixes signsgd and other gradient compression schemes",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "S.P. Karimireddy",
                "S. Kale",
                "M. Mohri",
                "S. Reddi",
                "S. Stich",
                "A.T. Suresh"
            ],
            "title": "Scaffold: Stochastic controlled averaging for federated learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "A. Khaled",
                "P. Richt\u00e1rik"
            ],
            "title": "Better theory for SGD in the nonconvex world",
            "venue": "arXiv preprint arXiv:2002.03329,",
            "year": 2020
        },
        {
            "authors": [
                "J. Kone\u010dn\u1ef3",
                "P. Richt\u00e1rik"
            ],
            "title": "Randomized distributed mean estimation: Accuracy vs. communication",
            "venue": "Frontiers in Applied Mathematics and Statistics,",
            "year": 2018
        },
        {
            "authors": [
                "J. Kone\u010dn\u1ef3",
                "H.B. McMahan",
                "D. Ramage",
                "P. Richt\u00e1rik"
            ],
            "title": "Federated optimization: Distributed machine learning for on-device intelligence",
            "venue": "arXiv preprint arXiv:1610.02527,",
            "year": 2016
        },
        {
            "authors": [
                "J. Kone\u010dn\u1ef3",
                "H.B. McMahan",
                "F.X. Yu",
                "P. Richt\u00e1rik",
                "A.T. Suresh",
                "D. Bacon"
            ],
            "title": "Federated learning: Strategies for improving communication efficiency",
            "venue": "arXiv preprint arXiv:1610.05492,",
            "year": 2016
        },
        {
            "authors": [
                "A. Krizhevsky",
                "G Hinton"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "year": 2009
        },
        {
            "authors": [
                "X. Lian",
                "Y. Huang",
                "Y. Li",
                "J. Liu"
            ],
            "title": "Asynchronous parallel stochastic gradient for nonconvex optimization",
            "venue": "Advances in neural information processing systems,",
            "year": 2015
        },
        {
            "authors": [
                "X. Liu",
                "Y. Li",
                "J. Tang",
                "M. Yan"
            ],
            "title": "A double residual compression algorithm for efficient distributed learning",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2020
        },
        {
            "authors": [
                "Y. Lyubarskii",
                "R. Vershynin"
            ],
            "title": "Uncertainty principles and vector quantization",
            "venue": "IEEE Transactions on Information Theory,",
            "year": 2010
        },
        {
            "authors": [
                "B. McMahan",
                "D. Ramage"
            ],
            "title": "Federated learning: Collaborative machine learning without centralized training data",
            "venue": "https://ai.googleblog.com/2017/04/ federated-learning-collaborative.html,",
            "year": 2017
        },
        {
            "authors": [
                "B. McMahan",
                "E. Moore",
                "D. Ramage",
                "S. Hampson",
                "B.A. y Arcas"
            ],
            "title": "Communication-efficient learning of deep networks from decentralized data",
            "venue": "In Artificial intelligence and statistics,",
            "year": 2017
        },
        {
            "authors": [
                "H.B. McMahan",
                "A. Thakurta",
                "G. Andrew",
                "B. Balle",
                "P. Kairouz",
                "D. Ramage",
                "S. Song",
                "T. Steinke",
                "A. Terzis",
                "O Thakkar"
            ],
            "title": "Federated learning with formal differential privacy guarantees",
            "venue": "Google AI Blog,",
            "year": 2022
        },
        {
            "authors": [
                "D. Ng",
                "X. Lan",
                "Yao",
                "M.M.-S",
                "W.P. Chan",
                "M. Feng"
            ],
            "title": "Federated learning: a collaborative effort to achieve better medical imaging models for individual sites that have small labelled datasets",
            "venue": "Quantitative Imaging in Medicine and Surgery,",
            "year": 2021
        },
        {
            "authors": [
                "A. Paszke",
                "S. Gross",
                "F. Massa",
                "A. Lerer",
                "J. Bradbury",
                "G. Chanan",
                "T. Killeen",
                "Z. Lin",
                "N. Gimelshein",
                "L Antiga"
            ],
            "title": "Pytorch: An imperative style, high-performance deep learning library",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "C. Philippenko",
                "A. Dieuleveut"
            ],
            "title": "Bidirectional compression in heterogeneous settings for distributed or federated learning with partial participation: tight convergence guarantees",
            "venue": "arXiv preprint arXiv:2006.14591,",
            "year": 2020
        },
        {
            "authors": [
                "C. Philippenko",
                "A. Dieuleveut"
            ],
            "title": "Preserved central model for faster bidirectional compression in distributed settings",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "T. Rabbani",
                "B. Feng",
                "Y. Yang",
                "A. Rajkumar",
                "A. Varshney",
                "F. Huang"
            ],
            "title": "Comfetch: Federated Learning of Large Networks on Memory-Constrained Clients via Sketching",
            "venue": "arXiv preprint arXiv:2109.08346,",
            "year": 2021
        },
        {
            "authors": [
                "A. Ramezani-Kebrya",
                "F. Faghri",
                "I. Markov",
                "V. Aksenov",
                "D. Alistarh",
                "D.M. Roy"
            ],
            "title": "NUQSGD: Provably Communication-efficient Data-parallel SGD via Nonuniform Quantization",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2021
        },
        {
            "authors": [
                "S.J. Reddi",
                "Z. Charles",
                "M. Zaheer",
                "Z. Garrett",
                "K. Rush",
                "J. Kone\u010dn\u00fd",
                "S. Kumar",
                "H.B. McMahan"
            ],
            "title": "Adaptive Federated Optimization",
            "venue": "In 9th International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "P. Richt\u00e1rik",
                "I. Sokolov",
                "I. Fatkhullin"
            ],
            "title": "EF21: A new, simpler, theoretically better, and practically faster error feedback",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "D. Rothchild",
                "A. Panda",
                "E. Ullah",
                "N. Ivkin",
                "I. Stoica",
                "V. Braverman",
                "J. Gonzalez",
                "R. Arora"
            ],
            "title": "FetchSGD: Communication-efficient federated learning with sketching",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "M. Safaryan",
                "E. Shulgin",
                "P. Richt\u00e1rik"
            ],
            "title": "Uncertainty principle for communication compression in distributed and federated learning and the search for an optimal compressor",
            "venue": "Information and Inference: A Journal of the IMA,",
            "year": 2022
        },
        {
            "authors": [
                "F. Seide",
                "H. Fu",
                "J. Droppo",
                "G. Li",
                "D. Yu"
            ],
            "title": "1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns",
            "venue": "In Fifteenth annual conference of the international speech communication association,",
            "year": 2014
        },
        {
            "authors": [
                "S.M. Shah",
                "V.K. Lau"
            ],
            "title": "Model compression for communication efficient federated learning",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems,",
            "year": 2021
        },
        {
            "authors": [
                "N. Shlezinger",
                "M. Chen",
                "Y.C. Eldar",
                "H.V. Poor",
                "S. Cui"
            ],
            "title": "UVeQFed: Universal vector quantization for federated learning",
            "venue": "IEEE Transactions on Signal Processing,",
            "year": 2020
        },
        {
            "authors": [
                "S.U. Stich",
                "S.P. Karimireddy"
            ],
            "title": "The error-feedback framework: Better rates for SGD with delayed gradients and compressed communication",
            "year": 1909
        },
        {
            "authors": [
                "H. Sumra"
            ],
            "title": "Best and Worst Countries for WiFi Access. https://www.ooma.com/blog/ best-worst-wifi-countries",
            "year": 2020
        },
        {
            "authors": [
                "A.T. Suresh",
                "X.Y. Felix",
                "S. Kumar",
                "H.B. McMahan"
            ],
            "title": "Distributed mean estimation with limited communication",
            "venue": "In International conference on machine learning,",
            "year": 2017
        },
        {
            "authors": [
                "C. Tai",
                "T. Xiao",
                "X. Wang"
            ],
            "title": "Convolutional neural networks with low-rank regularization",
            "venue": "4th International Conference on Learning Representations,",
            "year": 2016
        },
        {
            "authors": [
                "H. Tang",
                "C. Yu",
                "X. Lian",
                "T. Zhang",
                "J. Liu"
            ],
            "title": "Doublesqueeze: Parallel stochastic gradient descent with double-pass error-compensated compression",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "S. Vargaftik",
                "R. Ben-Basat",
                "A. Portnoy",
                "G. Mendelson",
                "Y. Ben-Itzhak",
                "M. Mitzenmacher"
            ],
            "title": "DRIVE: onebit distributed mean estimation",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "T. Vogels",
                "S.P. Karimireddy",
                "M. Jaggi"
            ],
            "title": "PowerSGD: Practical low-rank gradient compression for distributed optimization",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "J. Wang",
                "Q. Liu",
                "H. Liang",
                "G. Joshi",
                "H.V. Poor"
            ],
            "title": "Tackling the objective inconsistency problem in heterogeneous federated optimization",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "W. Wen",
                "C. Xu",
                "F. Yan",
                "C. Wu",
                "Y. Wang",
                "Y. Chen",
                "H. Li"
            ],
            "title": "Terngrad: Ternary gradients to reduce communication in distributed deep learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "W. Yang",
                "Y. Zhang",
                "K. Ye",
                "L. Li",
                "Xu",
                "C.-Z"
            ],
            "title": "FFD: A federated learning based method for credit card fraud detection",
            "venue": "In International conference on big data,",
            "year": 2019
        },
        {
            "authors": [
                "X. Zhang",
                "J. Zhao",
                "Y. LeCun"
            ],
            "title": "Character-level convolutional networks for text classification",
            "venue": "Advances in neural information processing systems,",
            "year": 2015
        },
        {
            "authors": [
                "S. Zheng",
                "Z. Huang",
                "J. Kwok"
            ],
            "title": "Communicationefficient distributed blockwise momentum SGD with error-feedback",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [],
            "title": "DSGD enjoys the same asymptotic convergence rate as SGD; Cohen et al. (2021) later improved the dependence on the maximal delay to average delay with a variant of DSGD, allowing for arbitrary delays",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "In recent years, there has been an increasing interest in federated learning (FL) as a paradigm for large-scale machine learning over decentralized data (Konec\u030cny\u0300 et al., 2016a; Kairouz et al., 2021). FL enables organizations and/or devices, collectively termed clients, to jointly build better and more robust models by relying on their collective data and processing power. Importantly, the FL training procedure occurs without exchanging or sharing client-specific data, thus ensuring some degree of privacy and compliance with data access rights and regulations (e.g., the General Data Protection Regulation (GDPR) implemented by the European Union in May, 2018). Instead, in each round, clients perform local optimization using their local data and send only model updates to a central coordinator, also known as\n1VMware Research 2Viterby Faculty of Electrical and Computer Engineering, Technion, Haifa, Israel. Correspondence to: \u2020Ron Dorfman <rdorfman@campus.technion.ac.il>.\nProceedings of the 40 th International Conference on Machine Learning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright 2023 by the author(s).\nthe parameter server (PS). The PS aggregates these updates and updates the global model, which is then utilized by the clients in subsequent rounds.\nOne of the main challenges in FL is the communication bottleneck introduced during the distributed training procedure. To illustrate this bottleneck, consider the following example of a real FL deployment presented by McMahan et al. (2022): their training involves a small neural network with 1.3 million parameters; in each round there are 6500 participating clients; and the model is trained over 2000 rounds. A simple calculation shows that the total required bandwidth to and from the PS during this training is \u2248 61.5 TB. Since modern machine learning models have many millions (or even billions) of parameters and we might have more participants, FL may result in excessive communication overhead.\nTo deal with this overhead, many bandwidth reduction techniques have been proposed. These include taking multiple (rather than a single) local optimization steps (McMahan et al., 2017), quantization techniques (Seide et al., 2014; Alistarh et al., 2017; Wen et al., 2017; Bernstein et al., 2018a;b; Karimireddy et al., 2019; Jin et al., 2020; Shlezinger et al., 2020), low-rank decomposition (Vogels et al., 2019), sketching (Ivkin et al., 2019; Rothchild et al., 2020), and distributed mean estimation (Lyubarskii & Vershynin, 2010; Suresh et al., 2017; Konec\u030cny\u0300 & Richta\u0301rik, 2018; Vargaftik et al., 2021; 2022; Safaryan et al., 2022). However, as we detail in \u00a72, a direct application of these techniques is less suitable for downlink compression, i.e., from the PS to the clients, in the cross-device setup in which new and heterogeneous clients may participate at each round and thus must download the model parameters. This is in contrast to the cross-silo setup in which the PS can compress and send a global update (i.e., clients\u2019 aggregated update) to all silos.\nTo the best of our knowledge, only a handful of works consider bi-directional compression, i.e., compression from the clients to the PS and vice versa. These works mainly rely on per-client memory mechanism (Tang et al., 2019; Zheng et al., 2019; Liu et al., 2020; Philippenko & Dieuleveut, 2020; 2021; Gruntkowska et al., 2022) or require keeping an updated copy of the model on all clients (Horvo\u0301th et al., 2022), thus targeting either distributed learning or FL with full or partial but recurring participation (e.g., cross-silo FL). Such solutions are less suitable for large-scale cross-device\nar X\niv :2\n30 2.\n00 54\n3v 2\n[ cs\n.L G\n] 1\n3 Ju\nl 2 02\n3\nFL, where a client may appear only a handful of times, or even just once, during the entire training procedure.\nIt is important to stress that the significance of bi-directional bandwidth reduction for cross-device FL goes far beyond cost reduction, energy efficiency, and carbon footprint considerations. In fact, inclusion, fairness, and bias are at the very heart of cross-device FL as, according to recent sources (Sumra, 2020; Howdle, 2022), the price of a wireless connection and its quality admits differences of orders of magnitude among countries. This may prevent large populations from contributing to cross-device FL training due to costly and unstable connectivity, resulting in biased and less accurate models.\nAccordingly, in this work we introduce DoCoFL, a novel downlink compression framework specifically designed for cross-device FL. Importantly, it operates independently of many uplink compression techniques, making it suitable for bi-directional compression in cross-device setups.\nThe primary challenge addressed by DoCoFL is that clients must download model parameters (i.e., weights) instead of model updates. Unlike updates, which are proportional to gradients and thus their norm is expected to decrease during training, the model parameters do not decay, rendering low-bit compression methods undesirable. As a result, and since clients can only download the updated model weights during their designated participation round, this can lead to a network bottleneck for low-resourced clients.\nTo address this bottleneck, DoCoFL decomposes the download burden by utilizing previous models, referred to as anchors, which clients can download prior to their participation round. Then, at the designated participation round, clients only need to download the correction, i.e., the difference between the updated model and the anchor. As the correction is proportionate to the sum of previous updates, it is expected to decay, allowing for the use of low-bit compression methods. To ensure the correction term, PS memory footprint, and PS computational overhead remain manageable, the available anchors are updated periodically. This approach reduces the amount of bandwidth required by the clients online (i.e., at their participation round). To reduce the overall downlink bandwidth usage, we further develop and utilize an efficient anchor compression technique with an appealing bandwidth-accuracy tradeoff.\nContributions. We summarize our contributions below, \u2022 We propose a new framework (DoCoFL) that both en-\nlarges the time window during which clients can obtain the model parameters and reduces the total downlink bandwidth requirements for cross-device FL.\n\u2022 We show that DoCoFL provably converges to a stationary point when not compressing anchors and give an asymptotic convergence rate.\n\u2022 We design a new compression technique with strong empirical results, which DoCoFL uses for anchor compression and can be of independent interest. We provide the theoretical intuition and empirical evidence for why DoCoFL with anchor compression works.\nFinally, we show over image classification and language processing tasks that DoCoFL consistently achieves model accuracy that is competitive with an uncompressed baseline, namely, FedAvg (McMahan et al., 2017) while reducing bandwidth usage in both directions by order of magnitude."
        },
        {
            "heading": "2. Background and Related Work",
            "text": "In this section, we overview mostly related work and detail the challenges in designing a bi-directional bandwidth reduction framework for cross-device FL."
        },
        {
            "heading": "2.1. Uplink vs. Downlink Compression",
            "text": "In the context of FL, uplink (i.e., client to PS) and downlink (i.e., PS to client) compression are inherently different and should not be treated in the same manner. In particular, many recent uplink compression solutions (e.g., Konec\u030cny\u0300 et al. (2016b); Alistarh et al. (2017)) partially rely on two properties to obtain their effectiveness:\nAveraging. A fundamental property arises when many clients send their compressed gradients for averaging at the PS. If the clients\u2019 estimates are independent and unbiased, the error in estimating their mean by calculating their estimations\u2019 mean is decreasing linearly with respect to the number of clients. Thus, having more clients in each round allows for more aggressive and more accurate compression.\nError Decay. Essentially, unbiased compression of updates results in an increased variance in their estimation. This increase can be compensated by decreasing the learning rate. Moreover, the effect of update compression is expected to diminish since the expected update decays as the training process approaches a stationary point. This is not the case when compression model parameters.\nFor downlink compression, we immediately lose the averaging property since, by design, there is only one source with whom the clients communicate, namely, the PS. Regarding the error decay property, we must further distinguish between different FL setups as described next."
        },
        {
            "heading": "2.2. Cross-silo vs. Cross-device FL",
            "text": "FL can be divided into two types based on the nature of the participating clients (Kairouz et al. (2021), Table 1).\nSilos. In cross-silo FL, the clients are typically assumed to be active throughout the training procedure and with sufficient compute and network resources. Silos are typi-\nTable 1. Averaging and Error Decay in different setups.\nAveraging Error Decay Uplink \u2713 \u2713\nCross-silo \u00d7 \u2713Downlink Cross-device \u00d7 \u00d7\ncally associated with entities such as hospitals that jointly train a model for better diagnosis and treatment (Ng et al., 2021) or banks that jointly build better models for fraud and anomalous activity detection (Yang et al., 2019).\nIndeed, silos allow for the design of efficient compression techniques that rely on client persistency and per-client memory mechanisms that are used for, e.g., compressing gradient differences, employing error feedback, and learning control variates (Alistarh et al., 2018; Karimireddy et al., 2020; Philippenko & Dieuleveut, 2020; Gorbunov et al., 2021; Richta\u0301rik et al., 2021). While most of these techniques consider only uplink compression, some recent works target bi-directional compression by utilizing the same property of using per-client memory and relying on repeated client participation (Tang et al., 2019; Liu et al., 2020; Philippenko & Dieuleveut, 2020; 2021; Gruntkowska et al., 2022).\nDevices. In cross-device FL, clients are typically assumed to be heterogeneous and not persistent to the extent that a client often participates in a single out of many thousands of training rounds. Also, in this setup, clients may often admit compute and network constraints. Devices are usually associated with entities such as laptops, smartphones, smartwatches, tablets, and IoT devices. A typical example of a cross-device FL application is keyboard completion for android devices (McMahan & Ramage, 2017).\nUnlike in silos with full or partial but repeated participation, compression techniques for devices that appear only once or a handful of times cannot rely on having some earlier state for or on that device. This renders methods that rely on perclient memory or learned control variates less suitable for such cross-device FL setups. Indeed, recent gradient compression techniques can be readily used for bi-directional compression in the cross-silo setup or only uplink compression in both setups (Konec\u030cny\u0300 et al., 2016b; Alistarh et al., 2017; Suresh et al., 2017; Ramezani-Kebrya et al., 2021; Vargaftik et al., 2021; 2022; Safaryan et al., 2022)."
        },
        {
            "heading": "2.3. Putting It All Together",
            "text": "As summarized in Table 1, differences in the clients\u2019 nature and the compression direction (i.e., uplink vs. downlink) significantly affect the efficiency of bandwidth reduction techniques. In the considered setups, downlink compression is more challenging than uplink compression due to the lack of averaging and received considerably less attention in the\nliterature. Moreover, for the cross-device setup, the problem is more acute due to not having error decay as well."
        },
        {
            "heading": "3. DoCoFL",
            "text": "In this section, we present DoCoFL. We start with describing our design goals, which are derived from the challenges outlined in the previous section, followed by a formal definition of the federated optimization problem. Then, in \u00a73.1, we give intuition and introduce our framework. In \u00a73.2, we detail about an important element of DoCoFL, namely, the client selection process employed by the PS. Finally, we provide a theoretical convergence result in \u00a73.3.\nDesign Goals. Motivated by the discussion in the previous section, we aim at achieving two goals to deal with the low bandwidth and slow and unstable connectivity conditions that edge devices may experience:\n1. Enlarging the time window during which a client can download the model weights from the PS.\n2. Reducing the bandwidth requirements in the downlink direction.\nAchieving both these goals will enable more heterogeneous clients to participate in the training process, which in turn may reduce bias and improve fairness1.\nPreliminaries. We use \u2225\u00b7\u2225 to denote the L2 norm and for every n \u2208 N, [n] := {1, . . . , n}. Let N be the number of clients participating in the federated training procedure. Each client i \u2208 [N ] is associated with a local loss function fi, and our goal is to minimize the loss with respect to all clients, i.e., to solve\nmin w\u2208Rd\nf(w) := 1\nN N\u2211 i=1 fi(w) . (1)\nUnlike in standard distributed optimization or cross-silo FL with full or partial but repeated participation, in crossdevice FL, only a subset of S clients participate in each optimization round and typically S \u226a N (e.g., S in the hundreds/thousands and N in many millions). Thus, clients are not expected to repeatedly participate in the optimization.\nSince FL mostly considers non-convex optimization (e.g., neural networks), and global loss minimization of such models is generally intractable, we focus on finding an approximate stationary point, i.e., a point w for which the expected gradient norm E\u2225\u2207f(w)\u2225 tends to zero. For the purpose of formal analysis, we make a few standard assumptions, namely, that f is bounded from below by\n1By fairness, we refer to the situation where clients in regions with limited, unstable, and costly connectivity are keen to participate in the training procedure.\nf\u2217, the local functions {fi} are \u03b2-smooth, i.e., \u2225\u2207fi(w)\u2212 \u2207fi(u)\u2225 \u2264 \u03b2\u2225w \u2212 u\u2225,\u2200w, u \u2208 Rd, and the access to each local function is done via a stochastic gradient oracle, i.e.,\nAssumption 3.1. For any w \u2208 Rd, client i computes an unbiased gradient estimator gi(w) with a variance that is upper bounded by \u03c32, i.e.,\nE[gi(w)] = \u2207fi(w), E\u2225gi(w)\u2212\u2207fi(w)\u22252\u2264\u03c32 . (2)\nAdditionally, we assume that the dissimilarity of the local gradients is bounded (i.e., limited client data heterogeneity).\nAssumption 3.2. There exist constants G,B \u2208 R+ such that for every w \u2208 Rd:\n1\nN N\u2211 i=1 \u2225\u2207fi(w)\u22252 \u2264 G2 +B2\u2225\u2207f(w)\u22252 . (3)\nWhile some works consider milder (Khaled & Richta\u0301rik, 2020; Haddadpour et al., 2021) or no (Gorbunov et al., 2021) assumptions on client heterogeneity in some settings (e.g., exact gradients and/or full participation), this assumption is standard in heterogeneous federated learning (Karimireddy et al., 2020; Wang et al., 2020; Reddi et al., 2021)."
        },
        {
            "heading": "3.1. Overview",
            "text": "A naive approach to reduce bandwidth in the downlink direction is to apply some compression to the model weights and have all participating clients download the compressed weights. That is, in each round t, the participating clients St \u2286 [N ] obtain a compressed version of the model weights w\u0302t = Cw(wt), for some compression operator Cw. The clients can then compute an unbiased gradient estimator at w\u0302t and send it back to the PS for aggregation.\nWhile this method is fairly simple, it has inherent disadvantages with respect to our goals. First, there is no enlarged time window during which clients can download the compressed model weights, as they can only do so at their participation round. Second, unlike with gradient compression, convergence in this setting can be guaranteed only to a proximity that is proportional to the compression error, rendering standard low-bit compression schemes unusable. Indeed, this is the case even for strongly convex functions, as shown by Chraibi et al. (2019) and reinforced by our counter-example in Appendix A.\nTo tackle these challenges, our approach relies on the following relation: for any \u03c4 \u2265 0, we can decompose wt into two ingredients: Anchor and Correction. Formally,\nwt = wt\u2212\u03c4\ufe38 \ufe37\ufe37 \ufe38 (i) Anchor +wt \u2212 wt\u2212\u03c4\ufe38 \ufe37\ufe37 \ufe38 (ii) Correction .\nThis implies that:\nAlgorithm 1 DoCoFL \u2013 Parameter Server Input: Initial weights w0 \u2208 Rd, learning rate \u03b7, weights (anchors) compression Cw , correction compression Cc, anchor compression rate K, compressed anchors queue Q\u2190 \u2205 with capacity V , client participation process P(\u00b7) for t = 0, . . . , T \u2212 1 do\n\u25b7 Anchor Deployment if t mod K == 0 then\nCompress anchor, Cw(wt) Q.enqueue(Cw(wt)) \u25b7 If Q is full, Q.dequeue()\nend if \u25b7 Client Participation Process St \u2190 P(t) \u25b7 |St| = S; see \u00a73.2 \u25b7 Optimization for client i \u2208 St in parallel do\nSend compressed correction, \u2206\u0302it \u25b7 See Algorithm 2 Obtain compressed local gradient, Cg(g\u0302it)\nend for Aggregate local gradients, g\u0302t := 1S \u2211 i\u2208St Cg(g\u0302 i t)\nUpdate weights, wt+1 = wt \u2212 \u03b7g\u0302t end for\nAlgorithm 2 DoCoFL \u2013 Client i Input: Gradient compression Cg Notification round s (by process P): Obtain participation round t \u25b7 i \u2208 P(t), s \u2264 t Obtain latest compressed anchor, yit \u2190 Q.top() \u25b7 Within time window [s, t]\nParticipation round t: Obtain compressed correction, \u2206\u0302it = Cc(wt \u2212 yit) Construct current model estimate, w\u0302it = yit + \u2206\u0302it Compute local gradient, g\u0302it = git(w\u0302it) Compress and send local gradient, Cg(g\u0302it), to PS\n(i) If a client is notified at round t\u2212 \u03c4 about its upcoming participation at round t, it can start downloading the anchor, that is, wt\u2212\u03c4 , ahead of its participation round,2\n(ii) and thus, at round t, the client only needs to download the correction, that is, wt \u2212 wt\u2212\u03c4 .\nYet, merely relying on this relation is not sufficient to achieve our goals; additionally, we seek to compress both (i) and (ii). However, these terms are inherently different and therefore, should not be treated in the same manner. Essentially, the client has more time to download (i), which is the main ingredient that forms the model weights. Introducing a large error in this term may prevent the model from converging. Conversely, (ii) must be downloaded at the participation round of the client, but it is just the sum of \u03c4 recent gradients.\nFor (i), we develop a new compression technique (see \u00a74), that achieves a better accuracy-bandwidth tradeoff than gradient compression techniques at the cost of higher complex-\n2The client can start downloading wt\u2212\u03c4 \u2032 at round t \u2212 \u03c4 \u2032 for some \u03c4 \u2032 \u2264 \u03c4 , as long as the download is complete before round t.\nity; we amortize its complexity over several rounds. Using this technique with only several bits per coordinate (e.g., 4) results in a negligible error. For (ii), we use standard gradient compression techniques with as low as 0.5 bit per coordinate. Since (ii) is only a sum of \u03c4 recent gradients, this error is expected to decay as training progresses.\nOverall, we achieve both goals as (1) the download time window is enlarged, with only a small bandwidth fraction that must be used online; and (2) the total downlink bandwidth usage is reduced by up to an order of magnitude compared to existing solutions and without degrading model accuracy.\nIn Figure 1, we show a timeline that illustrates the training procedure of DoCoFL, as we now formally detail on the role of the PS and the clients in our framework.\nParameter Server. As detailed in Algorithm 1, our PS executes three separate processes throughout the training procedure. First, it performs 1 \u2018anchor deployment\u2019 \u2013 once in every K rounds, it compresses the model weights and stores the compressed weights in a queue Q of length V . Second, the PS employs a client participation process, which we elaborate on in \u00a73.2; this process determines which clients will participate in a given round. Finally, in each round, the PS obtains the local model updates (i.e., gradients) from the clients participating in that round, computes their average, and 7 updates the model weights.\nClient. Consider a client that is 2 chosen by the PS at some round s to participate in some future round t. This means that in round s \u2264 t, the client is notified about its upcoming participation. It can then start 3 downloading the latest anchor stored by the PS; note that the client has a time window of length t\u2212 s rounds to download the compressed anchor. At its participation round t, the client 4 obtains the compressed correction from the PS, i.e., the compressed difference between the updated model and the compressed anchor obtained earlier, to 5 construct an unbiased estimate\nof the updated model weights.3 It then 6 locally computes a stochastic gradient, compresses it, and sends the compressed gradient to the PS; see Algorithm 2."
        },
        {
            "heading": "3.2. Client Participation Process",
            "text": "An important element in DoCoFL is the client participation process P . For a round number t, it returns a subset of clients St \u2286 [N ] of size S to participate in that round. Crucially, in DoCoFL, clients can be notified about their participation prior to their actual participation round.\nThis discrepancy between the notification and the participation rounds gives DoCoFL the desired versatility that opens the door for more clients to participate in the optimization process. While current frameworks follow a selection process that notifies a client about its participation just before it takes place, it is possible to consider useful selection/notification processes where some clients (e.g., with weaker connectivity) are notified earlier than others.\nAnother point to consider is the bias-utility tradeoff, where some choices of P can allow more clients to participate but may introduce bias in the participation rounds of clients with an untractable effect on the optimization process. Instead, we focus on processes that preserve the property where at each round, the PS can obtain an unbiased estimate of the gradient, which means that we require P to satisfy the following property: all clients have the same probability of participating in any given round t, i.e., P(i \u2208 P(t)) = S/N . Surprisingly, such a restriction allows for a wide range of useful selection policies.\nFor example, consider a simple scenario where the PS has\n3The client obtains the (unbiasedly) compressed difference between wt and the compressed anchor rather than the exact anchor. Thus, the model estimate is unbiased, even if the compressor Cw is biased. A similar mechanism was used by Horva\u0301th & Richta\u0301rik (2020) for gradient compression, i.e., \u2018induced compressor\u2019.\npredetermined time windows Ts and Tw that it associates with \u201cstrongly connected\u201d and \u201cweakly connected\u201d devices, respectively. Then, at each round t, the PS randomly selects clients but assigns their participation rounds to t + Ts or t+ Tw according to their strength. Observe that this simple, yet very useful scenario satisfies the property we seek after Tw rounds (the first Tw rounds may take longer since, during these initial rounds, the weakly connected clients cannot be notified enough rounds prior to their participation)."
        },
        {
            "heading": "3.3. Theoretical Guarantee",
            "text": "The primary challenge in analyzing downlink compression schemes for cross-device FL is that, even when using an unbiased compression method, for which E[Cw(w)] = w, the resulting gradient estimate\u2207f(Cw(w)) may be biased. This is because, in general, the gradient is not a linear mapping. As mentioned, the resulting bias can hinder convergence; in Appendix A we show that gradient descent with weights compression may not reach the optimal solution even for strongly convex functions.\nAccordingly, we show that DoCoFL converges to a stationary point when Cw is the identity mapping, i.e., Cw(w) = w,\u2200w \u2208 Rd \u2013 a setup that achieves our first goal (i.e., enlarged time window). As our analysis suggests, this identity assumption enables us to effectively bound the gradient bias resulting from the compression. For simplicity, we also assume no uplink compression (i.e., Cg is also identity), although including it in our analysis is straightforward for unbiased Cg4, and we do incorporate it in our experiments. Following this result, and the result of Chraibi et al. (2019) in the convex case, in Appendix B we give a theoretical intuition and empirical evidence for why DoCoFL works well in setups of interest, when Cw is not the identity function \u2013 achieving our second goal (i.e., total bandwidth reduction).\nBefore we state our convergence result, we require an additional standard assumption about the correction compression operator Cc, namely, that it has a bounded Normalized Mean-Squared-Error (NMSE) (Philippenko & Dieuleveut, 2020; Richta\u0301rik et al., 2021; Vargaftik et al., 2021).\nAssumption 3.3. There exists an \u03c9 \u2208 R+ such that\nE [ \u2225Cc(w)\u2212 w\u22252 ] \u2264 \u03c92\u2225w\u22252, \u2200w \u2208 Rd . (4)\nWe now give a convergence result for DoCoFL, namely, Theorem 3.4. Its full proof is deferred to Appendix C; here, we discuss the result and give a proof sketch.\n4Unbiasedness in the uplink direction is highly desired since (together with independence) it ensures linearly decaying mean estimation error with respect to the number of clients. For biased Cg , in light of existing results on biased compressors (Beznosikov et al., 2020), it may be the case that for some biased compressors the theoretical guarantee, with additional challenges, holds.\nTheorem 3.4. Let M=f(w0)\u2212f\u2217, \u03c3\u03032=\u03c32+4 ( 1\u2212 SN ) G2,\nand \u03b3 = 1 + ( 1\u2212 SN ) B2\nS . Then, DoCoFL with Cw and Cg as identity mappings (and appropriate \u03b7) guarantees\nE\n[ 1\nT T\u22121\u2211 t=0\n\u2225\u2207f(wt)\u22252 ] \u2208 O (\u221a M\u03b2\u03c3\u03032\nTS +\n(M2\u03b22\u03c92KV\u03c3\u03032)1/3 T 2/3S1/3 + \u03b3M\u03b2(\u03c9KV + 1) T\n) .\nThe convergence rate in Theorem 3.4 consists of three terms: \u2022 The first term \u221a M\u03b2\u03c3\u03032\nTS is a slow statistical term that depends only on the noise level \u03c3\u03032 (and the objective\u2019s properties); importantly, it is independent of DoCoFL\u2019s hyperparameters, K,V , and \u03c9.\n\u2022 The last term \u03b3M\u03b2(\u03c9KV+1)T is a fast deterministic term. When \u03c9KV \u2208 O(1) it decreases proportionally to 1/T , and otherwise, it is proportional to \u03c9KV/T .\n\u2022 The middle term (M 2\u03b22\u03c92KV\u03c3\u03032)1/3 T 2/3S1/3\nis a moderate term that depends on both noise level and DoCoFL\u2019s hyperparameters through the multiplication \u03c92KV\u03c3\u03032; it is proportionate to (T 2/3S1/3)\u22121.\nWe next derive observations from Theorem 3.4. Henceforth, we omit from O(\u00b7) the dependence on M,\u03b2, and \u03b3. Corollary 3.5. When Cc is the identity mapping, i.e., \u03c9=0, clients obtain the exact model, and thus our method is equivalent to FedAvg. Indeed, we get the same asymptotic rate as FedAvg (McMahan et al., 2017; Karimireddy et al., 2019), namely, O( \u221a \u03c3\u03032/TS + 1/T ). Corollary 3.6. Suppose \u03c9KV \u2208 \u0398(1). In that case, we get the following asymptotic rate:\nO (\u221a \u03c3\u03032\nTS +\n(\u03c9\u03c3\u03032)1/3 T 2/3S1/3 + 1 T\n) .\nCompared to Corollary 3.5, we note that the middle term is the additional cost incurred for utilizing compression. Importantly, it decreases when we improve the correction compression, i.e., reduce \u03c9. Corollary 3.7. Consider \u03c9=\u0398(1). If KV \u2208 O( \u221a \u03c3\u03032T/S),\nthe slow term dominates the rate, which is O( \u221a \u03c3\u03032/TS);\nthat is, we can set KV as large as O( \u221a \u03c3\u03032T/S) and still get, similarly to FedAvg, a speed-up with S, the number of participating clients per-round.\nProof Sketch. Denote: \u2207t :=\u2207f(wt) and \u2207\u0302t :=E[g\u0302t]. By the update rule, the smoothness of the objective and standard arguments, we obtain that\nE[f(wt+1)\u2212 f(wt)] \u2264\u2212 \u03b7\n2 E\u2225\u2207t\u22252 +\n\u03b2\u03b72\n2 E\u2225g\u0302t\u22252\n+ \u03b7\n2 E\u2225\u2207\u0302t \u2212\u2207t\u22252 . (5)\nUsing the smoothness of f , we can bound the last term in the right-hand side, corresponding to the gradient bias, by the clients\u2019 average compression error:\nE\u2225\u2207\u0302t \u2212\u2207t\u22252 \u2264 \u03b22 \u00b7 E [ 1\nS \u2211 i\u2208St \u2225w\u0302it \u2212 wt\u22252 ] . (6)\nAdditionally, in Lemma C.1, we derive the following bound on the second moment of the stochastic aggregated gradient:\nE\u2225g\u0302t\u22252\u2264 \u03c3\u03032\nS +4\u03b3E\u2225\u2207t\u22252+\n2\u03b22 S E \u2211 i\u2208St \u2225w\u0302it \u2212 wt\u22252. (7)\nPlugging these bounds back to Eq. (5), we obtain:\nE[f(wt+1)\u2212 f(wt)] \u2264 ( \u2212\u03b7 2 +2\u03b3\u03b2\u03b72 ) E\u2225\u2207t\u22252+ \u03b2\u03b72\u03c3\u03032 2S\n+\n( \u03b22\u03b7\n2 +\u03b23\u03b72\n) E [ 1\nS \u2211 i\u2208St \u2225w\u0302it \u2212 wt\u22252 ] . (8)\nRecall that each client constructs the current model estimate by summing an anchor and a compressed correction, i.e., w\u0302it = y i t + Cc(wt \u2212 yit), where yit (i.e., the anchor) is some model from up to KV rounds ago; for simplicity, assume that all clients obtain the oldest anchor, i.e., yit = wt\u2212KV . Therefore, using Assumption 3.3, we can bound the compression error by the difference between the current model and the obtained anchor, which is proportional to the sum of the last few (aggregated) gradients: E\u2225w\u0302it\u2212wt\u22252 \u2264 \u03c92E\u2225wt \u2212 yit\u22252 = \u03c92\u03b72E \u2225\u2225\u2225\u2225 t\u22121\u2211\nk=t\u2212KV\ng\u0302k \u2225\u2225\u2225\u22252. Denote the client compression error by eit := E\u2225w\u0302it \u2212 wt\u22252. Decomposing each gradient into bias and variance as g\u0302k = \u2207\u0302k + \u03be\u0302k, where E[\u03be\u0302k] = 0, we get: eit \u2264 2\u03c92\u03b72E \u2225\u2225\u2225\u2225 t\u22121\u2211\nk=t\u2212KV\n\u2207\u0302k \u2225\u2225\u2225\u22252 + 2\u03c92\u03b72E\u2225\u2225\u2225\u2225 t\u22121\u2211\nk=t\u2212KV\n\u03be\u0302k \u2225\u2225\u2225\u22252\n\u2264 2\u03c92\u03b72KV t\u22121\u2211\nk=t\u2212KV\nE\u2225\u2207\u0302k\u22252+2\u03c92\u03b72 t\u22121\u2211\nk=t\u2212KV\nE\u2225\u03be\u0302k\u22252,\nwhere we used the orthogonality of the noises, i.e., E[\u03be\u0302\u22a4k \u03be\u0302\u2113] = 0 for k \u0338= \u2113. Plugging-in \u03be\u0302k = g\u0302k \u2212 \u2207\u0302k, we obtain: eit\u22646\u03c92\u03b72KV t\u22121\u2211\nk=t\u2212KV\nE\u2225\u2207\u0302k\u22252+4\u03c92\u03b72 t\u22121\u2211\nk=t\u2212KV\nE\u2225g\u0302k\u22252.\nUsing Eq. (6) and (7) to bound E\u2225\u2207\u0302k\u22252 and E\u2225g\u0302k\u22252, respectively, we get a recursive relation as the client compression error at round t depends on all prior errors. This is due to error accumulation from computing the aggregated gradients\nat inaccurate iterates. Lemma C.2 provides a (non-recursive) bound on the compression error at round t. Plugging this bound back to Eq. (8), summing over t = 0, . . . , T \u2212 1, and using some algebra, we get:\nE[f(wT )\u2212 f(w0)] \u2264\u2212 \u03b7\n4 T\u22121\u2211 t=0 E\u2225\u2207t\u22252\n+\n( \u03b2\u03b72\n2 + 12\u03b22\u03c92KV\u03b73\n) T \u03c3\u03032\nS .\nRearranging terms and tuning \u03b7 concludes the proof. \u25a1\nIt is important to note that our framework may also introduce some opportunities for system-wise improvements that are not captured by standard analysis. For example, with a larger pool of clients that are able to participate in a training procedure, it may be easier and faster to reach the desired threshold of participants in each round. Also, it may offer access to more data overall with a different resulting model. How to capture and model such potential benefits in a way that is consistent with and useful in real deployments? Indeed, this is an interesting and significant challenge for future work that may yield new FL policies."
        },
        {
            "heading": "4. Anchor Compression",
            "text": "Compressing the anchors is an essential building block of DoCoFL for reducing the total downlink bandwidth. While many compression techniques exist, most techniques were designed for gradient compression. Although we can use many such methods in our framework, it is less desirable to use a gradient compression scheme for anchor compression since the compression error of the anchor has a larger impact on the resulting model accuracy than the correction error; recall that the model weights, unlike the correction, do not decay throughout training. Accordingly, we designed a compression technique for that purpose.\nWe first observe that this technique is considerably less restricted on the PS side (i.e., compression) than on the client\u2019s side (i.e., decompression). On the PS side, we typically have more resources and time (a new anchor is deployed only every K rounds) to employ more complex calculations, where at the client side we seek speed and lighter computations.\nConsequently, we devised a compression method called Entropy-Constrained Uniform Quantization (ECUQ). The main idea behind this approach is to approximate EntropyConstrained Quantization (ECQ), which is an optimal scheme among a large family of quantization techniques (Chou et al., 1989). Intuitively, given some vector, ECQ finds the best quantization values (i.e., those that minimize the mean squared error) such that after quantization and entropy encoding (e.g., Huffman coding, Huffman (1952)) of the resulting quantized vector, a given budget constraint is respected. However, this approach is slow,\n2 3 4 5 6 7 8 10\u22125 10\u22124 10\u22123 10\u22122 10\u22121 100 101 N M S E\nCNN\n2 3 4 5 6 7 8 10\u22125 10\u22124 10\u22123 10\u22122 10\u22121 100 101 LSTM 2 3 4 5 6 7 8 10\u22125 10\u22124 10\u22123 10\u22122 10\u22121 100 101 Lognormal(0,1)\n2 3 4 5 6 7 8 10\u22123 10\u22122 10\u22121 100 101 Ti m e [s ] 2 3 4 5 6 7 8 10\u22123 10\u22122 10\u22121 100 101 2 3 4 5 6 7 8 10\u22123 10\u22122 10\u22121 100 101\nBits/Coordinate\nHadamard SQ Kashin SQ QSGD EDEN ECUQ\nFigure 2. ECUQ vs. gradient compression methods: NMSE (top) and encoding time (bottom) for three different cases \u2013 recorded model parameters of a CNN (left); LSTM (middle); and vectors drawn from synthetic LogNormal(0, 1) distribution (right).\ncomplex, and unstable (sensitive to hyperparameters), which renders it unsuitable for online compression of large vectors.\nAs we detail in Appendix D, ECUQ employs a double binary search to efficiently find the maximal number of uniformly spaced quantization values (between the minimal and maximal values of the input vector), such that after quantization, the entropy of the vector would be within a small threshold from a given bandwidth constraint. Since computing the entropy of a vector does not require to actually encode it, the double binary search is executed fast, and only after finding these quantization values, we encode the vector.\nIn Appendix D, we compare ECUQ, ECQ and a technique based on K-Means clustering (ECK-Means), which also approximates ECQ (see Figure 6); our results indicate that ECUQ is always better than ECK-Means and competitive with ECQ while being orders of magnitude faster.\nWe also compare ECUQ with four recent gradient compression techniques: (1) Hadamard followed by stochastic quantization (SQ) (Suresh et al., 2017); (2) Kashin\u2019s representation followed by SQ (Lyubarskii & Vershynin, 2010; Safaryan et al., 2022); (3) QSGD followed by Elias Gamma encoding (Alistarh et al., 2017); and (4) EDEN (Vargaftik et al., 2022). We test these in three different scenarios: (1) Model parameters of a convolutional neural network (CNN) with \u2248 11M parameters; (2) Model parameters of an LSTM network with \u2248 8M parameters; and (3) Vectors from a LogNormal(0, 1) distribution with 1M entries. We repeat each experiment ten times and report the mean.\nAs shown in Figure 2, ECUQ consistently offers the best NMSE, which is by up to an order of magnitude better than that of the second best. We also find that ECUQ is sufficiently fast to be used by the PS every several rounds (a typical cross-device FL round may take minutes to hours).\nWhile our comparison here focuses on quantization-based\nTable 2. Tasks configuration.\nDataset Net. (# params) # clients (S) Partition CIFAR-100 ResNet-9 (4.9M) 200 (10) I.I.D EMNIST LeNet (65K) 1000 (20) Non-I.I.D Amazon LSTM (8.3M) 500 (10) I.I.D Shakespeare LSTM (820K) 1129 (20) Non-I.I.D\nmethods, in Appendix E we compare ECUQ with three popular compression techniques that do not rely on quantization, namely, Rand-K, Top-K (Alistarh et al., 2018), and CountSketch (Charikar et al., 2002) and show similar trends. Nevertheless, we note that quantization is mostly orthogonal to such techniques and they can be used in conjunction5."
        },
        {
            "heading": "5. Experiments",
            "text": "As previously mentioned, most prior downlink compression methods rely on repeated client participation and/or control variates and are, therefore, less suitable for large-scale cross-device FL where a client may participate only once or a handful of times during the training procedure. Also, there are prior methods that target model size reduction via sketching (Rabbani et al., 2021) and sparsification (Shah & Lau, 2021), but rely on restrictive assumptions and typically result in longer training times and lower accuracy with an increasing number of clients and decreasing participation ratio. Some other model size reduction methods, such as low-rank approximation (e.g., Tai et al. (2016)) are orthogonal to DoCoFL and they can be used in conjunction.\nAccordingly, we compare DoCoFL with an uncompressed baseline obtained by running FedAvg (McMahan et al., 2017) without any (i.e., uplink or downlink) compression, utilizing full precision (i.e., 32-bit floats) in both directions. Then, we perform an ablation study that shows the consistency of DoCoFL with respect to its hyperparameters.\nWe cover a wide range of use cases that include two image classification and two language processing tasks with different configurations and data partitioning, as shortly summarized in Table 2 and further detailed in Appendix F.\nImage Classification. We use the CIFAR-100 and EMNIST datasets. For CIFAR-100 (Krizhevsky et al., 2009), the data distribution among the clients is i.i.d. For EMNIST (Cohen et al., 2017), the dataset of each client is composed of 10% i.i.d samples from the entire dataset and 90% i.i.d samples of 2 out of 47 classes (Karimireddy et al., 2020).\nLanguage Processing. For language processing, we perform a sentiment analysis task on the Amazon Reviews dataset (Zhang et al., 2015) with i.i.d data partitioning; and a next-character prediction task on the Shakespeare\n5For example, Vargaftik et al. (2022) use Rand-K as a subroutine alongside quantization to reach a sub-bit compression ratio.\nTable 3. Best validation accuracy for different tasks. The configuration triplet (bw, bc, bg) means using bw, bc, and bg bits per coordinate for the anchor, correction, and gradient (uplink) compression, respectively. For all tasks, we use K = 10 and V = 3.\nCIFAR-100 EMNIST Amazon Shakespeare (bw, bc, bg) Accuracy (bw, bc, bg) Accuracy (bw, bc, bg) Accuracy (bw, bc, bg) Accuracy\nFedAvg \u2013 65.03 \u2013 85.85 \u2013 92.59 \u2013 46.10\nDoCoFL Config 1 (2, 2, 1) 64.94 (4, 4, 3) 85.94 (6, 6, 2) 92.51 (4, 4, 4) 45.86Config 2 (2, 1/2, 1) 65.81 (2, 2, 3) 86.83 (4, 4, 2) 92.24 (2, 2, 4) 46.55\ndataset (McMahan et al., 2017), where each client holds data associated with a single role and play.\nIn all simulations, we run DoCoFL with ECUQ for anchor compression (i.e., Cw), and EDEN (Vargaftik et al., 2022) for correction and uplink compression (i.e., Cc and Cg). Main Results. In Table 3, we report the best validation accuracy achieved during training for FedAvg and two representative configurations of DoCoFL. It is evident that the validation accuracy of DoCoFL and FedAvg is always competitive; in some tasks, DoCoFL performs somewhat better. For example, for EMNIST, DoCoFL reduces the online and total downlink bandwidth by 16\u00d7and 8\u00d7, respectively, while achieving higher validation accuracy.\nAs is often the case in FL, our evaluation indicates that using more bandwidth does not necessarily lead to higher validation accuracy. While using less bandwidth usually impacts the train accuracy, as it implies a larger compression error, it may positively affect the model\u2019s generalization ability. We further reinforce these observations in Appendix G.\nHyperparameters Ablation. In Figure 3, we report the final train accuracy of DoCoFL for the CIFAR-100 task with varying values of K \u2208 {10, 50, 100, 500} and V \u2208 {3, 5, 10} under two bandwidth configurations. The results indicate that our framework performs as expected for a wide range of anchor deployment rates and queue capacities. Additionally, in line with our theoretical findings, when the multiplication KV is too large, the norm of the correction becomes sizable, which can hinder the final accuracy and even convergence. To allow the use of large KV , one may increase the correction bandwidth, trading online bandwidth for a larger anchor download time window. We defer an ablation study of the anchor and correction bandwidth budgets to Appendix G.2. These results indicate that DoCoFL performs well for a wide range of budgets and provide further intuition for configuring these parameters.\nThe Value of the Correction Term. When ignoring the correction, DoCoFL may resemble other frameworks such as delayed gradients (e.g., Stich & Karimireddy (2019)) and asynchronous SGD (e.g., Lian et al. (2015)). In Appendix G.3 we discuss this similarity and convey that ignoring the correction leads to a significant performance drop.\nDoCoFL and EF21. In Appendix G.4, we focus on recent advancements based on the EF21 technique (Richta\u0301rik et al., 2021), which relies on client-side memory. Specifically, we extend EF21-PP (Fatkhullin et al., 2021) to support downlink bandwidth reduction using DoCoFL while matching baseline accuracy, where naive model compression results in performance degradation. Also, we discuss some similarities with EF21-P + DIANA (Gruntkowska et al., 2022)."
        },
        {
            "heading": "6. Conclusion",
            "text": "In this work, we presented DoCoFL, a framework for downlink compression in the challenging cross-device FL setup. By enlarging the clients\u2019 model download time window, reducing total downlink bandwidth requirements, and allowing for uplink compression, DoCoFL is designed to allow more resource-constrained and diverse clients to participate in the training procedure. Experiments over various tasks indicate that DoCoFL indeed significantly reduces bi-directional bandwidth usage while performing competitively with an uncompressed baseline. In Appendix H, we discuss some directions for future research."
        },
        {
            "heading": "Acknowledgements",
            "text": "We thank the reviewers and the area chair for their helpful suggestions. KYL is supported by the Israel Science Foundation (grant No. 447/20) and the Technion Center for Machine Learning and Intelligent Systems (MLIS)."
        },
        {
            "heading": "A. Suboptimality of Gradient Descent with Weights Compression",
            "text": "In this section, we give an example of a (strongly) convex function on R, for which we show that running gradient descent with gradients computed at estimated (i.e., lossily compressed and then decompressed) iterates (rather than at the exact iterates) does not converge to the global minimum. Instead, it converges to a suboptimal solution.\nLet f : R\u2192 R be the following convex and smooth function:\nf(w) = 1 2 (w \u2212 1)2 + 1 2 [w \u2212 1]2+ ,\nwhere [w]+ = max (0, w). Note that w \u2217 = 1 is the global minimizer of f . We analyze the following update rule:\nwt+1 = wt \u2212 \u03b7\u2207f(w\u0302t) w\u0302t = Cw(wt) ,\nwhere \u03b7 > 0 is the step size, and C : R\u2192 R is a randomized, unbiased compression operator with bounded NMSE, i.e.,\nE[Cw(w)] = w, E|Cw(w)\u2212 w|2 \u2264 \u03c92w|w|2, \u2200w \u2208 R .\nWe can alternatively write: w\u0302t = wt + \u03f5t|wt|, where E[\u03f5t] = 0, E[\u03f52t ] \u2264 \u03c92w, and {\u03f5t}t are independent. Thus, we can rewrite the above update rule as wt+1 = wt \u2212 \u03b7\u2207f(wt + \u03f5t|wt|) . (9) In Eq. (9), we repeatedly apply the stochastic mapping: w 7\u2192 w \u2212 \u03b7\u2207f(w + \u03f5|w|). If this process converges in expectation, it converges to a point w\u0303 for which E[\u2207f(w\u0303 + \u03f5|w\u0303|)] = 0. We show that w\u2217 = 1 does not satisfy this condition, which will imply that this process does not converge to w\u2217. First, note that f is differentiable, and \u2207f(w) = (w \u2212 1) + [w \u2212 1]+. Thus,\nE [\u2207f(w\u2217 + \u03f5|w\u2217|)] = E [\u2207f(1 + \u03f5)] = E [ \u03f5+ [\u03f5]+ ] = E [\u03f5]+ ,\nwhere the last equality follows from the linearity of expectation, E[\u03f5] = 0. Now, note that unless \u03f5 = 0 almost surely, we necessarily have E[max {0, \u03f5}] > 0, which implies that the iterative update in Eq. (9) does not converge in expectation to w\u2217 = 1.\nB. Why DoCoFL with Anchor Compression Works To support Theorem 3.4, in which we establish the convergence of DoCoFL when the anchor compression Cw is identity, in this section we give theoretical intuition and numerical results that convey as for why DoCoFL works when Cw is not the identity mapping.\nConsider the framework we analyze in Appendix C, namely the generalization of DoCoFL given by Algorithm 3. Adding an anchor compressor (i.e., Cw) implies that each client now obtains a compressed outdated model y\u0302it = Cw(yit) and a corresponding correction \u2206\u0302it = Cc(wt \u2212 y\u0302it), and constructs w\u0302it = y\u0302it + \u2206\u0302it. Thus, adding an anchor compression affects the client\u2019s model estimation error E\u2225w\u0302it \u2212 wt\u22252, which we bound in Eq. (23). Denote by e2t,i := \u2225yit + \u2206\u0302it \u2212 wt\u22252 the client\u2019s squared estimation error when not using anchor compression (i.e., when Cw is identity), and by e\u03022t,i := \u2225y\u0302it + \u2206\u0302it \u2212 wt\u22252 the squared estimation error when using anchor compression. If one could show that the following condition holds: E[e\u03022t,i] \u2264 C2E[e2t,i] , (10) for some moderate C > 0, then we can simply bound E[e\u03022t,i] in the left-hand side of Eq. (23), and the rest of our analysis holds. However, we know that, in general, this condition does not hold (recall the counter-example in Appendix A).\nNevertheless, we empirically show that it holds in our evaluation where we have non-convex and noisy optimization. More generally, in cross-device FL, client sampling and stochastic gradient estimation add natural noise to the optimization process, and we empirically show that the additional estimation error due to anchor compression with ECUQ is sufficiently low and allows convergence, as conveyed above.\nIn Figure 4 we present the ratio \u03c1t :=\n\u2211\ni\u2208St e\u0302 2 t,i/\n\u2211\ni\u2208St e 2 t,i for different anchor compression budgets in CIFAR-100 and Amazon Reviews experiments. First, note that the ratio is mostly stable throughout the entire training. Additionally, when we increase the bandwidth for anchor compression, the ratio decreases, to the extent that for 8 bits per coordinate, the ratio is \u2248 1; this is when the error induced by the correction compression dominates the estimation error. We note that our intuition gives rise to using an adaptive budget for anchor compression; since \u03c1t can be measured by the PS (i.e., it has access to yit, y\u0302 i t, wt and the correction compressor Cc), we can keep track of it, and increase the anchor compression budget if \u03c1t is too large. We leave such investigation to future work."
        },
        {
            "heading": "C. Proof of Theorem 3.4",
            "text": "In this section we prove Theorem 3.4, which we restate here for convenience,\nTheorem 3.4. Let \u03c3\u03032 := \u03c32 + 4 ( 1\u2212 SN ) G2, \u03b3 := 1 + ( 1\u2212 SN ) B2\nS , and M := f(w0)\u2212 f\u2217. Then, running DoCoFL with Cw and Cg as the identity mappings (and with appropriately selected \u03b7) guarantees\nE\n[ 1\nT T\u22121\u2211 t=0\n\u2225\u2207f(wt)\u22252 ] \u2208 O (\u221a M\u03b2\u03c3\u03032\nTS + (M2\u03b22\u03c92KV\u03c3\u03032)1/3 T 2/3S1/3 + \u03b3M\u03b2(\u03c9KV + 1) T\n) .\nProof. To simplify mathematical notations and computations, we analyze a more general framework than DoCoFL, where at each round, each client can download any model from up to T rounds prior to their participation round as anchor. This generalized policy is described in Algorithm 3.\nUsing Theorem 1, that proves the convergence of Algorithm 3 (see Appendix C.1), we prove Theorem 3.4. Namely, DoCoFL with Cw and Cg as identity mappings is a private case of Algorithm 3 where T = KV and clients can only download models from specific prior rounds (multiplications of K). Thus, plugging-in T = KV to Theorem 1 concludes the proof.\nC.1. Proof of Theorem 1 Theorem 1. Suppose Assumptions 3.1-3.3 are satisfied. Let \u03c3\u03032 := \u03c32+4 ( 1\u2212 SN ) G2, \u03b3 := 1+ ( 1\u2212 SN ) B2\nS , \u03b8 := \u03c9T +1, and M := f(w0)\u2212 f\u2217. Then, running Algorithm 3 with \u03b7 = min { 1 30\u03b3\u03b2\u03b8 , \u221a 2MS \u03b2\u03c3\u03032T , ( MS 12\u03b22\u03c92T \u03c3\u03032T )1/3} guarantees\nE\n[ 1\nT T\u22121\u2211 t=0\n\u2225\u2207f(wt)\u22252 ] \u2264 4 \u221a 2M\u03b2\u03c3\u03032\nTS + 8 (12M2\u03b22\u03c92T \u03c3\u03032)1/3 T 2/3S1/3 + 120\u03b3M\u03b2\u03b8 T . (11)\nAlgorithm 3 Meta-Algorithm (generalization of DoCoFL)\nInput: Initial weights w0 \u2208 Rd, learning rate \u03b7, correction compression Cc, client participation process P(\u00b7) for t = 0, . . . , T \u2212 1 do\nObtain participating clients, St \u2190 P(t) \u25b7 |St| = S for client i \u2208 St in parallel do\nObtain model weights (anchor), yit = wt\u2212\u03c4 it \u25b7 \u03c4 i t \u2208 [0, T ] Obtain compressed correction, \u2206\u0302it = Cc(wt \u2212 yit) Construct model estimate, w\u0302it = y i t + \u2206\u0302 i t Compute local gradient, g\u0302it = g i t(w\u0302 i t)\nCommunicate g\u0302it back to server end for Aggregate local gradients, g\u0302t := 1S \u2211 i\u2208St g\u0302 i t\nUpdate weights, wt+1 = wt \u2212 \u03b7g\u0302t end for\nProof. For the ease of notation, let \u2207t := \u2207f(wt) and \u03c3\u03032S := \u03c3\u03032/S. Throughout our analysis, we sometimes use w\u0302it even when i /\u2208 St, which is not well-defined. To resolve this, one can think about the following mathematically equivalent process, where at each round, all clients i \u2208 [N ] obtain some previous model (anchor) yit and the corresponding correction \u2206\u0302it, but only i \u2208 St actually participate in the optimization. In that sense, for all i /\u2208 St, w\u0302it is the estimated model of client i if it were to participate in round t.\nLet \u2207\u0302t := 1N \u2211 i\u2208[N ]\u2207fi(w\u0302it) = E[g\u0302t]. From the \u03b2-smoothness of the objective,\nE[f(wt+1)\u2212 f(wt)] \u2264 \u2212\u03b7E[g\u0302\u22a4t \u2207t] + \u03b2\u03b72\n2 E\u2225g\u0302t\u22252\n= \u2212\u03b7E[\u2207\u0302\u22a4t \u2207t] + \u03b2\u03b72\n2 E\u2225g\u0302t\u22252\n= \u2212\u03b7E\u2225\u2207t\u22252 + \u03b7E[\u2207\u22a4t (\u2207t \u2212 \u2207\u0302t)]\ufe38 \ufe37\ufe37 \ufe38 =(A) + \u03b2\u03b72 2 E\u2225g\u0302t\u22252 , (12)\nwhere the first equality follows from the law of total expectation, and the second equality from the linearity of expectation.\nBounding (A): Using the inequality a\u22a4b \u2264 12\u2225a\u22252 + 12\u2225b\u22252, we get that\n\u03b7E[\u2207\u22a4t (\u2207t \u2212 \u2207\u0302t)] \u2264 \u03b7\n2 E\u2225\u2207t\u22252 +\n\u03b7 2 E\u2225\u2207t \u2212 \u2207\u0302t\u22252 .\nFocusing on the second term in the right-hand side, we have:\nE\u2225\u2207t \u2212 \u2207\u0302t\u22252 = E \u2225\u2225\u2225\u2225\u2225 1N N\u2211 i=1 ( \u2207fi(wt)\u2212\u2207fi(w\u0302it) )\u2225\u2225\u2225\u2225\u2225 2\n\u2264 1 N N\u2211 i=1 E\u2225\u2207fi(wt)\u2212\u2207fi(w\u0302it)\u22252\n\u2264 \u03b2 2\nN N\u2211 i=1 E\u2225w\u0302it \u2212 wt\u22252 , (13)\nwhere in the first inequality we used Lemma C.5, and the second inequality follows from the \u03b2-smoothness of each fi. Plugging back this bound, we get:\n\u03b7E[\u2207\u22a4t (\u2207t \u2212 \u2207\u0302t)] \u2264 \u03b7\n2 E\u2225\u2207t\u22252 +\n\u03b22\u03b7\n2N N\u2211 i=1 E\u2225w\u0302it \u2212 wt\u22252 .\nUsing Lemma C.1 to bound E\u2225g\u0302t\u22252 and the bound on (A), we get from Eq. (12) that\nE[f(wt+1)\u2212 f(wt)] \u2264\u2212 \u03b7\n2 E\u2225\u2207t\u22252 +\n\u03b22\u03b7\n2N N\u2211 i=1 E\u2225w\u0302it \u2212 wt\u22252\n+ \u03b2\u03b72\n2\n( \u03c3\u03032S + 4\u03b3E\u2225\u2207t\u22252 + 2\u03b22\nN N\u2211 i=1 E\u2225w\u0302it \u2212 wt\u22252 )\n= ( \u2212\u03b7 2 + 2\u03b3\u03b2\u03b72 ) E\u2225\u2207t\u22252 + \u03b2\u03b72\u03c3\u03032S 2\n+\n( \u03b22\u03b7\n2 + \u03b23\u03b72 ) \u00b7 1 N N\u2211 i=1 E\u2225w\u0302it \u2212 wt\u22252 .\nApplying Lemma C.2, we can bound 1N \u2211N i=1 E\u2225w\u0302it \u2212 wt\u22252 to get that\nE[f(wt+1)\u2212 f(wt)] \u2264 ( \u2212\u03b7 2 + 2\u03b3\u03b2\u03b72 ) E\u2225\u2207t\u22252 + \u03b2\u03b72\u03c3\u03032S 2\n+\n( \u03b22\u03b7\n2 + \u03b23\u03b72\n)[ \u03b1 ( 1 +\nt\u2211 k=1\nk(\u03c1T )k ) \u03c3\u03032S + 2\u03b3\n\u03b22T t\u2211 k=1 (\u03c1T )k t\u2212k\u2211 \u2113=t\u2212kT\nE\u2225\u2207\u2113\u22252 ]\n= ( \u2212\u03b7 2 + 2\u03b3\u03b2\u03b72 ) E\u2225\u2207t\u22252 +\n[ \u03b2\u03b72\n2 + \u03b1\n( \u03b22\u03b7\n2 + \u03b23\u03b72\n)( 1 +\nt\u2211 k=1\nk(\u03c1T )k )] \u03c3\u03032S\n+\n( \u03b22\u03b7\n2 + \u03b23\u03b72\n) 2\u03b3\n\u03b22T t\u2211 k=1 (\u03c1T )k t\u2212k\u2211 \u2113=t\u2212kT\nE\u2225\u2207\u2113\u22252\ufe38 \ufe37\ufe37 \ufe38 =(B) , (14)\nwhere \u03b1 = 4\u03c92\u03b72T and \u03c1 = 20\u03b22\u03c92\u03b72T . Since \u03b7 \u2264 130\u03b3\u03b2(\u03c9T +1) \u2264 1\u221a40\u03b2\u03c9T , it holds that \u03c1T = 20\u03b2 2\u03c92T 2\u03b72 \u2264 1/2 < 1, and thus we can bound the coefficient of \u03c3\u03032S using Lemma C.7 as\nt\u2211 k=1 k(\u03c1T )k \u2264 \u221e\u2211 k=1 k(\u03c1T )k = \u03c1T (1\u2212 \u03c1T )2 \u2264 4\u03c1T \u2264 2 , (15)\nwhere we used 1(1\u2212\u03c1T )2 \u2264 4, and \u03c1T \u2264 1/2.\nBounding (B): To bound (B), we change the summation order. Consider a fixed \u2113 \u2208 N. Note that (\u03c1T )k appears as a coefficient of E\u2225\u2207\u2113\u22252 if and only if t\u2212 kT \u2264 \u2113 \u2264 t\u2212 k, which is equivalent to t\u2212\u2113T \u2264 k \u2264 t\u2212 \u2113. Therefore, we have\nt\u2211 k=1 (\u03c1T )k t\u2212k\u2211 \u2113=t\u2212kT E\u2225\u2207\u2113\u22252 = t\u22121\u2211 \u2113=0  t\u2212\u2113\u2211 k=\u2308 t\u2212\u2113T \u2309 (\u03c1T )k E\u2225\u2207\u2113\u22252\n\u2264 t\u22121\u2211 \u2113=0  \u221e\u2211 k=\u2308 t\u2212\u2113T \u2309 (\u03c1T )k E\u2225\u2207\u2113\u22252\n= 1 1\u2212 \u03c1T t\u22121\u2211 \u2113=0 (\u03c1T )\u2308 t\u2212\u2113T \u2309E\u2225\u2207\u2113\u22252\nPlugging this bound and Eq. (15) back to Eq. (14) gives E[f(wt+1)\u2212 f(wt)] \u2264 ( \u2212\u03b7 2 + 2\u03b3\u03b2\u03b72 ) E\u2225\u2207t\u22252 + ( \u03b2\u03b72 2 + 3\u03b1 ( \u03b22\u03b7 2 + \u03b23\u03b72 )) \u03c3\u03032S\n+ ( \u03b7 + 2\u03b2\u03b72 ) \u03b3 (1\u2212 \u03c1T )T t\u22121\u2211 k=0 (\u03c1T )\u2308 t\u2212kT \u2309E\u2225\u2207k\u22252 .\nSumming over t = 0, . . . , T \u2212 1, we obtain\nE[f(wT )\u2212 f(w0)] = T\u22121\u2211 t=0 E[f(wt+1)\u2212 f(wt)]\n\u2264 ( \u2212\u03b7 2 + 2\u03b3\u03b2\u03b72 ) T\u22121\u2211 t=0 E\u2225\u2207t\u22252 + ( \u03b2\u03b72 2 + 3\u03b1 ( \u03b22\u03b7 2 + \u03b23\u03b72 )) T \u03c3\u03032S\n+ ( \u03b7 + 2\u03b2\u03b72 ) \u03b3 (1\u2212 \u03c1T )T T\u22121\u2211 t=0 t\u22121\u2211 k=0\n(\u03c1T )\u2308 t\u2212kT \u2309E\u2225\u2207k\u22252\ufe38 \ufe37\ufe37 \ufe38 =(C) . (16)\nFocusing on (C), we can change the outer summation bounds as\nT\u22121\u2211 t=0 t\u22121\u2211 k=0 (\u03c1T )\u2308 t\u2212kT \u2309E\u2225\u2207k\u22252 = T\u22121\u2211 t=1 t\u22121\u2211 k=0 (\u03c1T )\u2308 t\u2212kT \u2309E\u2225\u2207k\u22252 \u2264 T\u2211 t=1 t\u22121\u2211 k=0 (\u03c1T )\u2308 t\u2212kT \u2309E\u2225\u2207k\u22252 . (17)\nNow, we can bound the right-hand side using Lemma C.8 with a = \u03c1T < 1 and xk = E\u2225\u2207k\u22252 \u2265 0 to get that\nT\u2211 t=1 t\u22121\u2211 k=0 (\u03c1T )\u2308 t\u2212kT \u2309E\u2225\u2207k\u22252 \u2264 T \u03c1T 1\u2212 \u03c1T T\u22121\u2211 t=0 E\u2225\u2207t\u22252 .\nPlugging this bound back to Eq. (16) and using 1(1\u2212\u03c1T )2 \u2264 4 gives\nE[f(wT )\u2212 f(w0)] \u2264 ( \u2212\u03b7 2 + 2\u03b3\u03b2\u03b72 ) T\u22121\u2211 t=0 E\u2225\u2207t\u22252 + ( \u03b2\u03b72 2 + 3\u03b1 ( \u03b22\u03b7 2 + \u03b23\u03b72 )) T \u03c3\u03032S\n+ ( \u03b7 + 2\u03b2\u03b72 ) \u03b3\u03c1T (1\u2212 \u03c1T )2 T\u22121\u2211 t=0 E\u2225\u2207t\u22252\n\u2264 ( \u2212\u03b7 2 + 2\u03b3\u03b2\u03b72 + 4\u03b3\u03c1T ( \u03b7 + 2\u03b2\u03b72 )) T\u22121\u2211 t=0 E\u2225\u2207t\u22252\n+\n( \u03b2\u03b72\n2 +\n3\u03b1\u03b22\n2\n( \u03b7 + 2\u03b2\u03b72 )) T \u03c3\u03032S\n\u2264 ( \u2212\u03b7 2 + 2\u03b3\u03b2\u03b72 + 8\u03b3\u03c1T \u03b7 ) T\u22121\u2211 t=0 E\u2225\u2207t\u22252 + ( \u03b2\u03b72 2 + 3\u03b1\u03b22\u03b7 ) T \u03c3\u03032S ,\nwhere in the last inequality we used the fact that \u03b7 \u2264 130\u03b3\u03b2\u03b8 \u2264 130\u03b2 to bound 2\u03b2\u03b72 \u2264 \u03b7. Substituting \u03b1 and \u03c1, we obtain:\nE[f(wT )\u2212 f(w0)] \u2264 ( \u2212\u03b7 2 + 2\u03b3\u03b2\u03b72 + 160\u03b3\u03b22\u03c92T 2\u03b73 ) T\u22121\u2211 t=0 E\u2225\u2207t\u22252\n+\n( \u03b2\u03b72\n2 + 12\u03b22\u03c92T \u03b73\n) T \u03c3\u03032S .\nSince \u03b7 \u2264 130\u03b3\u03b2\u03b8 , we can bound the coefficient of \u2211T\u22121 t=0 E\u2225\u2207t\u22252 using Lemma C.9. We get:\nE[f(wT )\u2212 f(w0)] \u2264 \u2212 \u03b7\n4 T\u22121\u2211 t=0 E\u2225\u2207t\u22252 + ( \u03b2\u03b72 2 + 12\u03b22\u03c92T \u03b73 ) T \u03c3\u03032S . (18)\nRearranging terms, multiplying by 4/\u03b7T , and plugging \u03c3\u03032S = \u03c3\u0303 2/S then gives\nE\n[ 1\nT T\u22121\u2211 t=0\n\u2225\u2207t\u22252 ] \u2264 4M\n\u03b7T +\n2\u03b2\u03c3\u03032\nS \u03b7 + 48\u03b22\u03c92T \u03c3\u03032 S \u03b72 ,\nwhere we also used E[f(w0)\u2212 f(wT )] \u2264 f(w0)\u2212 f\u2217 \u2264M . Applying Lemma C.10 with our learning rate \u03b7, we finally obtain:\nE\n[ 1\nT T\u22121\u2211 t=0\n\u2225\u2207t\u22252 ] \u22644M\nT\n( 30\u03b3\u03b2\u03b8 + \u221a \u03b2\u03c3\u03032T\n2MS +\n( 12\u03b22\u03c92T \u03c3\u03032T\nMS\n)1/3) + 2\u03b2\u03c3\u03032 S \u00b7 \u221a 2MS \u03b2\u03c3\u03032T\n+ 48\u03b22\u03c92T \u03c3\u03032 S \u00b7 (\nMS\n12\u03b22\u03c92T \u03c3\u03032T )2/3 =4 \u221a 2M\u03b2\u03c3\u03032\nTS + 8 (12M2\u03b22\u03c92T \u03c3\u03032)1/3 T 2/3S1/3 + 120\u03b3\u03b2\u03b8 T ,\nwhich concludes the proof.\nC.2. Technical Lemmata\nIn this section, we introduce some technical results used throughout our analysis. We start with the following lemma, yielding a bound on the second moment of the aggregated gradients that our PS uses to update its model.\nLemma C.1. Consider the notations of Theorem 1. For every t \u2208 [T ], it holds that\nE\u2225g\u0302t\u22252 \u2264 \u03c3\u03032S + 4\u03b3E\u2225\u2207t\u22252 + 2\u03b22 \u00b7 1\nN N\u2211 i=1 E\u2225w\u0302it \u2212 wt\u22252 . (19)\nProof. Since g\u0302t is an aggregation of local gradient, we can write,\nE\u2225g\u0302t\u22252 = E \u2225\u2225\u2225\u2225\u2225 1S \u2211\ni\u2208St\ng\u0302it \u2225\u2225\u2225\u2225\u2225 2 = E \u2225\u2225\u2225\u2225\u2225 1S \u2211 i\u2208St ( g\u0302it \u2212\u2207fi(w\u0302it) +\u2207fi(w\u0302it) )\u2225\u2225\u2225\u2225\u2225 2\n= E \u2225\u2225\u2225\u2225\u2225 1S \u2211 i\u2208St ( g\u0302it \u2212\u2207fi(w\u0302it) )\u2225\u2225\u2225\u2225\u2225 2 + E \u2225\u2225\u2225\u2225\u2225 1S \u2211 i\u2208St \u2207fi(w\u0302it) \u2225\u2225\u2225\u2225\u2225 2 ,\nwhere the last equality follows from Assumption 3.1 as E[(g\u0302it(w\u0302it)\u2212\u2207fi(w\u0302it))\u22a4\u2207fi(w\u0302it)] = 0. Note that the first term in the right-hand side is the variance of the average of S independent random variables with zero mean and variance bounded by \u03c32; therefore, it is bounded by \u03c32/S. Thus, we get that\nE\u2225g\u0302t\u22252 \u2264 \u03c32\nS + E \u2225\u2225\u2225\u2225\u2225 1S \u2211 i\u2208St \u2207fi(w\u0302it) \u2225\u2225\u2225\u2225\u2225 2 . (20)\nFocusing on the second term in the right-hand side, we have that\nE \u2225\u2225\u2225\u2225\u2225 1S \u2211 i\u2208St \u2207fi(w\u0302it) \u2225\u2225\u2225\u2225\u2225 2 \u2264 2E \u2225\u2225\u2225\u2225\u2225 1S \u2211 i\u2208St ( \u2207fi(w\u0302it)\u2212\u2207fi(wt) )\u2225\u2225\u2225\u2225\u2225 2\n\ufe38 \ufe37\ufe37 \ufe38 (A)\n+2E \u2225\u2225\u2225\u2225\u2225 1S \u2211 i\u2208St \u2207fi(wt) \u2225\u2225\u2225\u2225\u2225 2\n\ufe38 \ufe37\ufe37 \ufe38 =(B)\n, (21)\nwhere we used the inequality \u2225a+ b\u22252 \u2264 2\u2225a\u22252 + 2\u2225b\u22252.\nBounding (A): Using Lemma C.5 and the \u03b2-smoothness of the objective, we get that\n2E \u2225\u2225\u2225\u2225\u2225 1S \u2211 i\u2208St ( \u2207fi(w\u0302it)\u2212\u2207fi(wt) )\u2225\u2225\u2225\u2225\u2225 2 \u2264 2 S E [\u2211 i\u2208St \u2225\u2207fi(w\u0302it)\u2212\u2207fi(wt)\u22252 ]\n\u2264 2\u03b2 2\nS E [\u2211 i\u2208St \u2225w\u0302it \u2212 wt\u22252 ]\n= 2\u03b22\nS E [ N\u2211 i=1 \u2225w\u0302it \u2212 wt\u22252 \u00b7 1{i\u2208St} ]\n= 2\u03b22\nN N\u2211 i=1 E\u2225w\u0302it \u2212 wt\u22252 ,\nwhere the last equality follows from our assumption about the client participation process P(\u00b7), which guarantees that P(i \u2208 St) = S/N , independently of the optimization process.\nBounding (B): By the law of total expectation, (B) can be written as follows,\n2E \u2225\u2225\u2225\u2225\u2225 1S \u2211 i\u2208St \u2207fi(wt) \u2225\u2225\u2225\u2225\u2225 2 = 2E \u2225\u2225\u2225\u2225\u2225 N\u2211 i=1 ( 1 S \u2207fi(wt) \u00b7 1{i\u2208St} )\u2225\u2225\u2225\u2225\u2225 2 = 2E E \u2225\u2225\u2225\u2225\u2225 N\u2211 i=1 ( 1 S \u2207fi(wt) \u00b7 1{i\u2208St} )\u2225\u2225\u2225\u2225\u2225 2 wt  . (22) Thus, we can use Lemma C.3 with Xi = 1S\u2207fi(wt) \u00b7 1{i\u2208St}, i \u2208 [N ] to bound the inner expectation. Using P(i \u2208 St) = S/N , we have that\nE[Xi|wt] = 1\nS \u2207fi(wt) \u00b7\nS N = 1 N \u2207fi(wt),\nand,\nE[\u2225Xi \u2212 E[Xi|wt]\u22252|wt] = \u2225\u2207fi(wt)\u22252 \u00b7 E [( 1\nS \u00b7 1{i\u2208St} \u2212\n1\nN\n)2] = \u2225\u2207f(wt)\u22252 \u00b7 Var ( 1\nS \u00b7 1{i\u2208St} ) = \u2225\u2207f(wt)\u22252\nS2 \u00b7 S N\n( 1\u2212 S\nN\n) = \u2225\u2207fi(wt)\u22252\nSN\n( 1\u2212 S\nN\n) ,\nwhere we used the fact that for any event A, the following holds: Var(1A) = P(A) \u00b7 (1 \u2212 P(A)). Therefore, using Lemma C.3, we obtain that\nE \u2225\u2225\u2225\u2225\u2225 N\u2211 i=1 ( 1 S \u2207fi(wt) \u00b7 1{i\u2208St} )\u2225\u2225\u2225\u2225\u2225 2 wt  \u2264 2 \u2225\u2225\u2225\u2225\u2225 1N N\u2211 i=1 \u2207fi(wt) \u2225\u2225\u2225\u2225\u2225 2 + 2 SN ( 1\u2212 S N ) N\u2211 i=1 \u2225\u2207fi(wt)\u22252\n\u2264 2\u2225\u2207t\u22252 + 2\nS\n( 1\u2212 S\nN\n)( G2 +B2\u2225\u2207t\u22252 ) = ( 1\u2212 S\nN\n) 2G2\nS + 2\n( 1 + ( 1\u2212 S\nN\n) B2\nS ) \ufe38 \ufe37\ufe37 \ufe38\n:=\u03b3\n\u2225\u2207t\u22252 ,\nwhere in the second inequality we used the bounded gradient dissimilarity assumption (Assumption 3.2). Plugging back to Eq. (22), we get the following bound on (B):\n2E \u2225\u2225\u2225\u2225\u2225 1S \u2211 i\u2208St \u2207fi(wt) \u2225\u2225\u2225\u2225\u2225 2 \u2264 ( 1\u2212 S N ) 4G2 S + 4\u03b3E\u2225\u2207t\u22252 .\nPlugging the bounds on (A) and (B) in Eq. (20) finally gives\nE\u2225g\u0302t\u22252 \u2264 \u03c32\nS +\n( 1\u2212 S\nN\n) 4G2\nS\ufe38 \ufe37\ufe37 \ufe38 =\u03c3\u03032/S\n+4\u03b3E\u2225\u2207t\u22252 + 2\u03b22\nN N\u2211 i=1 E\u2225w\u0302it \u2212 wt\u22252\n= \u03c3\u03032S + 4\u03b3E\u2225\u2207t\u22252 + 2\u03b22\nN N\u2211 i=1 E\u2225w\u0302it \u2212 wt\u22252 ,\nwhich concludes the proof.\nThe next result establishes a bound on the model estimation error of the clients. Lemma C.2. Consider the notations of Theorem 1. Let \u03b1 := 4\u03c92\u03b72T , \u03c1 := 20\u03b22\u03c92\u03b72T , and \u2207\u2212\u2113 := 0, \u2200\u2113 \u2208 N. Then, the following result holds:\n1\nN N\u2211 i=1 E\u2225w\u0302it \u2212 wt\u22252 \u2264 \u03b1 ( 1 + t\u2211 k=1 k(\u03c1T )k ) \u03c3\u03032S + 2\u03b3 \u03b22T t\u2211 k=1 (\u03c1T )k t\u2212k\u2211 \u2113=t\u2212kT E\u2225\u2207\u2113\u22252\nProof. We use strong induction. Particularly, to prove the result holds at round t, we rely on its correctness over the T prior rounds, i.e., for every s = t\u2212 T , . . . , t\u2212 1. Thus, in our base case, we show that the result holds up to round T . We start with some general observations that hold for any t. Recall that w\u0302it = y i t + C(wt \u2212 yit). From Assumption 3.3, we have E\u2225w\u0302it \u2212 wt\u22252 = E\u2225Cc(wt \u2212 yit)\u2212 (wt \u2212 yit)\u22252 \u2264 \u03c92E\u2225wt \u2212 yit\u22252 . (23) Unrolling the update rule for wt, we have for all i \u2208 [N ] that\nwt = wt\u2212\u03c4 it \u2212 \u03b7 t\u22121\u2211\nk=t\u2212\u03c4 it\ng\u0302k = y i t \u2212 \u03b7 t\u22121\u2211 k=t\u2212\u03c4 it g\u0302k .\nLet g\u0302\u2212k := 0 for all k \u2208 N. Additionally, let \u03be\u0302k = g\u0302k \u2212 \u2207\u0302k for all k, where \u2207\u0302k = E[g\u0302k], as defined in the proof of Theorem 1. Plugging back to Eq. (23), we get that\nE\u2225w\u0302it \u2212 wt\u22252 \u2264 \u03c92\u03b72E\u2225 t\u22121\u2211\nk=t\u2212\u03c4 it\ng\u0302k\u22252 \u2264 2\u03c92\u03b72E\u2225 t\u22121\u2211\nk=t\u2212\u03c4 it\n\u2207\u0302k\u22252 + 2\u03c92\u03b72E\u2225 t\u22121\u2211\nk=t\u2212\u03c4 it\n\u03be\u0302k\u22252 , (24)\nwhere the last inequality follows from \u2225a+ b\u22252 \u2264 2\u2225a\u22252 + 2\u2225b\u22252. Using Lemma C.5, we can bound the first term in the right-hand side, as\nE\u2225 t\u22121\u2211\nk=t\u2212\u03c4 it\n\u2207\u0302k\u22252 \u2264 \u03c4 it t\u22121\u2211\nk=t\u2212\u03c4 it\nE\u2225\u2207\u0302k\u22252 \u2264 T t\u22121\u2211\nk=t\u2212T\nE\u2225\u2207\u0302k\u22252 ,\nwhere the last inequality follows from \u03c4 it \u2264 T . Since E[\u03be\u0302k] = 0, and E[\u03be\u0302\u22a4k \u03be\u0302\u2113] = 0 for all k, \u2113, we can apply Lemma C.4 to bound the second term in the right-hand side as follows:\nE\u2225 t\u22121\u2211\nk=t\u2212\u03c4 it\n\u03be\u0302k\u22252 = t\u22121\u2211\nk=t\u2212\u03c4 it\nE\u2225\u03be\u0302k\u22252 \u2264 t\u22121\u2211\nk=t\u2212T\nE\u2225\u03be\u0302k\u22252 \u2264 2 t\u22121\u2211\nk=t\u2212T\nE\u2225\u2207\u0302k\u22252 + 2 t\u22121\u2211\nk=t\u2212T\nE\u2225g\u0302k\u22252 ,\nwhere we used \u03c4 it \u2264 T , and \u2225a\u2212 b\u22252 \u2264 2\u2225a\u22252 + 2\u2225b\u22252. Plugging-in both bounds to Eq. (24), we obtain:\nE\u2225w\u0302it \u2212 wt\u22252 \u2264 (2\u03c92\u03b72T + 4\u03c92\u03b72) t\u22121\u2211\nk=t\u2212T\nE\u2225\u2207\u0302k\u22252 + 4\u03c92\u03b72 t\u22121\u2211\nk=t\u2212T\nE\u2225g\u0302k\u22252\n\u2264 6\u03c92\u03b72T t\u22121\u2211\nk=t\u2212T\nE\u2225\u2207\u0302k\u22252 + 4\u03c92\u03b72 t\u22121\u2211\nk=t\u2212T\nE\u2225g\u0302k\u22252 . (25)\nNote that we can bound E\u2225\u2207\u0302k\u22252 as:\nE\u2225\u2207\u0302k\u22252 \u2264 2E\u2225\u2207\u0302k \u2212\u2207k\u22252 + 2E\u2225\u2207k\u22252 \u2264 2\u03b22\nN N\u2211 i=1 E\u2225w\u0302ik \u2212 wk\u22252 + 2E\u2225\u2207k\u22252 ,\nwhere in the last inequality we used Eq. (13) to bound E\u2225\u2207\u0302k \u2212\u2207k\u22252. For the ease of notation, denote: eit := E\u2225w\u0302it \u2212 wt\u22252, and et := 1N \u2211N i=1 e i t. Therefore, we obtain from Eq. (25) that\neit \u2264 12\u03b22\u03c92\u03b72T t\u22121\u2211\nk=t\u2212T\nek + 12\u03c9 2\u03b72T t\u22121\u2211 k=t\u2212T E\u2225\u2207k\u22252 + 4\u03c92\u03b72 t\u22121\u2211 k=t\u2212T E\u2225g\u0302k\u22252 . (26)\nBase Case: For t = 0, each client obtains the exact model weights, i.e., w\u0302i0 = w0, which trivially implies result. For every t = 1, . . . , T and i \u2208 [N ], we have from Eq. (26) that\neit \u2264 12\u03b22\u03c92\u03b72T t\u22121\u2211 k=0 ek + 12\u03c9 2\u03b72T t\u22121\u2211 k=0 E\u2225\u2207k\u22252 + 4\u03c92\u03b72 t\u22121\u2211 k=0 E\u2225g\u0302k\u22252 . (27)\nUsing Lemma C.1 to bound E\u2225g\u0302k\u22252, we get:\neit \u2264 12\u03b22\u03c92\u03b72T t\u22121\u2211 k=0 ek + 12\u03c9 2\u03b72T t\u22121\u2211 k=0 E\u2225\u2207k\u22252 + 4\u03c92\u03b72 t\u22121\u2211 k=0 ( \u03c3\u03032S + 4\u03b3E\u2225\u2207k\u22252 + 2\u03b22ek ) \u2264 4\u03c92\u03b72T \u03c3\u03032S + ( 12\u03c92\u03b72T + 16\u03b3\u03c92\u03b72\n) t\u22121\u2211 k=0 E\u2225\u2207k\u22252 + ( 12\u03b22\u03c92\u03b72T + 8\u03b22\u03c92\u03b72 ) t\u22121\u2211 k=0 ek\n\u2264 4\u03c92\u03b72T \u03c3\u03032S + 28\u03b3\u03c92\u03b72T t\u22121\u2211 k=0 E\u2225\u2207k\u22252 + 20\u03b22\u03c92\u03b72T t\u22121\u2211 k=0 ek .\nNote that this bound on eit is independent of i, and thus, it holds for the average of e i t over i \u2208 [N ], namely, et. Therefore, Eq. (26) implies a recursive bound on et; for every t = 1, . . . , T :\net \u2264 \u03b1\u03c3\u03032S + \u03bd t\u22121\u2211 k=0 E\u2225\u2207k\u22252 + \u03c1 t\u22121\u2211 k=0 ek , (28)\nwhere we denoted \u03bd := 28\u03b3\u03c92\u03b72T . Plugging-in this bound instead of ek in the right-hand side, we obtain:\net \u2264 \u03b1\u03c3\u03032S + \u03bd t\u22121\u2211 k=0 E\u2225\u2207k\u22252 + \u03c1 t\u22121\u2211 k=0 ( \u03b1\u03c3\u03032S + \u03bd k\u22121\u2211 \u2113=0 E\u2225\u2207\u2113\u22252 + \u03c1 k\u22121\u2211 \u2113=0 e\u2113 )\n\u2264 \u03b1 (1 + \u03c1T ) \u03c3\u03032S + \u03bd t\u22121\u2211 k=0 E\u2225\u2207k\u22252 + \u03bd\u03c1 t\u22121\u2211 k=0 k\u22121\u2211 \u2113=0 E\u2225\u2207\u2113\u22252 + \u03c12 t\u22121\u2211 k=0 k\u22121\u2211 \u2113=0 e\u2113 ,\nwhere we used t \u2264 T . Note that we can bound the double sums in right-hand side using Lemma C.6 as\nt\u22121\u2211 k=0 k\u22121\u2211 \u2113=0 E\u2225\u2207\u2113\u22252 \u2264 t t\u22122\u2211 k=0 E\u2225\u2207k\u22252 \u2264 T t\u22122\u2211 k=0 E\u2225\u2207k\u22252 ,\nand similarly, t\u22121\u2211 k=0 k\u22121\u2211 \u2113=0 e\u2113 \u2264 T t\u22122\u2211 k=0 ek .\nPlugging-back, we get:\net \u2264 \u03b1 (1 + \u03c1T ) \u03c3\u03032S + \u03bd t\u22121\u2211 k=0 E\u2225\u2207k\u22252 + \u03bd\u03c1T t\u22122\u2211 k=0 E\u2225\u2207\u2113\u22252 + \u03c12T t\u22122\u2211 k=0 ek .\nWe can once again apply Eq. (28) to bound ek, and obtain:\net \u2264\u03b1 (1 + \u03c1T ) \u03c3\u03032S + \u03bd t\u22121\u2211 k=0 E\u2225\u2207k\u22252 + \u03bd\u03c1T t\u22122\u2211 k=0 E\u2225\u2207\u2113\u22252 + \u03c12T t\u22122\u2211 k=0 ( \u03b1\u03c3\u03032S + \u03bd k\u22121\u2211 \u2113=0 E\u2225\u2207\u2113\u22252 + \u03c1 k\u22121\u2211 \u2113=0 e\u2113 )\n\u2264\u03b1 ( 1 + \u03c1T + \u03c12T 2 ) \u03c3\u03032S + \u03bd t\u22121\u2211 k=0 E\u2225\u2207k\u22252 + \u03bd\u03c1T t\u22122\u2211 k=0 E\u2225\u2207\u2113\u22252 + \u03bd\u03c12T t\u22122\u2211 k=0 k\u22121\u2211 \u2113=0 E\u2225\u2207\u2113\u22252 + \u03c13T t\u22122\u2211 k=0 k\u22121\u2211 \u2113=0 e\u2113 \u2264\u03b1 ( 1+\u03c1T +\u03c12T 2 ) \u03c3\u03032S+\u03bd\nt\u22121\u2211 k=0 E\u2225\u2207k\u22252+\u03bd\u03c1T t\u22122\u2211 k=0 E\u2225\u2207\u2113\u22252+\u03bd\u03c12T 2 t\u22123\u2211 k=0 E\u2225\u2207\u2113\u22252 + \u03c13T 2 t\u22123\u2211 k=0 ek ,\nwhere in the last inequality we used Lemma C.6. Repeating this process of alternately applying Eq. (28) to bound ek and Lemma C.6, finally gives:\net \u2264 \u03b1 ( 1 +\nt\u2211 k=1 (\u03c1T )k ) \u03c3\u03032S + \u03bd \u03c1T t\u2211 k=1 (\u03c1T )k t\u2212k\u2211 \u2113=0 E\u2225\u2207\u2113\u22252 .\nPlugging \u03bd and \u03c1, we can bound the coefficient \u03bd/\u03c1T as:\n\u03bd \u03c1T = 28\u03b3\u03c92\u03b72T 20\u03b22\u03c92\u03b72T 2 \u2264 2\u03b3 \u03b22T .\nUsing (\u03c1T )k \u2264 k(\u03c1T )k, which holds for any k \u2265 1, we then obtain:\net \u2264 \u03b1 ( 1 +\nt\u2211 k=1 k(\u03c1T )k ) \u03c3\u03032S + 2\u03b3 \u03b22T t\u2211 k=1 (\u03c1T )k t\u2212k\u2211 \u2113=0 E\u2225\u2207\u2113\u22252 .\nNote that for all t \u2264 T and k \u2265 1, we have t\u2212 kT \u2264 0. Therefore, since for \u2207\u2212\u2113 = 0 for all \u2113 \u2208 N, we can equivalently write:\net \u2264 \u03b1 ( 1 +\nt\u2211 k=1 k(\u03c1T )k ) \u03c3\u03032S + 2\u03b3 \u03b22T t\u2211 k=1 (\u03c1T )k t\u2212k\u2211 \u2113=t\u2212kT E\u2225\u2207\u2113\u22252 ,\nwhich establishes the result for the base case.\nInduction step: The induction hypothesis is that the following holds:\nes \u2264 \u03b1 ( 1 +\ns\u2211 k=1 k(\u03c1T )k ) \u03c3\u03032S + 2\u03b3 \u03b22T s\u2211 k=1 (\u03c1T )k s\u2212k\u2211 \u2113=s\u2212kT E\u2225\u2207\u2113\u22252, \u2200s = t\u2212 T , . . . , t\u2212 1 . (29)\nWe focus on Eq. (26). Using Lemma C.1 to bound E\u2225g\u0302k\u22252 and following similar steps to those used to derive Eq. (28), we get:\neit \u2264 12\u03b22\u03c92\u03b72T t\u22121\u2211\nk=t\u2212T\nek + 12\u03c9 2\u03b72T t\u22121\u2211 k=t\u2212T E\u2225\u2207k\u22252 + 4\u03c92\u03b72 t\u22121\u2211 k=t\u2212T ( \u03c3\u03032S + 4\u03b3E\u2225\u2207k\u22252 + 2\u03b22ek ) \u2264 \u03b1\u03c3\u03032S + \u03bd\nt\u22121\u2211 k=t\u2212T E\u2225\u2207k\u22252 + \u03c1 t\u22121\u2211 k=t\u2212T\nek\ufe38 \ufe37\ufe37 \ufe38 =(\u2020) . (30)\nFrom the induction hypothesis (29), we can bound ek for every k \u2208 [t\u2212 T , t\u2212 1] as follows: ek \u2264 \u03b1 ( 1 +\nk\u2211 \u2113=1 \u2113(\u03c1T )\u2113 ) \u03c3\u03032S + 2\u03b3 \u03b22T k\u2211 \u2113=1 (\u03c1T )\u2113 k\u2212\u2113\u2211 m=k\u2212\u2113T E\u2225\u2207m\u22252 .\nDenote this bound by B(k) := \u03b1 ( 1 + \u2211k \u2113=1 \u2113(\u03c1T )\u2113 ) \u03c3\u03032S + 2\u03b3 \u03b22T \u2211k \u2113=1 (\u03c1T )\u2113 \u2211k\u2212\u2113 m=k\u2212\u2113T E\u2225\u2207m\u22252; that is, ek \u2264 B(k). We can therefore bound (\u2020) as t\u22121\u2211\nk=t\u2212T\nek \u2264 t\u22121\u2211\nk=t\u2212T\nB(k) \u2264 T B(t\u2212 1) ,\nwhere the last inequality holds because B(k) is monotonically increasing. Plugging back to Eq. (30) and substituting B(t\u2212 1) gives\neit \u2264\u03b1\u03c3\u03032S + \u03bd t\u22121\u2211\nk=t\u2212T\nE\u2225\u2207k\u22252 + \u03c1 \u00b7 T B(t\u2212 1)\n=\u03b1\u03c3\u03032S+\u03bd t\u22121\u2211 k=t\u2212T\nE\u2225\u2207k\u22252+\u03c1T ( \u03b1 ( 1 +\nt\u22121\u2211 k=1 k(\u03c1T )k ) \u03c3\u03032S + 2\u03b3 \u03b22T t\u22121\u2211 k=1 (\u03c1T )k t\u22121\u2212k\u2211 \u2113=t\u22121\u2212kT E\u2225\u2207\u2113\u22252 )\n=\u03b1 ( 1+\u03c1T +\nt\u22121\u2211 k=1\nk(\u03c1T )k+1 )\n\ufe38 \ufe37\ufe37 \ufe38 =(A)\n\u03c3\u03032S + \u03bd t\u22121\u2211 k=t\u2212T E\u2225\u2207k\u22252 + 2\u03b3 \u03b22T t\u22121\u2211 k=1 (\u03c1T )k+1 t\u22121\u2212k\u2211 \u2113=t\u22121\u2212kT\nE\u2225\u2207\u2113\u22252\ufe38 \ufe37\ufe37 \ufe38 =(B) . (31)\nBounding (A): Using simple algebra, we have that\n\u03c1T + t\u22121\u2211 k=1 k(\u03c1T )k+1 = \u03c1T + t\u2211 k=2 (k \u2212 1)(\u03c1T )k \u2264 \u03c1T + t\u2211 k=2 k(\u03c1T )k = t\u2211 k=1 k(\u03c1T )k . (32)\nThis implies that (A) is bounded by \u03b1 ( 1 + \u2211t k=1 k(\u03c1T )k ) .\nBounding (B): Focusing on the first term in (B), we can bound:\n\u03bd t\u22121\u2211 k=t\u2212T E\u2225\u2207k\u22252 = \u03bd \u03c1T \u00b7 \u03c1T t\u22121\u2211 k=t\u2212T E\u2225\u2207k\u22252 \u2264 2\u03b3 \u03b22T \u00b7 \u03c1T t\u22121\u2211 k=t\u2212T E\u2225\u2207k\u22252 . (33)\nFocusing on the second sum in (B), we can bound\n2\u03b3 \u03b22T t\u22121\u2211 k=1 (\u03c1T )k+1 t\u22121\u2212k\u2211 \u2113=t\u22121\u2212kT E\u2225\u2207\u2113\u22252 = 2\u03b3 \u03b22T t\u2211 k=2 (\u03c1T )k t\u22121\u2212(k\u22121)\u2211 \u2113=t\u22121\u2212(k\u22121)T E\u2225\u2207\u2113\u22252\n= 2\u03b3 \u03b22T t\u2211\nk=2\n(\u03c1T )k t\u2212k\u2211\n\u2113=t\u2212kT +T \u22121\nE\u2225\u2207\u2113\u22252\n\u2264 2\u03b3 \u03b22T t\u2211 k=2 (\u03c1T )k t\u2212k\u2211 \u2113=t\u2212kT E\u2225\u2207\u2113\u22252 , (34)\nwhere the last inequality holds since T \u2212 1 \u2265 0 and E\u2225\u2207\u2113\u22252 \u2265 0 for all \u2113. Combining the bounds in Eq. (33) and (34), we can then bound (B) as\n\u03bd t\u22121\u2211 k=t\u2212T E\u2225\u2207k\u22252+ 2\u03b3 \u03b22T t\u22121\u2211 k=1 (\u03c1T )k+1 t\u22121\u2212k\u2211 \u2113=t\u22121\u2212kT E\u2225\u2207\u2113\u22252 \u2264 2\u03b3 \u03b22T \u00b7 \u03c1T t\u22121\u2211 k=t\u2212T E\u2225\u2207k\u22252 + 2\u03b3 \u03b22T t\u2211 k=2 (\u03c1T )k t\u2212k\u2211 \u2113=t\u2212kT E\u2225\u2207\u2113\u22252\n= 2\u03b3 \u03b22T t\u2211\nk=1\n(\u03c1T )k t\u2212k\u2211\n\u2113=t\u2212kT\nE\u2225\u2207\u2113\u22252 . (35)\nPlugging back to Eq. (31) the bounds on (A) and (B) (from Eq. (32) and (35), respectively), we get: eit \u2264 \u03b1 ( 1 + t\u2211 k=1 k(\u03c1T )k ) \u03c3\u03032S + 2\u03b3 \u03b22T t\u2211 k=1 (\u03c1T )k t\u2212k\u2211 \u2113=t\u2212kT E\u2225\u2207\u2113\u22252 .\nSince this bound is independent of i, it also holds for the average et = 1N \u2211N i=1 e i t, establishing the result.\nIn the following two lemmas, we characterize the second moment of the sum of independent random variables. Lemma C.3 (Lemma 4, Karimireddy et al., 2020). Let X1, . . . , XN \u2208 Rd be N independent random variables. Suppose that E[Xi] = \u00b5i and E\u2225Xi \u2212 \u00b5i\u22252 \u2264 \u03c32i . Then, the following holds\nE \u2225\u2225\u2225\u2225\u2225 N\u2211 i=1 Xi \u2225\u2225\u2225\u2225\u2225 2 \u2264 2 \u2225\u2225\u2225\u2225\u2225 N\u2211 i=1 \u00b5i \u2225\u2225\u2225\u2225\u2225 2 + 2 N\u2211 i=1 \u03c32i .\nLemma C.4. Let X1, . . . , XN \u2208 Rd be N orthogonal, zero mean random variables, i.e., E[Xi] = 0 for all i \u2208 [N ], and E[X\u22a4i Xj ] = 0 for all i \u0338= j. Then, the following holds:\nE \u2225\u2225\u2225\u2225\u2225 N\u2211 i=1 Xi \u2225\u2225\u2225\u2225\u2225 2 = N\u2211 i=1 E\u2225Xi\u22252 .\nProof. By the linearity of expectation, and the following property: E[X\u22a4i Xj ] = 0, \u2200i \u0338= j, we immediately get that E\u2225\u2211Ni=1 Xi\u22252 = E [\u2211Ni=1\u2211Nj=1 X\u22a4i Xj] =\u2211Ni=1 E\u2225Xi\u22252. Next, we state a simple result about the squared norm of the sum of vectors. Lemma C.5. For any u1, . . . , uN \u2208 Rd, it holds that \u2225 \u2211N i=1 ui\u22252 \u2264 N \u2211N i=1 \u2225ui\u22252. Proof. By the convexity of \u2225\u00b7\u22252 and Jensen\u2019s inequality: \u2225 1N \u2211N i=1 ui\u22252\u2264 1N \u2211N i=1 \u2225ui\u22252, which implies the result.\nThe next result is a simple bound on a double sum of non-negative numbers. Lemma C.6. Let t, \u03c4 \u2208 N such that t \u2265 \u03c4+1. For any sequence of non-negative numbers x0, x1, . . . , xt\u2212\u03c4\u22121, the following holds\nt\u2212\u03c4\u2211 k=0 k\u22121\u2211 \u2113=0 x\u2113 \u2264 t \u00b7 t\u2212\u03c4\u22121\u2211 k=0 xk .\nProof. Immediately: \u2211t\u2212\u03c4\nk=0 \u2211k\u22121 \u2113=0 x\u2113 = \u2211t\u2212\u03c4\u22121 k=0 (t\u2212 \u03c4 \u2212 k)xk \u2264 t \u00b7 \u2211t\u2212\u03c4\u22121 k=0 xk.\nThe following lemma gives a bound on the derivative of a power series. Lemma C.7. Let a < 1. Then,\n\u221e\u2211 k=1 kak = a (1\u2212 a)2 .\nProof. Let fk(a) = ak. \u221e\u2211 k=1 kak = a \u221e\u2211 k=1 kak\u22121 = a \u221e\u2211 k=1 f \u2032k(a) .\nUsing term-by-term differentiation (Stewart, 2015), we have that\n\u221e\u2211 k=1 f \u2032k(a) = ( \u221e\u2211 k=1 fk(a) )\u2032 = ( \u221e\u2211 k=1 ak )\u2032 = ( a 1\u2212 a )\u2032 =\n1\n(1\u2212 a)2 .\nMultiplying by a gives the result.\nNext, we state a non-trivial inequality to bound the double sum that appears on the right-hand side of Eq. (17).\nLemma C.8. Let a \u2208 (0, 1) and T, T \u2208 N. Moreover, let x0, . . . , xT\u22121 be a sequence of non-negative numbers. Then, T\u2211\nt=1 t\u22121\u2211 k=0 a\u2308 t\u2212k T \u2309xk \u2264 T a 1\u2212 a T\u22121\u2211 k=0 xk .\nProof. We start with changing the order of summation in the left-hand side. Note that for any fixed k, the element xk appears in the inner sum if k \u2264 t\u2212 1, or equivalently, t \u2265 k + 1. Therefore,\nT\u2211 t=1 t\u22121\u2211 k=0 a\u2308 t\u2212k T \u2309xk = T\u22121\u2211 k=0\n( T\u2211\nt=k+1\na\u2308 t\u2212k T \u2309 ) xk =\nT\u22121\u2211 k=0 ( T\u2212k\u2211 t=1 a\u2308 t T \u2309 ) xk . (36)\nFocusing on the inner sum in the right-hand side, \u2211T\u2212k\nt=1 a \u2308t/T \u2309, we can divide the interval of integers from 1 to T \u2212 k into\nnon-overlapping intervals of length T (and possibly a small residual) and get that\nT\u2212k\u2211 t=1 a\u2308 t T \u2309 \u2264 \u2308T\u2212kT \u2309\u2211 m=1 T\u2211 \u2113=1 a\u2308 (m\u22121)T +\u2113 T \u2309 (\u2020) = \u2308T\u2212kT \u2309\u2211 m=1 T\u2211 \u2113=1 am = T \u2308T\u2212kT \u2309\u2211 m=1 am \u2264 T a 1\u2212 a .\nwhere (\u2020) holds because for every \u2113 = 1, . . . , T we have \u2308 (m\u22121)T +\u2113T \u2309 = m, and the last inequality follows from\u2211\u2308T\u2212kT \u2309 m=1 a m \u2264\u2211\u221em=1 am = a1\u2212a as a < 1. Plugging back to Eq. (36) concludes the proof. The next lemma establishes that for small enough \u03b7, we have \u2212\u03b7/2 +O(\u03b72) \u2264 \u2212\u03b7/4. Lemma C.9. Let \u03b3, \u03b8 \u2265 1. For every \u03b7 \u2264 130\u03b3\u03b2\u03b8 , it holds that\n\u2212\u03b7 2 + 2\u03b3\u03b2\u03b72 + 160\u03b3\u03b22\u03b82\u03b73 \u2264 \u2212\u03b7 4 .\nProof. We equivalently prove that 2\u03b3\u03b2\u03b72 + 160\u03b3\u03b22\u03b82\u03b73 \u2264 \u03b7\n4 .\nSince both \u03b3 \u2265 1 and \u03b8 \u2265 1, we have\n2\u03b3\u03b2\u03b72 + 160\u03b3\u03b22\u03b82\u03b73 \u2264 2\u03b3\u03b2\u03b8\u03b72 + 160\u03b32\u03b22\u03b82\u03b73\n= \u03b7\n4\n( 8\u03b3\u03b2\u03b8\u03b7 + 640\u03b32\u03b22\u03b82\u03b72 ) \u2264 \u03b7\n4\n( 8\u03b3\u03b2\u03b8\n30\u03b3\u03b2\u03b8 +\n640\u03b32\u03b22\u03b82\n900\u03b32\u03b22\u03b82\n) = \u03b7\n4 \u00b7 44 45 \u2264 \u03b7 4 ,\nwhere the second inequality follows from the upper bound on \u03b7.\nWe also make use of the following result, which we prove using simple algebra.\nLemma C.10. Suppose \u03b7 = min {\u03b71, \u03b72, \u03b73} for some \u03b71, \u03b72, \u03b73 > 0, and let A,B,C > 0. Then, the following holds:\nA \u03b7 +B\u03b7 + C\u03b72 \u2264 A\n( 1\n\u03b71 +\n1\n\u03b72 +\n1\n\u03b73\n) +B\u03b72 + C\u03b7 2 3 .\nProof. Since \u03b7 is the minimum of three terms, 1/\u03b7 is the maximum of their inverses. Thus, we can bound 1/\u03b7 by the sum of the inverses as follows:\nA \u03b7 = Amax\n{ 1\n\u03b71 , 1 \u03b72 , 1 \u03b73\n} \u2264 A ( 1\n\u03b71 +\n1\n\u03b72 +\n1\n\u03b73\n) .\nThe terms B\u03b7 and C\u03b72 are monotonically increasing with \u03b7. We can therefore bound \u03b7 by \u03b72 and \u03b72 by \u03b723 ."
        },
        {
            "heading": "D. Entropy-Constrained Uniform Quantization",
            "text": "In this section, we describe a new compression technique entitled Entropy-Constrained Uniform Quantization (ECUQ), which we developed for anchor compression, although it can be of independent interest. ECUQ is described in Algorithm 4. Let x = (x(1), . . . , x(d)) \u2208 Rd be some input vector we wish to compress using ECUQ. Denote: xmin := mini x(i), xmax := maxi x(i). Given some bandwidth budget of b bits/coordinate, ECUQ initially divided the interval [xmin, xmax] into K = 2b non-overlapping bins of equal size \u2206 = (xmax \u2212 xmin)/K. Then, it sets the quantization values, which we denote by Q, to be the centers of these bins. Afterwards, the vector x is quantized into elements of Q, that is, each element x(i) is assigned to its closest quantization value q \u2208 Q to generate the quantized vector x\u0302Q, whose elements are all in Q. We then compute the empirical distribution of the quantized vector by counting for every q \u2208 Q the number of times it appears in x\u0302Q, and the entropy of the resulting distribution. Note that the entropy is upper bounded by logK = b. Finally, for some small tolerance parameter \u03f5 (we use \u03f5 = 0.1), we check whether the entropy is within \u03f5 distance from the budget b: if it is not the case, then we perform a double binary search, repeating the above procedure with increased number of quantization values K, to find the maximal number of uniformly spaced quantization values such that the entropy of the empirical distribution of the resulting quantized vector is within \u03f5 distance from b. Only after this entropy condition is satisfied, we encode x\u0302Q using some entropy encoding (we use Huffman coding).\nAlgorithm 4 Entropy-Constrained Uniform Quantization (ECUQ)\nInput: Vector x \u2208 Rd, bandwidth budget b (bits/coordinate), tolerance \u03f5. xmax \u2190 maxi x(i), xmin \u2190 mini x(i) \u25b7 Get max/min values of input vector K \u2190 2b, \u2206\u2190 (xmax \u2212 xmin)/K \u25b7 Initialize # of quantization values and bin length Q \u2190 { xmin + ( k + 12 ) \u00b7\u2206 : k = 0, . . . ,K \u2212 1 } \u25b7 Set uniformly spaced quantization values x\u0302Q \u2190 Quantize(x,Q) \u25b7 x\u0302Q(i) = argminq\u2208Q \u2225x(i)\u2212 q\u2225 pQ \u2190 Empirical Density(x\u0302Q) \u25b7 pQ(q) = 1N \u2211 i\u2208[N ] 1 {x\u0302Q(i) = q}, \u2200q \u2208 Q\nH(pQ)\u2190 Entropy(pQ) \u25b7H(pQ) = \u2212 \u2211\nq\u2208Q pQ(q) log pQ(q) ifH(pQ) < b\u2212 \u03f5 then\nx\u0302Q \u2190 DOUBLE BINARY SEARCH NUM QUANTIZATION LEVELS(x, b) end if x\u0302e \u2190 Huffman Coding(x\u0302Q) \u25b7 Entropy encoding of the quantized vector Return: x\u0302e Procedure DOUBLE BINARY SEARCH NUM QUANTIZATION LEVELS(x, b)\nInitialize: low\u2190 2b, high\u2190\u221e, p\u2190 \u22121 while low \u2264 high do\nif high ==\u221e then p\u2190 p+ 1 mid\u2190 2b + 2p \u25b7 Increase # of levels exponentially else mid\u2190 (low + high)/2 end if K \u2190 mid, \u2206\u2190 (xmax \u2212 xmin)/mid Q \u2190 { xmin + ( k + 12 ) \u00b7\u2206 : k = 0, . . . ,K \u2212 1\n} x\u0302Q \u2190 Quantize(x,Q) pQ \u2190 Empirical Density(x\u0302Q) H(pQ)\u2190 Entropy(pQ) ifH(pQ) > b then\nhigh\u2190 mid\u2212 1 else ifH(pQ) < b\u2212 \u03f5 then\nlow\u2190 mid + 1 else\nreturn x\u0302Q end if\nFigure 5 illustrates ECUQ\u2019s encoder, as described in the text above. The corresponding decoder is fairly simple as it only performs entropy decoding in linear time (Huffman decoding).\nWhile devising ECUQ, we also considered an additional method to approximate ECQ. It is similar to ECUQ, but instead of using uniformly spaced quantization values, it uses K-Means clustering to find the quantization values that minimize the overall squared error. We used a double binary search to find the largest number of levels K such that, after entropy encoding, the bandwidth constraint is satisfied. We termed this method Entropy-Constrained K-Means (ECK-Means).\nWe compare the performance of ECUQ with ECQ and ECKMeans in terms of their NMSE, and we also measure their encoding time. As we mentioned in the Section 4, ECQ is sensitive to hyperparameters; thus, we implemented it using a grid search over its hyperparameters to guarantee nearoptimal performance.6 We evaluate the three methods on vectors drawn from three different synthetic distributions: (1) LogNormal(0, 1); (2) Normal(0, 1); and (3) Normal(1, 0.1). In Figure 6 (top) we show the NMSE and encoding time for different sizes of input vectors when the budget constraint is b = 2 bits/coordinate. As a complementary result, in Figure 6 (bottom) we fix the dimension of the input vectors to d = 212 = 4096 and vary the bandwidth budget constraint from 2 to 5 bits/coordinate. The results imply that ECUQ exhibits a good speed-accuracy trade-off: it consistently outperforms ECK-Means while being an order of magnitude faster, and it is competitive with ECQ but about three orders of magnitude faster. Note additionally that it takes\u2248 20 minutes for ECQ to encode even a small vectors of size 212 with budget constraint of 4 bits/coordinate; this means that ECQ without some acceleration is not suitable for compressing neural networks with millions and even billions of parameters.\n6While such implementation may increase the encoding time, we are not aware of any other approach to guarantee an optimal performance. ECQ aims at solving a hard non-convex problem, and different hyperparameters may result in different local minima."
        },
        {
            "heading": "E. Additional ECUQ Evaluations",
            "text": "Since ECUQ is a quantization-based method, in Section 4 and Appendix D we compare it with quantization-based techniques. In Figure 7, we give a complementary result comparing it with sparsification methods (Rand-K, Top-K) and sketching (CountSketch), where similar trends are observed. Note, however, that such techniques are mostly orthogonal to quantization-based methods and they can be used in conjunction."
        },
        {
            "heading": "F. Experimental Details",
            "text": "We implemented DoCoFL in PyTorch (Paszke et al., 2019). In all experiments, the PS uses Momentum SGD as optimizer with a momentum of 0.9 and L2 regularization (i.e., weight decay) with parameter 10\u22125. The clients, on the other hand, use vanilla SGD for all tasks but Amazon Reviews, for which Adam provided better results. In Table 4 we report the hyperparameters used in our experiments. To ease the computational burden and long training times, in the Shakespeare task we reduced the amount of train and validation data for each speaker (i.e., client) by a factor of 10 by using only the first 10% of train and validation data, but no less than 2 samples per speaker."
        },
        {
            "heading": "G. Additional Results",
            "text": "In this section we present additional results that were deferred from the main text.\nG.1. Learning Curves\nWe next provide the learning curves for the experiments we conducted in \u00a7 5. In Figures 8 and 9 we show the validation and train accuracy throughout training, respectively. We measure train and validation accuracy every 50 rounds for EMNIST and Amazon Reviews, every 500 rounds for CIFAR-100, and every 1000 rounds for Shakespeare.\nFollowing the discussion in \u00a7 5, Figure 8 demonstrates that using less bandwidth may improve the generalization ability as it can serve as a form of regularization. For example, consider the EMNIST task, where DoCoFL(2, 2, 3) (i.e., 2 bits per coordinate for anchor and correction compression, and 3 bits per coordinate for uplink compression) outperforms both FedAvg and DoCoFL(4, 4, 3). Unsurprisingly, examining Figure 9 reveals a reverse image \u2013 less bandwidth implies lower train accuracy. This suggests that in some settings using less bandwidth (but not too little) may help to prevent overfitting.\nG.2. Bandwidth Budget Ablation\nNext, we provide numerical results that demonstrate the effect of the downlink (anchor and correction) bandwidth budget on DoCoFL\u2019s performance. We consider the CIFAR-100 with ResNet-9 experiment with anchor deployment rate K = 10 and anchor queue capacity V = 3.\nIn Figure 10 we show the train and validation accuracy for different anchor bandwidth budgets bw, namely, 32 (full-precision), 3, 2 and 1.5 bits per coordinate, while the correction budget is fixed and equals bc = 2 bits per coordinate, as a function of both number of rounds and number of communicated bits in the downlink direction. The results indicate that one can significantly reduce the bandwidth used for communicating the anchors and use as low as bw = 2 bits per coordinate for anchor compression (16\u00d7 reduction), without degrading validation accuracy. Again, similarly to evidence from the previous section, less bandwidth typically results in lower train accuracy.\nIn Figure 11 we show the train and validation accuracy for different correction bandwidth budgets bc (32, 4, 2, 1 and 0.5 bits per coordinate), while the anchor budget is fixed and equals bw = 2 bits per coordinate. We observe similar trends, where less bandwidth leads to lower train accuracy but possibly higher validation accuracy. Additionally, we see that one may even use a sub-bit compression ratio for the correction term, allowing for significant online bandwidth reduction, which is especially important in our context.\nG.3. The Value of the Correction term\nIn this section, we discuss the effect of ignoring the correction term on DoCoFL\u2019s performance, namely, we consider the case where clients only obtain an anchor (i.e., a previous model) and use it perform local optimization. As mentioned in \u00a75, ignoring the correction may resemble other frameworks such as delayed gradients. Delayed SGD (DSGD, Arjevani et al. (2020)) is well-studied in the literature both theoretically and empirically. Indeed, theory supports that optimization with delay can work, e.g., Stich & Karimireddy (2019) showed that as long as the maximal delay is bounded by O( \u221a T ), DSGD enjoys the same asymptotic convergence rate as SGD; Cohen et al. (2021) later improved the dependence on the maximal delay to average delay with a variant of DSGD, allowing for arbitrary delays. However, it has also been observed that, in practice, introducing delay can slow down and even destabilize convergence, and as a result hyperparameters should be chosen with great care to ensure stability (Giladi et al., 2020). We thus convey that sending the correction is crucial and allows for improved performance.\nTo reinforce this, we conducted an experiment to numerically evaluate the effect of ignoring the correction. We consider the CIFAR-100 with ResNet-9 experiment. We test DoCoFL with and without sending the correction to the clients for three different configurations: (1) V = 3 and no anchor compression (i.e., 32 bits per coordinate); (2) V = 3 and Cw is ECUQ with 2 bits per coordinate; and (3) V = 1 with Cw as in the second configuration. For all configurations, we use an anchor deployment rate of K = 10. In Figure 12 we present the train and validation accuracy, and also the average client squared model estimation error, i.e., 1S \u2211 i\u2208St \u2225wt \u2212 w\u0302it\u22252, for the first configuration. The results clearly indicate that accounting for the correction term results in faster convergence. While ignoring the correction may eventually still result in similar performance, it is expected to take significantly more communication rounds; this is evident even when the anchor is sent with full precision. Examining the rightmost plot, we observe that ignoring the correction leads to larger model estimation error, which provides insight into why the performance deteriorates when the correction is ignored.\nG.4. DoCoFL and EF21\nWhile our focus in on setups where a client may participate in training only once or a few times, in some setups, partial but repeated participation can be expected. For such setups, we consider some additional related work. Specifically, we focus on EF21 (Richta\u0301rik et al., 2021) and some of its extensions. To assess the value of DoCoFL in this context, we attempted to extend EF21-BC (Algorithm 5 of Fatkhullin et al. (2021)) to the partial participation setting, but were not able to achieve convergence. We suspect that it is attributed to an accumulated discrepancy between the models of the clients and the server, and thus a more sophisticated extension is required, which is out of scope. Instead, we extended EF21-PP (Algorithm 4 of Fatkhullin et al. (2021)) to support downlink compression in two different ways: (1) direct compression of the model parameters using EDEN; and (2) using DoCoFL. We compare these approaches with a baseline that sends the exact model to the clients (i.e., no downlink compression).\nWe consider two tasks: (1) EMNIST + LeNet with N = 200 clients and S = 10 participating clients per-round; and (2) CIFAR-100 + ResNet-9 with N = 25 clients and S = 5 participating clients per-round. We used less clients here compared to the experiments in the main text due to GPU memory limitations (EF21 requires keeping all N clients persistent). In both experiments we use EDEN with 1 bit/coordinate for uplink compression. Figures 13 and 14 depict the train and validation accuracy as a function of the number of communication rounds, the total number of communicated bits in the downlink direction, and the number of communicated bits in the downlink direction required online (i.e., at the clients\u2019 participation round) for EMNIST and CIFAR, respectively. We note that using a direct compression of the model with 1 or 2 bits per coordinate results in a notable drop in validation accuracy compared to the baseline. Indeed, using EDEN with 3 bits per coordinate performs similarly to the baseline. Examining DoCoFL with 2 and 1 bits/coordinate for anchor and correction, respectively, reveals that it performs similarly to direct downlink compression with 3 bits/coordinate, i.e., when using the same overall downlink bandwidth; however, it requires 3\u00d7 less online bandwidth. This is especially important in our context since online bandwidth demand directly translates to client delays; indeed, this is a main design goal of DoCoFL. Additionally, one may improve the results even further by increasing the anchor budget to 3 bits/coordinate, while keeping the online bandwidth usage the same or even lower (e.g., see Figure 11).\nAnother important point of comparison is the EF21-P + DIANA method (Gruntkowska et al., 2022), which supports bi-directional compression. In particular, their server compression mechanism is similar to ours in the following sense: their server and clients hold control variates that track the global model; these control variates can be seen as an anchor that is being updated in each round and the server sends to the clients a compressed correction with respect to the control variates. However, their approach requires client-side memory with full participation (i.e., updated control variates). The authors propose to study an extension of their framework to partial participation as future work. It is interesting to investigate whether DoCoFL can be used in conjunction with this framework to achieve this."
        },
        {
            "heading": "H. Future Work",
            "text": "We point out several directions for future research: (1) an interesting avenue would be to investigate how to combine DoCoFL with the delayed gradients framework (Stich & Karimireddy, 2019). While delayed gradients do not reduce downlink bandwidth, they are especially useful for clients that may require a long time to perform local updates and communicate them back to the PS. Thus, accounting for delayed gradients may enhance DoCoFL\u2019s versatility and robustness in real FL deployments; (2) our theoretical framework focuses on the SGD optimizer. Exploring the implications of using adaptive optimizers, such as Adam, on the theoretical analysis and guarantees would be of great interest; (3) as we convey in Appendix B, an intriguing extension of DoCoFL involves the incorporation of adaptive bandwidth budget for anchor and correction compression; although it introduces a significant theoretical challenge due to the coupling between optimization and compression, it may yield a convergence guarantee for DoCoFL with anchor compression and achieve even larger bandwidth savings; (4) while we employ extensive simulations and account for various overheads of DoCoFL, it is desired to further strengthen our conclusions through real deployments."
        }
    ],
    "title": "DoCoFL: Downlink Compression for Cross-Device Federated Learning",
    "year": 2023
}