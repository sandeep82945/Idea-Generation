{
    "abstractText": "Conditional language models still generate unfaithful output that is not supported by their input. These unfaithful generations jeopardize trust in real-world applications such as summarization or human-machine interaction, motivating a need for automatic faithfulness metrics. To implement such metrics, NLI models seem attractive, since they solve a strongly related task that comes with a wealth of prior research and data. But recent research suggests that NLI models require costly additional machinery to perform reliably across datasets, e.g., by running inference on a cartesian product of input and generated sentences, or supporting them with a question-generation/answering step. In this work we show that pure NLI models can outperform more complex metrics when combining task-adaptive data augmentation with robust inference procedures. We propose: (1) Augmenting NLI training data to adapt NL inferences to the specificities of faithfulness prediction in dialogue; (2) Making use of both entailment and contradiction probabilities in NLI, and (3) Using Monte-Carlo dropout during inference. Applied to the TRUE benchmark, which combines faithfulness datasets across diverse domains and tasks, our approach strongly improves a vanilla NLI model and significantly outperforms previous work, while showing favourable computational cost.",
    "authors": [
        {
            "affiliations": [],
            "name": "Julius Steen"
        },
        {
            "affiliations": [],
            "name": "Juri Opitz"
        },
        {
            "affiliations": [],
            "name": "Anette Frank"
        },
        {
            "affiliations": [],
            "name": "Katja Markert"
        }
    ],
    "id": "SP:2efcd91a4e7cd2f9db322a7183ecb8e40f805a50",
    "references": [
        {
            "authors": [
                "Samuel R. Bowman",
                "Gabor Angeli",
                "Christopher Potts",
                "Christopher D. Manning."
            ],
            "title": "A large annotated corpus for learning natural language inference",
            "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2015
        },
        {
            "authors": [
                "Yanran Chen",
                "Steffen Eger."
            ],
            "title": "Menli: Robust evaluation metrics from natural language inference",
            "venue": "arXiv preprint arXiv:2208.07316.",
            "year": 2022
        },
        {
            "authors": [
                "Emily Dinan",
                "Stephen Roller",
                "Kurt Shuster",
                "Angela Fan",
                "Michael Auli",
                "Jason Weston."
            ],
            "title": "Wizard of wikipedia: Knowledge-powered conversational agents",
            "venue": "International Conference on Learning Representations.",
            "year": 2019
        },
        {
            "authors": [
                "Nouha Dziri",
                "Hannah Rashkin",
                "Tal Linzen",
                "David Reitter."
            ],
            "title": "Evaluating Attribution in Dialogue Systems: The BEGIN Benchmark",
            "venue": "Transactions of the Association for Computational Linguistics, 10:1066\u20131083. Note: TRUE uses an earlier ver-",
            "year": 2022
        },
        {
            "authors": [
                "Alexander R. Fabbri",
                "Wojciech Kry\u015bci\u0144ski",
                "Bryan McCann",
                "Caiming Xiong",
                "Richard Socher",
                "Dragomir Radev."
            ],
            "title": "SummEval: Re-evaluating summarization evaluation",
            "venue": "Transactions of the Association for Computational Linguistics, 9:391\u2013409.",
            "year": 2021
        },
        {
            "authors": [
                "Tobias Falke",
                "Leonardo F.R. Ribeiro",
                "Prasetya Ajie Utama",
                "Ido Dagan",
                "Iryna Gurevych"
            ],
            "title": "Ranking generated summaries by correctness: An interesting but challenging application for natural language",
            "year": 2019
        },
        {
            "authors": [
                "Purvi Goel",
                "Li Chen."
            ],
            "title": "On the robustness of monte carlo dropout trained with noisy labels",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pages 2219\u20132228.",
            "year": 2021
        },
        {
            "authors": [
                "Tanya Goyal",
                "Greg Durrett."
            ],
            "title": "Evaluating factuality in generation with dependency-level entailment",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3592\u20133603, Online. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Prakhar Gupta",
                "Chien-Sheng Wu",
                "Wenhao Liu",
                "Caiming Xiong."
            ],
            "title": "DialFact: A benchmark for fact-checking in dialogue",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages",
            "year": 2022
        },
        {
            "authors": [
                "Pengcheng He",
                "Xiaodong Liu",
                "Jianfeng Gao",
                "Weizhu Chen."
            ],
            "title": "Deberta: Decoding-enhanced bert with disentangled attention",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "Karl Moritz Hermann",
                "Tom\u00e1\u0161 Ko\u010disk\u00fd",
                "Edward Grefenstette",
                "Lasse Espeholt",
                "Will Kay",
                "Mustafa Suleyman",
                "Phil Blunsom."
            ],
            "title": "Teaching machines to read and comprehend",
            "venue": "Proceedings of the 28th International Conference on Neural Information Processing",
            "year": 2015
        },
        {
            "authors": [
                "Or Honovich",
                "Roee Aharoni",
                "Jonathan Herzig",
                "Hagai Taitelbaum",
                "Doron Kukliansy",
                "Vered Cohen",
                "Thomas Scialom",
                "Idan Szpektor",
                "Avinatan Hassidim",
                "Yossi Matias."
            ],
            "title": "TRUE: Re-evaluating factual consistency evaluation",
            "venue": "Proceedings of the 2022",
            "year": 2022
        },
        {
            "authors": [
                "Or Honovich",
                "Leshem Choshen",
                "Roee Aharoni",
                "Ella Neeman",
                "Idan Szpektor",
                "Omri Abend."
            ],
            "title": "q2: Evaluating factual consistency in knowledgegrounded dialogues via question generation and question answering",
            "venue": "Proceedings of the 2021 Confer-",
            "year": 2021
        },
        {
            "authors": [
                "Wojciech Kryscinski",
                "Bryan McCann",
                "Caiming Xiong",
                "Richard Socher."
            ],
            "title": "Evaluating the factual consistency of abstractive text summarization",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "Philippe Laban",
                "Tobias Schnabel",
                "Paul N. Bennett",
                "Marti A. Hearst."
            ],
            "title": "SummaC: Re-visiting NLIbased models for inconsistency detection in summarization",
            "venue": "Transactions of the Association for Computational Linguistics, 10:163\u2013177.",
            "year": 2022
        },
        {
            "authors": [
                "Moritz Laurer",
                "W v Atteveldt",
                "Andreu Casas",
                "Kasper Welbers"
            ],
            "title": "Less annotating, more classifying\u2013addressing the data scarcity issue of supervised machine learning with deep transfer learning and bert-nli",
            "year": 2022
        },
        {
            "authors": [
                "Alisa Liu",
                "Swabha Swayamdipta",
                "Noah A Smith",
                "Yejin Choi."
            ],
            "title": "Wanli: Worker and ai collaboration for natural language inference dataset creation",
            "venue": "arXiv preprint arXiv:2201.05955.",
            "year": 2022
        },
        {
            "authors": [
                "Joshua Maynez",
                "Shashi Narayan",
                "Bernd Bohnet",
                "Ryan McDonald."
            ],
            "title": "On faithfulness and factuality in abstractive summarization",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1906\u20131919, On-",
            "year": 2020
        },
        {
            "authors": [
                "Yixin Nie",
                "Adina Williams",
                "Emily Dinan",
                "Mohit Bansal",
                "Jason Weston",
                "Douwe Kiela."
            ],
            "title": "Adversarial NLI: A new benchmark for natural language understanding",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "Artidoro Pagnoni",
                "Vidhisha Balachandran",
                "Yulia Tsvetkov."
            ],
            "title": "Understanding factuality in abstractive summarization with FRANK: A benchmark for factuality metrics",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Asso-",
            "year": 2021
        },
        {
            "authors": [
                "Alicia Parrish",
                "William Huang",
                "Omar Agha",
                "Soo-Hwan Lee",
                "Nikita Nangia",
                "Alexia Warstadt",
                "Karmanya Aggarwal",
                "Emily Allaway",
                "Tal Linzen",
                "Samuel R. Bowman"
            ],
            "title": "Does putting a linguist in the loop improve NLU data",
            "year": 2021
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu"
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2020
        },
        {
            "authors": [
                "Hannah Rashkin",
                "David Reitter",
                "Gaurav Singh Tomar",
                "Dipanjan Das."
            ],
            "title": "Increasing faithfulness in knowledge-grounded dialogue with controllable features",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics",
            "year": 2021
        },
        {
            "authors": [
                "Nitish Srivastava",
                "Geoffrey Hinton",
                "Alex Krizhevsky",
                "Ilya Sutskever",
                "Ruslan Salakhutdinov."
            ],
            "title": "Dropout: A simple way to prevent neural networks from overfitting",
            "venue": "Journal of Machine Learning Research, 15(56):1929\u20131958.",
            "year": 2014
        },
        {
            "authors": [
                "Emma Strubell",
                "Ananya Ganesh",
                "Andrew McCallum."
            ],
            "title": "Energy and policy considerations for deep learning in NLP",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3645\u20133650, Florence, Italy. Asso-",
            "year": 2019
        },
        {
            "authors": [
                "James Thorne",
                "Andreas Vlachos",
                "Christos Christodoulopoulos",
                "Arpit Mittal."
            ],
            "title": "FEVER: a large-scale dataset for fact extraction and VERification",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of",
            "year": 2018
        },
        {
            "authors": [
                "Prasetya Utama",
                "Joshua Bambrick",
                "Nafise Moosavi",
                "Iryna Gurevych."
            ],
            "title": "Falsesum: Generating document-level NLI examples for recognizing factual inconsistency in summarization",
            "venue": "Proceedings of the 2022 Conference of the North American Chap-",
            "year": 2022
        },
        {
            "authors": [
                "Alex Wang",
                "Kyunghyun Cho",
                "Mike Lewis."
            ],
            "title": "Asking and answering questions to evaluate the factual consistency of summaries",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5008\u20135020, Online. Asso-",
            "year": 2020
        },
        {
            "authors": [
                "Alex Wang",
                "Yada Pruksachatkun",
                "Nikita Nangia",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel Bowman."
            ],
            "title": "Superglue: A stickier benchmark for general-purpose language understanding systems",
            "venue": "Advances in neural information",
            "year": 2019
        },
        {
            "authors": [
                "Alex Wang",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel Bowman."
            ],
            "title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
            "venue": "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing",
            "year": 2018
        },
        {
            "authors": [
                "Adina Williams",
                "Nikita Nangia",
                "Samuel Bowman."
            ],
            "title": "A broad-coverage challenge corpus for sentence understanding through inference",
            "venue": "Proceedings of the 2018 Conference of the North American",
            "year": 2018
        },
        {
            "authors": [
                "Teven Le Scao",
                "Sylvain Gugger",
                "Mariama Drame",
                "Quentin Lhoest",
                "Alexander Rush."
            ],
            "title": "Transformers: State-of-the-art natural language processing",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System",
            "year": 2020
        },
        {
            "authors": [
                "Yuan Zhang",
                "Jason Baldridge",
                "Luheng He."
            ],
            "title": "PAWS: Paraphrase adversaries from word scrambling",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "In this work we show that pure NLI models can outperform more complex metrics when combining task-adaptive data augmentation with robust inference procedures. We propose: (1) Augmenting NLI training data to adapt NL inferences to the specificities of faithfulness prediction in dialogue; (2) Making use of both entailment and contradiction probabilities in NLI, and (3) Using Monte-Carlo dropout during inference. Applied to the TRUE benchmark, which combines faithfulness datasets across diverse domains and tasks, our approach strongly improves a vanilla NLI model and significantly outperforms previous work, while showing favourable computational cost."
        },
        {
            "heading": "1 Introduction",
            "text": "Conditional language models suffer from a tendency to hallucinate information (Maynez et al., 2020), resulting in generations that are not faithful to their input documents, which limits the trustworthiness of such models. This raises a need for automatic faithfulness metrics. In this context, models trained on natural language inference (NLI) (Bowman et al., 2015) are attractive since, intuitively, a generation being faithful implies it must be entailed by the source (Falke et al., 2019).\nHowever, pure NLI models have seen mixed success in faithfulness evaluation (Falke et al., 2019; Kryscinski et al., 2020; Wang et al., 2020; Maynez et al., 2020). While in recent evaluation on the TRUE benchmark (Honovich et al., 2022), which contains datasets from knowledge-grounded dialogue, summarization and paraphrasing, NLIderived metrics perform best overall, they require impractically large models, or costly additional machinery such as question generation and answering models at inference, while still showing robustness issues. Thus we ask: What is still needed for pure NLI models to perform robustly across faithfulness datasets \u2013 while remaining cheap enough to serve as a lean and practical evaluation tool?\nWe enhance a relatively small NLI model to make it work robustly across tasks in three ways:\nTask-Adaptive Data Augmentation. In NLI, a hypothesis must be fully entailed by its supporting premise. However, in faithfulness, not all parts of the generation always need to be grounded. We identify an instance of this phenomenon in dialogue where parts of a turn can fulfill communicative functions such as hedging or establishing emotional connection and are often disregarded in faithfulness annotation. Hence, when applying NLI models to complete dialogue turns that may include statements irrelevant for grounding, we run a risk of producing incorrect unfaithfulness predictions.\nTo alleviate this issue, we propose a simple data augmentation method to adapt NLI models to genres where they need to be aware of statements that must be exempt from NLI-based faithfulness evaluation. Our approach is computationally attractive, as it avoids an increase of cost at inference time.\nIntegration of NLI Contradiction Scores. Existing NLI faithfulness metrics typically use the entailment score for their predictions (Honovich et al., 2022; Falke et al., 2019; Kryscinski et al., 2020). However, Chen and Eger (2022) show that subtracting the contradiction score from the entail-\nar X\niv :2\n30 5.\n16 81\n9v 1\n[ cs\n.C L\n] 2\n6 M\nay 2\n02 3\nment score (referred to as e-c ) can improve NLI performance in certain evaluation tasks. We show that there also is a strong positive effect of e-c for faithfulness prediction, and demonstrate that this is due to a high contradiction probability being a more reliable predictor of unfaithfulness than low entailment probability.\nMonte-Carlo Dropout Inference. Applying NLI models to faithfulness prediction involves a domain shift from largely human-written data to automatically generated text. To make NLI model scores more robust under this shift, we propose to use Monte-Carlo dropout during inference (Srivastava et al., 2014). This essentially creates a cheap ensemble and has been shown to deal better with noisy labels (Goel and Chen, 2021). This approach leads to consistent score improvements in our tasks.\nThe combination of all modifications not only strongly improves over a baseline NLI model, but also outperforms all other metrics on TRUE, on average, while being cheaper and smaller.1"
        },
        {
            "heading": "2 Method Details",
            "text": ""
        },
        {
            "heading": "2.1 Task-adaptive Data Augmentation",
            "text": "To illustrate that task requirements can be incompatible between faithfulness and NLI, consider the following instance from the Q2 dialogue corpus (Honovich et al., 2021) that is labelled as faithful:\nGrounding: American pancakes are similar to Scotch pancakes or drop scones. Generation: yes , i love american pancakes , they are like scotch pancakes\nFrom an NLI perspective, the generation is clearly not entailed, since the statement \u201cI love american pancakes\u201d is not supported by the input.\nTo better prepare an NLI system for such genre or task-specific cases, we manually curate a small list of statements that should not influence the faithfulness prediction. We augment NLI data from the ANLI corpus (Nie et al., 2020) by adding a randomly chosen phrase from this set to each instance, while preserving the label. We then train an already fine-tuned NLI model on a concatenation of these augmented samples and original ANLI data. For training details see Appendix A.\n1All code is available at https://github.com/julmaxi/ with_a_little_push"
        },
        {
            "heading": "2.2 Monte-Carlo Dropout",
            "text": "To compute scores under Monte-Carlo dropout, we randomly sample k dropout masks and compute the average of the model predictions. We set k = 15, since preliminary experiments showed that performance did not profit from additional samples."
        },
        {
            "heading": "3 Experimental Setup",
            "text": "We run experiments on TRUE (Honovich et al., 2022), a benchmark that compiles a wide variety of faithfulness tasks in a standardized format. It contains summarization (Pagnoni et al., 2021; Maynez et al., 2020; Wang et al., 2020; Fabbri et al., 2021), knowledge-grounded dialog (Honovich et al., 2021; Gupta et al., 2022; Dziri et al., 2022)2 and paraphrasing (Zhang et al., 2019) datasets.3 Following recommendations in TRUE, we evaluate using Area under the ROC Curve (AUC).\nAs our BASE model, we use the DeBERTa-large (He et al., 2020) model of Laurer et al. (2022), trained on MultiNLI (Williams et al., 2018), FeverNLI (Thorne et al., 2018), ANLI (Nie et al., 2020), LingNLI (Parrish et al., 2021) and WANLI (Liu et al., 2022). The metric All uses all three of our proposed modifications to Base. We also investigate a variant without MC dropout inference (-MC) as a more cost efficient alternative.\nWe compare to the strongest models on TRUE: T5 ANLI (Honovich et al., 2022) is a T5-11B\n(Raffel et al., 2020) model trained on ANLI.4\nSummacZS (Laban et al., 2022) evaluates an NLI model on all pairs of input and generated sentences and then averages maximum entailment probabilities for each generated sentence. Q2 (Honovich et al., 2021) combines a question generation/answering pipeline with an NLI score. Finally, Honovich et al. (2022) introduce a strong ensemble of these 3 methods (Eorig). To further verify our approach, we construct a new ensemble (Eour) by replacing T5 with All."
        },
        {
            "heading": "4 Results",
            "text": "Table 1 shows the AUC scores for each metric. Our model All not only significantly improves over\n2TRUE uses an earlier variant of BEGIN that is described in https://arxiv.org/pdf/2105.00071v1.pdf\n3TRUE also has a fact-checking part, which was not included in average metric performance. We also exclude it here, as our base NLI model was trained on parts of it.\n4The original T5 model is also pretrained on GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019) data, which contains additional NLI data.\nBase on six out of nine corpora, but also significantly outperforms all other competitors on average, while being more computationally efficient.\nAs expected, we find the biggest gains in dialogue, where the All model even outperforms Eorig on 2 out of 3 corpora. We do not improve on BEGIN, which is likely due to bias in the dataset construction, which we elaborate on in Section 5.1. On the summarization part, All improves significantly over Base on 3 out of 5 corpora, while not significantly harming performance on any corpus. However, it still falls short of the best models in TRUE. The strong showing of T5 on these corpora suggests that this might be alleviated with a stronger base model.\nOverall, a very similar behaviour is exhibited by -MC, presenting an attractive option when the added overhead of multiple samples is undesirable. Eour is on par with Eorig, despite massively\nreduced costs; it even significantly outperforms it on two dialog and the paraphrasing corpora.\nWe also investigate the performance of each individual modification to our model (Table 2). They all improve average scores, while only leading to a notable decrease on BEGIN for both e-c and dialogue augmentations and on MNBM for e-c .\nOutside of dialogue, we find that the augmentation methods have a positive impact on PAWS, as well as all summarization corpora that are at least partially based on summaries for the CNN/DM dataset (Hermann et al., 2015) (Frank, QAGS-C, and SummEval). While we do not have a definitive explanation for this phenomenon, we hypothesize that on these datasets our augmentations aid in making the model robust in the presence of noise\nor irrelevant context since our augmentations are label-neutral and must similarly be \u2019ignored\u2019 during training."
        },
        {
            "heading": "5 Analysis",
            "text": ""
        },
        {
            "heading": "5.1 Effect of Dialogue Adaptation",
            "text": "We investigate whether the improvements via our augmentation approach are indeed due to them improving the handling of personal statements.\nWe use the occurrences of the pronoun I in a generation as a proxy measure5 and compute its correlation with human labels and metrics (see Table 3). On both Q2 and Dialfact, our proxy measure, while uncorrelated with human labels, is strongly correlated with the scores of both Base and T5. This indicates these metrics indeed tend to incorrectly reject generations with personal statements. All on the other hand reduces this dependency.\nOur results also help explain why All fails to improve on BEGIN, since BEGIN gold labels are\n5We use spacy (spacy.io) for POS tagging to identify pronouns.\nnegatively correlated with first person pronouns. This is likely due to a bias in dataset construction: The BEGIN dataset used in TRUE has generations from two models, one of which is both more likely to generate pronouns and more likely to generate unfaithful output (see Appendix B)."
        },
        {
            "heading": "5.2 Effect of integrating contradiction scores",
            "text": "To isolate the effect of e-c we compare score distributions of Base and Base+e-c in Figure 1. The lefthand side of the figure shows that in Base ca. 2700 faithful instances are predicted as non-entailed (i.e., e-score near 0), which implies they are labelled as contradictory or neutral. e-c , on the other hand, further differentiates these instances into instances with high contradiction (negative e-c score) and high neutral probability (e-c score near 0). We observe that almost all low-scoring faithful generations are classified as neutral, whereas nearly all instances that are classified as contradictory are indeed unfaithful. Where Base has no way to make use of this information, e-c allows to reliably label contradictory instances as unfaithful."
        },
        {
            "heading": "5.3 Cost comparison to other approaches",
            "text": "There is increasing awareness of the resource-hungriness of deep learning (Strubell et al., 2019). Especially for faithfulness, cheap and reliable metrics are critical, given rising demands for NLG in research and industry. Table 4 shows that our model\nrequires fewer parameters than any other metric, including a more than 30x reduction compared to T5. During inference our model always requires a constant number of calls which can be reduced to a single call when ablating MC dropout. On the other hand, the number of calls in SummacZS scales with the number of input and output sentences. Q2 needs to generate questions by calling an auto-regressive QG model n times, where n factors in the amount and length of questions (#Q\u00d7Ql), answer #Q questions with the QA model and finally check #Q answers with an NLI model (#Q \u00d7 2).\nIn sum, our model compares favourably with other approaches, while also allowing for a performance/cost tradeoff by forgoing MC dropout."
        },
        {
            "heading": "5.4 Phrase Selection Robustness",
            "text": "To ensure that our augmentation is robust and not overly reliant on any particular choice of phrases, we repeat our dataset augmentation process multiple times with five randomly chosen augmentation phrases out of the original ten. We sample ten such datasets and retrain our model for each. Table 5 shows the average score, minimum and maxi-\nmum score, as well as the standard deviation of the scores. We also report results of a model with both MC dropout and e-c but without any additional training and augmentations to directly quantify whether the augmentations are still helpful in their reduced form. This corresponds to applying MC dropout and e-c to Base.\nAs expected, we find that reducing the variety of available phrases leads to a drop in performance across almost all datasets, compared to All. The only exception is BEGIN, where we instead see a slight improvement. This is likely to be related to the construction of BEGIN (see the discussion in Section 5.1).\nWhen comparing our limited augmentation models to the non-augmented model, we find that they still outperform the non-augmented model in almost all cases. In particular for Q2 and DialFact, for which we expect the strongest impact of our augmentations, we find that even the worst run still outperforms non-augmented model. This suggests that our augmentations can robustly adapt the model to the dialogue task.\nFinally, we observe a relatively large drop in scores for all datasets that are at (least partially) derived from CNN/DM (Frank, SummEval and QAGS-C). This mirrors our earlier observation in Section 4 that these datasets profit from our augmentation procedure."
        },
        {
            "heading": "6 Related Work",
            "text": "Previous work on the utility of NLI for faithfulness led to mixed conclusions. In summarization, Falke et al. (2019) and Kryscinski et al. (2020) find out-of-the-box models have only limited utility in a faithfulness setting. In Wang et al. (2020), an NLI model is outperformed by a question generation/answering (QA/QG)-based method. In contrast, Maynez et al. (2020) find that a similar NLI model vastly outperforms a QA/QG metric on their data. In knowledge-grounded dialogue, Dziri et al. (2022), Gupta et al. (2022) and Honovich et al. (2021) find out-of-the-box models underperform.\nTo improve NLI models for faithfulness in summarization, Kryscinski et al. (2020) propose FactCC, which is trained on artificially noised summaries. Utama et al. (2022) propose a controllable generation model to generate artificial faithfulness data. In knowledge-grounded dialogue, Dziri et al. (2022) and Gupta et al. (2022) combine noising techniques to generate additional training data for\nNLI-based faithfulness models. In contrast to our work, these approaches a) generate training data from external sources, instead of directly augmenting NLI data, and b) do not explicitly focus on reconciling differences between NLI and faithfulness with their augmentation. Outside of augmentationbased approaches, Goyal and Durrett (2020) propose to train NLI models to label faithfulness at the dependency arc level."
        },
        {
            "heading": "7 Conclusion",
            "text": "We have demonstrated that with a small number of focused adaptations, even a relatively small NLI model can robustly predict faithfulness. We have:\n1. Shown that NLI-based metrics can be incompatible with task-specific requirements and identified and fixed one such incompatibility in dialogue with an augmentation strategy.\n2. Demonstrated the importance of contradiction probability for scoring and that the underlying mechanism is the high reliability of NLI contradiction scores for detecting unfaithfulness\n3. Shown that using Monte-Carlo dropout improves metric performance.\nOur improved NLI model significantly improves over its baseline across many corpora and outperforms all competitors in average score on TRUE, while being much more efficient at inference.\nOur work suggests that strong improvements are possible for NLI-based faithfulness metrics, by combining data augmentation with adapted NLI score computation. We hope this finding will spurn advances in cheap and robust NLI for faithfulness."
        },
        {
            "heading": "8 Limitations",
            "text": "Some of the summarization datasets annotated for faithfulness are relatively small, which makes score estimates uncertain. Furthermore, many datasets contain only output from a limited number of generation systems, which makes it hard to properly account for potential biases towards certain generation systems that may confound scores (see Pagnoni et al. (2021)). These concerns are, however, alleviated to some extent since we study trends across many independently created datasets, which makes it less likely for a single bias to persist in all of them. Furthermore the availability of generation and thus annotated faithfulness data limits our experiments to English. Finally, it remains\nunclear whether our results would still provide advantages when applied to larger models such as T5-11B, whose parameter count makes experimentation infeasible on the hardware available to us."
        },
        {
            "heading": "9 Ethics Statement",
            "text": "Faithfulness metrics help reduce the amount of incorrect information generated by NLG systems, reducing the risk associated which such generations. However, faulty or unreliable faithfulness metrics might cause harm by incorrectly classifying faithful content as unfaithful and vice versa.\nWe run all experiments on publicly available data that has been specifically constructed for faithfulness evaluation. The underlying publication has been published at a conference whose review process involved an ethics review. For a specific discussion of the human effort involved in creation of the datasets we refer the reader to the original publications."
        },
        {
            "heading": "A Augmentation Training Details",
            "text": "A.1 Augmentation Phrases Table 6 lists our manually curated list of phrases inserted during data augmentation. All phrases were derived via a small manual error analysis on the Base model.\nWe broadly divide our phrases into three categories: introductory statements, hedging, and sentiment statements. For each instance in ANLI, one random phrase from the list is prepended to the hypothesis. We use all three rounds of ANLI annotations. This results in 162,865 augmented instances\nwhich, together with the original ANLI instances, leads to a total of 325,730 training instances.\nA.2 Hyperparameters Table 7 lists the hyperparameter settings for our model. We use the same optimizer hyperparameters as Laurer et al. (2022) except for an increased batch size and the learning rate. For the latter we tested three learning rates (5e\u2212 6, 5e\u2212 2, 5e\u2212 1) and select the one that provided the best loss on the augmented ANLI validation set. We initially ran models for 10,000 steps with a checkpoint every 1,000 steps and selected the checkpoint with the lowest loss on the augmented ANLI validation set. Later we reduced the number of training steps to 2,000 since we found we would usually select an early checkpoint as validation loss increased later in training, likely related to overfitting on the augmented data.\nA.3 Training We use the DeBERTa implementation in the huggingface transformers library (Wolf et al., 2020) and trained our model on a single node using two RX6800 GPUs, with one training run taking about three hours. Later experiments with fewer steps cut that time by 80%."
        },
        {
            "heading": "B Dataset Bias in BEGIN",
            "text": "BEGIN is the only dialogue corpus on which first person pronoun occurrence shows a strong (negative) correlation with faithfulness (see Table 3). Since there is nothing in the annotation guidelines that would explain this correlation, we instead hypothesize that this is the consequence of a model induced bias in the data. Specifically, we hypothesize that one of the two models in BEGIN is (1) more likely to generate personal statements and (2) less likely to generate faithful responses.\nTo avoid confusion in the remainder of this section, we highlight that there are two variants of BEGIN:\nBEGIN-v1 is the variant used in TRUE. It contains labeled generations by a fine-tuned GPT-\n2 base (Radford et al., 2019) and a fine-tuned T5 base model (Raffel et al., 2020) on the Wizard of Wikipedia dataset (Dinan et al., 2019).6\nBEGIN-v2 is a more recent variant of BEGIN that is not part of TRUE. In addition to new instances generated by T5 and GPT-2 it contains outputs from two additional models. It also has a revised annotation procedure. When we refer to BEGIN-v2, we exclusively mean the Wizard of Wikipedia subset.\nUnfortunately, BEGIN-v1 does not allow us to retrieve which model generated which instance. This makes it impossible to directly investigate for model bias. However, BEGIN-v2 includes outputs by the same two models, fine-tuned on the same data. Since we only need corpus level statistics to verify our assumptions, we conduct our analysis on the GPT-2 and T5 instances in BEGIN-v2.\nTo verify (1), we compute the correlation between a binary variable indicating which model generated each instance (T5: 0, GPT-2: 1) and firstperson pronoun occurrence. We find a positive correlation (Kendall\u2019s \u03c4 wrt. to I-pronoun occurrence: 0.18, p < 0.001), indicating that GPT-2 generates outputs including more first-person pronouns.\nTo investigate whether GPT-2 is also more likely to be unfaithful, i.e. to verify (2), we compute the correlation between the binary model indicator variable and a faithfulness variable that is 1 when the output is labelled as Fully attributable and 0 otherwise. We find a negative correlation (Kendall\u2019s \u03c4 wrt. to Faithfulness: \u22120.25, p < 0.001), supporting our hypothesis that GPT-2 is also overall less faithful. To ensure that this is not an effect of additional personal statements leading to more unfaithful generations, we conduct the same analysis only on instances where we identify no first-person pronouns. We find a similarly strong negative correlation of \u22120.29 (p < 0.001).\nOur analysis shows that GPT-2 produces both overall less faithful outputs and more first-person pronouns than T5. Since BEGIN-v1 contains only outputs from T5 and GPT-2 this suggests that the root cause for the negative correlation between faithfulness label and first-person pronoun occurrence in BEGIN-v1 is model bias confounding faithfulness and first-person pronoun occurrence.\n6The relevant data can be found at https://raw. githubusercontent.com/google/BEGIN-dataset/ 5fa0cb0dde0e653d2016724a52a5ca27fe8b6a3f/dev_05_ 24_21.tsv\nB.1 Dataset Bias in BEGIN-v2 We conduct a preliminary study to investigate whether similar biases also exist in BEGIN-v2.\nWe observe that while BEGIN-v2 uses data from four dialogue systems, a majority of faithful generations is produced by a single system called CTRLDIALOG (Rashkin et al., 2021). CTRL-DIALOG is specifically trained to generate less subjective text, which we hypothesize might result in fewer first person pronouns. Since CTRL-DIALOG also produces more faithful texts, this would lead to a negative correlation between faithfulness and first person pronouns, similar to what we observe on BEGIN-v1.\nWe verify this assumption by computing the correlation of a binary variable indicating an instance has been generated by CTRL-DIALOG with a) the faithfulness labels on BEGIN-v2 and b) first-person pronoun occurrence. We find that an instance being generated by CTRL-DIALOG is positively correlated with it having a faithful label (Kendall \u03c4 w.r.t. faithfulness: 0.48, p< 0.001) while being negatively correlated with the number of pronouns (Kendall \u03c4 w.r.t. I-pronoun occurrence: - 0.34, p< 0.001). This suggests future evaluations on the BEGIN-v2 might run into similar bias issues."
        },
        {
            "heading": "C Dataset Statistics",
            "text": "We report the number of instances, as well as the class distribution of TRUE in Table 8."
        }
    ],
    "title": "With a Little Push, NLI Models can Robustly and Efficiently Predict Faithfulness",
    "year": 2023
}