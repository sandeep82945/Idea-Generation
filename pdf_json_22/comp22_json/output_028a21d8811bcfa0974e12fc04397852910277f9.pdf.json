{
    "abstractText": "We present a neural text-to-speech (TTS) method that models natural vocal effort variation to improve the intelligibility of synthetic speech in the presence of noise. The method consists of first measuring the spectral tilt of unlabeled conventional speech data, and then conditioning a neural TTS model with normalized spectral tilt among other prosodic factors. Changing the spectral tilt parameter and keeping other prosodic factors unchanged enables effective vocal effort control at synthesis time independent of other prosodic factors. By extrapolation of the spectral tilt values beyond what has been seen in the original data, we can generate speech with high vocal effort levels, thus improving the intelligibility of speech in the presence of masking noise. We evaluate the intelligibility and quality of normal speech and speech with increased vocal effort in the presence of various masking noise conditions, and compare these to well-known speech intelligibility-enhancing algorithms. The evaluations show that the proposed method can improve the intelligibility of synthetic speech with little loss in speech quality.",
    "authors": [
        {
            "affiliations": [],
            "name": "Tuomo Raitio"
        },
        {
            "affiliations": [],
            "name": "Petko Petkov"
        },
        {
            "affiliations": [],
            "name": "Jiangchuan Li"
        },
        {
            "affiliations": [],
            "name": "Muhammed Shifas"
        },
        {
            "affiliations": [],
            "name": "Andrea Davis"
        },
        {
            "affiliations": [],
            "name": "Yannis Stylianou"
        }
    ],
    "id": "SP:74f4c789579b85649090eab07e1cb10c109fd981",
    "references": [
        {
            "authors": [
                "M. Cooke",
                "C. Mayo",
                "C. Valentini-Botinhao"
            ],
            "title": "Intelligibilityenhancing speech modifications: The Hurricane Challenge",
            "venue": "Interspeech, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "J. Rennies",
                "H. Schepker",
                "C. Valentini-Botinhao",
                "M. Cooke"
            ],
            "title": "Intelligibility-enhancing speech modifications \u2013 The Hurricane Challenge 2.0",
            "venue": "Interspeech, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "B. Langner",
                "A.W. Black"
            ],
            "title": "Improving the understandability of speech synthesis by modeling speech in noise",
            "venue": "ICASSP, 2005.",
            "year": 2005
        },
        {
            "authors": [
                "E. Lombard"
            ],
            "title": "Le signe de l\u2019elevation de la voix",
            "venue": "Ann. Maladies Oreille, Larynx, Nez, Pharynx, vol. 37, no. 101\u2013119, p. 25, 1911.",
            "year": 1911
        },
        {
            "authors": [
                "M. Cooke",
                "Y. Lu"
            ],
            "title": "Spectral and temporal changes to speech produced in the presence of energetic and informational maskers",
            "venue": "J. Acoust. Soc. Am., vol. 128, pp. 2059\u201369, 10 2010.",
            "year": 2010
        },
        {
            "authors": [
                "B. Lindblom"
            ],
            "title": "Explaining Phonetic Variation: A Sketch of the H&H Theory",
            "year": 1990
        },
        {
            "authors": [
                "\u2014\u2014"
            ],
            "title": "Role of articulation in speech perception: Clues from production",
            "venue": "The Journal of the Acoustical Society of America, vol. 99, no. 3, pp. 1683\u20131692, 1996.",
            "year": 1996
        },
        {
            "authors": [
                "H. Traunm\u00fcller",
                "A. Eriksson"
            ],
            "title": "Acoustic effects of variation in vocal effort by men, women, and children",
            "venue": "J. Acoust. Soc. Am., vol. 107, no. 6, pp. 3438\u20133451, 2000.",
            "year": 2000
        },
        {
            "authors": [
                "C.T. Ishi",
                "H. Ishiguro",
                "N. Hagita"
            ],
            "title": "Analysis of the roles and the dynamics of breathy and whispery voice qualities in dialogue speech",
            "venue": "EURASIP J. Audio Speech Music Process., vol. 3, pp. 1\u201312, 2010.",
            "year": 2010
        },
        {
            "authors": [
                "C. Gobl",
                "A. N\u0131\u0301 Chasaide"
            ],
            "title": "The role of voice quality in communicating emotion, mood and attitude",
            "venue": "Speech Comm., vol. 40, no. 1\u20132, pp. 189\u2013212, 2003.",
            "year": 2003
        },
        {
            "authors": [
                "B. Sauert",
                "P. Vary"
            ],
            "title": "Near end listening enhancement: Speech intelligibility improvement in noisy environments",
            "venue": "ICASSP, 2006.",
            "year": 2006
        },
        {
            "authors": [
                "M. Skowronski",
                "J. Harris"
            ],
            "title": "Applied principles of clear and Lombard speech for automated intelligibility enhancement in noisy environments",
            "venue": "Speech Comm., vol. 48, pp. 549\u2013558, 2006.",
            "year": 2006
        },
        {
            "authors": [
                "S.D. Yoo",
                "J.R. Boston",
                "A. El-Jaroudi",
                "C.-C. Li",
                "J.D. Durrant",
                "K. Kovacyk",
                "S. Shaiman"
            ],
            "title": "Speech signal modification to increase intelligibility in noisy environments",
            "venue": "J. Acoust. Soc. Am., vol. 122, no. 2, pp. 1138\u20131149, 2007.",
            "year": 2007
        },
        {
            "authors": [
                "T.-C. Zoril\u0103",
                "V. Kandia",
                "Y. Stylianou"
            ],
            "title": "Speech-in-noise intelligibility improvement based on spectral shaping and dynamic range compression",
            "venue": "Interspeech, 2012.",
            "year": 2012
        },
        {
            "authors": [
                "R.J. Niederjohn",
                "J.H. Grotelueschen"
            ],
            "title": "The enhancement of speech intelligibility in high noise levels by high-pass filtering followed by rapid amplitude compression",
            "venue": "IEEE Trans. Acous., Speech and Sig. Proc., vol. 24, no. 4, pp. 277\u2013282, 1976.",
            "year": 1976
        },
        {
            "authors": [
                "T. Raitio",
                "A. Suni",
                "M. Vainio",
                "P. Alku"
            ],
            "title": "Analysis of HMMbased Lombard speech synthesis",
            "venue": "Interspeech, 2011.",
            "year": 2011
        },
        {
            "authors": [
                "\u2014\u2014"
            ],
            "title": "Synthesis and perception of breathy, normal, and Lombard speech in the presence of noise",
            "venue": "Computer Speech & Language, vol. 28, no. 2, pp. 648\u2013664, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "Q. Hu",
                "T. Bleisch",
                "P. Petkov",
                "T. Raitio",
                "E. Marchi",
                "V. Lakshminarasimhan"
            ],
            "title": "Whispered and lombard neural speech synthesis",
            "venue": "2021 IEEE Spoken Language Technology Workshop (SLT), 2021.",
            "year": 2021
        },
        {
            "authors": [
                "N. Campbell",
                "P. Mokhtari"
            ],
            "title": "Voice quality: The 4th prosodic dimension",
            "venue": "15th Intl. Congress of Phonetic Sciences, 2003.",
            "year": 2003
        },
        {
            "authors": [
                "A.M.C. Sluijter",
                "V.J. van Heuven"
            ],
            "title": "Spectral balance as an acoustic correlate of linguistic stress",
            "venue": "J. Acoust. Soc. Am., vol. 100 4 Pt 1, pp. 2471\u20132485, 1996.",
            "year": 1996
        },
        {
            "authors": [
                "T. Raitio",
                "R. Rasipuram",
                "D. Castellani"
            ],
            "title": "Controllable neural text-to-speech synthesis using intuitive prosodic features",
            "venue": "Interspeech, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "T. Raitio",
                "J. Li",
                "S. Seshadri"
            ],
            "title": "Hierarchical prosody modeling and control in non-autoregressive parallel neural TTS",
            "venue": "ICASSP, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "J.-C. Junqua"
            ],
            "title": "The Lombard reflex and its role on human listeners and automatic speech recognizers",
            "venue": "J. Acoust. Soc. Am., vol. 93, no. 1, pp. 510\u2013524, 1993.",
            "year": 1993
        },
        {
            "authors": [
                "D. Rostolland"
            ],
            "title": "Acoustic features of shouted voice",
            "venue": "Acustica, vol. 50, pp. 118\u2013125, 1982.",
            "year": 1982
        },
        {
            "authors": [
                "W.V. Summers",
                "D. Pisoni",
                "R. Bernacki",
                "R. Pedlow",
                "M. Stokes"
            ],
            "title": "Effects of noise on speech production: Acoustic and perceptual analyses",
            "venue": "J. Acoust. Soc. Am., vol. 84, no. 3, pp. 917\u2013 928, 1988.",
            "year": 1988
        },
        {
            "authors": [
                "E. Godoy",
                "M. Koutsogiannaki",
                "Y. Stylianou"
            ],
            "title": "Approaching speech intelligibility enhancement with inspiration from Lombard and clear speaking styles",
            "venue": "Computer Speech & Language, vol. 28, no. 2, pp. 629\u2013647, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "D. O\u2019Shaughnessy"
            ],
            "title": "Speech communications: Human and Machine, 2nd Edition",
            "year": 2000
        },
        {
            "authors": [
                "C.V. Botinhao",
                "J. Yamagishi",
                "S. King",
                "R. Maia"
            ],
            "title": "Intelligibility enhancement of HMM-generated speech in additive noise by modifying Mel cepstral coefficients to increase the Glimpse Proportion",
            "venue": "Computer Speech & Language, vol. 28, no. 2, pp. 665\u2013686, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "P.N. Petkov",
                "W.B. Kleijn"
            ],
            "title": "Spectral dynamics recovery for enhanced speech intelligibility in noise",
            "venue": "IEEE Trans. Audio Speech Lang. Proc., vol. 23, no. 2, pp. 327\u2013338, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "C. Chermaz",
                "S. King"
            ],
            "title": "A sound engineering approach to near end listening enhancement",
            "venue": "Interspeech, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "M. Cooke",
                "C. Mayo",
                "C.V. Botinhao",
                "Y. Stylianou",
                "B. Sauert",
                "Y. Tang"
            ],
            "title": "Evaluating the intelligibility benefit of speech modifications in known noise conditions",
            "venue": "Speech Comm., vol. 55, pp. 572\u2013585, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "P.S. Chanda",
                "S. Park"
            ],
            "title": "Speech intelligibility enhancement using tunable equalization filter",
            "venue": "ICASSP, 2007.",
            "year": 2007
        },
        {
            "authors": [
                "B. Sauert",
                "P. Vary"
            ],
            "title": "Near end listening enhancement optimized with respect to speech intelligibility index and audio power limitations",
            "venue": "Proc. Europ. Sig. Proc. Conf., 2010, pp. 1919\u20131923.",
            "year": 2010
        },
        {
            "authors": [
                "C. Taal",
                "J. Jensen",
                "A. Leijon"
            ],
            "title": "On optimal linear filtering of speech for near-end listening enhancement",
            "venue": "IEEE Sig. Proc. Letters, vol. 20, no. 3, pp. 225\u2013228, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "D. Paul",
                "M.P. Shifas",
                "Y. Pantazis",
                "Y. Stylianou"
            ],
            "title": "Enhancing speech intelligibility in text-to-speech synthesis using speaking style conversion",
            "venue": "Interspeech, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "G. Li",
                "R. Hu",
                "S. Ke",
                "R. Zhang",
                "X. Wang",
                "L. Gao"
            ],
            "title": "Speech intelligibility enhancement using non-parallel speaking style conversion with StarGAN and dynamic range compression",
            "venue": "ICME, 2020, pp. 1\u20136.",
            "year": 2020
        },
        {
            "authors": [
                "J. Shen",
                "R. Pang",
                "R.J. Weiss",
                "M. Schuster",
                "N. Jaitly",
                "Z. Yang",
                "Z. Chen",
                "Y. Zhang",
                "Y. Wang",
                "R. Skerry-Ryan",
                "R.A. Saurous",
                "Y. Agiomyrgiannakis",
                "Y. Wu"
            ],
            "title": "Natural TTS synthesis by conditioning WaveNet on Mel spectrogram predictions",
            "venue": "ICASSP, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "W. Ping",
                "K. Peng",
                "A. Gibiansky",
                "S.O. Arik",
                "A. Kannan",
                "S. Narang",
                "J. Raiman",
                "J. Miller"
            ],
            "title": "Deep Voice 3: 2000-speaker neural text-to-speech",
            "venue": "ICLR, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "Y. Ren",
                "C. Hu",
                "X. Tan",
                "T. Qin",
                "S. Zhao",
                "Z. Zhao",
                "T. Liu"
            ],
            "title": "FastSpeech 2: Fast and high-quality end-to-end text to speech",
            "venue": "arXiv:2006.04558, 2020.",
            "year": 2006
        },
        {
            "authors": [
                "A. van den Oord",
                "S. Dieleman",
                "H. Zen",
                "K. Simonyan",
                "O. Vinyals",
                "A. Graves",
                "N. Kalchbrenner",
                "A. Senior",
                "K. Kavukcuoglu"
            ],
            "title": "Wavenet: A generative model for raw audio",
            "venue": "SSW9, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "N. Kalchbrenner",
                "E. Elsen",
                "K. Simonyan",
                "S. Noury",
                "N. Casagrande",
                "E. Lockhart",
                "F. Stimberg",
                "A. van den Oord",
                "S. Dieleman",
                "K. Kavukcuoglu"
            ],
            "title": "Efficient neural audio synthesis",
            "venue": "arXiv:1802.08435, 2018.",
            "year": 1802
        },
        {
            "authors": [
                "IEEE Subcommittee on Subjective Measurements"
            ],
            "title": "IEEE recommended practices for speech quality measurements",
            "venue": "IEEE Transactions on Audio and Electroacoustics, vol. 17, pp. 227\u201346, 1969.",
            "year": 1969
        },
        {
            "authors": [
                "S. Achanta",
                "A. Antony",
                "L. Golipour",
                "J. Li",
                "T. Raitio",
                "R. Rasipuram",
                "F. Rossi",
                "J. Shi",
                "J. Upadhyay",
                "D. Winarsky",
                "H. Zhang"
            ],
            "title": "Ondevice neural speech synthesis",
            "venue": "ASRU, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "ITU-T"
            ],
            "title": "Objective measurement of active speech level",
            "venue": "Recommendation P.56, 1993.",
            "year": 1993
        },
        {
            "authors": [
                "D. Bates",
                "M. M\u00e4chler",
                "B. Bolker",
                "S. Walker"
            ],
            "title": "Fitting linear mixed-effects models using lme4",
            "venue": "Journal of Statistical Software, vol. 67, no. 1, pp. 1\u201348, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "V. R"
            ],
            "title": "Lenth, emmeans: Estimated marginal means, aka leastsquares means, 2022, R package version 1.7.2",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "The intelligibility of natural and synthetic speech decreases in adverse listening conditions [1, 2, 3]. To improve speech intelligibility in the presence of noise, humans adapt their voice, referred to as the Lombard effect [4, 5], by increasing the intensity of the signal, and therefore the signal-to-noise ratio (SNR), and by modifying voice characteristics as a result of increased vocal effort [6, 7]. Change in the vocal effort can also be triggered by the need to communicate over a distance [8] or a change in emotional expression [9, 10]. Similarly, it is desirable for speech synthesizers to adapt to adverse environments and different contexts for improved intelligibility and more natural delivery of the message. Increasing the signal intensity increases the SNR and intelligibility, but this approach alone is not ideal for either the listener (discomfort) or the output device (power consumption, mechanical limits). Therefore, a preferable approach is to change the speech characteristics to increase the intelligibility, also called near-end listening enhancement [11, 12, 13, 14]. This is especially important for text-to-speech (TTS) where the intelligibility is generally lower than in natural speech [3].\nAssuming an equal power constraint, there are two main approaches that have been successfully used to improve the intelligibility of synthetic speech in noise. First, using signal processing techniques, such as spectral shaping and dynamic range compression [15, 14], to shift the signal power to frequencies that are most relevant for intelligibility and to amplify low-energy speech sounds, such as plosives and stops. Second, mimicking human speech production in noise, which can be achieved by training or adapting a TTS synthesizer with specifi-\ncally recorded Lombard or high vocal effort speech [16, 17, 18]. Both approaches have been shown to improve the intelligibility of speech in noise. The signal processing based methods have the benefit that no additional speech data are required to enhance the intelligibility, and the intelligibility gains can be substantial [1, 2]. On the other hand, modeling vocal effort in speech synthesis not only increases the intelligibility of speech in noise, but also makes the synthetic voice more natural and more suitable for the listening condition [16, 17]. However, specifically recorded Lombard or high vocal effort speech data is usually required for training the TTS system.\nTo overcome the limitations of requiring specifically recorded and labeled Lombard or high vocal effort speech, we propose a method that automatically estimates the vocal effort level from normal speech data using spectral tilt [19, 20]. The natural variation of vocal effort is then modeled and controlled at synthesis time. Moreover, the model is able to extrapolate vocal effort level from the observed neutrally spoken speech data to reproduce speech with very low (soft) or high (loud) vocal effort. Also, the vocal effort modification at synthesis time has minimal effect on other prosodic factors, such as duration and pitch. Finally, the proposed method provides additional benefits, such as overall control of the prosodic space independently from other prosodic factors [21, 22], and higher naturalness in comparison to signal processing based methods1."
        },
        {
            "heading": "2. Background",
            "text": "The ability of speakers to adapt the speech generation process to the environment has been studied thoroughly in the past. We summarize some of the main findings, relevant to this work, in Sec. 2.1. The results of such studies have contributed to developing various techniques for artificially enhancing the intelligibility of natural and synthetic speech signals. To provide the context for evaluating vocal effort adjustment for intelligibility enhancement, we summarize the relevant methods in Sec. 2.2."
        },
        {
            "heading": "2.1. Speech modification for enhanced intelligibility",
            "text": "Changes in vocal effort induced by background noise are referred to as the Lombard effect [4, 23, 5]. Communication over a distance [8], as well as emotional expression [9, 10] also modulate vocal effort as a means of efficient message delivery. The associated effects on naturally-produced speech have been studied, e.g., in [24, 25, 23, 8].\nKey speech characteristics affected in the process of modulating vocal effort include pitch, duration, intensity and spectral tilt [4]. Isolating each one of these in turn and probing its impact on intelligibility in background noise has shown that pitch and duration do not have a significant contribution [26, 2]. Such changes are explained in view of physiological constraints in the voice production mechanism. Intensity and spectral tilt, on\n1Speech samples can be found at https://apple.github. io/vocal-effort-modeling-tts-intelligibility/.\nar X\niv :2\n20 3.\n10 63\n7v 2\n[ ee\nss .A\nS] 2\n9 M\nar 2\n02 2\nthe other hand, have a direct and significant impact on human ability to understand speech in noise.\nIn the case of intensity, intelligibility increases as a result of an overall increase in SNR. Reduction of spectral tilt, on the other hand, boosts the mid and high frequencies at the expense of the low frequencies. This provides an increased amount of intelligibility cues to the listener due to: 1) typical low-pass characteristics of environmental noise, and 2) the higher importance of the mid relative to the low frequency range of the speech signal [27]. Fixing the average intensity and exploring time-variant as well as time-invariant frequency-dependent speech modifications has produced a numbers of methods for intelligibility enhancement. Key methods, from the perspective of TTS, are summarized in the following section."
        },
        {
            "heading": "2.2. Methods for intelligibility enhancement of TTS signals",
            "text": "Intelligibility enhancement of TTS can be performed at synthesis time, e.g., by parametrizing and adjusting properties of the speech generation process as a function of environment, which is the approach explored in this work. Alternatively, it can be achieved by post-processing a pre-synthesized signal. While any existing method for intelligibility enhancement can be applied in the latter case, the quality of synthetic speech must be sufficiently high to prevent signal quality degradation [28]. State-of-the-art neural TTS satisfies the requirement for quality and is suitable for combination with post-processing methods.\nOne of the more successful post-processing methods combines dynamic range compression (DRC) with high-pass filtering [15]. More recently, spectral shaping and dynamic range compression (SSDRC) similarly exploits the combination of a linear time-invariant (LTI) filter and DRC [14]. A multi-band parametric time-varying DRC model recovering the spectral dynamics of the signal is presented in [29]. A multi-band timeinvariant DRC approach tuned to mitigate the combined effect of noise and reverberation is considered in [30].\nLarge-scale evaluations through headphones indicate that DRC-based techniques outperform alternative methodologies in terms of intelligibility gain [31, 2]. On the other hand, LTI and linear time variant (LTV) systems offer the benefit of lower signal processing distortion and robustness to noise in the input signal. Thus, where DRC would decrease SNR in the input signal, LTI and LTV would preserve it. Among others, some recently-proposed techniques are found in [32, 33, 34].\nIntelligibility enhancement at synthesis time in the parametric space of a neural TTS system is a novel paradigm and differs from prior art due to its capability to produce both linear (filtering) and non-linear (DRC) signal modifications by non-linear transformations (neural networks). Existing work in this domain relies on reference training data, which is either created with a pre-existing signal-processing approach [35], or taken from a categorical speaking style such as clear or Lombard speech [16, 17, 36, 18]. The effectiveness of such methods is constrained by the quality and amount of the training data and the capability of the neural network model for adaptation. To our knowledge, this work is the first example of neural TTS providing both: 1) the capability for parametric adaptation of TTS synthesis to environmental conditions, and 2) the possibility of adaptation without dedicated and labeled training data."
        },
        {
            "heading": "3. Modeling of vocal effort in neural TTS",
            "text": "In this section, we describe the measurement of vocal effort using spectral tilt, and how that can be used in the modeling of\nvocal effort in neural TTS."
        },
        {
            "heading": "3.1. Measuring vocal effort using spectral tilt",
            "text": "To model vocal effort without changing other prosodic aspects of speech, we extract and model the following set of prosodic features, averaged per utterance: pitch, pitch range, phone duration, speech energy, and spectral tilt. These features are easy to calculate and are robust against background noise or other recordings conditions. These features are also disentangled so that they can be varied independently [21, 22].\nThe extraction of the acoustic features is described in [22], however, we will describe the extraction of spectral tilt here in detail. First, we resample the speech signal to 16 kHz and use a high-pass filter with a cut-off frequency of 70 Hz to remove any low-frequency fluctuations. Then we calculate the frame-wise spectral tilt of voiced speech using the predictor coefficient of a first order all-pole filter. The spectral tilt values are averaged per utterance and then normalized to [\u22121,1] by first calculating the median (M ) and the standard deviation (\u03c3) and then projecting the data in the range [M\u22123\u03c3, M+3\u03c3] into [\u22121,1]. Finally, we clip values |x| > 1 so that all data is in the range [\u22121,1]."
        },
        {
            "heading": "3.2. Neural TTS architecture",
            "text": "A typical neural text-to-speech (TTS) system consists of two components: a neural front-end [37, 38, 39] that maps from phoneme input to Mel-spectrograms, and a neural back-end [40, 41] that maps from the Mel-spectrogram into a sequence of speech samples. These two networks are trained with a large amount of speech data to generate high-quality speech. Since prosody and vocal effort are mainly modeled by the neural front-end, this paper will focus on the front-end architecture.\nPreviously, we presented hierarchical prosody modeling and control using Tacotron 2 [21] and non-autoregressive parallel TTS [22], both including spectral tilt modeling. In this work we focus specifically on modeling vocal effort using spectral tilt and its effect on speech intelligibility in noise.\nThe proposed front-end architecture, shown in Fig. 1, is similar to FastSpeech 2 [39], but we extend the model with a new variance adaptor at the utterance level [22]. We predict the utterance-wise prosodic features from the encoder outputs, namely, pitch, pitch range, duration, energy, and spectral tilt. The architecture of the new utterance-level variance adaptor is\nidentical to the phone-level adaptors except that we predict all five features using the same module. We use teacher-forcing of both utterance-wise and phone-wise prosodic features using ground-truth targets to efficiently train the network. At synthesis time, vocal effort control is achieved by adding bias to the spectral tilt predictions while keeping the other prosodic features unchanged to minimize undesired changes in prosody. Factorizing the prosody and only changing intended prosodic aspects is crucial in order to achieve desired changes with high quality. We observed overall lower synthesis quality with models that were conditioned only with spectral tilt due to the high correlation between speaking style and spectral tilt."
        },
        {
            "heading": "4. Experiments",
            "text": ""
        },
        {
            "heading": "4.1. Data",
            "text": "We use data from two American English speakers, a highpitched speaker with 36-hour dataset (Voice 1), and a lowpitched speaker with 23-hour dataset (Voice 2). All speech data were sampled at 24 kHz. For evaluating the intelligibility and quality of synthetic speech, we used the 720 phonetically balanced Harvard sentences [42] as the source text for synthesis."
        },
        {
            "heading": "4.2. Model training",
            "text": "We trained the proposed vocal effort control models for both voices. We use phone-wise duration, pitch, and energy as the fine-grained features, and utterance-wise pitch, pitch range, phone duration, speech energy, and spectral tilt as the higherlevel prosodic features. 80-dimensional Mel-spectrograms are computed from pre-emphasized speech using STFT with 25 ms frame length and 10 ms shift. The encoder has 4 feed-forward Transformer layers each with a self-attention layer having 2 attention heads and 256 hidden units, and two 1-D convolution layers each having a kernel size of 9 and 1024 filters. The decoder has 2 dilated convolution blocks with six 1-D convolution layers with dilation rates of 1, 2, 4, 8, 16 and 32, respectively, kernel size of 3, and 256 filters. The feature predictors have two 1-D convolution layers each having a kernel size of 3 and 256 filters. The dropout rate of 0.2 and for layer normalization of 10\u22126 were used. We trained the models for 300k steps using 16 GPUs and a batch size of 512. We use WaveRNN [41, 43] to generate speech from the Mel-spectrograms, trained separately for each speaker. The [M\u22123\u03c3, M+3\u03c3] spectral tilt values for Voice 1 and 2 are [\u22120.984, \u22120.926] and [\u22120.990, \u22120.931], respectively. These are mapped to [\u22121,1], where a positive value represents flatter spectral tilt and thus higher vocal effort."
        },
        {
            "heading": "4.3. Systems",
            "text": "We use the following systems in our evaluation: 1. Baseline: Synthetic speech without modification [22]. 2. Proposed: Synthetic speech with increased vocal effort. 3. SS: Baseline processed with spectral shaping (SS) [14]. 4. SSDRC: Baseline processed with spectral shaping and dynamic range compression (SSDRC) [14]. The baseline represents high-quality neural TTS [22] without any adaptation to listening conditions. The proposed system uses an increased vocal effort of +3, which means that the aimed vocal effort is 9\u03c3 higher than the median of the data. To compare the effectiveness of the proposed method to traditional approaches, we integrated two post-processing intelligibility enhancement modules to the baseline system, SS and SSDRC, which have been shown to provide state-of-the-art results [1, 2].\nThe SS algorithm comprises three-stage filtering of the input speech in the short-time Fourier transform (STFT) domain. Two of the filtering stages perform phoneme-adaptive spectral shaping based on the probability of voicing, while the final filter is a fixed spectral shaper. On the adaptive spectral shaping, the local maxima (reflective of formants) are sharpened by a spectral sharpening filter followed by a high-frequency booster. Both of the filters update their coefficients based on the voicing probability of each frame. A non-adaptive pre-emphasis filter then modifies the spectra by enhancing the frequency components between 1000 Hz and 4000 Hz by a factor of 12 dB, while reducing the energy for the frequencies below 500 Hz by 6 dB/octave. Inverse Fourier transform and overlap-add are applied to get the final spectral-shaped waveform.\nThe DRC of the SSDRC is a time-domain operation where the objective is to reduce the envelope variation of speech. This is done by modifying the speech waveform in each frame, adaptive to the temporal envelopes. DRC is also a two-step process. In the first stage, the time-domain envelope is dynamically compressed with recursive smoothing. The smoothed envelope is projected onto the input-output envelope characteristic (IOEC) curve which gives the DRC gain. Finally, the spectral-shaped waveform from the SS module is multiplied by the estimated gains in the DRC to generate the SSDRC waveform."
        },
        {
            "heading": "4.4. Objective measures",
            "text": "We used the 720 Harvard utterances [42] to synthesize speech from each system and voice. In addition, to demonstrate the change in the vocal effort in the proposed system, we generated synthetic speech with various target spectral tilt values from -3 to +3, and then measured the output spectral tilt. The results, illustrated in Fig. 2, show that the (normalized) spectral tilt, and therefore the vocal effort, increases as the target vocal effort is increased. However, the SS and SSDRC systems have higher (flatter) spectral tilt in comparison to the proposed TTS system with adjusted spectral tilt of +3. We also calculated the longterm average spectra (LTAS) over all the speech samples for each system and voice. The LTAS, shown in Fig. 3, illustrate that all methods show a shift of energy from low to mid and high frequencies in comparison to the baseline."
        },
        {
            "heading": "4.5. Speech intelligibility in noise",
            "text": "We generated speech samples for each system and each speaker using the 720 the Harvard utterances [42]. We used two types of masking noises in the test: a fluctuating masker using competing speaker (CS) and a stationary masker using speech shaped noise (SSN). The CS masker was recorded read speech with alternate voice (Voice 2 for evaluating Voice 1, and vice versa) with SNRs of -7, -14, and -21 dB. The SSN masker was con-\nstructed by measuring the LTAS of the alternate voice with linear prediction and then filtering white noise with the resulting spectrum. The SNRs used were 1, -4, and -9 dB. The active speech level of the target samples were normalized according to ITU-T P.56 standard [44] before adding the masking noise according to the SNRs. The target speech content was centrally embedded in the noise with 0.5 seconds of lead and lag time. Overall, the test design was similar to [1].\nWe organized a large intelligibility test where test subjects used a graphical UI and were asked to listen to a speech sample in noise only once, and then type what they heard. Each listener was restricted to evaluate only a maximum of 720 utterances so that they never heard the same target utterance twice. The total number of samples in the test was 34,560, consisting of 2 voices \u00d7 4 systems \u00d7 2 noise types \u00d7 3 SNRs \u00d7 720 utterances. We added 10 reference speech samples without noise for each listener to evaluate their performance. The listeners had to achieve 80% word recognition rate (WRR) on the reference samples and more than 10% WRR on the test samples in order to qualify.\nA total of 29,520 ratings were given by 41 qualified listeners using earbuds (47%), headphones (38%), or loudspeakers (16%). The WRR was measured as the proportion of content words that were recognized correctly and in the right order. The intelligibility test results are shown in Fig. 4.\nLinear mixed effects models with system as the fixed effect and voice, listener, utterance and audio device as random intercepts were fit to the data using the lme4 package [45]. Comparing models with and without system, a likelihood ratio test (\u03c72 = 1105.2, p<.0001) and a drop in Akaike information criterion showed that system significantly predicted WRR. Dunnett\u2019s multiple comparison test from the emmeans package [46] shows that the proposed system had significantly better intelligibility than baseline and SS on average (p<.0001), while SSDRC had significantly better intelligibility (p<.0001) on average than\nother systems. As expected, the audio device had a clear effect on intelligibility, with the following average WRRs: earbuds: 37.9\u00b10.6 %, headphones 42.5\u00b10.7 %, speakers: 32.8\u00b11.0 %."
        },
        {
            "heading": "4.6. Quality without noise",
            "text": "To measure the effect of the intelligibility enhancement on the overall quality of speech without noise, we performed a mean opinion score (MOS) test with each of the 4 systems and 2 speakers using the 720 Harvard sentences. A 5-point MOS test was performed by 68 individual American English native speakers (51 female, 17 male, 18-69 years old) using either headphones (33) or loudspeakers (35). Listeners provided 5 ratings per utterance, resulting in a total of 28,800 responses.\nThe results are shown in Fig. 5. Linear mixed effects models with system and voice as fixed effects and listener and utterance as random intercepts were fit to the data. Likelihood ratio tests showed no significant interaction between system and voice (p=.57), and significant effects of system (p<.0001). Pairwise comparisons using the Tukey adjustment method showed significant differences between all systems (p<.0001). In the absence of noise, synthetic speech without any modification is preferred by listeners, which we would expect, but the proposed method was significantly preferred above other intelligibility enhancement methods (across both voices)."
        },
        {
            "heading": "5. Discussion and conclusions",
            "text": "The focus of our experiment was on the average effect of intelligibility, and we do observe a consistent directional improvement for the proposed method across SNRs for both voices. Nonetheless, the data suggest that the strength of the effect may be contextual. In particular, the difference of most systems from baseline seems to generally be higher for Voice 1 than Voice 2. It may be that intelligibility enhancement methods work better with some voices than others. Our experiment was designed with sufficient power to detect main effects only, as that is the focus of this paper, and thus any exploration of the effect of context or voice is more speculative and worthy of exploration in future work. Also, developing a framework for automatically determining optimal vocal effort level with respect to masking noise type and level is left for future work.\nWe proposed a neural TTS system that models the natural vocal effort variation in the training data without specifically designed or labeled database. We use spectral tilt to measure vocal effort, and condition our models with spectral tilt among other prosodic features to factorize the prosodic space. By altering the input spectral tilt at inference time, we can successfully control the vocal effort of synthetic speech. Our experiments show that the proposed system achieved significantly higher word recognition rate than the baseline, while in the absence of noise, the proposed method achieves higher quality than other intelligibility enhancement methods."
        },
        {
            "heading": "6. References",
            "text": "[1] M. Cooke, C. Mayo, and C. Valentini-Botinhao, \u201cIntelligibility-\nenhancing speech modifications: The Hurricane Challenge,\u201d in Interspeech, 2013.\n[2] J. Rennies, H. Schepker, C. Valentini-Botinhao, and M. Cooke, \u201cIntelligibility-enhancing speech modifications \u2013 The Hurricane Challenge 2.0,\u201d in Interspeech, 2020.\n[3] B. Langner and A. W. Black, \u201cImproving the understandability of speech synthesis by modeling speech in noise,\u201d in ICASSP, 2005.\n[4] E. Lombard, \u201cLe signe de l\u2019elevation de la voix,\u201d Ann. Maladies Oreille, Larynx, Nez, Pharynx, vol. 37, no. 101\u2013119, p. 25, 1911.\n[5] M. Cooke and Y. Lu, \u201cSpectral and temporal changes to speech produced in the presence of energetic and informational maskers,\u201d J. Acoust. Soc. Am., vol. 128, pp. 2059\u201369, 10 2010.\n[6] B. Lindblom, Explaining Phonetic Variation: A Sketch of the H&H Theory. Springer Netherlands, 1990, pp. 403\u2013439.\n[7] \u2014\u2014, \u201cRole of articulation in speech perception: Clues from production,\u201d The Journal of the Acoustical Society of America, vol. 99, no. 3, pp. 1683\u20131692, 1996.\n[8] H. Traunmu\u0308ller and A. Eriksson, \u201cAcoustic effects of variation in vocal effort by men, women, and children,\u201d J. Acoust. Soc. Am., vol. 107, no. 6, pp. 3438\u20133451, 2000.\n[9] C. T. Ishi, H. Ishiguro, and N. Hagita, \u201cAnalysis of the roles and the dynamics of breathy and whispery voice qualities in dialogue speech,\u201d EURASIP J. Audio Speech Music Process., vol. 3, pp. 1\u201312, 2010.\n[10] C. Gobl and A. N\u0131\u0301 Chasaide, \u201cThe role of voice quality in communicating emotion, mood and attitude,\u201d Speech Comm., vol. 40, no. 1\u20132, pp. 189\u2013212, 2003.\n[11] B. Sauert and P. Vary, \u201cNear end listening enhancement: Speech intelligibility improvement in noisy environments,\u201d in ICASSP, 2006.\n[12] M. Skowronski and J. Harris, \u201cApplied principles of clear and Lombard speech for automated intelligibility enhancement in noisy environments,\u201d Speech Comm., vol. 48, pp. 549\u2013558, 2006.\n[13] S. D. Yoo, J. R. Boston, A. El-Jaroudi, C.-C. Li, J. D. Durrant, K. Kovacyk, and S. Shaiman, \u201cSpeech signal modification to increase intelligibility in noisy environments,\u201d J. Acoust. Soc. Am., vol. 122, no. 2, pp. 1138\u20131149, 2007.\n[14] T.-C. Zorila\u0306, V. Kandia, and Y. Stylianou, \u201cSpeech-in-noise intelligibility improvement based on spectral shaping and dynamic range compression,\u201d in Interspeech, 2012.\n[15] R. J. Niederjohn and J. H. Grotelueschen, \u201cThe enhancement of speech intelligibility in high noise levels by high-pass filtering followed by rapid amplitude compression,\u201d IEEE Trans. Acous., Speech and Sig. Proc., vol. 24, no. 4, pp. 277\u2013282, 1976.\n[16] T. Raitio, A. Suni, M. Vainio, and P. Alku, \u201cAnalysis of HMMbased Lombard speech synthesis,\u201d in Interspeech, 2011.\n[17] \u2014\u2014, \u201cSynthesis and perception of breathy, normal, and Lombard speech in the presence of noise,\u201d Computer Speech & Language, vol. 28, no. 2, pp. 648\u2013664, 2014.\n[18] Q. Hu, T. Bleisch, P. Petkov, T. Raitio, E. Marchi, and V. Lakshminarasimhan, \u201cWhispered and lombard neural speech synthesis,\u201d in 2021 IEEE Spoken Language Technology Workshop (SLT), 2021.\n[19] N. Campbell and P. Mokhtari, \u201cVoice quality: The 4th prosodic dimension,\u201d in 15th Intl. Congress of Phonetic Sciences, 2003.\n[20] A. M. C. Sluijter and V. J. van Heuven, \u201cSpectral balance as an acoustic correlate of linguistic stress,\u201d J. Acoust. Soc. Am., vol. 100 4 Pt 1, pp. 2471\u20132485, 1996.\n[21] T. Raitio, R. Rasipuram, and D. Castellani, \u201cControllable neural text-to-speech synthesis using intuitive prosodic features,\u201d in Interspeech, 2020.\n[22] T. Raitio, J. Li, and S. Seshadri, \u201cHierarchical prosody modeling and control in non-autoregressive parallel neural TTS,\u201d in ICASSP, 2022.\n[23] J.-C. Junqua, \u201cThe Lombard reflex and its role on human listeners and automatic speech recognizers,\u201d J. Acoust. Soc. Am., vol. 93, no. 1, pp. 510\u2013524, 1993.\n[24] D. Rostolland, \u201cAcoustic features of shouted voice,\u201d Acustica, vol. 50, pp. 118\u2013125, 1982.\n[25] W. V. Summers, D. Pisoni, R. Bernacki, R. Pedlow, and M. Stokes, \u201cEffects of noise on speech production: Acoustic and perceptual analyses,\u201d J. Acoust. Soc. Am., vol. 84, no. 3, pp. 917\u2013 928, 1988. [26] E. Godoy, M. Koutsogiannaki, and Y. Stylianou, \u201cApproaching speech intelligibility enhancement with inspiration from Lombard and clear speaking styles,\u201d Computer Speech & Language, vol. 28, no. 2, pp. 629\u2013647, 2014. [27] D. O\u2019Shaughnessy, Speech communications: Human and Machine, 2nd Edition. IEEE, 2000. [28] C. V. Botinhao, J. Yamagishi, S. King, and R. Maia, \u201cIntelligibility enhancement of HMM-generated speech in additive noise by modifying Mel cepstral coefficients to increase the Glimpse Proportion,\u201d Computer Speech & Language, vol. 28, no. 2, pp. 665\u2013686, 2013. [29] P. N. Petkov and W. B. Kleijn, \u201cSpectral dynamics recovery for enhanced speech intelligibility in noise,\u201d IEEE Trans. Audio Speech Lang. Proc., vol. 23, no. 2, pp. 327\u2013338, 2015. [30] C. Chermaz and S. King, \u201cA sound engineering approach to near end listening enhancement,\u201d in Interspeech, 2020. [31] M. Cooke, C. Mayo, C. V. Botinhao, Y. Stylianou, B. Sauert, and Y. Tang, \u201cEvaluating the intelligibility benefit of speech modifications in known noise conditions,\u201d Speech Comm., vol. 55, pp. 572\u2013585, 2013. [32] P. S. Chanda and S. Park, \u201cSpeech intelligibility enhancement using tunable equalization filter,\u201d in ICASSP, 2007. [33] B. Sauert and P. Vary, \u201cNear end listening enhancement optimized with respect to speech intelligibility index and audio power limitations,\u201d in Proc. Europ. Sig. Proc. Conf., 2010, pp. 1919\u20131923. [34] C. Taal, J. Jensen, and A. Leijon, \u201cOn optimal linear filtering of speech for near-end listening enhancement,\u201d IEEE Sig. Proc. Letters, vol. 20, no. 3, pp. 225\u2013228, 2013. [35] D. Paul, M. P. Shifas, Y. Pantazis, and Y. Stylianou, \u201cEnhancing speech intelligibility in text-to-speech synthesis using speaking style conversion,\u201d in Interspeech, 2020. [36] G. Li, R. Hu, S. Ke, R. Zhang, X. Wang, and L. Gao, \u201cSpeech intelligibility enhancement using non-parallel speaking style conversion with StarGAN and dynamic range compression,\u201d in ICME, 2020, pp. 1\u20136. [37] J. Shen, R. Pang, R. J. Weiss, M. Schuster, N. Jaitly, Z. Yang, Z. Chen, Y. Zhang, Y. Wang, R. Skerry-Ryan, R. A. Saurous, Y. Agiomyrgiannakis, and Y. Wu, \u201cNatural TTS synthesis by conditioning WaveNet on Mel spectrogram predictions,\u201d in ICASSP, 2017. [38] W. Ping, K. Peng, A. Gibiansky, S. O. Arik, A. Kannan, S. Narang, J. Raiman, and J. Miller, \u201cDeep Voice 3: 2000-speaker neural text-to-speech,\u201d in ICLR, 2018. [39] Y. Ren, C. Hu, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T. Liu, \u201cFastSpeech 2: Fast and high-quality end-to-end text to speech,\u201d arXiv:2006.04558, 2020. [40] A. van den Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior, and K. Kavukcuoglu, \u201cWavenet: A generative model for raw audio,\u201d in SSW9, 2016. [41] N. Kalchbrenner, E. Elsen, K. Simonyan, S. Noury, N. Casagrande, E. Lockhart, F. Stimberg, A. van den Oord, S. Dieleman, and K. Kavukcuoglu, \u201cEfficient neural audio synthesis,\u201d arXiv:1802.08435, 2018. [42] IEEE Subcommittee on Subjective Measurements, \u201cIEEE recommended practices for speech quality measurements,\u201d IEEE Transactions on Audio and Electroacoustics, vol. 17, pp. 227\u201346, 1969. [43] S. Achanta, A. Antony, L. Golipour, J. Li, T. Raitio, R. Rasipuram, F. Rossi, J. Shi, J. Upadhyay, D. Winarsky, and H. Zhang, \u201cOndevice neural speech synthesis,\u201d in ASRU, 2021. [44] ITU-T, \u201cObjective measurement of active speech level,\u201d Recommendation P.56, 1993. [45] D. Bates, M. Ma\u0308chler, B. Bolker, and S. Walker, \u201cFitting linear mixed-effects models using lme4,\u201d Journal of Statistical Software, vol. 67, no. 1, pp. 1\u201348, 2015. [46] R. V. Lenth, emmeans: Estimated marginal means, aka leastsquares means, 2022, R package version 1.7.2."
        }
    ],
    "title": "Vocal effort modeling in neural TTS for improving the intelligibility of synthetic speech in noise",
    "year": 2022
}