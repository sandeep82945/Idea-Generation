{
    "abstractText": "Neural operators as novel neural architectures for fast approximating solution operators of partial differential equations (PDEs), have shown considerable promise for future scientific computing. However, the mainstream of training neural operators is still data-driven, which needs an expensive ground-truth dataset from various sources (e.g., solving PDEs\u2019 samples with the conventional solvers, real-world experiments) in addition to training stage costs. From a computational perspective, marrying operator learning and specific domain knowledge to solve PDEs is an essential step for data-efficient and low-carbon learning. We propose a novel data-efficient paradigm that provides a unified framework of training neural operators and solving PDEs with the domain knowledge related to the variational form, which we refer to as the variational operator learning (VOL). We develop Ritz and Galerkin approach with finite element discretization for VOL to achieve matrix-free approximation of system functional and residual. We then propose direct minimization and iterative update as two possible optimization strategies. Various types of experiments based on reasonable benchmarks about variable heat source, Darcy flow, and variable stiffness elasticity are conducted to demonstrate the effectiveness of VOL. With a label-free training set and a 5-label-only shift set, VOL learns solution operators with its test errors decreasing in a power law with respect to the amount of unlabeled data. To the best of the authors\u2019 knowledge, this is the first study that integrates the perspectives of the weak form and efficient iterative methods for solving sparse linear systems into the end-to-end operator learning task.",
    "authors": [
        {
            "affiliations": [],
            "name": "Tengfei Xu"
        },
        {
            "affiliations": [],
            "name": "Dachuan Liu"
        },
        {
            "affiliations": [],
            "name": "Peng Hao"
        },
        {
            "affiliations": [],
            "name": "Bo Wang"
        }
    ],
    "id": "SP:673b95510244c68f181a66da3b09d25c113af29c",
    "references": [
        {
            "authors": [
                "Kaushik Bhattacharya",
                "Bamdad Hosseini",
                "Nikola B. Kovachki",
                "Andrew M. Stuart"
            ],
            "title": "Model reduction and neural networks for parametric pdes",
            "venue": "The SMAI journal of computational mathematics, 7:121\u2013157,",
            "year": 2021
        },
        {
            "authors": [
                "Ke Chen"
            ],
            "title": "Matrix preconditioning techniques and applications",
            "year": 2005
        },
        {
            "authors": [
                "Tianping Chen",
                "Hong Chen"
            ],
            "title": "Approximation capability to functions of several variables, nonlinear functionals, and operators by radial basis function neural networks",
            "venue": "IEEE Transactions on Neural Networks,",
            "year": 1995
        },
        {
            "authors": [
                "Tianping Chen",
                "Hong Chen"
            ],
            "title": "Universal approximation to nonlinear operators by neural networks with arbitrary activation functions and its application to dynamical systems",
            "venue": "IEEE Transactions on Neural Networks,",
            "year": 1995
        },
        {
            "authors": [
                "Yuyan Chen",
                "Bin Dong",
                "Jinchao Xu"
            ],
            "title": "Meta-mgnet: Meta multigrid networks for solving parameterized partial differential equations",
            "venue": "Journal of Computational Physics,",
            "year": 2022
        },
        {
            "authors": [
                "Chen Cui",
                "Kai Jiang",
                "Yun Liu",
                "Shi Shu"
            ],
            "title": "Fourier neural solver for large sparse linear algebraic systems",
            "venue": "arXiv preprint arXiv:2210.03881,",
            "year": 2022
        },
        {
            "authors": [
                "Weinan E",
                "Bing Yu"
            ],
            "title": "The deep Ritz method: A deep learning-based numerical algorithm for solving variational problems",
            "venue": "Communications in Mathematics and Statistics,",
            "year": 2018
        },
        {
            "authors": [
                "Michael Eldred",
                "Anthony Giunta",
                "S Collis"
            ],
            "title": "Second-order corrections for surrogate-based optimization with model hierarchies. In 10th AIAA/ISSMO multidisciplinary analysis and optimization conference",
            "year": 2004
        },
        {
            "authors": [
                "Han Gao",
                "Luning Sun",
                "Jian Xun Wang"
            ],
            "title": "PhyGeoNet: Physics-informed geometry-adaptive convolutional neural networks for solving parameterized steady-state pdes on irregular domain",
            "venue": "Journal of Computational Physics,",
            "year": 2021
        },
        {
            "authors": [
                "Somdatta Goswami",
                "Katiana Kontolati",
                "Michael D Shields",
                "George Em Karniadakis"
            ],
            "title": "Deep transfer operator learning for partial differential equations under conditional shift",
            "venue": "Nature Machine Intelligence, 4:1155\u20131164,",
            "year": 2022
        },
        {
            "authors": [
                "Somdatta Goswami",
                "Minglang Yin",
                "Yue Yu",
                "George Em Karniadakis"
            ],
            "title": "A physics-informed variational deeponet for predicting crack path in quasi-brittle materials",
            "venue": "Computer Methods in Applied Mechanics and Engineering,",
            "year": 2022
        },
        {
            "authors": [
                "Zafer G\u00fcrdal",
                "Reynaldo Olmedo"
            ],
            "title": "In-plane response of laminates with spatially varying fiber orientations: Variable stiffness concept",
            "venue": "AIAA Journal,",
            "year": 1993
        },
        {
            "authors": [
                "Zafer G\u00fcrdal",
                "Brian. F. Tatting",
                "C.K. Wu"
            ],
            "title": "Variable stiffness composite panels: Effects of stiffness variation on the in-plane and buckling response",
            "venue": "Composites Part A: Applied Science and Manufacturing, 39:911\u2013922,",
            "year": 2008
        },
        {
            "authors": [
                "Jiequn Han",
                "Linfeng Zhang",
                "Roberto Car",
                "Weinan E"
            ],
            "title": "Deep Potential: A general representation of a many-body potential energy surface",
            "venue": "Communications in Computational Physics,",
            "year": 2018
        },
        {
            "authors": [
                "Zhongkai Hao",
                "Chengyang Ying",
                "Zhengyi Wang",
                "Hang Su",
                "Yinpeng Dong",
                "Songming Liu",
                "Ze Cheng",
                "Jun Zhu",
                "Jian Song"
            ],
            "title": "GNOT: A general neural operator transformer for operator learning",
            "venue": "arXiv preprint arXiv:2302.14376,",
            "year": 2023
        },
        {
            "authors": [
                "Juncai He",
                "Jinchao Xu"
            ],
            "title": "MgNet: A unified framework of multigrid and convolutional neural network",
            "venue": "Science China Mathematics, 62:1331\u20131354,",
            "year": 2019
        },
        {
            "authors": [
                "Jan S Hesthaven",
                "Gianluigi Rozza",
                "Benjamin Stamm"
            ],
            "title": "Certified reduced basis methods for parametrized partial differential equations, volume",
            "year": 2016
        },
        {
            "authors": [
                "Mengcheng Huang",
                "Zongliang Du",
                "Chang Liu",
                "Yonggang Zheng",
                "Tianchen Cui",
                "Yue Mei",
                "Xiao Li",
                "Xiaoyu Zhang",
                "Xu Guo"
            ],
            "title": "Problem-independent machine learning (PIML)-based topology optimization\u2014a universal approach",
            "venue": "Extreme Mechanics Letters,",
            "year": 2022
        },
        {
            "authors": [
                "Xiang Huang",
                "Zhanhong Ye",
                "Hongsheng Liu",
                "Beiji Shi",
                "Zidong Wang",
                "Kang Yang",
                "Yan Li",
                "Bingya Weng",
                "Min Wang",
                "Haotian Chu",
                "Jing Zhou",
                "Fan Yu",
                "Bei Hua",
                "Lei Chen",
                "Bin Dong"
            ],
            "title": "Meta-autodecoder for solving parametric partial differential equations",
            "venue": "arXiv preprint arXiv:2111.08823,",
            "year": 2021
        },
        {
            "authors": [
                "Haoliang Jiang",
                "Zhenguo Nie",
                "Roselyn Yeo",
                "Amir Barati Farimani",
                "Levent Burak Kara"
            ],
            "title": "StressGAN: A generative deep learning model for two-dimensional stress distribution prediction",
            "venue": "Journal of Applied Mechanics, Transactions ASME,",
            "year": 2021
        },
        {
            "authors": [
                "Pengzhan Jin",
                "Shuai Meng",
                "Lu Lu"
            ],
            "title": "Mionet: Learning multiple-input operators via tensor product",
            "venue": "SIAM Journal on Scientific Computing,",
            "year": 2022
        },
        {
            "authors": [
                "Ayano Kaneda",
                "Osman Akar",
                "Jingyu Chen",
                "Victoria Kala",
                "David Hyde",
                "Joseph Teran"
            ],
            "title": "A deep conjugate direction method for iteratively solving linear systems",
            "venue": "arXiv preprint arXiv:2205.10763,",
            "year": 2022
        },
        {
            "authors": [
                "Jared Kaplan",
                "Sam McCandlish",
                "Tom Henighan",
                "Tom B. Brown",
                "Benjamin Chess",
                "Rewon Child",
                "Scott Gray",
                "Alec Radford",
                "Jeffrey Wu",
                "Dario Amodei"
            ],
            "title": "Scaling laws for neural language models",
            "year": 2001
        },
        {
            "authors": [
                "Reza Khodayi-Mehr",
                "Michael Zavlanos"
            ],
            "title": "VarNet: Variational neural networks for the solution of partial differential equations",
            "venue": "In Proceedings of the 2nd Conference on Learning for Dynamics and Control,",
            "year": 2020
        },
        {
            "authors": [
                "Georgios Kissas",
                "J S Seidman",
                "Leonardo Ferreira Guilhoto",
                "V\u0131\u0301ctor M Preciado",
                "George J Pappas",
                "Paris Perdikaris"
            ],
            "title": "Learning operators with coupled attention",
            "venue": "arXiv preprint arXiv:2201.01032,",
            "year": 2022
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Geoffrey E. Hinton"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "venue": "Technical report,",
            "year": 2009
        },
        {
            "authors": [
                "Samuel Lanthaler"
            ],
            "title": "Operator learning with pca-net: upper and lower complexity bounds",
            "venue": "arXiv preprint arXiv:2303.16317,",
            "year": 2023
        },
        {
            "authors": [
                "Zongyi Li",
                "Nikola Kovachki",
                "Kamyar Azizzadenesheli",
                "Burigede Liu",
                "Kaushik Bhattacharya",
                "Andrew Stuart",
                "Anima Anandkumar"
            ],
            "title": "Fourier neural operator for parametric partial differential equations",
            "venue": "arXiv preprint arXiv:2010.08895,",
            "year": 2020
        },
        {
            "authors": [
                "Zongyi Li",
                "Nikola B Kovachki",
                "Kamyar Azizzadenesheli",
                "Burigede Liu",
                "Kaushik Bhattacharya",
                "Andrew M Stuart",
                "Anima Anandkumar"
            ],
            "title": "Neural operator: Graph kernel network for partial differential equations",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Zongyi Li",
                "Nikola Borislavov Kovachki",
                "Chris Choy",
                "Boyi Li",
                "Jean Kossaifi",
                "Shourya Prakash Otta",
                "Mohammad Amin Nabian",
                "Maximilian Stadler",
                "Christian Hundt",
                "Kamyar Azizzadenesheli",
                "Anima Anandkumar"
            ],
            "title": "Geometry-informed neural operator for large-scale 3d pdes",
            "venue": "arXiv preprint arXiv:2309.00583,",
            "year": 2023
        },
        {
            "authors": [
                "Zongyi Li",
                "Hongkai Zheng",
                "Nikola Kovachki",
                "David Jin",
                "Haoxuan Chen",
                "Burigede Liu",
                "Kamyar Azizzadenesheli",
                "Anima Anandkumar"
            ],
            "title": "Physics-informed neural operator for learning partial differential equations",
            "venue": "arXiv preprint arXiv:2111.03794,",
            "year": 2021
        },
        {
            "authors": [
                "Lu Lu",
                "Pengzhan Jin",
                "Guofei Pang",
                "Zhongqiang Zhang",
                "George Em Karniadakis"
            ],
            "title": "Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators",
            "venue": "Nature Machine Intelligence, 3:218\u2013229,",
            "year": 2021
        },
        {
            "authors": [
                "Lu Lu",
                "Xuhui Meng",
                "Shengze Cai",
                "Zhiping Mao",
                "Somdatta Goswami",
                "Zhongqiang Zhang",
                "George Em Karniadakis"
            ],
            "title": "A comprehensive and fair comparison of two neural operators (with practical extensions) based on fair data",
            "venue": "Computer Methods in Applied Mechanics and Engineering, 393:114778,",
            "year": 2022
        },
        {
            "authors": [
                "Alexander Maloney",
                "Daniel A Roberts",
                "James Sully"
            ],
            "title": "A solvable model of neural scaling laws",
            "venue": "arXiv preprint arXiv:2210.16859,",
            "year": 2022
        },
        {
            "authors": [
                "Zhenguo Nie",
                "Haoliang Jiang",
                "Levent Burak Kara"
            ],
            "title": "Stress field prediction in cantilevered structures using convolutional neural networks",
            "venue": "Journal of Computing and Information Science in Engineering,",
            "year": 2020
        },
        {
            "authors": [
                "Adam Paszke",
                "Sam Gross",
                "Francisco Massa",
                "Adam Lerer",
                "James Bradbury",
                "Gregory Chanan",
                "Trevor Killeen",
                "Zeming Lin",
                "Natalia Gimelshein",
                "Luca Antiga",
                "Alban Desmaison",
                "Andreas Kopf",
                "Edward Yang",
                "Zachary DeVito",
                "Martin Raison",
                "Alykhan Tejani",
                "Sasank Chilamkurthy",
                "Benoit Steiner",
                "Lu Fang",
                "Junjie Bai",
                "Soumith Chintala"
            ],
            "title": "PyTorch: An imperative style, high-performance deep learning library",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2019
        },
        {
            "authors": [
                "Maziar Raissi",
                "Paris Perdikaris",
                "George Em Karniadakis"
            ],
            "title": "Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations",
            "venue": "Journal of Computational Physics, 378:686\u2013707,",
            "year": 2019
        },
        {
            "authors": [
                "Rishikesh Ranade",
                "Chris Hill",
                "Jay Pathak"
            ],
            "title": "DiscretizationNet: A machine-learning based solver for navier\u2013stokes equations using finite volume discretization",
            "venue": "Computer Methods in Applied Mechanics and Engineering,",
            "year": 2021
        },
        {
            "authors": [
                "Chengping Rao",
                "Pu Ren",
                "Qi Wang",
                "Oral Buyukozturk",
                "Hao Sun",
                "Yang Liu"
            ],
            "title": "Encoding physics to learn reaction\u2013diffusion processes",
            "venue": "Nature Machine Intelligence, 5:765\u2013779,",
            "year": 2023
        },
        {
            "authors": [
                "Y Saad"
            ],
            "title": "Iterative methods for sparse linear systems",
            "year": 2003
        },
        {
            "authors": [
                "E. Samaniego",
                "C. Anitescu",
                "S. Goswami",
                "V.M. Nguyen-Thanh",
                "H. Guo",
                "K. Hamdia",
                "X. Zhuang",
                "T. Rabczuk"
            ],
            "title": "An energy approach to the solution of partial differential equations in computational mechanics via machine learning: Concepts, implementation and applications",
            "venue": "Computer Methods in Applied Mechanics and Engineering, 362:112790,",
            "year": 2020
        },
        {
            "authors": [
                "Jacob Seidman",
                "Georgios Kissas",
                "Paris Perdikaris",
                "George J. Pappas"
            ],
            "title": "Nomad: Nonlinear manifold decoders for operator learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Burr Settles"
            ],
            "title": "Active learning literature survey",
            "venue": "Computer Sciences Technical Report 1648,",
            "year": 2009
        },
        {
            "authors": [
                "Justin Sirignano",
                "Konstantinos Spiliopoulos"
            ],
            "title": "DGM: A deep learning algorithm for solving partial differential equations",
            "venue": "Journal of Computational Physics, 375:1339\u20131364,",
            "year": 2018
        },
        {
            "authors": [
                "Antonio Stanziola",
                "Simon R. Arridge",
                "Ben T. Cox",
                "Bradley E. Treeby"
            ],
            "title": "A helmholtz equation solver using unsupervised learning: Application to transcranial ultrasound",
            "venue": "Journal of Computational Physics,",
            "year": 2021
        },
        {
            "authors": [
                "N. Sukumar",
                "Ankit Srivastava"
            ],
            "title": "Exact imposition of boundary conditions with distance functions in physics-informed deep neural networks",
            "venue": "Computer Methods in Applied Mechanics and Engineering,",
            "year": 2022
        },
        {
            "authors": [
                "Luning Sun",
                "Han Gao",
                "Shaowu Pan",
                "Jian-Xun Wang"
            ],
            "title": "Surrogate modeling for fluid flows based on physics-constrained deep learning without simulation data",
            "venue": "Computer Methods in Applied Mechanics and Engineering, 361:112732,",
            "year": 2020
        },
        {
            "authors": [
                "Alasdair Tran",
                "Alexander Mathews",
                "Lexing Xie",
                "Cheng Soon Ong"
            ],
            "title": "Factorized fourier neural operators",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Sifan Wang",
                "Hanwen Wang",
                "Paris Perdikaris"
            ],
            "title": "Learning the solution operator of parametric partial differential equations with physics-informed DeepONets",
            "venue": "Science Advances,",
            "year": 2021
        },
        {
            "authors": [
                "Jinchao Xu",
                "Ludmil Zikatanov"
            ],
            "title": "Algebraic multigrid methods",
            "venue": "Acta Numerica, 26:591\u2013721,",
            "year": 2017
        },
        {
            "authors": [
                "Houpu Yao",
                "Yi Gao",
                "Yongming Liu"
            ],
            "title": "FEA-Net: A physics-guided data-driven model for efficient mechanical response prediction",
            "venue": "Computer Methods in Applied Mechanics and Engineering, 363:112892,",
            "year": 2020
        },
        {
            "authors": [
                "Huaiqian You",
                "Quinn Zhang",
                "Colton J. Ross",
                "Chung-Hao Lee",
                "Yue Yu"
            ],
            "title": "Learning deep implicit fourier neural operators (IFNOs) with applications to heterogeneous material modeling",
            "venue": "Computer Methods in Applied Mechanics and Engineering, 398:115296,",
            "year": 2022
        },
        {
            "authors": [
                "Hong Wu Zhang",
                "Jing Kai Wu",
                "Jun L\u00fc",
                "Zhen Dong Fu"
            ],
            "title": "Extended multiscale finite element method for mechanical analysis of heterogeneous materials",
            "venue": "Acta Mechanica Sinica/Lixue Xuebao, 26:899\u2013920,",
            "year": 2010
        },
        {
            "authors": [
                "Kunpeng Zhang",
                "Dachuan Liu",
                "Qun Wang",
                "Peng Hao",
                "Yuhui Duan",
                "Hao Tang",
                "Bo Wang"
            ],
            "title": "Multilevel intelligent design of variable angle tow laminates via image-driven method",
            "venue": "Composite Structures,",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Solving partial differential equations (PDEs) has widespread applications in science and engineering. Conventional solvers based on a variety of classical numerical approaches (e.g., finite element methods (FEMs), finite volume methods (FVMs), finite difference methods (FDMs), meshfree methods) have been developed and achieve great achievements in the last decades, and they can give solutions that meet engineering accuracy requirements under appropriate settings. Commercial software developed on the basis of these conventional solvers has been used extensively in engineering. However, the conventional solvers cannot learn knowledge or experience from their history of solving previous problems, which means they have to solve a new problem from scratch each time, even if it is highly similar to the ones they have solved before. The limitation leads to heavy computational costs in scenarios that require multiple simulations of different parameters, such as inverse problems, optimum design and uncertainty quantification. With the current rapid development of deep learning algorithms, software and hardware, training neural networks to learn\n\u2217Corresponding author: haopeng@dlut.edu.cn\nar X\niv :2\n30 4.\n04 23\n4v 3\n[ cs\n.L G\n] 9\nN ov\nthe solution operators [32, 29, 28, 19, 42, 11, 10, 21, 15, 48] becomes a promising way to break through the above limitation of the conventional solver. But the training process of neural networks requires often a number of labels from simulation or experiments as ground truth data, which can be scarce and expensive to obtain. Many attempts [7, 44, 37, 24, 41, 9, 38, 50, 31, 11, 39] (see Table 1) to embed domain knowledge like physical laws and principles into neural networks have been made to make the training process label-efficient and even label-free. Like the classical numerical approaches that solve PDEs with different forms, these techniques also deal with different forms of PDEs: the differential form (the strong form), the equivalent integral form, the variational form (the weak form) and the minimization form (see Table 1). The weak form transforms differential equations into integral equations, reducing the difficulty of solving by lowering the order of PDEs. Compared to the strong form, the weak form reduces the continuity requirements of the solution, which is less strict and more robust, making it appropriate for describing many of the real-world phenomena with non-smooth and even discontinuous physics. Besides, many powerful classical numerical approaches, especially FEMs and FVMs, are based on the weak form, which have developed mature mechanisms for handling boundary conditions and determining convergence of solutions. Embedding the weak form with these well-developed mechanisms in deep neural networks can help us overcome some of the difficulties like boundary condition management [46] encountered in the strong form embedding. Moreover, the classical Ritz method and Galerkin method based on the weak form transform the original PDEs problem into solving a sparse linear system, which a lot of iterative techniques [40] can be applied to solve with low costs. We provide Table 1, where we list of some of the representative existing literature of solving and learning PDEs, from classical approaches to some recent work combining machine learning. From\nTable 1, we observe three such combinations, which are physics-informed DeepONet (PI-DeepONet) [50], physics-informed neural operator (PINO) [31] and physics-informed variational DeepONet (V-DeepOnet) [11]. However, both PI-DeepONet and PINO are mostly based on the strong form. V-DeepONet, like the deep Ritz method [7] and the literature [41], chooses to set the system functional as the loss function, which is based on the minimization form. Combining operator learning model and the specific domain knowledge, especially the weak form, is worth exploring and will lead us to new numerical approaches suitable for solving parameterized PDEs.\nOur contributions. In this Article, we propose the variational operator learning (VOL), a novel paradigm that combines training neural operators and solving partial differential equations with the variational form. To the best of the authors\u2019 knowledge, this is the first study that integrates the perspectives of the weak form and efficient iterative methods for solving sparse linear systems into the end-to-end operator learning task. The proposed VOL trains neural operators with a label-free training set. The distribution-shift operation with a 5-label-only shift set (see section 5), which also exists in the conventional data-driven paradigm in the previous work [28], is the only part requiring labels in the VOL algorithm. The main contributions of this work are summarized as follows:\n\u2022 Based on the idea of Ritz method and Galerkin method, we propose Ritz approach and Galerkin approach respectively in the framework of VOL. These two approaches can approximate system functional and system residual with FEM discretization in a matrix-free manner.\n\u2022 We introduce direct minimization and iterative update as two optimization strategies into the framework of VOL to minimize the residual norm. Specifically, for iterative update strategy, we integrate steepest decent method (SD) and conjugate gradient method (CG) into VOL with an efficient restart-update manner.\n\u2022 We investigate VOL with various experimental results. Our scaling experiments show the proposed VOL can learn operators and solve the PDEs effectively across different benchmarks given enough\nlabel-free data. We also conduct resolution experiments, comparative experiments verifying generalization benefits of VOL, and comparative experiments for different optimization strategies."
        },
        {
            "heading": "2 Related work",
            "text": "Surrogate modeling. Surrogate modeling has been proposed to alleviate the computational burden of conventional solvers. Two main strategies, reduced order modeling methods (ROMs) and data-fit modeling methods (DFMs), have been adopted in the surrogate modeling. The main idea of these strategies is to seek a surrogate model of the original complex model so as to reduce or avoid the use of the conventional solver at the evaluation stage. ROMs [17] use limited available snapshots from the original high-fidelity computational models to build simplified ones in the space of reduced basis. The constructed reduced-order models are computationally efficient compared with original full-order models. On the other hand, DFMs [8] including response surface methods, Kriging methods and neural networks, perform interpolation or regression of parameter-response pairs sampled from the high-fidelity dataset. The popular deep learning approach can also be considered as a kind of DFMs. An enormous amount of work of deep neural networks, including the neural operators [35, 20, 9, 29, 28, 32], has focused on utilizing deep learning techniques to design excellent neural surrogate models.\nTwo stages are usually required for both ROMs and DFMs to get a surrogate model in a purely data-driven manner:\n(1) Data preparation. At this stage, high-fidelity labels from the simulation of the conventional solvers or real-world experiments are produced and collected. Both ROMs and DFMs require often a fair number of such expensive labels.\n(2) Model construction (model training). Available labeled data from stage (1) are utilized to construct the surrogate model, which brings another computational cost.\nIt is worth noting that, despite the existence of techniques such as active learning [43] that tightly couple these two stages, they are in reality isolated from each other. From a model perspective, the data preparation stage just provides limited labels as examples, and keeps itself a black box to the surrogate models to be trained in the whole process of simulating or experimenting. Besides, from a solver perspective, the model training stage has no influence on the solving process in the data preparation stage, for example, it cannot accelerate the convergence of solving. Models trained in such a purely data-driven fashion can only acquire knowledge indirectly through the labels. If the model can learn directly from the domain knowledge, it is possible to skip the data preparation stage, and the model training process is to a certain extent equivalent to solve the original PDEs. Two ways to help coupling the model with domain knowledge, i.e., domain knowledge embedding and deep model embedding, are discussed in the next paragraph.\nDomain knowledge embedding and deep model embedding. Domain knowledge embedding is to embed domain knowledge (e.g., governing equations, discretization schemes, symmetries, variational principles) into the architecture of the (deep learning) model and the training process. We have listed some of the representative domain knowledge embedding methods in Table 1. Broadly speaking, domain knowledge embedding methods can either be mesh-free [7, 44, 37, 24, 41, 50, 11] or mesh-based [9, 38, 31]. In the mesh-free framework, residual of certain forms of PDEs is computed at some sampled positions in the solution domain, where the residual computation is heavily relied on automatic differentiation with the neural ansatz. On the other hand, in the mesh-based framework, the residual is computed with a certain discretization scheme, e.g., FDMs [9], FVMs [38]. We introduce them with some representative work:\n(1) Mesh-free framework with automatic differentiation. Automatic differentiation is a powerful tool to calculate derivatives in deep learning. The derivatives of the output with respect to input parameters such as spatial coordinates and time, the Jacobian matrix, and even the Hessian matrix can be easily obtained with\nautomatic differentiation. Thus, the governing equations also their weak forms with derivative terms can be implemented elegantly in deep learning. Here we list some representative research that uses automatic differentiation to implement domain knowledge embedding. Deep Ritz method [7] constructs a functional with automatic differentiation approximating derivatives of the trial function, and numerical integration, which sets the minimization objective as the functional. Deep Galerkin method (DGM) [44] and physicsinformed neural network (PINN) [37] are very similar, both of which use the residual error of the strong form as part of the loss function, deriving the residual term with automatic differentiation. Besides, both of them also treat the boundary conditions as penalty terms. PI-DeepONet [50] also calculates derivatives in loss function with automatic differentiation. Note that treating the strong form residual as a loss function can be viewed as embedding an equivalent integral form obtained through the collocation method with the delta function as the test function in the loss function. However, we have not classified it as an equivalent integral form in Table 1 for the sake of clarity.\n(2) Mesh-based framework with a certain discretization scheme. Discretization technique, on the other hand, computes the derivatives and the residual of PDEs with various discretization schemes. Tricks about convolution filters have been adopted to represent various discretization schemes. Motivated by the domain knowledge of the multigrid algorithm, a deep neural architecture called MgNet [16] is designed and successfully used in dataset CIFAR-10 and CIFAR-100 [26] for image classification. The following Meta-MgNet [5] use convolutions to represent discrete forms of differential operators, providing improved smoothers for the multi-grid algorithm. FEA-Net [52] uses convolution kernel to express stiffness feature of the structure discretized by finite elements. PhyGeoNet [9] and DiscretizationNet [38] discretize the output of the deep learning model with the finite difference discretization scheme and finite volume discretization scheme respectively to approximate the derivative terms and the residual in the governing equations. The proposed VOL in this Article chooses FEMs as the discretization scheme. For VOL, we design Ritz approach and Galerkin approach (see section 5.1)to form the global algebraic equation system in FEMs and calculate the residual in a matrix-free manner. Compared the mesh-free framework, Galerkin approach in VOL derives residual with no automatic differentiation, which is more resource-efficient in terms of computation and video memory. Compared with existing domain knowledge embedding work that also utilizes FEMs as the discretization scheme, for example, Meta-MgNet [5], which reduces its Q1-element implementation to FDM, FEA-Net [52], which derives its convolution kernel analytically, VOL considers standard isoparametric elements. Moreover, VOL is allowed to perform Gauss quadrature of arbitrary order and shape function of arbitrary order, which is more practical and closer to actual engineering.\nIn contrast to domain knowledge embedding, deep model embedding is to embed the deep learning model in the classical numerical approach to enhance the capabilities of the classical numerical approaches. Deep potential [14] leverages deep neural network representation of the potential energy surface for atoms and molecules system, which is a promising alternative to the classical potential representation in molecular dynamics and Monte Carlo simulations. Problem-independent machine learning [18] embeds a simple feedforward neural network in the framework of extended multi-scale finite element method [54] to learn the mapping between discretized material density field of the coarse element and the multi-scale numerical shape functions of the element. Deep conjugate direction method [22] uses the output of a deep convolutional neural network embedded in the algorithm to construct a good search direction which accelerates the convergence for solving large, sparse, symmetric, positive-definite linear systems. Fourier neural solver [6] embeds a neural network in the stationary iterative method to help to eliminate error components in frequency space.\nFor both approaches, we just list some of the representative research in recent years. It is also noted that these two approaches are not mutually exclusive, instead, they are complementary to each other, which means a method can be both domain knowledge embedding and deep model embedding, and we can use either one or both of them to solve the problem.\nOperator learning and neural operators. Operator learning is to let the model learn the operator between two function spaces. Operator learning models, which we say are operator-based, can give prediction over a whole parameter set of parameterized PDEs. In Table 1, we classify some of the representative existing methods according to whether they are function-based or operator-based. Neural operators refer to specific neural network architectures designed for operator learning. According to the type of input and output of the neural networks, the existing architectures of neural operators can be divided into two categories:\n(1) Point-wise. Inspired by the universal approximation theorem for operators [3, 4], deep operator network (DeepONet) [32], as a representative architecture of this category, receives the parameter field and a query point, and then output solution of the query points in the computational domain, which has a branch net for encoding discrete function space and a trunk net for encoding the coordinate information of query points, and the output of which is the inner product of the output of two nets. Following work based on DeepONet includes learning multiple-input operators [21], combing DeepONet with physics-informed machine learning [50], replacing the trunk net with basis functions precomputed by proper orthogonal decomposition [33]. Recently, a general neural operator transformer (GNOT) [15] is proposed, with a heterogeneous normalized attention layer design, and GNOT is also designed to handle multiple input functions and irregular meshes.\n(2) Field-wise. These architectures [29, 28, 53, 48] input discrete parameter fields and output discrete solution fields. Fourier neural operators (FNO) [28] parameterize the integral kernel in the Fourier space and utilize the idea of shortcut connection, producing a powerful Fourier layer. The implicit Fourier neural operator (IFNO) [9] utilizes the Fourier layer in an implicit manner, which lets the data flow pass through the Fourier layer recurrently and has better training stability. Factorized Fourier neural operator (F-FNO) [48] adopts Fourier factorization and a handful of other techniques about network design and training settings to enhance the model performance. In this Article, we focus on training the field-wise neural operators due to the natural similarity between field-wise output and mesh settings in FEMs.\nIt is worth noting that, in addition to these neural operators specifically designed for operator learning, the idea of operator learning has also been combined with various neural architectures and approaches, including convolutional neural networks [35] and the generative neural networks [20], principal component analysis [1, 27], meta-learning [19], transfer learning [10], attention mechanism [25, 15], manifold learning [42]."
        },
        {
            "heading": "3 Preliminaries",
            "text": ""
        },
        {
            "heading": "3.1 Forms of partial differential equations",
            "text": "Consider \u2126 \u2286 Rd, where d is a fixed number in N+. S denotes the boundary of \u2126. Without loss of generality, we start with the strong form of the stationary PDEs:\nA (u (x)) = 0,x \u2208 \u2126 BC (u (x)) = 0,x \u2208 S (1)\nMultiply Eq. 1 by an arbitrary test function v \u2208 C\u221ec (\u2126), we get the weighted integral form or equivalent integral form of PDEs\n\u222b\n\u2126 vTA(u)d\u2126 +\n\u222b\nS vTBC(u)dS \u2261 0. (2)\nThen, with integration by parts lowering the order of variables, we can get the weak form of PDEs \u222b\n\u2126 C(v)TD(u)d\u2126 +\n\u222b\nS E(v)TF(u)dS \u2261 0. (3)\nFu nc\ntio n-\nba se\nd O\npe ra\nto r-b\nas ed\nTy pe\nof m\net ho\nds Fo\nrm of\nPD Es\nC la\nss ic\nal nu\nm er\nic al\nap pr\noa ch\nD at\nadr\niv en\nm od\nel in\ng D\nom ai\nn kn\now le\ndg e\nem be\ndd in\ng D\niff er\nen tia\nlf or\nm Eq\nui va\nle nt\nin te\ngr al\nfo rm\nM in\nim iz\nat io\nn fo\nrm Va\nria tio\nna lf\nor m\nA na\nly tic\nal m\net ho\nds \u2713\n\u2713 \u2713\nFD M\ns \u2713\n\u2713 \u2713\nW ei\ngh te\nd re\nsi du\nal m\net ho\nds \u2713\n\u2713 \u2713\nR itz\nm et\nho d\n\u2713 \u2713 \u2713 G al er ki n m et ho d \u2713 \u2713 \u2713 FE M s \u2713 \u2713 \u2713 FV M s \u2713 \u2713 \u2713 RO M s \u2713 \u2713\nD ee\np R\nitz m\net ho\nd [7\n] \u2713\n\u2713 \u2713\nD G\nM [4\n4] \u2713\n\u2713 \u2713\nPI N\nN [3\n7] \u2713\n\u2713 \u2713\nVa rN\net [2\n4] \u2713\n\u2713 \u2713\nLi te\nra tu\nre [4\n1] \u2713\n\u2713 \u2713\nPh yG\neo N\net [9\n] \u2713\n\u2713 \u2713\nD is\ncr et\niz at\nio nN\net [3\n8] \u2713\n\u2713 \u2713\nD ee\npO N\net [3\n2] \u2713 \u2713 FN O [2 8] \u2713 \u2713 PI -D ee pO N et [5 0] \u2713\n\u2713 \u2713\nPI N\nO [3\n1] \u2713\n\u2713 \u2713 \u2713 VD ee pO N et [1 1] \u2713 \u2713 \u2713 VO L (o ur s) \u2713\n\u2713 \u2713\nTa bl\ne 1:\nSu m\nm ar\ny of\nso m\ne of\nth e\nre pr\nes en\nta tiv\ne ex\nist in\ng m\net ho\nds of\nso lv\nin g\nan d\nle ar\nni ng\nPD Es\n.\nAnd note the weak form can be expressed in such an abstract form\nFind u \u2208 V , such that a (u,v) = L (v) , (4) When a (u,v) is symmetric, we can also write the minimization form of the PDEs. By treating the test function v as the variation of u, we can derive the functional \u03a0 of the system from the weak form. In this case, the original problem (solving PDEs) has been transformed into a functional minimization problem\n\u03a0 = \u03a0(u) \u2192 min u \u03a0(u). (5)\nNote \u03a0 can also be written in such an abstract form\n\u03a0 = 1\n2 a(u,u)\u2212 L (u) . (6)\nThe solutions to Eq. 3, Eq. 4 and Eq. 5, Eq. 6 are called weak solutions. Eq. 5 and Eq. 6 are also called \u201dweak form\u201d sometimes, but to emphasize the Eq. 5 and Eq. 6 minimize the system functional, while the weak form provides a more general approach, we call them minimization form in this Article."
        },
        {
            "heading": "3.2 Ritz method and Galerkin method",
            "text": "Ritz method and Galerkin method are two representative ways of utilizing the weak form and minimization form to solve PDEs. They both approximate weak solutions in finite-dimensional spaces. We introduce these two methods with stationary problems of PDEs in Hilbert spaces.\n\u2022 Ritz method Ritz method focuses on minimizing functional in Eq. 5 and Eq. 6 of the system. Choose Vn a finite dimensional subspace of V , dim(Vn) = n, and then construct a basis ( \u03d51, \u00b7 \u00b7 \u00b7 ,\u03d5n) of Vn. Then, u can be approximated with the un uniquely decomposed on the basis\nun = n\u2211\nj=1\nuj\u03d5j . (7)\nThen, \u03a0 can be written as\n\u03a0(un) = 1\n2 a (un,un)\u2212 L (un)\n= 1\n2 a\n  n\u2211\nj=1\nuj\u03d5j ,\nn\u2211\ni=1\nui\u03d5i\n \u2212 L ( n\u2211\ni=1\nui\u03d5i\n)\n= 1\n2\nn\u2211\ni=1\nn\u2211\nj=1\na (uj\u03d5j , ui\u03d5i)\u2212 n\u2211\ni=1\nL (ui\u03d5i)\n= 1\n2\nn\u2211\ni=1\nn\u2211\nj=1\nujuia (\u03d5j ,\u03d5i)\u2212 n\u2211\ni=1\nuiL (\u03d5i) .\n(8)\nThus, \u03a0 can be rewritten under algebraic form\n\u03a0 = 1\n2 uTAu\u2212 uTb, (9)\nwhere Aij = a (\u03d5j ,\u03d5i), bi = L (\u03d5i). To minimize \u03a0, Ritz method lets the gradient of the function \u03a0(u1, \u00b7 \u00b7 \u00b7 , un) be a zero vector\n\u2202\u03a0 \u2202u = Au\u2212 b = 0. (10)\n\u2022 Galerkin method Galerkin method is based on the weak form of PDEs. Like Ritz method, Galerkin method also uses the basis ( \u03d51, \u00b7 \u00b7 \u00b7 ,\u03d5n) of Vn to construct approximation of u. Based on Eq. 4 and Eq. 7, we get the abstract form of Galerkin method\nFind un \u2208 Vn, such that: a (un,vi) = L (vi) . (11)\nBy assuming finite number of test functions v1,v2, ...vn, Galerkin method turns the abstract form into a set of linear equations that can be solved numerically\nAu = b, (12)\nwhere Aij = a (\u03d5j ,vi), bi = L (vi). The test functions can be either in Vn or not, and the type of Galerkin method where v is not taken in Vn is called Petrov-Galerkin method. Galerkin method is more general than Ritz method. Compared to Ritz method, Galerkin method does not require the symmetry of the bilinear form, allowing it to handle problems where the minimization form of PDEs does not exist.\nInspired by these two classical numerical methods, we develop Ritz approach and Galerkin approach of VOL respectively to approximate system functional and residual with deep learning toolkit (see section 5.1 and Fig. 6)."
        },
        {
            "heading": "3.3 Finite element methods",
            "text": "FEMs is a special case of Ritz method and Galerkin method, where finite subdomains (elements) are designed to be supports of trial functions and test functions. In this Article, VOL utilizes the discretization scheme of FEMs, and handles the global algebraic equation system of FEMs in a matrix-free and iterative manner (see section 5.1)."
        },
        {
            "heading": "4 Results",
            "text": "Various experimental results are shown and discussed in this section to demonstrate the effectiveness the proposed VOL. Specifically, we investigate VOL on steady heat transfer problem with variable heat source, Darcy flow with variable conductivity, and variable stiffness elasticity (Fig. 1). We first introduce our problem settings briefly. We conduct scaling experiments to investigate the influence of the different data sizes on the performance of VOL. We then conduct the resolution experiments to verify the capability of VOL at different resolutions. We then design two comparative experiments between VOL and iterative methods to verify the generalization benefits of VOL. We also conduct experiments to compare the proposed optimization strategies in VOL with the data-driven strategy."
        },
        {
            "heading": "4.1 Problem settings",
            "text": ""
        },
        {
            "heading": "4.1.1 Variable stiffness elasticity",
            "text": "\u2022 General elasticity Consider a linear elastic body B \u2282 R3 (Extended Data Fig. 1a). Let the boundary of the body be S = S\u03c3 \u222a Su. The governing equations and boundary conditions of B are\n   \u03c3ij,j + fi = 0, in \u2126 X\u0304i = \u03c3ijnj , on S\u03c3 ui = u\u0304i, on Su.\n(13)\nThe weighted integral form of Eq. 13 is \u222b\n\u2126 (\u03c3ij,j + fi) \u03b4uid\u2126 +\n\u222b\nS\u03c3\n( X\u0304i \u2212 \u03c3ijnj ) \u03b4uidS = 0, \u2200\u03b4ui \u2208 {v \u2208 C[\u2126]; v = 0 on Su} . (14)\nAccording to Green\u2019s formula, \u222b\n\u2126 \u03c3ij,j\u03b4uid\u2126\u2212\n\u222b\nS\u03c3 \u03c3ijnj\u03b4uidS =\n\u222b\n\u2126 \u03c3ij,j\u03b4uid\u2126\u2212\n\u222b\nS \u03c3ijnj\u03b4uidS = \u2212\n\u222b\n\u2126 \u03c3ij\u03b4ui,jd\u2126\n= \u2212 \u222b\n\u2126\n1 2 \u03c3ij (\u03b4ui,j + \u03b4uj,i) d\u2126 = \u2212\n\u222b\n\u2126 \u03c3ij\u03b4\u03b5ijd\u2126,\n(15)\nSubstituting Eq. 15 into Eq. 14, we have \u222b\n\u2126 fi\u03b4uid\u2126 +\n\u222b\nS\u03c3 X\u0304i\u03b4uidS =\n\u222b\n\u2126 \u03c3ij\u03b4\u03b5ijd\u2126,\n\u2200\u03b4ui \u2208 {v \u2208 C[\u2126]; v = 0, on Su} . (16)\nEq. 16 is also known as the principle of virtual work, which is a variational form of Eq. 13. The left term of Eq. 16 is the virtual work done by the external forces, and the right term is the strain energy of the elastic body B (also the opposite of the virtual work done by the internal forces). Thus, we have\n\u03b4Wint + \u03b4Wext = \u2212\u03b4 [U + Ue] = 0, (17) where \u03b4Wint, \u03b4Wext are the virtual work done by the internal forces and the virtual work done by the external forces respectively. U is the strain energy of the elastic body B, and Ue is the potential energy of the external forces. Let the system functional be \u03a0, and we have\n\u03b4\u03a0 = 0,\n\u03a0 = U + Ue, \u2200\u03b4ui \u2208 {v \u2208 C[\u2126]; v = 0, on Su} . (18)\nNote Eq. 18 is equivalent to Eq. 13, and here we only give the derivation of Eq. 18 from Eq. 13. For convenience, we rewrite the stress tensor and strain tensor in their vector form, i.e.,\n\u03c3 = [\u03c3x, \u03c3y, \u03c3z, \u03c4yz, \u03c3xz, \u03c3xy] T , \u03b5 = [\u03b5x, \u03b5y, \u03b5z, \u03c4yz, \u03b5xz, \u03b5xy] T .\nThen the strain energy of the elastic body and the potential energy of the external forces is\nU =\n\u222b\n\u2126\n1 2 \u03b5TC\u03b5d\u2126,\nUe = \u2212 \u222b\n\u2126 fTud\u2126\u2212\n\u222b\nS\u03c3 X\u0304TudS.\n(19)\nThus, functional \u03a0 is:\n\u03a0 = U + Ue =\n\u222b\n\u2126\n1 2 \u03b5TC\u03b5d\u2126\u2212\n\u222b\n\u2126 fTud\u2126\u2212\n\u222b\nS\u03c3 X\u0304TudS, (20)\nwhere C is the matrix of material properties. Note \u03a0 is the total potential energy of the system. In context of operator learning, for example, C can be a function of coordinates C (x, y, z) in random function spaces, and our goal is to learn an operator learning model, which outputs the displacement field u that lets \u03b4\u03a0 = 0 hold as close as possible for each input instance with C (x, y, z) in the test set. In our experiments, we consider a 2-D variable stiffness case, where C = C(\u03b8(x, y)).\n\u2022 Variable stiffness elasticity An elastic variable stiffness square plate with in-plane deformation is considered in our experiments, and it is considered to be made of the fiber reinforced material, as shown in Fig. 1. The thickness of the plate is 0.125mm. Due to spatial variation of fiber orientation, the material property of the plate shows anisotropy. The material property matrix in x-y coordinate system Cxy can be written as:\nCxy = T \u22121C12T\u2212T (21)\nwhere\nT =\n \ncos2 \u03b8 sin2 \u03b8 2 sin \u03b8 cos \u03b8 sin2 \u03b8 cos2 \u03b8 \u22122 sin \u03b8 cos \u03b8\n\u2212 sin \u03b8 cos \u03b8 sin \u03b8 cos \u03b8 cos2 \u03b8 \u2212 sin2 \u03b8\n  (22)\nAnd C12 is the material property matrix in the principle material coordinates, which is not effected by the fiber angle. Eq. 23 gives the formulation of compliance matrix S12, i.e., the inverse of C12:\nS12 =\n  1 E1\n\u2212\u03bd12E1 0 \u2212\u03bd12E1 1 E2 0 0 0 1G12\n  (23)\nThe fiber angle field \u03b8 = \u03b8 (x, y) of the plate is characterized in two ways as problem settings: (1) Elasticity A: Linear 1-D variation [12, 13]. (2) Elasticity B: B-splines surface [55]. The goal of VOL is to learn the mapping between the fiber angle field space and the vector space of displacement components [u1, u2]T. Specifically, we study a 100mm\u00d7100mm fiber-reinforced panel. For details about problem settings of variable stiffness elasticity, including material properties, boundary conditions, elements and parameter range, see Fig 1 and Supplementary material S2.2."
        },
        {
            "heading": "4.1.2 Steady heat transfer with variable heat source and Darcy flow",
            "text": "Steady heat transfer with variable heat source in 1m\u00d71m\u00d71m cube and Darcy flow with variable conductivity in 1\u00d71 square are studied respectively (See Fig. 1). The governing equation of both steady heat transfer and Darcy flow is described as \n  \u2212\u2207 \u00b7 ( \u03ba\u2207T ) = Q in \u2126, T = T\u0304 on SD, (\u03ba\u2207T ) \u00b7 n = q\u0304 on SN, (\u03ba\u2207T ) \u00b7 n = h\u0304 (T\u221e \u2212 T ) on SR,\n(24)\nwhere T represents temperature in the variable heat source problem and the hydraulic head in the Darcy flow problem, \u03ba represents conductivity tensor, and Q is source term. Let v be test function that satisfies Dirichlet boundary condition, leading to the weak form of Eq. 24\n\u222b \u2126 [\u2207v \u00b7 (\u03ba\u2207T )\u2212 vQ] d\u2126 = \u222b \u2126 [ \u2207Tv\u03ba\u2207T \u2212 vQ ] d\u2126 = 0. (25)\nWhen the \u03ba is symmetric, the minimization form of Eq. 24 exists\nmin T\nI = 1\n2\n\u222b\n\u2126\n[ \u2207TT\u03ba\u2207T \u2212 2QT ] d\u2126, (26)\nwhere T satisfies Dirichlet boundary condition. For variable heat source problem, the goal of VOL is to learn the mapping between heat source field and the temperature field. For Darcy flow problem, the goal of VOL is to learn the mapping between the conductivity field and the hydraulic head. For more details about problem settings of variable heat source problem and Darcy flow problem, see Supplementary material S2.1.\n4.2 Scaling experiments\na b\nTo verify the effectiveness of VOL with different unlabeled data sizes, we design scaling experiments for all cases in section 4.1 with a specific resolution. For all cases, we chose 7 different sizes for the training set: 100, 200, 500, 1000, 2000, 5000, 10000, and set the size of the test set to 2000. All scaling experiments are conducted with VOL+CG(2) to demonstrate the capability and effectiveness of VOL to learn operators with small update steps. For more details about scaling experiments, see Supplementary material S6.1. With the increase of the number of unlabeled data for training, the performance of VOL should be improved considerably, which has been observed in Fig 2a. With a log scale in data size and test error, an approximately linear convergence versus the size of training set is observed among all cases. From our size scaling results, the test error and the training set size matches a power law, which has also been observed in research about large language models (LLMs) [23, 34] and DeepONet with the data-driven strategy [32],\ni.e., y = axb, where y refers to the test error and x refers to the training set size. Related coefficients have also been reported in Fig 2a. In addition to the average metrics of VOL on the test set, we also study the worst-case scenario. Fig 2b shows that the test errors of the worst prediction decrease for all benchmarks as the training set size increases. Extended Data Figure 3, Extended Data Figure 4 and Extended Data Figure 5 visualize the worst predicted samples, which has largest test errors of all problems at (Ntraining set = 10000, Ntest set = 2000), where we observe VOL still provides approximately correct solution fields."
        },
        {
            "heading": "4.3 Resolution experiments",
            "text": "To demonstrate the capability of VOL at different resolutions, we conduct resolution experiments on two problems: the Darcy flow and elasticity B. We keep the size of the training set and the test set at all resolutions be 2000 for these two problems. For experiments at all resolutions in one problem, we use the same model architecture settings. For complete training settings and other details, see Supplementary material S6.2. For darcy problem, we observe the test error slightly rises as the resolution increases. For elasticity B problem, we observe the test error also slightly rises from resolution 33 to resolution 257\u00d7257. With resolution 513\u00d7513, we observe the test error drops to an average value of 1.89%, we consider which is likely due to the adjustment of batch size at resolution 513\u00d7513. Benefited from the resolution-invariant feature of FNOs [28], VOL is able to learn solution operators mostly effectively at different resolutions with the same amount of parameters."
        },
        {
            "heading": "4.4 Generalization benefits",
            "text": "From the perspective of the neural solver, the generalization, as a unique advantage of the machine learning, especially of the deep learning, can be utilized to provide better initial solutions for the solver solving new problems. In the context of VOL, generalization benefits means that the neural operator module keeps learning and provides more accurate initial solution for the iterative solver. To demonstrate the generalization benefits of VOL, we conduct two experiments, which compare VOL+CG(i) and CG(i) on same and different datasets respectively with various numbers of update steps on the elasticity B problem. Specifically, for a comprehensive and fair comparison, we choose restarted conjugate gradient CG(i) with a set of numbers of update steps, i \u2208 {1, 2, 5, 10, 25, 50, 100}, to compare with VOL that uses the same set of CG update step numbers for these two experiments (VOL+CG(i)) in these two experiments. When i = 1, conjugate gradient method degenerates itself to one-step steepest decent. We only study SD = CG(1) case for these two experiments, and do not research steepest decent with more update steps, for SD has a well-known slow convergence speed, while more update steps of SD also brings more times of calculating matrix-vector products like CG. We run CG(i) with its initial guess as the average of labels (CG(i)-A) in the shift set used by VOL and with a random normal distribution initialization (CG(i)-R) independently. For every epoch, CG(i) solves each sample in the training set with i update steps and restarts itself for the next epoch. We conduct these two experiments at resolution 33\u00d733, 129\u00d7129 and 257\u00d7257. See Supplementary material S6.3 for more details of the experimental design.\nFor the first experiment, we compare VOL and restarted conjugate gradient method on a same dataset containing 2000 samples, as shown in Fig. 3a to Fig. 3c. We set no test set for VOL in this experiment. At\nresolution 33\u00d733, when the number of update step is set to 1, 2, 5, 10, the train error of VOL is approximately one order of magnitude smaller than restarted CG at then end of iterations. As the number of update steps increases, restarted CG starts to converge faster. At resolution 33\u00d733, restarted CG has a faster convergence speed than VOL at 25, 50, 100 update steps, and has also a far smaller average train errors than VOL at 50 and 100 update steps at the end of training. At higher resolutions, however, classical iterative methods converge much slower. For resolution 257\u00d7257, compared with CG(i)-R, the training errors of VOL+CG(i) are approximately one order to even two orders of magnitude smaller for all considered update steps at the end of iterations. Compared with CG(i)-A, VOL+CG(i) also has around one order of magnitude smaller training errors at the end of iterations. From Fig. 3a to Fig. 3c, we observe the error lines of CG(i)-R and CG(i)-A with the same update step are nearly parallel before their convergence, which indicates that, in our case, the difference of initialization has no significant effect on the convergence speed of the restart conjugate gradient method. We also note that generalization benefits, which can improve the magnitude of test errors, do not significantly improve the convergence speed of VOL+CG(i). In fact, the condition number of coefficient matrix K is the key factor affecting convergence speed [40]. We discuss possible solutions to accelerate the convergence as our future work (see section 6).\nTo verify the competitiveness of generalization for unseen data of VOL against the classical restarted iterative method, we conduct the second experiment, where the neural operators are first trained with VOL+CG(i) on a 2000-sample training set, and then tested on a separate 2000-sample test set at the same resolution. Results (Fig. 3d) show that VOL can generalize to unseen data at various resolutions with different update step numbers, and the superiority of VOL becomes more and more obvious as the resolution grows. However, we also observe that VOL improves but has no significant improvement on the test error as the number of update steps grows, which needs further investigation.\n4.5 Comparison on different optimization strategies\nIn this section, we compare different optimization strategies for training neural operators with experimental results and discussion. We consider three strategies, iterative update (VOL+CG), direct minimization (VOL+DM), and purely data-driven strategy. For VOL+CG, we also chose 2 and 500 conjugate update steps respectively for VOL to further investigate the effect of update step number on performance of VOL. For all strategies, the experiment is conducted on Darcy flow problem at 512\u00d7512 resolution, with 2000 training data and 2000 test data. To ensure a fair comparison, we keep all training settings (number of epochs, learning rate, scheduler, batch size, etc., for details see Supplementary material S6.4) same for each\nstrategy. We observe no significant difference between VOL+CG(2) and VOL+CG(500) in training, but for test error VOL+CG(500) is a bit better than VOL+CG(2) on average. We then observe that while the converged residual norm of each strategy satisfies relationship \u201ddata-driven strategy > VOL+CG > VOL+DM\u201d, the relationship of test errors is the exact opposite of that, which is \u201ddata-driven strategy < VOL+CG < VOL+DM\u201d. For the data-driven strategy, the residual norm first rises to \u223c100, and then decreases with a steep slope. As the training proceeds, the slope of the data-driven strategy steadily gets smaller util the end of training. For VOL+CG and VOL+DM, however, no rise of the residual norm was observed at start. In contrast, a sharp drop of the residual norm is observed from the very beginning of the training (\u223c10 steps) for VOL+CG and VOL+DM. After the sharp drop, the residual norm of both VOL+CG and VOL+DM falls a bit in significant fluctuations. The distinguishable fluctuations of VOL+DM almost stop after \u223c1000 iterations, while that of the VOL+CG stop after \u223c3000 iterations. We also notice that though VOL can achieve a smaller residual norm than the data-driven strategy, the residual norm nearly stagnated after initial progress. In section 5.2, we propose that VOL+CG can be interpreted as training neural operator module with provisional labels. Provisional labels, as intermediate solutions provided by the iterative solver, contain noise and are not accurate as ground truth labels, which might explain the gap of performance on test set between VOL and data-driven strategy. From experimental results, we also observe direct minimization has the worst test error in all strategies. Different from the strong form case [44, 37], it might not be a good choice to penalize the residual by putting it in the loss function in our finite element settings. In fact, we can prove that VOL+DM strategy feeds the direct iterative update on the normal equation of the original linear system to the neural operator module (see Supplementary material S8 for proof), which might explain the gap of performance on test set between VOL+DM and other strategies. It is also noted that, even though comparative experiments show all strategies of VOL have larger average test errors than the data-driven strategy, the test errors of VOL+CG are just passable."
        },
        {
            "heading": "5 Methods",
            "text": "In this section, the variational operator learning (VOL) algorithm is developed. VOL does not focus on creating novel neural operator architectures, instead, it focuses on training the existing state-of-the-art neural operators with the smallest possible amount of labeled data, even no label. VOL is applicable to any fieldwise neural architecture in principle, but for the purpose of a clear demonstration of the basic effectiveness of VOL, in all experiments we just employ the Fourier neural operator (FNO) [28] in the neural operator module of VOL with moderate modification. Related network flowchart is shown in Fig. 5a.\nAlgorithm 1 Variational operator learning algorithm Input: neural operator module Noperator (\u03d1), number of epochs N , optimization strategy Opt, learning rule\n\u03b7, and training set D={Ui}Ntraini=1 , max iteration number of one epoch maxiter, mask operation Mask, Shift set Shift, network optimizer Optnet and learning rate scheduler Scheduler\n1: for 1 \u2264 i \u2264 N do 2: for 1 \u2264 j \u2264 maxiter do 3: Sample a minibatch of bs examples Ubsj from D 4: absj = Noperator (\u03d1) ( Ubsj ,Mask, Shift ) 5: Get Rbsj with Ritz approach or Galerkin approach 6: Rbsj = Mask ( Rbsj ) 7: \u2206absj = Opt ( Rbsj ,a bs j ,U bs j ,Mask ) 8: Get the current learning rate \u03b7\u0302 according to \u03b7 and Scheduler 9: Update \u03d1 with \u2206absj , Optnet and \u03b7\u0302\n10: end for 11: end for Return: learned neural operator module Noperator (\u03d1)\nThe proposed VOL can be seamlessly integrated into deep learning training pipeline. The Algorithm 1 demonstrates a complete training process including the outer epoch loop and the inner dataset loop. Here we just start discussion at the inner loop.\nLine 4 of the Algorithm 1 shows a standard forward propagation from unlabeled input parameters to the node solution. Each channel of the node solution tensor contains one component of the node solution. Parameters of PDEs are first discretized at Gauss points and nodes, and then are aggregated into the parameter tensor Para |G and Para |N respectively (Fig. 5b). For Para |G, parameters at Gauss points that at the same position of all elements are encoded into one channel of the parameter tensor. Thus, the number of the channel of the parameter tensor equals to number of Gauss points in one element, and the resolution of Para |G is equal to the mesh size. For Para |N, one channel of parameter tensor corresponds to one component of the parameter. For a concerned parameter of PDEs, the neural operator module (Fig. 5a) receives Para |G or Para |N as input. Light feature engineering is first adopted for the input parameter tensor. The alignment operation (Fig. 5c) is needed only for Para |G input, which is implemented with a trainable transposed convolution, mapping the tensor from the mesh size to the node size. Then, the lifting layer lifts the tensor to a higher channel space, N Fourier layers are adopted, and the projection layer projects tensor to the solution space, which is the main operations of FNO [28]. Two additional operations are performed on the node solution predicted by the neural operator module:\n(1) Mask operation. The mask operation is designed to apply the essential boundary condition, such as displacement boundary condition in elasticity and temperature boundary condition in heat transfer to the system, which is equivalent to the constraint imposition process in the FEMs. First, a mask tensor that contains 0 and 1 is constructed, which has the same shape as the node solution. Every element in the mask tensor corresponds to a certain degree of freedom of a certain node in the computational mesh. If a element of the mask tensor is \u201d0\u201d, it means that the corresponding degree of freedom of the corresponding node is constrained, while \u201d1\u201d means not constrained. Then, the element-wise product between the solution tensor and the mask tensor is calculated, as shown in Fig. 5d. The technique of the mask operation can be categorized into the so-called hard manners [47, 9, 39] to enforce the boundary condition of PDEs, which is different to the soft manner, where the BCs are treated as penalty terms in the loss function. The mask operation can also be extended to the inhomogeneous case. As shown in Fig. 5d, we simply add\na shift tensor with the same shape after the element-wise product operation, the elements of which are 0 where the corresponding elements of the mask tensor are 1 (unconstrained), and the other elements are the inhomogeneous terms of constraints.\n(2) Distribution-shift. Distribution-shift operation that exists in the prior work [28, 53] is remained in this work, but we only use a very small number labeled data (5 labels) rather than label the whole training set. It first computes the mean mean and the standard deviation std of all labels of the training set, and then use the following equation to shift the output of the neural operators to the distribution of the labels:\noutput = output\u2297 std+mean (27)\nSuch a shift operation can stabilize the training process. Note the training set is totally label-free. To generate labels for the shift operation, an extra small batch of parameters of PDEs are randomly sampled and labeled, and these labeled data are only used in the distribution-shift operation, which we call a shift set."
        },
        {
            "heading": "5.1 Matrix-free approximation of the system functional and the system residual",
            "text": "In line 5 of Algorithm 1, Ritz approach and Galerkin approach of VOL are developed respectively to approximate the system functional and the system residual. In this section, we introduce these two approaches with theory consideration and implementation details."
        },
        {
            "heading": "5.1.1 Ritz approach",
            "text": "Ritz approach of VOL is composed of two parts, system functional approximation (forward propagation) and residual calculation (backpropagation). Ritz approach is matrix-free and can approximate system functional and system residual numerically. In this section, an approximation of the system functional used by Ritz approach is first introduced with the context of elasticity, the formulation of which is a bit different from that in FEMs, making VOL possible to approximate system functional without calculating stiffness matrices. Then, we introduce how to calculate system functional numerically with deep learning toolkit and the residual calculation. It is worth noting that it is possible for VOL to use various discretization scheme, notwithstanding VOL uses the same discretization scheme (piecewise polynomial interpolation) as FEMs in this Article:\nu (x, y, z) = N (r, s, t)ae,\nx = N (r, s, t)xe. (28)\nNote u (x, y, z) and ae are node-related quantities, they can represent the displacement in the context of solid mechanics, and the temperature in heat transfer, etc.\nWe take an elastic body discretized by isoparametric elements as an example. In this case, the node solution is the node displacement tensor of the elastic body. The classical expression of functional approximation in FEMs is:\n\u03a0\u0303 = \u2211\ne\n\u03a0e = 1\n2\n\u2211\ne\n( aeT \u222b\n\u2126e\nBTDBd\u2126ae )\n\u2212 \u2211\ne\n( aeT \u222b\n\u2126e\nNTfd\u2126 ) \u2212 \u2211\ne\n( aeT \u222b\nSe\u03c3 NTX\u0304dS\n) .\n(29)\nThe \u222b \u2126e BTDBd\u2126, \u222b \u2126e NTfd\u2126 and \u222b Se\u03c3 N\nTX\u0304dS in Eq. 29 are element stiffness matrix, element volume load vector and element face load vector respectively. Denote them as\nKe =\n\u222b\n\u2126e\nBTDBd\u2126, Pe =\n\u222b\n\u2126e\nNTfd\u2126 +\n\u222b\nSe\u03c3 NTX\u0304dS. (30)\nNumber of Gauss points in one element\nNumber of components of parameters\nMesh size\nNode size\nTransposed convolution\na\nd\nb\nc\nLifting operation\nFourier layers\nProjection operation\nMask operation\nDistribution-shift\nNode solution\nNeural operator module\nAlignment operation\nFeature engineering\nFeature engineering\nWe have \u03a0\u0303 = 1 2 aT \u2211\ne\n( GeTKeGe ) a\u2212 aT \u2211\ne\n( GeTPe ) , (31)\nand K = \u2211\ne\nGeTKeGe, P = \u2211\ne\nGeTPe. (32)\nIn FEMs, element stiffness matrices in Eq. 30 are first obtained by numerical integration and then are assembled together with Eq. 32 to form the global stiffness matrix. On the other hand, the whole process of Ritz approach is matrix-free, i.e., it does not calculate any stiffness matrices or restore them, which is elaborated in the following paragraphs. The formulation of element stiffness matrix given by Gaussian quadrature method is:\nKe =\n\u222b\n\u2126e\nBTDBd\u2126 \u2248 ng\u2211\nl=1\nHlB T l DlBl |Jel | . (33)\nSimilarly,\nPe \u2248 ng\u2211\nl=1\nHlN T l fl |Jel |+\nnSe\u03c3\u2211\nm=1\nnm\u2211\nl=1\nImlN T mlX\u0304ml \u2223\u2223\u2223JS e \u03c3 ml \u2223\u2223\u2223 . (34)\nSubstitute Eq. 33, Eq. 34 into Eq. 29, and note\n\u03b5 = Bae. (35)\nThen we have another kind of formulation to express the functional, which is slightly different from Eq. 29 and Eq. 31:\n\u03a0\u0303 = \u2211\ne\n\u03a0e \u2248 1 2\n\u2211\ne\nng\u2211\nl=1\nHla eTBTl DlBla e |Jel |\n\u2212 \u2211\ne\nng\u2211\nl=1\nHla eTNTl fl |Jel | \u2212\n\u2211\ne\nnSe\u03c3\u2211\nm=1\nnm\u2211\nl=1\nImla eTNTmlX\u0304ml \u2223\u2223\u2223JS e \u03c3 ml \u2223\u2223\u2223\nor\n\u03a0\u0303 = \u2211\ne\n\u03a0e \u2248 1 2\n\u2211\ne\nng\u2211\nl=1\nHl\u03b5 T l Dl\u03b5l |Jel |\n\u2212 \u2211\ne\nng\u2211\nl=1\nHla eTNTl fl |Jel | \u2212\n\u2211\ne\nnSe\u03c3\u2211\nm=1\nnm\u2211\nl=1\nImla eTNTmlX\u0304ml \u2223\u2223\u2223JS e \u03c3 ml \u2223\u2223\u2223\n(36)\nRitz approach uses formulations like Eq. 36 to approximate functional, while FEMs use Eq. 29 or Eq. 31. It is clear that neither the element stiffness matrix nor the global stiffness matrix appears in Eq. 36. Thus, to calculate Eq. 36, the calculation and the assembly process of the element stiffness matrices can be simply skipped.\nAccording to Eq. 18 and Eq. 28, the node solution field should satisfy\n\u03b4\u03a0\u0303 = \u2202\u03a0\u0303\n\u2202a1 \u03b4a1 +\n\u2202\u03a0\u0303 \u2202a2 \u03b4a2 + \u00b7 \u00b7 \u00b7+ \u2202\u03a0\u0303 \u2202anf \u03b4anf = 0 (37)\nEq. 37 is also called the stationary condition of \u03a0\u0303. Since \u03b4a1, \u03b4a2, \u00b7 \u00b7 \u00b7 , \u03b4anf are the virtual displacement of nodes, we have\n\u2202\u03a0\u0303 \u2202a = 0 (38)\nSubstituting Eq. 31 and Eq. 32 into Eq. 38, we have\nKa = P (39)\nFEMs approximate the functional (Eq. 31), perform variational operation (Eq. 38) offline, form and solve Eq. 39 online. On the other hand, Ritz approach of approximates the functional (Eq. 36) and performs the variational operation online after every forward propagation. Ritz approach of VOL performs the variational operation to get the residual R of the predicted node solution field:\nR = \u2202\u03a0\u0303\n\u2202a = Ka\u2212P (40)\nThe deep Ritz method [7] and the literature [41] also construct the system functional, but they just simply set functional minimization as the goal of optimization (minimization form), that is, they treat the system functional as the loss function in deep learning pipeline. Compared to the idea of deep Ritz method, our approach goes a step further. Rather than minimize the functional directly, we choose to set minimizing the norm of residual that derived from the variational operation as the optimization objective. Apart from a basic strategy of setting the norm of the residual itself as the loss function, we also turn to solve the corresponding linear system with iterative methods, by utilizing the connection between the computational mesh (see Extended Data Figure 2), the residual and the linear system to minimize the norm of the residual. Such a scheme allows us to train on a mesh. (see section 5.2 for details of optimization strategy of VOL)\nFour steps are needed to conduct numerical integration and get the system functional approximation of Eq. 36 in the forward propagation of Ritz approach:\n(1) Calculate the weighted sum of ae and other types of node-related physical quantities (such as discrete parameter field at nodes) weighted by the interpolation function and its spatial derivatives at Gauss points. Here we introduce the weighted sum of node solution. The same operations will be performed on other types of node-related physical quantities if needed in VOL. Observe Eq. 28, the left term of which can be seen as the weighted sum of ae:\nui =\nM\u2211\nj=1\nNja e j,i, (41)\nand take the derivative of both sides\n\u2202ui \u2202x =\nM\u2211\nj=1\nJe\u22121 \u2202Nj \u2202r aej,i. (42)\nEq. 41 and Eq. 42 are essentially equivalent to convolution operations on discrete node solution field, if the weight of the convolution filters is set as values of terms Nj and Je\u22121 \u2202Nj \u2202r at Gauss points. On this basis, we develop the method of calculating Eq. 41 and Eq. 42 based on the standard non-trainable convolution operation, which is implemented easily with tensor-based deep learning engines:\n\u2460 Compute the value of Nj and Je\u22121 \u2202Nj \u2202r at Gauss points, organize and restore them as convolution\nfilters, which we name a trial kernel Ktrial. The trial kernel reflects the ansatz of the trial function.\n\u2461 Convolve a with the Ktrial, unit stride and no padding to get the feature map at Gauss points: [ui |G , \u2202ui\u2202x |G] = Ktrial \u229b a.\nAn illustrative example of weighted sum convolution and calculating Ktrial is given in Fig. 6a and Fig. 6d respectively.\n(2) Calculate the integrand. In step (1), we get two types of weighted sum at Gauss points of all elements, i.e., ui and \u2202ui\u2202xj , that gathered as tensors. In this step, as shown in Fig. 6b, these weighted sum tensors are first reorganized into feature maps Fphysics of physical properties in problems of concern (see section 4.1.1) to facilitate the calculation of integrand with Eq. 36 and the following Galerkin approach. Then, the integrand is then formed by calculating Eq. 36 with Fphysics. The process utilizes the idea of domain knowledge embedding in the section 2. Note that this step varies in implementation details for governing equations in different domains.\n(3) Multiply Jacobian (Fig. 6c). The Jacobian tensor that contains the values of the Jacobian at Gauss points of all elements is pre-computed, according to the configuration of all elements in the physical space. Then, the element-wise product of the integrand tensor and the Jacobian tensor are computed to obtain the integrand tensor that considers Jacobian effect.\n(4) Calculate the functional approximation of the system with numerical integration (Fig. 6c). First, perform Gaussian quadrature for all elements, i.e., calculate weighted sum of the integrand by weight of Gauss points in every element. The element of result tensor is the functional approximation of the element \u03a0e. Then, sum up \u03a0e of all elements to get the functional approximation \u03a0\u0303.\nSince the forward propagation from node solution to the functional has been constructed, the gradient of the functional to the node solution can be easily derived with automatic differentiation. A backward propagation from the functional to the node solution is conducted, to obtain the gradient of the functional with respect to the node solution. The gradient of the functional to the node solution is just the residual of the linear system, so it is denoted as the residual tensor R.\nAs discussed above,a forward-backward propagation loop between the node solution and the functional approximation has been constructed. The developed loop allows us to derive the residual of the linear system without acquiring element stiffness matrices and assembling the global stiffness matrix. The residual tensor R = \u2202\u03a0\u0303\u2202a is derived by just running the loop once, which has the same shape as the node solution."
        },
        {
            "heading": "5.1.2 Galerkin approach",
            "text": "In this section, Galerkin approach is introduced. Galerkin approach is another matrix-free method in VOL of calculating matrix-vector product. Compared to Ritz approach, Galerkin approach is much simpler and applicable even when the minimization form of PDEs does not exist. In contrast to Ritz approach, which follows a \u2192 \u03a0\u0303 \u2192 R procedure, Galerkin approach can deriveRwithout calculating \u03a0\u0303 and backpropagation.\nLike Ritz approach, Galerkin approach also needs Fphysics. So it also shares exactly the same convolution operation and kernelKtrial as Ritz approach to calculate a |G and \u2202a\u2202x |G. Following classical Galerkin method, Galerkin approach first constructs the test function v in the deep learning engine. Here we simply consider v as the basis of Vn. Observing v of one node in Fig. 6d, which has compact support and is non-zero over the adjacent elements of the node, we find it possible to arrange value v and \u2202v\u2202x at Gauss points into groups of filters according to the following rules:\n(1) For each component of v and \u2202v\u2202x , the value at the same Gauss point of all adjacent elements of the node is put into a separate filter.\n(2) Value from different elements is arranged in the same order for all filters.\nWe name the filters as the test kernel Ktest, a 2-dim example of which is given in 6d. Sometimes we may need to construct more sophisticated filters than v and \u2202v\u2202x . For example, in elasticity we need to construct virtual strain as a test kernel Ktest = \u03b4\u03b5 = 12(vi,j + vj,i). In this regard, we just calculate the components of the desired virtual quantity.\nAfter constructing Ktest, we can express the weak form of Eq. 4 with a single standard convolution operation on Fphysics with the kernel Ktest (Fig. 6e). We also need to use the Jacobian tensor to scale Fphysics\nbefore the convolution. To ensure that our operation is valid for nodes on the boundary, we need to add a zero padding to Fphysics, which represent a layer of elements wrapped around the solution domain with their Fphysics set to 0. The convolution stride is set to 1.\nIn VOL, we use Ritz approach to calculate quadratic forms in SD and CG iterations. System residual and other matrix-vector product (like Kp in Algorithm 2 and 3) are mainly calculated with Galerkin approach. We do not use Ritz approach to calculate system residual because doing so would bring additional backpropagation operations and cannot be extended to more general cases where the minimization form of PDEs do not exist."
        },
        {
            "heading": "5.2 Optimization strategy",
            "text": "The optimization objective of VOL is to find parameters of the neural operator that minimize the average of residual norm of the whole training set:\nmin ai\n1\nD\n\u2211 ||Ri||Di=1\ns.t. Ri = Kai \u2212P (43)\nFor implementation, the residual R is treated as a tensor. Thus, we just calculate the norm of the flattened R as the residual norm. The element of the residual tensor is the just the residual of corresponding equation of the linear system Eq. 39, the same one that solved by FEMs, as shown in Extended Data Fig. 2. Thus, the goal of VOL (Eq. 43) has turned to solve all linear systems derived from the parameters of PDEs in the training set. Note that the residual tensor R now contains all residuals of the linear system. We need to zero out those elements in R that correspond to the constrained part of a, so that the coefficient matrix of the linear system is non-singular. Thus, an additional mask operation (Fig. 5) needs to be imposed on the residual tensor R in line 6.\nIn the framework of VOL, we mainly consider two optimization strategies, direct minimization (VOL+DM) and iterative update to optimize Eq. 43.\n\u2022 Direct minimization. VOL+DM sets the residual norm as the loss function directly, which is similar to many existing domain knowledge embedding methods [37, 9, 38]. But according to our experimental results, see section 4.5, VOL+DM performs not as great as the iterative update.\n\u2022 Iterative update. In this Article, we consider two common cases of the iterative update, i.e., the steepest decent update (SD) and the conjugate gradient update (CG). They are wrapped as Opt used in line 7 of Algorithm 1. Instead of running the iterative method until it converges, we only use a fixed number of update steps of the iterative method in one forward propagation, which is more computationally efficient and proves to be effective in our experiments (See section 4.2, where VOL+CG(2) shows a power law). SD and CG with n steps update are shown in Algorithm 2 and Algorithm 3 respectively. They take all of the residual tensors of jth batch Rbsj , the prediction of current batch absj and input parameter Ubsj as input, and return the update of current prediction \u2206absj . To calculate RTKR in Algorithm 2 and pTKp in Algorithm 3, it is also unnecessary to obtain K. All we need to do is to use Ritz approach to R, even with a more simplified calculation than Eq. 36:\nRTKR = \u2211\ne\nng\u2211\nl=1\nHlR eTBTl DlBlR e |Jel |\npTKp = \u2211\ne\nng\u2211\nl=1\nHlp eTBTl DlBlp e |Jel | (44)\nSimilarly, the forward-backward propagation loop between a and \u03a0\u0303temp can also be constructed by Ritz approach to derive the residual tensor of the next update step R in line 10 of Algorithm 2, and between p and pTKp to derive Kp in line 9 of Algorithm 3. Besides, we can also choose Galerkin approach to operate on a, p to derive R and Kp. For efficiency, we always use Galerkin approach to calculate matrix-vector products. Then the same mask operation is also imposed on R and Kp respectively. With the update of current prediction \u2206absj , we can manage to update the parameter \u03d1 of the neural operator module. Here, we adopt a simple way to derive the update of \u03d1. First, we assume absj +\u2206absj to be a \u201dprovisional label\u201d of the current batch. Then, we can define the loss metric that corresponds to the provisional label a\u0302bsj = absj +\u2206absj and the current prediction absj , and we choose to use sum of squares error (SSE) as the loss metric:\nLSSE = 1\n2 sum\n( square ( a\u0302bsj \u2212 absj )) (45)\nThen in line 9 of Algorithm 1, The gradient of \u03d1 is calculated with the chain rule1, which is easy to implement with Function class in torch.autograd module in Pytorch [36], then, the update of \u03d1 is calculated with Optnet\n\u03d1 = \u03d1\u2212 \u03b7\u0302 \u00b7Optnet ( einsum ( \u2019 \u00b7 \u00b7 \u00b7 t, \u00b7 \u00b7 \u00b7 ->t\u2019,grad ( absj ,\u03d1 ) ,grad ( LSSE,absj )))\n= \u03d1+ \u03b7\u0302 \u00b7Optnet ( einsum ( \u2019 \u00b7 \u00b7 \u00b7 t, \u00b7 \u00b7 \u00b7 ->t\u2019,grad ( absj ,\u03d1 ) ,\u2206absj )) (46)\nAlgorithm 2 Steepest descent (n steps) Input: residual tensor R, current node solution a, input parameter U, mask operation Mask\n1: R = \u2212R 2: \u2206a = 0 3: for 1 \u2264 i \u2264 n do 4: Calculate RTKR corresponding to (U,R) 5: \u03b1 = R\nTR RTKR\n6: \u2206a+=\u03b1R 7: if n > 1 then 8: a+=\u03b1R 9: Get R with Ritz approach or Galerkin approach\n10: R = Mask (R) 11: R = \u2212R 12: end if 13: end for Return: \u2206a\n1For convenience, we consider the all trainable parameters \u03d1 to form a vector.\nAlgorithm 3 Conjugate gradient decent (n steps) Input: residual tensor R, current node solution a, input parameter U, mask operation Mask\n1: R = \u2212R 2: p = R 3: \u2206a = 0 4: for 1 \u2264 i \u2264 n do 5: Calculate pTKp corresponding to (U,p) 6: \u03b1 = R\nTR pTKp\n7: \u2206a+=\u03b1p 8: if n > 1 then 9: Get Kp with Ritz approach or Galerkin approach\n10: Kp = Mask (Kp) 11: Rnew = R\u2212 \u03b1Kp 12: \u03b2 = R\nT newRnew RTR\n13: R = Rnew 14: p = R+ \u03b2p 15: end if 16: end for Return: \u2206a"
        },
        {
            "heading": "6 Conclusion and outlook",
            "text": "Both the conventional solver and the data-driven surrogate modeling have their own merits and shortcomings. The conventional solvers, which are based on the domain knowledge, can give reliable solutions to a wide variety of PDEs, and the conventional solvers are affordable when dealing with a single instance of PDEs or small number of samples, but they often have low efficiency and bring a heavy computational burden when the mass sampling of parameters of PDEs is required, because they usually only solve a single parameter each time. On the contrary, the data-driven surrogate modeling can give reasonable prediction of solutions to a range of parameters at a fast inference speed. However, as mentioned in Introduction (see section 1), the data-driven surrogate modeling needs a data preparation stage and a model training stage, which are isolated from each other. Besides, the access to substantial labeled data brings also quite a computational burden to the data-driven surrogate modeling. In machine learning field, more efficient training and less use of labels mean lower carbon, lower dataset costs and shorter experimental cycles. In this work, a novel data-efficient paradigm that has the merits of both and complements the shortcomings has been proposed, which we refer to as the variational operator learning (VOL). Our proposed VOL is part of an important effort to reduce the carbon footprint of research in the field of machine learning for PDEs and the numerical simulation. The proposed VOL achieves matrix-free approximation of system functional and residual with Ritz and Galerkin approaches. Direct residual minimization and iterative update, as two optimization strategies, are then proposed in VOL to learn PDEs\u2019 solution operators with a label-free training set. We have conducted various experiments on the variable heat source problem, the Darcy flow problem, and variable stiffness elasticity problem with two cases to demonstrate the effectiveness of VOL. Scaling experiments show test errors of VOL also follows a power law like LLMs [23, 34], and is able to learn solution operators with satisfactory results provided with enough cheap unlabeled data. Resolution experiments show VOL can learn solution operators largely efficiently when the solution field is discretized at different resolutions. We then design comparative experiments to verify generalization benefits of VOL, where we observe VOL has more superiority than classical iteration methods as the resolution increases. However, we also observe that, compared to classical iterative methods, the convergence speed is not significantly improved by the current\nVOL algorithm, and the test errors of VOL have little improvement with larger number of update steps. Though we focus on training a neural operator module with iterative updates for the real-time inference, rather than accelerating the iterative solvers in the current work, it is still necessary to utilize techniques like preconditioners [2], multigrid methods [49], neural-accelerated solvers [45, 5], etc., to accelerate iterations. We then conduct comparative experiments for optimization strategies of VOL and the conventional datadriven strategy, results show the smaller residual norm does not refer to a smaller test error. While VOL achieves a smaller residual norm, it has a slightly larger test error than the data-driven strategy with the same training settings.\nThere are several aspects to be explored for the proposed VOL in the future work. Although VOL shows promise for label-free training of neural operators, it uses standard convolution operations, which limits its application on solution domains of arbitrary shapes. Graph operations and meshless methods will be considered in our future work to enhance the flexibility of the proposed VOL to handle solution domains with arbitrary shapes. In this Article, we mainly consider heat transfer, Darcy flow and elasticity physics at their steady-state cases, and in the future work, we will consider VOL on more complex physics phenomena, such as fluid dynamics, electromagnetics, and quantum mechanics. More iteration techniques and neural architectures will also be considered in VOL, such as generalized minimum residual method [40], multigrid algorithms [49, 51], factorized Fourier neural operators [48] and geometry-informed neural operators [30]."
        },
        {
            "heading": "7 Acknowledgments",
            "text": "This work was supported by National Key Research and Development Program of China (2021YFF0306404), and National Natural Science Foundation of China (U21A20429 and U11772078)."
        },
        {
            "heading": "8 Author contributions",
            "text": "T.X. and P.H. contributed to the original idea and design of the research. T.X. and D.L generated the datasets. T.X were responsible for software, data curation and formal analysis. P.H. acquired funding, and was responsible for the administration, resources and supervision of the project. All authors wrote, reviewed and edited the manuscript."
        }
    ],
    "title": "Variational operator learning: A unified paradigm marrying training neural operators and solving partial differential equations",
    "year": 2023
}