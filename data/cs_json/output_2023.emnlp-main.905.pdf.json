{
    "abstractText": "We propose a method that ensembles N -best hypotheses to improve natural language generation. Previous studies have achieved notable improvements in generation quality by explicitly reranking N -best candidates. These studies assume that there exists a hypothesis of higher quality. We expand the assumption to be more practical as there exist partly higher quality hypotheses in the N -best yet they may be imperfect as the entire sentences. By merging these high-quality fragments, we can obtain a higher-quality output than the single-best sentence. Specifically, we first obtain N -best hypotheses and conduct token-level quality estimation. We then apply tokens that should or should not be present in the final output as lexical constraints in decoding. Empirical experiments on paraphrase generation, summarisation, and constrained text generation confirm that our method outperforms the strong N -best reranking methods.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ryota Miyano"
        },
        {
            "affiliations": [],
            "name": "Tomoyuki Kajiwara"
        },
        {
            "affiliations": [],
            "name": "Yuki Arase"
        }
    ],
    "id": "SP:5bf78bd270cadaf76307d3fb1e9f619384eed693",
    "references": [
        {
            "authors": [
                "Sumanta Bhattacharyya",
                "Amirmohammad Rooshenas",
                "Subhajit Naskar",
                "Simeng Sun",
                "Mohit Iyyer",
                "Andrew McCallum."
            ],
            "title": "Energy-based reranking: Improving neural machine translation using energybased models",
            "venue": "Proceedings of the 59th Annual",
            "year": 2021
        },
        {
            "authors": [
                "Pawe\u0142 Budzianowski",
                "Tsung-Hsien Wen",
                "Bo-Hsiang Tseng",
                "I\u00f1igo Casanueva",
                "Stefan Ultes",
                "Osman Ramadan",
                "Milica Ga\u0161i\u0107."
            ],
            "title": "MultiWOZ - a largescale multi-domain Wizard-of-Oz dataset for taskoriented dialogue modelling",
            "venue": "Proceedings of the",
            "year": 2018
        },
        {
            "authors": [
                "Rajen Chatterjee",
                "Matteo Negri",
                "Marco Turchi",
                "Marcello Federico",
                "Lucia Specia",
                "Fr\u00e9d\u00e9ric Blain."
            ],
            "title": "Guiding neural machine translation decoding with external knowledge",
            "venue": "Proceedings of the Second Conference on Machine Translation, pages 157\u2013168,",
            "year": 2017
        },
        {
            "authors": [
                "Mohammad Dehghan",
                "Dhruv Kumar",
                "Lukasz Golab."
            ],
            "title": "GRS: Combining generation and revision in unsupervised sentence simplification",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022, pages 949\u2013960, Dublin, Ireland. Association",
            "year": 2022
        },
        {
            "authors": [
                "Bryan Eikema",
                "Wilker Aziz."
            ],
            "title": "Sampling-based approximations to minimum Bayes risk decoding for neural machine translation",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 10978\u201310993, Abu",
            "year": 2022
        },
        {
            "authors": [
                "Mihail Eric",
                "Rahul Goel",
                "Shachi Paul",
                "Abhishek Sethi",
                "Sanchit Agarwal",
                "Shuyang Gao",
                "Adarsh Kumar",
                "Anuj Goyal",
                "Peter Ku",
                "Dilek Hakkani-Tur"
            ],
            "title": "MultiWOZ 2.1: A consolidated multi-domain dialogue dataset with state corrections and state tracking",
            "year": 2020
        },
        {
            "authors": [
                "Patrick Fernandes",
                "Ant\u00f3nio Farinhas",
                "Ricardo Rei",
                "Jos\u00e9 G.C. de Souza",
                "Perez Ogayo",
                "Graham Neubig",
                "Andre Martins."
            ],
            "title": "Quality-aware decoding for neural machine translation",
            "venue": "Proceedings of the 2022 Conference of the North American Chap-",
            "year": 2022
        },
        {
            "authors": [
                "Karl Moritz Hermann",
                "Tom\u00e1s Kocisk\u00fd",
                "Edward Grefenstette",
                "Lasse Espeholt",
                "Will Kay",
                "Mustafa Suleyman",
                "Phil Blunsom."
            ],
            "title": "Teaching machines to read and comprehend",
            "venue": "NeurIPS, pages 1693\u20131701.",
            "year": 2015
        },
        {
            "authors": [
                "Chris Hokamp",
                "Qun Liu."
            ],
            "title": "Lexically constrained decoding for sequence generation using grid beam search",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1535\u20131546,",
            "year": 2017
        },
        {
            "authors": [
                "Tomoyuki Kajiwara."
            ],
            "title": "Negative lexically constrained decoding for paraphrase generation",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6047\u2013 6052, Florence, Italy. Association for Computational",
            "year": 2019
        },
        {
            "authors": [
                "Philipp Koehn."
            ],
            "title": "Statistical significance tests for machine translation evaluation",
            "venue": "Proceedings of the",
            "year": 2004
        },
        {
            "authors": [
                "Philipp Koehn",
                "Rebecca Knowles."
            ],
            "title": "Six challenges for neural machine translation",
            "venue": "Proceedings of the First Workshop on Neural Machine Translation, pages 28\u201339, Vancouver. Association for Computational Linguistics.",
            "year": 2017
        },
        {
            "authors": [
                "Ann Lee",
                "Michael Auli",
                "Marc\u2019Aurelio Ranzato"
            ],
            "title": "Discriminative reranking for neural machine translation",
            "venue": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natu-",
            "year": 2021
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Bill Yuchen Lin",
                "Wangchunshu Zhou",
                "Ming Shen",
                "Pei Zhou",
                "Chandra Bhagavatula",
                "Yejin Choi",
                "Xiang Ren."
            ],
            "title": "CommonGen: A constrained text generation challenge for generative commonsense reasoning",
            "venue": "Findings of the Association for Computa-",
            "year": 2020
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "ROUGE: A package for automatic evaluation of summaries",
            "venue": "Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain. Association for Computational Linguistics.",
            "year": 2004
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized BERT pretraining approach",
            "venue": "CoRR, abs/1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Ximing Lu",
                "Sean Welleck",
                "Peter West",
                "Liwei Jiang",
                "Jungo Kasai",
                "Daniel Khashabi",
                "Ronan Le Bras",
                "Lianhui Qin",
                "Youngjae Yu",
                "Rowan Zellers",
                "Noah A. Smith",
                "Yejin Choi"
            ],
            "title": "NeuroLogic a*esque decoding: Constrained text generation with lookahead heuris",
            "year": 2022
        },
        {
            "authors": [
                "Mathias M\u00fcller",
                "Rico Sennrich."
            ],
            "title": "Understanding the properties of minimum Bayes risk decoding in neural machine translation",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint",
            "year": 2021
        },
        {
            "authors": [
                "Shashi Narayan",
                "Shay B. Cohen",
                "Mirella Lapata."
            ],
            "title": "Don\u2019t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Lan-",
            "year": 2018
        },
        {
            "authors": [
                "Nathan Ng",
                "Kyra Yee",
                "Alexei Baevski",
                "Myle Ott",
                "Michael Auli",
                "Sergey Edunov."
            ],
            "title": "Facebook FAIR\u2019s WMT19 news translation task submission",
            "venue": "Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day",
            "year": 2019
        },
        {
            "authors": [
                "Daiki Nishihara",
                "Tomoyuki Kajiwara",
                "Yuki Arase."
            ],
            "title": "Controllable text simplification with lexical constraint loss",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 260\u2013",
            "year": 2019
        },
        {
            "authors": [
                "Myle Ott",
                "Michael Auli",
                "David Grangier",
                "Marc\u2019Aurelio Ranzato"
            ],
            "title": "Analyzing uncertainty in neural machine translation",
            "venue": "In Proceedings of the 35th International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318, Philadelphia,",
            "year": 2002
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Stefan Riezler",
                "John T. Maxwell."
            ],
            "title": "On some pitfalls in automatic evaluation and significance testing for MT",
            "venue": "Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages",
            "year": 2005
        },
        {
            "authors": [
                "Abigail See",
                "Peter J. Liu",
                "Christopher D. Manning."
            ],
            "title": "Get to the point: Summarization with pointergenerator networks",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1073\u2013",
            "year": 2017
        },
        {
            "authors": [
                "Libin Shen",
                "Anoop Sarkar",
                "Franz Josef Och."
            ],
            "title": "Discriminative reranking for machine translation",
            "venue": "Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics:",
            "year": 2004
        },
        {
            "authors": [
                "Felix Stahlberg",
                "Bill Byrne"
            ],
            "title": "On NMT search errors and model errors: Cat got your tongue",
            "venue": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan-",
            "year": 2019
        },
        {
            "authors": [
                "Junya Takayama",
                "Tomoyuki Kajiwara",
                "Yuki Arase."
            ],
            "title": "DIRECT: Direct and indirect responses in conversational text corpus",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1980\u20131989, Punta Cana, Dominican Republic.",
            "year": 2021
        },
        {
            "authors": [
                "Ramakrishna Vedantam",
                "C. Lawrence Zitnick",
                "Devi Parikh."
            ],
            "title": "Cider: Consensus-based image description evaluation",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).",
            "year": 2015
        },
        {
            "authors": [
                "Sean Welleck",
                "Ximing Lu",
                "Peter West",
                "Faeze Brahman",
                "Tianxiao Shen",
                "Daniel Khashabi",
                "Yejin Choi."
            ],
            "title": "Generating sequences by learning to self-correct",
            "venue": "The Eleventh International Conference on Learning Representations.",
            "year": 2023
        },
        {
            "authors": [
                "Teven Le Scao",
                "Sylvain Gugger",
                "Mariama Drame",
                "Quentin Lhoest",
                "Alexander Rush."
            ],
            "title": "Transformers: State-of-the-art natural language processing",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System",
            "year": 2020
        },
        {
            "authors": [
                "Yilin Yang",
                "Liang Huang",
                "Mingbo Ma."
            ],
            "title": "Breaking the beam search curse: A study of (re-)scoring methods and stopping criteria for neural machine translation",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Process-",
            "year": 2018
        },
        {
            "authors": [
                "Kyra Yee",
                "Yann Dauphin",
                "Michael Auli."
            ],
            "title": "Simple and effective noisy channel modeling for neural machine translation",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
            "year": 2019
        },
        {
            "authors": [
                "Tatsuya Zetsu",
                "Tomoyuki Kajiwara",
                "Yuki Arase."
            ],
            "title": "Lexically constrained decoding with edit operation prediction for controllable text simplification",
            "venue": "Proceedings of the Workshop on Text Simplification, Accessibility, and Readability (TSAR-2022),",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 14653\u201314661 December 6-10, 2023 \u00a92023 Association for Computational Linguistics"
        },
        {
            "heading": "1 Introduction",
            "text": "While the beam search is one of the most common decoding methods in natural language generation, it suffers from the beam search curse (Koehn and Knowles, 2017; Yang et al., 2018; Ott et al., 2018; Stahlberg and Byrne, 2019) where a large beam size degrades the quality of generation. As a remedy to this problem, previous studies explored better alternatives from N -best hypotheses (Fernandes et al., 2022) as represented as reranking and minimum Bayes decoding (M\u00fcller and Sennrich, 2021; Eikema and Aziz, 2022), which only modify the decoding procedures. There are two types of reranking approaches. Discriminative methods train rerankers to predict specific evaluation metric scores of each hypothesis (Shen et al., 2004; Bhattacharyya et al., 2021; Lee et al., 2021). In contrast, generative methods use generic rerankers that have been used for other purposes, such as language models (Yee et al., 2019; Ng et al., 2019).\nDifferent from methods that involve computationally expensive model training such as the minimum risk training (M\u00fcller and Sennrich, 2021; Eikema and Aziz, 2022), these ranking-based methods are efficient and easily applicable to trained models.\nNonetheless, these reranking methods assume that there is a single hypothesis of higher quality in the N -best, which may not be practical depending on the generation model and also inputs. Therefore, we enhance the assumption; there should be candidates that are partly high-quality but may be imperfect as the entire sentences. Our method identifies and merges these higher-quality fragments to derive a high-quality output using lexically constrained decoding (Lu et al., 2022). Specifically, our method trains a token-level quality estimator that predicts whether a token in a hypothesis should be or should not be included in the final output. It then uses the quality estimation (QE) results of the N -best hypotheses to compose positive and negative lexical constraints and generates the final output using the generation model.\nAs a contribution of this study, we propose the N -best ensembling method for improving the quality of language generation, which is easy to apply to a variety of language generation tasks. Empirical experiments on paraphrasing (Takayama et al., 2021), summarisation (See et al., 2017; Hermann et al., 2015; Narayan et al., 2018), and constrained text generation (Lin et al., 2020) confirm that our assumption holds and the proposed method outperforms strong reranking-based methods."
        },
        {
            "heading": "2 Preliminary: Lexically Constrained Decoding",
            "text": "Lexically constrained decoding has been employed in various language generation tasks to apply taskspecific knowledge on generation, e.g., for machine translation using a bilingual dictionary of technical terms as constraints (Chatterjee et al., 2017; Hokamp and Liu, 2017), for text simplification us-\n14653\ning difficult words as constraints (Nishihara et al., 2019; Dehghan et al., 2022; Zetsu et al., 2022), for style transfer using style-specific vocabulary as constraints (Kajiwara, 2019), and for table-to-text generation using keywords in tables as constraints (Lu et al., 2022). Different from these studies that assume the availability of task-specific knowledge, our method creates lexical constraints based purely on the N -best hypotheses of language generation.\nWe use the state-of-the-art lexically constrained decoding method, namely, NeuroLogic-A* (Lu et al., 2022). NeuroLogic-A* searches for output candidates with high generation probabilities and constraint satisfaction rates by tracking states of satisfaction by the following steps. (1) For each candidate token at a time step, NeuroLogic-A* looks ahead to future tokens to be generated. (2) Based on the look-ahead results, it computes satisfaction rates of lexical constraints and prunes the candidate tokens. (3) It groups the remaining candidates based on the states of the constraint satisfaction and selects output tokens from the best candidates in each group to preserve a broad search space."
        },
        {
            "heading": "3 Proposed Method",
            "text": "Figure 1 shows the overview of the proposed method. The main component is the token-level QE model that predicts whether each token in N - best hypotheses should be used or avoided in generating the final output. Tokens predicted as the former is included in positive constraints and those predicated as the latter are included in negative constraints to be considered by NeuroLogic-A*.\nSpecifically, we fine-tune a pretrained masked language model to conduct binary token classification as illustrated in Figure 2. For each N - best hypothesis of training sentences obtained by the language generation model, token-level labels are automatically assembled using their references. Hypothesis tokens appearing in the corresponding reference are labelled as positive and otherwise labelled as negative. At inference, the QE model takes the concatenation of a source and a\nhypothesis in the N -best as input and conducts binary classification for each token. We expect that the masked language model acquires the sense of synonyms and multi-word expressions through pre-training and transfers that knowledge to our token-level QE.\nBecause the token-level QE is contextdependent, the same token appearing in different hypotheses may be predicted both positive and negative labels, respectively. Our model determines the final label by majority voting. If the numbers of positive and negative predictions are tie, the corresponding token is excluded from lexical constraints."
        },
        {
            "heading": "4 Experimental Settings",
            "text": "The proposed method is widely applicable to language generation tasks. We thus evaluate it on paraphrasing (\u00a7 5.1), summarisation (\u00a7 5.2), and constrained text generation (\u00a7 5.3).\nProposed Method For each evaluation dataset (Table 1), we constracted a token-level QE model by fine-tuning a RoBERTa-base (Liu et al., 2019). Specifically, we sought the beam size of N by a grid search in [1, 5, 10, 20, 30, \u00b7 \u00b7 \u00b7 , 100] to achieve the best performance on the validation set measured by the corresponding evaluation metrics. When decoding with NeuroLogic-A* for the final output, we use the same beam size as baselines for fair comparison. For more details of the implementation, please refer to Appendix A.\nBaselines As the most basic baseline, we compared our method to beam search, of which size was borrowed from previous studies that tuned the value for underlying language generation models for each task. Wherever applicable, we also compared the strong discriminative and generative reranking methods. For the former, we used the Discriminative Reranker for Neural Machine Translation (DrNMT) (Lee et al., 2021)1 that predicts the distribution of sentence-level evaluation metric scores given the source and N -best hypotheses. For the latter, we used the Noisy-Channel Decoding (NCD) (Yee et al., 2019)2 that scores N -best candidates by linearly combining the probabilities of generation, target-side language model, and targetto-source generation. We trained these methods using the authors\u2019 implementations with datasets and metrics of each experiment task, where the beam sizes (the sizes of N ) were searched in the same way as our method using the validation sets.\nAblation As ablation studies, we evaluated the performance of the proposed method using only positive lexical constraints (denoted as NeuroLogicA* (P)), only negative lexical constraints (denoted as NeuroLogic-A* (N)), and both (denoted as NeuroLogic-A* (P & N)).\n1https://github.com/facebookresearch/fairseq/ tree/main/examples/discriminative_reranking_nmt\n2https://github.com/facebookresearch/fairseq/ tree/main/examples/noisychannel"
        },
        {
            "heading": "5 Experimental Results",
            "text": "This section discusses the experimental results. The details of implementation on each task are described in Appendix B."
        },
        {
            "heading": "5.1 Paraphrasing",
            "text": "Setting We used DIRECT (Direct and Indirect REsponses in Conversational Text) (Takayama et al., 2021), which provides paraphrases between indirect and direct utterances in conversation histories. We conduct both the Indirect-to-Direct and Direct-to-Indirect paraphrasing with and without the dialogue histories. Following Takayama et al. (2021), we fine-tuned BART (Lewis et al., 2020) as the underlying language generation models with the beam size of 4.\nResults Table 2 shows the test set BLEU (Papineni et al., 2002) scores. The upper rows show the experimental results of baselines and the proposed methods using the predicted lexical constraints. In all subtasks, the proposed methods outperformed the baselines. In particular, the proposed method using only positive lexical constraints (NeuroLogicA* (P)) achieves the best performance.\nThe lower rows in Table 2 show the \u2018oracle\u2019 performance of the proposed and reranking methods. The \u2018Rankingoracle\u2019 indicates the performance when selecting the hypothesis with the highest sentencelevel BLEU score against the reference. In our method, we used the \u2018oracle\u2019 lexical constraints obtained by accessing the references. As expected, all of the BLEU scores are much higher than the\nupper rows. Remarkably, NeuroLogic-A* (P & N)oracle largely outperforms Rankingoracle. This result confirms that ensembling N -best hypotheses is more effective than simply selecting the best hypothesis. It supports our assumption that there exist high-quality fragments in N -best even though they are imperfect as the entire sentences. Moreover, the impressively higher scores under the oracle setting indicate that improving the token-level QE is a promising direction as further discussed in \u00a7 6."
        },
        {
            "heading": "5.2 Summarisation",
            "text": "Setting We used the CNN/Daily Mail (See et al., 2017; Hermann et al., 2015) (version 3.0.0) and XSum (The Extreme summarisation) (Narayan et al., 2018) datasets. As underlying language generation models, we used the publicly available finetuned BART-large models on CNN/Daily Mail and XSum released by Lewis et al. (2020) with suggested beam sizes of 4 and 6, respectively.\nResults Table 3 shows the ROUGE-L (Lin, 2004) scores measured on the test sets. Note that NCD is not applicable to summarisation due to the unavailability of target-to-source generation model. For both CNN/Daily Mail and XSum, the proposed method using both positive and negative lexical\nconstraints (NeuroLogic-A* (P & N)) outperforms the baselines and achieves the highest ROUGE-L score, which has a high correlation with the human evaluation (Lin, 2004). These results confirm that the proposed method is also effective in summarisation. The full results are available in Appendix B.3."
        },
        {
            "heading": "5.3 Constrained Text Generation",
            "text": "Setting We used COMMONGEN (Lin et al., 2020) dataset that tasks to generate coherent sentences given a set of words. We fine-tuned GPT-2 (Radford et al., 2019) as the underlying language generation model with the beam size of 5 (Welleck et al., 2023). Different from paraphrasing and summarisation, there are a variety of possible generations as reflected in the multiple references of diverse contents. To adapt our QE model training to this task, we selected the single reference for each hypothesis that has the highest lexical overlap against the corresponding hypothesis.\nResults Table 4 shows the CIDEr (Vedantam et al., 2015) scores measured on the test set, where SELF-CORRECT (Welleck et al., 2023) is the stateof-the-art method.3 While our method ensembles N -best to improve the generation quality, SELFCORRECT iteratively edits the initial one-best output. As the results show, our method using only positive lexical constraints (NeuroLogic-A* (P)) outperformed SELF-CORRECT, which confirms the effectiveness of ensembling high-quality fragments in the N -best. Nonetheless, the best method is NCD for this task. We conjecture this is because considering tokens from different hypotheses may deteriorate the generation due to the diversity in acceptable outputs. This feature is also troublesome for training discriminative reranking models as implied by the inferior performance of DrNMT. In such tasks, generative reranking models like NCD may be suitable. The full results are available in Appendix B.5."
        },
        {
            "heading": "6 Discussion and Future Work",
            "text": "As we discussed in \u00a7 5, the quality of token-level QE is critical for the performance of our method. Table 6 shows the ratio of reference tokens mistakenly included in the negative constraints (P\u0304neg) and the recall of positive constraints (Rpos) in the eval-\n3As SELF-CORRECT is contemporaneous with our study, we borrowed these scores from the original paper. The unavailability of model outputs at the time of publication hindered further comparisons.\nuation tasks. The results indicate that 20% to 28% of reference tokens were in the negative constraints while the recalls of positive constraints were limited to 31% to 41%. Improvement of these metrics directly enhances our method, which constitutes our future work. We will explore a QE method to model interactions within and across hypotheses.\nTable 5 shows examples of constraints predicted by our method on the paraphrase generation task (Indirect-to-Direct transformation without history). In the first example, synonyms of \u201cawesome\u201d, \u201cgreat\u201d, and \u201cgood\u201d are predicted, while in the second example, multi-word expressions of \u201ccontact number\u201d and \u201cphone number\u201d are predicted as positive constraints. These results indicate that our QE model preserves the ability to consider these to some extent. We should need a more sophisticated model to better handle synonyms and multi-word expressions, which constitutes our future work."
        },
        {
            "heading": "Limitations",
            "text": "Our model conducts decoding twice to generate a final sentence; furthermore, the second one is lexically constrained decoding, which increases the computational cost of language generation. We measured the decoding times of the proposed and compared methods on the paraphrase generation task (Indirect-to-Direct transformation without history) under the same settings of Table 2. The programs ran on a single GPU of NVIDIA RTX A6000\nwith 48GB memory installed on a Linux server with 1TB memory and AMD EPYC 7552 CPU. Our naive implementation needs 1.9 sec/sent while DrNMT (Lee et al., 2021) and NCD (Yee et al., 2019) do 0.3 sec/sent on average. A straightforward remedy is to adaptively decide whether to conduct the second decoding based on the tokenlevel QE results. For example, if there is a hypothesis of which token-level QE results imply that it satisfies a quality standard needed by a downstream task, we can directly output the hypothesis. If all the hypotheses are unsatisfactory, we can conduct the second decoding using lexical constraints.\nCurrently, all constraints are treated equally in lexically constrained decoding, but we assume their importance can be diverse and may change depending on the status of generation. This expansion is beyond the scope of the current paper but surely worth exploring, which constitutes our future work."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was supported by JSPS KAKENHI Grant Number JP21H03564."
        },
        {
            "heading": "B Experiment Details",
            "text": ""
        },
        {
            "heading": "B.1 Paraphrasing Experiment",
            "text": "The DIRECT corpus is an extension of the multidomain, multi-turn, task-oriented dialogue corpus of MultiWOZ 2.1 (Multi-Domain Wizard-of-Oz 2.1) (Budzianowski et al., 2018; Eric et al., 2020). DIRECT provides the dialogue histories in MultiWOZ, the original responses, indirect paraphrases of the original responses, and direct paraphrases of the original responses.\nWe fine-tuned a \u2018facebook/bart-base\u20196 model using the HuggingFace Transformers library with the same setting as Takayama et al. (2021). The beam size was set to 4 following the experiments in the original paper."
        },
        {
            "heading": "B.2 Summarisation Experiment",
            "text": "The CNN/Daily Mail dataset is a collection of CNN and Daily Mail articles and highlights (summaries), and consists of about 310k news articles and highlight pairs. The average number of sentences in the CNN/Daily Mail dataset is 30.7 for articles and 3.8 for highlights. The XSum dataset is a collection of BBC articles and their summaries and consists of about 230k article-summary pairs. The average number of sentences in XSum is 19.8 for\n4https://huggingface.co/roberta-base 5https://github.com/GXimingLu/a_star_\nneurologic 6https://huggingface.co/facebook/bart-base\narticles and 1.0 for summaries. The XSum dataset requires less number of summary sentences than the CNN/Daily Mail dataset; therefore, it requires more abstract summarisation. The maximum input length of our token-level QE model is 512. If an input length exceeds that limit, we split the article into two and input to the model, and then merge the prediction results.\nAs underlying language generation models for summarisation, we used \u2018facebook/bart-large-cnn\u20197 and \u2018facebook/bart-large-xsum\u20198. These models have been fine-tuned on CNN/Daily Mail and XSum datasets, respectively. Their beam sizes are suggested as 4 and 6, respectively."
        },
        {
            "heading": "B.3 Summarisation Results",
            "text": "Table 7 shows test set results of all evaluation metrics. The bottom three rows indicate the performance when using the oracle lexical constraints created by accessing references."
        },
        {
            "heading": "B.4 Constrained Text Generation Experiment",
            "text": "The COMMONGEN dataset consists of 35, 141 concept sets associated with 77, 449 sentences. The average length of reference sentences in the COMMONGEN dataset is 10.86.\nWe fine-tuned a \u2018gpt2-large\u20199 model with the same setting as Lin et al. (2020). The evaluation metrics were computed using the official script10."
        },
        {
            "heading": "B.5 Constrained Text Generation Results",
            "text": "Table 8 shows test set results of all evaluation metrics. The bottom rows present the results under the oracle setting. Different from paraphrasing and summarisation, the oracle reranking, which chooses a hypothesis with the highest evaluation score, outperformed our methods with oracle lexical constraints. Our manual investigation confirmed that the references of a source sentence are diverse in COMMONGEN, and thus considering tokens from different references can be harmful. This result implies that the diversity in possible generations affects the performance of the proposed method.\n7https://huggingface.co/facebook/ bart-large-cnn\n8https://huggingface.co/facebook/ bart-large-xsum\n9https://huggingface.co/gpt2-large 10https://github.com/INK-USC/CommonGen"
        }
    ],
    "title": "Self-Ensemble of N -best Generation Hypotheses by Lexically Constrained Decoding",
    "year": 2023
}