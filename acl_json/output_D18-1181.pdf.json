{
    "abstractText": "Monolingual dictionaries are widespread and semantically rich resources. This paper presents a simple model that learns to compute word embeddings by processing dictionary definitions and trying to reconstruct them. It exploits the inherent recursivity of dictionaries by encouraging consistency between the representations it uses as inputs and the representations it produces as outputs. The resulting embeddings are shown to capture semantic similarity better than regular distributional methods and other dictionary-based methods. In addition, the method shows strong performance when trained exclusively on dictionary data and generalizes in one shot.",
    "authors": [
        {
            "affiliations": [],
            "name": "Tom Bosc"
        },
        {
            "affiliations": [],
            "name": "Pascal Vincent"
        }
    ],
    "id": "SP:a49b86849fddd8734895c5c6fa231282b00ad6d6",
    "references": [
        {
            "authors": [
                "Robert Alfred Amsler"
            ],
            "title": "The structure of the merriam-webster pocket dictionary",
            "year": 1980
        },
        {
            "authors": [
                "Dzmitry Bahdanau",
                "Tom Bosc",
                "Stanislaw Jastrzebski",
                "Edward Grefenstette",
                "Pascal Vincent",
                "Yoshua Bengio."
            ],
            "title": "Learning to compute word embeddings on the fly",
            "venue": "CoRR, abs/1706.00286.",
            "year": 2017
        },
        {
            "authors": [
                "Yoshua Bengio",
                "R\u00e9jean Ducharme",
                "Pascal Vincent",
                "Christian Jauvin."
            ],
            "title": "A neural probabilistic language model",
            "venue": "Journal of machine learning research, 3(Feb):1137\u20131155.",
            "year": 2003
        },
        {
            "authors": [
                "Jean-Louis Binot",
                "Karen Jensen."
            ],
            "title": "A semantic expert using an online standard dictionary",
            "venue": "Proceedings of the 10th international joint conference on Artificial intelligence-Volume 2, pages 709\u2013714. Morgan Kaufmann Publishers Inc.",
            "year": 1987
        },
        {
            "authors": [
                "Elia Bruni",
                "Nam-Khanh Tran",
                "Marco Baroni."
            ],
            "title": "Multimodal distributional semantics",
            "venue": "J. Artif. Intell. Res.(JAIR), 49(2014):1\u201347.",
            "year": 2014
        },
        {
            "authors": [
                "Nicoletta Calzolari."
            ],
            "title": "Detecting patterns in a lexical data base",
            "venue": "Proceedings of the 10th International Conference on Computational Linguistics and 22nd annual meeting on Association for Computational Linguistics, pages 170\u2013173. Association for",
            "year": 1984
        },
        {
            "authors": [
                "Xinxiong Chen",
                "Zhiyuan Liu",
                "Maosong Sun."
            ],
            "title": "A unified model for word sense representation and disambiguation",
            "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1025\u20131035.",
            "year": 2014
        },
        {
            "authors": [
                "Martin S Chodorow",
                "Roy J Byrd",
                "George E Heidorn."
            ],
            "title": "Extracting semantic hierarchies from a large on-line dictionary",
            "venue": "Proceedings of the 23rd annual meeting on Association for Computational Linguistics, pages 299\u2013304. Association for Com-",
            "year": 1985
        },
        {
            "authors": [
                "Ronan Collobert",
                "Jason Weston",
                "L\u00e9on Bottou",
                "Michael Karlen",
                "Koray Kavukcuoglu",
                "Pavel Kuksa."
            ],
            "title": "Natural language processing (almost) from scratch",
            "venue": "Journal of Machine Learning Research, 12(Aug):2493\u20132537.",
            "year": 2011
        },
        {
            "authors": [
                "Manaal Faruqui",
                "Jesse Dodge",
                "Sujay K. Jauhar",
                "Chris Dyer",
                "Eduard Hovy",
                "Noah A. Smith."
            ],
            "title": "Retrofitting Word Vectors to Semantic Lexicons",
            "venue": "arXiv:1411.4166 [cs]. ArXiv: 1411.4166.",
            "year": 2014
        },
        {
            "authors": [
                "Manaal Faruqui",
                "Yulia Tsvetkov",
                "Pushpendre Rastogi",
                "Chris Dyer."
            ],
            "title": "Problems with evaluation of word embeddings using word similarity tasks",
            "venue": "arXiv preprint arXiv:1605.02276.",
            "year": 2016
        },
        {
            "authors": [
                "Christiane Fellbaum."
            ],
            "title": "WordNet",
            "venue": "Wiley Online Library.",
            "year": 1998
        },
        {
            "authors": [
                "Lev Finkelstein",
                "Evgeniy Gabrilovich",
                "Yossi Matias",
                "Ehud Rivlin",
                "Zach Solan",
                "Gadi Wolfman",
                "Eytan Ruppin."
            ],
            "title": "Placing search in context: The concept revisited",
            "venue": "Proceedings of the 10th international conference on World Wide Web, pages 406\u2013",
            "year": 2001
        },
        {
            "authors": [
                "Daniela Gerz",
                "Ivan Vuli\u0107",
                "Felix Hill",
                "Roi Reichart",
                "Anna Korhonen."
            ],
            "title": "Simverb-3500: A largescale evaluation set of verb similarity",
            "venue": "arXiv preprint arXiv:1608.00869.",
            "year": 2016
        },
        {
            "authors": [
                "Guy Halawi",
                "Gideon Dror",
                "Evgeniy Gabrilovich",
                "Yehuda Koren."
            ],
            "title": "Large-scale learning of word relatedness with constraints",
            "venue": "Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 1406\u2013",
            "year": 2012
        },
        {
            "authors": [
                "Zellig S Harris."
            ],
            "title": "Distributional structure",
            "venue": "Word, 10(2-3):146\u2013162.",
            "year": 1954
        },
        {
            "authors": [
                "Aur\u00e9lie Herbelot",
                "Marco Baroni."
            ],
            "title": "High-risk learning: acquiring new word vectors from tiny data",
            "venue": "arXiv preprint arXiv:1707.06556.",
            "year": 2017
        },
        {
            "authors": [
                "Felix Hill",
                "Kyunghyun Cho",
                "S\u00e9bastien Jean",
                "Coline Devin",
                "Yoshua Bengio."
            ],
            "title": "Embedding word similarity with neural machine translation",
            "venue": "CoRR, abs/1412.6448.",
            "year": 2014
        },
        {
            "authors": [
                "Felix Hill",
                "Kyunghyun Cho",
                "Anna Korhonen",
                "Yoshua Bengio."
            ],
            "title": "Learning to understand phrases by embedding the dictionary",
            "venue": "arXiv preprint arXiv:1504.00548.",
            "year": 2015
        },
        {
            "authors": [
                "Felix Hill",
                "Roi Reichart",
                "Anna Korhonen."
            ],
            "title": "Simlex-999: Evaluating semantic models with (genuine) similarity estimation",
            "venue": "Computational Linguistics.",
            "year": 2016
        },
        {
            "authors": [
                "Sepp Hochreiter",
                "J\u00fcrgen Schmidhuber."
            ],
            "title": "Long short-term memory",
            "venue": "Neural computation, 9(8):1735\u20131780.",
            "year": 1997
        },
        {
            "authors": [
                "Eric H. Huang",
                "Richard Socher",
                "Christopher D. Manning",
                "Andrew Y. Ng."
            ],
            "title": "Improving word representations via global context and multiple word prototypes",
            "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguis-",
            "year": 2012
        },
        {
            "authors": [
                "Ignacio Iacobacci",
                "Mohammad Taher Pilehvar",
                "Roberto Navigli."
            ],
            "title": "Sensembed: Learning sense embeddings for word and relational similarity",
            "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the",
            "year": 2015
        },
        {
            "authors": [
                "Douwe Kiela",
                "Felix Hill",
                "Stephen Clark."
            ],
            "title": "Specializing word embeddings for similarity or relatedness",
            "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2044\u20132048.",
            "year": 2015
        },
        {
            "authors": [
                "Diederik Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980.",
            "year": 2014
        },
        {
            "authors": [
                "Jiwei Li",
                "Dan Jurafsky"
            ],
            "title": "Do multi-sense embeddings improve natural language understanding",
            "venue": "EMNLP",
            "year": 2015
        },
        {
            "authors": [
                "Tal Linzen."
            ],
            "title": "Issues in evaluating semantic spaces using word analogies",
            "venue": "arXiv preprint arXiv:1606.07736.",
            "year": 2016
        },
        {
            "authors": [
                "Bart van Merri\u00ebnboer",
                "Dzmitry Bahdanau",
                "Vincent Dumoulin",
                "Dmitriy Serdyuk",
                "David Warde-Farley",
                "Jan Chorowski",
                "Yoshua Bengio."
            ],
            "title": "Blocks and fuel: Frameworks for deep learning",
            "venue": "CoRR, abs/1506.00619.",
            "year": 2015
        },
        {
            "authors": [
                "Tomas Mikolov",
                "Ilya Sutskever",
                "Kai Chen",
                "Greg S Corrado",
                "Jeff Dean."
            ],
            "title": "Distributed representations of words and phrases and their compositionality",
            "venue": "Advances in neural information processing systems, pages 3111\u20133119.",
            "year": 2013
        },
        {
            "authors": [
                "Nikola Mrk\u0161i\u0107",
                "Ivan Vuli\u0107",
                "Diarmuid \u00d3 S\u00e9aghdha",
                "Ira Leviant",
                "Roi Reichart",
                "Milica Ga\u0161i\u0107",
                "Anna Korhonen",
                "Steve Young."
            ],
            "title": "Semantic specialisation of distributional word vector spaces using monolingual and cross-lingual constraints",
            "venue": "arXiv",
            "year": 2017
        },
        {
            "authors": [
                "Nikola Mrk\u0161i\u0107",
                "Diarmuid \u00d3 S\u00e9aghdha",
                "Blaise Thomson",
                "Milica Ga\u0161i\u0107",
                "Lina M. Rojas-Barahona",
                "PeiHao Su",
                "David Vandyke",
                "Tsung-Hsien Wen",
                "Steve Young."
            ],
            "title": "Counter-fitting word vectors to linguistic constraints",
            "venue": "Proceedings of the 2016",
            "year": 2016
        },
        {
            "authors": [
                "Roberto Navigli."
            ],
            "title": "Word sense disambiguation: A survey",
            "venue": "ACM Computing Surveys (CSUR), 41(2):10.",
            "year": 2009
        },
        {
            "authors": [
                "Roberto Navigli",
                "Simone Paolo Ponzetto."
            ],
            "title": "Babelnet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network",
            "venue": "Artificial Intelligence, 193:217\u2013 250.",
            "year": 2012
        },
        {
            "authors": [
                "Thanapon Noraset",
                "Chen Liang",
                "Larry Birnbaum",
                "Doug Downey."
            ],
            "title": "Definition Modeling: Learning to Define Word Embeddings in Natural Language",
            "venue": "AAAI, pages 3259\u20133266.",
            "year": 2017
        },
        {
            "authors": [
                "Jeffrey Pennington",
                "Richard Socher",
                "Christopher Manning."
            ],
            "title": "Glove: Global vectors for word representation",
            "venue": "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532\u20131543.",
            "year": 2014
        },
        {
            "authors": [
                "Olivier Picard",
                "Alexandre Blondin-Mass\u00e9",
                "Stevan Harnad",
                "Odile Marcotte",
                "Guillaume Chicoisne",
                "Yassine Gargouri."
            ],
            "title": "Hierarchies in dictionary definition space",
            "venue": "arXiv preprint arXiv:0911.5703.",
            "year": 2009
        },
        {
            "authors": [
                "Kira Radinsky",
                "Eugene Agichtein",
                "Evgeniy Gabrilovich",
                "Shaul Markovitch."
            ],
            "title": "A word at a time: computing word relatedness using temporal semantic analysis",
            "venue": "Proceedings of the 20th international conference on World wide web,",
            "year": 2011
        },
        {
            "authors": [
                "Anna Rogers",
                "Aleksandr Drozd",
                "Bofang Li."
            ],
            "title": "The (Too Many) Problems of Analogical Reasoning with Word Vectors",
            "venue": "Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (* SEM 2017), pages 135\u2013148.",
            "year": 2017
        },
        {
            "authors": [
                "Herbert Rubenstein",
                "John B Goodenough."
            ],
            "title": "Contextual correlates of synonymy",
            "venue": "Communications of the ACM, 8(10):627\u2013633.",
            "year": 1965
        },
        {
            "authors": [
                "Magnus Sahlgren."
            ],
            "title": "The distributional hypothesis",
            "venue": "Italian journal of linguistics, 20(1):33\u201354.",
            "year": 2008
        },
        {
            "authors": [
                "Tobias Schnabel",
                "Igor Labutov",
                "David M Mimno",
                "Thorsten Joachims"
            ],
            "title": "Evaluation methods for unsupervised word embeddings",
            "year": 2015
        },
        {
            "authors": [
                "Richard S Sutton",
                "Andrew G Barto"
            ],
            "title": "Introduction to reinforcement learning",
            "year": 1998
        },
        {
            "authors": [
                "Theano Development Team."
            ],
            "title": "Theano: A Python framework for fast computation of mathematical expressions",
            "venue": "arXiv e-prints, abs/1605.02688.",
            "year": 2016
        },
        {
            "authors": [
                "Julien Tissier",
                "Christophe Gravier",
                "Amaury Habrard."
            ],
            "title": "Dict2vec: Learning Word Embeddings using Lexical Dictionaries",
            "venue": "Conference on Empirical Methods in Natural Language Processing (EMNLP 2017), pages 254\u2013263.",
            "year": 2017
        },
        {
            "authors": [
                "Joseph Turian",
                "Lev Ratinov",
                "Yoshua Bengio."
            ],
            "title": "Word representations: a simple and general method for semi-supervised learning",
            "venue": "Proceedings of the 48th annual meeting of the association for computational linguistics, pages 384\u2013394. Association for",
            "year": 2010
        },
        {
            "authors": [
                "Ivan Vuli\u0107",
                "Nikola Mrk\u0161i\u0107."
            ],
            "title": "Specialising word vectors for lexical entailment",
            "venue": "arXiv preprint arXiv:1710.06371.",
            "year": 2017
        },
        {
            "authors": [
                "Anna Wierzbicka."
            ],
            "title": "Semantics: Primes and universals: Primes and universals",
            "venue": "Oxford University Press, UK.",
            "year": 1996
        },
        {
            "authors": [
                "John Wieting",
                "Mohit Bansal",
                "Kevin Gimpel",
                "Karen Livescu."
            ],
            "title": "Charagram: Embedding words and sentences via character n-grams",
            "venue": "arXiv preprint arXiv:1607.02789.",
            "year": 2016
        },
        {
            "authors": [
                "Yadollah Yaghoobzadeh",
                "Hinrich Sch\u00fctze."
            ],
            "title": "Intrinsic subspace evaluation of word embedding representations",
            "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 236\u2013",
            "year": 2016
        }
    ],
    "sections": [
        {
            "text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1522\u20131532 Brussels, Belgium, October 31 - November 4, 2018. c\u00a92018 Association for Computational Linguistics\n1522"
        },
        {
            "heading": "1 Introduction",
            "text": "Dense, low-dimensional, real-valued vector representations of words known as word embeddings have proven very useful for NLP tasks (Turian et al., 2010). They can be learned as a by-product of solving a particular task (Collobert et al., 2011). Alternatively, one can pretrain generic embeddings based on co-occurrence counts or using an unsupervised criterion such as predicting nearby words (Bengio et al., 2003; Mikolov et al., 2013). These methods implicitly rely on the distributional hypothesis (Harris, 1954; Sahlgren, 2008), which states that words that occur in similar contexts tend to have similar meanings.\nIt is common to study the relationships captured by word representations in terms of either similarity or relatedness (Hill et al., 2016). \u201cCoffee\u201d is related to \u201ccup\u201d as coffee is a beverage often drunk in a cup, but \u201ccoffee\u201d is not similar to \u201ccup\u201d in that coffee is a beverage and cup is a container. Methods relying on the distributional hypothesis often capture relatedness very well, reaching human performance, but fare worse in capturing sim-\nilarity and especially in distinguishing it from relatedness (Hill et al., 2016).\nIt is useful to specialize word embeddings to focus on either relation in order to improve performance on specific downstream tasks. For instance, Kiela et al. (2015) report that improvements on relatedness benchmarks also yield improvements on document classification. In the other direction, embeddings learned by neural machine translation models capture similarity better than distributional unsupervised objectives (Hill et al., 2014).\nThere is a wealth of methods that postprocess embeddings to improve or specialize them, such as retrofitting (Faruqui et al., 2014). On similarity benchmarks, they are able to reach correlation coefficients close to inter-annotator agreements. But these methods rely on additional resources such as paraphrase databases (Wieting et al., 2016) or graphs of lexical relations such as synonymy, hypernymy, and their converse (Mrk\u0161ic\u0301 et al., 2017).\nRather than relying on such curated lexical resources that are not readily available for the majority of languages, we propose a method capable of improving embeddings by leveraging the more common resource of monolingual dictionaries.1 Lexical databases such as WordNet (Fellbaum, 1998) are often built from dictionary definitions, as was proposed earlier by Amsler (1980). We propose to bypass the process of explicitly building a lexical database \u2013 during which information is structured but information is also lost \u2013 and instead directly use its detailed source: dictionary definitions. The goal is to obtain better representations for more languages with less effort.\nThe ability to process new definitions is also desirable for future natural language understanding systems. In a dialogue, a human might want to explain a new term by explaining it in his own words,\n1See Appendix A for a list of online monolingual dictionaries.\nand the chatbot should understand it. Similarly, question-answering systems should also be able to grasp definitions of technical terms that often occur in scientific writing.\nWe expect the embedding of a word to represent its meaning compactly. For interpretability purposes, it would be desirable to be able to generate a definition from that embedding, as a way to verify what information it captured. Case in point: to analyse word embeddings, Noraset et al. (2017) used RNNs to produce definitions from pretrained embeddings, manually annotated the errors in the generated definitions, and found out that more than half of the wrong definitions fit either the antonyms of the defined words, their hypernyms, or related but different words. This points in the same direction as the results of intrinsic evaluations of word embeddings: lexical relationships such as lexical entailment, similarity and relatedness are conflated in these embeddings. It also suggests a new criterion for evaluating word representations, or even learning them: they should contain the necessary information to reproduce their definition (to some degree). In this work, we propose a simple model that exploits this criterion. The model consists of a definition autoencoder: an LSTM processes the definition of a word to yield its corresponding word embedding. Given this embedding, the decoder attempts to reconstruct the bag-of-words representation of the definition. Importantly, to address and leverage the recursive nature of dictionaries \u2013 the fact that words that are used inside a definition have their own associated definition \u2013 we train this model with a consistency penalty that ensures proximity of the embeddings produced by the LSTM and those that are used by the LSTM.\nOur approach is self-contained: it yields good representations when trained on nothing but dictionary data. Alternatively, it can also leverage existing word embeddings and is then especially apt at specializing them for the similarity relation. Finally, it is also extremely data-efficient, as it permits to create representations of new words in one shot from a short definition."
        },
        {
            "heading": "2 Model",
            "text": ""
        },
        {
            "heading": "2.1 Setting and motivation",
            "text": "We suppose that we have access to a dictionary that maps words to one or several definitions. Definitions themselves are sequences of words. Our\ntraining criterion is built on the following principle: we want the model to be able to recover the definition from which the representation was built. This objective should produce similar embeddings for words which have similar definitions. Our hypothesis is that this will help capture semantic similarity, as opposed to relatedness. Reusing the previous example, \u201ccoffee\u201d and \u201ccup\u201d should get different representations in virtue of having very different definitions, while \u201ccoffee\u201d and \u201ctea\u201d should get similar representations as they are both defined as beverages and plants.\nWe chose to compute a single embedding per word in order to avoid having to disambiguate word senses. Indeed, word sense disambiguation remains a challenging open problem with mixed success on downstream task applications (Navigli, 2009). Also, recent papers have shown that a single word vector can capture polysemy and that having several vectors per word is not strictly necessary (Li and Jurafsky, 2015) (Yaghoobzadeh and Sch\u00fctze, 2016). Thus, when a word has several definitions, we concatenate them to produce a single embedding."
        },
        {
            "heading": "2.2 Autoencoder model",
            "text": "Let VD be the set of all words that are used in definitions and VK the set of all words that are defined. We let w \u2208 VK be a word and Dw = (Dw,1, . . . , Dw,T ) be its definition, where Dw,t is the index of a word in vocabulary VD. We encode such a definitionDw by processing it with an\nLSTM (Hochreiter and Schmidhuber, 1997). The LSTM is parameterized by \u2126 and a matrix E of size |VD| \u00d7m, whose ith row Ei contains an m-dimensional input embedding for the ith word of VD. These input embeddings can either be learned by the model or be fixed to a pretrained embedding. The last hidden state computed by this LSTM is then transformed linearly to yield an m-dimensional definition embedding h. Thus the encoder whose parameters are \u03b8 = {E,\u2126,W, b} computes this embedding h as\nh = f\u03b8(Dw) = W LSTME,\u2126(Dw) + b.\nThe subsequent decoder can be seen as a conditional language model trained by maximum likelihood to regenerate definition Dw given definition embedding h = f\u03b8(Dw). We use a simple conditional unigram model with a linear parametrization \u03b8\u2032 = {E\u2032, b\u2032} where E\u2032 is a |VD| \u00d7m matrix and b\u2032 is a bias vector. 2\nWe maximize the log-probability of definition Dw under that model:\nlog p\u03b8\u2032(Dw|h) = \u2211 t log p\u03b8\u2032(Dw,t|h)\n= \u2211 t log e\n\u2329 E\u2032Dw,t ,h \u232a\n+b\u2032Dw,t\u2211 k e \u3008E\u2032k,h\u3009+b\u2032k\n(1)\nwhere \u3008, \u3009 denotes an ordinary dot product. We call E\u2032 the output embedding matrix. The basic autoencoder training objective to minimize over the dictionary can then be formulated as\nJr(\u03b8 \u2032, \u03b8) = \u2212 \u2211 w\u2208VK log p\u03b8\u2032(Dw|f\u03b8(Dw))."
        },
        {
            "heading": "2.3 Consistency penalty",
            "text": "We introduced 3 different embeddings: a) definition embeddings h, produced by the definition encoder, are the embeddings we are ultimately interested in computing; b) input embeddings E are used by the encoder as inputs; c) output embeddings E\u2032 are compared to definition embeddings to yield a probability distribution over the words in the definition. We propose a soft weight-tying\n2We have tried using a LSTM decoder but it didn\u2019t yield good representations. It might overfit because the set of dictionary definitions is small. Also, using teacher forcing, we condition on ground-truth words, making it easier to predict the next words. More work is needed to address these issues.\nscheme that brings the input embeddings closer to the definition embeddings. We call this term a consistency penalty because its goal is to to ensure that the embeddings used by the encoder (input embeddings) and the embeddings produced by the encoder (definition embeddings) are consistent with each other. It is implemented as\nJp(\u03b8) = \u2211\nw\u2208VD\u2229VK d(Ew, f\u03b8(Dw))\nwhere d is a distance. In our experiments, we choose d to be the Euclidian distance. The penalty is only applied to some words because VD 6= VK. Indeed, some words are defined but are not used in definitions and some words are used in definitions but not defined. In particular, inflected words are not defined. To balance the two terms, we introduce two hyperparameters \u03bb, \u03b1 \u2265 0 and the complete objective is\nJ(\u03b8\u2032, \u03b8) = \u03b1Jr(\u03b8 \u2032, \u03b8) + \u03bbJp(\u03b8).\nWe call the model CPAE, for Consistency Penalized AutoEncoder when \u03b1 > 0 and \u03bb > 0 (see Figure 1).3\nThe consistency penalty is a cheap proxy for dealing with the circularity found in dictionary definitions. We want the embeddings of the words in definitions to be compositionally built from their definition as well. The recursive process of fetching definitions of words in definitions does not terminate, because all words are defined using other words. To counter that, our model uses input embeddings that are brought closer to definition embeddings and vice versa in an asynchronous manner.\nMoreover, if \u03bb is chosen large enough, then Ew \u2248 f\u03b8(Dw) after optimisation. This means that the definition embedding for w is close enough to the corresponding input embedding to be used by the encoder for producing other definition embeddings for other words. In that case, the model could enrich its vocabulary by computing embeddings for new words and consistently reusing them as inputs for defining other words.\nFinally, the consistency penalty can be used to leverage pretrained embeddings and bootstrap the learning process. For that purpose, the encoder\u2019s input embeddingsE can be fixed to pretrained embeddings. These provide targets to the encoder but\n3Our implementation is available at https://github.com/tombosc/cpae\nalso helps the encoder to produce better definition embeddings in virtue of using input embeddings that already contain meaningful information.\nTo summarize, the consistency penalty has several motivations. Firstly, it deals with the fact that the recursive process of building representation of words out of definitions does not terminate. Secondly, it is a way to enrich the vocabulary with new words dynamically. Finally, it is a way to integrate prior knowledge in the form of pretrained embeddings.\nIn order to study the two terms of the objective in isolation, we introduce two special cases. When \u03bb = 0 and \u03b1 > 0, the model reduces to AE for Autoencoder. When \u03b1 = 0 and \u03bb > 0, we retrieve Hill\u2019s model, as presented by Hill et al. (2015).4 Hill\u2019s model is simply a recurrent encoder that uses pretrained embeddings as targets so it only makes sense in the case we use fixed pretrained embeddings."
        },
        {
            "heading": "3 Related work",
            "text": ""
        },
        {
            "heading": "3.1 Extracting lexical knowledge from dictionaries",
            "text": "There is a long history of attempts to extract and structure the knowledge contained in dictionaries. Amsler (1980) studies the possibility of automatically building taxonomies out of dictionaries, relying on the syntactic and lexical regularities that definitions display. One relation is particularly straightforward to identify: it is the is a relation that translates to hypernymy. Dictionary definitions often contain a genus which is the hypernym of the defined word, as well as a differentia which differentiates the hypernym from the defined word. For example, the word \u201chostage\u201d is defined as \u201ca prisoner who is held by one party to insure that another party will meet specified terms\u201d, where \u201cprisoner\u201d is the genus and the rest is the differentia.\nTo extract such relations, early works by Chodorow et al. (1985) and Calzolari (1984) use string matching heuristics. Binot and Jensen (1987) operate at the syntactic parse level to detect\n4It is not exactly their model as we use Euclidian distance instead of the cosine distance or the ranking loss. They also explore several variants where the input embeddings are learned, which we didn\u2019t find to produce any improvement. We haven\u2019t experimented with the ranking loss, but the cosine distance does not seem to improve over Euclidian. Finally, they also use a simple encoder that averages word vectors, which we found to be inferior.\nthese relations. Whether based on the string representation or the parse tree of a definition, these rule-based systems have helped to create large lexical databases. We aim to reduce the manual labor involved in designing the rules and directly obtaining representations from raw definitions."
        },
        {
            "heading": "3.2 Improving word embeddings using lexical resources",
            "text": "Postprocessing methods for word embeddings use lexical resources to improve already trained word embeddings irrespective of how they were obtained. When it is used with fixed pretrained embeddings, our method can be seen as a postprocessing method.\nPostprocessing methods typically have two terms for trading off conservation of distributional information that is brought by the original vectors with the new information from lexical resources. There are two main ways to preserve distributional information: Attract-Repel (Vulic\u0301 and Mrk\u0161ic\u0301, 2017), retrofitting (Mrk\u0161ic\u0301 et al., 2017) and our method control the distance between the original vector and the postprocessed vector so that the new vector does not drift too far away from the original vector. Counter-Fitting (Mrk\u0161ic\u0301 et al., 2016) and dict2vec (Tissier et al., 2017) ensure that the neighbourhood of a vector in the original space is roughly the same as the neighbourhood in the new space.\nFinally, methods differ by the nature of the lexical resources they use. To our knowledge, dict2vec is the only technique that uses dictionaries. Other postprocessing methods use various data from WordNet: sets of synonyms and sometimes antonyms, hypernyms, and hyponyms. For instance, Lexical Entailment Attract-Repel (LEAR) uses all of these (Vulic\u0301 and Mrk\u0161ic\u0301, 2017). Other methods rely on paraphrase databases (Wieting et al., 2016)."
        },
        {
            "heading": "3.3 Dictionaries and word embeddings",
            "text": "We now turn to the most relevant works that involve dictionaries and word embeddings.\nDict2vec (Tissier et al., 2017) combines the word2vec skip-gram objective (predicting all the words that appear in the context of a target word) with a cost for predicting related words. These related words either form strong pairs or weak pairs with the target word. Strong pairs have a greater influence in the cost. They are pairs of words that are in the neighbourhood of the target word in the\noriginal embedding, as well as pairs of words for which the definitions make reference to each other. Weak pairs are pairs of words where only one word appears in the definition of the other. Unlike dict2vec, our method can be used as either a standalone or a postprocessing method (when used with pretrained embeddings). It also focuses on handling and leveraging the recursivity found in dictionary definitions with the consistency penalty whereas dict2vec ignores this aspect of the structure of dictionaries.\nBesides dict2vec, Hill et al. (2015) train neural language models to predict a pretrained word embedding given a definition. Their goal was to learn a general-purpose sentence encoder useful for downstream tasks. Noraset et al. (2017) propose the task of generating definitions based on word embeddings for interpretability purposes. Our model unifies these two approaches into an autoencoder. However, we have a different goal: that of creating or improving word representations. Their methods assume that pretrained embeddings are available to provide either targets or inputs, whereas our model is unsupervised, and the use of pretrained embeddings is optional.\nBahdanau et al. (2017) present a related model that produces embeddings from definitions such that it improves performance on a downstream task. By contrast our approach is used either stand-alone or as as a postprocessing step, to produce general-purpose embeddings at a lesser computational cost. The core novelty is the way we leverage the recursive structure of dictionaries.\nFinally, Herbelot and Baroni (2017) also aim at learning representations for word embeddings in a few shots. The method consists of fine-tuning word2vec hyperparameters and can learn in one or several passes, but it is not specifically designed to handle dictionary definitions."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Setup",
            "text": "We experiment on English to benefit from the many evaluation benchmarks available. The dictionary we use is that of WordNet (Fellbaum, 1998). WordNet contains graphs of linguistic relations such as synonymy, antonymy, hyponymy, etc. but also definitions. We emphasize that our method trains exclusively on the definitions and is thus applicable to any electronic dictionary.\nHowever, in order to evaluate the quality of em-\nbeddings on unseen definitions, WordNet relations comes in handy: we use the sets of synonyms to split the dictionary into a train set and a test set, as explained in Section 7. Moreover, WordNet has a wide coverage and high quality, so we do not need to aggregate several dictionaries as done by Tissier et al. (2017). Finally, WordNet is explicitly made available for research purposes, therefore we avoid technical and legal difficulties associated with crawling proprietary online dictionaries.\nWe do not include part of speech tags that go with definitions. WordNet does not contain function words but contains homonyms of function words. We filter these out."
        },
        {
            "heading": "4.2 Similarity and relatedness benchmarks",
            "text": "Evaluating the learned representations is a complex issue (Faruqui et al., 2016). Indeed, different evaluation methods yield different rankings of embeddings: there is no single embedding that outperforms others on all tasks (Schnabel et al., 2015) and thus no single best evaluation method.\nWe focus on intrinsic evaluation methods. In particular, we study how different models trade off similarity and relatedness. We use benchmarks which consist of pairs of words scored according to some criteria. They vary in terms of annotation guidelines, number of annotators, selection of the words, etc. To evaluate our embeddings, we score each pair by computing the cosine similarity between the corresponding word vectors. Then the predicted scores and the ground truth are ranked and the correlation between the ranks is measured by Spearman\u2019s \u03c1. We leave aside analogy prediction benchmarks as they suffer from many problems (Linzen, 2016; Rogers et al., 2017).\nWe adopt one of the methods proposed by Faruqui et al. (2016) and use separate datasets for model selection. We choose the development set to be the development set of SimVerb3500 (Gerz et al., 2016) and MEN (Bruni et al., 2014), the only benchmarks with a standard train/test split.\nWe justified our emphasis on the similarity relation in Section 1: capturing this relation remains a challenge, and we hypothesize that dictionary data should improve representations in that respect. The model selection procedure reflects that we want embeddings specialized in similarity. To do that, we set the validation loss as a weighted mean which weights SimVerb twice as MEN."
        },
        {
            "heading": "4.3 Baselines",
            "text": "The objective function presented in section 2 gives us 3 different models: CPAE, AE, and Hill\u2019s model. The objective of CPAE comprises the sum of the objective of Hill\u2019s model and of AE. We compare the CPAE model to both of these to evaluate the individual contribution of the two terms to the performance. In addition, when we use external corpora to pretrain embeddings, we compare these models to dict2vec and retrofitting. The hyperparameter search is described in Appendix C.\nThe test benchmarks for the similarity relation includes SimLex999 (Hill et al., 2016) and more particularly SimLex333, a challenging subset of SimLex999 which contains only highly related pairs but in which similarity scores vary a lot. For relatedness, we use MEN (Bruni et al., 2014), RG (Rubenstein and Goodenough, 1965), WS353 (Finkelstein et al., 2001), SCWS (Huang et al., 2012), and MTurk (Radinsky et al., 2011; Halawi et al., 2012). The evaluation is carried out by a modified version of the Word Embeddings Benchmarks project.5 Conveniently, all these benchmarks contain mostly lemmas, so we do not suffer too much from the problem of missing words.6"
        },
        {
            "heading": "5 Results in the dictionary-only setting",
            "text": "In the first evaluation round, we train models only using a single monolingual dictionary. This allows us to check our hypothesis that dictionaries contain information for capturing the similarity relation between words.\nOur baselines are regular distributional models: GloVe (Pennington et al., 2014) and word2vec (Mikolov et al., 2013). They are trained on the concatenation of defined words with their definitions.\nSuch a formatting introduces spurious cooccurrences that do not otherwise appear in free text. But these baselines are not designed for dictionaries and cannot deal with their particular structure.\nWe compare these models to the autoencoder model without (AE) and with (CPAE) the consistency penalty. In this setting, we cannot use Hill\u2019s\n5Original project available at https://github.com/kudkudak/ word-embeddings-benchmarks, modified version distributed with our code.\n6Missing words are not removed from the dataset, but they are assigned a null vector.\nmodel as it requires pretrained embeddings as targets. We also trained an additional CPAE model with pretrained word2vec embeddings trained on the concatenated definitions. The results are presented in Table 1.\nGloVe is outperformed by word2vec by a large margin so we ignore this model in later experiments. Word2vec captures more relatedness than CPAE (+10.7 on MEN-t, +13.5 on MT, +13.2 on WS353) but less similarity than CPAE. The difference in the nature of the relations captured is exemplified by the scores on SimLex333. This subset of SimLex999 focuses on pairs of words that are very related but that can be either similar or dissimilar. On this subset, CPAE fares better than word2vec (+13.1).\nThe consistency penalty improves performance on every dataset. This penalty provides targets to the encoder, but these targets are themselves learned and change during the learning process. The exact dynamics of the system are unknown. It can be seen as a regularizer because it puts strong weight-sharing constraints on both types of embeddings. It also resembles bootstrapping in reinforcement learning, which consists of building estimates of values functions on top of over estimates (Sutton and Barto, 1998).\nThe last model is the CPAE model that uses the word2vec embeddings pretrained on the dictionary data. This combination not only equals other models on some benchmarks but outperforms them, sometimes by a large margin (+6.3 on SimLex999 and +7.5 on SimVerb3500 compared to CPAE, +6.1 on SCWS, +5.4 on MT compared to word2vec). Thus, the two kinds of algorithms are complementary through the different relationships that they capture best. The pretraining helps in two different ways, by providing quality input embeddings and targets to the encoder. The pretrained word2vec targets are already remarkably good. That is why the chosen consistency penalty coefficient selected is very high (\u03bb = 64). The model can pay a small cost and deviate from the targets in order to encode information about the definitions.\nTo sum up, dictionary data contains a lot of data relevant to modeling the similarity relationship. Autoencoder based models learn different relationships than regular distributional methods. The consistency penalty is a very helpful prior and regularizer for dictionary data, as it always helps,\nregardless of what relationship we focus on. Finally, our model can drastically improve embeddings that were trained on the same data but with a different algorithm."
        },
        {
            "heading": "6 Improving pretrained embeddings",
            "text": "We have seen that CPAE with pretraining is very efficient. But does this result generalizes to other kind of pretraining data? To answer this question, we experiment using embeddings pretrained on the first 50 million tokens of a Wikipedia dump, as well as the entire Wikipedia dump. We compare our method to existing postprocessing methods such as dict2vec and retrofitting, which also aims at improving embeddings with external lexical resources.\nRetrofitting, which operates on graphs, is not tailored for dictionary data, which consists in pairs of words along with their definitions. We build a graph where nodes are words and edges between nodes correspond to the presence of one of the words into the definition of another. Obviously, we lose word order in the process.\nThe results for the small corpus are presented in Table 2. By comparing Table 2 with Table 1, we see that word2vec does worse on similarity than when trained on dictionary data, but better on relatedness. Both dict2vec and retrofitting improve with regards to word2vec on similarity benchmarks and seem roughly on par. However, dict2vec fails to improve on relatedness benchmarks, whereas retrofitting sometimes improves (as in RG, MEN, and MT), sometimes equals (SCWS) and does worse (353).\nWe do an ablation study by comparing Hill\u2019s model and AE with CPAE. Recall that Hill\u2019s model lacks the reconstruction cost while AE lacks the consistency penalty. Firstly, CPAE al-\nways improves over AE. Thus, we confirm the results of the previous section on the importance of the consistency penalty. In that setting, it is more obvious why this penalty helps, as it now provides pretrained targets to the encoder. Secondly, CPAE improves over Hill on all similarity benchmarks by a large margin (+12.2 on SL999, +13.7 on SL333, +16.1 on SV3500). It is sometimes slightly worse on relatedness benchmarks (\u22123.3 on MEN-t, \u22125.6 on MT), other times better or equal. We conclude that both terms of the CPAE objective matter.\nWe see identical trends when using the full Wikipedia dump. As expected, CPAE can still improve over even higher quality embeddings by roughly the same margins. The results are presented in Appendix D.\nRemarkably, the best model among all our experiments is CPAE in Table 1 and uses only the dictionary data. This supports our hypothesis that dictionaries contain similarity-specific information."
        },
        {
            "heading": "7 Generalisation on unseen definitions",
            "text": "A model that uses definitions to produce word representations is appealing because it could be extremely data-efficient. Unlike regular distributional methods which iteratively refine their representation as occurrences accumulate, such a model could output a representation in one shot. We now evaluate CPAE in a setting where some definitions are not seen during training.\nThe dictionary is split into train, validation (for early stopping) and test splits. The algorithm for splitting the dictionary puts words in batches. It ensures two things: firstly, that words which share at least one definition are in the same batch, and secondly, that each word in a batch is associated\nwith all its definitions. We can then group batches to build the training and the test sets such that the test set does not contain synonyms of words from the other sets. We sort the batches by the number of distinct definitions they contain. We use the largest batch returned by the algorithm as the training set: it contains mostly frequent and polysemous words. The validation and the test sets, on the contrary, contain many multiword expressions, proper nouns, and rarer words. More details are given in Appendix B.1.\nWe train CPAE only on the train split of the dictionary, with randomly initialized input embeddings. Table 3 presents the same correlation coefficients as in the previous tables but also distinguishes between two subsets of the pairs: the pairs for which all the definitions were seen during training (train) and the pairs for which at least one word was defined in the test set (test). Unfortunately, there are not enough pairs of words which both appear in the test set to be able to compute significant correlations. On small-sized benchmarks, correlation coefficients are sometimes not significant so we do not report them (when p-value > 0.01).\nThe scores of CPAE on the test pairs are quite correlated with the ground truth: except on SimLex999 and SCWS, there is no drop in correlation\ncoefficients between the two sets. The scores of Hill\u2019s model follow similar trends, but are lower on every benchmark so we do not report them. This shows that recurrent encoders are able to generalize and produce coherent embeddings as a function of other embeddings in one pass."
        },
        {
            "heading": "8 Conclusion and future work",
            "text": "We have focused on capturing the similarity relation. It is a challenging task which we have proposed to solve using dictionaries, as definitions seem to encode the relevant kind of information.\nWe have presented an alternative for learning word embeddings that uses dictionary definitions. As a definition autoencoder, our approach is selfcontained, but it can alternatively be used to improve pretrained embeddings, and includes Hill\u2019s model (Hill et al., 2015) as a special case.\nIn addition, our model leverages the inherent recursivity of dictionaries via a consistency penalty, which yields significant improvements over the vanilla autoencoder.\nOur method outperforms dict2vec and retrofitting on similarity benchmarks by a quite large margin. Unlike dict2vec, our method can be used as a postprocessing method which does not require going through the original\npretraining corpus, it has fewer hyperparameters, and it generalises to new words.\nWe see several directions for future work. Firstly, more work is needed to evaluate the representations on other languages and tasks. Secondly, solving downstream tasks requires representations for the inflected words as well. We have set aside this issue by focusing on benchmarks involving lemmas. To address it in future work, we might want to split word representations into a lexical and a morphological part. With such a split representation, we could postprocess only the lexical component, and all the words, whether inflected or not, would benefit from this. This seems desirable for postprocessing methods in general and would make them more suitable for synthetic languages.\nThirdly, dictionary defines every sense of words, so we could produce one embedding per sense (Chen et al., 2014) (Iacobacci et al., 2015). This requires potentially complicated modifications to our model as we would need to disambiguate senses inside each definition. However, some class of words might benefit a lot from such representations, for example words that can be used as different parts of speech.\nLastly, a more speculative direction could be to study iterative constructions of the set of embeddings. As our algorithm can generalize in one shot, we could start the training with a small set of words and their definitions and iteratively broaden the vocabulary and refine the representations without retraining the model. This could be useful in discovering a set of semantic primes from which one can define all the other words (Wierzbicka, 1996)."
        },
        {
            "heading": "Acknowledgements",
            "text": "We thank NSERC and Facebook for financial support, Calcul Canada for computational resources, Dzmitry Bahdanau and Stanis\u0142aw Jastrze\u0328bski for contributing to the code on which the implementation is based, the developers of Theano (Theano Development Team, 2016), Blocks and Fuel (van Merri\u00ebnboer et al., 2015) as well as Laura Ball for proofreading."
        }
    ],
    "title": "Auto-Encoding Dictionary Definitions into Consistent Word Embeddings",
    "year": 2018
}