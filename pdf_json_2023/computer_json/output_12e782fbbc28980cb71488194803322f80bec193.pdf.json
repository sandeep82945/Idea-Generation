{
    "abstractText": "Constrained Markov Decision Processes (CMDPs) are one of the common ways to model safe reinforcement learning problems, where constraint functions model the safety objectives. Lagrangian-based dual or primal-dual algorithms provide efficient methods for learning in CMDPs. For these algorithms, the currently known regret bounds in the finite-horizon setting allow for a cancellation of errors; one can compensate for a constraint violation in one episode with a strict constraint satisfaction in another. However, we do not consider such a behavior safe in practical applications. In this paper, we overcome this weakness by proposing a novel model-based dual algorithm OPTAUG-CMDP for tabular finite-horizon CMDPs. Our algorithm is motivated by the augmented Lagrangian method and can be performed efficiently. We show that during K episodes of exploring the CMDP, our algorithm obtains a regret of \u00d5( \u221a K) for both the objective and the constraint violation. Unlike existing Lagrangian approaches, our algorithm achieves this regret without the need for the cancellation of errors.",
    "authors": [
        {
            "affiliations": [],
            "name": "Adrian M\u00fcller"
        }
    ],
    "id": "SP:d11fa946536333ce128a59a8e0a2998f863a8409",
    "references": [
        {
            "authors": [
                "Jacob D Abernethy",
                "Jun-Kun Wang"
            ],
            "title": "On frank-wolfe and equilibrium computation",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Eitan Altman"
            ],
            "title": "Constrained Markov decision processes, volume 7",
            "venue": "CRC press,",
            "year": 1999
        },
        {
            "authors": [
                "Peter Auer",
                "Thomas Jaksch",
                "Ronald Ortner"
            ],
            "title": "Near-optimal regret bounds for reinforcement learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2008
        },
        {
            "authors": [
                "Qinbo Bai",
                "Amrit Singh Bedi",
                "Mridul Agarwal",
                "Alec Koppel",
                "Vaneet Aggarwal"
            ],
            "title": "Achieving zero constraint violation for constrained reinforcement learning via primal-dual approach",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Amir Beck"
            ],
            "title": "First-order methods in optimization",
            "year": 2017
        },
        {
            "authors": [
                "Richard Bellman"
            ],
            "title": "A markovian decision process",
            "venue": "Journal of Mathematics and Mechanics,",
            "year": 1957
        },
        {
            "authors": [
                "Dimitri Bertsekas",
                "Angelia Nedic",
                "Asuman Ozdaglar"
            ],
            "title": "Convex analysis and optimization, volume 1",
            "venue": "Athena Scientific,",
            "year": 2003
        },
        {
            "authors": [
                "Dimitri P Bertsekas"
            ],
            "title": "Constrained optimization and Lagrange multiplier methods",
            "venue": "Academic press,",
            "year": 2014
        },
        {
            "authors": [
                "Vivek S. Borkar"
            ],
            "title": "A convex analytic approach to markov decision processes",
            "venue": "Probability Theory and Related Fields,",
            "year": 1988
        },
        {
            "authors": [
                "Archana Bura",
                "Aria HasanzadeZonuzy",
                "Dileep Kalathil",
                "Srinivas Shakkottai",
                "Jean-Francois Chamberland"
            ],
            "title": "Dope: Doubly optimistic and pessimistic exploration for safe reinforcement learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Christoph Dann",
                "Tor Lattimore",
                "Emma Brunskill"
            ],
            "title": "Unifying pac and regret: Uniform pac bounds for episodic reinforcement learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Dongsheng Ding",
                "Kaiqing Zhang",
                "Tamer Basar",
                "Mihailo Jovanovic"
            ],
            "title": "Natural policy gradient primal-dual method for constrained markov decision processes",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Dongsheng Ding",
                "Kaiqing Zhang",
                "Tamer Ba\u015far",
                "Mihailo R Jovanovi\u0107"
            ],
            "title": "Convergence and optimality of policy gradient primal-dual method for constrained markov decision processes",
            "venue": "American Control Conference (ACC),",
            "year": 2022
        },
        {
            "authors": [
                "Dongsheng Ding",
                "Kaiqing Zhang",
                "Jiali Duan",
                "Tamer Ba\u015far",
                "Mihailo R Jovanovi\u0107"
            ],
            "title": "Convergence and sample complexity of natural policy gradient primal-dual methods for constrained mdps",
            "venue": "arXiv preprint arXiv:2206.02346,",
            "year": 2022
        },
        {
            "authors": [
                "Yonathan Efroni",
                "Nadav Merlis",
                "Mohammad Ghavamzadeh",
                "Shie Mannor"
            ],
            "title": "Tight regret bounds for model-based reinforcement learning with greedy policies",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Yonathan Efroni",
                "Shie Mannor",
                "Matteo Pirotta"
            ],
            "title": "Exploration-exploitation in constrained mdps",
            "venue": "arXiv preprint arXiv:2003.02189,",
            "year": 2020
        },
        {
            "authors": [
                "Marguerite Frank",
                "Philip Wolfe"
            ],
            "title": "An algorithm for quadratic programming",
            "venue": "Naval research logistics quarterly,",
            "year": 1956
        },
        {
            "authors": [
                "Matthieu Geist",
                "Julien P\u00e9rolat",
                "Mathieu Lauri\u00e8re",
                "Romuald Elie",
                "Sarah Perrin",
                "Olivier Bachem",
                "R\u00e9mi Munos",
                "Olivier Pietquin"
            ],
            "title": "Concave utility reinforcement learning: the mean-field game viewpoint",
            "venue": "arXiv preprint arXiv:2106.03787,",
            "year": 2021
        },
        {
            "authors": [
                "Elad Hazan",
                "Sham Kakade",
                "Karan Singh",
                "Abby Van Soest"
            ],
            "title": "Provably efficient maximum entropy exploration",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "R Magnus"
            ],
            "title": "Hestenes. Multiplier and gradient methods",
            "venue": "Journal of optimization theory and applications,",
            "year": 1969
        },
        {
            "authors": [
                "Martin Jaggi"
            ],
            "title": "Revisiting frank-wolfe: Projection-free sparse convex optimization",
            "venue": "In International conference on machine learning,",
            "year": 2013
        },
        {
            "authors": [
                "Chi Jin",
                "Tiancheng Jin",
                "Haipeng Luo",
                "Suvrit Sra",
                "Tiancheng Yu"
            ],
            "title": "Learning adversarial mdps with bandit feedback and unknown transition",
            "year": 1912
        },
        {
            "authors": [
                "Vikram Krishnamurthy"
            ],
            "title": "Self Learning Control of Constrained Markov Decision Processes: a Gradient Approach",
            "venue": "Groupe d\u2019e\u0301tudes et de recherche en analyse des de\u0301cisions,",
            "year": 2003
        },
        {
            "authors": [
                "Vikram Krishnamurthy",
                "Felisa J V\u00e1zquez Abad"
            ],
            "title": "Gradient based policy optimization of constrained markov decision processes",
            "venue": "In Stochastic Processes, Finance and Control: A Festschrift in Honor of Robert J Elliott,",
            "year": 2012
        },
        {
            "authors": [
                "Vikram Krishnamurthy",
                "Felisa Vazquez Abad"
            ],
            "title": "Real-time reinforcement learning of constrained markov decision processes with weak derivatives",
            "venue": "arXiv preprint arXiv:1110.4946,",
            "year": 2011
        },
        {
            "authors": [
                "Jingqi Li",
                "David Fridovich-Keil",
                "Somayeh Sojoudi",
                "Claire J Tomlin"
            ],
            "title": "Augmented lagrangian method for instantaneously constrained reinforcement learning problems",
            "venue": "60th IEEE Conference on Decision and Control (CDC),",
            "year": 2021
        },
        {
            "authors": [
                "Tianjiao Li",
                "Ziwei Guan",
                "Shaofeng Zou",
                "Tengyu Xu",
                "Yingbin Liang",
                "Guanghui Lan"
            ],
            "title": "Faster algorithm and sharper analysis for constrained markov decision process",
            "venue": "arXiv preprint arXiv:2110.10351,",
            "year": 2021
        },
        {
            "authors": [
                "Tao Liu",
                "Ruida Zhou",
                "Dileep Kalathil",
                "P.R. Kumar",
                "Chao Tian"
            ],
            "title": "Policy optimization for constrained mdps with provable fast global convergence, 2021a. URL https://arxiv.org/abs/2111.00552",
            "year": 2021
        },
        {
            "authors": [
                "Tao Liu",
                "Ruida Zhou",
                "Dileep Kalathil",
                "Panganamala Kumar",
                "Chao Tian"
            ],
            "title": "Learning policies with zero or bounded constraint violation for constrained mdps",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Songtao Lu"
            ],
            "title": "A single-loop gradient descent and perturbed ascent algorithm for nonconvex functional constrained optimization",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Ted Moskovitz",
                "Brendan O\u2019Donoghue",
                "Vivek Veeriah",
                "Sebastian Flennerhag",
                "Satinder Singh",
                "Tom Zahavy"
            ],
            "title": "Reload: Reinforcement learning with optimistic ascent-descent for last-iterate convergence in constrained mdps",
            "venue": "arXiv preprint arXiv:2302.01275,",
            "year": 2023
        },
        {
            "authors": [
                "Mojmir Mutny",
                "Tadeusz Janik",
                "Andreas Krause"
            ],
            "title": "Active exploration via experiment design in markov chains",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2023
        },
        {
            "authors": [
                "Santiago Paternain",
                "Luiz FO Chamon",
                "Miguel Calvo-Fullana",
                "Alejandro Ribeiro"
            ],
            "title": "Constrained reinforcement learning has zero duality gap",
            "venue": "arXiv preprint arXiv:1910.13393,",
            "year": 2019
        },
        {
            "authors": [
                "M.J.D. Powell"
            ],
            "title": "A method for nonlinear constraints in minimization problems",
            "year": 1969
        },
        {
            "authors": [
                "R Tyrrell Rockafellar"
            ],
            "title": "Augmented lagrangians and applications of the proximal point algorithm in convex programming",
            "venue": "Mathematics of operations research,",
            "year": 1976
        },
        {
            "authors": [
                "Aviv Rosenberg",
                "Yishay Mansour"
            ],
            "title": "Online convex optimization in adversarial markov decision processes",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Lior Shani",
                "Yonathan Efroni",
                "Aviv Rosenberg",
                "Shie Mannor"
            ],
            "title": "Optimistic policy optimization with bandit feedback",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Adam Stooke",
                "Joshua Achiam",
                "Pieter Abbeel"
            ],
            "title": "Responsive safety in reinforcement learning by pid lagrangian methods",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Richard S Sutton",
                "Andrew G Barto"
            ],
            "title": "Reinforcement learning: An introduction",
            "venue": "MIT press,",
            "year": 2018
        },
        {
            "authors": [
                "Jean Tarbouriech",
                "Alessandro Lazaric"
            ],
            "title": "Active exploration in markov decision processes",
            "venue": "In The 22nd International Conference on Artificial Intelligence and Statistics,",
            "year": 2019
        },
        {
            "authors": [
                "Jean Tarbouriech",
                "Shubhanshu Shekhar",
                "Matteo Pirotta",
                "Mohammad Ghavamzadeh",
                "Alessandro Lazaric"
            ],
            "title": "Active model estimation in markov decision processes",
            "venue": "In Conference on Uncertainty in Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Yangyang Xu"
            ],
            "title": "Iteration complexity of inexact augmented lagrangian methods for constrained convex programming",
            "venue": "Mathematical Programming,",
            "year": 2021
        },
        {
            "authors": [
                "Shen Yan",
                "Niao He"
            ],
            "title": "Bregman augmented lagrangian method and its acceleration",
            "venue": "Journal of Environmental Sciences (China) English Ed,",
            "year": 2020
        },
        {
            "authors": [
                "Donghao Ying",
                "Yuhao Ding",
                "Javad Lavaei"
            ],
            "title": "A dual approach to constrained markov decision processes with entropy regularization",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2022
        },
        {
            "authors": [
                "Tom Zahavy",
                "Brendan O\u2019Donoghue",
                "Guillaume Desjardins",
                "Satinder Singh"
            ],
            "title": "Reward is enough for convex mdps",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Andrea Zanette",
                "Emma Brunskill"
            ],
            "title": "Tighter problem-dependent regret bounds in reinforcement learning without domain knowledge using value function bounds",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Yan",
                "He"
            ],
            "title": "Consider a closed and convex set X \u2282 R and a closed convex function f ",
            "venue": "X \u2192 R. Furthermore, let A \u2208 R and b \u2208 R. With this,",
            "year": 2020
        },
        {
            "authors": [
                "Xu"
            ],
            "title": "2021) shows a non-asymptotic convergence result for the augmented Lagrangian method, including the case of an inexact subroutine for solving Eq. (8). Notably, the convergence result concerns the last iterate of the method. That is if f is the optimal value of Eq",
            "year": 2021
        },
        {
            "authors": [
                "issue. A"
            ],
            "title": "Review of OPTDUAL-CMDP In this section, we review the related OPTDUAL-CMDP algorithm of Efroni et al. (2020). This model-based dual algorithm is based on the dual projected gradient method (see Appendix A.1) rather than on the augmented Lagrangian method (but builds the model with the same notion",
            "year": 2020
        },
        {
            "authors": [
                "Efroni"
            ],
            "title": "Notably, this bound only covers the weak regret. This is because the dual projected gradient method does not allow for a non-ergodic convergence analysis, and its iterates will generally oscillate around an optimal feasible solution. It is worth mentioning that unlike LP-based approaches (OPTCMDP and OPTCMDP-BONUS",
            "year": 2020
        },
        {
            "authors": [
                "CMDP of Efroni"
            ],
            "title": "2020) suffers from the same problem as OPTDUAL-CMDP. Moreover, we remark that this is not just a hypothetical issue but that Lagrangian-based algorithms indeed suffer from the mentioned oscillations in practical applications (Stooke et al., 2020",
            "venue": "CMDPs and Occupancy Measures",
            "year": 2023
        },
        {
            "authors": [
                "di",
                "h(sh",
                "ah"
            ],
            "title": "Summary of CMDP notation To view the CMDP as a convex optimization problem, we will express it via the common notion of occupancy measures (Borkar",
            "year": 1988
        },
        {
            "authors": [
                "Bertsekas"
            ],
            "title": "We remark that if we assume affine constraints g and X being a polytope, then we can drop",
            "venue": "(Beck,",
            "year": 2017
        },
        {
            "authors": [
                "R c\u0303k"
            ],
            "title": "We first use the extended LP trick (Rosenberg and Mansour, 2019; Efroni et al., 2020) to switch to a convex optimization problem in the state-action-state occupancy measure. That is, in the problem above, substitute zh(s",
            "year": 2020
        },
        {
            "authors": [
                "Efroni"
            ],
            "title": "Fk\u2032\u22121 is the \u03c3-algebra induced by all random variables up to and including episode k \u2212 1",
            "venue": "Efroni et al",
            "year": 2020
        },
        {
            "authors": [
                "Zanette",
                "Brunskill"
            ],
            "title": "2019, Lemma 13) for a proof of the statement. The following lemma provides an on-policy error bound based on the value difference lemma. Together with the preliminaries from Appendix E.1, it allows us to bound the estimation error. Lemma 10 (On-policy errors; Lemma 29",
            "venue": "Efroni et al",
            "year": 2020
        },
        {
            "authors": [
                "Efroni"
            ],
            "title": "2020, Appendix A.1) give a proof of this. Note that the proof does not require any specific properties of the used policy iterates",
            "year": 2020
        },
        {
            "authors": [
                "Jin"
            ],
            "title": "\u03c0k)k\u2208[K] but only uses that the collected costs and transitions are i.i.d. across episodes. Moreover, we have the following result, allowing us to bound the estimation error. The absolute constants could be specified via a short calculation, but we omit this for brevity",
            "year": 2019
        },
        {
            "authors": [
                "Jin"
            ],
            "title": "2019, Lemma 8) for a proof. E.1.2 Regret Decomposition The simple observation that we can decompose the regrets",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "ar X\niv :2\n30 6.\n07 00\n1v 2\n[ cs\n.L G\nIn this paper, we overcome this weakness by proposing a novel model-based dual algorithm OPTAUG-CMDP for tabular finite-horizon CMDPs. Our algorithm is motivated by the augmented Lagrangian method and can be performed efficiently. We show that during K episodes of exploring the CMDP, our algorithm obtains a regret of O\u0303( \u221a K) for both the objective and the constraint violation. Unlike existing Lagrangian approaches, our algorithm achieves this regret without the need for the cancellation of errors."
        },
        {
            "heading": "1 Introduction",
            "text": "In classical reinforcement learning (RL, Sutton and Barto, 2018), the goal is to learn an optimal policy when interacting with an unknown Markov decision process (MDP, Bellman, 1957). In MDPs, an agent aims to minimize the expected cumulative cost incurred during an episode. However, the learned policy must often adhere to certain safety constraints in practical scenarios. For example, when navigating a car on a race track, one would want to avoid crossing the boundaries of the track too often. Such safety requirements are commonly modeled via constrained Markov decision processes (CMDPs, Altman, 1999). We consider the problem of learning an optimal feasible policy in a CMDP. That is, the goal of the agent is to minimize the cost while satisfying the constraints1. Since the CMDP is unknown, we formalize these desiderata by considering the regret with respect to an optimal feasible solution for the cost and the constraint violation, respectively.\nImportantly, we do not consider it sufficient to provide an agent whose cumulative cost suboptimality and cumulative constraint violation are sublinear. This is because an agent can have a negative\n1i.e., being feasible for the CMDP, which we also refer to as being safe\n16th European Workshop on Reinforcement Learning (EWRL 2023).\nconstraint violation (by being very safe but incurring a higher cost than an optimal safe policy) or a positive constraint violation (by being unsafe but incurring a lower cost than an optimal safe policy). Thus, terms from these two cases can cancel each other out, which we refer to as the so-called cancellation of errors (Efroni et al., 2020). An agent for which these cumulative terms are sublinear may violate the safety constraints heavily during learning by oscillating around an optimal safe policy. While such a method converges on average to an optimal safe policy2, it neither allows for directly extracting an optimal feasible policy nor does it guarantee safety during learning. We consider a stronger notion of regret, which overcomes this issue by considering the sum of the positive parts of the error terms instead. As pointed out by Efroni et al. (2020), it is of major theoretical interest whether Lagrangian approaches can achieve sublinear bounds for this notion of regret.\nThe approaches to learning CMDPs are split into linear programming (LP) and Lagrangian approaches3 (Altman, 1999; Efroni et al., 2020). While LP-based algorithms generally allow for sublinear regret bounds without the need for cancellations (Efroni et al., 2020), they can be expensive when dealing with large state-action spaces. In contrast, in Lagrangian methods, we can solve the optimization problem arising in each episode using dynamic programming (DP), offering a computational benefit over solving LPs. However, the currently known bounds for Lagrangian approaches only concern a weaker form of regret that allows for the aforementioned cancellation of errors. As Efroni et al. (2020) pointed out, this is due to the underlying optimization methods rather than a weakness of the analysis. The main goal of this paper is to provide a Lagrangian-based algorithm that guarantees sublinear regret without the cancellation of errors. To achieve this, the key problem we solve is stopping the agent from oscillating around an optimal safe policy. Our contributions can be summarized as follows:\n\u2022 We propose a novel model-based dual algorithm, OPTAUG-CMDP, for learning an optimal feasible policy in an unknown CMDP (Section 3). The algorithm is split into a model pre-training phase and an optimistic exploration phase motivated by the augmented Lagrangian method.\n\u2022 We show that a sub-problem required for OPTAUG-CMDP can be reformulated as a convex optimization problem. We provide an efficient algorithm to solve it (Section 3) despite the nonlinearity introduced by considering the augmented Lagrangian.\n\u2022 We prove that with high probability, during K episodes OPTAUG-CMDP achieves regrets for the cost and the constraint violations of O\u0303( \u221a K) when only highlighting the dependency on K .\nNotably, we achieve this bound for the stronger notion of regret, which does not allow for the cancellation of errors. This partly settles the open problem posed by Efroni et al. (2020)."
        },
        {
            "heading": "1.1 Related Work",
            "text": "The most relevant foundation for our work is the work by Efroni et al. (2020), which reviews modelbased algorithms for CMDPs and establishes regret bounds for them. The authors analyze the LP-based algorithms OPTCMDP and OPTCMDP-BONUS that achieve sublinear regret without cancellations. However, the Lagrangian-based algorithms they analyze, OPTDUAL-CMDP and OPTPRIMALDUAL-CMDP, only achieve sublinear regret with cancellations. This is because the oscillatory behavior of dual and primal-dual descent methods prevents the individual iterates from being approximately feasible. Therefore, the authors pose the open question of whether one can devise Lagrangian-based algorithms that do not suffer from this issue.\nThe majority of relevant work providing guarantees for Lagrangian approaches to CMDPs is concerned with model-free primal-dual algorithms (Ding et al., 2020; Bai et al., 2022; Ding et al., 2022a,b) or model-based dual algorithms (Liu et al., 2021b; Efroni et al., 2020). However, in both cases, the existing literature does not address the issue of the cancellation of errors when exploring the CMDP and thus does not provide a method for safely finding an optimal feasible policy.4 While there is work on analyzing different forms of regularization to the Lagrangian-based algorithms, their guarantees either require the cancellation of errors (Liu et al., 2021a; Li et al., 2021b) or assume access to exact value functions (Ying et al., 2022). Moskovitz et al. (2023) propose a first approach to address the cancellation of errors by replacing gradients with their optimistic gradient counterparts in well-known Lagrangian-based RL algorithms. While they show the empirical success of their methods, their theoretical analysis only covers a hypothetical algorithm with implicit\n2Here, we refer to the value functions for the underlying CMDP. 3i.e., dual and primal-dual algorithms 4That is, there are no guarantees on the constraint violations of the individual iterates for these methods.\nupdates and requires full knowledge of the CMDP. Stooke et al. (2020) address the underlying problem of oscillations of Lagrangian methods for CMDPs via PID control in the context of deep RL, providing experimental successes but no guarantees. Thus, to the best of our knowledge, none of the existing works address the open question of Efroni et al. (2020) in the setup of an unknown CMDP.\nWhile there are mentions of using the augmented Lagrangian method for CMDPs (Li et al., 2021a; Lu, 2022; Krishnamurthy, 2003; Krishnamurthy and Abad, 2011, 2012), all such works are concerned with research questions rather different from ours. The only one similar to ours is that of Li et al. (2021a). The authors propose a surrogate reward inspired by the augmented Lagrangian to promote safety during learning. However, their method significantly differs from ours as it is concerned with instantaneous constraints in an infinite-horizon CMDP. Moreover, their analysis only shows that an optimal policy for their surrogate MDP is optimal for the original CMDP (under certain assumptions)."
        },
        {
            "heading": "2 Background and Problem Formulation",
            "text": "Notation: For any n \u2208 N, we use the short-hand notation [n] to refer to the set of integers {1, . . . , n}. For any finite set X , we denote by \u2206(X) the probability simplex over X , i.e., \u2206(X) = {v \u2208 [0, 1]X |\u2211x\u2208X v(x) = 1}. For a \u2208 R, we set [a]+ := max{0, a} to be the positive part of a. For a vector b \u2208 Rn, we write [b]+ for the vector whose entries are the positive parts of the corresponding entries of b. Similarly, for two vectors a, b \u2208 Rn, we write a \u2264 b as a short-hand for ai \u2264 bi, for all i \u2208 [n]. Throughout the paper, we denote the Euclidean norm by \u2016 \u00b7 \u2016. We define a finite-horizon CMDP as a tuple M = (S,A, H, p, c, (di)i\u2208[I], (\u03b1i)i\u2208[I], s1) with the following components. S and A are the state and action space, respectively, and H > 0 denotes the horizon. Every episode consists of H steps, starting from the initial state s1 \u2208 S. At every step h \u2208 [H ], ph(s\u2032|s, a) denotes the probability of transitioning to state s\u2032 if the current state and action are s and a. Moreover, ch : S \u00d7A \u2192 [0, 1] denotes the objective cost function at step h. For i \u2208 [I], di,h : S \u00d7 A \u2192 [0, 1] refers to the cost function of the i-th constraint at step h, and \u03b1i \u2208 [0, H ] denotes the threshold for the i-th constraint. We assume the state and action space are finite, with cardinalities S and A, respectively. Furthermore, we assume the agent does not know the transition probabilities, objective costs, or constraint costs beforehand. Whenever the agent takes an action a in state s at time h, it observes costs sampled from random variables Ch(s, a) \u2208 [0, 1] and (Di,h(s, a))i\u2208[I] \u2208 [0, 1]I such that E[Ch(s, a)] = ch(s, a) and E[Di,h(s, a)] = di,h(s, a), for all i \u2208 [I]. The agent interacts with the CMDP by playing a policy \u03c0 = (\u03c0h)h\u2208[H], meaning that if in state s at step h \u2208 [H ], the agent samples its next action from \u03c0h(\u00b7|s) \u2208 \u2206(A). For an arbitrary cost function l = (lh)h\u2208[H] and transition probabilities p\n\u2032 = (p\u2032h)h\u2208[H], the expected cumulative cost under policy \u03c0 is measured by the value function defined as follows:\nV \u03c0(l, p\u2032) :=E\n[ H\u2211\nh=1\nlh(sh, ah) | s1, \u03c0, p\u2032 ] ,\nwhere (sh, ah) denotes the state-action pair at step h under transitions p \u2032 and policy \u03c0. We fix an optimal solution of the CMDP, given by a policy \u03c0\u2217, defined as follows:\n\u03c0\u2217 \u2208 argmin \u03c0\u2208\u03a0 V \u03c0(c, p) s.t. V \u03c0(di, p) \u2264 \u03b1i (\u2200i \u2208 [I]). (1)\nFor brevity, we write V \u03c0((li)i\u2208[I], p \u2032) := (V \u03c0(l1, p \u2032), . . . , V \u03c0(lI , p \u2032))T \u2208 RI in the presence of I different cost functions li = (li,h)h\u2208[H] (i \u2208 [I]), and \u03b1 := (\u03b11, . . . , \u03b1I)T \u2208 RI . Furthermore, we denote by \u03a0 := {\u03c0 = (\u03c0h)h\u2208[H]|\u03c0h : S \u2192 \u2206(A)} the entire policy space. Strong duality and dual methods: Altman (1999); Paternain et al. (2019) proved that CMDPs possess the strong duality property; i.e., given a feasible CMDP M, the following relation holds:\nV \u03c0 \u2217\n(c, p) = min \u03c0\u2208\u03a0 max \u03bb\u2208RI\n+ L(\u03c0, \u03bb) \ufe38 \ufe37\ufe37 \ufe38\nPrimal problem\n= max \u03bb\u2208RI\n+\nmin \u03c0\u2208\u03a0 L(\u03c0, \u03bb) \ufe38 \ufe37\ufe37 \ufe38\nDual problem\n, (2)\nwhere L(\u03c0, \u03bb) := V \u03c0(c, p) + \u03bbT ( V \u03c0((di)i\u2208[I], p)\u2212 \u03b1 ) denotes the Lagrangian. The strong duality property gives theoretical justification to dual methods (Altman, 1999; Efroni et al., 2020;\nPaternain et al., 2019). These methods are popular, as the dual problem can be solved via a sequence of (extended5) unconstrained MDPs, each of which can be solved efficiently via DP (as opposed to using LPs for solving a sequence of CMDPs, for which the Bellman optimality principle does not hold).\nIn this work, we solve the min-max problem in Eq. (2) using the augmented Lagrangian method. This is beneficial since the analysis of the augmented Lagrangian method allows for convergence guarantees concerning the last iterate and not just the averaged iterates. Since the occurring subproblems are not MDPs anymore, we justify in Section 3 how they can still be solved efficiently by leveraging a Frank-Wolfe scheme and DP. We are now ready to state the main problem formulation of our work.\nProblem formulation: In our setting, the agent interacts with the unknown CMDP over a fixed number of K > 0 episodes. In every episode k \u2208 [K], the agent plays a policy \u03c0k \u2208 \u03a0 and its goal is to (simultaneously) minimize its regrets, defined as follows:\nR(K; c) := \u2211\nk\u2208[K]\n[V \u03c0k(c, p)\u2212 V \u03c0\u2217(c, p)]+, (Objective strong regret)\nR(K; d) := max i\u2208[I]\n\u2211\nk\u2208[K]\n[V \u03c0k(di, p)\u2212 \u03b1i]+ . (Constraint strong regret)\nFor simplicity, we will write regret when referring to the strong regret throughout the paper. As we pointed out, existing works on Lagrangian-based algorithms (Liu et al., 2021b; Efroni et al., 2020; Bai et al., 2022; Ding et al., 2022a,b) only prove sublinear guarantees on a weaker notion of regret, defined as follows:\nR\u00b1(K; c) := \u2211\nk\u2208[K]\n(V \u03c0k(c, p)\u2212 V \u03c0\u2217(c, p)), (Objective weak regret)\nR\u00b1(K; d) := max i\u2208[I]\n\u2211\nk\u2208[K]\n(V \u03c0k(di, p)\u2212 \u03b1i) . (Constraint weak regret)\nThe weak regrets allow for the aforementioned cancellation of errors; i.e., even if they are sublinear in K , the agent can continue compensating for a constraint violation in one episode with strict constraint satisfaction in another. On the other hand, a sublinear bound on the stronger notion of regret guarantees that the agent achieves a low constraint violation in most episodes.6 While this is crucial for practical applications, providing a bound for the strong regrets is strictly more challenging than for the weaker notion."
        },
        {
            "heading": "3 Algorithm and Main Result",
            "text": "In this section, we introduce our algorithm OPTAUG-CMDP (see Algorithm 1) and state its regret guarantees in Theorem 1. In OPTAUG-CMDP, the agent interacts with the unknown CMDP over a fixed number of K > 0 episodes. To encourage exploration of the CMDP, the agent follows the well-known optimism in the face of uncertainty principle (Auer et al., 2008) and builds an optimistic estimate of the CMDP in every episode k \u2208 [K]. That is, in every episode k \u2208 [K], the agent builds optimistic estimates c\u0303k for the objective cost c, optimistic estimates d\u0303i,k for the constraint costs di, and a set of plausible transition probabilities Bpk , which we specify in the following paragraph.\nOptimistic estimates: Let nk\u22121h (s, a) := \u2211k\u22121 l=1 1{slh=s, a l h =a} count the number of times that the state-action pair (s, a) is visited at step h before episode k. Here, (slh, a l h) denotes the state-action pair visited at step h in episode l. First, we compute the empirical averages of the cost and transition\n5If the CMDP is unknown, backward induction involves an extra optimization step over the possible transitions (Jin et al., 2019).\n6Indeed, fix \u01eb > 0 and suppose R(K; d) \u2264 O\u0303( \u221a K). Then there exist at most O\u0303( \u221a K/\u01eb) episodes with a constraint violation of at least \u01eb. In other words, only a small fraction O\u0303(1/(\u01eb \u221a K)) of the iterates is not \u01eb-safe. In comparison, this is by no means guaranteed by a sublinear bound on R\u00b1(K; d).\nprobabilities as follows:\nc\u0304k\u22121h (s, a) :=\n\u2211k\u22121 l=1 C l h(s, a)1{slh=s, alh=a}\nnk\u22121h (s, a) \u2228 1 ,\nd\u0304k\u22121i,h (s, a) :=\n\u2211k\u22121 l=1 D l i,h(s, a)1{slh=s, alh=a}\nnk\u22121h (s, a) \u2228 1 (\u2200i \u2208 [I]),\np\u0304k\u22121h (s \u2032|s, a) :=\n\u2211k\u22121 l=1 ,1{slh=s, alh=a, slh+1=s\u2032}\nnk\u22121h (s, a) \u2228 1 ,\nwhere a\u2228b := max{a, b}. With this, we define the optimistic costs and the set of plausible transition probabilities as\nc\u0303k,h(s, a) := c\u0304 k\u22121 h (s, a)\u2212 \u03b2ck,h(s, a),\nd\u0303i,k,h(s, a) := d\u0304 k\u22121 i,h (s, a)\u2212 \u03b2di,k,h(s, a) (\u2200i \u2208 [I]), (3)\nBpk,h(s, a) := {p\u0303h(\u00b7|s, a) \u2208 \u2206(S) | \u2200s\u2032 \u2208 S : |p\u0303h(s\u2032|s, a)\u2212 p\u0304k\u22121h (s\u2032|s, a)| \u2264 \u03b2 p k,h(s, a, s \u2032)}, Bpk := {p\u0303 | \u2200s, a, h : p\u0303h(\u00b7|s, a) \u2208 B p k,h(s, a)}.\nHere, \u03b2ck,h(s, a) = \u03b2 d i,k,h(s, a) > 0 denote the exploration bonus for the costs and \u03b2 p k,h(s, a, s \u2032) > 0 denotes the confidence threshold for the transitions. For any \u03b4 \u2208 (0, 1), we specify the correct values for those quantities in Appendix E.1 to obtain our regret guarantees with probability at least 1 \u2212 \u03b4. In the next paragraph, we describe how the agent computes its policy in episode k.\nPolicy update: Given the optimistic CMDP at episode k, we derive the next policy \u03c0k using a scheme motivated by the augmented Lagrangian method (cf. Eqs. (4) and (5)). At the end of this section, we explain how we can perform the optimization step in Eq. (4) efficiently, up to a specified accuracy \u01ebk. For now, we treat this part of the algorithm as a black-box subroutine.\nOptimistic exploration alone with the augmented Lagrangian, however, is insufficient to obtain sublinear regret guarantees for our algorithm. For technical reasons, our analysis also requires the optimistic CMDPs with costs c\u0303k, (d\u0303i,k)i\u2208[I] and transitions p\u0303k \u2208 Bpk (cf. Eq. (4)) to be strictly feasible, in every episode k \u2208 [K]. Our analysis in Section 4.2 explains the need for this technical assumption. To address this issue, we propose a pre-training phase before the optimistic exploration phase, which we describe in the following paragraph.\nPre-training phase: In this phase, the agent repeatedly executes a fixed policy \u03c0\u0304 for K \u2032 \u2264 K episodes. The policy \u03c0\u0304 must be strictly feasible for the true CMDP, which we formally state in the following assumption.\nAssumption 1 (Strictly feasible policy). We have access to a policy \u03c0\u0304 such that V \u03c0\u0304(di, p) < \u03b1i for all i \u2208 [I]. Furthermore, we assume that the slack \u03b3, defined below, is known7:\n\u03b3 := min i\u2208[I]\n(\u03b1i \u2212 V \u03c0\u0304(di, p)) \u2208 (0, H ].\nNote that this is stronger than only assuming the existence of a strictly feasible policy. However, making this assumption is realistic in many practical setups (Liu et al., 2021b; Bura et al., 2022). For example, in the case of a race car that should not exceed the boundary of a track, it would be sufficient to have access to the policy of a car that strictly stays within the boundaries but may be arbitrarily slow. To address the technical issue mentioned earlier, we need to set K \u2032 such that the following condition holds for some \u03bd \u2208 (0, 1), with high probability:\nV \u03c0\u0304(d\u0303i,k, p\u0303k) \u2264 \u03b1i \u2212 \u03bd\u03b3 (\u2200i \u2208 [I] \u2200k \u2208 {K \u2032, . . . ,K}),\nwhere p\u0303k is defined by the update in Eq. (4). 8 In particular, the fixed policy \u03c0\u0304 is strictly feasible for the optimistic CMDP at every episode k \u2265 K \u2032. Indeed, if the agent plays \u03c0\u0304 for the first K \u2032 episodes 7In other words, there is a Slater point for the constraint set of the true CMDP. Note that knowing a lower bound instead of the exact slack \u03b3 is sufficient as well. 8For k = K\u2032, we just take any p\u0303K\u2032 \u2208 BpK\u2032\nof the algorithm with a large enough constant K \u2032, then for all future episodes, the constraint value function of \u03c0\u0304 under the estimated model is close to the constraint value function of \u03c0\u0304 under the true model. Thus, we can ensure the above condition. Leveraging an adaption of an on-policy error bound (see Appendix C), we prove that it is sufficient to set K \u2032 as follows:\nLemma 1. Suppose that Assumption 1 holds, i.e., the agent has access to a strictly feasible \u03c0\u0304 and its slack \u03b3 > 0. Fix any \u03bd \u2208 (0, 1), and suppose the agent executes \u03c0\u0304 for K \u2032 = O\u0303 ( max { S2AH3\n(1\u2212\u03bd)\u03b3 , NSAH4 (1\u2212\u03bd)2\u03b32\n})\nepisodes, where N := maxs,a,h |{s\u2032 | ph(s\u2032|s, a) > 0}| denotes the maximum number of transitions. Then, if the agent updates the optimistic CMDP based on the observations from those episodes (cf. Eq. (3)), with probability at least 1\u2212 \u03b4 the following condition is satisfied for every k \u2208 {K \u2032, . . . ,K}:\nV \u03c0\u0304(d\u0303i,k, p\u0303k) \u2264 \u03b1i \u2212 \u03bd\u03b3 (\u2200i \u2208 [I]).\nWe present the resulting OPTAUG-CMDP algorithm in Algorithm 1.\nAlgorithm 1 OPTAUG-CMDP Require: K (total number of episodes), K \u2032 \u2264 K (number of pre-training episodes), (\u03b7k)k\u2265K\u2032+1 (step sizes), (\u01ebk)k\u2265K\u2032+1 (accuracies), \u03c0\u0304 (strictly feasible policy), \u03b1 (constraint thresholds), \u03bbK\u2032+1 := 0 \u2208 RI\n// Phase 1: Pre-training the model for k = 1, . . . ,K \u2032 do\nPlay policy \u03c0k = \u03c0\u0304, update estimates of the costs c\u0303k+1, (d\u0303i,k+1)i\u2208[I] and transitions B p k+1 (Eq. (3)).\n// Phase 2: Optimistic exploration with pre-trained model for k = K \u2032 + 1, . . . ,K do\nUpdate policy (by finding \u03c0k, p\u0303k such that the objective is \u01ebk-close to the minimum):\n\u03c0k, p\u0303k := arg min \u03c0\u2208\u03a0 p\u2032\u2208Bp\nk\n(\nV \u03c0(c\u0303k, p \u2032) +\n1\n2\u03b7k \u2016[\u03bbk + \u03b7k(V \u03c0((d\u0303i,k)i\u2208[I], p\u2032)\u2212 \u03b1)]+\u20162\n)\n(4)\nUpdate dual variables:\n\u03bbk+1 := [\u03bbk + \u03b7k(V \u03c0k((d\u0303i,k)i\u2208[I], p\u0303k)\u2212 \u03b1)]+ (5)\nPlay \u03c0k , update estimates of the costs c\u0303k+1, (d\u0303i,k+1)i\u2208[I] and transitions B p k+1 (Eq. (3)).\nWe are now ready to state the regret guarantees for OPTAUG-CMDP.\nTheorem 1. Suppose that Assumption 1 holds, let \u03b4 \u2208 (0, 1) and \u03bd > 0. Then there exist K \u2032 = O\u0303 ( max { S2AH3\n(1\u2212\u03bd)\u03b3 , NSAH4 (1\u2212\u03bd)2\u03b32\n})\nand \u03b7k, \u01ebk such that with probability at least 1\u2212 \u03b4, OPTAUGCMDP achieves a total regret of\nR(K; c) = O\u0303 (\u221a NSAH4K + S2AH3 +K \u2032H ) , R(K; d) = O\u0303 (\u221a NSAH4K + S2AH3 ) .\nWe remark that to achieve this bound, using step sizes \u03b7K\u2032+k = \u0398(k 2.5) and accuracies \u01ebK\u2032+k = \u0398(1/\u03b7K\u2032+k) (when only highlighting the dependency on k) is sufficient, as we discuss in Appendix D.3.\nComparison with OPTDUAL-CMDP: Crucially, our bound holds for the stronger notion of regret. In contrast, the one for the related OPTDUAL-CMDP algorithm (see Appendix A.2) by Efroni et al. (2020) only concerns the weak regret, which allows for the cancellation of errors. Apart from this, the bound we obtain is similar in spirit. However, our regret bound does not depend on the number\nI of constraints up to polylogarithmic factors. Moreover, we get slightly different (but not worse) constants and the constant extra term K \u2032H due to the model pre-training phase. In addition, it is important to note that we can choose \u03b7k, \u01ebk in terms of \u03b3 (Assumption 1) such that in the leading term of the regret bound, there is no dependency on mini\u2208[I](\u03b1i \u2212 V \u03c0\u0304(di, p)), as opposed to OPTDUALCMDP.\nSolving the inner problem: We now elaborate on the subroutine for solving the optimization problem in Eq. (4) that defines the policy update in Algorithm 1. Importantly, we can reformulate Eq. (4) as a constrained optimization problem that is convex in the state-action-state occupancy measure (see Appendix B.1). However, the resulting problem is neither an LP nor an extended MDP (due to the nonlinear objective), which prevents solving it with a single DP or LP solver call. Albeit related, it is not a standard convex RL problem either (due to the additional optimization over the constraint set Bpk). Moreover, computing projections onto the high-dimensional domain is prohibitive, making it impossible to run projected gradient descent.\nThe projection-free method we propose in Appendix B.2 overcomes this difficulty by combining a Frank-Wolfe scheme with DP in a sequence of (extended) MDPs. In every iteration of this inner method, we consider the linear minimization step needed for a Frank-Wolfe iteration. When switching back to optimization over \u03a0 and Bpk , we can then perform this minimization step by solving an extended but unconstrained MDP via DP.9 The smoothness properties of the objective of Eq. (4) then determine the iteration complexity of the Frank-Wolfe scheme. Formally, we have the following.\nProposition 1. In episode k, fix any accuracy of \u01ebk > 0. There exists an algorithm for solving Eq. (4) such that the objective at its output (\u03c0k, p\u0303k) is \u01ebk-close to the optimum of Eq. (4), by solving O ( \u03b7kIS 2AH\n\u01ebk\n)\n(extended) MDPs via DP."
        },
        {
            "heading": "4 Sketch of the Regret Analysis",
            "text": "In this section, we outline the key steps in our proof of Theorem 1 and defer the detailed proofs to Appendix E. We will condition our regret analysis on a success event G, which we formally define in Appendix E.1. G ensures that (a) the optimistic cost estimates in Eq. (3) are, in fact, optimistic and (b) the true transitions are contained in the set of plausible models from Eq. (3), i.e.:\nc\u0303k \u2264 c, d\u0303i,k \u2264 di (\u2200i \u2208 [I]), p \u2208 Bpk,\nfor every episode k \u2208 [K]. In the following lemma, we prove that G occurs with high probability.\nLemma 2. Fix \u03b4 \u2208 (0, 1) and define the optimistic model in Eq. (3) accordingly. Then, the success event G occurs with probability at least 1\u2212 \u03b4, i.e., P [G] \u2265 1\u2212 \u03b4.\nWe proceed with the regret analysis and first split the regrets between the two phases of the algorithm:\nR(K; c) = K\u2032\u2211\nk=1 [V \u03c0k(c, p)\u2212 V \u03c0\u2217(c, p)]+ \ufe38 \ufe37\ufe37 \ufe38\nPre-Training\n+\nK\u2211\nk=K\u2032+1 [V \u03c0k(c, p)\u2212 V \u03c0\u2217(c, p)]+ \ufe38 \ufe37\ufe37 \ufe38\nOptimistic Exploration\n,\nR(K; d) \u2264 max i\u2208[I]\nK\u2032\u2211\nk=1 [V \u03c0k(di, p)\u2212 \u03b1i]+ \ufe38 \ufe37\ufe37 \ufe38\nPre-Training\n+max i\u2208[I]\nK\u2211\nk=K\u2032+1 [V \u03c0k(di, p)\u2212 \u03b1i]+ \ufe38 \ufe37\ufe37 \ufe38\nOptimistic Exploration\n.\nThen, applying Lemma 1, we can trivially bound the objective regret during the pre-training phase by K \u2032H . Since \u03c0\u0304 is strictly feasible, there is no constraint regret during pre-training. We now focus on the regrets incurred in the optimistic exploration phase. For this, we further decompose the\n9Formally, this is because a version of the Bellman optimality principle applies after dualizing the constraints, even if we need to optimize over the confidence intervals for the transitions during backward induction.\nregrets as follows (see Appendix E.1):\nR(K; c) \u2264 K \u2032H+ K\u2211\nk=K\u2032+1 [V \u03c0k(c, p)\u2212V \u03c0k(c\u0303k, p\u0303k)]+ \ufe38 \ufe37\ufe37 \ufe38\nEstimation Error\n+\nK\u2211\nk=K\u2032+1\n[V \u03c0k(c\u0303k, p\u0303k)\u2212V \u03c0 \u2217 (c, p)]+\n\ufe38 \ufe37\ufe37 \ufe38\nOptimization Error\n,\nR(K; d) \u2264 max i\u2208[I]\nK\u2211\nk=K\u2032+1 [V \u03c0k(di, p)\u2212V \u03c0k(d\u0303i,k, p\u0303k)]+ \ufe38 \ufe37\ufe37 \ufe38\nEstimation Error\n+max i\u2208[I]\nK\u2211\nk=K\u2032+1 [V \u03c0k(d\u0303i,k, p\u0303k)\u2212\u03b1i]+ \ufe38 \ufe37\ufe37 \ufe38\nOptimization Error\n.\nWe have thus decomposed the regrets into\n(A) estimation errors that are due to the estimated model, and\n(B) optimization errors that we can analyze via the underlying optimization method.\nConditioning on the success event G, we will obtain bounds sublinear in K for both parts of the decomposition. Note that we cannot adapt the analysis by Efroni et al. (2020) to achieve this goal since it only allows for bounds on the averages of the signed optimization errors. We proceed with bounding the estimation errors in the next section."
        },
        {
            "heading": "4.1 Estimation Errors (Optimistic Exploration)",
            "text": "Leveraging on-policy error bounds for optimistic exploration in MDPs (Appendix C), we establish the desired bound on the estimation errors.\nLemma 3 (Estimation errors). Let (\u03c0k) K k=K\u2032+1 be the sequence of policies obtained by OPTAUGCMDP. Then, conditioned on G, we can bound the estimation errors as follows:\nK\u2211\nk=K\u2032+1\n[V \u03c0k(c, p)\u2212 V \u03c0k(c\u0303k, p\u0303k)]+ \u2264O\u0303 (\u221a NSAH4K + S2AH3 ) ,\nmax i\u2208[I]\nK\u2211\nk=K\u2032+1\n[V \u03c0k(di, p)\u2212 V \u03c0k(d\u0303i,k, p\u0303k)]+ \u2264O\u0303 (\u221a NSAH4K + S2AH3 ) .\nWe refer to Appendix E.2 for the proof. Lemma 3 proves that the estimation errors for both the objective and constraints can indeed be bounded by a term that is sublinear in K . In the next section, we provide a bound for the optimization errors."
        },
        {
            "heading": "4.2 Optimization Errors (Optimistic Exploration)",
            "text": "Recall that by Proposition 1, our solver for the inner problem (Eq. (4)) has the following guarantee, for every episode k \u2265 K \u2032 + 1:\nLk(\u03c0k, p\u0303k) \u2264 min \u03c0\u2208\u03a0 p\u2032\u2208Bp\nk\nLk(\u03c0, p\u2032) + \u01ebk,\nwhere Lk(\u03c0, p\u2032) := V \u03c0(c\u0303k, p\u2032) + 12\u03b7k \u2016[\u03bbk + \u03b7k(V \u03c0((d\u0303i,k)i\u2208[I], p \u2032)\u2212 \u03b1)]+\u20162 denotes the objective at episode k. If the true CMDP is known, i.e., no exploration is required, then Xu (2021) proves a sublinear regret bound for the optimization error if the step sizes \u03b7k and accuracies \u01ebk are chosen suitably.10 They obtain this result by bounding the dual variables \u03bbk across the iterations k. In our setting, however, since the objective and constraint set of the optimization problem (Eq. (4)) change in every episode, we require a novel type of analysis.\nAs a first step, we show that we can bound the optimization errors in episode k by expressions that depend on the dual variables \u03bbk and \u03bbk+1.\n10That is, such that \u2211K k=K\u2032+1 1/\u03b7k = o(K) and \u2211K k=K\u2032+1 \u01ebk = o(K), see Xu (2021, Remark 7).\nLemma 4. Conditioned on G, for each k \u2208 {K \u2032 + 1, . . . ,K}, in OPTAUG-CMDP we have\nV \u03c0k(c\u0303k, p\u0303k)\u2212 V \u03c0 \u2217 (c, p) \u2264 \u01ebk + \u2016\u03bbk\u20162 \u2212 \u2016\u03bbk+1\u20162\n2\u03b7k ,\nV \u03c0k(d\u0303i,k, p\u0303k)\u2212 \u03b1i \u2264 \u03bbk+1(i)\u2212 \u03bbk(i)\n\u03b7k (\u2200i \u2208 [I]).\nTo further bound the norm of the dual iterates, for each episode k \u2265 K \u2032, we consider the k-th optimistic CMDP, which we define as follows:\nmin \u03c0\u2208\u03a0\nV \u03c0(c\u0303k, p\u0303k) s.t. V \u03c0(d\u0303i,k, p\u0303k) \u2264 \u03b1i (\u2200i \u2208 [I]). (6)\nNote that Eq. (6) indeed is a CMDP. By Lemma 1, \u03c0\u0304 is strictly feasible for Eq. (6) for all k \u2265 K \u2032, with a slack of \u2265 \u03bd\u03b3 uniformly bounded away from zero. By strong duality (Paternain et al., 2019), there exist primal-dual pairs (\u03c0\u2217k, \u03bb \u2217 k) satisfying\nV \u03c0 \u2217\nk(c\u0303k, p\u0303k) = min \u03c0\u2208\u03a0\n(\nV \u03c0(c\u0303k, p\u0303k) + (\u03bb \u2217 k) T (V \u03c0((d\u0303i,k)i\u2208[I], p\u0303k)\u2212 \u03b1) ) .\nWe formalize this with Lemma 18 in Appendix E.3.2 by using the fact that we can formulate Eq. (6) as a convex optimization problem using the LP formulation of CMDPs (Appendices A.3 and A.4). With this, we can establish the following bound on the dual iterates.\nLemma 5. Let k \u2208 {K \u2032 + 1, . . . ,K} and suppose Eq. (6) is strictly feasible for every k\u2032 \u2208 {K \u2032, . . . ,K}. Let (\u03c0\u2217k\u2032 , \u03bb\u2217k\u2032 ) be pairs of primal-optimal and dual-optimal solutions for Eq. (6). Then the iterates of OPTAUG-CMDP satisfy\n\u2016\u03bbk+1\u2016 \u2264 2 k\u2211\nt=K\u2032\n\u2016\u03bb\u2217t \u2016+ k\u2211\nt=K\u2032+1\n\u221a\n2\u03b7t\u01ebt.\nHaving achieved a bound on the dual iterates \u03bbk+1 in terms of the dual maximizers \u03bb \u2217 k\u2032 (k\u2032 \u2208 {K \u2032, . . . , k}), we can now aim to provide bounds for the latter. Indeed, we can leverage results from constrained convex optimization (Appendix A.4) to arrive at the following lemma.\nLemma 6. Suppose Assumption 1 holds. Let \u03bd \u2208 (0, 1) and choose K \u2032 as in Lemma 1. Let k \u2208 {K \u2032, . . . ,K}, and let (\u03c0\u2217k, \u03bb\u2217k) be a pair of primal-optimal and dual-optimal solutions for Eq. (6). Then, conditioned on G, we have\n\u2016\u03bb\u2217k\u2016 \u2264 \u2016\u03bb\u2217k\u20161 \u2264 H\n\u03bd\u03b3 .\nPlugging Lemma 5 into the bounds from Lemma 4 and replacing the norms of the \u03bb\u2217k using the bound from Lemma 6, we obtain sublinear optimization errors when choosing \u03b7k, \u01ebk correctly:\nLemma 7 (Optimization errors). Suppose Assumption 1 holds. Let \u03bd \u2208 (0, 1) and choose K \u2032 as in Lemma 1. Suppose that the event G occurs. When using step sizes \u03b7K\u2032+k = \u0398(k\n2.5) and \u01ebK\u2032+k = \u0398(1/\u03b7K\u2032+k), we have\nK\u2211\nk=K\u2032+1\n[V \u03c0k(c\u0303k, p\u0303k)\u2212 V \u03c0 \u2217 (c, p)]+ \u2264 K\u2211\nk=K\u2032+1\n( (O(\u03c3k) + \u2211k t=K\u2032+1 \u221a 2\u03b7t\u01ebt) 2\n2\u03b7k + \u01ebk\n)\n\u2264 O( \u221a K),\nmax i\u2208[I]\nK\u2211\nk=K\u2032+1\n[V \u03c0k(d\u0303i,k, p\u0303k)\u2212 \u03b1i]+ \u2264 K\u2211\nk=K\u2032+1\nO(\u03c3k) + \u2211k\nt=K\u2032+1\n\u221a 2\u03b7t\u01ebk\n\u03b7k \u2264 O(\n\u221a K),\nwhere \u03c3 = H\u03bd\u03b3 and in fact O(\u03c3k) can be replaced by (2 + 2(k \u2212K \u2032))\u03c3. Remark: We need to choose \u03b7k large enough (increasing) and \u01ebk small enough (decreasing) to ensure a sublinear error bound. At the same time, we do not want to choose \u03b7k larger than necessary or \u01ebk smaller than necessary for computational reasons (see Proposition 1). We refer to Appendix D.3 for a discussion. In the case of an exact subroutine, we can plug in \u01ebk = 0 to achieve an analogous result.\nAccording to our regret decomposition, by adding up the errors of the pre-training phase, the estimation errors in the second phase, and the optimization errors in the second phase, we can indeed deduce our main result (Theorem 1). We showed how to bound the estimation errors using the optimism paradigm (Lemma 3). For the analysis of the optimization errors, we had to generalize the convergence analysis of the inexact augmented Lagrangian method (Lemma 7). We showed that if we have access to a safe baseline policy, the pre-training phase guarantees all assumptions required for this and adds a constant term to the regret (Lemma 1)."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this work, we showed how to overcome the problem of the cancellation of errors, i.e., the oscillation of standard Lagrangian-based algorithms for CMDPs around an optimal safe policy. We leveraged the augmented Lagrangian method to design our algorithm OPTAUG-CMDP. Unlike the related OPTDUAL-CMDP algorithm of Efroni et al. (2020), this requires a subroutine that solves a non-linear optimization problem in each episode. We devised an efficient algorithm for this, avoiding projections or LP. We then provided a regret analysis that, unlike previous works, does not require the cancellation of errors to arrive at sublinear regret guarantees. This means that in contrast to existing Lagrangian-based algorithms, our algorithm is provably safe while exploring the unknown CMDP.\nThis first partial answer to the open problem posed by Efroni et al. (2020) leads to several further questions: Can we obtain tighter bounds for the inner sub-routine and the regret, as our problem has a richer structure than the general convex optimization setup? While OPTAUG-CMDP enjoys stronger regret guarantees, the proposed inner subroutine has a higher computational cost than the one in OPTDUAL-CMDP, which may be possible to improve. Moreover, it remains open whether one can remove the requirement of access to a strictly feasible policy. Finally, we aim to extend our approach to the more practical function approximation setup."
        },
        {
            "heading": "Appendix",
            "text": ""
        },
        {
            "heading": "Table of Contents",
            "text": ""
        },
        {
            "heading": "A Background 14",
            "text": "A.1 Review of the Augmented Lagrangian Method . . . . . . . . . . . . . . . . . . 14 A.2 Review of OPTDUAL-CMDP . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 A.3 CMDPs and Occupancy Measures . . . . . . . . . . . . . . . . . . . . . . . . . 16 A.4 Convex Optimization Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . 17"
        },
        {
            "heading": "B Solving the Inner Optimization Problem 18",
            "text": "B.1 Derivation of the Policy Update via Frank-Wolfe . . . . . . . . . . . . . . . . . 18 B.2 Pseudocode for the Inner Problem . . . . . . . . . . . . . . . . . . . . . . . . . 20"
        },
        {
            "heading": "C On-Policy Error Bounds 21",
            "text": ""
        },
        {
            "heading": "D Omitted Proofs for Section 3 25",
            "text": "D.1 Pre-Training Phase . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 D.2 Iteration Complexity of the Inner Method . . . . . . . . . . . . . . . . . . . . . 26 D.3 Main Result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27"
        },
        {
            "heading": "E Omitted Proofs for Section 4 29",
            "text": "E.1 Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 E.2 Omitted Proofs for Section 4.1 . . . . . . . . . . . . . . . . . . . . . . . . . . 31 E.3 Omitted Proofs for Section 4.2 . . . . . . . . . . . . . . . . . . . . . . . . . . 32"
        },
        {
            "heading": "A Background",
            "text": ""
        },
        {
            "heading": "A.1 Review of the Augmented Lagrangian Method",
            "text": "In this section, we review the fundamentals of the augmented Lagrangian method (Rockafellar, 1976; Bertsekas, 2014), which was first introduced by Hestenes (1969); Powell (1969). Our review is partly inspired by the review by Yan and He (2020).\nConsider a closed and convex set X \u2282 Rn and a closed convex function f : X \u2192 R. Furthermore, let A \u2208 Rm\u00d7n and b \u2208 Rm. With this, consider the constrained problem\nmin x\u2208X f(x) (7)\ns.t. Ax \u2264 b. After initializing x0 \u2208 X and \u03bb0 \u2208 Rm\u22650, the augmented Lagrangian method performs the following updates in step k \u2265 0:\nxk+1 \u2208 argmin x\u2208X\n(\nf(x) + 1\n2\u03b7k \u2016[\u03bbk + \u03b7k(Ax \u2212 b)]+\u20162\n)\n, (8)\n\u03bbk+1 =[\u03bbk + \u03b7k(Axk+1 \u2212 b)]+. (9) It can easily be verified that the augmented Lagrangian method is the proximal point method applied to the Lagrangian dual. Thus, with the Lagrangian L(x, \u03bb) := f(x) + \u03bbT (Ax\u2212 b), we have\n(xk+1, \u03bbk+1) \u2208 argmax \u03bb\u22650 min x\u2208X\n(\nL(x, \u03bb) \u2212 1 2\u03b7k\n\u2016\u03bb\u2212 \u03bbk\u20162 ) .\nXu (2021) shows a non-asymptotic convergence result for the augmented Lagrangian method, including the case of an inexact subroutine for solving Eq. (8). Notably, the convergence result concerns the last iterate of the method. That is if f\u2217 is the optimal value of Eq. (7), then f(xk) \u2192 f\u2217 as k \u2192 \u221e and the convergence rate is determined by the step sizes \u03b7k (and, in the case of an inexact subroutine, by the accuracy parameter \u01ebk).\nAlternatively, we could apply the dual projected gradient method to Eq. (7). The updates would then read\nx\u0303k+1 \u2208 argmin x\u2208X\n( f(x) + \u03bb\u0303Tk (Ax\u2212 b) ) , (10)\n\u03bb\u0303k+1 =[\u03bb\u0303k + \u03b7\u0303k(Ax\u0303k+1 \u2212 b)]+. (11) While Eq. (11) coincides with Eq. (9), the update of the primal variable differs from the one in the augmented Lagrangian method. We can view the dual projected gradient method as iterative play between a primal player xk and a dual player \u03bbk in a min-max setup, where the objective is the Lagrangian L(x, \u03bb). The primal player here plays best response, while the dual player plays online projected gradient ascent. While this is similar in spirit to applying the proximal point method to the Lagrangian dual, the known non-asymptotic convergence guarantees for this method are ergodic, i.e., they only concern convergence of the averaged iterates. Indeed, simple simulations show that this is not a weakness in the analysis but that the primal and dual iterates indeed oscillate around an optimal solution pair. This is illustrated by Beck (2017, Chapter 8). The iterates of the dual projected gradient method may alternate between satisfying f(xk) > f\n\u2217 and Axk < b for a couple of iterations and then f(xk) < f\n\u2217 and Axk > b for a couple of iterations. While the average objective value converges to f\u2217 and the average constraint violation to 0, this is not true for the individual iterates. Note that these oscillations are not due to estimating the problem but are present even if the optimization problem is fixed, as in the setup above. Apart from the augmented Lagrangian method, other methods, such as extra gradient or optimistic gradient descent-ascent, offer solutions to this issue."
        },
        {
            "heading": "A.2 Review of OPTDUAL-CMDP",
            "text": "In this section, we review the related OPTDUAL-CMDP algorithm of Efroni et al. (2020). This model-based dual algorithm is based on the dual projected gradient method (see Appendix A.1) rather than on the augmented Lagrangian method (but builds the model with the same notion of optimism).\nAlgorithm 2 OPTDUAL-CMDP\nSet \u03b7k := \u221a \u03c12\nH2IK and \u03bb1 := 0 \u2208 RI for k = 1, . . . ,K do\nUpdate policy:\n\u03c0k, p\u0303k := arg min \u03c0\u2208\u03a0 p\u2032\u2208Bp\nk\n(\nV \u03c0(c\u0303k, p \u2032) + \u03bbTk (V \u03c0((d\u0303i,k)i\u2208[I], p \u2032)\u2212 \u03b1)\n)\n(12)\nUpdate dual variables:\n\u03bbk+1 := [\u03bbk + \u03b7k(V \u03c0k((d\u0303i,k)i\u2208[I], p\u0303k)\u2212 \u03b1)]+ (13)\nPlay \u03c0k , update estimates of the costs c\u0303k+1, (d\u0303i,k+1)i\u2208[I] and transitions B p k+1 (Eq. (3)).\nDual approaches like the algorithm above turn the CMDP into a series of linearly regularized, (extended) unconstrained MDPs (here with objectiveV \u03c0(c\u0303k, p \u2032)+\u03bbTk (V \u03c0((d\u0303i,k)i\u2208[I], p \u2032)\u2212\u03b1)), that can be solved efficiently with DP. With the augmented Lagrangian approach, the inner problem (Eq. (4)) has a more complicated structure. In Appendix B, we show that the inner problem can be reformulated as a convex optimization problem and provide an efficient method based on Frank-Wolfe and DP.\nFor OPTDUAL-CMDP, we have the following guarantee (Efroni et al., 2020, Theorem 5).\nTheorem 2. Suppose there exists a strictly feasible policy \u03c0 such that for all i \u2208 [I] we have V \u03c0(di, p) < \u03b1i. Set\n\u03c1 := V \u03c0(c, p)\u2212 V \u03c0\u2217(c, p)\nmini\u2208[I](\u03b1i \u2212 V \u03c0(di, p)) .\nThen, for any \u03b4 \u2208 (0, 1), with probability at least 1 \u2212 \u03b4, OPTDUAL-CMDP achieves the following regret bounds:\nR\u00b1(K; c) = O\u0303 (\u221a SNH4K + \u03c1 \u221a H2IK + ( \u221a N +H)H2SA ) ,\nR\u00b1(K; d) = O\u0303 ( (1 + 1 \u03c1 )( \u221a ISNH4K + (H \u221a N + S) \u221a IH2SA) ) .\nNotably, this bound only covers the weak regret. This is because the dual projected gradient method does not allow for a non-ergodic convergence analysis, and its iterates will generally oscillate around an optimal feasible solution. It is worth mentioning that unlike LP-based approaches (OPTCMDP and OPTCMDP-BONUS of Efroni et al. (2020)), the related primal-dual method OPTPRIMALDUALCMDP of Efroni et al. (2020) suffers from the same problem as OPTDUAL-CMDP. Moreover, we remark that this is not just a hypothetical issue but that Lagrangian-based algorithms indeed suffer from the mentioned oscillations in practical applications (Stooke et al., 2020; Moskovitz et al., 2023)."
        },
        {
            "heading": "A.3 CMDPs and Occupancy Measures",
            "text": "We summarize the relevant quantities of the CMDP M as follows.\nTo view the CMDP as a convex optimization problem, we will express it via the common notion of occupancy measures (Borkar, 1988).\nDefinition 1. The state-action occupancy measure q\u03c0 of a policy \u03c0 for a CMDP M is defined as q\u03c0h(s, a; p) := E [ 1{sh=s,ah=a} | s1, p, \u03c0 ] = P [sh = s, ah = a | s1, p, \u03c0],\nfor s \u2208 S, a \u2208 A, h \u2208 [H ]. We denote the stacked vector of these values as q\u03c0(p) \u2208 RSAH , with the element at index (s, a, h) being q\u03c0h (s, a; p).\nFor transition probabilities p\u2032, we can now define\nQ(p\u2032) := { q\u03c0(p\u2032) \u2208 RSAH | \u03c0 \u2208 \u03a0 }\nas the state-action occupancy measure polytope. Note that Q(p\u2032) is indeed a polytope (Altman, 1999; Efroni et al., 2020). Moreover, for any p\u2032 we have a surjective map \u03c0 7\u2192 q\u03c0(p\u2032) between \u03a0 and Q(p\u2032), for which we can explicitly compute an element in the pre-image of q \u2208 Q(p\u2032) via \u03c0h(a|s) = qh(s, a)/( \u2211 a\u2032 qh(s, a \u2032)).\nWe can stack the expected costs ch(s, a) and constraint costs dih(s, a) in the same way as qh(s, a) to obtain vectors c \u2208 RSAH and di \u2208 RSAH . Note that we then have V \u03c0(c, p) =\u2211\nh,s,a q \u03c0 h(s, a; p)ch(s, a) = c T q\u03c0(p) by linearity of expectation. Similarly, for all i \u2208 [I], we have\nV \u03c0(di, p) = d T i q \u03c0(p). Moreover, if we stack D = (di)i\u2208[I] \u2208 RI\u00d7SAH and \u03b1 = (\u03b1i)i\u2208[I] \u2208 RI as\nD :=\n\n  dT1 ...\ndTI\n\n  , \u03b1 :=\n\n  \u03b11 ... \u03b1I\n\n  ,\nwe obtain V \u03c0(D, p) = Dq\u03c0(p) \u2208 [0, H ]I for the vector of the constraint value functions. We can thus write\n\u03c0\u2217 \u2208 argmin \u03c0\u2208\u03a0 V \u03c0(c, p) s.t. V \u03c0(di, p) \u2264 \u03b1i (\u2200i \u2208 [I])\nequivalently as\nq\u03c0 \u2217 \u2208 arg min q\u03c0\u2208Q(p) cT q\u03c0(p) s.t. Dq\u03c0(p) \u2264 \u03b1,\nwhich is an LP. In particular, if we assume feasibility, then by compactness of the state-action occupancy polytope and continuity of the objective, there is an optimal solution \u03c0\u2217."
        },
        {
            "heading": "A.4 Convex Optimization Preliminaries",
            "text": "We state some well-known results from constrained convex optimization that will be useful to bound the dual iterates \u03bbk appearing in Lemma 4. The results are standard, and we refer, for example, to the work by Beck (2017).\nConsider the (primal) optimization problem\nf\u2217 := min f(x)\ns.t. g(x) \u2264 0 (14) x \u2208 X\nwith the following assumptions.\nAssumption 2 (Assumption 8.41, Beck (2017)). In Eq. (14),\n(a) X \u2282 Rn is convex (b) f : Rn \u2192 R is convex (c) g(\u00b7) := (g1(\u00b7), . . . , gm(\u00b7))T with gi : Rn \u2192 R convex (d) Eq. (14) has a finite optimal value f\u2217, which is attained by exactly the elements of X\u2217 6= \u2205 (e) There exists x\u0304 \u2208 X such that g(x\u0304) < 0 (f) For all \u03bb \u2208 Rm\u22650, minx\u2208X(f(x) + \u03bbT g(x)) has an optimal solution\nIn this setup, we define the dual objective as\nq(\u03bb) := min x\u2208X\n( f(x) + \u03bbT g(x) ) ,\nwhere L : Rn \u00d7Rm \u2192 R, L(x;\u03bb) := f(x) + \u03bbT g(x) is the Lagrangian of the problem in Eq. (14). The dual problem is then defined as\nq\u2217 := max q(\u03bb)\ns.t. \u03bb \u2265 0. In this setup, we have the following results connecting the primal and the dual problem.\nTheorem 3 (Theorem A.1, Beck (2017)). Under Assumption 2, strong duality holds in the following sense: We have\nf\u2217 = q\u2217\nand the optimal solution of the dual problem is attained, with the set of optimal solutions \u039b\u2217 6= \u2205.\nProof. Proposition 6.4.4 of Bertsekas et al. (2003) proves the more general Theorem A.1 of Beck (2017). We remark that if we assume affine constraints g and X being a polytope, then we can drop assumption (e) (Beck, 2017, Theorem A.1).\nTheorem 4. Suppose Assumption 2 holds. Let x\u2217 \u2208 X\u2217, \u03bb\u2217 \u2208 \u039b\u2217 and x \u2208 X . Then\nf(x)\u2212 f(x\u2217) + (\u03bb\u2217)T g(x) \u2265 0.\nProof. We have\nf(x) =f(x) + (\u03bb\u2217)T g(x)\u2212 (\u03bb\u2217)T g(x) \u2265q(\u03bb\u2217)\u2212 (\u03bb\u2217)T g(x) (definition of q(\u00b7)) =f(x\u2217)\u2212 (\u03bb\u2217)T g(x), (since by Theorem 3, q\u2217 = f\u2217)\nand rearranging this proves the claim. Again, we can drop assumption (e) if we consider affine constraints g and a polytope X .\nTheorem 5. Under Assumption 2, for all \u03bb\u2217 \u2208 \u039b\u2217 and x\u0304 as in (e), we have\n\u2016\u03bb\u2217\u2016 \u2264 \u2016\u03bb\u2217\u20161 \u2264 f(x\u0304)\u2212 f\u2217\nmini\u2208[m](\u2212gi(x\u0304)) .\nProof. The first relation holds since \u03bb\u2217 \u2265 0. We show the second relation as follows (cf. Beck (2017, Theorem 8.42)). We have\nf(x\u2217) =q(\u03bb\u2217) (Theorem 3)\n\u2264f(x\u0304) + (\u03bb\u2217)T g(x\u0304) (definition of q(\u00b7)) \u2264f(x\u0304) + \u2016\u03bb\u2217\u20161 max\ni\u2208[m] gi(x\u0304) (since \u03bb\n\u2217 \u2265 0)\n=f(x\u0304)\u2212 \u2016\u03bb\u2217\u20161 min i\u2208[m] (\u2212gi(x\u0304))\nand rearranging this proves the claim. We remark that this theorem needs assumption (e), even in the affine case."
        },
        {
            "heading": "B Solving the Inner Optimization Problem",
            "text": "There are numerous works leveraging Frank-Wolfe schemes for RL (including planning) with convex objectives, most commonly in the context of pure/active exploration (Hazan et al., 2019; Tarbouriech and Lazaric, 2019; Tarbouriech et al., 2020; Mutny et al., 2023).\nTo the best of our knowledge, only Tarbouriech and Lazaric (2019, Appendix A.3) remark that a combination of Frank-Wolfe UCB and planning in extended MDPs would be possible with a convex objective and plausible transitions. Tarbouriech et al. (2020) follow this approach but solve an extended LP rather than an extended MDP in each Frank-Wolfe iteration. Therefore, we make the former idea explicit and show how we can devise an efficient algorithm for our case."
        },
        {
            "heading": "B.1 Derivation of the Policy Update via Frank-Wolfe",
            "text": "In the following, we provide an efficient algorithm for solving the inner optimization problem in Eq. (4) based on the extended LP formulation of CMDPs, Frank-Wolfe, and DP. Recall the first update in OPTAUG-CMDPwhile rewriting the value functions in terms of occupancy measures (see Appendix A.3):\n\u03c0k, p\u0303k := arg min \u03c0\u2208\u03a0 p\u2032\u2208Bp\nk\n(\nc\u0303Tk q \u03c0(p\u2032) +\n1\n2\u03b7k\n\u2225 \u2225 \u2225[\u03bbk + \u03b7k(D\u0303kq \u03c0(p\u2032)\u2212 \u03b1)]+ \u2225 \u2225 \u2225\n2 )\n, (15)\nwhere q\u03c0(p\u2032) \u2208 RSAH , D\u0303k \u2208 RI\u00d7SAH and c\u0303k \u2208 RSAH are defined as in Appendix A.3. We first use the extended LP trick (Rosenberg and Mansour, 2019; Efroni et al., 2020) to switch to a convex optimization problem in the state-action-state occupancy measure. That is, in the problem above, substitute\nzh(s, a, s \u2032) := z\u03c0h(s, a, s \u2032; p\u2032) :=p\u2032h(s \u2032|s, a)q\u03c0h(s, a; p\u2032) (16)\nand note that\nq\u03c0h(s, a; p \u2032) =\n\u2211\ns\u2032\nz\u03c0h(s, a, s \u2032; p\u2032). (17)\nStacked across (s, a, h) \u2208 S \u00d7A\u00d7 [H ], this simply reads\nq\u03c0(p\u2032) = \u2211\ns\u2032\nz\u03c0(s\u2032; p\u2032) \u2208 RSAH ,\nfor all s\u2032 \u2208 S. The objective of Eq. (15) then reads\nf(z) := \u2211\ns\u2032\nc\u0303Tk z(s \u2032) +\n1\n2\u03b7k \u2016[\u03bbk + \u03b7k(\n\u2211\ns\u2032\nD\u0303kz(s \u2032)\u2212 \u03b1)]+\u20162,\nand we need to minimize it over the set Z \u2282 RS2AH which is given by the constraints \n \n \n\u2211\na,s\u2032 zh(s, a, s \u2032) =\n\u2211\ns\u2032,a\u2032 zh\u22121(s \u2032, a\u2032, s) (\u2200h > 1, s)\n\u2211\na,s\u2032 z1(s, a, s \u2032) = \u00b5(s) (\u2200s)\nzh(s, a, s \u2032) \u2265 0 (\u2200s, a, s\u2032, h)\nzh(s, a, s \u2032)\u2212 (p\u0304k\u22121h (s\u2032|s, a) + \u03b2 p k,h(s, a, s\n\u2032)) \u2211\ns\u2032\u2032 zh(s, a, s \u2032\u2032) \u2264 0 (\u2200s, a, s\u2032, h)\n\u2212zh(s, a, s\u2032) + (p\u0304k\u22121h (s\u2032|s, a)\u2212 \u03b2 p k,h(s, a, s\n\u2032)) \u2211\ns\u2032\u2032 zh(s, a, s \u2032\u2032) \u2264 0 (\u2200s, a, s\u2032, h),\nwhere \u00b5(s) = 1 if s = s1 and 0 otherwise. Note that Z is a bounded polytope and thus compact and convex. We can thus equivalently solve\nmin z\u2208Z f(z), (18)\nwhich is a standard convex optimization problem. Note that, due to the nonlinear objective, even with the convex formulation in Eq. (18), Eq. (4) cannot be written as an LP. It cannot be rewritten as a standard convex RL problem (Zahavy et al., 2021; Geist et al., 2021) either since the transition probabilities are part of the optimization, and we thus had to switch to the extended convex program in the state-action-state occupancy measure z. Note that for efficiency reasons, we aim to avoid LP (which is needed for solving CMDPs due to the lack of the Bellman optimality principle) and projections onto the high-dimensional constraint set Z . Instead, we will make use of DP and gradient-based methods.\nIf we can approximately solve Eq. (18), we can later retrieve transitions and policy via\np\u0303k,h(s \u2032|s, a) := zh(s, a, s \u2032) \u2211\ns\u2032\u2032 zh(s, a, s \u2032\u2032)\n, (19)\n\u03c0k,h(a|s) := \u2211 s\u2032 zh(s, a, s \u2032) \u2211\na\u2032,s\u2032 zh(s, a \u2032, s\u2032)\n. (20)\nWe use Frank-Wolfe to solve Eq. (18). Frank-Wolfe is a well-known iterative method for constrained nonlinear optimization, which minimizes a smooth convex function over a convex domain and avoids projections. To apply it to Eq. (18), we need a linear minimization oracle (LMO) that, in step t (of episode k), solves\nmin g\u2208Z\ngT\u2207f(zt) (21)\nand then, after finding a minimizer g, updates\nzt+1 := (1 \u2212 \u03b3t)zt + \u03b3tg, where \u03b3t := 2/(t + 2). It is well known that Frank-Wolfe converges with a rate of O(1/T ) for smooth objectives. This is given here, but the smoothness parameter depends on \u03b7k. We refer to Abernethy and Wang (2017); Jaggi (2013); Frank and Wolfe (1956) for the relevant background.\nIt remains to show that we can provide an efficient LMO that uses DP instead of LP and avoids projections onto Z . Let (\u2207f(z))(s, a, s\u2032, h) := \u2202f\u2202zh(s,a,s\u2032)(z) be the gradient of f with respect to z at index (s, a, s\u2032, h). Then the s\u2032-th component of the gradient of f with respect to z is\n(\u2207f(z))(\u00b7, \u00b7, s\u2032, \u00b7) = c\u0303k + D\u0303Tk [\u03bbk + \u03b7k(D\u0303k \u2211\ns\u2032\u2032\nz(s\u2032\u2032)\u2212 \u03b1)]+ \u2208 RSAH , (22)\nwhich we can compute explicitly and efficiently. We note that this gradient does not depend on s\u2032, and thus the gradient of f with respect to the whole vector z \u2208 RS2AH is simply the vector above, repeatedly stacked S times.\nWe now show how switching back to the optimization over \u03a0\u00d7Bpk allows for an efficient LMO via DP. For g \u2208 Z , there are \u03c0, p\u2032 such that\ngh(s, a, s \u2032) = g\u03c0h(s, a, s \u2032; p\u2032) = ph(s \u2032|a, s)q\u03c0h(s, a; p),\nvia Eqs. (19) and (20). The LMO then needs to minimize\ngT\u2207f(zt) = \u2211\ns,a,s\u2032,h\ngh(s, a, s \u2032)(\u2207f(zt))(s, a, 1, h)\n= \u2211\ns,a,s\u2032,h\np\u2032h(s \u2032|a, s)q\u03c0h(s, a; p\u2032)(\u2207f(zt))(s, a, 1, h)\n= \u2211\ns,a,h\nq\u03c0h(s, a; p \u2032)(\u2207f(zt))(s, a, 1, h) (23)\nover \u03c0 \u2208 \u03a0 and p\u2032 \u2208 Bpk . This corresponds to solving the extended MDP M+ := {(M = (S,A, r+, p+)) | \u2200s, a, h : r+h (s, a) := \u2207f(zt)(s, a, 1, h), p+h (\u00b7|s, a) \u2208 B p k,h(s, a)}. We can do so via backward induction (i.e., DP) that optimizes\nQkh(s, a) := r + h (s, a) + min\np\u2032(\u00b7|s,a)\u2208Bp k,h (s,a)\n\u2211\ns\u2032\np\u2032(s\u2032|s, a)min a\u2032 Qkh+1(s \u2032, a\u2032) (24)\nand starts with QkH+1(s, a) = 0. We can retrieve the transitions and the policy by storing the minimizers in each step of the DP. To compute the solution g of the LMO from \u03c0, p\u2032, one can then use a simple and efficient DP scheme to compute q\u03c0h(p\n\u2032), which is explained in Appendix B.2. We can use this, in turn, to retrieve g using the substitution in Eq. (16). Then, we perform the second step of Frank-Wolfe to get a convex combination of g and zt, which concludes the Frank-Wolfe iteration.\nWe can already see that the computational complexity of the k-th step of OPTAUG-CMDP is larger than the one of OPTDUAL-CMDP by a factor of O(1/\u01ebk) (and a dependency on \u03b7k) because we need this many Frank-Wolfe iterations, which is the price we pay for the stronger regret bound. We provide a complete analysis of the iteration complexity in Appendix D.2."
        },
        {
            "heading": "B.2 Pseudocode for the Inner Problem",
            "text": "More formally, the algorithm for solving the inner problem (Eq. (4)) reads as follows.\nAlgorithm 3 INNEROPT-FW\nInput: Current estimates c\u0303k, D\u0303k, B p k , and \u03bbk Output: Next policy \u03c0k and p\u0303k according to Eq. (15)\nSet T = 2\u03b7kIS 2AH\n\u01ebk , \u03b3t = 2/(2 + t)\nInitialize z0 \u2208 Z arbitrarily for t = 0, . . . , T \u2212 1 do\nCompute \u2207f(zt) according to Eq. (22) Minimize Eq. (23) via DP algorithm from Eq. (24) to find \u03c0t, pt Retrieve minimizer gt from \u03c0t, pt via DP algorithm from Eq. (26) and Eq. (16) Set zt+1 := \u03b3tg\nt + (1\u2212 \u03b3t)zt Construct \u03c0k, p\u0303k from z\nT via Eqs. (19) and (20) return \u03c0k , p\u0303k\nAs a final step, we now describe how to retrieve q := q\u03c0(p\u2032) from \u03c0 and p\u2032 in Line 6 (Jin et al., 2019). For s \u2208 S, set qh(s) := P [sh = s|s1; p\u2032, \u03c0] = \u2211 a\u2032\u2208A qh(s, a \u2032). Given a policy \u03c0 and transition probabilities p\u2032, we need to compute\nqh(s, a) = qh(s) \u00b7 \u03c0h(a|s), (25) for every h \u2208 [H ], s \u2208 S and a \u2208 A. This is easily achieved via DP. Indeed, we have\nq1(s) =\n{ 1 (s = s1)\n0 (else),\nand for h > 1\nqh(s) =P [sh = s|s1; p\u2032, \u03c0] = \u2211\ns\u2032\u2208S\nP [sh = s|sh\u22121 = s\u2032; s1, p\u2032, \u03c0]P [sh\u22121 = s\u2032|s1; p\u2032, \u03c0]\n= \u2211\ns\u2032\u2208S\n\u2211\na\u2208A\n\u03c0h\u22121(a|s\u2032)ph\u22121(s|s\u2032, a)P [sh\u22121 = s\u2032|s1, ; p\u2032, \u03c0]\n= \u2211\ns\u2032\u2208S\n\u2211\na\u2208A\n\u03c0h\u22121(a|s\u2032)ph\u22121(s|s\u2032, a)qh\u22121(s\u2032), (26)\nwhich together with Eq. (25) enables us to retrieve the state-action occupancy measure. Clearly, we can perform the above DP scheme efficiently."
        },
        {
            "heading": "C On-Policy Error Bounds",
            "text": "We consider arbitrary polices (\u03c0k)k\u2208[K]. We suppose that in the CMDP M, the agent plays \u03c0k in episode k \u2208 [K] and uses it to update the optimistic model according to Eq. (3), with some fixed \u03b4 = 3\u03b4\u2032 > 0.\nWe first establish Lemmas 8 to 10, which will allow us to bound the estimation errors (Lemma 3). For a definition of the occupancy measure q\u03c0(s, a; p), see Appendix A.3. We write . for an inequality up to polylogarithmic factors.\nNote that in the following two lemmas, the exponent of H differs from the one in the referenced proofs. This is because the referenced works consider the case of stationary transition probabilities, whereas we consider non-stationary dynamics. See Shani et al. (2020, Lemmas 18, 19).\nLemma 8 (Lemma 36, Efroni et al. (2020)). Suppose for all s, a, h, k \u2208 [K] we have\nnk\u22121h (s, a) > 1\n2\n\u2211\nj<k\nq \u03c0j h (s, a; p)\u2212H log\n( SAH\n\u03b4\u2032\n)\n.\nThen for all K \u2032 \u2264 K K\u2032\u2211\nk\u2032=1\nH\u2211\nh=1\nE\n\n 1\n\u221a\nnk \u2032\u22121 h (s k\u2032 h , a k\u2032 h )\n| Fk\u2032\u22121\n  \u2264 O\u0303( \u221a SAH2K \u2032 + SAH),\nwhere Fk\u2032\u22121 is the \u03c3-algebra induced by all random variables up to and including episode k\u2032 \u2212 1.\nProof. We refer to Efroni et al. (2019, Lemma 38) for a proof of the statement.\nLemma 9 (Lemma 37, Efroni et al. (2020)). Suppose for all s, a, h, k \u2208 [K] we have\nnk\u22121h (s, a) > 1\n2\n\u2211\nj<k\nq \u03c0j h (s, a; p)\u2212H log\n( SAH\n\u03b4\u2032\n)\n.\nThen for all K \u2032 \u2264 K K\u2032\u2211\nk\u2032=1\nH\u2211\nh=1\nE\n[\n1\nnk \u2032\u22121 h (s k\u2032 h , a k\u2032 h )\n| Fk\u2032\u22121 ] \u2264 O\u0303(SAH2),\nwhere Fk\u2032\u22121 is the \u03c3-algebra induced by all random variables up to and including episode k\u2032 \u2212 1.\nProof. We refer to Zanette and Brunskill (2019, Lemma 13) for a proof of the statement.\nThe following lemma provides an on-policy error bound based on the value difference lemma. Together with the preliminaries from Appendix E.1, it allows us to bound the estimation error.\nLemma 10 (On-policy errors; Lemma 29, Efroni et al. (2020)). Consider an MDP with transition dynamics p and arbitrary estimated transition dynamics p\u0302k (for k \u2208 [K], each forming a conditional probability measure). Consider policy iterates (\u03c0k)k\u2208[K] and suppose \u03c0k is played in episode k \u2208 [K] and used to update the counters. Let lh(s, a), l\u0303k,h(s, a) be the cost and corresponding optimistic cost with l = c or l = di as discussed in Eq. (3). For a policy \u03c0, let V \u03c0 h (s; l, p), V \u03c0 h (s; l\u0303k, p\u0302k) be the values of \u03c0 according to the true resp. estimated model. Assume that for all s, a, h, k, we have\nnk\u22121h (s, a) > 1\n2\n\u2211\nj<k\nq \u03c0j h (s, a; p)\u2212H log\n( SAH\n\u03b4\u2032\n)\n, (a)\n|l\u0303k,h(s, a)\u2212 lh(s, a)| \u2264 O\u0303\n\n 1\n\u221a\nnk\u22121h (s, a) \u2228 1\n\n , (b)\n|p\u0302k,h(s\u2032|s, a)\u2212 ph(s\u2032|s, a)| \u2264 O\u0303 (\u221a\nph(s\u2032|s, a)Lp\u03b4 nk\u22121h (s, a) \u2228 1 + Lp\u03b4 nk\u22121h (s, a) \u2228 1\n)\n. (c)\nThen we have\nK\u2211\nk=1\n|V \u03c0k(l, p)\u2212 V \u03c0k(l\u0303k, p\u0302k)| \u2264 O\u0303 (\u221a NSAH4K + S2AH3 ) . (27)\nProof. From the value difference lemma (Dann et al., 2017, Lemma E.15), we get\nK\u2211\nk=1\n|V \u03c0k(l, p)\u2212 V \u03c0k(l\u0303k, p\u0302k)|\n\u2264 K\u2211\nk=1\nH\u2211\nh=1\nE[|lh(sh, ah)\u2212 l\u0303k,h(sh, ah)| | s1, p, \u03c0k]\n+\nK\u2211\nk=1\nH\u2211\nh=1\nE\n[ \u2211\ns\u2032\n|ph(s\u2032|sh, ah)\u2212 p\u0302k(s\u2032|sh, ah)|V \u03c0kh+1(s\u2032; l\u0303k, p\u0302k) | s1, p, \u03c0k ] .\nWe can bound the first of the two terms as follows.\nK\u2211\nk=1\nH\u2211\nh=1\nE[|lh(sh, ah)\u2212 l\u0303k,h(sh, ah)| | s1, p, \u03c0k]\n(b) .\nK\u2211\nk=1\nH\u2211\nh=1\nE\n[ 1\n\u221a nk\u22121h (sh, ah) \u2228 1 | s1, p, \u03c0k\n]\n= K\u2211\nk=1\nH\u2211\nh=1\nE\n[ 1\n\u221a\nnk\u22121h (s k h, a k h) \u2228 1\n| Fk\u22121 ]\nLemma 8 \u2264 O\u0303 (\u221a SAH2K + SAH ) ,\nwhere the second to last inequality holds since \u03c0k is played in episode k, and Lemma 8 applies due to assertion (a). For the second term, we first note that |V \u03c0kh+1(s\u2032; l\u0303k, p\u0302k)| . H since |l\u0303k,h(s, a)| \u2264\nlh(s, a) + 1\u221a\nnk\u22121 h\n(s,a)\u22281 by (b). Hence\nK\u2211\nk=1\nH\u2211\nh=1\nE\n[ \u2211\ns\u2032\n|ph(s\u2032|sh, ah)\u2212 p\u0302k(s\u2032|sh, ah)|V \u03c0kh+1(s\u2032; l\u0303k, p\u0302k) | s1, p, \u03c0k ]\n(c) .H\nK\u2211\nk=1\nH\u2211\nh=1\nE\n[ 1\n\u221a\nnk\u22121h (sh, ah) \u2228 1\n(\u2211\ns\u2032\n\u221a\nph(s\u2032|sh, ah) ) +\nS\nnk\u22121h (sh, ah) \u2228 1 | s1, p, \u03c0k\n]\nJensen \u2264 H K\u2211\nk=1\nH\u2211\nh=1\nE\n[ 1\n\u221a\nnk\u22121h (sh, ah) \u2228 1\n\u221a N \u221a \u2211\ns\u2032\nph(s\u2032|sh, ah) + S\nnk\u22121h (sh, ah) \u2228 1 | s1, p, \u03c0k\n]\n=H\nK\u2211\nk=1\nH\u2211\nh=1\nE [ \u221aN \u221a\nnk\u22121h (s k h, a k h) \u2228 1\n+ S\nnk\u22121h (s k h, a k h) \u2228 1\n| Fk\u22121 ]\n.H \u221a N \u00b7 \u221a SAH2K +H \u221a N \u00b7 SAH +HS \u00b7 SAH2 . \u221a NSAH4K + S2AH3,\nwhere the second to last relation holds due to Lemma 8 and Lemma 9 (which in turn apply due to assertion (a)), and the one before holds since \u03c0k is played in episode k.\nNext, we prove Lemma 13, which is a variation of Lemma 10 and will allow us to establish the bound on the pre-training duration in Lemma 1. For this, we first need to establish Lemmas 11 and 12, which are a consequence of Lemmas 8 and 9, respectively.\nLemma 11. Let K \u2032 \u2208 [K]. Assume that for all s, a, h, k \u2208 [K] we have\nnk\u22121h (s, a) > 1\n2\n\u2211\nj<k\nq \u03c0j h (s, a; p)\u2212H log\n( SAH\n\u03b4\u2032\n)\n.\nLet \u03c0 be any fixed policy and suppose that in episode k \u2208 [K], policy \u03c0k is played, with \u03c0k\u2032 = \u03c0 for all k\u2032 \u2208 [K \u2032]. Then, for all k \u2265 K \u2032, we have\nH\u2211\nh=1\nE\n\n 1\n\u221a nk\u22121h (sh, ah) \u2228 1 | s1, \u03c0, p\n  \u2264 O\u0303 (\u221a SAH2(K \u2032)\u22121/2 + SAH(K \u2032)\u22121 ) .\nProof. Note that for any realization of all random variables up to and including episode K , we have (for k \u2208 {0, . . . ,K \u2212 1})\nnk+1h (s, a) \u2265 nkh(s, a), since the counters can only increase across episodes, and thus, for all k\u2032 \u2264 K \u2032 < k, we have\n1 \u221a nk\u22121h (sh, ah) \u2228 1 \u2264 1\u221a nk \u2032\u22121 h (sh, ah) \u2228 1 .\nFor k \u2265 K \u2032, we thus find H\u2211\nh=1\nE\n[ 1\n\u221a\nnk\u22121h (s k h, a k h)\n| s1, \u03c0, p ]\n\u2264 1 K \u2032\nK\u2032\u2211\nk\u2032=1\nH\u2211\nh=1\nE\n[ 1\n\u221a\nnk \u2032\u22121 h (sh, ah) | s1, \u03c0, p\n]\n= 1\nK \u2032\nK\u2032\u2211\nk\u2032=1\nH\u2211\nh=1\nE\n[ 1\n\u221a\nnk \u2032\u22121 h (s k\u2032 h , a k\u2032 h )\n| Fk\u2032\u22121 ]\n\u2264 1 K \u2032\nO\u0303( \u221a SAH2K \u2032 + SAH),\nwhere Fk\u2032\u22121 is the \u03c3-algebra induced by all random variables up to and including episode k\u2032 \u2212 1 and where the first relation holds by monotonicity of the counters, the second relation holds since \u03c0 is played in the k\u2032-th episode. The final relation holds by Lemma 8.\nLemma 12. Let K \u2032 \u2208 [K]. Assume that for all s, a, h, k \u2208 [K] we have\nnk\u22121h (s, a) > 1\n2\n\u2211\nj<k\nq \u03c0j h (s, a; p)\u2212H log\n( SAH\n\u03b4\u2032\n)\n.\nLet \u03c0 be any fixed policy and suppose that in episode k \u2208 [K], policy \u03c0k is played, with \u03c0k\u2032 = \u03c0 for all k\u2032 \u2208 [K \u2032]. Then, for all k \u2265 K \u2032, we have\nH\u2211\nh=1\nE\n[\n1\nnk\u22121h (sh, ah) \u2228 1 | s1, \u03c0, p\n]\n\u2264 O\u0303 ( SAH2(K \u2032)\u22121 ) .\nProof. Note that for any realization of all random variables up to and including episode K , we have (for k \u2208 {0, . . . ,K \u2212 1})\nnk+1h (s, a) \u2265 nkh(s, a), since the counters can only increase across episodes, and thus, for all k\u2032 \u2264 K \u2032 < k, we have\n1 nk\u22121h (sh, ah) \u2228 1 \u2264 1 nk \u2032\u22121 h (sh, ah) \u2228 1 .\nFor k \u2265 K \u2032, we thus find H\u2211\nh=1\nE\n[ 1\nnk\u22121h (sh, ah) | s1, \u03c0, p\n]\n\u2264 1 K \u2032\nK\u2032\u2211\nk\u2032=1\nH\u2211\nh=1\nE\n[ 1\nnk \u2032\u22121 h (sh, ah) | s1, \u03c0, p\n]\n= 1\nK \u2032\nK\u2032\u2211\nk\u2032=1\nH\u2211\nh=1\nE\n[ 1\nnk \u2032\u22121 h (s k\u2032 h , a k\u2032 h )\n| Fk\u2032\u22121 ]\n\u2264 1 K \u2032 O\u0303(SAH2),\nwhere Fk\u2032\u22121 is the \u03c3-algebra induced by all random variables up to and including episode k\u2032 \u2212 1 and where the first relation holds by monotonicity of the counters, the second relation holds since \u03c0 is played in the k\u2032-th episode. The final relation holds by Lemma 9.\nWe are now ready to prove the needed variation of Lemma 10.\nLemma 13. (Last-iterate fixed policy errors) Consider an MDP with transition dynamics p and arbitrary estimated transition dynamics p\u0302k (for k \u2208 [K], each forming a probability measure). Consider policy iterates (\u03c0k)k\u2208[K] and suppose \u03c0k is played in episode k \u2208 [K] and used to update the counters. Let lh(s, a), l\u0303k,h(s, a) be cost and corresponding optimistic cost with l = c or l = di as discussed in Eq. (3). Consider K \u2032 \u2208 [K] and a policy \u03c0. Suppose that for all k\u2032 \u2208 [K \u2032], \u03c0k\u2032 = \u03c0. Let V \u03c0 h (s; l, p), V \u03c0 h (s; l\u0303k, p\u0302k) be the values of \u03c0 according to the true and estimated model, respectively. Assume that for all s, a, h, k we have\nnk\u22121h (s, a) > 1\n2\n\u2211\nj<k\nq \u03c0j h (s, a; p)\u2212H log\n( SAH\n\u03b4\u2032\n)\n, (a)\n|l\u0303k,h(s, a)\u2212 lh(s, a)| \u2264 O\u0303\n\n 1\n\u221a\nnk\u22121h (s, a) \u2228 1\n\n , (b)\n|p\u0302k,h(s\u2032|s, a)\u2212 ph(s\u2032|s, a)| \u2264 O\u0303 (\u221a\nph(s\u2032|s, a)Lp\u03b4 nk\u22121h (s, a) \u2228 1 + Lp\u03b4 nk\u22121h (s, a) \u2228 1\n)\n. (c)\nThen, for all k \u2265 K \u2032, we have\n|V \u03c0(l, p)\u2212 V \u03c0(l\u0303k, p\u0302k)| \u2264 O\u0303 (\u221a NSAH4(K \u2032)\u22121/2 + S2AH3(K \u2032)\u22121 ) .\nIn other words, if the agent plays \u03c0 for the first K \u2032 episodes, then for all future episodes, the value differences for \u03c0 are upper bounded by the value on the RHS.\nProof. From the value difference lemma (Dann et al., 2017, Lemma E.15) we get\n|V \u03c0(l, p)\u2212 V \u03c0(l\u0303k, p\u0302k)|\n\u2264 H\u2211\nh=1\nE[|lh(sh, ah)\u2212 l\u0303h(sh, ah)| | s1, p, \u03c0]\n+\nH\u2211\nh=1\nE\n[ \u2211\ns\u2032\n|ph(s\u2032|sh, ah)\u2212 p\u0302k(s\u2032|sh, ah)|V \u03c0h+1(s\u2032; l\u0303k, p\u0302k) | s1, p, \u03c0 ] .\nWe can bound the first of the two terms as follows.\nH\u2211\nh=1\nE[|lh(sh, ah)\u2212 l\u0303h(sh, ah)| | s1, p, \u03c0]\n(b) .\nH\u2211\nh=1\nE\n[ 1\n\u221a nk\u22121h (sh, ah) \u2228 1 | s1, p, \u03c0\n]\nLemma 11 \u2264 O\u0303 (\u221a SAH2(K \u2032)\u22121/2 + SAH(K \u2032)\u22121 ) ,\nwhere Lemma 11 applies due to assertion (a). For the second term, we first note that |V \u03c0h+1(s\u2032; l\u0303k, p\u0302k)| . H since |l\u0303k,h(s, a)| \u2264 lh(s, a) + 1\u221ank\u22121 h (s,a)\u22281 by (b). Hence\nH\u2211\nh=1\nE\n[ \u2211\ns\u2032\n|ph(s|sh, ah)\u2212 p\u0302k(s|sh, ah)|V \u03c0h+1(s\u2032; l\u0303k, p\u0302k) | s1, p, \u03c0 ]\n(c) .H\nH\u2211\nh=1\nE\n[ 1\n\u221a\nnk\u22121h (sh, ah) \u2228 1\n(\u2211\ns\u2032\n\u221a\nph(s\u2032|sh, ah) ) +\nS\nnk\u22121h (sh, ah) \u2228 1 | s1, p, \u03c0\n]\nJensen \u2264 H H\u2211\nh=1\nE\n[ 1\n\u221a\nnk\u22121h (sh, ah) \u2228 1\n\u221a N \u221a \u2211\ns\u2032\nph(s\u2032|sh, ah) + S\nnk\u22121h (sh, ah) \u2228 1 | s1, p, \u03c0\n]\n.H \u221a N \u00b7 \u221a SAH2(K \u2032)\u22121/2 +H \u221a N \u00b7 SAH(K \u2032)\u22121 +HS \u00b7 SAH2(K \u2032)\u22121 . \u221a NSAH4(K \u2032)\u22121/2 + S2AH3(K \u2032)\u22121,\nwhere the second to last relation holds due to Lemma 11 and Lemma 12 (which in turn apply due to assertion (a))."
        },
        {
            "heading": "D Omitted Proofs for Section 3",
            "text": ""
        },
        {
            "heading": "D.1 Pre-Training Phase",
            "text": "We now establish which constant duration K \u2032 of playing \u03c0\u0304 is sufficient to guarantee the desired strict feasibility of Eq. (6) (with the choice of K \u2032 favoring readability over tightness). Note that the conclusion holds conditioned on the success event G.\nLemma 1. Suppose that Assumption 1 holds, i.e., the agent has access to a strictly feasible \u03c0\u0304 and its slack \u03b3 > 0. Fix any \u03bd \u2208 (0, 1), and suppose the agent executes \u03c0\u0304 for K \u2032 = O\u0303 ( max { S2AH3\n(1\u2212\u03bd)\u03b3 , NSAH4 (1\u2212\u03bd)2\u03b32\n})\nepisodes, where N := maxs,a,h |{s\u2032 | ph(s\u2032|s, a) > 0}| denotes the maximum number of transitions. Then, if the agent updates the optimistic CMDP based on the observations from those episodes (cf. Eq. (3)), with probability at least 1\u2212 \u03b4 the following condition is satisfied for every k \u2208 {K \u2032, . . . ,K}:\nV \u03c0\u0304(d\u0303i,k, p\u0303k) \u2264 \u03b1i \u2212 \u03bd\u03b3 (\u2200i \u2208 [I]).\nProof. Condition on the success event G, which by Lemma 2 happens with probability \u2265 1 \u2212 \u03b4. Then by construction of G, the assumptions (a) and (b) in Lemma 13 are met for l = di for all constraints i. By Lemma 14 also the assumption (c) in Lemma 13 is met with p\u0302k := p\u0303k from the k-th iteration of OPTAUG-CMDP. Thus, the conclusion from Lemma 13 holds for \u03c0 = \u03c0\u0304 and lh(s, a) = di,h(s, a) (for all i \u2208 [I]) and we have for all i \u2208 [I] and k \u2265 K \u2032\n|V \u03c0(di, p)\u2212 V \u03c0(d\u0303i,k, p\u0303k)| \u2264 b(K \u2032)\u22121/2 + a(K \u2032)\u22121, where\nb = O\u0303 (\u221a NSAH4 ) , a = O\u0303 ( S2AH3 ) .\nThus, if K \u2032 \u2265 max { 2S2AH3\n(1\u2212\u03bd)\u03b3 , 4NSAH4 (1\u2212\u03bd)2\u03b32\n}\n, we have\n|V \u03c0\u0304(di, p)\u2212 V \u03c0\u0304(d\u0303i,k, p\u0303k)| \u2264 O\u0303((1 \u2212 \u03bd)\u03b3). By further correcting for the missing polylogarithmic terms, this shows that there exists K \u2032 = O\u0303 ( max { S2AH3\n(1\u2212\u03bd)\u03b3 , NSAH4 (1\u2212\u03bd)2\u03b32\n})\nsuch that for all i \u2208 [I] and k \u2265 K \u2032, we have\n|V \u03c0\u0304(di, p)\u2212 V \u03c0\u0304(d\u0303i,k, p\u0303k)| \u2264 (1\u2212 \u03bd)\u03b3. At the same time, Assumption 1 guarantees\nV \u03c0\u0304(di, p) \u2264 \u03b1i \u2212 \u03b3. Hence\nV \u03c0\u0304(d\u0303i,k, p\u0303k) =V \u03c0\u0304(d\u0303i,k, p\u0303k)\u2212 V \u03c0\u0304(di, p) + V \u03c0\u0304(di, p)\n\u2264(1 \u2212 \u03bd)\u03b3 + \u03b1i \u2212 \u03b3 =\u03b1i \u2212 \u03bd\u03b3,\nwhich proves the claim.\nRemark 1. Notice that we chose K \u2032 such that both the terms b(K \u2032)\u22121/2 and a(K \u2032)\u22121 are less than 1/2, which is sufficient. For a tighter but less concise bound, we can alternatively solve the quadratic equation\nb(K \u2032)\u22121/2 + a(K \u2032)\u22121 \u2264 (1\u2212 \u03bd)\u03b3 (28) for K \u2032 to obtain the smallest possible pre-training time, up to polylogarithmic factors. Furthermore, since a and b are only specified in O\u0303-notation, we only have an asymptotic bound up to polylogarithmic factors on how large to choose K \u2032. An exact bound could be established by carrying along all factors dropped in the O\u0303-notation, but for brevity, we only specify how large to choose K \u2032 asymptotically. In practice, one can take the bound above and choose K \u2032 slightly larger.\nD.2 Iteration Complexity of the Inner Method\nRecall Proposition 1:\nProposition 1. In episode k, fix any accuracy of \u01ebk > 0. There exists an algorithm for solving Eq. (4) such that the objective at its output (\u03c0k, p\u0303k) is \u01ebk-close to the optimum of Eq. (4), by solving O ( \u03b7kIS 2AH\n\u01ebk\n)\n(extended) MDPs via DP.\nIn Corollary 1, we prove Proposition 1 by showing that the method proposed in Appendix B achieves the desired iteration complexity for solving Eq. (4). As seen in Appendix B.1, every iteration of the Frank-Wolfe scheme INNEROPT-FW (Appendix B.2) above can be performed efficiently via DP, i.e., in (low-degree) polynomial time in the parameters defining the CMDP. To analyze the number of such Frank-Wolfe iterations needed to reach an \u01ebk-close solution, we recall the following result due to Jaggi (2013).\nTheorem 6. The iterates (zk)k\u22650 of the Frank-Wolfe algorithm with an exact LMO applied to a convex objective f : D \u2192 R (with closed, convex and bounded domain D \u2282 Rd) satisfy\nf(zk)\u2212 f(z\u2217) \u2264 2Cf k + 2 , (29)\nwhere z\u2217 is a minimizer of f over D and Cf is the curvature constant of f . Moreover, if \u2207f is L-Lipschitz continuous w.r.t. an arbitrary norm \u2016 \u00b7 \u2016c on Rd, then\nCf \u2264 diam\u2016\u00b7\u2016c(D)2L. (30)\nIn our case, this yields the following bound on the iteration complexity of the inner optimization procedure.\nCorollary 1. After T = 2\u03b7kIS 2AH\n\u01ebk Frank-Wolfe steps, INNEROPT-FW returns an \u01ebk-close solution\nto the inner problem (Eq. (4)).\nProof. Consider the problem of minimizing f over Z , as defined in Appendix B.1. Let y, z \u2208 Z . With respect to \u2016 \u00b7 \u2016c = \u2016 \u00b7 \u2016\u221e we have, using the expression of the gradients from Eq. (22),\n\u2016\u2207f(y)\u2212\u2207f(z)\u2016\u221e =\u2016(\u2207f(y))(\u00b7, \u00b7, 1, \u00b7)\u2212 (\u2207f(z))(\u00b7, \u00b7, 1, \u00b7)\u2016\u221e =\u2016D\u0303Tk [\u03bbk + \u03b7k(D\u0303k \u2211\ns\u2032\u2032\ny(s\u2032\u2032)\u2212 \u03b1)]+ \u2212 D\u0303Tk [\u03bbk + \u03b7k(D\u0303k \u2211\ns\u2032\u2032\nz(s\u2032\u2032)\u2212 \u03b1)]+\u2016\u221e\n\u2264\u2016D\u0303Tk \u2016\u221e \u00b7 \u2016[\u03bbk + \u03b7k(D\u0303k \u2211\ns\u2032\u2032\ny(s\u2032\u2032)\u2212 \u03b1)]+ \u2212 [\u03bbk + \u03b7k(D\u0303k \u2211\ns\u2032\u2032\nz(s\u2032\u2032)\u2212 \u03b1)]+\u2016\u221e\nand using[a]+ \u2212 [b]+ \u2264 [a\u2212 b]+ \u2264 |a\u2212 b|\n\u2264\u2016D\u0303Tk \u20161 \u00b7 \u2016[\u03bbk + \u03b7k(D\u0303k \u2211\ns\u2032\u2032\ny(s\u2032\u2032)\u2212 \u03b1)] \u2212 [\u03bbk + \u03b7k(D\u0303k \u2211\ns\u2032\u2032\nz(s\u2032\u2032)\u2212 \u03b1)]\u2016\u221e\n=\u2016D\u0303Tk \u2016\u221e \u00b7 \u03b7k\u2016D\u0303k \u2211\ns\u2032\u2032\n(y(s\u2032\u2032)\u2212 z(s\u2032\u2032))\u2016\u221e\n\u2264\u2016D\u0303Tk \u2016\u221e \u00b7 \u03b7k\u2016D\u0303k\u2016\u221e \u2211\ns\u2032\u2032\n\u2016y(s\u2032\u2032)\u2212 z(s\u2032\u2032)\u2016\u221e\n\u2264SAH \u00b7 \u03b7k \u00b7 I \u00b7 S\u2016y \u2212 z\u2016\u221e,\nwhere the final bound holds since the entries of D\u0303k are in [0, 1]. Thus, \u2207f(\u00b7) is (S2AH \u00b7 \u03b7k \u00b7 I)Lipschitz continuous and thus f is (S2AH \u00b7 \u03b7k \u00b7 I)-smooth. Moreover, for all z \u2208 Z we have 0 \u2264 zh(s, a, s\n\u2032) \u2264 1 in every component, so for y, z \u2208 Z we have \u2016z\u2212y\u2016\u221e \u2264 1. Thus diam\u2016\u00b7\u2016\u221e(Z)2 \u2264 1. Plugging both into Theorem 6 yields the result.\nIt may be possible to improve this bound using a norm different from \u2016 \u00b7 \u2016c = \u2016 \u00b7 \u2016\u221e for the analysis."
        },
        {
            "heading": "D.3 Main Result",
            "text": "Our main result shows that with probability 1 \u2212 \u03b4, OPTAUG-CMDP achieves a sublinear regret in the number of episodes K and polylogarithmic in 1/\u03b4.\nTheorem 1. Suppose that Assumption 1 holds, let \u03b4 \u2208 (0, 1) and \u03bd > 0. Then there exist K \u2032 = O\u0303 ( max { S2AH3\n(1\u2212\u03bd)\u03b3 , NSAH4 (1\u2212\u03bd)2\u03b32\n})\nand \u03b7k, \u01ebk such that with probability at least 1\u2212 \u03b4, OPTAUGCMDP achieves a total regret of\nR(K; c) = O\u0303 (\u221a NSAH4K + S2AH3 +K \u2032H ) , R(K; d) = O\u0303 (\u221a NSAH4K + S2AH3 ) .\nProof. According to our regret decomposition (Observation 2), we have\nR(K; c) \u2264 K \u2032H+ K\u2211\nk=K\u2032+1 [V \u03c0k(c, p)\u2212V \u03c0k(c\u0303k, p\u0303k)]+ \ufe38 \ufe37\ufe37 \ufe38\nEstimation Error\n+\nK\u2211\nk=K\u2032+1\n[V \u03c0k(c\u0303k, p\u0303k)\u2212V \u03c0 \u2217 (c, p)]+\n\ufe38 \ufe37\ufe37 \ufe38\nOptimization Error\n,\nR(K; d) \u2264 max i\u2208[I]\nK\u2211\nk=K\u2032+1 [V \u03c0k(di, p)\u2212V \u03c0k(d\u0303i,k, p\u0303k)]+ \ufe38 \ufe37\ufe37 \ufe38\nEstimation Error\n+max i\u2208[I]\nK\u2211\nk=K\u2032+1 [V \u03c0k(d\u0303i,k, p\u0303k)\u2212\u03b1i]+ \ufe38 \ufe37\ufe37 \ufe38\nOptimization Error\n.\nSuppose the success event G occurs, which happens with probability at least 1 \u2212 \u03b4 as shown in Lemma 2. By Lemma 3, the estimation errors satisfy\nK\u2211\nk=K\u2032+1\n[V \u03c0k(c, p)\u2212 V \u03c0k(c\u0303k, p\u0303k)]+ \u2264O\u0303 (\u221a NSAH4K + S2AH3 ) ,\nmax i\u2208[I]\nK\u2211\nk=K\u2032+1\n[ V \u03c0k(di, p)\u2212 V \u03c0k(d\u0303i,k, p\u0303k) ]\n+ \u2264O\u0303\n(\u221a NSAH4K + S2AH3 ) .\nBy Lemma 7, the optimization errors satisfy\nK\u2211\nk=K\u2032+1\n[V \u03c0k(c\u0303k, p\u0303k)\u2212 V \u03c0 \u2217 (c, p)]+ \u2264 K\u2211\nk=K\u2032+1\n( (O(\u03c3k) + \u2211k t=K\u2032+1 \u221a 2\u03b7t\u01ebt) 2\n2\u03b7k + \u01ebk\n) \u2264 O( \u221a K),\nmax i\u2208[I]\nK\u2211\nk=K\u2032+1\n[V \u03c0k(d\u0303i,k, p\u0303k)\u2212 \u03b1i]+ \u2264 K\u2211\nk=K\u2032+1\nO(\u03c3k) + \u2211k\nt=K\u2032+1\n\u221a 2\u03b7t\u01ebk\n\u03b7k \u2264 O(\n\u221a K),\nwhere \u03c3 = H\u03bd\u03b3 and the term O(\u03c3k) can be replaced by (2 + 2(k \u2212K \u2032))\u03c3. As shown in Lemma 7, we can choose step sizes \u03b7K\u2032+k = \u0398(k 2.5) and \u01ebK\u2032+k = \u0398(1/\u03b7K\u2032+k) such that both of the above\nbounds are O( \u221a K) up to constant factors. Summing up concludes the proof.\nRemark 2. Note that while the bound for the estimation errors does not depend on the choices for \u03b7k and \u01ebk, the one for the optimization errors does. For any such choice, the general regret bound we obtain reads\nR(K; c) \u2264 O\u0303 (\u221a NSAH4K + S2AH3 +K \u2032H )\n+\nK\u2211\nk=K\u2032+1\n( (O(\u03c3k) + \u2211k t=K\u2032+1 \u221a 2\u03b7t\u01ebt) 2\n2\u03b7k + \u01ebk\n)\n, (31)\nR(K; d) \u2264 O\u0303 (\u221a NSAH4K + S2AH3 ) + K\u2211\nk=K\u2032+1\nO(\u03c3k) + \u2211k\nt=K\u2032+1\n\u221a 2\u03b7t\u01ebk\n\u03b7k ,\nwhere \u03c3 = H\u03bd\u03b3 and the term O(\u03c3k) can be replaced by (2 + 2(k \u2212K \u2032))\u03c3. We remark that there are multiple ways to choose \u03b7k, \u01ebk in the bound above that yield different regret guarantees and complexities of the inner algorithm INNEROPT-FW. One possible way is to choose a decreasing approximation error \u01ebk in every episode and \u03b7k large enough so that the parameterdependent terms above are of order O( \u221a K). This yields vanishing average regret for both the objective and the constraints. One such choice is11\n\u01ebK\u2032+k := 1\n2\u03b7K\u2032+k (k \u2265 1),\n\u03b7K\u2032+k := ((2 + 3k)\u03c3) 2.5 (k \u2265 1),\n11Note that if \u03b3 is not known, having an estimate of \u03c3 = H \u03bd\u03b3 that is larger than the true value is clearly\nsufficient.\nas we discuss in Lemma 7.\nIt is worth noting that when choosing \u03b7k very large and \u01ebk very small, the bound we get on the parameter-dependent terms in Eq. (31) gets better and even becomes constant when choosing \u03b7k, \u01ebk exponential in k \u2212K \u2032. However, the inner problem (Eq. (4)) we solve in every episode becomes computationally harder. This is due to the changing smoothness of the objective (with respect to the occupancy measure). Indeed, Proposition 1 quantifies the resulting iteration complexity, formalizing this tradeoff between statistical guarantees and computational efficiency. Thus, in practice, it is advised to choose \u03b7k as large (\u01ebk as small) as needed to ensure sublinear regret but as small as possible to avoid a higher computational cost."
        },
        {
            "heading": "E Omitted Proofs for Section 4",
            "text": ""
        },
        {
            "heading": "E.1 Preliminaries E.1.1 Preliminary Confidence Bounds",
            "text": "Fix an arbitrary number of episodes K and 0 < \u03b4 < 1, corresponding to a confidence of 1 \u2212 \u03b4. Moreover, let (\u03c0k) K k=1 be the policies according to which the optimistic estimates are updated (Section 3).\nDefine the following confidence sets\nBpk,h(s, a) := { p\u0303h(\u00b7|s, a) \u2208 \u2206(S)S | \u2200s\u2032 \u2208 S : |p\u0303h(s\u2032|s, a)\u2212 p\u0304k\u22121h (s\u2032|s, a)| \u2264 \u03b2 p k,h(s, a, s \u2032) } , Bck,h(s, a) := [ c\u0304k\u22121h (s, a)\u2212 \u03b2ck,h(s, a), c\u0304k\u22121h (s, a) + \u03b2ck,h(s, a) ] ,\nBdi,k,h(s, a) := [ d\u0304k\u22121i,h (s, a)\u2212 \u03b2di,k,h(s, a), d\u0304k\u22121i,h (s, a) + \u03b2di,k,h(s, a) ] ,\nwhere (derived using Hoeffding for the costs and empirical Bernstein for the transitions)\n\u03b2pk,h(s, a, s \u2032) :=2\n\u221a\np\u0304k\u22121h (s \u2032|s, a)(1 \u2212 p\u0304k\u22121h (s\u2032|s, a))L p \u03b4\nnk\u22121h (s, a) \u2228 1 +\n14 3 L p \u03b4\nnk\u22121h (s, a) \u2228 1 ,\n\u03b2ck,h(s, a) := \u03b2 d i,k,h(s, a) :=\n\u221a\nL\u03b4\nnk\u22121h (s, a) \u2228 1 ,\nwith\nLp\u03b4 := log\n( 6SAHK\n\u03b4\n)\nL\u03b4 := log\n( 6SAH(I + 1)K\n\u03b4\n)\n.\nWe want that, with probability at least 1 \u2212 \u03b4, the true MDP is contained within these bounds across all k \u2208 [K]. This will be needed for bounding the optimization error. With the same thresholds \u03b2pk,h, \u03b2 c k,h, \u03b2 d i,k,h as above we define the following failure events\nF pk := { \u2203s, a, s\u2032, h : |ph(s\u2032|s, a)\u2212 p\u0304k\u22121h (\u00b7|s, a)| > \u03b2 p k,h(s, a, s \u2032) } , F ck := { \u2203s, a, h : |c\u0304k\u22121h (s, a)\u2212 ch(s, a)| > \u03b2ck,h(s, a) } , F dk := { \u2203s, a, i, h : |d\u0304k\u22121i,h (s, a)\u2212 di,h(s, a)| > \u03b2di,k,h(s, a) } ,\nFNk :=\n   \u2203s, a, h : nk\u22121h (s, a) \u2264 \u2211\nj<k\nq \u03c0j h (s, a; p)\u2212H log\n( SAH\n\u03b4\u2032\n) \n  ,\nwhere we set \u03b4\u2032 := \u03b4/3. With this notation, set F p := \u222aKk=1F pk , F c := \u222aKk=1F ck , F d := \u222aKk=1F dk , FN := \u222aKk=1FNk and finally denote the event that none of the failure events ever occurs by\nG := ( F p \u22c3 F c \u22c3 F d \u22c3 FN ) ,\nwhich we will refer to as the success event.\nWe immediately have the following by construction G and the optimistic estimates.\nObservation 1. Conditioned on the success event G, for every k \u2208 [K] the CMDP with cost c\u0303k and constraint costs (d\u0303i,k)i\u2208[I] is optimistic and all B p k(s, a) contain the true respective transition probability distribution, i.e.,\nc\u0303k \u2264 c, d\u0303i,k \u2264 di, and p \u2208 Bpk, where Bpk := {p\u0303 | \u2200s, a, h : p\u0303h(\u00b7|s, a) \u2208 B p k,h(s, a)} is the set of plausible transition probabilities.\nConcentration bounds on the individual random variables and a union bound over all indices now allow us to establish the following result. In Appendix D.3, we show that conditioned on G, the regrets are sublinear in K and polylogarithmic in 1/\u03b4.\nLemma 2. Fix \u03b4 \u2208 (0, 1) and define the optimistic model in Eq. (3) accordingly. Then, the success event G occurs with probability at least 1\u2212 \u03b4, i.e., P [G] \u2265 1\u2212 \u03b4.\nProof. Efroni et al. (2020, Appendix A.1) give a proof of this. Note that the proof does not require any specific properties of the used policy iterates (\u03c0k)k\u2208[K] but only uses that the collected costs and transitions are i.i.d. across episodes.\nMoreover, we have the following result, allowing us to bound the estimation error. The absolute constants could be specified via a short calculation, but we omit this for brevity.\nLemma 14 (Lemma 8, Jin et al. (2019)). In the setup above, conditioned on G, there exist absolute constants C1, C2 > 0 such that for all indices k, h, s, a, s \u2032 we have\n|ph(s\u2032|s, a)\u2212 p\u0304k\u22121h (s\u2032|s, a)| \u2264 C1 \u221a\nph(s\u2032|s, a)Lp\u03b4 nk\u22121h (s, a) \u2228 1 + C2 Lp\u03b4 nk\u22121h (s, a) \u2228 1 ,\nwith Lp\u03b4 = log ( 6SAHK \u03b4 ) as before.\nProof. We refer to Jin et al. (2019, Lemma 8) for a proof."
        },
        {
            "heading": "E.1.2 Regret Decomposition",
            "text": "The simple observation that we can decompose the regrets can be seen as follows.\nObservation 2 (Regret decomposition).\nR(K; c) \u2264 K \u2032H+ K\u2211\nk=K\u2032+1 [V \u03c0k(c, p)\u2212V \u03c0k(c\u0303k, p\u0303k)]+ \ufe38 \ufe37\ufe37 \ufe38\nEstimation Error\n+\nK\u2211\nk=K\u2032+1\n[V \u03c0k(c\u0303k, p\u0303k)\u2212V \u03c0 \u2217 (c, p)]+\n\ufe38 \ufe37\ufe37 \ufe38\nOptimization Error\nR(K; d) \u2264 max i\u2208[I]\nK\u2211\nk=K\u2032+1 [V \u03c0k(di, p)\u2212V \u03c0k(d\u0303i,k, p\u0303k)]+ \ufe38 \ufe37\ufe37 \ufe38\nEstimation Error\n+max i\u2208[I]\nK\u2211\nk=K\u2032+1 [V \u03c0k(d\u0303i,k, p\u0303k)\u2212\u03b1i]+ \ufe38 \ufe37\ufe37 \ufe38\nOptimization Error\nProof. We first split the regrets between the two phases of the algorithm.\nR(K; c) = K\u2032\u2211\nk=1 [V \u03c0k(c, p)\u2212 V \u03c0\u2217(c, p)]+ \ufe38 \ufe37\ufe37 \ufe38\nPre-Training, \u2264K\u2032H\n+ K\u2211\nk=K\u2032+1 [V \u03c0k(c, p)\u2212 V \u03c0\u2217(c, p)]+ \ufe38 \ufe37\ufe37 \ufe38\nOptimistic Exploration\nR(K; d) \u2264 max i\u2208[I]\nK\u2032\u2211\nk=1 [V \u03c0k(di, p)\u2212 \u03b1i]+ \ufe38 \ufe37\ufe37 \ufe38\nPre-Training, =0\n+max i\u2208[I]\nK\u2211\nk=K\u2032+1 [V \u03c0k(di, p)\u2212 \u03b1i]+ \ufe38 \ufe37\ufe37 \ufe38\nOptimistic Exploration\nWe can trivially bound the objective regret during the pre-training phase by K \u2032H (since the expected costs are in [0, 1] and the time horizon is H , so the value functions are in [0, H ]). Since \u03c0\u0304 is strictly feasible, there is no constraint regret during pre-training.\nWe now focus on the regrets incurred in the optimistic exploration phase. We split the sum and use that [a+ b]+ \u2264 [a]+ + [b]+. For the objective, regret we have\nR(K; c) =K \u2032H + K\u2211\nk=K\u2032+1\n[V \u03c0k(c, p)\u2212 V \u03c0\u2217(c, p)]+\n=K \u2032H +\nK\u2211\nk=K\u2032+1\n[V \u03c0k(c, p)\u2212 V \u03c0k(c\u0303k, p\u0303k) + V \u03c0k(c\u0303k, p\u0303k)\u2212 V \u03c0 \u2217 (c, p)]+\n\u2264K \u2032H + K\u2211\nk=K\u2032+1\n[V \u03c0k(c, p)\u2212 V \u03c0k(c\u0303k, p\u0303k)]+ + K\u2211\nk=K\u2032+1\n[V \u03c0k(c\u0303k, p\u0303k)\u2212 V \u03c0 \u2217 (c, p)]+\nand similarly, for the constraint regret, we get\nR(K; d) =0 +max i\u2208[I]\n( K\u2211\nk=K\u2032+1\n[V \u03c0k(di, p)\u2212 \u03b1i]+\n)\n=max i\u2208[I]\nK\u2211\nk=K\u2032+1\n[ V \u03c0k(di, p)\u2212 \u03b1i \u2212 (V \u03c0k(d\u0303i,k, p\u0303k)\u2212 \u03b1i) + (V \u03c0k(d\u0303i,k, p\u0303k)\u2212 \u03b1i) ]\n+\n\u2264max i\u2208[I]\n( K\u2211\nk=K\u2032+1\n[ V \u03c0k(di, p)\u2212 V \u03c0k(d\u0303i,k, p\u0303k) ]\n+ +\nK\u2211\nk=K\u2032+1\n[ V \u03c0k(d\u0303i,k, p\u0303k)\u2212 \u03b1i ]\n+\n)\n\u2264max i\u2208[I]\nK\u2211\nk=K\u2032+1\n[ V \u03c0k(di, p)\u2212 V \u03c0k(d\u0303i,k, p\u0303k) ]\n+ +max i\u2208[I]\nK\u2211\nk=K\u2032+1\n[ V \u03c0k(d\u0303i,k, p\u0303k)\u2212 \u03b1i ]\n+ ."
        },
        {
            "heading": "E.2 Omitted Proofs for Section 4.1",
            "text": "We leverage the on-policy error bound from Lemma 10 and the preliminaries from Appendix E.1.1 to establish the desired bound on the estimation errors.\nLemma 3 (Estimation errors). Let (\u03c0k) K k=K\u2032+1 be the sequence of policies obtained by OPTAUGCMDP. Then, conditioned on G, we can bound the estimation errors as follows:\nK\u2211\nk=K\u2032+1\n[V \u03c0k(c, p)\u2212 V \u03c0k(c\u0303k, p\u0303k)]+ \u2264O\u0303 (\u221a NSAH4K + S2AH3 ) ,\nmax i\u2208[I]\nK\u2211\nk=K\u2032+1\n[V \u03c0k(di, p)\u2212 V \u03c0k(d\u0303i,k, p\u0303k)]+ \u2264O\u0303 (\u221a NSAH4K + S2AH3 ) .\nProof. Condition on the success event G. Then by construction of G, the assumptions (a) and (b) in Lemma 10 are met for l = c and l = di for all constraints i \u2208 [I]. By Lemma 14, also assumption (c) in Lemma 10 is met with p\u0302k := p\u0303k from the k-th iteration of OPTAUG-CMDP.\nFirst, consider the terms [V \u03c0k(c, p) \u2212 V \u03c0k(c\u0303k, p\u0303k)]+. For the cost l = c, the values of the true and the estimated MDPs are V \u03c0k(c, p) and V \u03c0k(c\u0303k, p\u0303k), respectively. Thus, invoking Lemma 10 we find\nK\u2211\nk=K\u2032+1\n[V \u03c0k(c, p)\u2212 V \u03c0k(c\u0303k, p\u0303k)]+ \u2264 K\u2211\nk=K\u2032+1\n|V \u03c0k(c, p)\u2212 V \u03c0k(c\u0303k, p\u0303k)|\n\u2264 K\u2211\nk=1\n|V \u03c0k(c, p)\u2212 V \u03c0k(c\u0303k, p\u0303k)|\n\u2264O\u0303 (\u221a NSAH4K + S2AH3 ) .\nSimilarly, let i \u2208 [I] and consider the terms [V \u03c0k(di, p) \u2212 V \u03c0k(d\u0303i,k, p\u0303k)]+. For the cost l = di, the values of the true and the estimated MDPs are V \u03c0k(di, p) and V\n\u03c0k(d\u0303i,k, p\u0303k), respectively. Thus, invoking Lemma 10 we find\nK\u2211\nk=K\u2032+1\n[V \u03c0k(di, p)\u2212 V \u03c0k(d\u0303i,k, p\u0303k)]+ \u2264 K\u2211\nk=1\n|V \u03c0k(di, p)\u2212 V \u03c0k(d\u0303i,k, p\u0303k)|\n\u2264O\u0303 (\u221a NSAH4K + S2AH3 ) ."
        },
        {
            "heading": "E.3 Omitted Proofs for Section 4.2",
            "text": "For notational convenience, we write D = (di)i\u2208[I] and D\u0303k = (d\u0303i,k)i\u2208[I] throughout this section."
        },
        {
            "heading": "E.3.1 Preliminary Bounds",
            "text": "Recall that in OPTAUG-CMDP, we find an \u01ebk-close solution to the inner problem Eq. (4) in every episode k \u2208 {K \u2032 + 1, . . . ,K}.\nLemma 4. Conditioned on G, for each k \u2208 {K \u2032 + 1, . . . ,K}, in OPTAUG-CMDP we have\nV \u03c0k(c\u0303k, p\u0303k)\u2212 V \u03c0 \u2217 (c, p) \u2264 \u01ebk + \u2016\u03bbk\u20162 \u2212 \u2016\u03bbk+1\u20162\n2\u03b7k ,\nV \u03c0k(d\u0303i,k, p\u0303k)\u2212 \u03b1i \u2264 \u03bbk+1(i)\u2212 \u03bbk(i)\n\u03b7k (\u2200i \u2208 [I]).\nProof. We first bound the error for the objective cost. Let k \u2208 {K \u2032 + 1, . . . ,K}. Conditioned on the success event, we have (componentwise) c \u2265 c\u0303k, D \u2265 D\u0303k and p \u2208 Bpk (Observation 1), and thus\nV \u03c0 \u2217 (c, p)\n=\n(\nV \u03c0 \u2217 (c, p) + 1\n2\u03b7k \u2016[\u03bbk + \u03b7k(V \u03c0\n\u2217 (D, p)\u2212 \u03b1)]+\u20162 )\n\u2212 1 2\u03b7k \u2016[\u03bbk + \u03b7k(V \u03c0 \u2217 (D, p)\u2212 \u03b1)]+\u20162\n\u2265 ( V \u03c0 \u2217 (c\u0303k, p) + 1\n2\u03b7k \u2016[\u03bbk + \u03b7k(V \u03c0\n\u2217 (D\u0303k, p)\u2212 \u03b1)]+\u20162 )\n\u2212 1 2\u03b7k \u2016[\u03bbk + \u03b7k(V \u03c0 \u2217 (D, p)\u2212 \u03b1)]+\u20162\n\u2265 min \u03c0\u2208\u03a0 p\u2032\u2208Bp\nk\n(\nV \u03c0(c\u0303k, p \u2032) +\n1\n2\u03b7k \u2016[\u03bbk + \u03b7k(V \u03c0(D\u0303k, p\u2032)\u2212 \u03b1)]+\u20162\n)\n\u2212 1 2\u03b7k \u2016[\u03bbk + \u03b7k(V \u03c0 \u2217 (D, p)\u2212 \u03b1)]+\u20162\n\u2265 ( V \u03c0k(c\u0303k, p\u0303k) + 1\n2\u03b7k \u2016[\u03bbk + \u03b7k(V \u03c0k(D\u0303k, p\u0303k)\u2212 \u03b1)]+\u20162 \u2212 \u01ebk\n)\n\u2212 1 2\u03b7k \u2016[\u03bbk + \u03b7k(V \u03c0 \u2217 (D, p)\u2212 \u03b1)]+\u20162\n=V \u03c0k(c\u0303k, p\u0303k)\u2212 \u01ebk + 1\n2\u03b7k \u2016[\u03bbk + \u03b7k(V \u03c0k(D\u0303k, p\u0303k)\u2212 \u03b1)]+\u20162\n\u2212 1 2\u03b7k \u2016[\u03bbk + \u03b7k(V \u03c0 \u2217 (D, p)\u2212 \u03b1)]+\u20162,\nwhere the first inequality is due to optimism and monotonicity of \u2016[\u00b7]+\u20162, the second due to p \u2208 Bpk , and the third due to the \u01ebk-closeness of \u03c0k, p\u0303k. Now since \u03c0 \u2217 is primal-feasible we have V \u03c0 \u2217 (D, p)\u2212 \u03b1 \u2264 0 and thus \u2016[\u03bbk + \u03b7k(V \u03c0 \u2217 (D, p)\u2212 \u03b1)]+\u20162 \u2264 \u2016\u03bbk\u20162 by monotonicity of \u2016[\u00b7]+\u20162.\nPlugging in the update for \u03bbk+1 thus shows\n1\n2\u03b7k \u2016[\u03bbk + \u03b7k(V \u03c0k(D\u0303k, p\u0303k)\u2212 \u03b1)]+\u20162 \u2212\n1\n2\u03b7k \u2016[\u03bbk + \u03b7k(V \u03c0\n\u2217\n(D, p)\u2212 \u03b1)]+\u20162\n\u2265 1 2\u03b7k \u2016[\u03bbk + \u03b7k(V \u03c0k(D\u0303k, p\u0303k)\u2212 \u03b1)]+\u20162 \u2212 1 2\u03b7k \u2016\u03bbk\u20162 = 1\n2\u03b7k \u2016\u03bbk+1\u20162 \u2212\n1\n2\u03b7k \u2016\u03bbk\u20162,\nconcluding the proof after plugging this into the previous inequality and rearranging.\nWe now proceed by proving the bound on the constraint cost. Let k \u2208 {K \u2032 +1, . . . ,K}. Let i \u2208 [I] and recall that \u03bbk+1 = [\u03bbk + \u03b7k(V\n\u03c0k(D\u0303k, p\u0303k) \u2212 \u03b1)]+. If \u03bbk+1(i) > 0, then we have \u03bbk+1(i) \u2212 \u03bbk(i) = \u03b7k(V\n\u03c0k(d\u0303i,k, p\u0303k) \u2212 \u03b1i) with equality. On the other hand, if \u03bbk+1(i) = 0, then \u03bbk(i) + \u03b7k(V\n\u03c0k(d\u0303i,k, p\u0303k) \u2212 \u03b1i) \u2264 0 and thus \u03bbk+1(i) \u2212 \u03bbk(i) = \u2212\u03bbk(i) \u2265 (\u03bbk(i) + \u03b7k(V \u03c0k(d\u0303i,k, p\u0303k) \u2212 \u03b1i))\u2212\u03bbk(i) = \u03b7k(V \u03c0k(d\u0303i,k, p\u0303k)\u2212\u03b1i). Thus in both cases \u03bbk+1(i)\u2212\u03bbk(i) \u2265 \u03b7k(V \u03c0k(d\u0303i,k, p\u0303k)\u2212\u03b1i), which proves the claim."
        },
        {
            "heading": "E.3.2 Bounding the Dual Iterates",
            "text": "In order to prove Lemma 19, which implies Lemma 5, we first establish (Lemmas 15 to 17), following the analysis of Xu (2021).\nLemma 15. Let k \u2208 {K \u2032 + 1, . . . ,K}. In the k-th iteration of OPTAUG-CMDP we have, for all \u03bb \u2208 RI\u22650,\n1\n2\u03b7k\n( \u2016\u03bbk+1 \u2212 \u03bb\u20162 \u2212 \u2016\u03bbk \u2212 \u03bb\u20162 + \u2016\u03bbk+1 \u2212 \u03bbk\u20162 )\n= I\u2211\ni=1\n(\u03bbk+1(i)\u2212 \u03bb(i))max{\u2212 \u03bbk(i)\n\u03b7k , (V \u03c0k(D\u0303k, p\u0303k)\u2212 \u03b1)i}.\nProof. Using 2uTv = \u2016u\u20162 + \u2016v\u20162 \u2212 \u2016u \u2212 v\u20162 with u = \u03bbk+1 \u2212 \u03bbk, v = \u03bbk+1 \u2212 \u03bb we find that the LHS in the Lemma reads\n1\n2\u03b7k\n( \u2016\u03bbk+1 \u2212 \u03bb\u20162 \u2212 \u2016\u03bbk \u2212 \u03bb\u20162 + \u2016\u03bbk+1 \u2212 \u03bbk\u20162 ) = 1\n2\u03b7k 2(\u03bbk+1 \u2212 \u03bb)T (\u03bbk+1 \u2212 \u03bbk).\nThe update rule of for \u03bbk+1 can equivalently be written as\n\u03bbk+1(i) = \u03bbk(i) + \u03b7k max{\u2212 \u03bbk(i)\n\u03b7k , (V \u03c0k(D\u0303k, p\u0303k)\u2212 \u03b1)i},\nfor all i \u2208 [I], from which we obtain \u03bbk+1(i) \u2212 \u03bbk(i) = \u03b7k max{\u2212\u03bbk(i)\u03b7k , (V \u03c0k(D\u0303k, p\u0303k) \u2212 \u03b1)i}. Plugging this into the equation above proves the Lemma.\nLemma 16. Let k \u2208 {K \u2032 + 1, . . . ,K}. In the k-th iteration of OPTAUG-CMDP we have for, any \u03bb \u2208 RI\u22650,\nI\u2211\ni=1\n(\u03bbk+1(i)\u2212 \u03bb(i))max{\u2212 \u03bbk(i)\n\u03b7k , (V \u03c0k(D\u0303k, p\u0303k)\u2212 \u03b1)i}\n\u2264 I\u2211\ni=1\n(\u03bbk+1(i)\u2212 \u03bb(i))(V \u03c0k(D\u0303k, p\u0303k)\u2212 \u03b1)i.\nProof. Fix k and set I+ := {i \u2208 [I] | \u03bbk(i) + \u03b7k(V \u03c0k(D\u0303k, p\u0303k) \u2212 \u03b1)i > 0} = {i \u2208 [I] | \u2212\u03bbk(i)\u03b7k < (V \u03c0k(D\u0303k, p\u0303k) \u2212 \u03b1)i} and I\u2212 := [I] \\ I+. Then subtracting the RHS from the LHS in the Lemma\nwe get I\u2211\ni=1\n(\u03bbk+1(i)\u2212 \u03bb(i))max{\u2212 \u03bbk(i)\n\u03b7k , (V \u03c0k(D\u0303k, p\u0303k)\u2212 \u03b1)i}\n\u2212 I\u2211\ni=1\n(\u03bbk+1(i)\u2212 \u03bb(i))(V \u03c0k(D\u0303k, p\u0303k)\u2212 \u03b1)i\n= \u2211\ni\u2208I+\n(\u03bbk+1(i)\u2212 \u03bb(i))((V \u03c0k(D\u0303k, p\u0303k)\u2212 \u03b1)i \u2212 (V \u03c0k(D\u0303k, p\u0303k)\u2212 \u03b1)i)\n+ \u2211\ni\u2208I\u2212\n\u03bb(i)((V \u03c0k(D\u0303k, p\u0303k)\u2212 \u03b1)i + \u03bbk(i)\n\u03b7k )\n= \u2211\ni\u2208I\u2212\n\u03bb(i)((V \u03c0k(D\u0303k, p\u0303k)\u2212 \u03b1)i + \u03bbk(i)\n\u03b7k ),\nand using that \u03bb \u2265 0 as well as (V \u03c0k(D\u0303k, p\u0303k) \u2212 \u03b1)i + \u03bbk(i)\u03b7k \u2264 0 for i \u2208 I\u2212 shows that this is \u2264 0, which proves the claim.\nCombining the previous two lemmas, we find the analogous result of the one-step progress inequality of the standard inexact augmented Lagrangian method analysis:\nLemma 17 (One-step progress of iALM). Let k \u2208 {K \u2032+1, . . . ,K}. In step k of OPTAUG-CMDP, for all \u03c0 \u2208 \u03a0 and all \u03bb \u2265 0 we have\nV \u03c0k(c\u0303k, p\u0303k) + \u03bb T (V \u03c0k(D\u0303k, p\u0303k)\u2212 \u03b1) +\n1\n2\u03b7k \u2016\u03bbk+1 \u2212 \u03bb\u20162\n\u2264 L\u0303k(\u03c0, \u03bbk) + 1\n2\u03b7k \u2016\u03bbk \u2212 \u03bb\u20162 + \u01ebk,\nwhere for notational convenience\nL\u0303k(\u03c0, \u03bb) := V \u03c0(c\u0303k, p\u0303k) +\n1\n2\u03b7k \u2016[\u03bb+ \u03b7k(V \u03c0(D\u0303k, p\u0303k)\u2212 \u03b1)]+\u20162 \u2212\n1\n2\u03b7k \u2016\u03bb\u20162.\nProof. By \u01ebk-closeness, for all \u03c0 \u2208 \u03a0 we have V \u03c0k(c\u0303k, p\u0303k) + 1\n2\u03b7k \u2016\u03bbk+1\u2016 \u2264V \u03c0(c\u0303k, p\u0303k) +\n1 2\u03b7k \u2016 [ \u03bbk + \u03b7k(V \u03c0(D\u0303k, p\u0303k)\u2212 \u03b1)]+ \u2225 \u2225 \u2225 2 + \u01ebk\n=L\u0303k(\u03c0, \u03bbk) + 1\n2\u03b7k \u2016\u03bbk\u20162 + \u01ebk. (32)\nIn addition, again by distinguishing between indices in I+ and I\u2212 (see proof of Lemma 16), we see 1\n2\u03b7k \u2016[\u03bbk + \u03b7k(V \u03c0k(D\u0303k, p\u0303k)\u2212 \u03b1)]+\u20162 \u2212\n1\n2\u03b7k \u2016\u03bbk\u20162\n\u2212 \u2211\ni\u2208[I]\n[\u03bbk(i) + \u03b7k(V \u03c0k(D\u0303k, p\u0303k)\u2212 \u03b1)i]+(V \u03c0k(D\u0303k, p\u0303k)\u2212 \u03b1)i\n= 1\n2\u03b7k\n\u2211\ni\u2208I+\n(\n(\u03bbk(i) + \u03b7k(V \u03c0k(D\u0303k, p\u0303k)\u2212 \u03b1)i)2 \u2212 \u03bbk(i)2\n\u2212 2(\u03bbk(i) + \u03b7k(V \u03c0k(D\u0303k, p\u0303k)\u2212 \u03b1)i)\u03b7k(V \u03c0k(D\u0303k, p\u0303k)\u2212 \u03b1)i )\n\u2212 1 2\u03b7k\n\u2211\ni\u2208I\u2212\n\u03bbk(i) 2\n= 1\n2\u03b7k\n\u2211\ni\u2208I+\n\u2212(\u03b7k(V \u03c0k(D\u0303k, p\u0303k)\u2212 \u03b1)i)2 \u2212 1\n2\u03b7k\n\u2211\ni\u2208I\u2212\n\u03bbk(i) 2\n= 1\n2\u03b7k\n\u2211\ni\u2208I+\n\u2212(\u03bbk+1(i)\u2212 \u03bbk(i))2 \u2212 1\n2\u03b7k\n\u2211\ni\u2208I\u2212\n(\u03bbk+1(i)\u2212 \u03bbk(i))2\n=\u2212 1 2\u03b7k \u2016\u03bbk+1 \u2212 \u03bbk\u20162,\nand with Eq. (32) this shows\nV \u03c0k(c\u0303k, p\u0303k) + \u2211\ni\u2208[I]\n[\u03bbk(i) + \u03b7k(V \u03c0k(D\u0303k, p\u0303k)\u2212 \u03b1)i]+(V \u03c0k(D\u0303k, p\u0303k)\u2212 \u03b1)i\n\u2212 1 2\u03b7k \u2016\u03bbk+1 \u2212 \u03bbk\u20162\n=V \u03c0k(c\u0303k, p\u0303k) + 1\n2\u03b7k \u2016\u03bbk+1\u20162 \u2212\n1\n2\u03b7k \u2016\u03bbk\u20162\nEq. (32)\n\u2264 L\u0303k(\u03c0, \u03bbk) + \u01ebk. (33) Now combining Lemma 15 and Lemma 16, we also have\n\u2016\u03bbk+1 \u2212 \u03bb\u20162 \u2212 \u2016\u03bbk \u2212 \u03bb\u20162 + \u2016\u03bbk+1 \u2212 \u03bbk\u20162 2\u03b7k\n\u2264 \u2211\ni\u2208[I]\n([\u03bbk(i) + \u03b7k(V \u03c0k(D\u0303k, p\u0303k)\u2212 \u03b1)i]+ \u2212 \u03bb(i))(V \u03c0k(D\u0303k, p\u0303k)\u2212 \u03b1)i.\nHence\nV \u03c0k(c\u0303k, p\u0303k) + 1\n2\u03b7k \u2016\u03bbk+1 \u2212 \u03bb\u20162 \u2212\n1\n2\u03b7k \u2016\u03bbk \u2212 \u03bb\u20162\n\u2264V \u03c0k(c\u0303k, p\u0303k) + \u2211\ni\u2208[I]\n([\u03bbk(i) + \u03b7k(V \u03c0k(D\u0303k, p\u0303k)\u2212 \u03b1)i]+ \u2212 \u03bb(i))(V \u03c0k(D\u0303k, p\u0303k)\u2212 \u03b1)i\n\u2212 1 2\u03b7k \u2016\u03bbk+1 \u2212 \u03bbk\u20162\nEq. (33) \u2264 \u2212 \u2211\ni\u2208[I]\n\u03bb(i)(V \u03c0k(D\u0303k, p\u0303k)\u2212 \u03b1)i + L\u0303k(\u03c0, \u03bbk) + \u01ebk,\nand rearranging this yields the desired inequality.\nBefore proving Lemma 5, we must finally establish Lemma 18. This result is based on standard properties from constrained convex optimization (Appendix A.4) and the LP formulation of CMDPs (Appendix A.3). Recall that we have V \u03c0(c\u0303k, p\u0303k) = c\u0303 T k q \u03c0(p\u0303k) and V \u03c0(D\u0303k, p\u0303k) = D\u0303kq \u03c0(p\u0303k), where q\u03c0(p\u0303k) \u2208 RSAH , c\u0303k \u2208 RSAH , d\u0303i,k \u2208 RSAH , and D\u0303k \u2208 RI\u00d7SAH are defined as in Appendix A.3. Thus, when switching to occupancy measures, the optimistic problem (Eq. (6)) equivalently reads\nmin q\u03c0\u2208Q(p\u0303k)\nc\u0303Tk q \u03c0 s.t. D\u0303kq \u03c0 \u2212 \u03b1 \u2264 0, (34)\nwhere Q(p\u0303k) \u2282 RSAH is defined as in Appendix A.3. This is a convex optimization problem over the set of occupancy measures Q(p\u0303k).\nFormally, we make the following assumption. By Lemma 1, the assumption will be guaranteed to hold with (with \u03c00 = \u03c0\u0304 and \u03c3 = H\u03bd\u03b3 ), conditioned on G.\nAssumption 3 (Slater points). There exists \u03c00 \u2208 \u03a0 such that for all k \u2208 {K \u2032, . . . ,K} and all i \u2208 [I] we have V \u03c00(d\u0303i,k, p\u0303k) < \u03b1i. In particular, Eq. (6) is feasible. Let \u03c0\u2217k be an optimal solution for Eq. (6) and suppose there is a fixed constant \u03c3 > 0 such that, for all k \u2208 {K \u2032, . . . ,K},\nV \u03c0 0 (c\u0303k, p\u0303k)\u2212 V \u03c0 \u2217 k(c\u0303k, p\u0303k)\nmini\u2208[I](\u03b1i \u2212 V \u03c00(d\u0303i,k, p\u0303k)) \u2264 \u03c3.\nRemark 3. Note that, under Assumption 3, we can view Eq. (6) as the convex optimization problem in Eq. (34) over Q(p\u0303k) that satisfies all parts of Assumption 2 from Appendix A.4. Indeed,\n(a) X := Q(p\u0303k) is a polytope and thus convex\n(b) the objective f(\u00b7) := c\u0303Tk (\u00b7) is affine and thus convex\n(c) the constraints gi(\u00b7) := d\u0303Ti,k(\u00b7)\u2212 \u03b1i are affine and thus convex (d) by Assumption 3, Eq. (34) is feasible, and thus its minimum is attained (since the domain\nis compact and the objective continuous)\n(e) a Slater point exists by Assumption 3, namely q\u03c0 0\n(p\u0303k)\n(f) all dual problems have an optimal solution since the domainX is compact and the objective f(\u00b7) + \u03bbT g(\u00b7) is continuous,\nwhere Q(p\u0303k) \u2282 RSAH , c\u0303k \u2208 RSAH and d\u0303i,k \u2208 RSAH are defined as in Appendix A.3, and the setup is in line with the general convex optimization setup described in Appendix A.4.\nLemma 18. If Eq. (34) is stricly feasible, then there exists a point (\u03c0\u2217k, \u03bb \u2217 k) with q \u03c0\u2217k = q\u03c0 \u2217\nk(p\u0303k) optimal for Eq. (34) and \u03bb\u2217k optimal for its dual problem. For any such pair and any \u03c0 \u2208 \u03a0 we have\nV \u03c0(c\u0303k, p\u0303k)\u2212 V \u03c0 \u2217 k(c\u0303k, p\u0303k) + (\u03bb \u2217 k) T (V \u03c0(D\u0303k, p\u0303k)\u2212 \u03b1) \u2265 0.\nProof. By Remark 3, we are in the setup of Eq. (14) under Assumption 2. Thus, by applying Theorem 4, in the notation of Appendix A.3 we immediately get\nc\u0303Tk q \u03c0(p\u0303k)\u2212 c\u0303Tk q\u03c0\n\u2217 k(p\u0303k) + (\u03bb \u2217 k) T (D\u0303kq \u03c0(p\u0303k)\u2212 \u03b1) \u2265 0,\nwhich proves the claim by plugging in the value functions.\nThe following lemma allows us to deduce Lemma 5.\nLemma 19. Let k \u2208 {K \u2032 + 1, . . . ,K} and suppose Eq. (6) is strictly feasible. Let (\u03c0\u2217k, \u03bb\u2217k) be a pair of primal-optimal and dual-optimal solutions for Eq. (6) (see Lemma 18). Then the iterates of OPTAUG-CMDP satisfy\n\u2016\u03bbk+1 \u2212 \u03bb\u2217k\u20162 \u2264\u2016\u03bbk \u2212 \u03bb\u2217k\u20162 + 2\u03b7k\u01ebk.\nProof. Fix an arbitrary point (\u03c0\u2217k, \u03bb \u2217 k) as in Lemma 18. Then Lemma 18 with \u03c0 = \u03c0k (note that \u03c0 need not satisfy the constraints) we have\n0 \u2264V \u03c0k(c\u0303k, p\u0303k)\u2212 V \u03c0 \u2217 k(c\u0303k, p\u0303k) + (\u03bb \u2217 k) T (V \u03c0k(D\u0303k, p\u0303k)\u2212 \u03b1),\nand by feasibility of \u03c0\u2217k we have L\u0303k(\u03c0 \u2217 k, \u03bbk) \u2264 V \u03c0\n\u2217\nk(c\u0303k, p\u0303k), with L\u0303k(\u03c0, \u03bb) as defined in Lemma 17. Thus, the above becomes\n0 \u2264V \u03c0k(c\u0303k, p\u0303k)\u2212 L\u0303k(\u03c0\u2217k, \u03bbk) + (\u03bb\u2217k)T (V \u03c0k(D\u0303k, p\u0303k)\u2212 \u03b1). (35) Moreover, from Lemma 17 we know that\n2\u03b7k(V \u03c0k(c\u0303k, p\u0303k) + \u03bb T (V \u03c0k(D\u0303k, p\u0303k)\u2212 \u03b1)) + \u2016\u03bbk+1 \u2212 \u03bb\u20162\n\u2264 2\u03b7kL\u0303k(\u03c0, \u03bbk) + \u2016\u03bbk \u2212 \u03bb\u20162 + 2\u03b7k\u01ebk, and we can choose (\u03c0, \u03bb) = (\u03c0\u2217k, \u03bb \u2217 k) in this inequality to get\n2\u03b7k(V \u03c0k(c\u0303k, p\u0303k) + (\u03bb \u2217 k) T (V \u03c0k(D\u0303k, p\u0303k)\u2212 \u03b1)) + \u2016\u03bbk+1 \u2212 \u03bb\u2217k\u20162\n\u2264 2\u03b7kL\u0303k(\u03c0\u2217k, \u03bbk) + \u2016\u03bbk \u2212 \u03bb\u2217k\u20162 + 2\u03b7k\u01ebk. Adding 2\u03b7k times Eq. (35) to this and cancelling terms yields\n\u2016\u03bbk+1 \u2212 \u03bb\u2217k\u20162 \u2264\u2016\u03bbk \u2212 \u03bb\u2217k\u20162 + 2\u03b7k\u01ebk.\nFrom Lemma 19, we readily obtain Lemma 5.\nLemma 5. Let k \u2208 {K \u2032 + 1, . . . ,K} and suppose Eq. (6) is strictly feasible for every k\u2032 \u2208 {K \u2032, . . . ,K}. Let (\u03c0\u2217k\u2032 , \u03bb\u2217k\u2032 ) be pairs of primal-optimal and dual-optimal solutions for Eq. (6). Then the iterates of OPTAUG-CMDP satisfy\n\u2016\u03bbk+1\u2016 \u2264 2 k\u2211\nt=K\u2032\n\u2016\u03bb\u2217t \u2016+ k\u2211\nt=K\u2032+1\n\u221a\n2\u03b7t\u01ebt.\nProof. Taking square root and using \u221a a+ b \u2264 \u221aa + \u221a b in Lemma 19 yields \u2016\u03bbk+1 \u2212 \u03bb\u2217k\u2016 \u2264\n\u2016\u03bbk \u2212 \u03bb\u2217k\u2016+ \u221a 2\u03b7k\u01ebk. By triangle inequality we get\n\u2016\u03bbk+1 \u2212 \u03bb\u2217k\u2016 \u2264\u2016\u03bbk \u2212 \u03bb\u2217k\u2016+ \u221a 2\u03b7k\u01ebk \u2264 \u2016\u03bbk \u2212 \u03bb\u2217k\u22121\u2016+ \u2016\u03bb\u2217k\u22121 \u2212 \u03bb\u2217k\u2016+ \u221a 2\u03b7k\u01ebk,\nand thus by induction\n\u2016\u03bbk+1 \u2212 \u03bb\u2217k\u2016 \u2264\u2016\u03bbK\u2032+1 \u2212 \u03bb\u2217K\u2032\u2016+ k\u2211\nt=K\u2032+1\n\u2016\u03bb\u2217t\u22121 \u2212 \u03bb\u2217t \u2016+ k\u2211\nt=K\u2032+1\n\u221a\n2\u03b7t\u01ebt\n\u2264\u2016\u03bbK\u2032+1\u2016+ \u2016\u03bb\u2217K\u2032\u2016+ k\u2211\nt=K\u2032+1\n(\u2016\u03bb\u2217t\u22121\u2016+ \u2016\u03bb\u2217t \u2016) + k\u2211\nt=K\u2032+1\n\u221a\n2\u03b7t\u01ebt\n=\u2016\u03bb\u2217k\u2016+ 2 k\u22121\u2211\nt=K\u2032\n\u2016\u03bb\u2217t \u2016+ k\u2211\nt=K\u2032+1\n\u221a\n2\u03b7t\u01ebt,\nagain using the triangle inequality and that \u03bbK\u2032+1 = 0. The claim now follows by invoking the inverse triangle inequality on the LHS and rearranging."
        },
        {
            "heading": "E.3.3 Bounding the Dual Maximizers",
            "text": "We can now deduce Lemma 6 from the preliminaries shown in Appendix A.4 as follows. See Lemma 18 from the previous section for a formal introduction of the primal-dual pairs (\u03c0\u2217k, \u03bb \u2217 k).\nLemma 6. Suppose Assumption 1 holds. Let \u03bd \u2208 (0, 1) and choose K \u2032 as in Lemma 1. Let k \u2208 {K \u2032, . . . ,K}, and let (\u03c0\u2217k, \u03bb\u2217k) be a pair of primal-optimal and dual-optimal solutions for Eq. (6). Then, conditioned on G, we have\n\u2016\u03bb\u2217k\u2016 \u2264 \u2016\u03bb\u2217k\u20161 \u2264 H\n\u03bd\u03b3 .\nProof. Conditioned on G, the conclusion from Lemma 1 holds. That is, for every k \u2208 {K \u2032, . . . ,K} and every i \u2208 [I] we have\nV \u03c0\u0304(d\u0303i,k, p\u0303k) \u2264 \u03b1i \u2212 \u03bd\u03b3,\nor equivalently, mini\u2208[I](\u03b1i \u2212 V \u03c0\u0304(d\u0303i,k, p\u0303k)) \u2265 \u03bd\u03b3. Moreover, since the expected costs are in [0, 1] and the time horizon is H , the value functions are in [0, H ], so we have V \u03c0\u0304(c\u0303k, p\u0303k)\u2212V \u03c0 \u2217\nk(c\u0303k, p\u0303k) \u2264 H . Hence Assumption 3 holds, with \u03c00 = \u03c0\u0304 and \u03c3 = H\u03bd\u03b3 .\nAs discussed in Remark 3, under Assumption 3, we can apply the general bound on dual maximizers from Theorem 5. For this, we write Eq. (6) as a convex optimization problem in the occupancy measure (see Eq. (34)), with V \u03c0(c\u0303k, p\u0303k) = c\u0303 T k q \u03c0(p\u0303k) and V \u03c0(D\u0303k, p\u0303k) = D\u0303kq \u03c0(p\u0303k). Then, set X = Q(p\u0303k), x\u0304 = q \u03c0\u0304(p\u0303k), f(\u00b7) = c\u0303Tk (\u00b7) and gi(\u00b7) = d\u0303Ti,k(\u00b7) \u2212 \u03b1i as in Remark 3. Plugging this into Theorem 5 indeed yields the second of the claimed inequalities since we have shown that Assumption 3 holds with \u03c00 = \u03c0\u0304, \u03c3 = H\u03bd\u03b3 . The first inequality holds because \u03bb \u2217 k only has nonnegative entries."
        },
        {
            "heading": "E.3.4 Optimization Error Bound",
            "text": "We are now ready to prove the desired bound on the optimization errors.\nLemma 7 (Optimization errors). Suppose Assumption 1 holds. Let \u03bd \u2208 (0, 1) and choose K \u2032 as in Lemma 1. Suppose that the event G occurs. When using step sizes \u03b7K\u2032+k = \u0398(k\n2.5) and \u01ebK\u2032+k = \u0398(1/\u03b7K\u2032+k), we have\nK\u2211\nk=K\u2032+1\n[V \u03c0k(c\u0303k, p\u0303k)\u2212 V \u03c0 \u2217 (c, p)]+ \u2264 K\u2211\nk=K\u2032+1\n( (O(\u03c3k) + \u2211k t=K\u2032+1 \u221a 2\u03b7t\u01ebt) 2\n2\u03b7k + \u01ebk\n)\n\u2264 O( \u221a K),\nmax i\u2208[I]\nK\u2211\nk=K\u2032+1\n[V \u03c0k(d\u0303i,k, p\u0303k)\u2212 \u03b1i]+ \u2264 K\u2211\nk=K\u2032+1\nO(\u03c3k) + \u2211k\nt=K\u2032+1\n\u221a 2\u03b7t\u01ebk\n\u03b7k \u2264 O(\n\u221a K),\nwhere \u03c3 = H\u03bd\u03b3 and in fact O(\u03c3k) can be replaced by (2 + 2(k \u2212K \u2032))\u03c3.\nProof. Recall that conditioned on G, Lemma 4 shows (k \u2208 {K \u2032 + 1, . . . ,K})\n[V \u03c0k(c\u0303k, p\u0303k)\u2212 V \u03c0 \u2217 (c, p)]+ \u2264\u01ebk + \u2016\u03bbk\u20162 2\u03b7k , (36)\n[V \u03c0k(d\u0303i,k, p\u0303k)\u2212 \u03b1i]+ \u2264 \u2016\u03bbk+1\u2016\n\u03b7k , (37)\nwhen dropping the negative terms in Lemma 4. Moreover, from Lemma 5 and Lemma 6 we get\n\u2016\u03bbk+1\u2016 \u2264 2 k\u2211\nt=K\u2032\n\u2016\u03bb\u2217t \u2016+ k\u2211\nt=K\u2032+1\n\u221a\n2\u03b7t\u01ebt\n\u2264 2 k\u2211\nt=K\u2032\n\u03c3 +\nk\u2211\nt=K\u2032+1\n\u221a\n2\u03b7t\u01ebt\n=(2 + 2(k \u2212K \u2032))\u03c3 + k\u2211\nt=K\u2032+1\n\u221a\n2\u03b7t\u01ebt, (38)\nwhere \u03c3 = H\u03bd\u03b3 . Note that in the bound from Eq. (36), for k = K \u2032 + 1 we have \u2016\u03bbK\u2032+1\u20162 = 0 \u2264\n((2 + 2 \u00b7 1)\u03c3 + \u221a2\u03b7K\u2032+1\u01ebK\u2032+1)2 since \u03bbK\u2032+1 = 0. And for k \u2265 K \u2032 + 2, by Eq. (38) we have \u2016\u03bbk\u20162 \u2264 ((2+2((k\u22121)\u2212K \u2032))\u03c3+ \u2211k\u22121 t=K\u2032+1 \u221a 2\u03b7t\u01ebt) 2 \u2264 ((2+2(k\u2212K \u2032))\u03c3+\u2211kt=K\u2032+1 \u221a 2\u03b7t\u01ebt)\n2. Hence, the parameter-dependent bound on the objective errors follows by plugging this into Eq. (36) and summing up. For the constraint errors, we can directly plug in the bound from Eq. (38) into Eq. (37), thus obtaining the second parameter-dependent bound. From this, we now show how to obtain bounds of order O( \u221a K). Note that we can always choose \u01ebk := 1\n2\u03b7k so that \u2211k t=K\u2032+1 \u221a 2\u03b7t\u01ebt = k \u2212K \u2032 \u2264 \u03c3(k \u2212K \u2032), since \u03c3 = H\u03bd\u03b3 > 1. If we do so, then\nwe can loosely bound the term in Eq. (38) by\n(2 + 2(k \u2212K \u2032))\u03c3 + k\u2211\nt=K\u2032+1\n\u221a\n2\u03b7t\u01ebt \u2264 (2 + 3(k \u2212K \u2032))\u03c3.\nAs seen above, with this, Eq. (36) and Eq. (37) become\n[V \u03c0k(c\u0303k, p\u0303k)\u2212 V \u03c0 \u2217 (c, p)]+ \u2264 \u01ebk + ((2 + 3(k \u2212K \u2032))\u03c3)2\n2\u03b7k , (39)\n[V \u03c0k(d\u0303i,k, p\u0303k)\u2212 \u03b1i]+ \u2264 (2 + 3(k \u2212K \u2032))\u03c3\n\u03b7k . (40)\nWe can now choose12\n\u03b7K\u2032+k := ((2 + 3k)\u03c3) 2.5 (k > 0)\n12Note that if \u03b3 is not known, having an estimate of \u03c3 = H \u03bd\u03b3 that is larger than the true value is clearly\nsufficient.\nand sum each of the two errors above to bound the regret due to optimization. For the objective cost, we get (recall that we had set \u01ebk = 1/(2\u03b7k))\nK\u2211\nk=K\u2032+1\n[V \u03c0k(c\u0303k, p\u0303k)\u2212 V \u03c0 \u2217 (c, p)]+\nEq. (39) \u2264 K\u2211\nk=K\u2032+1\n\u01ebk + 1\n2\nK\u2211\nk=K\u2032+1\n((2 + 3(k \u2212K \u2032))\u03c3)2 ((2 + 3(k \u2212K \u2032))\u03c3)2.5\n\u2264 1 2\nK\u2211\nk=1\n1\n((2 + 3k)\u03c3)2.5 +\n1\n2\nK\u2211\nk=1\n1 \u221a\n(2 + 3k)\u03c3\n= O(K1/2),\nup to absolute constants, where the last step follows by considering the dominating series \u2211\nk\u22651 1/k 2 = \u03c02/6 and using \u2211K k=1 1/\n\u221a k = \u0398( \u221a K), as well as \u03c3 > 1. Similarly, for the\nconstraint cost, we get\nK\u2211\nk=K\u2032+1\n[V \u03c0k(d\u0303i,k, p\u0303k)\u2212 \u03b1i]+ \u2264 K\u2211\nk=K\u2032+1\n((2 + 3(k \u2212K \u2032))\u03c3) ((2 + 3k)\u03c3)2.5\n\u2264 K\u2211\nk=1\n1\n((2 + 3k)\u03c3)1.5\n=O(K1/2),\nup to absolute constants.\nWe remark that the choices for \u03b7k and \u01ebk are not necessarily optimal, and there is a trade-off between the regret due to the optimization error and the iteration complexity of the inner loop, as we discuss in Remark 2."
        }
    ],
    "title": "Cancellation-Free Regret Bounds for Lagrangian Approaches in Constrained Markov Decision Processes",
    "year": 2023
}