{
    "abstractText": "In this paper, we design a technologically intelligent wheelchair with eye-movement control for patients with ALS in a natural environment. The system consists of an electric wheelchair, a vision system, a two-dimensional robotic arm, and a main control system. The smart wheelchair obtains the eye image of the controller through a monocular camera and uses deep learning and an attention mechanism to calculate the eye-movement direction. In addition, starting from the relationship between the trajectory of the joystick and the wheelchair speed, we establish a motion acceleration model of the smart wheelchair, which reduces the sudden acceleration of the smart wheelchair during rapid motion and improves the smoothness of the motion of the smart wheelchair. The lightweight eye-movement recognition model is transplanted into an embedded AI controller. The test results show that the accuracy of eye-movement direction recognition is 98.49%, the wheelchair movement speed is up to 1 m/s, and the movement trajectory is smooth, without sudden changes.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jun Xu"
        },
        {
            "affiliations": [],
            "name": "Liangyuan Liu"
        },
        {
            "affiliations": [],
            "name": "Xinghua Li"
        },
        {
            "affiliations": [],
            "name": "Kai Wei"
        }
    ],
    "id": "SP:203a2eb1febf30fe492176f5b8f1fbc3eb184142",
    "references": [
        {
            "authors": [
                "J. Callupe Luna",
                "J. Martinez Rocha",
                "E. Monacelli",
                "G. Foggea",
                "Y. Hirata",
                "S. Delaplace"
            ],
            "title": "WISP, Wearable Inertial Sensor for Online Wheelchair Propulsion Detection",
            "venue": "Sensors 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Y. Li"
            ],
            "title": "Hand gesture recognition using kinect",
            "venue": "Proceedings of the International Conference on Software Engineering and Service Science, Beijing, China,",
            "year": 2012
        },
        {
            "authors": [
                "O.O. Adebayo",
                "E. Adetiba",
                "O.T. Ajayi"
            ],
            "title": "Hand Gesture Recognition-Based Control of Motorized Wheelchair using Electromyography Sensors and Recurrent Neural Network",
            "venue": "In Proceedings of the International Conference on Engineering for Sustainable World (ICESW 2020),",
            "year": 2020
        },
        {
            "authors": [
                "R.K. Nasare",
                "G.K. Yenurkar"
            ],
            "title": "Hand gesture based navigation control for automated wheelchair",
            "venue": "Int. J. Latest Trends Eng. Technol. 2016,",
            "year": 2016
        },
        {
            "authors": [
                "S. Ashley",
                "D. Jaydip"
            ],
            "title": "Hand Gesture-based Artificial Neural Network Trained Hybrid Human\u2013machine Interface System to Navigate a Powered Wheelchair",
            "venue": "J. Bionic Eng. 2021,",
            "year": 2021
        },
        {
            "authors": [
                "A.I. Iskanderani",
                "F.R. Tamim",
                "M.M. Rana",
                "W. Ahmed",
                "I.M. Mehedi",
                "A.J. Aljohani",
                "A. Latif",
                "S.A. Shaikh",
                "M. Shorfuzzaman",
                "F Akther"
            ],
            "title": "Voice Controlled Artificial Intelligent Smart Wheelchair",
            "venue": "In Proceedings of the 2020 8th International Conference on Intelligent and Advanced Systems (ICIAS), Kuching, Malaysia,",
            "year": 2021
        },
        {
            "authors": [
                "A. Jayakody",
                "A. Nawarathna",
                "I. Wijesinghe",
                "S. Liyanage",
                "J. Dissanayake"
            ],
            "title": "Smart Wheelchair to Facilitate Disabled Individuals",
            "venue": "In Proceedings of the 2019 International Conference on Advancements in Computing (ICAC), Malabe, Sri Lanka,",
            "year": 2019
        },
        {
            "authors": [
                "A.B. Karim",
                "A. Haq",
                "A. Noor",
                "B. Khan",
                "Z. Hussain"
            ],
            "title": "Raspberry Pi Based Voice Controlled Smart Wheelchair",
            "venue": "In Proceedings of the 2022 International Conference on Emerging Trends in Smart Technologies (ICETST), Karachi, Pakistan,",
            "year": 2022
        },
        {
            "authors": [
                "W. Cao",
                "H. Yu",
                "X. Wu",
                "S. Li",
                "Q. Meng",
                "C. Chen"
            ],
            "title": "Voice controlled wheelchair integration rehabilitation training and posture transformation for people with lower limb motor dysfunction",
            "venue": "Technol. Health Care Off. J. Eur. Soc. Eng. Med",
            "year": 2020
        },
        {
            "authors": [
                "Mokhles",
                "M. Abdulghani"
            ],
            "title": "Wheelchair Neuro Fuzzy Control and Tracking System",
            "venue": "Based on Voice Recognition. Sensors 2020,",
            "year": 2020
        },
        {
            "authors": [
                "D. Cojocaru",
                "L.F. Manta",
                "I.C. Vladu",
                "A. Dragomir",
                "A.M. Mariniuc"
            ],
            "title": "Using an Eye Gaze New Combined Approach to Control a Wheelchair Movement",
            "venue": "In Proceedings of the 2019 23rd International Conference on System Theory, Control and Computing (ICSTCC), Sinaia, Romania,",
            "year": 2019
        },
        {
            "authors": [
                "N. Wanluk",
                "S. Visitsattapongse",
                "A. Juhong",
                "C. Pintavirooj"
            ],
            "title": "Smart wheelchair based on eye tracking",
            "venue": "In Proceedings of the 2016 9th Biomedical Engineering International Conference (BMEiCON), Laung Prabang, Laos,",
            "year": 2016
        },
        {
            "authors": [
                "E. W\u00e4stlund",
                "K. Sponseller",
                "O. Pettersson",
                "A. Bared"
            ],
            "title": "Evaluating gaze-driven power wheelchair with navigation support for persons with disabilities",
            "venue": "J. Rehabil. Res. Dev",
            "year": 2015
        },
        {
            "authors": [
                "D. Bai",
                "Z. Liu",
                "Q. Hu",
                "J. Yang",
                "G. Yang",
                "C. Ni",
                "D. Yang",
                "L. Zhou"
            ],
            "title": "Design of an eye movement-controlled wheelchair using Kalman filter algorithm",
            "venue": "In Proceedings of the 2016 IEEE International Conference on Information and Automation (ICIA), Ningbo, China,",
            "year": 2016
        },
        {
            "authors": [
                "G. Gautam",
                "G. Sumanth",
                "K.C. Karthikeyan",
                "S. Sundar",
                "D. Venkataraman"
            ],
            "title": "Eye movement based electronic wheel chair for physically challenged persons",
            "venue": "Int. J. Sci. Technol. Res",
            "year": 2014
        },
        {
            "authors": [
                "E. Antoniou",
                "P. Bozios",
                "V. Christou",
                "K.D. Tzimourta",
                "K. Kalafatakis",
                "M.G. Tsipouras",
                "N. Giannakeas",
                "A.T. Tzallas"
            ],
            "title": "EEG-Based Eye Movement Recognition Using Brain\u2013Computer Interface and Random Forests",
            "venue": "Sensors 2021,",
            "year": 2021
        },
        {
            "authors": [
                "E.V.K. Rao",
                "N.Y. Reddy",
                "B.V.S.S. Greeshma",
                "Y.S.S.V. Reddy"
            ],
            "title": "EEG Based Smart Wheelchair For Disabled Persons Using NonInvasive BCI",
            "venue": "In Proceedings of the 2022 International Conference on Computational Intelligence and Sustainable Engineering Solutions (CISES), Greater Noida, India,",
            "year": 2022
        },
        {
            "authors": [
                "B.V. Ngo",
                "T.H. Nguyen",
                "D.K. Tran",
                "D.D. Vo"
            ],
            "title": "Control of a Smart Electric Wheelchair Based on EEG Signal and Graphical User Interface for Disabled People",
            "venue": "In Proceedings of the 2021 International Conference on System Science and Engineering (ICSSE), Ho Chi Minh City, Vietnam,",
            "year": 2021
        },
        {
            "authors": [
                "K. Joshi",
                "P. Soni",
                "S. Joshi",
                "A. Vyas",
                "R. Joshi"
            ],
            "title": "Cognitive-Chair: AI based advanced Brain Sensing Wheelchair for Paraplegic/Quadriplegic people",
            "venue": "In Proceedings of the 2022 4th International Conference on Artificial Intelligence and Speech Technology (AIST), Delhi, India,",
            "year": 2022
        },
        {
            "authors": [
                "B.A. Fadheel",
                "A.J. Mahdi",
                "H.F. Jaafar",
                "M.S. Nazir",
                "M.S. Obaid",
                "S.H. Musa"
            ],
            "title": "Speed Control of a Wheelchair Prototype Driven by a DC Motor Through Real EEG Brain Signals",
            "venue": "In Proceedings of the 3rd International Conference on Engineering Sciences, Kerbala, Iraq,",
            "year": 2019
        },
        {
            "authors": [
                "F. Ferracuti",
                "A. Freddi",
                "S. Iarlori",
                "S. Longhi",
                "A. Monteri\u00f9",
                "C. Porcaro"
            ],
            "title": "Augmenting robot intelligence via EEG signals to avoid trajectory planning mistakes of a smart wheelchair",
            "venue": "J. Ambient. Intell. Humaniz. Comput",
            "year": 2021
        },
        {
            "authors": [
                "P. Griss",
                "P. Enoksson",
                "H.K. Tolvanen-Laakso",
                "P. Merilainen",
                "S. Ollmar",
                "G. Stemme"
            ],
            "title": "Micromachined electrodes for biopotential measurements",
            "venue": "J. Microelectromechanical Syst",
            "year": 2001
        },
        {
            "authors": [
                "X. Li",
                "W.G. Wee"
            ],
            "title": "An efficient method for eye tracking and eye-gazed FOV estimation",
            "venue": "In Proceedings of the 2009 16th IEEE International Conference on Image Processing (ICIP), Cairo, Egypt,",
            "year": 2009
        },
        {
            "authors": [
                "F.B. Taher",
                "N.B. Amor",
                "M. Jallouli"
            ],
            "title": "A multimodal wheelchair control system based on EEG signals and Eye tracking fusion",
            "venue": "In Proceedings of the 2015 International Symposium on Innovations in Intelligent SysTems and Applications (INISTA),",
            "year": 2015
        },
        {
            "authors": [
                "D. \u00c7etinta\u015f",
                "T.T. Firat"
            ],
            "title": "Eye-Tracking Analysis with Deep Learning Method",
            "venue": "In Proceedings of the 2021 International Conference on Innovation and Intelligence for Informatics, Computing, and Technologies (3ICT), Zallaq, Bahrain,",
            "year": 2021
        },
        {
            "authors": [
                "L. Hu",
                "J. Gao"
            ],
            "title": "Research on real-time distance measurement of mobile eye tracking system based on neural network",
            "venue": "In Proceedings of the 2020 IEEE 5th Information Technology and Mechatronics Engineering Conference (ITOEC), Chongqing, China,",
            "year": 2020
        },
        {
            "authors": [
                "D. Dragusin",
                "M.I. Baritz"
            ],
            "title": "Development of a System for Correlating Ocular Biosignals to Achieve the Movement of a Wheelchair",
            "venue": "In Proceedings of the 2020 International Conference on e-Health and Bioengineering (EHB), Iasi, Romania,",
            "year": 2020
        },
        {
            "authors": [
                "A. Juhong",
                "T. Treebupachatsakul",
                "C. Pintavirooj"
            ],
            "title": "Smart eye-tracking system",
            "venue": "In Proceedings of the 2018 International Workshop on Advanced Image Technology (IWAIT), Chiang Mai, Thailand,",
            "year": 2018
        },
        {
            "authors": [
                "S. Higa",
                "K. Yamada",
                "S. Kamisato"
            ],
            "title": "Intelligent Eye-Controlled Electric Wheelchair Based on Estimating Visual Intentions Using One-Dimensional Convolutional Neural Network and Long Short-Term Memory",
            "venue": "Sensors 2023,",
            "year": 2023
        },
        {
            "authors": [
                "W. Fuhl",
                "G. Kasneci",
                "E. Kasneci"
            ],
            "title": "TEyeD: Over 20 Million Real-World Eye Images with Pupil, Eyelid, and Iris 2D and 3D Segmentations, 2D and 3D Landmarks, 3D Eyeball, Gaze Vector, and Eye Movement Types",
            "venue": "In Proceedings of the 2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR), Bari, Italy,",
            "year": 2021
        },
        {
            "authors": [
                "R. Fusek",
                "E. Iris Center Sojka"
            ],
            "title": "Localization Using Geodesic Distance and CNN",
            "venue": "In Proceedings of the 9th Iberian Conference on Pattern Recognition and Image Analysis, IbPRIA 2019,",
            "year": 2019
        },
        {
            "authors": [
                "P.A. Constable",
                "M. Bach",
                "L.J. Frishman",
                "B.G. Jeffrey",
                "A.G. Robson"
            ],
            "title": "ISCEV Standardbfor Clinical Electro-oculography (EOG) 2019",
            "venue": "Doc. Ophthalmol",
            "year": 2019
        },
        {
            "authors": [
                "D.A. Robinson"
            ],
            "title": "A Method of Measuring Eye Movement Using a Scleral Search Coil in a Magnetic Field",
            "venue": "IEEE Trans. Bio-Med. Electron. 2021,",
            "year": 2021
        },
        {
            "authors": [
                "T. Ohno",
                "N. Mukawa",
                "A. Yoshikawa"
            ],
            "title": "Abstract FreeGaze: A Gaze Tracking System for Everyday Gaze Interaction",
            "venue": "In Proceedings of the Eye Tracking Research & Application Symposium, DBLP 2018,",
            "year": 2018
        },
        {
            "authors": [
                "F.L. Coutinho",
                "C.H. Morimoto"
            ],
            "title": "Augmenting the robustness of cross-ratio gaze tracking methods to head movement",
            "venue": "In Symposium on Eye Tracking Research ",
            "year": 2019
        },
        {
            "authors": [
                "J.J. Cerrolaza",
                "A. Villanueva",
                "R. Cabeza"
            ],
            "title": "Taxonomic Study of Polynomial Regressions Applied to the Calibration of VideoOculographic Systems",
            "venue": "In Proceedings of the 2008 Symposium on Eye Tracking Research & Applications (ETRA",
            "year": 2008
        },
        {
            "authors": [
                "C. Hennessey",
                "P. Lawrence"
            ],
            "title": "Improving the accurancy and reliability of remote systemcalibration-free eye-gaze tracking",
            "venue": "IEEE Trans Biomed. Eng",
            "year": 2009
        },
        {
            "authors": [
                "L.Q. Xu",
                "D. Machin",
                "P. Sheppard"
            ],
            "title": "A Novel Approach to Real-Time Non-Intrusive Gaze Finding",
            "venue": "In Proceedings of the BMVC, London, UK,",
            "year": 2017
        },
        {
            "authors": [
                "K. Tan",
                "D.J.K.N. Ahuja"
            ],
            "title": "Appearance-based Eye Gaze Estimation",
            "venue": "In Proceedings of the IEEE Workshop on Applications of Computer Vision, Nashville, TN, USA,",
            "year": 2021
        },
        {
            "authors": [
                "C. Jeong",
                "T. Kim"
            ],
            "title": "Eye Blink Detection Using Algorithm Based On dlib And OpenCV Library for Game Players",
            "venue": "In Competitive Environments. J. Int. Res. Med. Pharm. Sci. 2021,",
            "year": 2021
        },
        {
            "authors": [
                "G. Robert",
                "C. Lucas",
                "L. Nicholas",
                "M.S. Jedediah",
                "Z. Alexander"
            ],
            "title": "New guidance for using t-SNE: Alternative defaults, hyperparameter selection automation, and comparative evaluation",
            "venue": "Vis. Inform. 2022,",
            "year": 2022
        },
        {
            "authors": [
                "G.S. Sunsuhi",
                "S. Albin Jose"
            ],
            "title": "An Adaptive Eroded Deep Convolutional neural network for brain image segmentation and classification using Inception ResnetV2",
            "venue": "Biomed. Signal Process. Control",
            "year": 2022
        },
        {
            "authors": [
                "J. Zhang",
                "Y. Zhu",
                "W. Li",
                "W. Fu",
                "L. Cao"
            ],
            "title": "DRNet: A Deep Neural Network with Multi-Layer Residual Blocks Improves Image Denoising",
            "venue": "IEEE Access 2021,",
            "year": 2021
        },
        {
            "authors": [
                "B. Wang",
                "X. Yan",
                "D. Li"
            ],
            "title": "An End-to-End Lane Detection Model with Attention and Residual Block",
            "venue": "Comput. Intell. Neurosci",
            "year": 2022
        },
        {
            "authors": [
                "R. Ma",
                "J. Wang",
                "W. Zhao",
                "H. Guo",
                "D. Dai",
                "Y. Yun",
                "L. Li",
                "F. Hao",
                "J. Bai",
                "D. Ma"
            ],
            "title": "Identification of Maize Seed Varieties Using MobileNetV2 with Improved Attention Mechanism CBAM",
            "venue": "Agriculture 2022,",
            "year": 2022
        },
        {
            "authors": [
                "T. Zhang",
                "Y. Sui",
                "S. Wu",
                "F. Shao",
                "R. Sun"
            ],
            "title": "Table Structure Recognition Method Based on Lightweight Network and Channel Attention",
            "venue": "Electronics 2023,",
            "year": 2023
        },
        {
            "authors": [
                "Y. Cao"
            ],
            "title": "An Expression Recognition Model Based on Channel and Spatial Attention Fusion",
            "venue": "J. Phys. Conf. Ser",
            "year": 2022
        },
        {
            "authors": [
                "J. Imen",
                "A. Ihsen",
                "B.K. Anouar",
                "M.M. Ali"
            ],
            "title": "Deep learning-based hard spatial attention for driver in-vehicle action monitoring",
            "venue": "Expert Syst. Appl. 2023,",
            "year": 2023
        },
        {
            "authors": [
                "L. Li",
                "M. Doroslovacki",
                "M.H. Loew"
            ],
            "title": "Approximating the Gradient of Cross-Entropy Loss Function",
            "venue": "IEEE Access 2020,",
            "year": 2020
        },
        {
            "authors": [
                "A. Huber Holly",
                "K. Georgia Senta",
                "D. Finley Stacey"
            ],
            "title": "Systematic Bayesian Posterior Analysis Guided by Kullback-Leibler Divergence Facilitates Hypothesis Formation",
            "venue": "J. Theor. Biol",
            "year": 2022
        },
        {
            "authors": [
                "K. Akihiro",
                "N. Kazu",
                "H. Rin",
                "K. Hideaki",
                "N. Yoshihisa"
            ],
            "title": "Eye fatigue estimation using blink detection based on Eye Aspect Ratio Mapping (EARM)",
            "venue": "Cogn. Robot. 2022,",
            "year": 2022
        },
        {
            "authors": [
                "L. Yovan",
                "P. Kumar",
                "P. Singh",
                "J. Chand",
                "U.H. Narayana",
                "S.B.D. Sarojam",
                "B. Keshava"
            ],
            "title": "Submarine Groundwater Discharge (SGD): Impacts, challenges, limitations, and management recommendations",
            "venue": "Groundw. Sustain. Dev. 2023,",
            "year": 2023
        },
        {
            "authors": [
                "S. Ren",
                "H. Wu",
                "W. Chen",
                "D. Li"
            ],
            "title": "Polarization Domain Spectrum Sensing Algorithm Based on AlexNet",
            "venue": "Sensors 2022,",
            "year": 2022
        },
        {
            "authors": [
                "B. Huang",
                "J. Liu",
                "Q. Zhang",
                "K. Liu",
                "K. Li",
                "X. Liao"
            ],
            "title": "Identification and Classification of Aluminum",
            "venue": "Scrap Grades Based on the Resnet18 Model. Appl. Sci",
            "year": 2022
        },
        {
            "authors": [
                "H. Wang"
            ],
            "title": "Residual Mask Based on MobileNet-V2 for Driver\u2019s Dangerous Behavior Recognition",
            "venue": "In Proceedings of the 2019 3rd International Conference on Computer Science and Artificial Intelligence, Normal, IL, USA,",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "Citation: Xu, J.; Huang, Z.; Liu, L.; Li,\nX.; Wei, K. Eye-Gaze Controlled\nWheelchair Based on Deep Learning.\nSensors 2023, 23, 6239. https://\ndoi.org/10.3390/s23136239\nAcademic Editor: Antonio\nFern\u00e1ndez-Caballero\nReceived: 31 May 2023\nRevised: 22 June 2023\nAccepted: 30 June 2023\nPublished: 7 July 2023\nCopyright: \u00a9 2023 by the authors.\nLicensee MDPI, Basel, Switzerland.\nThis article is an open access article\ndistributed under the terms and\nconditions of the Creative Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n4.0/).\nKeywords: eye-tracking; deep learning; CBAM attention; wheelchair acceleration model"
        },
        {
            "heading": "1. Introduction",
            "text": "ALS is a progressive and fatal neurodegenerative disease that causes the degeneration of the patient\u2019s upper and lower motor neurons, thereby weakening the muscles. Therefore, although many ALS patients are conscious, they cannot perform physical movements and verbal expressions. A growing body of research is dedicated to the application of artificial intelligence technologies to modify power wheelchairs to improve the quality of life of people with ALS. In the past few decades, researchers have carried out corresponding research on wheelchair motion control methods, including gesture control [1\u20135], voice control [6\u201310], eye-tracking control [11\u201315], and brain\u2013computer interfaces [16\u201322]. These control methods can replace the rocker to complete the reading of the user\u2019s motion direction intention and realize the motion control of the wheelchair. However, due to the loss of limb control and language communication abilities in patients with amyotrophic lateral sclerosis (ALS), gesture control and voice control are not viable options. In brain\u2013computer interfaces, although the collection of brain signals largely eliminates noise interference, the semi-invasive or invasive electrodes used in these interfaces can pose risks to human health [23]. Compared with the above-mentioned control methods, eye-tracking control has unique advantages in terms of safety, portability, and practicality for patients with ALS. Currently, research on the eye-tracking control of wheelchairs mainly focuses on two aspects: eye-tracking recognition and wheelchair control. Xiaokun Li et al. designed a head-mounted device based on an energy-controlled iterative curve-fitting method of infrared light, which can achieve precise pupil detection and tracking. The experimental results showed that the average tracking accuracy of the method for pupil rotation was at least 1.38% higher than that of conventional methods [24]. Fatma et al. proposed a method that uses a front camera to capture the user\u2019s face information and determines the pupil center coordinates through a fuzzy logic controller to output wheelchair control decisions.\nSensors 2023, 23, 6239. https://doi.org/10.3390/s23136239 https://www.mdpi.com/journal/sensors\nSensors 2023, 23, 6239 2 of 25\nThis method features an adaptive threshold adjustment, enabling eye-tracking even under poor or unstable lighting conditions [25]. In terms of eye-tracking recognition, although the eye-tracker equipment has a high accuracy rate, the infrared light it generates can cause irreversible damage to the human eye, and it is far inferior to the camera recognition solution in terms of comfort and economic practicability. In recent years, deep learning technology has been widely used in the fields of nonverbal communication, human\u2013computer interaction, and intention prediction. Dliber et al. used the AlexNet network structure for eye-movement direction determination and trained the network model using a private dataset. The visualization results showed that the recognition accuracy of the model reached 97.88% [26]. Moayad Mokatren et al. designed a faster region-based convolutional neural network (RCNN) to detect pupils in infrared and RGB images. The experimental results showed that the method had a 3D gaze error of only 2.12\u25e6 and could be used with high accuracy for various real-world moving scenes [27]. Ling Hu et al. proposed a model called SSD to detect eyeballs in images using a single deep neural network. The total average accuracy of target detection for this scheme was 91.85%, and the total average relative error after distortion removal was reduced to 1.25% [28]. The above-mentioned research based on deep learning provides a new direction for the development of eye-tracking technology. For wheelchair control, Delia Dragusin et al. used a pupil-corneal reflex-based approach to accomplish sight estimation on a PC, and the wheelchair control chip received eye-movement commands from a PC via Bluetooth and controlled relay closures to drive motor motion [29]. Razvan Solea et al. used the PCI protocol to send the eye-tracking signal from a PC via a data acquisition card to a servo amplifier, which, in turn, sent a PWM control signal to the DC motor [30]. Aniwat Juhong et al. used x-axis\u2013y-axis servos to control the wheelchair rocker so that the wheelchair could theoretically have a 360\u25e6 movement direction, which greatly improved the maneuverability of the wheelchair [31]. Sho Higa et al. bypassed the joystick and directly sent the eye-tracking signal output via an embedded AI computing device to the control system of a WHILL CR electric wheelchair through a USB to complete the control of the wheelchair movement. Although this control method is simple and reliable, the electric wheelchair it is paired with is very expensive [32]. Deep learning-based eye-tracking methods heavily rely on the completeness and richness of the training data. For the public eye-gaze dataset, Wolfgang et al. compiled a dataset (TEyeD) from more than 20 million real-world eye images and pupil information, and the data were carefully labeled [33]. The MRL (Media Research Lab) built a large-scale dataset of human eye images with 15,000 pupil points (images) using different devices to capture drivers\u2019 facial images under realistic conditions [34]. However, the existing dataset suffers from a lack of labeled samples, physical constraints on the recording process, and poor applicability to specific scenarios. To sum up, to develop an intelligent eye-tracking wheelchair with low economic and operational barriers, here, we use a human eye-gaze model that combines convolutional neural networks and attention mechanisms in the eye-tracking recognition scheme. To further improve the generalizability and accuracy of the model, a large-range, multidimensional eye-tracking dataset is established. For wheelchair control, a structure that controls the movement of the wheelchair by manipulating the rocker with a tiller is developed to achieve precise control of the wheelchair. The intelligent eye-tracking wheelchair based on deep learning designed in this paper is an example of the application of artificial intelligence in medical care, elderly care, and other fields, and can serve as a reference for the use of emerging technologies to improve the quality of human life. The rest of the paper is organized as follows. Section 2 presents a review of the related works. Section 3 outlines the intelligent eye-tracking wheelchair scheme developed in this paper and describes the acquisition process of the multidimensional eye-tracking dataset. Section 4 introduces the eye-tracking model-building and training methods. Section 5 describes the design of the eye-tracking wheelchair control system. Section 6 discusses\nSensors 2023, 23, 6239 3 of 25\nthe experiments and analyzes the results. Section 7 summarizes the article and suggests directions for future work."
        },
        {
            "heading": "2. Related Work",
            "text": "Eye-tracking methods can be divided into invasive and non-invasive methods. Invasive methods mainly include EEG (electroencephalogram) and EOG (electrooculogram) methods, whereas non-invasive methods can be further divided into appearance-based and model-based methods. Intrusive methods use specific intrusive sensors to detect changes in the voltage, current, and magnetic field of eye movements to determine the direction of sight. The EOG method is an objective and quantitative retinal function detection method for detecting the electrostatic potential of the eye, which slowly changes with adaptation to light. To detect the horizontal movement of the eyeball, a pair of electrodes are placed on the inner and outer corners of the eye, respectively. To detect vertical movement, they are placed on the upper line of the eyelid, and the potential difference between the two is recorded. There is no potential difference when the eyeball is facing straight ahead, but when the eyeball is rotated, the electrode on the corneal side is more positive than the other electrodes [35,36]. The contact lens method involves implanting a contact lens consisting of reflective lenses into the human eye. The principle is to measure the reflection of different light rays by the reflective lens after different light rays enter the human eye, obtain the position of the center of the pupil by calculating the angle of the reflected light rays, and then judge the direction of sight. This method of directly embedding measuring equipment in the pupil is insensitive to changes in the external environment and has a high measurement accuracy; however, it is expensive, requires special equipment, and causes serious interference to users [37]. The above two methods determine the direction of the line-of-sight through a contact sensor device, although they are highly robust to changes in the external environment, such as lighting and head movement. However, in the process of use, the user needs to wear a special eye-contact device with large interference, which can have a significant impact on the comfort of the user and limit the degree of freedom of the user. On the non-invasive side, many studies have applied model-based principles. Morimoto et al. proposed a sight estimation technique based on the pupil vector reflection method, which uses polynomial equations to describe the mapping relationship between the pupil corneal vector and the sight fall point. This method has high accuracy, but its head must remain absolutely still to interfere with the human degree of freedom [38]. Cerrolaza et al. used a camera and two infrared light-emitting tubes to calculate the relative distance between two spots formed by the corneal reflected infrared light, and then to calculate the mapping relationship between the pupil corneal reflection vector and the gaze point, and this method can solve the error caused by the small displacement of the head to some extent [39]. Hennessey et al. calculated the quadrilateral formed by four infrared reflected spots with a similar calibration. However, such methods require the passage of infrared light, and prolonged exposure to near-infrared light may cause damage to the eyes [40]. Appearance-based eye-tracking methods use image data as input and do not need to build 2D or 3D eye models to directly map image data to gaze points. In addition, the appearance-based approach [41,42] relies on the completeness and richness of the training data, i.e., the training data should contain all the situations present in the application scene, otherwise the approach cannot complete the mapping of image data to the gaze point. In the current application, a large number of image datasets are mainly used as the input to the neural network for training to achieve the mapping of image data to gaze points. The appearance-based method does not perform complicated operations such as camera and geometric scene calibration, which makes it less difficult to use and facilitates practical applications, without a complicated calibration process in the application. Therefore, we designed a neural network model combining the convolutional neural network and attention mechanism for line-of-sight direction estimation from the aspects of economy, safety, practicality, and comfort of use.\nSensors 2023, 23, 6239 4 of 25"
        },
        {
            "heading": "3. Materials and Methods",
            "text": ""
        },
        {
            "heading": "3.1. Eye-Tracking Wheelchair Program",
            "text": "The eye-tracking wheelchair system consists of eye-tracking data acquisition, data preprocessing, eye-tracking direction estimation, and wheelchair motion control. The camera in front of the wheelchair transmits the face images collected in real time to the embedded AI computing module. The latter inputs the processed image into the neural network model to obtain the estimated direction of eye-tracking and transmits the signal to the Arduino; then, the Arduino controls the 2D servo to adjust the wheelchair rocker and change the motion state of the wheelchair. The user\u2019s eye state and the estimated direction of eye-tracking are updated in real time on the display. The general block diagram of the eye-tracking wheelchair system is shown in Figure 1.\nSensors 2023, 23, x FOR PEER REVIEW 4 of 26\nTherefore, we designed a neural network model combining the convolutional neural\nnetwork and attention mechanism for line-of-sight direction estimation from the aspects\nof economy, safety, practicality, and comfort of use.\n3. Materials and Methods\n3.1. Eye-Tracking Wheelchair Program\nThe eye-tracking wheelchair system consists of eye-tracking data acquisition, data\npreprocessing, eye-tracking direction estimation, and wheelchair motion control. The\ncamera in front of the heelchair transmits the face images collected in real time to the\nembedded AI computing module. The latter inputs the processed image into the neural\nnetwork model to obtain the estimated direction of eye-tracking and transmits the signal\nto the Arduino; then, the Arduino controls the 2D servo to adjust the wheelchair rocker\nand change the motion state of the wheelchair. The user\u2019s eye state and the estimated di-\nrection of eye-tracking are updated in real time on the display. The general block diagram\nof the eye-tracking wheelchair syste is shown in Figure 1.\nFigure 1. General block diagram of intelligent eye-tracking wheelchair system.\n3.2. Dataset Creation\nDatasets are the basis for training deep learning models, and the performance of deep\nlearning models heavily depends on the quality and size of the datasets they are trained\non. In order to further improve the practicability and accuracy of the model, 100 Chinese\nvolunteers were recruited for this dataset collection. Through the two major scenes of vir-\ntual and reality, we recorded videos of volunteers gazing in different directions while\ncompleting tasks, extracted human eye images frame-by-frame using the OpenCv pro-\ngram and the Dlib algorithm, and automatically labeled the obtained data by the task at-\ntributes of the time period in which the frame was located.\n3.2.1. Multidimensional Eye-Tracking Data Acquisition\nIn this paper, we built the eye-tracking dataset through two dimensions: virtual and\nreality. Multidimensional datasets can capture the complex relationships between\nFigure 1. General bl ra of intelligent eye-tracking wheelchair system."
        },
        {
            "heading": "3.2. Dataset Creation",
            "text": "Datasets are the basis for training deep learning models, and the performance of deep learning models heavily depends on the quality and size of the datasets they are trained on. In order to further improve the practicability and accuracy of the model, 100 Chinese volunteers were recruited f r this dataset collection. Thro gh the two ajor scenes of virtual and r ality, we r corded videos of volunteers gazing in diff rent directions while completing tasks, extracted human eye images fram -by-frame using th OpenCv program and the Dlib algorithm, and auto atically labeled the obtained data by the task attributes of the time period in which the frame was located.\n3.2.1. Multidimensional Eye-Tracking Data Acquisition\nIn this paper, we built the eye-tracking dataset through two dimensions: virtual and reality. Multidimensional datasets can capture the complex relationships between different features, better descri e the characteristics and ttributes of the data, provide a more comprehensive and accurate representation of the data, and enhance model performance.\nSensors 2023, 23, 6239 5 of 25\n(1) Virtual scene acquisition\nIn this paper, a virtual scene for eye-tracking direction detection was built under the robot operating system ROS using Gazebo software, as shown in Figure 2. The virtual scene is a maze, and there is a car in the maze, which matches the virtual camera so that the perspective of the volunteer is consistent with that of the car. The volunteers keep their heads still while watching the road in front of the car with their eyes and control the movement of the car through the keyboard, simulating the movement of a wheelchair in a real-life scenario. When the state of the car is shown (as in Figure 2), volunteers look at the feasible road on the right side of the wall, and at the same time control the car to the right through the keyboard. The volunteer\u2019s facial image is captured and recorded by a camera placed in the center of the computer screen, while the volunteer\u2019s actions on the keyboard are recorded by a script to achieve hand\u2013eye synergy data collection. Through this method, the eye-tracking dataset in the virtual scene was established.\nSensors 2023, 23, x FOR PEER REVIEW 5 of 26\ndifferent features, better describe the characteristics and attributes of the data, provide a more comprehensive and accurate representation of the data, and enhance model performance. (1) Virtual scene acquisition\nIn this paper, a virtual scene for eye-tracking direction detection was built under the robot operating system ROS using Gazebo software, as shown in Figure 2. The virtual scene is a maze, and there is a car in the maze, which matches the virtual camera so that the perspective of the volunteer is consistent with that of the car. The volunteers keep their heads still while watching the road in front of the car with their eyes and control the movement of the car through the keyboard, simulating the movement of a wheelchair in a reallife scenario. When the state of the car is shown (as in Figure 2), volunteers look at the feasible road on the right side of the wall, and at the same time control the car to the right through the keyboard. The volunteer\u2019s facial image is captured and recorded by a camera placed in the center of the co puter screen, hile the volunteer\u2019s actions on the keyboard are recorded by a script to achieve hand\u2013eye synergy data collection. Through this method, the eye-tracking dataset in the virtual scene was established.\n(2) Real scene acquisition We set up an environment for eye-tracking data acquisition in a real scene, as shown in Figure 3b. The scene consisted of a wall with a nine-grid (as shown in Figure 3a), a laser pointer, and a wheelchair with a fixed camera. The nine-grid was a square area of 210 cm \u00d7 210 cm in size, each grid size was 70 cm \u00d7 70 cm, and a red sign was pasted in the center. When the volunteer\u2019s eye gazes at the designated red sign, the direction of the human eye gaze becomes a nine-classification problem, which reduces the influence of the volunteer\u2019s subjective behavior. In the real scene, the experimental assistant pointed to the red mark in the center of the grid with a laser pointer row-by-row from left to right, while the volunteer sat in a wheelchair and kept his head still, staring at the positions illuminated by the experimental assistant, and each position was maintained for 10 s. At the same time, the facial changes of the volunteers were recorded in real time by the camera on the wheelchair. Each frame of facial data was automatically marked according to the time the frame belonged to.\nFig re 2. irt al eye-tracki g ata acquisition.\n(2)\ne set a e iron ent for eye-trac i ata ac isiti i a real sce e, as s i i . The scene con isted of a wall with a nine-grid (as shown in Figure 3a), a laser pointer, and a w eelchair with a fixed camera. The nine-grid was a squ re area of 210 cm \u00d7 210 cm in size, each grid size was 70 cm \u00d7 70 cm, nd a red sign was pasted in the c nter. When the volunteer\u2019s ye gazes at the designated red sign, the t e an e e es i - l i c ti roble , hich reduces the i ence of t e volunteer\u2019s s jective e avior. In t e real sc e, t ri e tal assistant ointed to t e re ark i t e ce ter f t ri it l i t r - -ro fro left t ri t, ile t e volunte r sat in a wheelchair and kept his head still, staring at the positions llum nated by the experimental assistant, and each position was maintained for 10 s. At the same time, the facial changes of the volunteers w re recorded in r al tim by th camera on the wheelchair. Each frame of f cial dat was utomatically marked according to the time t e frame belonged to.\nSensors 2023, 23, 6239 6 of 25Sensors 2023, 23, x FOR PEER REVIEW 6 of 26\n(a) (b)\nFigure 3. Real eye-tracking data acquisition: (a) nine-grid and (b) real scene.\n3.2.2. Data Preprocessing\nTo ensure the accuracy of convolutional neural network-based algorithms, images\nneed to be preprocessed before they are analyzed. The quality of the image has a direct\nimpact on the accuracy of the algorithm. Different tasks require different image prepro-\ncessing methods to remove irrelevant information and enhance relevant information to\nimprove task reliability. In this paper, the convolutional neural network was used to de-\ntect the human eye-tracking state, so redundant information other than human eyes\nneeded to be removed in the preprocessing stage.\nWe chose to use the detector function of the Dlib library [43] to draw 68 feature points\nof the recognized face, used OpenCv to extract frames from the video, and used the face-\ndetection algorithm of the Dlib library for each frame of the image. In order to effectively\nextract the irrelevant area information of the bridge of the nose in the two eyes, the images\nof the left and the right eyes were extracted, respectively, instead of directly extracting the\nimages of both eyes. The comparison between the two is shown in Figure 4.\nFigure 4. Collecting Dlib key points to extract human eye pictures.\nThe area delineated by the feature points of serial numbers 42\u201347 is the correspond-\ning area of the left eye, and the feature points of the corresponding area of the right eye\nare serial numbers 36\u201341. Taking the extraction of the left-eye image as an example, the\nminimum and maximum values of the abscissa and ordinate coordinates of the six points\nare the boundary area coordinates of the left-eye image. The following formula was used\nto obtain the boundary coordinates of the eye image. Among them, \ud835\udc65\ud835\udc56 , \ud835\udc66\ud835\udc56 are the corresponding abscissa and ordinate of each feature point, \ud835\udc4e, \ud835\udc4f are the minimum abscissa and\nordinate of the eye area, and \ud835\udc34, \ud835\udc35 are the maximum abscissa and ordinate of the eye area.\nFigure 3. Real eye-tracking data acquisition: (a) nine-grid and (b) real scene.\n3.2.2. Data Preprocessing\nTo ensure the accuracy of convolutional neural network-based algorithms, images need to be preprocessed before they are analyzed. The quality of the image has a direct impact on the accuracy of the algorithm. Different tasks require different image preprocessing methods to remove irrelevant information and enhance relevant information to improve task reliability. In this paper, the convolutional neural network was used to detect the human eye-tracking state, so redundant information other than human eyes needed to be removed in the preprocessing stage. We chose to use the detector function of the Dlib library [43] to draw 68 feature points of the recognized face, used OpenCv to extract frames from the video, and used the facedetection algorithm of the Dlib library for each frame of the image. In order to effectively extract the irrelevant area information of the bridge of the nose in the two eyes, the images of the left and the right eyes were extracted, respectively, instead of directly extracting the images of both eyes. The comparison between the two is shown in Figure 4.\nSensors 2023, 23, x FOR PEER REVIEW 6 of 26\n(a) (b)\nFigure 3. Real eye-tracking data acquisition: (a) nine-grid and (b) real scene.\n3.2.2. Data Preprocessing"
        },
        {
            "heading": "To ensure the accuracy of convolutional neural network-based algorithms, images",
            "text": "need to b preprocessed before they are analyzed. The quality of the image has direct impact on the accuracy f the algorithm. Different tasks requir differ nt image preprocessing methods to remove irrelevant information and enhance levant info mation to improve task r liability. In this paper, the convolutional neural network was used to detect the human eye-tracking state, s red ndant information other than human eyes needed to be removed in the preprocessing stage. We chose to use the detector function of the Dlib library [43] to draw 68 feature points of the recognized face, used OpenCv to extract frames from the video, and used the facedetection algorithm of the Dlib library for each frame of the image. In order to effectively extract the irrelevant area information of the bridge of the nose in the two eyes, the images of the left and the right eyes were extracted, respectively, instead of directly extracting the images of both eyes. The comparison between the two is shown in Figure 4.\nFigure 4. Collecting Dlib key points to extract human eye pictures.\nThe area delineated by the feature points of serial numbers 42\u201347 is the corresponding area of the left eye, and the feature points of the corresponding area of the right eye are serial numbers 36\u201341. Taking the extraction of the left-eye image as an example, the minimum and maximum values of the abscissa and ordinate coordinates of the six points are the boundary area coordinates of the left-eye image. The following formula was used to obtain the boundary coordinates of the eye image. Among them, \ud835\udc65 , \ud835\udc66 are the corresponding abscissa and ordinate of each feature point, \ud835\udc4e, \ud835\udc4f are the minimum abscissa and ordinate of the eye area, and \ud835\udc34, \ud835\udc35 are the maximum abscissa and ordinate of the eye area.\nFigure 4. Collecting Dlib key points to extract human eye pictures.\nThe area delineated by the feature points of serial numbers 42\u201347 is the corresponding area of the left eye, and the feature points of the corresponding area of the right eye are serial numbers 36\u201341. Taking the extraction of the left-eye image as an example, the minimum and maximum values of the abscissa and ordinat o dinates of th six points are the bound ry area coordinates of the left- ye image. The fo lowing formul was used to obtain the boundary coordinates of the ey image. Among them, xi, yi are the corresponding abscissa and ordinate of each feature point, a, b are the minimum abscissa and ordinat of the eye area, and A, B are the maximum abscissa and ordinate of the eye area. a = min(xi) b = min(yi) A = max(xi)\nB = max(yi)\n(1)\nSensors 2023, 23, 6239 7 of 25\nA total of 200 videos of volunteers staring in different directions in virtual and real scenes were saved. In order to extract the human eye information in each frame of the image, we processed the video frame-by-frame, and cut out the binocular pictures according to the feature point coordinates of human eyes. The part between the two eyes may have some noise and redundant information that interferes with the prediction of the gaze direction. For example, the nose bridge, eyeglass frames, eye shadows, and eye spacing between the two eyes may have an impact on the feature extraction of the eyes, thereby reducing the accuracy of estimating the gaze direction. By removing the part between the two eyes, the attention can be focused on the separate eye regions and the performance of the gaze direction recognition can be improved. Considering the interference of the distance between the eyes on the human eye information, we intercepted the eye information of each frame of the picture, adjusted the size of the extracted picture to 100 \u00d7 50, and merged the eyes together, horizontally. According to the task attributes of the time of each frame of the image, the human eye information in each frame of the image was automatically marked, and finally, the collection of gaze data of 100 volunteers in virtual and real scenes was completed.\n3.2.3. Unification of Datasets\nThe data collected in the real environment in Section 3.2 had nine different labels, but after visualization by t-SNE [44], these labels could be directly transformed into three categories, which is also consistent with reality. For example, when the test volunteers gazed at the leftmost column, their eye features were approximately distributed in the same region regardless of the column they gazed at. Therefore, the three category labels could be cleverly used to uniformly mark the datasets of the real scene, which is exactly the same as the label classification of the datasets collected in the virtual environment, which is convenient for further dataset screening. The t-SNE visualization effect is shown in Figure 5.\nSensors 2023, 23, x FOR PEER REVIEW 7 of 26\n{\n\ud835\udc4e = min(\ud835\udc65\ud835\udc56)\n\ud835\udc4f = min(\ud835\udc66\ud835\udc56)\n\ud835\udc34 = max(\ud835\udc65\ud835\udc56)\n\ud835\udc35 = max(\ud835\udc66\ud835\udc56)\n(1)\nA total of 200 videos of volunteers staring in different directions in virtual and real\nscenes were saved. In rder t extract the human ey information in each fr me of the\nimage, we proc ssed the video frame-by-frame, and cut out the bino ular pictures accord-\ning to the fea ure point coordinates of human eyes. The part between the two eyes may\nh ve some oise and redundant information that int rferes with the prediction of the gaz\ndirection. For example, the nose bridge, y glass frames, eye shadows, and eye spacing\nbetween th two eyes may have an impact on the feature extraction of the eyes, ther by\nreducing the accuracy of estimating the gaze direction. By removing the part between the\ntwo eyes, the attention can be focused on the separate eye regions and the performance of\nthe gaze direction recognition can be improved.\nConsidering the interference of the distance between the eyes on the human eye in-\nformation, we intercepted the eye information of each frame of the picture, adjusted the\nsize of the extracted picture to 100 \u00d7 50, and merged the eyes together, horizontally. Ac-\ncording to the task attributes of the time of each frame of the image, the human eye infor-\nmation in each frame of the image was automatically marked, and finally, the collection\nof gaze data of 100 volunteers in virtual and real scenes was completed."
        },
        {
            "heading": "3.2.3. Unification of Datasets",
            "text": "The data collected in the real environment in Section 3.2 had nine different labels, but\nafter visualization by t-SNE [44], these labels could be directly transformed into three cat-\negories, which is also consistent with reality. For example, when the test volunteers gazed\nat the leftmost column, their eye features were approximately distributed in the same re-\ngion regardless of the column they gazed at. Therefore, the three category labels could be\ncleverly used to uniformly mark the datasets of the real scene, which is exactly the same\nas the label classification of the datasets collected in the virtual environment, which is\nconvenient for further dataset screening. The t-SNE visualization effect is shown in Figure\n5.\nFor the convenience of description, we introduced some symbols: \ud835\udc4b = {\ud835\udc4b1, \ud835\udc4b2, \u2026 , \ud835\udc4b\ud835\udc5b} and \ud835\udc5b = 1,2, \u2026 ,100; \ud835\udc4c = {\ud835\udc661, \ud835\udc662, \ud835\udc663}. Among them, the \ud835\udc4b set represents 100 volunteers participating in the data collection. Each \ud835\udc4b\ud835\udc56 contains 1350 photos, and each\ni re 5. t-S is alizati effect ia ra .\nor the convenience of description, we introduc d some symbols: X = {X1, X2, . . . , Xn} and n = 1, 2, . . . , 100; Y = {y , y2, y3}. Among them, the X set represents 100 volunteers participating in the data collection. Each Xi contains 1350 photos, and each photo has a corresponding label. Y represents the set of labels, y1 corresponds to the label left, y2 corresponds to the label forward, and y3 corresponds to the label right. A total of 135,000 pictures with gaze labels in different directions were selected to provide training data for the algorithm training in the next section. The established dataset is shown in Figure 6.\nSensors 2023, 23, 6239 8 of 25\nSensors 2023, 23, x FOR PEER REVIEW 8 of 26\nphoto has a corresponding label. \ud835\udc4c represents the set of labels, \ud835\udc661 corresponds to the label left, \ud835\udc662 corresponds to the label forward, and \ud835\udc663 corresponds to the label right. A total of 135,000 pictures with gaze labels in different directions were selected to provide\ntraining data for the algorithm training in the next section. The established dataset is\nshown in Figure 6.\nFigure 6. Eye-tracking dataset."
        },
        {
            "heading": "4. Eye-Tracking Model Building",
            "text": "In this paper, we used deep learning for the estimation of the eye-gaze direction in\nthe human eye-tracking recognition task for eye-tracking wheelchairs, which is divided\ninto feature extraction networks and classification. This was completed by training a deep\nlearning network (GazeNet) using the human eye database we have built, which incorpo-\nrates several modules for feature extraction optimization, and tri-classifying the features\nfor output using a cross-entropy loss function."
        },
        {
            "heading": "4.1. Eye-Gaze Direction Estimation",
            "text": "Figure 6. Eye-tracking dataset."
        },
        {
            "heading": "4. Eye-Tracking odel Building",
            "text": "In this paper, we used deep learning for the estimation of the eye-gaze direction in the human eye-tracking recognition task for eye-tracking wheelchairs, which is divided into feature extraction networks and classification. This was completed by training a deep learning network (GazeNet) using the human eye database we have built, which incorporates several modules for feature extraction optimization, and tri-classifying the features for output using a cross-entropy loss function."
        },
        {
            "heading": "4.1. Eye-Gaze Direction Estimation",
            "text": "Figure 7. GazeNet structure diagram.\n4.1.1. Inception Module The parallel structure adopted by the Inception module enables the input image to be processed by multiple convolutional kernels of different scales and pooling operations to obtain different levels of feature information [45]. The purpose is to extract features at different scales while keeping the size of the output feature map of the convolutional layer unchanged, to efficiently expand the depth and width of the network, and to prevent overfitting while improving the accuracy of the deep learning network. The specific implementation uses a combination of multiple convolutional kernels of different scales and pooling operations to increase the nonlinear representation of the model without increasing the number of model parameters. In this paper, we used the Inception module to decompose a 5 \u00d7 5 convolution kernel (Figure 8a) into two 3 \u00d7 3 convolution kernels (Figure 8b), so that we could effectively use only about (3 \u00d7 3 + 3 \u00d7 3)/(5 \u00d7 5) = 72% of the computational overhead. This reduced the number of model parameters and computation while maintaining the same perceptual field, reducing the computational burden, as shown in Figure 8.\nFigure 8. Sensory fields corresponding to two different convolution operations: (a) 5 \u00d7 5 convolution kernel and (b) 3 \u00d7 3 convolution kernel.\nSensors 2023, 23, 6239 9 of 25\n4.1.1. Inception Module\nThe parallel structure adopted by the Inception module enables the input image to be processed by multiple convolutional kernels of different scales and pooling operations to obtain different levels of feature information [45]. The purpose is to extract features at different scales while keeping the size of the output feature map of the convolutional layer unchanged, to efficiently expand the depth and width of the network, and to prevent overfitting while improving the accuracy of the deep learning network. The specific implementation uses a combination of multiple convolutional kernels of different scales and pooling operations to increase the nonlinear representation of the model without increasing the number of model parameters. In this paper, we used the Inception module to decompose a 5 \u00d7 5 convolution kernel (Figure 8a) into two 3 \u00d7 3 convolution kernels (Figure 8b), so that we could effectively use only about (3\u00d7 3 + 3\u00d7 3)/(5\u00d7 5) = 72% of the computational overhead. This reduced the number of model parameters and computation while maintaining the same perceptual field, reducing the computational burden, as shown in Figure 8.\nSensors 2023, 23, x FOR PEER REVIEW 9 of 26\nFigure 7. GazeNet structure diagram."
        },
        {
            "heading": "4.1.1. Inception Module",
            "text": "The parallel structure adopted by the Inception module enables the input image to\nbe processed by multiple convolutional kernels of different scales and pooling operations to obtain different levels of feature information [45]. The purpose is to extract features at different scales while keeping the size of the output feature map of the convolutional layer unchanged, to efficiently expand the depth and width of the network, and to prevent overfitting while improving the accuracy of the deep learning network. The specific implem ntation uses a combinatio of multiple convolutio al kernels f different scales and pooling operations to increase the nonlinear represe tation of the model without increasing the number of model parameters. In this paper, we used the Inception module to decompose a 5 \u00d7 5 convolution kernel (Figure 8a) into two 3 \u00d7 3 convolution kernels (Figure 8b), so that we could effectively use only about (3 3 + 3 \u00d7 3)/(5 \u00d7 5) = 72% of the computati nal overhead. This reduced the number of model parameters and computation while maintaining the same perceptual field, reducing the computational burden, as shown in Figure 8.\nigure 8. Sensory fields corresponding to two different convolution perations: (a) 5 \u00d7 5 convolution ernel and (b) 3 \u00d7 3 conv luti n kernel.\n4.1.2. ResBlock Module\nConvolutional neural networks can extract a rich feature hierarchy, but there is also the hidden danger of gradient disappearance or gradient explosion, and the use of regularization processing may produce degradation problems. Therefore, we chose to add the ResBlock module in the middle layer of the network, which achieved a jump connection by directly adding the output and input of the convolutional layer, thus ensuring better gradient transfer during backpropagation and reducing the number of model parameters [46]. The F(x) that needs to be taught can be written in the form of \u201cresiduals\u201d, as follows:\nF(x) = H(x)\u2212 x (2)\nDuring the training process, if the model finds that the gradient becomes very small when the neural network becomes deeper and deeper, i.e., the \u201cdegeneration phenomenon\u201d appears, it can directly add the output and input to achieve a constant mapping [47]. The advantage of this is that even if the network is deep enough, it can guarantee effective feature extraction at each layer. In this way, the performance of the network can be maintained even if the network becomes deeper and deeper, avoiding the degradation problem that occurs in traditional neural networks, as shown in Figure 9.\nSensors 2023, 23, 6239 10 of 25\nSensors 2023, 23, x FOR PEER REVIEW 10 of 26"
        },
        {
            "heading": "4.1.2. ResBlock Module",
            "text": "Convolutional neural networks can extract a rich feature hierarchy, but there is also the hidden danger of gradient disappearance or gradient explosion, and the use of regularization processing may produce degradation problems. Therefore, we chose to add the ResBlock module in the middle layer of the network, which achieved a jump connection by directly adding the output and input of the convolutional layer, thus ensuring better gradient transfer during backpropagation and reducing the number of model parameters [46]. The \ud835\udc39(\ud835\udc65) that needs to be taught can be written in the form of \u201cresiduals\u201d, as follows: \ud835\udc39(\ud835\udc65) = \ud835\udc3b(\ud835\udc65) \u2212 \ud835\udc65 (2) During the training process, if the model finds that the gradient becomes very small\nwhen the neural network becomes deeper and deeper, i.e., the \u201cdegeneration phenome-\nnon\u201d appears, it can directly add the output and input to achieve a constant mapping [47].\nThe advantage of this is that even if the network is deep enough, it can guarantee effective\nfeature extraction at each layer. In this way, the performance of the network can be main-\ntained even if the network becomes deeper and deeper, avoiding the degradation problem\nthat occurs in traditional neural networks, as shown in Figure 9.\nFigure 9. ResBlock module.\n\ud835\udc4b in Figure 9 represents the input, \ud835\udc3b(\ud835\udc4b) represents the output obtained after a series\nof transformations in the network, and \ud835\udc39(\ud835\udc4b) represents the residual, which is the differ-\nence between the network output \ud835\udc3b(\ud835\udc4b) and the input \ud835\udc4b. During the training process, the\nnetwork automatically learns a set of appropriate weights, so that \ud835\udc39(\ud835\udc4b) converges to 0.\nThe introduction of the residual concept allowed the network to better learn the mapping\nrelationship between the input and the output, while reducing the risk of gradient disap-\npearance or gradient explosion, thus improving the performance and generalization of\nthe model."
        },
        {
            "heading": "4.1.3. CBAM Module",
            "text": "In addition, since we wanted the feature processing to focus on the direction of the\npupil in the eye, the Convolutional Block Attention Model (CBAM) was inserted at the\nback of the ResBlock module. The CBAM module can improve the performance of convo-\nlutional neural networks for eye-tracking direction estimation by better learning and rep-\nresenting specific image features through the attention mechanism [48]. The overall pro-\ncess of the CBAM module can be divided into two parts, as shown in Figure 10.\nFigure 9. ResBlock module.\nX in Figure 9 represents the input, H(X) represents the output obtained after a series of transformations in the network, and F(X) represents the residual, which is the difference between the network output H(X) and the input X. During the training process, the network automatically learns a set of appropriate weights, so that F(X) converges to 0. The intr duction of the residual conce t allowed the network to better lear the mapping relationship betwe n the input and the utput, whil reducing th risk of gradient disappearance or gradient explosion, thus improving the performance and generalization of the model.\n4.1.3. CBAM Module\nIn addition, since we wanted the feature processing to focus on the direction of the pupil in the eye, the Convolutional Block Attention Model (CBAM) was inserted at the back of the ResBlock module. The CBAM module c n improve the performance of convolutional neural networks for eye-tracking direction estimation by better learning and representing specific image features through the attention mechanism [48]. The overall process of the CBAM module can be divided into two parts, as shown in Figure 10. Sensors 2023, 23, x FOR PEER REVIEW 11 of 26\nFigure 10. Convolutional Block Attention Model.\nChannel attention is mainly used to capture the correlation between different chan-\nnels [49]. This module first obtained two channels through global average pooling and\nmaximum pooling operations: the 1 \u00d7 1 \u00d7 C channels. Then, it was mapped and activated\nby a two-layer neural network, with the number of neurons in the first layer being \ud835\udc36/\ud835\udc5f.\nThe number of neurons in the first layer is Relu, and the number of neurons in the second\nlayer is \ud835\udc36. A summary of the two obtained features was produced and passed through\nthe Sigmoid activation function to obtain the weight coefficients, \ud835\udc40\ud835\udc36, which was multiplied with the input feature mapping to obtain the output with enhanced channel feature\nrepresentation. The features of an H \u00d7 W \u00d7 C input are F. The output feature formula is\nshown below:\n\ud835\udc40\ud835\udc36(\ud835\udc39) = \ud835\udf0e (\ud835\udc40\ud835\udc3f\ud835\udc43(\ud835\udc34\ud835\udc63\ud835\udc54\ud835\udc43\ud835\udc5c\ud835\udc5c\ud835\udc59(\ud835\udc39)) +\ud835\udc40\ud835\udc3f\ud835\udc43(\ud835\udc40\ud835\udc4e\ud835\udc65\ud835\udc43\ud835\udc5c\ud835\udc5c\ud835\udc59(\ud835\udc39)))\n= \ud835\udf0e (\ud835\udc4a1 (\ud835\udc4a0(\ud835\udc39\ud835\udc4e\ud835\udc63\ud835\udc54 \ud835\udc36 )) +\ud835\udc4a1(\ud835\udc4a0(\ud835\udc39\ud835\udc5a\ud835\udc4e\ud835\udc65\n\ud835\udc36 ))) (3)\nIn Equation (3), \ud835\udf0e denotes the Sigmoid function, MLP denotes the multilayer per-\nceptron, AvgPool denotes the average pooling layer, MaxPool denotes the maximum\npooling layer, \ud835\udc4a0 and \ud835\udc4a1 denote the two weight matrices, \ud835\udc39\ud835\udc4e\ud835\udc63\ud835\udc54 and \ud835\udc39\ud835\udc5a\ud835\udc4e\ud835\udc65 denote the results of the input data after the AvgPool and MaxPool operations, and the superscript \ud835\udc36\nis Channel, indicating the average pooling and maximum pooling operations in the chan-\nnel dimension.\nSpatial attention is mainly used to capture the correlation between different locations\non the feature map [50]. This module first performed average pooling and maximum pool-\ning to obtain two H \u00d7W\u00d7 1 channels, respectively, which were stitched together and\npassed through a 7 \u00d7 7 convolutional layer with the Sigmoid activation function to obtain\nthe weight coefficients, \ud835\udc40\ud835\udc46. This weight was multiplied with the input feature mapping by means of element multiplication to obtain the output of the enhanced spatial feature\nrepresentation [51]. An H \u00d7 W \u00d7 C input is characterized by F, and the output feature\nformula is shown below:\n\ud835\udc40\ud835\udc46(\ud835\udc39) = \ud835\udf0e(\ud835\udc537 \u00d7 7([\ud835\udc34\ud835\udc63\ud835\udc54\ud835\udc43\ud835\udc5c\ud835\udc5c\ud835\udc59(\ud835\udc39),\ud835\udc40\ud835\udc4e\ud835\udc65\ud835\udc43\ud835\udc5c\ud835\udc5c\ud835\udc59(\ud835\udc39)]))\n= \ud835\udf0e (\ud835\udc537 \u00d7 7([\ud835\udc39\ud835\udc4e\ud835\udc63\ud835\udc54 \ud835\udc46 ; \ud835\udc39\ud835\udc5a\ud835\udc4e\ud835\udc65\n\ud835\udc46 ])) (4)\nIn Equation (4), \ud835\udf0e denotes the Sigmoid function, AvgPool denotes the average pool-\ning layer, MaxPool denotes the maximum pooling layer, W0 and W1 denote two weight\nmatrices, \ud835\udc537 \u00d7 7 denotes a convolution kernel of size 7 \u00d7 7, \ud835\udc39\ud835\udc4e\ud835\udc63\ud835\udc54 and \ud835\udc39\ud835\udc5a\ud835\udc4e\ud835\udc65 denote the results of the input data after the AvgPool and MaxPool operations, and the superscript \ud835\udc46\nis Spatial, indicating the average pooling and maximum pooling operations in the spatial\ndimension.\n4.2. Classification Network\n4.2.1. Fully Connected Layer\nThe classification estimation task was implemented by a fully connected layer inte-\ngrating the local features after the convolution operation through a weight matrix. In the\neye-tracking direction classification task of this paper, it was necessary to obtain the result\nFigure 10. Convolutional Block Attention Model.\nChannel attention is mainly used to capture the correlation between different channels [49]. This module first obtained two channels through global average pooling and maximum pooling operations: the 1\u00d7 1\u00d7C channels. Then, it was mapped and activated by a two-layer neural network, with the number of neurons in the first layer being C/r. The number of neurons in the first layer is Relu, and the number of neurons in the second layer is C. A summary of the two obtained features was produced and passed through the Sigmoid activation function to obtain the weight coefficients, MC, which was multiplied with the input feature mapping to obtain the output with enhanced channel feature representation. The features of an H \u00d7W \u00d7 C input are F. The output feature formula is shown below:\nMC(F) = \u03c3(MLP(AvgPool(F)) + MLP(MaxPool(F))) = \u03c3 ( W1 ( W0 ( FCavg )) + W1 ( W0 ( FCmax ))) (3)\nSensors 2023, 23, 6239 11 of 25\nIn Equation (3), \u03c3 denotes the Sigmoid function, MLP denotes the multilayer perceptron, AvgPool denotes the average pooling layer, MaxPool denotes the maximum pooling layer, W0 and W1 denote the two weight matrices, Favg and Fmax denote the results of the input data after the AvgPool and MaxPool operations, and the superscript C is Channel, indicating the average pooling and maximum pooling operations in the channel dimension. Spatial attention is mainly used to capture the correlation between different locations on the feature map [50]. This module first performed average pooling and maximum pooling to obtain two H\u00d7W\u00d7 1 channels, respectively, which were stitched together and passed through a 7 \u00d7 7 convolutional layer with the Sigmoid activation function to obtain the weight coefficients, MS. This weight was multiplied with the input feature mapping by means of element multiplication to obtain the output of the enhanced spatial feature representation [51]. An H\u00d7W\u00d7 C input is characterized by F, and the output feature formula is shown below:\nMS(F) = \u03c3( f 7\u00d7 7([AvgPool(F), MaxPool(F)])) = \u03c3 ( f 7\u00d7 7 ([ FSavg; FSmax ])) (4) In Equation (4), \u03c3 denotes the Sigmoid function, AvgPool denotes the average pooling\nlayer, MaxPool denotes the maximum pooling layer, W0 and W1 denote two weight matrices, f 7 \u00d7 7 denotes a convolution kernel of size 7 \u00d7 7, Favg and Fmax denote the results of the input data after the AvgPool and MaxPool operations, and the superscript S is Spatial, indicating the average pooling and maximum pooling operations in the spatial dimension."
        },
        {
            "heading": "4.2. Classification Network",
            "text": "4.2.1. Fully Connected Layer\nThe classification estimation task was implemented by a fully connected layer integrating the local features after the convolution operation through a weight matrix. In the eye-tracking direction classification task of this paper, it was necessary to obtain the result of determining whether the eye is forward, left, or right, and this, therefore, is a triple-classification problem. The output of the feature extraction network in this paper was 2 \u00d7 2 \u00d7 4, and the fully connected layer had 3 neurons. First, the output of the feature extraction network was flattened into a one-dimensional column vector: x = [x1, x2, \u00b7 \u00b7 \u00b7 , x15, x16]T ; then, for each neuron in the fully connected layer: Z = [Z1, Z2, Z3]\nT , a linear operation was performed with each element in x. In the forwardpropagation process, the fully connected layer can be viewed as a linear weighted summation process. Specifically, each node in the previous layer was multiplied by a weighting factor, w, and a bias, b, was added to obtain the corresponding output on the fully connected layer, z. The computational process of the fully connected layer can be expressed using Equation (5):\nz1z2 z3  = w11w21 w12 \u00b7 \u00b7 \u00b7 w16w22 \u00b7 \u00b7 \u00b7 w26 w31 w32 \u00b7 \u00b7 \u00b7 w36  \u2217  x1 x2 ...\nx15 x16\n+ b1b2 b3  (5)\nThe output was normalized using the SoftMax function, which maps a vector into a probability distribution, where each element is a non-negative number, and the sum of all elements is 1. Thus, for a multiclassification problem with n classes, the SoftMax function can convert an n-dimensional vector into a probability distribution, where each element represents the predicted probability of that class. The neuron Z = [Z1, Z2, Z3] T on the\nSensors 2023, 23, 6239 12 of 25\nfully connected layer was normalized to y = [y1, y2, y3] T by the SoftMax function with the constraint: y1 + y2 + y3 = 1. The transformation relation between yi and Zj is:\nyi = eZj\n\u22113j=1e Zj\n(6)\n4.2.2. Cross-Entropy Loss Function\nDuring the training process, we wanted to make the output probability distribution of the model as close as possible to the true probability distribution, so we needed to design a suitable loss function to measure the difference between them. The cross-entropy function can effectively measure the difference between two probability distributions and is, therefore, widely used in classification problems [52]. In particular, if for a sample x with a true label of y, the predicted output of the SoftMax model for it is y\u0302, then the cross-entropy loss function can be expressed as:\nLoss(y, y\u0302) = \u2212\u2211 i yiln y\u0302 (7)\nThis loss function can be regarded as the KL dispersion (Kullback\u2013Leibler divergence) between the true and predicted labels [53]. The cross-entropy loss function obtained the minimum value of 0 when the output probability distribution of the model was exactly the same as the true probability distribution; however, when the difference between them increased, the value of the cross-entropy loss function also increased. Since the SoftMax function mapped the yi values to between 0 and 1, and according to the constraint \u2211iyi = 1, it can be deduced that:\nezi\n\u22113j=1e zj = 1\u2212\n\u2211j 6=iezj \u22113j=1e zj (8)\nwhen yi = 1, the loss function is:\nLossi(y, y\u0302) = \u2212\u2211 i ln yi (9)\nThe derivative procedure for Lossi(y, y\u0302) is as follows:\n\u2202Lossi \u2202zi = \u2212 \u2202ln yi\u2202zi = \u2202\n( \u2212ln e zi\n\u2211j e zj ) \u2202zi = \u2212 1 ezi\n\u2211j e zj\n\u00b7 \u2202\n( ezi\n\u2211j e zj ) \u2202zi\n= \u2211je\nzj \u00b7\u2211j 6=ie zj\nezi \u00b7 \u2212ezi\n(\u2211je zj)\n2 = \u2212 \u2211j 6=ie\nzj\n\u2211je zj\n= yi \u2212 1\n(10)\nFrom the derivation of the formula, it can be seen that the clever use of the crossentropy loss function with SoftMax for the triple-classification task makes it very easy to calculate the gradient in the backpropagation. The gradient of the backward update was obtained by simply taking the yi \u2212 1 calculated in the forward direction."
        },
        {
            "heading": "5. The Design of the Eye-Tracking Wheelchair Control System",
            "text": "In the previous section, the eye-tracking recognition algorithm was investigated. A complete eye-tracking wheelchair control system design solution should also include data acquisition, data processing, motion control, human\u2013machine interaction, and system optimization. The physical diagram and hardware data diagram of the wheelchair designed in this paper are shown in Figure 11.\nSensors 2023, 23, 6239 13 of 25\nSensors 2023, 23, x FOR PEER REVIEW 13 of 26 \ud835\udf15\ud835\udc3f\ud835\udc5c\ud835\udc60\ud835\udc60\ud835\udf15\ud835\udc67 = \u2212 \ud835\udf15 ln \ud835\udc66\ud835\udf15\ud835\udc67 = \ud835\udf15 \u2212 ln \ud835\udc52\u2211 \ud835\udc52\ud835\udf15\ud835\udc67 = \u2212 1\ud835\udc52\u2211 \ud835\udc52 \u2219 \ud835\udf15 \ud835\udc52\u2211 \ud835\udc52\ud835\udf15\ud835\udc67 = \u2211 \ud835\udc52 \u2219 \u2211 \ud835\udc52\ud835\udc52 \u2219 \u2212\ud835\udc52\u2211 \ud835\udc52 = \u2212 \u2211 \ud835\udc52\u2211 \ud835\udc52 = \ud835\udc66 \u2212 1 (10) From the derivation of the formula, it can be seen that the clever use of the crossentropy loss function with SoftMax for the triple-classification task makes it very easy to calculate the gradient in the backpropagation. The gradient of the backward update was obtained by simply taking the \ud835\udc66 \u2212 1 calculated in the forward direction."
        },
        {
            "heading": "5. The Design of the Eye-Tracking Wheelchair Control System",
            "text": "In the previous section, the eye-tracking recognition algorithm was investigated. A complete eye-tracking wheelchair control system design solution should also include data acquisition, data processing, motion control, human\u2013machine interaction, and system optimization. The physical diagram and hardware data diagram of the wheelchair designed in this paper are shown in Figure 11.\nFigure 11. Hardware design diagram: (a) wheelchair physical picture and (b) hardware data flow diagram.\nAccording to the role of hardware in the system, it can be divided into three sections: the data acquisition section, the data processing section, and the motion control section. The data acquisition area consists of the Gook HD98 HD camera with a resolution of 1920 \u00d7 1080 and a 10-inch touchscreen. The camera is responsible for capturing face images and the touch screen displays eye-tracking information in real time. The data processing section consists of the Jeston Tx2, which has 256 CUDA cores and up to 8 GB of memory. Its arithmetic power is comparable to that of a desktop-class graphics card GTX750, and it can easily handle the computational task of eye-tracking recognition. The motion control section is composed of Arduino as well as MG995 servo. Arduino receives an eye-tracking signal and drives MG995 to control the rocker to change the wheelchair motion. Common power wheelchair modifications are adjustments to the hardware portion of an existing wheelchair. Changes to the wheelchair motion can be achieved by simply controlling the rocker during the wheelchair motion. Therefore, in this paper, a mechanical structure (as shown in Figure 12) was installed on the wheelchair rocker controller. The structure consists of a base, a servo bracket, a control rod extension, and a control arm. The two servos are the x-axis and y-axis servos, which cooperate with each other to realize the control of the electric wheelchair rocker by turning in different directions\nFigure 11. Hardware design diagram: (a) wheelchair physical picture and (b) hardware data flow diagram.\nAccording to the role of hardware in the system, it can be divided into three sections: the data acquisition section, the data processing section, and the motion control section. The data acquisition area consists of the Gook HD98 HD camera with a resolution of 1920 \u00d7 1080 and a 10-inch touchscreen. The camera is responsible for capturing face images and the touch screen displays eye-tracking information in real time. The data processing section consists of the Jeston Tx2, which has 256 CUDA cores and up to 8 GB of memory. Its arithmetic power is comparable to that of a desktop-class graphics card GTX750, and it can easily handle the computational task of eye-tracking recognition. The motion control section is composed of Arduino as well as MG995 servo. Arduino receives an eye-tracking signal and drives MG995 to control the rocker to change the wheelchair motion. Common power wheelchair modifications are adjustments to the hardware portion of an existing wheelchair. Changes to the wheelchair motion can be achieved by simply controlling the rocker during the wheelchair motion. Therefore, in this paper, a mechanical structure (as shown in Figure 12) was installed on the wheelchair rocker controller. The structure consists of a base, a servo bracket, a control rod extension, and a control arm. The two servos are the x-axis and y-axis servos, which cooperate with each other to realize the control of the electric wheelchair rocker by turning in different directions according to the control command, so as to fulfill the control of the direction of wheelchair movement. Sensors 2023, 23, x FOR PEER REVIEW 14 of 26 according to the control comm nd, so as t fulfill the control of the direct of wheelchair movement.\nFigure 12. Modified wheelchair controller: (a) stop, (b) left, (c) forward, and (d) right.\nIn the upper computer TX2, we designed the intelligent eye-tracking wheelchair con-\ntrol platform through PYQT5. The intelligent eye-tracking wheelchair control platform\ncontains a data acquisition section and a wheelchair control section. The data acquisition\nsection collects face and human eye images in real time. The wheelchair control section\ncontains a wheelchair start button, wheelchair motion direction display, camera selection,\nand face count display functions. The intelligent eye-tracking wheelchair control platform\nmakes it easier to achieve wheelchair activation and monitor wheelchair movement. The\ninteractive interface is shown in Figure 13.\nFigure 12. Modified wheelchair controller: (a) stop, (b) left, (c) forward, and (d) right.\nIn the upper computer TX2, we designed the intelligent eye-tracking wheelchair control platform through PYQT5. The intelligent eye-tracking wheelchair control platform contains a data acquisition section and a heelchair control section. The data acquisition section collects face and hu an eye i ages in real ti e. The heelchair control section\nSensors 2023, 23, 6239 14 of 25\ncontains a wheelchair start button, wheelchair motion direction display, camera selection, and face count display functions. The intelligent eye-tracking wheelchair control platform makes it easier to achieve wheelchair activation and monitor wheelchair movement. The interactive interface is shown in Figure 13.\nSensors 2023, 23, x FOR PEER REVIEW 14 of 26 according to the control command, so as to fulfill the control of the direction of wheelchair movement.\nFigure 12. Modified wheelchair controller: (a) stop, (b) left, (c) forward, and (d) right.\nIn the upper computer TX2, we designed the intelligent eye-tracking wheelchair control platform through PYQT5. The intelligent eye-tracking wheelchair control platf rm contains a data acquisition section and a wheelchair control section. The data acquisition section collects face and human eye images in real time. The wheelchair control section contains a wheelchair start button, wheelchair motion direction display, camera selection, and face count display functions. The intelligent eye-tracking wheelchair control platform\nakes it easier to achieve wheelchair activation and onitor wheelchair ove ent. The interactive interface is sho n in Figure 13.\nFigure 13. Interactive interface."
        },
        {
            "heading": "5.1. Motion Control Optimization",
            "text": "In the process of wheelchair steering, the motion path of the rocker will have an important impact on the acceleration change in the system because of the non-linear relationship between the left and right wheel speeds and the rocker angle, as shown in Figure 14. In order to make the wheelchair motion change smoothly and weaken the risk of\nFigure 13. Interactive interface.\ni\nt process of wh elchair steering, the motion path of the rocker will h ve an important impact on the acceleration change in the system because of the non-line r relationship between the left and right wh l spee s and th rocker ngle, as shown in Figure 14. In order to make t e w eelchair motion change smoothly and w aken the risk of vibration and even rollover caused by the sudden change of acceleration, this section presents the research on the rocker trajectory-tracking control.\nSensors 2023, 23, x FOR PEER REVIEW 15 of 26\nvibration and even ollover caus d by the sudden change of acceleration, this section pre-\nsents the research on the rocker tr jectory-tracking co trol.\nFigure 14. Fitting diagram of wheelchair speed and rocker angle.\nIn order to establish the rocker control model, we introduced a polar coordinate sys-\ntem in the rocker motion plane, specifying the distance from the rocker to the reset as \ud835\udc5f,\nand the angle between the projection line of the rocker on the plane and the positive left\nas \ud835\udf03. The coordinate diagram is shown in Figure 15.\nFigure 15. Polar coordinate diagram of rocker position.\nSince the speed of the left and right wheels is symmetrical with respect to the change\nin the rocker angle, and the speed of the right wheel is almost constant in the interval of 0\u00b0 \u2212 90\u00b0 , therefore, it is only necessary to analyze the velocity change curve of the left wheel at 0\u00b0 \u2212 90\u00b0. The acceleration, \ud835\udc4e, of the left wheel is related to the velocity, \ud835\udc49\ud835\udc64, and the angle, \u03b8, of the rocker in polar coordinates, as follows:\n\ud835\udc4e = \ud835\udc51\ud835\udc49\ud835\udc64 \ud835\udc51\ud835\udf03 \u2219 \ud835\udc51\ud835\udf03 \ud835\udc51\ud835\udc61 (11)\nIn the range of 0\u00b0 \u2212 90\u00b0 , \ud835\udc51\ud835\udc49\ud835\udc64\n\ud835\udc51\ud835\udf03 continuously decreased as \ud835\udf03 increased. In order to\nmake the \ud835\udc4e change more smoothly, it was necessary to make \ud835\udc51\ud835\udf03\n\ud835\udc51\ud835\udc61 continuously increase as\n\ud835\udf03 increased. Accordingly, the following \ud835\udf03 \u2212 \ud835\udc61 diagram was drawn, as in Figure 16.\nFigure 14. Fitting diagram of wheelchair speed and rocker angle.\nIn order to establish the rocker control odel, e introduced a polar coordinate system in the rocker motion plane, specifying the distance from the rocker to the reset as r, and the angle between the projection line of the rocker on the plane and the positive left as \u03b8. The coordinate diagram is shown in Figure 15.\nSensors 2023, 23, 6239 15 of 25\nSensors 2023, 23, x FOR PEER REVIEW 15 of 26 vibration and even rollover caused by the sudden change of acceleration, this section presents the research on the rocker trajectory-tracking control.\nFigure 14. Fitting diagram of wheelchair speed and rocker angle.\nIn order to establish the rocker control model, we introduced a polar coordinate sys-\ntem in the rocker motion plane, specifying the distance from the rocker to the reset as \ud835\udc5f,\nand the angle between the projection line of the rocker on the plane and the positive left\nas \ud835\udf03. The coordinate diagram is shown in Figure 15.\nFigure 15. Polar coordinate diagram of rocker position.\nSince the speed of the left and right wheels is symmetrical with respect to the change\nin the rocker angle, and the speed of the right wheel is almost constant in the interval of 0\u00b0 \u2212 90\u00b0 , therefore, it is only necessary to analyze the velocity change curve of the left wheel at 0\u00b0 \u2212 90\u00b0. The acceleration, \ud835\udc4e, of the left wheel is related to the velocity, \ud835\udc49\ud835\udc64, and the angle, \u03b8, of the rocker in polar coordinates, as follows:\n\ud835\udc4e = \ud835\udc51\ud835\udc49\ud835\udc64 \ud835\udc51\ud835\udf03 \u2219 \ud835\udc51\ud835\udf03 \ud835\udc51\ud835\udc61 (11)\nIn the range of 0\u00b0 \u2212 90\u00b0 , \ud835\udc51\ud835\udc49\ud835\udc64\n\ud835\udc51\ud835\udf03 continuously decreased as \ud835\udf03 increased. In order to\nmake the \ud835\udc4e change more smoothly, it was necessary to make \ud835\udc51\ud835\udf03\n\ud835\udc51\ud835\udc61 continuously increase as\n\ud835\udf03 increased. Accordingly, the following \ud835\udf03 \u2212 \ud835\udc61 diagram was drawn, as in Figure 16.\nSince the speed of the left and right wheels is symmetrical with respect to the change in the rocker angle, and the speed of the right wheel is almost constant in the interval of 0 \u25e6 \u2212 90\u25e6 , therefore, it is only necessary to analyze the velocity change curve of the left wheel at 0 \u25e6 \u2212 90\u25e6 . The acceleration, a, of the left wheel is related to the velocity, Vw, and the angle, \u03b8, of the rocker in polar coordinates, as follows:\na = dVw d\u03b8 \u00b7d\u03b8 dt (11)\nIn the range of 0 \u25e6 \u2212 90\u25e6 , dVwd\u03b8 continuously decreased as \u03b8 increased. In order to\nmake the a change more smoothly, it was necessary to make d\u03b8dt continuously increase as \u03b8 increased. Accordingly, the following \u03b8 \u2212 t diagram was drawn, as in Figure 16.\nSensors 2023, 23, x FOR PEER REVIEW 16 of 26\nFigure 16. \u03b8-t diagram.\nBy:\n{\n\ud835\udf03(\ud835\udc61) = \u222b \ud835\udf14(\ud835\udc61)\ud835\udc51\ud835\udc61\n\ud835\udc61\n0\n\ud835\udf14(\ud835\udc61) = \ud835\udc49\ud835\udc45 \ud835\udc5f(\ud835\udc61)\n(12)\nIt was obtained as follows:\n\ud835\udc5f(\ud835\udc61) = \ud835\udc49\ud835\udc45 \ud835\udf03\u2032(\ud835\udc61)\n(13)\nwhere \ud835\udf14(\ud835\udc61) is the rocker angular velocity and \ud835\udc49\ud835\udc45 is the rocker velocity, which can be considered constant in magnitude. Then, \ud835\udc5f(\ud835\udc61) could be obtained according to Equation\n(13) with the derived \ud835\udf03(\ud835\udc61), and the rocker motion trajectory could be determined accord-\ningly.\nTo simplify the analysis, \ud835\udc49\ud835\udc64(\ud835\udf03) as well as \ud835\udf03(\ud835\udc61) can be expressed as Equations (14)\nand (15):\n\ud835\udc49\ud835\udc64(\ud835\udf03) = \ud835\udc4e1\ud835\udf03 2 + \ud835\udc4f1\ud835\udf03(\ud835\udf03 < 90 \u00b0) (14)\n\ud835\udf03(\ud835\udc61) = \ud835\udc4e2\ud835\udc61 2 + \ud835\udc4f2\ud835\udc61(\ud835\udc61 < \ud835\udc47\ud835\udc52) (15)\nwhere \ud835\udc4e1, \ud835\udc4f1, and \ud835\udc47\ud835\udc52 are known, \ud835\udc47\ud835\udc52 denotes the time required for the rocker to move from the starting position to the end position, and \ud835\udf03(\ud835\udc47\ud835\udc52) = \ud835\udf0b\n2 .\nThen, substituting Equations (14) and (15) into (11) yields:\n\ud835\udc4e(\ud835\udc61) = [2\ud835\udc4e1(\ud835\udc4e2\ud835\udc61 2 + \ud835\udc4f1\ud835\udc61) + \ud835\udc4f1](2\ud835\udc4e2\ud835\udc61 + \ud835\udc4f2) (16)\nThe simplification yields:\n\ud835\udc4e(\ud835\udc61) = \ud835\udc34(\ud835\udc61 \u2212 \ud835\udc611)(\ud835\udc61 \u2212 \ud835\udc612)(\ud835\udc61 \u2212 \ud835\udc613), (17)\nwith the following constraints:\n{\n\ud835\udc4e(0) = 0 \ud835\udc4e(\ud835\udc47\ud835\udc52) = 0\n\u222b \ud835\udc4e(\ud835\udc61) = \ud835\udc49\ud835\udc52\n\ud835\udc47\ud835\udc52\n0\n(18)\nSubstituting Equation (17) into (18) yields:\n\ud835\udc4e(\ud835\udc61) = 12\ud835\udc49\ud835\udc52\n2\ud835\udc613\ud835\udc47\ud835\udc52 3 \u2212 \ud835\udc47\ud835\udc52\n4 \ud835\udc61(\ud835\udc61 \u2212 \ud835\udc47\ud835\udc52)(\ud835\udc61 \u2212 \ud835\udc613) (19)\nFigure 16. \u03b8-t diagram. By: \u03b8(t) =\n\u222b t 0 \u03c9(t)dt\n\u03c9(t) = VRr(t)\n(12)\nIt was obtained as follows: r(t) =\nVR \u03b8\u2032(t)\n(13)\nwhere \u03c9(t) is the rocker angular velocity and VR is the rocker velocity, which can be considered constant in magnitude. Then, r(t) could be obtained according to Equation (13)\nith the derived \u03b8(t), and the rock r motio trajectory could be determined ac ordingly. To simplify the a alysis, Vw(\u03b8) as well as \u03b8(t) can be expressed as Equations (14) and (15):\nVw(\u03b8) = a1\u03b82 + b1\u03b8 ( \u03b8 < 90 \u25e6 )\n(14)\n\u03b8(t) = a2t2 + b2t(t < Te) (15)\nwhere a1, b1, and Te are known, Te denotes the time required for the rocker to move from the starting position to the end position, and \u03b8(Te) = \u03c02 .\nSensors 2023, 23, 6239 16 of 25\nThen, substituting Equations (14) and (15) into (11) yields:\na(t) = [ 2a1 ( a2t2 + b1t ) + b1 ] (2a2t + b2) (16)\nThe simplification yields:\na(t) = A(t\u2212 t1)(t\u2212 t2)(t\u2212 t3) (17)\nwith the following constraints:  a(0) = 0\na(Te) = 0\u222b Te 0 a(t) = Ve\n(18)\nSubstituting Equation (17) into (18) yields:\na(t) = 12Ve\n2t3Te3 \u2212 T4e t(t\u2212 Te)(t\u2212 t3) (19)\nwhere Te indicates the time taken for the rocker to move from 0 \u25e6 to 90 \u25e6 . According to the characteristics of the cubic function, it is necessary to satisfy t3 > Te in order to make the acceleration change more smoothly in 0\u2212 Te. Then, Matlab can be used to plot the curve of a with time t when t3 changes. From Figure 17, it can be seen that the curve of t3 with t tended to flatten out during the increase of t3 from Te, and the maximum value tended to be 1.5VeTe . Accordingly, \u03b8(t) and r(t) under the target conditions could be derived, and the rocker motion trajectory could then be derived from them.\nSensors 2023, 23, x FOR PEER REVIEW 17 of 26\nwhere \ud835\udc47\ud835\udc52 indicates the time taken for the rocker to move from 0 \u00b0 to 90\u00b0. According to the characteristics of the cubic function, it is necessary to satisfy \ud835\udc613 > \ud835\udc47\ud835\udc52 in order to make t e acceleration change more smoothly i 0 \u2212 \ud835\udc47\ud835\udc52. Then, Matlab can be used to plot t e curv of \ud835\udc4e with time \ud835\udc61 when \ud835\udc613 changes.\nFro Figure 17, it c b seen that the curve of \ud835\udc613 with \ud835\udc61 tended to flatten out during\nthe increase of \ud835\udc613 from \ud835\udc47\ud835\udc52, and the m ximum alue ended to be 1.5\ud835\udc49\ud835\udc52\n\ud835\udc47\ud835\udc52 . Accordingly, \ud835\udf03(\ud835\udc61)\nand \ud835\udc5f(\ud835\udc61) under the target conditions could be derived, and the rocker motion trajectory\ncould then be derived from them."
        },
        {
            "heading": "5.2. System Flow",
            "text": ""
        },
        {
            "heading": "5.2.1. Blink Detection",
            "text": "The face-detection algorithm using the Dlib library with OpenCV image processing\nto obtain periocular feature point data was described in Section 3.2.2, and the eye-tracking\nestimation model was built in Section 3. Taking the left eye as an example, the outline of\nthe eye was first located in the eye image, and points p1\u2013p6 were used to represent the key\npoints of the eye, as shown in Figure 18. The actual use of the system control needs to\ndetect whether the human eye is open before the line-of-sight estimation. The eye opening\nand closing states can be determined by calculating the eye aspect ratio (EAR) [54] in real\ntime and comparing it with the set threshold, and there is no need to enter the line-of-\nsight estimation procedure if the eye opening and closing do not meet the EAR threshold.\nFigure 18. Key points of the eye.\nThe face-detection algorithm using the Dlib library with OpenCV image processing to obtain periocular feature point data was described in Section 3.2.2, and the eye-tracking estimation model was built in Section 3. Taking the left eye as an example, the outline of the eye was first located in the eye image, and points p1\u2013p6 were used to represent the key points of the eye, as shown in Figure 18. The actual use of the system control needs to detect whether the human eye is open before the line-of-sight estimation. The eye opening and closing states can be determined by calculating the eye aspect ratio (EAR) [54] in real\nSensors 2023, 23, 6239 17 of 25\ntime and comparing it with the set threshold, and there is no need to enter the line-of-sight estimation procedure if the eye opening and closing do not meet the EAR threshold.\nSensors 2023, 23, x FOR PEER REVIEW 17 of 26 where \ud835\udc47\ud835\udc52 indicates the time taken for the rocker to move from 0 \u00b0 to 90\u00b0. According to the characteristics of the cubic function, it is necessary to satisfy \ud835\udc613 > \ud835\udc47\ud835\udc52 in order to make the acceleration change more smoothly in 0 \u2212 \ud835\udc47\ud835\udc52. Then, Matlab can be used to plot the curve of \ud835\udc4e with time \ud835\udc61 when \ud835\udc613 changes. From Figure 17, it can be seen that the curve of \ud835\udc613 with \ud835\udc61 tended to flatten out during the increase of \ud835\udc613 from \ud835\udc47\ud835\udc52, and the maximum value tended to be 1.5\ud835\udc49\ud835\udc52 \ud835\udc47\ud835\udc52 . Accordingly, \ud835\udf03(\ud835\udc61) and \ud835\udc5f(\ud835\udc61) under the target conditions could be derived, and the rocker motion trajectory could then be derived from them. Figure 17. Variation curve of \ud835\udc4e with \ud835\udc61 at different \ud835\udc613."
        },
        {
            "heading": "5.2. System Flow",
            "text": ""
        },
        {
            "heading": "5.2.1. Blink Detection",
            "text": "The face-detection algorithm using the Dlib library with OpenCV image processing\nto obtain periocular feature point data was described in Section 3.2.2, and the eye-tracking\nestimation model was built in Section 3. Taking the left eye as an example, the outline of\nthe eye was first located in the eye image, and points p1\u2013p6 were used to represent the key\npoints of the eye, as shown in Figure 18. The actual use of the system control needs to\ndetect whether the human eye is open before the line-of-sight estimation. The eye opening\nand closing states can be determined by calculating the eye aspect ratio (EAR) [54] in real\nti e and comparing it with the set threshold, and there is no need to enter the line-of-\nsight estimation procedure if the eye ope ing and closi g do not meet the EAR threshold.\nFigure 18. Key points of the eye.\nFigure 18. Key points of the eye.\nThe respective EAR values of the left and right eyes were calculated, and finally, the average EAR value of the left and right eyes was obtained. When the eyes were open, EAR maintained a constant value; when the eyes were closed, the EAR value tended to 0. If the calculated EAR value was less than the set threshold, it was determined to be a blink action. The EAR calculation formula is shown below:\nEARle f t = \u2016p2 \u2212 p6\u2016+ \u2016p3 \u2212 p5\u2016\n2\u2016p1 \u2212 p4\u2016 (20)\nEAR = EARle f t + EARright\n2 (21)\n5.2.2. Saccades Processing\nIn addition to the impact that blinking can have on wheelchair control, the sweeping behavior of the user due to unexpected events during use may also affect the control of the wheelchair and the safety of the user. If the user rapidly changes the direction of gaze in a short period of time, the wheelchair may continuously receive movement commands in different directions, causing the wheelchair to swing from side to side. To reduce the risk of this unexpected situation, we used the results that occurred four or more times out of every eight classification results as the motion control command according to the majority principle, and if there was no majority result out of eight classification results, the wheelchair motion stopped. Since the network model is capable of generating approximately 16 classification results in 1 s, the above treatment had less impact on wheelchair maneuverability.\n5.2.3. System Flow Chart\nAfter analyzing the workflow of the eye-tracking wheelchair in a previous paper, the system flowchart of the eye-tracking wheelchair is presented in this paper, as shown in Figure 19. The control program begins in the host computer, and the camera in front of the wheelchair is automatically turned on by the program, and then the camera starts to extract facial features and collect eye-movement data. Jeston Tx2 calculates the EAR of each frame of the acquired eye-movement data. If the latter does not exceed the threshold, the eye-movement data are reacquired, and if the threshold is exceeded, the frame is fed into the GazeNet network model to calculate the eye-tracking direction and the classification result is output to the motion controller, Arduino. The latter manipulates the rocker to complete the control of the wheelchair movement based on the eye-movement signal. If the wheelchair is turned off at this point, the system stops working; otherwise, it will return to the eye-movement data acquisition process and continue to complete the calculation and judgment of the eye-movement data EAR, and so on.\nSensors 2023, 23, 6239 18 of 25Sensors 2023, 23, x FOR PEER REVIEW 19 of 26\nFigure 19. System flow chart.\n6. Experiments and Results Analysis In the previous sections, we completed the establishment of the model and the construction of the system. In order to verify the practicality and reliability of the eye-tracking wheelchair, in this section, we outline the test experiments conducted on the accuracy of the model recognition results and the accuracy of the wheelchair control. The first experiment was a comparative experimental evaluation of the GazeNet proposed in this paper with three existing models, such as AlexNet, ResNet18, and MobileNet-V2, to demonstrate the superiority of the model used in this paper in eye-tracking recognition. The second experiment quantified the wheelchair control accuracy by testing the deviation of the actual wheelchair motion trajectory from the target path, and the third experiment tested the effect of motion control optimization in Section 4.2 by measuring the Arduino output PWM signal.\nFigure 19. System flow chart."
        },
        {
            "heading": "6. Experiments and Results Analysis",
            "text": "In the previous sections, we completed the establish ent of the odel and the construction of the system. In order to verify the practicality and reliability of the eye-tracking whe lchair, in this section, we outline the t st experiments conducted on the accuracy of the model recognitio results and the accuracy of the wheelc air control. The first experiment was a comparative experimental evaluation of the GazeNet proposed in this paper with three existing models, such as AlexNet, ResNet18, and MobileNet-V2, to demonstrate the superiority of the model used in this paper in eye-tracking recognition. The second experiment quantified the wheelchair control accuracy by testing the deviation of the actual wheelchair motion trajectory from the target path, and the third experiment tested the effect of motion control optimization in Section 4.2 by measuring the Arduino output PWM signal.\nSensors 2023, 23, 6239 19 of 25\n6.1. Evaluation of GazeNet\u2019s Effectiveness 6.1.1. Hyperparameter Optimization\nIn the training and testing of the GazeNet network, each image in X was subjected to the preprocessing operation described in Section 3.2.2 to obtain X, which was named the multi-environment attentional gaze dataset. The 135,000 binocular images in X were divided into a training set, validation set, and test set in the ratio of 98:1:1 and put into the network for training. The initial parameters were set with random initialization, the learning rate was 0.001, the optimizer used SGD (stochastic gradient descent) [55], the loss function used cross-entropy, and accuracy was used as an important evaluation index to measure the model performance. The accuracy rate was calculated as follows:\nAccuracy = TP + TN\nTP + TN + FP + FN (22)\nwhere TP is true positive, TN is true negative, FP is false positive, and FN is false negative. As shown in Figure 20, a total of 30 rounds were trained, and after 18 epochs of training, the accuracy curve gradually leveled off and no longer significantly improved, which indicated that the training of the classification network was completed. In this paper, we chose to save the weight parameters of the model at the 29th round, and its accuracy rate was 0.98494.\nSensors 2023, 23, x FOR PEER REVIEW 20 of 26"
        },
        {
            "heading": "6.1. Evaluation of GazeNet\u2019s Effectiveness",
            "text": ""
        },
        {
            "heading": "6.1.1. Hyperparameter Optimization",
            "text": "In the training and testing of the GazeNet network, each image in ?\u0305? was subjected\nto the preprocessing operation described in Section 3.2.2 to obtain ?\u0305?, which was named\nthe multi-enviro ment tte tional gaze dataset. The 135,000 binocular images in ?\u0305? were\ndivided into a training set, validation set, and t st set in the ratio of 98:1:1 and put into the\nnetwork for training. The initial parameters were set with random initializatio , the learn-\ning rate was 0.001, the optimizer used SGD (stochastic gradient descent) [55], the loss func-\ntion used cross-entropy, and accuracy was used as an important evaluation index to meas-\nure the model performance. The accuracy rate was calculated as follows:\nAccuracy = TP + TN\nTP + TN + FP + FN (22)\nwhere TP is true positive, TN is true negative, FP is false positive, and FN is false negative.\nAs shown in Figure 20, a total of 30 rounds were trained, and after 18 epochs of train-\ning, the accuracy curve gradually leveled off and no longer significantly improved, which\nindicated that the training of the classification network was completed. In this paper, we\nchose to save the weight para et rs f t e odel at the 29th round, and its accuracy rate\nwas 0.984 4.\nFigure 20. GazeNet training process."
        },
        {
            "heading": "6.1.2. Assessment Measures and Methods",
            "text": "The GazeNet network described above was trained on the MEAGaze dataset using\nthe same training set as the widely used AlexNet [56], ResNet18 [57], and MobileNet-V2\n[58] models, and tested under the same test set. The test results are shown in Table 1 and\nFigure 21. The GazeNet network in this paper achieved an accuracy of 98.49% and used the smallest number of parameters, only 125,749, which was significantly better than the other three models. The comparison showed that the GazeNet network was about 1% point more accurate than the ResNet18 model, with the second-highest accuracy rate, while the former had only 22.3% of the number of participants of the AlexNet model, with the second-lowest parameter amount. The GazeNet network has a minimum number of parameters with excellent accuracy, which makes it an ideal lightweight model that can meet the needs of both high accuracy and low resource consumption.\nigure 20. GazeNet training process.\n6.1.2. Assessment Measures and Methods\nThe GazeNet network described above was trained on the MEAGaze dataset using the same training set as the idely used AlexNet [56], ResNet18 [57], and MobileNet-V2 [58] models, and tested under t sam test s t. The test results are shown in Table 1 and Figure 21. The GazeNe network in this pap r achieved an accuracy of 98.49% and used the\nsmallest number of parameters, only 125,749, which was significantly better than the other\nthree models. The comparison showed that the GazeNet network was about 1% point more\naccurate than the ResNet18 model, with the second-highest accuracy rate, while the former\nhad only 22.3% of the number of participants of the AlexNet model, with the second-lowest\nparameter amount. The GazeNet network has a minimum number of parameters with\nexcellent accuracy, which makes it an ideal lightweight model that can meet the needs of\nboth high accuracy and low resource consumption.\nSensors 2023, 23, 6239 20 of 25\nTable 1. Comparison of the experimental results of different networks.\nModel Accuracy Parameter Amount\nGazeNet 98.49% 125,749 AlexNet 96.5% 564,003\nResNet18 97.5% 11,178,051 MobileNet-V2 96.9% 2,227,715\nSensors 2023, 23, x FOR PEER REVIEW 21 of 26\nTable 1. Comparison of the experimental results of different networks.\nModel Accuracy Parameter Amount\nGazeNet 98.49% 125,749\nAlexNet 96.5% 564,003\nResNet18 97.5% 11,178,051\nMobileNet-V2 96.9% 2,227,715\nThe comparative experiments in the previous section demonstrated the high recog-\nnition rate of the eye-tracking model used in this paper. In order to verify the reliability\nand control accuracy of the eye-tracking wheelchair, we launched experiments related to\nthe eye-tracking wheelchair in specific scenarios.\nThe experimental route diagram is shown in Figure 22. The solid part represents the\ntarget path of the wheelchair center, and the dashed part is the target path of both wheels.\nSince the width of the wheelchair that we used was 60 cm, the dashed path spacing was\n60 cm. An experiment designed in this paper consisted of two laps of the line: the yellow\nline in Figure 22a is the target path of the first lap, the blue line in Figure 22c is the target\npath of the second lap, and Figure 22b is the real target line diagram combining the two,\nwhere the green line represents the overlapping part of the two laps of the line. The start\nand end points of both laps are the red points in Figure 22. The experimental design al-\nlowed the wheelchair to complete an experiment with an equal number of left and right\nturns, both eight times, making the experimental data more meaningful.\nTo make the data of the experiment generalizable, we recruited 20 volunteers aged\n20\u201335 years to participate in the experiment. These 20 volunteers were 50% men and 50%\nwomen and wore glasses. Each volunteer was trained to operate a wheelchair while main-\ntaining head immobilization to complete the experiment, during which the speed of the\nwheelchair was set to 1 m/s. We installed a laser distance measurement module on each\nside of the wheelchair, as shown in Figure 23, which had a range of up to 80 m, a meas-\nurement accuracy of 1.0 mm, and a measurement frequency of 20 Hz. It can measure and\nrecord the distance to the wall in real time and obtain the deviation of the wheelchair\ncenter from the target path based on the distance to the wall. The values of deviation with\na distance of movement at each experiment were totaled and divided by the total number\nof people, and a graph of deviation with a distance of movement was obtained, as shown\nin Figure 24. The fluctuating part of this figure is the deviation of the wheelchair during\nthe turn, and the maximum deviation appeared at the second circle of 8 m, which was 6.76\nFigure 21. Training models for four different networks."
        },
        {
            "heading": "6.2. Reliability Analysis of Eye-Tracking Wheelchair Control",
            "text": "The comparative experiments in the previous section demonstrated the high recognition rate of the eye-tracking model used in this paper. In order to verify the reliability and control accuracy of the eye-tracking wheelchair, we launched experiments related to the eye-tracking wheelchair in specific scenarios. The experimental route diagram is shown in Figure 22. The solid part represents the target path of the wheelchair center, and the dashed part is the target path of both wheels. Since the width of the wheelchair that we used was 60 cm, the dashed path spacing was 60 cm. An experiment designed in this paper consisted of two laps of the line: the yellow line in Figure 22a is the target path of the first lap, the blue line in Figure 22c is the target path of the second lap, and Figure 22b is the real target line diagram combining the two, where the green line represents the overlapping part of the two laps of the line. The start and end points of both laps are the red points in Figure 22. The experimental design allowed the wheelchair to complete an experiment with an equal number of left and right turns, both eight times, making the experimental data more meaningful. To make the data of the experiment generalizable, we recruited 20 volunteers aged 20\u201335 years to participate in the experiment. These 20 volunteers were 50% men and 50% women and wore glasses. Each volunteer was trained to operate a wheelchair while maintaining head immobilization to complete the experiment, during which the speed of the wheelchair was set to 1 m/s. We installed a laser distance measurement module on each side of the wheelchair, as shown in Figure 23, which had a range of up to 80 m, a measurement accuracy of 1.0 mm, and a measurement frequency of 20 Hz. It can measure and record the distance to the wall in real time and obtain the deviation of the wheelchair center from the target path based on the distance to the wall. The values of deviation with a distance of movement at each experiment were totaled and divided by the total number of people, and a graph of deviation with a distance of movement was obtained, as shown in Figure 24. The fluctuating part of this figure is the deviation of the wheelchair during the turn, and the maximum deviation appeared at the second circle of 8 m, which was 6.76 cm.\nSensors 2023, 23, 6239 21 of 25\nIt can be seen that the deviation during the wheelchair movement was small, the accuracy of the model and control was high, and the eye-activated wheelchair we designed had high practicality.\nSensors 2023, 23, x FOR PEER REVIEW 22 of 26\ncm. It can be seen t at the deviation during t w eelchair ovement w s small, the ac-\ncuracy of the model and control was high, and the eye-activated w eelchair we designe\nad high pr cticality.\nFigure 22. Experimental circuit diagram. (a) First lap, (b) actual route, and (c) second lap.\nFigure 23. Actual test diagram.\nFigure 24. Variation of deviation with distance travelled: (a) lap 1 and (b) lap 2.\nFigure 22. Experimental circuit diagram. (a) First lap, (b) actual route, and (c) second lap.\nSensors 2023, 23, x FOR PEER REVIEW 22 of 26\ncm. It can be seen that the deviation during the wheelchair movement was small, the ac-\ncuracy of the model and control was high, and the eye-activated wheelchair we designed\nhad high practicality.\nFigure 22. Experimental circuit diagram. (a) First lap, (b) actual route, and (c) second lap.\nFigure 23. Actual test diagram.\nFigure 24. Variation of deviation with distance travelled: (a) lap 1 and (b) lap 2.\nFigure 23. Actual test diagram.\nIn Section 4.2, we optimized the rocker control. To verify the effect of this work on the stability of the wheelchair control, we had the experimenter sit on the motorized wheelchair, disconnect the motor power, and connect the Arduino output pins with an oscilloscope. The experimenter simulated the eye-movement state in the previous experiment while keeping the head immobile. The average curve of the rudder duty cycle over time during the experiment was obtained by superimposing the numerical curves of the rudder duty cycle over time during the experiment for the 20 volunteers and applying the total number of people, as shown in Figure 25.\nSensors 2023, 23, 6239 22 of 25\nSensors 2023, 23, x FOR PEER REVIEW 22 of 26 cm. It can be seen that the deviation during the wheelchair movement was small, the accuracy of the model and control was high, and the eye-activated wheelchair we designed had high practicality. Figure 22. Experimental circuit diagram. (a) First lap, (b) actual route, and (c) second lap.\nFigure 23. Actual test diagram.\nFigure 24. Variation of deviation with distance travelled: (a) lap 1 and (b) lap 2. Figure 24. Variation of deviation with distance travelled: (a) lap 1 and (b) lap 2.\nSensors 2023, 23, x FOR PEER REVIEW 23 of 26\nIn Section 4.2, we optimized the rocker control. To verify the effect of this work on\nthe stability of the wheelchair control, we had the experimenter sit on the motorized\nwheelchair, disconnect the motor power, and connect the Arduino output pins with an\noscilloscope. The experimenter simulated the eye-movement state in the previous experi-\nment while keeping the head immobile. The average curve of the rudder duty cycle over\ntime during the experiment was obtained by superimposing the numerical curves of the\nrudder duty cycle over time during the experiment for the 20 volunteers and applying the\ntotal number of people, as shown in Figure 25.\nFigure 25. Servo duty cycle variation curve with motion time: (a) servo No. 1 and (b) servo No. 2.\nAs can be seen from Figure 25, the control of the servo was smoother and without\nnoise, which can also indicate the high accuracy of the neural network triple classification\nand the absence of several types of jumping outputs.\n7. Conclusions\nWe collected an eye-movement dataset with 135,000 annotated images through vir-\ntual and real scenes and proposed a GazeNet eye-movement neural network model based\non the three-category dataset. The model comparison experiment results showed that the\nGazeNet model proposed in this paper had a faster convergence speed and higher accu-\nracy than the other three models. In terms of wheelchair control, we used a 2D steering\ngear to control the joystick and optimize the steering gear control signal. The follow-up\nArduino output waveform experiment showed that the steering gear control signal was\nsmooth and gentle, and the optimization results were good. In addition, the experiment\nalso proved that the three-category result of the model was accurate, and there was no\ncase of jumping outputs of several eye-movement categories. In the wheelchair control\nreliability analysis, we obtained the deviation between the target and the actual trajectory\nduring the movement process through laser ranging and concluded that the accuracy of\nthe motion control and eye-movement model was high.\nHowever, the current motion control part is relatively complicated, and the upper\nlimit of optimization is low. If the signal of Jeston Tx2 can be directly output to control the\nwheelchair, the motion control can be made simpler and more reliable. In addition, the\neye-movement dataset was also collected in a specific scene and did not fully consider\napplications in scenes in daily life, such as crossing the road, rainy days, etc. After the\nexperiment, many volunteers reported that continuous eye-movement control increased\nthe burden on users, which reduced the accuracy of control to a certain extent and in-\ncreased the risk for users. Considering that the current wheelchair control scheme lacks\nadaptability to the environment, we expect to add a visual slam and path planning [59] to\nthe control part in the follow-up work. During the process, the direction of sight can be\nfreely controlled, and the wheelchair can autonomously perceive the surrounding envi-\nronment and make adjustments to reach the destination. For safety reasons, we plan to\nFigure 25. Servo duty cycle variation curve with motion time: (a) servo No. 1 and (b) servo No. 2.\nAs can be seen from Figure 25, the control of the servo was smoother and without noise, which can also indicate the high accuracy of the neural network triple classification and the absence of several types of jumping outputs."
        },
        {
            "heading": "7. Conclusions",
            "text": "We collected an eye-movement dataset with 135,000 annotated images through virtual and real scenes and proposed a GazeNet ey -movement neural network od l based on the three-category dataset. The model comparison experiment results showed that the GazeNet model proposed in this paper had a faster convergence speed and higher accuracy than the other three models. In terms of wheelchair control, we used a 2D steering gear to control the joystick and optimize the steering gear control signal. The follow-up Arduino output waveform experiment showed that the steering gear control signal was smooth and gentle, and the optimization results were good. In addition, the experiment also proved that the three-category result of the model was accurate, and there was no case of jumping outputs of several eye-movement categories. In the wheelchair control reliability analysis, we obtained the deviation betwe n the target and the actu l trajectory during the movement process through laser ranging and concluded that the accuracy of the motion control and eye-movement model was high. However, the current motion control part is relatively complicated, and the upper limit of optimization is low. If the signal of Jeston Tx2 can be directly output to control the wheelchair, the motion control can be made simpler and more reliable. In addition, the eye-movement dataset was also collected in a specific scene and did not fully consider applications in scenes in daily life, such as crossing the road, rainy days, etc. After the\nSensors 2023, 23, 6239 23 of 25\nexperiment, many volunteers reported that continuous eye-movement control increased the burden on users, which reduced the accuracy of control to a certain extent and increased the risk for users. Considering that the current wheelchair control scheme lacks adaptability to the environment, we expect to add a visual slam and path planning [59] to the control part in the follow-up work. During the process, the direction of sight can be freely controlled, and the wheelchair can autonomously perceive the surrounding environment and make adjustments to reach the destination. For safety reasons, we plan to add a positioning system to the wheelchair. When the user operates the wheelchair, their family members can remotely know the user\u2019s specific location through the mobile phone program.\nAuthor Contributions: J.X., writing\u2014review and editing, funding acquisition; Z.H., methodology, supervision; L.L., software, formal analysis; X.L., visualization; K.W., data curation. All authors have read and agreed to the published version of the manuscript.\nFunding: Heilongjiang Provincial Education Department Scientific Research Fund Project \u201cIntelligent Eye-Movement Wheelchair Based on Deep Learning\u201d, Project No. S202210214067.\nInstitutional Review Board Statement: The study was conducted in accordance with the Declaration of Helsinki and approved by the Institutional Review Board of the Harbin University of Science and Technology (project identification code: R4-12-2, approval code: 56).\nInformed Consent Statement: Informed consent was obtained from all subjects involved in the study.\nData Availability Statement: The research data is available at: https://github.com/crzzx1/dataset (accessed on 1 May 2023).\nConflicts of Interest: The authors declare no conflict of interest.\nReferences 1. Callupe Luna, J.; Martinez Rocha, J.; Monacelli, E.; Foggea, G.; Hirata, Y.; Delaplace, S. WISP, Wearable Inertial Sensor for Online Wheelchair Propulsion Detection. Sensors 2022, 22, 4221. [CrossRef] [PubMed] 2. Li, Y. Hand gesture recognition using kinect. In Proceedings of the International Conference on Software Engineering and Service Science, Beijing, China, 22\u201324 June 2012; IEEE Computer Society: Washington, DC, USA, 2011; pp. 196\u2013199. 3. Adebayo, O.O.; Adetiba, E.; Ajayi, O.T. Hand Gesture Recognition-Based Control of Motorized Wheelchair using Electromyog-\nraphy Sensors and Recurrent Neural Network. In Proceedings of the International Conference on Engineering for Sustainable World (ICESW 2020), Ota, Nigeria, 10\u201314 August 2020; 1107.\n4. Nasare, R.K.; Yenurkar, G.K. Hand gesture based navigation control for automated wheelchair. Int. J. Latest Trends Eng. Technol. 2016, 8, 230\u2013238. 5. Ashley, S.; Jaydip, D. Hand Gesture-based Artificial Neural Network Trained Hybrid Human\u2013machine Interface System to Navigate a Powered Wheelchair. J. Bionic Eng. 2021, 18, 1045\u20131058. 6. Iskanderani, A.I.; Tamim, F.R.; Rana, M.M.; Ahmed, W.; Mehedi, I.M.; Aljohani, A.J.; Latif, A.; Shaikh, S.A.; Shorfuzzaman, M.; Akther, F.; et al. Voice Controlled Artificial Intelligent Smart Wheelchair. In Proceedings of the 2020 8th International Conference on Intelligent and Advanced Systems (ICIAS), Kuching, Malaysia, 13\u201315 July 2021; pp. 1\u20135. [CrossRef] 7. Jayakody, A.; Nawarathna, A.; Wijesinghe, I.; Liyanage, S.; Dissanayake, J. Smart Wheelchair to Facilitate Disabled Individuals. In Proceedings of the 2019 International Conference on Advancements in Computing (ICAC), Malabe, Sri Lanka, 5\u20137 December 2019; pp. 249\u2013254. [CrossRef] 8. Karim, A.B.; Haq, A.; Noor, A.; Khan, B.; Hussain, Z. Raspberry Pi Based Voice Controlled Smart Wheelchair. In Proceedings of the 2022 International Conference on Emerging Trends in Smart Technologies (ICETST), Karachi, Pakistan, 23\u201324 September 2022; pp. 1\u20135. 9. Cao, W.; Yu, H.; Wu, X.; Li, S.; Meng, Q.; Chen, C. Voice controlled wheelchair integration rehabilitation training and posture transformation for people with lower limb motor dysfunction. Technol. Health Care Off. J. Eur. Soc. Eng. Med. 2020, 29, 609\u2013614. [CrossRef] [PubMed] 10. Mokhles, M. Abdulghani et al. Wheelchair Neuro Fuzzy Control and Tracking System Based on Voice Recognition. Sensors 2020, 20, 2872. 11. Cojocaru, D.; Manta, L.F.; Vladu, I.C.; Dragomir, A.; Mariniuc, A.M. Using an Eye Gaze New Combined Approach to Control a Wheelchair Movement. In Proceedings of the 2019 23rd International Conference on System Theory, Control and Computing (ICSTCC), Sinaia, Romania, 9\u201311 October 2019; pp. 626\u2013631. [CrossRef] 12. Wanluk, N.; Visitsattapongse, S.; Juhong, A.; Pintavirooj, C. Smart wheelchair based on eye tracking. In Proceedings of the 2016 9th Biomedical Engineering International Conference (BMEiCON), Laung Prabang, Laos, 7\u20139 December 2016; pp. 1\u20134. [CrossRef]\nSensors 2023, 23, 6239 24 of 25\n13. Luo, W.; Cao, J.; Ishikawa, K.; Ju, D. A Human-Computer Control System Based on Intelligent Recognition of Eye Movements and Its Application in Wheelchair Driving. Multimodal Technol. Interact. 2021, 5, 50. [CrossRef] 14. W\u00e4stlund, E.; Sponseller, K.; Pettersson, O.; Bared, A. Evaluating gaze-driven power wheelchair with navigation support for persons with disabilities. J. Rehabil. Res. Dev. 2015, 52, 815\u2013826. [CrossRef] 15. Bai, D.; Liu, Z.; Hu, Q.; Yang, J.; Yang, G.; Ni, C.; Yang, D.; Zhou, L. Design of an eye movement-controlled wheelchair using Kalman filter algorithm. In Proceedings of the 2016 IEEE International Conference on Information and Automation (ICIA), Ningbo, China, 1\u20133 August 2016; IEEE: Piscateville, NJ, USA, 2017. 16. Gautam, G.; Sumanth, G.; Karthikeyan, K.C.; Sundar, S.; Venkataraman, D. Eye movement based electronic wheel chair for physically challenged persons. Int. J. Sci. Technol. Res. 2014, 3, 206\u2013212. 17. Antoniou, E.; Bozios, P.; Christou, V.; Tzimourta, K.D.; Kalafatakis, K.; G. Tsipouras, M.; Giannakeas, N.; Tzallas, A.T. EEG-Based Eye Movement Recognition Using Brain\u2013Computer Interface and Random Forests. Sensors 2021, 21, 2339. [CrossRef] 18. Rao, E.V.K.; Reddy, N.Y.; Greeshma, B.V.S.S.; Reddy, Y.S.S.V. EEG Based Smart Wheelchair For Disabled Persons Using NonInvasive BCI. In Proceedings of the 2022 International Conference on Computational Intelligence and Sustainable Engineering Solutions (CISES), Greater Noida, India, 20\u201321 May 2022; pp. 440\u2013446. [CrossRef] 19. Ngo, B.V.; Nguyen, T.H.; Tran, D.K.; Vo, D.D. Control of a Smart Electric Wheelchair Based on EEG Signal and Graphical User Interface for Disabled People. In Proceedings of the 2021 International Conference on System Science and Engineering (ICSSE), Ho Chi Minh City, Vietnam, 26\u201328 August 2021; pp. 257\u2013262. [CrossRef] 20. Joshi, K.; Soni, P.; Joshi, S.; Vyas, A.; Joshi, R. Cognitive-Chair: AI based advanced Brain Sensing Wheelchair for Paraplegic/Quadriplegic people. In Proceedings of the 2022 4th International Conference on Artificial Intelligence and Speech Technology (AIST), Delhi, India, 9\u201310 December 2022; pp. 1\u20136. [CrossRef] 21. Fadheel, B.A.; Mahdi, A.J.; Jaafar, H.F.; Nazir, M.S.; Obaid, M.S.; Musa, S.H. Speed Control of a Wheelchair Prototype Driven by a DC Motor Through Real EEG Brain Signals. In Proceedings of the 3rd International Conference on Engineering Sciences, Kerbala, Iraq, 4\u20136 November 2019; 2020; Volume 671, p. 012036. 22. Ferracuti, F.; Freddi, A.; Iarlori, S.; Longhi, S.; Monteri\u00f9, A.; Porcaro, C. Augmenting robot intelligence via EEG signals to avoid trajectory planning mistakes of a smart wheelchair. J. Ambient. Intell. Humaniz. Comput. 2021, 14, 223\u2013235. [CrossRef] 23. Griss, P.; Enoksson, P.; Tolvanen-Laakso, H.K.; Merilainen, P.; Ollmar, S.; Stemme, G. Micromachined electrodes for biopotential measurements. J. Microelectromechanical Syst. 2001, 10, 10\u201316. [CrossRef] 24. Li, X.; Wee, W.G. An efficient method for eye tracking and eye-gazed FOV estimation. In Proceedings of the 2009 16th IEEE International Conference on Image Processing (ICIP), Cairo, Egypt, 7\u201310 November 2009; pp. 2597\u20132600. [CrossRef] 25. Taher, F.B.; Amor, N.B.; Jallouli, M. A multimodal wheelchair control system based on EEG signals and Eye tracking fusion. In Proceedings of the 2015 International Symposium on Innovations in Intelligent SysTems and Applications (INISTA), Madrid, Spain, 2\u20134 September 2015; pp. 1\u20138. [CrossRef] 26. \u00c7etintas\u0327, D.; Firat, T.T. Eye-Tracking Analysis with Deep Learning Method. In Proceedings of the 2021 International Conference on Innovation and Intelligence for Informatics, Computing, and Technologies (3ICT), Zallaq, Bahrain, 29\u201330 September 2021; pp. 512\u2013515. [CrossRef] 27. Mokatren, M.; Kuflik, T.; Shimshoni, I. 3D Gaze Estimation Using RGB-IR Cameras. Sensors 2023, 23, 381. [CrossRef] [PubMed] 28. Hu, L.; Gao, J. Research on real-time distance measurement of mobile eye tracking system based on neural network. In\nProceedings of the 2020 IEEE 5th Information Technology and Mechatronics Engineering Conference (ITOEC), Chongqing, China, 12\u201314 June 2020; pp. 1561\u20131565. [CrossRef]\n29. Dragusin, D.; Baritz, M.I. Development of a System for Correlating Ocular Biosignals to Achieve the Movement of a Wheelchair. In Proceedings of the 2020 International Conference on e-Health and Bioengineering (EHB), Iasi, Romania, 29\u201330 October 2020; pp. 1\u20134. [CrossRef] 30. Solea, R.; Margarit, A.; Cernega, D.; Serbencu, A. Head Movement Control of Powered Wheelchair. In Proceedings of the 2019 23rd International Conference on System Theory, Control and Computing (ICSTCC), Sinaia, Romania, 9\u201311 October 2019; pp. 632\u2013637. [CrossRef] 31. Juhong, A.; Treebupachatsakul, T.; Pintavirooj, C. Smart eye-tracking system. In Proceedings of the 2018 International Workshop on Advanced Image Technology (IWAIT), Chiang Mai, Thailand, 7\u20139 January 2018; pp. 1\u20134. [CrossRef] 32. Higa, S.; Yamada, K.; Kamisato, S. Intelligent Eye-Controlled Electric Wheelchair Based on Estimating Visual Intentions Using One-Dimensional Convolutional Neural Network and Long Short-Term Memory. Sensors 2023, 23, 4028. [CrossRef] [PubMed] 33. Fuhl, W.; Kasneci, G.; Kasneci, E. TEyeD: Over 20 Million Real-World Eye Images with Pupil, Eyelid, and Iris 2D and 3D Segmentations, 2D and 3D Landmarks, 3D Eyeball, Gaze Vector, and Eye Movement Types. In Proceedings of the 2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR), Bari, Italy, 4\u20138 October 2021; pp. 367\u2013375. [CrossRef] 34. Fusek, R.; Sojka, E. Iris Center Localization Using Geodesic Distance and CNN. In Proceedings of the 9th Iberian Conference on Pattern Recognition and Image Analysis, IbPRIA 2019, Madrid, Spain, 1\u20134 July 2019; pp. 76\u201385. Available online: http: //mrl.cs.vsb.cz/eyedataset (accessed on 1 January 2023). 35. Constable, P.A.; Bach, M.; Frishman, L.J.; Jeffrey, B.G.; Robson, A.G. ISCEV Standardbfor Clinical Electro-oculography (EOG) 2019. Doc. Ophthalmol. 2019, 113, 205\u2013212. 36. Robinson, D.A. A Method of Measuring Eye Movement Using a Scleral Search Coil in a Magnetic Field. IEEE Trans. Bio-Med. Electron. 2021, 10, 137\u2013145.\nSensors 2023, 23, 6239 25 of 25\n37. Ohno, T.; Mukawa, N.; Yoshikawa, A. Abstract FreeGaze: A Gaze Tracking System for Everyday Gaze Interaction. In Proceedings of the Eye Tracking Research & Application Symposium, DBLP 2018, Warsaw, Poland, 14\u201317 June 2018; pp. 125\u2013132. 38. Coutinho, F.L.; Morimoto, C.H. Augmenting the robustness of cross-ratio gaze tracking methods to head movement. In Symposium on Eye Tracking Research & Applications; ACM: New York, NY, USA, 2019; pp. 59\u201366. 39. Cerrolaza, J.J.; Villanueva, A.; Cabeza, R. Taxonomic Study of Polynomial Regressions Applied to the Calibration of VideoOculographic Systems. In Proceedings of the 2008 Symposium on Eye Tracking Research & Applications (ETRA 2008), Savannah, GA, USA, 26\u201328 March 2008; pp. 259\u2013266. 40. Hennessey, C.; Lawrence, P. Improving the accurancy and reliability of remote systemcalibration-free eye-gaze tracking. IEEE Trans Biomed. Eng. 2009, 56, 1891\u20131900. [CrossRef] 41. Xu, L.Q.; Machin, D.; Sheppard, P. A Novel Approach to Real-Time Non-Intrusive Gaze Finding. In Proceedings of the BMVC, London, UK, 4\u20137 September 2017. 42. Tan, K.; Ahuja, D.J.K.N. Appearance-based Eye Gaze Estimation. In Proceedings of the IEEE Workshop on Applications of Computer Vision, Nashville, TN, USA, 19\u201325 June 2021; IEEE Computer Society: Washington, DC, USA, 2019. 43. Jeong, C.; Kim, T. Eye Blink Detection Using Algorithm Based On dlib And OpenCV Library for Game Players In Competitive Environments. J. Int. Res. Med. Pharm. Sci. 2021, 16, 33\u201345. 44. Robert, G.; Lucas, C.; Nicholas, L.; Jedediah, M.S.; Alexander, Z. New guidance for using t-SNE: Alternative defaults, hyperparameter selection automation, and comparative evaluation. Vis. Inform. 2022, 6, 87\u201397. 45. Sunsuhi, G.S.; Albin Jose, S. An Adaptive Eroded Deep Convolutional neural network for brain image segmentation and classification using Inception ResnetV2. Biomed. Signal Process. Control. 2022, 78, 103863. [CrossRef] 46. Zhang, J.; Zhu, Y.; Li, W.; Fu, W.; Cao, L. DRNet: A Deep Neural Network with Multi-Layer Residual Blocks Improves Image Denoising. IEEE Access 2021, 9, 79936\u201379946. [CrossRef] 47. Wang, B.; Yan, X.; Li, D. An End-to-End Lane Detection Model with Attention and Residual Block. Comput. Intell. Neurosci. 2022, 2022, 5852891. [CrossRef] 48. Ma, R.; Wang, J.; Zhao, W.; Guo, H.; Dai, D.; Yun, Y.; Li, L.; Hao, F.; Bai, J.; Ma, D. Identification of Maize Seed Varieties Using MobileNetV2 with Improved Attention Mechanism CBAM. Agriculture 2022, 13, 11. [CrossRef] 49. Zhang, T.; Sui, Y.; Wu, S.; Shao, F.; Sun, R. Table Structure Recognition Method Based on Lightweight Network and Channel Attention. Electronics 2023, 12, 673. [CrossRef] 50. Cao, Y. An Expression Recognition Model Based on Channel and Spatial Attention Fusion. J. Phys. Conf. Ser. 2022, 2363, 012016. [CrossRef] 51. Imen, J.; Ihsen, A.; Anouar, B.K.; Ali, M.M. Deep learning-based hard spatial attention for driver in-vehicle action monitoring. Expert Syst. Appl. 2023, 219, 119629. 52. Li, L.; Doroslovacki, M.; Loew, M.H. Approximating the Gradient of Cross-Entropy Loss Function. IEEE Access 2020, 8, 111626\u2013111635. [CrossRef] 53. Huber Holly, A.; Georgia Senta, K.; Finley Stacey, D. Systematic Bayesian Posterior Analysis Guided by Kullback-Leibler Divergence Facilitates Hypothesis Formation. J. Theor. Biol. 2022, 558, 111341. [CrossRef] [PubMed] 54. Akihiro, K.; Kazu, N.; Rin, H.; Hideaki, K.; Yoshihisa, N. Eye fatigue estimation using blink detection based on Eye Aspect Ratio Mapping (EARM). Cogn. Robot. 2022, 2, 50\u201359. 55. Yovan, L.; Kumar, P.; Singh, P.; Chand, J.; Narayana, U.H.; Sarojam, S.B.D.; Keshava, B. Submarine Groundwater Discharge (SGD): Impacts, challenges, limitations, and management recommendations. Groundw. Sustain. Dev. 2023, 21, 100903. 56. Ren, S.; Wu, H.; Chen, W.; Li, D. Polarization Domain Spectrum Sensing Algorithm Based on AlexNet. Sensors 2022, 22, 8946. [CrossRef] 57. Huang, B.; Liu, J.; Zhang, Q.; Liu, K.; Li, K.; Liao, X. Identification and Classification of Aluminum Scrap Grades Based on the Resnet18 Model. Appl. Sci. 2022, 12, 11133. [CrossRef] 58. Wang, H. Residual Mask Based on MobileNet-V2 for Driver\u2019s Dangerous Behavior Recognition. In Proceedings of the 2019 3rd International Conference on Computer Science and Artificial Intelligence, Normal, IL, USA, 6\u20138 December 2019. 59. Durrant-Whyte, H.; Bailey, T. Simultaneous Localization and Mapping (SLAM): Part I The Essential Algorithms. IEEE Robot. Autom. Mag. 2019, 2, 206\u2013215.\nDisclaimer/Publisher\u2019s Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content."
        }
    ],
    "title": "Eye-Gaze Controlled Wheelchair Based on Deep Learning",
    "year": 2023
}