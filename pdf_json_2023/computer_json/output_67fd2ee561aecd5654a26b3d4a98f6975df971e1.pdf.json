{
    "abstractText": "The problem of two-player zero-sum Markov games has recently attracted increasing interests in theoretical studies of multi-agent reinforcement learning (RL). In particular, for finite-horizon episodic Markov decision processes (MDPs), it has been shown that model-based algorithms can find an \u01eb-optimal Nash Equilibrium (NE) with the sample complexity of O(HSAB/\u01eb), which is optimal in the dependence of the horizon H and the number of states S (where A and B denote the number of actions of the two players, respectively). However, none of the existing model-free algorithms can achieve such an optimality. In this work, we propose a model-free stage-based Q-learning algorithm and show that it achieves the same sample complexity as the best model-based algorithm, and hence for the first time demonstrate that model-free algorithms can enjoy the same optimality in the H dependence as model-based algorithms. The main improvement of the dependency on H arises by leveraging the popular variance reduction technique based on the reference-advantage decomposition previously used only for single-agent RL. However, such a technique relies on a critical monotonicity property of the value function, which does not hold in Markov games due to the update of the policy via the coarse correlated equilibrium (CCE) oracle. Thus, to extend such a technique to Markov games, our algorithm features a key novel design of updating the reference value functions as the pair of optimistic and pessimistic value functions whose value difference is the smallest in the history in order to achieve the desired improvement in the sample efficiency.",
    "authors": [
        {
            "affiliations": [],
            "name": "Songtao Feng"
        },
        {
            "affiliations": [],
            "name": "Ming Yin"
        },
        {
            "affiliations": [],
            "name": "Yu-Xiang Wang"
        },
        {
            "affiliations": [],
            "name": "Jing Yang"
        },
        {
            "affiliations": [],
            "name": "Yingbin Liang"
        }
    ],
    "id": "SP:86f915bf783863861df4eea8afa8099cfebb7548",
    "references": [
        {
            "authors": [
                "Peter Auer",
                "Thomas Jaksch",
                "Ronald Ortner"
            ],
            "title": "Near-optimal regret bounds for reinforcement learning",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2008
        },
        {
            "authors": [
                "Mohammad Gheshlaghi Azar",
                "Ian Osband",
                "R\u00e9mi Munos"
            ],
            "title": "Minimax regret bounds for reinforcement learning",
            "venue": "In Proceedings of the 34th International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "Yu Bai",
                "Chi Jin"
            ],
            "title": "Provable self-play algorithms for competitive reinforcement learning",
            "venue": "In Proceedings of the 37th International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Yu Bai",
                "Chi Jin"
            ],
            "title": "Provable self-play algorithms for competitive reinforcement learning",
            "venue": "In Proceedings of the 37th International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Yu Bai",
                "Chi Jin",
                "Tiancheng Yu"
            ],
            "title": "Near-optimal reinforcement learning with self-play",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Bowen Baker",
                "Ingmar Kanitscheider",
                "Todor Markov",
                "Yi Wu",
                "Glenn Powell",
                "Bob McGrew",
                "Igor Mordatch"
            ],
            "title": "Emergent tool use from multi-agent autocurricula",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Fan Chen",
                "Song Mei",
                "Yu Bai"
            ],
            "title": "Unified Algorithms for RL with Decision-Estimation Coefficients: No-Regret, PAC, and Reward-Free Learning",
            "year": 2022
        },
        {
            "authors": [
                "Zixiang Chen",
                "Dongruo Zhou",
                "Quanquan Gu"
            ],
            "title": "Almost optimal algorithms for two-player zero-sum linear mixture markov games",
            "venue": "In Proceedings of The 33rd International Conference on Algorithmic Learning",
            "year": 2022
        },
        {
            "authors": [
                "Qiwen Cui",
                "Simon S Du"
            ],
            "title": "When are offline two-player zero-sum markov games solvable",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Qiwen Cui",
                "Kaiqing Zhang",
                "Simon S. Du"
            ],
            "title": "Breaking the Curse of Multiagents in a Large State Space: RL in Markov Games with Independent Linear Function Approximation",
            "venue": "arXiv e-prints,",
            "year": 2023
        },
        {
            "authors": [
                "Christoph Dann",
                "Tor Lattimore",
                "Emma Brunskill"
            ],
            "title": "Unifying PAC and regret: Uniform PAC bounds for episodic reinforcement learning",
            "venue": "In Proceedings of the 31st International Conference on Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Constantinos Daskalakis",
                "Noah Golowich",
                "Kaiqing Zhang"
            ],
            "title": "The Complexity of Markov Equilibrium in Stochastic Games",
            "venue": "arXiv e-prints,",
            "year": 2022
        },
        {
            "authors": [
                "Songtao Feng",
                "Ming Yin",
                "Ruiquan Huang",
                "Yu-Xiang Wang",
                "Jing Yang",
                "Yingbin Liang"
            ],
            "title": "Nonstationary reinforcement learning under general function approximation",
            "venue": "In Proceedings of the 40th International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "J. Filar",
                "K. Vrieze"
            ],
            "title": "Competitive Markov Decision Processes",
            "venue": "Springer",
            "year": 1997
        },
        {
            "authors": [
                "Thomas Dueholm Hansen",
                "Peter Bro Miltersen",
                "Uri Zwick"
            ],
            "title": "Strategy iteration is strongly polynomial for 2-player turn-based stochastic games with a constant discount factor",
            "venue": "J. ACM,",
            "year": 2013
        },
        {
            "authors": [
                "Junling Hu",
                "Michael P. Wellman"
            ],
            "title": "Nash Q-learning for general-sum stochastic games",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2003
        },
        {
            "authors": [
                "Baihe Huang",
                "Jason D. Lee",
                "Zhaoran Wang",
                "Zhuoran Yang"
            ],
            "title": "Towards general function approximation in zero-sum markov games",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Zeyu Jia",
                "Lin F. Yang",
                "Mengdi Wang"
            ],
            "title": "Feature-Based Q-Learning for Two-Player Stochastic Games",
            "venue": "arXiv e-prints,",
            "year": 2019
        },
        {
            "authors": [
                "Chi Jin",
                "Zeyuan Allen-Zhu",
                "Sebastien Bubeck",
                "Michael I Jordan"
            ],
            "title": "Is Q-learning provably efficient",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Chi Jin",
                "Qinghua Liu",
                "Yuanhao Wang",
                "Tiancheng Yu"
            ],
            "title": "V-learning \u2013 a simple, efficient, decentralized algorithm for multiagent RL",
            "venue": "In ICLR 2022 Workshop on Gamification and Multiagent Solutions,",
            "year": 2022
        },
        {
            "authors": [
                "Chi Jin",
                "Qinghua Liu",
                "Tiancheng Yu"
            ],
            "title": "The power of exploiter: Provable multi-agent RL in large state spaces",
            "venue": "In Proceedings of the 39th International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Gen Li",
                "Yuejie Chi",
                "Yuting Wei",
                "Yuxin Chen"
            ],
            "title": "Minimax-optimal multi-agent RL in markov games with a generative model",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Michael L. Littman"
            ],
            "title": "Markov games as a framework for multi-agent reinforcement learning",
            "venue": "In Proceedings of the Eleventh International Conference on International Conference on Machine Learning,",
            "year": 1994
        },
        {
            "authors": [
                "Qinghua Liu",
                "Tiancheng Yu",
                "Yu Bai",
                "Chi Jin"
            ],
            "title": "A sharp analysis of model-based reinforcement learning with self-play",
            "venue": "In Proceedings of the 38th International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Weichao Mao",
                "Tamer Ba\u015far"
            ],
            "title": "Provably Efficient Reinforcement Learning in Decentralized General- Sum Markov Games",
            "venue": "arXiv e-prints,",
            "year": 2021
        },
        {
            "authors": [
                "Weichao Mao",
                "Lin Yang",
                "Kaiqing Zhang",
                "Tamer Basar"
            ],
            "title": "On improving model-free algorithms for decentralized multi-agent reinforcement learning",
            "venue": "In Proceedings of the 39th International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Chengzhuo Ni",
                "Yuda Song",
                "Xuezhou Zhang",
                "Zihan Ding",
                "Chi Jin",
                "Mengdi Wang"
            ],
            "title": "Representation learning for low-rank general-sum markov games",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Shai Shalev-Shwartz",
                "Shaked Shammah",
                "Amnon Shashua"
            ],
            "title": "Safe, Multi-Agent, Reinforcement Learning for Autonomous Driving",
            "venue": "arXiv e-prints,",
            "year": 2016
        },
        {
            "authors": [
                "Lloyd S. Shapley"
            ],
            "title": "Stochastic games",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 1953
        },
        {
            "authors": [
                "Aaron Sidford",
                "Mengdi Wang",
                "Lin Yang",
                "Yinyu Ye"
            ],
            "title": "Solving discounted stochastic two-player games with near-optimal time and sample complexity",
            "venue": "In Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics,",
            "year": 2020
        },
        {
            "authors": [
                "David Silver",
                "Aja Huang",
                "Chris J Maddison",
                "Arthur Guez",
                "Laurent Sifre",
                "George Van Den Driessche",
                "Julian Schrittwieser",
                "Ioannis Antonoglou",
                "Veda Panneershelvam",
                "Marc Lanctot"
            ],
            "title": "Mastering the game of go with deep neural networks and tree",
            "venue": "search. nature,",
            "year": 2016
        },
        {
            "authors": [
                "David Silver",
                "Julian Schrittwieser",
                "Karen Simonyan",
                "Ioannis Antonoglou",
                "Aja Huang",
                "Arthur Guez",
                "Thomas Hubert",
                "Lucas baker",
                "Matthew Lai",
                "Adrian Bolton",
                "Yutian Chen",
                "Timothy P. Lillicrap",
                "Fan Hui",
                "L. Sifre",
                "George van den Driessche",
                "Thore Graepel",
                "Demis Hassabis"
            ],
            "title": "Mastering the game of go without human knowledge",
            "year": 2017
        },
        {
            "authors": [
                "Ziang Song",
                "Song Mei",
                "Yu Bai"
            ],
            "title": "When can we learn general-sum markov games with a large number of players sample-efficiently",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Oriol Vinyals",
                "Igor Babuschkin",
                "Wojciech M. Czarnecki",
                "Micha\u00ebl Mathieu",
                "Andrew Dudzik",
                "Junyoung Chung",
                "David H. Choi",
                "Richard Powell",
                "Timo Ewalds",
                "Petko Georgiev",
                "Junhyuk Oh",
                "Dan Horgan",
                "Manuel Kroiss",
                "Ivo Danihelka",
                "Aja Huang",
                "L. Sifre",
                "Trevor Cai",
                "John P. Agapiou",
                "Max Jaderberg",
                "Alexander Sasha Vezhnevets",
                "R\u00e9mi Leblond",
                "Tobias Pohlen",
                "Valentin Dalibard",
                "David Budden",
                "Yury Sulsky",
                "James Molloy",
                "Tom Le Paine",
                "Caglar Gulcehre",
                "Ziyun Wang",
                "Tobias Pfaff",
                "Yuhuai Wu",
                "Roman Ring",
                "Dani Yogatama",
                "Dario W\u00fcnsch",
                "Katrina McKinney",
                "Oliver Smith",
                "Tom Schaul",
                "Timothy P. Lillicrap",
                "Koray Kavukcuoglu",
                "Demis Hassabis",
                "Chris Apps",
                "David Silver"
            ],
            "title": "Grandmaster level in starcraft II using multi-agent reinforcement learning",
            "venue": "Nature, pages",
            "year": 2019
        },
        {
            "authors": [
                "Yuanhao Wang",
                "Qinghua Liu",
                "Yu Bai",
                "Chi Jin"
            ],
            "title": "Breaking the Curse of Multiagency: Provably Efficient Decentralized Multi-Agent RL with Function Approximation",
            "venue": "arXiv e-prints,",
            "year": 2023
        },
        {
            "authors": [
                "Chen-Yu Wei",
                "Yi-Te Hong",
                "Chi-Jen Lu"
            ],
            "title": "Online reinforcement learning in stochastic games",
            "venue": "In Proceedings of the 31st International Conference on Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Chen-Yu Wei",
                "Chung-Wei Lee",
                "Mengxiao Zhang",
                "Haipeng Luo"
            ],
            "title": "Linear Last-iterate Convergence in Constrained Saddle-point Optimization",
            "venue": "arXiv e-prints,",
            "year": 2020
        },
        {
            "authors": [
                "Qiaomin Xie",
                "Yudong Chen",
                "Zhaoran Wang",
                "Zhuoran Yang"
            ],
            "title": "Learning zero-sum simultaneous-move markov games using function approximation and correlated equilibrium",
            "venue": "In Proceedings of Thirty Third Conference on Learning Theory,",
            "year": 2020
        },
        {
            "authors": [
                "Wei Xiong",
                "Han Zhong",
                "Chengshuai Shi",
                "Cong Shen",
                "Tong Zhang"
            ],
            "title": "A self-play posterior sampling algorithm for zero-sum Markov games",
            "venue": "In Proceedings of the 39th International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Ming Yin",
                "Yu Bai",
                "Yu-Xiang Wang"
            ],
            "title": "Near-optimal provable uniform convergence in offline policy evaluation for reinforcement learning",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2021
        },
        {
            "authors": [
                "Ming Yin",
                "Yu-Xiang Wang"
            ],
            "title": "Towards instance-optimal offline reinforcement learning with pessimism",
            "venue": "Advances in neural information processing systems,",
            "year": 2021
        },
        {
            "authors": [
                "Wenhao Zhan",
                "Jason D. Lee",
                "Zhuoran Yang"
            ],
            "title": "Decentralized optimistic hyperpolicy mirror descent: Provably no-regret learning in markov games",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Kaiqing Zhang",
                "Sham M. Kakade",
                "Tamer Ba\u015far",
                "Lin F. Yang"
            ],
            "title": "Model-based multi-agent RL in zero-sum markov games with near-optimal sample complexity",
            "venue": "In Proceedings of the 34th International Conference on Neural Information Processing Systems,",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "ar X\niv :2\n30 8.\n08 85\n8v 1\n[ cs\n.L G\n] 1\n7 A\nug 2\nThe problem of two-player zero-sum Markov games has recently attracted increasing interests in theoretical studies of multi-agent reinforcement learning (RL). In particular, for finite-horizon episodic Markov decision processes (MDPs), it has been shown that model-based algorithms can find an \u01eb-optimal Nash Equilibrium (NE) with the sample complexity of O(H3SAB/\u01eb2), which is optimal in the dependence of the horizon H and the number of states S (where A and B denote the number of actions of the two players, respectively). However, none of the existing model-free algorithms can achieve such an optimality. In this work, we propose a model-free stage-based Q-learning algorithm and show that it achieves the same sample complexity as the best model-based algorithm, and hence for the first time demonstrate that model-free algorithms can enjoy the same optimality in the H dependence as model-based algorithms. The main improvement of the dependency on H arises by leveraging the popular variance reduction technique based on the reference-advantage decomposition previously used only for single-agent RL. However, such a technique relies on a critical monotonicity property of the value function, which does not hold in Markov games due to the update of the policy via the coarse correlated equilibrium (CCE) oracle. Thus, to extend such a technique to Markov games, our algorithm features a key novel design of updating the reference value functions as the pair of optimistic and pessimistic value functions whose value difference is the smallest in the history in order to achieve the desired improvement in the sample efficiency.\n1 Introduction\nMulti-agent reinforcement learning (MARL) commonly refers to the sequential decision making framework, in which more than one agent learn to make decisions in an unknown shared environment to maximize their cumulative rewards. MARL has achieved great success in a variety of practical applications, including the game of GO [31, 32], real-time strategy games involving team play [34], autonomous driving [28], and behavior learning in complex social scenarios [6]. Despite the great empirical success, one major bottleneck for many\n\u2217Department of Electrical and Computer Engineering, The Ohio State University, OH 43210, USA; e-mail: feng.1359@osu.edu \u2020Department of Computer Science, UC Santa Barbara, CA 93106, USA; e-mail: ming_yin@ucsb.edu \u2021Department of Computer Science, UC Santa Barbara, CA 93106, USA; e-mail: yuxiangw@cs.ucsb.edu \u00a7School of Electrical Engineering and Computer Science, The Pennsylvania State University, University Park, PA 16802, USA; e-mail: yangjing@psu.edu \u00b6Department of Electrical and Computer Engineering, The Ohio State University, OH 43210, USA; e-mail: liang.889@osu.edu\nRL algorithms is that they require enormous samples. For example, in many practical MARL scenarios, a large number of samples are often required to achieve human-like performance due to the necessity of exploration. It is thus important to understand how to design sample-efficient algorithms.\nAs a prevalent approach to the MARL, model-based methods use the existing visitation data to estimate the model, run a planning algorithm on the estimated model to obtain the policy, and execute the policy in the environment. In two-player zero-sum Markov games, an extensive series of studies [3, 24, 43] have shown that model-based algorithms are provably efficient in MARL, and can achieve minimax-optimal sample complexity O(H3SAB/\u01eb2) except for the term AB [24, 43], where H denotes the horizon, S denotes the number of states, and A and B denote the numbers of actions of the two players, respectively. On the other hand, model-free methods directly estimate the (action-)value functions at the equilibrium policies instead of estimating the model. However, none of the existing model-free algorithms can achieve the aforementioned optimality (attained by model-based algorithms) [5, 20, 25, 26, 33]. Specifically, the number of episodes required for model-free algorithms scales sub-optimally in step H , which naturally motivates the following open question:\nCan we design model-free algorithms with the optimal sample dependence on the time\nhorizon\nfor learning two-player zero-sum Markov games?\nIn this paper, we give an affirmative answer to the above question. We highlight our main contributions\nas follows. Algorithm design. We design a new model-free algorithm of Q-learning with min-gap based referenceadvantage decomposition. In particular, we extend the reference-advantage decomposition technique [44] proposed for single-agent RL to zero-sum Markov games with the following key novel design. Unlike the single-agent scenario, the optimistic (or pessimistic) value function in Markov games does not necessarily preserve the monotone property due to the nature of the CCE oracle. In order to obtain the \u201cbest\" optimistic and pessimistic value function pair, we update the reference value functions as the pair of optimistic and pessimistic value functions whose value difference is the smallest (i.e., with the minimal gap) in the history. Moreover, our algorithm relies on the stage-based approach, which simplifies the algorithm design and subsequent analysis. Sample complexity bound. We show that our algorithm provably finds an \u01eb-optimal Nash equilibrium for the two-player zero-sum Markov game in O\u0303(H3SAB/\u01eb2) episodes, which improves upon the sample complexity of all existing model-free algorithms for zero-sum Markov game. Further, comparison to the existing lower bound shows that it is minimax-optimal on the dependence of H , S and \u01eb. This is the first result that establishes such optimality for model-free algorithms, although model-based algorithms have been shown to achieve such optimality in the past [24]. Technical analysis. We establish a few new properties on the cumulative occurrence of the large V-gap and the cumulative bonus term to enable the upper-bounding of several new error terms arising due to the incorporation of the new min-gap based reference-advantage decomposition technique. These properties have not been established for the single-agent RL with such a technique, because our properties are established for policies generated by the CCE oracle in zero-sum Markov games. Further, the analysis of both the optimistic and pessimistic accumulative bonus terms requires a more refined analysis compared to their counterparts in single-agent RL [44].\n1.1 Related Work\nMarkov games. The Markov game, also known as the stochastic game, was first proposed in [29] to model the multi-agent RL. Early attempts to find the Nash equilibra of Markov games include [15, 16, 23, 37]. However, they often relied on strong assumptions such as known transition matrix and reward, or focused on the asymptotic setting. Thus, these results do not apply to the non-asymptotic setting where the transition and reward are unknown and only limited data is available.\nThere is a line of works focusing on non-asymptotic guarantees with certain reachability assumptions. A popular approach is to assume access to simulators, which enables the agent to sample transition and reward directly for any state-action pair [18, 22, 30, 43]. Alternatively, [36] studied the Markov game under the assumption that one player can always reach all states by playing certain policy no matter what strategy the other player sticks to. Two-player zero-sum games. [3, 38] initialized the study of non-asymptotic guarantee for two-player zero-sum Markov games without reachability assumptions. [3] proposed a model-based algorithm for tabular Markov game while [38] considered linear function approximation in game and adopted a model-free approach. [24] proposed a model-based algorithm which achieves the minimax-optimal samples complexity O(H3SAB/\u01eb) except for the AB term. For the discounted setting and having access to a generative model, [43] developed a model-based algorithm that achieves the minimax-optimal sample complexity except for the AB term. Then, model-free Nash Q-learning and Nash V-learning were proposed in [5] for two-player zero-sum game to achieve optimal dependence on actions (i.e., (A + B) instead of AB). Further, [8, 17] studied the two-player zero-sum game under linear and general function approximation. Multi-player general-sum games. [24] developed model-free algorithm in episodic setting, which suffers from the curse of multi-agent. To alleviate this issue, [20, 25, 26, 33] proposed V-learning algorithm, coupled with the adversarial bandit subroutine, to break the curse of multi-agent. [25] considered learning an \u01eboptimal CCE and used V-learning with stabilized online mirror descent as the adversarial bandit subroutine. Both [20, 33] utilized the weighted follow the regularized leader (FTRL) algorithm as the adversarial subroutine, and considered \u01eb-optimal CCE and \u01eb-optimal correalted equilibrium (CE). The work [26] featured the standard uniform weighted FTRL and staged-based design, both of which simplifies the algorithm design and the corresponding analysis. While the V-learning algorithms generate non-Markov, history dependent policies, [12, 35] learned an approximate CCEs that is guaranteed to be Markov. Markov games with function approximation. Recently, a few works considered learning in Markov games with linear function approximation [8, 38] and general function approximation [7, 17, 21, 27, 39, 42]. While all of the works require centralized function classes and suffer from the curse of multi-agency, [10, 35] proposed decentralized MARL algorithms to resolve the issue under linear and general function approximation. Single-agent RL. Broadly speaking, our work is also related to single-agent RL [1, 2, 11, 19, 40, 44]. As a special case of Markov games, only one agent interacts with the environment in single-agent RL. For tabular episodic setting, the minimax-optimal sample complexity is O\u0303(H3SA/\u01eb2), achieved by a model-based algorithm in [2] and a model-free algorithm in [44]. Technically, the reference-advantage decomposition used in our algorithm is similar to that of [44], as both employ variance reduction techniques for faster convergence. However, our approaches differ significantly, particularly in the way of handling the interplay between the CCE oracle and the reference-advantage decomposition in the context of two-player zero-sum Markov game.\n2 Preliminaries\nWe consider the tabular episodic two-player zero-sum Markov game MG(H,S,A,B, P, r), where H is the number of steps in each episode, S is the set of states with |S| = S, (A,B) are the sets of actions of the max-player and the min-player respectively with |A| = A and |B| = B, P = {Ph}h\u2208[H] is the collection of the transition matrices with Ph : S \u00d7 A \u00d7 B 7\u2192 S, r = {rh}h\u2208[H] is the collection of deterministic reward functions with rh : S \u00d7A\u00d7 B 7\u2192 [0, 1]. Here the reward represents both the gain of the max-player and the loss of the min-player. We assume each episode starts with a fixed initial state s1.\nSuppose the max-player and the min-player interact with the environment sequentially captured by the Markov game MG(H,S,A,B, P, r). At each step h \u2208 [H ], both players observe the state sh \u2208 S, take their actions ah \u2208 A and bh \u2208 B simultaneously, receive the reward rh(sh, ah, bh), and then the Markov game evolves into the next state sh+1 \u223c Ph(\u00b7|sh, ah, bh). The episode ends when sH+1 is reached. Markov policy, value function. A Markov policy \u00b5 of the max-player is the collection of the functions {\u00b5h : S 7\u2192 \u2206A}h\u2208[H], each of which maps from a state to a distribution over actions. Similarly, a policy \u03bd of the min-player is the collection of functions {\u03bdh : S 7\u2192 \u2206B}h\u2208[H]. We use \u00b5h(a|s) and \u03bdh(b|s) to denote the probability of taking actions a and b given the state s under the Markov policies \u00b5 and \u03bd at step h, respectively.\nGiven a max-player policy \u00b5, a min-player policy \u03bd, and a state s at step h, the value function is defined\nas\nV \u00b5,\u03bdh (s) = E (sh\u2032 ,ah\u2032 ,bh\u2032 )\u223c(\u00b5,\u03bd)\n[ H\u2211\nh\u2032=h\nrh\u2032(sh\u2032 , ah\u2032 , bh\u2032) \u2223\u2223\u2223\u2223\u2223sh = s ] .\nFor a given (s, a, b) \u2208 S \u00d7A\u00d7B under a max-player policy \u00b5 and a min-player policy \u03bd at step h, we define\nQ\u00b5,\u03bdh (s, a, b) = E (sh\u2032 ,ah\u2032 ,bh\u2032 )\u223c(\u00b5,\u03bd)\n[ H\u2211\nh\u2032=h\nrh\u2032(sh\u2032 , ah\u2032 , bh\u2032) \u2223\u2223\u2223\u2223\u2223sh = s, ah = a, bh = b ] .\nFor ease of exposition, we define (Phf)(s, a, b) = Es\u2032\u223cPh(\u00b7|s,a,b)[f(s \u2032)] for any function f : S 7\u2192 R, and (D\u03c0g)(s) = E(a,b)\u223c\u03c0(\u00b7,\u00b7|s)[g(s, a, b)] for any function g : S \u00d7 A \u00d7 B. Then, the following Bellman equations hold for all (s, a, b, h) \u2208 S \u00d7A\u00d7 B \u00d7 [H ]:\nQ\u00b5,\u03bdh (s, a, b) = (rh + PhV \u00b5,\u03bd h+1)(s, a, b), V \u00b5,\u03bd h (s) = (D\u00b5h\u00d7\u03bdhQ \u00b5,\u03bd h )(s), V \u00b5,\u03bd H+1(s) = 0.\nBest response, Nash equilibrium (NE). For any Markov policy \u00b5 of the max-player, there exists a best response of the min-player, which is a policy \u03bd\u2020(\u00b5) satisfying V \u00b5,\u03bd \u2020(\u00b5)\nh (s) = inf\u03bd V \u00b5,\u03bd h for any (s, h)\u00d7S \u00d7 [H ].\nWe denote V \u00b5,\u2020h = V \u00b5,\u03bd\u2020(\u00b5) h . Similarly, the best response of the max-player with respect to the Markov policy \u03bd of the min-player is a policy \u00b5\u2020(\u03bd) satisfying V \u00b5 \u2020(\u03bd),\u03bd\nh (s) = sup\u00b5 V \u00b5,\u03bd h for any (s, h)\u00d7 S \u00d7 [H ], and we use\nV \u2020,\u03bdh to denote V \u00b5\u2020(\u03bd),\u03bd h . Further, there exists Markov policies \u00b5 \u2217, \u03bd\u2217, which are optimal against the best responses of the other player [14], i.e.,\nV \u00b5 \u2217,\u2020 h (s) = sup \u00b5 V \u00b5,\u2020h (s), V \u2020,\u03bd h (s) = inf\u03bd V \u2020,\u03bdh ,\nfor all (s, h) \u2208 S \u00d7 [H ]. We call the strategies (\u00b5\u2217, \u03bd\u2217) the Nash equilibrium of a Markov game, if they satisfy the following minimax equation\nsup \u00b5 inf \u03bd V \u00b5,\u03bdh (s) = V\n\u00b5\u2217,\u03bd\u2217\nh (s) = inf\u03bd sup \u00b5 V \u00b5,\u03bdh (s).\nLearning objective. We consider the Nash equilibrium of Markov games. We measure the sub-optimality of any pair of general policies (\u00b5, \u03bd) using the following gap between their performance and the performance of the optimal strategy (i.e., Nash equilibrium) when playing against the best responses respectively:\nV \u2020,\u03bd1 (s1)\u2212 V \u00b5,\u20201 (s1) = ( V \u2020,\u03bd1 (s1)\u2212 V \u22171 (s1) ) + ( V \u22171 (s1)\u2212 V \u00b5,\u20201 (s1) ) .\nDefinition 2.1 (\u01eb-optimal Nash equilibrium (NE)). A pair of general policies (\u00b5, \u03bd) is an \u01eb-optimal Nash equilibrium if V \u2020,\u03bd1 (s1)\u2212 V \u00b5,\u20201 (s1) \u2264 \u01eb.\nOur goal is to design algorithms for two-player zero-sum Markov games that can find an \u01eb-optimal NE\nusing a number episodes that is small in its dependency on S,A,B,H as well as 1/\u01eb.\n3 Algorithm Design\nIn this section, we propose an algorithm of Q-learning with min-gap based reference-advantage decomposition, as detailed in Algorithm 1, for learning \u01eb-optimal NE in two-player zero-sum Markov games. Our algorithm builds upon the Nash Q-learning framework [5] but incorporates a novel min-gap based reference-advantage decomposition technique and stage-based update design. We update the (action-)value functions at the end of each stage, and the policy is computed by the CCE oracle (see below). Min-gap based reference-advantage decomposition. We start by reviewing the Q-learning algorithm proposed in [44] for single-agent RL. In single-agent RL, we greedily select an action to maximize the action-value function Qh(s, a), and obtain the optimistic value function V h(s) = maxaQh(s, a). One key observation is that the optimistic value function V h(s) preserves the monotonic structure if the optimistic action-value function Qh(s) is non-increasing, since V k+1 h (s) = maxaQ k+1 h (s, a) \u2264 maxaQ k h(s, a) = V k h(s). Then, the reference value is updated as the latest optimistic value function, which we remark is also the smallest value function in the up-to-date learning history.\nIn the two-player zero-sum game, we keep track of both the optimistic and the pessimistic action-value functions, and update the value functions using the CCE oracle at the end of each stage. Unlike the singleagent scenario, the optimistic (or pessimistic) value function does not necessarily preserve the monotone property even if the optimistic (or pessimistic) action-value function is non-increasing (or non-decreasing) due to the nature of the CCE oracle. In order to obtain the \u201cbest\u201d optimistic and pessimistic value function pair, we come up with a key novel design, where we update the reference value functions (line 17-20) as the pair of optimistic and pessimistic value functions whose value difference is the smallest in the history.\nNow we introduce reference-advantage decomposition to the two-player zero-sum game. We focus on the reference-advantage decomposition for the optimistic value functions in the following, and the decomposition for the pessimistic value functions proceeds similarly. In standard update rule, we have Qh(s, a, b) \u2190 rh(s, a, b)+ \u0302PhV h+1(s, a, b)+bonus where \u0302PhV h+1 is the empirical estimate of PhV h+1. One major restriction of such an estimate is that the earlier the samples are collected, the more deviation one would expect between the V h+1 learned at that moment and the value of Nash equilibrium. In order not to ruin the whole estimate, we have to use only the samples from the last stage (i.e., the latest O(1/H) fraction of samples, see stagebased update approach below) to estimate PhV h+1. Besides the standard update rule, we incorporate the reference-advantage decomposition in our update rule, inspired by [44] for single-agent RL, which is the key to reducing the horizon dependence. At high level, we aim to learn a \u03b2-optimal estimate of the value of Nash equilibrium V \u2217 and call it as the reference value function V ref . The accuracy level \u03b2 is small and\nindependent of K. Now our reference-advantage decomposition gives\nQh(s, a, b)\u2190 rh(s, a, b) + \u0302 PhV ref h+1(s, a, b) + \u0302 Ph(V h+1 \u2212 V ref h+1)(s, a, b) + bonus,\nwhere the two middle terms are empirical estimates of PhV ref h+1 and Ph(V h+1\u2212V ref h+1) respectively, and bonus is defined in line 10-11. For ease of exposition, assume we have access to a \u03b2-optimal V ref . For the third term, we still use the samples from the last stage to control the deviation error. However, since V is learned based on V ref\nh+1, and V ref is already an accurate estimate of V \u2217, it turns out that estimating V \u2212V ref instead of directly estimating V offsets the weakness of using only O(1/H) fraction of data. Further, since V ref is fixed, we are able to use all samples collected to estimate the second term, without suffering any deviation. Now we remove the assumption that V ref is fixed. Note that \u03b2 is selected independently of K. Therefore, learning a \u03b2-optimal reference value function V ref only incurs lower order terms in our final result. Stage-based update approach. For each tuple (s, a, b, h) \u2208 S \u00d7 A \u00d7 B \u00d7 [H ], we divide the visitations for the tuple into consecutive stages. The length of each stage increases exponentially with a growth rate (1 + 1/H). Specifically, we define e1 = H , and ei+1 = \u230a(1 + 1/H)ei\u230b for all i \u2265 1, to denote the lengths of stages. Further, we also define L = {\u2211ji=1 ei|j = 1, 2, 3, . . .} to denote the the set of ending indices of the stages. For each (s, a, b, h) tuple, we update both the optimistic and pessimistic value estimates at the end of each stage (i.e., when the total number of visitations of (s, a, b, h) lies in L), using samples only from this single stage (line 8-21). This updating rule ensures that only the last O(1/H) fraction of the collected samples are used to estimate the value estimates. Coarse correlated equilibrium (CCE). We use the CCE oracle to update the policy (line 14). The CCE oracle was first introduced in [38]. For any pair of matrices Q,Q \u2208 [0, H ]A\u00d7B, CCE(Q,Q) returns a distribution \u03c0 \u2208 \u2206A\u00d7B such that\nE(a,b)\u223c\u03c0Q(a, b) \u2265 sup a\u2217 E(a,b)\u223c\u03c0Q(a \u2217, b), E(a,b)\u223c\u03c0Q(a, b) \u2264 inf b\u2217 E(a,b)\u223c\u03c0Q(a, b \u2217).\nThe players choose their actions in a potentially correlated way so that no one can benefit from unilateral unconditional deviation. Since Nash equilibrium is also a CCE and a Nash equilibrium always exists, a CCE therefore always exist. Moreover, CCE can be efficiently implemented by linear programming in polynomial time. We remark that the policies generated by CCE are in general correlated, and executing such policies requires the cooperation of the two players (line 6). Algorithm description. c1, c2, c3 are some sufficiently large universal constants so that the concentration inequalities can be applied in the analysis. Besides the standard optimistic and pessimistic value estimates Qh(s, a, b), V h(s), Qh(s, a, b), V h(s), and the reference value functions V ref h (s), V ref h (s), the algorithm keeps multiple different accumulators to facilitate the update: 1) Nh(s, a, b) and N\u030ch(s, a, b) are used to keep the total visit number and the visits counting for the current stage with respect to (s, a, b, h), respectively. 2) Intra-stage accumulators are used in the latest stage and are reset at the beginning of each stage. The update rule of the intra-stage accumulators are as follows:\nv\u030ch(sh, ah, bh) +\u2190 V h+1(sh+1), v\u030ch(sh, ah, bh) +\u2190 V h+1(sh+1), (1) \u00b5\u030ch(sh, ah, bh) +\u2190 V h+1(sh+1)\u2212 V ref h+1(sh+1), \u00b5\u030ch(sh, ah, bh) +\u2190 V h+1(sh+1)\u2212 V refh+1(sh+1), (2) \u03c3\u030ch(sh, ah, bh) +\u2190 (V h+1(sh+1)\u2212 V ref h+1(sh+1)) 2, \u03c3\u030ch(sh, ah, bh) +\u2190 (V h+1(sh+1)\u2212 V refh+1(sh+1))2. (3)\n3) The following global accumulators are used for the samples in all stages:\n\u00b5refh (sh, ah, bh) +\u2190 V refh+1(sh+1), \u00b5refh (sh, ah, bh) +\u2190 V refh+1(sh+1), (4)\nAlgorithm 1 Q-learning with min-gap based reference-advantage decomposition\n1: Initialize: Set all accumulators to 0. For all (s, a, b, h) \u2208 S \u00d7 A \u00d7 B \u00d7 [H ], set V h(s), Qh(s, a, b) to H \u2212 h+ 1, set V refh (s) to H , set V h(s), Qh(s, a, b), V ref h (s, a, b) to 0; and 2: let \u03c0h(s) \u223c Unif(A)\u00d7Unif(B), \u2206(sh) = H , V\u0303 h(sh) = H , V\u0303 h(sh) = 0. 3: for episodes k \u2190 1, 2, . . . ,K do 4: Observe s1. 5: for h\u2190 1, 2, . . . , H do 6: Take action (ah, bh)\u2190 \u03c0h(sh), receive rh(sh, ah, bh), and observe sh+1. 7: Update accumulators n := Nh(sh, ah, bh)\n+\u2190 1, n\u030c := N\u030ch(sh, ah, bh) +\u2190 1 and (1)-(5). 8: if n \u2208 L then 9: \u03b3 \u2190 2 \u221a H2\nn\u030c \u03b9.\n10: \u03b2 \u2190 c1\n\u221a\n\u03c3ref/n\u2212(\u00b5ref/n)2\nn \u03b9+ c2\n\u221a\n\u03c3\u030c/n\u030c\u2212(\u00b5\u030c/n\u030c)2\nn\u030c \u03b9+ c3( H\u03b9 n + H\u03b9 n\u030c\n+ H\u03b9 3/4\nn3/4 + H\u03b9\n3/4\nn\u030c3/4 ).\n11: \u03b2 \u2190 c1\n\u221a\n\u03c3ref/n\u2212(\u00b5ref/n)2\nn \u03b9+ c2\n\u221a\n\u03c3\u030c/n\u030c\u2212(\u00b5\u030c/n\u030c)2\nn\u030c \u03b9+ c3( H\u03b9 n + H\u03b9 n\u030c\n+ H\u03b9 3/4\nn3/4 + H\u03b9\n3/4\nn\u030c3/4 ).\n12: Qh(sh, ah, bh)\u2190 min{rh(sh, ah, bh) + v\u030c n\u030c + \u03b3, rh(sh, ah, bh) +\n\u00b5ref\nn + \u00b5\u030c n\u030c + \u03b2,Qh(sh, ah, bh)}.\n13: Q h (sh, ah, bh)\u2190 max{rh(sh, ah, bh) + v\u030c n\u030c \u2212 \u03b3, rh(sh, ah, bh)) +\n\u00b5ref\nn +\n\u00b5\u030c n\u030c \u2212 \u03b2,Q h (sh, ah, bh)}.\n14: \u03c0h(sh)\u2190 CCE(Q(sh, \u00b7, \u00b7), Qh(sh, \u00b7, \u00b7)). 15: V h(sh)\u2190 E(a,b)\u223c\u03c0k+1(sh)Qh(sh, a, b), and V h(sh)\u2190 E(a,b)\u223c\u03c0k+1(sh)Qh(sh, a, b). 16: Reset all intra-stage accumulators to 0. 17: if V h(sh)\u2212 V h(sh) < \u2206(s, h) then 18: \u2206(s, h) = V h(sh)\u2212 V h(sh). 19: V\u0303 h(sh) = V h(sh), V\u0303 h(sh) = V h(sh). 20: end if 21: end if 22: if \u2211\na,bNh(sh, a, b) = N0 then\n23: V ref h (sh)\u2190 V\u0303 h(sh), V refh (sh)\u2190 V\u0303 h(sh). 24: end if 25: end for 26: end for\n\u03c3refh (sh, ah, bh) +\u2190 (V refh+1(sh+1))2, \u03c3refh (sh, ah, bh) +\u2190 (V refh+1(sh+1))2. (5)\nAll accumulators are initialized to 0 at the beginning of the algorithm. The algorithm set \u03b9 = log(2/\u03b4),\n\u03b2 = O(1/H) and N0 = c4SABH 5/\u03b22 for some sufficiently large universal constant c4.\nCertified policy. Based on the policy trajectories collected from Algorithm 1, we construct an output policy profile (\u00b5out, \u03bdout) that we will show is an approximate NE. For any step h \u2208 [H ], an episode k \u2208 [K] and any state, we let \u00b5kh(\u00b7|s) \u2208 \u2206(A) and \u03bdkh(\u00b7|s) \u2208 \u2206(B) be the distribution prescribed by Algorithm 1 at this step. Let N\u030ckh (s) be the value N\u030c k h (s) at the beginning of the k-th episode. Our construction of the output policy \u00b5out is presented in Algorithm 2 (whereas the certified policy \u03bdout of the min-player can be obtained similarly), which follows the \u201ccertified policies\u201d introduced in [3]. We remark that the episode index from the previous stage is uniformly sampled in our algorithm while the certified policies in [3] uses a weighted mixture.\nAlgorithm 2 Certified policy \u00b5out (max-player version)\n1: Sample k \u2190 Unif([K]). 2: for step h\u2190 1, . . . , H do 3: Receive sh, and take action ah \u223c \u00b5kh(\u00b7|sh). 4: Observe bh, and sample j \u2190 Unif([Nkh (sh, ah, bh)]). 5: Set k \u2190 \u2113\u030ckh,j. 6: end for\n4 Theoretical Analysis\n4.1 Main Result\nIn this subsection, we present the main theoretical result for Algorithm 1. The following theorem presents the sample complexity guarantee for Algorithm 1 to learn a near-optimal Nash equilibrium policy in two-player zero-sum Markov games, which improves the best-known model-free algorithms in the same setting.\nTheorem 4.1. For any \u03b4 \u2208 (0, 1), let the agents run Algorithm 1 for K episodes with K \u2265 O\u0303(H3SAB/\u01eb2). Then, with probability at least 1 \u2212 \u03b4, the output policy (\u00b5out, \u03bdout) of Algorithm 2 is an \u01eb-approximate Nash equilibrium.\nCompared to the lower bound \u2126(H3S(A+B)/\u01eb2) on the sample complexity to find a near-optimal Nash equilibrium established in [4], the sample complexity in Theorem 4.1 is minimax-optimal on the dependence of H , S and \u01eb. This is the first result that establishes such optimality for model-free algorithms, although model-based algorithms have been shown to achieve such optimality in the past [24].\nWe also note that the result in Theorem 4.1 is not tight on the dependence on the cardinality of actions A,B. Such a gap has been closed by popular V-learning algorithms [24, 26], which achieve the sample complexity of O(H5S(A+B)/\u01eb2) [26]. Clearly, V-learning achieves a tight dependence on A,B, but suffers from worse horizon dependence on H . More specifically, one H factor is due to the nature of implementing the adversarial bandit subroutine in exchange for a better action dependence A + B. The other H factor could potentially be improved via the reference-advantage decomposition technique that we adopt here for our Q-learning algorithm. We leave this promising yet challenging direction as a future study.\n4.2 Proof Outline\nIn this subsection, we present the proof sketch of Theorem 4.1, and defer all the details to the appendix.\nOur main technical development lies in establishing a few new properties on the cumulative occurrence of the large V-gap and the cumulative bonus term, which enable the upper-bounding of several new error terms arising due to the incorporation of the new min-gap based reference-advantage decomposition technique. These properties have not been established for the single-agent RL with such a technique, because our properties are established for policies generated by the CCE oracle in zero-sum Markov games. Further, we perform a more refined analysis for both the optimistic and pessimistic accumulative bonus terms in order to obtain the desired result.\nFor certain functions, we use the superscript k to denote the value of the function at the beginning of the k-th episode, and use the superscript K + 1 to denote the value of the function after all K episodes are played. For instance, we denote Nkh (s, a, b) as the value of Nh(s, a, b) at the beginning of the k-th episode, and NK+1h (s, a, b) to denote the total number of visits of (s, a, b) at step h after K episodes. When h and k are clear from the context, we omit the subscript h and superscript k for notational convenience. For example, we use \u2113i and \u2113\u030ci to denote \u2113 k h,i and \u2113\u030c k h,i when h and k are obvious. Preliminary step. We build connection between the certified policy generated by Algorithm 2, and the difference between the optimistic and pessimistic value functions.\nLemma 4.2. Let (\u00b5out, \u03bdout) be the output policy induced by the certified policy algorithm (Algorithm 2), then, we have\nV \u2020,\u03bd out 1 (s1)\u2212 V \u00b5 out,\u2020 1 (s1) \u2264 1\nK\nK\u2211\nk=1\n(V k\n1 \u2212 V k1)(s1).\nIn the remaining steps, we aim to bound \u2211K\nk=1(V k 1 \u2212 V k1)(s1). Step I: We show that the Nash equilibrium (action-)value functions are always bounded between the optimistic and pessimistic (action-)value functions.\nLemma 4.3. With high probability, it holds that for any s, a, b, k, h,\nQk h (s, a, b) \u2264 Q\u2217h(s, a, b) \u2264 Q k h(s, a, b), V k h(s) \u2264 V \u2217h (s) \u2264 V k h(s).\nOur new technical development lies in proving the inequality with respect to the action-value function,\nwhose update rule features the min-gap reference-advantage decomposition.\nThe proof is by induction. We will focus on the optimistic (action-)value function and the other direction can be proved similarly. Suppose the two inequalities hold in episode k. We first establish the inequality for action-value function, and then prove the inequality for value functions. Based on the update rule of the optimistic action-value functions (line 12 in Algorithm 1), the action-value function is determined by the first two non-trial terms and last trivial term. While the first term is shown to upper bound the action-value function at Nash equilibrium Q\u2217h(s, a, b), we make the effort to showcase that the second term involving the min-gap based reference-advantage decomposition also upper bounds Q\u2217h(s, a, b). Since the optimistic actionvalue function takes the minimum of the three terms, we conclude that the optimistic action-value function in episode k + 1 satisfy the inequality. The proof of the inequality for value function (second inequality in Lemma 4.3) is based on the property of the policy distribution output by the CCE oracle.\nNote that the optimistic (or pessimistic) action-value function is non-increasing (or non-decreasing).\nHowever, the optimistic and the pessimistic value functions do not preserve such monotonic property due to the nature of the CCE oracle. This motivates our design of the min-gap based reference-advantage decomposition. Step II: We show that the reference value can be learned with bounded sample complexity in the following lemma. Lemma 4.4. With high probability, it holds that \u2211K\nk=1 1{V k h(s k h)\u2212 V kh(skh) \u2265 \u01eb} \u2264 O(SABH5\u03b9/\u01eb2).\nWe show that in the two-player zero-sum Markov game, the occurrence of the large V-gap, induced by the policy generated by the CCE oracle, is bounded independent of the number of episodes K. Our new development in proving this lemma lies in handling an additional martingale difference arising due to the CCE oracle.\nIn order to extract the best pair of optimistic and pessimistic value functions, a key novel min-gap based reference-advantage decomposition is proposed (see Section 3), based on which we pick up the pair of optimistic and pessimistic value functions whose gap is the smallest in the history (line 17-20 in Algorithm 1). By the selection of the reference value functions, Lemma 4.4 with \u01eb set to \u03b2, and the definition of N0, we have the following corollary.\nCorollary 4.5. Conditioned on the successful events of Proposition 4.3 and Lemma 4.4, for every state s, we have\nnkh(s) \u2265 N0 =\u21d2 V ref,k h (s)\u2212 V ref,kh (s) \u2264 \u03b2.\nStep III: We bound \u2211K\nk=1(V k 1 \u2212V k1)(s1). Compared to single-agent RL, the CCE oracle leads to a possibly mixed policy and we need to bound the additional term due to the CCE oracle.\nFor ease of exposition, define \u2206kh = (V k h \u2212 V kh)(skh), and martingale difference \u03b6kh = \u2206kh \u2212 (Q k h \u2212 Qk\nh )(skh, a k h, b k h). Note that n k h = N k h (s k h, a k h, b k h) and n\u030c k h = N\u030c k h (s k h, a k h, b k h) when N k h (s k h, a k h, b k h) \u2208 L. Following\nthe update rule, we have (omitting the detail)\n\u2206kh = \u03b6 k h + (Q\nk h \u2212Qkh)(s k h, a k h, b k h) \u2264 \u03b6kh +H1{nkh = 0}+\n1\nn\u030ckh\nn\u030ckh\u2211\ni=1\n\u2206\u2113\u030cih+1 + \u039b k h+1,\nwhere the definition of \u0393kh+1 is provided in the appendix.\nSumming over k \u2208 [K], we have\nK\u2211\nk=1\n\u2206kh \u2264 K\u2211\nk=1\n\u03b6kh +\nK\u2211\nk=1\nH1{nkh = 0}+ K\u2211\nk=1\n1\nn\u030ckh\nn\u030ckh\u2211\ni=1\n\u2206 \u2113\u030ckh,i h+1 +\nK\u2211\nk=1\n\u039bkh+1\n\u2264 K\u2211\nk=1\n\u03b6kh + SABH 2 + (1 +\n1\nH )\nK\u2211\nk=1\n\u2206kh+1 + K\u2211\nk=1\n\u039bkh+1,\nwhere in the last inequality, we use the pigeon-hole argument for the second term, and the third term is due to the (1 + 1/H) growth rate of the length of the stages.\nIterating over h = H,H \u2212 1, . . . , 1 gives K\u2211\nk=1\n\u2206k1 \u2264 O ( SABH3 + H\u2211\nh=1\nK\u2211\nk=1\n(1 + 1\nH )h\u22121\u03b6kh +\nH\u2211\nh=1\nK\u2211\nk=1\n(1 + 1\nH )h\u22121\u039bkh+1\n) .\nThe additional term \u2211H\nh=1 \u2211K k=1(1 + 1 H ) h\u22121\u03b6kh is new in the two-player zero-sum Markov game, which\ncan be bounded by Azuma-Hoeffding\u2019s inequality. I.e., it holds that with probability at least 1\u2212 T\u03b4, H\u2211\nh=1\nK\u2211\nk=1\n(1 + 1\nH )h\u22121\u03b6kh \u2264 O(\n\u221a H2T \u03b9),\nwhich turns out to be a lower-order term compared to \u2211H\nh=1 \u2211K k=1(1 + 1 H ) h\u22121\u039bkh+1.\nStep IV: We bound \u2211H\nh=1 \u2211K k=1(1 + 1 H ) h\u22121\u039bkh+1 in the following lemma.\nLemma 4.6. With high probability, it holds that\n\u2211H h=1 \u2211K k=1(1 + 1 H ) h\u22121\u039bkh+1 = O (\u221a SABH2\u03b9+H \u221a T \u03b9 logT + S2(AB) 3 2H8\u03b9T 1 4 ) .\nWe capture the accumulative error of the bonus terms \u2211H\nh=1 \u2211K k=1(1 + 1 H ) h\u22121(\u03b2 k h+1 + \u03b2 k h+1 ) in the\nexpression \u2211H\nh=1 \u2211K k=1(1 + 1 H ) h\u22121\u039bkh+1. Since we first implement the reference-advantage decomposition\ntechnique in the two-player zero-sum game, our accumulative bonus term is much more challenging to analyze than the existing Q-learning algorithms for games. Compared to the analysis for the model-free algorithm with reference-advantage decomposition in single-RL [44], our analysis features the following new developments. First, we need to bound both the optimistic and pessimistic accumulative bonus terms, and the analysis is not identical. Second, the analysis of the optimistic accumulative bonus term differs due to the CCE oracle and the new min-gap base reference-advantage decomposition for two-player zero-sum Markov game.\nFinally, combining all steps, we conclude that with high probability,\nV \u2020,\u03bd out 1 (s1)\u2212 V \u00b5 out,\u2020 1 (s1) \u2264 1K \u2211K k=1 \u2206 k h = O\n( H3SAB\n\u01eb2\n) .\n5 Conclusion\nIn this paper, we proposed a new model-free algorithm Q-learning with min-gap based reference-advantage decomposition for two-player zero-sum Markov games, which improved the existing results and achieved a near-optimal sample complexity O(H3SAB/\u01eb2) except for the AB term. Due to the nature of the CCE oracle employed in the algorithm, we designed a novel min-gap based reference-advantage decomposition to learn the pair of optimistic and pessimistic reference value functions whose value difference has the minimum gap in the history. An interesting future direction would be to study whether the horizon dependence could be further tightened in model-free V-learning. Other interesting directions include understanding learning in the offline regimes [9, 41] and considering how nonstationarity [13] would affect learning complexity in the game settings.\nReferences\n[1] Peter Auer, Thomas Jaksch, and Ronald Ortner. Near-optimal regret bounds for reinforcement learning.\nIn Advances in Neural Information Processing Systems, 2008.\n[2] Mohammad Gheshlaghi Azar, Ian Osband, and R\u00e9mi Munos. Minimax regret bounds for reinforcement\nlearning. In Proceedings of the 34th International Conference on Machine Learning, 2017.\n[3] Yu Bai and Chi Jin. Provable self-play algorithms for competitive reinforcement learning. In Proceedings\nof the 37th International Conference on Machine Learning, 2020.\n[4] Yu Bai and Chi Jin. Provable self-play algorithms for competitive reinforcement learning. In Proceedings\nof the 37th International Conference on Machine Learning, 2020.\n[5] Yu Bai, Chi Jin, and Tiancheng Yu. Near-optimal reinforcement learning with self-play. In Advances\nin Neural Information Processing Systems, 2020.\n[6] Bowen Baker, Ingmar Kanitscheider, Todor Markov, Yi Wu, Glenn Powell, Bob McGrew, and Igor\nMordatch. Emergent tool use from multi-agent autocurricula. In International Conference on Learning Representations, 2020.\n[7] Fan Chen, Song Mei, and Yu Bai. Unified Algorithms for RL with Decision-Estimation Coefficients:\nNo-Regret, PAC, and Reward-Free Learning. arXiv e-prints, 2022.\n[8] Zixiang Chen, Dongruo Zhou, and Quanquan Gu. Almost optimal algorithms for two-player zero-sum\nlinear mixture markov games. In Proceedings of The 33rd International Conference on Algorithmic Learning Theory, 2022.\n[9] Qiwen Cui and Simon S Du. When are offline two-player zero-sum markov games solvable? Advances\nin Neural Information Processing Systems, 35:25779\u201325791, 2022.\n[10] Qiwen Cui, Kaiqing Zhang, and Simon S. Du. Breaking the Curse of Multiagents in a Large State Space:\nRL in Markov Games with Independent Linear Function Approximation. arXiv e-prints, 2023.\n[11] Christoph Dann, Tor Lattimore, and Emma Brunskill. Unifying PAC and regret: Uniform PAC bounds\nfor episodic reinforcement learning. In Proceedings of the 31st International Conference on Neural Information Processing Systems, 2017.\n[12] Constantinos Daskalakis, Noah Golowich, and Kaiqing Zhang. The Complexity of Markov Equilibrium\nin Stochastic Games. arXiv e-prints, 2022.\n[13] Songtao Feng, Ming Yin, Ruiquan Huang, Yu-Xiang Wang, Jing Yang, and Yingbin Liang. Non-\nstationary reinforcement learning under general function approximation. In Proceedings of the 40th International Conference on Machine Learning, 2023.\n[14] J. Filar and K. Vrieze. Competitive Markov Decision Processes. Springer, 1997.\n[15] Thomas Dueholm Hansen, Peter Bro Miltersen, and Uri Zwick. Strategy iteration is strongly polynomial\nfor 2-player turn-based stochastic games with a constant discount factor. J. ACM, 60(1), feb 2013.\n[16] Junling Hu and Michael P. Wellman. Nash Q-learning for general-sum stochastic games. J. Mach. Learn.\nRes., 4(null):1039\u20131069, dec 2003.\n[17] Baihe Huang, Jason D. Lee, Zhaoran Wang, and Zhuoran Yang. Towards general function approximation\nin zero-sum markov games. In International Conference on Learning Representations, 2022.\n[18] Zeyu Jia, Lin F. Yang, and Mengdi Wang. Feature-Based Q-Learning for Two-Player Stochastic Games.\narXiv e-prints, 2019.\n[19] Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I Jordan. Is Q-learning provably efficient?\nIn Advances in Neural Information Processing Systems, 2018.\n[20] Chi Jin, Qinghua Liu, Yuanhao Wang, and Tiancheng Yu. V-learning \u2013 a simple, efficient, decentralized\nalgorithm for multiagent RL. In ICLR 2022 Workshop on Gamification and Multiagent Solutions, 2022.\n[21] Chi Jin, Qinghua Liu, and Tiancheng Yu. The power of exploiter: Provable multi-agent RL in large\nstate spaces. In Proceedings of the 39th International Conference on Machine Learning, 2022.\n[22] Gen Li, Yuejie Chi, Yuting Wei, and Yuxin Chen. Minimax-optimal multi-agent RL in markov games\nwith a generative model. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.\n[23] Michael L. Littman. Markov games as a framework for multi-agent reinforcement learning. In Proceedings\nof the Eleventh International Conference on International Conference on Machine Learning, 1994.\n[24] Qinghua Liu, Tiancheng Yu, Yu Bai, and Chi Jin. A sharp analysis of model-based reinforcement\nlearning with self-play. In Proceedings of the 38th International Conference on Machine Learning, 2021.\n[25] Weichao Mao and Tamer Ba\u015far. Provably Efficient Reinforcement Learning in Decentralized General-\nSum Markov Games. arXiv e-prints, 2021.\n[26] Weichao Mao, Lin Yang, Kaiqing Zhang, and Tamer Basar. On improving model-free algorithms for\ndecentralized multi-agent reinforcement learning. In Proceedings of the 39th International Conference on Machine Learning, 2022.\n[27] Chengzhuo Ni, Yuda Song, Xuezhou Zhang, Zihan Ding, Chi Jin, and Mengdi Wang. Representation\nlearning for low-rank general-sum markov games. In The Eleventh International Conference on Learning Representations, 2023.\n[28] Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. Safe, Multi-Agent, Reinforcement Learn-\ning for Autonomous Driving. arXiv e-prints, 2016.\n[29] Lloyd S. Shapley. Stochastic games. Proceedings of the National Academy of Sciences, 39:1095 \u2013 1100,\n1953.\n[30] Aaron Sidford, Mengdi Wang, Lin Yang, and Yinyu Ye. Solving discounted stochastic two-player\ngames with near-optimal time and sample complexity. In Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics, 2020.\n[31] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,\nJulian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484\u2013489, 2016.\n[32] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,\nThomas Hubert, Lucas baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy P. Lillicrap, Fan Hui, L. Sifre, George van den Driessche, Thore Graepel, and Demis Hassabis. Mastering the game of go without human knowledge. Nature, 550:354\u2013359, 2017.\n[33] Ziang Song, Song Mei, and Yu Bai. When can we learn general-sum markov games with a large number\nof players sample-efficiently? In International Conference on Learning Representations, 2022.\n[34] Oriol Vinyals, Igor Babuschkin, Wojciech M. Czarnecki, Micha\u00ebl Mathieu, Andrew Dudzik, Junyoung\nChung, David H. Choi, Richard Powell, Timo Ewalds, Petko Georgiev, Junhyuk Oh, Dan Horgan, Manuel Kroiss, Ivo Danihelka, Aja Huang, L. Sifre, Trevor Cai, John P. Agapiou, Max Jaderberg, Alexander Sasha Vezhnevets, R\u00e9mi Leblond, Tobias Pohlen, Valentin Dalibard, David Budden, Yury Sulsky, James Molloy, Tom Le Paine, Caglar Gulcehre, Ziyun Wang, Tobias Pfaff, Yuhuai Wu, Roman Ring, Dani Yogatama, Dario W\u00fcnsch, Katrina McKinney, Oliver Smith, Tom Schaul, Timothy P. Lillicrap, Koray Kavukcuoglu, Demis Hassabis, Chris Apps, and David Silver. Grandmaster level in starcraft II using multi-agent reinforcement learning. Nature, pages 1\u20135, 2019.\n[35] Yuanhao Wang, Qinghua Liu, Yu Bai, and Chi Jin. Breaking the Curse of Multiagency: Provably\nEfficient Decentralized Multi-Agent RL with Function Approximation. arXiv e-prints, 2023.\n[36] Chen-Yu Wei, Yi-Te Hong, and Chi-Jen Lu. Online reinforcement learning in stochastic games. In\nProceedings of the 31st International Conference on Neural Information Processing Systems, 2017.\n[37] Chen-Yu Wei, Chung-Wei Lee, Mengxiao Zhang, and Haipeng Luo. Linear Last-iterate Convergence in\nConstrained Saddle-point Optimization. arXiv e-prints, 2020.\n[38] Qiaomin Xie, Yudong Chen, Zhaoran Wang, and Zhuoran Yang. Learning zero-sum simultaneous-move\nmarkov games using function approximation and correlated equilibrium. In Proceedings of Thirty Third Conference on Learning Theory, 2020.\n[39] Wei Xiong, Han Zhong, Chengshuai Shi, Cong Shen, and Tong Zhang. A self-play posterior sampling\nalgorithm for zero-sum Markov games. In Proceedings of the 39th International Conference on Machine Learning, 2022.\n[40] Ming Yin, Yu Bai, and Yu-Xiang Wang. Near-optimal provable uniform convergence in offline pol-\nicy evaluation for reinforcement learning. In International Conference on Artificial Intelligence and Statistics, pages 1567\u20131575. PMLR, 2021.\n[41] Ming Yin and Yu-Xiang Wang. Towards instance-optimal offline reinforcement learning with pessimism.\nAdvances in neural information processing systems, 34:4065\u20134078, 2021.\n[42] Wenhao Zhan, Jason D. Lee, and Zhuoran Yang. Decentralized optimistic hyperpolicy mirror descent:\nProvably no-regret learning in markov games. In The Eleventh International Conference on Learning Representations, 2023.\n[43] Kaiqing Zhang, Sham M. Kakade, Tamer Ba\u015far, and Lin F. Yang. Model-based multi-agent RL in\nzero-sum markov games with near-optimal sample complexity. In Proceedings of the 34th International Conference on Neural Information Processing Systems, 2020.\n[44] Zihan Zhang, Yuan Zhou, and Xiangyang Ji. Almost optimal model-free reinforcement learning via\nreference-advantage decomposition. In Proceedings of the 34th International Conference on Neural Information Processing Systems, 2020.\nSupplementary Materials\nA Notations\nFor any function f : S 7\u2192 R, we use Ps,a,bf and (Phf)(s, a, b) interchangeably. Define V(x, y) = x\u22a4(y2) \u2212 (x\u22a4y)2 for two vectors of the same dimension, where y2 is obtained by squaring each entry of y.\nFor ease of exposition, we define \u03bdref,kh = \u03c3ref,kh nkh \u2212 (\u00b5 ref,k h nkh )2, \u03bdref,kh = \u03c3ref,kh nkh \u2212 (\u00b5\nref,k h nkh )2 and \u03bd\u030c k h = \u03c3\u030c k h n\u030ckh \u2212 ( \u00b5\u030c k h n\u030ckh )2,\n\u03bd\u030ckh = \u03c3\u030ckh n\u030ckh \u2212 ( \u00b5\u030c\nk h\nn\u030ckh )2. Moreover, we define \u2206kh = V\nk h(s k h) \u2212 V kh(skh) and \u03b6kh = \u2206kh \u2212 (Q k h \u2212 Qkh)(s k h, a k h, b k h). For\nconvenience, we also define \u03bbkh(s) = 1 { nkh(s) < N0 } .\nFor certain functions, we use the superscript k to denote the value of the function at the beginning of the k-th episode, and use the superscript K + 1 to denote the value of the function after all K episodes are played. For instance, we denote Nkh (s, a, b) as the value of Nh(s, a, b) at the beginning of the k-th episode, and NK+1h (s, a, b) to denote the total number of visits of (s, a, b) at step h after K episodes. When it is clear from the context, we omit the subscript h and the superscript k for notational convenience. For example, we use \u2113i and \u2113\u030ci to denote \u2113 k h,i and \u2113\u030c k h,i when it is obvious what values that the indices h and k take.\nB Proof of Theorem 4.1\nIn this section, we provide the proof of Theorem 4.1, which consists of four main steps and one final step. In order to provide a clear proof flow here, we defer the proofs of the main lemmas in these steps to later sections (i.e., Appendix C-Appendix F).\nWe start by replacing \u03b4 by \u03b4/poly(H,T ), and it suffices to show the desired bound for V \u2020,\u03bd out\n1 (s1) \u2212 V \u00b5\nout,\u2020 1 (s1) with probability 1\u2212 poly(H,T )\u03b4.\nStep I: We show that the Nash equilibrium (action-)value functions are always bounded between the\noptimistic and pessimistic (action-)value functions.\nLemma B.1 (Restatement of Lemma 4.3). Let \u03b4 \u2208 (0, 1). With probability at least 1\u2212 2T (2H2T 3 + 7)\u03b4, it holds that for any s, a, b, k, h,\nQk h (s, a, b) \u2264 Q\u2217h(s, a, b) \u2264 Q k h(s, a, b), V kh(s) \u2264 V \u2217h (s) \u2264 V k h(s).\nThe proof of Lemma B.1 is provided in Appendix C. The new technical development lies in proving the inequality with respect to the action-value function, whose update rule features the min-gap referenceadvantage decomposition.\nStep II: We show that the occurrence of the large V-gap has bounded sample complexity independent\nof the number of episodes K.\nLemma B.2 (Restatement of Lemma 4.4). With probability 1\u2212O(T\u03b4), it holds that\nK\u2211\nk=1\n1{V kh(skh)\u2212 V kh(skh) \u2265 \u01eb} \u2264 O(SABH5\u03b9/\u01eb2).\nThe proof is provided in Appendix D. By the selection of the reference value functions, Lemma B.2 with \u01eb setting to \u03b2, and the definition of\nN0, we have the following corollary.\nCorollary B.3 (Restatement of Corollary 4.5). Conditioned on the successful events of Lemma B.1 and Lemma B.2, for every state s, we have\nnkh(s) \u2265 N0 =\u21d2 V ref,k h (s)\u2212 V ref,kh (s) \u2264 \u03b2.\nStep III: We bound \u2211K\nk=1(V k 1 \u2212V k1)(s1). Compared to single-agent RL, the CCE oracle leads to a possibly mixed policy and we need to bound the additional term due to the CCE oracle.\nRecall the definition of \u2206kh = V k h(s k h)\u2212V kh(skh) and \u03b6kh = \u2206kh\u2212(Q k h\u2212Qkh)(s k h, a k h, b k h). Following the update\nrule, we have\n\u2206kh = \u03b6 k h + (Q\nk h \u2212Qkh)(s k h, a k h, b k h)\n\u2264 H1{nkh = 0}+ 1\nnkh\nnkh\u2211\ni=1\nV ref,\u2113i h+1 (s \u2113i h+1)\u2212\n1\nnkh\nnkh\u2211\ni=1\nV ref,\u2113ih+1 (s \u2113i h+1)\n+ 1\nn\u030ckh\nn\u030ckh\u2211\ni=1\n(V \u2113\u030ci h+1 \u2212 V ref,\u2113\u030ci h+1 )(s \u2113\u030ci h+1)\u2212\n1\nn\u030ckh\nn\u030ckh\u2211\ni=1\n(V \u2113\u030cih+1 \u2212 V ref,\u2113\u030ci h+1 )(s \u2113\u030ci h+1) + \u03b2\nk h + \u03b2 k\nh\n\u2264 \u03b6kh +H1{nkh = 0}+ 1\nnkh\nnkh\u2211\ni=1\nPskh,akh,bkh,hV ref,\u2113i h+1 \u2212\n1\nnkh\nnkh\u2211\ni=1\nPskh,akh,bkh,hV ref,\u2113i h+1\n+ 1\nn\u030ckh\nn\u030ckh\u2211\ni=1\nPskh,akh,bkh,h(V \u2113\u030ci h+1 \u2212 V ref,\u2113\u030ci h+1 )\u2212\n1\nn\u030ckh\nn\u030ckh\u2211\ni=1\nPskh,akh,bkh,h(V \u2113\u030ci h+1 \u2212 V ref,\u2113\u030ci h+1 ) + 2\u03b2\nk h + 2\u03b2 k\nh (6)\n= \u03b6kh +H1{nkh = 0}+ Pskh,akh,bkh,h   1 nkh nkh\u2211\ni=1\nV ref,\u2113i h+1 \u2212\n1\nn\u030ckh\nn\u030ckh\u2211\ni=1\nV ref,\u2113\u030ci h+1\n \n\u2212 Pskh,akh,bkh,h   1 nkh nkh\u2211\ni=1\nV ref,\u2113ih+1 \u2212 1\nn\u030ckh\nn\u030ckh\u2211\ni=1\nV ref,\u2113\u030cih+1  + Pskh,akh,bkh,h 1\nn\u030ckh\nn\u030ckh\u2211\ni=1\n( V\n\u2113\u030ci h+1 \u2212 V \u2113\u030cih+1\n)\n+ 2\u03b2 k\nh + 2\u03b2 k\nh\n\u2264 \u03b6kh +H1{nkh = 0}+ Pskh,akh,bkh,h   1 nkh nkh\u2211\ni=1\nV ref,\u2113i h+1 \u2212 V REF h+1\n \n\u2212 Pskh,akh,bkh,h   1 nkh nkh\u2211\ni=1\nV ref,\u2113ih+1 \u2212 V REFh+1  + Pskh,akh,bkh,h 1\nn\u030ckh\nn\u030ckh\u2211\ni=1\n( V\n\u2113\u030ci h+1 \u2212 V \u2113\u030cih+1\n)\n+ 2\u03b2 k h + 2\u03b2 k\nh (7)\n= \u03b6kh +H1{nkh = 0}+ 1\nn\u030ckh\nn\u030ckh\u2211\ni=1\n\u2206\u2113\u030cih+1 + \u039b k h+1, (8)\nwhere we define\n\u039bkh+1 = \u03c8 k h+1 + \u03be k h+1 + 2\u03b2\nk h + 2\u03b2 k\nh ,\n\u03c8kh+1 = Pskh,akh,bkh,h   1 nkh nkh\u2211\ni=1\n( V\nref,\u2113i h+1 \u2212 V ref,\u2113ih+1\n) \u2212 ( V REF h+1 \u2212 V REFh+1 )   ,\n\u03bekh+1 = 1\nn\u030ckh\nn\u030ckh\u2211\ni=1\n( Pskh,akh,bkh,h \u2212 1s\u2113\u030cih+1 )( V \u2113\u030ci h+1 \u2212 V \u2113\u030cih+1 ) .\nHere, (6) follows from the successful event of martingale concentration (25) and (39) in Lemma B.1, (7) follows from the fact that V ref,u\nh+1 (s) (or V ref,u h+1 (s)) is non-increasing (or non-decreasing) in u, because V\nref h (s)\n(or V refh (s)) for a pair (s, h) can only be updated once and the updated value is obviously greater (or less) than the initial value, and (8) follows from the definition of \u039bkh+1 defined above.\nTaking the summation over k \u2208 [K] gives\nK\u2211\nk=1\n\u2206kh \u2264 K\u2211\nk=1\n\u03b6kh +\nK\u2211\nk=1\nH1{nkh = 0}+ K\u2211\nk=1\n1\nn\u030ckh\nn\u030ckh\u2211\ni=1\n\u2206 \u2113\u030ckh,i h+1 +\nK\u2211\nk=1\n\u039bkh+1. (9)\nNote that nkh \u2265 H if Nkh (skh, akh, bkh) \u2265 H . Therefore \u2211K k=1 1{nkh = 0} \u2264 SABH , and\nK\u2211\nk=1\nH1{nkh = 0} \u2264 SABH2. (10)\nNow we focus on the term \u2211K\nk=1 1 n\u030ckh\n\u2211n\u030ckh i=1 \u2206 \u2113\u030ckh,i h+1. The following lemma is useful.\nLemma B.4. For any j \u2208 [K], we have \u2211Kk=1 1n\u030ckh \u2211n\u030ckh i=1 1{j = \u2113\u030ckh,i} \u2264 1 + 1H . Proof. Fix an episode j. Note that \u2211n\u030ckh\ni=1 1{j = \u2113\u030ckh,i} = 1 if and only if (s j h, a j h, b j h) = (s k h, a k h, b k h) and (j, h)\nfalls in the previous stage that (k, h) falls in with respect to (skh, a k h, b k h, h). Define K = {k \u2208 [K] : \u2211n\u030ckh i=1 1{j = \u2113\u030ckh,i} = 1}. Then every element k \u2208 K has the same value of n\u030ckh, i.e., there exists an integer Nj > 0 such that n\u030ckh = Nj for all k \u2208 K. By the definition of stages, |K| \u2264 (1 + 1H )Nj . Therefore, for any j, we have\u2211K\nk=1 1 n\u030ck h\n\u2211n\u030ckh i=1 1{j = \u2113\u030ckh,i} \u2264 (1 + 1H ).\nBy Lemma B.4, we have\nK\u2211\nk=1\n1\nn\u030ckh\nn\u030ckh\u2211\ni=1\n\u2206 \u2113\u030ckh,i h+1 =\nK\u2211\nk=1\n1\nn\u030ckh\nK\u2211\nj=1\n\u2206jh+1\nn\u030ckh\u2211\ni=1\n1{j = \u2113\u030ckh,i}\n=\nK\u2211\nj=1\n\u2206jh+1\nK\u2211\nk=1\n1\nn\u030ckh\nn\u030ckh\u2211\ni=1\n1{j = \u2113\u030ckh,i}\n\u2264 (1 + 1 H )\nK\u2211\nk=1\n\u2206kh+1. (11)\nCombining (9), (10) and (11), we have\nK\u2211\nk=1\n\u2206kh \u2264 SABH2 + (1 + 1\nH )\nK\u2211\nk=1\n\u2206kh+1 +\nK\u2211\nk=1\n\u039bkh+1.\nIterating over h = H,H \u2212 1, . . . , 1 gives\nK\u2211\nk=1\n\u2206k1 \u2264 O ( SABH3 + H\u2211\nh=1\nK\u2211\nk=1\n(1 + 1\nH )h\u22121\u03b6kh +\nH\u2211\nh=1\nK\u2211\nk=1\n(1 + 1\nH )h\u22121\u039bkh+1\n) .\nBy Azuma\u2019s inequality, it holds that with probability at least 1\u2212 T\u03b4,\nK\u2211\nk=1\n\u2206k1 \u2264 O ( SABH3 + \u221a H2T \u03b9+ H\u2211\nh=1\nK\u2211\nk=1\n(1 + 1 H )h\u22121\u039bkh+1\n) . (12)\nStep IV: We bound \u2211H\nh=1 \u2211K k=1(1 + 1 H ) h\u22121\u039bkh+1 in the following lemma.\nLemma B.5 (Restatement of Lemma 4.6). With probability at least 1\u2212O(H2T 4)\u03b4, it holds that\nH\u2211\nh=1\nK\u2211\nk=1\n(1 + 1\nH )h\u22121\u039bkh+1 = O\n(\u221a SABH2T \u03b9+H \u221a T \u03b9 logT + S2(AB) 3 2H8\u03b9 3 2 T 1 4 ) .\nThe proof of Lemma B.5 is provided in Appendix E. Final step: We show the value difference induced by the certified policies is bounded, as summarized in\nthe next lemma.\nLemma B.6 (Restatement of Lemma 4.2). Conditioned on the successful event of Lemma B.1, let (\u00b5out, \u03bdout) be the output policy induced by the certified policy algorithm (Algorithm 2). Then we have\nV \u2020,\u03bd out 1 (s1)\u2212 V \u00b5 out,\u2020 1 (s1) \u2264 1\nK\nK\u2211\nk=1\n(V k 1 \u2212 V k1)(s1).\nThe proof of Lemma B.6 is provided in Appendix F. Combining (12), Lemma B.5 and Lemma B.6, and taking the union bound over all probability events,\nwe conclude that with probability at least 1\u2212O(H2T 4)\u03b4, it holds that\nV \u2020,\u03bd out 1 (s1)\u2212 V \u00b5 out,\u2020 1 (s1) \u2264 1 K O (\u221a SABH2T \u03b9+H \u221a T \u03b9 logT + S2(AB) 3 2H8\u03b9 3 2 T 1 4 ) , (13)\nwhich gives the desired result.\nC Proof of Lemma B.1 (Step I)\nThe proof is by induction on k. We establish the inequalities for the optimistic action-value and value functions in step i, and the inequalities for the pessimistic counterparts in step ii.\nStep i: We establish the inequality for the optimistic action-value and value functions in the following.\nIt is clear that the conclusion holds for the based case with k = 1. For k \u2265 2, assume Q\u2217h(s, a, b) \u2264 Q u\nh(s, a, b) and V \u2217 h (s) \u2264 V\nu\nh(s) for any (s, a, h) \u2208 S \u00d7 A \u00d7 [H ] and u \u2208 [1, k]. Fix tuple (s, a, b, h). We next show that the conclusion holds for k + 1.\nFirst, we show the inequality with respect to the action-value function. If Qh(s, a, b), V h(s) are not\nupdated in the k-th episode, then\nQ\u2217h(s, a, b) \u2264 Q k h(s, a, b) = Q k+1 h (s, a, b),\nV \u2217h (s) \u2264 V k h(s) = V k+1 h (s).\nOtherwise, we have\nQ k+1 h (s, a, b)\u2190 min { rh(s, a, b) + v\u030c\nn\u030c + \u03b3, rh(s, a, b) +\n\u00b5ref\nn + \u00b5\u030c n\u030c + \u03b2,Q k h(s, a, b)\n} .\nBesides the last term, there are two non-trivial cases.\nFor the first case, by Hoeffding\u2019s inequality, with probability at least 1\u2212 \u03b4 it holds that\nQ k+1 h (s, a, b) = rh(s, a, b) + v\u030c\nn\u030c + \u03b3\n= rh(s, a, b) + 1\nn\u030c\nn\u030c\u2211\ni=1\nV \u2113\u030ci h+1(s \u2113\u030ci h+1) + 2\n\u221a H2\nn\u030c \u03b9\n\u2265 rh(s, a, b) + 1\nn\u030c\nn\u030c\u2211\ni=1\nV \u2217h+1(s \u2113\u030ci h+1) + 2\n\u221a H2\nn\u030c \u03b9 (14)\n\u2265 rh(s, a, b) + (PhV \u2217h+1)(s, a, b) (15) = Q\u2217h(s, a, b),\nwhere (14) follows from the induction hypothesis V u h+1(s) \u2265 V \u2217(s) for all u \u2208 [k], and (15) follows from Azuma-Hoeffding\u2019s inequality.\nFor the second case, we have\nQ k+1 h (s, a, b) = rh(s, a, b) + \u00b5ref n + \u00b5\u030c n\u030c + \u03b2\n= rh(s, a, b) + 1\nn\nn\u2211\ni=1\nV ref,\u2113i h+1 (s \u2113i h+1) +\n1 n\u030c\nn\u030c\u2211\ni=1\n( V\n\u2113\u030ci h+1 \u2212 V ref,\u2113\u030ci h+1 ) (s\u2113\u030cih+1) + \u03b2\n= rh(s, a, b) + ( Ph ( 1\nn\nn\u2211\ni=1\nV ref,\u2113i h+1\n)) (s, a, b) + ( Ph ( 1\nn\u030c\nn\u030c\u2211\ni=1\n( V\n\u2113\u030ci h+1 \u2212 V ref,\u2113\u030ci h+1\n))) (s, a, b)\n+ \u03c71 + \u03c72 + \u03b2\n\u2265 rh(s, a, b) + ( Ph ( 1\nn\u030c\nn\u030c\u2211\ni=1\nV \u2113\u030ci h+1 )) (s, a, b) + \u03c71 + \u03c72 + \u03b2 (16)\n\u2265 rh(s, a, b) + ( PhV \u2217 h+1 ) (s, a, b) + \u03c71 + \u03c72 + \u03b2 (17) = Q \u2217 h(s, a, b) + \u03c71 + \u03c72 + \u03b2,\nwhere\n\u03c71(k, h) = 1\nn\nn\u2211\ni=1\n( V ref,\u2113i h (s \u2113i h+1)\u2212 ( PhV ref,\u2113i h+1 ) (s, a, b) ) ,\nW \u2113 h+1 = V \u2113 h+1 \u2212 V ref,\u2113 h+1\n\u03c72(k, h) = 1\nn\u030c\nn\u030c\u2211\ni=1\n( W\n\u2113\u030ci h+1(s \u2113\u030ci h+1)\u2212 ( PhW \u2113\u030ci h+1 ) (s, a, b) ) .\nHere, (16) follows from the fact that V ref,u h+1 (s) is non-increasing in u (since V ref h (s) for a pair (s, h) can only be updated once and the updated value is obviously smaller than the initial value H), and (17) follows from the the induction hypothesis V k\nh+1(s) \u2265 V \u2217h+1(s).\nBy Lemma G.2 with \u01eb = 1T 2 , with probability at least 1\u2212 2(H2T 3 + 1)\u03b4 it holds\n|\u03c71(k, h)| \u2264 2\n\u221a\u2211n i=1 V(Ps,a,b,h, V ref,\u2113i h+1 )\u03b9\nn2 +\n2 \u221a \u03b9 Tn + 2H\u03b9 n , (18)\n|\u03c72(k, h)| \u2264 2\n\u221a\u2211n\u030c i=1 V(Ps,a,b,h, V ref,\u2113i h+1 )\u03b9\nn\u030c2 +\n2 \u221a \u03b9 T n\u030c + 2H\u03b9 n\u030c . (19)\nLemma C.1. With probability at least 1\u2212 2\u03b4, it holds that n\u2211\ni=1\nV(Ps,a,b,h, V ref,\u2113i h+1 ) \u2264 n\u03bdref + 3H2 \u221a n\u03b9. (20)\nProof: Note that\nn\u2211\ni=1\nV(Ps,a,b,h, V ref,\u2113i h+1 ) =\nn\u2211\ni=1\n( Ps,a,b,h(V ref,\u2113i h+1 ) 2 \u2212 (Ps,a,b,hV ref,\u2113i h+1 ) 2 )\n=\nn\u2211\ni=1\n(V ref,\u2113i h+1 (s \u2113i h+1)) 2 \u2212 1 n\n( n\u2211\ni=1\nV ref,\u2113i h+1 (s \u2113i h+1) )2 + \u03c73 + \u03c74 + \u03c75\n= n\u03bdref + \u03c73 + \u03c74 + \u03c75, (21)\nwhere\n\u03c73 =\nn\u2211\ni=1\n( (Ps,a,b,h(V ref,\u2113i h+1 ) 2 \u2212 (V ref,\u2113ih+1 (s\u2113ih+1))2 ) ,\n\u03c74 = 1\nn\n( n\u2211\ni=1\nV ref,\u2113i h+1 (s \u2113i h+1) )2 \u2212 1 n ( n\u2211\ni=1\nPs,a,b,hV ref,\u2113i h+1 )2 ,\n\u03c75 = 1\nn\n( n\u2211\ni=1\nPs,a,b,hV ref,\u2113i h+1\n)2 \u2212 n\u2211\ni=1\n(Ps,a,b,hV ref,\u2113i h+1 ) 2.\nBy Azuma\u2019s inequality, with probability at least 1\u2212 \u03b4 it holds that |\u03c73| \u2264 H2 \u221a 2n\u03b9.\nBy Azuma\u2019s inequality, with probability at least 1\u2212 \u03b4, it holds that\n|\u03c74| = 1\nn \u2223\u2223\u2223\u2223\u2223\u2223 ( n\u2211\ni=1\nV ref,\u2113i h+1 (s \u2113i h+1)\n)2 \u2212 ( n\u2211\ni=1\nPs,a,b,hV ref,\u2113i h+1 )2\u2223\u2223\u2223\u2223\u2223\u2223\n\u2264 2H \u2223\u2223\u2223\u2223\u2223 n\u2211\ni=1\nV ref,\u2113i h+1 (s \u2113i h+1)\u2212\nn\u2211\ni=1\nPs,a,b,hV ref,\u2113i h+1 \u2223\u2223\u2223\u2223\u2223\n\u2264 2H2 \u221a 2n\u03b9.\nMoreover, \u03c75 \u2264 0 by Cauchy-Schwartz inequality. Plugging the above inequalities gives the desired result.\nCombining (18) with (20) gives\n|\u03c71| \u2264 2 \u221a \u03bdref\u03b9\nn +\n5H\u03b9 3 4\nn 3 4\n+ 2 \u221a \u03b9\nTn +\n2H\u03b9\nn . (22)\nSimilar to Lemma C.1, we have the following lemma.\nLemma C.2. With probability at least 1\u2212 2\u03b4, it holds that n\u030c\u2211\ni=1\nV(Ps,a,b,h,W ref,\u2113i h+1 ) \u2264 n\u030c\u03bd\u030c + 3H2\n\u221a n\u030c\u03b9. (23)\nCombining (19) with (23) gives\n|\u03c72| \u2264 2 \u221a \u03bd\u030c\u03b9\nn\u030c +\n5H\u03b9 3 4\nn\u030c 3 4\n+ 2 \u221a \u03b9\nT n\u030c +\n2H\u03b9\nn\u030c . (24)\nFinally, combining (22) and (24), noting the definition of \u03b2 with (c1, c2, c3) = (2, 2, 5), and taking a union\nbound over all probability events, we have that with probability at least 1\u2212 2(H2T 3 + 3)\u03b4, it holds that\n\u03b2 \u2265 |\u03c71|+ |\u03c72|. (25)\nwhich means Q k+1 h (s, a, b) \u2265 Q\u2217h(s, a, b). Combining the two cases and taking the union bound over all steps, we have with probability at least 1\u2212 T (2H2T 3 + 7)\u03b4, it holds that Qk+1h (s, a, b) \u2265 Q\u2217h(s, a, b). Next, we show that V \u2217h (s) \u2264 V k+1 h (s). Note that\nV k+1\nh (s) = (D\u03c0k+1h Q\nk+1 h )(s)\n\u2265 sup \u00b5\u2208\u2206A (D\u00b5\u00d7\u03bdk+1h Q\nk+1 h )(s) (26)\n\u2265 sup \u00b5\u2208\u2206A (D\u00b5\u00d7\u03bdk+1h Q\u2217h)(s) (27) \u2265 sup \u00b5\u2208\u2206A inf \u03bd\u2208\u2206B (D\u00b5\u00d7\u03bdQ \u2217 h)(s) = V \u2217h (s),\nwhere (26) follows from the property of the CCE oracle, (27) follows because Q k+1 h (s, a, b) \u2265 Q \u2217 h(s, a, b), which has just been proved.\nStep ii: We show the inequalities for the pessimistic action-value function and value function below.\nThe two inequalities with respect to pessimistic (action-)value functions clearly hold for k = 1. For k \u2265 2, suppose Q\u2217h(s, a, b) \u2265 Quh(s, a, b) and V \u2217 h (s) \u2265 V uh(s) for any (s, a, h) \u2208 S \u00d7 A \u00d7 [H ] and u \u2208 [1, k]. Now we fix tuple (s, a, b, h) and we only need to consider the case when Q h (s, a, b) and V h(s) are updated.\nWe show Q\u2217h(s, a, b) \u2265 Qk+1h (s, a, b). Note that\nQk+1 h\n(s, a, b)\u2190 min { rh(s, a, b) + v\u030c\nn\u030c + \u03b3, rh(s, a, b) +\n\u00b5ref\nn + \u00b5\u030c n\u030c + \u03b2,Qk h (s, a, b)\n} ,\nand we have two non-trivial cases.\nFor the first case, by Hoeffding\u2019s inequality, with probability at least 1\u2212 \u03b4, it holds that\nQk+1 h\n(s, a, b) = rh(s, a, b) + v\u030c\nn\u030c \u2212 \u03b3\n= rh(s, a, b) + 1\nn\u030c\nn\u030c\u2211\ni=1\nV \u2113\u030cih+1(s \u2113\u030ci h+1)\u2212 2\n\u221a H2\nn\u030c \u03b9\n\u2264 rh(s, a, b) + 1\nn\u030c\nn\u030c\u2211\ni=1\nV \u2217h+1(s \u2113\u030ci h+1)\u2212 2\n\u221a H2\nn\u030c \u03b9 (28)\n\u2264 rh(s, a, b) + (PhV \u2217h+1)(s, a, b) (29) = Q\u2217h(s, a, b),\nwhere (28) follows from the induction hypothesis V uh+1(s) \u2265 V \u2217(s) for all u \u2208 [k], and (29) follows from Azuma-Hoeffding\u2019s inequality.\nFor the second case, we have\nQk+1 h\n(s, a, b) = rh(s, a, b) + \u00b5ref n + \u00b5\u030c n\u030c \u2212 \u03b2\n= rh(s, a, b) + 1\nn\nn\u2211\ni=1\nV ref,\u2113ih+1 (s \u2113i h+1) +\n1 n\u030c\nn\u030c\u2211\ni=1\n( V \u2113\u030cih+1 \u2212 V ref,\u2113\u030ci h+1 ) (s\u2113\u030cih+1)\u2212 \u03b2\n= rh(s, a, b) + ( Ph ( 1\nn\nn\u2211\ni=1\nV ref,\u2113ih+1\n)) (s, a, b) + ( Ph ( 1\nn\u030c\nn\u030c\u2211\ni=1\n( V \u2113\u030cih+1 \u2212 V ref,\u2113\u030ci h+1 ))) (s, a, b)\n+ \u03c7 1 + \u03c7 2 \u2212 \u03b2\n\u2264 rh(s, a, b) + ( Ph ( 1\nn\u030c\nn\u030c\u2211\ni=1\nV \u2113\u030cih+1\n)) (s, a, b) + \u03c7\n1 + \u03c7 2 \u2212 \u03b2 (30)\n\u2264 rh(s, a, b) + ( PhV \u2217 h+1 ) (s, a, b) + \u03c7\n1 + \u03c7 2 \u2212 \u03b2 (31)\n= Q\u2217 h (s, a, b) + \u03c7 1 + \u03c7 2 \u2212 \u03b2,\nwhere\n\u03c7 1 (k, h) =\n1 n\nn\u2211\ni=1\n( V ref,\u2113ih (s \u2113i h+1)\u2212 ( PhV ref,\u2113i h+1 ) (s, a, b) ) ,\nW \u2113h+1 = V \u2113 h+1 \u2212 V ref,\u2113h+1\n\u03c7 2 (k, h) =\n1 n\u030c\nn\u030c\u2211\ni=1\n( W \u2113\u030cih+1(s \u2113\u030ci h+1)\u2212 ( PhW \u2113\u030ci h+1 ) (s, a, b) ) .\nHere, (30) follows from the fact that V ref,uh+1 (s) is non-decreasing in u (since V ref h (s) for a pair (s, h) can only be updated once and the updated value is obviously greater than the initial value 0), and (31) follows from the induction hypothesis V kh+1(s) \u2264 V \u2217h+1(s). By Lemma G.2 with \u01eb = 1T 2 , with probability at least 1\u2212 2(H2T 3 + 1)\u03b4 it holds\n|\u03c7 1 (k, h)| \u2264 2\n\u221a\u2211n i=1 V(Ps,a,b,h, V ref,\u2113i h+1 )\u03b9\nn2 +\n2 \u221a \u03b9 Tn + 2H\u03b9 n , (32)\n|\u03c7 2 (k, h)| \u2264 2\n\u221a\u2211n\u030c i=1 V(Ps,a,b,h, V ref,\u2113i h+1 )\u03b9\nn\u030c2 +\n2 \u221a \u03b9 T n\u030c + 2H\u03b9 n\u030c . (33)\nLemma C.3. With probability at least 1\u2212 2\u03b4, it holds that n\u2211\ni=1\nV(Ps,a,b,h, V ref,\u2113i h+1 ) \u2264 n\u03bdref + 3H2\n\u221a n\u03b9 (34)\nProof: Note that n\u2211\ni=1\nV(Ps,a,b,h, V ref,\u2113i h+1 ) =\nn\u2211\ni=1\n( Ps,a,b,h(V ref,\u2113i h+1 ) 2 \u2212 (Ps,a,b,hV ref,\u2113ih+1 )2 )\n= n\u2211\ni=1\n(V ref,\u2113ih+1 (s \u2113i h+1)) 2 \u2212 1 n\n( n\u2211\ni=1\nV ref,\u2113ih+1 (s \u2113i h+1)\n)2 + \u03c7\n3 + \u03c7 4 + \u03c7 5\n= n\u03bdref + \u03c7 3 + \u03c7 4 + \u03c7 5 , (35)\nwhere\n\u03c7 3 =\nn\u2211\ni=1\n( (Ps,a,b,h(V ref,\u2113i h+1 ) 2 \u2212 (V ref,\u2113ih+1 (s\u2113ih+1))2 ) ,\n\u03c7 4 =\n1 n\n( n\u2211\ni=1\nV ref,\u2113ih+1 (s \u2113i h+1) )2 \u2212 1 n ( n\u2211\ni=1\nPs,a,b,hV ref,\u2113i h+1\n)2 ,\n\u03c7 5 =\n1 n\n( n\u2211\ni=1\nPs,a,b,hV ref,\u2113i h+1\n)2 \u2212 n\u2211\ni=1\n(Ps,a,b,hV ref,\u2113i h+1 ) 2.\nBy Azuma\u2019s inequality, with probability at least 1\u2212 \u03b4 it holds that |\u03c7 3 | \u2264 H2\n\u221a 2n\u03b9.\nBy Azuma\u2019s inequality, with probability at least 1\u2212 \u03b4, it holds that\n|\u03c7 4 | = 1\nn \u2223\u2223\u2223\u2223\u2223\u2223 ( n\u2211\ni=1\nV ref,\u2113ih+1 (s \u2113i h+1)\n)2 \u2212 ( n\u2211\ni=1\nPs,a,b,hV ref,\u2113i h+1 )2\u2223\u2223\u2223\u2223\u2223\u2223\n\u2264 2H \u2223\u2223\u2223\u2223\u2223 n\u2211\ni=1\nV ref,\u2113ih+1 (s \u2113i h+1)\u2212\nn\u2211\ni=1\nPs,a,b,hV ref,\u2113i h+1 \u2223\u2223\u2223\u2223\u2223\n\u2264 2H2 \u221a 2n\u03b9.\nMoreover, \u03c75 \u2264 0 by Cauchy-Schwartz inequality. Substituting the above inequalities gives the desired result.\nCombining (32) with (34) gives\n|\u03c7 1 | \u2264 2\n\u221a \u03bdref\u03b9\nn +\n5H\u03b9 3 4\nn 3 4\n+ 2 \u221a \u03b9\nTn +\n2H\u03b9\nn . (36)\nSimilar to Lemma C.3, we have the following lemma.\nLemma C.4. With probability at least 1\u2212 2\u03b4, it holds that n\u030c\u2211\ni=1\nV(Ps,a,b,h,W ref,\u2113i h+1 ) \u2264 n\u030c\u03bd\u030c + 3H2\n\u221a n\u030c\u03b9. (37)\nCombining (33) with (37) gives\n|\u03c7 2 | \u2264 2\n\u221a \u03bd\u030c\u03b9\nn\u030c +\n5H\u03b9 3 4\nn\u030c 3 4\n+ 2 \u221a \u03b9\nT n\u030c +\n2H\u03b9\nn\u030c . (38)\nFinally, combining (36) and (38), noting the definition of \u03b2 with (c1, c2, c3) = (2, 2, 5), and taking a union\nbound over all probability events, we have that with probability at least 1\u2212 2(H2T 3 + 3)\u03b4, it holds that\n\u03b2 \u2265 |\u03c7 1 |+ |\u03c7 2 |. (39)\nwhich gives Qk+1 h (s, a, b) \u2264 Q\u2217h(s, a, b). Combining the two cases and taking union bound over all steps, we have with probability at least 1\u2212 T (2H2T 3 + 7)\u03b4, it holds that Qk+1 h\n(s, a, b) \u2264 Q\u2217h(s, a, b). We show that V \u2217h (s) \u2264 V kh(s). Note that\nV k+1h (s) = (D\u03c0k+1 h Qk+1 h )(s)\n\u2264 inf \u03bd\u2208\u2206B (D\u00b5k+1h \u00d7\u03bd Qk+1 h )(s) (40) \u2264 inf \u03bd\u2208\u2206B (D\u00b5k+1h \u00d7\u03bd Q\u2217h)(s) (41) \u2264 inf \u03bd\u2208\u2206B sup \u00b5\u2208\u2206A (D\u00b5\u00d7\u03bdQ \u2217 h)(s) = V \u2217h (s),\nwhere (40) follows from the property of the CCE oracle, (41) follows because Qk+1 h (s, a, b) \u2264 Q\u2217 h (s, a, b), which has just been proved.\nThe entire proof is completed by combining step i and step ii, and taking a union bound over all\nprobability events.\nD Proof of Lemma B.2 (Step II)\nFirst, by Hoeffing\u2019s inequality, for any (k, h) \u2208 [K]\u00d7 [H ], with probability at least 1\u2212 2T\u03b4 it holds that \u2223\u2223\u2223\u2223\u2223\u2223 1 n\u030ckh n\u030ckh\u2211\ni=1\nV \u2113\u030ci h+1(s \u2113\u030ci h+1)\u2212Q k h(s k h, a k h, b k h) \u2223\u2223\u2223\u2223\u2223\u2223 \u2264 \u03b3kh ,\n\u2223\u2223\u2223\u2223\u2223\u2223 1 n\u030ckh n\u030ckh\u2211\ni=1\nV \u2113\u030cih+1(s \u2113\u030ci h+1)\u2212Qkh(s k h, a k h, b k h) \u2223\u2223\u2223\u2223\u2223\u2223 \u2264 \u03b3kh .\nThe entire proof will be conditioned on the above event.\nFor any weight sequence {wk}Kk=1 where wk \u2265 0, let \u2016w\u2016\u221e = max1\u2264k\u2264K wk and \u2016w\u20161 = \u2211K k=1 wk. By the update rule of the action-value function, we have\n\u2206kh = (V k h \u2212 V kh)(skh)\n= \u03b6kh + (Q k h \u2212Qkh)(s k h, a k h, b k h)\n\u2264 \u03b6kh + 2\u03b3kh + 1\nn\u030ckh\nn\u030ckh\u2211\ni=1\n(V \u2113\u030ci h+1 \u2212 V \u2113\u030cih+1)(s\u2113\u030cih+1) +H1{nkh = 0}\n= \u03b6kh + 2\u03b3 k h +\n1\nn\u030ckh\nn\u030ckh\u2211\ni=1\n\u2206\u2113\u030cih+1 +H1{nkh = 0}. (42)\nNote that\nK\u2211\nk=1\nwk n\u030ckh\nn\u030ckh\u2211\ni=1\n\u2206\u2113\u030cih+1 =\nK\u2211\nj=1\nwj n\u030cjh\nn\u030cjh\u2211\ni=1\n\u2206 \u2113\u030cjh,i h+1\n= K\u2211\nj=1\nwj n\u030cjh\nK\u2211\nk=1\n\u2206kh+1\nn\u030cjh\u2211\ni=1\n1{k = \u2113\u030cjh,i}\n= K\u2211\nk=1\n\u2206kh+1\nK\u2211\nj=1\nwj n\u030cjh\nn\u030cjh\u2211\ni=1\n1{k = \u2113\u030cjh,i}\n=\nK\u2211\nk=1\nw\u0303k\u2206 k h+1, (43)\nwhere we define w\u0303k = \u2211K j=1 wj\nn\u030cjh\n\u2211n\u030cjh i=1 1{k = \u2113\u030c j h,i}. Similar to the proof of Lemma B.4, we have\n\u2016w\u0303\u2016\u221e = maxk w\u0303k \u2264 (1 + 1 H ) \u2016w\u2016\u221e . (44)\nMoreover,\n\u2016w\u0303\u20161 = K\u2211\nk=1\nK\u2211\nj=1\nwj n\u030cjh\nn\u030cjh\u2211\ni=1\n1{k = \u2113\u030cjh,i} = K\u2211\nj=1\nwj n\u030cjh\nK\u2211\nk=1\nn\u030cjh\u2211\ni=1\n1{k = \u2113\u030cjh,i} = K\u2211\nj=1\nwj = \u2016w\u20161 . (45)\nCombining (42), (43), (44) and (45), we have\nK\u2211\nk=1\nwk\u2206 k h \u2264\nK\u2211\nk=1\nwk\u03b6 k h + 2\nK\u2211\nk=1\nwk\u03b3 k h +\nK\u2211\nk=1\nw\u0303k\u2206 k h+1 +H\nK\u2211\nk=1\nwk1{nkh = 0}\n\u2264 K\u2211\nk=1\nwk\u03b6 k h + 2\nK\u2211\nk=1\nwk\u03b3 k h +\nK\u2211\nk=1\nw\u0303k\u2206 k h+1 + SABH 2 \u2016w\u2016\u221e . (46)\nBy Azuma-Hoeffding\u2019s inequality, with probability at least 1\u2212H\u03b4, it holds that for any h \u2208 [H ]\nK\u2211\nk=1\nwk\u03b6 k h \u2264 \u221a 2H\u03b9\n\u221a\u221a\u221a\u221a K\u2211\nk=1\nwk \u2264 \u221a 2H\u03b9 \u2016w\u2016\u221e . (47)\nWe now bound the second term of (46). Define \u039e(s, a, b, j) = \u2211K\nk=1 wk1{n\u030ckh = ej, (skh, akh, bkh) = (s, a, b)} and \u039e(s, a, b) =\n\u2211 j\u22651 \u039e(s, a, b, j). Similar to (44) and (45), we then have \u039e(s, a, b, j) \u2264 \u2016w\u2016\u221e (1 + 1H )ej and\u2211\ns,a \u039e(s, a, b) = \u2211 k wk. Then\n\u2211\nk\nwk\u03b3 k h =\n\u2211\nk\n2 \u221a H2\u03b9wk\n\u221a 1\nn\u030ckh\n= 2 \u221a H2\u03b9 \u2211\ns,a,b,j\n\u221a 1\nej\nK\u2211\nj=1\nwk1{n\u030ckh = ej , (skh, akh, bkh) = (s, a, b)}\n= 2 \u221a H2\u03b9 \u2211\ns,a,b\n\u2211 j\u22651 \u039e(s, a, b, j)\n\u221a 1\nej .\nFix (s, a, b) and consider \u2211 j\u22651 \u039e(s, a, b, j) \u221a 1 ej . Note that \u221a 1 ej is decreasing in j. Given \u2211 j\u22651 \u039e(s, a, b, j) = \u039e(s, a, b) is fixed, rearranging the inequality gives\n\u2211 j\u22651 \u039e(s, a, b, j)\n\u221a 1\nej \u2264 \u2211\nj\u22651\n\u221a 1\nej \u2016w\u2016\u221e (1 +\n1\nH )ej1\n{ j\u22121\u2211\ni=1\n\u2016w\u2016\u221e (1 + 1\nH )ei \u2264 \u039e(s, a, b)\n}\n= \u2016w\u2016\u221e (1 + 1 H ) \u2211\nj\n\u221a ej1\n{ j\u22121\u2211\ni=1\n\u2016w\u2016\u221e ei \u2264 \u039e(s, a, b) }\n\u2264 10(1 + 1 H\n) \u221a \u2016w\u2016\u221eH\u039e(s, a, b).\nTherefore, by Cauchy-Schwartz inequality, we have\nK\u2211\nk=1\nwk\u03b3 k h \u2264 2\n\u221a H2\u03b9 \u2211\ns,a,b\n10(1 + 1 H ) \u221a \u2016w\u2016\u221eH \u221a \u039e(s, a, b)\n\u2264 20 \u221a H2\u03b9(1 + 1 H ) \u221a \u2016w\u2016\u221e SABH \u2016w\u20161. (48)\nCombining (46), (47) and (48), we have\nK\u2211\nk=1\nwk\u2206 k h \u2264\nK\u2211\nk=1\nw\u0303k\u2206 k h+1 + ( \u221a 2H\u03b9+ SABH2) \u2016w\u2016\u221e + 80H \u221a \u2016w\u2016\u221e SABH \u2016w\u20161 \u03b9. (49)\nWe expand the expression by iterating over step h+ 1, \u00b7 \u00b7 \u00b7 , H , K\u2211\nk=1\nwk\u2206 k h \u2264 (1 +\n1\nH )H \u00b7H \u00b7\n( ( \u221a 2H\u03b9+ SABH2) \u2016w\u2016\u221e + 80H \u221a \u2016w\u2016\u221e SABH \u2016w\u20161 \u03b9 )\n\u2264 6(H2\u03b9+ SABH3) \u2016w\u2016\u221e + 240H 5 2 \u221a \u2016w\u2016\u221e SAB \u2016w\u20161 \u03b9.\nNow we set wk = 1{\u2206kh \u2265 \u01eb}, and obtain K\u2211\nk=1\n1{\u2206kh \u2265 \u01eb}\u2206kh \u2264 6(H2\u03b9+ SABH3) \u2016w\u2016\u221e + 240H 5 2 \u221a\u221a\u221a\u221a\u2016w\u2016\u221e SAB\u03b9 K\u2211\nk=1\n1{\u2206kh \u2265 \u01eb}.\nNote that \u2016w\u2016\u221e is either 0 or 1. If \u2016w\u2016\u221e = 0, the claim obviously holds. In the case when \u2016w\u2016\u221e = 1, solving the following quadratic equation (ignoring coefficients) with respect to (\u2211K k=1 1{\u2206kh \u2265 \u01eb} )1/2 gives the desired result\n\u01eb\n( K\u2211\nk=1\n1{\u2206kh \u2265 \u01eb} ) \u2212H5/2(SAB\u03b9)1/2 ( K\u2211\nk=1\n1{\u2206kh \u2265 \u01eb} )1/2 \u2212 (SABH3 +H2\u03b9) \u2264 0.\nE Proof of Lemma B.5 (Step IV)\nThe entire proof is conditioned on the successful events of Lemma B.1 and Lemma B.2, which occur with probability at least 1\u2212O(H2T 4)\u03b4. By the definition of \u039bkh+1, we have\nH\u2211\nh=1\nK\u2211\nk=1\n(1 + 1\nH )h\u22121\u039bkh+1 =\nH\u2211\nh=1\nK\u2211\nk=1\n(1 + 1\nH )h\u22121\u03c8kh+1\n\ufe38 \ufe37\ufe37 \ufe38 T1\n+\nH\u2211\nh=1\nK\u2211\nk=1\n(1 + 1\nH )h\u22121\u03bekh+1\n\ufe38 \ufe37\ufe37 \ufe38 T2\n+ 2\nH\u2211\nh=1\nK\u2211\nk=1\n(1 + 1 H )h\u22121\u03b2 k h\n\ufe38 \ufe37\ufe37 \ufe38 T3\n+2\nH\u2211\nh=1\nK\u2211\nk=1\n(1 + 1 H )h\u22121\u03b2k h\n\ufe38 \ufe37\ufe37 \ufe38 T4\n. (50)\nWe next bound each of the above four terms in one subsection, and summarize the final result in Appendix E.5.\nE.1 Bound T1 Recall the definition \u03bbkh(s) = 1 { nkh(s) < N0 } . Since \u03c8 is always non-negative, we have\nH\u2211\nh=1\nK\u2211\nk=1\n(1 + 1 H )h\u22121\u03c8kh+1\n\u2264 3 H\u2211\nh=1\nK\u2211\nk=1\n\u03c8kh+1\n= 3 H\u2211\nh=1\nK\u2211\nk=1\nPskh,akh,bkh,h   1 nkh nkh\u2211\ni=1\n( V\nref,\u2113i h+1 \u2212 V ref,\u2113ih+1\n) \u2212 ( V REF h+1 \u2212 V REFh+1 )  \n\u2264 3H H\u2211\nh=1\nK\u2211\nk=1\nPskh,akh,bkh,h   1 nkh nkh\u2211\ni=1\n\u03bb\u2113ih+1\n \n\u2264 3H H\u2211\nh=1\nK\u2211\nj=1\nK\u2211\nk=1\nPskh,akh,bkh,h\u03bb j h+1\n1\nnkh\nnkh\u2211\ni=1\n1{j = \u2113kh,i}\n\u2264 3H H\u2211\nh=1\nK\u2211\nj=1\nPsjh,a j h,b j h,h \u03bbjh+1\nK\u2211\nk=1\n1\nnkh\nnkh\u2211\ni=1\n1{j = \u2113kh,i} (51)\n\u2264 6(logT + 1)H H\u2211\nh=1\nK\u2211\nk=1\nPskh,akh,bkh,h\u03bb k h+1 (52)\n\u2264 6(logT + 1)H ( H\u2211\nh=1\nK\u2211\nk=1\n\u03bbkh+1(s k h+1) +\nH\u2211\nh=1\nK\u2211\nk=1\n( Pskh,akh,bkh,h \u2212 1skh+1 ) \u03bbkh+1\n)\n\u2264 6(logT + 1)H ( HSN0 + H\u2211\nh=1\nK\u2211\nk=1\n( Pskh,akh,bkh,h \u2212 1skh+1 ) \u03bbkh+1\n)\n\u2264 6(logT + 1)H ( HSN0 + 2 \u221a T \u03b9 ) , (53)\nwhere (51) follows from the fact that 1 nkh\n\u2211nkh i=1 1{j = \u2113kh,i} 6= 0 only if (skh, akh, bkh) = (s j h, a j h, b j h), (52) follows\nbecause\nK\u2211\nk=1\n1\nnkh\nnkh\u2211\ni=1\n1{j = \u2113kh,i} \u2264 \u2211\nz:j\u2264 \u2211z\u22121\ni=1 ei\u2264T\nez\u2211z\u22121 i=1 ei \u2264 2(logT + 1),\nand (53) holds with probability at least 1\u2212 \u03b4 by Azuma\u2019s inequality. To conclude, with probability at least 1\u2212 \u03b4, it holds that\nH\u2211\nh=1\nK\u2211\nk=1\n(1 + 1\nH )h\u22121\u03c8kh+1 \u2264 O(log T ) \u00b7 (H2SN0 +H\n\u221a T \u03b9). (54)\nE.2 Term T2\nWe first derive\nH\u2211\nh=1\nK\u2211\nk=1\n(1 + 1\nH )h\u22121\u03bekh+1\n= H\u2211\nh=1\nK\u2211\nk=1\n1\nn\u030ckh\nn\u030ckh\u2211\ni=1\n( Psk\nh ,ak h ,bk h ,h \u2212 1s\u2113\u030cih+1\n)( V\n\u2113\u030ci h+1 \u2212 V \u2113\u030cih+1\n)\n=\nH\u2211\nh=1\nK\u2211\nk=1\nK\u2211\nj=1\n(1 + 1 H )h\u22121 1\nn\u030ckh\nn\u030ckh\u2211\ni=1\n( Pskh,akh,bkh,h \u2212 1sjh+1 )( V j h+1 \u2212 V jh+1 ) 1{\u2113\u030ckh,i = j}.\nNote that \u2113\u030ckh,i = j if and only if (s k h, a k h, b k h) = (s j h, a j h, b j h). Therefore,\nH\u2211\nh=1\nK\u2211\nk=1\n(1 + 1\nH )h\u22121\u03bekh+1\n\u2264 H\u2211\nh=1\nK\u2211\nj=1\n(1 + 1\nH )h\u22121 ( Psjh,a j h,b j h,h \u2212 1sjh+1 )( V j h+1 \u2212 V jh+1 ) K\u2211\nk=1\n1\nn\u030ckh\nn\u030ckh\u2211\ni=1\n1{\u2113\u030ckh,i = j}\n=\nH\u2211\nh=1\nK\u2211\nk=1\n\u03b8jh+1 ( Psjh,a j h,b j h,h \u2212 1sjh+1 )( V j h+1 \u2212 V jh+1 ) ,\nwhere in the last equation we define \u03b8jh+1 = (1 + 1 H ) h\u22121\u2211K k=1\n1 n\u030ckh\n\u2211n\u030ckh i=1 1{\u2113\u030ckh,i = j}.\nFor (j, h) \u2208 [K]\u00d7 [H ], let xjh be the number of elements in current state with respect to (s j h, a j h, b j h, h) and\n\u03b8\u0303jh+1 := (1 + 1 H )\nh\u22121 \u230a(1+ 1H )x j h\u230b\nxjh \u2264 3. Define K = {(k, h) : \u03b8kh+1 = \u03b8\u0303kh+1}. Note that if k is before the second\nlast stage of the tuple (skh, a k h, b k h, h), then we have that \u03b8 k h+1 = \u03b8\u0303 k h+1 and (k, h) \u2208 K. Given (k, h) \u2208 K, skh+1 follows the transition Pskh,akh,bkh,h.\nLet K\u22a5h (s, a, b) = {k : (skh, akh, bkh) = (s, a, b),where k is in the second last stage of (s, a, b, h)}. Note that for different j, k, if (skh, a k h, b k h) = (s j h, a j h, b j h) and j, k are in the same stage of (s k h, a k h, b k h, h), then \u03b8 k h+1 = \u03b8 j h+1 and \u03b8\u0303kh+1 = \u03b8\u0303 j h+1. Denote \u03b8h+1 and \u03b8\u0303h+1 as \u03b8h+1(s, a, b) and \u03b8\u0303h+1(s, a, b) respectively for some k \u2208 K\u22a5h (s, a, b).\nWe have\nH\u2211\nh=1\nK\u2211\nk=1\n(1 + 1\nH )h\u22121\u03bekh+1\n= \u2211\n(k,h)\n\u03b8\u0303kh+1 ( Psjh,a j h,b j h,h \u2212 1sjh+1 )( V j h+1 \u2212 V jh+1 )\n+ \u2211\n(k,h)\n(\u03b8kh+1 \u2212 \u03b8\u0303kh+1) ( Psjh,a j h,b j h,h \u2212 1sjh+1 )( V j h+1 \u2212 V jh+1 )\n= \u2211\n(k,h)\n\u03b8\u0303kh+1 ( Psj\nh ,aj h ,bj h ,h \u2212 1sj h+1\n)( V j h+1 \u2212 V jh+1 )\n+ \u2211\n(k,h)\u2208K\n(\u03b8kh+1 \u2212 \u03b8\u0303kh+1) ( Psjh,a j h,b j h,h \u2212 1sjh+1 )( V j h+1 \u2212 V jh+1 ) . (55)\nSince \u03b8\u0303kh+1 is independent of s k h+1, by Azuma\u2019s inequality, with probability at least 1\u2212 \u03b4, it holds that\n\u2211\n(k,h)\n\u03b8\u0303kh+1 ( Pskh,akh,bkh,h \u2212 1skh+1 )( V k h+1 \u2212 V kh+1 ) \u2264 6 \u221a TH2\u03b9. (56)\nMoreover, we have \u2211\n(k,h)\u2208K\n(\u03b8kh+1 \u2212 \u03b8\u0303kh+1) ( Pskh,akh,bkh,h \u2212 1skh+1 )( V k h+1 \u2212 V kh+1 )\n= \u2211\ns,a,b,h\n\u2211\n(k,h)\u2208K\n1{(skh, akh, bkh) = (s, a, b)}(\u03b8kh+1 \u2212 \u03b8\u0303kh+1) ( Pskh,akh,bkh,h \u2212 1skh+1 )( V k h+1 \u2212 V kh+1 )\n= \u2211\ns,a,b,h\n(\u03b8h+1(s, a, b)\u2212 \u03b8\u0303h+1(s, a)) \u2211\n(k,h)\u2208K\u22a5h (s,a)\n(\u03b8kh+1 \u2212 \u03b8\u0303kh+1) ( Pskh,akh,bkh,h \u2212 1skh+1 )( V k h+1 \u2212 V kh+1 )\n\u2264 \u2211\ns,a,b,h\nO(H) \u221a |K\u22a5h (s, a, b)|\u03b9 (57)\n\u2264 \u2211\ns,a,b,h\nO(H) \u221a N\u030cK+1h (s, a, b)\u03b9\n\u2264 O(H) \u221a SABH\u03b9 \u2211\ns,a,b,h\nN\u030cK+1h (s, a, b) (58)\n\u2264 O(H) \u221a SABH\u03b9(T/H), (59)\nwhere (57) holds with probability at least 1\u2212T\u03b4 by Azuma\u2019s inequality and a union bound over all steps in K, (58) follows from Cauchy-Schwartz inequality, and (59) follows from the fact that the length of the last two stages for each (s, a, b, h) tuple is only O(1/H) fraction of the total number of visits.\nCombining (55), (56) and (59), we obtain that with probability at least 1\u2212 (T + 1)\u03b4, it holds that H\u2211\nh=1\nK\u2211\nk=1\n(1 + 1\nH )h\u22121\u03bekh+1 \u2264 O(\n\u221a H2SABT \u03b9). (60)\nE.3 Term T3\nNote that\nH\u2211\nh=1\nK\u2211\nk=1\n(1 + 1\nH )h\u22121\u03b2 k h\n\u2264 3 H\u2211\nh=1\nK\u2211\nk=1\n c1 \u221a \u03bdref,kh nkh \u03b9+ c2 \u221a \u03bd\u030c k h n\u030ckh \u03b9+ c3 ( H\u03b9 nkh + H\u03b9 n\u030ckh + H\u03b9 3 4 (nkh) 3 4 + H\u03b9 3 4 (n\u030ckh) 3 4 ) \n\u2264 O\n  H\u2211\nh=1\nK\u2211\nk=1\n  \u221a \u03bdref,kh nkh \u03b9+ \u221a \u03bd\u030c k h n\u030ckh \u03b9     +O(SABH3\u03b9 logT + (SAB\u03b9) 34H 52 T 14 ), (61)\nwhere (61) follows from Lemma G.3 with \u03b1 = 34 and \u03b1 = 1.\nStep i: We bound \u2211H\nh=1 \u2211K k=1\n\u221a \u03bdref,k h\nnk h\n\u03b9. We begin with the following technical lemmas.\nLemma E.1. With probability at least 1\u2212 2T\u03b4, it holds that for all s, a, b, h, k,\nQ k h(s, a, b) \u2264 Q\u03c0 k h (s, a, b) + (H \u2212 h) ( \u03b2 + HSN0\nn\u030ckh\n) ,\nQk h (s, a, b) \u2265 Q\u03c0kh (s, a, b)\u2212 (H \u2212 h)\n( \u03b2 +\nHSN0 n\u030ckh\n) ,\nV k h(s) \u2264 V \u03c0 k h (s) + (H \u2212 h) ( \u03b2 + HSN0\nn\u030ckh\n) ,\nV kh(s) \u2265 V \u03c0 k h (s)\u2212 (H \u2212 h) ( \u03b2 + HSN0\nn\u030ckh\n) .\nThe proof is provided in Appendix E.3.1.\nLemma E.2. Conditioned on the successful event of Lemma E.1, with probability at least 1 \u2212 4\u03b4, it holds that\n\u03bdref,kh \u2212 V(Pskh,akh,bkh,h, V \u03c0k h+1) \u2264 4H\u03b2 +\n12H2\u03b2 + 18H3SN0\nnkh + 20H2\n\u221a \u03b9\nnkh .\nThe proof is provided in Appendix E.3.2.\nLemma E.3 (Lemma C.5 in [19]). With probability at least 1\u2212 \u03b4, it holds that\nV(Pskh,akh,bkh,h, V \u03c0k h+1) \u2264 O(HT +H3\u03b9).\nCombining Lemma E.2, Lemma E.3 and Lemma G.3 (see Appendix G), we have\nH\u2211\nh=1\nK\u2211\nk=1\n\u221a \u03bdref,kh nkh \u03b9\n\u2264 H\u2211\nh=1\nK\u2211\nk=1\n\u221a V(Pskh,akh,bkh,h, V \u03c0k h+1)\nnkh \u03b9\n+ H\u2211\nh=1\nK\u2211\nk=1\n\u221a\u221a\u221a\u221a ( 4H\u03b2\nnkh +\n12H2\u03b2 + 18H3SN0\n(nkh) 2\n+ 20H2 \u03b9 1 2\n(nkh) 3 2\n) \u03b9\n\u2264 O\n  \u2211\ns,a,b,h\n\u221a NK+1h (s, a, b)V(Ps,a,b,h, V \u03c0k h+1)\u03b9\n \n+O\n  \u2211\ns,a,b,h\n\u221a NK+1h (s, a, b)H\u03b2\u03b9+ (S 3 2ABH 5 2N 1 2 0 + SABH 2\u03b2 1 2 )\u03b9 1 2 logT + (SAB\u03b9) 3 4H 7 4T 1 4\n \n\u2264 O (\u221a SABH2T \u03b9+ \u221a SABH2\u03b2T \u03b9+ (S 3 2ABH 5 2N 1 2\n0 + SABH 2\u03b2 1 2 )\u03b9 1 2 logT + (SAB\u03b9) 3 4H 7 4 T 1 4\n) . (62)\nStep ii: We bound \u2211H\nh=1 \u2211K k=1\n\u221a \u03bd\u030c k h\nn\u030ckh \u03b9.\nBy Lemma B.1, Lemma B.2 and Corollary B.3, we have\n\u03bd\u030c k h \u2264\n1\nn\u030ckh\nn\u030ckh\u2211\ni=1\n( V\n\u2113\u030ci h+1 \u2212 V ref,\u2113\u030ci h+1 )2 (s\u2113\u030cih+1)\n\u2264 1 n\u030ckh\nn\u030ckh\u2211\ni=1\n( V\n\u2113\u030ci h+1 \u2212 V \u2113\u030cih+1 )2 (s\u2113\u030cih+1) + 1\nn\u030ckh\nn\u030ckh\u2211\ni=1\n( V\nref,\u2113\u030ci h+1 \u2212 V ref,\u2113\u030cih+1 )2 (s\u2113\u030cih+1)\n\u2264 2 n\u030ckh H2SN0 + 2\u03b2 2.\nCombining the above inequality with Lemma G.3, we obtain\nH\u2211\nh=1\nK\u2211\nk=1\n\u221a \u03bd\u030c k h\u03b9\nn\u030ckh \u2264 O\n(\u221a SABH3\u03b22T \u03b9+ SABH3 \u221a SN0\u03b9 logT ) . (63)\nCombining (61), (62) and (63), we obtain that with probability at least 1\u2212O(T )\u03b4, it holds that\nH\u2211\nh=1\nK\u2211\nk=1\n(1 + 1\nH )h\u22121\u03b2 k\nh \u2264 O (\u221a SABH2T \u03b9+ \u221a SABH2\u03b2T \u03b9+ \u221a SABH3\u03b22T \u03b9\n+ S 3 2ABH3N\n1 2\n0 \u03b9 logT + SABH 2\u03b2 1 2 \u03b9 1 2 logT + (SAB\u03b9) 3 4H 5 2T 1 4\n) . (64)\nE.3.1 Proof of Lemma E.1\nFix an episode k. The proof is based on induction over h = H,H \u2212 1, . . . , 1. Note first that the claim clearly holds for h = H . Assume the inequalities hold at step h+ 1.\nBy the update rule of the action-value function, we have\nQ k h(s, a, b) \u2264 rh(s, a, b) + 1\nn\u030c\nn\u030c\u2211\ni=1\nV \u2113\u030ci h+1(s \u2113\u030ci h+1) + \u03b3\n= rh(s, a, b) + 1\nn\u030c\nn\u030c\u2211\ni=1\nV k h+1(s \u2113\u030ci h+1) + \u03b3 +\n1 n\u030c\nn\u030c\u2211\ni=1\n( V\n\u2113\u030ci h+1(s \u2113\u030ci h+1)\u2212 V k h+1(s \u2113\u030ci h+1)\n)\n\u2264 rh(s, a, b) + Ps,a,b,hV k h+1 + 1\nn\u030c\nn\u030c\u2211\ni=1\n( V\n\u2113\u030ci h+1(s \u2113\u030ci h+1)\u2212 V k h+1(s \u2113\u030ci h+1)\n) (65)\n\u2264 rh(s, a, b) + Ps,a,b,hV \u03c0 k h+1 + (H \u2212 h+ 1) ( \u03b2 +\nHSN0 n\u030c\n)\n+ 1\nn\u030c\nn\u030c\u2211\ni=1\n( V \u2113\u030ci h+1(s \u2113\u030ci h+1)\u2212 V k h+1(s \u2113\u030ci h+1) ) (66)\n\u2264 Q\u03c0kh + (H \u2212 h+ 1) ( \u03b2 +\nHSN0 n\u030c\n) + 1\nn\u030c\nn\u030c\u2211\ni=1\n( V \u2113\u030ci h+1(s \u2113\u030ci h+1)\u2212 V \u2113\u030cih+1(s\u2113\u030cih+1) ) (67)\n\u2264 Q\u03c0kh (s, a, b) + (H \u2212 h+ 1) ( \u03b2 +\nHSN0 n\u030c\n) + 1\nn\u030c\nn\u030c\u2211\ni=1\n(H\u03bb\u2113\u030cih+1 + \u03b2)\n\u2264 Q\u03c0kh (s, a, b) + (H \u2212 h) ( \u03b2 +\nHSN0 n\u030c\n) , (68)\nwhere (65) holds with probability at least 1 \u2212 \u03b4 by Azuma\u2019s inequality, (66) follows from the induction hypothesis, and (67) follows from Lemma B.1.\nMoreover, by the update rule of the value function, we have\nV k h(s) = E(a,b)\u223c\u03c0kQ k h(s, a, b)\n\u2264 E(a,b)\u223c\u03c0kQ\u03c0 k h (s, a, b) + (H \u2212 h) ( \u03b2 +\nHSN0 n\u030c\n)\n\u2264 V \u03c0kh (s) + (H \u2212 h) ( \u03b2 +\nHSN0 n\u030c\n) .\nThe other direction for the pessimistic (action-)value function can be proved similarly. Finally, taking\nthe union bound over all steps gives the desired result.\nE.3.2 Proof of Lemma E.2\nWe first provide bound on \u03bdref,kh \u2212 V(Pskh,akh,bkh,h, V ref,\u2113i h+1 ). Recall (21) that\n\u03bdref \u2212 1 nkh\nnkh\u2211\ni=1\nV(Pskh,akh,bkh,h, V ref,\u2113i h+1 ) = \u2212\n1\nnkh (\u03c76 + \u03c77 + \u03c78),\nwhere\n\u03c76 =\nnkh\u2211\ni=1\n( (Psk\nh ,ak h ,bk h ,h(V\nref,\u2113i h+1 ) 2 \u2212 (V ref,\u2113ih+1 (s\u2113ih+1))2 ) ,\n\u03c77 = 1\nnkh\n  nkh\u2211\ni=1\nV ref,\u2113i h+1 (s \u2113i h+1)\n  2\n\u2212 1 nkh\n  nkh\u2211\ni=1\nPskh,akh,bkh,hV ref,\u2113i h+1\n  2\n,\n\u03c78 = 1\nnkh\n  nkh\u2211\ni=1\nPskh,akh,bkh,hV ref,\u2113i h+1\n  2 \u2212 nkh\u2211\ni=1\n(Pskh,akh,bkh,hV ref,\u2113i h+1 ) 2.\nBy Azuma\u2019s inequality, with probability at least 1\u2212 2\u03b4, it holds that\n|\u03c76| \u2264 H2 \u221a 2nkh\u03b9, |\u03c77| \u2264 2H2 \u221a 2nkh\u03b9.\nMoreover, we have\n\u2212\u03c78 = nkh\u2211\ni=1\n( Pskh,akh,bkh,hV ref,\u2113i h+1 )2 \u2212 1 nkh\n  nkh\u2211\ni=1\nPskh,akh,bkh,hV ref,\u2113i h+1\n  2\n\u2264 nkh\u2211\ni=1\n( Pskh,akh,bkh,hV ref,\u2113i h+1 )2 \u2212 1 nkh\n  nkh\u2211\ni=1\nPskh,akh,bkh,hV REF h+1\n  2\n(69)\n=\nnkh\u2211\ni=1\n(( Pskh,akh,bkh,hV ref,\u2113i h+1 )2 \u2212 ( Pskh,akh,bkh,hV REF h+1 )2)\n\u2264 2H2 nkh\u2211\ni=1\nPskh,akh,bkh,h\u03bb \u2113i h+1\n= 2H2\n  nkh\u2211\ni=1\n\u03bb\u2113ih+1(s \u2113i h+1) +\nnkh\u2211\ni=1\n(Pskh,akh,bkh,h \u2212 1s\u2113ih+1)\u03bb \u2113i h+1\n \n\u2264 2H2SN0 + 2H2 \u221a 2nkh\u03b9, (70)\nwhere (69) follows from the fact that V ref,k h+1 \u2265 V REF h+1 for any k, h, and (70) holds with probability at least 1\u2212 \u03b4 by Azuma\u2019s inequality. We have\n\u03bdref,kh \u2212 1\nnkh\nnkh\u2211\ni=1\nV(Pskh,akh,bkh,h, V ref,\u2113i h+1 ) \u2264 2H2SN0 nkh\n+ 8H2 \u221a \u03b9\nnkh . (71)\nTherefore,\n\u03bdref,kh \u2212 V(Pskh,akh,bkh,h, V \u03c0k h+1)\n= 1\nnkh\nnkh\u2211\ni=1\n( V(Pskh,akh,bkh,h, V ref,\u2113i h+1 )\u2212 V(Pskh,akh,bkh,h, V \u03c0k h+1) )\n+  \u03bdref,kh \u2212 1\nnkh\nnkh\u2211\ni=1\nV(Pskh,akh,bkh,h, V ref,\u2113i h+1 )\n \n\u2264 1 nkh\nnkh\u2211\ni=1\n( V(Pskh,akh,bkh,h, V ref,\u2113i h+1 )\u2212 V(Pskh,akh,bkh,h, V \u03c0k h+1) ) + 2H2SN0\nnkh + 8H2\n\u221a \u03b9\nnkh (72)\n\u2264 4H nkh\nnkh\u2211\ni=1\n\u2223\u2223\u2223Pskh,akh,bkh,h(V ref,\u2113i h+1 \u2212 V \u03c0 k h+1) \u2223\u2223\u2223+ 2H 2SN0\nnkh + 8H2\n\u221a \u03b9\nnkh\n= 4H\nnkh\nnkh\u2211\ni=1\n\u2223\u2223\u2223\u2223Pskh,akh,bkh,h(V ref,\u2113i h+1 \u2212 V \u03c0 k h+1 + V \u2217 h+1 \u2212 V \u2217h+1)\u2212H ( \u03b2 + HSN0\nn\u030ckh\n) +H ( \u03b2 + HSN0\nn\u030ckh\n)\u2223\u2223\u2223\u2223\n+ 2H2SN0\nnkh + 8H2\n\u221a \u03b9\nnkh\n\u2264 4H nkh\nnkh\u2211\ni=1\nPskh,akh,bkh,h(V ref,\u2113i h+1 \u2212 V \u2217h+1) +\n4H nkh\nnkh\u2211\ni=1\nPskh,akh,bkh,h\n( V \u03c0 k h+1 \u2212 V \u2217h+1 +H ( \u03b2 + HSN0\nn\u030ckh\n))\n+ 4H2\nnkh\n( \u03b2 + HSN0\nn\u030ckh\n) + 2H2SN0\nnkh + 8H2\n\u221a \u03b9\nnkh (73)\n\u2264 4H nkh\nnkh\u2211\ni=1\n(V ref,\u2113i h+1 \u2212 V \u2217h+1)(s\u2113ih+1) +\n4H nkh\nnkh\u2211\ni=1\n( (V \u03c0 k h+1 \u2212 V \u2217h+1)(s\u2113ih+1) +H ( \u03b2 + HSN0\nn\u030ckh\n))\n+ 4H2\nnkh\n( \u03b2 + HSN0\nn\u030ckh\n) + 2H2SN0\nnkh + 20H2\n\u221a \u03b9\nnkh (74)\n\u2264 ( 4H\u03b2 + 4H2SN0\nnkh\n) + 8H2\nnkh\n( \u03b2 + HSN0\nn\u030ckh\n)\n+ 4H2\nnkh\n( \u03b2 + HSN0\nn\u030ckh\n) + 2H2SN0\nnkh + 20H2\n\u221a \u03b9\nnkh (75)\n= 4H\u03b2 + 12H2\u03b2\nnkh + 20H2\n\u221a \u03b9\nnkh +\n6H2SN0\nnkh +\n12H3SN0\nnkhn\u030c k h\n\u2264 4H\u03b2 + 12H 2\u03b2 + 18H3SN0\nnkh + 20H2\n\u221a \u03b9\nnkh ,\nwhere (72) follows from (71), (73) follows from Lemma B.1 and Lemma E.1, (74) holds with probability at least 1\u2212 2\u03b4 by Azuma\u2019s inequality, and (75) follows from Lemma B.1 and Lemma E.1.\nE.4 Term T4 The proof is similar to that for the term \u2211H\nh=1 \u2211K k=1(1+ 1 H ) h\u22121\u03b2 k h. In the following, we will present the key\nsteps, and provide the proof whenever necessary.\nBy Lemma G.3, we have\nH\u2211\nh=1\nK\u2211\nk=1\n(1 + 1 H )h\u22121\u03b2 k h\n\u2264 3 H\u2211\nh=1\nK\u2211\nk=1\n c1 \u221a \u03bdref,kh nkh \u03b9+ c2 \u221a \u03bd\u030c k h n\u030ckh \u03b9+ c3 ( H\u03b9 nkh + H\u03b9 n\u030ckh + H\u03b9 3 4 (nkh) 3 4 + H\u03b9 3 4 (n\u030ckh) 3 4 ) \n\u2264 O\n  H\u2211\nh=1\nK\u2211\nk=1\n  \u221a \u03bdref,kh nkh \u03b9+ \u221a \u03bd\u030c k h n\u030ckh \u03b9     +O(SABH3\u03b9 logT + (SAB\u03b9) 34H 52 T 14 ). (76)\nStep i: Bound term \u2211H\nh=1 \u2211K k=1\n\u221a \u03bdref,k h\nnkh \u03b9.\nLemma E.4. Conditioned on the successful event of Lemma E.1, with probability at least 1 \u2212 4\u03b4, it holds that\n\u03bdref,kh \u2212 V(Pskh,akh,bkh,h, V \u03c0k h+1) \u2264 4H\u03b2 +\n12H2\u03b2 + 18H3SN0\nnkh + 20H2\n\u221a \u03b9\nnkh .\nThe proof is provided in Appendix E.4.1. Combining Lemma E.3, Lemma E.4 and Lemma G.3, we have\nH\u2211\nh=1\nK\u2211\nk=1\n\u221a \u03bdref,kh nkh \u03b9\n\u2264 O (\u221a SABH2T \u03b9+ \u221a SABH2\u03b2T \u03b9+ (S 3 2ABH 5 2N 1 2\n0 + SABH 2\u03b2 1 2 )\u03b9 1 2 logT + (SAB\u03b9) 3 4H 7 4 T 1 4\n) . (77)\nStep ii: Bound \u2211H\nh=1 \u2211K k=1 \u221a \u03bd\u030ckh n\u030ckh \u03b9. By Lemma B.1, Lemma B.2 and Corollary B.3, we have\n\u03bd\u030ckh \u2264 1\nn\u030ckh\nn\u030ckh\u2211\ni=1\n( V \u2113\u030cih+1 \u2212 V ref,\u2113\u030ci h+1 )2 (s\u2113\u030cih+1)\n\u2264 1 n\u030ckh\nn\u030ckh\u2211\ni=1\n( V\n\u2113\u030ci h+1 \u2212 V \u2113\u030cih+1 )2 (s\u2113\u030cih+1) + 1\nn\u030ckh\nn\u030ckh\u2211\ni=1\n( V\nref,\u2113\u030ci h+1 \u2212 V ref,\u2113\u030cih+1 )2 (s\u2113\u030cih+1)\n\u2264 2 n\u030ckh H2SN0 + 2\u03b2 2.\nCombining the above inequality with Lemma G.3, we obtain\nH\u2211\nh=1\nK\u2211\nk=1\n\u221a \u03bd\u030ckh\u03b9\nn\u030ckh \u2264 O\n(\u221a SABH3\u03b22T \u03b9+ SABH3 \u221a SN0\u03b9 logT ) . (78)\nTherefore, combining (76), (77) and (78) gives that with probability at least 1\u2212O(T )\u03b4, it holds that\nH\u2211\nh=1\nK\u2211\nk=1\n(1 + 1\nH )h\u22121\u03b2 k\nh \u2264 O (\u221a SABH2T \u03b9+ \u221a SABH2\u03b2T \u03b9+ \u221a SABH3\u03b22T \u03b9\n+ S 3 2ABH3N\n1 2\n0 \u03b9 logT + SABH 2\u03b2 1 2 \u03b9 1 2 logT + (SAB\u03b9) 3 4H 5 2T 1 4\n) . (79)\nE.4.1 Proof of Lemma E.4\nRecall (35) that\n\u03bdref \u2212 1 nkh\nnkh\u2211\ni=1\nV(Pskh,akh,bkh,h, V ref,\u2113i h+1 ) = \u2212\n1\nnkh (\u03c7 6 + \u03c7 7 + \u03c7 8 ),\nwhere\n\u03c7 6 =\nnkh\u2211\ni=1\n( (Pskh,akh,bkh,h(V ref,\u2113i h+1 ) 2 \u2212 (V ref,\u2113ih+1 (s\u2113ih+1))2 ) ,\n\u03c7 7 =\n1\nnkh\n  nkh\u2211\ni=1\nV ref,\u2113ih+1 (s \u2113i h+1)\n  2\n\u2212 1 nkh\n  nkh\u2211\ni=1\nPskh,akh,bkh,hV ref,\u2113i h+1\n  2\n,\n\u03c7 8 =\n1\nnkh\n  nkh\u2211\ni=1\nPskh,akh,bkh,hV ref,\u2113i h+1\n  2 \u2212 nkh\u2211\ni=1\n(Pskh,akh,bkh,hV ref,\u2113i h+1 ) 2.\nBy Azuma\u2019s inequality, with probability at least 1\u2212 2\u03b4, it holds that\n|\u03c7 6 | \u2264 H2 \u221a 2nkh\u03b9, |\u03c77| \u2264 2H 2 \u221a 2nkh\u03b9.\nThe term \u03c7 8 is bounded slightly differently from \u03c78 as follows:\n\u2212\u03c7 8 =\nnkh\u2211\ni=1\n( Pskh,akh,bkh,hV ref,\u2113i h+1 )2 \u2212 1 nkh\n  nkh\u2211\ni=1\nPskh,akh,bkh,hV ref,\u2113i h+1\n  2\n\u2264 nkh\u2211\ni=1\n( Pskh,akh,bkh,hV REF h+1 )2 \u2212 1 nkh\n  nkh\u2211\ni=1\nPskh,akh,bkh,hV ref,\u2113i h+1\n  2\n(80)\n= 1\nnkh\n  nkh\u2211\ni=1\nPskh,akh,bkh,hV REF h+1\n  2\n\u2212 1 nkh\n  nkh\u2211\ni=1\nPskh,akh,bkh,hV ref,\u2113i h+1\n  2\n\u2264 2H2 nkh\u2211\ni=1\nPsk h ,ak h ,bk h ,h\u03bb \u2113i h+1\n= 2H2\n  nkh\u2211\ni=1\n\u03bb\u2113ih+1(s \u2113i h+1) +\nnkh\u2211\ni=1\n(Pskh,akh,bkh,h \u2212 1s\u2113ih+1)\u03bb \u2113i h+1\n \n\u2264 2H2SN0 + 2H2 \u221a 2nkh\u03b9, (81)\nwhere (80) follows from the fact that V ref,kh+1 \u2264 V REFh+1 for any k, h, and (81) holds with probability at least 1\u2212 \u03b4 due to Azuma\u2019s inequality. Therefore,\n\u03bdref,kh \u2212 1\nnkh\nnkh\u2211\ni=1\nV(Pskh,akh,bkh,h, V ref,\u2113i h+1 ) \u2264\n2H2SN0\nnkh + 8H2\n\u221a \u03b9\nnkh . (82)\nBy a similar argument as in Appendix E.3.2, we can obtain the desired result\n\u03bdref,kh \u2212 V(Pskh,akh,bkh,h, V \u03c0k h+1)\n= 1\nnkh\nnkh\u2211\ni=1\n( V(Pskh,akh,bkh,h, V ref,\u2113i h+1 )\u2212 V(Pskh,akh,bkh,h, V \u03c0k h+1) )\n+  \u03bdref,kh \u2212 1\nnkh\nnkh\u2211\ni=1\nV(Pskh,akh,bkh,h, V ref,\u2113i h+1 )\n \n\u2264 1 nkh\nnkh\u2211\ni=1\n( V(Pskh,akh,bkh,h, V ref,\u2113i h+1 )\u2212 V(Pskh,akh,bkh,h, V \u03c0k h+1) ) + 2H2SN0\nnkh + 8H2\n\u221a \u03b9\nnkh (83)\n\u2264 4H nkh\nnkh\u2211\ni=1\n\u2223\u2223\u2223Pskh,akh,bkh,h(V ref,\u2113i h+1 \u2212 V \u03c0 k h+1) \u2223\u2223\u2223+ 2H 2SN0\nnkh + 8H2\n\u221a \u03b9\nnkh\n= 4H\nnkh\nnkh\u2211\ni=1\n\u2223\u2223\u2223\u2223Pskh,akh,bkh,h(V ref,\u2113i h+1 \u2212 V \u03c0 k h+1 + V \u2217 h+1 \u2212 V \u2217h+1)\u2212H ( \u03b2 + HSN0\nn\u030ckh\n) +H ( \u03b2 + HSN0\nn\u030ckh\n)\u2223\u2223\u2223\u2223\n+ 2H2SN0\nnkh + 8H2\n\u221a \u03b9\nnkh\n\u2264 4H nkh\nnkh\u2211\ni=1\nPskh,akh,bkh,h(V \u2217 h+1 \u2212 V ref,\u2113ih+1 ) +\n4H nkh\nnkh\u2211\ni=1\nPskh,akh,bkh,h\n( V \u2217h+1 \u2212 V \u03c0 k h+1 +H ( \u03b2 + HSN0\nn\u030ckh\n))\n+ 4H2\nnkh\n( \u03b2 + HSN0\nn\u030ckh\n) + 2H2SN0\nnkh + 8H2\n\u221a \u03b9\nnkh (84)\n\u2264 4H nkh\nnkh\u2211\ni=1\n(V \u2217h+1 \u2212 V ref,\u2113ih+1 )(s\u2113ih+1) + 4H\nnkh\nnkh\u2211\ni=1\n( (V \u2217h+1 \u2212 V \u03c0 k h+1)(s \u2113i h+1) +H ( \u03b2 + HSN0\nn\u030ckh\n))\n+ 4H2\nnkh\n( \u03b2 + HSN0\nn\u030ckh\n) + 2H2SN0\nnkh + 20H2\n\u221a \u03b9\nnkh (85)\n\u2264 ( 4H\u03b2 + 4H2SN0\nnkh\n) + 8H2\nnkh\n( \u03b2 + HSN0\nn\u030ckh\n)\n+ 4H2\nnkh\n( \u03b2 + HSN0\nn\u030ckh\n) + 2H2SN0\nnkh + 20H2\n\u221a \u03b9\nnkh (86)\n= 4H\u03b2 + 12H2\u03b2\nnkh + 20H2\n\u221a \u03b9\nnkh +\n6H2SN0\nnkh +\n12H3SN0\nnkhn\u030c k h\n\u2264 4H\u03b2 + 12H 2\u03b2 + 18H3SN0\nnkh + 20H2\n\u221a \u03b9\nnkh ,\nwhere (83) follows from (82), (84) follows from Lemma B.1 and Lemma E.1, (85) holds with probability at least 1\u2212 2\u03b4 by Azuma\u2019s inequality, and (86) follows from Lemma B.1 and Lemma E.1.\nE.5 Summarizing Terms T1-T4 Together\nRecall that \u03b2 = 1\u221a H , and N0 = c4SABH 5\u03b9 \u03b22 = O(SABH 6\u03b9). By combining (50), (54), (60), (64) and (79), we conclude that with probability at least 1\u2212O(H2T 4)\u03b4, the following bound holds:\nH\u2211\nh=1\nK\u2211\nk=1\n(1 + 1\nH )h\u22121\u039bkh+1\n\u2264 O(log T ) \u00b7 (H2SN0 +H \u221a T \u03b9) +O(H \u221a SABT \u03b9)\n+O (\u221a SABH2T \u03b9+ \u221a SABH2\u03b2T \u03b9+ \u221a SABH3\u03b22T \u03b9\n+ S 3 2ABH3N\n1 2\n0 \u03b9 logT + SABH 2\u03b2 1 2 \u03b9 1 2 logT + (SAB\u03b9) 3 4H 5 2T 1 4\n)\n= O (\u221a SABH2T \u03b9+H \u221a T \u03b9 logT + (SAB\u03b9) 3 4H 5 2T 1 4 )\n+O (\u221a SABH2\u03b2T \u03b9+ \u221a SABH3\u03b22T \u03b9+ SABH2\u03b2 1 2 \u03b9 1 2 logT ) +O ( (H2SN0 + S 3 2ABH3N 1 2 0 \u03b9) logT )\n= O (\u221a SABH2T \u03b9+H \u221a T \u03b9 logT + S2(AB) 3 2H8\u03b9 3 2T 1 4 ) . (87)\nF Proof of Lemma B.6 (Final Step)\nOur construction of the correlated policy is inspired by the \u201ccertified policies\u201d in [5].\nBased on the trajectory of the distributions {\u03c0kh}h\u2208[H],k\u2208[K] specified by Algorithm 1, we construct a correlated policy \u03c0\u0302kh = \u00b5\u0302 k h\u00d7 \u03bd\u0302kh for each (h, k) \u2208 [H ]\u00d7 [K]. The max-player\u2019s policies \u00b5\u0302kh and \u00b5\u0302kh+1[s, a, b] are defined in Algorithm 3, and the min-player\u2019s policies can be defined similarly. Further, we define the final output policy \u03c0out in Algorithm 2, which first uniformly samples an index k from [K], and then proceeds with \u03c0\u0302k1 . We remark that based on Algorithm 3 and Algorithm 4, the policies \u00b5\u0302 k h, \u03bd\u0302 k h , \u00b5\u0302 k h+1[s, a, b], \u03bd\u0302 k h+1[s, a, b] do not depend on the history before step h. Therefore, the action-value functions are well-defined for the corresponding steps.\nAlgorithm 3 Certified policy \u00b5\u0302kh (max-player version)\n1: Initialize k\u2032 \u2190 k. 2: for step h\u2032 \u2190 h, h+ 1, . . . , H do 3: Receive sh\u2032 , and take action ah\u2032 \u223c \u00b5k \u2032\nh (\u00b7|sh\u2032). 4: Observe bh\u2032 , and sample j \u2190 Unif([Nk \u2032 h\u2032 (sh\u2032 , ah\u2032 , bh\u2032)]) 5: Set k\u2032 \u2190 \u2113\u030ck\u2032h\u2032,j . 6: end for\nIn order to show Lemma B.6, it suffices to show the following inequalities\nQ k h(s, a, b) \u2265 Q \u2020,\u03bd\u0302kh+1[s,a,b] h (s, a, b), V k h(s) \u2265 V \u2020,\u03bd\u0302kh h (s), Qk h (s, a, b) \u2265 Q\u00b5\u0302 k h+1[s,a,b],\u2020 h (s, a, b), V k h(s) \u2265 V \u00b5\u0302kh,\u2020 h (s).\ndue to the definition of output policy in Algorithm 2.\nAlgorithm 4 Policy \u00b5\u0302kh+1[s, a, b] (max-player version)\n1: Sample j \u2190 Unif([Nkh (s, a, b)]) 2: k\u2032 \u2190 \u2113\u030ckh,j . 3: for step h\u2032 \u2190 h+ 1, . . . , H do 4: Receive sh\u2032 , and take action ah\u2032 \u223c \u00b5k \u2032\nh (\u00b7|sh\u2032). 5: Observe bh\u2032 , and sample j \u2190 Unif([Nk \u2032 h\u2032 (sh\u2032 , ah\u2032 , bh\u2032)]) 6: Set k\u2032 \u2190 \u2113\u030ck\u2032h\u2032,j . 7: end for\nConsider a fixed tuple (s, a, b, h, k). Note that the result clearly holds for any s, a, b that is in its first\nstage, due to our initialization of Q k\nh(s, a, b), Q k\nh (s, a, b) and V\nk h(s), V k h(s). In the following, we focus on the\ncase where those values have been updated at least once before the k-th episode.\nOur proof is based on induction on k. Note first that the claim clearly holds for k = 1. For k \u2265 2, assume the claim holds for all u \u2208 [1 : k \u2212 1]. If those values are not updated in the k-th episode, then the claim clearly holds.In the following, we consider the case where those values has just been updated.\n(I) We show Q k h(s, a, b) \u2265 Q \u2020,\u03bd\u0302kh+1[s,a,b] h (s, a, b). Recall the update rule of the optimistic action-value function\nQh(s, a, b)\u2190 min { rh(s, a, b) + v\u030c\nn\u030c + \u03b3, rh(s, a, b) +\n\u00b5ref\nn + \u00b5\u030c n\u030c + \u03b2,Q k h(s, a, b)\n} .\nBesides the last term, there are two non-trivial cases and we will show both of the first two terms are lower-bounded by Q \u2020,\u03bd\u0302kh+1[s,a,b] h (s, a, b).\nFor the first case, we have\nQ k h(s, a, b) = rh(s, a, b) + 1\nn\u030ckh\nn\u030ckh\u2211\ni=1\nV \u2113\u030ci h+1(s \u2113\u030ci h+1) + \u03b3 k h\n\u2265 rh(s, a, b) + 1\nn\u030ckh\nn\u030ckh\u2211\ni=1\nV \u2020,\u03bd\u0302 \u2113\u030cih+1 h+1 (s \u2113\u030ci h+1) + \u03b3 k h (88)\n\u2265 1 n\u030ckh\nn\u030ckh\u2211\ni=1\nQ \u2020,\u03bd\u0302 \u2113\u030cih+1 h (s, a, b) (89)\n\u2265 sup \u00b5\n1\nn\u030ckh\nn\u030ckh\u2211\ni=1\nQ \u00b5,\u03bd\u0302\n\u2113\u030ci h+1\nh (s, a, b) (90)\n\u2265 Q\u2020,\u03bd\u0302 k h+1[s,a,b]\nh (s, a, b), (91)\nwhere (88) follows from the induction hypothesis, (89) follows from the Azuma\u2019s inequality, (90) follows from the fact that taking the maximum out of the summation does not increase the sum, and (91) follows from the construction of policy \u03bd\u0302kh+1[s, a, b] (obtained via the min-player\u2019s counterpart of Algorithm 4).\nFor the second case,\nQ k h(s, a, b) = rh(s, a, b) + 1\nnkh\nnkh\u2211\ni=1\nV ref,\u2113i h+1 (s \u2113i h+1) +\n1\nn\u030ckh\nn\u030ckh\u2211\ni=1\n( V\n\u2113\u030ci h+1 \u2212 V ref,\u2113\u030ci h+1 ) (s\u2113\u030cih+1) + \u03b2 k h\n\u2265 rh(s, a, b) + Ph   1 n\u030ckh n\u030ckh\u2211\ni=1\nV \u2113\u030ci h+1   (s, a, b) + \u03c71 + \u03c72 + \u03b2 k h\n\u2265 rh(s, a, b) + Ph   1 n\u030ckh n\u030ckh\u2211\ni=1\nV \u2020,\u03bd\u0302 \u2113\u030cih+1 h+1   (s, a, b) (92)\n= 1\nn\u030ckh\nn\u030ckh\u2211\ni=1\nQ \u2020,\u03bd\u0302 \u2113\u030cih+1 h (s, a, b)\n\u2265 sup \u00b5\n1\nn\u030ckh\nn\u030ckh\u2211\ni=1\nQ \u00b5,\u03bd\u0302\n\u2113\u030ci h+1\nh (s, a, b) (93)\n\u2265 Q\u2020,\u03bd\u0302 k h+1[s,a,b]\nh (s, a, b), (94)\nwhere\n\u03c71(k, h) = 1\nn\nn\u2211\ni=1\n( V\nref,\u2113i h (s \u2113i h+1)\u2212 ( PhV ref,\u2113i h+1 ) (s, a, b) ) ,\nW \u2113 h+1 = V \u2113 h+1 \u2212 V ref,\u2113 h+1\n\u03c72(k, h) = 1\nn\u030c\nn\u030c\u2211\ni=1\n( W\n\u2113\u030ci h+1(s \u2113\u030ci h+1)\u2212 ( PhW \u2113\u030ci h+1 ) (s, a, b) ) .\nHere, (92) follows from the concentration result \u03b2 \u2265 \u03c71+\u03c72 (see (25)), (93) follows from the fact that taking the maximum out of summation does not increase the sum, and (94) follows from the construction of policy \u03bd\u0302kh+1[s, a, b] (obtained via the min-player\u2019s counterpart of Algorithm 4).\n(II) We show V k+1 h (s) \u2265 V \u2020,\u03bd\u0302kh h (s). Note that\nV k h(s) = (D\u03c0khQ k h)(s) \u2265 sup \u00b5 (D\u00b5\u00d7\u03bdkhQ k h)(s)\n\u2265 sup \u00b5\nEa\u223c\u00b5,b\u223c\u03bdkhQ \u2020,\u03bd\u0302kh+1[s,a,b] h (s, a, b) = V \u2020,\u03bd\u030ckh h (s),\nwhere the first inequality follows from the property of the CCE oracle and the second inequality follows from the induction hypothesis.\nThe other side of bounds can be proved similarly forQk h (s, a, b), Q \u00b5\u030ckh+1[s,a,b],\u2020 h (s, a, b), V k h(s), andQ \u00b5\u030ckh+1,\u2020 h (s).\nG Supporting Lemmas\nLemma G.1 (Azuma-Hoefdding\u2019s inequality). Suppose {Xk}k\u22650 is a martingale and |Xk \u2212 Xk\u22121| \u2264 ck almost surely. Then, for all positive integers N and all positive \u01eb, it holds that\nP[|XN \u2212X0| \u2265 \u01eb] \u2264 2 exp ( \u2212 \u01eb 2\n2 \u2211N\nk=1 c 2 k\n) .\nLemma G.2 (Lemma 10 in [44]). Let {Mn}n\u22650 be martingale such that M0 = 0 and |Mn \u2212 Mn\u22121| \u2264 c for some c > 0 and any n \u2265 1. Let Varn = \u2211n k=1 E[(Mk \u2212 Mk\u22121)2|Fk\u22121] for n \u2265 0, where Fk = \u03c3(M1,M2, . . . ,Mk). Then for any positive integer n, and any \u01eb, p > 0, we have\nP [ |Mn| \u2265 2 \u221a Varn log 1\np + 2\n\u221a \u01eb log 1\np + 2c log\n1 p\n] \u2264 ( 2nc2\n\u01eb + 2\n) p.\nLemma G.3 (Variant of Lemma 11 in [44]). For any \u03b1 \u2208 (0, 1) and non-negative weights {wh(s, a)}s\u2208S,a\u2208A,b\u2208B,h\u2208[H], it holds that\nK\u2211\nk=1\nH\u2211\nh=1\nwh(s k h, a k h, b k h)\n(nkh) \u03b1\n\u2264 2 \u03b1 1\u2212 \u03b1 \u2211\ns,a,b,h\nwh(s, a, b)(N K+1 h (s, a, b)) 1\u2212\u03b1,\nK\u2211\nk=1\nH\u2211\nh=1\nwh(s k h, a k h, b k h)\n(n\u030ckh) \u03b1\n\u2264 2 2\u03b1H\u03b1 1\u2212 \u03b1 \u2211\ns,a,b,h\nwh(s, a, b)(N K+1 h (s, a, b)) 1\u2212\u03b1.\nIn the case \u03b1 = 1, it holds that\nK\u2211\nk=1\nH\u2211\nh=1\nwh(s k h, a k h, b k h)\nnkh \u2264 2\n\u2211\ns,a,b,h\nwh(s, a, b) log(N K+1 h (s, a, b)),\nK\u2211\nk=1\nH\u2211\nh=1\nwh(s k h, a k h, b k h)\nn\u030ckh \u2264 4H\n\u2211\ns,a,b,h\nwh(s, a, b) log(N K+1 h (s, a, b))."
        }
    ],
    "title": "Model-Free Algorithm with Improved Sample Efficiency for Zero-Sum Markov Games",
    "year": 2023
}