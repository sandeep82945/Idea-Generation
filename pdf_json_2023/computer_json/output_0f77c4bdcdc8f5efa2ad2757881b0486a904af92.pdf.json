{
    "abstractText": "A fitness function is a type of objective function that quantifies the optimality of a solution; the correct formulation of this function is relevant, in evolutionary-based ATS systems, because it must indicate the quality of the summaries. Several unsupervised evolutionary methods for the automatic text summarization (ATS) task proposed in current standards require authors to manually construct an objective function that guides the algorithms to create good-quality summaries. In this sense, it is necessary to test each fitness function created to measure its performance; however, this process is time consuming and only a few functions are analyzed. This study proposes the automatic generation of heuristic functions, through genetic programming (GP), to be applied in the ATS task. Therefore, our proposed method for ATS provides an automatically generated fitness function for cluster-based unsupervised approaches. The results of this study, using two standard collections, demonstrate to automatically obtain an orientation function that leads to good quality abstracts. INDEX TERMS Automatic text summarization, clustering, genetic programming, genetic algorithms, heuristic functions.",
    "authors": [
        {
            "affiliations": [],
            "name": "\u00c1NGEL HERN\u00c1NDEZ-CASTA\u00d1EDA"
        },
        {
            "affiliations": [],
            "name": "ARNULFO GARC\u00cdA-HERN\u00c1NDEZ"
        },
        {
            "affiliations": [],
            "name": "YULIA LEDENEVA"
        },
        {
            "affiliations": [],
            "name": "Ren\u00e9 Arnulfo Garc\u00eda-Hern\u00e1ndez"
        }
    ],
    "id": "SP:493219710ea33d8c3b5fb8980bae2e1e607aa7b8",
    "references": [
        {
            "authors": [
                "A. Elsaid",
                "A. Mohammed",
                "L.F. Ibrahim",
                "M.M. Sakre"
            ],
            "title": "A comprehensive review of Arabic text summarization",
            "venue": "IEEE Access, vol. 10, pp. 38012\u201338030, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "A.A. Syed",
                "F.L. Gaol",
                "T. Matsuo"
            ],
            "title": "A survey of the state-of-the-art models in neural abstractive text summarization",
            "venue": "IEEE Access, vol. 9, pp. 13248\u201313265, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "J.N. Madhuri",
                "R.G. Kumar"
            ],
            "title": "Extractive text summarization using sentence ranking",
            "venue": "Proc. Int. Conf. Data Sci. Commun. (IconDSC), Mar. 2019, pp. 1\u20133.",
            "year": 2019
        },
        {
            "authors": [
                "R. Elbarougy",
                "G. Behery",
                "A. El Khatib"
            ],
            "title": "Extractive Arabic text summarization using modified PageRank algorithm",
            "venue": "Egyptian Informat. J., vol. 21, no. 2, pp. 73\u201381, Jul. 2020.",
            "year": 2020
        },
        {
            "authors": [
                "S.G. Jindal",
                "A. Kaur"
            ],
            "title": "Automatic keyword and sentence-based text summarization for software bug reports",
            "venue": "IEEE Access, vol. 8, pp. 65352\u201365370, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "H. Gupta",
                "M. Patel"
            ],
            "title": "Method of text summarization using LSA and sentence based topicmodellingwith BERT",
            "venue": "inProc. Int. Conf. Artif. Intell. Smart Syst. (ICAIS), Mar. 2021, pp. 511\u2013517.",
            "year": 2021
        },
        {
            "authors": [
                "S. Ghodratnama",
                "A. Beheshti",
                "M. Zakershahrak",
                "F. Sobhanmanesh"
            ],
            "title": "Extractive document summarization based on dynamic feature space mapping",
            "venue": "IEEE Access, vol. 8, pp. 139084\u2013139095, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "R.M. Alguliyev",
                "R.M. Aliguliyev",
                "N.R. Isazade",
                "A. Abdi",
                "N. Idris"
            ],
            "title": "COSUM: Text summarization based on clustering and optimization",
            "venue": "Exp. Syst., vol. 36, no. 1, Feb. 2019, Art. no. e12340.",
            "year": 2019
        },
        {
            "authors": [
                "N.H. Casta\u00f1eda",
                "R.A.G. Hern\u00e1ndez",
                "Y. Ledeneva",
                "\u00c1.H. Casta\u00f1eda"
            ],
            "title": "Evolutionary automatic text summarization using cluster validation indexes",
            "venue": "Computaci\u00f3n Y Sistemas, vol. 24, no. 2, pp. 583\u2013595, Jun. 2020.",
            "year": 2020
        },
        {
            "authors": [
                "M.G. Smith",
                "L. Bull"
            ],
            "title": "Genetic programming with a genetic algorithm for feature construction and selection",
            "venue": "Genetic Program. Evolvable Mach., vol. 6, no. 3, pp. 265\u2013281, Sep. 2005.",
            "year": 2005
        },
        {
            "authors": [
                "R. Alqaisi",
                "W. Ghanem",
                "A. Qaroush"
            ],
            "title": "Extractive multi-document Arabic text summarization using evolutionarymulti-objective optimization with K-medoid clustering",
            "venue": "IEEE Access, vol. 8, pp. 228206\u2013228224, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "T. U\u00e7kan",
                "A. Karci"
            ],
            "title": "Extractive multi-document text summarization based on graph independent sets",
            "venue": "Egyptian Informat. J., vol. 21, no. 3, pp. 145\u2013157, Sep. 2020.",
            "year": 2020
        },
        {
            "authors": [
                "L. Dong",
                "M.N. Satpute",
                "W. Wu",
                "D. Du"
            ],
            "title": "Two-phase multidocument summarization through content-attention-based subtopic detection",
            "venue": "IEEE Trans. Computat. Social Syst., vol. 8, no. 6, pp. 1379\u20131392, Dec. 2021.",
            "year": 2021
        },
        {
            "authors": [
                "A. Sahu",
                "S.G. Sanjeevi"
            ],
            "title": "Better fine-tuning with extracted important sentences for abstractive summarization",
            "venue": "Proc. Int. Conf. Commun., Control Inf. Sci. (ICCISc), Jun. 2021, pp. 11328\u201311339.",
            "year": 2021
        },
        {
            "authors": [
                "M.T. Nayeem",
                "T.A. Fuad",
                "Y. Chali"
            ],
            "title": "Abstractive unsupervised multidocument summarization using paraphrastic sentence fusion",
            "venue": "Proc. 27th Int. Conf. Comput. Linguistics, 2018, pp. 1191\u20131204.",
            "year": 2018
        },
        {
            "authors": [
                "N. Alami",
                "M. Meknassi",
                "N. En-nahnahi",
                "Y. El Adlouni",
                "O. Ammor"
            ],
            "title": "Unsupervised neural networks for automatic Arabic text summarization using document clustering and topic modeling",
            "venue": "Exp. Syst. Appl., vol. 172, Jun. 2021, Art. no. 114652.",
            "year": 2021
        },
        {
            "authors": [
                "M.M. Haider",
                "Md. A. Hossin",
                "H.R. Mahi",
                "H. Arif"
            ],
            "title": "Automatic text summarization using Gensim Word2Vec and K-means clustering algorithm",
            "venue": "Proc. IEEE Region Symp. (TENSYMP), Jun. 2020, pp. 283\u2013286.",
            "year": 2020
        },
        {
            "authors": [
                "R. Rautray",
                "R.C. Balabantaray"
            ],
            "title": "An evolutionary framework for multi document summarization using cuckoo search approach: MDSCSA",
            "venue": "Appl. Comput. Informat., vol. 14, no. 2, pp. 134\u2013144, Jul. 2018. VOLUME 11, 2023 51463 \u00c1. Hern\u00e1ndez-Casta\u00f1eda et al.: Toward the Automatic Generation of an Objective Function",
            "year": 2018
        },
        {
            "authors": [
                "J.M. Sanchez-Gomez",
                "M.A. Vega-Rodr\u00edguez",
                "C.J. P\u00e9rez"
            ],
            "title": "Extractive multi-document text summarization using a multi-objective artificial bee colony optimization approach",
            "venue": "Knowl.-Based Syst., vol. 159, pp. 1\u20138, Nov. 2018.",
            "year": 2018
        },
        {
            "authors": [
                "I. Arroyo-Fern\u00e1ndez",
                "C.-F. M\u00e9ndez-Cruz",
                "G. Sierra",
                "J.-M. Torres- Moreno",
                "G. Sidorov"
            ],
            "title": "Unsupervised sentence representations as word information series: Revisiting TF\u2013IDF",
            "venue": "Comput. Speech Lang., vol. 56, pp. 107\u2013129, Jul. 2019.",
            "year": 2019
        },
        {
            "authors": [
                "M.K. Dahouda",
                "I. Joe"
            ],
            "title": "A deep-learned embedding technique for categorical features encoding",
            "venue": "IEEE Access, vol. 9, pp. 114381\u2013114391, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "D.M. Blei",
                "A.Y. Ng",
                "M.I. Jordan"
            ],
            "title": "Latent Dirichlet allocation",
            "venue": "J. Mach. Learn. Res., vol. 3, pp. 993\u20131022, Mar. 2003.",
            "year": 2003
        },
        {
            "authors": [
                "T. Mikolov",
                "L. Sutskever",
                "K. Chen",
                "G. Corrado",
                "J. Dean"
            ],
            "title": "Distributed representations of words and phrases and their compositionality",
            "venue": "Proc. Adv. Neural Inf. Process. Syst., vol. 2, Dec. 2013, pp. 3111\u20133119.",
            "year": 2013
        },
        {
            "authors": [
                "Q.V. Le",
                "T. Mikolov"
            ],
            "title": "Distributed representations of sentences and documents",
            "venue": "Proc. 31st Int. Conf. Mach. Learn., vol. 32, Jan. 2014, pp. 1188\u20131196.",
            "year": 2014
        },
        {
            "authors": [
                "Y. Liu",
                "Z. Li",
                "H. Xiong",
                "X. Gao",
                "J. Wu"
            ],
            "title": "Understanding of internal clustering validation measures",
            "venue": "Proc. IEEE Int. Conf. Data Mining, Dec. 2010, pp. 911\u2013916.",
            "year": 2010
        },
        {
            "authors": [
                "E. Rend\u00f3n",
                "I. Abundez",
                "A. Arizmendi",
                "E.M. Quiroz"
            ],
            "title": "Internal versus external cluster validation indexes",
            "venue": "Int. J. Comput. Commun., vol. 5, no. 1, pp. 27\u201334, 2011.",
            "year": 2011
        },
        {
            "authors": [
                "J.C. Dunn"
            ],
            "title": "Well-separated clusters and optimal fuzzy partitions",
            "venue": "J. Cybern., vol. 4, no. 1, pp. 95\u2013104, Jan. 1974.",
            "year": 1974
        },
        {
            "authors": [
                "D.L. Davies",
                "D.W. Bouldin"
            ],
            "title": "A cluster separation measure",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. PAMI-1, no. 2, pp. 224\u2013227, Apr. 1979.",
            "year": 1979
        },
        {
            "authors": [
                "P.J. Rousseeuw"
            ],
            "title": "Silhouettes: A graphical aid to the interpretation and validation of cluster analysis",
            "venue": "J. Comput. Appl. Math., vol. 20, pp. 53\u201365, Nov. 1987.",
            "year": 1987
        },
        {
            "authors": [
                "C.-Y. Lin"
            ],
            "title": "Rouge: A package for automatic evaluation of summaries",
            "venue": "Text Summarization Branches Out. Barcelona, Spain: Association for Computational Linguistics, 2004, pp. 74\u201381.",
            "year": 2004
        },
        {
            "authors": [
                "F. Neumann"
            ],
            "title": "Computational complexity analysis of multi-objective genetic programming",
            "venue": "Proc. 14th Annu. Conf. Genetic Evol. Comput., Jul. 2012, pp. 799\u2013806.",
            "year": 2012
        },
        {
            "authors": [
                "Z. Nopiah",
                "M. Khairir",
                "S. Abdullah",
                "M. Baharin",
                "A. Arifin"
            ],
            "title": "Time complexity analysis of the genetic algorithm clustering method",
            "venue": "Proc. 9th WSEAS Int. Conf. Signal Process., Robot. Autom., vol. 10, 2010, pp. 171\u2013176.",
            "year": 2010
        },
        {
            "authors": [
                "A. Vaswani",
                "N. Shazeer",
                "N. Parmar",
                "J. Uszkoreit",
                "L. Jones",
                "A.N. Gomez",
                "L. Kaiser",
                "I. Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Proc. Adv. Neural Inf. Process. Syst., vol. 30, 2017, pp. 1\u201311.",
            "year": 2017
        },
        {
            "authors": [
                "M. Pranjic",
                "M. Robnik Sikonja",
                "S. Pollak"
            ],
            "title": "An evaluation of BERT and Doc2Vec model on the IPTC subject codes prediction dataset",
            "venue": "Proc. 24th Int. Multiconference, D.Mladenic andM. Grobelnik, Eds. 2021, pp. 25\u201328.",
            "year": 2021
        },
        {
            "authors": [
                "M. Gambhir",
                "V. Gupta"
            ],
            "title": "Deep learning-based extractive text summarization with word-level attention mechanism",
            "venue": "Multimedia Tools Appl., vol. 81, no. 15, pp. 20829\u201320852, Jun. 2022.",
            "year": 2022
        },
        {
            "authors": [
                "R. Nallapati",
                "F. Zhai",
                "B. Zhou"
            ],
            "title": "SummaRuNNer: A recurrent neural network based sequence model for extractive summarization of documents",
            "venue": "Proc. AAAI Conf. Artif. Intell., vol. 31, 2017, pp. 3075\u20133081.",
            "year": 2017
        },
        {
            "authors": [
                "Y. Liu",
                "I. Titov",
                "M. Lapata"
            ],
            "title": "Single document summarization as tree induction",
            "venue": "Proc. Conf. North, 2019, pp. 1745\u20131755.",
            "year": 2019
        },
        {
            "authors": [
                "K.M. Hermann",
                "T. Kocisky",
                "E. Grefenstette",
                "L. Espeholt",
                "W. Kay",
                "M. Suleyman",
                "P. Blunsom"
            ],
            "title": "Teaching machines to read and comprehend",
            "venue": "Proc. Adv. Neural Inf. Process. Syst.Cambridge, MA, USA:MIT Press, 2015, pp. 1693\u20131701.",
            "year": 2015
        },
        {
            "authors": [
                "I. Tanfouri",
                "G. Tlik",
                "F. Jarray"
            ],
            "title": "An automatic Arabic text summarization system based on genetic algorithms",
            "venue": "Proc. Comput. Sci., vol. 189, pp. 195\u2013202, Jan. 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Q.A. Al-Radaideh",
                "D.Q. Bataineh"
            ],
            "title": "A hybrid approach for Arabic text summarization using domain knowledge and genetic algorithms",
            "venue": "Cognit. Comput., vol. 10, no. 4, pp. 651\u2013669, Aug. 2018.",
            "year": 2018
        },
        {
            "authors": [
                "E. V\u00e1zquez",
                "R.A. Garcia-Hern\u00e1ndez",
                "Y. Ledeneva"
            ],
            "title": "Sentence features relevance for extractive text summarization using genetic algorithms",
            "venue": "J. Intell. Fuzzy Syst., vol. 35, no. 1, pp. 353\u2013365, Jul. 2018.",
            "year": 2018
        },
        {
            "authors": [
                "W. Song",
                "L.C. Choi",
                "S.C. Park",
                "X.F. Ding"
            ],
            "title": "Fuzzy evolutionary optimization modeling and its applications to unsupervised categorization and extractive summarization",
            "venue": "Exp. Syst. Appl., vol. 38, no. 8, pp. 9112\u20139121, Aug. 2011.",
            "year": 2011
        },
        {
            "authors": [
                "R.A. Garcia-Hern\u00e4ndez",
                "Y. Ledeneva"
            ],
            "title": "Single extractive text summarization based on a genetic algorithm",
            "venue": "Proc. Mex. Conf. Pattern Recognit. Cham, Switzerland: Springer, 2013, pp. 374\u2013383.",
            "year": 2013
        },
        {
            "authors": [
                "X. Wan"
            ],
            "title": "Towards a unified approach to simultaneous single-document and multi-document summarizations",
            "venue": "Proc. 23rd Int. Conf. Comput. Linguistics, 2010, pp. 1137\u20131145.",
            "year": 2010
        },
        {
            "authors": [
                "K. Svore",
                "L. Vanderwende",
                "C. Burges"
            ],
            "title": "Enhancing single-document summarization by combining ranknet and third-party sources",
            "venue": "Proc. Joint Conf. Empirical Methods Natural Lang. Process. Comput. Natural Lang. Learn. (EMNLP-CoNLL), 2007, pp. 448\u2013457.",
            "year": 2007
        },
        {
            "authors": [
                "D. Shen",
                "J.-T. Sun",
                "H. Li",
                "Q. Yang",
                "Z. Chen"
            ],
            "title": "Document summarization using conditional random fields",
            "venue": "Proc. IJCAI, vol. 7. Burlington, MA, USA: Morgan Kaufmann, 2007, pp. 2862\u20132867.",
            "year": 2007
        },
        {
            "authors": [
                "S. Narayan",
                "S.B. Cohen",
                "M. Lapata"
            ],
            "title": "Ranking sentences for extractive summarization with reinforcement learning",
            "venue": "Proc. Conf. North Amer. Chapter Assoc. Comput. Linguistics, Human Lang. Technol., 2018, pp. 1747\u20131759.",
            "year": 2018
        },
        {
            "authors": [
                "A. See",
                "P.J. Liu",
                "C.D.Manning"
            ],
            "title": "Get to the point: Summarizationwith pointer-generator networks",
            "venue": "Proc. 55th Annu. Meeting Assoc. Comput. Linguistics, 2017, pp. 1073\u20131083.",
            "year": 2017
        }
    ],
    "sections": [
        {
            "text": "INDEX TERMS Automatic text summarization, clustering, genetic programming, genetic algorithms, heuristic functions.\nI. INTRODUCTION Recently, the internet has held a large amount of textual information of different kinds, such as academic, disclosure and general knowledge documents. A search on the internet can lead to the selection of a subset of documents that are appropriate for the user objective (e.g., conducting research or writing an essay). Each document needs to be analyzed to understand the main purpose of the writer; in this process, the main ideas are classified from those that are secondary. Thus, the selected key ideas form the condensed version of the original document that should preserve the central, relevant or vital information. As a result, the summary can provide a general idea of a complex document (e.g., a book, scientific paper, etc.) allowing the reader learn the main points on it.\nIn the natural language processing (NLP) area, a specific area of artificial intelligence, different approaches have been proposed to automatically build summaries simulating human\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Yu-Da Lin .\nability. Automatic text summarization (ATS) is a task that automatically produces summaries to identify key ideas of a source document [1]. In this sense, the ATS task is addressed by two approaches: abstractive [2] and extractive [3], [4] summarization.\nOn the one hand, abstractive summarization methods generate new text that cannot be contained in the original document. To do this, the internal semantic representation of the source documents is commonly learned to create a language model. The obtained model could create new sections paraphrasing the content of documents to generate the condensed document. The abstractive method may produce more strongly condensed documents but could lose the main meaning of the original document.\nOn the other hand, extractive summarization methods make use of the content in the source documents to generate the condensed version (summary). To that end, extractive systems may use different basic units such as words, sentences or paragraphs; most of the state of the art methods [5], [6] use sentences as a basic unit because it\nVOLUME 11, 2023 This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/ 51455\ncould express a complete idea of an author. Because of the complexity of the abstractive approach, most summarization systems are extractive.\nIn turn, extractive-based summaries can be addressed by supervised and unsupervised methods [7]. The supervised approach commonly needs a prelabeled corpus in which the key ideas are highlighted to train a supervised algorithm; then, it is possible to recognize key ideas in a new document. The unsupervised approach, for example, identifies key sentences based on a word count.\nThe unsupervised extractive ATS system has the advantage of not needing a training step, and in addition, it may be more appropriate for real cases where a labeled dataset is not always available. In this sense, clustering-based schemes are widely used by grouping sentences to discover general topics and then selecting the main idea of each group.\nIn addition, it is common to combine evolutionary and clustering algorithms for unsupervised approaches to address the combinatorial problem in group formation [8]. In turn, evolutionary methods need a guidance function to find the best clustering configurations, but finding optimal clusters is not guaranteed to generate a good-quality summary [9]. The guidance or fitness function is a means to know the quality of solutions generated by evolutionary approaches and is generally established based on the author\u2019s intuition.\nIn this study, the main objective is the automatic search for a fitness function that correlates with the quality of summaries by combining genetic programming (GP) and a genetic algorithm (GA). Integration of these algorithms has been used in previous studies using the GP to find the hidden relationships between features to build general structures and then using GA to identify relevance between them [10]. On the one hand, we propose a GP system that considers the internal validation indices to automatically build functions. On the other hand, the GA creates summaries using each function created by the GP as its fitness function. Finally, the relationship between the quality of the summaries and the clustering is established using the Rouge measure.\nThe general contributions of this study are the following: a) a system for the automatic generation of aptitude functions for the ATS task; b) performance analysis of the GP and GA integration for the ATS task; c) the generation of fitness functions correlated with human behavior for the generation of summaries; d) performance analysis of various text representation methods; e) comparison of our proposed method with attention-based methods.\nThe rest of the paper is organized as follows. Section II describes different approaches that use a fitness function as a guide to evolutionary methods. Section III details the basic concept applied in this study. Section IV describes the framework of the proposed approach for the ATS task. The performance of the proposed method is addressed in Section V. Finally, conclusions are drawn in Section VI.\nII. RELATED WORK The ATS task attempts to create a condensed version of a document synthesizing or abbreviating ideas of a\nmore complex document while preserving the most relevant information. In the literature, various kinds of summarization problems have been raised, and in turn, various methods to solve those problems have been proposed.\nFor example, with regard to the source documents, an ATS system could automatically create an abstract by condensing a single document or multiple documents [11], [12], [13]; this makes the task more difficult because it increases redundancy in selected ideas and the amount of information to analyze.\nIn addition, on the one hand, summaries can be extractives, which are generated using only the information included in the original document, such as words, sentences or paragraphs; on the other hand, summaries can be abstract [14], [15], which could convey new information not included in the original document, by commonly combining a paraphrasing process with a language model.\nThe ATS systems can also be classified according to the approach used to select the relevant information as supervised and unsupervised. Supervised systems need a training process and, in turn, a prelabeled dataset highlighting the relevant information. Unlike unsupervised systems, unsupervised systems are more practical in the analysis of multidomain documents because they are not limited by training information.\nClustering-based unsupervised approaches [16], [17] typically use an evolutionary algorithm to optimize groups of sentences to generate good-quality abstracts by selecting the best candidate sentences from each group. Thus, the most relevant part of evolutionary algorithms is the definition of a fitness function that evaluates each solution. For example, Alguliyev et al. [8] use a differential evolution algorithm to maximize the fitness function that measures the summary quality. This function is focused on the relevance and diversity of the information contained in summaries.\nAnother evolutionary approach was proposed by Rautray and Balabantaray [18], where a Cuckoo search is performed to address the problem of multidocument summarization. The authors consider different aspects to build the fitness function, such as coverage, nonredundancy, cohesion and readability.\nSimilar to the works above, Sanchez-Gomez et al. [19] proposed a multiobjective artificial bee colony to automatically generate good-quality summaries. The objective function proposed by the authors addresses coverage, where the main topics in the source document should be considered, and redundancy reduction, where similar sentences existing in the source document should not be repeated in the generated summary.\nAs mentioned above, authors who propose clusteringbased ATS systemsmanually configure their fitness functions considering different aspects of texts, such as sentence relevance, topic diversity, non-redundancy between sentences, and readability, among others. This information is obtained by processing texts at different levels (for example, words, sentences, or paragraphs) and then formulating an objective function that, according to the author\u2019s intuition, correlates with the quality of the summaries.\n51456 VOLUME 11, 2023\nTherefore, the information considered to formulate the fitness function, which is commonly a linear function, must result in a single value that determines the quality of the solution.\nAccordingly, unsupervised evolutionary approaches for generating summaries do not require a prelabeled corpus to learn; however, they need a guidance function (fitness function) to build good-quality summaries. Fitness functions can be maximized or minimized and are usually set manually based on intuition. Manual configuration limits the exploration of new functions that could improve results; therefore, in this work, we propose to create guidance functions automatically using genetic programming.\nIII. METHODOLOGY The proposed method (detailed in Section IV) aims to create fitness functions for evolutionary cluster-based methods for automatic text summarization. In this process, on the one hand, texts or documents to be summarized should be represented as numeric vectors by means of different mapping methods, as described in Section III-A; on the other hand, internal quality measures, detailed in Section III-B, are provided as operands to the genetic programming with the purpose of identifying its correlations with the quality of summaries.\nA. MAPPING METHODS In this study, four methods to represent documents as numerical vectors are proposed to explore the relevance of lexical and semantic information for the identification of relevant sentences in the ATS task. These methods are detailed below."
        },
        {
            "heading": "1) FEATURES BASED ON TERM FREQUENCY\u2013INVERSE DOCUMENT FREQUENCY",
            "text": "The term frequency - inverse document frequency (tf \u2212 idf ) [20] is widely used in natural language processing and information retrieval. In the ATS task, tf \u2212 idf (equation 1) provides information about how relevant a word within a document is in relation to the collection. Specifically, tf (equation 2) is the ratio between the number of times that a word appears in a document and the total number of words in that document. Instead, IDF (equation 3) shows how relevant a word is relative to the collection of documents by computing the ratio between the number of documents in the collection and the number of documents in the collection that contains the word. Thus, tf shows how relevant a word is in a specific document, while idf assigns more importance to those unique words to a small percentage of documents than to those words that are very common (e.g., the, a, and).\ntf \u2212 idf = (t, d,D) = tf (t, d) \u2217 idf (t,D) (1)\ntf (t, d) = fd (t)\nmax w\u2208d\nfd (w) (2)\nidf (t,D) = ln( |D|\n|d \u2208 D : t \u2208 d | ) (3)\nwhere: fd (t) = frequency of term t in document d D = collection of documents\nTo create the vector representation of a document, the vocabulary V of the collection of documents is listed, that is, a list of each different word w in the collection (namely, types). Then, given a document d , the tf \u2212 idf value is calculated for each wi \u2208 V and set in position i of the representative vector of d . Therefore, each w has some relevance relative to each d in the collection, and in turn, |V |-dimentional vectors are created."
        },
        {
            "heading": "2) ONE-HOT ENCODING",
            "text": "One-hot encoding (OHE) [21] is one of the simpler methods for representing text as a numeric vector; however, it has proven to provide relevant information to NLP models. The process to create vectors is similar to tf \u2212 idf ; however, in the OHEmethod, the values of each position of the representative vector are binary. Therefore, for each wi \u2208 V , position i of the representative vector is set to 1 if w \u2208 d and is set to 0 otherwise. As a result, the resultant OHE vectors consist of |V | dimensions."
        },
        {
            "heading": "3) LATENT DIRICHLET ALLOCATION",
            "text": "Latent Dirichlet allocation (LDA) [22] is a topic modeling algorithm that, in NLP, commonly attempts to generate documents based on a priori sampled distributions of documents over topics and, in turn, distributions of words over topics. Therefore, LDA sees a document as a mixture of topics and sees a topic as a mixture of words. To obtain the correct distribution given a collection of documents, this topic modeling algorithm attempts to maximize the following formula:\np(W ,Z , \u03b8, \u03d5; \u03b1, \u03b2) = M\u220f j=1 p(\u03b8j; \u03b1) K\u220f i=1 p(\u03d5i; \u03b2)\n\u00d7 N\u220f t=1 p(Zj,t |\u03b8j)p(Wj,t |\u03d5zj,t )\nwhere the first two factors are related to the Dirichlet distribution of topics over terms and the distribution of documents over topics, respectively, while the last two factors represent the probability of a topic appearing given a document and the probability of a word appearing given a topic.\nIn this study, we use the latent topic distribution obtained by the LDA model to represent documents in terms of the themes that make them up.\nThe main advantage of LDA is that it allows obtaining the latent structure of a document; that is, we can obtain a distribution of topics in a vector representation used as input to the clustering phase (Section IV-B). Therefore, this representation provides a topic-based grouping that the fitness function evaluates to determine the quality of the summaries.\nVOLUME 11, 2023 51457"
        },
        {
            "heading": "4) Doc2Vec",
            "text": "Commonly, text representation ignores the relationship in a sequence of words, such as many bag of words methods that ignore the word order in phrases. Doc2Vec [23], [24] is a method that learns vector representations of words and, in turn, of sentences and documents. To that end, the doc2vec algorithm considers the context of words by computing the probability that a certain word is in the context of other words.\nSpecifically, given a training set of words W = w1,w2, . . . ,wT , the goal is to maximize the probability of wt appearing, such as nwords appearing before (Equation 4). Thus, the prediction of wt can be performed by the softmax multiclass classifier (Equation 5).\nT\u2212k\u2211 t=k log p(wt | wt\u2212k , . . . ,wt+k ), (4) p(wt | wt \u2212 k, . . . ,wt+k ) = eywt\u2211 eyi (5)\nwhere each yi is given by y = b + Uh(wt\u2212k , . . . ,wt+k ;W ), h is constructed by the concatenation of vectors in the word matrixW and U , b are the softmax parameters. As a result of the training process and the inclusion of the paragraph context information, vectors of fixed dimensions are generated. Furthermore, these vectors involve semantic relations where sentences or paragraphs with similar meaning are closer in the vector space.\nB. INTERNAL QUALITY MEASURES In pattern recognition, clustering is an unsupervised classification where an algorithm attempts to organize objects or patterns into k-groups. The main goal of this task is that objects in the same group should be as compact as possible, while objects of different groups should be as different as possible.\nThe cluster validation indices are measures to evaluate the quality of a clustering given two main characteristics: the internal homogeneity and the external separability. The former evaluates how compact a group is, while the latter evaluates how far apart one group is from another.\nIn the works of Liu et al. [25] and Rend\u00f3n et al. [26], different cluster validation indices are tested through different synthetic datasets. Both studies conclude that the Dunn, Davies Bouldin and Silhouette indices are highlighted from other indices on the proposed synthetic datasets. Hern\u00e1ndezCasta\u00f1eda et al. [9], based on the results of Liu and Rend\u00f3n et al., search for the correlation between Dunn, Davies Bouldin and Silhouette indices and the quality of summaries. The authors propose three forms to generate groups called baselines: 1) top-line, where summaries written by humans are used as reference; 2) first-line, where key ideas are those n first sentences of the documents; and 3) random-line, where key ideas were selected randomly from the documents. Research results show that the Silhouette index has more correlation with the quality of summaries because it shows high performance when relevant information is selected by humans (top-line), while it shows low\nperformance when relevant information is selected randomly (random-line).\nIn this study, in view of the above, we propose to use the three indices, defined below, that show the best performance in the clustering tasks.\nThe Dunn index [27] measures the relation between the maximal distance in the same group and the minimum distance between groups of the partition. That is, for each cluster, the pairwise distance between each of the objects in the cluster and the objects of the remainder of the clusters is computed. Then, the minimum pairwise distance (minseparation) is obtained. Next, for each cluster, the distance between all objects of the same group is calculated; the maximum distance (max-diameter) is selected. Formally, the Dunn index is defined as follows:\nDunn = min1\u2264i<j\u2264cf (ci, cj) max1\u2264k\u2264c(d(Xk ))\nwhere f (ci, cj) defines the intercluster separation and d(Xk ) stands for the intracluster compactness. Thus, the Dunn index should be maximized.\nTheDavies Bouldin index [28] computes, for each cluster, the average distance between the objects and its centroid to measure the compactness of the clusters. In addition, to identify the cluster separation, the distance between centroids is computed. This index is defined as follows:\nDB = 1 c c\u2211 i=1,i\u0338=j Max{ \u03b4i + \u03b4j d(ci, cj) }\nwhere c is the number of clusters, \u03b4i defines the average distance between each object in Cluster i and its centroid (\u03b4j follows the same process), and d(ci, cj) defines the distance between the centroids of the clusters. Small values of the index stand for compact clusters whose centroids are well separated from each other. Thus, the partition that minimizes the Davies Bouldin index is considered optimal.\nThe Silhouette coefficient [29] measures how close each centroid in the cluster is to each other object in the neighboring clusters. Thus, for each object i, compute the average proximity ai between i and all other objects in the cluster to which i belongs. Then, for the remaining Clusters c, calculate the average proximity f (i, c) to all objects in c. The smallest value of f (i, c) is defined as bi = mincf (i, c). The coefficient is defined as follows:\ns(i) = bi \u2212 ai\nmax{ai, bi} where SC = 1c \u2211c\ni=1 s(i) computes the coefficient for the complete partition.\nIV. PROPOSED METHOD The proposed approach of this study (figure 1) is performed in two general stages; in the first stage, a genetic programming (GP) algorithm generates aptitude functions, and in the second stage, a genetic algorithm (GA) evolves clusters of sentences to produce summaries based on the aptitude\n51458 VOLUME 11, 2023\nfunction built by the GP (see Algorithm 1). These two steps are detailed as follows.\nFirst, the GP creates a set of functions, considering the selected internal validation indices (Davies Bouldin, Dunn and Silhouette) as operands. Then, each expression built by the GP is taken as a fitness function in the evolutionary clustering approach where the summaries are created. Therefore, the fitness function should evaluate the clustering that makes up the good-quality summaries as detailed below.\nIn the clustering representation, each document is divided into sentences. To encode a document as an individual of the GA (genotype), it is represented by a binary vector where each gen represents a sentence. The active genes from this codification are considered the centroid sentences in the clustering; therefore, a vector with n active genes represents a clustering of n groups of sentences. Then, to measure the quality of solutions, the clusters are evaluated using the fitness function generated by the GP. Finally, the active genes of the best solution indicate which sentences will take part in the summary. This process is repeated through each document in the collection.\nTo ensure that the summaries created are of good quality, the Rouge measure is used. Rouge [30] is a measure to automatically determine the quality of a summary by comparing it to ideal summaries written by humans. This measure has different versions that count the number of overlapping units such as n-grams (Rouge-1 and Rouge-2), word sequences (Rouge-L), and word pairs (Rouge- SU) between the computer-generated summary to be evaluated and the ideal summaries.\nTo establish the correlation between the quality of the summaries and the grouping, our proposed method pursues two objectives: to maximize the fitness function created by theGP (whichmeasures the quality of the clusters) andRouge measure (which measures the quality of the summaries). Therefore, we attempt to establish the correlation between the Rouge measure and the function created by the GP. Specifically, the GP algorithm invokes the GA algorithm, the latter creates the summaries and returns their best fitness value. Finally, the GP calculates the Rouge measure on the generated summaries and seeks to optimize both the best fitness value obtained by the GA and the value obtained by Rouge (i.e., the GP has a multiobjective fitness function).\nTo build the GP functions, 10% of the DUC02 dataset is used; thus, the rest of DUC02 and the CNN/Dailymail dataset are used to test the efficiency of each generated function.\nA. POSSIBLE VARIATIONS AND IMPROVEMENTS OF THE CURRENT FRAMEWORK The model proposed in this work would be able to analyze more mapping methods that provide a vector representation of texts at different linguistic levels. For example, Large Language Models (LLM) can be used as a mapping method (Section III-A) for a possible improvement of the proposed framework. In accordance with the above, our method can\nAlgorithm 1 Proposed System Pseudocode Input: Source documents Output: Summaries\nInitialization: 1: Randomly create the initial population P(0) LOOP Process\n2: for t = 1 to NumberOfGenerations do 3: P\u2032(t) = \u2205 4: Evaluate each function in P(t) with the GA 5: Copy the best individual from P(t) to P\u2032(t) 6: while P\u2032(t) is not filled do 7: if insertion probability Pi < rand[0 \u2212 1] then 8: Select an individual i based on roulette 9: Insert i in P\u2032(t) 10: end if 11: if crossover probability Pc < rand[0 \u2212 1] then 12: Select two individuals (i, j) based on tournament 13: i = crossover(i, j) 14: Insert i into P\u2032(t) 15: end if 16: if mutation probability Pm < rand[0 \u2212 1] then 17: Randomly select an individual i 18: i = mutate(i) 19: Insert i into P\u2032(t) 20: end if 21: end while 22: end for 23: Select the best individual gpBest from p\u2032(t) 24: Use gpBest as GA fitness function and create\nsummaries 25: return Summaries\nbenefit from the advantages of LLMs, and consequently also acquire the disadvantages. For example, LLMs like BERT or GPT are often resource intensive to train and generate text representations.\nIn this sense, on the one hand, the model presented in this study is an evolutionary-based algorithm; and according to Neumann [31] and Nopiah et al. [32], the time complexity of this type of algorithms (in general cases) isO(n) orO(n log n). Figure 2 shows the execution time of our proposed summary system with respect to the percentage of data used; our summary system generates a model on average over three hours (when all data are considered) and the resulting plot shows linear complexity. On the other hand, the attentionbased systems have a time complexity of O(n2) [33] (per attention layer) which implies a major time of processing in exchange for a more accurate language model. However, despite the large amount of data to train these models, traditional word embeddings methods are still better at some tasks [34].\nIt is worth noting that we consider some improvements to the current framework as future work, such as the use of LLMs as mapping methods. An important advantage of LLMs over other context-based methods, such as Doc2Vec, is that the former can evaluate the context of a word bidirectionally. This feature produces more accurate representations of the text. Also, unlike recurrent neural networks (RNNs),\nVOLUME 11, 2023 51459\nLLMs have attentional mechanisms that allow parallel data processing [33]. Therefore, in our proposal, LMMs can provide more precise semantic information of the sentences in a document and thus improve the process of selecting key information.\nAnother possible improvement is the expansion of the set of terminals of the GP. That is, the objective functions generated by the GP, in the current configuration, only consider internal information of the clusters (external separability and internal homogeneity), but they can be improved by adding external information such as: title similarity, redundancy and length of sentences, coverage, etc.\nDespite the improvements that can be made to our method, as can be seen in Table 1, the results of this work outperform the established baselines [35] by the basic systems for the analyzed datasets. SumaRuNNer [36] and SUMO [37] are RNN and transformer-based systems, respectively; and Lead-3 is the selection of the first three sentences of the documents. It is worth noting that some attention-based systems baselines, such as SUMO, show similar performance with our proposed model.\nB. CLUSTERING REPRESENTATION The partitional clustering strategy is used in this study to organize the sentences of the documents. To that end, documents are divided into sentences, and each sentence is converted to a numeric vector using feature generation methods (see Section III-A).\nIn the next step, we built a Euclidean distance matrix M for each document Dn in the collection, where n represents the number of sentences in D. That is, the Euclidean distance is obtained between each sentence si and sj \u2208 Dn. As a result, M is a bidimentional matrix of n x n. Thus, given a set of objects = Xi \u2208 Rd ; i = 1, . . . ,N , a partional clustering has the goal of organizing the objects in K Clusters K = C1,C2, . . . ,CK , while a criterion function is maximized or minimized.\nFinally, groups are generated by selecting the objects closer to each centroid (group representative) and following the next rules:\n1) Ci \u0338= \u2205, i = 1, . . . ,K\n2) K\u22c3 i=1 Ci =\n3) Ci \u22c2 Cj = \u2205, i, j = 1, . . . ,K y i \u0338= j\nIn addition, the proposed method uses a genetic algorithm to optimize the clustering process that becomes a combinatorial problem.\nC. GENETIC ALGORITHM CODIFICATION The genetic algorithm (GA) is an optimization method based on the theory of natural selection, where the survival and reproduction of individuals depends on their genetic characteristics. GA randomly creates a population that evolves by g generations with the aim of improving the individuals (solutions) while applying crossover and mutation operators.\nEach individual or chromosome in the population is a possible solution for some specific problem (phenotype), commonly represented by a binary vector (genotype). In this study, each individual is the representation of a document. Each position of the vector, namely, gen, represents a\n51460 VOLUME 11, 2023\nsentence of the document, and its value {1, 0} indicates if the sentence is taken as the centroid. In turn, centroids represent the key sentences that make up the summary.\nThe operators of the GA used in this study are the crossover in two points and the standard mutation, and the selection method is roulette. On the one hand, the crossover and mutation rates are set to 0.7 and 0.1, respectively; on the other hand, the population evolves over 50 generations.\nThe fitness function is a heuristic function that assigns a value to each individual and indicates the quality of the solutions. Instead of other works where this function is set manually, we propose to generate it automatically with genetic programming."
        },
        {
            "heading": "D. THE SEARCH OF A FITNESS FUNCTION WITH GENETIC PROGRAMMING",
            "text": "Genetic programming (GP) is a technique that evolves computer programs. Similar to genetic algorithms (GA), GP has a series of operators to evolve the population, such as crossover and mutation. Instead, the representation of individuals in GP is through a tree structure. This structure allows us to represent a mathematical expression where each nonterminal node has an operator function and every terminal node has an operand.\nIn this study, a GP algorithm is performed to build an objective function to guide the search for good-quality summaries. To that end, the internal validation indices were selected to be part of the operands; that is, theDavies Bouldin, Dunn and Silhouette index could be located at the terminal nodes of the tree. It is worth noting that these indices have proven to be correlated with the quality of abstracts [9]. Additionally, a random constant in the range of [0,1] could also be added to the terminal nodes to provide a weighting of terms. On the other hand, the basic operators could be located in the nonterminal nodes.\nThe Rouge measure is defined as the fitness function of the GP because it is widely used to assess the quality of summaries. Thus, the better the quality of the summaries determined by Rouge, the better the ability of the GP-generated function to detect good-quality summaries.\nThe advantage of the generated function is that it evaluates the summaries considering only the internal information, that is, it evaluates the clustering representation in which the key ideas are selected based on the configuration of each group.\nIn the GP system, we define the following parameters: population of 100 individuals, 60% crossover rate and 10% mutation rate, and the population evolved over 500 generations.\nE. DATASETS To validate the proposed approach of this study, two standard collections are analyzed: DUC02 and CNN/Daily Mail. Table 2 details the basic statistics of the source documents and abstracts.\nThe DUC02 dataset was selected because every news item was written by two expert humans; this fact works as a\nreference point between automatic and manual summaries. In addition, the evolutionary process can generate objective functions by learning the process of generating human abstracts.\nIn addition, the widely used CNN/Daily Mail dataset [38] was selected to measure the performance of our proposal relative to other current standards addressing both supervised and unsupervised methods.\nV. RESULTS AND DISCUSSION In this study, internal validation indices are used to provide information on the quality of the clusters and, in turn, the quality of the summaries. The inference is that each index may have some degree of relevance in the ATS task. In this sense, the main goal is to create an objective function that only considers the internal information of the documents to create good-quality summaries. To that end, a GP algorithm is performed to select and bind the correct components and weights (operands) and the correct operators to automatically generate a fitness function. As a result, the decision to add some operand or operator to the objective function is made automatically. It should be noted that in previous works, the fitness functions were adjusted manually according to the author\u2019s intuition [39], [40], [41]; this makes it very timeconsuming for the authors to analyze various functions.\nAs detailed above (Section IV-D), each function generated by the GP is sent to the genetic algorithm that creates\nVOLUME 11, 2023 51461\nand selects the best summary. Once all the summaries are generated, the GP fitness function calculates the quality of the collection using Rouge and selects the best generated function.\nFigure 3 shows an automatically generated function for the text summary task where the variables x, y and w represent the Dunn, Davies Bouldin and Silhouette index, respectively. Specifically, this figure shows the graphical representation of a binary tree generated by the GP; this tree is made up of binary or unary operators (parent nodes), and operands (leaf nodes). Therefore, the GP is in charge of adjusting the tree structure by applying genetic operators (i.e., crossover and mutation) to find new solutions. In this sense, the GP is capable of omitting operators and operands if they are not relevant for the solution; however, all indices were added to the best solutions found.\nIt should be noted that the fitness function construction process is carried out considering only 10% of the DUC02 collection. The resultant function is then tested on the remaining DUC02 documents and the CNN/Daily mail dataset.\nIn Tables 3 and 4, the results on DUC02 are shown. Various feature generation methods, which obtain information from texts at the lexical and semantic levels, are compared and combined to achieve the best performance. As seen, the combination of LDA, Doc2Vec and TF-IDF obtained the best result for both corpora. This suggests that topic information, context-based semantics, and word relevance is the combination that provides the best clustering representation for selecting key sentences. Therefore, the generation of good quality summaries, within the framework of our proposal, depends on: 1) the topic: the topics that make up the sentence; 2) the semantic context: how similar are the sentences in a semantic space; and 3) the relevance of sentences to a document in a collection.\nTables 5 and 6 show a comparison between the approach proposed in this study and other studies that proposed supervised and unsupervised methods. Our proposed approach achieves the best performance on the DUC02 collection for the Rouge-1, Rouge-2 and Rouge-SU measures. It is worth noting that our system outperforms the results with respect to studies that focus on clustering [42] or evolutionary [43] methods, where the objective function is created based on the author\u2019s intuition. Additionally, this study shows competitive performance for the CNN/Daily mail collection compared to supervised methods based on neural networks.\nTable 7 shows a couple of examples of automatically created summaries from the CNN/Daily mail dataset and its respective human-made sums built from the same document. As seen, the summarymade by humans ismore compact since this feature is proper of the abstractive summaries; however,\n51462 VOLUME 11, 2023\nmost ideas of this summary can be inferred by the summary obtained automatically.\nVI. CONCLUSION This study proposes the automatic generation of an objective function for the unsupervised text summary task. A combination of a genetic algorithm and genetic programming was performed to build a maximization function that maintains a close correlation with the quality of the summaries (i.e., the higher the value of the objective function, the better the quality of summaries generated).\nAccording to the results shown in this work (Section V), the combination of lexical and semantic information (LDA+Doc2Vec+TF-IDF) achieves the best results in detecting key ideas to form a summary. This combination of features includes information about the relevance of words (TF-IDF), the topics involved (LDA) and the contexts around a window of words (Doc2Vec).\nThe resulting objective function for the extractive ATS task considers only the internal information since it is formed from internal validation indices; that is, the created function only considers the quality of clustering. This fact allows the function to be applied without external information such as the true labels of each document.\nThe Rouge measure was used to correlate the fitness function created by the GP with the quality of summaries; this correlation allowed this study to automatically create objective functions and yield competitive results for the DUC02 and CNN/Daily mail datasets.\nACKNOWLEDGMENT The authors would like to thank the Mexican Government (C\u00e1tedras CONACYT, SNI, Universidad Aut\u00f3noma del Estado de M\u00e9xico) for its support.\nREFERENCES [1] A. Elsaid, A. Mohammed, L. F. Ibrahim, and M. M. Sakre, \u2018\u2018A\ncomprehensive review of Arabic text summarization,\u2019\u2019 IEEE Access, vol. 10, pp. 38012\u201338030, 2022. [2] A. A. Syed, F. L. Gaol, and T. Matsuo, \u2018\u2018A survey of the state-of-the-art models in neural abstractive text summarization,\u2019\u2019 IEEE Access, vol. 9, pp. 13248\u201313265, 2021.\n[3] J. N. Madhuri and R. G. Kumar, \u2018\u2018Extractive text summarization using sentence ranking,\u2019\u2019 in Proc. Int. Conf. Data Sci. Commun. (IconDSC), Mar. 2019, pp. 1\u20133. [4] R. Elbarougy, G. Behery, and A. El Khatib, \u2018\u2018Extractive Arabic text summarization using modified PageRank algorithm,\u2019\u2019 Egyptian Informat. J., vol. 21, no. 2, pp. 73\u201381, Jul. 2020. [5] S. G. Jindal and A. Kaur, \u2018\u2018Automatic keyword and sentence-based text summarization for software bug reports,\u2019\u2019 IEEE Access, vol. 8, pp. 65352\u201365370, 2020. [6] H. Gupta and M. Patel, \u2018\u2018Method of text summarization using LSA and sentence based topicmodellingwith BERT,\u2019\u2019 inProc. Int. Conf. Artif. Intell. Smart Syst. (ICAIS), Mar. 2021, pp. 511\u2013517. [7] S. Ghodratnama, A. Beheshti, M. Zakershahrak, and F. Sobhanmanesh, \u2018\u2018Extractive document summarization based on dynamic feature space mapping,\u2019\u2019 IEEE Access, vol. 8, pp. 139084\u2013139095, 2020. [8] R. M. Alguliyev, R. M. Aliguliyev, N. R. Isazade, A. Abdi, and N. Idris, \u2018\u2018COSUM: Text summarization based on clustering and optimization,\u2019\u2019 Exp. Syst., vol. 36, no. 1, Feb. 2019, Art. no. e12340. [9] N. H. Casta\u00f1eda, R. A. G. Hern\u00e1ndez, Y. Ledeneva, and \u00c1. H. Casta\u00f1eda, \u2018\u2018Evolutionary automatic text summarization using cluster validation indexes,\u2019\u2019 Computaci\u00f3n Y Sistemas, vol. 24, no. 2, pp. 583\u2013595, Jun. 2020. [10] M. G. Smith and L. Bull, \u2018\u2018Genetic programming with a genetic algorithm for feature construction and selection,\u2019\u2019 Genetic Program. Evolvable Mach., vol. 6, no. 3, pp. 265\u2013281, Sep. 2005. [11] R. Alqaisi, W. Ghanem, and A. Qaroush, \u2018\u2018Extractive multi-document Arabic text summarization using evolutionarymulti-objective optimization with K-medoid clustering,\u2019\u2019 IEEE Access, vol. 8, pp. 228206\u2013228224, 2020. [12] T. U\u00e7kan and A. Karci, \u2018\u2018Extractive multi-document text summarization based on graph independent sets,\u2019\u2019 Egyptian Informat. J., vol. 21, no. 3, pp. 145\u2013157, Sep. 2020. [13] L. Dong, M. N. Satpute, W. Wu, and D. Du, \u2018\u2018Two-phase multidocument summarization through content-attention-based subtopic detection,\u2019\u2019 IEEE Trans. Computat. Social Syst., vol. 8, no. 6, pp. 1379\u20131392, Dec. 2021. [14] A. Sahu and S. G. Sanjeevi, \u2018\u2018Better fine-tuning with extracted important sentences for abstractive summarization,\u2019\u2019 in Proc. Int. Conf. Commun., Control Inf. Sci. (ICCISc), Jun. 2021, pp. 11328\u201311339. [15] M. T. Nayeem, T. A. Fuad, and Y. Chali, \u2018\u2018Abstractive unsupervised multidocument summarization using paraphrastic sentence fusion,\u2019\u2019 in Proc. 27th Int. Conf. Comput. Linguistics, 2018, pp. 1191\u20131204. [16] N. Alami, M. Meknassi, N. En-nahnahi, Y. El Adlouni, and O. Ammor, \u2018\u2018Unsupervised neural networks for automatic Arabic text summarization using document clustering and topic modeling,\u2019\u2019 Exp. Syst. Appl., vol. 172, Jun. 2021, Art. no. 114652. [17] M. M. Haider, Md. A. Hossin, H. R. Mahi, and H. Arif, \u2018\u2018Automatic text summarization using Gensim Word2Vec and K-means clustering algorithm,\u2019\u2019 in Proc. IEEE Region Symp. (TENSYMP), Jun. 2020, pp. 283\u2013286. [18] R. Rautray and R. C. Balabantaray, \u2018\u2018An evolutionary framework for multi document summarization using cuckoo search approach: MDSCSA,\u2019\u2019 Appl. Comput. Informat., vol. 14, no. 2, pp. 134\u2013144, Jul. 2018.\nVOLUME 11, 2023 51463\n[19] J. M. Sanchez-Gomez, M. A. Vega-Rodr\u00edguez, and C. J. P\u00e9rez, \u2018\u2018Extractive multi-document text summarization using a multi-objective artificial bee colony optimization approach,\u2019\u2019 Knowl.-Based Syst., vol. 159, pp. 1\u20138, Nov. 2018. [20] I. Arroyo-Fern\u00e1ndez, C.-F. M\u00e9ndez-Cruz, G. Sierra, J.-M. TorresMoreno, and G. Sidorov, \u2018\u2018Unsupervised sentence representations as word information series: Revisiting TF\u2013IDF,\u2019\u2019 Comput. Speech Lang., vol. 56, pp. 107\u2013129, Jul. 2019. [21] M. K. Dahouda and I. Joe, \u2018\u2018A deep-learned embedding technique for categorical features encoding,\u2019\u2019 IEEE Access, vol. 9, pp. 114381\u2013114391, 2021. [22] D. M. Blei, A. Y. Ng, and M. I. Jordan, \u2018\u2018Latent Dirichlet allocation,\u2019\u2019 J. Mach. Learn. Res., vol. 3, pp. 993\u20131022, Mar. 2003. [23] T. Mikolov, L. Sutskever, K. Chen, G. Corrado, and J. Dean, \u2018\u2018Distributed representations of words and phrases and their compositionality,\u2019\u2019 in Proc. Adv. Neural Inf. Process. Syst., vol. 2, Dec. 2013, pp. 3111\u20133119. [24] Q. V. Le and T. Mikolov, \u2018\u2018Distributed representations of sentences and documents,\u2019\u2019 in Proc. 31st Int. Conf. Mach. Learn., vol. 32, Jan. 2014, pp. 1188\u20131196. [25] Y. Liu, Z. Li, H. Xiong, X. Gao, and J. Wu, \u2018\u2018Understanding of internal clustering validation measures,\u2019\u2019 in Proc. IEEE Int. Conf. Data Mining, Dec. 2010, pp. 911\u2013916. [26] E. Rend\u00f3n, I. Abundez, A. Arizmendi, and E. M. Quiroz, \u2018\u2018Internal versus external cluster validation indexes,\u2019\u2019 Int. J. Comput. Commun., vol. 5, no. 1, pp. 27\u201334, 2011. [27] J. C. Dunn, \u2018\u2018Well-separated clusters and optimal fuzzy partitions,\u2019\u2019 J. Cybern., vol. 4, no. 1, pp. 95\u2013104, Jan. 1974. [28] D. L. Davies and D. W. Bouldin, \u2018\u2018A cluster separation measure,\u2019\u2019 IEEE Trans. Pattern Anal. Mach. Intell., vol. PAMI-1, no. 2, pp. 224\u2013227, Apr. 1979. [29] P. J. Rousseeuw, \u2018\u2018Silhouettes: A graphical aid to the interpretation and validation of cluster analysis,\u2019\u2019 J. Comput. Appl. Math., vol. 20, pp. 53\u201365, Nov. 1987. [30] C.-Y. Lin, \u2018\u2018Rouge: A package for automatic evaluation of summaries,\u2019\u2019 in Text Summarization Branches Out. Barcelona, Spain: Association for Computational Linguistics, 2004, pp. 74\u201381. [31] F. Neumann, \u2018\u2018Computational complexity analysis of multi-objective genetic programming,\u2019\u2019 in Proc. 14th Annu. Conf. Genetic Evol. Comput., Jul. 2012, pp. 799\u2013806. [32] Z. Nopiah, M. Khairir, S. Abdullah, M. Baharin, and A. Arifin, \u2018\u2018Time complexity analysis of the genetic algorithm clustering method,\u2019\u2019 in Proc. 9th WSEAS Int. Conf. Signal Process., Robot. Autom., vol. 10, 2010, pp. 171\u2013176. [33] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, \u2018\u2018Attention is all you need,\u2019\u2019 in Proc. Adv. Neural Inf. Process. Syst., vol. 30, 2017, pp. 1\u201311. [34] M. Pranjic, M. Robnik Sikonja, and S. Pollak, \u2018\u2018An evaluation of BERT and Doc2Vec model on the IPTC subject codes prediction dataset,\u2019\u2019 in Proc. 24th Int. Multiconference, D.Mladenic andM. Grobelnik, Eds. 2021, pp. 25\u201328. [35] M. Gambhir and V. Gupta, \u2018\u2018Deep learning-based extractive text summarization with word-level attention mechanism,\u2019\u2019 Multimedia Tools Appl., vol. 81, no. 15, pp. 20829\u201320852, Jun. 2022. [36] R. Nallapati, F. Zhai, and B. Zhou, \u2018\u2018SummaRuNNer: A recurrent neural network based sequence model for extractive summarization of documents,\u2019\u2019 in Proc. AAAI Conf. Artif. Intell., vol. 31, 2017, pp. 3075\u20133081. [37] Y. Liu, I. Titov, and M. Lapata, \u2018\u2018Single document summarization as tree induction,\u2019\u2019 in Proc. Conf. North, 2019, pp. 1745\u20131755. [38] K. M. Hermann, T. Kocisky, E. Grefenstette, L. Espeholt, W. Kay, M. Suleyman, and P. Blunsom, \u2018\u2018Teaching machines to read and comprehend,\u2019\u2019 in Proc. Adv. Neural Inf. Process. Syst.Cambridge, MA, USA:MIT Press, 2015, pp. 1693\u20131701. [39] I. Tanfouri, G. Tlik, and F. Jarray, \u2018\u2018An automatic Arabic text summarization system based on genetic algorithms,\u2019\u2019 Proc. Comput. Sci., vol. 189, pp. 195\u2013202, Jan. 2021. [40] Q. A. Al-Radaideh and D. Q. Bataineh, \u2018\u2018A hybrid approach for Arabic text summarization using domain knowledge and genetic algorithms,\u2019\u2019 Cognit. Comput., vol. 10, no. 4, pp. 651\u2013669, Aug. 2018. [41] E. V\u00e1zquez, R. A. Garcia-Hern\u00e1ndez, and Y. Ledeneva, \u2018\u2018Sentence features relevance for extractive text summarization using genetic algorithms,\u2019\u2019 J. Intell. Fuzzy Syst., vol. 35, no. 1, pp. 353\u2013365, Jul. 2018. [42] W. Song, L. C. Choi, S. C. Park, and X. F. Ding, \u2018\u2018Fuzzy evolutionary optimization modeling and its applications to unsupervised categorization and extractive summarization,\u2019\u2019 Exp. Syst. Appl., vol. 38, no. 8, pp. 9112\u20139121, Aug. 2011.\n[43] R. A. Garcia-Hern\u00e4ndez and Y. Ledeneva, \u2018\u2018Single extractive text summarization based on a genetic algorithm,\u2019\u2019 in Proc. Mex. Conf. Pattern Recognit. Cham, Switzerland: Springer, 2013, pp. 374\u2013383. [44] X. Wan, \u2018\u2018Towards a unified approach to simultaneous single-document and multi-document summarizations,\u2019\u2019 in Proc. 23rd Int. Conf. Comput. Linguistics, 2010, pp. 1137\u20131145. [45] K. Svore, L. Vanderwende, and C. Burges, \u2018\u2018Enhancing single-document summarization by combining ranknet and third-party sources,\u2019\u2019 in Proc. Joint Conf. Empirical Methods Natural Lang. Process. Comput. Natural Lang. Learn. (EMNLP-CoNLL), 2007, pp. 448\u2013457. [46] D. Shen, J.-T. Sun, H. Li, Q. Yang, and Z. Chen, \u2018\u2018Document summarization using conditional random fields,\u2019\u2019 in Proc. IJCAI, vol. 7. Burlington, MA, USA: Morgan Kaufmann, 2007, pp. 2862\u20132867. [47] S. Narayan, S. B. Cohen, and M. Lapata, \u2018\u2018Ranking sentences for extractive summarization with reinforcement learning,\u2019\u2019 in Proc. Conf. North Amer. Chapter Assoc. Comput. Linguistics, Human Lang. Technol., 2018, pp. 1747\u20131759. [48] A. See, P. J. Liu, and C. D.Manning, \u2018\u2018Get to the point: Summarizationwith pointer-generator networks,\u2019\u2019 in Proc. 55th Annu. Meeting Assoc. Comput. Linguistics, 2017, pp. 1073\u20131083. [49] H. Zheng and M. Lapata, \u2018\u2018Sentence centrality revisited for unsupervised summarization,\u2019\u2019 in Proc. 57th Annu. Meeting Assoc. Comput. Linguistics, 2019, pp. 6236\u20136247.\n\u00c1NGEL HERN\u00c1NDEZ-CASTA\u00d1EDA received the M.Sc. and Ph.D. degrees (Hons.) in computer science from the Centre for Computing Research (CIC), National Polytechnic Institute (IPN), in 2013 and 2017, respectively. He is currently a Research Professor with the Autonomous University of Mexico State and a member of the National System of Researchers (SNI) of Mexico. His research interests include natural language processing, data mining, and pattern recognition.\nREN\u00c9 ARNULFO GARC\u00cdA-HERN\u00c1NDEZ received the B.E. degree in computer systems engineering from the Toluca Institute of Technology, Mexico, in 2001, the M.S. degree in computer science from the National Centre of Research and Technology Development (Cenidet), Mexico, in 2003, and the Ph.D. degree in computer science from the National Institute of Astrophysics, Optics, and Electronics (INAOE), Mexico, in 2017. He is currently a Full Research\nProfessor with the School of Software Engineering and the Postgraduate School, Autonomous University of the State of Mexico (UAEM). He has authored over 70 articles in top journals and international conferences, and three books. He is an adviser of 34 theses. He is recognized as a secondlevel national researcher, a higher level. His research interests include pattern recognition, evolutionary computation, text mining, and natural language processing. He is a member of the Mexican Association for the Natural Language Processing.\nYULIA LEDENEVA received the B.Sc. and M.Sc. degrees in engineering from the Peoples\u2019 Friendship University of Russia, in 2002 and 2004, respectively, the M.Sc. degree in computer science from the National Institute for Astrophysics, Optics, and Electronics, Mexico, in 2006, and the Ph.D. degree in computer science from the Centre for Computing Research, National Polytechnic Institute (IPN), Mexico. She is currently a Research Professor with the Autonomous Uni-\nversity of the State of Mexico and a member of the National System of Researchers (SNI) of Mexico. She is the author of more than 70 publications. Her research interests include computational linguistics, natural language processing, text mining, graph, and genetic algorithms. She received the Presea L\u00e1zaro C\u00e1rdenas from the hands of the President of Mexico, in 2009.\n51464 VOLUME 11, 2023"
        }
    ],
    "title": "Toward the Automatic Generation of an Objective Function for Extractive Text Summarization",
    "year": 2023
}