{
    "abstractText": "Moving Object Detection (MOD) is a critical vision task for successfully achieving safe autonomous driving. Despite plausible results of deep learning methods, most existing approaches are only frame-based and may fail to reach reasonable performance when dealing with dynamic traffic participants. Recent advances in sensor technologies, especially the Event camera, can naturally complement the conventional camera approach to better model moving objects. However, event-based works often adopt a pre-defined time window for event representation, and simply integrate it to estimate image intensities from events, neglecting much of the rich temporal information from the available asynchronous events. Therefore, from a new perspective, we propose RENet, a novel RGB-Event fusion Network, that jointly exploits the two complementary modalities to achieve more robust MOD under challenging scenarios for autonomous driving. Specifically, we first design a temporal multi-scale aggregation module to fully leverage event frames from both the RGB exposure time and larger intervals. Then we introduce a bi-directional fusion module to attentively calibrate and fuse multi-modal features. To evaluate the performance of our network, we carefully select and annotate a sub-MOD dataset from the commonly used DSEC dataset. Extensive experiments demonstrate that our proposed method performs significantly better than the state-of-the-art RGB-Event fusion alternatives. The source code and dataset are publicly available at: https://github.com/ZZY-Zhou/RENet.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zhuyun Zhou"
        },
        {
            "affiliations": [],
            "name": "Zongwei Wu"
        },
        {
            "affiliations": [],
            "name": "R\u00e9mi Boutteau"
        },
        {
            "affiliations": [],
            "name": "Fan Yang"
        },
        {
            "affiliations": [],
            "name": "C\u00e9dric Demonceaux"
        },
        {
            "affiliations": [],
            "name": "Dominique Ginhac"
        }
    ],
    "id": "SP:5fe73135dfc75358fcdef6960ae4053cbdaae356",
    "references": [
        {
            "authors": [
                "M. Siam",
                "H. Mahgoub",
                "M. Zahran",
                "S. Yogamani",
                "M. Jagersand",
                "A. El-Sallab"
            ],
            "title": "Modnet: Motion and appearance based moving object detection network for autonomous driving",
            "venue": "International Conference on Intelligent Transportation Systems (ITSC), 2018.",
            "year": 2018
        },
        {
            "authors": [
                "H. Rashed",
                "M. Ramzy",
                "V. Vaquero",
                "A. El Sallab",
                "G. Sistu",
                "S. Yogamani"
            ],
            "title": "Fusemodnet: Real-time camera and lidar based moving object detection for robust low-light autonomous driving",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops (ICCVW), 2019.",
            "year": 2019
        },
        {
            "authors": [
                "S.A. Baur",
                "D.J. Emmerichs",
                "F. Moosmann",
                "P. Pinggera",
                "B. Ommer",
                "A. Geiger"
            ],
            "title": "Slim: Self-supervised lidar scene flow and motion segmentation",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021.",
            "year": 2021
        },
        {
            "authors": [
                "A. Mackin",
                "F. Zhang",
                "D.R. Bull"
            ],
            "title": "A study of high frame rate video formats",
            "venue": "IEEE Transactions on Multimedia (TMM), vol. 21, no. 6, pp. 1499\u20131512, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "P.C. Madhusudana",
                "N. Birkbeck",
                "Y. Wang",
                "B. Adsumilli",
                "A.C. Bovik"
            ],
            "title": "St-greed: Space-time generalized entropic differences for frame rate dependent video quality prediction",
            "venue": "IEEE Transactions on Image Processing (TIP), vol. 30, pp. 7446\u20137457, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "G. Sun",
                "Y. Liu",
                "H. Ding",
                "T. Probst",
                "L. Van Gool"
            ],
            "title": "Coarse-to-fine feature mining for video semantic segmentation",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022.",
            "year": 2022
        },
        {
            "authors": [
                "G. Chen",
                "H. Cao",
                "J. Conradt",
                "H. Tang",
                "F. Rohrbein",
                "A. Knoll"
            ],
            "title": "Event-based neuromorphic vision for autonomous driving: A paradigm shift for bio-inspired visual sensing and perception",
            "venue": "IEEE Signal Processing Magazine (SPM), vol. 37, no. 4, pp. 34\u201349, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "J. Li",
                "S. Dong",
                "Z. Yu",
                "Y. Tian",
                "T. Huang"
            ],
            "title": "Event-based vision enhanced: A joint detection framework in autonomous driving",
            "venue": "2019 IEEE International Conference on Multimedia and Expo (ICME), 2019.",
            "year": 2019
        },
        {
            "authors": [
                "A.I. Maqueda",
                "A. Loquercio",
                "G. Gallego",
                "N. Garc\u0131\u0301a",
                "D. Scaramuzza"
            ],
            "title": "Event-based vision meets deep learning on steering prediction for self-driving cars",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.",
            "year": 2018
        },
        {
            "authors": [
                "W. Zhen",
                "S. Scherer"
            ],
            "title": "Estimating the localizability in tunnel-like environments using lidar and uwb",
            "venue": "International Conference on Robotics and Automation (ICRA), 2019.",
            "year": 2019
        },
        {
            "authors": [
                "Z. Wu",
                "G. Allibert",
                "C. Stolz",
                "C. Ma",
                "C. Demonceaux"
            ],
            "title": "Modalityguided subnetwork for salient object detection",
            "venue": "International Conference on 3D Vision (3DV), 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Z. Wu",
                "S. Gobichettipalayam",
                "B. Tamadazte",
                "G. Allibert",
                "D.P. Paudel",
                "C. Demonceaux"
            ],
            "title": "Robust rgb-d fusion for saliency detection",
            "venue": "International Conference on 3D Vision (3DV), 2022.",
            "year": 2022
        },
        {
            "authors": [
                "J. Lin",
                "F. Zhang"
            ],
            "title": "R3 live: A robust, real-time, rgb-colored, lidarinertial-visual tightly-coupled state estimation and mapping package",
            "venue": "International Conference on Robotics and Automation (ICRA), 2022.",
            "year": 2022
        },
        {
            "authors": [
                "N. Messikommer",
                "D. Gehrig",
                "M. Gehrig",
                "D. Scaramuzza"
            ],
            "title": "Bridging the gap between events and frames through unsupervised domain adaptation",
            "venue": "IEEE Robotics and Automation Letters (RAL), vol. 7, no. 2, pp. 3515\u20133522, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "A. Tomy",
                "A. Paigwar",
                "K.S. Mann",
                "A. Renzaglia",
                "C. Laugier"
            ],
            "title": "Fusing event-based and rgb camera for robust object detection in adverse conditions",
            "venue": "International Conference on Robotics and Automation (ICRA), 2022.",
            "year": 2022
        },
        {
            "authors": [
                "S. Tulyakov",
                "A. Bochicchio",
                "D. Gehrig",
                "S. Georgoulis",
                "Y. Li",
                "D. Scaramuzza"
            ],
            "title": "Time lens++: Event-based frame interpolation with parametric non-linear flow and multi-scale fusion",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022.",
            "year": 2022
        },
        {
            "authors": [
                "M. Gehrig",
                "M. Millh\u00e4usler",
                "D. Gehrig",
                "D. Scaramuzza"
            ],
            "title": "E-raft: Dense optical flow from event cameras",
            "venue": "International Conference on 3D Vision (3DV), 2021.",
            "year": 2021
        },
        {
            "authors": [
                "L. Sun",
                "C. Sakaridis",
                "J. Liang",
                "Q. Jiang",
                "K. Yang",
                "P. Sun",
                "Y. Ye",
                "K. Wang",
                "L. Van Gool"
            ],
            "title": "Event-based fusion for motion deblurring with cross-modal attention",
            "venue": "European Conference on Computer Vision (ECCV), 2022.",
            "year": 2022
        },
        {
            "authors": [
                "S. Tulyakov",
                "D. Gehrig",
                "S. Georgoulis",
                "J. Erbach",
                "M. Gehrig",
                "Y. Li",
                "D. Scaramuzza"
            ],
            "title": "Time lens: Event-based video frame interpolation",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021.",
            "year": 2021
        },
        {
            "authors": [
                "S. Tulyakov",
                "A. Bochicchio",
                "D. Gehrig",
                "S. Georgoulis",
                "Y. Li",
                "D. Scaramuzza"
            ],
            "title": "Time lens++: Event-based frame interpolation with parametric non-linear flow and multi-scale fusion",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022.",
            "year": 2022
        },
        {
            "authors": [
                "M. Gehrig",
                "W. Aarents",
                "D. Gehrig",
                "D. Scaramuzza"
            ],
            "title": "Dsec: A stereo event camera dataset for driving scenarios",
            "venue": "IEEE Robotics and Automation Letters (RAL), vol. 6, no. 3, pp. 4947\u20134954, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Hu",
                "S.-C. Liu",
                "T. Delbruck"
            ],
            "title": "v2e: From video frames to realistic dvs events",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021.",
            "year": 2021
        },
        {
            "authors": [
                "J. Hidalgo-Carri\u00f3",
                "D. Gehrig",
                "D. Scaramuzza"
            ],
            "title": "Learning monocular dense depth from events",
            "venue": "International Conference on 3D Vision (3DV), 2020.",
            "year": 2020
        },
        {
            "authors": [
                "A.Z. Zhu",
                "D. Thakur",
                "T. \u00d6zaslan",
                "B. Pfrommer",
                "V. Kumar",
                "K. Daniilidis"
            ],
            "title": "The multivehicle stereo event camera dataset: An event camera dataset for 3d perception",
            "venue": "IEEE Robotics and Automation Letters (RAL), vol. 3, no. 3, pp. 2032\u20132039, 2018.",
            "year": 2032
        },
        {
            "authors": [
                "J. Redmon",
                "A. Farhadi"
            ],
            "title": "Yolov3: An incremental improvement",
            "venue": "arXiv preprint arXiv:1804.02767, 2018.",
            "year": 1804
        },
        {
            "authors": [
                "Ultralytics"
            ],
            "title": "Yolov5",
            "venue": "https://github.com/ultralytics/yolov5."
        },
        {
            "authors": [
                "W. Cheng",
                "H. Luo",
                "W. Yang",
                "L. Yu",
                "S. Chen",
                "W. Li"
            ],
            "title": "Det: A high-resolution dvs dataset for lane extraction",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2019.",
            "year": 2019
        },
        {
            "authors": [
                "S. Kundu",
                "G. Datta",
                "M. Pedram",
                "P.A. Beerel"
            ],
            "title": "Spike-thrift: Towards energy-efficient deep spiking neural networks by limiting spiking activity via attention-guided compression",
            "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2021.",
            "year": 2021
        },
        {
            "authors": [
                "S. Wo\u017aniak",
                "A. Pantazi",
                "T. Bohnstingl",
                "E. Eleftheriou"
            ],
            "title": "Deep learning incorporating biologically inspired neural dynamics and inmemory computing",
            "venue": "Nature Machine Intelligence (Nat. Mach. Intell), vol. 2, no. 6, pp. 325\u2013336, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "S.B. Shrestha",
                "G. Orchard"
            ],
            "title": "Slayer: Spike layer error reassignment in time",
            "venue": "Advances in Neural Information Processing Systems (NIPS), 2018.",
            "year": 2018
        },
        {
            "authors": [
                "J.H. Lee",
                "T. Delbruck",
                "M. Pfeiffer"
            ],
            "title": "Training deep spiking neural networks using backpropagation",
            "venue": "Frontiers in Neuroscience (Front. Neurosci.), vol. 10, p. 508, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "B. Rueckauer",
                "I.-A. Lungu",
                "Y. Hu",
                "M. Pfeiffer",
                "S.-C. Liu"
            ],
            "title": "Conversion of continuous-valued deep networks to efficient event-driven networks for image classification",
            "venue": "Frontiers in Neuroscience (Front. Neurosci.), vol. 11, p. 682, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "G. Gallego",
                "T. Delbr\u00fcck",
                "G. Orchard",
                "C. Bartolozzi",
                "B. Taba",
                "A. Censi",
                "S. Leutenegger",
                "A.J. Davison",
                "J. Conradt",
                "K. Daniilidis"
            ],
            "title": "Eventbased vision: A survey",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), vol. 44, no. 1, pp. 154\u2013180, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "A. Zhu",
                "L. Yuan",
                "K. Chaney",
                "K. Daniilidis"
            ],
            "title": "Ev-flownet: Selfsupervised optical flow estimation for event-based cameras",
            "venue": "Robotics: Science and Systems (RSS), 2018.",
            "year": 2018
        },
        {
            "authors": [
                "D. Gehrig",
                "H. Rebecq",
                "G. Gallego",
                "D. Scaramuzza"
            ],
            "title": "Eklt: Asynchronous photometric feature tracking using events and frames",
            "venue": "International Journal of Computer Vision (IJCV), vol. 128, no. 3, pp. 601\u2013618, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "M. Liu",
                "T. Delbruck"
            ],
            "title": "Adaptive time-slice block-matching optical flow algorithm for dynamic vision sensors",
            "venue": "British Machine Vision Conference (BMVC), 2018.",
            "year": 2018
        },
        {
            "authors": [
                "A. Mondal",
                "S. R",
                "J.H. Giraldo",
                "T. Bouwmans",
                "A.S. Chowdhury"
            ],
            "title": "Moving object detection for event-based vision using graph spectral clustering",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops (ICCVW), 2021.",
            "year": 2021
        },
        {
            "authors": [
                "A. Sironi",
                "M. Brambilla",
                "N. Bourdis",
                "X. Lagorce",
                "R. Benosman"
            ],
            "title": "Hats: Histograms of averaged time surfaces for robust event-based object classification",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.",
            "year": 2018
        },
        {
            "authors": [
                "J. Manderscheid",
                "A. Sironi",
                "N. Bourdis",
                "D. Migliore",
                "V. Lepetit"
            ],
            "title": "Speed invariant time surface for learning to detect corner points with event-based cameras",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Zhou",
                "G. Gallego",
                "H. Rebecq",
                "L. Kneip",
                "H. Li",
                "D. Scaramuzza"
            ],
            "title": "Semi-dense 3d reconstruction with a stereo event camera",
            "venue": "European Conference on Computer Vision (ECCV), 2018.",
            "year": 2018
        },
        {
            "authors": [
                "H. Rebecq",
                "R. Ranftl",
                "V. Koltun",
                "D. Scaramuzza"
            ],
            "title": "Eventsto-video: Bringing modern computer vision to event cameras",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019.",
            "year": 2019
        },
        {
            "authors": [
                "A.Z. Zhu",
                "L. Yuan",
                "K. Chaney",
                "K. Daniilidis"
            ],
            "title": "Unsupervised event-based learning of optical flow, depth, and egomotion",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019.",
            "year": 2019
        },
        {
            "authors": [
                "P. Bardow",
                "A.J. Davison",
                "S. Leutenegger"
            ],
            "title": "Simultaneous optical flow and intensity estimation from an event camera",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.",
            "year": 2016
        },
        {
            "authors": [
                "S. Zhaoning",
                "M. Nico",
                "G. Daniel",
                "S. Davide"
            ],
            "title": "Ess: Learning eventbased semantic segmentation from still images",
            "venue": "European Conference on Computer Vision (ECCV), 2022.",
            "year": 2022
        },
        {
            "authors": [
                "N. Messikommer",
                "S. Georgoulis",
                "D. Gehrig",
                "S. Tulyakov",
                "J. Erbach",
                "A. Bochicchio",
                "Y. Li",
                "D. Scaramuzza"
            ],
            "title": "Multi-bracket high dynamic range imaging with event cameras",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Li",
                "Z. Wang",
                "L. Wang",
                "G. Wu"
            ],
            "title": "Actions as moving points",
            "venue": "European Conference on Computer Vision (ECCV), 2020.",
            "year": 2020
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.",
            "year": 2016
        },
        {
            "authors": [
                "S. Woo",
                "J. Park",
                "J.-Y. Lee",
                "I.S. Kweon"
            ],
            "title": "Cbam: Convolutional block attention module",
            "venue": "European Conference on Computer Vision (ECCV), 2018.",
            "year": 2018
        },
        {
            "authors": [
                "J. Hu",
                "L. Shen",
                "G. Sun"
            ],
            "title": "Squeeze-and-excitation networks",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.",
            "year": 2018
        },
        {
            "authors": [
                "W. Qilong",
                "W. Banggu",
                "Z. Pengfei",
                "L. Peihua",
                "Z. Wangmeng",
                "H. Qinghua"
            ],
            "title": "ECA-Net: Efficient channel attention for deep convolutional neural networks",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020.",
            "year": 2020
        },
        {
            "authors": [
                "X. Chen",
                "K.-Y. Lin",
                "J. Wang",
                "W. Wu",
                "C. Qian",
                "H. Li",
                "G. Zeng"
            ],
            "title": "Bi-directional cross-modality feature propagation with separationand-aggregation gate for rgb-d semantic segmentation",
            "venue": "European Conference on Computer Vision (ECCV), 2020.",
            "year": 2020
        },
        {
            "authors": [
                "W. Ji",
                "J. Li",
                "S. Yu",
                "M. Zhang",
                "Y. Piao",
                "S. Yao",
                "Q. Bi",
                "K. Ma",
                "Y. Zheng",
                "H. Lu"
            ],
            "title": "Calibrated rgb-d salient object detection",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021.",
            "year": 2021
        },
        {
            "authors": [
                "T. Zhou",
                "H. Fu",
                "G. Chen",
                "Y. Zhou",
                "D.-P. Fan",
                "L. Shao"
            ],
            "title": "Specificitypreserving rgb-d saliency detection",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021.",
            "year": 2021
        },
        {
            "authors": [
                "P. Weinzaepfel",
                "Z. Harchaoui",
                "C. Schmid"
            ],
            "title": "Learning to track for spatio-temporal action localization",
            "venue": "Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2015.",
            "year": 2015
        },
        {
            "authors": [
                "G. Gkioxari",
                "J. Malik"
            ],
            "title": "Finding action tubes",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.",
            "year": 2015
        },
        {
            "authors": [
                "V. Kalogeiton",
                "P. Weinzaepfel",
                "V. Ferrari",
                "C. Schmid"
            ],
            "title": "Action tubelet detector for spatio-temporal action localization",
            "venue": "Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2017.",
            "year": 2017
        }
    ],
    "sections": [
        {
            "text": "I. INTRODUCTION\nWith the development of deep learning methods, researchers have reported great success in modeling the motion behaviors of different traffic participants [1]\u2013[3]. However, most existing methods are frame-based and their performances are highly dependent on video quality, i.e., the frame rate and the image quality [4]\u2013[6]. The former (frame rate) is essential to model the temporal consistency and the motion behavior, while the latter (image quality) influences the accuracy of object detection within a single frame. Therefore, despite significant advances in deep networks, the inherent limits of frame-based cameras are one of the main performance bottlenecks for moving object detection.\nRecent advances in bio-inspired event cameras have drawn great research attention for autonomous vehicles [7]\u2013[9]. The\nThis work was supported by the French National Research Agency through ANR CERBERE (ANR-21-CE22-0006).\n1Zhuyun Zhou, Zongwei Wu, Fan Yang, Ce\u0301dric Demonceaux, and Dominique Ginhac are with ImViA, University of Burgundy (Universite\u0301 de Bourgogne), Dijon, France. {Zhuyun Zhou@etu., Zongwei Wu@etu., fanyang@, Cedric.Demonceaux@, dginhac@} u-bourgogne.fr\n2Zongwei Wu is also with CVL, ETH Zurich. 3Re\u0301mi Boutteau is with Univ Rouen Normandie, LITIS, UR 4108, F76000 Rouen, France. remi.boutteau@univ-rouen.fr 4Ce\u0301dric Demonceaux is also with Universite\u0301 de Lorraine, CNRS, Inria, LORIA, Nancy, France. \u2217Corresponding author: zongwei.wu.97@gmail.com\nrich temporal information provided by events can help to better avoid collisions with moving traffic agents and thus yield safer autonomous driving. In fact, when dealing with complex lighting scenes, such as driving during night or passing through a tunnel, the performance of frame-based methods may degrade severely [10]\u2013[13]. Meanwhile, event cameras are more robust in these adverse visual conditions since they can quickly adapt to light changes thanks to its low latency. Nevertheless, event data often provide wider object boundaries than RGB frames. This is mainly due to the synchronization between RGB and events since RGB frames are obtained during a short exposure time, while event cameras work in an asynchronous mode at a higher temporal resolution. Therefore, how to fully leverage the heterogeneous but mutually complementary modalities is yet an open research topic.\nIn the literature, a common practice [15], [18]\u2013[20] is to gradually merge event features in the RGB stream to guide the feature modeling as shown in Fig. 1. Despite the achieved plausible results, these works are often unidirectional and cannot thoroughly leverage modality-specific cues during the encoding stages, i.e., RGB-only, event-only, and fused features. Moreover, most existing works choose a pre-defined time interval as a hyper-parameter for event representation, which yields two major shortages: first, the fixed event representation cannot adapt to different scenarios without handcraft adjusting; secondly, a small range of event intervals, such as the exposure time of RGB camera, cannot fully leverage the rich temporal cues from the event\nar X\niv :2\n20 9.\n08 32\n3v 2\n[ cs\n.C V\n] 9\nM ar\n2 02\n3\ndata, while a long range of event intervals adds significant misaligned response.\nTo address the aforementioned issues, in this paper, we propose a novel RGB-Event network, named RENet, for moving object detection. Different from previous works basing on single-range events, as shown in Fig. 1, our event stream takes multi-range events as inputs to better benefit from the rich temporal cues. Specifically, for each event representation, we first apply convolutions to project the input event maps to a latent space. We then apply pooling with different kernel sizes to deal with events from different time ranges, i.e., larger pooling windows for longer-range events. Our motivation comes from the observation that the event frames of long-time ranges introduce local motion blur around the object boundaries. The motion blurs are essential for the detection of moving objects, while being not wellaligned with the image frame obtained during the exposure time. Therefore, we apply different sizes of max-pooling to preserve the most informative motion features and improve the scale invariance within a local region, yielding a simple yet efficient way to aggregate rich temporal events in a coarse-to-fine manner.\nOnce the multi-range events are fused in the latent space, we feed them through the dual-residual networks along with RGB features. To attentively integrate multi-modal features, we introduce a bi-directional calibration module that first improves each input feature by attending to the channel and the spatial dimensions of the input features and then realizes a cross-modal calibration. Finally, the cross-calibrated features are merged and further integrated with the hierarchical features, if any, to form the shared output. To the best of our knowledge, we are the first to apply attention modules for RGB-Event (moving) object detection tasks. Additionally, we provide DSEC-MOD, a new dataset dedicated to moving object detection from the widely used RGB-Event dataset DSEC [21]. DSEC-MOD contains moving objects with automatically labeled and manually checked bounding boxes. Extensive comparisons performed on DSEC-MOD demonstrate the effectiveness of RENet against state-of-theart RGB-Event fusion alternatives.\nTo summarize, our main contributions are three-fold: \u2022 We design a multi-range aggregation module to fully\nleverage the rich temporal information from events, which are crucial for moving object detection. \u2022 We introduce a novel middle fusion design for RGBEvent fusion that models both modality-specific and shared representations. \u2022 We propose a novel DSEC-MOD dataset with automatically labeled and manually verified annotation to encourage the development of moving object detection with events. We benchmark various state-of-the-art (SOTA) fusion modules on our dataset and show that RENet significantly outperforms the fusion alternatives."
        },
        {
            "heading": "II. RELATED WORK",
            "text": "Event Datasets: Unlike traditional datasets for global shutter RGB cameras, the number of available public event datasets\nis much fewer, and the types of tasks that may be practiced with these datasets are also limited. For example, several datasets [21]\u2013[24] only provide depth information as ground truth, including the two most widely used DSEC [21] and MVSEC [24]. To encourage the development of segmentation and detection tasks for autonomous driving, recent researches propose to induce from these two datasets and create sub-datasets for specific tasks. For example, MVSECNIGHT [22] induces from MVSEC and generates car annotations with the help of the pre-trained YOLO-v3 [25] for night scenarios. Sharing a similar motivation, [15] uses the pre-trained YOLO-v5 [26] to create the bounding boxes for objects from the DSEC dataset. There are also several other datasets targeting segmentation tasks, such as the DET dataset [27] which provides annotation for lane detection. However, these works focus more on 2D segmentation, with few works taking the temporal consistency and object motion into account. For autonomous driving, understanding object motions is crucial for driving safety. Therefore, we propose an RGB-Event dataset, called DSEC-MOD, to encourage further research on moving object detection. Event Processing: Event cameras have recently drawn great attention thanks to their asynchronous characteristic. As bioinspired sensors, several event-by-event researches aim to develop novel spiking neural network [28]\u2013[30] to process this novel sensor information [31], [32]. However, there are few works dealing with RGB-Event inputs, especially for multi-modal sensor fusion.\nFrom another perspective, events can be naturally considered as 4D inputs (x,y, p, t), where (x,y) stands for the spatial resolution as that of images, p stands for the polarity, and t stands for the temporal axis. Until nowadays, there is no conventional event representation for video applications as suggested in previous works [18], [33]. In the literature, there are three popular representations: frame-like methods [34]\u2013 [37] that focus more on dealing with (x,y, p) information, time-surface methods [38]\u2013[40] that focus more on (x,y, t) cues, and voxel-based methods [19], [41]\u2013[43] that deal with all dimensions at the same time (x,y, p, t). While voxel-based methods can fully explore both space-time information, its 4D structure requires higher computational cost compared to frame-based and time-surface methods, which are not suitable for real-time applications such as autonomous driving. In this paper, we propose a novel event representation for moving object detection. We build upon frame-based approaches and additionally integrate the time information through a temporal multi-scale aggregation, resulting in a simple yet effective manner to explore spatio-temporal cues. RGB-Event Fusion: With the development of event cameras, RGB-Event multi-modal fusion has drawn increasing research attention. Some works adopt a pre-trained network to realize event-frame conversion. Then, several works directly process the generated video in the same way as dealing with RGB video to achieve the target application. A recent work [44] applies knowledge distillation to guide the learning on videos generated from events. However, we argue that there is a significant difference between knowledge\ndistillation and multi-modal fusion. Other works take RGB and events from the input side and realize features during the feature modeling. [16], [45] proposes a late-fusion design to integrate events into RGB streaming to guide the feature decoding. Meanwhile, [15] adopts a middle-fusion design through simple concatenation convolution. [18] further leverages the transformer\u2019s attention to better guide the feature fusion. Sharing a similar idea, we also follow the middlefusion design to fuse RGB and events. Different from [15], [18] with uni-directional fusion design, as shown in Fig. 1, we explicitly model both modality-specific features and shared representation. Furthermore, we introduce a crossmodal calibration module to attentively correct the noisy response in each modality before feature fusion. Different from [18] which focuses on the channel axis, our attention module further attends to the spatial dimension, which plays an important role for object localization within a frame."
        },
        {
            "heading": "III. OUR WORK",
            "text": "Fig. 2 presents the overall framework of our network RENet. For simplicity, we only illustrate the case for singleframe RGB-Event inputs. In practice, inputs use several frames from video as well as the associated events.\nOur network is composed of a tailored stemming layer to merge temporal multi-scale events (E-TMA see Sec. III-A), a dual encoder network to extract features, a bi-directional calibration module to firstly improve and then generate the shared representation (BDC see Sec. III-C), and a decoder to output the bounding box of moving objects. Specifically, the multi-scale events are first fed into our proposed E-TMA to fully leverage the rich temporal motion cues. Then, we adopt a dual but discrepant residual streaming design to extract semantic maps from each modality. To realize the multi-modal fusion, we attend to both the channel and spatial dimensions of each modality and then enable a cross-modal calibration. This process aims to neglect inherent singlemodal noisy response and improve the feature modeling. Then, the enhanced multi-modal features together with the hierarchical features are fused in a coarse-to-fine manner. Finally, the encoded features are fed into the MOD detector\n[46] to produce the bounding box. The details will be introduced in the following sections.\nA. E-TMA: Event-based Temporal Multi-scale Aggregation\nWe observe from Fig. 3 that objects can slightly move from one position to another, i.e., there are more events on object boundaries while the temporal range of events becomes lager. However, the position changes are not significant nor sudden. Inspired by this observation, we design an aggregation module to establish the temporal relationship between multi-range event features E \u2032i . The aggregation must have two properties. First, it must maintain the homogeneous semantic cues across different ranges since the overall environment remains the same. Second, it must also be sensitive to spatially and temporally moving positions. Most existing works often process events within a pre-defined range, neglecting rich dynamic characteristics of event cameras.\nTo address this issue, we propose a multi-scale event aggregation module. Here, the term Scale does Not stand for the spatial resolution as conventionally used in image-based tasks. Instead, it stands for the different Temporal ranges widely used in the signal processing domain. Mathematically, during a specific temporal pool T = {Ti|i \u2208 N\u2217} under the rule that Ti \u2264 Ti+k with k \u2208 N, the groups of events \u03b5i are generated respectively. In our case, we take three scales of events as inputs, i.e, the events obtained from the RGB frame exposure time (\u03b51), the events obtained from an intermediate scale which is double the exposure time in our case (\u03b52), and the events obtained during one RGB frame time (\u03b53). To deal with multi-scale events, we first project the input events into a latent feature space. The projection module, denoted as \u03b7 , is similar to the first VGG layer which is composed of a combination of Convolution, Batch Normalization, and Relu activation. Each scale of events is fed into the same projection module to generate the event feature map. In other words, the projection weights are shared for each scale event input. The projection step can be expressed as:\nE \u2032i = \u03b7(\u03b5i), i \u2208 1,2,3. (1)\nTo model the objects moving within a local region, pooling operations with different window sizes are applied to the event features: features from long-range events are processed with coarser pooling, while features from small-range events are processed with finer pooling. Formally, let the pooling with different window sizes pooli = {pool1; pool2; pool3|size(pool1) < size(pool2) < size(pool3)}, we apply max pooling on top multi-scale event features E \u2032i :\ne\u2032i = pooli(E \u2032 i ), i \u2208 1,2,3. (2)\nNote that due to the different sizes of the pooling kernels, the pooled features do not share the same resolution. To merge them together, upsampling is applied to smaller-shape features to adjust the resolution. Finally, all features are concatenated together and fed into a convolution to generate the event representation fe. We have:\nfe =Conv(Concat(e\u20321;up(e \u2032 2);up(e \u2032 3))), (3)\nwhere up denotes the upsampling to match the resolution. The aggregated feature is later fed into the residual encoder [47] for semantic modeling.\nOur event modeling is significantly different from previous works. Several works simply take a pre-defined range of events as input [15], [16], which cannot fully leverage the rich temporal cues. A recent concurrent work [18] also proposes to deal with multi-scale events. However, they simply accumulate the polarities of multi-scale events from the input side. Despite its demonstrated effectiveness on image deblurring, this method fails to explicitly distinguish the object boundary and the background, yielding a noisy representation for object detection tasks. Differently, we propose to merge the multi-scale events at the latent space with the help of pooling operations. The choice of pooling is mainly based on its spatial scale and rotation invariant property which fits our application extremely well, as discussed previously. We particularly leverage the max pooling operation to extract the most informative cues in the event features. The motivation is based on the intuition that the object boundary should provide a strong response\nto distinguish the traffic participant from the background. To the best of our knowledge, we are the first to introduce a learnable aggregation module to extract, preserve, and fuse the most informative event features across different temporal scales. We show in Table I the superiority of our proposed event representation approach over our counterparts.\nB. Discrepant Two-Streaming Encoder\nMost existing multi-model models apply dual encoders for feature extraction. These encoders do not share weight but share the same architecture. However, we observe that RGB and event maps are significantly different, i.e., the former (RGB) provides dense textual information at each pixel, while the latter (event) only provides sparse activation for the pixels with brightness changes. Moreover, through our empirical experiments, as shown in Table I, we observe that event-only streaming yields significantly lower performance compared to RGB-only streaming. This observation motivates us to design a discrepant two-streaming encoder for feature extraction as shown in Fig. 2, i.e., a deeper encoder for RGB processing with ResNet-101 and a shallower encoder for event processing with Resnet-18. In such a way, we distinguish RGB and event modalities that the event stream plays an assist and auxiliary role. This discrepant design can also help to reduce computational costs.\nTo realize the multi-modal fusion, RGB and event features must share the same size. However, due to the discrepant encoders, the channel size for our RGB and event features are not the same. Therefore, before realizing the feature fusion, we apply a projection on the encoded event features to match the channel size of RGB features.\nC. BDC: Bi-Directional Calibration\nRGB-Event encoder fusion is a relatively novel research topic with very few papers compared to other multi-modal fusions such as RGB-Depth and RGB-Point cloud. Previous RGB-Event works or simply merge multi-modal features through simple concatenation convolution [15], or attend to the channel dimension with the help of transformer attention [18]. However, these methods do not explicitly tackle the spatial cues which are essential for segmentation/detection tasks. A recent tentative on knowledge transfer [44] has also shown great success for event processing. Nevertheless, we argue that there is a huge difference between multimodal fusion and knowledge distillation where the latter cannot fully leverage the cross-modal features compared to the former. Therefore, we seek to design an attentive fusion that can leverage the informative cues from both spatial and channel axis during the encoding stage.\nFor simplicity, we take the fusion at the highest semantic level as an example. Formally, let the RGB feature fR \u2208 RC\u00d7h\u00d7w and the projected event feature fE \u2208 RC\u00d7h\u00d7w. We first apply a transformation module based on 1\u00d71 convolution for activation. We obtain the activated feature maps fr and fe by:\nfr =Conv1\u00d71( fR); fe =Conv1\u00d71( fE). (4)\nThen we attentively fuse RGB and event features in a coarse-to-fine manner. Specifically, we first use a pixel-wise multiplication along with addition to coarsely enhance the most informative features in each modality. Formally, the enhanced features f \u2032r and f \u2032 e are computed by:\nf \u2032r = fr\u2297 fe + fr; f \u2032e = fr\u2297 fe + fe. (5)\nThen we refine the features by explicitly attending to the channel and spatial axes in a separate and successive manner. For each axis, we enable a bi-directional calibration, i.e., we learn the features from one modality and apply it to the other. Then, we merge calibrated multi-modal features while maintaining the most informative features. For simplicity, we show the example with channel calibration. Let CA be the channel attention module from [48]. The cross-calibration along the channel axis can be formulated as:\nf CAr =CA( f \u2032 e)\u2297 f \u2032r + f \u2032r ; f CAe =CA( f \u2032r)\u2297 f \u2032e + f \u2032e. (6)\nSharing the same protocol, we further refine the features spatially. Let SA be the spatial attention from [48], we obtain the final enhanced features f enhr and f enh e as follows:\nf enhr = SA( f CA e )\u2297 f CAr + f CAr ; f enhe = SA( f CAr )\u2297 f CAe + f CAe .\n(7) These designs aim to find the most truthful and confident channels/pixels within each modality-specific feature through the deep network. Moreover, cross-calibration can selectively calibrate the noisy features while maintaining the informative cues, yielding a simple yet efficient manner to boost the feature modeling with the help of complementary cues. Finally, the calibrated features are attentively merged through a convolution that learns the contribution weights from the most informative components to form the shared output:\nf =Conv3\u00d73(Concat( f enhr \u2297 f enhe ;max( f enhr , f enhe ))). (8)\nFurther, the fused output is integrated with hierarchical features, i.e., the output from the previous layer (if any)."
        },
        {
            "heading": "IV. EXPERIMENTS",
            "text": "A. Our RGB-Event Moving Object Dataset\nTo the best of our knowledge, DSEC is the largest RGBEvent dataset for autonomous driving. However, DSEC only proposes the depth ground truth, without object bounding box annotation. [15] proposes to generate object annotation with the pre-trained YOLO-V5 network. However, the used dataset is not available. Moreover, the generated annotation takes both statistics and dynamic objects into account. Therefore, to promote the research of moving object detection with events, we introduce a novel DSEC-MOD dataset.\nSpecifically, we first calibrate the multi-modal input. Following [21], we project the RGB frames to the event-based coordinates with the help of camera matrices. In this process, RGB and event maps have the same field of view and the same resolution. Then, based on the calibrated RGB videos, we manually label the annotation for moving objects. In our dataset, 8 object classes are taken into account: car, truck, bus, train, pedestrian, cyclist, motorcyclist, and others. Since\nwe aim to build a challenging dataset with multiple moving objects in the scene, our sequences contain at least 3 different types of moving objects. In total, our DSEC-MOD dataset contains 16 sequences (13314 frames), with 11 sequences (10495 frames) for training and 5 other sequences (2819 frames) for testing. To the best of our knowledge, our moving object dataset is the largest of its kind.\nB. Experimental Setup\nImplementation Details: We choose ResNet-101 and ResNet-18 as our backbones. The input frames are resized to 288\u00d7288. We apply classical data augmentations such as photometric transformation, scale, and location jittering. We follow [46] to use adam optimizer with the initial learning rate 5e-4, then decreasing by a factor of 10 at the 10th epochs and 15th epochs. The whole model is trained for 30 epochs. Evaluation Metrics: We use frame mean average precision (F. mAP) and video mean average precision (V. mAP) for evaluation. Frame mAP evaluates the detection quality in a single frame, while video mAP also considers the linking of bounding boxes from a temporal perspective [54]\u2013[56].\nC. Comparison\nComparison with SOTA fusion alternatives: To the best of our knowledge, there are very few works on RGB-Event fusion during the encoder stage. Moreover, we are the first to conduct a study on moving object detection with RGBEvent input. Therefore, to purely and fairly analyze our performance, we replace our proposed modules with the state-of-the-art fusion alternatives from RGB-Event domain: FPN-Fusion [15], EFNet [18]; and from RGB-D domain: SAGate [51], DCF [52], SPNet [53]. We also compare with famous self-attention modules such as SENet [49], ECAnet [50], and CBAM [48]. Specifically, we maintain the same multi-scale event input with our E-TMA (if with event input), same backbone, same decoder, loss, and same training settings as ours. The only difference is in the RGB-Event fusion module. Note that for all listed SOTA alternatives, we apply the attention modules through middle fusion.\nAs shown in Table I, compared to both self-attention and cross-modal attention works, our RENet outperforms its counterparts by a wide margin, validating our proposed fusion design. Specifically, compared to FPN-Fusion [15] which is based on middle fusion with concat-conv, our fusion is based on attention which can better deal with informative features across different modalities. Different from EFnet [18] which attends only to the channel dimension, our fusion leverages both channel and spatial attention and realizes in addition a cross-calibration to improve the feature modeling. Comparison under different illumination conditions: To better understand the contribution of event cameras, we analyze the performance gain under different illumination conditions. The detailed comparison can be found in Table II. It can be seen that with the complementary events, our proposed modules can significantly and consistently boost the baseline performance under different lighting conditions.\nQualitative Comparison: Fig. 4 depicts the qualitative comparison between our method and the RGB baseline. We can visualize the significant improvement in detecting objects with the help of events, especially in detecting small objects on the scene that are extremely challenging for RGB baseline. Moreover, when RGB baseline results in false positive due to the inferior lighting condition, our RGBEvent network can reason about more robust detection.\nD. Ablation Studies\nWe also conduct ablation studies to analyze the contribution of each component. Specifically, we gradually add our proposed modules on top of the RGB baseline. It can be seen that each proposed method is helpful and can enable\nperformance gain. We also replace our proposed multi-scale event aggregation with the recently proposed multi-scale accumulation [18]. The main difference is that our methods aggregate multi-scale events at the semantic level in a coarseto-fine manner, while [18] simply concatenates multiple event frames from the input side. Empirically, we can see that the performance drops significantly, which demonstrates the superiority of our proposed module in modeling event data for (moving) object detection."
        },
        {
            "heading": "V. CONCLUSION",
            "text": "In this paper, we proposed a novel fusion architecture for moving object detection with RGB-Event inputs. Different from previous works based on single-scale event inputs, we introduce a multi-scale events aggregation to explicitly leverage the rich temporal cues from the asynchronous event sensor. Furthermore, we gradually fuse the heterogeneous RGB and event features in a coarse-to-fine manner, yielding a simple yet efficient cross-calibration mechanism with the help of different forms of attention. Finally, we propose a novel DSEC-MOD dataset to encourage further research on RGB-Event fusion for moving object detection. Extensive comparisons of our proposed dataset validate the effectiveness and robustness of our approach compared to the stateof-the-art fusion alternatives. In our future work, we will test our methods on more recent approaches. We will also collect a real-world RGB-Event dataset for autonomous driving."
        }
    ],
    "title": "RGB-Event Fusion for Moving Object Detection in Autonomous Driving",
    "year": 2023
}