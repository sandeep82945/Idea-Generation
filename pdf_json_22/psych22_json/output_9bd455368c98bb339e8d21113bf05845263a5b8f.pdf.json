{
    "abstractText": "Deep Convolutional Neural Networks (CNNs) are criticised for their reliance on local shape features and texture rather than global shape. We test whether CNNs are able to process global shape information in the absence of local shape cues and texture by testing their performance on Mooney stimuli, which are face images thresholded to binary values. More specifically, we assess whether CNNs classify these abstract stimuli as face-like, and whether they exhibit the face inversion effect (FIE), where upright stimuli are classified positively at a higher rate compared to inverted. We tested two standard networks, one (CaffeNet) trained for general object recognition and another trained specifically for facial recognition (DeepFace). We found that both networks perform perceptual completion and exhibit the FIE, which is present over all levels of specificity. By matching the false positive rate of CNNs to humans, we found that both networks performed closer to the human average (85.73% for upright, 57.25% for inverted) for both conditions (61.31% and 62.70% for upright, 48.61% and 42.26% for inverted, for CaffeNet and DeepFace respectively). Rank order correlation between CNNs and humans across individual stimuli shows a significant correlation in upright and inverted conditions, indicating a relationship in image difficulty between observers and the model. We conclude that in spite of the texture and local shape bias of CNNs, which makes their performance distinct from humans, they are still able to process object images holistically.",
    "authors": [
        {
            "affiliations": [],
            "name": "Astrid Zeman"
        },
        {
            "affiliations": [],
            "name": "Tim Leers"
        },
        {
            "affiliations": [],
            "name": "Hans Op de Beeck"
        }
    ],
    "id": "SP:b054d4eac791c219cd2691de2c1a7cbee47c40e2",
    "references": [
        {
            "authors": [
                "N. Baker",
                "H. Lu",
                "G. Erlikhman",
                "P.J. Kellman"
            ],
            "title": "Deep convolutional networks do not classify based on global object shape",
            "venue": "PLoS Computational Biology,",
            "year": 2018
        },
        {
            "authors": [
                "W. Brendel",
                "M. Bethge"
            ],
            "title": "Approximating CNNs with Bag-of-local-Features models works surprisingly well on ImageNet",
            "venue": "International Conference on Learning Representations (ICLR)",
            "year": 2019
        },
        {
            "authors": [
                "J.J. DiCarlo",
                "D. Zoccolan",
                "N.C. Rust"
            ],
            "title": "How Does the Brain Solve Visual Object Recognition",
            "year": 2012
        },
        {
            "authors": [
                "R. Geirhos",
                "P. Rubisch",
                "C. Michaelis",
                "M. Bethge",
                "F.A. Wichmann",
                "W. Brendel"
            ],
            "title": "ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness",
            "venue": "International Conference on Learning Representations (ICLR)",
            "year": 2019
        },
        {
            "authors": [
                "K. Grill-Spector",
                "R. Malach"
            ],
            "title": "The Human Visual Cortex",
            "venue": "Annual Review of Neuroscience,",
            "year": 2004
        },
        {
            "authors": [
                "U. G\u00fc\u00e7l\u00fc",
                "M.A. Gerven"
            ],
            "title": "Deep neural networks reveal a gradient in the complexity of neural representations across the ventral stream",
            "venue": "Journal of Neuroscience,",
            "year": 2015
        },
        {
            "authors": [
                "Y. Jia",
                "E. Shelhamer",
                "J. Donahue",
                "S. Karayev",
                "J. Long",
                "R. Girshick",
                "T. . . Darrell"
            ],
            "title": "Caffe: Convolutional Architecture for Fast Feature Embedding",
            "venue": "arXiv preprint arXiv:1408.5093",
            "year": 2014
        },
        {
            "authors": [
                "N. Kanwisher",
                "F. Tong",
                "K. Nakayama"
            ],
            "title": "The effect of face inversion on the human fusiform face",
            "venue": "area. Cognition,",
            "year": 1998
        },
        {
            "authors": [
                "Ke",
                "T.-W",
                "S.X. Yu",
                "D. Whitney"
            ],
            "title": "Mooney face classification and prediction by learning across tone",
            "venue": "IEEE International Conference on Image Processing (ICIP)",
            "year": 2017
        },
        {
            "authors": [
                "Khaligh-Razavi",
                "S.-M",
                "N. Kriegeskorte"
            ],
            "title": "Deep Supervised, but Not Unsupervised, Models May Explain IT Cortical Representation",
            "venue": "PLoS Computational Biology,",
            "year": 2014
        },
        {
            "authors": [
                "S.R. Kheradpisheh",
                "M. Ghodrati",
                "M. Ganjtabesh",
                "T. Masquelier"
            ],
            "title": "Deep Networks Can Resemble Human Feed-forward Vision inInvariant Object Recognition",
            "venue": "Scientific Reports,",
            "year": 2016
        },
        {
            "authors": [
                "B. Kim",
                "E. Reif",
                "M. Wattenberg",
                "S. Bengio"
            ],
            "title": "Do Neural Networks Show Gestalt Phenomena",
            "venue": "An Exploration of the Law of Closure. arXiv,",
            "year": 2019
        },
        {
            "authors": [
                "A. Krizhevsky",
                "I. Sutskever",
                "G.E. Hinton"
            ],
            "title": "ImageNet Classification with Deep Convolutional Neural Networks",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2012
        },
        {
            "authors": [
                "J. Kubilius",
                "S. Bracci",
                "H.P. Op de Beeck"
            ],
            "title": "Deep Neural Networks as a Computational Model for Human Shape Sensitivity",
            "venue": "PLoS Computational Biology,",
            "year": 2016
        },
        {
            "authors": [
                "M. Latinus",
                "M.J. Taylor"
            ],
            "title": "Holistic Processing of Faces: Learning Effects with Mooney Faces",
            "venue": "Journal of Cognitive Neuroscience,",
            "year": 2005
        },
        {
            "authors": [
                "Q.V. Le",
                "M. Ranzato",
                "R. Monga",
                "M. Devin",
                "K. Chen",
                "G.S. Corrado",
                "A.Y. . . Ng"
            ],
            "title": "Building high-level features using large scale unsupervised learning",
            "year": 2012
        },
        {
            "authors": [
                "C.M. Mooney"
            ],
            "title": "Age in the development of closure ability in children",
            "venue": "Canadian Journal of Psychology/Revue canadienne de psychologie, 11(4), 219-226.",
            "year": 1957
        },
        {
            "authors": [
                "D.W. Piepers",
                "R.A. Robbins"
            ],
            "title": "A review and clarification of the terms \u201cholistic,",
            "venue": "\u201cconfigural,\u201d and \u201crelational\u201d in the face perception literature. Frontiers in Psychology",
            "year": 2012
        },
        {
            "authors": [
                "R. Rajalingham",
                "E. Issa",
                "P. Bashivan",
                "K. Kar",
                "K. Schmidt",
                "J. DiCarlo"
            ],
            "title": "Large-scale, high-resolution comparison of the core visual object recognition behavior of humans, monkeys, and state-of-the-art deep artificial neural networks",
            "venue": "The Journal of Neuroscience,",
            "year": 2018
        },
        {
            "authors": [
                "C.M. Schwiedrzik",
                "L. Melloni",
                "A. Schurger"
            ],
            "title": "2018). Mooney face stimuli for visual perception research",
            "venue": "PLoS ONE, 13(7),",
            "year": 2001
        },
        {
            "authors": [
                "J. Tanaka",
                "M. Farah"
            ],
            "title": "Parts and wholes in face recognition",
            "venue": "The Quarterly Journal of Experimental Psychology,",
            "year": 1993
        },
        {
            "authors": [
                "H. Tang",
                "M. Schrimpf",
                "W. Lotter",
                "C. Moerman",
                "A. Paredes",
                "J.O. Caro",
                "G. . . Kreiman"
            ],
            "title": "Recurrent computations for visual pattern completion",
            "year": 2018
        },
        {
            "authors": [
                "J. Taubert",
                "D. Apthorp",
                "D. Aagten-Murphy",
                "D. Alais"
            ],
            "title": "The role of holistic processing in face perception: evidence from the face inversion effect",
            "venue": "Vision Research,",
            "year": 2011
        },
        {
            "authors": [
                "M. Wertheimer"
            ],
            "title": "Untersuchungen zur Lehre von der Gestalt, II",
            "venue": "Psychologische, 4, 301\u2013 350.",
            "year": 1923
        },
        {
            "authors": [
                "D.L. Yamins",
                "H. Hong",
                "C.F. Cadieu",
                "E.A. Solomon",
                "D. Seibert",
                "J.J. DiCarlo"
            ],
            "title": "Performance-optimized hierarchical models predict neural responses in higher visual cortex",
            "year": 2014
        },
        {
            "authors": [
                "R.K. Yin"
            ],
            "title": "Looking at upside-down faces",
            "venue": "Journal of Experimental Psychology, 81, 141145.",
            "year": 1969
        },
        {
            "authors": [
                "A.W. Young",
                "D. Hellawell",
                "D.C. Hay"
            ],
            "title": "Configurational information in face perception",
            "year": 1987
        },
        {
            "authors": [
                "A.A. Zeman",
                "J.B. Ritchie",
                "S. Bracci",
                "H. Op de Beeck"
            ],
            "title": "Orthogonal Representations of Object Shape and Category in Deep Convolutional Neural Networks and Human Visual Cortex",
            "venue": "Scientific Reports,",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "Deep Convolutional Neural Networks (CNNs) are criticised for their reliance on local shape features and texture rather than global shape. We test whether CNNs are able to process global shape information in the absence of local shape cues and texture by testing their performance on Mooney stimuli, which are face images thresholded to binary values. More specifically, we assess whether CNNs classify these abstract stimuli as face-like, and whether they exhibit the face inversion effect (FIE), where upright stimuli are classified positively at a higher rate compared to inverted. We tested two standard networks, one (CaffeNet) trained for general object recognition and another trained specifically for facial recognition (DeepFace). We found that both networks perform perceptual completion and exhibit the FIE, which is present over all levels of specificity. By matching the false positive rate of CNNs to humans, we found that both networks performed closer to the human average (85.73% for upright, 57.25% for inverted) for both conditions (61.31% and 62.70% for upright, 48.61% and 42.26% for inverted, for CaffeNet and DeepFace respectively). Rank order correlation between CNNs and humans across individual stimuli shows a significant correlation in upright and inverted conditions, indicating a relationship in image difficulty between observers and the model. We conclude that in spite of the texture and local shape bias of CNNs, which makes their performance distinct from humans, they are still able to process object images holistically.\nKeywords: deep neural networks, mooney faces, CNNs, texture, perceptual completion\nMOONEY IMAGES IN DEEP NETWORKS 3\nIntroduction\nHumans are able to quickly recognise and categorise objects (Thorpe, Fize, & Marlot,\n1996). The underlying neural system is conceptualised as an hierarchical information processing system that starts from a simple analysis of the visual input in terms of local features and ends with a high-level representation of complex object and category properties (Grill-Spector & Malach, 2004; DiCarlo, Zoccolan, & Rust, 2012). Deep convolutional neural networks (CNNs) have been developed with a similarly deep and hierarchical architecture, demonstrating success in various object recognition tasks. The representations developed in these networks through learning show interesting similarities with human brain and behaviour (G\u00fc\u00e7l\u00fc & Gerven, 2015; Khaligh-Razavi & Kriegeskorte, 2014; Kheradpisheh, Ghodrati, Ganjtabesh, & Masquelier, 2016; Yamins, et al., 2014). For example, CNNs represent visual object shape (Kubilius, Bracci, & Op de Beeck, 2016), independently from object category, which corresponds well with orthogonal representations of shape and category along the visual ventral stream (Zeman, Ritchie, Bracci, & Op de Beeck, 2020).\nDespite their success in object recognition, deep convolutional neural networks (CNNs)\nreveal a shortcoming in processing global over local shape information (Baker, Lu, Erlikhman, & Kellman, 2018; Geirhos, et al., 2019; Brendel & Bethge, 2019; Xu, et al., 2018). Baker et al. (2018) found that the classification performance of CNNs was resilient to disruptions in global shape, but susceptible to local contour changes, an effect opposite to that of human observers. Brendel & Bethge (2019) argued that CNN performance is strongly correlated with local feature information by demonstrating that high classification performance in ImageNet (Russakovsky, et al., 2015) could be achieved by a bag-of-features network that had reduced patch size with disrupted spatial relationships between patches. Geirhos et al. (2019) assessed network\nMOONEY IMAGES IN DEEP NETWORKS 4\nclassification using a cue conflict paradigm, where the texture of one object was superimposed over another object, and CNNs demonstrated a preference for object texture over shape, in contrast to humans. Xu et al. (2018) found that CNNs trained to identify faces are more sensitive to changes in texture compared to shape. These studies provide mounting evidence for an emphasis on local processing measures in CNNs, making their performance distinct from humans.\nTo interrogate the level of local versus global processing in CNNs, we assess their\nperformance using a large battery of Mooney images, which are low-information images of faces that invoke face perception despite their high ambiguity (Mooney, 1957). Mooney images contain only isoluminant black and white areas and therefore have no texture and reduced local shape information. Global shape information processing is necessary for perceiving faces in these two-tone images, in which an observer must be able to integrate incomplete segments into a perceptual whole, known as \u201cperceptual closure\u201d (Wertheimer, 1923). The ability to separate Mooney faces from scrambled images is therefore a relevant index to assess this form of global shape processing. Recently, CNNs have been found to exhibit perceptual closure when trained on natural images (Kim, Reif, Wattenberg, & Bengio, 2019). CNNs that are randomly initialised or trained on random data do not exhibit closure, and closure ability is correlated with higher layers, peaking in the layer prior to prediction read-out (Kim, Reif, Wattenberg, & Bengio, 2019). Feed-forward networks display moderately high levels of pattern completion, which can be improved upon with the addition of recurrent connections to achieve human-level performance (Tang, et al., 2018). Given that CNNs are capable of closure and pattern completion, and are able to detect faces (Le, et al., 2012), it is logical that they are able to detect\nMOONEY IMAGES IN DEEP NETWORKS 5\nfaces within Mooney images, which has indeed been established in a set of Mooney images artificially generated by a CNN (Ke, Yu, & Whitney, 2017).\nHere we investigate closure ability in CNNs at a finer level than in previous studies,\nallowing us to establish a relationship with human perception of Mooney faces. One major aim is to test the effect of face inversion (Yin, 1969; Taubert, Apthorp, Aagten-Murphy, & Alais, 2011), which is one paradigm used to interrogate one form of global processing referred to as holistic processing (Piepers & Robbins, 2012), next to composite tasks (Young, Hellawell, & Hay, 1987) and part-whole tasks (Tanaka & Farah, 1993). Given that the parts are no longer accessible in Mooney faces, complicating the use of composite and part-whole tasks, the effect of inversion is the most obvious candidate paradigm to further test global processing in Mooney faces. Human observers show a clear face inversion effect (FIE) with Mooney faces, where a vertically mirrored Mooney image produces lower detection rates, slower reaction times and reduced activation in face-specific areas of the brain (Latinus & Taylor, 2005; Kanwisher, Tong, & Nakayama, 1998).\nAnother major aim is to investigate whether there is a correspondence between human\nand CNN performance in terms of which stimuli are more easily detected as containing a face. Previous studies in other domains have shown that patterns of errors can be similar between humans and CNNs, yet there is not always a tight relationship up to the image level (Lindsay, 2020; Rajalingham, et al., 2018). Here we make use of a recent extension of the original set of 50 Mooney faces (Mooney, 1957) to over 500 Mooney faces, with accompanying human observer accuracy and reaction times (Schwiedrzik, Melloni, & Schurger, 2018). With this larger dataset, we can thoroughly test CNNs and their ability to process local versus global shape information\nMOONEY IMAGES IN DEEP NETWORKS 6\nand compare directly to human behavioural ratings (which is absent from Ke, Yu, & Whitney, 2017).\nIn sum, we address the following questions. First, can deep networks detect faces in a\nlarge set of Mooney stimuli and demonstrate robustness in handling low information images that contain shadow, incomplete contours, no colour information, etc? Second, is there an inversion effect for Mooney faces in CNNs? Third, are the images that are difficult for humans also difficult for networks?\nMethods"
        },
        {
            "heading": "Stimuli",
            "text": "We use the Mooney face online dataset containing 96 scrambled, 504 upright and 504\ninverted images from Schwiedrzik, Melloni, & Schurger (2018). Scrambled images contain contiguous regions with no sharp boundaries and were classified by humans to contain \u201cno face\u201d over 85% of the time in a pilot study (the false positive rate was higher during the actual study). Images were resized to 256 x 256 pixels prior to standard model input transformations. The online database of Schwiedrzik, Melloni, & Schurger (2018) has slightly fewer images than reported in their study, which were excluded due to copyright reasons. To ensure a fair comparison between human and CNN performance, we report only on data that relates directly to the images available online.\nMOONEY IMAGES IN DEEP NETWORKS 7"
        },
        {
            "heading": "CNNs",
            "text": "We test classification performance of two standard CNNs, one that has been trained for\ngeneral object recognition (CaffeNet), and another that is trained specifically for facial recognition (DeepFace).\nCaffeNet \u2013 is an 8-layer, sequentially organised network with 5 convolutional layers\nfollowed by 3 fully-connected layers. CaffeNet is an implementation of AlexNet (Krizhevsky, Sutskever, & Hinton, 2012) in the Caffe Framework (Jia, et al., 2014). CaffeNet is trained for general object recognition using the ImageNet database (Russakovsky, et al., 2015) and to distinguish between 1000 classes that do not explicitly contain faces.\nWe assess positive performance of CaffeNet by taking the average of its classification of\nimages into 2 categories that may be considered as face-like, namely Mask and Ski Mask. These face-like categories are most likely to contain a feature configuration with two-eyes above a mouth, and sometimes contain human faces within the actual images (wearing the mask). For classification performance, we take the top 5 model predictions, where chance level is calculated to be 1.002% using the following reasoning. Chance level for selecting one of the 2 face-like categories as the Top 1 selection is simply 2/1000 or 0.2%. To calculate chance level for the Top 2 results, we add 0.2% with the possibility of 2 out of 999 remaining selections (having nonreplacement of categories), assuming that one of the 2 possible categories was not selected as the top 1 result. We repeat this process to calculate the Top 5 chance level, which is 2/1000 + 2/999 + 2/998 + 2/997 + 2/996 = 1.002%.\nDeepFace \u2013 is a serial network with 6 parameterised layers (4 convolutional followed by\n2 fully-connected) and 3 max pooling layers that follow every convolutional layer (Chen, 2016). DeepFace performs face detection, alignment and recognition, and is trained on the CASIA-\nMOONEY IMAGES IN DEEP NETWORKS 8\nWebFace database. We assess positive performance of DeepFace using binary recognition, where chance level is 50%."
        },
        {
            "heading": "ROC curve and rank-order correlations",
            "text": "A Receiver Operating Characteristic or ROC curve is created by plotting the true positive\nrate (sensitivity) against the false positive rate at different threshold levels. For our purposes, an ROC curve allows us to determine how well each CNN is able to detect faces in the Mooney images under differing levels of specificity. To construct the ROC curves, we take each image and run a forward pass through the model and read out the probability predicted by the model that it belongs to the face class. This probability is thresholded so that the images associated with a probability above the threshold are classified as faces and the other images as nonfaces. Then, under different threshold levels, we determine the percentage of face images that are classified as belonging to the face class as well as the percentage of nonface images that are classified as a face. To calculate the rank-order correlations for each model, we also use the probability scores that are generated for each image by each model. For CaffeNet, we take the average of the two probabilities that the image belongs to two of the face-like classes, and use this to obtain the rank order.\nResults\nPerceptual Completion and the FIE in CNNs\nWe first tested the networks (CaffeNet, DeepFace) in their ability to classify an image as containing a face (in the case of DeepFace) or a face-like object (in the case of CaffeNet), for each of the three conditions (upright, inverted and scrambled). CaffeNet was evaluated using the top 5 category ranking, for two face-like categories (Mask or Ski-mask). DeepFace was\nMOONEY IMAGES IN DEEP NETWORKS 9\nevaluated based on the binary classification of whether each image contained a face or not. Average human accuracy was 85.73% for upright Mooney faces, and 57.25% for inverted, calculated from raw data in Schwiedrzik, Melloni, & Schurger (2018). On average, humans recognised faces 27.36% of the time in the 96 scrambled images, determining the average false positive level.\nIf the networks are able to perform perceptual completion of the Mooney face stimuli, we\nexpect to see higher classification accuracy for upright versus scrambled images. If the networks exhibit an FIE, we expect to see higher performance for upright versus inverted images. Figure 1 illustrates the classification results for the two CNNs. Both networks were able to perform perceptual completion, showing upright performance as higher than scrambled (CaffeNet: difference of 28.03%, DeepFace: 28.92). Both CNNs exhibited an FIE, with upright performance being higher than inverted (CaffeNet: 15.48% difference between upright and inverted, DeepFace: 23.41 difference). Both networks showed results qualitatively consistent with human performance \u2013 with both upright and inverted accuracy levels being above the false positive (scrambled) rate. Even though both CNNs showed higher performance for upright (Caffenet: 47.82%, DeepFace: 38.29%) versus inverted (CaffeNet: 32.34%, DeepFace: 14.88%) images, this was still below the average human response of 85.73% and 57.25% for these same images, however the false positive rate was also lower for the CNNs (CaffeNet: 19.79%, DeepFace: 9.38%) compared to humans (27.36%).\nMOONEY IMAGES IN DEEP NETWORKS 10\nConsidering that the false positive rate (scrambled image performance) for both CNNs was below the human average, we plotted an ROC curve to better determine the upright versus inverted performance of each CNN when the false positive rate matched humans (see Methods). CaffeNet ROC curves for upright (blue) and inverted (green) conditions are shown in Figure 2, against human performance taken from Schwiedrzik, Melloni, & Schurger (2018). Average human accuracy levels are indicated by asterisks at the average false positive mark, and individual subject performance is indicated by squares. To plot CaffeNet performance, we measured the model\u2019s predicted probability scores generated for the two face-like categories (Mask and Ski Mask) for each stimulus condition and averaged these. We then adjusted the detection threshold to determine the true positive rate (upright or inverted) versus the false positive rate (scrambled). When the false positive rate of CaffeNet was 27.08% (roughly equivalent to the human false positive rate of 27.36%), upright performance was 61.31% and\nMOONEY IMAGES IN DEEP NETWORKS 11\ninverted was 48.61 . These values are now closer to the average human performance levels of 85.73% for upright and 57.25% for inverted (illustrated by the asterisks in Figure 2), although they are still not as high as human levels. For DeepFace, when the false positive rate was 27.08%, upright performance was 62.70% and inverted was 42.26%. One interesting feature to note is that for both networks, upright performance is always higher than inverted, demonstrating an FIE over all false positive levels. The size of the inversion effect remains roughly consistent across different levels of specificity.\nRank order correlation between CNNs and humans We tested the rank order correlation of CNN accuracy with human accuracy levels for\nupright versus inverted conditions. CaffeNet accuracy was calculated using the average prediction probabilities of Mask and Ski Mask categories. The noise ceiling was calculated from human performance using split-half cross validation across two subgroups of human participants. Results are shown in Table 1. Before Noise Correction (NC), Spearman correlation for CaffeNet was \u03c1 = 0.26685 (p < 0.0001) for upright images and \u03c1 = 0.17241 (p = 0.0001) for inverted. After NC, upright correlation was \u03c1 = 0.31727 and inverted was \u03c1 = 0.20814. For DeepFace, before NC, Spearman correlation was \u03c1 = 0.28807 (p < 0.0001) for upright images and \u03c1 = 0.21943 (p < 0.0001) for inverted. After NC, Spearman correlation was \u03c1 = 0.34250 (p < 0.0001) for upright images and \u03c1 = 0.26491 (p = 0.014) for inverted. These values demonstrate a significant, although not a large relationship, between image difficulty for humans and CNNs. The strength of rank order correlation appears to also correlate with performance, given that higher rho values correspond with higher accuracy levels, in agreement with Yamins, et al. (2014).\nMOONEY IMAGES IN DEEP NETWORKS 12\na) ROC Curve for CaffeNet\nMOONEY IMAGES IN DEEP NETWORKS 13\nIn this paper we examined two standard CNN models and their ability to detect faces in\nambiguous Mooney images. We first analysed network performance using the default discrimination levels for each CNN and found that the false positive rate for each model (using scrambled images) was lower than in human observers. We then examined the face detection levels of each network under differing levels of specificity, to determine levels of performance when the false positive rate was matched to humans. Finally, with each model, we ranked images in their predicted probability to contain a face and correlated this with human observers to quantify any overlapping similarity in image difficulty. From these experiments, we present three main findings: 1. Deep networks are able to detect faces in Mooney stimuli, demonstrating robust detection in these low information images; 2. CNNs exhibit a face inversion effect with Mooney faces, which is evident across all degrees of specificity; and 3. Mooney images that are difficult for humans are also difficult for CNNs, as demonstrated by a significant rank order correlation between CNNs and humans for upright and inverted images.\nMOONEY IMAGES IN DEEP NETWORKS 14\nIn this study, we selected two CNNs that were trained for very different purposes using\nvery different datasets. One network (CaffeNet) was trained for general object recognition, whereas the other (DeepFace) was trained for face recognition, which is more relevant to this task. Despite the large differences in training datasets, both networks, surprisingly, behaved quite similarly. The large agreement between both networks is most evident in the ROC curves, which show similar levels of performance. Note that neither network was trained using Mooney images, which we would expect would improve performance and also potentially improve the correlation with human observers. The results that we present here showcase the generalisability and robustness of these networks.\nWhile we do not claim that these networks are equivalent to human object and face\nprocessing, it is clear that they are capable of similar feats, albeit to a lesser extent. For example, while CNNs exhibit a bias for texture over shape (Baker, Lu, Erlikhman, & Kellman, 2018; Geirhos, et al., 2019; Brendel & Bethge, 2019; Xu, et al., 2018), and a bias for local over global shape (Baker, Lu, Erlikhman, & Kellman, 2018;), they are still able to classify objects with no texture information, in the form of silhouettes (Kubilius, Bracci, & Op de Beeck, 2016). Nevertheless, when controlling for local shape information, as tested by the GIST model, CNNs represent global shape information in multiple network layers (Zeman, Ritchie, Bracci, & Op de Beeck, 2020). We hypothesize that these models could achieve better levels of performance, on par with humans, with the integration of recurrent connections at higher network layers, as implemented in Tang et al. (2018). In addition, we also expect effects of changes in training history that go beyond the differences in training regime between the two networks that we have tested here. Shape or texture preferences in networks can be adjusted via the training regime of networks, as demonstrated recently by Geirhos et al. (2019). The authors conclude that texture\nMOONEY IMAGES IN DEEP NETWORKS 15\nbias can be reduced, or even overcome, with the adjustment of weights within networks, to better promote global processing. In relation to our work, we can see that although there is a difference between human and CNN performance, the difference is in weighting schemes, rather than representational capabilities. To reduce the gap between human and CNN performance, we also suggest adopting a similar training regime as described by Geirhos et al. (2019).\nWe conducted ROC analyses to determine the discrimination levels of each network\nunder varying degrees of false positive rates, particularly noting when the level was matched to that of humans. At all false positive levels, we found upright discrimination was higher than inverted, demonstrating the robustness of the inversion effect. Effects in CNNs were in the same direction as humans, but the effect sizes were smaller in CNNs. When examining CNN versus human performance more closely using rank order correlations, the correlations reached a certain level but were definitely not at ceiling. This suggests that there are still differences between CNNs and humans at the image level.\nHere we show that CNNs can classify low information images of quite complex visual\nstimuli to perform perceptual completion on images with no texture and no obvious local cues to dissociate face from nonface images. CNNs also exhibit holistic processing on these zero texture images, at all layers of specificity, as demonstrated by the inversion effect. In conclusion, despite the preference of CNNs to take into account texture over shape, which differs from humans, they still process object images in a holistic manner, demonstrating both perceptual completion and the inversion effect.\nMOONEY IMAGES IN DEEP NETWORKS 16\nAcknowledgements\nWe thank Caspar Schwiedrzik for providing additional raw data and help in answering\nquestions regarding his study. This work was funded by grant C14/16/031 of the KU Leuven Research Council.\nReferences\nBaker, N., Lu, H., Erlikhman, G., & Kellman, P. J. (2018). Deep convolutional networks do not\nclassify based on global object shape. PLoS Computational Biology, 14(12), e1006613. doi:10.1371/journal.pcbi.1006613\nBrendel, W., & Bethge, M. (2019). Approximating CNNs with Bag-of-local-Features models\nworks surprisingly well on ImageNet. International Conference on Learning Representations (ICLR).\nChen, R. (2016, 9 11). RiweiChen/DeepFace. Retrieved from GitHub:\nhttps://github.com/RiweiChen/DeepFace\nDiCarlo, J. J., Zoccolan, D., & Rust, N. C. (2012). How Does the Brain Solve Visual Object\nRecognition? Neuron, 73(3), 415-434. doi:10.1016/j.neuron.2012.01.010\nGeirhos, R., Rubisch, P., Michaelis, C., Bethge, M., Wichmann, F. A., & Brendel, W. (2019).\nImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness. International Conference on Learning Representations (ICLR).\nGrill-Spector, K., & Malach, R. (2004). The Human Visual Cortex. Annual Review of\nNeuroscience, 27, 649-677. doi:10.1146/annurev.neuro.27.070203.144220\nG\u00fc\u00e7l\u00fc, U., & Gerven, M. A. (2015). Deep neural networks reveal a gradient in the complexity of\nneural representations across the ventral stream. Journal of Neuroscience, 35(27), 10005- 10014. doi:10.1523/JNEUROSCI.5023-14.2015\nMOONEY IMAGES IN DEEP NETWORKS 17\nJia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., . . . Darrell, T. (2014).\nCaffe: Convolutional Architecture for Fast Feature Embedding. arXiv preprint arXiv:1408.5093.\nKanwisher, N., Tong, F., & Nakayama, K. (1998). The effect of face inversion on the human\nfusiform face area. Cognition, 68, B1\u2013B11.\nKe, T.-W., Yu, S. X., & Whitney, D. (2017). Mooney face classification and prediction by\nlearning across tone. IEEE International Conference on Image Processing (ICIP).\nKhaligh-Razavi, S.-M., & Kriegeskorte, N. (2014). Deep Supervised, but Not Unsupervised,\nModels May Explain IT Cortical Representation. PLoS Computational Biology, 10(11), e1003915. doi:10.1371/journal.pcbi.1003915\nKheradpisheh, S. R., Ghodrati, M., Ganjtabesh, M., & Masquelier, T. (2016). Deep Networks\nCan Resemble Human Feed-forward Vision inInvariant Object Recognition. Scientific Reports, 6, 32672. doi:10.1038/srep32672\nKim, B., Reif, E., Wattenberg, M., & Bengio, S. (2019). Do Neural Networks Show Gestalt\nPhenomena? An Exploration of the Law of Closure. arXiv, 1903.01069. Retrieved from https://arxiv.org/abs/1903.01069\nKrizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). ImageNet Classification with Deep\nConvolutional Neural Networks. Advances in Neural Information Processing Systems 25 (NIPS 2012) (pp. 1097-1105). Lake Tahoe: Curran Associates, Inc.\nKubilius, J., Bracci, S., & Op de Beeck, H. P. (2016). Deep Neural Networks as a Computational\nModel for Human Shape Sensitivity. PLoS Computational Biology, 12(4), e1004896. doi:10.1371/journal.pcbi.1004896\nMOONEY IMAGES IN DEEP NETWORKS 18\nLatinus, M., & Taylor, M. J. (2005). Holistic Processing of Faces: Learning Effects with Mooney\nFaces. Journal of Cognitive Neuroscience, 17(8), 1316\u20131327.\nLe, Q. V., Ranzato, M., Monga, R., Devin, M., Chen, K., Corrado, G. S., . . . Ng, A. Y. (2012).\nBuilding high-level features using large scale unsupervised learning. ICML. Retrieved from http://arxiv.org/abs/1112.6209\nLindsay, G. (2020, Feb 6). Convolutional Neural Networks as a Model of the Visual System:\nPast, Present, and Future. Journal of Cognitive Neuroscience, 1-15. doi:10.1162/jocn_a_01544\nMooney, C. M. (1957). Age in the development of closure ability in children. Canadian Journal\nof Psychology/Revue canadienne de psychologie, 11(4), 219-226.\nPiepers, D. W., & Robbins, R. A. (2012). A review and clarification of the terms \u201cholistic,\u201d\n\u201cconfigural,\u201d and \u201crelational\u201d in the face perception literature. Frontiers in Psychology. doi:10.3389/fpsyg.2012.00559\nRajalingham, R., Issa, E., Bashivan, P., Kar, K., Schmidt, K., & DiCarlo, J. (2018). Large-scale,\nhigh-resolution comparison of the core visual object recognition behavior of humans, monkeys, and state-of-the-art deep artificial neural networks. The Journal of Neuroscience, 38(33), 7255-7269. doi:10.1523/JNEUROSCI.0388-18.2018\nRussakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., . . . Fei-Fei, L. (2015).\nImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3), 211-252. doi:10.1007/s11263-015-0816-y\nSchwiedrzik, C. M., Melloni, L., & Schurger, A. (2018). Mooney face stimuli for visual\nperception research. PLoS ONE, 13(7), e0200106. doi:10.1371/journal.pone.0200106\nMOONEY IMAGES IN DEEP NETWORKS 19\nTanaka, J., & Farah, M. (1993). Parts and wholes in face recognition. The Quarterly Journal of\nExperimental Psychology, 46(2), 225\u2013245. doi:10.1080/14640749308401045\nTang, H., Schrimpf, M., Lotter, W., Moerman, C., Paredes, A., Caro, J. O., . . . Kreiman, G.\n(2018). Recurrent computations for visual pattern completion. PNAS, 115(35), 8835- 8840. doi:10.1073/pnas.1719397115\nTaubert, J., Apthorp, D., Aagten-Murphy, D., & Alais, D. (2011). The role of holistic processing\nin face perception: evidence from the face inversion effect. Vision Research, 51(11), 1273-8. doi:10.1016/j.visres.2011.04.002\nThorpe, S., Fize, D., & Marlot, C. (1996). Speed of processing in the human visual system.\nNature, 381, 520\u2013522.\nWertheimer, M. (1923). Untersuchungen zur Lehre von der Gestalt, II. Psychologische, 4, 301\u2013\n350.\nXu, T., Zhan, J., Garrod, O. G., Torr, P. H., Zhu, S.-C., Ince, R. A., & Schyns, P. G. (2018).\nDeeper Interpretability of Deep Networks. arXiv:1811.07807.\nYamins, D. L., Hong, H., Cadieu, C. F., Solomon, E. A., Seibert, D., & DiCarlo, J. J. (2014).\nPerformance-optimized hierarchical models predict neural responses in higher visual cortex. PNAS, 111(23), 8619-8624.\nYin, R. K. (1969). Looking at upside-down faces. Journal of Experimental Psychology, 81, 141-\n145.\nYoung, A. W., Hellawell, D., & Hay, D. C. (1987). Configurational information in face\nperception. Perception, 16(6), 747\u2013759. doi:10.1068/p160747\nMOONEY IMAGES IN DEEP NETWORKS 20\nZeman, A. A., Ritchie, J. B., Bracci, S., & Op de Beeck, H. (2020). Orthogonal Representations\nof Object Shape and Category in Deep Convolutional Neural Networks and Human Visual Cortex. Scientific Reports, 10, 2453. doi:10.1101/555193"
        }
    ],
    "title": "Mooney Face Image Processing in Deep Convolutional Neural Networks Compared to Humans",
    "year": 2022
}