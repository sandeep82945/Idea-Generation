{
    "abstractText": "Meta-learning algorithms are able to learn a new task using previously learned knowledge, but they often require a large number of meta-training tasks which may not be readily available. To address this issue, we propose a method for fewshot learning with fewer tasks, which we call MetaModulation. The key idea is to use a neural network to increase the density of the metatraining tasks by modulating batch normalization parameters during meta-training. Additionally, we modify parameters at various network levels, rather than just a single layer, to increase task diversity. To account for the uncertainty caused by the limited training tasks, we propose a variational MetaModulation where the modulation parameters are treated as latent variables. We also introduce learning variational feature hierarchies by the variational MetaModulation, which modulates features at all layers and can consider task uncertainty and generate more diverse tasks. The ablation studies illustrate the advantages of utilizing a learnable task modulation at different levels and demonstrate the benefit of incorporating probabilistic variants in few-task meta-learning. Our MetaModulation and its variational variants consistently outperform state-of-the-art alternatives on four few-task meta-learning benchmarks.",
    "authors": [
        {
            "affiliations": [],
            "name": "Wenfang Sun"
        },
        {
            "affiliations": [],
            "name": "Yingjun Du"
        },
        {
            "affiliations": [],
            "name": "Xiantong Zhen"
        },
        {
            "affiliations": [],
            "name": "Fan Wang"
        },
        {
            "affiliations": [],
            "name": "Ling Wang"
        },
        {
            "affiliations": [],
            "name": "Cees G.M. Snoek"
        }
    ],
    "id": "SP:2a330e109b0eb7dfe170c0eb4d7ec56582615fd8",
    "references": [
        {
            "authors": [
                "M. Andrychowicz",
                "M. Denil",
                "S. Gomez",
                "M.W. Hoffman",
                "D. Pfau",
                "T. Schaul",
                "B. Shillingford",
                "N. De Freitas"
            ],
            "title": "Learning to learn by gradient descent by gradient descent",
            "venue": "NeurIPS,",
            "year": 2016
        },
        {
            "authors": [
                "K. Cao",
                "M. Brbic",
                "J. Leskovec"
            ],
            "title": "Concept learners for few-shot learning",
            "venue": "ICLR, 2020",
            "year": 2020
        },
        {
            "authors": [
                "H. De Vries",
                "F. Strub",
                "J. Mary",
                "H. Larochelle",
                "O. Pietquin",
                "A.C. Courville"
            ],
            "title": "Modulating early visual processing by language",
            "venue": "In NeurIPS, 2017",
            "year": 2017
        },
        {
            "authors": [
                "J. Deng",
                "W. Dong",
                "R. Socher",
                "Li",
                "L.-J",
                "K. Li",
                "L. Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "In CVPR,",
            "year": 2009
        },
        {
            "authors": [
                "V. Dumoulin",
                "J. Shlens",
                "M. Kudlur"
            ],
            "title": "A learned representation for artistic style",
            "venue": "arXiv preprint arXiv:1610.07629,",
            "year": 2016
        },
        {
            "authors": [
                "C. Finn",
                "P. Abbeel",
                "S. Levine"
            ],
            "title": "Model-agnostic metalearning for fast adaptation of deep networks",
            "venue": "In ICLR,",
            "year": 2017
        },
        {
            "authors": [
                "Y. He",
                "W. Liang",
                "D. Zhao",
                "Zhou",
                "H.-Y",
                "W. Ge",
                "Y. Yu",
                "W. Zhang"
            ],
            "title": "Attribute surrogates learning and spectral tokens pooling in transformers for few-shot learning",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "S.X. Hu",
                "D. Li",
                "J. St\u00fchmer",
                "M. Kim",
                "T.M. Hospedales"
            ],
            "title": "Pushing the limits of simple pipelines for few-shot learning: External data and fine-tuning make a difference",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "S. Ioffe",
                "C. Szegedy"
            ],
            "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
            "venue": "In ICML,",
            "year": 2015
        },
        {
            "authors": [
                "M.A. Jamal",
                "Qi",
                "G.-J"
            ],
            "title": "Task agnostic meta-learning for few-shot learning",
            "venue": "In CVPR,",
            "year": 2019
        },
        {
            "authors": [
                "D.P. Kingma",
                "J. Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv: Learning,",
            "year": 2014
        },
        {
            "authors": [
                "D.P. Kingma",
                "M. Welling"
            ],
            "title": "Auto-encoding variational bayes",
            "venue": "arXiv preprint arXiv:1312.6114,",
            "year": 2013
        },
        {
            "authors": [
                "H.B. Lee",
                "T. Nam",
                "E. Yang",
                "S.J. Hwang"
            ],
            "title": "Meta dropout: Learning to perturb latent features for generalization",
            "venue": "In ICLR,",
            "year": 2020
        },
        {
            "authors": [
                "J. Lee",
                "Y. Lee",
                "J. Kim",
                "A. Kosiorek",
                "S. Choi",
                "Y.W. Teh"
            ],
            "title": "Set transformer: A framework for attention-based permutation-invariant neural networks",
            "venue": "In ICML,",
            "year": 2019
        },
        {
            "authors": [
                "S. Lee",
                "B. Andreis",
                "K. Kawaguchi",
                "J. Lee",
                "S.J. Hwang"
            ],
            "title": "Set-based meta-interpolation for few-task metalearning",
            "venue": "NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Y. Liu",
                "W. Zhang",
                "C. Xiang",
                "T. Zheng",
                "D. Cai",
                "X. He"
            ],
            "title": "Learning to affiliate: Mutual centralized learning for few-shot classification",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "M.A.A. Milton"
            ],
            "title": "Automated skin lesion classification using ensemble of deep neural networks in isic 2018: Skin lesion analysis towards melanoma detection challenge",
            "venue": "arXiv preprint arXiv:1901.10802,",
            "year": 1901
        },
        {
            "authors": [
                "S. Murty",
                "T.B. Hashimoto",
                "C.D. Manning"
            ],
            "title": "Dreca: A general task augmentation strategy for few-shot natural language inference",
            "venue": "In ACL,",
            "year": 2021
        },
        {
            "authors": [
                "R. Ni",
                "M. Goldblum",
                "A. Sharaf",
                "K. Kong",
                "T. Goldstein"
            ],
            "title": "Data augmentation for meta-learning",
            "venue": "In ICML,",
            "year": 2021
        },
        {
            "authors": [
                "E. Perez",
                "F. Strub",
                "H. De Vries",
                "V. Dumoulin",
                "A. Courville"
            ],
            "title": "Film: Visual reasoning with a general conditioning layer",
            "venue": "In AAAI,",
            "year": 2018
        },
        {
            "authors": [
                "S. Ravi",
                "H. Larochelle"
            ],
            "title": "Optimization as a model for few-shot learning",
            "venue": "In ICLR,",
            "year": 2017
        },
        {
            "authors": [
                "D.J. Rezende",
                "S. Mohamed",
                "D. Wierstra"
            ],
            "title": "Stochastic backpropagation and approximate inference in deep generative models",
            "venue": "In ICML,",
            "year": 2014
        },
        {
            "authors": [
                "J. Schmidhuber"
            ],
            "title": "Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta-.",
            "venue": "hook. PhD thesis, Technische Universita\u0308t Mu\u0308nchen,",
            "year": 1987
        },
        {
            "authors": [
                "J. Snell",
                "K. Swersky",
                "R. Zemel"
            ],
            "title": "Prototypical networks for few-shot learning",
            "venue": "In NeurIPS,",
            "year": 2017
        },
        {
            "authors": [
                "S. Thrun",
                "L. Pratt"
            ],
            "title": "Learning to learn",
            "venue": "Springer Science & Business Media,",
            "year": 1998
        },
        {
            "authors": [
                "V. Verma",
                "A. Lamb",
                "C. Beckham",
                "A. Najafi",
                "I. Mitliagkas",
                "D. Lopez-Paz",
                "Y. Bengio"
            ],
            "title": "Manifold mixup: Better representations by interpolating hidden states",
            "venue": "In ICML,",
            "year": 2019
        },
        {
            "authors": [
                "O. Vinyals",
                "C. Blundell",
                "T. Lillicrap",
                "K. Kavukcuoglu",
                "D. Wierstra"
            ],
            "title": "Matching networks for one shot learning",
            "venue": "In NeurIPS,",
            "year": 2016
        },
        {
            "authors": [
                "T. Vu",
                "Luong",
                "M.-T",
                "Q.V. Le",
                "G. Simon",
                "M. Iyyer"
            ],
            "title": "Strata: Self-training with task augmentation for better few-shot learning",
            "venue": "arXiv preprint arXiv:2109.06270,",
            "year": 2021
        },
        {
            "authors": [
                "H. Wang",
                "Deng",
                "Z.-H"
            ],
            "title": "Cross-domain few-shot classification via adversarial task augmentation",
            "venue": "arXiv preprint arXiv:2104.14385,",
            "year": 2021
        },
        {
            "authors": [
                "H. Wang",
                "H. Mai",
                "Y. Gong",
                "Deng",
                "Z.-H"
            ],
            "title": "Towards well-generalizing meta-learning via adversarial task augmentation",
            "venue": "Artificial Intelligence,",
            "year": 2023
        },
        {
            "authors": [
                "X. Wang",
                "Y. Peng",
                "L. Lu",
                "Z. Lu",
                "M. Bagheri",
                "R.M. Summers"
            ],
            "title": "Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases",
            "venue": "In CVPR,",
            "year": 2017
        },
        {
            "authors": [
                "Y. Wu",
                "Huang",
                "L.-K",
                "Y. Wei"
            ],
            "title": "Adversarial task upsampling for meta-learning",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "H. Yao",
                "Huang",
                "L.-K",
                "L. Zhang",
                "Y. Wei",
                "L. Tian",
                "J. Zou",
                "J Huang"
            ],
            "title": "Improving generalization in metalearning via task augmentation",
            "venue": "In ICML,",
            "year": 2021
        },
        {
            "authors": [
                "H. Yao",
                "L. Zhang",
                "C. Finn"
            ],
            "title": "Meta-learning with fewer tasks through task interpolation",
            "venue": "arXiv preprint arXiv:2106.02695,",
            "year": 2021
        },
        {
            "authors": [
                "J. Zhou",
                "Y. Zheng",
                "J. Tang",
                "J. Li",
                "Z. Yang"
            ],
            "title": "Flipda: Effective and robust data augmentation for few-shot learning",
            "venue": "arXiv preprint arXiv:2108.06332,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Learning to learn or meta-learning (Schmidhuber, 1987; Thrun & Pratt, 1998), offers a powerful tool for few-shot\n*Equal contribution 1 Hefei Institutes of Physical Science, Chinese Academy of Sciences, Hefei 230031, China/P. R. China. 2University of Science and Technology of China, Hefei 230026, China/P. R. China. 3University of Amsterdam, Amsterdam, the Netherlands. 4United Imaging Healthcare, Beijing, China.. Correspondence to: Wenfang Sun <swf@mail.ustc.edu.cn>, Yingjun Du <y.du@uva.nl>.\nProceedings of the 40 th International Conference on Machine Learning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright 2023 by the author(s).\nlearning (Andrychowicz et al., 2016; Ravi & Larochelle, 2017; Finn et al., 2017). The crux for few-shot metalearning is to accrue prior meta-knowledge from a set of meta-training tasks, which enables fast adaptation to a new task with limited data. Despite remarkable achievements of existing meta-learning algorithms for few-shot learning (Finn et al., 2017; Snell et al., 2017; Liu et al., 2022; Hu et al., 2022; He et al., 2022) these works depend on a large number of meta-training tasks during training. However, an extensive collection of meta-training tasks is unlikely to be available for many real-world applications. For example, in medical image diagnosis, a shortage of data samples and tasks arises due to the need for specialist labeling by physicians and patient privacy concerns. Additionally, rare disease types (Wang et al., 2017) present challenges for few-shot learning. In this paper, we focus on few-task metalearning, where the number of available tasks at training time is limited.\nTo tackle the few-task meta-learning problem, a variety of task augmentation (Ni et al., 2021; Yao et al., 2021a) and task interpolation (Lee et al., 2022; Yao et al., 2021b) methods have been proposed. The key idea of task augmentation (Ni et al., 2021; Yao et al., 2021a) is to increase the number of tasks from the support set and query set during meta-training. The weakness of these approaches is that they are only able to capture the global task distribution within the distribution of the provided tasks. Task interpolation (Lee et al., 2022; Yao et al., 2021b) generates a new task by interpolating the support and query sets of different tasks by Mixup (Verma et al., 2019) or a neural set function (Lee et al., 2019). Here, a key question is how to combine tasks and at what feature level. For example, the state-of-the-art MLTI by (Yao et al., 2021b) randomly selects the features of a single layer from two known tasks for a linear mixup but ignores all other feature layers for new task generation. It leads to a sub-optimal interpolated task diversity. To address this limitation, we propose a new task modulation strategy that captures the knowledge from one known task at different levels.\nOne key aspect of task modulation is the ability to leverage the representation of a single task at different levels of abstraction. This allows the model to modulate representa-\nar X\niv :2\n30 5.\n10 30\n9v 1\n[ cs\n.L G\n] 1\n7 M\nay 2\n02 3\ntions of other tasks at varying levels of detail, depending on the specific needs of the new task. Conditional batch normalization (De Vries et al., 2017; Dumoulin et al., 2016; Perez et al., 2018) has been successfully applied to visual question answering and other multi-modal applications. In conditional batch normalization, the normalization parameters (i.e., the scale and shift parameters) are learned from a set of additional input conditions, which can be represented as a set of auxiliary variables or as a separate input branch to the network. This allows the network to adapt to the specific task at hand and improve its performance. Inspired by these general-purpose conditional batch normalization methods, we make in this paper three contributions.\nIn this paper, we propose a method for few-shot learning with fewer tasks called MetaModulation. It contains three key contributions. First, a meta-training task is randomly selected as a base task, and additional task information is introduced as a condition. We predict the scale and shift of the batch normalization for the base task from the conditional task. This allows the model to modulate the statistics of the conditional task on the base task for a more effective task representation. It is also worth noting that our modulation operates on each layer of the neural network, while previous methods (Yao et al., 2021b; Lee et al., 2022) only select a single layer for modulation. Thus, the model can generate more diverse tasks during meta-training, as it utilizes the statistical information of each level of the conditional task. As a second contribution, we introduce variational task modulation, which treats the conditional scale and shifts as latent variables inferred from the conditional task. The optimization is formulated as a variational inference problem, and new evidence lower bound is derived under the meta-learning framework. In doing so, the model obtains probabilistic conditional scale and shift values that are more informative and better represent the distribution of real tasks. As a third contribution, we propose hierarchical variational task modulation, which obtains the probabilistic conditional scale and shifts at each layer of the network. We cast the optimization as a hierarchical variational inference problem in the Bayesian framework; the inference parameters of the conditional scale and shift are jointly optimized in conjunction with the modulated task training.\nTo verify our method, we conduct experiments on four few-task meta-learning benchmarks: miniImagenet-S, ISIC, DermNet-S, and Tabular Murris. We perform a series of ablation studies to investigate the benefits of using a learnable task modulation method at various levels of complexity. Our goal is to illustrate the advantages of increasing task diversity through such a method, as well as demonstrate the benefits of incorporating probabilistic variations in the fewtask meta-learning framework. Our experiments show that MetaModulation consistently outperforms state-of-the-art few-task meta-learning methods on the four benchmarks."
        },
        {
            "heading": "2. Preliminaries",
            "text": "Problem statement. For the traditional few-shot metalearning problem, we deal with tasks Ti, as sampled from a task distribution p(T ). We sample N -way k-shot tasks from the meta-training tasks, where k is the number of labeled examples for each of the N classes. Each t-th task includes a support set St={(xi,yi)}N\u00d7ki=1 and query set Qt={(x\u0303i, y\u0303i)}mi=1 (St,Qt \u2286 X ). Given a learning model f\u03c6, where \u03c6 denotes the model parameters, few-shot learning algorithms attempt to learn \u03c6 to minimize the loss on the query set Qi for each of the sampled tasks using the data-label pairs from the corresponding support set Si. After that, during the testing stage, the trained model f\u03c6 and the support set Sj for new tasks Tj perform inference and evaluate performance on the corresponding query setQj . In this paper, we focus on few-task meta-learning. In this setting, the main challenge is that the number of meta-training tasks Ti is limited, which causes the model to overfit easily.\nPrototype-based meta-learning. We develop our method based on the prototypical network (ProtoNet) by Snell et al. (2017). Specifically, ProtoNet leverages a non-parametric classifier that assigns a query point to the class having the nearest prototype in the learned embedding space. The prototype ck of an object class c is obtained by: ck= 1K \u2211 k f\u03c6(xc,k), where f\u03c6(xc,k) is the feature embedding of the sample xc,k, which is usually obtained by a convolutional neural network. For each query sample xq, the distribution over classes is calculated based on the softmax over distances to the prototypes of all classes in the embedding space:\np(yqn = k|xq) = exp(\u2212d(f\u03c6(xq), ck))\u2211 k\u2032 exp(\u2212d(f\u03c6(xq), ck\u2032)) , (1)\nwhere yq denotes a random one-hot vector, with yqc indicating its n-th element, and d(\u00b7, \u00b7) is some (Euclidean) distance function. Due to its non-parametric nature, the ProtoNet enjoys high flexibility and efficiency, achieving considerable success in few-shot learning.\nConditional batch normalization. The aim of Batch Normalization (Ioffe & Szegedy, 2015) is to accelerate the training of deep networks by reducing internal covariate shifts. For a layer with d-dimensional input x=(x(1)...x(d)) and activation x(k), batch normalization normalizes each scalar feature as follows:\ny(k) = \u03b3(k) x(k) \u2212 E[x(k)]\u221a Var[x(k)] + + \u03b2(k), (2)\nwhere is a constant added to the variance for numerical stability. \u03b3(k) and \u03b2(k) are the scale and shift for batch normalization. Conditional batch normalization (CBN) (De Vries et al., 2017) is a class-conditional variant of conventional\nbatch normalization. The key idea of CBN is to predict the transformation parameters \u03b3 and \u03b2 from a conditional embedding (i.e., a language embedding). CBN enables the language embedding to manipulate the entire vision feature map by scaling them up or down, negating them, or shutting them off completely. Specifically, CBN uses two feedforward multi-layer perceptrons (MLPs) to predict these changes instead of predicting the original transformations, which benefits training stability:\n\u2206\u03b2 = MLP(eq) \u2206\u03b3 = MLP(eq), (3)\nwhere eq is an additional language embedding. So, given a feature map with C channels, these MLPs output a vector of size C. They then add these changes to the \u03b2 and \u03b3 parameters:\n\u03b2\u0302c = \u03b2c + \u2206\u03b2c \u03b3\u0302c = \u03b3c + \u2206\u03b3c. (4)\nFinally, the updated \u03b2\u0302 and \u03b3\u0302 are used as transformation parameters for the batch normalization (eq. ( 2)) of vision features. Rather than using a language embedding for the conditioning, we randomly select one additional task as a condition to predict the scale and shift of the batch normalization for another task.\nMeta-learning task interpolation. Several methods (Yao et al., 2021b; Lee et al., 2022) have been suggested as ways to increase the diversity of the tasks used for meta-training. MLTI (Yao et al., 2021b) generates additional tasks by randomly sampling a pair of tasks and interpolating the corresponding features and labels using Manifold Mixup (Verma et al., 2019). Specifically, given examples from class n in task Ti and class n\u2032 in task Tj , the interpolated features are defined as:\nH\u0302s,ln = \u03bbH s,l i;n + (1\u2212 \u03bb)H s,l j;n\u2032 , (5)\nH\u0302q,ln = \u03bbH q,l i;n + (1\u2212 \u03bb)H q,l j;n\u2032 , (6)\nwhere l indicates the l-th layer (0 \u2264 l \u2264 L), and \u03bb \u2208 [0, 1] is sampled from a Beta distribution Beta(\u03b1, \u03b2). The interpolated support samples H\u0302s,lcn;n and query samples H\u0302 q,l cn;n can be regarded as the new classes in the interpolated task. However, MLTI (Yao et al., 2021b) randomly selects only the features of a single layer from two known tasks to be mixed and ignores all the other feature layers. It leads to the interpolated task\u2019s diversity being limited and therefore does not increase the generalizability of the model."
        },
        {
            "heading": "3. MetaModulation",
            "text": "In this paper, we propose MetaModulation for few-task meta-learning. We first introduce meta task modulation in section 3.1. To obtain more diverse meta-training tasks,\nwe then propose variational task modulation in section 3.2, which introduces variational inference into the modulation. We also introduce hierarchical meta variational modulation in section 3.3, which adds variational modulation to each network layer to obtain a richer task distribution."
        },
        {
            "heading": "3.1. Meta task modulation",
            "text": "To address the single layer limitation in MLTI (Yao et al., 2021b), we introduce meta task modulation for few-task meta-learning, which modulates the features of two different tasks at different layers. We modulate all layers of samples from a meta-training task Tj by predicting the \u03b3 and \u03b2 of the batch normalization from base task Ti. Following CBN (De Vries et al., 2017), we only predict the change \u2206\u03b2c and \u2206\u03b3c on the original scalars from the task Ti, which benefits training stability.\nSpecifically, to infer the conditional scale and shift \u2206\u03b2c and \u2206\u03b3c, we deploy two functions f l\u03b2(\u00b7) and f l\u03b3(\u00b7) that take the activations Hli;n from task Ti as input, and the output are \u2206\u03b2li;n;c and \u2206\u03b3 l i;n;c. The functions f l \u03b2(\u00b7) and f l\u03b3(\u00b7) are parameterized by two feed-forward multi-layer perceptrons:\n\u2206\u03b2s,li;n;c = MLP(H s,l i;n) \u2206\u03b3 s,l i;n;c = MLP(H s,l i;n) (7)\nwhere \u2206\u03b3s,li;n;c and \u2206\u03b3 s,l i;n;c are the changes of the support set. We obtain \u2206\u03b3q,li;n;c and \u2206\u03b3 q,l i;n;c of the query set by the same strategy. Note that the functions f l\u03b2(\u00b7) and f l\u03b3(\u00b7) are shared by different channels in same layer and we learn L pairs of those functions if we have L convolutional layers.\nUsing the above functions, we generate the changes for the batch normalization scale and shift, then following eq. (4), we add these changes to the original \u03b2lj;n;c and \u03b3 l j;n;c from task Tj :\n\u03b2\u0302s,lj;n;c = \u03b2 s,l j;n;c+\u2206\u03b2 s,l i;n;c \u03b3\u0302 s,l j;n;c = \u03b3 s,l j;n;c+\u2206\u03b3 s,l i;n;c (8)\nOnce we obtain the modulated scale \u03b3\u0302li;n;c and shift \u03b2\u0302 l i;n;c, we compute the modulated features for the support and query set from task Tj based on eq. (2):\nH\u0302s,ln = \u03b3\u0302 s,l i;n;c\nHs,lj;n \u2212 E[H s,l j;n]\u221a\nVar[Hs,lj;n] + + \u03b2\u0302s,lj;n;c, (9)\nH\u0302q,ln = \u03b3\u0302 q,l i;n;c\nHq,lj;n \u2212 E[H q,l j;n]\u221a\nVar[Hq,lj;n] + + \u03b2\u0302q,lj;n;c, (10)\nwhere E[Hli;n] and Var[Hli;n] are the mean and variance of samples features from Tj . We illustrate the meta task modulation process in Figure 1.\nHowever, the deterministic conditional scale and shift are not sufficiently representative of modulated tasks. Moreover, uncertainty is inevitable due to the scarcity of data and tasks, which should also be encoded into the conditional scale and shift. In the next section, we derive a probabilistic latent variable model by modeling conditional scale and shift as distributions, which we learn by variational inference."
        },
        {
            "heading": "3.2. Variational task modulation",
            "text": "In this section, we introduce variational task modulation using a latent variable model in which we treat the conditional scale \u2206\u03b2s,li;n;c and shift \u2206\u03b3 s,l i;n;c as latent variables z inferred from one known task. We formulate the optimization of variational task modulation as a variational inference problem by deriving a new evidence lower bound (ELBO) under the meta-learning framework.\nFrom a probabilistic perspective, the conditional latent scale and shift maximize the conditional predictive log-likelihood from two known tasks Ti, Tj .\nmax p\nlog p(y\u0302|Ti, Tj)\n= max p log\n\u222b p(y\u0302|x\u0302q, x\u0302s)p(x\u0302q, x\u0302s|Ti, Tj)dx\u0302qdx\u0302s\n= max p log\n\u222b p(y\u0302|x\u0302q, x\u0302s)p(x\u0302q, x\u0302s|z, Tj)p(z|Ti)dzdx\u0302qdx\u0302s\n(11) where x\u0302s, x\u0302q are the support sample and query sample of the modulated task T . Since p(z, x\u0302q, x\u0302s|Ti, Tj)=p(x\u0302q, x\u0302s|z, Tj)p(z|Ti) is generally intractable, we resort to a variational posterior q(z, x\u0302q, x\u0302s|Tj) for its approximation. We obtain the variational distribution by minimizing the Kullback-Leibler (KL) divergence:\nDKL[q(z, x\u0302 q, x\u0302s|Tj)||p(z, x\u0302q, x\u0302s|Ti, Tj). (12)\nBy applying the Baye\u2019s rule to the posterior q(z, x\u0302q, x\u0302s|Ti) , we derive the ELBO as: log p(y\u0302|Ti, Tj) \u2265Eq(z,x\u0302q,x\u0302s) [log p(y\u0302|x\u0302q, x\u0302s)]\n\u2212DKL [q(z, x\u0302q, x\u0302s|Tj)||p(z, x\u0302q, x\u0302s|Ti, Tj)] (13)\nThe second term in the ELBO can also be simplified. Since\nDKL [q(z, x\u0302 q, x\u0302s)|Ti||p(z, z\u0302|Ti, Tj)]\n= Eq(z,x\u0302q,x\u0302s) log q(z, \u02c6x|Ti)\np(z, \u02c6x|Ti, Tj) ,\n(14)\nand q(z, x\u0302q, x\u0302s|Tj) = p(x\u0302q, x\u0302s|z, Tj)q(z), (15)\nwe then combine eq. (14), eq. (15) and eq. (11), to obtain:\nEq(z,x\u0302q,x\u0302s) log q(z, x\u0302q, x\u0302s|Tj)\np(z, x\u0302q, x\u0302s|Ti, Tj)\n= Eq(z,x\u0302q,x\u0302s) log p(x\u0302q, x\u0302s|z, Ti)q(z) p(x\u0302q, x\u0302s|z, Ti)p(z|Ti)\n= Eq(z) log q(z) p(z|Ti) = DKL [q(z)||p(z|Ti)] .\n(16)\nThis provides the final ELBO for the variational task modulation:\nq(z, x\u0302q, x\u0302s|Ti) \u2265 Eq(z,x\u0302q,x\u0302s) [log p(y\u0302|x\u0302q, x\u0302s)] \u2212DKL [q(z)||p(z|Ti)]\n(17)\nThe overall computation graph of variational task modulation is shown in Figure 2.\nDirectly optimizing the above objective does not take into account the task information of all model layers, since it only focuses on the conditional latent scale and shift at a specific layer. Thus, we introduce hierarchical variational inference into the variational task modulation by conditioning the posterior on both the known tasks and the conditional latent scale and shift from the previous layers."
        },
        {
            "heading": "3.3. Hierarchical variational task modulation",
            "text": "We replace variational distribution in eq. (12) with a new conditional distribution q(zl, x\u0302q, x\u0302s|zl\u22121, Tj) that makes latent scale and shift of current l-th layer also dependent on the latent scale and shift from the upper l\u22121-th layers.\nThe hierarchical variational inference gives rise to a new ELBO, as follows:\nq(z, x\u0302q, x\u0302s|Ti) \u2265 Eq(zl,x\u0302q,x\u0302s|zl\u22121) [log p(y\u0302|x\u0302q, x\u0302s)] \u2212DKL [ q(zl|zl\u22121)||p(zl|zl\u22121, Ti) ] (18) The graphical model of hierarchical variational task modulation is shown in Figure 3.\nIn practice, the prior p(zl|zl\u22121, Ti) is implemented by an amortization network (Kingma & Welling, 2013) that takes the concatenation of the average feature representations of samples in the support set from Ti and the upper layer latent scale and shift zl\u22121 and returns the mean and variance of the current layer latent scale and shift zl. To enable back-propagation with the sampling operation during training, we adopt the reparametrization trick (Rezende et al., 2014; Kingma & Welling, 2013) as z=z\u00b5 + z\u03c3 , where \u223c N (0, I). The hierarchical probabilistic scale and shift provide a more informative task representation than the deterministic meta task modulation and have the ability to capture different representation levels, thus modulating more diverse tasks for few-task meta-learning.\nIn the meta-training stage, we use the known meta-training tasks Ti with our meta task modulation and its variational variants to generate the new tasks T\u0302 for the meta-training. To ensure that the original tasks are also trained together, we train the generated tasks together with the original tasks. Thus the loss function of our meta task modulation LMTM is as follows:\nLMTM = 1\nT T\u2211 i ( \u2211 (S\u0302i,Q\u0302i)\u223cT\u0302i LCE + \u03bb \u2211 (Si,Qi)\u223cTi LCE ) . (19)\nThe loss of variational task modulation LVTM is\nLVTM = 1\nT T\u2211 i,j ( \u2211 (x\u0302q,y\u0302)\u2208Q\u0302 \u2212Eq(z,x\u0302q,x\u0302s) [log p(y\u0302|x\u0302q, x\u0302s)] + \u03b2DKL [q(z)||p(z|Ti)] ) + \u03bb 1\nT T\u2211 i \u2211 (Si,Qi)\u223cTi LCE.\n(20) And the loss of hierarchical variational task modulation can\nbe written as\nLHVTM = 1\nT T\u2211 i,j ( \u2211 (x\u0302q,y\u0302)\u2208Q\u0302 \u2212Eq(zl,x\u0302q,x\u0302s|zl\u22121) [log p(y\u0302|x\u0302 q, x\u0302s)]\n\u2212 \u03b2DKL [ q(zl|zl\u22121)||p(zl|zl\u22121, Ti) ] ) + \u03bb 1\nT T\u2211 i \u2211 (Si,Qi)\u223cTi LCE,\n(21) where LCE is the cross-entropy loss,\nLCE = 1\nNCNQ\n[ d(f\u03c6(x q), ck) + log \u2211 k\u2032 exp(\u2212d(f\u03c6(xq), ck)) ] ,\n(22) NC andNQ are the number of prototypes and query samples in each task, and \u03bb > 0 and \u03b2 > 0 are the regularization hyper-parameters.\nIn the meta-test stage, we directly input the support set S using the meta-trained feature extractor f\u03c6(\u00b7) to obtain the prototype ck from the test task. Then we obtain the prediction of the query set xq for performance evaluation based on eq. (1)."
        },
        {
            "heading": "4. Experiments",
            "text": ""
        },
        {
            "heading": "4.1. Experimental setup",
            "text": "Datasets. We conduct experiments on four few-task metalearning challenges, i.e., miniImagenet, ISIC, DermNet and Tabular Murris (Cao et al., 2020). miniImagenet (Vinyals et al., 2016) is constructed from ImageNet (Deng et al., 2009) and comprises a total of 100 different classes (each with 600 instances). All images are downsampled to 84 \u00d7 84. We follow (Yao et al., 2021b) and reduce the number of tasks by limiting the number of meta-training classes to obtain miniImagenet-S, with 12 meta-training classes and 20 meta-test classes. ISIC (Milton, 2019) aims to classify dermoscopic images among nine different diagnostic categories. 10,015 images are available for training across 8 different categories. We select 4 categories as the meta-training classes. DermNet is one of the largest open resources of images of skin diseases, with more than 23,000 images. Following (Yao et al., 2021b), we construct Dermnet-S, which selects 30 diseases as the meta-training classes. Tabular Murris considers cell type classification across organs and contains nearly 100,000 cells from 20 organs and tissues. Following (Yao et al., 2021b), we choose 57 base classes as the meta-training classes. For our ablation studies we report on miniImagenet-S, ISIC and Dermnet-S, for our comparison with the state-of-the-art, we also consider Tabular Murris. Sample images from all datasets are provided in the appendix.\nImplementation details. For miniImagenet-S, ISIC, DermNet-S and Tabular Murris, we follow (Yao et al.,\nNetwork layer\n2021b) using a network containing four convolutional blocks and a classifier layer. Each block comprises a 32- filter 3\u00d7 3 convolution, a batch normalization layer, a ReLU nonlinearity, and a 2 \u00d7 2 max pooling layer. We train a ProtoNet (Snell et al., 2017) using Euclidean distance in the 1-shot and 5-shot scenarios with training episodes. Each image is re-scaled to the size of 84 \u00d7 84 \u00d7 3. For all experiments, we use an initial learning rate of 10\u22123 and an SGD optimizer with Adam (Kingma & Ba, 2014). The variational neural network is parameterized by three feed-forward multiple-layer perception networks and a ReLU activation layer. The number of Monte Carlo samples is 20. The batch and query sizes of all datasets are set as 4 and 15. The total training iterations are 50,000. The average few-task meta-learning classification accuracy (%, top-1) is reported across all test images and tasks. Code available at: https: //github.com/lmsdss/MetaModulation."
        },
        {
            "heading": "4.2. Results",
            "text": "Benefit of meta task modulation. To show the benefit of meta task modulation, we first compare our method with a vanilla Prototypical network (Snell et al., 2017) on all tasks, without using task interpolation, in Table 1. Our model performs better under various shot configurations on all few-task meta-learning benchmarks. We then compare our model with the state-of-the-art MLTI (Yao et al., 2021b) in Table 5, which interpolates the task distribution by Mixup (Verma et al., 2019). Our meta task modulation also compares favorably to MLTI under various shot configurations. On ISIC, for example, we surpass MLTI by 2.71% on the 5-way 5-shot setting. This is because our model can learn how to modulate the base task features to better capture the task distribution instead of using linear interpolation as described in the (Yao et al., 2021b).\nBenefit of variational task modulation. We investigate the benefit of variational task modulation by comparing it\nwith deterministic meta task modulation. The results are reported on miniImageNet-S under various shots in Table 2. {1st, 2nd, 3rd, 4th}, random and, all are the selected determined layer, the randomly chosen one layer and all the layers to be modulated, respectively. The variational task modulation consistently outperforms the deterministic meta task modulation on any selected layers, demonstrating the benefit of probabilistic modeling. By using probabilistic task modulation, the base task can be modulated in a more informative way, allowing it to encompass a larger range of task distributions and ultimately improve performance on the meta-test task.\nHierarchical vs. flat variational task modulation. We compare hierarchical modulation with flat variational modulation, which only selects one layer to modulate. As shown in Table 3, the hierarchical variational modulation improves the overall performance under both the 1-shot and 5-shot settings on all three benchmarks. The hierarchical structure is well-suited for increasing the density of the task distribution across different levels of features, which leads to better performance compared to flat variational modulation. This makes sense because the hierarchical structure allows for more informative transformations of the base task, enabling it to encompass a broader range of task distributions. Note that, we use hierarchical variational task modulation to compare the state-of-the-art methods in the subsequent experiments.\nInfluence of the number of meta-training tasks. In Figure 4, we analyze the effect of the number of available meta-\ntraining tasks on the performance of our model under a 5- shot setting on miniImageNet-S. Naturally, our model\u2019s performance improves, as the number of meta-training classes increases. The number of meta-training tasks is important for making the model more generalizable through metalearning. More interesting, our model\u2019s performance is considerably improved by using a learnable modulation that incorporates information from different levels of the task. Compared to the best result of a vanilla prototype network, 63.7% for 64 meta-training classes, we can reduce the number of classes to 40 for the same accuracy.\nCross-domain adaptation ability. To further evaluate the effectiveness of our proposed method, we conducted additional tests to assess the performance of MetaModulation in cross-domain adaptation scenarios. We trained MetaModulation on one source domain and then evaluated it on a different target domain. Specifically, we chose the miniImagenet-S and Dermnet-S domains. The results, as shown in Table 4, indicate MetaModulation generalizes better even in this more challenging scenario.\nAnalysis of modulated tasks. To understand how our MetaModulation is able to improve performance, we plotted the similarity between the vanilla, interpolated and modulated tasks and the meta-test tasks in Figure 5. Red numbers indicate the accuracy per model on each task. Specifically, we select 4 meta-test tasks and 300 meta-train tasks per model from the 1-shot miniImagenet-S setting to compute the task representation of each model. We then used instance pooling to obtain the representation of each task. Instance pooling involves combining a task\u2019s support and query sets and averaging the feature vectors of all instances to obtain a fixed-size prototype representation. This approach allows us to represent each task by a single vector that captures the essence of the task. We calculated the similarity between meta-train and meta-test tasks using Euclidean distance. When using the vanilla prototype model (Snell et al., 2017) directly, the similarity between meta-train and meta-test tasks is extremely low, indicating a significant difference in task distribution between meta-train and meta-test. This results in poor performance as seen in Figure 5 red numbers due to the distribution shift. However, the tasks modulated by our MetaModulation have a higher similarity with the meta-test tasks compared to the vanilla (Snell et al., 2017) and MLTI (Yao et al., 2021b), resulting in high accuracy.\nBut, the similarity between the modulated tasks by our MetaModulation and T4 is also relatively low and performance is also poor. This may be because the task distribution of T4 is an outlier in the entire task distribution, making it hard to mimic this task during meta-training. Future work could investigate ways to mimic these outlier tasks in the meta-training tasks.\nComparison with state-of-the-art. We evaluate MetaModulation on the four different datasets under 5-way 1-shot and 5-way 5-shot in Table 5. Our model achieves stateof-the-art performance on all four few-task meta-learning benchmarks under each setting. On miniImagenet-S, our model achieves 43.21% under 1-shot, surpassing the secondbest MLTI (Yao et al., 2021b), by a margin of 1.85%. On ISIC (Milton, 2019), our method delivers 76.40% for 5- shot, outperforming MLTI (Yao et al., 2021b) with 4.88%. Even on the most challenging DermNet-S, which forms the largest dermatology dataset, our model delivers 50.45% on the 5-way 1-shot setting. The consistent improvements on all benchmarks under various configurations confirm that our approach is effective for few-task meta-learning."
        },
        {
            "heading": "5. Related work",
            "text": "Few-task meta-learning. In few-task meta-learning, the goal is to develop meta-learning algorithms that learn quickly and efficiently from a small number of examples with limited tasks in order to adapt to new tasks with minimal additional training. A common strategy for few-task meta-learning is task augmentation (Yao et al., 2021a; Vu et al., 2021; Murty et al., 2021; Zhou et al., 2021; Wang & Deng, 2021; Wu et al., 2022; Wang et al., 2023), which adds additional tasks to the training data. One such approach is to generate additional tasks by perturbing the original tasks in some way (Yao et al., 2021a; Murty et al., 2021; Zhou et al., 2021; Wu et al., 2022; Wang et al., 2023). For example, MetaMix (Yao et al., 2021a) mixes support and query sets with Manifold Mixup (Verma et al., 2019) to construct a\nnew query set. Another approach is to rely on unsupervised or self-supervised learning to generate additional tasks from the training data (Vu et al., 2021; Wang & Deng, 2021). An alternative few-task meta-learning strategy is task interpolation (Yao et al., 2021b; Lee et al., 2022), which trains a model to learn from a set of interpolated tasks. For example, MLTI (Yao et al., 2021b) performs Manifold Mixup on support and query sets from two tasks for task augmentation. Set-based meta-interpolation (Lee et al., 2022) leverages expressive neural set functions (Lee et al., 2019) to interpolate a given set of tasks and trains the interpolating function using bilevel optimization so that the meta-learner trained with the augmented tasks generalizes to meta-validation tasks. Both task augmentation and interpolation methods often randomly mix the features of two known tasks in a linear way without considering the features of other layers. This limits the diversity of the interpolated task and its potential benefit for increasing model generalizability. In contrast, we propose a learnable task modulation method that enables the model to learn a more diverse set of tasks by considering the features of each layer and allowing for a non-linear modulation between tasks.\nConditional batch normalization. Batch normalization (Ioffe & Szegedy, 2015) is a crucial milestone in the development of deep neural networks. Conditional batch normalization (CBN) (De Vries et al., 2017) allows a neural network to learn different normalization parameters per class of input data. Note the contrast to traditional batch normalization, which uses the same normalization parameters for all inputs to a network layer. By conditioning the normalization on additional information, such as the class labels of the training examples, CBN allows the network to adapt its normalization parameters to the specific class characteristics. Similarly, Perez et al. (Perez et al., 2018) propose the feature-wise linear modulation layer for deep neural networks. In this paper, we take inspiration from conditional batch normalization and propose meta task modulation for few-task meta-learning, where the condition stems from the samples of a meta-training task. We use the conditional\ntask as the condition, instead of data from another modality as in (De Vries et al., 2017), to predict the scale and shift parameters of the batch normalization for the base task."
        },
        {
            "heading": "6. Conclusion",
            "text": "In this paper, we addressed the issue of meta-learning algorithms requiring a large number of meta-training tasks which may not be readily available in real-world situations. We propose MetaModulation, which is to use a neural network to increase the density of the meta-training tasks by modulating batch normalization parameters during metatraining. Our MetaModulation consists of three different implementations. First is the meta task modulation, which modified parameters at various levels of the neural network to increase task diversity. Furthermore, we proposed a variational meta task modulation where the modulation parameters are treated as latent variables. We also introduced learning variational feature hierarchies by the variational meta task modulation. Our ablation studies showed the advantages of utilizing a learnable task modulation at different levels and the benefit of incorporating probabilistic variants in few-task meta-learning. Our MetaModulation and its variational variants consistently outperformed stateof-the-art few-task meta-learning methods on four few-task meta-learning benchmarks."
        },
        {
            "heading": "Acknowledgment",
            "text": "This work is financially supported by the Inception Institute of Artificial Intelligence, the University of Amsterdam and the allowance Top consortia for Knowledge and Innovation (TKIs) from the Netherlands Ministry of Economic Affairs and Climate Policy, the National Key R&D Program of China (2022YFC2302704), the Special Foundation of President of the Hefei Institutes of Physical Science (YZJJ2023QN06), and the Postdoctoral Researchers\u2019 Scientific Research Activities Funding of Anhui Province (2022B653)."
        },
        {
            "heading": "A. Dataset.",
            "text": "We apply our method to four few-task meta-learning image classification benchmarks. Sample images from each dataset are provided in Figure 6."
        },
        {
            "heading": "B. Effect of the \u03b2.",
            "text": "We test the impact of \u03b2 in (20) and (21). The value of \u03b2 control how much information in the base task will be modulated during the meta-training stage. The experimental results on the three datasets under both 1-shot and 5-shot setting are shown in Figure 7 and 8. We can see that the performance achieves the best when the values of \u03b2 are 0.01. This means that in each modulate we need to keep the majority of base task."
        },
        {
            "heading": "C. Effect of the \u03bb.",
            "text": "We would like to emphasize that the hyper-parameters \u03bb (Eq. 19, 20, 21) enable us to introduce constraints on new tasks, beyond just minimizing prediction loss. By adjusting the value of \u03bb, we can control the trade-off between the prediction loss of the new tasks and the constraints imposed by the meta-training tasks. To clarify the impact of \u03bb, we performed an ablation on the HVTM (Eq. 21). The results in Table 6 show that when the original tasks have higher weight, the performance is worse. Additionally, we have conducted experiments to investigate the distribution differences between the meta-training and generated tasks. Specifically, in Table 6, we analyze the task representations of meta-training and generated tasks and show that they are similar, indicating that the generated tasks do indeed have a similar distribution as the meta-training tasks."
        }
    ],
    "title": "MetaModulation: Learning Variational Feature Hierarchies for Few-Shot Learning with Fewer Tasks",
    "year": 2023
}