{
    "abstractText": "With the demand for autonomous control and personalized speech generation, the style control and transfer in Text-toSpeech (TTS) is becoming more and more important. In this paper, we propose a new TTS system that can perform style transfer with interpretability and high fidelity. Firstly, we design a TTS system that combines variational autoencoder (VAE) and diffusion refiner to get refined mel-spectrograms. Specifically, a two-stage and a one-stage system are designed respectively, to improve the audio quality and the performance of style transfer. Secondly, a diffusion bridge of quantized VAE is designed to efficiently learn complex discrete style representations and improve the performance of style transfer. To have a better ability of style transfer, we introduce ControlVAE to improve the reconstruction quality and have good interpretability simultaneously. Experiments on LibriTTS dataset demonstrate that our method is more effective than baseline models .",
    "authors": [
        {
            "affiliations": [],
            "name": "Wenhao Guan"
        },
        {
            "affiliations": [],
            "name": "Tao Li"
        },
        {
            "affiliations": [],
            "name": "Yishuang Li"
        },
        {
            "affiliations": [],
            "name": "Hukai Huang"
        },
        {
            "affiliations": [],
            "name": "Qingyang Hong"
        },
        {
            "affiliations": [],
            "name": "Lin Li"
        }
    ],
    "id": "SP:d9cd80ec0a7006d2d904fc734c656f663ddc45ab",
    "references": [
        {
            "authors": [
                "X. Tan",
                "J. Chen",
                "H. Liu",
                "J. Cong",
                "C. Zhang",
                "Y. Liu",
                "X. Wang",
                "Y. Leng",
                "Y. Yi",
                "L. He"
            ],
            "title": "Naturalspeech: End-to-end text to speech synthesis with human-level quality",
            "venue": "arXiv preprint arXiv:2205.04421, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Y.-J. Zhang",
                "S. Pan",
                "L. He",
                "Z.-H. Ling"
            ],
            "title": "Learning latent representations for style control and transfer in end-to-end speech synthesis",
            "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2019, pp. 6945\u20136949.",
            "year": 2019
        },
        {
            "authors": [
                "X. An",
                "F.K. Soong",
                "L. Xie"
            ],
            "title": "Disentangling style and speaker attributes for tts style transfer",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 30, pp. 646\u2013658, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Ren",
                "Y. Ruan",
                "X. Tan",
                "T. Qin",
                "S. Zhao",
                "Z. Zhao",
                "T.-Y. Liu"
            ],
            "title": "Fastspeech: fast, robust and controllable text to speech",
            "venue": "Proceedings of the 33rd International Conference on Neural Information Processing Systems, 2019, pp. 3171\u20133180.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Ren",
                "C. Hu",
                "X. Tan",
                "T. Qin",
                "S. Zhao",
                "Z. Zhao",
                "T.-Y. Liu"
            ],
            "title": "Fastspeech 2: Fast and high-quality end-to-end text to speech",
            "venue": "9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "J. Kim",
                "S. Kim",
                "J. Kong",
                "S. Yoon"
            ],
            "title": "Glow-tts: A generative flow for text-to-speech via monotonic alignment search",
            "venue": "Advances in Neural Information Processing Systems, vol. 33, pp. 8067\u20138077, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "J. Kim",
                "J. Kong",
                "J. Son"
            ],
            "title": "Conditional variational autoencoder with adversarial learning for end-to-end text-to-speech",
            "venue": "International Conference on Machine Learning. PMLR, 2021, pp. 5530\u20135540.",
            "year": 2021
        },
        {
            "authors": [
                "D.P. Kingma",
                "M. Welling"
            ],
            "title": "Auto-encoding variational bayes",
            "venue": "arXiv preprint arXiv:1312.6114, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "G. Sun",
                "Y. Zhang",
                "R.J. Weiss",
                "Y. Cao",
                "H. Zen",
                "A. Rosenberg",
                "B. Ramabhadran",
                "Y. Wu"
            ],
            "title": "Generating diverse and natural text-to-speech samples using a quantized fine-grained vae and autoregressive prosody prior",
            "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 6699\u20136703.",
            "year": 2020
        },
        {
            "authors": [
                "G. Sun",
                "Y. Zhang",
                "R.J. Weiss",
                "Y. Cao",
                "H. Zen",
                "Y. Wu"
            ],
            "title": "Fullyhierarchical fine-grained prosody modeling for interpretable speech synthesis",
            "venue": "ICASSP 2020-2020 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2020, pp. 6264\u20136268.",
            "year": 2020
        },
        {
            "authors": [
                "X. Li",
                "C. Song",
                "J. Li",
                "Z. Wu",
                "J. Jia",
                "H. Meng"
            ],
            "title": "Towards Multi- Scale Style Control for Expressive Speech Synthesis",
            "venue": "Proc. Interspeech 2021, 2021, pp. 4673\u20134677.",
            "year": 2021
        },
        {
            "authors": [
                "M. Jeong",
                "H. Kim",
                "S.J. Cheon",
                "B.J. Choi",
                "N.S. Kim"
            ],
            "title": "Difftts: A denoising diffusion model for text-to-speech",
            "venue": "proceedings of INTERSPEECH, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "D. Min",
                "D.B. Lee",
                "E. Yang",
                "S.J. Hwang"
            ],
            "title": "Meta-stylespeech: Multi-speaker adaptive text-to-speech generation",
            "venue": "International Conference on Machine Learning. PMLR, 2021, pp. 7748\u20137759.",
            "year": 2021
        },
        {
            "authors": [
                "K. Lee",
                "K. Park",
                "D. Kim"
            ],
            "title": "Styler: Style factor modeling with rapidity and robustness via speech decomposition for expressive and controllable neural text to speech",
            "venue": "INTERSPEECH 2021. International Speech Communication Association, 2021, pp. 3431\u20133435.",
            "year": 2021
        },
        {
            "authors": [
                "R. Huang",
                "Y. Ren",
                "J. Liu",
                "C. Cui",
                "Z. Zhao"
            ],
            "title": "Generspeech: Towards style transfer for generalizable out-of-domain text-tospeech synthesis",
            "venue": "arXiv preprint arXiv:2205.07211, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "D. Yang",
                "S. Liu",
                "J. Yu",
                "H. Wang",
                "C. Weng",
                "Y. Zou"
            ],
            "title": "Norespeech: Knowledge distillation based conditional diffusion model for noise-robust expressive tts",
            "venue": "arXiv preprint arXiv:2211.02448, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Wang",
                "D. Stanton",
                "Y. Zhang",
                "R.-S. Ryan",
                "E. Battenberg",
                "J. Shor",
                "Y. Xiao",
                "Y. Jia",
                "F. Ren",
                "R.A. Saurous"
            ],
            "title": "Style tokens: Unsupervised style modeling, control and transfer in end-to-end speech synthesis",
            "venue": "International Conference on Machine Learning. PMLR, 2018, pp. 5180\u20135189.",
            "year": 2018
        },
        {
            "authors": [
                "C. Qiang",
                "P. Yang",
                "H. Che",
                "X. Wang",
                "Z. Wang"
            ],
            "title": "Style-labelfree: Cross-speaker style transfer by quantized vae and speakerwise normalization in speech synthesis",
            "venue": "2022 13th International Symposium on Chinese Spoken Language Processing (ISC- SLP), 2022, pp. 61\u201365.",
            "year": 2022
        },
        {
            "authors": [
                "J. Ho",
                "A. Jain",
                "P. Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "Advances in Neural Information Processing Systems, vol. 33, pp. 6840\u20136851, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Ren",
                "X. Tan",
                "T. Qin",
                "Z. Zhao",
                "T.-Y. Liu"
            ],
            "title": "Revisiting oversmoothness in text to speech",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2022, pp. 8197\u20138213.",
            "year": 2022
        },
        {
            "authors": [
                "H. Shao",
                "S. Yao",
                "D. Sun",
                "A. Zhang",
                "S. Liu",
                "D. Liu",
                "J. Wang",
                "T. Abdelzaher"
            ],
            "title": "Controlvae: Controllable variational autoencoder",
            "venue": "International Conference on Machine Learning. PMLR, 2020, pp. 8655\u20138664.",
            "year": 2020
        },
        {
            "authors": [
                "J. Sohl-Dickstein",
                "E. Weiss",
                "N. Maheswaranathan",
                "S. Ganguli"
            ],
            "title": "Deep unsupervised learning using nonequilibrium thermodynamics",
            "venue": "International Conference on Machine Learning. PMLR, 2015, pp. 2256\u20132265.",
            "year": 2015
        },
        {
            "authors": [
                "R. Huang",
                "Z. Zhao",
                "H. Liu",
                "J. Liu",
                "C. Cui",
                "Y. Ren"
            ],
            "title": "Prodiff: Progressive fast diffusion model for high-quality text-to-speech",
            "venue": "Proceedings of the 30th ACM International Conference on Multimedia, 2022, pp. 2595\u20132605.",
            "year": 2022
        },
        {
            "authors": [
                "Z. Kong",
                "W. Ping",
                "J. Huang",
                "K. Zhao",
                "B. Catanzaro"
            ],
            "title": "Diffwave: A versatile diffusion model for audio synthesis",
            "venue": "International Conference on Learning Representations, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "D. Snyder",
                "D. Garcia-Romero",
                "G. Sell",
                "D. Povey",
                "S. Khudanpur"
            ],
            "title": "X-vectors: Robust dnn embeddings for speaker recognition",
            "venue": "2018 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2018, pp. 5329\u20135333.",
            "year": 2018
        },
        {
            "authors": [
                "M. Cohen",
                "G. Quispe",
                "S. Le Corff",
                "C. Ollion",
                "E. Moulines"
            ],
            "title": "Diffusion bridges vector quantized variational autoencoders",
            "venue": "International Conference on Machine Learning. PMLR, 2022, pp. 4141\u20134156.",
            "year": 2022
        },
        {
            "authors": [
                "J. Liu",
                "C. Li",
                "Y. Ren",
                "F. Chen",
                "Z. Zhao"
            ],
            "title": "Diffsinger: Singing voice synthesis via shallow diffusion mechanism",
            "venue": "Proceedings of the AAAI conference on artificial intelligence, vol. 36, no. 10, 2022, pp. 11 020\u201311 028.",
            "year": 2022
        },
        {
            "authors": [
                "H. Zen",
                "V. Dang",
                "R. Clark",
                "Y. Zhang",
                "R.J. Weiss",
                "Y. Jia",
                "Z. Chen",
                "Y. Wu"
            ],
            "title": "LibriTTS: A Corpus Derived from LibriSpeech for Text-to-Speech",
            "venue": "Proc. Interspeech 2019, 2019, pp. 1526\u20131530. [Online]. Available: http://dx.doi.org/10. 21437/Interspeech.2019-2441",
            "year": 2019
        },
        {
            "authors": [
                "D.P. Kingma",
                "J. Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "J. Kong",
                "J. Kim",
                "J. Bae"
            ],
            "title": "Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis",
            "venue": "Advances in Neural Information Processing Systems, vol. 33, pp. 17 022\u2013 17 033, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "A. van den Oord",
                "O. Vinyals",
                "K. Kavukcuoglu"
            ],
            "title": "Neural discrete representation learning",
            "venue": "Proceedings of the 31st International Conference on Neural Information Processing Systems, 2017, pp. 6309\u20136318.",
            "year": 2017
        },
        {
            "authors": [
                "M. Heusel",
                "H. Ramsauer",
                "T. Unterthiner",
                "B. Nessler",
                "S. Hochreiter"
            ],
            "title": "Gans trained by a two time-scale update rule converge to a local nash equilibrium",
            "venue": "Proceedings of the 31st International Conference on Neural Information Processing Systems, 2017, pp. 6629\u20136640.",
            "year": 2017
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "With the rapid development of deep learning, the common speech synthesis task that only takes text as input has recently almost reached the human level in the public dataset [1]. More researchers focused on the controllable and expressive TTS [2, 3] because the controllability provides users with more flexibility, and the improvement of expressiveness makes people have better auditory experience.\nAt present, most TTS systems [4, 5, 6, 7] can model controllable style attributes in different ways. FastSpeech [4] which is a non-autoregressive model uses the trained autoregressive model as the teacher model to train the duration predictor, and utilizes the length regulator to control the duration information, thus indirectly controls the overall style. FastSpeech2 [5] employs the external duration aligner Montreal Forced Aligner (MFA) [8] to train the duration predictor on the basis of FastSpeech, furthermore, it trains the pitch predictor, energy predictor and predictors of other styles in a supervised way to achieve more precise style control. Most TTS system pipelines have two modules. The first module is to produce intermediate representations and the second module is to generate raw waveforms conditioned on the intermediate representations. VITS [7] connects the two modules, generated by VAE [9], through latent variables to enable efficient end-to-end training, and it proposes a stochastic duration predictor to synthesize speech with different variations. However, they don\u2019t do style transfer tasks so\n*Corresponding authors 1Audio samples are publicly available at https://gwh22.\ngithub.io/.\nthey do not need reference speech as input. Subsequently, there were many works focused on designing powerful style encoders for style transfer tasks. Some works based on autoregressive models obtain fine-grained style representations by designing multi-level or multi-scale style modeling methods [10, 11, 12]. Due to the problem of low decoding speed in the autoregressive models, the following style modeling tasks mostly adopt a non-autoregressive architecture[4, 5, 13]. Meta-StyleSpeech [14] adopts the base architecture upon FastSpeech2, applying style adaptive layer norm and meta-learning algorithm to effectively synthesize style-transferred speech. Styler [15] models style factors by speech decomposition via information bottleneck. GenerSpeech [16] proposes a multi-level style adaptor and a generalizable content adaptor to efficiently model the style information. Norespeech [17] proposes a knowledge distillation based conditional diffusion model to generate style representation from noisy reference audios. But these methods often cannot perform style interpretability. Some works [2, 18] are proposed to demonstrate that they have the ability of style interpretability and transfer simultaneously. Global style token (GST) [18] designs a style token layer and a reference encoder to explore the expressiveness of TTS systems unsupervisedly. VAE-Tacotron [2] learns the style representation through VAE [9]. A recent work Style-Label-Free [19] extends VAETacotron by proposing Quantized VAE and speaker-wise normalization in cross-speaker style transfer.\nIn this paper, we focus on high-fidelity style transfer and interpretability in speech synthesis which requires a better style representation and a interpretable disentangled style latent space. We utilize VAE based style encoder to have access to the interpretable latent space and integrate VAE within diffusion probabilistic models (DPM) [20] to overcome the oversmoothness problem [21]. We also propose a diffusion bridge of Quantized VAE to improve the diversity of generated style representations. We further introduce ControlVAE [22] to our system instead of original VAE to have better reconstruction quality and good interpretability.\nOur main contributions can be summarized as follows:\n\u2022 We propose a new TTS system that incorporates VAE and DPM to get refined mel-spectrograms. Specifically, a twostage and a one-stage training pipeline are designed respectively to improve the performance of style transfer.\n\u2022 A diffusion bridge of Quantized VAE is proposed to model the diversity of style representations in latent space so that the TTS system achieves better performance of style transfer.\n\u2022 We introduce ControlVAE in our system to replace original VAE to a better reconstruction ability so that improve the style transfer quality and have good style interpretability.\nar X\niv :2\n30 6.\n04 30\n1v 2\n[ cs\n.S D\n] 1\n1 Ju\nl 2 02\n3"
        },
        {
            "heading": "2. Related Work",
            "text": ""
        },
        {
            "heading": "2.1. Diffusion probabilistic models",
            "text": "The concept of diffusion was first defined in [23] and then researchers proposed denoising diffusion probabilistic model (DDPM) [20] which greatly promotes the development of generative models. DDPM has recently succeeded to advance the state-of-the-art results in speech synthesis [13, 24].\nDiffusion process and reverse process are given by diffusion probabilistic models, which could be used for the denoising neural networks \u03b8 to learn data distribution. Similar as previous work [23, 20], we define the data distribution as q(x0). Let x1, \u00b7 \u00b7 \u00b7 , xT be a sequence of variables with the same dimension. The diffusion process is defined by a fixed Markov chain from data x0 to the latent variable xT :\nq(xt|xt\u22121) = N(xt; \u221a 1\u2212 \u03b2txt\u22121, \u03b2tI) (1)\nq(x1, \u00b7 \u00b7 \u00b7 , xT |x0) = T\u220f\nt=1\nq(xt|xt\u22121) (2)\nThe reverse process aims to recover samples from Gaussian noise, which is Markov chain from xT to x0 parameterized by shared \u03b8:\np\u03b8(xt\u22121|xt) = N(xt\u22121;\u00b5\u03b8(xt, t), \u03c32t I) (3)\np\u03b8(x0, \u00b7 \u00b7 \u00b7 , xT\u22121|xT ) = T\u220f\nt=1\np\u03b8(xt\u22121|xt) (4)\nwhere \u03b1t = 1 \u2212 \u03b2t, \u03b1\u0304t = \u220ft t=1 \u03b1t, \u00b5\u03b8 and \u03c3 2 t represent the mean and standard derivation respectively. Finally, we can get the training objective as follows:\nLDDPM = Et,x0,\u03f5[||\u03f5\u2212 \u03f5\u03b8( \u221a \u03b1\u0304tx0 + \u221a 1\u2212 \u03b1\u0304t\u03f5, t)||22] (5)\nwhere \u03f5 is the Gaussian noise and \u03f5\u03b8(\u00b7) is the output of model. For sampling phase, the sampling formulation is computed as follows:\nxt\u22121 = 1\u221a \u03b1t (xt \u2212 \u03b2t\u221a 1\u2212 \u03b1\u0304t \u03f5\u03b8(xt, t)) + \u03c3tz, (6)\nwhere \u03f5 \u223c N(0, I), pz = N(z; 0, I) and \u03c3t = \u221a\n1\u2212\u03b1\u0304t\u22121 1\u2212\u03b1\u0304t \u03b2t.\nAs a result, the final data distribution px0 is obtained through iterative sampling over all of the time steps."
        },
        {
            "heading": "2.2. Variational Autoencoder",
            "text": "VAE has been applied for latent representation learning of natural speech for years [2, 19]. It is assumed that the observed data distribution p(x) is generated by some random process from a random latent variable z. The true posterior distribution p\u03b8(z|x) is intractable because of the undifferentiable marginal likelihood p\u03b8(x). To address this problem, q\u03d5(z|x) is introduced as an approximation to the true posterior distribution p\u03b8(z|x). Finally, we can get the formulation of logp\u03b8(x):\nlogp\u03b8(x) \u2265 Eq\u03d5(z|x)[log p\u03b8(x, z)\nq\u03d5(z|x) ]\n= Eq\u03d5(z|x)[logp\u03b8(x|z)]\u2212DKL(q\u03d5(z|x)||p\u03b8(z)) (7)\nThe encoder of the VAE is chosen to model a multivariate Gaussian with diagonal covariance, and the prior is often assumed to be a standard multivariate Gaussian:\nq\u03d5(z|x) = N(z;\u00b5\u03d5(x), \u03c32\u03d5(x)I) (8)\npz = N(z; 0, I) (9)\nwhere \u00b5(x) and \u03c32(x) in q\u03d5(z|x) are learned via neural network, and the reparameterization trick is introduced to VAE framework to avoid non-derivable problem. Thus, each z is computed as a deterministic function of input x and auxiliary noise variable \u03f5, where \u2299 represents an element-wise product.\nz = \u00b5\u03d5(x) + \u03c3\u03d5(x)\u2299 \u03f5 (10)"
        },
        {
            "heading": "2.3. Quantized VAE",
            "text": "Quantized VAE is proposed in [19] to improve the representation ability of original VAE encoder. Quantized VAE simply extends VAE by adding a discrete codebook component to the VAE output z. z is compared with all the vectors in the codebook, and the closest codebook vector is fed into the VAE decoder. The vector quanatization (VQ) loss which consists of commitment loss and codebook loss is as follows:\nLQ = ||sg[z]\u2212 q||22 + \u03b3||z \u2212 sg[q]||22, (11)\nwhere z is referred as VAE output, q is referred as the codebook vector, \u03b3 represents the weight of commitment loss, sg[\u00b7] refers to the operation of stop gradient."
        },
        {
            "heading": "3. Proposed Method",
            "text": "Figure 1 shows our proposed TTS method named IST-TTS (The abbreviation of Interpretable Style Transfer for Text-toSpeech). Our proposed method consists of three parts: the (1) diffusion refiner, (2) diffusion bridge, and (3) ControlVAE."
        },
        {
            "heading": "3.1. Model architecture",
            "text": "The overall architecture of our method is illustrated in Figure 1 (a). A reference Mel-spectrogram is fed into a reference encoder to extract style information, and the style information is then pass through ControlVAE to obtain the interpretable latent space Z. The quantized embedding is obtained by proposed diffusion bridge, and it is fed into the acoustic model. We use the same architecture for diffusion bridge as [25] to learn diverse style representations.\nThe acoustic model is shown as Figure 1 (b), which is based on the architecture of FastSpeech [4]. And a diffusion refiner [13] is designed for two-stage and one-stage training pipeline to explore the effectiveness of combining VAE and DPM. Additionally, the speaker embedding is extracted by x-vector [26]. Note that we use MFA [8] to replace the previous distillation method to effectively train the duration predictor."
        },
        {
            "heading": "3.2. Diffusion Refiner",
            "text": "We propose to incorporate VAE framework and DPM in twostage and one-stage training pipeline respectively to explore the effectiveness of our method.\n3.2.1. Two-stage and one-stage training pipeline\nAs shown in the left of Figure 1 (b), in two-stage training pipeline, the model firstly generates an intermediate melspectrogram and it can be fed into the vocoder to get the intermediate waveform, the model is named as VAEFS in this paper. And then the intermediate mel-spectrogram which is processed by a linear layer is fed into diffusion refiner as the condition of diffusion model, the model is named as VAEFS+2s in this paper.\nAs shown in the right of Figure 1 (b), in one-stage training pipeline, it will degrade into the method of Diff-TTS [13] in acoustic model, which is named as VAEFS+1s in this paper. Note that the designs in the rest of this paper are based on the acoustic model in one-stage training pipeline.\n3.2.2. Conditional diffusion model\nDiffusion refiner in our method is a conditional diffusion model because the input needs external intermediate mel-spectrogram or decoder input to be as the condition of diffusion model. c represents the condition. The training objective is as follows:\nLR = Et,x0,\u03f5,c[||\u03f5\u2212 \u03f5\u03b8( \u221a \u03b1\u0304tx0 + \u221a 1\u2212 \u03b1\u0304t\u03f5, t, c)||22] (12)"
        },
        {
            "heading": "3.3. Diffusion Bridge",
            "text": "Quantized VAE discretizes the latent features using vector quantization to generate more expressive samples. Inspired by recent work [27], we propose a new Diffusion Bridge to improve the ability of expressiveness in Quantized VAE. Specifically, we consider using diffusion models in a continuous space, which is the z latent space of VAE output in our work, to efficiently learn complex discrete distributions. Note that the sampling process of diffusion bridge is only used in inference stage. The process is shown in Figure 1 (a) and the training loss is:\nLB = Et,z0,\u03f5[||\u03f5\u2212 \u03f5\u03b8( \u221a \u03b1\u0304tz0 + \u221a 1\u2212 \u03b1\u0304t\u03f5, t)||22] (13)"
        },
        {
            "heading": "3.4. ControlVAE",
            "text": "The original VAE models may suffer from KL vanishing and low reconstruction quality problem. To address the issuses, we propose ControlVAE [22] which combines a controller with the basic VAE as an alternative of original VAE. Specifically, a new non-linear proportional-integral (PI) controller is designed to automatically tune the weight added in the VAE objective using the output KL-divergence as feedback during model training. The PI controller algorithm can be illustrated as Figure 1 (c), the weight \u03b2(t) and the loss of ControlVAE are as follows:\n\u03b2(t) = Kp\n1 + exp(e(t)) \u2212Ki t\u2211 j=0 e(j) + \u03b2min (14)\nwhere Kp and Ki represent the coefficient of propositional term and integral term. e(t) is the error between current KL value and expected KL value. \u03b2min is a constant.\nLC = Eq\u03d5(z|x)[logp\u03b8(x|z)]\u2212\u03b2(t)DKL(q\u03d5(z|x)||p(z)) (15)\nThe reconstruction loss is computed with the help of auxiliary Feed-Forward Transformer Decoder like DiffSpeech[28] in one-stage training pipeline.\nFinally, the total training loss of our method is:\nLAll = LC + LR + LQ + LB (16)"
        },
        {
            "heading": "4. Experimental Setup",
            "text": ""
        },
        {
            "heading": "4.1. Dataset",
            "text": "As for dataset, we use LibriTTS [29], which is a multispeaker English corpus containing 586 hours of speech clips with 2456 speakers, to evaluate our method. And we convert the sampling rate to 22050Hz and extract the spectrogram with the FFT size of 1024, hop size of 256, and window size of 1024."
        },
        {
            "heading": "4.2. Training setting",
            "text": "The proposed model was trained for 320K iterations using Adam optimizer [30] on a single NVIDIA TELSA V100 GPU. Additionally, we utilize a pretrained HiFi-GAN [31] as the neural vocoder to convert mel-spectrogram to waveform.\nThe hyperparameters of reference encoder is the same as [2], the dimension of z, the codebook size and \u03b3 of vector quantization are set to 32, 1024 and 0.25, the hyperparameters \u03b2min, Kp, Ki and expected KL value in ControlVAE are set to 0, 0.01, 0.0001 and 3 respectively. Exponential moving averages (EMA) [32] is used instead of codebook loss in Quantized VAE to get faster convergence speed. We use cost annealing method in original VAE. T in diffusion refiner (bridge) is set to 1000."
        },
        {
            "heading": "4.3. Baseline models",
            "text": "We compare the quality and similarity of generated audios by our IST-TTS with public baseline model GenerSpeech [16] and\nTable 2: The subjective results of style transfer. 1s and 2s represent one stage and two stage training with diffusion refiner respectively. B,S,O denote baseline, same and ours respectively. The last three rows show the results of ablation study.\nMethod 5-scale Score Parallel Style Transfer Non-Parallel Style Transfer\nMOS SMOS 7-point score Preference (%) 7-point score Preference (%) B S O B S O\nReference 4.52\u00b10.062 \u2212 \u2212 Reference(Mel+Voc) 4.49\u00b10.076 4.41\u00b10.065 \u2212\nVAEFS 3.31\u00b10.081 2.85\u00b10.083 1.75 12 14 74 1.60 22 28 50 VAEFS+2s 3.75\u00b10.092 3.21\u00b10.076 1.30 15 18 67 1.52 26 33 41 VAEFS+1s 3.79\u00b10.079 3.26\u00b10.105 1.21 20 25 55 1.32 15 38 47 GenerSpeech 3.96\u00b10.086 3.99\u00b10.089 0.21 32 34 34 0.51 32 32 36 IST-TTS (ours) 4.17\u00b10.055 4.02\u00b10.091 \u2212\nw/o ControlVAE 4.09\u00b10.056 3.86\u00b10.083 0.76 25 35 40 0.60 26 40 34 w/o VQ 3.85\u00b10.061 3.81\u00b10.076 0.91 19 37 44 0.69 29 36 35 w/o Diffusion Bridge 4.01\u00b10.081 3.79\u00b10.073 0.71 30 27 43 0.55 34 28 38\n(a) VAEFS (b) VAEFS+2s (c) VAEFS+1s\nour proposed baseline models, VAEFS(+1s/2s)."
        },
        {
            "heading": "5. Experiment Results and Analysis",
            "text": ""
        },
        {
            "heading": "5.1. Style Transfer Quality and Similarity",
            "text": "We evaluate the style transfer quality and similarity by objective metrics and subjective evaluations. The objective metrics include frechet distance (FD) [33] metric and mel cepstral distortion (MCD) metric. The FD in audio indicates the similarity between generated samples and target samples and MCD evaluates the compatibility between the spectra of two audio clips. As for subjective evaluations, we conduct 5-scale Mean Opinion Score (MOS) and Similarity Mean Opinion Score (SMOS) test between IST-TTS and the baselines. To further evaluate the style transfer performance, an AXY test used in [16] is conducted, the range of 7-point score is from -3 to 3, 0 represents \u201cBoth are about the same distance\u201d.\n5.1.1. Parallel Style Transfer\nAs for parallel style transfer, The objective results of FD and MCD shown in Table 1 show that our proposed IST-TTS outperform baseline models, indicating that the proposed model generates higher quality results. The MOS,SMOS and parallel style transfer results in Table 2 show our IST-TTS achieve higher quality and similarity.\nFurthermore, we show the results of comparison between VAEFS, VAEFS+2s and VAEFS+1s in Figure 2, inferring that\nour diffusion refiner can overcome the oversmoothness problem in VAEFS so that obtain better performance of quality.\n5.1.2. Non-parallel Style Transfer\nNon-parallel style transfer represents that the text is changed from the reference utterance. The non-parallel style transfer results are shown in Table 2, which denote that our method has a better performance in non-parallel style transfer than baseline models by using ControlVAE and diffusion bridge to get more expressive results.\n5.1.3. Ablation Study\nAs shown in the last three rows of Table 1 and Table 2, we conduct ablation studies to demonstrate the effectiveness of several designs in IST-TTS, including ControlVAE, vector quantization (VQ) and diffusion bridge. Both the objective metrics and subjective scores drop when removing VQ or diffusion bridge, and replacing ControlVAE with the original VAE results in decreased quality and similarity. These demonstrate the efficiency of the proposed method in modelling expressive style representations."
        },
        {
            "heading": "5.2. Style Interpretability",
            "text": "To evaluate the performance of style interpretability, we experiment on single dimension of ControlVAE latent space z to explore the ability of disentangling as shown in Figure 3 which represents some different speaking styles of energy, pitch variation and pitch level. The limitation in the analysis of style interpretability is that the dataset we chose does not have detailed style labels, so there is no way to conduct quantitative interpretable comparisons."
        },
        {
            "heading": "6. Conclusions",
            "text": "In this paper, we propose a TTS method for interpretable style transfer named IST-TTS, which incorporates VAE with diffusion refiner to improve the audio quality and the performance of style transfer. Furtherly we propose diffusion bridge to perform better style transfer. And finally we introduce ControlVAE to achieve better style transfer quality and good disentanglement. In the future, we will explore better method for interpretability."
        },
        {
            "heading": "7. Acknowledgements",
            "text": "Thanks to the National Natural Science Foundation of China (Grant No.62276220, No.62001405 and No.61876160) for funding."
        },
        {
            "heading": "8. References",
            "text": "[1] X. Tan, J. Chen, H. Liu, J. Cong, C. Zhang, Y. Liu, X. Wang,\nY. Leng, Y. Yi, L. He et al., \u201cNaturalspeech: End-to-end text to speech synthesis with human-level quality,\u201d arXiv preprint arXiv:2205.04421, 2022.\n[2] Y.-J. Zhang, S. Pan, L. He, and Z.-H. Ling, \u201cLearning latent representations for style control and transfer in end-to-end speech synthesis,\u201d in ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2019, pp. 6945\u20136949.\n[3] X. An, F. K. Soong, and L. Xie, \u201cDisentangling style and speaker attributes for tts style transfer,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 30, pp. 646\u2013658, 2022.\n[4] Y. Ren, Y. Ruan, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T.-Y. Liu, \u201cFastspeech: fast, robust and controllable text to speech,\u201d in Proceedings of the 33rd International Conference on Neural Information Processing Systems, 2019, pp. 3171\u20133180.\n[5] Y. Ren, C. Hu, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T.-Y. Liu, \u201cFastspeech 2: Fast and high-quality end-to-end text to speech,\u201d in 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021, 2021.\n[6] J. Kim, S. Kim, J. Kong, and S. Yoon, \u201cGlow-tts: A generative flow for text-to-speech via monotonic alignment search,\u201d Advances in Neural Information Processing Systems, vol. 33, pp. 8067\u20138077, 2020.\n[7] J. Kim, J. Kong, and J. Son, \u201cConditional variational autoencoder with adversarial learning for end-to-end text-to-speech,\u201d in International Conference on Machine Learning. PMLR, 2021, pp. 5530\u20135540.\n[8] M. McAuliffe, M. Socolof, S. Mihuc, M. Wagner, and M. Sonderegger, \u201cMontreal forced aligner: Trainable text-speech alignment using kaldi.\u201d in Interspeech, vol. 2017, 2017, pp. 498\u2013502.\n[9] D. P. Kingma and M. Welling, \u201cAuto-encoding variational bayes,\u201d arXiv preprint arXiv:1312.6114, 2013.\n[10] G. Sun, Y. Zhang, R. J. Weiss, Y. Cao, H. Zen, A. Rosenberg, B. Ramabhadran, and Y. Wu, \u201cGenerating diverse and natural text-to-speech samples using a quantized fine-grained vae and autoregressive prosody prior,\u201d in ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 6699\u20136703.\n[11] G. Sun, Y. Zhang, R. J. Weiss, Y. Cao, H. Zen, and Y. Wu, \u201cFullyhierarchical fine-grained prosody modeling for interpretable speech synthesis,\u201d in ICASSP 2020-2020 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2020, pp. 6264\u20136268.\n[12] X. Li, C. Song, J. Li, Z. Wu, J. Jia, and H. Meng, \u201cTowards MultiScale Style Control for Expressive Speech Synthesis,\u201d in Proc. Interspeech 2021, 2021, pp. 4673\u20134677.\n[13] M. Jeong, H. Kim, S. J. Cheon, B. J. Choi, and N. S. Kim, \u201cDifftts: A denoising diffusion model for text-to-speech,\u201d proceedings of INTERSPEECH, 2021.\n[14] D. Min, D. B. Lee, E. Yang, and S. J. Hwang, \u201cMeta-stylespeech: Multi-speaker adaptive text-to-speech generation,\u201d in International Conference on Machine Learning. PMLR, 2021, pp. 7748\u20137759.\n[15] K. Lee, K. Park, and D. Kim, \u201cStyler: Style factor modeling with rapidity and robustness via speech decomposition for expressive and controllable neural text to speech,\u201d in INTERSPEECH 2021. International Speech Communication Association, 2021, pp. 3431\u20133435.\n[16] R. Huang, Y. Ren, J. Liu, C. Cui, and Z. Zhao, \u201cGenerspeech: Towards style transfer for generalizable out-of-domain text-tospeech synthesis,\u201d arXiv preprint arXiv:2205.07211, 2022.\n[17] D. Yang, S. Liu, J. Yu, H. Wang, C. Weng, and Y. Zou, \u201cNorespeech: Knowledge distillation based conditional diffusion model for noise-robust expressive tts,\u201d arXiv preprint arXiv:2211.02448, 2022.\n[18] Y. Wang, D. Stanton, Y. Zhang, R.-S. Ryan, E. Battenberg, J. Shor, Y. Xiao, Y. Jia, F. Ren, and R. A. Saurous, \u201cStyle tokens: Unsupervised style modeling, control and transfer in end-to-end speech synthesis,\u201d in International Conference on Machine Learning. PMLR, 2018, pp. 5180\u20135189.\n[19] C. Qiang, P. Yang, H. Che, X. Wang, and Z. Wang, \u201cStyle-labelfree: Cross-speaker style transfer by quantized vae and speakerwise normalization in speech synthesis,\u201d in 2022 13th International Symposium on Chinese Spoken Language Processing (ISCSLP), 2022, pp. 61\u201365.\n[20] J. Ho, A. Jain, and P. Abbeel, \u201cDenoising diffusion probabilistic models,\u201d Advances in Neural Information Processing Systems, vol. 33, pp. 6840\u20136851, 2020.\n[21] Y. Ren, X. Tan, T. Qin, Z. Zhao, and T.-Y. Liu, \u201cRevisiting oversmoothness in text to speech,\u201d in Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2022, pp. 8197\u20138213.\n[22] H. Shao, S. Yao, D. Sun, A. Zhang, S. Liu, D. Liu, J. Wang, and T. Abdelzaher, \u201cControlvae: Controllable variational autoencoder,\u201d in International Conference on Machine Learning. PMLR, 2020, pp. 8655\u20138664.\n[23] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli, \u201cDeep unsupervised learning using nonequilibrium thermodynamics,\u201d in International Conference on Machine Learning. PMLR, 2015, pp. 2256\u20132265.\n[24] R. Huang, Z. Zhao, H. Liu, J. Liu, C. Cui, and Y. Ren, \u201cProdiff: Progressive fast diffusion model for high-quality text-to-speech,\u201d in Proceedings of the 30th ACM International Conference on Multimedia, 2022, pp. 2595\u20132605.\n[25] Z. Kong, W. Ping, J. Huang, K. Zhao, and B. Catanzaro, \u201cDiffwave: A versatile diffusion model for audio synthesis,\u201d in International Conference on Learning Representations, 2021.\n[26] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, and S. Khudanpur, \u201cX-vectors: Robust dnn embeddings for speaker recognition,\u201d in 2018 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2018, pp. 5329\u20135333.\n[27] M. Cohen, G. Quispe, S. Le Corff, C. Ollion, and E. Moulines, \u201cDiffusion bridges vector quantized variational autoencoders,\u201d in International Conference on Machine Learning. PMLR, 2022, pp. 4141\u20134156.\n[28] J. Liu, C. Li, Y. Ren, F. Chen, and Z. Zhao, \u201cDiffsinger: Singing voice synthesis via shallow diffusion mechanism,\u201d in Proceedings of the AAAI conference on artificial intelligence, vol. 36, no. 10, 2022, pp. 11 020\u201311 028.\n[29] H. Zen, V. Dang, R. Clark, Y. Zhang, R. J. Weiss, Y. Jia, Z. Chen, and Y. Wu, \u201cLibriTTS: A Corpus Derived from LibriSpeech for Text-to-Speech,\u201d in Proc. Interspeech 2019, 2019, pp. 1526\u20131530. [Online]. Available: http://dx.doi.org/10. 21437/Interspeech.2019-2441\n[30] D. P. Kingma and J. Ba, \u201cAdam: A method for stochastic optimization,\u201d arXiv preprint arXiv:1412.6980, 2014.\n[31] J. Kong, J. Kim, and J. Bae, \u201cHifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis,\u201d Advances in Neural Information Processing Systems, vol. 33, pp. 17 022\u2013 17 033, 2020.\n[32] A. van den Oord, O. Vinyals, and K. Kavukcuoglu, \u201cNeural discrete representation learning,\u201d in Proceedings of the 31st International Conference on Neural Information Processing Systems, 2017, pp. 6309\u20136318.\n[33] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter, \u201cGans trained by a two time-scale update rule converge to a local nash equilibrium,\u201d in Proceedings of the 31st International Conference on Neural Information Processing Systems, 2017, pp. 6629\u20136640."
        }
    ],
    "title": "Interpretable Style Transfer for Text-to-Speech with ControlVAE and Diffusion Bridge",
    "year": 2023
}