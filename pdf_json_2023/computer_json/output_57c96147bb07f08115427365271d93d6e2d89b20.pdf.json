{
    "abstractText": "This paper proposes a fine-grained self-localization method for outdoor robotics that utilizes a flexible number of onboard cameras and readily accessible satellite images. The proposed method addresses limitations in existing cross-view localization methods that struggle to handle noise sources such as moving objects and seasonal variations. It is the first sparse visual-only method that enhances perception in dynamic environments by detecting view-consistent key points and their corresponding deep features from ground and satellite views, while removing off-the-ground objects and establishing homography transformation between the two views. Moreover, the proposed method incorporates a spatial embedding approach that leverages camera intrinsic and extrinsic information to reduce the ambiguity of purely visual matching, leading to improved feature matching and overall pose estimation accuracy. The method exhibits strong generalization and is robust to environmental changes, requiring only geo-poses as ground truth. Extensive experiments on the KITTI and Ford Multi-AV Seasonal datasets demonstrate that our proposed method outperforms existing state-of-the-art methods, achieving median spatial accuracy errors below 0.5 meters along the lateral and longitudinal directions, and a median orientation accuracy error below 2\u25e6 1.",
    "authors": [
        {
            "affiliations": [],
            "name": "Shan Wang"
        },
        {
            "affiliations": [],
            "name": "Yanhao Zhang"
        },
        {
            "affiliations": [],
            "name": "Akhil Perincherry"
        },
        {
            "affiliations": [],
            "name": "Ankit Vora"
        },
        {
            "affiliations": [],
            "name": "Hongdong Li"
        }
    ],
    "id": "SP:7f7ee4a1ec837a5141c65e1fb5f78ca2c7c0247d",
    "references": [
        {
            "authors": [
                "Ioan Andrei Barsan",
                "Shenlong Wang",
                "Andrei Pokrovsky",
                "Raquel Urtasun"
            ],
            "title": "Learning to localize using a lidar intensity map",
            "venue": "arXiv preprint arXiv:2012.10902,",
            "year": 2020
        },
        {
            "authors": [
                "Daniel DeTone",
                "Tomasz Malisiewicz",
                "Andrew Rabinovich"
            ],
            "title": "Superpoint: Self-supervised interest point detection and description",
            "year": 2017
        },
        {
            "authors": [
                "Jakob Engel",
                "Vladlen Koltun",
                "Daniel Cremers"
            ],
            "title": "Direct sparse odometry",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2017
        },
        {
            "authors": [
                "Yanming Feng",
                "Jinling Wang"
            ],
            "title": "Gps rtk performance characteristics and analysis. Positioning",
            "year": 2008
        },
        {
            "authors": [
                "Florian Fervers",
                "Sebastian Bullinger",
                "Christoph Bodensteiner",
                "Michael Arens",
                "Rainer Stiefelhagen"
            ],
            "title": "Continuous self-localization on aerial images using visual and lidar sensors",
            "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),",
            "year": 2022
        },
        {
            "authors": [
                "Andreas Geiger",
                "Philip Lenz",
                "Christoph Stiller",
                "Raquel Urtasun"
            ],
            "title": "Vision meets robotics: The kitti dataset",
            "venue": "The International Journal of Robotics Research,",
            "year": 2013
        },
        {
            "authors": [
                "Frank R Hampel",
                "Elvezio M Ronchetti",
                "Peter J Rousseeuw",
                "Werner A Stahel"
            ],
            "title": "Robust statistics: the approach based on influence functions, volume 196",
            "year": 2011
        },
        {
            "authors": [
                "Sixing Hu",
                "Mengdan Feng",
                "Rang MH Nguyen",
                "Gim Hee Lee"
            ],
            "title": "Cvm-net: Cross-view matching network for imagebased ground-to-aerial geo-localization",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Christian Kerl",
                "J\u00fcrgen Sturm",
                "Daniel Cremers"
            ],
            "title": "Dense visual slam for rgb-d cameras",
            "venue": "In 2013 IEEE/RSJ International Conference on Intelligent Robots and Systems,",
            "year": 2013
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980,",
            "year": 2014
        },
        {
            "authors": [
                "Liu Liu",
                "Hongdong Li"
            ],
            "title": "Lending orientation to neural networks for cross-view geo-localization",
            "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2019
        },
        {
            "authors": [
                "Liu Liu",
                "Hongdong Li",
                "Yuchao Dai"
            ],
            "title": "Efficient global 2d-3d matching for camera localization in a large-scale 3d map",
            "venue": "In Proceedings of the IEEE International Conference on Computer Vision,",
            "year": 2017
        },
        {
            "authors": [
                "Ian D Miller",
                "Anthony Cowley",
                "Ravi Konkimalla",
                "Shreyas S Shivakumar",
                "Ty Nguyen",
                "Trey Smith",
                "Camillo Jose Taylor",
                "Vijay Kumar"
            ],
            "title": "Any way you look at it: Semantic crossview localization and mapping with lidar",
            "venue": "IEEE Robotics and Automation Letters,",
            "year": 2021
        },
        {
            "authors": [
                "Raul Mur-Artal",
                "Juan D Tard\u00f3s"
            ],
            "title": "Orb-slam2: An opensource slam system for monocular",
            "venue": "stereo, and rgb-d cameras. IEEE transactions on robotics,",
            "year": 2017
        },
        {
            "authors": [
                "David Nist\u00e9r",
                "Oleg Naroditsky",
                "James Bergen"
            ],
            "title": "Visual odometry",
            "venue": "In Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition,",
            "year": 2004
        },
        {
            "authors": [
                "Qi Qian",
                "Lei Shang",
                "Baigui Sun",
                "Juhua Hu",
                "Hao Li",
                "Rong Jin"
            ],
            "title": "Softtriple loss: Deep metric learning without triplet sampling",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Tyler G.R. Reid",
                "Sarah E. Houts",
                "Robert Cammarata",
                "Graham Mills",
                "Siddharth Agarwal",
                "Ankit Vora",
                "Gaurav Pandey"
            ],
            "title": "Localization requirements for autonomous vehicles",
            "venue": "SAE International Journal of Connected and Automated Vehicles,",
            "year": 2019
        },
        {
            "authors": [
                "Paul-Edouard Sarlin",
                "Ajaykumar Unagar",
                "M\u00e5ns Larsson",
                "Hugo Germain",
                "Carl Toft",
                "Viktor Larsson",
                "Marc Pollefeys",
                "Vincent Lepetit",
                "Lars Hammarstrand",
                "Fredrik Kahl",
                "Torsten Sattler"
            ],
            "title": "Back to the Feature: Learning robust camera localization from pixels to pose",
            "year": 2021
        },
        {
            "authors": [
                "Yujiao Shi",
                "Hongdong Li"
            ],
            "title": "Beyond cross-view image retrieval: Highly accurate vehicle localization using satellite image",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Yujiao Shi",
                "Xin Yu",
                "Dylan Campbell",
                "Hongdong Li"
            ],
            "title": "Where am i looking at? joint location and orientation estimation by cross-view matching",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Yujiao Shi",
                "Xin Yu",
                "Liu Liu",
                "Tong Zhang",
                "Hongdong Li"
            ],
            "title": "Optimal feature transport for cross-view image geolocalization",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Young-Sik Shin",
                "Yeong Sang Park",
                "Ayoung Kim"
            ],
            "title": "Dvlslam: Sparse depth enhanced direct visual-lidar slam",
            "venue": "Autonomous Robots,",
            "year": 2020
        },
        {
            "authors": [
                "Tim Yuqing Tang",
                "Daniele De Martini",
                "Dan Barnes",
                "Paul Newman"
            ],
            "title": "Rsl-net: Localising in satellite images from a radar on the ground",
            "venue": "IEEE Robotics and Automation Letters,",
            "year": 2020
        },
        {
            "authors": [
                "Tim Y Tang",
                "Daniele De Martini",
                "Paul Newman"
            ],
            "title": "Get to the point: Learning lidar place recognition and metric localisation using overhead imagery",
            "venue": "Proceedings of Robotics: Science and Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Tim Y Tang",
                "Daniele De Martini",
                "Shangzhe Wu",
                "Paul Newman"
            ],
            "title": "Self-supervised learning for using overhead imagery as maps in outdoor range sensor localization",
            "venue": "The International Journal of Robotics Research,",
            "year": 2021
        },
        {
            "authors": [
                "Aysim Toker",
                "Qunjie Zhou",
                "Maxim Maximov",
                "Laura Leal-Taix\u00e9"
            ],
            "title": "Coming down to earth: Satellite-to-street view synthesis for geo-localization",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Frank Van Diggelen",
                "Per Enge"
            ],
            "title": "The world\u2019s first gps mooc and worldwide laboratory using smartphones. In Proceedings of the 28th international technical meeting of the satellite division of the institute of navigation (ION GNSS+",
            "year": 2015
        },
        {
            "authors": [
                "Lukas Von Stumberg",
                "Patrick Wenzel",
                "Nan Yang",
                "Daniel Cremers"
            ],
            "title": "Lm-reloc: Levenberg-marquardt based direct visual relocalization",
            "venue": "In 2020 International Conference on 3D Vision (3DV),",
            "year": 2020
        },
        {
            "authors": [
                "Ankit Vora",
                "Siddharth Agarwal",
                "Gaurav Pandey",
                "James McBride"
            ],
            "title": "Aerial imagery based lidar localization for autonomous vehicles, 2020",
            "year": 2020
        },
        {
            "authors": [
                "Shan Wang",
                "Yanhao Zhang",
                "Ankit Vora",
                "Akhil Perincherry",
                "Hengdong Li"
            ],
            "title": "Satellite image based cross-view localization for autonomous vehicle",
            "venue": "IEEE International Conference on Robotics and Automation (ICRA),",
            "year": 2023
        },
        {
            "authors": [
                "Xinkai Wei",
                "Ioan Andrei B\u00e2rsan",
                "Shenlong Wang",
                "Julieta Martinez",
                "Raquel Urtasun"
            ],
            "title": "Learning to localize through compressed binary maps",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Ryan W. Wolcott",
                "Ryan M. Eustice"
            ],
            "title": "Visual localization within lidar maps for automated urban driving",
            "venue": "In 2014 IEEE/RSJ International Conference on Intelligent Robots and Systems,",
            "year": 2014
        },
        {
            "authors": [
                "Zimin Xia",
                "Olaf Booij",
                "Marco Manfredi",
                "Julian FP Kooij"
            ],
            "title": "Visual cross-view metric localization with dense uncertainty estimates",
            "venue": "In Computer Vision\u2013ECCV 2022: 17th European Conference,",
            "year": 2022
        },
        {
            "authors": [
                "Ji Zhang",
                "Sanjiv Singh"
            ],
            "title": "Loam: Lidar odometry and mapping in real-time",
            "venue": "In Robotics: Science and Systems,",
            "year": 2014
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Accurate self-localization is a fundamental problem in mobile robotics, particularly in the context of autonomous driving. While Global Positioning System (GPS) is a widely adopted solution, its accuracy hardly meets the stringent requirements of autonomous driving [20]. Real-Time Kinematic (RTK) positioning systems provide an alternative by correcting GPS errors, but their implementation is hindered by the need for signal reference stations [13], rendering them an expensive solution. On the other hand, odometry [18, 4, 37, 32] or simultaneous localization and mapping (SLAM) [17, 11, 25, 32] methods can generate\n1Our project page is https://shanwang-shan.github.io/PureACLwebsite/\naccurate short-term trajectories, however, they experience drift accumulation over time that can only be alleviated through loop closures if the agent\u2019s trajectories overlap. Lastly, other self-localization techniques [35, 15, 31, 21] that rely on a pre-constructed 3D High Definition (HD) maps face limitations in terms of the extensive time and resources required for map acquisition and maintenance.\nUsing off-the-shelf satellite images as ready-to-use maps to achieve cross-view localization brings an alternative and promising way for low-cost localization. However, due to the significant disparity between overhead views captured by satellites and views seen by robots, cross-view localization is more challenging than traditional methods. To address this, it is crucial to purify view-consistent features that can support the localization process. Furthermore, satellite views can be captured at different times, leading to variations in seasonal and temporal conditions. The crossview consistent purification can also minimize the impact of moving and seasonal objects.\nMost previous cross-view localization methods [24, 10, 14, 29, 23, 38] approach the task as an image retrieval problem, leading to coarse localization accuracy that is inferior to commercial GPS which can achieve an error of up to 4.9 meters in open sky conditions [30]. In contrast, our method utilizes a coarse pose that is easily obtainable from the Autonomous Vehicles system, to estimate the fine-grained 3- DoF (lateral, longitudinal, yaw) pose of the robot. This is accomplished through visual cross-view matching, utilizing ground-view images captured by onboard cameras and a\nar X\niv :2\n30 8.\n08 11\n0v 1\n[ cs\n.C V\n] 1\n6 A\nug 2\n02 3\nspatially-consistent satellite map. Additionally, our method supports multiple camera inputs, which extend the field of view of the query robot. The setting is illustrated in Fig. 1.\nOur fine-grained visual localization method utilizes sparse (keypoint) feature matching, a departure from prior methods that rely on dense feature matching. To reduce the inherent ambiguity in purely visual matching, the method incorporates a camera intrinsic and extrinsic aware spatial embedding. Homography transformation is used to establish correspondences between the two views. An onground confidence map is employed to ensure the validity of the transformation and eliminate off-the-ground objects. Additionally, a view consistency confidence map is utilized to mitigate the impact of moving objects and viewpoint variation. The localization process begins with the extraction of spatially aware deep features and the generation of view-consistent, on-ground confidence maps for both views. View-consistent key points are then detected from the ground view confidence map and matched with their corresponding points in the satellite view. The optimal pose is determined through an iterative search using a differentiable Levenberg-Marquardt (LM) algorithm.\nUsing Google Maps [8] as the satellite view, we evaluate our method on two datasets: the Ford MultiAV Seasonal (FMAVS) [1] and the KITTI Datasets [7]. The results demonstrate the superiority of our proposed method, achieving mean localization error of less than {0.14m, 3.57\u25e6} on KITTI with one front-facing onboard camera, and less than {0.88m, 0.74\u25e6} on FMAVS with four surrounding onboard cameras.\nWe summarize our contributions as below:\n\u2022 the first sparse visual-only cross-view localization method that estimates accurate pose with low spatial and angular errors.\n\u2022 a view-consistent on-ground key point detector that reduces the impact of dynamic objects and viewpoint variations, as well as removes off-the-ground objects.\n\u2022 a spatial embedding that fully utilizes camera intrinsic and extrinsic information to improve the extraction of spatially aware visual features.\n\u2022 a multi-camera fusion approach that significantly improves localization accuracy."
        },
        {
            "heading": "2. Related work",
            "text": "Depth Aware Accurate Cross-view Localization. The task of accurate cross-view localization has gained attention in recent years. Researchers have mainly focused on developing solutions for Radar and LiDAR cross-view localization as depth information helps in aligning the ground and satellite perspectives. RSL-Net [26] estimates the robot\npose by registering Radar scans on a satellite image. This method was later extended to a self-supervised learning framework in [28]. Another work [27] matches the topdown representation of a LiDAR scan with 2D points detected from satellite images. These methods have limitations and are only effective in environments with strong prior structure knowledge, failing in general, non-urban environments. [2] performs localization on bird\u2019s eye view (BEV) LiDAR intensity maps using deep feature matching between LiDAR scan and the intensity map. [34] extends this method by incorporating compressed binary maps. Hybrid sensor solutions have also been explored, such as in [16] where an aerial robot achieves global localization through the use of egocentric 3D semantically labelled LiDAR, IMU, and visual information. CSLA [6] and SIBCL [33] extract visual features from ground and satellite images and use LiDAR points to establish correspondence between the two views. CSLA [6] aims to estimate 2-DoF translation, while SIBCL [33] aims to estimate 3-DoF pose, including an additional orientation. All these methods critically rely on depth information to build the correspondence across the two views. In contrast, our method is a visualonly solution that aims to achieve comparable localization accuracy using cheaper commodity sensors.\nVisual Accurate Cross-view Localization. Most visualonly cross-view localization methods rely on homography transformations of the ground plane, as they lack reliable depth information. [36] aims to estimate 2-DoF translation using similarity matching and produces a dense spatial distribution to address localization ambiguities. HighlyAccurate [22] projects satellite features into the ground view and optimizes the robot pose through dense feature matching. One of its drawbacks is the limited ability to effectively eliminate outliers, such as noise caused by off-theground objects (which violates the assumption of homography transformation of the ground plane) and dynamic objects. As a result, their overall performance is limited. In contrast, our method constructs geometric correspondences across sparse view-consistent on-ground keypoints, ensuring that the pose estimation is based on accurate correspondences leading to improved precision."
        },
        {
            "heading": "3. Our Method",
            "text": "Our work aims to achieve fine-grained cross-view localization by accurately estimating the 3-DoF pose, denoted by Ppred = {\u03d5pred, \u03c6pred, \u03b8pred}, where \u03d5 and \u03c6 represent lateral and longitudinal translations, respectively, and \u03b8 is the yaw angle. We are given a coarse initial pose Pinit = {\u03d5init, \u03c6init, \u03b8init}, a reference satellite view image Is, and a set of ground-view images Ig = {Ii}Ni=1 captured by onboard cameras, where N is the total num-\nber of onboard cameras 2. An overview of the proposed PureACL is shown in Fig. 2. It builds upon three innovative modules: 1) Spatially Aware Feature and Confidence Extractor (SAFCE) (Sec. 3.2), 2) View-consistent On-ground Keypoint Detector (VOKD) (Sec. 3.3), and 3) Multi-camera Fusion (Sec. 3.4). Additionally, our approach utilizes two branches of objective functions inherited from the SIBCL method [33]: the Pose-Aware Branch (PAB) and the Recursive Pose Refine Branch (RPRB). In the following sections, we provide a detailed explanation of each module."
        },
        {
            "heading": "3.1. Preliminary",
            "text": "For completeness, we provide a brief description of the inherited PAB and RPRB. The PAB utilizes a triplet loss [19] that encourages accurate pose (ground truth) and penalizes incorrect (initial) poses by differentiating the residual between the ground truth and initial pose. Specifically, we compute the loss as follows:\nLtriplet = log(1 + e \u03b1(1\u2212\n\u2211 p w[p]Pinit\n\u03c1(\u2225r[p]Pinit\u2225 2 2)\u2211\np w[p]Pgt \u03c1(\u2225r[p]Pgt\u2225 2 2)\n)\n), (1)\nwhere \u03b1 is a hyper-parameter set to 10 based on experimental results, \u2211 p represents the sum of all key points, and \u03c1 is a robust cost function as defined in [9]. The RPRB, on the other hand, aims to refine the initial pose iteratively using the LM algorithm to approach the ground truth pose. It starts with the coarsest level and\n2Our method supports varying onboard camera quantities. In the experiments, we employed N = 4 for FMAVS and N = 1 for Kitti-CVL.\nuses features from each level successively, with each subsequent level initialized with the output of the previous level. Specifically, we update the pose as follows:\n\u03b4t+1 = \u03b4t \u2212 (H+ \u03bb diag(H))\u22121J\u22a4W\u03a5, (2)\nwhere \u03b4 represents an individual element in the 3-DoF pose. t \u2208 {1, \u00b7 \u00b7 \u00b7 ,M \u00d7 L} represents the current iteration, and M and L represent the iteration count per level and the total number of levels, respectively. The matrices \u03a5 and W are formed by stacking the residuals r[p]P and weights w[p]P, while \u03bb is the damping factors [21]. The Jacobian and Hessian matrices are defined as follows:\nJ = \u2202r[p]P \u2202\u03b4 = \u2202F s[p]\n\u2202[ps2D]P\n\u2202[ps2D]P \u2202\u03b4 and H = J\u22a4WJ, (3)\nwhere [ps2D]P is the 2D projection of keypoints p onto the satellite image using the pose P, as shown in the right section of Fig. 6. Finally, we supervise the optimized pose by computing the re-projection error as follows:\nLreproject(Ppred) = \u2211 \u2225[ps2D]Ppred \u2212 [ps2D]Pgt\u222522. (4)"
        },
        {
            "heading": "3.2. Spatially Aware Feature/Confidence Extractor",
            "text": "Our approach improves the spatial embedding concept proposed in [14] by leveraging the camera\u2019s intrinsic and extrinsic parameters to obtain highly accurate spatial information. The spatial embedding Eg/s \u2208 Rh\u00d7w\u00d73 has 3 channels: heading, distance, and height information. The explanation of these channels is shown in Fig. 3. To incorporate additional spatial embedding information between\nthe ground and satellite images, we transform the pixels in the onboard camera and satellite images into a common set of query world coordinates ( e.g., the GPS coordinates of the robot). In this coordinate system, the x-axis corresponds to the direction of motion, the y-axis points to the right, and the z-axis points downward. To perform this transformation, we use an inverse projection formula, which is shown in Eq. 5:\npj2g3D = Rj2gK \u22121 j (p j 2D \u2295 1), (5)\nwhere Kj is the intrinsic matrix of camera j, which can be either an onboard camera or a satellite camera j \u2208 {iN1 , s}, and \u22951 concatenates 1 to generate the homogeneous coordinate. The rotation from camera j to the ground coordinate, Rj2g , is obtained from the extrinsic information provided in the datasets for onboard cameras and from the initial coarse pose for the satellite camera. For onboard camera images, the 3D coordinate pi2g3D is a homogeneous coordinate with an unknown scale, while for satellite images, ps2g3D represents a world coordinate with an unknown down axis. This is because satellite images are approximated as parallel projections, and the equation for the calculating ps2D is given by:\nps2D = ( 1/\u03b3 0 cu 0 1/\u03b3 cv ) ps3D, (6)\nwhere (cu, cv) represents the center of the satellite image, and \u03b3 represents the meter-per-pixel ratio calculated using:\n\u03b3 = r\u0303earth \u00d7 cos(L\u0303\u00d7 \u03c0180\u25e6 )\n2z\u0303 \u00d7 s\u0303 , (7)\nwhere r\u0303earth = 156543.03392 is radius of the Earth, L\u0303 is the latitude, z\u0303 = 18 and s\u0303 = 2 is the zoom factor and the scale of Google Maps [8], respectively.\nThe heading information is embedded using the cosine value, which is symmetric to both positive and negative orientation noise. This enables distinction between 360- degree views, calculated using the x-axis (pj2g3D [0]) and yaxis (pj2g3D [1]) through trigonometric functions, as shown below:\nEj [0] = pj2g3D [0]/ \u221a pj2g3D [0] 2 + pj2g3D [1] 2. (8)\nThe normalized distance embedding of ground images is obtained by assuming all pixels lie on the ground plane:\nEj [1] = \u221a\npj2g 3\u0303D [0]2 + pj2g 3\u0303D [1]2/D, (9)\nwhere D is the maximum visible distance, set to 200 meters according to the satellite maps size and\npi2g 3\u0303D\n= hi\npi2g3D [2] \u00d7pi2g3D+ ti2g and p s2g 3\u0303D = ps2g3D + ts2g, (10)\nwhere hi is the onboard camera height relative to the ground plane. For ground view images, the height embedding E[2] is equal to the value along the down axis, represented as pi2g3D [2]. In the case of satellite images, we set the height embedding to the minimal value to indicate a top-down perspective. Fig. 4 demonstrates that our approach effectively directs greater attention towards the features located in front of the robot by leveraging spatial embedding when using only the front onboard camera.\nThe SAFCE employs a U-Net structure (F\u03bd) to extract the satellite and ground-view feature maps, represented as F j = F\u03bd(Ij \u2295 Ej), where j \u2208 {iN1 , s}, and \u2295 denotes channel concatenation. The maps are then processed by a convolutional layer followed by a reverse sigmoid active function (C\u03c8) to produce view-consistent confidence maps (V j) and on-ground confidence maps (Oj) represented as V j , Oj = C\u03c8(F j). Each map has multiple resolutions, for example, F = {Fl \u2208 Rhl\u00d7wl\u00d7cl}Ll=1 (Rhl\u00d7wl for V and O), where L = 3 is adopted in our setting. The maps are ordered from coarsest to finest level as l = {1, 2, 3}. The feature and confidence extraction from each image is performed in parallel using a shared-weight model, allowing for a flexible number of onboard cameras (N).\nThe view-consistent confidence map V represents the confidence of objects appearing in both satellite and ground-view images. V is used as a multiplying factor for the point weights supervised by PAB and RPRB, and is penalized through the network training for the points with high residual (indicating distinct features between the cross-view). Considering the temporal gap between the two views, V effectively filters out objects that are temporally\nor seasonally inconsistent, e.g. vehicles, pedestrians, and leaves. Additionally, it highlights view consistent reference objects, including road marks, lanes, building edges, and tree roots. An example is shown in Fig. 4 (row 2). More visualizations are shown in the supplementary.\nThe on-ground confidence map O is designed to validate the homography transformation between the ground and satellite views. As a multiplying factor for the point weights, off-ground points that cause incorrect Geocorrespondence between the ground and satellite views, resulting in high residuals, have their on-ground confidence penalized to reduce the overall loss. Given that an incorrect height assumption in points can lead to erroneous projections on the satellite map, penalizing the satellite on-ground confidence map is not meaningful. So we only apply the backpropagation to the ground-view on-ground confidence map. An example of the learned confidence maps is shown in Fig. 4 (row 3)."
        },
        {
            "heading": "3.3. View-consistent On-ground Keypoint Detector",
            "text": "Fig. 5 illustrates the details of the proposed VOKD. The view-consistent and on-ground confidence maps of different resolutions are fused to generate the final confidence:\nCi = L\u2211 l=1 \u039e(N (V il \u2297Oil), (hL, wL)), (11)\nwhere hL and wL represents the resolution of the fine level confidence map, \u039e is an interpolation function, and N is a min-max normalisation, and \u2297 represents element-wise multiplication. The bottom row of Fig. 4 demonstrates the efficacy of the fused confidence map in filtering out off-theground objects and emphasizing temporal stability and view consistency in cues such as road markings and curbs for the subsequent pose estimation. More visual examples can be found in the supplementary.\nIn order to achieve on-ground keypoint detection, our focus is limited to the area below the focal point, which corresponds to the on-ground area and is our primary interest. From this area, we select the top-K points with the highest confidence score from the fused confidence map. To avoid overcrowding of keypoints, we partition the fused confidence map into smaller patches of size 8\u00d7 8 and enforce a limit of one detected keypoint per patch. This approach ensures that the selected keypoints are well-distributed across the on-ground area, thereby improving the accuracy of subsequent pose estimation. The left part of Fig. 6 displays the detected view-consistent on-ground 2D keypoints. These 2D keypoint coordinates pi2D are used to calculate their corresponding 3D ground world coordinates pi2g\n3\u0303D through the\nequations Eq. (5) and Eq. (10). The right part of Fig. 6 shows the projection of these 3D coordinates onto the satellite image (ps2D = Ks(Rg2sp i2g 3\u0303D + tg2s))."
        },
        {
            "heading": "3.4. Multi-camera Fusion",
            "text": "Our method is flexible and can handle multiple cameras as input, without any restrictions on the field of view. In case there is a potential overlap between the views captured by adjacent cameras, keypoints detected in one camera may be visible in another camera as well. In such cases, we select the point feature with the highest weight:\nwg[p] = N\nmax i\n(V i \u2297Oi)[pi2D], (12)\nF g[p] = F i[pi2D], i = arg N\nmax i\n(V i \u2297Oi)[pi2D]. (13)"
        },
        {
            "heading": "4. Datasets",
            "text": "To evaluate the effectiveness of the proposed method, we followed the existing methods [22, 33] and conducted experiments on two widely used autonomous driving datasets: the FMAVS dataset [1] and KITTI dataset [7]. We adopted the augmentation method proposed by [33], which involved incorporating spatially-consistent satellite images obtained from Google Maps [8] using the GPS tags provided in the datasets. The satellite images had a resolution of 1, 280 \u00d7 1, 280 pixels and a scale of 0.22m per pixel for FordAVCVL, and 0.2m per pixel for KITTI-CVL.\nIn the FMAVS dataset, we utilized query images from four cameras (front left, rear right, side left, and side right) to capture the surrounding environment, providing an almost 360-degree field of view with minimal overlap. Since the KITTI dataset provides only front-facing stereo camera images, we used the images from the left camera of the stereo pair as query images. The FMAVS includes multiple vehicle traversals over a consistent route. To evaluate our proposed method, we split the three traversals of the \u2018Log4\u2019 trajectory into training, validation, and test sets, following the split strategy described in [33]. The KITTI dataset [7] comprises various trajectories taken at different times. To assess our model\u2019s generalization ability, we selected test sets from different trajectories based on [22]."
        },
        {
            "heading": "5. Experiments",
            "text": "Metrics. Our objective is to estimate the 3-DoF pose, which includes lateral, longitudinal, and yaw information. We measure the accuracy of our proposed method by reporting the median and mean errors in lateral and longitudinal translations (in meters) and yaw rotation (in degrees). In addition to these metrics, we also follow the evaluation criteria outlined in [33] and report the average localization recall 3 at distances of 0.25m, 0.5m, 1m, and 2m, as well as at yaw rotation angles of 1\u25e6, 2\u25e6, and 4\u25e6. Implementation Details. In our experiments, we use an input size of 432 \u00d7 816 for the ground-view images in the Multi-AV Seasonal Dataset, and 384\u00d7 1248 for the KITTI Dataset. RTK GPS 4 is used as the ground truth pose. We add some noise to the RTK GPS poses to generate the initial pose. Unless otherwise stated, the initial pose is randomly sampled with a yaw angle error of \u00b115\u25e6 and lateral, longitudinal shifts of \u00b15 meters, as the accuracy of GPS is within 4.9 meters in open sky conditions [30]. We detect 256 ground keypoints from each input ground-view image. We set the batch size to b = 3 for training on an NVIDIA RTX 3090 GPU, and use the Adam optimizer [12] with a learning rate of 10\u22124. The feature extractor weights are initialized with the pre-trained weights from [33], which are trained on the KITTI-CVL dataset. The weights of the confidence generator are initially randomly initialized to values near 0. Through the application of the inverse sigmoid activation function, these weights are tuned to initialize the confidence values in proximity to 50%. Inference Speed. The SAFCE processes four query ground-view images and one satellite image in approximately 200ms. The detection time for all ground keypoints is about 3.5ms. The optimization process, which runs for 20 iterations at each of the three levels, takes a total of approximately 200ms. Qualitative Results. We compare our method with recent state-of-the-art (SOTA) visual-only methods, CVML [36] and HighlyAccurate [22], as well as the LiDAR-visual hybrid method SIBCL [33]. We present the evaluation results on the KITTI-CVL and FordAV-CVL datasets in Tab. 1 and Tab. 2. To ensure a fair comparison, we trained HighlyAccurate [22] and SIBCL [33] under the same image resolution and initial pose noise range. Since CVML [36] is unable to accurately estimate fine-grained orientations, we only evaluated its performance in terms of location estimation. We trained their model with ground truth orientation.\nTab. 1 presents an evaluation of our method\u2019s ability to generalize to previously unseen routes in the KITTI-CVL dataset using a front camera. For translation accuracy, our method exhibits superior performance compared to SOTA\n3The percentage of the prediction pose that is within a certain range. 4RTK GPS achieves an accuracy of 2 cm or better [5].\nmethods, with a significant reduction in the translation error. Specifically, our method achieves a reduction of 86% and 94% in mean lateral and longitudinal localization error. While our orientation accuracy is slightly less accurate than the LiDAR-based method, it maintains a comparable performance to SOTA visual-only method [22] in terms of rotation error. These results demonstrate the ability of our method to generalize to a wide range of scenes.\nThe performance of our method on cross-season generalization is presented in \u2018Log4\u20195 of Tab. 2. The test set in this case includes data from different time and seasons compared to the training set, which allows us to evaluate the performance of our method under varying lighting and seasonal conditions. Furthermore, in \u2018Log4\u21925\u2019 of Tab. 2, we analyze our method\u2019s generalization capability on an unseen route. In both cases, our method outperforms existing SOTA methods by significant margins. Specifically, we achieve a reduction of 52% and 43% in mean localization lateral error, 62% and 52% in mean localization longitudinal error, and 67% and 17% in mean orientation error in terms of seen and unseen routes, respectively. These results once again demonstrate the strong performance and robust generalization capabilities of our proposed method. Performance with Varying Numbers of Camera Inputs. We investigate the impact of multiple onboard cameras on\n5The trajectory of \u2018Log4\u2019 was selected for method evaluation in SIBCL [33] due to its relatively good satellite view alignment. Additionally, we evaluated other logs and the evaluation results can be found in the supplementary material.\nthe FordAV-CVL dataset and evaluate our method using different camera setups. These setups include the front camera (Front) in the 1-camera setting, two side cameras (2Sides), the front and rear cameras (2FR) in the 2-camera setting, and all front, rear, and two side cameras (4Cams) in the 4-camera setting. Our findings indicate that even with the use of a single front camera (\u2018Ours (Front)\u2019 in Tab. 2), our method outperforms the SOTA methods. Additional camera inputs lead to further improvements in performance, particularly with regards to orientation estimation, which can be attributed to the fact that a larger field of view (FoV) provides more information to accurately estimate orientation. Furthermore, our study reveals that the front and rear cameras provide more information for localization, whereas the left and right cameras contribute more to the lateral estimation. This could be attributed to the limited visibility of noticeable localization features such as road marks in the side cameras or the sensitivity of the side cameras to the roll angle. It is noteworthy that our method, despite utilizing four onboard cameras, consumes less memory (4499 MB) than HighlyAccurate [22], which requires 6445 MB due to its use of sparse purification. Performance under Different Initial Poses. The proposed method utilizes the LM algorithm and is subject to a convergence range 6 constraint. If the provided initial pose falls outside of this range, the method may fail to converge. To evaluate the method\u2019s robustness under a more stringent\n6The convergence range refers to the region in the pose space where the method can converge to the ground truth pose.\nscenario, we conducted experiments using a comprehensive set of initial poses. The results, shown in Fig. 7, indicate that our approach achieves a satisfactory level of accuracy even when the initial pose is subjected to yaw angle errors of up to \u00b160\u25e6 and lateral and longitudinal shifts of up to \u00b115m. The longitudinal estimation is found to be more sensitive to the initial pose compared to the lateral estimation. Moreover, in KITTI-CVL datasets that rely solely on a front onboard camera, a larger difference between the mean and median values suggests more cases falling outside the convergence range. Therefore, the use of multiple camera inputs, such as in the FordAV-CVL dataset with four cameras, can significantly expand both the translation and orientation coverage ranges, with the orientation coverage range being notably more improved."
        },
        {
            "heading": "6. Ablation Study",
            "text": "Two Confidence Maps. The proposed method adopts two types of confidence maps (\u201c2c w/o SE\u201d), i.e., viewconsistent and on-ground maps. An alternative approach was to use a single confidence map (\u201c1c w/o SE\u201d), which combined both on-ground and view-consistent confidences, and disabled gradient backpropagation from the satellite view. A comparison of using different types of confidence maps is reported in Tab. 3. We can see that using two confidence maps with distinct gradient backpropagation mechanisms leads to better performance compared to the alternative approach.\nSpatial Embedding. We study the impact of Spatial Embedding by comparing the performance of our algorithm with (\u201cFull\u201d) and without Spatial Embedding (\u201c2c w/o SE\u201d), as shown in Tab. 3. The results demonstrate that incorporating Spatial Embedding significantly improves the performance of the PureACL algorithm. View-consistent On-ground Keypoint Detector. We compare our keypoint detection design with the SOTA SuperPoint [3]. In this comparison, we use SuperPoint to detect keypoints and combine it with the two confidence maps to reduce the weights of points located on dynamic objects or above the ground plane. The results are presented in Tab. 3 as \u201cSuperPoint\u201d. Our view-consistent on-ground point detector (\u201cFull\u201d) outperforms \u201cSuperPoint\u201d as it detects a sufficient number of on-ground keypoints, which is more beneficial for cross-view localization.\nMulti-camera Fusion Method. We compare two fusion methods for keypoints captured by multiple onboard cameras: selecting the highest-confidence 2D projection (\u201cFull\u201d), which is used in our proposed method, and computing the mean of features and confidence scores across all visible onboard camera images (\u201cMean fusion\u201d). The results in Tab. 3 show that highest-confidence fusion outperforms Mean fusion due to more reliable selection."
        },
        {
            "heading": "7. Conclusion",
            "text": "This paper presents PureACL, a novel cross-view localization approach for accurate 3-DoF pose estimation that supports flexible multi-camera inputs. Our approach utilizes a view-consistent on-ground keypoint detector to handle dynamic objects and viewpoint variations while removing off-the-ground objects to establish the homography transformer assumption. Additionally, PureACL incorporates a spatial embedding that maximizes the use of camera intrinsic and extrinsic information to reduce visual matching ambiguity. PureACL is the first sparse visual-only approach and the first visual-only cross-view method capable of achieving a mean translation error of less than one meter. Our future plan is to integrate PureACL into the SLAM system for reduced loop closure dependence. Ultimately, PureACL has the potential to lead to robust, reliable, accurate, and low-cost localization systems."
        },
        {
            "heading": "8. Acknowledgements",
            "text": "The research is funded in part by an ARC Discovery Grant (grant ID: DP220100800) to HL."
        },
        {
            "heading": "A. Evaluation of Other FordAV-CVL Dataset Logs 12",
            "text": ""
        },
        {
            "heading": "B. Performance with Different Initial Poses 12",
            "text": "C. Visualization of Confidence Maps 13"
        },
        {
            "heading": "A. Evaluation of Other FordAV-CVL Dataset Logs",
            "text": "The \u2019Log4\u2019 trajectory was chosen for method evaluation in SIBCL [33] owing to its alignment accuracy with the satellite image. Furthermore, we also evaluated other logs from the FordAV-CVL Dataset in Tab. 4 to supplement the results presented in Tab. 2 of the main paper. There are three travelings included in every log: \u20192017-08-04-26\u2019, \u20192017-07-24\u2019, and \u20192017-10- 26\u2019. For the purpose of training, evaluation, and test dataset split, we use \u20192017-07-24\u2019 as the evaluation dataset for all logs. We select the traveling sequence with a higher number of images as the training dataset. Specifically, the training dataset of \u2019Log1\u2019 and \u2019Log3\u2019 is \u20192017-10-26\u2019, whereas the training dataset of \u2019Log4\u2019 and \u2019Log5\u2019 is \u20192017-08-04-26\u2019. The results demonstrate that our method is capable of estimating accurate 3-DoF pose with low spatial and angular errors in various scenarios, including freeway (log1), residential (log3, log5), university (log4) and vegetation (log4, log5)."
        },
        {
            "heading": "B. Performance with Different Initial Poses",
            "text": "In our main paper, we presented a chart illustration in Fig.7. Here, we further provide complete metrics results in Tab. 5 and Tab. 6. By presenting these tables, we aim to provide a comprehensive view of the data and enable readers to analyze the metrics more thoroughly.\nIn order to facilitate comparison, we have included a performance analysis with a single front onboard camera from the FordAV-CVL dataset, as depicted in Fig. 8.\nTo bring a comprehensive view of the data and enable readers to analyze the metrics more thoroughly, we further provide complete metrics results in Tab. 5 and Tab. 6, as a supplementary of the chat illustration in Fig. 7 in our main paper. In order to facilitate comparison, we have included a performance analysis with a single front onboard camera from the FordAV-CVL dataset, as depicted in Fig. 8.\nC. Visualization of Confidence Maps The main paper provides an example of the view-consistent confidence map V , the on-ground confidence map O, and the fused confidence map C of the KITTI-CVL dataset. Additionally, we present an example of the FordAV-CVL dataset in Fig. 9. In addition, we have generated confidence map videos of continuous trajectories in both the KITTI-CVL and FordAV-CVL datasets, which can be found in the supplementary video."
        }
    ],
    "title": "View Consistent Purification for Accurate Cross-View Localization",
    "year": 2023
}