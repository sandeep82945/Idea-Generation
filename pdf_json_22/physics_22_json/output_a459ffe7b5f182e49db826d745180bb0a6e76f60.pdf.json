{
    "abstractText": "Streaming anomaly detection refers to the problem of detecting anomalous data samples in streams of data. This problem poses challenges that classical and deep anomaly detection methods are not designed to cope with, such as conceptual drift and continuous learning. State-of-the-art flow anomaly detection methods rely on fixed memory using hash functions or nearest neighbors that may not be able to constrain high frequency values as in a moving average or remove seamless outliers and cannot be trained in an end-to-end deep learning architecture. We present a new incremental anomaly detection method that performs continuous density estimation based on random Fourier features and the mechanism of quantum measurements and density matrices that can be viewed as an exponential moving average density. It can process potentially endless data and its update complexity is constant O(1). A systematic evaluation against 12 state-of-the-art streaming anomaly detection algorithms and using 12 streaming datasets is presented.",
    "authors": [],
    "id": "SP:92d153bc8d8552a9a792f4dc7533b4ed5cc38dc2",
    "references": [
        {
            "authors": [
                "M. Ahmed",
                "A.N. Mahmood",
                "M.R. Islam"
            ],
            "title": "A survey of anomaly detection techniques in financial domain",
            "venue": "Future Generation Computer Systems",
            "year": 2016
        },
        {
            "authors": [
                "J. An",
                "S. Cho"
            ],
            "title": "Variational autoencoder based anomaly detection using reconstruction probability",
            "venue": "Special Lecture on IE 2(1),",
            "year": 2015
        },
        {
            "authors": [
                "F. Angiulli",
                "F. Fassetti"
            ],
            "title": "Detecting distance-based outliers in streams of data",
            "year": 2007
        },
        {
            "authors": [
                "S. Bhatia",
                "A. Jain",
                "P. Li",
                "R. Kumar",
                "B. Hooi"
            ],
            "title": "Mstream: Fast anomaly detection in multi-aspect streams. pp. 3371\u20133382",
            "venue": "Association for Computing Machinery,",
            "year": 2021
        },
        {
            "authors": [
                "S. Bhatia",
                "A. Jain",
                "S. Srivastava",
                "K. Kawaguchi",
                "B. Hooi"
            ],
            "title": "Memstream: Memory-based streaming anomaly detection",
            "venue": "WWW 2022 - Proceedings of the ACM Web Conference",
            "year": 2022
        },
        {
            "authors": [
                "M.M. Breunig",
                "H.P. Kriegel",
                "R.T. Ng",
                "J. Sander"
            ],
            "title": "Lof: identifying density-based local outliers",
            "venue": "Proceedings of the 2000 ACM SIGMOD international conference on Management of data. pp",
            "year": 2000
        },
        {
            "authors": [
                "R. Chalapathy",
                "A.K. Menon",
                "S. Chawla"
            ],
            "title": "Anomaly detection using one-class neural networks. arXiv preprint",
            "year": 2018
        },
        {
            "authors": [
                "V. Chandola"
            ],
            "title": "Anomaly detection : A survey",
            "year": 2009
        },
        {
            "authors": [
                "A. Dimokranitou"
            ],
            "title": "Adversarial autoencoders for anomalous event detection in images",
            "venue": "Ph.D. thesis,",
            "year": 2017
        },
        {
            "authors": [
                "Z. Ding",
                "M. Fei"
            ],
            "title": "An anomaly detection approach based on isolation forest algorithm for streaming data using sliding window",
            "year": 2013
        },
        {
            "authors": [
                "G. Fernandes",
                "J.J. Rodrigues",
                "L.F. Carvalho",
                "J.F. Al-Muhtadi",
                "M.L. Proen\u00e7a"
            ],
            "title": "A comprehensive survey on network anomaly detection",
            "venue": "Telecommunication Systems",
            "year": 2019
        },
        {
            "authors": [
                "J.A. Gallego",
                "F.A. Gonz\u00e1lez"
            ],
            "title": "Quantum adaptive fourier features for neural density estimation (2022)",
            "year": 2022
        },
        {
            "authors": [
                "F.A. Gonz\u00e1lez",
                "A. Gallego",
                "S. Toledo-Cort\u00e9s",
                "V. Vargas-Calder\u00f3n"
            ],
            "title": "Learning with density matrices and random features",
            "year": 2021
        },
        {
            "authors": [
                "F.A. Gonz\u00e1lez",
                "V. Vargas-Calder\u00f3n",
                "H. Vinck-Posada"
            ],
            "title": "Classification with quantum measurements",
            "venue": "Journal of the Physical Society of Japan 90(4),",
            "year": 2021
        },
        {
            "authors": [
                "S. Guha",
                "N. Mishra",
                "G. Roy",
                "O. Schrijvers"
            ],
            "title": "Robust random cut forest based anomaly detection on streams (2016",
            "year": 2016
        },
        {
            "authors": [
                "S. Hariri",
                "M.C. Kind",
                "R.J. Brunner"
            ],
            "title": "Extended isolation forest",
            "year": 2018
        },
        {
            "authors": [
                "A.H.L.I. Sharafaldin",
                "A.A. Ghorbani"
            ],
            "title": "Toward generating a new intrusion detection dataset and intrusion traffic characterization (2018) 10 InQMAD: Incremental Quantum Measurement Anomaly Detection",
            "year": 2018
        },
        {
            "authors": [
                "D. Kwon",
                "H. Kim",
                "J. Kim",
                "S.C. Suh",
                "I. Kim",
                "K.J. Kim"
            ],
            "title": "A survey of deep learning-based network anomaly detection",
            "venue": "Cluster Computing",
            "year": 2019
        },
        {
            "authors": [
                "L.J. Latecki",
                "A. Lazarevic",
                "D. Pokrajac"
            ],
            "title": "Outlier detection with kernel density functions. In: International Workshop on Machine Learning and Data Mining in Pattern Recognition",
            "year": 2007
        },
        {
            "authors": [
                "Y. Li",
                "K. Zhang",
                "J. Wang",
                "S. Kumar"
            ],
            "title": "Learning adaptive random features",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
            "year": 2019
        },
        {
            "authors": [
                "L.M. Manevitz",
                "M. Yousef"
            ],
            "title": "One-class svms for document classification",
            "venue": "Journal of machine Learning research 2(Dec),",
            "year": 2001
        },
        {
            "authors": [
                "E. Manzoor",
                "H. Lamba",
                "L. Akoglu"
            ],
            "title": "Xstream: Outlier dete\u2019x\u2019ion in feature-evolving data streams. pp. 1963\u20131972",
            "venue": "Association for Computing Machinery",
            "year": 2201
        },
        {
            "authors": [
                "L. Mart\u00ed",
                "N. Sanchez-Pi",
                "J.M. Molina",
                "Garcia"
            ],
            "title": "A.C.B.: Anomaly detection based on sensor data in petroleum industry applications",
            "venue": "Sensors (Switzerland)",
            "year": 2015
        },
        {
            "authors": [
                "Y. Mirsky",
                "T. Doitshman",
                "Y. Elovici",
                "A. Shabtai"
            ],
            "title": "Kitsune: An ensemble of autoencoders for online network intrusion detection",
            "venue": "Internet Society",
            "year": 2018
        },
        {
            "authors": [
                "N. Moustafa",
                "J. Slay"
            ],
            "title": "Unsw-nb15: a comprehensive data set for network intrusion detection systems (unsw-nb15 network data",
            "year": 2015
        },
        {
            "authors": [
                "G.S. Na",
                "D. Kim",
                "H. Yu"
            ],
            "title": "Dilof: Effective and memory efficient local outlier detection in data streams. pp. 1993\u20132002",
            "venue": "Association for Computing Machinery",
            "year": 2200
        },
        {
            "authors": [
                "H. Nguyen",
                "K.P. Tran",
                "S. Thomassey",
                "M. Hamad"
            ],
            "title": "Forecasting and anomaly detection approaches using lstm and lstm autoencoder techniques with the applications in supply chain management",
            "venue": "International Journal of Information Management",
            "year": 2021
        },
        {
            "authors": [
                "G. Pang",
                "C. Aggarwal",
                "C. Shen",
                "N. Sebe"
            ],
            "title": "Editorial deep learning for anomaly detection",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems",
            "year": 2022
        },
        {
            "authors": [
                "T. Pevn\u00fd"
            ],
            "title": "Loda: Lightweight on-line detector of anomalies",
            "venue": "Machine Learning",
            "year": 2016
        },
        {
            "authors": [
                "R. Popat",
                "J. Chaudhary"
            ],
            "title": "A survey on credit card fraud detection using machine learning",
            "year": 2018
        },
        {
            "authors": [
                "A. Rahimi",
                "B. Recht"
            ],
            "title": "Random features for large-scale kernel machines",
            "venue": "Proceedings of the 20th International Conference on Neural Information Processing Systems. p. 1177\u20131184",
            "year": 2007
        },
        {
            "authors": [
                "S. Rayana"
            ],
            "title": "ODDS library (2016), http://odds.cs.stonybrook.edu",
            "year": 2016
        },
        {
            "authors": [
                "L. Ruff",
                "J.R. Kauffmann",
                "R.A. Vandermeulen",
                "G. Montavon",
                "W. Samek",
                "M. Kloft",
                "T.G. Dietterich",
                "K.R. Muller"
            ],
            "title": "A unifying review of deep and shallow anomaly detection",
            "venue": "Proceedings of the IEEE",
            "year": 2021
        },
        {
            "authors": [
                "S. Sathe",
                "C.C. Aggarwal"
            ],
            "title": "Subspace outlier detection in linear time with randomized hashing",
            "venue": "IEEE 16th International Conference on Data Mining (ICDM). pp. 459\u2013468",
            "year": 2016
        },
        {
            "authors": [
                "A.A. Sodemann",
                "M.P. Ross",
                "B.J. Borghetti"
            ],
            "title": "A review of anomaly detection in automated surveillance",
            "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)",
            "year": 2012
        },
        {
            "authors": [
                "S.C. Tan",
                "K.M. Ting",
                "T.F. Liu"
            ],
            "title": "Fast anomaly detection for streaming data. In: Twenty-second international joint conference on artificial intelligence",
            "year": 2011
        },
        {
            "authors": [
                "M. Tavallaee",
                "E. Bagheri",
                "W. Lu",
                "A.A. Ghorbani"
            ],
            "title": "A detailed analysis of the kdd cup 99 data set",
            "year": 2009
        },
        {
            "authors": [
                "Y. Wang",
                "X. Luo",
                "L. Ding",
                "S. Fu",
                "X. Wei"
            ],
            "title": "Detection based visual tracking with convolutional neural network. Knowledge-Based Systems",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "Keywords incremental learning \u00b7 anomaly detection \u00b7 density matrix \u00b7 random Fourier features \u00b7 kernel density estimation \u00b7 approximations of kernel density estimation \u00b7 quantum machine learning"
        },
        {
            "heading": "1 Introduction",
            "text": "Anomaly detection is a well-studied problem [9, 30, 35]. The main idea is to detect data points or a group of data points that deviate from a \u2018normality\u2019 in a specific context (note that normality is not related to the Gaussian normal distribution) . This problem arises in several domains, such as network security [26], telecommunications [12], retail industry [29], network traffic [40], financial transactions [2], and wired and wireless sensors [41]. In recent years, particular interest has been given to methods that can deal with problems where data is continuously generated as a stream rather than as a batch of data points. This behavior is natural in credit card fraud detection [32], network system intrusion detection [26], camera surveillance [37], Internet of Things (IOT) device problems [25], among others. Data in this streaming environment is rapidly generated, potentially infinite, has tremendous volume, and can exhibit concept drift.\nClassical anomaly detection algorithms in batch environments are based on density estimation, such as kernel density estimation [20], classification, such as one-class support vector machines [23], and distance, such as the isolation forest [22]. Another type of recent solutions are based on deep neural networks that have shown good properties in the anomaly detection task. They are mainly based on variational autoencoders [3], deep belief networks [19], one-class deep networks [8] and adversarial autoencoders [10]. These methods assume that all training data points are available in the training phase. However, in a streaming context the data arrives continuously. Some of these methods have poor adaptability and extensibility, or inability to detect new anomalies continuously, where they have high model update cost and/or slow update speed.\n\u2217Citation: Joseph et al., InQMAD: Incremental Quantum Measurement Anomaly Detection.\nar X\niv :2\n21 0.\n05 06\n1v 1\n[ cs\n.L G\n] 1\n1 O\nTo pave the way, several methods have been developed in the last decade. Methods such as iForestASD [11], RCF [16], xStream [24] and Ex. IF [17] present a modification of the Isolation forest batch anomaly detection method. One of the problems of these methods is how to improve their inference complexity which is related to the depth of the trees which typically will be log(n) where n is the number of data points. To solve this problem, HS-Tree [38], RS-Hash [36] and MStream [5] use a hash structure to avoid traversing each tree. Other state-of-the-art methods use a modification of the well-studied local outlier factor (LOF) [7]. Methods such as LODA [31], DILOF [28] and MemStream [5] use a modification of the k-nearest neighbor algorithm, the roots of LOF, to score the outlierness of data points. However, some of them are based on a memory that stores m-data points which, viewed as a moving average, may not be able to detect high frequency points or possible outliers.\nIn this paper we present the novel method incremental quantum measurement anomaly detection (InQMAD). This method uses adaptive Fourier features to map the original space to a Hilbert space. In this space, a density matrix is used to capture the density of the dataset. A new point passes through each stage and provides a score that is used as an anomaly score in the final stage. An important feature of the method is that it is able to build a density estimation model that is continuously updated with the incoming data and it has the ability to give more importance to recent samples similar to an exponential moving average. The method works in streaming, is an unsupervised algorithm and retraining can be performed continuously. The proposed method requires constant memory to process new data points, can process data in a single pass, and process potentially endless streaming data or even massive datasets. It can update the model in constant time, and its complexity is O(1).\nIn summary, the contributions of the present work are:\n\u2022 A novel streaming anomaly detection method: the new method works in a streaming, potentially infinite and with potentially concept drift environment.\n\u2022 A systematic evaluation of the proposed method: the algorithm is evaluated in 12 streaming datasets and compared it against 12 state-of-the-art streaming anomaly detection methods.\n\u2022 An ablation study analyzing the new method: a systematic evaluation of each component of the method is performed.\nReproducibility: the code used in this paper is released as open source and can be found in https://github.com/Joaggi/Incremental-Anomaly-Detection-using-Quantum-Measurements/ and zenodo https://doi.org/10.5281/zenodo.7183564\nThe outline of the paper is as follows: in Section 2, we describe anomaly detection in streaming and present the benchmark methods with which we will compare our algorithm. In Section 3, we present the new method, explaining all the stages of the algorithm. In Section 4, we systematically evaluate the proposed method against state-of-the-art streaming anomaly detection algorithms. In Section 5, we state conclusions and outline future lines of research."
        },
        {
            "heading": "2 Background and Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Streaming Anomaly Detection",
            "text": "An anomaly can be broadly defined as an observation or data that deviates significantly from some kind of normality. Several types of anomalies can occur in real datasets, such as point anomalies, group anomalies, contextual anomalies, low-level texture anomalies, and high-level semantic anomalies. Classical methods have been used to solve this problem, but they suffer with high-dimensional datasets [19]. Most of the recent deep learning methods capture a lot of attention for their good properties, such as automatic feature extraction. However, their training time and streamwise inference is prohibitively long [6].\nStream anomaly detection can be viewed as a generalization of the typical anomaly detection problem where data grows infinitely. Therefore, it is impractical, impossible or unnecessary to store every data point that arrives as a stream. In this context, the method has to distinguish between normal and anomalous data points, where concept drift can occur and the number of anomalies is scarce compared to normal data [11]. Several types of concept drift can arise in streaming datasets, such as sudden, gradual, incremental or recurrent drift. Concept drift occurs in data streams, where usually old data is less important than new data. This trend in data has an evolutionary pattern, where recent behavior should be of greater importance than older patterns [31]. In order to solve this problem, the method must use a constant memory and a nearly constant inference processing time. Therefore, it will process the data in a single pass. In the following subsection 2.3, 12 methods for streaming anomaly detection are presented."
        },
        {
            "heading": "2.2 Streaming Anomaly Detection Problem",
            "text": "In streaming anomaly detection, the data points X = {x1, \u00b7 \u00b7 \u00b7 ,xt} arrive as a d-feature-dimension sequence. This sequence can be the sequence of transactions for a given credit card or the temperature recorded by an IOT sensor. The challenge in this configuration in that the concept of \"normality\" evolves over time, meaning that a point that was considered normal behavior can drift and become anomalous behavior. To solve this problem, there is a need to develop algorithms that can learn on-line with high speed and low memory consumption."
        },
        {
            "heading": "2.3 Streaming Anomaly Detection Baseline Methods",
            "text": "\u2022 STORM [4]: A Stream Manager and a Query Manager have been proposed. The former is an indexed stream buffer whose job is to store the number of successive neighbors and the identifiers of the most recent preceding neighbors. The latter is a procedure that efficiently answers queries about whether a certain data point is an inlier or an outlier.\n\u2022 HS-Tree [38]: This method constructs complete binary trees where each tree has at most 2h+1 \u2212 1 node. Each subtree is constructed by randomly selecting a feature and breaking it in half. A random perturbation of each subtree is performed to create diverse subtrees. Each point has to traverse each binary tree to capture the mass profile. Finally, a scoring function is computed using the mass function of a query data point passing through each data node.\n\u2022 iForestASD [11]: This algorithm has its roots in the Isolation Forest method. The method uses a window of the streaming data and is sent to the Isolation Forest. An abnormality score is calculated using the average of the depth of the point on each tree in the forest. If the point is normal, it is joined to the Isolation Forest.\n\u2022 RS-Hash [36]: This method uses the Isolation Forest method at its roots. A tree is constructed as a sequence of randomly selected features. Anomalous points are those that are easily separated from the normal data points. With the scoring function given by the Isolation Forest, a score of anomalous sliding windows is computed to detect concept drift.\n\u2022 RCF [16]: The algorithm constructs a robust randomized cutting tree (rrct) using a random selection of the dimension weighted by its range. A uniform distribution is used to cut the selected dimension. This process is repeated n times, with n being the maximum depth. A forest is constructed using several rrct. A new point is classified as an anomaly using the comparison of its insertion and deletion complexity.\n\u2022 LODA [31]: The algorithm uses a set of histograms for each dimension. Each histogram is mapped to a projection space using w parameters that capture the importance of the feature. The log likelihood in the projection space is then calculated. An anomalous data point is expected to have a lower value of the log likelihood.\n\u2022 Kitsune [26]: Kitsune is an algorithm that uses an ensemble of autoencoders to provide an anomaly score. Features are sent to l autoencoders of 3 layers each. The reconstruction error calculated as root mean square error (RMSE) is sent to a final 3-layer autoencoder. Finally, the RMSE of the reconstruction is calculated and used as the anomaly score.\n\u2022 DILOF [28]: The method uses the k nearest neighbor information as in the Local Outlier Factor (LOF) but improves it in the case of streaming data. The algorithm has two phases: a detection phase and a summary phase. In the first phase it decides whether a point is anomalous or not. In the second, it uses an approximation algorithm with a sampling strategy to update the memory of the k nearest neighbors.\n\u2022 xStream [24]: The method uses a hash structure to reduce the dimensionality of the data points, which allows the evolution of features in the stream. After reduction, the method partitions the space into so-called half-space chains. These partitions capture the density estimate at a different granularity. The outlier score is calculated based on the density estimate score. For streaming, a previous window is used to score the point outlier.\n\u2022 MStream [5]: This algorithm uses two locality-sensitive hash functions: feature hashing and record hashing. The former computes a hash for each feature in the data point. The latter computes a hash for all features simultaneously. The anomaly score is calculated using a chi-square density function. The algorithm is combined with an autoencoder to reduce the dimensionality of the data.\n\u2022 Ex. IF [17]: This algorithm uses a random slope to cut the hyperplane and a random intercept. This differs from the isolation forest because the latter chooses random features and generates a random cut on that feature. The method shows better results when compared to the isolation forest for anomaly detection.\n\u2022 MemStream [6]: The method proposes a nearest neighbor memory based algorithm. Each data point passes through a shallow autoencoder to reduce the dimensionality of the original space. A memory is built using\nn-normal data points as initialization. Then, when a new point arrives it is forwarded to the autoencoder and compared to the memory. An anomalous point will have a high mean square error compared to its neighbors. The memory is updated only with normal data points."
        },
        {
            "heading": "3 Incremental Quantum Measurement Anomaly Detection (InQMAD)",
            "text": "In this section, we present the new method new Incremental Quantum Measurement Anomaly Detection (InQMAD). Figure 1 shows each of the steps of the Algorithms 1 and 2. The method consists of five steps: (1) an adaptive Fourier features, (2) a density matrix initialization, (3) a quantum measurement of the new streaming data points, (4) a decision anomaly detection threshold for the new data, and (5) an update of the density matrix \u03c1 in case of new normal data points. Each stage is explained in detail in this section."
        },
        {
            "heading": "3.1 Adaptive Fourier Features",
            "text": "All streaming data points are mapped into a Hilbert space using adaptive Fourier features, first proposed as random Fourier features by [33]. They showed that a Gaussian kernel can be approximated as:\nk(x,y) ' Ew(\u3008\u03c6\u0302rff,w(x), \u03c6\u0302rff,w(y)\u3009) (1)\nwhere \u03c6\u0302rff,w = \u221a 2 cos ( wTx+ b ) , w \u223c N(0, Id) and b \u223c Uniform(0, 2\u03c0). The expectation is possible to Bochner\u2019s Theorem and the fact that using Equation 1 that defines a randomized map converge in probability to the Gaussian kernel. However, the randomized map can be further refined using an optimization step, in particular using gradient descent, as shown in [13, 21]. Given two pairs of random data points (xi, xj), the parameters w, b can be optimized using the following equation:\nw\u2217, b\u2217 = argmin w,b\n1\nm \u2211 xi,xj\u2208s (k\u03c3(xi,xj)\u2212 k\u0302w,b(xi,xj))2\nwhere k\u0302w,b(xi,xj))2 is the Gaussian kernel of Fourier feature approximation and k\u03c3(xi,xj) is the Gaussian kernel with a bandwidth parameter \u03c3. Adaptive Fourier features allow us to compute the feature space avoiding the explicit kernel computation and will be used as input to the density matrix in the next step. We performed an empirical evaluation of this improvement algorithm and propose an intermediate enhancement step. Assume that all points are normalized between [0, 1]d, where d is the number of features, generate random samples from Uniform(\u22120.5, 1.5)d. The increase in the range of the sampling space is to account for future points outside the range. The next question that arises is how many points we should generate. We use an empirical approach depending on how large the initial training dataset is. If the training set is small (< 1000), we will sample until it consists of 10,000 data points. Otherwise,\nwe will sample twice the size of the initial training dataset. The intuition here is that if we have few data points, the algorithm will be prone to overfitting in a local space near initial training dataset. The algorithm 1 shows the step in 1."
        },
        {
            "heading": "3.2 Density Matrix Initialization",
            "text": "The second step of the algorithm consists of calculating the density matrix using the mapping obtained by passing each xt to the adaptive Fourier feature step explained above. The density matrix is a formalism of quantum mechanics that was used as a base tool by [13, 14] to create a density estimation method. The authors derive a new algorithm that uses random Fourier features to store the density matrix \u03c1. The following equation is a slight modification of the density matrix in terms of stream data:\n\u03c1t = 1\nn N\u2211 i=1 qi \u00b7 \u03c6aff(xi)\u03c6taff(xi) (2)\nwhere \u2211t i=1 qi = 1. To initially compute the density matrix, an initial portion of the stream {x1, \u00b7 \u00b7 \u00b7 ,xn} is selected and sent to the equation 2 to calculate the density matrix \u03c1. The initial training dataset size can degrade the final performance of the algorithm; therefore, it is necessary to find it using a cross-validation approach. It should be noted that the density matrix is computed using adaptive Fourier features avoiding explicit kernel computation as in kernel density estimation; however, other feature mappings can be used as shown in [15]. The density matrix is initialized in Algorithm 1 in Step 1."
        },
        {
            "heading": "3.3 Quantum Measurement",
            "text": "The third step of the algorithm is the quantum measurement. A quantum measurement can be obtained by using the density matrix \u03c1 and mapping a data point xt+1 at time t+ 1 as:\nf\u0302(xt+1) = 1\nM\u03c3 \u03c6aff(xt+1)\nT \u03c1t \u03c6aff(x) (3)\nwhere \u03c1 is the density matrix defined in the equation 2 and M\u03c3 is a normalization constant. This step gives us an estimate of the density of the given data point that will be used in the next step. The initialized Algorithm 1 uses the quantum measurement in Step 1 and the inference Algorithm 2 uses it in Step 2."
        },
        {
            "heading": "3.4 Anomaly Detection Classification",
            "text": "The fourth step of the algorithm is the anomaly detection classification stage. The threshold is defined as \u03c4 , where a new point xi is defined as anomalous according to the following equation:\ny\u0302(xi) =\n{ \u2018normal\u2019 if f\u0302(xi) \u2265 \u03c4\n\u2018anomaly\u2019 otherwise\nAfter initialization of the density matrix using the initial training dataset, the threshold is found using two different approaches. The first is that the threshold can be found using prior knowledge of the proportion of anomalies (\u03b2). The second approach is to use an optimization metric computed over {f\u0302(x1), \u00b7 \u00b7 \u00b7 , f\u0302(xn)}, for instance using the best threshold with respect to AUC-ROC in initial memory. The \u03c4 is obtained in Step 1 of the Algorithm 1 and the anomaly detection classification occurs in Step 2 of the Algorithm 2."
        },
        {
            "heading": "3.5 Density Matrix Update",
            "text": "The fifth and final step of the algorithm is to update the density matrix. If a new point xt at time t of the flow is classified as a normal data point, the density matrix \u03c1t+1 will be calculated as follows:\n\u03c1t+1 = (1\u2212 \u03b1) \u00b7 \u03c1t + \u03b1 \u00b7 \u03c6aff(xt+1)\u03c6Taff(xt+1) (4) Proposition 1. The resulting matrix \u03c1t+1 in Equation 4 is a valid density matrix of the form \u03c1t+1 =\u2211t+1 i=1 qi\u03c6(xi)\u03c6 T (xi) with q1 = (1\u2212 \u03b1)t+1, qi = (1\u2212 \u03b1)t\u2212i+1 \u00b7 \u03b1 \u2200i \u2208 {1, \u00b7 \u00b7 \u00b7 , t} and \u03b1 \u2208 [0, 1].\nAlgorithm 1: InQMAD initialization process Input: Training dataset D = {xt}t=1,\u00b7\u00b7\u00b7 ,n,xt \u2208 Rd \u03b1: forgetting trade-off, \u03c3: bandwith parameter \u03b2: proportion of anomalies Output: wAFF, bAFF, \u03c1, \u03c4\n1: w\u2217AFF, b \u2217 AFF = argminw,b 1 m \u2211 xi,xj\u2208D(k\u03c3(xi,xj)\u2212 k\u0302w,b(xi,xj)) 2 2: for xt \u2208 D do 3: \u03c1t = (1\u2212 \u03b1) \u00b7 \u03c1t\u22121 + \u03b1 \u00b7 \u03c6aff(xt)\u03c6Taff(xt) 4: f\u0302(xt) =\n1 M\u03c3 \u03c6aff(xt) T \u03c1t \u03c6aff(xt)\n5: end for 6: \u03c4 = q\u03b2(f\u0302(x1), \u00b7 \u00b7 \u00b7 , f\u0302(xn)) 7: return wAFF, bAFF, \u03c1n, \u03c4\nProposition 1 shows that an exponential decay defines a valid density matrix. This matrix consists of all data points that have reached the method up to time t, where older examples are less important and have a lower weight compared to more recent ones similar to an exponential moving average.\nFigure 2 shows the difference between a constant memory and an exponential decay method. The figure shows a memory-based algorithm using a window width of 200 and compares it against an exponential decay memory using a \u03b1 = 2memory window width . The black memory-based line shows a constant memory whose points are of equal importance to the method. The red exponential decay line shows an exponentially decaying method whose points become less and less important as they age. The density matrix update only occurs if the new point is classified as \u2018normal\u2019 and is performed in Step 2 of Algorithm 2."
        },
        {
            "heading": "3.6 Complexity Analysis",
            "text": "In this subsection, d will be the number of features and D will be the number of adaptive Fourier features. For time complexity, in the anomaly detection phase, a dot product is computed between each data point and the adaptive Fourier feature whose time complexity is proportional to O(dD). In addition, a dot product is computed from the above result and the density matrix whose time complexity is proportional to O(D2). In terms of memory, InQMAD needs to maintain a density matrix whose size is proportional to O(D2) and stores the weights of the adaptive Fourier features whose size is proportional to O(dD)."
        },
        {
            "heading": "4 Experimental Evaluation",
            "text": "We designed and tested an experimental setup in order to answer the following questions:\nAlgorithm 2: InQMAD inference and density matrix update Input: xt+1 \u03c1t: density matrix \u03b1: forgetting trade-off, waff, baff: adaptive Fourier features parameters \u03c4 : threshold anomaly detector Output: \u03c1t+1-quantum measurement KDE parameter, y\u0302t+1 classification of the given data point\n1: f\u0302(xt+1) = 1 M\u03c3 \u03c6aff(xt+1) T \u03c1t \u03c6aff(xt+1) 2: if f\u0302(xt+1) \u2265 \u03c4 then 3: \u03c1t+1 = (1\u2212 \u03b1) \u00b7 \u03c1t\u22121 + \u03b1 \u00b7 \u03c6aff(xt+1)\u03c6Taff(xt+1) 4: y\u0302t+1 = \u2018normal\u2032 5: else 6: \u03c1t+1 = \u03c1t 7: y\u0302t+1 = \u2018anomaly\u2032 8: end if 9: return \u03c1t+1, y\u0302t+1\n\u2022 Q1. Streaming Method Comparison. Does our method performs accurately in anomaly detection over streams of data, when compared it against to state-of-the-art baseline methods?\n\u2022 Q2. Adaptability. How well our method does adapt to \u201cconcept drift\", that is, sudden changes in the data stream inner structure?\n\u2022 Q3. Ablation Study. What are the effects of removing some stages of the algorithm (in particular, the Adaptive Fourier features embedding) on the overall performance of our method?"
        },
        {
            "heading": "4.1 Comparison to Streaming Methods",
            "text": "Experimental Setup\nThe experimental setup presented in this paper took inspiration from the setup proposed in [6]. There, it was performed a comparison of a dozen of algorithms, based on area under the ROC curve (commomly known as AUC or AUROC), by applying them over a series of streaming and anomaly detection datasets. Our method was implemented using the JAX framework, a Python library designed for high-performance machine learning. All the experiments were carried out on a machine with a 2.1GHz Intel Xeon 64-Core processor with 128GB RAM and two NVIDIA RTX A5000 graphic processing units, that run Ubuntu 20.04.2 operating system.\nTo handle the inherent randomness that the proposed method can present in some of its stages (particularly in dataset splitting and neural network training), we selected a unique, invariant value for all the random seeds that affect the behavior of our method.\nDatasets\nIn this framework, we selected twelve different datasets, that can be divided into two groups: seven datasets (Cardio, Ionosphere, Mammography, Pima, Satellite, Satimage and Synthetic) with relatively low dimensions and low number of records, mainly used to perform proof-of-concept in anomaly detection, and five datasets (KDD99, NSL, DoS, UNSW and Cover) with hundreds of thousands of registers and a high number of dimensions, that require an intensive use of resources to be processed. A summary of the main features of all the datasets can be seen in Table 1, and a brief description of each dataset is presented in the Supplemental Material.\nParameter Search\nThe behavior of InQMAD depends on a series of parameters that regulate the Adaptive Fourier features embedding, the density estimation stage and the size of the initialization dataset of the method. Particularly, we tried to find which of these parameters had a larger impact over the performance of the algorithm, and we selected four to carry out a parameter grid search over them in order to find the combination of parameters that showed the best performance. The selected parameters are the following:\nThe winning combinations of parameters for each dataset can be found in the Supplemental Material.\nEvaluation Metrics\nFollowing the framework established in [6], we selected the area under the ROC curve (also known as AUC or AUCROC) as the main metric to establish the performance of our algorithm and compare it with the baseline methods. This metric was chosen because it represents the overall capability of the model to distinguish between the classes, regardless of the thresholds used in specific situations.\nResults and discussion\nThe AUCROC score obtained for every pair of dataset and algorithm is presented in Table 2. The metrics corresponding to our method can be found in the column labeled InQMAD. The absent values correspond to cases where the algorithms were not able to handle that particular dataset. For each row, the best value is marked in bold, the second is underlined and the third is marked in italics.\nWhen looking at the average performance, InQMAD is the best method, being slightly better than MemStream, and notably better than all other methods, thus showing a noticeable improvement over the state-of-the-art algorithms in the area. When considering the size of the datasets, there is a clear advantage of InQMAD over the smaller datasets (the ones in the top rows of Table I), due to the fact that it shows the best performance for all of these datasets. For bigger datasets (the ones in the bottom rows of Table I), it is still competitive but some algorithms do better, mainly in the biggest datasets like DoS or UNSW.\nOn the other hand, the number of dimensions does not seem to be as strongly related to the performance of our method as the size, considering that it had a mixed behavior for datasets with low dimensionality (high for Pima or Satellite, lower for Cover) and for datasets with high dimensions (high for KDD or NSL, lower for UNSW). In general, memory-based methods, such as MemStream, underperform InQMAD. Our intuition for this behavior is that memory-based algorithms are similar to moving averages that may be prone to a high frequency point or outlier. However, InQMAD can be considered an exponential moving average that dampens the weight of high frequency points."
        },
        {
            "heading": "4.2 Ablation Study",
            "text": "Being one of the most important stages of our algorithm, we wanted to determine the degree of influence of the Adaptive Fourier Features embedding over the algorithm performance. This comparison was made by building alternate versions of our method: InQM-NoAdp, that instead of using adaptive Fourier features as mapping, uses the random Fourier features approach stated in previous works like [14], and InQMAD-200, where the number refers to the size of the encoding given by adaptive Fourier features. Since the size of the encoding was chosen to be 2000 in all the experiments of InQMAD, we expect to determine if the size of the adaptive features encoding enhances or diminishes the overall performance of the method.\nUsing these different versions of our method over the datasets in the experimental setup, we calculated the metrics that we present in Table 3. In order to make a fair comparison, in every dataset we used the same parameters for all the versions. The best result of each row is marked in bold.\nFrom these results, there is a clear advantage in using the adaptive Fourier Features in the majority of datasets, regardless of their size or dimensionality. Only in one of the datasets (DoS) there is a small decrease in performance when using many Adaptive features, in favor of a lower embedding size. Although, the difference in this case is way smaller than the differences in favor of the use of Adaptive features on other datasets, particularly Ionosphere, Syn and Satellite."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this paper we present a new method for data stream anomaly detection called Incremental Quantum Measurement Anomaly Detection (InQMAD). The new method uses the adaptive Fourier feature to map the data to a higher space and density matrices to capture the density estimate. The new method has an initial linear training complexity in terms of the number of training data points. In addition, it has a linear update complexity, which makes it suitable for streaming problems. A systematic evaluation has been performed against 12 state-of-the-art methods on 12 streaming datasets, showing similar performance. We give a theoretical guarantee that the update stage of the algorithm generates a valid density matrix. In addition, an ablation study shows the importance of the new empirical random sampling applied to the adaptive Fourier features and the importance of the update memory parameter \u03b1. For future research, we will use a dimensionality reduction step such as principal component analysis or autoencoders to help with the curse of dimensionality in massive datasets."
        },
        {
            "heading": "6 Supplemental material",
            "text": "Proposition 2. The resulting matrix \u03c1t+1 = (1\u2212\u03b1) \u00b7\u03c1t+\u03b1 \u00b7\u03c6(xt+1)\u03c6T (xt+1) in Equation 4 is a valid density matrix of the form \u03c1t+1 = \u2211t+1 i=1 qi\u03c6(xi)\u03c6 T (xi) with q1 = (1\u2212\u03b1)t+1, qi = (1\u2212\u03b1)t\u2212i+1 \u00b7\u03b1 \u2200i \u2208 {1, \u00b7 \u00b7 \u00b7 , t} and \u03b1 \u2208 [0, 1]. Proof. \u03c1t+1 is a valid density matrix if (1\u2212 \u03b1)t+1 + \u2211t i=1(1\u2212 \u03b1)t\u2212i+1 \u00b7 \u03b1 = 1. We give a proof by induction on t.\nBase Case: for t = 1, define \u03c1t = \u03c6(x1)\u03c6T (x1). for t = 2, define q1 = (1\u2212 \u03b1) and q2 = \u03b1 then q1 + q2 = 1 Induction Step: Show that for every t \u2265 2, if \u03c1t holds, then \u03c1t+1 also holds. Define\npt = (1\u2212 \u03b1)t + \u03b1 \u00b7 t\u22121\u2211 i=1 (1\u2212 \u03b1)t\u2212i+1 (5)\nUsing \u2211n k=1 x k = (x\u2212 xn+1)/(1\u2212 x), then\npt = (1\u2212 \u03b1)t + \u03b1 \u00b7 ( (1\u2212 \u03b1)\u2212 (1\u2212 \u03b1)t\n1\u2212 (1\u2212 \u03b1) ) = 1 (6)\nNow, for t+1 :\nqt+1 = q1 \u00b7 ( (1\u2212 \u03b1)t + \u03b1 \u00b7\nt\u22121\u2211 i=1\n(1\u2212 \u03b1)t\u2212i+1 ) + q2 (7)\n= (1\u2212 \u03b1) \u00b7 ( (1\u2212 \u03b1)t + \u03b1 \u00b7\nt\u22121\u2211 i=1\n(1\u2212 \u03b1)t\u2212i+1 ) + \u03b1 (8)\n= (1\u2212 \u03b1)t+1 + \u03b1 \u00b7 t\u2211 i=1 (1\u2212 \u03b1)t\u2212i+1 + \u03b1 (9)\n= (1\u2212 \u03b1)t+1 + \u03b1 \u00b7 t+1\u2211 i=0 (1\u2212 \u03b1)t\u2212i+1 (10)\n= 1 (11)"
        },
        {
            "heading": "6.1 Dataset Description",
            "text": "We present a summarized description of the datasets used in the experimental setup to apply the algorithms on.\n\u2022 Cardio [34]: consists of measurements of fetal heart rate, where the original classes are normal, suspect, and pathologic; to adapt it to outlier detection, the normal class formed the inliers and the pathologic class is downsampled and labeled as outliers, while the suspect class is discarded.\n\u2022 Ionosphere [34]: originally a binary classification dataset from UCI ML repository, the \u2018bad\u2019 class is considered as outliers and the \u2018good\u2019 class as inliers.\n\u2022 Mammography [34]: an open dataset about breast calcification, for outlier detection tasks the minority class of \u2018calcification\u2019 is considered as outliers and the \u2018non-calcification\u2019 class as inliers.\n\u2022 Pima [34]: also from UCI ML repository, it includes data of female Indian patients with the objective of classifying them as diabetic or healthy.\n\u2022 Satellite [34]: derived from the Statlog dataset from UCI ML repository, it is a multiclass dataset. For anomaly detection, the smallest three classes (2, 4, 5) are combined to form the outliers, while all the other classes are combined to form the inliers.\n\u2022 Satimage [34]: coming from the previous Satellite dataset, here class 2 has been downsampled and considered as outliers, while the other classes are labeled as normal data. This dataset and the latter came from ODDS virtual library.\n\u2022 Synthetic: this dataset was created in [6] to analyze if their method could afford sudden changes in data distribution. For this, the authors of the dataset combined two sinusoidal waves, and contaminated 10% of the samples by adding Gaussian noise to simulate anomalous data.\n\u2022 KDD99 [1]: one of the best-known datasets in anomaly detection, the original dataset contains 34 numerical dimensions and 7 categorical dimensions, that were transformed using one-hot encoding to obtain a dataset of 121 dimensions. We treat normal data as outliers in this experiment, given the fact that only 20% of all records are labeled as normal.\n\u2022 NSL [39]: coming from the previous KDD dataset, it adds some dimensions and solves redundant and duplicate records.\n\u2022 Cover [34]: originally a multiclass dataset from UCI ML repository, it is used to predict forest cover type from wilderness areas in Colorado. Instances from class 2 are considered as normal and instances from class 4 are labeled as anomalies. Instances from other classes are omitted.\n\u2022 DoS [18]: this dataset was created by the Canadian Institute of Cybersecurity. Each record corresponds to a network package, and they were captured from simulations of normal network traffic and synthetic attacks.\n\u2022 UNSW [27]: created by ACCS (an Australian institute of computer science and cybersecurity), it contains both real network normal activities and synthetic attacks. Originally it included nine types of attacks. It has 13% of anomalies."
        },
        {
            "heading": "6.2 Parameters",
            "text": "The winning combinations of parameters for our method are presented in Table 4. For each dataset, their respective parameters are presented as an array.\nThe parameters selected to run the baseline methods were the same for all datasets, and follow the recommended values in the original proposal of each method. Their values are presented in Table 5."
        },
        {
            "heading": "6.3 Statistical Tests",
            "text": "Using the Table 2 as starting point, the Friedman test, a well-known statistical method to compare different populations or groups, was applied in order to determine whether there are statistically significant differences between the different methods. Friedman test uses the following formula:\nQ =  12 Nk(k + 1) k\u2211 j=1 R2j \u2212 3N(k + 1) where N is the number of datasets, k is the number of algorithms, and R2j is the squared sum of the observations for each particular algorithm. Given a confidence value of 0.05 and the p-value given by P[\u03c72n\u22121 \u2265 Q], if the p-value is\nlower than the confidence value, there is statistically significant evidence supporting that the methods are different. Applying the test returns a p-value of 3.06\u00d7 10\u221214, clearly showing that such difference does exist. Friedman test does not indicate which of the methods are responsible for this difference, so to compare them two by two, we use the Friedman-Nemenyi test, applying it over all pairs of methods. This test generates coefficients for each pair of datasets that indicate if they are different (near to 0) or not (near to 1).\nFigure 3 shows the results of Friedman-Nemenyi test, where black squares correspond to pairs of datasets that differ significantly, and white squares correspond to pairs that do not. The most different methods are LODA (due to their poor results) and InQMAD (the best overall method). Other methods that differ notably from others include MemStream (the second best method) and iForestASD (the second worst method). The remaining methods differ with others in only one or two cases."
        }
    ],
    "year": 2022
}