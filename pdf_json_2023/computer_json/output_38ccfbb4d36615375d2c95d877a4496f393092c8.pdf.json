{
    "abstractText": "Avatars are important to create interactive and immersive experiences in virtual worlds. One challenge in animating these characters to mimic a user\u2019s motion is that commercial AR/VR products consist only of a headset and controllers, providing very limited sensor data of the user\u2019s pose. Another challenge is that an avatar might have a different skeleton structure than a human and the mapping between them is unclear. In this work we address both of these challenges. We introduce a method to retarget motions in real-time from sparse human sensor data to characters of various morphologies. Our method uses reinforcement learning to train a policy to control characters in a physics simulator. We only require human motion capture data for training, without relying on artist-generated animations for each avatar. This allows us to use large motion capture datasets to train general policies that can track unseen users from real and sparse data in real-time. We demonstrate the feasibility of our approach on three characters with different skeleton structure: a dinosaur, a mouse-like creature and a human.We show that the avatar poses often match the user surprisingly well, despite having no sensor information of the lower body available. We discuss and ablate the important components in our framework, specifically the kinematic retargeting step, the imitation, contact and action reward as well as our asymmetric actor-critic observations. We further explore the robustness of our method in a variety of settings including unbalancing, dancing and sports motions.",
    "authors": [
        {
            "affiliations": [],
            "name": "DANIELE REDA"
        },
        {
            "affiliations": [],
            "name": "YUTING YE"
        }
    ],
    "id": "SP:16678d16b22871e6a2e7c3f59230c7636da7c9f2",
    "references": [
        {
            "authors": [
                "Kfir Aberman",
                "Peizhuo Li",
                "Dani Lischinski",
                "Olga Sorkine-Hornung",
                "Daniel Cohen-Or",
                "Baoquan Chen."
            ],
            "title": "Skeletonaware networks for deep motion retargeting",
            "venue": "ACM Transactions on Graphics (TOG) 39, 4 (2020), 62\u20131.",
            "year": 2020
        },
        {
            "authors": [
                "Mazen Al Borno",
                "Ludovic Righetti",
                "Michael J Black",
                "Scott L Delp",
                "Eugene Fiume",
                "Javier Romero."
            ],
            "title": "Robust Physicsbased Motion Retargeting with Realistic Body Shapes",
            "venue": "Computer Graphics Forum, Vol. 37. Wiley Online Library, 81\u201392.",
            "year": 2018
        },
        {
            "authors": [
                "Sadegh Aliakbarian",
                "Pashmina Cameron",
                "Federica Bogo",
                "Andrew Fitzgibbon",
                "Tom Cashman."
            ],
            "title": "FLAG: Flowbased 3D Avatar Generation from Sparse Observations",
            "venue": "2022 Computer Vision and Pattern Recognition. https: //www.microsoft.com/en-us/research/publication/flag-flow-based-3d-avatar-generation-from-sparse-observations/",
            "year": 2022
        },
        {
            "authors": [
                "Kevin Bergamin",
                "Simon Clavet",
                "Daniel Holden",
                "James Richard Forbes."
            ],
            "title": "DReCon: data-driven responsive control of physics-based characters",
            "venue": "ACM Transactions On Graphics (TOG) 38, 6 (2019), 1\u201311.",
            "year": 2019
        },
        {
            "authors": [
                "Z. Cao",
                "G. Hidalgo Martinez",
                "T. Simon",
                "S. Wei",
                "Y.A. Sheikh."
            ],
            "title": "OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (2019).",
            "year": 2019
        },
        {
            "authors": [
                "Nuttapong Chentanez",
                "Matthias M\u00fcller",
                "Miles Macklin",
                "Viktor Makoviychuk",
                "Stefan Jeschke."
            ],
            "title": "Physics-based motion capture imitation with deep reinforcement learning",
            "venue": "Proceedings of the 11th annual international conference on motion, interaction, and games. 1\u201310.",
            "year": 2018
        },
        {
            "authors": [
                "Stelian Coros",
                "Philippe Beaudoin",
                "Michiel Van de Panne."
            ],
            "title": "Generalized biped walking control",
            "venue": "ACM Transactions On Graphics (TOG) 29, 4 (2010), 1\u20139.",
            "year": 2010
        },
        {
            "authors": [
                "Andrea Dittadi",
                "Sebastian Dziadzio",
                "Darren Cosker",
                "Ben Lundell",
                "Tom Cashman",
                "Jamie Shotton."
            ],
            "title": "Full-Body Motion From a Single Head-Mounted Device: Generating SMPL Poses From Partial Observations",
            "venue": "International Conference on Computer Vision 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Levi Fussell",
                "Kevin Bergamin",
                "Daniel Holden."
            ],
            "title": "SuperTrack: motion tracking for physically simulated characters using supervised learning",
            "venue": "ACM Transactions on Graphics (TOG) 40, 6 (2021), 1\u201313.",
            "year": 2021
        },
        {
            "authors": [
                "Thomas Geijtenbeek",
                "Nicolas Pronost",
                "Frank van der Stappen."
            ],
            "title": "Simple data-driven control for simulated bipeds",
            "venue": "Eurographics/ACM SIGGRAPH Symposium on Computer Animation (SCA).",
            "year": 2012
        },
        {
            "authors": [
                "Thomas Geijtenbeek",
                "Michiel van de Panne",
                "A Frank Van Der Stappen."
            ],
            "title": "Flexible muscle-based locomotion for bipedal creatures",
            "venue": "ACM Transactions on Graphics (TOG) 32, 6 (2013), 1\u201311.",
            "year": 2013
        },
        {
            "authors": [
                "R\u0131za Alp G\u00fcler",
                "Natalia Neverova",
                "Iasonas Kokkinos."
            ],
            "title": "Densepose: Dense human pose estimation in the wild",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 7297\u20137306.",
            "year": 2018
        },
        {
            "authors": [
                "F\u00e9lix G. Harvey",
                "Mike Yurick",
                "Derek Nowrouzezahrai",
                "Christopher Pal."
            ],
            "title": "Robust Motion In-Betweening",
            "venue": "39, 4 (2020).",
            "year": 2020
        },
        {
            "authors": [
                "Yinghao Huang",
                "Manuel Kaufmann",
                "Emre Aksan",
                "Michael J. Black",
                "Otmar Hilliges",
                "Gerard Pons-Moll."
            ],
            "title": "Deep Inertial Poser: Learning to Reconstruct Human Pose from Sparse Inertial Measurements in Real Time",
            "venue": "ACM TOG 37, 6 (12 2018).",
            "year": 2018
        },
        {
            "authors": [
                "Yifeng Jiang",
                "Yuting Ye",
                "Deepak Gopinath",
                "JungdamWon",
                "Alexander WWinkler",
                "C Karen Liu."
            ],
            "title": "Transformer Inertial Poser: Real-time Human Motion Reconstruction from Sparse IMUs with Simultaneous Terrain Generation",
            "venue": "journal = ACM Trans. Graph. (2022).",
            "year": 2022
        },
        {
            "authors": [
                "Angjoo Kanazawa",
                "Jason Y. Zhang",
                "Panna Felsen",
                "Jitendra Malik."
            ],
            "title": "Learning 3D Human Dynamics from Video",
            "venue": "Computer Vision and Pattern Recognition (CVPR).",
            "year": 2019
        },
        {
            "authors": [
                "Jongmin Kim",
                "Yeongho Seol",
                "Taesoo Kwon."
            ],
            "title": "Interactive multi-character motion retargeting",
            "venue": "Computer Animation and Virtual Worlds 32, 3-4 (2021), e2015.",
            "year": 2021
        },
        {
            "authors": [
                "Sunwoo Kim",
                "Maks Sorokin",
                "Jehee Lee",
                "Sehoon Ha."
            ],
            "title": "Human Motion Control of Quadrupedal Robots using Deep Reinforcement Learning",
            "venue": "Proceedings of Robotics: Science and Systems. New York, USA.",
            "year": 2022
        },
        {
            "authors": [
                "Ariel Kwiatkowski",
                "Eduardo Alvarado",
                "Vicky Kalogeiton",
                "C Karen Liu",
                "Julien Pettr\u00e9",
                "Michiel van de Panne",
                "Marie-Paule Cani."
            ],
            "title": "A survey on reinforcement learning methods in character animation",
            "venue": "Computer Graphics Forum, Vol. 41. Wiley Online Library, 613\u2013639.",
            "year": 2022
        },
        {
            "authors": [
                "Yoonsang Lee",
                "Sungeun Kim",
                "Jehee Lee."
            ],
            "title": "Data-driven biped control",
            "venue": "ACM SIGGRAPH 2010 papers. 1\u20138.",
            "year": 2010
        },
        {
            "authors": [
                "Libin Liu",
                "KangKang Yin",
                "Michiel van de Panne",
                "Tianjia Shao",
                "Weiwei Xu."
            ],
            "title": "Sampling-based contact-rich motion control",
            "venue": "ACM SIGGRAPH 2010 papers. 1\u201310.",
            "year": 2010
        },
        {
            "authors": [
                "Matthew Loper",
                "Naureen Mahmood",
                "Javier Romero",
                "Gerard Pons-Moll",
                "Michael J. Black."
            ],
            "title": "SMPL: A Skinned Multi-Person Linear Model",
            "venue": "ACM TOG 34, 6 (Oct. 2015), 248:1\u2013248:16.",
            "year": 2015
        },
        {
            "authors": [
                "Viktor Makoviychuk",
                "Lukasz Wawrzyniak",
                "Yunrong Guo",
                "Michelle Lu",
                "Kier Storey",
                "Miles Macklin",
                "David Hoeller",
                "Nikita Rudin",
                "Arthur Allshire",
                "Ankur Handa",
                "Gavriel State."
            ],
            "title": "Isaac Gym: High Performance GPU-Based Physics Simulation For Robot Learning",
            "venue": "https://doi.org/10.48550/ARXIV.2108.10470",
            "year": 2021
        },
        {
            "authors": [
                "Meta."
            ],
            "title": "The World Beyond",
            "venue": "https://github.com/oculus-samples/Unity-TheWorldBeyond.",
            "year": 2023
        },
        {
            "authors": [
                "Jean-S\u00e9bastien Monzani",
                "Paolo Baerlocher",
                "Ronan Boulic",
                "Daniel Thalmann."
            ],
            "title": "Using an intermediate skeleton and inverse kinematics for motion retargeting",
            "venue": "Computer Graphics Forum, Vol. 19. Wiley Online Library, 11\u201319.",
            "year": 2000
        },
        {
            "authors": [
                "Soohwan Park",
                "Hoseok Ryu",
                "Seyoung Lee",
                "Sunmin Lee",
                "Jehee Lee"
            ],
            "title": "Learning predict-and-simulate policies from unorganized human motion data",
            "venue": "ACM Transactions on Graphics (TOG) 38,",
            "year": 2019
        },
        {
            "authors": [
                "Adam Paszke",
                "Sam Gross",
                "Francisco Massa",
                "Adam Lerer",
                "James Bradbury",
                "Gregory Chanan",
                "Trevor Killeen",
                "Zeming Lin",
                "Natalia Gimelshein",
                "Luca Antiga"
            ],
            "title": "Pytorch: An imperative style, high-performance deep learning library",
            "venue": "Advances in neural information processing systems",
            "year": 2019
        },
        {
            "authors": [
                "Xue Bin Peng",
                "Pieter Abbeel",
                "Sergey Levine",
                "Michiel van de Panne."
            ],
            "title": "Deepmimic: Example-guided deep reinforcement learning of physics-based character skills",
            "venue": "ACM Transactions on Graphics (TOG) 37, 4 (2018), 1\u201314.",
            "year": 2018
        },
        {
            "authors": [
                "Xue Bin Peng",
                "Pieter Abbeel",
                "Sergey Levine",
                "Michiel van de Panne."
            ],
            "title": "DeepMimic: Example-guided Deep Reinforcement Learning of Physics-based Character Skills",
            "venue": "ACM Trans. Graph. 37, 4, Article 143 (July 2018), 143:1\u2013143:14 pages.",
            "year": 2018
        },
        {
            "authors": [
                "Xue Bin Peng",
                "Glen Berseth",
                "KangKang Yin",
                "Michiel van de Panne."
            ],
            "title": "Deeploco: Dynamic locomotion skills using hierarchical deep reinforcement learning",
            "venue": "ACM Transactions on Graphics (TOG) 36, 4 (2017), 1\u201313.",
            "year": 2017
        },
        {
            "authors": [
                "Lerrel Pinto",
                "Marcin Andrychowicz",
                "Peter Welinder",
                "Wojciech Zaremba",
                "Pieter Abbeel."
            ],
            "title": "Asymmetric Actor Critic for Image-Based Robot Learning",
            "venue": "Robotics (Robotics: Science and Systems), Hadas Kress-Gazit, Siddhartha S. Srinivasa, Tom Howard, and Nikolay Atanasov (Eds.). MIT Press Journals. https://doi.org/10.15607/RSS.2018.XIV.008 Publisher Copyright: \u00a9 2018, MIT Press Journals. All rights reserved.; 14th Robotics: Science and Systems, RSS 2018 ; Conference date: 26-06-2018 Through 30-06-2018.",
            "year": 2018
        },
        {
            "authors": [
                "Daniele Reda",
                "Tianxin Tao",
                "Michiel van de Panne."
            ],
            "title": "Learning to Locomote: Understanding How Environment Design Matters for Deep Reinforcement Learning",
            "venue": "Proc. ACM SIGGRAPH Conference on Motion, Interaction and Games.",
            "year": 2020
        },
        {
            "authors": [
                "Davis Rempe",
                "Tolga Birdal",
                "Aaron Hertzmann",
                "Jimei Yang",
                "Srinath Sridhar",
                "Leonidas J Guibas."
            ],
            "title": "Humor: 3d human motion model for robust pose estimation",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision. 11488\u201311499.",
            "year": 2021
        },
        {
            "authors": [
                "Yu Rong",
                "Takaaki Shiratori",
                "Hanbyul Joo."
            ],
            "title": "FrankMocap: A Monocular 3D Whole-Body Pose Estimation System via Regression and Integration",
            "venue": "IEEE International Conference on Computer Vision Workshops.",
            "year": 2021
        },
        {
            "authors": [
                "John Schulman",
                "Filip Wolski",
                "Prafulla Dhariwal",
                "Alec Radford",
                "Oleg Klimov."
            ],
            "title": "Proximal Policy Optimization Algorithms",
            "venue": "https://doi.org/10.48550/ARXIV.1707.06347",
            "year": 2017
        },
        {
            "authors": [
                "Yeongho Seol",
                "Carol O\u2019Sullivan",
                "Jehee Lee"
            ],
            "title": "Creature features: online motion puppetry for non-human characters",
            "venue": "In Proceedings of the 12th ACM SIGGRAPH/Eurographics Symposium on Computer Animation",
            "year": 2013
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141 ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is All you Need",
            "venue": "Advances in Neural Information Processing Systems, Vol. 30.",
            "year": 2017
        },
        {
            "authors": [
                "Ruben Villegas",
                "Duygu Ceylan",
                "Aaron Hertzmann",
                "Jimei Yang",
                "Jun Saito."
            ],
            "title": "Contact-Aware Retargeting of Skinned Motion",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision. 9720\u20139729.",
            "year": 2021
        },
        {
            "authors": [
                "Timo von Marcard",
                "Bodo Rosenhahn",
                "Michael Black",
                "Gerard Pons-Moll."
            ],
            "title": "Sparse Inertial Poser: Automatic 3D Human Pose Estimation from Sparse IMUs",
            "venue": "Computer Graphics Forum 36(2), Proceedings of the 38th Annual Conference of the European Association for Computer Graphics (Eurographics) (2017), 349\u2013360.",
            "year": 2017
        },
        {
            "authors": [
                "Kevin Wampler",
                "Zoran Popovi\u0107",
                "Jovan Popovi\u0107."
            ],
            "title": "Generalizing locomotion style to new animals with inverse optimal regression",
            "venue": "ACM Transactions on Graphics (TOG) 33, 4 (2014), 1\u201311.",
            "year": 2014
        },
        {
            "authors": [
                "Tingwu Wang",
                "Renjie Liao",
                "Jimmy Ba",
                "Sanja Fidler."
            ],
            "title": "Nervenet: Learning structured policy with graph neural networks",
            "venue": "International conference on learning representations.",
            "year": 2018
        },
        {
            "authors": [
                "Alexander Winkler",
                "Jungdam Won",
                "Yuting Ye."
            ],
            "title": "QuestSim: Human Motion Tracking from Sparse Sensors with Simulated Avatars",
            "venue": "SIGGRAPH Asia 2022 Conference Papers. 1\u20138.",
            "year": 2022
        },
        {
            "authors": [
                "Jungdam Won",
                "Deepak Gopinath",
                "Jessica Hodgins."
            ],
            "title": "A scalable approach to control diverse behaviors for physically simulated characters",
            "venue": "ACM Transactions on Graphics (TOG) 39, 4 (2020), 33\u20131.",
            "year": 2020
        },
        {
            "authors": [
                "Jungdam Won",
                "Jehee Lee."
            ],
            "title": "Learning body shape variation in physics-based characters",
            "venue": "ACM Transactions on Graphics (TOG) 38, 6 (2019), 1\u201312.",
            "year": 2019
        },
        {
            "authors": [
                "Jungdam Won",
                "Jongho Park",
                "Kwanyu Kim",
                "Jehee Lee."
            ],
            "title": "How to train your dragon: example-guided control of flapping flight",
            "venue": "ACM Transactions on Graphics (TOG) 36, 6 (2017), 1\u201313.",
            "year": 2017
        },
        {
            "authors": [
                "Yuanlu Xu",
                "Song-Chun Zhu",
                "Tony Tung."
            ],
            "title": "Denserac: Joint 3d pose and shape estimation by dense render-andcompare",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision. 7760\u20137770.",
            "year": 2019
        },
        {
            "authors": [
                "Katsu Yamane",
                "Yuka Ariki",
                "Jessica Hodgins."
            ],
            "title": "Animating Non-Humanoid Characters with Human Motion Data",
            "venue": "Eurographics/ ACM SIGGRAPH Symposium on Computer Animation, MZoran Popovic and Miguel Otaduy (Eds.). The Eurographics Association. https://doi.org/10.2312/SCA/SCA10/169-178",
            "year": 2010
        },
        {
            "authors": [
                "Yuting Ye",
                "C Karen Liu."
            ],
            "title": "Optimal feedback control for character animation using an abstract model",
            "venue": "ACM SIGGRAPH 2010 papers. 1\u20139.",
            "year": 2010
        },
        {
            "authors": [
                "Yongjing Ye",
                "Libin Liu",
                "Lei Hu",
                "Shihong Xia."
            ],
            "title": "Neural3Points: Learning to Generate Physically Realistic Full-body Motion for Virtual Reality Users",
            "venue": "Computer Graphics Forum (2022). https://doi.org/10.1111/cgf.14634 Proc. ACM Comput. Graph. Interact. Tech., Vol. 6, No. 2, Article . Publication date: August 2023.",
            "year": 2022
        },
        {
            "authors": [
                "Reda et al. KangKang Yin",
                "Kevin Loken",
                "Michiel Van de Panne"
            ],
            "title": "Simbicon: Simple biped locomotion control",
            "venue": "ACM Transactions on Graphics (TOG) 26,",
            "year": 2007
        }
    ],
    "sections": [
        {
            "text": "Physics-based Motion Retargeting from Sparse Inputs\nDANIELE REDA, University of British Columbia, Canada JUNGDAMWON, Seoul National University, South Korea YUTING YE, Reality Labs Research, Meta, United States of America MICHIEL VAN DE PANNE, University of British Columbia, Canada ALEXANDER WINKLER, Reality Labs Research, Meta, United States of America\nFig. 1. Our method uses only a headset and controller pose as input to generate a physically-valid pose for a variety of characters in real-time.\nAvatars are important to create interactive and immersive experiences in virtual worlds. One challenge in animating these characters to mimic a user\u2019s motion is that commercial AR/VR products consist only of a headset and controllers, providing very limited sensor data of the user\u2019s pose. Another challenge is that an avatar might have a different skeleton structure than a human and the mapping between them is unclear. In this work we address both of these challenges. We introduce a method to retarget motions in real-time from sparse human sensor data to characters of various morphologies. Our method uses reinforcement learning to train a policy to control characters in a physics simulator. We only require human motion capture data for training, without relying on artist-generated animations for each avatar. This allows us to use large motion capture datasets to train general policies that can track unseen users from real and sparse data in real-time. We demonstrate the feasibility of our approach on three characters with different skeleton structure: a dinosaur, a mouse-like creature and a human.We show that the avatar poses often match the user surprisingly well, despite having no sensor information of the lower body available. We discuss and ablate the important components in our framework, specifically the kinematic retargeting step, the imitation, contact and action reward as well as our asymmetric actor-critic observations. We further explore the robustness of our method in a variety of settings including unbalancing, dancing and sports motions.\nAuthors\u2019 addresses: Daniele Reda, dreda@cs.ubc.ca, University of British Columbia, Canada; Jungdam Won, Seoul National University, South Korea; Yuting Ye, Reality Labs Research, Meta, United States of America; Michiel van de Panne, University of British Columbia, Canada; Alexander Winkler, winklera@meta.com, Reality Labs Research, Meta, United States of America.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. \u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM. 2577-6193/2023/8-ART $15.00 https://doi.org/10.1145/3606928\nProc. ACM Comput. Graph. Interact. Tech., Vol. 6, No. 2, Article . Publication date: August 2023.\nar X\niv :2\n30 7.\n01 93\n8v 1\n[ cs\n.C V\n] 4\nJ ul\n2 02\n3\nAdditional Key Words and Phrases: retargeting, reinforcement learning, physics-based simulation, computer animation\nACM Reference Format: Daniele Reda, Jungdam Won, Yuting Ye, Michiel van de Panne, and Alexander Winkler. 2023. Physics-based Motion Retargeting from Sparse Inputs. Proc. ACM Comput. Graph. Interact. Tech. 6, 2 (August 2023), 19 pages. https://doi.org/10.1145/3606928"
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Augmented and Virtual Reality (AR/VR) has the potential to provide rich forms of self-expression. By using human characters it is easier to accurately reflect the motions of a user. However, many users might want to portray themselves via non-human characters. Games with non-human player characters already demonstrate the great appeal of this type of embodiment, albeit one that works within the limited immersion afforded by current gaming input devices and displays. How can we best allow users to embody themselves in non-human characters using current AR/VR systems? Our work seeks to make progress on this question. This entails multiple challenges, in particular: (a) AR/VR systems provide only sparse information regarding the pose of the user, obtained from a head-mounted device (HMD) and two controllers. (b) The target character may have significantly different dimensions and body types, as shown in Figure 1; and (c) Kinematic animation, including that resulting from kinematic retargeting, often lacks physical plausibility, producing movements that lack a feeling of weight.\nWe propose a method to address these challenges. In particular, we develop an imitation-based reinforcement learning (RL) method that uses the sparse sensor input of a user to drive a physicsbased simulation of the target character. This directly takes into account the physical properties of the given character, such as the heavy tail of a dinosaur or the short-legs of a mouse character, as shown in Figure 1. We only require human motion capture data for training, without relying on artist-generated animations for each avatar. This allows us to use large motion capture datasets to train general policies that can track unseen users from real and sparse data in real-time. We identify ingredients as being important to successful retargeting in this setting, including foot contact rewards, sparse mapping of key features for retargeting, and suitable reward terms that offer further style control. Many of the pieces that we rely on exist elsewhere in the literature. Our primary contribution lies with bringing them together in a way that enables a new retargeting capability well-suited to current AR/VR systems. We are the first to show a framework that works with real data from sparse sensors in real time while producing high-quality motions for non-human characters. We validate our design choices through a variety of ablations."
        },
        {
            "heading": "2 RELATEDWORK",
            "text": "In this literature review we focus on the most relevant works in motion tracking, retargeting, and physics-based control."
        },
        {
            "heading": "2.1 Human Motion Tracking",
            "text": "Many solutions exist for full-body tracking of human motion, varying in their choice of sensors, the number of sensors, and their placement. Optical marker-based systems with external cameras remain the most common choice for applications requiring high accuracy, e.g., [Vicon 2022]. Markerless and vision-based approaches rely on cameras alone to generate full body poses. Common approaches leverage human body models such as SMPL as a pose prior [Kanazawa et al. 2019; Loper et al. 2015; Rong et al. 2021; Xu et al. 2019], using extracted keypoints or correspondences from the images [Cao et al. 2019; G\u00fcler et al. 2018], or use physics-based priors, e.g., [Rempe et al. 2021] Wearable sensors are another common choice, relying on sensors attached on the user\u2019s body, such\nProc. ACM Comput. Graph. Interact. Tech., Vol. 6, No. 2, Article . Publication date: August 2023.\nas Inertial Measurement Unit (IMU) devices, e.g., [Huang et al. 2018; Jiang et al. 2022; von Marcard et al. 2017]. When using AR/VR devices, systems are further limited by the sparse sensors available. Most commonly available units are comprised of 3 tracker devices: a head-mounted device (HMD) and two controllers, one for each hand. As a human motion tracking device, these are handicapped by the lack of sensory information regarding the lower body and legs, which are essential to synthesizing believable full-body motion. Multiple methods have been proposed to address this, using transformers [Jiang et al. 2022; Vaswani et al. 2017], VAEs [Dittadi et al. 2021] and normalizing flows generative models [Aliakbarian et al. 2022]. Being kinematic-based approaches, however, these methods do not enforce physical properties and thus suffer from motion artifacts such as foot-skating and jitter. Physics-based approaches have also recently been proposed [Winkler et al. 2022; Ye et al. 2022]. These both make use of reinforcement learning and physics to learn general and robust policies that drive full-body avatars, conditioned on input from a VR device. These are closest to the work we present in this paper, and have great promise, although come with their own limitations. The Neural3Points method [Ye et al. 2022] is specific to a single user and uses auxiliary losses and an intermediate full-body pose predictor. Relatedly, Winkler et al. [2022] proposes a more direct approach that is able to control a simulated human avatar and generalizes to users of different heights and multiple type of motions. Our work generalizes the method of Winkler et al. [2022] in two important ways: (1) we learn physics-based retargeting to characters having different morphologies, and (2) we enable real-time retargeting."
        },
        {
            "heading": "2.2 Retargeting Motions",
            "text": "The motion retargeting problem is that of remapping motion from a source character or skeleton, often driven by motion capture data, to another character of possibly different dimensions. This is a long-standing problem for which many solutions have been proposed. Arguably the most challenging version of this problem arises when the source and target characters may differ significantly in terms of their morphology and skeleton, as is also the case for our work.\nProc. ACM Comput. Graph. Interact. Tech., Vol. 6, No. 2, Article . Publication date: August 2023.\nKinematic retargeting methods often approach the problem by allowing the user to specify directly, or alternatively to learn via examples, a model for source-to-target pose correpondences, e.g., [Monzani et al. 2000; Seol et al. 2013; Yamane et al. 2010]. This creates a puppetry system, where target motions can be further cleaned to respect contacts with the help of inverse kinematics. Kinematic motion deformation approaches can be used to adapt multiple characters trajectories for motions involving coordination such as moving boxes [Kim et al. 2021]. Recent work proposes a kinematic method to learn how to retarget without requiring any explicit pairing between motions [Aberman et al. 2020], and this is also demonstrated to work on skeletons with very different proportions. Other recent work examines how to learn efficient kinematic motion retargeting for human-like skeletons while preserving contact constraints, such as when hands and arms have self-contact with the body [Villegas et al. 2021]. Physics-based retargeting methods aim to produce a physics-based simulation of the output motion, which results in crisp contacts and physically-plausible motion of the target character. An offline approach to motion retargeting using spacetime trajectory optimization is presented in Al Borno et al. [2018]. The final output uses LQR trees, and thus the given motions can cope with some perturbations. A method is recently proposed for using interactive human motion to drive the motion of a quadruped robot [Kim et al. 2022]. A curated dataset of matching pairs of human-and-robot motions is used to develop relevant kinematic mappings for particular motions or tasks. A deep-RL policy is then learned that can track the target kinematic motions in real time, enabling a form of real-time human-to-real-robot puppetry. In our setting, we assume significantly sparser user input and motion specifications."
        },
        {
            "heading": "2.3 Physics-based Character Simulation",
            "text": "Controllers for physics-based characters have been extensively explored. The ability to imitate reference motions was first demonstrated to varying extents in a number of papers over the past 15 years, e.g., [Coros et al. 2010; Geijtenbeek et al. 2012; Lee et al. 2010; Liu et al. 2010; Ye and Liu 2010; Yin et al. 2007]. These methods often incorporated some iterative optimization to adapt to a specific motion and used a simple control law to provide robust balance feedback. Some of these methods were also adapted to produce motions for non-human characters, e.g., [Geijtenbeek et al. 2013; Wampler et al. 2014].\nNeural network policies, trained via deep reinforcement learning (RL), provide new capabilities to learn new skills from scratch, or to imitate artist-provided motions or motion capture clips, e.g., [Peng et al. 2018a, 2017; Won et al. 2017], including demonstrations for non-human characters. More recent methods provide more flexibility in sequencing motions for basketball [Park et al. 2019] or, more generally, to track online streams of motion capture data [Bergamin et al. 2019; Chentanez et al. 2018; Fussell et al. 2021; Won et al. 2020]. Control policies have also been learned which are conditioned on not only the desired motion, but also the specific morphology of a simulated character, which can then even be changed at run time [Won and Lee 2019]. We further refer the reader to a recent survey of RL-related animation methods [Kwiatkowski et al. 2022]. We build on the foundations provided above for our specific problem, namely how to retarget from sparse (and therefore potentially highly ambiguous) input data to a non-human physics-based character with very different dimensions and proportions."
        },
        {
            "heading": "3 METHOD",
            "text": "An overview of our system is shown in Figure 2. We use reinforcement learning to learn a policy that generates torques for a physics simulator. During training, we use human motion capture data to both synthesize HMD and controllers data for the policy, and to build a reward training signal.\nProc. ACM Comput. Graph. Interact. Tech., Vol. 6, No. 2, Article . Publication date: August 2023.\nIn the following we give an overview of reinforcement learning and then describe each component in detail."
        },
        {
            "heading": "3.1 Reinforcement Learning",
            "text": "We use deep reinforcement learning (RL) to learn a retargeting policy for each character. In RL, at each time step \ud835\udc61 , the control policy reacts to an environment state \ud835\udc60\ud835\udc61 by performing an action \ud835\udc4e\ud835\udc61 . Based on the action performed, the policy receives a reward signal \ud835\udc5f\ud835\udc61 = \ud835\udc5f (\ud835\udc60\ud835\udc61 , \ud835\udc4e\ud835\udc61 ). In deep RL, the control policy \ud835\udf0b\ud835\udf03 (\ud835\udc4e |\ud835\udc60) is a neural network. The goal of deep RL is to find the network parameters \ud835\udf03 which maximize the expected return defined as follows:\n\ud835\udc3d\ud835\udc45\ud835\udc3f (\ud835\udf03 ) = E [ \u221e\u2211\ufe01 \ud835\udc61=0 \ud835\udefe\ud835\udc61\ud835\udc5f (\ud835\udc60\ud835\udc61 , \ud835\udc4e\ud835\udc61 ) ] , (1)\nwhere \ud835\udefe \u2208 [0, 1) is the discount factor. Tuning \ud835\udefe affects the importance we give to future states. We solve this optimization problem using the proximal policy optimization (PPO) algorithm [Schulman et al. 2017], a policy gradient actor-critic algorithm. A review of PPO algorithm is provided in Appendix B."
        },
        {
            "heading": "3.2 Characters",
            "text": "We demonstrate our retargeting solution on three characters with unique features: Oppy [Meta 2023] is a mouse with a short lower body, a big head, big ears and a tail; Dino is a tall dinosaur, with a long and heavy tail and head, and short arms; Jesse is a human-like cartoon character with a skeleton structure similar to the mocap data. Figure 3 shows a visual representation of the characters and Table 1 details the structure of their skeletons.\nProc. ACM Comput. Graph. Interact. Tech., Vol. 6, No. 2, Article . Publication date: August 2023."
        },
        {
            "heading": "3.3 Observations",
            "text": "The observation contains two parts: simulated character data \ud835\udc5c\ud835\udc61,\ud835\udc60\ud835\udc56\ud835\udc5a and user\u2019s sparse sensor data \ud835\udc5c\ud835\udc61,\ud835\udc62\ud835\udc60\ud835\udc52\ud835\udc5f .\n\ud835\udc5c\ud835\udc61 = [\ud835\udc5c\ud835\udc61,\ud835\udc60\ud835\udc56\ud835\udc5a, \ud835\udc5c\ud835\udc61\u22121,\ud835\udc62\ud835\udc60\ud835\udc52\ud835\udc5f , \ud835\udc5c\ud835\udc61,\ud835\udc62\ud835\udc60\ud835\udc52\ud835\udc5f ] (2) \ud835\udc5c\ud835\udc61,\ud835\udc60\ud835\udc56\ud835\udc5a = [\ud835\udc5c\ud835\udc60\ud835\udc56\ud835\udc5a,\ud835\udc5e, \ud835\udc5c\ud835\udc60\ud835\udc56\ud835\udc5a, \u00a4\ud835\udc5e, \ud835\udc5c\ud835\udc60\ud835\udc56\ud835\udc5a,\ud835\udc65 , \ud835\udc5c\ud835\udc60\ud835\udc56\ud835\udc5a,\ud835\udc45] (3) \ud835\udc5c\ud835\udc61,\ud835\udc62\ud835\udc60\ud835\udc52\ud835\udc5f = [\u210e\ud835\udc61 , \ud835\udc59\ud835\udc61 , \ud835\udc5f\ud835\udc61 , \ud835\udc45\u210e,\ud835\udc61 , \ud835\udc45\ud835\udc59,\ud835\udc61 , \ud835\udc45\ud835\udc5f,\ud835\udc61 ] (4)\nThe simulated character\u2019s state is fully observable in the simulation. Therefore, even though the sensor signals is sparse, the policy can still rely on the full state of the simulated character. This observation consists of joint angles \ud835\udc5c\ud835\udc60\ud835\udc56\ud835\udc5a,\ud835\udc5e \u2208 R\ud835\udc57 and joint angle velocities \ud835\udc5c\ud835\udc60\ud835\udc56\ud835\udc5a, \u00a4\ud835\udc5e \u2208 R\ud835\udc57 of all degrees of freedom \ud835\udc57 of the character. We also provide Cartesian positions \ud835\udc5c\ud835\udc60\ud835\udc56\ud835\udc5a,\ud835\udc65 \u2208 R\ud835\udc59\u00d73 and orientations \ud835\udc5c\ud835\udc60\ud835\udc56\ud835\udc5a,\ud835\udc45 \u2208 R\ud835\udc59\u00d76 of a subset \ud835\udc59 of links of the character. The orientations consist of the first two columns of their rotation matrices. All positions and orientations are expressed with respect to a coordinate frame located on the floor below the character which rotates according to the character heading direction. This is useful to make the controller agnostic to the heading direction.\nThe sensor data, either coming from the real device or synthetically generated from the training data (described in subsection 3.4), consists of the position and orientation of the HMD \u210e, the left controller \ud835\udc59 and the right controller \ud835\udc5f . Positions and orientations are expressed in the same coordinate system as the simulated character observations. To allow the policy to infer velocities, we provide it two consecutive sensor observations [\ud835\udc5c\ud835\udc61\u22121,\ud835\udc62\ud835\udc60\ud835\udc52\ud835\udc5f , \ud835\udc5c\ud835\udc61,\ud835\udc62\ud835\udc60\ud835\udc52\ud835\udc5f ].\nInspired by Pinto et al. [2018], we use asymmetric observations. At training time we augment the value function observation by providing the full human mocap pose and future human mocap state information. This complete view of the state allows the value function to better estimate the returns. The better the return estimate, the easier it is for the policy to learn. We are allowed to provide this mocap state information, because the value function is required only for training. Real-time inference still only relies on the policy, which uses the sparse sensor input. We ablate this in subsection 5.3 and find that is essential for sparse real time retargeting."
        },
        {
            "heading": "3.4 Synthetic Training Data",
            "text": "During training, we require HMD and controller data for the observation paired with kinematic poses for each character \ud835\udc60\ud835\udc61,\ud835\udc58\ud835\udc56\ud835\udc5b from which the reward \ud835\udc5f\ud835\udc61 is computed. To synthetically generate the HMD and controller data we offset the mocap head and wrist joints to emulate the position and orientation of HMD, left and right controllers as if the subjects were equipped with an AR/VR device.\nProc. ACM Comput. Graph. Interact. Tech., Vol. 6, No. 2, Article . Publication date: August 2023.\nImportantly, our system does not require artist-generated animations for each specific character as training data, which would be infeasible to create with the diversity and quantity we require. Instead we reuse existing human motion capture data \ud835\udc60\ud835\udc54\ud835\udc61 and perform a rough kinematic retargeting \ud835\udc60\ud835\udc58\ud835\udc56\ud835\udc5b to the morphology of the simulated character (Figure 4). In this step, we manually match selected joint angles of the human to conceptually similar joints of the creature. For joints where no correspondence can be found, we just set them to their default pose (e.g. ears and tails). This provides a rough estimate of the creature\u2019s motion. However, this motion has many artifacts, such a feet sliding due to different leg lengths, self-collisions, floor collisions, and no motion of the tail and ears. Nonetheless, we can still use it as a reward signal to train our simulated character. The physical constraints imposed by the simulation then remove remaining artifacts. Importantly, after the simulated character is trained, it is driven only by a headset and controllers, without requiring any full-body information of the user or any kinematic retargeting."
        },
        {
            "heading": "3.5 Reward",
            "text": "The goal for the simulated character is to imitate the human motion as closely as possible, while respecting all the constraints imposed by physics. Our reward function includes a component for imitation, contact, and action regularization:\n\ud835\udc5f\ud835\udc61 = \ud835\udc5f\ud835\udc61 (imitation) + \ud835\udc5f\ud835\udc61 (contact) + \ud835\udc5f\ud835\udc61 (action) (5) \ud835\udc5f\ud835\udc61 (imitation) = \ud835\udc5f\ud835\udc61 (\ud835\udc5e) + \ud835\udc5f\ud835\udc61 ( \u00a4\ud835\udc5e) + \ud835\udc5f\ud835\udc61 (\ud835\udc65) + \ud835\udc5f\ud835\udc61 ( \u00a4\ud835\udc65) + \ud835\udc5f\ud835\udc61 (orientation) (6)\n\ud835\udc5f\ud835\udc61 (action) = \ud835\udc5f\ud835\udc61 (action diff) + \ud835\udc5f\ud835\udc61 (action min). (7)\nEach of the reward terms is expressed using a weighted Gaussian kernel:\n\ud835\udc5f\ud835\udc61 (\ud835\udc60) = \ud835\udc64\ud835\udc60e\u2212\ud835\udc58\ud835\udc60\ud835\udc51 (\ud835\udc60\ud835\udc61,\ud835\udc60\ud835\udc56\ud835\udc5a,\ud835\udc60\ud835\udc61,\ud835\udc58\ud835\udc56\ud835\udc5b ) (8)\nwhere for each term only the specific component of the state \ud835\udc60 is considered and \ud835\udc51 (\ud835\udc60\ud835\udc60\ud835\udc56\ud835\udc5a, \ud835\udc60\ud835\udc58\ud835\udc56\ud835\udc5b) represent the distance metric between the simulated and kinematic components of the state, \ud835\udc58 is the sensitivity of the Gaussian kernel, and\ud835\udc64 is the weight of the reward component. Parameter values and details of the distance metrics for each term are provided in Appendix A.\nProc. ACM Comput. Graph. Interact. Tech., Vol. 6, No. 2, Article . Publication date: August 2023.\n3.5.1 Imitation Reward. This reward matches the available information between the simulated character \ud835\udc60\ud835\udc60\ud835\udc56\ud835\udc5a and the kinematically retargeted ground truth pose \ud835\udc60\ud835\udc58\ud835\udc56\ud835\udc5b . The five terms represent a weighted sum of the difference between the matching joint angles (\ud835\udc5e), joint angle velocities ( \u00a4\ud835\udc5e), Cartesian coordinate positions (\ud835\udc65) and velocities ( \u00a4\ud835\udc65), and orientation. The imitation reward term captures the degrees of supervision we want to transfer between human motion data and the simulated character. For clarity, Equation 6 is the general form which includes all possible terms, but the way they are used differs according to each character. The less supervision the imitation term provides, the more we rely on physics and the other components to generate a sensible motion. Depending on the quality of our kinematically retargeted pose, we can choose which of the aspects of the pose we want the simulated character to imitate more closely. The least amount of supervision consists in only tracking their root position, which according to our experiments does not produce high-quality motions. On the other extreme, we also do not want to track every aspect of the kinematically retargeted pose. For example there is no tail motion in the human mocap data, so the kinematically retargeted pose has all tails set to a stiff default pose. However, a simulated character might want to move the tail to achieve balance and smoother motion. So we do not require these parts of the skeleton to imitate the kinematic pose. Orientations are skeleton independent, so we rely on the actual human mocap data, not the kinematically retargeted pose to formulate the orientation rewards. We always formulate a reward that matches the characters root with the human mocap root, as well as the characters head orientation with the human head orientation. Ablations without these terms are provided in subsection 4.3.\n3.5.2 Contact Reward. The contact reward is a boolean value that checks whether the simulated character\u2019s foot contact and the human\u2019s foot contact coincide. We estimate contact of the mocap data based on a velocity and height threshold. For the simulated character, we can directly access contact forces from the simulator and threshold those. In most cases the kinematically retargeted leg motion has a variety of artifacts, such as feet sliding or penetrating the ground. Imitating this pose is not physically-valid. Since this reward doesn\u2019t depend on the skeleton structure, it can be used for all bipedal characters equally and directly computed from human mocap. The contact reward is important to give further training supervision and generate the high-quality motions shown. Ablations are provided.\n3.5.3 Action Reward. The action reward is a regularization term to minimize total amount of energy consumed by the character. It consists of two terms that minimize the difference in torque between two subsequent actions and minimize the absolute action value and is defined as:\n\ud835\udc5f\ud835\udc61 (action diff) = 1 \ud835\udc41 \ud835\udc41\u2211\ufe01 \ud835\udc56 (\ud835\udc4e\ud835\udc61\u22121,\ud835\udc56 \u2212 \ud835\udc4e\ud835\udc61,\ud835\udc56 )2 (9)\n\ud835\udc5f\ud835\udc61 (action min) = 1 \ud835\udc41 \ud835\udc41\u2211\ufe01 \ud835\udc56 \ud835\udc4e2\ud835\udc61,\ud835\udc56 (10)\nwhere \ud835\udc41 is the total number of action values which the policy outputs. The purpose of these components is to incentivize overall lower energy movements and to minimize twitching with a smoother movement between poses."
        },
        {
            "heading": "3.6 Termination",
            "text": "As noted in multiple previous works [Peng et al. 2018b; Reda et al. 2020], early termination techniques are important for learning complex motions through reinforcement learning. We reset the environment when one of the following two termination conditions is satisfied: the character\nProc. ACM Comput. Graph. Interact. Tech., Vol. 6, No. 2, Article . Publication date: August 2023.\nenters an unrecoverable state, which we define as falling and touching the ground with the upper body, or when the character root position is more than 30cm apart from the scaled root of the motion capture data. Furthermore, to mitigate the imbalance of visiting and learning to retarget only the early parts of the motion trajectories, we reset the character every 500 steps. We randomly sample a pose from the human data and set the character using the kinematically retargeted pose."
        },
        {
            "heading": "3.7 Learning Control Policies",
            "text": "The policy for each simulated character outputs torque values in the range [\u22121, 1] which are then rescaled according to minimum and maximum torque values for each joint (provided in Appendix D). We find this to perform better and be more clear with respect to outputting PD target angles, as shown by previous works [Reda et al. 2020]. We train the policy with PPO and PyTorch auto differentiation software [Paszke et al. 2019; Schulman et al. 2017] and simulate physics with NVIDIA PhysX Isaac Gym physics simulator [Makoviychuk et al. 2021]. A complete set of hyperparameter details for reproducibility are summarized in Appendix C."
        },
        {
            "heading": "4 RESULTS",
            "text": "All experiments are performed on a single 12-core machine with one NVIDIA RTX 2070 GPU. All models are trained for 24 hours which translates to approximately 6 billion environment steps. We demonstrate comparable results with two different motion capture datasets. Our in-house mocap data consists of 4 hours of motion clips of 120 subjects. Specifically, the dataset contains 130 minutes of walking and 110 minutes of jogging. We also demonstrate robust and general results with the Ubisoft La Forge Animation (LaFAN1) dataset [Harvey et al. 2020], an open source motion capture dataset containing 5 subjects and 77 sequences. For the purposes of this work, we only considered actions themedWalk and Run, which consist of a total of 15 sequences and 74 minutes of data. We note that these motions are very different from the ones in our in-house dataset, containing diverse and hard behaviors and gaiting styles. At inference, we provide input to the policy with a Meta Quest headset and controllers device.\nProc. ACM Comput. Graph. Interact. Tech., Vol. 6, No. 2, Article . Publication date: August 2023.\n."
        },
        {
            "heading": "4.1 Real-time Retargeting with Headset and Controller",
            "text": "We thank the QuestSim [Winkler et al. 2022] authors for providing us with testing data and video references. With our method, we are able to control different characters in real time with only headset and controller information. Importantly, we are able to estimate the lower-body pose of the user from only three points in the upper body and correctly match the user action while transferring it to a character with a different morphology. Our virtual characters respect physical behaviors and do not suffer from jittering, foot sliding or penetration. Moreover, we are able to generalize to users not present in the training set and users of different heights. In Figure 7 we show a sequence in which all three characters are controlled by an unseen user."
        },
        {
            "heading": "4.2 Retargeting using only Headset",
            "text": "Some VR systems provide only a head-mounted device (HMD), without the two controllers. This provides an even more challenging domain, requiring the policy to predict a full-body pose and control a virtual character from a one-point input. Nonetheless, our trained models are robust to the lack of controller signal and are able to retarget real time user data from unseen users even from this extremely sparse input, albeit with a lower quality compared to before. We invite the reader to watch the video available in the supplementary material.\nProc. ACM Comput. Graph. Interact. Tech., Vol. 6, No. 2, Article . Publication date: August 2023."
        },
        {
            "heading": "4.3 Reward component ablations",
            "text": "Some reward components are essential to get good motions. Here we go through a few interesting examples.\n4.3.1 Contact Reward. The contact reward shapes the gait style of the character. Both Oppy and Dino display different locomotion behaviors when using this reward component. Furthermore, as the character size changes, more signal can be transferred to the simulated character. In Figure 5 we show Oppy in two different sizes. When Oppy\u2019s size matches the user, it performs the same gait style and distance motions; when it is smaller, by matching the correct gait style it will travel less distance, while it can perform a faster gait to keep up with the user, depending on the weighting of the reward components. Similarly, in Figure 7, the different frames show the matching gaits between the three characters and the user.\n4.3.2 Orientation Reward. Providing signal for mimicking head and root orientation is an essential component to support more fidelity in tracking user\u2019s head and overall movements. We show in Figure 6 how Dino without the head orientation component is unable to correctly move its head in the same way as the user. As shown in the supplementary video, both Oppy and Dino without head orientation reward component show the head wobbling left and right while walking. These characters have heavy heads needing learned control."
        },
        {
            "heading": "5 DISCUSSION",
            "text": "We discuss different capabilities and components of our system."
        },
        {
            "heading": "5.1 Physics-based control",
            "text": "Physics acts as a powerful helper in driving the motion of components with missing pose information, with the skeleton description as underlying prior. For the tail of Dino, the simulator affords several stylization options, i.e., whether we allow more joint mobility and passively actuate it through a PD controller with fixed-set input as secondary motion or we let the policy make active control decisions. In Figure 8 we show three examples, in which Dino\u2019s tail is fixed, passively actuated, or controlled by the learned policy. Tail and ears of Oppy are all treated as secondary dynamics. This stylization would not be possible in a kinematic retargeting setting."
        },
        {
            "heading": "5.2 Controlling the style",
            "text": "Our method is robust to different set of parameters. Once changed, most parameters still output a reasonable motion controller with different styles. As described in subsection 4.3, the contact\nProc. ACM Comput. Graph. Interact. Tech., Vol. 6, No. 2, Article . Publication date: August 2023.\nreward shapes the gait style of the character, and modified together with the size of the character would produce different gait styles.\nThe kinematic retargeting described in subsection 3.4 only requires a rough retargeting to produce sensible motions, as the physics dynamics correct the artifacts. Moreover, tuning the key joints for the kinematically retargeted motion produces an overall modification of the style. For example, it is possible to give Dino a more horizontal feeling, with the tail straight behind the back and not touching the ground, by tuning the spine parameter to be more bent over. An illustration of this tail is provided in Figure 4 and in the supplementary video, and noticeable difference can be observed compared to Figure 8."
        },
        {
            "heading": "5.3 Importance of asymmetric observations",
            "text": "During training we provide a richer observation to the value function compared to the one we provide to the policy. Specifically, while at inference the controller receives only real time sparse information (i.e. no future and no full-body pose), there is no need to constrain the value function since at training time this signal is available. In our experiments, we notice that the outcome of training a policy with a value function that receives no future and no full body pose, is an overall less robust policy. It is able to retarget easy walking examples coming from the training data, but it fails at harder motions like running and is incapable of generalizing to real data coming from an unseen user."
        },
        {
            "heading": "5.4 Quality of open-source datasets",
            "text": "We test our method with two different datasets, a 4 hour in-house dataset and a 74 minutes opensource dataset. While we notice that a larger and more diverse dataset improves the quality of the final motions, models trained with either of these datasets are robust and capable. Both are able to generalize to unseen users, and perform in real-time, even with headset-only sensory input."
        },
        {
            "heading": "6 CONCLUSIONS",
            "text": "We have presented a method to retarget a user\u2019s motion to simulated characters, in a challenging setting: the target characters can differ significantly in size and body morphology; we require a real-time remapping; and the mapping needs to be driven by the sparse motion data coming from an AR/VR device. We show that physics-based simulations, driven by asymmetric actor-critic RL policies, allow for effective retargeting in this difficult setting. The motions generated by the policies track those of the user while also being appropriate to the physics of the target character. We introduce a general reward description which allows for tuning of the degree of supervision and adapts to a range of character morphologies. Numerous ablations allow us to understand the impact of various parameters and design choices, including varying degrees of available tracking information, the impact of contact rewards, choices related to the secondary motion of tails and ears, and more. Our work still has a number of limitations. Our controller fails to track challenging motion sequences, where the user performs fast and dynamic movements or uncorrelated upper/lower body motions. In these scenarios, a kinematic-based controller acting directly in the pose space will still be able to produce a motion, albeit not of high quality, and it will be able to catch up as parts of the motion become easier by \"teleport\" between poses without correlation. Instead, our controller has to produce a correct sequence of joint torques to control the character and may suffer from compounding tracking errors until it fails. An approach that divides the pipeline in two stages, similar to Ye et al. [2022] where first a network predicts the full-body pose and then a high-frequency controller outputs torques, could allow to regain the advantages of kinematic-based systems when needed. While our framework allows richer forms of self expressions for users,\nProc. ACM Comput. Graph. Interact. Tech., Vol. 6, No. 2, Article . Publication date: August 2023.\nempowering them to control different kind of characters, we are only scratching the surface of the complexities that arise due to different target skeletons. Our characters are still bipeds. Increased character complexity might be achieved by supplying skeleton information to the policy [Won and Lee 2019], using graph neural networks to learn a flexible policy similarly to Wang et al. [2018], or training an auxiliary network to find a mapping between source and target skeletons.\nProc. ACM Comput. Graph. Interact. Tech., Vol. 6, No. 2, Article . Publication date: August 2023."
        },
        {
            "heading": "A REWARD DETAILS",
            "text": "Parameter values for each term of Equation 5 and Equation 8 are provided in Appendix A.\nGiven the state of the simulated character and the ground truth pose coming from the motion capture dataset, the distance metric for the different imitation reward components is formulated as a weighted sum of the Euclidean distance between the two values:\n\ud835\udc51 (\ud835\udc65\ud835\udc60\ud835\udc56\ud835\udc5a, \ud835\udc65\ud835\udc54\ud835\udc61 ) = \u2211\ufe01 \ud835\udc56 \ud835\udc64\ud835\udc56 \u2225\ud835\udc5e\ud835\udc65,\ud835\udc60\ud835\udc56\ud835\udc5a \u2212 \ud835\udc5e\ud835\udc65,\ud835\udc54\ud835\udc61 \u222522 (11)\nwhere \ud835\udc56 represent the joint angles or the link positions and weights vary according to the character. As described in subsection 3.5, the imitation reward defines the degree of supervision. As the two characters are closer alike, we can rely more on this reward. For Jesse, in fact, all joint weights are equal to 1. For Oppy and Dino, which have a different lower body size compared to a human, we rely more on the style reward for a good motion and decrease the weight of all lower body joints to 0.3. For link weights, for Oppy and Dino we set all weights to zero other than for the root, which is set to 1, for Jesse we track also end effectors.\nContact distance metric is also computed through the Euclidean distance between ground truth human motion data and simulated character data. We define that a human foot is in contact if its height is less than 20cm above the ground and the norm of its velocity is less than 0.4 m/s. For the simulated character, a force threshold of 1 N is set on the feet link. The orientation distance metric, given the two orientations in quaternions, first computes the composition of the ground truth quaternion with the inverse of the simulated quaternion. Then, takes the distance norm of its axis angle representation."
        },
        {
            "heading": "B PROXIMAL POLICY OPTIMIZATION",
            "text": "Let an experience tuple be \ud835\udc52\ud835\udc61 = (\ud835\udc5c\ud835\udc61 , \ud835\udc4e\ud835\udc61 , \ud835\udc5c\ud835\udc61+1, \ud835\udc5f\ud835\udc61 ) and a trajectory be \ud835\udf0f = {\ud835\udc520, \ud835\udc521, . . . , \ud835\udc52\ud835\udc47 }. We episodically collect trajectories for a fixed number of environment transitions and we use this data to\nProc. ACM Comput. Graph. Interact. Tech., Vol. 6, No. 2, Article . Publication date: August 2023.\ntrain the controller and the value function networks. The value function network approximates the expected future returns of each state, and is defined for a policy \ud835\udf0b as\n\ud835\udc49 \ud835\udf0b (\ud835\udc5c) = \ud835\udc38\ud835\udc5c0=\ud835\udc5c,\ud835\udc4e\ud835\udc61\u223c\ud835\udf0b ( \u00b7 |\ud835\udc5c\ud835\udc61 ) [ \u221e\u2211\ufe01 \ud835\udc61=0 \ud835\udefe \ud835\udc61\ud835\udc5f (\ud835\udc5c\ud835\udc61 , \ud835\udc4e\ud835\udc61 ) ] .\nThis function can be optimized using supervised learning due to its recursive nature:\n\ud835\udc49 \ud835\udf0b\ud835\udf03 (\ud835\udc5c\ud835\udc61 ) = \ud835\udefe \ud835\udc49 \ud835\udf0b\ud835\udf03 (\ud835\udc5c\ud835\udc61+1) + \ud835\udc5f\ud835\udc61 ,\nwhere\n\ud835\udc49 \ud835\udf0b\ud835\udf03 (\ud835\udc5c\ud835\udc47 ) = \ud835\udc5f\ud835\udc47 + \ud835\udefe\ud835\udc49 \ud835\udf0b\ud835\udf03\ud835\udc5c\ud835\udc59\ud835\udc51 (\ud835\udc5c\ud835\udc47+1).\nIn PPO, the value function is used for computing the advantage\n\ud835\udc34\ud835\udc61 = \ud835\udc49 \ud835\udf0b\ud835\udf03 \u2212\ud835\udc49 \ud835\udf0b\ud835\udf03\ud835\udc5c\ud835\udc59\ud835\udc51\nwhich is then used for training the policy by maximizing:\n\ud835\udc3f\ud835\udf0b (\ud835\udf03 ) = 1 \ud835\udc47 \ud835\udc47\u2211\ufe01 \ud835\udc61=1 min(\ud835\udf0c\ud835\udc61\ud835\udc34\ud835\udc61 , clip(\ud835\udf0c\ud835\udc61 , 1 \u2212 \ud835\udf16, 1 + \ud835\udf16)\ud835\udc34\ud835\udc61 ),\nwhere \ud835\udf0c\ud835\udc61 = \ud835\udf0b\ud835\udf03 (\ud835\udc4e\ud835\udc61 |\ud835\udc5c\ud835\udc61 ) / \ud835\udf0b\ud835\udf03\ud835\udc5c\ud835\udc59\ud835\udc51 (\ud835\udc4e\ud835\udc61 |\ud835\udc5c\ud835\udc61 ) is an importance sampling term used for calculating the expectation under the old policy \ud835\udf0b\ud835\udf03\ud835\udc5c\ud835\udc59\ud835\udc51 ."
        },
        {
            "heading": "C TRAINING PARAMETERS",
            "text": "Proc. ACM Comput. Graph. Interact. Tech., Vol. 6, No. 2, Article . Publication date: August 2023."
        },
        {
            "heading": "D TORQUE LIMITS",
            "text": ""
        }
    ],
    "title": "Physics-based Motion Retargeting from Sparse Inputs",
    "year": 2023
}