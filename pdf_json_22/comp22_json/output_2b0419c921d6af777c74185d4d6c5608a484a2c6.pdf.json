{
    "abstractText": "The invariance property across environments is at the heart of invariant learning methods for the Out-of-Distribution (OOD) Generalization problem. Although intuitively reasonable, strong assumptions on the availability and quality of environments have to be made for the learnability of the strict invariance property. Recently, to relax the requirements for environments empirically, some works propose to learn pseudo-environments for invariant learning. However, it could be misleading when pursuing strict invariance under latent heterogeneity, since the underlying invariance could have been violated during the pseudo-environment learning procedure. To this end, we come up with the distributional invariance property as a relaxed alternative to the strict invariance, which considers the invariance only among sub-populations down to a prescribed scale and allows a certain degree of variation. We reformulate the invariant learning problem under latent heterogeneity into a relaxed form that pursues the distributional invariance, based on which we propose our novel Distributionally Invariant Learning (DIL) framework as well as two implementations named DIL-MMD and DIL-KL. Theoretically, we provide the guarantees for the distributional invariance as well as bounds of the generalization error gap. Extensive experimental results validate the effectiveness of our proposed algorithms.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jiashuo Liu"
        },
        {
            "affiliations": [],
            "name": "Jiayun Wu"
        },
        {
            "affiliations": [],
            "name": "Jie Peng"
        },
        {
            "affiliations": [],
            "name": "Zheyan Shen"
        },
        {
            "affiliations": [],
            "name": "Peng Cui"
        }
    ],
    "id": "SP:6c181f20f0357a88954d68dcd462a547059b5b0e",
    "references": [
        {
            "authors": [
                "Kartik Ahuja",
                "Ethan Caballero",
                "Dinghuai Zhang",
                "Yoshua Bengio",
                "Ioannis Mitliagkas",
                "Irina Rish"
            ],
            "title": "Invariance principle meets information bottleneck for out-of-distribution generalization",
            "year": 2021
        },
        {
            "authors": [
                "Kartik Ahuja",
                "Karthikeyan Shanmugam",
                "Kush R. Varshney",
                "Amit Dhurandhar"
            ],
            "title": "Invariant risk minimization games",
            "venue": "In Proceedings of the 37th International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Peter L. Bartlett",
                "Shahar Mendelson"
            ],
            "title": "Rademacher and gaussian complexities: Risk bounds and structural results",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2002
        },
        {
            "authors": [
                "Elliot Creager",
                "J\u00f6rn-Henrik Jacobsen",
                "Richard S. Zemel"
            ],
            "title": "Environment inference for invariant learning",
            "venue": "Proceedings of the 38th International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Frances Ding",
                "Moritz Hardt",
                "John Miller",
                "Ludwig Schmidt"
            ],
            "title": "Retiring adult: New datasets for fair machine learning",
            "venue": "Advances in Neural Information Processing Systems: Annual Conference on Neural Information Processing Systems",
            "year": 2021
        },
        {
            "authors": [
                "John C. Duchi",
                "Hongseok Namkoong"
            ],
            "title": "Learning models with uniform performance via distributionally robust optimization",
            "year": 2018
        },
        {
            "authors": [
                "Charlie Frogner",
                "Sebastian Claici",
                "Edward Chien",
                "Justin Solomon"
            ],
            "title": "Incorporating unlabeled data into distributionally robust learning",
            "year": 1912
        },
        {
            "authors": [
                "Arthur Gretton",
                "Karsten M Borgwardt",
                "Malte J Rasch",
                "Bernhard Sch\u00f6lkopf",
                "Alexander Smola"
            ],
            "title": "A kernel two-sample test",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2012
        },
        {
            "authors": [
                "Weihua Hu",
                "Gang Niu",
                "Issei Sato",
                "Masashi Sugiyama"
            ],
            "title": "Does distributionally robust supervised learning give robust classifiers",
            "venue": "Proceedings of the 35th International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Pritish Kamath",
                "Akilesh Tangella",
                "Danica J. Sutherland",
                "Nathan Srebro"
            ],
            "title": "Does invariant risk minimization capture invariance",
            "venue": "The 24th International Conference on Artificial Intelligence and Statistics,",
            "year": 2021
        },
        {
            "authors": [
                "Masanori Koyama",
                "Shoichiro Yamaguchi"
            ],
            "title": "Out-of-distribution generalization with maximal invariant predictor",
            "year": 2008
        },
        {
            "authors": [
                "Kun Kuang",
                "Ruoxuan Xiong",
                "Peng Cui",
                "Susan Athey",
                "Bo Li"
            ],
            "title": "Stable prediction with model misspecification and agnostic distribution shift",
            "venue": "In The Thirty-Fourth AAAI Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Jiashuo Liu",
                "Zheyuan Hu",
                "Peng Cui",
                "Bo Li",
                "Zheyan Shen"
            ],
            "title": "Heterogeneous risk minimization",
            "venue": "Proceedings of the 38th International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Jiashuo Liu",
                "Zheyuan Hu",
                "Peng Cui",
                "Bo Li",
                "Zheyan Shen"
            ],
            "title": "Kernelized heterogeneous risk minimization",
            "venue": "Advances in Neural Information Processing Systems: Annual Conference on Neural Information Processing Systems",
            "year": 2021
        },
        {
            "authors": [
                "Jonas Peters",
                "Peter B\u00fchlmann",
                "Nicolai Meinshausen"
            ],
            "title": "Causal inference by using invariant prediction: identification and confidence intervals",
            "venue": "Journal of the Royal Statistical Society. Series B (Statistical Methodology),",
            "year": 2016
        },
        {
            "authors": [
                "Yong Ren",
                "Jun Zhu",
                "Jialian Li",
                "Yucen Luo"
            ],
            "title": "Conditional generative moment-matching networks",
            "venue": "Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems",
            "year": 2016
        },
        {
            "authors": [
                "Mateo Rojas-Carulla",
                "Bernhard Sch\u00f6lkopf",
                "Richard E. Turner",
                "Jonas Peters"
            ],
            "title": "Invariant models for causal transfer learning",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2018
        },
        {
            "authors": [
                "Elan Rosenfeld",
                "Pradeep Kumar Ravikumar",
                "Andrej Risteski"
            ],
            "title": "The risks of invariant risk minimization",
            "venue": "In 9th International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Shiori Sagawa",
                "Aditi Raghunathan",
                "Pang Wei Koh",
                "Percy Liang"
            ],
            "title": "An investigation of why overparameterization exacerbates spurious correlations",
            "venue": "In Proceedings of the 37th International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Aman Sinha",
                "Hongseok Namkoong",
                "John C. Duchi"
            ],
            "title": "Certifying some distributional robustness with principled adversarial training",
            "venue": "In 6th International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "Le Song",
                "Jonathan Huang",
                "Alexander J. Smola",
                "Kenji Fukumizu"
            ],
            "title": "Hilbert space embeddings of conditional distributions with applications to dynamical systems",
            "venue": "Proceedings of the 26th Annual International Conference on Machine Learning,",
            "year": 2009
        },
        {
            "authors": [
                "Haotian Ye",
                "Chuanlong Xie",
                "Tianle Cai",
                "Ruichen Li",
                "Zhenguo Li",
                "Liwei Wang"
            ],
            "title": "Towards a theoretical framework of out-of-distribution generalization",
            "venue": "Advances in Neural Information Processing Systems: Annual Conference on Neural Information Processing Systems",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Traditional machine learning algorithms with empirical risk minimization (ERM) suffer from vulnerability when exposed to data drawn out of the distribution. To mitigate the out-of-distribution (OOD) generalization failures, invariant learning [3, 2, 12, 1] is proposed to exploit the strict invariance property (i.e. absolutely invariant among arbitrary environments) based on multiple environments. Stemming from causality [16], the strict invariance property enables models to achieve the OOD optimality [18, 12] under distributional shifts.\nTo fulfill the promise of invariant learning, strong assumptions have to be made. Firstly, the learnability of the strict invariance imposes stringent requirements on the multiple-environment data: (a) the strict invariance property holds among environments; (b) the environments are diverse enough to exclude the undesired variant components. However, they are hard to meet in real applications. For example, the label noises in regression tasks are likely to differ in different environments, for which the strict invariance may not even exist. And the provided environments may be too similar to exclude undesired variant components. Secondly, multiple-source data are often pooled together during the collection, which makes environment labels unavailable. Therefore, the deployment of invariant learning in real applications is greatly limited. \u2217Corresponding Author\nPreprint. Under review.\nar X\niv :2\n20 6.\n02 99\n0v 1\n[ cs\n.L G\n] 7\nJ un\nTo mitigate the limitations of invariant learning in practice, growing attention has been paid to learning invariance via one heterogeneous pooled dataset [5, 14, 15]. And empirical algorithms are proposed to learn pseudo-environments for the backend invariant learning. Despite bypassing the requirements for environment labels, compared with multiple environments setting, it is less likely for the strict invariance property to hold among pseudo-environments, because the learning process of pseudo-environments inevitably aggravates the perturbations on the underlying invariance property. Therefore, it is misleading to pursue the strict invariance under latent heterogeneity, where such property could have been violated. Furthermore, given that true latent environments cannot be guaranteed to be consistently recovered, one cannot even tell where the learned invariance is targeted. Thus, the meanings of the learned invariance also remain vague.\nTo this end, we raise the distributional invariance property as a relaxed alternative to the strict invariance, which instead considers the invariance only among sub-populations down to a prescribed scale \u03b10 and allows a certain degree of variation (see section 2 for details). Based on it, in Problem 1, we reformulate the goal of invariant learning under latent heterogeneity as pursuing relaxed distributional invariance to sub-populations no less than the prescribed proportion \u03b10. Further, it is convenient to characterize the learnability of the newly-proposed distributional invariance under latent heterogeneity, assembling a theoretical framework for the reformulated problem. To certify the distributional invariance under latent heterogeneity, we propose an empirical algorithm named Distributionally Invariant Learning (DIL) in section 3, which contains two interactive stages, namely Variation Exploitation and Variation Elimination. Specifically, the sub-population with the most variation is exploited, and the revealed variant components are then eliminated via an invariance penalty. Theoretically, we provide the invariance guarantees as well as bounds of the generalization error gap in section 4, which guarantees the OOD performance of our DIL algorithm. Empirically, our DIL algorithm significantly outperforms all baselines in extensive experiments, which proves our DIL does not induce much violation on the underlying invariance and could effectively capture the distributional invariance under latent heterogeneity."
        },
        {
            "heading": "2 Relax the Strict Invariance in OOD Generalization",
            "text": "The goal of the Out-of-Distribution (OOD) Generalization problem [3, 12, 14] is to learn a predictor f(\u00b7) : X \u2192 Y with good OOD generalization performance, which is formalized as arg minf maxe\u2208supp(E) L(f |e), where L(f |e) = E[`(f(X), Y )|e] is the risk of predictor f(\u00b7) on environment e, and E is the random variable on indices of all possible environments. P e denotes the joint distribution in environment e, and for environments e1, e2 \u2208 supp(E), the data distribution P e1 , P e2 can be quite different."
        },
        {
            "heading": "1. Strict Invariance among Multiple Environments: Definitions and Limitations",
            "text": "Inspired by causal inference literature, the strict invariance property is commonly assumed in invariant learning literature [3, 13, 12, 14], which is formalized as: Definition 2.1 (Strict Invariance among Environments E [12, 14]). A representation \u03a6 is strictly invariant if Y \u22a5 E|\u03a6, where E is the random variable on indices of all possible environments. And the corresponding invariance set IE with respect to environments E is defined as IE = {\u03a6 : Y \u22a5 E|\u03a6} = {\u03a6 : P e1(Y |\u03a6) = P e2(Y |\u03a6), \u2200e1, e2 \u2208 supp(E)}.\nAssume the existence of the strict invariance, it is proved that the OOD optimality can be achieved via the maximal invariant predictor \u03a6\u2217 = arg max\u03a6\u2208IE I(\u03a6, Y ) [12], where I(\u00b7, \u00b7) stands for the Shannon Mutual Information. However, behind the promising theoretical results, Definition 2.1 implies strong assumptions on data. Except for the availability of environment labels, it assumes that P (Y |\u03a6) stays invariant across all possible environments, which is undoubtedly hard to meet in practice. For example, even though \u03a6 is the oracle\u2019s real causal feature, the relationship between \u03a6 and Y is likely to vary due to heterogeneous noises across environments. Therefore, even provided with all environments, the goal of strict invariance seems overstrict and may result in an invariance set with nothing but random noises. Worse still, the number of training environments is usually quite limited in practice (i.e. supp(Etr) \u2282 supp(E)), which will cause strong inconsistency between the learned invariance and underlying invariance, since a smaller environment set leads to a significantly larger invariance set containing many undesired components (i.e. IEtr \u2283 IE ) [14]. Consequently, the strong assumptions behind the strict invariance definition itself greatly limit the deployment of invariant learning algorithms in practice."
        },
        {
            "heading": "2. Distributional Invariance: A Relaxed Alternative",
            "text": "Recently, there are works [5, 14, 15] trying to relax the need for environment labels by learning pseudo-environment splits first. Even though empirically effective, the rationality of the problem under latent heterogeneity itself remains vague. Firstly, compared with the conventional multiple environment setting, the pseudo-environment learning process will inevitably aggravate the violations on the underlying invariance, making the goal of pursuing the strict invariance inappropriate. Secondly, since the true latent environments cannot be consistently recovered, one cannot even characterize towards what the learned models are invariant, making meaning of the learned invariance doubtful.\nTo mitigate such drawbacks, we propose a relaxed alternative to the strict invariance, named \u03b10Distributional Invariance as follows: Definition 2.2 (\u03b10-Distributional Invariance Property). Given observed data distribution P0(X,Y ) with latent heterogeneity, assume a lower bound \u03b10 \u2208 (0, 12 ) on the sub-population proportion \u03b1 and consider the set of potential minority sub-populations\nP\u03b10(P0) = {Q : P0 = \u03b1Q+ (1\u2212 \u03b1)Q0, for \u03b1 \u2208 [\u03b10, 1) and distribution Q0 2P0} (1)\nThen a representation \u03a6 is \u03b10-distributionally invariant if\nsup Q\u2208P\u03b10 (P0(X,Y ))\n\u03c1(Q(Y |\u03a6), P0(Y |\u03a6)) \u2264 \u03b4 with some \u03b4 > 0 (2)\nwhere \u03c1(\u00b7, \u00b7) is some distance metric between two distributions (e.g., MMD distance, KL divergence). For simplicity, for representation \u03a6 that is \u03b10-distributionally invariant, we denote it as Y \u22a5\u03b4 E\u03b10(P0)|\u03a6, where E\u03b10(P0) denotes the random variable on indices of distributions in P\u03b10(P0).\nHere we make some remarks. First, our \u03b10-distributional invariance is considered w.r.t. all subpopulations of P0(X,Y ) with ratio no less than \u03b10. Intuitively, the smaller \u03b10 is, the larger distribution set P\u03b10 is, and the stricter distributional invariance is required, since the relationship between \u03a6 and Y should remain invariant among more distributions. Second, by letting \u03b4 > 0, we allow for some violations of the invariant relationship, which relaxes the strict invariance and is appropriate in our latent heterogeneity setting. And in Theorem 2.1, we theoretically characterize the edge cases when our \u03b10-distributional invariance degenerates to the strict invariance. Theorem 2.1 (Connections with the Strict Invariance Property). Let \u03b4 = 0,P\u03b10 = {P e,\u2200e \u2208 supp(E)}, and choose \u03c1(\u00b7, \u00b7) as metrics like KL-divergence or MMD distance. Then the distributional invariance property is equivalent to the strict invariance in Definition 2.1. Remark (Connections with Distributionally Robust Optimization (DRO)). In DRO literature[7, 21], the goal is to optimize the worst-case performance inside the pre-defined uncertainty set. Therefore, it can only guarantee the robustness for distributions within the uncertainty set. As shown in Figure 3, when the testing distribution lies outside the set, the performance of DRO significantly drops. By contrasting the sub-populations, the distributional invariance learns the relaxed invariant relationship that may hold on a larger scale. As shown in Figure 3, even when the testing distribution is not included in training, there is only a slight drop of the performance of our DIL, indicating that our DIL could learn some inherent invariance properties that benefit the generalization.\nFinally, we reformulate the setting of invariant learning under latent heterogeneity in Problem 1. Problem 1 (Invariant Learning Problem Under Latent Heterogeneity). Given a heterogeneous dataset D = {(Xe, Y e)}e\u2208supp(Etr), containing different sub-population data collected from training environments e \u2208 supp(Etr) and environment labels are unavailable, the invariant learning problem under latent heterogeneity is defined as pursuing \u03b4-relaxed distributional invariance to prescribed sub-populations larger than proportion \u03b10 with dataset D."
        },
        {
            "heading": "3 Methods",
            "text": "To address the invariant learning problem under latent heterogeneity, in this work, we propose our Distributionally Invariant Learning (DIL) framework as well as an implementation. We first derive the whole DIL framework in Section 3.1, and then in Section 3.2 and 3.3 we introduce the implementation based on the Conditional Maximum Mean Discrepancy (CMMD) distance.\n2Q0 P0 means the support of Q0 is no larger than P0\nAlgorithm 1 Distributionally Invariant Learning (DIL) Input: Training Data D = {xi, yi}Ni=1, \u03bb, T , prescribed sub-population ratio \u03b10 Initialize: \u03a6(1) = X for t = 1 to T do\nStep 1. Variation Exploitation: Given \u03a6(t), find the worst sub-population Q(t) characterized by w\u2217 according to Equation 8. Step 2. Variation Elimination: Given the worst sub-populationQ(t), perform invariant learning on {Ptr, Q(t)} according to Equation 9 to obtain the representation \u03a6(t+1).\nend for\nNotations Denote the invariant predictor \u03a6\u03b8(X) parameterized by \u03b8, the predicting function Y\u0302 = h\u03b7(\u03a6\u03b8(X)) parameterized by \u03b7 (not restricted to linear h(\u00b7)), which gives the whole prediction model f\u03b7,\u03b8(X) = h\u03b7(\u03a6\u03b8(X)), and the training distribution Ptr. Denote the sample size n and the vector of sample weights w \u2208 Rn = [w1, . . . , wn]T with w \u2265 0 and wT 1 = 1. For simplicity, we omit the subscripts \u03b8, \u03b7 without causing misunderstanding."
        },
        {
            "heading": "3.1 Distributionally Invariant Learning Framework",
            "text": "To learn models with distributional invariance, inspired by some invariant learning literature [12], we propose to learn the maximal distributional invariant predictor as:\n\u03a6\u2217 = arg max \u03a6(X):Y\u22a5\u03b4E\u03b10 (Ptr)|\u03a6(X) I(Y ; \u03a6(X)) (3)\nThe constraint Y \u22a5\u03b4 E\u03b10(Ptr)|\u03a6(X) is defined in Definition 2.2, which requires the representation \u03a6(X) is \u03b10-distributionally invariant. And we propose to find a representation with the most mutual information with the target Y , which maximizes the predictive power. The complete objective function is:\n\u03b8\u2217 = arg max \u03b8\u2208\u0398 I(Y ; \u03a6\u03b8(X)) s.t. sup Q\u2208P\u03b10 (Ptr) \u03c1(Q(Y |\u03a6\u03b8(X)), Ptr(Y |\u03a6\u03b8(X))) \u2264 \u03b4 (4)\nGiven that it is hard to directly solve Equation 4, we give up the requirement of a prescribed amount \u03b4 of distributional invariance, and instead focus on the Lagrangian penalty problem:\nmax \u03b8\u2208\u0398\n{ R(\u03b8) = I(Y ; \u03a6\u03b8(X))\u2212 \u03bb \u00b7 sup\nQ\u2208P\u03b10 (Ptr) \u03c1(Q(Y |\u03a6\u03b8(X)), Ptr(Y |\u03a6\u03b8(X)))\n} (5)\nwhere I(\u00b7; \u00b7) denotes the Shannon Mutual Information. In Theorem 4.1, we provide the invariance guarantees for our Lagrangian penalty problem in convex cases. Since both the regularizer and the mutual information are hard to deal with, especially for complicated models like neural networks, we design a framework to approximately optimize this problem, involving two interactive stages, namely: (1) Variation Exploitation and (2) Variation Elimination. The variation exploitation stage continuously picks the worst-case sub-population to reflect the undesired variant components wrongly contained in the current predictor, since their relationships with Y vary a lot across sub-populations. And then the variation elimination stage utilizes such information to sweep them from the predictor via the invariance regularization. Notable that we deal with a tradeoff between the predictive power and the invariance in the variation elimination stage, instead of pursuing the strict invariance."
        },
        {
            "heading": "3.2 Step 1: Variation Exploitation",
            "text": "The variation exploitation stage aims to find the sub-populationQ \u2208 P\u03b10(Ptr) with the most different relation mapping \u03a6(X) \u2192 Y from that in the whole training distribution Ptr. Specifically, we characterize the mapping \u03a6(X)\u2192 Y by the conditional distribution Q(Y |\u03a6(X)) and Ptr(Y |\u03a6(X)). To measure the discrepancy between Q(Y |\u03a6(X)) and Ptr(Y |\u03a6(X)), we resort to the Conditional Maximal Mean Discrepancy (CMMD) distance, and we firstly introduce the empirical counterpart of CMMD distance in the following.\nMMD Firstly, we introduce the Hilbert space embedding, where distributions are represented by elements in a reproducing kernel Hilbert space (RKHS). A RKHS F on X with kernel k is a Hilbert space of functions f : X \u2192 R. Its inner product satisfies \u3008f(\u00b7), k(x, \u00b7)\u3009F = f(x), and \u03c6(x) = k(x, \u00b7)\nis viewed as a feature map of x in kernel space. A distribution can be embedded by \u00b5X = E[\u03c6(X)]. Secondly, for Maximum Mean Discrepancy (MMD) distance, let X = {xi}Ni=1 and Z = {zi}Mi=1 be the sets of samples from joint distributions Ptr and Q, MMD defines the difference measure as MMD[K, Ptr, Q] = supf\u2208K(EPtr [f(X)]\u2212 EQ[f(Z)]), where K is a class of functions. [9] proves that the class of functions in a universal RKHS F is rich enough to distinguish any two distributions and MMD can be expressed as the difference of their mean embeddings, which gives an empirical\nestimate of the MMD distance as D\u0302 2 MMD = \u2225\u2225\u2225 1N \u2211Ni=1 \u03c6(xi)\u2212 1M \u2211Mj=1 \u03c6(zj)\u2225\u2225\u22252F , which is an asymptotically unbiased estimator. Now we are ready to introduce the Conditional MMD (CMMD) distance, which measures the difference between two conditional distributions[22, 17].\nConditional MMD Similar to the estimate of MMD, one can compare the squared difference between Conditional Kernel Embeddings [22, 17]. Denote the RKHS F on X with kernel kx and \u03c8(x) = kx(x, \u00b7) \u2208 Rdx denotes the feature map of x \u2208 X . Denote the RKHS G on Y with kernel ky and \u03c5(y) = ky(y, \u00b7) \u2208 Rdy denotes the feature map of y \u2208 Y . Formally, the embedding of a conditional distribution P (Y |X) is defined as an operator CY |X satisfying (1) \u00b5Y |x = CY |X\u03c8(x) and (2) EY |x[g(Y )|x] = \u3008g, \u00b5Y |x\u3009G , where G is the RKHS corresponding to Y. Given a dataset D = {(xi, yi)}Ni=1 drawn i.i.d from P (X,Y ), the Conditional Kernel Embedding of P (Y |X) can be empirically estimated as:\nCY |X = CYXC \u22121 XX = \u03a5(Kx + \u03b2I) \u22121\u03a8T \u2208 Rdy\u00d7dx (6)\nwhere \u03a5 = [\u03c5(y1), . . . , \u03c5(yN )] \u2208 Rdy\u00d7N ,\u03a8 = [\u03c8(x1), . . . , \u03c8(xN )] \u2208 Rdx\u00d7N , Kx = \u03a8T\u03a8 and \u03b2 is the regularization. Notably the embedding of a conditional distribution is not a single element in RKHS, but an operator sweeping out a family of points in the RKHS, each indexed by x. By definition, for each Q(X,Y ) \u2208 P\u03b10(Ptr), we can empirically represent Q via Q\u0302 =\u2211N i=1 wi\u03b4(xi, yi) with {xi, yi}Ni=1 i.i.d sampled from Ptr. Therefore, for each sub-population Q \u2208 P\u03b10(Ptr), the CMMD distance between Q(Y |X) and Ptr(Y |X) can be estimated via:\nL2CMMD(Q(Y |X), Ptr(Y |X)) = \u2225\u2225CwY |X \u2212 CY |X\u2225\u22252F\u2297G (7)\nwith CwY |X = \u03a5H(HKxH + \u03b2I) \u22121(\u03a8H)T , H = Diag(\n\u221a w) \u2208 RN\u00d7N , and w \u2208 RN denotes the\nsample weights satisfying (1) 0 \u2264 wi \u2264 1/\u03b10N , for i \u2208 [1, N ] and (2) wT 1 = 1. Variation Exploitation To find a Q \u2208 P\u03b10(Ptr) where the mapping \u03a6 \u2192 Y varies the most compared with Ptr, we propose to learn sample weights w\u2217 that maximizes the CMMD distance in Equation 7 as:\nmax w L\u03022CMMD = Tr(\n(\u221a W (K\u0303x + \u03b2I) \u22121\u221aW \u2212 2 \u00b7 (Kx + \u03b2I)\u22121 ) Ky \u221a W (K\u0303x + \u03b2I) \u22121\u221aWKx)\ns.t. 0 \u2264 wi \u2264 1/\u03b10N and wT 1 = 1 (8)\nwhere W = Diag(w) \u2208 RN\u00d7N , K\u0303x = \u221a WKx \u221a W and Kx,Ky denotes the Gram Matrix of \u03a6(X)\nand Y , respectively (in our algorithm we simply use \u03c5(yi) = yi since y \u2208 R). Besides, denote the weighted distribution as Q, for K-classification tasks, we add one extra entropy-regularizer as\u2211 i\u2208[K]Q(Y = i) ln(Q(Y = i)/P (Y = i)) to align the marginal distribution of PY and QY ."
        },
        {
            "heading": "3.3 Step 2: Variation Elimination",
            "text": "Given the training distribution Ptr and the sub-population Q characterized by sample weights w\u2217 learned from Equation 8, as done in [5, 14, 12], we learn maximal invariant predictors as:\nmin \u03b8,\u03b7\n{ R(\u03b7, \u03b8) = E[`(h\u03b7(\u03a6\u03b8(X)), Y )] + \u03bb \u00b7 \u2016\u2207\u03b7,\u03b8LPtr \u2212\u2207\u03b7,\u03b8LQ\u201622 } (9)\nwhere LPtr ,LQ denotes the average errors in Ptr and Q respectively. Note thatR(\u03b7, \u03b8) is derived from Equation 5 with approximation and is suitable for complicated models such as neural networks. And here we make a few remarks: (1) The empirical risk term E[`(h\u03b7(\u03a6\u03b8(X)), Y )] is derived from the mutual information I(Y ; \u03a6\u03b8(X)) in Equation 5 and the proof can be found in Appendix. This equivalence also demonstrates that by maximizing the mutual information between \u03a6\u03b8(X) and Y , the predictive power is enhanced. (2) When optimizing model parameters \u03b8, \u03b7, the original penalty is computation-expensive for complicated models, such as neural networks. Therefore, we replace it with the gradient alignment penalty from invariant literature [12] to learn the invariant predictor\n\u03a6\u03b8(X), which forces the learned representation \u03a6 to satisfy Ptr(Y |\u03a6\u03b8(X)) = Q(Y |\u03a6\u03b8(X)) and continually sweeps the variant components revealed by Q. Note that different forms of invariance penalties can be plugged in. (3) The adjustable invariance penalty brings a tradeoff. Remark (Tradeoff in Equation 9.). There exists a tradeoff between predictive power and distributional invariance, which can be adjusted by the parameter \u03bb. And this corresponds with our analysis that it is no longer appropriate to seek the strict invariance in the latent heterogeneity setting, since one cannot guarantee the relationship between underlying invariant predictor \u03a6\u2217 and Y is not violated. Forcing the strict invariance in the latent heterogeneity is likely to obtain only random noises, and we think this also happens in the multiple-environment setting, because it is nearly impossible to guarantee or even verify the existence of the strict invariant predictors, especially in real scenarios (where noises are likely to differ)."
        },
        {
            "heading": "4 Theoretical Properties",
            "text": "1. Invariance Guarantee Firstly, we provide the distributional invariance guarantees for our Lagrangian penalty problem in Equation 5. Theorem 4.1 (Distributional Invariance Guarantees). Denote the learned predictor as \u03a6\u03b8\u2217(X) parameterized with \u03b8\u2217 = arg max\u03b8R(\u03b8), where R(\u03b8) is defined in Equation 5. Denote \u03b4\u2217 = supQ\u2208P\u03b10 (Ptr) \u03c1(Q(Y |\u03a6\u03b8\u2217(X)), Ptr(Y |\u03a6\u03b8\u2217(X))). Let \u03b4 = \u03b4\n\u2217, then it gives that \u03b8\u2217 is the solution of the original constrained optimization problem in Equation 4.\nTheorem 4.1 illustrates the equivalence between our Lagrangian penalty problem (Equation 5) and the original problem (Equation 4) under the choice of \u03b4\u2217. It demonstrates that we do not seek the strict invariance (\u03b4 = 0) but allow for some violations up to \u03b4\u2217, which is more reasonable in our setting. And the extent of the allowed violation depends on the parameter \u03bb of the invariance penalty.\n2. Learnability of Distributional Invariance To analyze the generalization ability of our proposed DIL framework, it is necessary to quantify to what extent the distributional invariance found from Ptr will hold in testing. Following [23], we define the expansion function as follows: Definition 4.1. A function s : R+\u222a{0} \u2192 R+\u222a{0,+\u221e} is an expansion function, iff the following properties hold: (1) s(\u00b7) is monotonically increasing and s(x) \u2265 x,\u2200x; (2) lim\nx\u21920+ s(x) = s(0) = 0.\nThen to rationalize the invariant learning problem under latent heterogeneity (Problem 1), we propose the invariance learnability term to characterize in what scenarios the distributional invariance can be learned from the heterogeneous distribution Ptr. Definition 4.2 (Invariance Learnability). Problem 1 is (s(\u00b7), \u03b10)-learnable if there exists an expansion function s(\u00b7) such that \u2200\u03a6 \u2208 supp(\u03a6), \u03b4(supp(E)) \u2264 s(\u03b4(P\u03b10(Ptr))), where \u03b4(P) denotes the variation with respect to the distribution set P as \u03b4(P) = supQ\u2208P \u03c1(Q(Y |\u03a6), Ptr(Y |\u03a6)) and supp(E) contains all possible environments.\nHere for predictor \u03a6, \u03b4(P) measures the extent of variation w.r.t. the distribution set P , and smaller \u03b4(P) means the relationship \u03a6\u2192 Y is more invariant among distributions in P . The learnability in Definition 4.2 addresses two requirements for Problem 1 to be learnable: (1) The \u03b10-distributional invariance property should approximately hold in testing distributions, that is, its variation among testing distributions (\u03b4(supp(Eall))) is bounded by the expansion function (s(\u03b4(P\u03b10(Ptr)))) and cannot change arbitrarily. Otherwise we view it an unlearnable problem for invariant learning, since the underlying invariance among testing distributions cannot be captured through Ptr. (2) Further, the steepness of the expansion function reflects the difficulty of a learnable invariant learning problem, since the steeper the expansion function is, the less likely the learned distributional invariance will hold in testing. (3) For a strictly invariant predictor (\u03b4 \u2192 0), it remains invariant in testing. Besides, for an invariance learnable problem, the generalization error gap between training and testing can be bounded by O( \u221a s(\u03b4)) in some special cases, which we leave in the Appendix.\n3. Limitations Empirically, severe label imbalance makes our DIL more prone to failure, since the variation exploitation stage may continuously find sub-population within one class, which provides no information since Y is constant in it. Theoretically, we find the effectiveness of DIL is affected by the strength of the underlying true invariance, which depends on the training data. The strength of the underlying invariant predictor \u03a6\u2217 for training distribution Ptr can be approximated\nas \u03b4\u2217 = supQ\u2208P\u03b10 (Ptr) \u03c1(Q(Y |\u03a6 \u2217), Ptr(Y |\u03a6\u2217)) (the same as Definition 2.2), which quantifies how much the invariance tends to be violated among sub-populations in training. The smaller \u03b4\u2217 is, the stronger the true invariance is, the easier the invariant learning problem is, and vise versa."
        },
        {
            "heading": "5 Experiments",
            "text": "Baselines We compare our proposed DIL algorithm with the following methods: Empirical Risk Minimization(ERM), Distributionally Robust Optimization(f -DRO [7]), Environment Inference for Invariant Learning(EIIL [5]), Kernelized Heterogeneous Risk Minimization(KerHRM [15]) and Invariant Risk Minimization(IRM [3]) with environment Etr labels. Note that IRM is based on multiple training environments, and we provide sub-population labels for IRM, while others do not need. In all experiments, we use the RBF kernel as \u03ba(x1, x2) = exp(\u2212\u03b3\u2016x1 \u2212 x2\u20162) for our DIL. Evaluation Metrics As for experiments with multiple testing distributions, we use Mean_Error defined as Mean_Error = 1|Etest| \u2211 e\u2208Etest L\ne, Std_Error defined as Std_Error =\u221a 1 |Etest\u22121| \u2211 e\u2208Etest(L\ne \u2212Mean_Error)2, and Max_Error = maxe\u2208Etest Le, which are mean error, standard deviation error, and the worst-case error across testing environments Etest"
        },
        {
            "heading": "5.1 Simulation Data",
            "text": "We adopt two mechanisms to simulate the varying relationships between covariates and the target across different sub-populations."
        },
        {
            "heading": "Regression with Selection Bias",
            "text": "In this setting, the relationships between covariates and the target are perturbed through the selection bias mechanism across sub-populations. We generate the data following the mechanism adopted by [13, 15], where we assume X = [S, V ]T and Y = f(S) + = \u03b2TS + S1S2S3 + N (0, 0.1). To generate different sub-populations, we maintain P (Y |S) the same across sub-populations and leverage a data selection mechanism to vary P (Y |V ). Specifically, we select data point (xi, yi) with probability P\u0302 (xi, yi) according to one certain variable Vb \u2208 V as P\u0302 (xi, yi) = |r|\u22125\u2217|yi\u2212sign(r)\u00b7Vb| where |r| > 1. Intuitively, r controls the strengths and direction of the spurious correlation between Vb and Y . The larger value of |r| means the stronger spurious correlation between Vb and Y , and r > 0 means positive correlation and vice versa (i.e. if r > 0, a data point whose Vb is close to its y is more probably to be selected.). Therefore, we use r to define different sub-populations.\nFor training data, to simulate training data with latent groups, we mix 2000 data points from varying r1 and 200 points from r2 = \u22121.1. For different testing groups, we sample 1000 data points from r \u2208 {\u22121.9,\u22122.1, . . . ,\u22122.9}, respectively. For our DIL methods and f -DRO, we set \u03b10 = 0.1 (the ground-truth is 0.09). We use linear models in this experiment for all methods."
        },
        {
            "heading": "Classification with Spurious Correlation",
            "text": "Following [20], for different groups, we induce spurious correlations between the label Y \u2208 {+1,\u22121}\nFigure 1: Certified Results for the Classification task.(Section 5.1) (a) F1 Score and Testing Accuracy. (b) Overall testing accuracy.\nFigure 2: Results of the real-world dataset. (Section 5.3)\nand a spurious attribute A \u2208 {+1,\u22121} with different strengths and directions. We assume X = [S, V ]T \u2208 R2d, where S \u2208 Rd is the invariant feature generated from the label Y and V \u2208 Rd the variant feature generated from the spurious attribute A:\nS|Y \u223c N (Y 1, \u03c32sId), V |A \u223c N (A1, \u03c32vId) (10)\nIn this setting, we characterize different groups with the bias rate r \u2208 (0, 1], which represents that for 100 \u00b7 r% data, A = Y , and for the other 100 \u00b7 (1 \u2212 r)% data, A = \u2212Y . Intuitively, r controls the spurious correlation between the label Y and spurious attribute A, which we use to characterize different sub-populations. In training, we generate 2000 data points, where 50% points are from group e1 with r1 = 0.9 and the other from group e2 with varying r2. In testing, we generate 1000 data points from group e3 with r3 = 0.0 to simulate the worst-case group, since the direction of spurious correlations is totally reversed from training. We vary the bias rate r2 as well as the dimension d of features. For our DIL methods and f -DRO, we set \u03b10 = 0.15 (the ground-truth is 0.17). We use a two-layer MLP for this experiment for all methods.\nOverall Results For the regression task, we report the Mean_Error, Std_Error and Max_Error in Table 4. For the classification task, we report the average Train_Acc and Test_Acc in Table 5. All results are averaged after 10 runs and experimental details can be found in Appendix. From the overall results, in both regression and classification scenarios, our DIL outperforms all baselines with respect to higher prediction accuracy and better invariance among distributional shifts, which validates that our DIL framework can capture the distributional invariance inside data. Explanatory Results To better demonstrate our DIL as well as its difference with the DRO, for the classification task with r2 = 0.75, we plot the curve of testing accuracy to the sub-population ratio \u03b10 for our DIL and f -DRO in Figure 3. For our DIL, \u03b10 controls scale of the sub-populations to which the distributional invariance is considered, and it also controls the radius of the uncertainty set for DRO. From the results in Figure 3, (1) our DIL\u2019s performances on Ptest only drops slightly when Ptest /\u2208 P\u03b10 (the red region) that is not considered in training, which shows that our DIL can learn some inherent invariance properties that benefit the generalization to larger scales. But DRO fails to provide good generalization performances in this case, which corresponds with our analysis that DRO is effective when Ptest belongs to the uncertainty set. (2) Our DIL\u2019s performance smoothly changes with \u03b10, which reflects \u03b10 controls the scale of sub-populations that we consider the distributional invariance to. With \u03b10 becoming smaller, our DIL considers more \u2019fine-grained\u2019 invariance. However, when \u03b10 is too small, our DIL is likely to be affected by the noise in data, which will harm the performance."
        },
        {
            "heading": "5.2 Pseudo-Real Data",
            "text": "To further validate the capacity of our proposed framework under general settings, we adopt the colored MNIST dataset proposed by [3], based on which we modify the original SVHN dataset to colored SVHN as another pseudo-real dataset.\nData Generation Following [3], we build a synthetic binary classification task on MNIST and SVHN, where each image is colored either red or green in a way that spuriously correlates with the class label Y . Firstly, a binary label Y is assigned to each image according to its digit: Y = 0 for digits 0\u223c4 and Y = 1 for digits 5\u223c9. Then, for MNIST data, we induce noisy labels by randomly flipping the label Y with a probability of 0.2. Finally, we sample the color id C by flipping Y with probability e and form environments, where e = 0.9 for the first training environment, e = 0.8 for the second training environment and e = 0.0 for the testing environment.\nIn training, we randomly sample 2500 images for each environment, and the two training environments are mixed without environment label Etr for ERM, f -DRO, EIIL, KerHRM and DIL, while for IRM, the Etr labels are provided. For IRM, we sample 1000 data from the two training environments respectively and select the hyper-parameters which maximize the minimum accuracy of the two validation environments. Note that we have no access to the testing environment while training, therefore we cannot resort to testing data to select the best one, which is more reasonable and different from that in [3]. For the two DIL algorithms, we set \u03b10 = 0.1 (the ground-truth is 0.15). For the others, since we have no access to E labels, we simply pool the 2000 data points for validation. The results are shown in Table 6, where Oracle represents the oracle results that can be achieved under this setting, and Unbiased ERM shows the performance of ERM without the spurious correlation between the color C and label Y . We run each method 10 times and report the average results.\nAnalysis From the results, our proposed DIL outperforms all baselines, and DIL even outperforms IRM significantly in an unfair setting where we provide perfect environment labels for IRM, which indicates that our DIL framework can capture the distributional invariance among heterogeneous data for better OOD generalization. Further, compared with KerHRM, the performance of DIL is much more stable, which shows the superiority of our Variation Exploitation Stage."
        },
        {
            "heading": "5.3 Real-World Data: Retiring Adults",
            "text": "To better validate the effectiveness of our DIL framework, we consider a much more challenging scenario on a real-world dataset, named Retiring Adult [6]. We adopt the ACSTravelTime task, which is to predict whether an individual has a commute to work that is longer than 20 minutes. In this task, we have 16 features and 1,428,642 data points in total from all 50 US states. Since there are 50 distinct environments, this dataset contains natural geographic shifts, which makes it quite suitable for comparing OOD methods. In training, we sample 2000 data points from MA (Massachusetts) and validate on the rest data from MA. In testing, we test different methods on all the other 49 states. We plot the results of different methods in Figure 4. Note that the original code released by KerHRM is too time-consuming to run on this data because of the large amount of data(over 1 million data points), therefore we use HRM[14] here to replace the KerHRM, which can only deal with the raw feature data. Since there is absolutely one environment in this experiment and we do not know the underlying environments, we cannot run IRM in this setting. And EIIL can be viewed as an alternative to IRM with learned environments.\nFrom the results in Figure 4(a), the average performance of our DIL locates in the top right of the figure, which shows that our methods achieve the best OOD generalization performance with respect to testing accuracy and F1 score. Further, in Figure 4(b), for our DIL, the performances of most environments are concentrated at high accuracy, and the variance of different environments is significantly smaller than baselines. It shows that our DIL framework can learn some distributional invariance among different sub-populations, which benefits the generalization performances."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this paper, we rationalize the problem of invariant learning under latent subpopulations, raising the definition of distributional invariance and the theoretical framework. Based on it, we design our Distributionally Invariant Learning framework, which exhibits good OOD performance in our\nextensive experiments. This work serves as the first attempt to theoretically formulate this problem. Although we analyze the learnability of this problem, we find further theoretical analysis becomes much more hard, since it depends on the specific algorithm, the training data, the model, etc., and this also leaves many open problems in this field. And we hope our work could inspire the further study on this problem."
        },
        {
            "heading": "A Appendix",
            "text": ""
        },
        {
            "heading": "A.1 Related Work",
            "text": "In this section, we discuss the related works in detail. There are mainly two branches of literatures related to our work, including invariant learning[3, 2, 12, 14, 15, 1, 5] and distributionally robust optimization[7, 21].\nFor invariant learning, Arjovsky et al.[3] first come up with the Out-of-Distribution (OOD) Generalization problem and design a regularizer to learn such representations that the optimal linear classifier remains the same across training environments, and this method is a typical method in invariant learning. Ahuja et al.[2] incorporate game theory and replace the linear classifier in IRM with an ensemble of classifiers from different environments. And Koyama et al.[12] theoretically characterize when the invariance will benefit OOD generalization and propose to learn the maximal invariant predictor to achieve OOD optimality. Ahuja et al.[1] combines invariant learning with information bottleneck for better OOD generalization performance. The proposed invariance definition requires an invariant relationship among all possible environments, which we call as the strict invariance. However, whether it exists in real applications remains doubtful, since the noises are likely to change in different environments and therefore violate the strict invariance. Further, the availability of multiple training environments itself is quite hard to meet with in real scenarios, since real data from multiple environments are pooled together without explicit environment labels, which makes many invariant learning methods inapplicable in real applications.\nIn order to mitigate such limitations, recently, some works [5, 14, 15] try to learn pseudo-environments first and then perform invariant learning. Creager et al.[5] directly maximize the regularizer of IRM with a given biased model to generate environments, while the environment division relies on both the given biased model and the invariant regularizer of IRM[3], which is heuristic. And since the effectiveness of IRM itself is questioned recently [19, 11], the effectiveness of [5] is also vague. Further, one may view [5] as a static implementation (by using a biased model and the regularizer of IRM without interaction) of \u2019some unknown metric \u03c1\u2019 for our DIL framework, which is coincident and demonstrate the generality of our framework. Liu et al.[14, 15] propose to iteratively learn the environment splits and the invariant predictors, although intuitively reasonable, the property of learned environments still remains vague, which renders the proposed framework unstable (as shown in Table 6). Since the property of learned environments cannot be analyzed or guaranteed, whether the invariance can be achieved also remains unclear and cannot be certified. However, although these methods relax the requirements for environment labels, the problem setting may be misleading, since the true underlying invariance is likely to be violated during the pseudo-environment learning procedure. Therefore, pursuing the strict invariance is no longer appropriate, since it is likely to obtain only random noises. Inspired by this, it is of paramount importance to reformulate the invariant learning problem under latent heterogeneity to a more reasonable one, which is important for the field of invariant learning.\nDistributionally robust optimization (DRO) methods, typified by f -DRO [7], propose to optimize the worst-case error with respect to a pre-defined distribution set that lies around the observed training distribution and characterizes the potential testing distributions. When the testing distribution lies in the pre-defined distribution set, the OOD generalization performance can be controlled by the worstcase. However, when the testing distribution is not captured by the pre-defined set, the performance of DRO depends on the relationship between the testing distribution and the worst-case distribution in the pre-defined set, which cannot be guaranteed and is likely to be quite random, which is also reflected in our Figure 3 (the curve of f -DRO is quite fluctuant). Unfortunately, such circumstances are quite likely to happen in real scenarios, since the pre-defined set cannot be set too large because of the over-pessimism problem of DRO[10, 8]. In this work, we borrow the idea of distribution set from DRO literature to characterize the sub-population set, based on which we come up with the distributional invariance property, which is a relaxed alternative of the strict invariance. Further, rather than optimize for the worst-case one, we try to learn predictors with good distributional invariance property to enable the model to generalize even outside the original distribution set."
        },
        {
            "heading": "A.2 Derivation of the Variation Elimination Stage",
            "text": "To maximize the mutual information between Y and \u03a6, we have:\nmax \u03a6\nI(Y ; \u03a6) (11)\n\u21d4 max \u03a6 H[Y ]\u2212H[Y |\u03a6] (12)\n\u21d4 min \u03b8 H[Y |\u03a6\u03b8(X)] (13)\n\u21d4 min \u03b8 EX,Y\u223cP0 [\u2212 logPw\u2217(Y |\u03a6\u03b8(X))] (14)\n\u21d4 min w,\u03b8 EP0 [\u2212 log ( Pw\u2217(Y |\u03a6\u03b8(X)) Pw(Y |\u03a6\u03b8(X)) \u00b7 Pw(Y |\u03a6\u03b8(X)) ) ] (15)\n\u21d4 min w,\u03b8 EP0 [\u2212 logPw(Y |\u03a6\u03b8(X))]\u2212 EX\u223cP0(X)[DKL(Pw\u2217(Y |\u03a6(X))\u2016Pw(Y |\u03a6(X)))] (16)\n\u2264 min w,\u03b8 EP0 [\u2212 logPw(Y |\u03a6\u03b8(X))] (17)\n\u21d4 min w,\u03b8 EP0 [L(fw,\u03b8(X), Y )] (18)\nFor given supp(E(t0) = {Ptr, Q(t)}, to find the maximal invariant predictor, the derivation of the regularizer can be found in [12]."
        },
        {
            "heading": "A.3 Derivation of the DIL-MMD",
            "text": "Denote \u03a5 = [\u03c5(y1), . . . , \u03c5(yn)] \u2208 Rdy\u00d7n and \u03a8 = [\u03c8(x1), . . . , \u03c8(xn)] \u2208 Rdx\u00d7n, so as to measure the distance between two conditional distributions, the Conditional Kernel Embedding is proposed as:\nCY |X = CY XC \u22121 XX = \u03a5(Kx + \u03bbI) \u22121\u03a8T \u2208 Rdy\u00d7dx (19) where CXY = EXY [\u03c8(X)\u2297 \u03c6(Y )]\u2212 \u00b5X \u2297 \u00b5Y . And let H = Diag( \u221a w) \u2208 Rn\u00d7n, where w \u2208 Rn, we can derive the Weighted Conditional Kernel Embedding as:\nCwY |X = \u03a5H(H\u03a8 T\u03a8H + \u03bbI)\u22121(\u03a8H)T = \u03a5H(HKxH + \u03bbI) \u22121(\u03a8H)T (20)\nAnd the corresponding Conditional Maximum Mean Discrepancy criterion is given as: L2CMMD(Pwtrain(Y |X), Ptrain(Y |X)) = \u2225\u2225\u2225CwY |X \u2212 CY |X\u2225\u2225\u22252F\u2297G (21)\nwhich gives that\nL2CMMD(Pwtrain(Y |X), Ptrain(Y |X)) (22) = Tr((CwY |X) TCwY |X) + Tr(C T Y |XCY |X)\u2212 2Tr((C w Y |X) TCY |X) (23)\nThen we have:\nTr(CTY |XCY |X) (24) = Tr ( \u03a8(\u03a8T\u03a8 + \u03bbI)\u22121\u03a5T\u03a5(\u03a8T\u03a8 + \u03bbI)\u22121\u03a8T ) (25)\n= Tr ( \u03a8T\u03a8(\u03a8T\u03a8 + \u03bbI)\u22121\u03a5T\u03a5(\u03a8T\u03a8 + \u03bbI)\u22121 ) (Tr(AB) = Tr(BA)) (26)\n= Tr ( Kx(Kx + \u03bbI) \u22121Ky(Kx + \u03bbI) \u22121) (27)\nand\nTr(CTY |XCY |X) = Tr ( HKxH(HKxH + \u03bbI) \u22121HKyH(HKxH + \u03bbI) \u22121) (28)\nand\nTr(CTY |XC w Y |X) (29) = Tr ( \u03a8(\u03a8T\u03a8 + \u03bbI)\u22121\u03a5T\u03a5H(H\u03a8T\u03a8H + \u03bbI)\u22121(\u03a8H)T ) (30)\n= Tr ( \u03a8(Kx + \u03bbI) \u22121KyH(HKxH + \u03bbI) \u22121(\u03a8H)T ) (31)\n= Tr ( HKx(Kx + \u03bbI) \u22121KyH(HKxH + \u03bbI) \u22121) (32)\nFinally, the Conditional MMD regularizer can be written as: L2CMMD(W ) = Tr (\u221a WKx \u221a W ( \u221a WKx \u221a W + \u03bbI)\u22121 \u221a WKy \u221a W ( \u221a WKx \u221a W + \u03bbI)\u22121 ) \u2212 2 \u00b7 Tr (\u221a WKx(Kx + \u03bbI) \u22121Ky \u221a W ( \u221a WKx \u221a W + \u03bbI)\u22121\n) = Tr ((\u221a W ( \u221a WKx \u221a W + \u03bbI)\u22121 \u221a W \u2212 2 \u00b7 (Kx + \u03bbI)\u22121 ) Ky \u221a W ( \u221a WKx \u221a W + \u03bbI)\u22121 \u221a WKx\n) (33)\nwhere W = Diag(w). Then the first stage can be formalized as a support-constraint version:\nw = arg max w L2CMMD(w) (34)\ns.t. w \u2208 [0, 1 \u03b10 ]n and wT1 = 1\u21d2 Pw(X) \u2208 P\u03b10(Ptrain(X)) (35)\nIn this work, we simply choose \u03c5(y) = y since y \u2208 R, and we use the Gaussian RBF kernel for kx(x1, x2) = \u3008\u03c8(x1), \u03c8(x2)\u3009."
        },
        {
            "heading": "A.4 Bounds of the Generalization Error Gap",
            "text": "Here we derive the bounds of the generalization error gap for our DIL framework only for some special cases, as done in [REF]. Theorem A.1 (Bounds of Generalization Error Gap with Conditional MMD Distance as \u03c1(\u00b7)). As for the choice of MMD distance as \u03c1(\u00b7, \u00b7), we derive an approximation of the generalization error gap for kernel regression. Given the kernel function \u03ba : X \u00d7X \u2192 R, \u03c0 : Y \u00d7Y \u2192 R and the corresponding features \u03c8(x) \u2208 F and \u03c5(y) \u2208 G and assume F and G are both RKHS. For an \u03b10-distributionally invariant predictor \u03a6(X) \u2208 X , assume C\u03a6\u03a6(\u00b7,\u00b7) = E[k(\u03a6(X), \u00b7) \u2297 k(\u03a6(X), \u00b7)] is invertible and \u3008\u03a6(x),\u03a6(x)\u3009X \u2264 \u03b12 for all \u03a6(x) \u2208 X . Then , we have:\nsup e\u2208E E [\u2225\u2225\u2225CeY |\u03a6k(\u03a6(X), \u00b7)\u2212 CtrainY |\u03a6 k(\u03a6(X), \u00b7)\u2225\u2225\u2225G |\u03a6(X) ] \u2264 O( \u221a s(\u03b4(P\u03b10))) (36)\nwhere s(\u03b4) is defined in Definition 4.2 and supp(E) contains all possible testing distributions.\nProof. Given the kernel function \u03ba : X\u00d7X \u2192 R, \u03c0 : Y\u00d7Y \u2192 R. SinceC\u03a6\u03a6(\u00b7,\u00b7) = E[k(\u03a6(X), \u00b7)\u2297 k(\u03a6(X), \u00b7)] is invertible, the conditional mean operator CY |\u03a6 can be expressed as CY |\u03a6 = CY \u03a6C\u22121\u03a6\u03a6 [22]. We approximately use the conditional mean embedding CeY |\u03a6k(\u03a6(X), \u00b7) to replace the true corresponding \u03c0(Y, \u00b7) for the testing distribution P e. Since \u3008\u03a6(x),\u03a6(x)\u3009X \u2264 \u03b12 for all \u03a6(x) \u2208 X , then for\n\u2225\u2225\u2225CeY |\u03a6k(\u03a6(X), \u00b7)\u2212 CtrainY |\u03a6 k(\u03a6(X), \u00b7)\u2225\u2225\u22252G , we have:\u2225\u2225\u2225CeY |\u03a6k(\u03a6(X), \u00b7)\u2212 CtrainY |\u03a6 k(\u03a6(X), \u00b7)\u2225\u2225\u22252 G \u2264 \u2225\u2225\u2225CeY |\u03a6 \u2212 CtrainY |\u03a6 \u2225\u2225\u22252 F\u2297G \u03b12 \u2264 s(\u03b4)\u03b12 = O(s(\u03b4)) (37)\nwhich naturally gives the result.\nHere we make a few remarks. (1) For the choice of Conditional MMD distance, as analyzed in the transfer learning literature [REF], we can only analyze the generalization bound for kernel methods. (2) We analyze the conditional generalization error bound here, where we require the marginal distributions on X remains the same, because the covariate shifts could also affect the generalization and they are not the target of this work. Also, from the results of the KL-divergence, we could theoretically characterize the effects brought by the covariate shifts, which may clarify this. (3) We\nomit the generalization error \u2225\u2225\u2225CtrainY |\u03a6 k(\u03a6(X), \u00b7)\u2212 C\u0302trainY |\u03a6 k(\u03a6(X), \u00b7)\u2225\u2225\u22252G in the i.i.d case, which is well developed in existing works [24]. Theorem A.2 (Bounds of Generalization Error with KL-divergence as \u03c1(\u00b7)). Define the conditional error gap as erre(f(\u03a6)) = E\u03a6[EPe [`(f(\u03a6), Y )|\u03a6]\u2212EPtr [`(f(\u03a6), Y )|\u03a6]]. Assume \u03a6 is \u03b10-distributionally invariant, s(\u00b7) is the expansion function for an invariance learnable problem, then for the choice of KL-divergence, we have sup\ne\u2208E erre(f(\u03a6)) \u2264 O(\n\u221a s(\u03b4(P\u03b10))), where supp(E) contains all possible\ntesting distributions. Further, we can derive the whole generalization bound for KL-divergence.\nAssume `(\u00b7, y) is \u00b5-Lipschitz for some \u00b5 > 0, then with probability at least 1 \u2212 , then for any e \u2208 supp(E), we have\nEPe [`(f(\u03a6), Y )] \u2264 1\nN \u2211 i `(f(\u03c6i), yi) + 2\u00b5RN (F) +O( \u221a s(\u03b4\u03c1(P\u03b10)) + 1 2 DKL(P e\u03a6\u2016P tr\u03a6 ) + \u221a ln(1/ )/N)\n(38) where F denotes the prediction function family andRN represents the Rademacher Complexity[4]\nand s(\u03b4) is defined in Definition 4.2.\nProof. As for the choice of KL-divergence, firstly, for any e \u2208 supp(E), denote P \u2032e(Y,\u03a6) = P e(Y |\u03a6)P (\u03a6) and P \u2032train(Y,\u03a6) = P train(Y |\u03a6)P (\u03a6), and then we have\nE [EP e [`(f(\u03a6), Y )|\u03a6]\u2212 Etrain[`(f(\u03a6), Y )|\u03a6]] (39) \u2264 2M \u00b7 TV(P \u2032e, P \u2032train) (40)\n\u2264 2M \u00b7 \u221a 1\n2 DKL(P e(Y |\u03a6)\u2016P train(Y |\u03a6)) (follows Pinsker\u2019s inequality) (41) \u2264 O( \u221a s(\u03b4)) (42)\nFurther, as for the whole generalization bound in environment e \u2208 supp(E), we have\nEP e [`(f(\u03a6), Y )]\u2212 Etrain[`(f(\u03a6), Y )] (43) \u2264 2M \u00b7 \u221a 1\n2 DKL(P e(Y |\u03a6)\u2016P train(Y |\u03a6)) +\n1 2 DKL(P e(\u03a6)\u2016P train(\u03a6)) (44)\n\u2264 2M \u00b7 \u221a s(\u03b4) + 1\n2 DKL(P e(\u03a6)\u2016P train(\u03a6)) (45)\nAnd the left part in Equation 38 directly follows from the results of Rademacher complexity regression bounds.\nHere we make some additional remarks. From the generalization bound in Equation 38, we can see that the additional error brought by distributional shifts comes from two aspects (s(\u03b4) and the covariate shifts). And in this work, we mainly address the shifts on the relation mappings \u03a6\u2192 Y , and our DIL framework could control the error from this. This bound could also inspire that in future works to enhance the OOD generalization performance, one should both address this two sources of shifts."
        },
        {
            "heading": "A.5 Proof of Theorem 4.1",
            "text": "Theorem A.3 (Distributional Invariance Guarantees). Denote the learned predictor as \u03a6\u03b8\u2217(X) parameterized with \u03b8\u2217 = arg max\u03b8R(\u03b8), where R(\u03b8) is defined in Equation 5. Denote \u03b4\u2217 = supQ\u2208P\u03b10 (Ptr) \u03c1(Q(Y |\u03a6\u03b8\u2217(X)), Ptr(Y |\u03a6\u03b8\u2217(X))). Let \u03b4 = \u03b4\n\u2217, then it gives that \u03b8\u2217 is the solution of the original constrained optimization problem in Equation 4.\nProof. Here we prove by contradiction. Assume \u03b8\u2032 is the supremum point of Equation 4, we have I(Y,\u03a6\u03b8\u2032(X)) \u2265 I(Y,\u03a6\u03b8\u2217(X)) as well as sup\nQ\u2208P\u03b10 \u03c1 \u2264 \u03b4\u2217, which together gives that R(\u03b8\u2032) \u2265 R(\u03b8\u2217).\nHowever, since \u03b8\u2217 = arg max\u03b8R(\u03b8), we haveR(\u03b8\u2217) \u2265 R(\u03b8\u2032). Therefore, we have I(Y,\u03a6\u03b8\u2032(X)) = I(Y,\u03a6\u03b8\u2217) and \u03b8\u2217 satisfies the constraints, which gives the conclusion."
        },
        {
            "heading": "A.6 Experimental Details",
            "text": "In this section, we demonstrate the details of our experiments."
        },
        {
            "heading": "A.6.1 Simulation Data",
            "text": "Regression In this setting, the correlations among covariates are perturbed through a selection bias mechanism. We assume X = [S, V ]T \u2208 Rd with S \u2208 Rns and V \u2208 Rnv . We assume Y = f(S) + and P (Y |S) remains invariant across environments while P (Y |V ) can arbitrarily change.\nTherefore, we generate training data points with the help of auxiliary variables Z \u2208 Rn\u03c6+1 as following:\nZ1, . . . , Zns+1 iid\u223c N (0, 2.0) (46)\nV1, . . . , Vnv iid\u223c N (0, 2.0) (47) Si = 0.8 \u2217 Zi + 0.2 \u2217 Zi+1 for i = 1, . . . , ns (48)\nTo induce model misspecification, we generate Y as:\nY = f(S) + = \u03b8s(S) T + S1S2S3 + (49)\nwhere \u03b8s = [12 ,\u22121, 1,\u2212 1 2 , 1,\u22121, . . . ] \u2208 R ns , and \u223c N (0, 1.0). As we assume that P (Y |S) remains unchanged while P (Y |V ) can vary across environments, we design a data selection mechanism to induce this kind of distribution shifts. For simplicity, we select data points according to a certain variable Vb \u2208 V :\nP\u0302 = |r|\u22125\u2217|y\u2212sign(r)\u2217vb| (50) \u00b5 \u223c Uni(0, 1) (51)\nM(r; (x, y)) = { 1, \u00b5 \u2264 P\u0302 0, otherwise\n(52)\nwhere |r| > 1. Given a certain r, a data point (x, y) is selected if and only if M(r; (x, y)) = 1 (i.e. if r > 0, a data point whose Vb is close to its Y is more probably to be selected.)\nClassification We set \u03c32s = 3.0 and \u03c32v = 0.3 to let the model more prone to use spurious V since V is more informative.\nAs for the hyper-parameter for DIL-MMD and f -DRO, for regression data, we set \u03b10 = 0.1 (the true minor subpopulation ratio is 0.09); for classification data, we set \u03b10 = 0.15 (the true minor subpopulation ratio is 0.17). As for the validation data, we sample i.i.d data as training data and compare both the worst-case performance of two subpopulations. As for IRM, we select the parameter of the regularizer \u03bb \u2208 {0.1, 0.3, . . . , 0.9, 1.5, 5.0, 10.0} according to the validation performance. As for EIIL, we set the epochs for splitting environments to 1e4 for good convergence, and other parameters are the same as IRM. As for KerHRM, we set the cluster_num to be the ground-truth 2. As for DIL-MMD, we use the Gaussian RBF kernel for both X and Y ."
        },
        {
            "heading": "A.6.2 Pseudo-Real Data and Real Data",
            "text": "For the experiments on pseudo-real data and real data, we use MLP model with two hidden layers. For the pseudo-real data, we select the best hyper-parameters according to both the average performances of two environments and the performance gap between them. For the real data, in order to better compare the performance of different methods, we use the extra data from MA (Massachusetts) for validation, and we select the best hyper-parameters according to the F1 score, which can mitigate the problem of label unbalance. Note that in this experiment, we do not have any knowledge for environments since we only have exactly one training environment. Therefore, we do not compare with IRM in this experiment, and EIIL can also be viewed as an alternative of IRM, which firstly splits data into two environments and then performs IRM."
        },
        {
            "heading": "A.7 Another Implementation of DIL Framework with KL-divergence: DIL-KL",
            "text": "Due to the space limitations, we leave another implementation of our DIL framework here, which chooses KL-divergence as \u03c1(\u00b7). The KL-divergence between two conditional distributions Q(Y |\u03a6) and Ptr(Y |\u03a6) is defined as:\nDKL(Q(Y |\u03a6)\u2016Ptr(Y |\u03a6)) = \u222b q(y, \u03c6) ln\nq(y|\u03c6) ptr(y|\u03c6) dyd\u03c6 (53)\nTo better characterize the Q(Y |\u03a6), we make some assumptions on the conditional distributions Q(Y |\u03a6) \u223c N (f\u03b8q (\u03a6), \u03c321) and Ptr(Y |\u03a6) \u223c N (f\u03b8tr (\u03a6), \u03c322) as:\nQ(Y |\u03a6) = 1\u221a 2\u03c6\u03c31 exp(\u2212 (Y \u2212 f\u03b8q (\u03a6))2 2\u03c321 ) (54)\nPtr(Y |\u03a6) = 1\u221a 2\u03c6\u03c32 exp(\u2212 (Y \u2212 f\u03b8tr (\u03a6)) 2 2\u03c322 ) (55)\nwhich is parameterized by \u03b8q and \u03b8tr. SinceQ \u2208 P\u03b10 , the sub-populationQ(\u03a6, Y ) can be formulated as:\nQ = N\u2211 i=1 wi\u03b4(\u03c6i, yi) where wi \u2208 [0, 1/\u03b10N ] and N\u2211 i=1 wi = 1 (56)\nThen we can optimize the sample weights w by:\nw\u2217 = arg max w\n{ D\u0302KL = 1\n2 n\u2211 i=1 wi [ 1 \u03c322 (yi \u2212 f\u03b8tr (\u03c6i))2 \u2212 1 \u03c321 (yi \u2212 f\u03b8q (\u03c6i))2 ]} (57)\ns.t. wi \u2208 [0, 1/\u03b10N ] and N\u2211 i=1 wi = 1 (58)\nwhere \u03b8q = arg min\u03b8 \u2211n i=1 wi(yi \u2212 f\u03b8(\u03c6i))2 and f\u03b8tr represents the empirical model trained on Ptr(Y,\u03a6) to approximate the Ptr(Y |\u03a6). Empirically, we alternatively learn the sample weights w and estimate the \u03b8q . And we find the noise scales \u03c31, \u03c32 do not have much influence, for which we simply let \u03c31 = \u03c32 in experiments."
        },
        {
            "heading": "A.7.1 Additional Experimental Results with DIL-KL",
            "text": "Here we add some additional results for DIL-KL.\nTable 6: Results for Colored MNIST and Colored SVHN. The first row indicates whether each method needs the environment label. The Oracle and Unbiased ERM represent the oracle performance and the performance of ERM without spurious correlation, respectively.\nMethod ERM DRO EIIL KerHRM IRM DIL-KL DIL-MMD Oracle UnbiasedERM Need Etr Label? % % % % ! % % - % Dataset 1: Colored MNIST (with label noise) Train Accuracy 0.867(\u00b10.002) 0.839(\u00b10.012) 0.891(\u00b10.012) 0.664(\u00b10.069) 0.861(\u00b10.014) 0.755(\u00b10.009) 0.765(\u00b10.018) 0.800 0.830(\u00b10.012) Test Accuracy 0.116(\u00b10.007) 0.420(\u00b10.018) 0.530(\u00b10.031) 0.580(\u00b10.205) 0.435(\u00b10.053) 0.699(\u00b10.017) 0.694(\u00b10.024) 0.800 0.734(\u00b10.005) Dataset 2: Colored SVHN (without label noise) Train Accuracy 1.000(\u00b10.003) 1.000(\u00b10.000) 0.881(\u00b10.010) 0.856(\u00b10.216) 0.995(\u00b10.001) 0.774(\u00b10.011) 0.821(\u00b10.065) 1.000 1.000(\u00b10.001) Test Accuracy 0.536(\u00b10.014) 0.575(\u00b10.009) 0.563(\u00b10.018) 0.585(\u00b10.029) 0.556(\u00b10.013) 0.725(\u00b10.009) 0.676(\u00b10.039) 1.000 0.797(\u00b10.006)\nFigure 3: Certified Results for the Classification task.(Section 5.1) (a) F1 Score and Testing Accuracy. (b) Overall testing accuracy.\nFigure 4: Results of the real-world dataset. (Section 5.3)"
        }
    ],
    "title": "Distributionally Invariant Learning: Rationalization and Practical Algorithms",
    "year": 2022
}