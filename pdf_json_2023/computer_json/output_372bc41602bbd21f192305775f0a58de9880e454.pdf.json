{
    "abstractText": "Incorporating human feedback has been shown to be crucial to align text generated by large language models to human preferences. We hypothesize that state-of-the-art instructional image editing models, where outputs are generated based on an input image and an editing instruction, could similarly benefit from human feedback, as their outputs may not adhere to the correct instructions and preferences of users. In this paper, we present a novel framework to harness human feedback for instructional visual editing (HIVE). Specifically, we collect human feedback on the edited images and learn a reward function to capture the underlying user preferences. We then introduce scalable diffusion model fine-tuning methods that can incorporate human preferences based on the estimated reward. Besides, to mitigate the bias brought by the limitation of data, we contribute a new 1M training dataset, a 3.6K reward dataset for rewards learning, and a 1K evaluation dataset to boost the performance of instructional image editing. We conduct extensive empirical experiments quantitatively and qualitatively, showing that HIVE is favored over previous state-of-the-art instructional image editing approaches by a large margin.",
    "authors": [
        {
            "affiliations": [],
            "name": "Shu Zhang"
        },
        {
            "affiliations": [],
            "name": "Xinyi Yang"
        },
        {
            "affiliations": [],
            "name": "Yihao Feng"
        },
        {
            "affiliations": [],
            "name": "Can Qin"
        },
        {
            "affiliations": [],
            "name": "Chia-Chih Chen"
        },
        {
            "affiliations": [],
            "name": "Ning Yu"
        },
        {
            "affiliations": [],
            "name": "Zeyuan Chen"
        },
        {
            "affiliations": [],
            "name": "Huan Wang"
        },
        {
            "affiliations": [],
            "name": "Silvio Savarese"
        },
        {
            "affiliations": [],
            "name": "Stefano Ermon"
        },
        {
            "affiliations": [],
            "name": "Caiming Xiong"
        },
        {
            "affiliations": [],
            "name": "Ran Xu"
        }
    ],
    "id": "SP:f62f5fffdbf005256b89a79b8bb18547c3afe5a3",
    "references": [
        {
            "authors": [
                "Jean-Baptiste Alayrac",
                "Jeff Donahue",
                "Pauline Luc",
                "Antoine Miech",
                "Iain Barr",
                "Yana Hasson",
                "Karel Lenc",
                "Arthur Mensch",
                "Katie Millican",
                "Malcolm Reynolds"
            ],
            "title": "Flamingo: a visual language model for few-shot learning",
            "venue": "arXiv preprint arXiv:2204.14198,",
            "year": 2022
        },
        {
            "authors": [
                "Omri Avrahami",
                "Ohad Fried",
                "Dani Lischinski"
            ],
            "title": "Blended latent diffusion",
            "venue": "arXiv preprint arXiv:2206.02779,",
            "year": 2022
        },
        {
            "authors": [
                "Omri Avrahami",
                "Dani Lischinski",
                "Ohad Fried"
            ],
            "title": "Blended diffusion for text-driven editing of natural images",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Yuntao Bai",
                "Andy Jones",
                "Kamal Ndousse",
                "Amanda Askell"
            ],
            "title": "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "venue": "arXiv preprint arXiv:2204.05862,",
            "year": 2022
        },
        {
            "authors": [
                "Omer Bar-Tal",
                "Dolev Ofri-Amar",
                "Rafail Fridman",
                "Yoni Kasten",
                "Tali Dekel"
            ],
            "title": "Text2live: Text-driven layered image and video editing",
            "venue": "In European Conference on Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Ralph Allan Bradley",
                "Milton E Terry"
            ],
            "title": "Rank analysis of incomplete block designs: I. the method of paired comparisons",
            "year": 1952
        },
        {
            "authors": [
                "Tim Brooks",
                "Aleksander Holynski",
                "Alexei A Efros"
            ],
            "title": "Instructpix2pix: Learning to follow image editing instructions",
            "venue": "arXiv preprint arXiv:2211.09800,",
            "year": 2022
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell",
                "Sandhini Agarwal",
                "Ariel Herbert-Voss",
                "Gretchen Krueger",
                "Tom Henighan",
                "Rewon Child",
                "Aditya Ramesh",
                "Daniel Ziegler",
                "Jeffrey Wu",
                "Clemens Winter",
                "Christopher Hesse",
                "Mark Chen",
                "Eric Sigler",
                "Mateusz Litwin",
                "Scott Gray",
                "Benjamin Chess",
                "Jack Clark",
                "Christopher Berner",
                "Sam McCandlish",
                "Alec Radford",
                "Ilyva Sutskever",
                "Dario Amodei"
            ],
            "title": "Language models are few-shot learners",
            "venue": "arXiv preprint arXiv:2005.14165,",
            "year": 2005
        },
        {
            "authors": [
                "Lili Chen",
                "Kevin Lu",
                "Aravind Rajeswaran",
                "Kimin Lee",
                "Aditya Grover",
                "Misha Laskin",
                "Pieter Abbeel",
                "Aravind Srinivas",
                "Igor Mordatch"
            ],
            "title": "Decision transformer: Reinforcement learning via sequence modeling",
            "venue": "Advances in neural information processing systems,",
            "year": 2021
        },
        {
            "authors": [
                "Paul Christiano",
                "Jan Leike",
                "Tom Brown",
                "Miljan Martic",
                "Shane Legg",
                "Dario Amodei"
            ],
            "title": "Deep reinforcement learning from human preferences. NeurIPS, 2017",
            "year": 2017
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805,",
            "year": 2018
        },
        {
            "authors": [
                "Prafulla Dhariwal",
                "Alexander Nichol"
            ],
            "title": "Diffusion models beat gans on image synthesis",
            "venue": "NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly",
                "Jakob Uszkoreit",
                "Neil Houlsby"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "arXiv preprint arXiv:2010.11929,",
            "year": 2020
        },
        {
            "authors": [
                "Rinon Gal",
                "Or Patashnik",
                "Haggai Maron",
                "Amit Bermano",
                "Gal Chechik",
                "Daniel Cohen-Or"
            ],
            "title": "Stylegan-nada: Clip-guided domain adaptation of image generators",
            "venue": "ACM Transactions on Graphics,",
            "year": 2022
        },
        {
            "authors": [
                "Amir Hertz",
                "Ron Mokady",
                "Jay Tenenbaum",
                "Kfir Aberman",
                "Yael Pritch",
                "Daniel Cohen-Or"
            ],
            "title": "Prompt-to-prompt image editing with cross attention control",
            "venue": "arXiv preprint arXiv:2208.01626,",
            "year": 2022
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Borja Ibarz",
                "Jan Leike",
                "Tobias Pohlen",
                "Geoffrey Irving",
                "Shane Legg",
                "Dario Amodei"
            ],
            "title": "Reward learning from human preferences and demonstrations in atari",
            "venue": "NeurIPS,",
            "year": 2018
        },
        {
            "authors": [
                "Phillip Isola",
                "Jun-Yan Zhu",
                "Tinghui Zhou",
                "Alexei A Efros"
            ],
            "title": "Image-to-image translation with conditional adversarial networks",
            "venue": "In CVPR,",
            "year": 2017
        },
        {
            "authors": [
                "Michael Janner",
                "Qiyang Li",
                "Sergey Levine"
            ],
            "title": "Offline reinforcement learning as one big sequence modeling problem",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Tero Karras",
                "Miika Aittala",
                "Timo Aila",
                "Samuli Laine"
            ],
            "title": "Elucidating the design space of diffusion-based generative models, 2022",
            "venue": "URL https://arxiv. org/abs/2206.00364,",
            "year": 2022
        },
        {
            "authors": [
                "Gwanghyun Kim",
                "Taesung Kwon",
                "Jong Chul Ye"
            ],
            "title": "Diffusionclip: Text-guided diffusion models for robust image manipulation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "Diederik Kingma",
                "Max Welling"
            ],
            "title": "Auto-encoding variational bayes",
            "venue": "arXiv preprint arXiv:1312.6114,",
            "year": 2013
        },
        {
            "authors": [
                "Aviral Kumar",
                "Aurick Zhou",
                "George Tucker",
                "Sergey Levine"
            ],
            "title": "Conservative q-learning for offline reinforcement learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Kimin Lee",
                "Hao Liu",
                "Moonkyung Ryu",
                "Olivia Watkins",
                "Yuqing Du",
                "Craig Boutilier",
                "Pieter Abbeel",
                "Mohammad Ghavamzadeh",
                "Shixiang Shane Gu"
            ],
            "title": "Aligning textto-image models using human feedback",
            "venue": "arXiv preprint arXiv:2302.12192,",
            "year": 2023
        },
        {
            "authors": [
                "Sergey Levine"
            ],
            "title": "Reinforcement learning and control as probabilistic inference: Tutorial and review",
            "venue": "arXiv preprint arXiv:1805.00909,",
            "year": 2018
        },
        {
            "authors": [
                "Sergey Levine",
                "Aviral Kumar",
                "George Tucker",
                "Justin Fu"
            ],
            "title": "Offline reinforcement learning: Tutorial, review, and perspectives on open problems",
            "venue": "arXiv preprint arXiv:2005.01643,",
            "year": 2020
        },
        {
            "authors": [
                "Junnan Li",
                "Dongxu Li",
                "Caiming Xiong",
                "Steven Hoi"
            ],
            "title": "Blip: Bootstrapping language-image pre-training for uni- 9 fied vision-language understanding and generation",
            "venue": "arXiv preprint arXiv:2201.12086,",
            "year": 2022
        },
        {
            "authors": [
                "Yuheng Li",
                "Haotian Liu",
                "Qingyang Wu",
                "Fangzhou Mu",
                "Jianwei Yang",
                "Jianfeng Gao",
                "Chunyuan Li",
                "Yong Jae Lee"
            ],
            "title": "Gligen: Open-set grounded text-to-image generation",
            "year": 2023
        },
        {
            "authors": [
                "Jun Hao Liew",
                "Hanshu Yan",
                "Daquan Zhou",
                "Jiashi Feng"
            ],
            "title": "Magicmix: Semantic mixing with diffusion models",
            "venue": "arXiv preprint arXiv:2210.16056,",
            "year": 2022
        },
        {
            "authors": [
                "Cheng Lu",
                "Yuhao Zhou",
                "Fan Bao",
                "Jianfei Chen",
                "Chongxuan Li",
                "Jun Zhu"
            ],
            "title": "Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps",
            "venue": "arXiv preprint arXiv:2206.00927,",
            "year": 2022
        },
        {
            "authors": [
                "Chenlin Meng",
                "Yutong He",
                "Yang Song",
                "Jiaming Song",
                "Jiajun Wu",
                "Jun-Yan Zhu",
                "Stefano Ermon"
            ],
            "title": "SDEdit: Guided image synthesis and editing with stochastic differential equations",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Chenlin Meng",
                "Yang Song",
                "Jiaming Song",
                "Jiajun Wu",
                "Jun- Yan Zhu",
                "Stefano Ermon"
            ],
            "title": "Sdedit: Image synthesis and editing with stochastic differential equations",
            "venue": "arXiv preprint arXiv:2108.01073,",
            "year": 2021
        },
        {
            "authors": [
                "Alex Nichol",
                "Prafulla Dhariwal",
                "Aditya Ramesh",
                "Pranav Shyam",
                "Pamela Mishkin",
                "Bob McGrew",
                "Ilya Sutskever",
                "Mark Chen"
            ],
            "title": "Glide: Towards photorealistic image generation and editing with text-guided diffusion models",
            "venue": "arXiv preprint arXiv:2112.10741,",
            "year": 2021
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeff Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray",
                "John Schulman",
                "Jacob Hilton",
                "Fraser Kelton",
                "Luke Miller",
                "Maddie Simens",
                "Amanda Askell",
                "Peter Welinder",
                "Paul Christiano",
                "Jan Leike",
                "Ryan Lowe"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "arXiv preprint arXiv:2203.02155,",
            "year": 2022
        },
        {
            "authors": [
                "Xue Bin Peng",
                "Aviral Kumar",
                "Grace Zhang",
                "Sergey Levine"
            ],
            "title": "Advantage-weighted regression: Simple and scalable off-policy reinforcement learning",
            "venue": "arXiv preprint arXiv:1910.00177,",
            "year": 1910
        },
        {
            "authors": [
                "Jan Peters",
                "Katharina Mulling",
                "Yasemin Altun"
            ],
            "title": "Relative entropy policy search",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2010
        },
        {
            "authors": [
                "Andre Susano Pinto",
                "Alexander Kolesnikov",
                "Yuge Shi",
                "Lucas Beyer",
                "Xiaohua Zhai"
            ],
            "title": "Tuning computer vision models with task rewards",
            "venue": "arXiv preprint arXiv:2302.08242,",
            "year": 2023
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "arXiv preprint arXiv:2103.00020,",
            "year": 2021
        },
        {
            "authors": [
                "Aditya Ramesh",
                "Prafulla Dhariwal",
                "Alex Nichol",
                "Casey Chu",
                "Mark Chen"
            ],
            "title": "Hierarchical text-conditional image generation with clip latents",
            "venue": "arXiv preprint arXiv:2204.06125,",
            "year": 2022
        },
        {
            "authors": [
                "Aditya Ramesh",
                "Mikhail Pavlov",
                "Gabriel Goh",
                "Scott Gray",
                "Chelsea Voss",
                "Alec Radford",
                "Mark Chen",
                "Ilya Sutskever"
            ],
            "title": "Zero-shot text-to-image generation",
            "venue": "In ICML,",
            "year": 2021
        },
        {
            "authors": [
                "Scott Reed",
                "Zeynep Akata",
                "Xinchen Yan",
                "Lajanugen Logeswaran",
                "Bernt Schiele",
                "Honglak Lee"
            ],
            "title": "Generative adversarial text-to-image synthesis",
            "venue": "In ICML,",
            "year": 2016
        },
        {
            "authors": [
                "Robin Rombach",
                "Andreas Blattmann",
                "Dominik Lorenz",
                "Patrick Esser",
                "Bj\u00f6rn Ommer"
            ],
            "title": "High-resolution image synthesis with latent diffusion models",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Chitwan Saharia",
                "William Chan",
                "Saurabh Saxena",
                "Lala Li",
                "Jay Whang",
                "Emily Denton",
                "Seyed Kamyar Seyed Ghasemipour",
                "Burcu Karagol Ayan",
                "S Sara Mahdavi",
                "Rapha Gontijo Lopes"
            ],
            "title": "Photorealistic text-to-image diffusion models with deep language understanding",
            "venue": "arXiv preprint arXiv:2205.11487,",
            "year": 2022
        },
        {
            "authors": [
                "Jeremy Scheurer",
                "Jon Ander Campos",
                "Jun Shern Chan",
                "Angelica Chen",
                "Kyunghyun Cho",
                "Ethan Perez"
            ],
            "title": "Training language models with language feedback",
            "venue": "arXiv preprint arXiv:2204.14146,",
            "year": 2022
        },
        {
            "authors": [
                "Christoph Schuhmann",
                "Richard Vencu",
                "Romain Beaumont",
                "Robert Kaczmarczyk",
                "Clayton Mullis",
                "Aarush Katta",
                "Theo Coombes",
                "Jenia Jitsev",
                "Aran Komatsuzaki"
            ],
            "title": "Laion-5b: An open large-scale dataset for training next generation imagetext models",
            "venue": "arXiv preprint arXiv:2111.02114,",
            "year": 2021
        },
        {
            "authors": [
                "John Schulman",
                "Filip Wolski",
                "Prafulla Dhariwal",
                "Alec Radford",
                "Oleg Klimov"
            ],
            "title": "Proximal policy optimization algorithms",
            "venue": "arXiv preprint arXiv:1707.06347,",
            "year": 2017
        },
        {
            "authors": [
                "Jascha Sohl-Dickstein",
                "Eric Weiss",
                "Niru Maheswaranathan",
                "Surya Ganguli"
            ],
            "title": "Deep unsupervised learning using nonequilibrium thermodynamics",
            "venue": "In Proceedings of International Conference on Machine Learning,",
            "year": 2015
        },
        {
            "authors": [
                "Jiaming Song",
                "Chenlin Meng",
                "Stefano Ermon"
            ],
            "title": "Denoising diffusion implicit models",
            "venue": "arXiv:2010.02502, October 2020",
            "year": 2020
        },
        {
            "authors": [
                "Yang Song",
                "Stefano Ermon"
            ],
            "title": "Generative modeling by estimating gradients of the data distribution",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Nisan Stiennon",
                "Long Ouyang",
                "Jeff Wu",
                "Daniel Ziegler",
                "Ryan Lowe",
                "Chelsea Voss",
                "Alec Radford",
                "Dario Amodei",
                "Paul Christiano"
            ],
            "title": "Learning to summarize from human feedback",
            "venue": "NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Bram Wallace",
                "Akash Gokul",
                "Nikhil Naik"
            ],
            "title": "Edict: Exact diffusion inversion via coupled transformations",
            "venue": "arXiv preprint arXiv:2211.12446,",
            "year": 2022
        },
        {
            "authors": [
                "Zhisheng Xiao",
                "Karsten Kreis",
                "Arash Vahdat"
            ],
            "title": "Tackling the generative learning trilemma with denoising diffusion gans",
            "venue": "arXiv preprint arXiv:2112.07804,",
            "year": 2021
        },
        {
            "authors": [
                "Tao Xu",
                "Pengchuan Zhang",
                "Qiuyuan Huang",
                "Han Zhang",
                "Zhe Gan",
                "Xiaolei Huang",
                "Xiaodong He"
            ],
            "title": "Attngan: Finegrained text to image generation with attentional generative adversarial networks",
            "venue": "In CVPR,",
            "year": 2018
        },
        {
            "authors": [
                "Jiahui Yu",
                "Yuanzhong Xu",
                "Jing Yu Koh",
                "Thang Luong",
                "Gunjan Baid",
                "Zirui Wang",
                "Vijay Vasudevan",
                "Alexander Ku",
                "Yinfei Yang",
                "Burcu Karagol Ayan"
            ],
            "title": "Scaling autoregressive models for content-rich text-to-image generation",
            "venue": "arXiv preprint arXiv:2206.10789,",
            "year": 2022
        },
        {
            "authors": [
                "Han Zhang",
                "Tao Xu",
                "Hongsheng Li",
                "Shaoting Zhang",
                "Xiaogang Wang",
                "Xiaolei Huang",
                "Dimitris Metaxas"
            ],
            "title": "Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks",
            "venue": "In ICCV,",
            "year": 2017
        }
    ],
    "sections": [
        {
            "text": "crucial to align text generated by large language models to human preferences. We hypothesize that state-of-the-art instructional image editing models, where outputs are generated based on an input image and an editing instruction, could similarly benefit from human feedback, as their outputs may not adhere to the correct instructions and preferences of users. In this paper, we present a novel framework to harness human feedback for instructional visual editing (HIVE). Specifically, we collect human feedback on the edited images and learn a reward function to capture the underlying user preferences. We then introduce scalable diffusion model fine-tuning methods that can incorporate human preferences based on the estimated reward. Besides, to mitigate the bias brought by the limitation of data, we contribute a new 1M training dataset, a 3.6K reward dataset for rewards learning, and a 1K evaluation dataset\nto boost the performance of instructional image editing. We conduct extensive empirical experiments quantitatively and qualitatively, showing that HIVE is favored over previous state-of-the-art instructional image editing approaches by a large margin."
        },
        {
            "heading": "1. Introduction",
            "text": "State-of-the-art (SOTA) text-to-image generative models have shown impressive performance in terms of both image quality and alignment between output images and captions [1, 45, 42]. Thanks to the impressive generation abilities of these models, instructional image editing has emerged as one of the most promising application scenarios for content generation [7]. Different from traditional image edit-\n*Denotes equal contribution. Primary contact: shu.zhang@salesforce.com. Our project page: https://shugerdou.github.io/hive/.\nar X\niv :2\n30 3.\ning [3, 16, 54, 30, 16, 54], where both the input and the edited caption are needed, instructional image editing only requires human-readable instructions. For instance, classic image editing approaches require an input caption \u201ca dog is playing a ball\u201d, and an edited caption \u201ca cat is playing a ball\u201d. In contrast, instructional image editing only needs editing instruction such as \u201cchange the dog to a cat\u201d. This experience mimics how humans naturally perform image editing.\nInstructional image editing was first proposed in InstructPix2Pix [7], which fine-tunes a pre-trained stable diffusion [45] by curating a triplet of the original image, instruction, and edited image, with the help of GPT-3 [8] and Prompt-to-Prompt image editing [16]. Though achieving promising results, the training data generation process of InstructPix2Pix lacks explicit alignment between editing instructions and edited images. As a result, the edited images may only partially align with the editing instructions (see the second column in Fig. 4). Moreover, as the editing instructions are written by human users, it is imperative that the resulting edited images align with the correct intentions and preferences of the users. For example, humans tend to make partial modifications to the original images, while such human preferences are not incorporated into the training data or learning objectives of InstructPix2Pix. Based on\nthe aforementioned observation and the recent accomplishments of ChatGPT [35], we propose to fine-tune stable diffusion using human feedback, such that the edited images better align with human editing instructions.\nFor large language models (LLMs) such as InstructGPT [35, 37], we often first learn a reward function to reflect what humans care about or prefer on the generated text output, and then leverage reinforcement learning (RL) algorithms such as proximal policy optimization (PPO) [49] to fine-tune the models. This process is often referred to as reinforcement learning with human feedback (RLHF). Leveraging RLHF to fine-tune diffusion-based generative models, however, remains challenging. Applying on-policy algorithms (e.g.,PPO) to maximize rewards during the finetuning process can be prohibitively expensive due to the hundreds or thousands of denoising steps required for each sampled image. Moreover, even with fast sampling methods [51, 55, 21, 31], it is still challenging to back-propagate the gradient signal to the parameters of the U-Net. 1\nTo address the technical issues described above, we propose Harnessing Human Feedback for Instructional Visual Editing (HIVE), which allows us to fine-tune diffusionbased generative models with human feedback. As shown\n1We present a rigorous discussion on the difficulty in Appendix C.1.\nin Fig. 2, HIVE consists of three steps: 1) We perform instructional supervised fine-tuning on the dataset that combines our newly collected 1M training data and the data from InstructPix2Pix. Since observing failure cases and suspecting the grounding visual components from image to instruction is still a challenging problem, we collect the 1M training data.\n2) For each input image and editing instruction pair, we ask human annotators to rank variant outputs of the finetuned model from step 1, which gives us a reward learning dataset. Using the collected dataset, we then train a reward model (RM) that reflects human preferences.\n3) We estimate the reward for each training data used in step 1, and integrate the reward to perform human feedback diffusion model finetuning using our proposed objectives presented in Sec. 3.4.\nOur main contribution can be summarized as follows: \u25cf To tackle the technical challenge of fine-tuning diffusion models using human feedback, we introduce two scalable fine-tuning approaches in Sec. 3.4, which are computationally efficient and offer similar costs compared with supervised fine-tuning. Moreover, we empirically show that human feedback is an essential component to boost the performance of instructional image editing models. \u25cf To explore the fundamental ability of instructional editing, we create a new dataset for HIVE that includes three sub-datasets: a new 1M training dataset, a 3.6K reward dataset for rewards learning, and a 1K evaluation dataset. \u25cf To increase the diversity of the data for training, we introduce cycle consistency augmentation based on the inversion of editing instruction. Our dataset has been enriched with one pair of data for bi-directional editing."
        },
        {
            "heading": "2. Related Work",
            "text": "Text-To-Image Generation. Text-to-image generative models have achieved tremendous success in the past decade. Generative adversarial nets (GANs) [15] is one of the fundamental methods that dominated the early-stage works [44, 58, 56]. Recently, diffusion models [50, 17, 52, 51] have achieved state-of-the-art text-to-image generation performance. [12, 34, 43, 42, 46, 57, 45, 29]. As a result, instead of training a text-to-image model from scratch, our work focuses on fine-tuning existing stable diffusion model [45], by leveraging additional human feedback. Image Editing. Similarly, diffusion models based image editing methods, e.g. SDEdit [33], BlendedDiffusion [3], BlendedLatentDiffusion [2], DiffusionClip [22], EDICT [54] or MagicMix [30], have garnered significant attention in recent years. To leverage a pre-trained image-text representation (e.g., CLIP [41], BLIP [28]) and text-to-image diffusion based pre-trained models [42, 46, 45], most existing works focus on text-based localized editing [5, 32, 16].\nPrompt-to-Prompt [16] edits the cross-attention layer in Imagen and stable diffusion to control the similarity of image and text prompt. More recently, InstructPix2Pix [7] tackled the problem via a different approach, requiring only humanreadable editing instruction to perform image editing. Our work follows the same direction as InstructPix2Pix[7] and leverages human feedback to address the misalignment between editing instructions and resulting edited images. Learning with Human Feedback. Incorporating human feedback into the learning process can be a highly effective way to enhance performance across various tasks such as fine-tuning LLMs [35, 4, 47, 37, 53], robotic simulation [10, 18], computer vision [40], and to name a few. Many existing works leverage PPO [49] to align to human feedback, however on-policy RL algorithms are not suitable for diffusion-based model fine-tuning (See more discussion in Appendix C.1). A concurrent work [25] also leverages human feedback to align text-to-image generation, where they intuitively consider reward as weights for negative loglikelihood. Our work tackles the problem of instructional image editing, where there are no ground truth data for the alignment between human-readable editing instructions and edited images, making human feedback extremely valuable."
        },
        {
            "heading": "3. Methodology",
            "text": "In this section, we introduce the new datasets we collected in Sec. 3.1, and explain the three major steps of HIVE in the rest of the section. Concretely, we introduce the instructional supervised training in Sec. 3.2, and describe how to train a reward model to score edited images in Sec. 3.3, then present two scalable fine-tuning methods to align diffusion models with human feedback in Sec. 3.4."
        },
        {
            "heading": "3.1. Dataset",
            "text": "Instructional Edit Training Dataset. We follow the same method of [7] to generate the training dataset. We collect 1K images and their corresponding captions. We ask three annotators to write three instructions and corresponding edited captions based on the collected input captions. Therefore, we obtain 9K prompt triplets: input caption, instruction, and edited caption. We fine-tune GPT-3 [8] with OpenAI API v0.25.0 [36] with them. We use the fine-tuned GPT-3 to generate five instructions and edited captions per input image-caption pair in Laion-Aesthetics V2 [48]. We observe that the captions from Laion are not always visually descriptive, so we use BLIP [28] to generate more diverse types of image captions. Later stable diffusion based Prompt-to-Prompt [16] is adopted to generate paired images. In addition, we design a cycle-consistent augmentation method (Sec. 3.2.1) to generate additional training data. We generate 1.04M training triplets in total. Combining the 281K training data from [7], we obtain 1.32M training image pairs along with instructions.\nReward Fine-tuning Dataset. We collect 3.6K imageinstruction pairs for the task of reward fine-tuning. Among them, 1.6K image-instruction pairs are manually collected, and the rest are from Laion-Aesthetics V2 with GPT-3 generated instructions. We use this dataset to ask annotators to rank various model outputs. Evaluation Dataset. We use two evaluation datasets: the test dataset in [7] for quantitative evaluation and a new 1K dataset collected for the user study. The quantitative evaluation dataset is generated following the same method as the training dataset, which means that the dataset does not contain real images. Our collected 1K dataset contains 200 real images, and each image is annotated with five humanwritten instructions. More details of annotation tooling, guidelines, and analysis are in Appendix A."
        },
        {
            "heading": "3.2. Instructional Supervised Training",
            "text": "We follow the instructional fine-tuning method in [7] with two major upgrades on dataset curation (Sec. 3.1) and cycle consistency augmentation (Sec. 3.2.1). A pre-trained stable diffusion model [45] is adopted as the backbone architecture. In instructional supervised training, the stable diffusion model has two conditions c = [cI , cE], where cE is the editing instruction, and cI is the latent space of the original input image. In the training process, a pre-trained auto-encoder [23] with encoder E and decoder D is used to convert between edited image x\u0303 and its latent representation z = E(x\u0303). The diffusion process is composed of an equally weighted sequence of denoising autoencoders \u03b8(zt, t, c), t = 1,\u22ef, T , which are trained to predict a denoised variant of their input zt, a noisy version of z. The objective of instructional supervised training is:\nL = EE(x\u0303),c, \u223cN (0,1),t[\u2225 \u2212 \u03b8(zt, t, c)\u222522] ."
        },
        {
            "heading": "3.2.1 Cycle Consistency Augmentation",
            "text": "Cycle consistency is a powerful technique that has been widely applied in image-to-image generation [59, 19]. It involves coupling and inverting bi-directional mappings of two variables X and Y , G \u2236 X \u2192 Y and F \u2236 Y \u2192 X , such that F (G(X)) \u2248X and vice versa. This approach has been shown to enhance generative mapping in both directions.\nWhile Instructpix2pix [7] considers instructional image editing as a single-direction mapping, we propose adding cycle consistency. Our approach involves a forward-pass editing step, F \u2236 x inst\u00d0\u2192 x\u0303. We then introduce instruction reversion to enable a reverse-pass mapping, R \u2236 x\u0303 \u223cinst\u00d0\u2192 x. In this way, we could close the loop of image editing as: x inst\u00d0\u2192 x\u0303 \u223cinst\u00d0\u2192 x, e.g. \u201cadd a dog\u201d to \u201cremove the dog\u201d. To ensure the effectiveness of this technique, we need to separate invertible and non-invertible instructions from\nthe dataset. We devised a rule-based method that combines speech tagging and template matching. We found that most instructions adhere to a particular structure, with the verb appearing at the start, followed by objects and prepositions. Thus, we grammatically tagged all instructions using the Natural Language Toolkit (NLTK) 2. We identified all invertible verbs and pairing verbs, and also analyzed the semantics of the objects and the prepositions used. By summarizing invertible instructions in predefined templates, we matched desired instructions. Our analysis revealed that 29.1% of the instructions in the dataset were invertible. We augmented this data to create more comprehensive training data, which facilitated cycle consistency. For more information, see Appendix B.1."
        },
        {
            "heading": "3.3. Human Feedback Reward Learning",
            "text": "The second step of HIVE is to learn a reward function R\u03c6(x\u0303, c), which takes the original input image, the text instruction condition c = [cI , cE], and the edited image x\u0303 that is generated by the fine-tuned stable diffusion as input, and outputs a scalar that reflects human preference.\nUnlike InstructGPT which only takes text as input, our reward modelR\u03c6(x\u0303, c) needs to measure the alignment between instructions and the edited images. To address the challenge, we present a reward model architecture in Fig. 3, which leverages pre-trained vision-language models such as BLIP [28]. More specifically, the reward model employs an image-grounded text encoder as the multi-modal encoder to take the joint image embedding and the text instruction as input and produce a multi-modal embedding. A linear layer is then applied to the multi-modal embedding to map it to a scalar value. More details are in Appendix B.2.\nWith the specifically designed network architecture, we train the reward function R\u03c6(x\u0303, c) with our collected reward fine-tuning dataset Dhuman induced in Sec. 3.1. For each input image cI and instruction cE pair, we have K edited images {x\u0303}Kk=1 ranked by human annotators, and denote the human preference of edited image x\u0303i over x\u0303j by x\u0303i \u227b x\u0303j . Then we can follow the Bradley-Terry model of preferences [6, 37] to define the pairwise loss function:\n`RM(\u03c6) \u2236= \u2212\u2211x\u0303i\u227bx\u0303j log [ exp(R\u03c6(x\u0303i,c)) \u2211k=i,j exp(R\u03c6(x\u0303k,c)) ] ,\nwhere (i, j) \u2208 [1 . . .K] and we can get (K 2 ) pairs of comparison for each condition c. Similar to [37], we put all the (K\n2 ) pairs for each condition c in a single batch to learn\nthe reward functions. We provide a detailed reward model training discussion in Appendix B.2."
        },
        {
            "heading": "3.4. Human Feedback based Model Fine-tuning",
            "text": "With the learned reward functionR\u03c6(c, x\u0303), the next step is to improve the instructional supervised training model by\n2https://www.nltk.org/\nreward maximization. As a result, we can obtain an instructional diffusion model that aligns with human preferences.\nTo address the difficulty of sampling-based methods as we discussed previously (also in Appendix C.1), we adapt offline RL techniques [27, 38, 9, 20] to fine-tune diffusion models, which allows us to align diffusion models with human feedback under acceptable training time and cost.\nThe RL fine-tuning techniques we present can be utilized for latent and pixel-based diffusion models. For simplicity, we introduce our methods for pixel-based generative diffusion models. With an input image and editing instruction condition c = [cI , cE], we define the edited image data distribution generated by the instructional supervised diffusion model as p(x\u0303\u2223 c), and the edited image data distribution generated by the current diffusion model we want to optimize as \u03c1(x\u0303\u2223 c) , then under the pessimistic principle of offline RL, we can optimize \u03c1 by the following objectives:\nJ(\u03c1) \u2236= max\u03c1Ec[Ex\u0303\u223c\u03c1(\u22c5\u2223c)[R\u03c6(x\u0303, c)]\u2212 \u03b7KL(\u03c1(x\u0303\u2223c)\u2223\u2223p(x\u0303\u2223c))] , (1)\nwhere \u03b7 is a hyper-parameter. The first term in Eq. (1) is the standard reward maximization in RL, and the second term is a regularization to stabilize learning, which is a widely used technique in offline RL [24], and also adopted for PPO fine-tuning of InstructGPT (a.k.a \u201cPPO-ptx\u201d) [37].\nTo avoid using sampling-based methods to optimize \u03c1, we can differentiate J(\u03c1) w.r.t \u03c1(x\u0303\u2223c) and solve for the optimal \u03c1\u2217(x\u0303\u2223c), resulting the following expression for the optimal solution of Eq. (1):\n\u03c1\u2217(x\u0303\u2223c)\u221d p(x\u0303\u2223c) exp (R\u03c6(x\u0303, c)/\u03b7) , (2)\nor \u03c1\u2217(x\u0303\u2223c) = 1 Z(c) p(x\u0303\u2223c) exp (R\u03c6(x\u0303, c)/\u03b7), with Z(c) = \u222b p(x\u0303\u2223c) exp (R\u03c6(x\u0303, c)/\u03b7)dx\u0303 being the partition function. A detailed derivation is in Appendix C.2.\nWeighted Reward Loss. The optimal target distribution \u03c1\u2217(x\u0303\u2223c) in Eq. (2) can be viewed as an exponential rewardweighted distribution for p(x\u0303\u2223c). Moreover, we have already obtained the empirical edited image data drawn from p(x\u0303\u2223c) when constructing the instructional editing dataset, and we can view the exponential reward weighted edited image x\u0303 from the instructional editing dataset as an empirical approximation of samples drawn from \u03c1\u2217(x\u0303\u2223c). Formally, we can fine-tune a diffusion model thus it generates data from \u03c1\u2217(x\u0303\u2223c), resulting in the weighted reward loss:\n`WR(\u03b8) \u2236= EE(x\u0303),c, \u223cN (0,1),t [\u03c9(x\u0303, c) \u22c5 \u2225 \u2212 \u03b8(zt, t, c)\u222522] ,\nwith \u03c9(x\u0303, c) = exp (R\u03c6(x\u0303, c)/\u03b7) being the exponential reward weight for edited image x\u0303 and condition c. Different from RL literature [39, 38] using exponential reward or advantage weights to learn a policy function, our weighted reward loss is derived for fine-tuning stable diffusion. Condition Reward Loss. We can also leverage the control-as-inference perspective of RL [26] to transform Eq. (2) to a conditional reward expression, thus we can directly view the reward as a conditional label to fine-tune diffusion models. Similar to [26], we introduce a new binary variable R\u2217 indicating whether human prefers the edited image or not, where R\u2217 = 1 denotes that human prefers the edited image, and R\u2217 = 0 denotes that human does not prefer, thus we have p(R\u2217 = 1 \u2223 x\u0303, c) \u221d exp (R\u03c6(x\u0303, c)). Together with Eq. (2), and applying Bayes rules gives us the following derivation: p(x\u0303\u2223c) exp (R\u03c6(x\u0303, c)/\u03b7) \u2236= q(x\u0303\u2223c) (p(R\u2217 = 1 \u2223 x\u0303, c))1/\u03b7\n= p(x\u0303\u2223c)(p(x\u0303\u2223 R \u2217 = 1, c)p(R\u2217 = 1\u2223 c) p(x\u0303\u2223c) ) 1/\u03b7\n\u221d p(x\u0303\u2223c)1\u22121/\u03b7p(x\u0303\u2223 R\u2217 = 1, c)1/\u03b7 ,\nwhere we drop p(R\u2217 = 1\u2223 c) since it is a constant w.r.t x\u0303. We can now view the reward for each edited image as an additional condition. Define the new condition c\u0303 = [cI , cE , cR], with cR as the reward label, we can fine-tune the diffusion model with the condition reward loss:\n`CR(\u03b8) = EE(x),c\u0303, \u223cN (0,1),t[\u2225 \u2212 \u03b8(zt, t, c\u0303)\u222522] .\nWe quantize the reward into five categories, based on the quantile of the empirical reward distribution of the training dataset, and convert the reward value into a text prompt. For instance, if the reward value of a training pair lies in the bottom 20% of the reward distribution of the dataset, then we convert the reward value as a text prompt condition cR \u2236=\u201cThe image quality is one out of five\u201d. And during the inference time to generate edited images, we fix the text prompt as cR \u2236=\u201cThe image quality is five out of five\u201d, indicating we want the generated edited images with the highest reward. We empirically find this technique improves the stability of fine-tuning."
        },
        {
            "heading": "4. Experiments",
            "text": "This section presents the experimental results and ablation studies of HIVE\u2019s technical choices, demonstrating the effectiveness of our method. For a fair comparison, we adopt the default guidance scale parameters in InstrcutPix2Pix. Through our experiments, we discovered that the conditional reward loss performs slightly better than the weighted reward loss, and therefore, we present our results based on the conditional reward loss. The detailed comparisons can be found in Sec. 4.2 and Appendix D.\nWe evaluate our method using two datasets: a synthetic evaluation dataset with 15,652 image pairs from [7] and a self-collected 1K evaluation dataset with real imageinstruction pairs. For the synthetic dataset, we follow InstructPix2Pix\u2019s quantitative evaluation metric and plot the\ntrade-offs between CLIP image similarity and directional CLIP similarity[14]. For the 1K dataset, we conduct a user study where for each instruction, the images generated by competing methods are reviewed and voted by three human annotators, and the winner is determined by majority votes."
        },
        {
            "heading": "4.1. Baseline Comparisons",
            "text": "We perform experiments with the same setup as InstructPix2Pix, where stable diffusion (SD) v1.5 is adopted. We compare three models: InstructPix2Pix, HIVE without human feedback, and HIVE with human feedback. We report the quantitative results on the synthetic evaluation dataset in Fig. 5. We observe that HIVE without human feedback improves notably over InstructPix2Pix (blue curve vs. green curve). Moreover, human feedback further boosts the per-\nformance of HIVE (red curve vs blue curve) by a large margin. In other words, with the same directional similarity value, HIVE with human feedback obtains better image consistency than that without feedback.\nTo test the effectiveness of HIVE on real-world images, we report the user study results on the 1K evaluation dataset. As shown in Fig. 6(a), HIVE without human feedback gets around 30% more votes than the InstructPix2Pix. The result is consistent with the user study on the synthetic dataset. We also demonstrate the user study outcome between HIVE without and with human feedback in Fig. 6(b). The user study indicates similar conclusions to the consistency plot, where the model with human feedback gets 10.8% more favorites than the model without human feedback.\nIn Fig. 4, we present representative edits that demonstrate the effectiveness of HIVE. The results show that while HIVE can partially learn editing instructions without human feedback, the reward model leads to better alignment between instruction and the edited image. For example, in the third row, HIVE without human feedback generates a doorlike object, but with the guidance of human feedback, the generated door matches human perception better.\nWe conducted another user study to assess the image quality of the edited images and found that HIVE with human feedback received the highest number of votes (as shown in Fig. 8). For example, in the last row of Fig. 4, the beer glass generated by InstructPix2Pix and HIVE without human feedback appears misleading or unfinished to human reviewers. We think that the reason is annotators\u2019 preferences are diverse, which leads to consistent and robust evaluation. Therefore, the image quality benefits from it. Additionally, our visual analysis of the results (Fig. 7) indicates that the HIVE model with human feedback tends to preserve the remaining part of the original image that is not instructed to be edited, while the models without human feedback lead to excessive image editing more often. For instance, in the first example of Fig. 7, HIVE with human feedback blends a pond naturally into the original image. While InstructPix2Pix and HIVE without human feedback fulfill the same instruction, but at the same time, alter the uninstructed part of the original background."
        },
        {
            "heading": "4.2. Ablation Study",
            "text": "Weighted Reward and Condition Reward Loss. We perform user study on HIVE with these two losses individually. As shown in Fig. 10, these two losses obtain very similar human preferences on the evaluation dataset. More comparisons are in Appendix D. SD v1.5 and v2.1. Considering the recent progress of stable diffusion, we upgrade the backbone of stable diffusion from v1.5 to the latest version v2.1, where OpenCLIP text encoder [48] replaces the CLIP text encoder [41], and\nexpect to see the backbone upgrading benefits. The quantitative consistency plot in Fig. 11 on the synthetic evaluation dataset confirms our assumption and shows that SD v2.1 improves performance over SD v1.5 by a small margin. Our user study in Fig. 12 using human feedback indicates a similar conclusion. We compare InstructPix2Pix v1.5 with v2.1 as well. An interesting observation is that we train InstructPix2Pix with SD v2.1 and show in Fig. 9 that its improvement over SD v1.5 is larger than HIVE. Failure Cases. We summarize representative failure cases in Fig. 13. First, some instructions cannot be understood. In the upper left example in Fig. 13, the prompt \u201czoom in\u201d or similar instructions can rarely be successful. We believe the root cause is current training data generation method fails to generate image pairs with this type of instruction. Second, counting and spatial reasoning are common failure cases (see the upper right example in Fig. 13). We find that the instruction \u201cone\u201d, \u201ctwo\u201d, or \u201con the right\u201d can lead to many undesired results. Third, the object understanding sometimes is wrong. In the bottom left example, the red color is changed on the wrong object. This is a common error. Other ablation studies can be found in Appendix D."
        },
        {
            "heading": "5. Conclusion and Discussion",
            "text": "In our paper, we introduce a novel framework called HIVE that enables instructional image editing with hu-\nman feedback. Our framework integrates human feedback, which is quantified as reward values, into the diffusion model fine-tuning process. We design two variants of the approach and both of them improve performance over previous state-of-the-art instructional image editing methods. Our work demonstrates instructional image editing with human feedback is a variable approach to align image generation with human preference, thus unlocking new opportunities and potential to scale up the model capabilities towards more powerful applications such as conversational image editing. While our method demonstrates impressive performance, we have also identified failure scenarios, as discussed in Sec. 4.2. In addition, it is possible that our trained model inherits bias and suffers from harmful content from pre-trained foundation models such as Stable Diffusion, GPT3 and BLIP. These limitations should be taken into consideration when interpreting our results, and we expect red teaming with human feedback to mitigate some of the risks in future work."
        },
        {
            "heading": "A. Data Collection and User Study",
            "text": "In the evaluation steps, we collect real-world images with instructions using Amazon Mechanical Turk (Mturk) 3. We randomly collect 200 real-world images. Then we ask Mturk annotators to write five instructions for each image, and encourage them to have wild imaginations and diversify the instruction types. We encourage annotators to not be limited to making the image realistic. For example, annotators can write \u201cadd a horse in the sky\u201d. A screenshot of the interface is illustrated in Fig. 14. We analyze the top five verbs and nouns in the evaluation dataset. It is shown in Fig. 16(a) that the verbs \u201cadd\u201d, \u201cchange\u201d, \u201cmake\u201d, \u201cremove\u201d and \u201cput\u201d make up around 85% of all verbs, which means that the editing instruction verbs have a long-tail distribution. In contrast, the distribution of nouns in Fig. 16(b) is close to uniform, where the top five nouns represent only around 20% of all nouns.\nIn user studies, we use Mturk to ask annotators to evaluate edited images. A screenshot of the interface is shown in Fig. 15. The annotators are provided with the original image, two edited images, and the editing instruction. They are asked to select the better edited image. The third option indicates that the edited images are equally good or equally bad. We ask three annotators to label one data sample, and use the majority votes to determine the results. We shuffle the edited images to avoid choosing the left image over the right and vice versa.\nB. Implementation Details\nB.1. Instructional Supervised Training\nWe use pre-trained stable diffusion models as the initial checkpoint to start instructional supervised training. We train HIVE on 40GB NVIDIA A100 GPUs for 500 epochs. We use the learning rate of 10\u22124 and the image size of 256. In the inference, we use 512 as the default image resolution.\n3https://www.mturk.com\nB.2. Human Feedback Rewards Learning\nAs shown in Fig. 3, the reward model takes in an input image cI , a text instruction cE , and an edited image x\u0303 and outputs a scalar value. Inspired by the recent work on the vision-language model, especially BLIP [28], we employ a visual transformer [13] as our image encoder and an image-grounded text encoder as the multimodal encoder for images and text. Finally, we set a linear layer on top of the image-grounded text encoder to map the multimodal embedding to a scalar value.\n(1) Visual transformer. We encode both the input image cI and edited image x\u0303 with the same visual transformer. Then we obtain the joint image embedding by concatenating the two image embeddings vit(cI), vit(x\u0303).\n(2) Image-grounded text encoder. The image-grounded text encoder is a multimodal encoder that inserts one additional cross-attention layer between the self-attention layer and the feed-forward network for each transformer block of BERT [11]. The additional cross-attention layer incorporates visual information into the text model. The output embedding of the image-grounded text encoder is used as the multimodal representation of the (cI , cE , x\u0303) triplet.\nWe gather a dataset comprising 3,634 images for the purpose of ranking. For each image, we generate five variant edited images, and ask an annotator to rank images from best to worst. Additionally, we ask annotators to indicate if any of the following scenarios apply: (1) all edited images are edited but none of them follow the instruction; (2) all edited images are visually the same as the original image; (3) all images are edited beyond the scope of instruction; (4) edited images have harmful content containing sex, violence, porn, etc; and (5) all edited images look similar to each other. We compare training reward models by filtering some/all of these options.\nWe note that a considerable portion of the collected data falls under at least one of the aforementioned categories, indicating that even for humans, ranking these images is challenging. As a result, we only use the data that did not include any nonrankable options in the reward model training. From a pool of 1,412 images, we select 1,285 for the training set, while the remaining images were used for the validation set. The reward model is trained on a dataset of comparisons between multiple model outputs on the same input. Each comparison sample contains an input image, an instruction, five edited versions of the image, and the corresponding rankings. We divide the dataset into training and validation sets based on the distribution of the corresponding instructions.\nWe apply the method in Sec. 3.3 on the reward data to develop a reward model. We initialize the reward model from the pre-trained BLIP, which was trained on paired images and captions using three objectives: image-text contrastive learning, image-text matching, and masked language modeling. Although there is a domain gap between BLIP\u2019s pre-training data and our reward data, where the captions in BLIP\u2019s data describe a single image, and the instructions in our data refer to the difference between image pairs. We hypothesized that leveraging the learned alignment between text and image in BLIP could enhance the reward model\u2019s ability to comprehend the relationship between the instruction and the image pairs.\nThe reward model is trained using 4 A100 GPUs for 10 epochs, employing a learning rate of 10\u22124 and weight decay of 0.05. The image encoder\u2019s and multimodal encoder\u2019s last layer outputs are utilized as image and multimodal representations, respectively. The encoders\u2019 final layer is the only fine-tuned component.\nWe use the trained reward model to generate a reward score on our training data. We perform two experiments. The first experiment takes the exponential rewards as weights and fine-tunes the diffusion model with weighted reward loss as described in Sec. 3.4. See Fig. 17 for the visualization of the method. The second experiment transforms the rewards to text prompts and fine-tunes the diffusion model with the condition reward loss as described in Sec. 3.4. The method is introduced in Fig. 2. We compare those two experiment settings, and results can be found in Sec. D.1."
        },
        {
            "heading": "C. Reward Maximization for Diffusion-Based Generative Models",
            "text": "C.1. Discussion on On-Policy based Reward Maximization for Diffusion Models\nDirectly adapting on-policy RL methods to the current training pipeline might be computationally expensive, but we do not conclude that sampling-based approaches are not doable for diffusion models. We consider developing more scalable sampling-based methods as future work.\nWe start the sampling methods derivation with the following objective:\nJ(\u03b8) \u2236= max \u03c0\u03b8 Ec\u223cpc[Ex\u0303\u223c\u03c0\u03b8(\u22c5\u2223c) [R\u03c6(x\u0303, c)] \u2212 \u03b7KL[pD(x\u0303\u2223c)\u2223\u2223\u03c0\u03b8(x\u0303\u2223c)]] , (3)\nwhere pc(c)pD(x\u0303\u2223c) is the joint distribution of the condition and edited images pair, and \u03c0\u03b8 denotes the policy or the diffusion model we want to optimize. Note that pD(x\u0303\u2223c) and \u03c0(x\u0303\u2223c) are swaped compared with the objective in Eq. (1). The second term in Eq. (3), is the KL Minimization formula for maximum likelihood estimation, equivalent to the loss of diffusion models. We represent the policy \u03c0\u03b8 via the reverse process of a conditional diffusion model:\n\u03c0\u03b8(x\u0303\u2223c) \u2236= p\u03b8(x\u03030\u2236T \u2223 c) = p0(x\u0303T ) T\n\u220f t=1 p\u03b8(x\u0303t\u22121\u2223x\u0303t; c) ,\nwhere p0(x\u0303T ) \u2236= N (x\u0303T ,0; I), and p\u03b8(x\u0303t\u22121\u2223x\u0303t; c) \u2236= N (x\u0303t\u2223\u00b5\u03b8(x\u0303t, t), \u03c32t )I is a Gaussian distribution, whose parameters are defined by score function \u03b8 and stepsize of noise scalings. So we can get a edited image sample x\u0303\n0 by running a reverse diffusion chain:\nx\u0303t\u22121\u2223x\u0303t = 1\u221a \u03b1t (x\u0303t \u2212 1 \u2212 \u03b1t\u221a 1 \u2212 \u03b1\u0304t \u03b8(x\u0303t, c, t)) + \u03c3tzt, z \u223c N (0, I), for t = T, . . . ,1 ,\nand x\u0303T \u223c N (0, I).\nAs a result, the reverse diffusion process can be viewed as a black box function defined by \u03b8 and noises \u2236= (zT , . . . ,z1, x\u0303T ), which we can view as a shared parameter network with noises. And for each layer, we can view the parameter is the score function \u03b8. Define the network as\nx\u03030 \u2236= f(c, ; \u03b8) , \u223c pnoise(\u22c5), c \u223c pc(\u22c5) ,\nwhere we can rewrite the first term as\nEc\u223cD, \u223cpnoise(\u22c5)[R\u03c6(f(c, ; \u03b8), c)] ,\nand we can optimize the parameter \u03b8 with path gradient if R\u22c5 is differentiable with path gradient. Similarly, suppose we want to optimize the first term via PPO. In that case, the main technical difficulty is to estimate \u2207\u03b8 log\u03c0\u03b8(x\u0303\u2223c), which can be estimated with the following derivation:\n\u2207\u03b8 log\u03c0\u03b8(x\u0303\u2223c) = \u2207\u03b8 log p\u03b8(x\u03030\u2236T \u2223 c) = T\n\u2211 t=1 \u2207\u03b8 log p\u03b8(x\u0303t\u22121\u2223x\u0303t; c) .\nNote that for both the end-to-end path gradient method and PPO we require to sample the reverse chain from x\u0303T to x\u03030, thus we can estimate \u2207\u03b8 log\u03c0(x\u0303\u2223c) using the empirical samples x\u03030\u2236T .\nFor the above two methods, to perform one step policy gradient update, we need to run the whole reverse chain to get an edited image sample x\u03030 to estimate the parameter gradient for the first term. As a result, the computational cost is the number of diffusion steps more extensive than the supervised fine-tuning cost. Now we need more than two days to fine-tune the stable diffusion model, so for standard LDM, where the number of steps is 1000, we can not finish the training within an acceptable training time. Even if we can use some fast sampling methods such as DDIM or variance preserve (VP) based noise scaling, the diffusion steps are still more than 5 or 10. Further, we haven\u2019t seen any previous work using such noise scaling to fine-tune stable diffusion. As a result, we think naive sampling methods might have high risk to obtain similar performance, compared with our current offline RL based approaches.\nC.2. Derivation for Eq. (2)\nTake a functional view of Eq. (2), and differentiate J(\u03c1) w.r.t \u03c1, we get\n\u2202J(\u03c1) \u2202\u03c1 =R\u03c6(x\u0303\u2223c) \u2212 \u03b7 (log \u03c1(x\u0303\u2223c) + 1 \u2212 log p(x\u0303\u2223c)) .\nSetting \u2202J(\u03c1) \u2202\u03c1 = 0 gives us\nlog \u03c1(x\u0303\u2223c) = 1 \u03b7 R\u03c6(x\u0303\u2223c) + log p(x\u0303\u2223c) \u2212 1 ,\n\u03c1(x\u0303\u2223c)\u221d p(x\u0303\u2223c) exp (R\u03c6(x\u0303, c)/\u03b7) .\nThus we can get the optimal \u03c1\u2217(x\u0303\u2223c)."
        },
        {
            "heading": "D. Additional Ablation Study",
            "text": "D.1. Weighted Reward and Conditional Reward Losses\nWe compare the weighted reward loss and conditional reward loss on the synthetic evaluation dataset. As shown in Fig. 20, the performances of these two losses are close to each other, while the conditional reward loss is slightly better. Therefore we adopt the conditional reward loss in all our experiments.\nD.2. Cycle Consistency\nWe analyze the impact of cycle consistency augmentation in Sec. 3.2.1. The top five augmentations in the cycle consistency are demonstrated in Fig. 18. As shown in Fig. 19, the cycle consistency augmentation improves the performance of HIVE by a notable margin.\nD.3. Training with Less Data\nWe analyze the effect of the training data size. We compare HIVE with SD v1.5 at four training dataset size ratios: 100%, 50%, 30% and 10%. As shown in Fig. 21, significantly decreasing the size of the dataset, e.g. 10% data, leads to worse ability to perform large image edits. On the other hand, reasonable decreasing dataset size can result in a similar yet slightly worse performance e.g. 50% data.\nD.4. Additional Visualized Results\nWe illustrate additional visualized results in Fig. 22."
        }
    ],
    "title": "HIVE: Harnessing Human Feedback for Instructional Visual Editing",
    "year": 2023
}