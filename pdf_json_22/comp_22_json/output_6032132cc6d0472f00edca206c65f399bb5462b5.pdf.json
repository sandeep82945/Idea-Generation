{
    "abstractText": "In clinical practice, a segmentation network is often required to continually learn on a sequential data stream from multiple sites rather than a consolidated set, due to the storage cost and privacy restriction. However, during the continual learning process, existing methods are usually restricted in either network memorizability on previous sites or generalizability on unseen sites. This paper aims to tackle the challenging problem of Synchronous Memorizability and Generalizability (SMG) and to simultaneously improve performance on both previous and unseen sites, with a novel proposed SMG-learning framework. First, we propose a Synchronous Gradient Alignment (SGA) objective, which not only promotes the network memorizability by enforcing coordinated optimization for a small exemplar set from previous sites (called replay buffer), but also enhances the generalizability by facilitating site-invariance under simulated domain shift. Second, to simplify the optimization of SGA objective, we design a Dual-Meta algorithm that approximates the SGA objective as dual meta-objectives for optimization without expensive computation overhead. Third, for efficient rehearsal, we configure the replay buffer comprehensively considering additional inter-site diversity to reduce redundancy. Experiments on prostate MRI data sequentially acquired from six institutes demonstrate that our method can simultaneously achieve higher memorizability and generalizability over state-of-the-art methods. Code is available at https://github.com/jingyzhang/SMG-Learning.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jingyang Zhang"
        },
        {
            "affiliations": [],
            "name": "Peng Xue"
        },
        {
            "affiliations": [],
            "name": "Ran Gu"
        },
        {
            "affiliations": [],
            "name": "Yuning Gu"
        },
        {
            "affiliations": [],
            "name": "Mianxin Liu"
        },
        {
            "affiliations": [],
            "name": "Yongsheng Pan"
        },
        {
            "affiliations": [],
            "name": "Zhiming Cui"
        },
        {
            "affiliations": [],
            "name": "Jiawei Huang"
        },
        {
            "affiliations": [],
            "name": "Lei Ma"
        },
        {
            "affiliations": [],
            "name": "Dinggang Shen"
        }
    ],
    "id": "SP:4a89bbfea1c7838b1e1f425bb701cf8f80dc49f1",
    "references": [
        {
            "authors": [
                "N. Bloch",
                "A. Madabhushi",
                "H. Huisman",
                "J. Freymann",
                "J. Kirby",
                "M. Grauer",
                "A. Enquobahrie",
                "C. Jaffe",
                "L. Clarke",
                "K. Farahani"
            ],
            "title": "NCI-ISBI 2013 challenge: Automated segmentation of prostate structures",
            "venue": "The Cancer Imaging Archive",
            "year": 2015
        },
        {
            "authors": [
                "F.M. Castro",
                "M.J. Ma\u0155\u0131n-Jim\u00e9nez",
                "N. Guil",
                "C. Schmid",
                "K. Alahari"
            ],
            "title": "End-to-end incremental learning",
            "venue": "Proceedings of the European conference on computer vision (ECCV). pp. 233\u2013248",
            "year": 2018
        },
        {
            "authors": [
                "C. Chen",
                "Q. Dou",
                "H. Chen",
                "P.A. Heng"
            ],
            "title": "Semantic-aware generative adversarial nets for unsupervised domain adaptation in chest x-ray segmentation",
            "venue": "International workshop on machine learning in medical imaging. pp. 143\u2013151. Springer",
            "year": 2018
        },
        {
            "authors": [
                "M. Delange",
                "R. Aljundi",
                "M. Masana",
                "S. Parisot",
                "X. Jia",
                "A. Leonardis",
                "G. Slabaugh",
                "T. Tuytelaars"
            ],
            "title": "A continual learning survey: Defying forgetting in classification tasks",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "year": 2021
        },
        {
            "authors": [
                "S.S. Dhruva",
                "J.S. Ross",
                "J.G. Akar",
                "B. Caldwell",
                "K. Childers",
                "W. Chow",
                "L. Ciaccio",
                "P. Coplan",
                "J. Dong",
                "Dykhoff",
                "H.J"
            ],
            "title": "Aggregating multiple real-world data sources using a patient-centered health-data-sharing platform",
            "venue": "NPJ digital medicine 3(1), 1\u20139",
            "year": 2020
        },
        {
            "authors": [
                "Q. Dou",
                "D. Coelho de Castro",
                "K. Kamnitsas",
                "B. Glocker"
            ],
            "title": "Domain generalization via model-agnostic learning of semantic features",
            "venue": "Advances in Neural Information Processing Systems 32, 6450\u20136461",
            "year": 2019
        },
        {
            "authors": [
                "C. Finn",
                "P. Abbeel",
                "S. Levine"
            ],
            "title": "Model-agnostic meta-learning for fast adaptation of deep networks",
            "venue": "International Conference on Machine Learning. pp. 1126\u2013 1135. PMLR",
            "year": 2017
        },
        {
            "authors": [
                "Y. Ganin",
                "E. Ustinova",
                "H. Ajakan",
                "P. Germain",
                "H. Larochelle",
                "F. Laviolette",
                "M. Marchand",
                "V. Lempitsky"
            ],
            "title": "Domain-adversarial training of neural networks",
            "venue": "The journal of machine learning research 17(1), 2096\u20132030",
            "year": 2016
        },
        {
            "authors": [
                "G. Gupta",
                "K. Yadav",
                "L. Paull"
            ],
            "title": "La-maml: Look-ahead meta learning for continual learning",
            "venue": "arXiv preprint arXiv:2007.13904",
            "year": 2020
        },
        {
            "authors": [
                "P.W. Koh",
                "S. Sagawa",
                "H. Marklund",
                "S.M. Xie",
                "M. Zhang",
                "A. Balsubramani",
                "W. Hu",
                "M. Yasunaga",
                "R.L. Phillips",
                "I Gao"
            ],
            "title": "Wilds: A benchmark of inthe-wild distribution shifts",
            "venue": "International Conference on Machine Learning. pp. 5637\u20135664. PMLR",
            "year": 2021
        },
        {
            "authors": [
                "G. Lem\u00e2\u0131tre",
                "R. Mart\u0301\u0131",
                "J. Freixenet",
                "J.C. Vilanova",
                "P.M. Walker",
                "F. Meriaudeau"
            ],
            "title": "Computer-aided detection and diagnosis for prostate cancer based on mono and multi-parametric MRI: A review",
            "venue": "Computers in Biology and Medicine 60, 8 \u2013 31",
            "year": 2015
        },
        {
            "authors": [
                "D. Li",
                "Y. Yang",
                "Y.Z. Song",
                "T.M. Hospedales"
            ],
            "title": "Learning to generalize: Metalearning for domain generalization",
            "venue": "Thirty-Second AAAI Conference on Artificial Intelligence",
            "year": 2018
        },
        {
            "authors": [
                "Z. Li",
                "C. Zhong",
                "R. Wang",
                "W.S. Zheng"
            ],
            "title": "Continual learning of new diseases with dual distillation and ensemble strategy",
            "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention. pp. 169\u2013178. Springer",
            "year": 2020
        },
        {
            "authors": [
                "G. Litjens",
                "R. Toth",
                "W. van de Ven",
                "C. Hoeks",
                "S. Kerkstra",
                "B. van Ginneken",
                "G. Vincent",
                "G. Guillard",
                "N. Birbeck",
                "J Zhang"
            ],
            "title": "Evaluation of prostate segmentation algorithms for MRI: The PROMISE12 challenge",
            "venue": "Medical Image Analysis 18(2), 359 \u2013 373",
            "year": 2014
        },
        {
            "authors": [
                "Q. Liu",
                "Q. Dou",
                "L. Yu",
                "P.A. Heng"
            ],
            "title": "MS-Net: Multi-site network for improving prostate segmentation with heterogeneous MRI data",
            "venue": "IEEE Transactions on Medical Imaging 39(9), 2713\u20132724",
            "year": 2020
        },
        {
            "authors": [
                "Q. Liu",
                "C. Chen",
                "J. Qin",
                "Q. Dou",
                "P.A. Heng"
            ],
            "title": "Feddg: Federated domain generalization on medical image segmentation via episodic learning in continuous frequency space",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 1013\u20131023",
            "year": 2021
        },
        {
            "authors": [
                "Q. Liu",
                "Q. Dou",
                "P.A. Heng"
            ],
            "title": "Shape-aware meta-learning for generalizing prostate MRI segmentation to unseen domains",
            "venue": "Medical Image Computing and Computer-Assisted Intervention. pp. 475\u2013485. Springer",
            "year": 2020
        },
        {
            "authors": [
                "D. Lopez-Paz",
                "M. Ranzato"
            ],
            "title": "Gradient episodic memory for continual learning",
            "venue": "Advances in neural information processing systems. pp. 6467\u20136476",
            "year": 2017
        },
        {
            "authors": [
                "M. McCloskey",
                "N.J. Cohen"
            ],
            "title": "Catastrophic interference in connectionist networks: The sequential learning problem",
            "venue": "Psychology of learning and motivation, vol. 24, pp. 109\u2013165. Elsevier",
            "year": 1989
        },
        {
            "authors": [
                "D. Nie",
                "L. Wang",
                "E. Adeli",
                "C. Lao",
                "W. Lin",
                "D. Shen"
            ],
            "title": "3-d fully convolutional networks for multimodal isointense infant brain image segmentation",
            "venue": "IEEE transactions on cybernetics 49(3), 1123\u20131136",
            "year": 2018
        },
        {
            "authors": [
                "S.A. Rebuffi",
                "A. Kolesnikov",
                "G. Sperl",
                "C.H. Lampert"
            ],
            "title": "ICARL: Incremental classifier and representation learning",
            "venue": "Proceedings of the IEEE conference on Computer Vision and Pattern Recognition. pp. 2001\u20132010",
            "year": 2017
        },
        {
            "authors": [
                "M. Riemer",
                "I. Cases",
                "R. Ajemian",
                "M. Liu",
                "I. Rish",
                "Y. Tu",
                "G. Tesauro"
            ],
            "title": "Learning to learn without forgetting by maximizing transfer and minimizing interference",
            "venue": "arXiv preprint arXiv:1810.11910",
            "year": 2018
        },
        {
            "authors": [
                "O. Ronneberger",
                "P. Fischer",
                "T. Brox"
            ],
            "title": "U-Net: Convolutional networks for biomedical image segmentation",
            "venue": "Medical Image Computing and Computer-Assisted Intervention. pp. 234\u2013241. Springer",
            "year": 2015
        },
        {
            "authors": [
                "O. Sener",
                "S. Savarese"
            ],
            "title": "Active learning for convolutional neural networks: A coreset approach",
            "venue": "arXiv preprint arXiv:1708.00489",
            "year": 2017
        },
        {
            "authors": [
                "Y. Shi",
                "J. Seely",
                "P.H. Torr",
                "N. Siddharth",
                "A. Hannun",
                "N. Usunier",
                "G. Synnaeve"
            ],
            "title": "Gradient matching for domain generalization",
            "venue": "arXiv preprint arXiv:2104.09937",
            "year": 2021
        },
        {
            "authors": [
                "K. Wang",
                "D. Zhang",
                "Y. Li",
                "R. Zhang",
                "L. Lin"
            ],
            "title": "Cost-effective active learning for deep image classification",
            "venue": "IEEE Transactions on Circuits and Systems for Video Technology 27(12), 2591\u20132600",
            "year": 2016
        },
        {
            "authors": [
                "L. Xiang",
                "Q. Wang",
                "D. Nie",
                "L. Zhang",
                "X. Jin",
                "Y. Qiao",
                "D. Shen"
            ],
            "title": "Deep embedding convolutional neural network for synthesizing ct image from t1-weighted mr image",
            "venue": "Medical image analysis 47, 31\u201344",
            "year": 2018
        },
        {
            "authors": [
                "J. Zhang",
                "R. Gu",
                "G. Wang",
                "L. Gu"
            ],
            "title": "Comprehensive importance-based selective regularization for continual segmentation across multiple sites",
            "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention. pp. 389\u2013399. Springer",
            "year": 2021
        },
        {
            "authors": [
                "J. Zhang",
                "G. Wang",
                "H. Xie",
                "S. Zhang",
                "N. Huang",
                "S. Zhang",
                "L. Gu"
            ],
            "title": "Weakly supervised vessel segmentation in x-ray angiograms by self-paced learning from noisy labels with suggestive annotation",
            "venue": "Neurocomputing 417, 114\u2013127",
            "year": 2020
        },
        {
            "authors": [
                "K. Zhou",
                "Z. Liu",
                "Y. Qiao",
                "T. Xiang",
                "C.C. Loy"
            ],
            "title": "Domain generalization: A survey",
            "venue": "arXiv preprint arXiv:2103.02503",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "Keywords: Continual Segmentation \u00b7 Generalizability \u00b7 Memorizability."
        },
        {
            "heading": "1 Introduction",
            "text": "Data aggregation of multiple clinical sites [15,5] is desired to train convolutional neural networks for medical image segmentation [29,20,27]. Compared with aggregating multi-site datasets as a large-scaled consolidated set, training network\nJ. Zhang and P. Xue - Equal contribution.\nar X\niv :2\n20 6.\n06 81\n3v 2\n[ ee\nss .I\nV ]\non a sequential data stream is more storage-efficient and privacy-preserving [28], where data of different sites arrives in sequence without consuming storage of most (or even all) data of previous sites [13]. However, consecutively fine-tuning model on only the data from different incoming sites would confront two challenges: 1) weak memorizability [19], causing significant performance drop on previously learned sites with data discrepancy due to their different imaging protocols; 2) poor generalizability [16], decreasing performance on unseen sites with out-of-distribution data [10] and thus impeding direct model deployment in clinical practice [3]. Therefore, a Synchronous Memorizability and Generalizability (SMG) is desirable for the network to simultaneously improve performance on both previous and unseen sites during consecutive training on a data stream.\nHowever, prevalent researches have identified network memorizability and generalizability as two isolated tasks with customized solutions, respectively. Specifically, Continual Learning (CL) [4] is proposed to enhance memorizability and mitigate forgetting for old knowledge of previously learned sites [28]. CL usually relies on the storage of a small exemplar set from previous sites (called replay buffer) for rehearsal [21,13,2]. Then a similarity constraint [18,9] is imposed between the gradient directions of losses on the replay buffer and the incoming site, enabling coordinated optimization for previous sites and thus relieving the model forgetting. Besides, Domain Generalization (DG) [30] aims to make a model generalize well to unseen sites, with a key aspect to consider learning invariance [8] across multiple sites with data discrepancy. In DG, the replay buffer collaborated with the data of incoming site serves as a union dataset to provide multi-site distribution for model training. Notably, it is arbitrarily split into virtual-train and virtual-test subsets within each minibatch to simulate real-world domain shift [12,6], boosting feature invariance for network generalizability.\nAlthough above-mentioned methods exhibit advantages on either network memorizability or generalizability, none of them is capable of Learning with SMG (i.e., SMG-Learning) that gains both advantages simultaneously [22] due to the following three reasons. First, for network memorizability, CL methods determine coordinated gradient for previous sites by optimizing a unidirectional transfer to the incoming site, which yet biases the learning process and hampers generalization capacity. Second, DG methods enhance feature invariance for a generalized model by using arbitrary data splitting to construct cross-site discrepancy, while without a particular consideration of performance balance between previous and incoming sites to consistently maintain network memorizability. Third, the replay buffer, used in both methods, is commonly populated with representative exemplars [24] from each individual previous site. This ignores data heterogeneity across sites and thus causes rehearsal redundancy, weakening both memorizability and generalizability. These motivate us to integrate the schemes of both CL and DG for SMG-Learning, by exploring gradient with coordination for previous sites and invariance across various sites for model updating, and further enforce additional inter-site diversity of representative exemplars in the replay buffer.\nIn this paper, we present, to our knowledge for the first, SMG-Learning framework for continual multi-site segmentation. Our contributions are: 1) We propose\na novel Synchronous Gradient Alignment (SGA) objective with two complementary operations, including an orientational alignment that determines coordinated gradient for the previous sites in replay buffer to strengthen memorizability, and an arbitrary alignment that encourages site-invariance across randomly split subsets to promote generalizability; 2) We design a Dual-Meta algorithm which approximates the SGA objective as dual meta-objectives for simplified optimization, avoiding costly computation of the second-order derivatives required in the naive direct optimization; 3) We propose a comprehensive configuration of replay buffer, where each exemplar is sampled considering intra-site representativeness and inter-site diversity, to relieve redundancy and improve rehearsal efficiency. We have evaluated our method on prostate MRI segmentation, using a sequential stream of multi-site datasets for model training. Experimental results show that our method achieves synchronously high memorizability and generalizability, and clearly outperforms the state-of-the-art CL and DG methods."
        },
        {
            "heading": "2 Method",
            "text": "For consecutive model training, we use a sequential data stream from T sites. Specifically, in round t \u2208 [1, T ] of this procedure, we can obtain only the data Dt of incoming site, and allow access to a small replay buffer P = {P1,P2, ...,Pt\u22121} that stores a small number of exemplars from previous sites for rehearsal. Data {Dt+1, ...,DT } of unseen sites is not used in the training process. Fig. 1 illustrates the proposed SMG-Learning framework, aiming to simultaneously perform well on both previous and unseen sites during the consecutive model training process."
        },
        {
            "heading": "2.1 Synchronous Gradient Alignment (SGA)",
            "text": "Learning only from data Dt causes overfitting on the incoming site and also cannot enable memorizability and generalizability. Fortunately, replay buffer P not only allows efficient rehearsal to mitigate model forgetting, but also provides multi-site data distribution for a generalized model. Therefore, it is crucial for SMG-Learning to collaborate the data of Dt and P for model training. Naive Joint Minimization (JM). For improved memorizability, a naive way is to adopt Joint Minimization (JM) for losses on Dt and P w.r.t. model parameters \u0398 for maintaining performance on both incoming and previous sites. Moreover, to strengthen generalizability, we collaborate Dt and P as a union set C, and then randomly split it into virtual-train Ctr and virtual-test Cte subsets within each minibatch, with a naive JM defined on both subsets for the training awareness of simulated cross-site discrepancy [6]. These JM objectives are formulated as:\nL {Dt,P} JM (\u0398) = LDt(\u0398) +LP(\u0398), and L {Ctr,Cte} JM (\u0398) = LCtr (\u0398) +LCte(\u0398). (1)\nHowever, these naive JM objectives would be optimized asymmetrically, e.g., for L {Dt,P} JM (\u0398), a dominated decrease comes from LDt(\u0398) which is easier to minimize without domain shift across sites [12], yet with a risk of increasing LP(\u0398). In this case, the model learns sufficiently on the incoming site while sacrificing performance on previous sites. In addition, the asymmetric optimization for L {Ctr,Cte} JM (\u0398) fails to obtain the consistent minimization on random subsets, and thus cannot extract site-invariant features that generalize well to unseen sites.\nDefinition of SGA Objective. To solve the problem of naive JM objectives, we need to simultaneously enforce coordinated descent of LDt(\u0398) and LP(\u0398) and extract invariant feature representation from LCtr (\u0398) and LCte(\u0398). Let us consider the gradients of losses on Dt, P, Ctr and Cte:\nGDt = \u2202LDt(\u0398)\n\u2202\u0398 , GP =\n\u2202LP(\u0398)\n\u2202\u0398 , GCtr =\n\u2202LCtr (\u0398)\n\u2202\u0398 , GCte =\n\u2202LCte(\u0398)\n\u2202\u0398 . (2)\nIf GDt and GP have a similar direction, i.e., inner product GDt \u00b7GP > 0, updating model with a step along GDt or GP improves performance on both Dt and P, achieving coordinated optimization and thus a high memorizability for previous sites. Moreover, if the directions of GCtr and GCte are similar, i.e., GCtr \u00b7GCte > 0, the features learned by either step can be invariant to the simulated discrepancy between arbitrarily split Ctr and Cte, strengthening potential generalizability.\nBased on this observation, we propose to simultaneously maximize GDt \u00b7GP for orientational gradient alignment between incoming site and replay buffer, and GCtr \u00b7GCte for arbitrary gradient alignment across randomly split subsets5. They enforce the gradient with coordination for previous sites and invariance across\n5 Orientational alignment seems a special case of arbitrary alignment when the subset splitting is Ctr = Dt and Cte = P, coincidently. However, orientational alignment cannot be omitted with a risk of suffering from potential interference [22], as empirically shown in Appendix A, due to arbitrary alignment for other subset splittings.\nvarious sites, benefiting to SMG-Learning. Formally, we name our objective as Synchronous Gradient Alignment (SGA), formalized by subtracting GDt \u00b7 GP and GCtr \u00b7GCte from the naive JM objectives with weight \u03b3 and \u03b2:\nLSGA(\u0398) = L {Dt,P} JM (\u0398) + L {Ctr,Cte} JM (\u0398)\u2212 \u03b3 GDt \u00b7GP \u2212 \u03b2 GCtr \u00b7GCte . (3)\nRelationship with CL and DG methods. If only the orientational alignment is used, the SGA objective can be regarded as a CL variant [9] with a difference: it ensures coordinated descent explicitly without inequality constraints [18]. If we only activate the arbitrary alignment, SGA objective borrows the spirit from DG with random data splitting [12] to simulate domain shift for feature invariance. A unified combination of them in our SGA objective leverages the complementary strengths of CL and DG and thereby makes the SMG-Learning promising."
        },
        {
            "heading": "2.2 Efficient Optimization by Dual-Meta Algorithm",
            "text": "A naive optimization of our SGA objective is costly since it requires to compute second-order derivatives due to the gradient inner product terms. To solve it, we propose a Dual-Meta algorithm that approximates SGA objective as dual metaobjectives for meta-optimization to avoid expensive second-order computation.\nFormally, the proposed SGA objective can be approximated as dual metaobjectives by the first-order Taylor expansion with omitted infinitesimal:\nLSGA(\u0398) = LDt(\u0398) + LP(\u0398\u2212 \u03b3GDt)\ufe38 \ufe37\ufe37 \ufe38 the first meta-objective +LCtr (\u0398) + LCte(\u0398\u2212 \u03b2GCtr )\ufe38 \ufe37\ufe37 \ufe38 the second meta-objective (4)\nwhere dual components correspond to the maximization of GDt \u00b7GP and GCtr \u00b7 GCte in SGA objective, respectively. Derivation details are given in Appendix B.\nBased on this formulation of dual meta-objectives [12], we perform a classical meta-optimization [7] including two stages. First, in inner-update, model parameters \u0398 are updated on Dt and Ctr with step size \u03b3 and \u03b2, respectively:\n\u0398Dt = \u0398\u2212 \u03b3GDt and \u0398Ctr = \u0398\u2212 \u03b2GCtr . (5)\nSecond, in meta-update, we rewrite the dual meta-objectives as LDt(\u0398)+LP(\u0398Dt) +LCtr (\u0398) + LCte(\u0398Ctr ). Notice that loss LP(\u0398Dt) and LCte(\u0398Ctr ) are computed on P and Ctr with the updated parameters \u0398Dt and \u0398Ctr , but optimized towards the original parameters \u0398 where only the first-order derivative is required."
        },
        {
            "heading": "2.3 Comprehensive Configuration of Replay Buffer",
            "text": "After completing the learning round t, we should sample a small exemplar set Pt from Dt of incoming site, and then add it into replay buffer P for rehearsal in the next learning round. For rehearsal efficacy, the selected exemplars are required with intra-site representativeness for the data distribution of Dt. Meanwhile, they are also demanded with inter-site diversity for exemplars of previous sites in\nP = {P1, ...,Pt\u22121} to reduce redundancy. Therefore, we propose a comprehensive configuration of Pt that considers both properties for replay buffer updating.\nIntra-Site Representativeness. Given a subject sit \u2208 Dt, we extract its feature vector f\u0398(s i t) by average-pooling its feature map from the bottleneck layer of the model parameterized with \u0398. Its intra-site representativeness is defined as the cosine similarity, i.e., sim(a, b) = aT b/\u2016a\u2016\u2016b\u2016, to the site prototype \u00b5t [24]:\nR(sit) = sim(f\u0398(s i t), \u00b5t), with \u00b5t = \u2211 skt\u2208Dt f\u0398(s k t )/NDt , (6)\nwhere \u00b5t is estimated as the mean feature over all NDt subjects in Dt. A high similarity R(sit) with respect to \u00b5t indicates a high representativeness for Dt.\nInter-Site Diversity. For a subject sit \u2208 Dt from a given individual site, intersite diversity V (sit) measures its diversity w.r.t. all previous sites in replay buffer:\nV (sit) = \u2211 Pk\u2208P\ndiv(sit,Pk)/NP , with div(sit,Pk) = min sjk\u2208Pk dsim(f\u0398(s i t), f\u0398(s j k)),\n(7) where div(sit,Pk) denotes the diversity of sit w.r.t. a previous site Pk \u2208 P in replay buffer, and the inter-site diversity V (sit) is the average over allNP previous sites. Notably, div(sit,Pk) is defined as the minimal feature dissimilarity, i.e., dsim(a, b)=\u2212sim(a, b), of all subjects in Pk, which is insensitive to outliers [26].\nComprehensive Configuration. Based on R(sit) and V (s i t), we adopt a hybrid measurement H(sit) with a weight \u03bb and then propose a comprehensive configuration of exemplar set Pt with size Ne to update replay buffer. The selected sit in Pt should have the top-Ne hybrid measurement, i.e., satisfying H(sit) \u2265 \u0393 (Ne):\nPt = {sit|sit \u2208 Dt, H(sit) \u2265 \u0393 (Ne)}, with H(sit) = R(sit) + \u03bbV (sit). (8)"
        },
        {
            "heading": "3 Experiments",
            "text": "Dataset. We employed a well-established multi-site T2-weighted MRI dataset for prostate segmentation [17]: 30 subjects in RUNMC [1] (Site A), 30 subjects in BMC [1] (Site B), 19 subjects in HCRUDB [11] (Site C), 13 subjects in UCL [14] (Site D), 12 subjects in BIDMC [14] (Site E) and 12 subjects in HK [14] (Site F). They were acquired using different protocols with in/through plane resolution ranging from 0.25/2.2-3 to 0.625/3.6 mm. For pre-processing, each image was resized to 384\u00d7384 in axial plane and normalized to zero mean and unit variance. In each site, we used 60%, 15% and 25% of subjects for training, validation and testing. This dataset is publicly released at https://liuquande.github.io/SAML/.\nExperimental Setting. For consecutive model training, we sequentially organized the training sets of site A\u2192B\u2192C\u2192D\u2192E as a training data stream SA\u2192E , while leaving site F as an unseen site without involved in training. Similarly, a reversed stream SF\u2192B with unseen site A was also validated in experiments.\nEvaluation Metrics. In each site, we evaluate segmentation accuracy by Dice Score Coefficient (DSC) and Average Surface Distance (ASD). Based on them, we define several specialized metrics [28,18] to evaluate memorizability and generalizability. First, for memorizability, we define a Backward Measure (BM) as the mean segmentation accuracy over previous and incoming sites for generic evaluation, and a Backward Transfer (BT) as the accuracy degradation on previous sites after learning on an incoming site for quantitative evaluation. Second, to evaluate generalizability, we define a Forward Measure (FM) as the segmentation accuracy on unseen site, and a Forward Transfer (FT) as the gap between the accuracy on unseen site obtained before and after learning on this unseen site. An advanced method should have high BM and FM for DSC while with low values for ASD, and its BT and FT should be as close as possible to zero.\nImplementation. We adopted a 2D-UNet [23] as segmentation backbone due to the large variance in slice thickness among different sites. For our SGA objective, both parameter \u03b3 and \u03b2 were empirically set as 5e\u22124 [12] for suitable trade-off. Weight \u03bb was set as 1.0 to comprehensively configure replay buffer, where each exemplar set contained only Ne = 2 subjects for storage efficiency. The network was trained by Dual-Meta algorithm, using Adam Optimizer with learning rate 5e\u22124, batch size 5 and iteration number 20K for inner-update and meta-update.\nComparison with State-of-the-arts. We compare our SMG-Learning framework, including SGA (optimized by Dual-Meta algorithm) and SGA+(c) (integrated with comprehensive replay buffer), with the state-of-the-art methods: 1) Baselines: FineTuning only on the incoming site, and Joint Minimization (JM) using only Eq.(1) with representative replay buffer [24]; 2) CL schemes: Gradient Episodic Memory (GEM) [18] and Continual Meta-learning (C-Meta) [9] for implicitly and explicitly coordinated optimization on previous sites; 3) DG schemes: Inter-Site Gradient Matching (ISGM) [25] for feature site-invariance and Generalized Meta-learning (G-Meta) [12] on randomly split subsets.\nAs listed in Table 1, baselines show a poor memorizability with the worst BM and BT values, and a weak generalizability with the worst FM and FT values. Although CL and DG perform better than baselines, their improvements are still limited either on memorizability or generalizability instead of in a synchronous way. The proposed SGA consistently outperforms these methods on almost all\nmetrics. Moreover, SGA+(c) achieves the best results on all two experimental settings, e.g., with an obvious advantage over C-Meta by 2.34% and 1.50% BT for DSC and also over G-Meta by 0.64% and 2.65% FT, indicating both high memorizability and generalizability of our SMG-Learning framework.\nFig. 2 visualizes the segmentation results on previous and unseen sites. The red curves on different sites are the results obtained immediately after training on these individual sites. They are close to the ground truth, revealing the high cross-site adaptation capacity of all methods. JM degrades the performance on previous site A and C after learning on incoming site E, and causes inaccuracy on unseen site F if not involved in training. Compared to other methods with only advantage either on previous or unseen site, our SGA achieves a synchronous superiority and especially SGA(+c) performs the best, with the highest overlap for previously obtained results and the highest accuracy on unseen site.\nEffect of Training Sequence Length. Fig. 3 shows the DSC curves on site A and F, revealing how the network memorizability and generalizability changes, as we gradually increase the training sequence ordered by site A\u2192B\u2192C\u2192D\u2192E. Intuitively, a longer sequence contains more complex multi-site information that would cause more optimization interference, thus aggravating model forgetting for previous site A. Contrarily, its covered more comprehensive data distribution helps the model generalize better to unseen site F. Notably, SGA and SGA(+c) consistently maintain higher accuracy than other methods with different lengths of training data sequence, confirming the stable efficacy of our SMG-Learning.\nAblation Study. C-Meta and G-Meta can be regarded as the variants of SGA using only orientational and arbitrary alignment, respectively. They exhibit limited advantages on either memorizability or generalizability, as shown in Table 1. Combining them in SGA outperforms each single component, and further using comprehensive replay buffer in SGA(+c) offers more superiority owing to its efficient rehearsal. Moreover, SGA optimized by Dual-Meta algorithm achieves a comparable segmentation accuracy with the direct optimization while costs only 3.31% computational time, revealing the efficiency of our Dual-Meta algorithm."
        },
        {
            "heading": "4 Conclusion",
            "text": "This paper presents a novel framework for SMG-Learning. We propose a SGA objective, cooperating with a comprehensive configuration of replay buffer, to enforce coordinated optimization for previous sites and feature invariance across sites. It is optimized by Dual-Meta algorithm without costly second-order computation. In the future, it is of interest to extend our method to federated learning with decentralized data from different sites for better privacy protection.\nAcknowledge This work was supported in part by National Natural Science Foundation of China (grant number 62131015), and Science and Technology Commission of Shanghai Municipality (STCSM) (grant number 21010502600)."
        }
    ],
    "title": "Learning towards Synchronous Network Memorizability and Generalizability for Continual Segmentation across Multiple Sites",
    "year": 2022
}