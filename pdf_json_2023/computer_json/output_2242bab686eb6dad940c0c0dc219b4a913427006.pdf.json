{
    "abstractText": "In this report, we present our champion solution for Ego4D Natural Language Queries (NLQ) Challenge in CVPR 2023. Essentially, to accurately ground in a video, an effective egocentric feature extractor and a powerful grounding model are required. Motivated by this, we leverage a two-stage pre-training strategy to train egocentric feature extractors and the grounding model on video narrations, and further fine-tune the model on annotated data. In addition, we introduce a novel grounding model GroundNLQ, which employs a multi-modal multiscale grounding module for effective video and text fusion and various temporal intervals, especially for long videos. On the blind test set, GroundNLQ achieves 25.67 and 18.18 for R1@IoU=0.3 and R1@IoU=0.5, respectively, and surpasses all other teams by a noticeable margin. Our code will be released at https://github. com/houzhijian/GroundNLQ.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zhijian Hou"
        },
        {
            "affiliations": [],
            "name": "Lei Ji"
        },
        {
            "affiliations": [],
            "name": "Difei Gao"
        },
        {
            "affiliations": [],
            "name": "Wanjun Zhong"
        },
        {
            "affiliations": [],
            "name": "Kun Yan"
        },
        {
            "affiliations": [],
            "name": "Chao Li"
        },
        {
            "affiliations": [],
            "name": "Wing-Kwong Chan"
        },
        {
            "affiliations": [],
            "name": "Chong-Wah Ngo"
        },
        {
            "affiliations": [],
            "name": "Nan Duan"
        },
        {
            "affiliations": [],
            "name": "Mike Zheng Shou"
        }
    ],
    "id": "SP:0e6b661e5003ed5601951d9f1e448428d038a5d3",
    "references": [
        {
            "authors": [
                "Kristen Grauman",
                "Andrew Westbury",
                "Eugene Byrne",
                "Zachary Chavis",
                "Antonino Furnari",
                "Rohit Girdhar",
                "Jackson Hamburger",
                "Hao Jiang",
                "Miao Liu",
                "Xingyu Liu"
            ],
            "title": "Ego4d: Around the world in 3,000 hours of egocentric video",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Kevin Qinghong Lin",
                "Alex Jinpeng Wang",
                "Mattia Soldan",
                "Michael Wray",
                "Rui Yan",
                "Eric Zhongcong Xu",
                "Difei Gao",
                "Rongcheng Tu",
                "Wenzhe Zhao",
                "Weijie Kong"
            ],
            "title": "Egocentric video-language pretraining@ ego4d challenge 2022",
            "venue": "arXiv preprint arXiv:2207.01622,",
            "year": 2022
        },
        {
            "authors": [
                "Guo Chen",
                "Sen Xing",
                "Zhe Chen",
                "Yi Wang",
                "Kunchang Li",
                "Yizhuo Li",
                "Yi Liu",
                "Jiahao Wang",
                "Yin-Dong Zheng",
                "Bingkun Huang"
            ],
            "title": "Internvideo-ego4d: A pack of champion solutions to ego4d challenges",
            "venue": "arXiv preprint arXiv:2211.09529,",
            "year": 2022
        },
        {
            "authors": [
                "Naiyuan Liu",
                "Xiaohan Wang",
                "Xiaobo Li",
                "Yi Yang",
                "Yueting Zhuang"
            ],
            "title": "Reler@ zju-alibaba submission to the ego4d natural language queries challenge 2022",
            "venue": "arXiv preprint arXiv:2207.00383,",
            "year": 2022
        },
        {
            "authors": [
                "Sicheng Mo",
                "Fangzhou Mu",
                "Yin Li"
            ],
            "title": "A simple transformer-based model for ego4d natural language queries challenge",
            "venue": "arXiv preprint arXiv:2211.08704,",
            "year": 2022
        },
        {
            "authors": [
                "Zhijian Hou",
                "Wanjun Zhong",
                "Lei Ji",
                "Difei Gao",
                "Kun Yan",
                "Wing-Kwong Chan",
                "Chong-Wah Ngo",
                "Zheng Shou",
                "Nan Duan"
            ],
            "title": "An efficient coarse-to-fine alignment framework@ ego4d natural language queries challenge 2022",
            "venue": "arXiv preprint arXiv:2211.08776,",
            "year": 2022
        },
        {
            "authors": [
                "Santhosh Kumar Ramakrishnan",
                "Ziad Al-Halah",
                "Kristen Grauman"
            ],
            "title": "Naq: Leveraging narrations as queries to supervise episodic memory",
            "venue": "arXiv preprint arXiv:2301.00746,",
            "year": 2023
        },
        {
            "authors": [
                "Hao Zhang",
                "Aixin Sun",
                "Wei Jing",
                "Joey Tianyi Zhou"
            ],
            "title": "Span-based localizing network for natural language video localization",
            "venue": "arXiv preprint arXiv:2004.13931,",
            "year": 2020
        },
        {
            "authors": [
                "Chen-Lin Zhang",
                "Jianxin Wu",
                "Yin Li"
            ],
            "title": "Actionformer: Localizing moments of actions with transformers",
            "venue": "In Computer Vision\u2013ECCV 2022: 17th European Conference,",
            "year": 2022
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In International conference on machine learning,",
            "year": 2021
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "In NAACL-HLT,",
            "year": 2019
        },
        {
            "authors": [
                "Iz Beltagy",
                "Matthew E Peters",
                "Arman Cohan"
            ],
            "title": "Longformer: The long-document transformer",
            "venue": "arXiv preprint arXiv:2004.05150,",
            "year": 2020
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Piotr Doll\u00e1r",
                "Ross Girshick",
                "Kaiming He",
                "Bharath Hariharan",
                "Serge Belongie"
            ],
            "title": "Feature pyramid networks for object detection",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Zhan Tong",
                "Yibing Song",
                "Jue Wang",
                "Limin Wang"
            ],
            "title": "Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training",
            "venue": "arXiv preprint arXiv:2203.12602,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "The Ego4D [1] NLQ challenge aims to localize a temporal window within a long-form first-person video corresponding to a natural language (NL) question. Existing approaches (summarized in Table 1) primarily explore two research directions: 1) pre-training a representative egocentric feature extractor using the Ego4D video-narration dataset [2, 3], or 2) deploying a powerful grounding model that processes the interaction between video and text features to predict the relevant temporal interval [4, 5, 6].\nTo develop a discriminative egocentric video representation, EgoVLP [2] and InternVideo [3] have been pre-trained on the Ego4D video-narration dataset, thus becoming go-to egocentric feature extractors. Recently, NaQ [7] proposes\n* This work was done during the first author\u2019s internship in MSR Asia.\nan effective data augmentation strategy to mitigate the data scarcity problem for egocentric video grounding, achieving substantial performance gains. Inspired by this, we employ a two-stage pre-training pipeline that includes both the feature extractor and model pre-training, followed by finetuning the grounding model on annotation data.\nThe long video input presents another challenge for this task. Existing models [8] are primarily designed to handle shorter videos. To address long videos, related works attempted to adapt literature models by sparse sampling [1, 4] or window-slicing [6]. However, these approaches lead to information loss or insufficient global contextual encoding. To input the complete video at once, the model of the Badger team [5] re-purposes ActionFormer [9] for video grounding, which integrates local self-attention to extract a feature pyramid from an input video. However, they adopt a late-stage multi-modal fusion network, which results in ineffective multi-modal interaction. Motivated by this, we introduce GroundNLQ, which incorporates well-designed multi-modal multi-scale modules. Our module integrates the textual query and long video deeply in the early stage and constructs the text-aware video feature pyramid to capture temporal intervals of various lengths.\nThrough the two-stage pre-training pipeline for both the egocentric video feature and the grounding model, our single model GroundNLQ and final ensemble submission surpasses all other teams by a noticeable margin regarding every evaluation metric (in Table 2). Notably, we also achieve a sizeable performance boost compared with the winner approach of the ECCV22 workshop (i.e., 10.06% +81%\u2212\u2212\u2212\u219218.18% on R1@0.5 in Table 1).)"
        },
        {
            "heading": "2. Methodology",
            "text": "As depicted in Figure 1, we implement a two-stage pretraining process using Ego4D narrations, followed by finetuning the model on the annotated dataset. Additionally, we\nar X\niv :2\n30 6.\n15 25\n5v 1\n[ cs\n.C V\n] 2\n7 Ju\ndesign a novel grounding model, GroundNLQ. This section elaborates on the specifics of both the features (\u00a7 2.1) and the grounding model (\u00a7 2.2), as well as the implementation details of the training pipeline (\u00a7 2.3)."
        },
        {
            "heading": "2.1. Text and Video Representations",
            "text": "For video features, we employ video feature extractors that have been pre-trained on Ego4D narrations, namely, InternVideo [3] and EgoVLP [2]. We concatenate these features along the channel dimension to formulate the final video feature, mirroring the approach used in [3]. Specifically, we extract each video feature for a short snippet (approximately 0.53s) and feed the long video input sequence into our grounding model. In contrast, models such as VSLNet [1, 8] and ReLER [4] employ sparse sampling for video features to generate a fixed-length frame sequence (e.g., 128 features in VSLNet). For textual features, we use the CLIP [10] text encoder to extract the textual token feature sequence of the natural language (NL) query.\nAdditionally, we conduct feature projection to map the video and text features into the same embedding space. Specially, we use 2 layers of the 1D Convolution network to project video features, and 2 layers of the linear network to project text features, respectively. Each layer is also stacked with layer normalization and ReLU activation. After the\nprojection, we add the sin-cos position embedding as in [11] into the projected video features to embed temporal-aware position information."
        },
        {
            "heading": "2.2. GroundNLQ Model",
            "text": "Multi-modal Transformer Encoder. For the text encoder, we stack 4 vanilla transformer encoder blocks to learn the contextual token features. Each text transformer encoder block incorporates the multi-head selfattention (MHA) layer and feed-forward network (FFN).\nRegarding the video encoder, we use another stack of four transformer encoder blocks to learn text-aware video features. Each video transformer encoder block consists of a local MHA layer, a cross-modal attention layer, and an FFN. We leverage window-based local self-attention to decrease the high computational burden of long video modeling, as demonstrated in previous studies [9, 12]. This approach efficiently handles long sequence inputs while significantly reducing computation costs and maintaining comparable performance. Additionally, we integrate crossmodal attention to infuse text information into the video features, enabling the early-stage fusion of text and video features. This contrasts with the models by the Badgers team [5], which employ late-stage fusion following unimodal feature encoding. This early-stage fusion is critical,\ngiven the inherent challenge of aligning video content with textual queries; it ensures comprehensive learning and fusion of multi-modal information.\nMulti-scale Transformer Encoder. We apply a stack of 6 multi-scale transformer encoder blocks to learn the textaware video feature pyramid from the text-aware video features. Each block within the multi-scale transformer encoder consists of an MHA layer, a max-pooling layer, and an FFN. We use a max-pooling operator with a stride of 2 to downsample the features, facilitating the capture of longer intervals. The feature pyramid output is a combined result of all 6 layers\u2019 outputs with the inputs. This produces 7 text-aware video feature sequence levels of varying lengths for moment prediction. Contrary to the approach in ReLER [4], which slices the video input sequence into different numbers of clip segments using varying window lengths, our multi-scale mechanism does not employ splitting and is more akin to the feature pyramid network [13].\nPrediction Head and Loss Function. Lastly, we implement two layers of a 1D Convolution network for the classification and regression heads, respectively, following a similar approach as [5, 9]. The classification head outputs a probability score for each interval feature of the pyramid, while the regression head outputs the boundary distances from the current interval. The model has dual learning objectives: background/foreground classification and boundary regression. The loss function is a sum of the binary classification loss and IoU regression losses."
        },
        {
            "heading": "2.3. Implementation Details",
            "text": "Egocentric Video Feature Extractor Pre-training. EgoVLP [2] undergoes pre-training with 3.8M paired egocentric video clips and corresponding narrations (i.e., EgoClip [2]) via a CLIP-like contrastive loss. Despite a significantly smaller amount of dataset than the 400M image-text pairs in CLIP or 136M video clip-text pairs in Howto100M, EgoVLP demonstrates the necessity of egocentric data pre-training for egocentric video tasks.\nAdditionally, InternVideo [3] translates textual narrations into corresponding verb and noun class labels for each EgoClip video clip, utilizing the VideoMAE [14] backbone to train the 1-of-K classification for separate verb and action labels. The result is three video features pre-trained on Ego4D narrations: EgoVLP, InternVideo-Verb, and InternVideo-Noun. We concatenate these features along the channel dimension for the grounding model\u2019s video input.\nGrounding Model Pre-training. NaQ [7] presents a data augmentation strategy that converts standard Ego4D videotext narrations into training data for the grounding model. We adhere to NaQ\u2019s data collection steps, assembling all training videos for episode memory benchmarks and collating corresponding <video, narration, moment> tuples. Ground-truth temporal boundaries are initialized using the EgoClip boundary and refined via the temporal response jittering strategy [7]. We then use this pre-training data to train our model with our training loss function. The training utilizes 4 V100 GPUs with a batch size of 4 per GPU, lasting approximately 4 days. The total and warmup epochs are 10 and 4, respectively, with a maximum learning rate of 2e-4. The best-performing model epoch is chosen based on inference on the validation split in zero-shot mode.\nGrounding Model Fine-tuning. The fine-tuning stage involves initializing the prediction head from scratch due to moment boundary discrepancies between pre-training and training data. We initially train our model on the train split, determine the optimal epoch number, and report results on the validation split. For leaderboard submission, we train on the combined train+val splits and apply the model with the optimal epoch number to the private test split. This stage uses 2 V100 GPUs with a batch size of 2 per GPU, taking about 8 hours. The total and warmup epochs are 10 and 4, respectively, with a maximum learning rate of 1e-4.\nModel Ensemble We also explore a model variant called GroundNLQ\u22c6, wherein the primary modification is the integration of the cross-modal layer with the multi-scale trans-\nformer encoder. This new multi-scale block comprises the MHA layer, the max-pooling layer, the cross-modal layer, and the FFN. For leaderboard submission, we ultimately ensemble the predictions from two models (i.e., GroundNLQ and GroundNLQ\u22c6)."
        },
        {
            "heading": "3. Experiment",
            "text": ""
        },
        {
            "heading": "3.1. Data Analysis",
            "text": "Table 3 presents the dataset statistics for training the model. For feature pre-training, Ego4D videos are segmented into short clips to learn video representation. For model training, we employ original long videos to train the grounding model."
        },
        {
            "heading": "3.2. Result Analysis",
            "text": "Table 2 displays our primary leaderboard results. Our best leaderboard submission originates from the ensembled model. Significantly, our results outpace all other teams by a substantial margin across all metrics, particularly the R5 metrics. Furthermore, our single model also exhibits robust performance, thereby demonstrating the superior efficacy of GroundNLQ and the staged training pipeline."
        },
        {
            "heading": "3.3. Ablation Analysis",
            "text": "Table 4 provides an ablation study on both features and models. Table 4a underlines the importance of all features. Using the EgoVLP feature alone, the R1@0.3 performance declines from 26.98% to 21.81%. This noticeable gap leads\nExample-A: Where is the hose pipe before I sprayed the air filter?\nto diminished performance in further ensembles. Additionally, we explore the fine-grained ranking in CONE [6]. We employ the top5 prediction of the GroundNLQ model and re-rank these predictions based on the matching score of the EgoVLP model. This also results in a drop in R1@0.3 performance from 26.98% to 20.52%, and the further ensemble does not enhance performance. Table4b emphasizes the importance of the pre-training stage. As found in NaQ [7], pretraining the grounding model substantially enhances performance."
        },
        {
            "heading": "3.4. Limitation Analysis",
            "text": "Figure 2 shows two failure examples contrasting the groundtruth and predicted segments. In Example-A, the error arises from the imprecise boundary. The first prediction of GroundNLQ captures not only the ground truth event (\u201cI pick up the hose pipe\u201d) but also subsequent events (\u201cI spray the air filter with the hose pipe and then put back the hose pipe\u201d). Consequently, the matched IoU with the ground truth is below the 0.3 threshold, classifying the prediction as false. In Example-B, the error results from an inadequate understanding of the video content and textual query\u2019s nuances. The first prediction of GroundNLQ identifies the event where \u201dI enter the room, take off the shoes and put the shoes in the shoebox near the wall\u201d. While the key objects (shoes and wall) are matched, the alignment neglects detailed attributes of the shoes (their color is white, and they lean against the wall)."
        },
        {
            "heading": "4. Conclusion",
            "text": "We present our solution to the Ego4D natural language queries challenge in CVPR 2023. Through our experiments, we highlight the importance of the two-stage pre-training of both the video feature extractor and the grounding model. Regarding the model, we find the importance of early-stage fusion between long video and textual query in the multimodal multi-scale module. Moreover, we also identify further challenges of the imprecise boundary and fine-grained attribute understanding issues for future work."
        }
    ],
    "title": "GroundNLQ @ Ego4D Natural Language Queries Challenge 2023",
    "year": 2023
}