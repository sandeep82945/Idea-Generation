{
    "abstractText": "Crafting effective deep learning models for medical image analysis is a complex task, particularly in cases where the medical image dataset lacks significant inter-class variation. This challenge is further aggravated when employing such datasets to generate synthetic images using generative adversarial networks (GANs), as the output of GANs heavily relies on the input data. In this research, we propose a novel filtering algorithm called Cosine Similarity-based Image Filtering (CosSIF). We leverage CosSIF to develop two distinct filtering methods: Filtering Before GAN Training (FBGT) and Filtering After GAN Training (FAGT). FBGT involves the removal of real images that exhibit similarities to images of other classes before utilizing them as the training dataset for a GAN. On the other hand, FAGT focuses on eliminating synthetic images with less discriminative features compared to real images used for training the GAN. Experimental results reveal that employing either the FAGT or FBGT method with modern transformer and convolutional-based networks leads to substantial performance gains in various evaluation metrics. FAGT implementation on the ISIC-2016 dataset surpasses the baseline method in terms of sensitivity by 1.59% and AUC by 1.88%. Furthermore, for the HAM10000 dataset, applying FABT outperforms the baseline approach in terms of recall by 13.75%, and with the sole implementation of FAGT, achieves a maximum accuracy of 94.44%. Code and implementation details are available at: https://github.com/mominul-ssv/cossif.",
    "authors": [
        {
            "affiliations": [],
            "name": "Mominul Islama"
        },
        {
            "affiliations": [],
            "name": "Hasib Zunairb"
        },
        {
            "affiliations": [],
            "name": "Nabeel Mohammeda"
        }
    ],
    "id": "SP:60de6a1de84448b0f0681171ec9665f87893d97a",
    "references": [
        {
            "authors": [
                "A.A. Adegun",
                "S. Viriri",
                "R.O. Ogundokun"
            ],
            "title": "Deep learning approach for medical image analysis",
            "venue": "Computational Intelligence and Neuroscience",
            "year": 2021
        },
        {
            "authors": [
                "F. Afza",
                "M. Sharif",
                "M.A. Khan",
                "U. Tariq",
                "H.S. Yong",
                "J. Cha"
            ],
            "title": "Multiclass skin lesion classification using hybrid deep features selection and extreme learning",
            "venue": "machine. Sensors",
            "year": 2022
        },
        {
            "authors": [
                "P. Bir",
                "V.E. Balas"
            ],
            "title": "A review on medical image analysis",
            "year": 2020
        },
        {
            "authors": [
                "S.S. Chaturvedi",
                "J.V. Tembhurne",
                "T. Diwan"
            ],
            "title": "A multi-class skin",
            "year": 2020
        },
        {
            "authors": [
                "A.A. Bharath"
            ],
            "title": "Generative adversarial networks: An overview",
            "year": 2018
        },
        {
            "authors": [
                "S.K. Datta",
                "M.A. Shaikh",
                "S.N. Srihari",
                "M. Gao"
            ],
            "title": "IEEE signal processing magazine",
            "year": 2021
        },
        {
            "authors": [
                "S. Thrun"
            ],
            "title": "Dermatologist-level classification of skin cancer",
            "year": 2017
        },
        {
            "authors": [
                "F. Garcea",
                "A. Serra",
                "F. Lamberti",
                "L. Morra"
            ],
            "title": "Data augmentation for",
            "year": 2022
        },
        {
            "authors": [
                "A. Halpern"
            ],
            "title": "Skin lesion analysis toward melanoma detection: A",
            "year": 2016
        },
        {
            "authors": [
                "T. Huynh",
                "A. Nibali",
                "Z. He"
            ],
            "title": "Semi-supervised learning for medical",
            "year": 2022
        },
        {
            "authors": [
                "A.A. Ilham",
                "I Nurtanio"
            ],
            "title": "Image search optimization with web",
            "year": 2020
        },
        {
            "authors": [
                "S. Kaur",
                "D. Aggarwal"
            ],
            "title": "Image content based retrieval system",
            "year": 2013
        },
        {
            "authors": [
                "S. qouni",
                "A. Mukhopadhyay"
            ],
            "title": "Gans for medical image analysis",
            "year": 2020
        },
        {
            "authors": [
                "Z. 1\u20137. Lan",
                "S. Cai",
                "X. He",
                "X. Wen"
            ],
            "title": "Fixcaps: An improved capsules",
            "year": 2022
        },
        {
            "authors": [
                "G. Litjens",
                "T. Kooi",
                "B.E. Bejnordi",
                "A.A.A. Setio",
                "F. Ciompi",
                "M. Ghafoorian",
                "J.A. Van Der Laak",
                "B. Van Ginneken",
                "C.I. S\u00e1nchez"
            ],
            "title": "A survey on deep learning in medical image analysis",
            "venue": "Medical image analysis",
            "year": 2017
        },
        {
            "authors": [
                "Z. Liu",
                "Y. Lin",
                "Y. Cao",
                "H. Hu",
                "Y. Wei",
                "Z. Zhang",
                "S. Lin",
                "B. Guo"
            ],
            "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
            "venue": "in: Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2021
        },
        {
            "authors": [
                "Z. Liu",
                "H. Mao",
                "C.Y. Wu",
                "C. Feichtenhofer",
                "T. Darrell",
                "S. Xie"
            ],
            "title": "A convnet for the 2020s",
            "venue": "in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "A.R. Lopez",
                "X. Giro-i Nieto",
                "J. Burdick",
                "O. Marques"
            ],
            "title": "Skin lesion classification from dermoscopic images using deep learning techniques",
            "venue": "13th IASTED international conference on biomedical engineering (BioMed),",
            "year": 2017
        },
        {
            "authors": [
                "M.P. McBee",
                "O.A. Awan",
                "A.T. Colucci",
                "C.W. Ghobadi",
                "N. Kadom",
                "A.P. Kansagra",
                "S. Tridandapani",
                "W.F. Auffermann"
            ],
            "title": "Deep learning in radiology",
            "venue": "Academic radiology",
            "year": 2018
        },
        {
            "authors": [
                "L. McInnes",
                "J. Healy",
                "J. Melville"
            ],
            "title": "Umap: Uniform manifold approximation and projection for dimension reduction",
            "year": 2018
        },
        {
            "authors": [
                "S.M. McKinney",
                "M. Sieniek",
                "V. Godbole",
                "J. Godwin",
                "N. Antropova",
                "H. Ashrafian",
                "T. Back",
                "M. Chesus",
                "G.S. Corrado",
                "A Darzi"
            ],
            "title": "International evaluation of an ai system for breast cancer screening",
            "venue": "Nature 577,",
            "year": 2020
        },
        {
            "authors": [
                "M.I. Razzak",
                "S. Naz",
                "A. Zaib"
            ],
            "title": "Deep learning for medical image processing: Overview, challenges and the future",
            "venue": "Classification in BioApps ,",
            "year": 2018
        },
        {
            "authors": [
                "V. Sampath",
                "I. Maurtua",
                "J.J. Aguilar Martin",
                "A. Gutierrez"
            ],
            "title": "A survey on generative adversarial networks for imbalance problems in computer vision tasks",
            "venue": "Journal of big Data",
            "year": 2021
        },
        {
            "authors": [
                "O. Sevli"
            ],
            "title": "A deep convolutional neural network-based pigmented skin lesion classification application and experts evaluation",
            "venue": "Neural Computing and Applications",
            "year": 2021
        },
        {
            "authors": [
                "Z. Tao",
                "H. Liu",
                "H. Fu",
                "Y. Fu"
            ],
            "title": "Image cosegmentation via saliencyguided constrained clustering with cosine similarity",
            "venue": "in: Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2017
        },
        {
            "authors": [
                "P. Tschandl",
                "C. Rosendahl",
                "H. Kittler"
            ],
            "title": "The ham10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions",
            "venue": "Scientific data",
            "year": 2018
        },
        {
            "authors": [
                "L. Yu",
                "H. Chen",
                "Q. Dou",
                "J. Qin",
                "P.A. Heng"
            ],
            "title": "Automated melanoma recognition in dermoscopy images via very deep residual networks",
            "venue": "IEEE transactions on medical imaging",
            "year": 2016
        },
        {
            "authors": [
                "J.R. Zech",
                "M.A. Badgeley",
                "M. Liu",
                "A.B. Costa",
                "J.J. Titano",
                "E.K. Oermann"
            ],
            "title": "Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: a cross-sectional study",
            "venue": "PLoS medicine 15,",
            "year": 2018
        },
        {
            "authors": [
                "J.Y. Zhu",
                "T. Park",
                "P. Isola",
                "A.A. Efros"
            ],
            "title": "Unpaired image-to-image translation using cycle-consistent adversarial networks",
            "venue": "in: Proceedings of the IEEE international conference on computer vision,",
            "year": 2017
        },
        {
            "authors": [
                "H. Zunair",
                "A.B. Hamza"
            ],
            "title": "Melanoma detection using adversarial training and deep transfer learning",
            "venue": "Physics in Medicine & Biology",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "CosSIF: Cosine similarity-based image filtering to overcome low inter-class variation in synthetic medical image datasets Mominul Islama,\u2217, Hasib Zunairb and Nabeel Mohammeda aDepartment of Electrical and Computer Engineering, North South University, Bashundhara, Dhaka, Bangladesh bConcordia Institute for Information Systems Engineering, Concordia University, Montreal, QC, Canada\nA R T I C L E I N F O Keywords: Medical image datasets Skin lesion classification Cosine similarity Generative adversarial networks Vision transformer Swin transformer ConvNeXt\nA B S T R A C T Crafting effective deep learning models for medical image analysis is a complex task, particularly in cases where the medical image dataset lacks significant inter-class variation. This challenge is further aggravated when employing such datasets to generate synthetic images using generative adversarial networks (GANs), as the output of GANs heavily relies on the input data. In this research, we propose a novel filtering algorithm called Cosine Similarity-based Image Filtering (CosSIF). We leverage CosSIF to develop two distinct filtering methods: Filtering Before GAN Training (FBGT) and Filtering After GAN Training (FAGT). FBGT involves the removal of real images that exhibit similarities to images of other classes before utilizing them as the training dataset for a GAN. On the other hand, FAGT focuses on eliminating synthetic images with less discriminative features compared to real images used for training the GAN. Experimental results reveal that employing either the FAGT or FBGT method with modern transformer and convolutional-based networks leads to substantial performance gains in various evaluation metrics. FAGT implementation on the ISIC-2016 dataset surpasses the baseline method in terms of sensitivity by 1.59% and AUC by 1.88%. Furthermore, for the HAM10000 dataset, applying FABT outperforms the baseline approach in terms of recall by 13.75%, and with the sole implementation of FAGT, achieves a maximum accuracy of 94.44%. Code and implementation details are available at: https://github.com/mominul-ssv/cossif."
        },
        {
            "heading": "1. Introduction",
            "text": "Medical image analysis is a critical component of modern healthcare, enabling accurate diagnosis, effective treatment, and continuous monitoring of various diseases Bir and Balas (2020). The advent of deep learning has created a new horizon in this field, delivering significant improvements in the early detection and classification of diseases. Numerous studies have highlighted the efficacy of deep learning in medical imaging Adegun, Viriri and Ogundokun (2021); Razzak, Naz and Zaib (2018), leading to its widespread adoption in multiple medical domains, including radiology, dermatology, and ophthalmology McBee, Awan, Colucci, Ghobadi, Kadom, Kansagra, Tridandapani and Auffermann (2018); Lopez, Giro-i Nieto, Burdick and Marques (2017); Wang, Keane, Chiang, Cheung, Wong and Ting (2022). In radiology, deep learning models have surpassed the diagnostic accuracy of radiologists in detecting breast cancer in mammography images McKinney, Sieniek, Godbole, Godwin, Antropova, Ashrafian, Back, Chesus, Corrado, Darzi et al. (2020). Similarly, in dermatology, deep learning has demonstrated outstanding performance in identifying skin cancer from dermoscopy images Esteva, Kuprel, Novoa, Ko, Swetter, Blau and Thrun (2017). In ophthalmology, deep learning models have been used to diagnose diabetic retinopathy and age-related macular degeneration from retinal images Gulshan, Peng, Coram, Stumpe, Wu, Narayanaswamy, Venugopalan, Widner, Madams, Cuadros\n\u2217Corresponding author mominul.ivi@gmail.com (M. Islam); hasibzunair@gmail.com (H.\nZunair); nabeel.mohammed@northsouth.edu (N. Mohammed) ORCID(s): 0009-0001-6409-964X (M. Islam); 0000-0002-5984-1731 (H. Zunair); 0000-0002-7661-3570 (N. Mohammed)\net al. (2016); Litjens, Kooi, Bejnordi, Setio, Ciompi, Ghafoorian, Van Der Laak, Van Ginneken and S\u00e1nchez (2017).\nOne of the major challenges in developing deep learning models for medical image analysis is the limited availability of datasets. This issue is particularly significant in classification tasks, where obtaining a balanced dataset with high inter-class variation is difficult Huynh, Nibali and He (2022). Inter-class variation refers to the differences in appearance between different classes of images Deng, Dong, Socher, Li, Li and Fei-Fei (2009). The scarcity of a balanced dataset arises when obtaining a sufficient number of images from certain classes is challenging, resulting in an imbalanced distribution of classes. For example, Zech et al. Zech, Badgeley, Liu, Costa, Titano and Oermann (2018) reported that training a deep learning model to detect pneumonia in chest radiographs was significantly impacted by the imbalanced nature of the dataset. The authors noted that obtaining a balanced dataset with a sufficient number of images of the positive class was difficult due to the low prevalence of pneumonia in the population. Therefore, low inter-class variation and class imbalance in medical image datasets significantly undermine the applicability of deep learning techniques in medical imaging.\nThe use of generative adversarial networks (GANs) has increasingly gained popularity in recent years to address class imbalances in datasets Sampath, Maurtua, Aguilar Martin and Gutierrez (2021). GANs are generative models that can produce synthetic images that closely resemble real images Karras, Aila, Laine and Lehtinen (2017). Several studies have reported success using GANs to address the class imbalance in various domains, including medical image analysis Kazeminia, Baur, Kuijper, van Ginneken,\nM. Islam et al.: Preprint submitted to Elsevier Page 1 of 19\nar X\niv :2\n30 7.\n13 84\n2v 2\n[ cs\n.C V\n] 1\n5 O\nct 2\n02 3\noversampling through GAN and transformation techniques, the adoption of our proposed FBGT and FAGT methods to mitigate low inter-class variation by leveraging our novel CosSIF algorithm, and ultimately training classifiers using the augmented dataset. In the case of multiple minority classes, the pipeline is repeated until the classifier training stage. It is recommended to view the illustration in color.\nNavab, Albarqouni and Mukhopadhyay (2020). The conventional approach for training a GAN involves utilizing every image of the minority class in a dataset. Creswell, White, Dumoulin, Arulkumaran, Sengupta and Bharath (2018). However, when dealing with images that have low inter-class variation, this method can be problematic. This is because the GAN may generate synthetic images that visually resemble images from other classes, resulting in a dataset that is technically balanced but presents challenges for a neural network attempting to distinguish differences between classes. The is largely attributed to the lack of diversity in the synthetic images that makes it difficult for a network to learn discriminative features required for accurate classification.\nIn response to the challenge of GANs producing visually similar images with less discriminative features when trained on datasets with low inter-class variation, we propose a novel filtering algorithm called Cosine Similaritybased Image Filtering (CosSIF). We utilize CosSIF to introduce two filtering methods: Filtering Before GAN Training (FBGT) and Filtering After GAN Training (FAGT). The CosSIF algorithm is employed to determine the similarity between two sets of images. For instance, in a dataset consisting of two classes, A and B, CosSIF calculates the similarity of each image from class A with all the images in class B. The resulting similarity scores generated by CosSIF are then used by FBGT or FAGT to filter out the most similar or dissimilar images. FBGT involves removing real images from the minority class that exhibit visual resemblance to\nimages from other classes before incorporating them into the training dataset of a GAN. This ensures that the GAN does not learn certain features that contribute to generating visually similar images. However, implementing FBGT requires retraining the GAN with the filtered images. In contrast, FAGT operates on a pre-trained GAN, where similarity calculations are conducted between the synthetic images generated by the GAN and the real images used for training the GAN. The architecture of our proposed algorithm and filtering methods is illustrated in Fig. 1. To evaluate the effectiveness of our approaches, we perform experiments using modern transformers such as Vision Transformer (ViT) and Swin Transformer, as well as convolutional-based networks like ConvNeXt. The key contributions of our work in this paper can be summarized as follows:\n\u2022 We propose CosSIF, an image similarity calculation algorithm with cosine similarity as its backbone, capable of identifying visually similar images of a specific class to images of another class/classes in a dataset.\n\u2022 We propose two filtering methods, FBGT and FAGT, to regulate GANs synthetic image generation capabilities in an effort to reduce low inter-class variability in medical image datasets.\n\u2022 We propose a reproducible train-test split for the HAM10000 dataset, which can facilitate the comparison of our proposed methods with future experiments conducted by others.\nM. Islam et al.: Preprint submitted to Elsevier Page 2 of 19\n\u2022 We experimentally demonstrate that the utilization of FAGT on the ISIC-2016 dataset surpasses the baseline method, MelaNet Zunair and Hamza (2020), in terms of sensitivity by 1.59% and AUC by 1.88%. Furthermore, the utilization of FBGT exceeds the baseline method, IRv2+SA Datta, Shaikh, Srihari and Gao (2021), in terms of recall by 13.75%, and with the sole implementation of FAGT, achieves a maximum accuracy of 94.44%.\nThe remaining sections of this paper are organized as follows: In Section 2, we discuss related studies on class imbalance and low inter-class variation in medical image classification. Moreover, we explore the usage of cosine similarity in computer vision. In Section 3, we present a comprehensive overview of our proposed CosSIF algorithm, as well as the FBGT and FAGT filtering methods. Furthermore, this section provides detailed descriptions of our selected GAN architecture and gives a brief overview of our chosen transformer and convolutional-based network models. In Section 4, we give an overview of the utilized datasets, present the selected configurations for classifier and GAN training. Subsequently, we perform experiments by employing our proposed algorithm and filtering methods. In Section 5, we conduct an ablation study of our experiments and compare the performance of our trained classifiers against baseline methods. Finally, Section 6 presents the conclusions and a discussion on possibilities for future work."
        },
        {
            "heading": "2. Related Work",
            "text": "Several studies have delved into a multitude of strategies to address class imbalance in medical image datasets. These approaches encompass oversampling techniques that involve either transformations or the implementation of generative adversarial networks (GANs) Garcea, Serra, Lamberti and Morra (2022). For instance, Zunair and Hamza Zunair and Hamza (2020) employed CycleGAN, a GAN model consisting of dual-generator and discriminator modules, to effectively increase the representation of the minority class in the dataset Zhu, Park, Isola and Efros (2017). On the other hand, researchers such as Datta et al. Datta et al. (2021) and Lan et al. Lan, Cai, He and Wen (2022) opted for alternative transformation methods, adjusting image rotation and focus to diversify the dataset without resorting to GANs. While these research papers used different methods to address the class imbalance issue, they did not propose any solutions to handle the low inter-class variation in the generated synthetic datasets. In this paper, we aim to address this issue by eliminating images that contain limited distinguishable features during the oversampling process.\nIdentifying visually similar images begins with mathematically calculating the similarity between two images, preferably in a higher dimension. Multiple formulas exist for this task, including Mean Square Error (MSE), Cosine Similarity, and Euclidean Distance. In the field of computer vision, the use of cosine similarity is fairly prevalent for calculating the similarity between images. Ilham et al. Ilham,\nNurtanio et al. (2020) used cosine similarity to compare the vector representation of a query image with the vector representations of all the images in the database. Similarly, Kaur et al. Kaur and Aggarwal (2013) proposed a contentbased image retrieval system (CBIR) to assist dermatologists in diagnosing skin diseases. The system utilized various techniques such as feature extraction, similarity matching, and cosine similarity to retrieve the most similar images to the query image. Tao et al. Tao, Liu, Fu and Fu (2017) introduced Saliency-Guided Constrained Clustering approach with cosine similarity (SGC3) for image cosegmentation. This method employs cosine similarity to calculate the feature similarity between data points and its cluster centroid. Given the extensive utilization of cosine similarity in similarity calculations, as demonstrated by the mentioned authors, we also choose to employ it as the foundation of our similarity calculation algorithm. Similar to these authors, we transform the two input images into vectors and calculate their cosine similarity. However, as we need to calculate similarities for thousands of images, it becomes necessary to reduce the pixel dimensions of the input images. Therefore, we optimize our algorithm by reducing the number of pixels to 64x64. This adjustment significantly improves the computation time of our algorithm. Moreover, we utilize cosine similarity to compute the cosine distance, ensuring that the calculated distances are non-negative values."
        },
        {
            "heading": "3. Proposed Method",
            "text": "This section describes the main components of our proposed algorithm and methods. It begins with a comprehensive overview of the CosSIF algorithm, followed by a detailed explanation of the FBGT and FAGT methods, along with a comparison between them. Next, the architecture of the GAN is described, along with a hybrid augmentation process designed specifically to suit the outlined GAN architecture. Finally, the section concisely discusses the transformers and convolutional-based network models used for training classifiers."
        },
        {
            "heading": "3.1. CosSIF",
            "text": "The details of the Cosine Similarity-based Image Filtering (CosSIF) algorithm are divided into several parts: class selection, image rescaling, similarity calculation, optimization, backbone, algorithm, filtering, and adaptability."
        },
        {
            "heading": "3.1.1. Class Selection",
            "text": "The CosSIF algorithm begins by selecting a class from a dataset, referred to as the target class. This target class acts as the anchor for similarity calculations, while the remaining classes are considered secondary classes. The target class is denoted as T[\ud835\udc84], and a secondary class is denoted as S[\ud835\udc84], where \ud835\udc84 represents the class name. The total number of images in the target class is represented by \ud835\udc91, and the total number of images in the secondary class is represented by \ud835\udc92. Eq. 1 and Eq. 2 represent all the images within the T[\ud835\udc84] and S[\ud835\udc84] classes, respectively.\nM. Islam et al.: Preprint submitted to Elsevier Page 3 of 19\n\ud835\udf3c is then sorted in descending order, with the first entry representing the maximum similarity score \ud835\udc70\ud835\udc8e\ud835\udc82\ud835\udc99. Once the similarity calculation for all images in \ud835\udc7b [\ud835\udc84\ud835\udfd0] has been completed, the resulting set of records \ud835\udc79 is obtained. Finally, \ud835\udc79 is sorted in ascending order based on the maximum similarity score \ud835\udc70\ud835\udc8e\ud835\udc82\ud835\udc99, thereby concluding the similarity calculation process.\nT[\ud835\udc84] = {\ud835\udc95[\ud835\udc84]\ud835\udfcf , \ud835\udc95 [\ud835\udc84] \ud835\udfd0 ,\u2026 , \ud835\udc95 [\ud835\udc84] \ud835\udc91 } (1)\nS[\ud835\udc84] = {\ud835\udc94[\ud835\udc84]\ud835\udfcf , \ud835\udc94 [\ud835\udc84] \ud835\udfd0 ,\u2026 , \ud835\udc94 [\ud835\udc84] \ud835\udc92 } (2)\nIn the case of multiple secondary classes, they are represented as a set \ud835\udc7f. Eq. 3 represents all the classes within the set \ud835\udc7f, where \ud835\udc8e denotes the total number of secondary classes.\n\ud835\udc7f = {S[\ud835\udc84\ud835\udfcf],S[\ud835\udc84\ud835\udfd0],\u2026 ,S[\ud835\udc84\ud835\udc8e]} (3)"
        },
        {
            "heading": "3.1.2. Image Rescaling",
            "text": "Upon selecting the target and secondary classes, all images belonging to these classes are resized to a smaller size, typically 64x64 pixels by default. This resizing step enables faster similarity calculations between images. To achieve even faster computation, the image size can be further reduced. Conversely, if a more detailed pixel-based computation is desired, the size can be increased beyond the default 64x64 pixels, albeit with an increase in computation time."
        },
        {
            "heading": "3.1.3. Similarity Calculation",
            "text": "Following the image rescaling process, the algorithm proceeds to perform the similarity calculation. It starts by selecting an image from the target class T[\ud835\udc84] and calculates\nits similarity score, denoted as \ud835\udc70 , with all the other images in the secondary class S[\ud835\udc84]. If there are multiple secondary classes, the similarity measure is computed for all images across the entire set \ud835\udc7f.\nDuring the similarity calculation, a record denoted as \ud835\udf3c is maintained for each image in T[\ud835\udc84], storing all the computed similarity scores along with the corresponding image identifiers (image and class names). The record \ud835\udf3c is then sorted in descending order based on the individual similarity scores \ud835\udc70 . Therefore, for any image in T[\ud835\udc84], the first entry in the record \ud835\udf3c contains the maximum similarity score, denoted as \ud835\udc70\ud835\udc8e\ud835\udc82\ud835\udc99,along with its associated image identifiers.\nThe algorithm iterates through all images in the target class T[\ud835\udc84] and records their similarities and corresponding image identifiers. Once the iteration process is complete, we obtain a set \ud835\udc79 of records, as defined in Eq. 4. It is important to note that the total number of records, denoted by \ud835\udc9b, in \ud835\udc79, is equal to the total number of images, denoted by \ud835\udc91, in T[\ud835\udc84].\n\ud835\udc79 = {\ud835\udf3c\ud835\udfcf, \ud835\udf3c\ud835\udfd0,\u2026 , \ud835\udf3c\ud835\udc9b} (4) Finally, the set \ud835\udc79 of records is sorted in ascending order based on the individual maximum similarity scores \ud835\udc70max. Fig. 2 provides a detailed illustration of the CosSIFalgorithm."
        },
        {
            "heading": "3.1.4. Optimization",
            "text": "To tackle the issue of the record size growing excessively large as the number of images in S[\ud835\udc84] or \ud835\udc7f increases, an\nM. Islam et al.: Preprint submitted to Elsevier Page 4 of 19\na color image into its three RGB (red, green, blue) layers, where each layer is represented as a square matrix. Each layer contains different pixel values, which are then normalized by dividing each pixel by 255. Next, all layers are flattened into a vector. Considering that each image has a resolution of 64x64 pixels and consists of 3 layers, the resulting vector dimension becomes 1x12288. This procedure is repeated for both Image 1 and Image 2, resulting in two vectors, \ud835\udc2e and \ud835\udc2f, respectively. The cosine similarity between these two vectors is then calculated. The graphs in (b) illustrate that as the similarity between vectors \ud835\udc2e and \ud835\udc2f increases, the angle \ud835\udf3d between them becomes smaller, and vice versa.\noptimization technique is introduced in the similarity calculation module. Rather than recording the similarity for each image in T[\ud835\udc84] with every other image in S[\ud835\udc84] or \ud835\udc7f, only a limited range of images with the highest similarity scores are recorded. However, the similarity calculation is still performed for all images in S[\ud835\udc84] or \ud835\udc7f. This approach effectively reduces the size of the record and addresses the scalability concern."
        },
        {
            "heading": "3.1.5. Backbone",
            "text": "The CosSIF algorithm analyzes images and computes their level of similarity. It utilizes cosine similarity to determine the degree of similarity between two images, as well as cosine distance to measure the positive distance between them.\nLet\u2019s assume that \ud835\udc2e and \ud835\udc2f are two arbitrary vectors. The cosine similarity between the vectors is defined by Eq. 5, where \ud835\udc2e \u22c5 \ud835\udc2f represents the dot product of \ud835\udc2e and \ud835\udc2f, and \u2016\ud835\udc2e\u2016 \u00d7 \u2016\ud835\udc2f\u2016 denotes the product of their magnitudes. For a more detailed visual representation, refer to Fig. 3.\ncosine similarity (\ud835\udc2e, \ud835\udc2f) = \ud835\udc84\ud835\udc90\ud835\udc94(\ud835\udf3d) = \ud835\udc2e \u22c5 \ud835\udc2f \u2016\ud835\udc2e\u2016 \u00d7 \u2016\ud835\udc2f\u2016 (5)\nThe range of cosine similarity spans from -1 to 1. To convert this range to 0 to 1, the cosine distance is calculated. For cosine distance, a similarity value approaching 1 indicates more similar image pairs. Cosine distance is defined the Eq. 6.\ncosine distance = 1 - cosine similarity (6)"
        },
        {
            "heading": "3.1.6. Algorithm",
            "text": "The detailed procedure of the CosSIF algorithm is presented in Algorithm 1. The input consists of the target class\nT[\ud835\udc84] and either a secondary class S[\ud835\udc84] or a set \ud835\udc7f of secondary classes. The output is a set \ud835\udc79 of records, sorted in ascending order, which holds the computed similarity scores for each image in T[\ud835\udc84] compared to all other images in S[\ud835\udc84] or \ud835\udc7f.\nAlgorithm 1 CosSIF Input: Target class T[\ud835\udc84] = {\ud835\udc95[\ud835\udc84]\ud835\udfcf , \ud835\udc95[\ud835\udc84]\ud835\udfd0 ,\u2026 , \ud835\udc95[\ud835\udc84]\ud835\udc91 }, secondary\nclass S[\ud835\udc84] = {\ud835\udc94[\ud835\udc84]\ud835\udfcf , \ud835\udc94[\ud835\udc84]\ud835\udfd0 ,\u2026 , \ud835\udc94[\ud835\udc84]\ud835\udc92 } or a set \ud835\udc7f of secondaryclasses. Output: Set \ud835\udc79 of records, sorted in ascending order.\n1: \ud835\udc79 = {} 2: Resize all images to 64x64. 3: for \ud835\udc95[\ud835\udc84]\ud835\udfcf in T[\ud835\udc84] do 4: \ud835\udf3c\ud835\udfcf = {} 5: if (secondary class != \ud835\udc7f) then 6: \ud835\udc7f = {S[\ud835\udc84]} 7: else 8: for S[\ud835\udc84] in \ud835\udc7f do 9: for \ud835\udc94[\ud835\udc84]\ud835\udfcf in S[\ud835\udc84] do\n10: Calculate cosine similarity of \ud835\udc95[\ud835\udc84]\ud835\udfcf and \ud835\udc94[\ud835\udc84]\ud835\udfcf . 511: Calculate cosine distance. 6 12: Similarity score \ud835\udc70 = cosine distance 13: Save record \ud835\udf3c\ud835\udfcf = {\ud835\udc95[\ud835\udc84]\ud835\udfcf , {\ud835\udc94[\ud835\udc84]\ud835\udfcf , \ud835\udc70}} 14: end for 15: end for 16: \ud835\udf3c\ud835\udfcf = {\ud835\udc95[\ud835\udc84]\ud835\udfcf , {\ud835\udc94 [\ud835\udc84] \ud835\udfcf , \ud835\udc70}}, {\ud835\udc94\n[\ud835\udc84] \ud835\udfd0 , \ud835\udc70},\u2026}17: Sort \ud835\udf3c\ud835\udfcf in descending order by similarity scores. 18: \ud835\udf3c\ud835\udfcf = {\ud835\udc95[\ud835\udc84]\ud835\udfcf , {\ud835\udc94 [\ud835\udc84] \ud835\udc8e\ud835\udc82\ud835\udc99, \ud835\udc70\ud835\udc8e\ud835\udc82\ud835\udc99}},\u2026} 19: end if 20: Append \ud835\udf3c\ud835\udfcf to set \ud835\udc79. 21: end for 22: Sort \ud835\udc79 in ascending order by \ud835\udc70\ud835\udc8e\ud835\udc82\ud835\udc99. 23: Set \ud835\udc79 = {\ud835\udf3c\ud835\udfcf, \ud835\udf3c\ud835\udfd0,\u2026 , \ud835\udf3c\ud835\udc9b}\nM. Islam et al.: Preprint submitted to Elsevier Page 5 of 19"
        },
        {
            "heading": "3.1.7. Filtering",
            "text": "Once the similarity calculation is completed, the filtering process is initiated. To recap, the similarity calculation generates a set \ud835\udc79 of records, as shown in Eq. 4, which is subsequently sorted in ascending order. For each record \ud835\udf3c in \ud835\udc79, \ud835\udc70\ud835\udc8e\ud835\udc82\ud835\udc99 represents the maximum similarity score, and theassociated image identifiers are used to identify the image in T[\ud835\udc84] that achieves this similarity score with an image in S[\ud835\udc84] or \ud835\udc7f. Since the set \ud835\udc79 is sorted in ascending order by \ud835\udc70\ud835\udc8e\ud835\udc82\ud835\udc99, thefirst record, \ud835\udf3c\ud835\udfcf, has the lowest \ud835\udc70\ud835\udc8e\ud835\udc82\ud835\udc99, while the last record, \ud835\udf3c\ud835\udc9b, has the highest. Therefore, the process of filtering outthe most similar images in begins with the \ud835\udf3c\ud835\udc9b. From there,the filtering process gradually moves up the list of \ud835\udf3c in \ud835\udc79, with each subsequent image having a lower \ud835\udc70\ud835\udc8e\ud835\udc82\ud835\udc99 than theprevious one. Conversely, to filter out the most dissimilar images, the filtering process starts from the \ud835\udf3c\ud835\udfcf. In this case,the filtering process gradually moves down the list of \ud835\udf3c in \ud835\udc79, with each subsequent image having a higher similarity score than the previous one.\nIn summary, the CosSIF algorithm enables the filtering of images based on their cosine similarity, identifying the most similar or dissimilar ones. However, the algorithm doesn\u2019t directly perform the filtering task; instead, it generates the essential information needed for filtering. The actual filtering is accomplished using our proposed FBGT and FAGT methods."
        },
        {
            "heading": "3.1.8. Adaptability",
            "text": "The CosSIF algorithm has been designed with future reusability in mind, allowing it to be applied to various tasks. To facilitate this adaptability, we have incorporated certain features that may not be immediately useful but could prove valuable in future works. One such feature relates to the similarity calculation process, where we provide the option to restrict the range of saved records. In the current implementation of our research, we have chosen to limit this range to only 1. This means that for each image in a selected target class, there is at most one image from the secondary class that is most similar. This suffices for our purposes, as the generated set \ud835\udc79 of sorted records obtained from the CosSIF algorithm already gives us the most similar or dissimilar images that can be filtered from the target class. However, let\u2019s consider a scenario where it is necessary to know all possible similarities that each image in the target class shares with other images in the secondary class. In such cases, we can easily modify and reuse the CosSIF algorithm by adjusting the similarity range. By increasing the range, we can obtain the desired results and retrieve all the relevant similarities for each image. Therefore, the flexibility of the CosSIF algorithm allows it to be applied to a variety of tasks, and with slight modifications, it can accommodate different requirements in future applications."
        },
        {
            "heading": "3.2. FBGT",
            "text": "The Filtering Before GAN Training (FBGT) aims to eliminate real images from the minority class that display resemblances to images from other classes before employing\nthem as the training dataset for a GAN. FBGT method commence by selecting the target and secondary classes. Here, the target class T[\ud835\udc84] represents the minority class within a given dataset, and the remaining classes are collectively referred to as a set \ud835\udc7f of secondary classes. Then, the CosSIF algorithm is employed to calculate the similarity scores for each image in T[\ud835\udc84] with all other images in \ud835\udc7f. CosSIF generates a set \ud835\udc79 of records which is shorted in ascending order.\nFollowing the completion of the similarity calculation and the generation of the set \ud835\udc79 of records, the subsequent step in FBGT focuses on filtering images from the target class T[\ud835\udc84]. In this step, the number of images to be filtered from T[\ud835\udc84] is determined by a hyperparameter denoted as \ud835\udf36, where \ud835\udfce < \ud835\udf36 < \ud835\udfcf. The value of \ud835\udf36 is calculated using the following formula:\n\ud835\udf36 = \ud835\udfcf\ud835\udfce\ud835\udfce \u2212 % of images to be removed\n\ud835\udfcf\ud835\udfce\ud835\udfce (7)\nThe formula for calculating the number of filtered images, \ud835\udc87 , is given by:\n\ud835\udc87 = \u2308\ud835\udc91 \u00d7 \ud835\udf36\u2309 (8) where, the symbol \u2308 \u2309 represents the ceiling function, which rounds up the result of the multiplication to the nearest integer. The value of \ud835\udc87 represents the threshold point, which indicates the number of images that have been filtered from T[\ud835\udc84]. The newly filtered target class, T[\ud835\udc84]\ud835\udc87\ud835\udc8a\ud835\udc8d\ud835\udc95\ud835\udc86\ud835\udc93\ud835\udc86\ud835\udc85 , composed of \ud835\udc87 images is given by:\nT[\ud835\udc84]\ud835\udc87\ud835\udc8a\ud835\udc8d\ud835\udc95\ud835\udc86\ud835\udc93\ud835\udc86\ud835\udc85 = {\ud835\udc95 [\ud835\udc84] \ud835\udfcf , \ud835\udc95 [\ud835\udc84] \ud835\udfd0 ,\u2026 , \ud835\udc95 [\ud835\udc84] \ud835\udc87 } (9)\nT[\ud835\udc84]\ud835\udc87\ud835\udc8a\ud835\udc8d\ud835\udc95\ud835\udc86\ud835\udc93\ud835\udc86\ud835\udc85 is the output of FBGT method. It contains thenewly filtered images that are going to be used for oversampling."
        },
        {
            "heading": "3.3. FAGT",
            "text": "The Filtering After GAN Training (FAGT) method calculates similarities between the synthetic images generated by a GAN and real images of the class on which the GAN was trained. In this method, the target class T[\ud835\udc84] consists of images that are synthetically generated via a trained GAN, while the secondary class S[\ud835\udc84] composed of real images, serves as the training dataset for that GAN. It is important to note that in FAGT, there is no possibility of having a set of secondary classes. Following the selection of T[\ud835\udc84] and S[\ud835\udc84], the FAGT method utilizes the CosSIF algorithm, leading to the generation of a set R of records.\nIn the FAGT method, the process of filtering images from the target class T[\ud835\udc84] becomes more a bit more complex, compared to FBGT. Unlike FBGT, where the number of images in the filtered target class \ud835\udc87 is not user-defined but rather calculated using Eq. 7 and Eq. 8, in FAGT, the value of \ud835\udc87 is determined by the user. This value represents both the number of images in the filtered target class T[\ud835\udc84]\ud835\udc87\ud835\udc8a\ud835\udc8d\ud835\udc95\ud835\udc86\ud835\udc93\ud835\udc86\ud835\udc85 and\nM. Islam et al.: Preprint submitted to Elsevier Page 6 of 19\nthe number of images required for oversampling. In FAGT, \ud835\udc87 is considered a constant value. To control the output of the filtering process, the hyperparameter \ud835\udf36 is used to calculate the value of \ud835\udc91, which denotes the number of synthetic images generated by a GAN. The formula for calculating \ud835\udc91 is given by:\n\ud835\udc91 = \u2308\ud835\udc87 \ud835\udf36 \u2309\n(10)\nWhile \ud835\udc91 is fixed in the FBGT method, it is variable in the FAGT method. This is because the quality of the synthetic images produced by a GAN can vary, leading to changes in the value of \ud835\udc91. If a GAN produces synthetic images with fewer discriminative features compared to the real images, more images need to be filtered out from a larger set of images, resulting in an increased value of \ud835\udc91. Conversely, if a GAN is capable of generating images with similar discriminative features compared to real images, then the value of \ud835\udc91 decreases. This implies that fewer images need to be filtered out from a smaller set of synthetic images. Thus, in the FAGT method, \ud835\udc91 behaves more like a hyperparameter. Fig. 4 depicts this dependency in two setups, namely \ud835\udebf\ud835\udfcfand \ud835\udebf\ud835\udfd0. In the first setup, \ud835\udebf\ud835\udfcf, it is assumed that the GANproduces more random images with significant deviations from the real images, thereby necessitating a higher filtering requirement. Conversely, the second setup, \ud835\udebf\ud835\udfd0, assumes thatthe GAN generates synthetic images that closely resemble the real images, resulting in a decreased need for filtering.\nof necessary filtering is influenced by the GAN\u2019s capacity to generate synthetic images that closely resemble the real images used to train the GAN."
        },
        {
            "heading": "3.4. Binary vs Multiclass Classification",
            "text": "In both the FBGT and FAGT methods, it is possible to eliminate similar and dissimilar images from a calculated set \ud835\udc79 of records. In binary classification, the removal of similar images from one class and dissimilar images from another class enhances the distinction between the two classes, resulting in improved filtering outcomes. However, in multiclass classification, it is crucial to avoid removing dissimilar images, as eliminating these images in relation to all other classes can lead to the loss of images that\npossess distinct features essential for accurate classification. Therefore, for multiclass classification, it is recommended to eliminate only the similar images."
        },
        {
            "heading": "3.5. FBGT vs FAGT",
            "text": "The FBGT and FAGT methods are two approaches that produces more robust oversampled datasets that reduce the issue of low inter-class variation. However, there are differences between these two methods that need to be considered.\nThe FBGT method requires retraining the GAN with the newly filtered dataset, which can be a time-consuming process. Therefore, it may not be practical to use this method when dealing with multiclass classification problems that require oversampling for several classes. In contrast, the FAGT method can be applied to a pre-trained GAN, making it faster than the FBGT as it does not require retraining. However, the FAGT method requires filtering more images since \ud835\udc91 is a variable for this method, resulting in longer computation time for filtering compared to the FBGT.\nIn the FBGT method, both the target class and secondary class consist of real images. This implies that the set R of records obtained after the similarity calculation is universal and can be utilized later by anyone to filter out images. However, in the FAGT method, the target class is composed of synthetic images randomly generated by a trained GAN, which necessitates the recalculation of similarity each time the method is employed. As a result, the FBGT method is more efficient than the FAGT method when it comes to filtering images.\nFurthermore, it is essential to address a potential question regarding the FAGT method. Although similarities between real images of a specific class and real images from other classes are not directly calculated, the effectiveness of the method lies in the context of medical image datasets. Typically, all classes in such datasets consist of similar types of images (e.g., skin lesions, CT scans) but at different stages. When the GAN generates images that do not distinctly resemble the real images used during its training, these synthetic images may end up closely resembling images from other classes. As a result, the removal of synthetic images that deviate from the real images serves as a multipurpose filtering method. On one hand, it contributes to generating images with greater discriminative features, while on the other hand, it effectively addresses the issue of low inter-class variation."
        },
        {
            "heading": "3.6. GAN Architecture",
            "text": "The FBGT and FAGT methods are independent of GAN architecture, meaning that the filtering process remains constant regardless of any selected GAN framework. However, the choice of GAN architecture is often determined by the total image size in the dataset. Typically, training a GAN requires a large number of images. However, in medical image analysis, the minority class often consists of an extremely low volume of images, posing a challenge for the GAN to converge during training. Consequently, we tend to choose a GAN architecture that performs well with a small set of images.\nM. Islam et al.: Preprint submitted to Elsevier Page 7 of 19\nIn this paper, we use StyleGAN2-ADA as our GAN architecture. StyleGAN2-ADA builds upon the improvements of StyleGAN2, with the key enhancement being the addition of adaptive data augmentation (ADA) techniques. Karras, Aittala, Hellsten, Laine, Lehtinen and Aila (2020a). This method allows for the generation of high-quality, diverse images even when the dataset is small or imbalanced, making it a valuable tool in medical image analysis and other applications where training data may be limited. StyleGAN2ADA consists of two main parts: the generator and the discriminator. The generator\u2019s objective is to create realistic images, while the discriminator\u2019s goal is to differentiate between real and generated images."
        },
        {
            "heading": "3.6.1. Generator",
            "text": "The generator \ud835\udc6e consists of several key components: the mapping network, the synthesis network, and the Adaptive Instance Normalization (AdaIN) layers. The mapping network \ud835\udc89 takes a latent code \ud835\udf50 and maps it to a style vector \ud835\udc98:\n\ud835\udc98 = \ud835\udc89(\ud835\udf50) (11) The synthesis network \ud835\udc88 takes the style vector \ud835\udc98 and a noise tensor \ud835\udc8f, and generates an image \ud835\udc99:\n\ud835\udc99 = \ud835\udc88(\ud835\udc98, \ud835\udc8f) (12) Adaptive instance normalization (AdaIN) Karras, Laine, Aittala, Hellsten, Lehtinen and Aila (2020b) is used to modulate the feature maps in the synthesis network with the style vector \ud835\udc98. Given a feature map \ud835\udc6d and the style vector \ud835\udc98, AdaIN produces a styled feature map \ud835\udc6d \u2032:\n\ud835\udc6d \u2032 = AdaIN(\ud835\udc6d ,\ud835\udc98) (13)"
        },
        {
            "heading": "3.6.2. Discriminator",
            "text": "The discriminator \ud835\udc6b is a convolutional neural network that classifies whether an input image \ud835\udc99 is real or generated. It takes an image \ud835\udc99 as input and outputs a scalar probability value \ud835\udc9a:\n\ud835\udc9a = \ud835\udc6b(\ud835\udc99) (14)"
        },
        {
            "heading": "3.6.3. Adaptive Discriminator Augmentation",
            "text": "In StyleGAN2-ADA, the discriminator is trained on both the real images and their augmented counterparts. The augmentation function \ud835\udc7d takes an image \ud835\udc99 and an augmentation parameter \ud835\udf41 to produce an augmented image \ud835\udc99\u2032:\n\ud835\udc99\u2032 = \ud835\udc7d (\ud835\udc99, \ud835\udf41) (15)"
        },
        {
            "heading": "3.6.4. Loss",
            "text": "Both the generator and discriminator losses are based on the binary cross-entropy loss function. The generator seeks to minimize its loss, which represents the difference between the discriminator\u2019s output on generated images and the target output. In essence, the generator aims to maximize\nthe probability of the discriminator classifying the generated images as real:\n\ud835\udc73\ud835\udc6e = \u2212\ud835\udd3c\ud835\udf50\u223c\ud835\udc9a(\ud835\udf50)[\ud835\udc25\ud835\udc28\ud835\udc20\ud835\udc6b(\ud835\udc88(\ud835\udc89(\ud835\udf50), \ud835\udc8f))] (16) Here, \ud835\udf50 is a random latent code sampled from the prior distribution \ud835\udc9a(\ud835\udf50), \ud835\udc89 is the mapping network, \ud835\udc88 is the synthesis network, \ud835\udc8f is the noise tensor, and \ud835\udc6b is the discriminator. \ud835\udd3c is expectation, which represents the average value of the expression inside the brackets.\nThe discriminator aims to minimize its loss, which consists of two parts: the difference between the discriminator\u2019s output on real images and the target output, and the difference between the discriminator\u2019s output on generated images and the target output:\n\ud835\udc73\ud835\udc6b = \u2212\ud835\udd3c\ud835\udc99\u223c\ud835\udc9adata(\ud835\udc99)[\ud835\udc25\ud835\udc28\ud835\udc20\ud835\udc6b(\ud835\udc99)] \u2212 \ud835\udd3c\ud835\udf50\u223c\ud835\udc9a(\ud835\udf50)[\ud835\udc25\ud835\udc28\ud835\udc20(\ud835\udfcf \u2212\ud835\udc6b(\ud835\udc88(\ud835\udc89(\ud835\udf50), \ud835\udc8f)))] (17)\nHere, \ud835\udc99 is an image sampled from the true data distribution \ud835\udc9adata(\ud835\udc99), and the other variables have the same meaning asin the generator loss."
        },
        {
            "heading": "3.7. Hybrid Augmentation",
            "text": "Typically, oversampling via a GAN involves merging the real images with generated synthetic images after the GAN training. However, in some datasets, certain classes have an extremely low volume of training images. This makes the learning process of a GAN really difficult, even when using StyleGAN2-ADA.\nThe architecture of StyleGAN2-ADA includes a component known as adaptive discriminator augmentation (ADA), which is vital for training with a small number of images. During training, this component takes an input image \ud835\udc99 and produces an augmented image \ud835\udc99\u2032, as expressed in Eq.15. To increase the variability of \ud835\udc99\u2032, it\u2019s important for the training dataset to contain sufficient variation. Therefore, we perform a minor oversampling of the minority classes by applying various transformations to the images before using them as a training dataset for StyleGAN2-ADA. These transformations include adjusting the focus, rotating the images, shifting their positions, and flipping them horizontally or vertically. The oversampling via transformations improves the variability of \ud835\udc99\u2032 during the process, which enhances the quality of the synthetically generated images produced by StyleGAN2-ADA. Refer to Fig. 1 to visualize the oversampling via transformation of the minority class before utilizing them in the training dataset of a GAN."
        },
        {
            "heading": "3.8. Model Architectures",
            "text": "To assess the efficacy of our FBGT and FAGT methods, we employ pre-trained transformer and convolutionalbased models. We train these models using the oversampled dataset, incorporating the FBGT and FAGT methods in some instances and excluding them in others for comparison purposes. For our experiments, we utilize pretrained Swin Transformer Liu, Lin, Cao, Hu, Wei, Zhang, Lin and Guo (2021), Vision Transformer (ViT) Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner,\nM. Islam et al.: Preprint submitted to Elsevier Page 8 of 19\nDehghani, Minderer, Heigold, Gelly et al. (2020), and ConvNeXt Liu, Mao, Wu, Feichtenhofer, Darrell and Xie (2022) models. We fine-tune these models by adapting their output layers to accommodate the classes within our dataset.\nThe Swin Transformer, proposed by Liu et al. Liu et al. (2021), is a hierarchical transformer model specifically designed for computer vision tasks. It introduces a local representation to capture both local and global context, using a shifted window-based self-attention mechanism and a hierarchical architecture. The Vision Transformer (ViT), introduced by Dosovitskiy et al. Dosovitskiy et al. (2020), applies the transformer architecture to computer vision tasks by dividing input images into patches and processing them as tokens with positional encodings. It has shown excellent performance on large-scale datasets but is known to be datahungry. ConvNeXt, a model introduced by Liu et al. Liu et al. (2022), demonstrates the potential of pure ConvNets by modernizing a standard ResNet to compete with the performance of Vision Transformers."
        },
        {
            "heading": "4. Experiments",
            "text": "In this section, extensive experiments are carried out to assess the performance of the proposed FBGT and FAGT methods, comparing them with strong baseline methods."
        },
        {
            "heading": "4.1. Datasets",
            "text": "The effectiveness of FBGT and FAGT methods is analyzed using two datasets: ISIC-2016 and HAM10000. The ISIC-2016 dataset is employed to test both methods for binary classification, while the HAM10000 dataset is used for multiclass classification. Both datasets exhibit significant class imbalance with low inter-class variation, making them ideal choices for testing the filtering methods."
        },
        {
            "heading": "4.1.1. ISIC-2016 Dataset",
            "text": "The ISIC-2016 Task 3 dataset contains 900 training and 379 testing dermoscopic images for skin lesion analysis and melanoma classification. With 173 malignant and 737 benign lesions in the training set, and 75 malignant and 304 benign lesions in the testing set, the dataset exhibits class imbalance Gutman, Codella, Celebi, Helba, Marchetti, Mishra and Halpern (2016). The images of malignant and benign lesions exhibit similar appearances, leading to low inter-class variation within the dataset, as observed in Fig. 5 (a).\nindicate low inter-class variation both in the ISIC-2016 dataset, as shown in (a), and the HAM10000 dataset, as shown in (b)."
        },
        {
            "heading": "4.1.2. HAM10000 Dataset",
            "text": "The HAM10000 dataset consists of 10,015 clinical images of skin lesions, sourced from various locations worldwide and annotated by dermatologists. The dataset includes seven classes of skin lesions: actinic keratoses and intraepithelial carcinoma (akiec) with 327 images, basal cell carcinoma (bcc) with 514 images, benign keratosis-like lesions (bkl) with 1,099 images, dermatofibroma (df) with 115 images, melanoma (mel) with 1,113 images, melanocytic nevi (nv) with 6,705 images, and vascular lesions (vasc) with 142 images Tschandl, Rosendahl and Kittler (2018). This distribution results in a highly unbalanced dataset. Moreover, the images also exhibit low inter-class variation, as can be seen in Fig. 5 (b).\nThe HAM10000 dataset does not include a predefined train-test split. This is particularly problematic as with no pre-defined split, it is difficult to compare the performance of our work with already existing state-of-the-art approaches. Some research papers focus on demonstrating high accuracy, which can sometimes involve manipulating the associated test data. Moreover, most work does not provide a reproducible train-test split, leading to difficulties in verifying the reported results.\nTherefore, to address this issue, we employ a reproducible train-test split in our work. We partition the dataset into 9,187 images for training and 828 for testing. This partitioning process entails removing duplicates from the test set, which are composed of identical images with slight visual augmentations. Consequently, the training set contains these augmented images, while the test set is devoid of different augmentations of the same images. To perform the split, we use the scikit-learn train-test split library, providing a random state of 42 as an input parameter. This approach ensures consistency in the images within the training and testing sets for future experiments. Fig. 6 depicts the quantity of images in each class along with the distribution of the train-test split.\nthe train-test split and the logarithmic scale depiction of the sample distribution across different classes within the HAM10000 dataset, revealing a significant class imbalance in both the training and testing datasets.\nM. Islam et al.: Preprint submitted to Elsevier Page 9 of 19\nFBGT and FAGT methods, which incorporate the CosSIF algorithm. The same filtering process is also applied to the benign class from the ISIC-2016 dataset, as well as the akiec, bcc, bkl, df, mel, and vasc classes from the HAM10000 dataset. In the visualization, the implication of FABT is shown for binary classification. In the case of multiclass classification, where there are multiple secondary classes, as observed in the HAM10000 dataset, CosSIF calculates the similarities against images of all secondary classes \ud835\udc7f = {S [\ud835\udc84\ud835\udfcf],S [\ud835\udc84\ud835\udfd0],\u2026 ,S [\ud835\udc84\ud835\udc8e]} with the chosen target class T [\ud835\udc84].\nM. Islam et al.: Preprint submitted to Elsevier Page 10 of 19\nTable 1\nThe variation in the number of filtered images, denoted as \ud835\udc87 , from the total number of real/synthetic images, denoted as \ud835\udc91, for three different values of \ud835\udf36 when implementing the FBGT or FAGT methods on the HAM10000 dataset.\nClasses akiec bcc bkl df mel vasc\n\ud835\udf36 Total \ud835\udc91 Filtered \ud835\udc87 Total \ud835\udc91 Filtered \ud835\udc87 Total \ud835\udc91 Filtered \ud835\udc87 Total \ud835\udc91 Filtered \ud835\udc87 Total \ud835\udc91 Filtered \ud835\udc87 Total \ud835\udc91 Filtered \ud835\udc87\n\ud835\udf36 = 0.80 244 391 827 88 864 106 \ud835\udf36 = 0.85 304 259 488 415 1033 879 109 93 1079 918 132 113\nFB G\nT\n\ud835\udf36 = 0.90 274 440 930 99 972 119 \ud835\udf36 = 0.75 5624 5368 3241 7201 3142 7181 \ud835\udf36 = 0.80 5272 4218 5032 4026 3038 2431 6751 5401 2946 2357 6732 5386\nFA G\nT\n\ud835\udf36 = 0.85 4962 4736 2860 6354 2772 6336"
        },
        {
            "heading": "4.2. Preprocessing",
            "text": "In our experiment, we resize images from both datasets to a resolution of 256x256 pixels to ensure that the minority classes meet the criteria for being utilized as training datasets for the GAN. We opt to oversample both benign and malignant classes in the ISIC-2016 dataset and oversample all classes, except for melanocytic nevi (nv), in the HAM10000 dataset, as it already has a sufficient number of real images available and is the majority class."
        },
        {
            "heading": "4.3. Dataset Filtering",
            "text": "The dataset filtering process involves the utilization of the FBGT and FAGT methods on both the ISIC-2016 and HAM10000 datasets. Furthermore, as outlined in the methodology, we apply hybrid augmentation techniques to both datasets. This involves oversampling images through transformations, while simultaneously training GANs to generate synthetic images. We conduct a total of three primary experiments in our study. Experiment I utilizes the FBGT method, Experiment II employs the FAGT method, and Experiment III does not employ either the FBGT or FAGT methods."
        },
        {
            "heading": "4.3.1. Experiment I",
            "text": "In Experiment I, we apply the FBGT method to both benign and malignant (Fig. 7, left) classes of the ISIC-2016 dataset and the akiec, bcc, bkl, df, mel, and vasc classes of the HAM10000 dataset. This involves conducting similarity calculations using CosSIF and filtering real images from the GAN training dataset that exhibit the highest similarity scores with images from other classes. We then perform minor oversampling via transformation with the newly filtered images associated with each class, followed by conducting GAN training for all the selected classes using the associated filtered GAN training datasets. Finally, we employ the trained GANs to generate synthetic images for each selected class, consequently resolving the issues of low inter-class variation. For a visual representation of this process, please refer to Fig. 1.\nIn the FBGT method, we have a hyperparameter called \ud835\udf36 that determines the number of images to be filtered from the real images. Instead of randomly selecting a number for filtering, we consider three specific values for \ud835\udf36. The variation in the number of filtered images for different \ud835\udf36\nTable 2\nThe variation in the number of filtered images, denoted as \ud835\udc87 , from the total number of real/synthetic images, denoted as \ud835\udc91, for three different values of \ud835\udf36 when implementing the FBGT and FAGT methods on the benign and malignant classes of the ISIC-2016 dataset.\nClasses Benign Malignant\n\ud835\udf36 Total \ud835\udc91 Filtered \ud835\udc87 Total \ud835\udc91 Filtered \ud835\udc87\n\ud835\udf36 = 0.80 582 139 \ud835\udf36 = 0.85 727 618 173 148\nFB G\nT\n\ud835\udf36 = 0.90 655 156 \ud835\udf36 = 0.75 1441 1530 \ud835\udf36 = 0.80 1351 1081 1435 1148\nFA G\nT\n\ud835\udf36 = 0.85 1271 1350\nvalues when using the FBGT method can be observed in Table 1 and 2 for the HAM10000 and ISIC-2016 datasets, respectively."
        },
        {
            "heading": "4.3.2. Experiment II",
            "text": "In Experiment II, we apply the FAGT method to pretrained GANs that are individually trained using the real images from the benign and malignant (Fig. 7, right) classes of ISIC-2016, as well as the akiec, bcc, bkl, df, mel, and vasc classes of the HAM10000 dataset. By utilizing these pre-trained GANs, we generate synthetic images for each class. Subsequently, we perform similarity calculations to compare the generated synthetic images with the real training images. Based on the similarity results obtained from the calculations, we selectively remove synthetic images with lower discriminative features associated with the selected class. This process ensures that the filtered synthetic images closely resemble features of the real images used to train the GAN.\nLike the FBGT method, the FAGT method also utilizes the hyperparameter known as \ud835\udf36 to determine the number of images to be filtered from the synthetic images. Similarly, we consider three specific values for \ud835\udf36, and the variations in the number of images can be observed in Table 1 and 2 for the HAM10000 and ISIC-2016 datasets, respectively. To visualize the internal processes of the FAGT method, please refer to Fig. 1.\nM. Islam et al.: Preprint submitted to Elsevier Page 11 of 19"
        },
        {
            "heading": "4.3.3. Experiment III",
            "text": "In Experiment III, neither the FBGT nor FAGT methods are employed. Instead, we utilize the same pre-trained GAN as used in the FAGT method, but without implementing the similarity calculation and filtering process. This experiment is referred to as No-Filtering. Unlike the other experiments, No-Filtering does not involve any hyperparameter tuning. This experiment is solely conducted to analyze the efficacy of the FBGT and FAGT methods."
        },
        {
            "heading": "4.4. Dataset Augmentation",
            "text": "Dataset augmentation is performed after the completion of dataset filtering. To achieve the final dataset augmentation for each class, we combine the real images with a batch of oversampled images obtained through transformations, as well as the synthetic images generated by GANs. During Experiment I, Experiment II, and Experiment III, the output is the final augmented dataset. The final augmented dataset, as shown in Fig. 8, consists of 2000 images for both the benign and malignant classes, effectively addressing the class imbalance in the ISIC-2016 dataset. Similarly, in Fig. 9, the final augmented HAM10000 dataset is displayed, with the akiec, bcc, bkl, df, mel, nv, and vasc classes each containing 6042 images. We chose this number as it matches the number of images in the overrepresented nv class, thus resolving the class imbalance present in the HAM10000 dataset by oversampling the remaining classes to this range.\nAlthough the final augmented ISIC-2016 dataset contains 2000 images for each class and the HAM10000 dataset contains 6042 images for each class, it is important to note that these numbers are predetermined at the beginning of our experiments. There is a relationship in the FAGT method between the number of generated synthetic images and the number of filtered images. In this case, the number of filtered images is fixed and determined based on the size of the final augmented dataset. For example, in the ISIC2016 dataset, we need to filter 1081 synthetic images from the benign class. This number is not randomly generated but derived from the size of the final augmented dataset. Therefore, if the final augmentation consists of 2000 images, with 727 real images and 192 images oversampled through transformations, the required number of synthetic images is calculated as (2000 - (727 + 192) = 1081). Thus, 1081 is the constant number used to control the output of the FAGT method. If the GAN generates more random images that don\u2019t resemble real images of the benign class, a larger pool of synthetic images must be generated, from which 1081 images are filtered. Conversely, if the GAN generates images that closely resemble real images of the benign class, a smaller pool of generated images is sufficient for filtering 1081 images."
        },
        {
            "heading": "4.5. GAN Configuration",
            "text": "As mentioned in the methodology section, we employ StyleGAN2-ADA as our GAN architecture. Each selected class for oversampling is trained using the same StyleGAN2-ADA configuration. In this configuration, we employ 400 kimg, which is equivalent to 400 epochs.\nmented datasets, indicating the contributions of real, transformed, and synthetic images for the benign and malignant classes of the ISIC-2016 dataset.\nthe augmented dataset, showcasing the contributions of real, transformed, and synthetic images for all seven classes of the HAM10000 dataset.\nTypically, StyleGAN2-ADA necessitates a larger number of epochs to generate realistic-looking synthetic images. However, for our experiment, we set kimg equal to 400 to accommodate our hardware constraints. Despite this limitation, we still acquire satisfactory synthetic images that fulfill our needs for analyzing the efficacy of our filtering methods."
        },
        {
            "heading": "4.6. Training Classifiers",
            "text": "As outlined in the methodology section, we utilize the Swin Transformer, ViT, and ConvNeXt models for training our classifiers. Specifically, we use the pre-trained versions of these models and fine-tune them using our final augmented datasets. To facilitate training, we resize the images in the final augmented datasets to 224x224 pixels. For optimization during training, we use AdamW, which is an algorithm designed for training deep learning models that extends the Adam optimizer to include weight decay regularization. We use a learning rate of \ud835\udfd3\ud835\udc86\u2212\ud835\udfd3 during training for all these models."
        },
        {
            "heading": "4.7. Evaluation Metrics",
            "text": "In classification tasks, it is essential to select appropriate evaluation metrics to accurately assess model performance. This section discusses various evaluation metrics, including\nM. Islam et al.: Preprint submitted to Elsevier Page 12 of 19\nrecall, F1-score, sensitivity, accuracy, and AUC, which are employed to analyze the performance of modern transformer and convolutional-based network models and evaluate the efficacy of the FBGT and FAGT methods."
        },
        {
            "heading": "4.7.1. Recall",
            "text": "Recall, also known as sensitivity, measures the proportion of actual positive instances that are correctly predicted as positive. For binary classification, recall can be defined as:\n\ud835\udc79\ud835\udc86\ud835\udc84\ud835\udc82\ud835\udc8d\ud835\udc8d = \ud835\udc7b\ud835\udc77 \ud835\udc7b\ud835\udc77 + \ud835\udc6d\ud835\udc75\n(18)\nwhere \ud835\udc7b\ud835\udc77 represents the number of true positives and \ud835\udc6d\ud835\udc75 represents the number of false negatives. For multiclass classification, macro-average recall is utilized. It is computed by calculating the recall for each class individually, treating each distinct class as a positive class and the remaining classes as negative classes. Then, the average of these recall values is taken. The formula for macro-average recall is given by:\n\ud835\udc79\ud835\udc86\ud835\udc84\ud835\udc82\ud835\udc8d\ud835\udc8d\ud835\udc8e\ud835\udc82\ud835\udc84\ud835\udc93\ud835\udc90 = \ud835\udfcf \ud835\udc8c\n\ud835\udc8c \u2211\n\ud835\udc8a=\ud835\udfcf\n\ud835\udc7b\ud835\udc77\ud835\udc8a \ud835\udc7b\ud835\udc77\ud835\udc8a + \ud835\udc6d\ud835\udc75\ud835\udc8a\n(19)\nwhere \ud835\udc8c represents the number of classes."
        },
        {
            "heading": "4.7.2. F1-score",
            "text": "The F1-score, which is the harmonic mean of precision and recall, provides a balance between the two metrics and is especially useful when dealing with imbalanced datasets or when both false positives and false negatives are of concern. In multiclass classification, we use the macro-average F1score, which is calculated by computing the F1-score for each class individually and then taking the average of these values. The formula for macro-average F1-score is given by:\n\ud835\udc6d \ud835\udfcf-\ud835\udc94\ud835\udc84\ud835\udc90\ud835\udc93\ud835\udc86\ud835\udc8e\ud835\udc82\ud835\udc84\ud835\udc93\ud835\udc90 = \ud835\udfcf\ud835\udc8c \ud835\udc8c \u2211\n\ud835\udc8a=\ud835\udfcf \ud835\udfd0\u22c5 \ud835\udc77 \ud835\udc93\ud835\udc86\ud835\udc84\ud835\udc8a\ud835\udc94\ud835\udc8a\ud835\udc90\ud835\udc8f\ud835\udc8a \u22c5\ud835\udc79\ud835\udc86\ud835\udc84\ud835\udc82\ud835\udc8d\ud835\udc8d\ud835\udc8a \ud835\udc77 \ud835\udc93\ud835\udc86\ud835\udc84\ud835\udc8a\ud835\udc94\ud835\udc8a\ud835\udc90\ud835\udc8f\ud835\udc8a +\ud835\udc79\ud835\udc86\ud835\udc84\ud835\udc82\ud835\udc8d\ud835\udc8d\ud835\udc8a (20)\nwhere \ud835\udc77 \ud835\udc93\ud835\udc86\ud835\udc84\ud835\udc8a\ud835\udc94\ud835\udc8a\ud835\udc90\ud835\udc8f = \ud835\udc7b\ud835\udc77\ud835\udc7b\ud835\udc77+\ud835\udc6d\ud835\udc77 . By using the macro-averageF1-score, we can obtain an overall performance measure of the multiclass classification model, while taking into account the performance of each individual class."
        },
        {
            "heading": "4.7.3. Accuracy",
            "text": "Accuracy measures the proportion of correctly predicted instances over the total number of instances. For binary classification, accuracy can be defined as:\n\ud835\udc68\ud835\udc84\ud835\udc84\ud835\udc96\ud835\udc93\ud835\udc82\ud835\udc84\ud835\udc9a = \ud835\udc7b\ud835\udc77 + \ud835\udc7b\ud835\udc75 \ud835\udc7b\ud835\udc77 + \ud835\udc7b\ud835\udc75 + \ud835\udc6d\ud835\udc77 + \ud835\udc6d\ud835\udc75\n(21)\nFor multiclass classification, accuracy can be calculated as:\n\ud835\udc68\ud835\udc84\ud835\udc84\ud835\udc96\ud835\udc93\ud835\udc82\ud835\udc84\ud835\udc9a = \u2211\ud835\udc8c \ud835\udc8a=\ud835\udfcf \ud835\udc7b\ud835\udc77\ud835\udc8a \u2211\ud835\udc8c\n\ud835\udc8a=\ud835\udfcf(\ud835\udc7b\ud835\udc77\ud835\udc8a + \ud835\udc7b\ud835\udc75\ud835\udc8a + \ud835\udc6d\ud835\udc77\ud835\udc8a + \ud835\udc6d\ud835\udc75\ud835\udc8a) (22)"
        },
        {
            "heading": "4.7.4. AUC",
            "text": "The AUC is the area under the receiver operating characteristic (ROC) curve, which is a plot of the true positive rate (TPR) against the false positive rate (FPR) at different classification thresholds. In the context of multiclass classification, the AUC is commonly computed using the onevs-rest (OVR) strategy. In the OVR approach, we treat each class as the positive class and the remaining classes as the negative class, and we compute the AUC for each class separately. Then, we take the average of these AUC values to obtain the overall AUC score."
        },
        {
            "heading": "4.8. Baselines",
            "text": "Our baseline for the ISIC-2016 dataset is the results achieved by the MelaNet model, designed by Zunair and Hamza Zunair and Hamza (2020). However, due to an inconsistency in the reported sensitivity result in their paper, we recomputed this metric using the publicly available pretrained MelaNet model. For the HAM10000 dataset, we use the results of the IRv2+SA model as our baseline, as reported by Datta et al. Datta et al. (2021). Both baselines demonstrate state-of-the-art classification performance."
        },
        {
            "heading": "4.9. Experimental Setup",
            "text": "The experiments were conducted in two different setups with different hardware configurations. Setup 1, which employed a Linux server with 2-core Intel(R) Xeon(R) CPU @ 2.20GHz, 13 GB RAM, and 1x NVIDIA P100 16GB GPU was utilized for training StyleGAN2-ADA for different minority classes and for training classifiers for the HAM10000 dataset. Setup 2, which used a Windows machine with 6- core AMD Ryzen 5 5600H CPU @ 3.30GHz, 16 GB RAM, and 1x NVIDIA RTX 3060 6GB GPU, was utilized for implementing the FBGT and FAGT methods and for training classifiers for the ISIC-2016 dataset."
        },
        {
            "heading": "5. Results",
            "text": "This section presents a thorough performance analysis of the FBGT and FAGT methods on each variation of the final augmented dataset. The analysis is conducted in two parts. Firstly, we investigate the difference in performance while utilizing the FBGT and FAGT methods against No-Filtering. Secondly, we compare our best models with state-of-the-art baseline models."
        },
        {
            "heading": "5.1. Ablation Study",
            "text": "From the analysis presented in Table 3 and 4, it is evident that our trained models exhibit improved performance across most evaluation metrics when utilizing either the FBGT or FAGT methods, as opposed to the No-Filtering approach, on both the ISIC-2016 and HAM10000 datasets. These performance gains are consistently observed across various distributions of augmented training datasets, each corresponding to a different \ud835\udf36 value. Furthermore, the graphs in Fig. 10 and Fig. 11 provide insights into the training phase of each model and the specific metric used to select the optimal variant during the classifier training process.\nM. Islam et al.: Preprint submitted to Elsevier Page 13 of 19"
        },
        {
            "heading": "5.1.1. ISIC-2016 Dataset",
            "text": "Table 3 and the graphs in Fig. 10 offer an in-depth analysis of the performance of various models trained on the ISIC-2016 dataset. Regarding individual model performance, when employing the FBGT method, the Swin Transformer model with a hyperparameter value of \ud835\udf36 = \ud835\udfce.\ud835\udfd6\ud835\udfce achieves the best performance, with a false negative (FN) count of 25, a sensitivity of 66.67%, and a recall of 77.41%. Similarly, when applying the FAGT method, the model with \ud835\udf36 = \ud835\udfce.\ud835\udfd5\ud835\udfd3 demonstrates the best performance, resulting in an FN count of 28, a sensitivity of 62.67%,\nand a recall of 76.73%. These optimal \ud835\udf36 values (\ud835\udf36 = \ud835\udfce.\ud835\udfd6\ud835\udfce for FBGT and \ud835\udf36 = \ud835\udfce.\ud835\udfd5\ud835\udfd3 for FAGT) are also applied to the dataset distributions incorporating the filtering methods when training the ViT and ConvNeXt models.\nFor the ViT model, applying the FAGT method with\ud835\udf36 = \ud835\udfce.\ud835\udfd5\ud835\udfd3 leads to significant improvements, surpassing all other trained models with a sensitivity of 72.00% and a FN count of 21. While the utilization of the FBGT method improves the ViT model\u2019s performance, yielding a sensitivity of 60% and an FN count of 30, this improvement is comparatively less significant compared to FBGT. Similarly, the ConvNeXt\nM. Islam et al.: Preprint submitted to Elsevier Page 14 of 19\nmodel demonstrates performance enhancements with both the FBGT and FAGT methods, although the gains are not as substantial as those observed in the other models."
        },
        {
            "heading": "5.1.2. HAM10000 Dataset",
            "text": "Table 4 and the graphs in Fig. 11 provide a performance analysis of models trained on the HAM10000 dataset. Concerning individual model performance, utilizing the FBGT method with \ud835\udf36 = \ud835\udfce.\ud835\udfd6\ud835\udfce yields the best results for the Swin\nTransformer model, achieving a recall of 81.82% and an F1score of 83.94%. Similarly, when the FAGT method is employed with an \ud835\udf36 = \ud835\udfce.\ud835\udfd6\ud835\udfd3, the best performance is observed, resulting in a recall of 82.48% and an F1-score of 81.90%. These optimal \ud835\udf36 values (\ud835\udf36 = \ud835\udfce.\ud835\udfd6\ud835\udfce for FBGT and \ud835\udf36 = \ud835\udfce.\ud835\udfd6\ud835\udfd3 for FAGT) are also applied to the dataset distributions incorporating the filtering methods when training the ViT and ConvNeXt models.\nIn the case of the ViT model, utilizing the FAGT method with \ud835\udf36 = \ud835\udfce.\ud835\udfd6\ud835\udfd3 achieves the highest recorded recall of\nM. Islam et al.: Preprint submitted to Elsevier Page 15 of 19\nComparison of our best-performing models against the baseline, and various approaches by researchers on the HAM10000 dataset, focusing on key metrics such as accuracy and recall, where boldface numbers indicate the best performance. The symbol (\u2191) indicates higher is better.\nMacro Average\nMethod Accuracy (%) (\u2191) Recall (%) (\u2191)\nKhan et al. Khan, Javed, Sharif, Saba and Rehman (2019) 89.80 \u2212 Onur Sevli Sevli (2021) 91.51 \u2212 Chaturvedi et al. Chaturvedi, Tembhurne and Diwan (2020) 93.20 \u2212 Afza et al. Afza, Sharif, Khan, Tariq, Yong and Cha (2022) 93.40 \u2212 IRv2+SA Datta et al. (2021) (Baseline) 93.48 71.93\nSwin T.+FBGT (\ud835\udf36 = \ud835\udfce.\ud835\udfd6\ud835\udfce) 94.04 81.82\nConvNeXt+FAGT (\ud835\udf36 = \ud835\udfce.\ud835\udfd6\ud835\udfd3) 94.44 81.80\n85.94% and the highest average AUC of 98.27%. As for the ConvNeXt model, applying the FAGT method with \ud835\udf36 = \ud835\udfce.\ud835\udfd6\ud835\udfd3 results in an accuracy of 94.44%, an F1-score of 84.06%, and a recall of 81.80%. This is by far the bestperforming model when we consider F1-score and accuracy as our evaluation metrics. The use of the FBGT method does improve the performance of the ViT and ConvNeXt models as well. However, this improvement isn\u2019t as significant as using the FAGT method."
        },
        {
            "heading": "5.2. Comparison Against Baselines",
            "text": "In this section, we compare our best-performing models against strong baseline models. As mentioned before, we choose the baseline model MelaNet Zunair and Hamza (2020) for both the ISIC-2016 dataset and the baseline IRv2+SA Datta et al. (2021) HAM10000 datasets. Furthermore, we compare our models with other strong models proposed by various researchers. The overall comparisons for the ISIC-2016 and HAM10000 datasets can be visualized in Tables 5 and 6."
        },
        {
            "heading": "5.2.1. ISIC-2016 Dataset",
            "text": "Our best-performing model trained on the ISIC-2016 dataset is the ViT model, utilizing the FAGT method with \ud835\udf36 = \ud835\udfce.\ud835\udfd5\ud835\udfd3. This model surpasses the baseline method, MelaNet, by 1.59% in sensitivity and 1.88% in AUC. Our baseline remained unmatched for pure classification without any segmentation until the introduction of our proposed method, which outperforms it in every evaluation category. In Table 5, a detailed comparison is presented, showcasing\nthe performance of our best-performing model against the baseline and other approaches proposed in different studies on the ISIC-2016 dataset."
        },
        {
            "heading": "5.2.2. HAM10000 Dataset",
            "text": "The ConvNeXt model, utilizing the FAGT method with \ud835\udf36 = \ud835\udfce.\ud835\udfd6\ud835\udfd3, is our best-performing model trained on the HAM10000 dataset. It outperforms the baseline method, IRv2+SA, by 13.72% in recall and 1.03% in accuracy. Similarly, the Swin Transformer model, employing the FABT method with an \ud835\udf36 = \ud835\udfce.\ud835\udfd6\ud835\udfce, surpasses the baseline IRv2+SA by 13.75% in recall and 0.60% in accuracy. Furthermore, our model utilizes 42,294 training images, whereas the baseline IRv2+SA uses 51,699 images, making our approach more sample-efficient. Table 6 provides a performance comparison of our best-performing models, against the baseline and other approaches proposed in different studies on the HAM10000 dataset."
        },
        {
            "heading": "5.3. Feature Visualization",
            "text": "To visualize the data distribution, we utilize the Uniform Manifold Approximation and Projection for Dimension Reduction (UMAP) technique McInnes, Healy and Melville (2018). We analyze four different variations of the ISIC2016 dataset, which are presented in of Fig. 12, and four variations of the HAM10000 dataset, which are shown in Fig. 13.\nVariation 1, depicted in Fig. 12 (a) and Fig 13 (a), represents the original dataset consisting of real images. Variation\nM. Islam et al.: Preprint submitted to Elsevier Page 16 of 19\ncomparative view of the dataset\u2019s distribution and clustering patterns for each variation.\nconvenient comparison of the distribution and clustering patterns across each variation.\n2, displayed in Fig. 12 (b) and Fig. 13 (b), comprises unfiltered oversampled datasets that include real, transformed, and GAN-generated synthetic images. This variation corresponds to the No-Filtering experiment. In variations 3 and 4, illustrated in Fig. 12 (c), Fig. 12 (d), Fig. 13 (c), and Fig. 13 (d), we have filtered oversampled datasets that also incorporate real, transformed, and GAN-generated synthetic images. These variations utilize the FBGT and FAGT methods, and the composition of the datasets is determined by the hyperparameter \ud835\udf36. The selection of the dataset composition is based on the highest performance observed on the Swin Transformer classifier.\nFig. 12 presents the two-dimensional (2D) UMAP embeddings of the ISIC-2016 dataset, where (a) contains 727 malignant and 173 benign lesions within the distribution, while (b), (c), and (d) in Fig. 12 depict oversampled datasets featuring 2000 malignant and 2000 benign lesions each; it is evident from Fig. 12 (b) that there is an overlap in the data distribution between the benign and malignant classes, whereas in Fig. 12 (c) and Fig. 12 (d), the distribution appears to separate into two distinct groups, highlighting the effectiveness of the FBGT and FAGT methods when compared to the No-Filtering approach.\nSimilarly, Fig. 13 illustrates the three-dimensional (3D) UMAP embeddings of the HAM10000 dataset, where (a) shows a distribution comprising 9186 skin lesions, and (b),\n(c), and (d) in Fig. 13 represent oversampled datasets, each displaying a subset of the 42,294 skin lesions; upon observing the 3D representations, it becomes evident that in Fig. 13 (c) and Fig. 13 (d), the data points for each class become more distinguishable within a three-dimensional space, whereas the unfiltered 3D representation in Fig. 13 (b) demonstrates sparse data points, posing challenges for the classifier to identify specific regions for each class and consequently making the classification task more difficult.\nThe formation of clustered regions, as opposed to a single concentrated area, further substantiates the effectiveness of the FBGT and FAGT methods, addressing the issue of low inter-class variation within a dataset. By utilizing these techniques, we can substantially refine the classification process, ultimately leading to more accurate and dependable outcomes."
        },
        {
            "heading": "6. Conclusion",
            "text": "This paper introduces Cosine Similarity-based Image Filtering (CosSIF), a robust dataset filtering algorithm. We utilize CosSIF to create two filtering approaches: FBGT and FAGT. These methods rely on cosine similarity as the main metric for similarity calculation and aim to reduce the volume of GAN-generated synthetic images from the minority class that resemble similarity to images from the majority class. Our experimental results demonstrate that\nM. Islam et al.: Preprint submitted to Elsevier Page 17 of 19\nmodels trained on datasets processed with either the FBGT or FAGT methods show improved performance compared to models without these filtering methods. Through comprehensive experiments, we demonstrate that the proposed FAGT method, when applied to the ISIC-2016 dataset and trained on the ViT model, improves sensitivity by 1.59% and AUC by 1.88% compared to the baseline MelaNet. When we apply the FAGT and FBGT methods to the HAM10000 dataset and train them on the ConvNeXt and Swin Transformer models, we observe significant improvements in recall. Specifically, the FAGT method achieves a recall improvement of 13.72% over the baseline IRv2+SA, with an accuracy of 94.44%. Similarly, the FBGT method achieves a recall improvement of 13.75% over the same baseline, with an accuracy of 94.04%. For future research, our aim is to enhance the similarity calculation algorithm by incorporating a feature extraction and feature-based similarity calculation module. Moreover, we aim to apply our algorithm and filtering methods to various medical domains, including X-rays, CT scans, and MRI images. Furthermore, we plan to utilize the proposed CosSIF algorithm to develop a downsampling technique suitable for all image classification tasks.\nFunding This research did not receive any specific grant from funding agencies in the public, commercial, or not-for-profit sectors.\nCRediT authorship contribution statement M. Islam: Conceptualization, Methodology, Software, Validation, Formal analysis, Investigation, Resources, Data Curation, Writing - Original Draft, Visualization, Project administration. H. Zunair, N. Mohammed: Resources, Investigation, Writing - Review & Editing, Supervision, Funding acquisition.\nDeclaration of competing interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.\nData availability The datasets used in this research are publicly available and can be found at the following URLs: https://challenge. isic-archive.com/data/ for the ISIC-2016 dataset and https: //dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10. 7910/DVN/DBW86T for the HAM10000 dataset."
        }
    ],
    "title": "CosSIF: Cosine similarity-based image filtering to overcome low inter-class variation in synthetic medical image datasets",
    "year": 2023
}