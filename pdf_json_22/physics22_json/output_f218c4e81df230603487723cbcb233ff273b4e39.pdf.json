{
    "abstractText": "The M series of chips produced by Apple have proven a capable and power-efficient alternative to mainstream Intel and AMD x86 processors for everyday tasks. Additionally, the unified design integrating the central processing and graphics processing unit, have allowed these M series chips to excel at many tasks with heavy graphical requirements without the need for a discrete graphical processing unit (GPU), and in some cases even outperforming discrete GPUs. In this work, we show how the M series chips can be leveraged using the Metal Shading Language (MSL) to accelerate typical array operations in C++. More importantly, we show how the usage of MSL avoids the typical complexity of CUDA or OpenACC memory management, by allowing the central processing unit (CPU) and GPU to work in unified memory. We demonstrate how performant the M series chips are on standard onedimensional and two-dimensional array operations such as array addition, SAXPY and finite difference stencils, with respect to serial and OpenMP accelerated CPU code. The reduced complexity of implementing MSL also allows us to accelerate an existing elastic wave equation solver (originally based on OpenMP accelerated C++) using MSL, with minimal effort, while retaining all CPU and OpenMP functionality. The resulting performance gain of simulating the wave equation is near an order of magnitude for specific settings. This gain attained from using MSL is similar to other GPU-accelerated wave-propagation codes with respect to their CPU variants, but does not come at much increased programming complexity that prohibits the typical scientific programmer to leverage these accelerators. This result shows how unified processing units can be a valuable tool to seismologists and computational scientists in general, lowering the bar to writing performant codes that leverage modern GPUs.",
    "authors": [
        {
            "affiliations": [],
            "name": "Lars Gebraad"
        },
        {
            "affiliations": [],
            "name": "Andreas Fichtner"
        }
    ],
    "id": "SP:311f32a0f8f636446426d8f30d186f787efe500a",
    "references": [
        {
            "authors": [
                "Andreas Fichtner Lars Gebraad"
            ],
            "title": "MSL for scientific C++ portal, 2022",
            "venue": "URL https://larsgeb. github.io/msl-portal/",
            "year": 2022
        },
        {
            "authors": [
                "Lars Gebraad",
                "Andreas Fichtner"
            ],
            "title": "psvWave: elastic wave propagation in 2d for Python and C++",
            "year": 2022
        },
        {
            "authors": [
                "Lars Gebraad",
                "Andreas Fichtner"
            ],
            "title": "psvWave, 2 2022",
            "venue": "URL https://github.com/larsgeb/ psvWave",
            "year": 2022
        },
        {
            "authors": [
                "Jean Virieux"
            ],
            "title": "P-SV wave propagation in heterogeneous media: Velocity-stress finitedifference",
            "venue": "method. Geophysics,",
            "year": 1986
        },
        {
            "authors": [
                "Jean Virieux",
                "St\u00e9phane Operto"
            ],
            "title": "An overview of full waveform inversion in exploration geophysics: Geophysics, 74, wcc127\u2013 wcc152",
            "venue": "FIGURE CAPTIONS,",
            "year": 2009
        },
        {
            "authors": [
                "Wenjie Lei",
                "Youyi Ruan",
                "Ebru Bozda\u011f",
                "Daniel Peter",
                "Matthieu Lefebvre",
                "Dimitri Komatitsch",
                "Jeroen Tromp",
                "Judith Hill",
                "Norbert Podhorszki",
                "David Pugmire"
            ],
            "title": "Global adjoint tomography\u2014model GLAD-M25",
            "venue": "Geophysical Journal International,",
            "year": 2020
        },
        {
            "authors": [
                "Solvi Thrastarson",
                "Dirk-Philip Van Herwaarden",
                "Lion Krischer",
                "Christian Boehm",
                "Martin van Driel",
                "Michael Afanasiev",
                "Andreas Fichtner"
            ],
            "title": "Data-adaptive global full-waveform inversion",
            "venue": "Geophysical Journal International,",
            "year": 2022
        },
        {
            "authors": [
                "Luan T. Nguyen",
                "Ryan T. Modrak"
            ],
            "title": "Ultrasonic wavefield inversion and migration in complex heterogeneous structures: 2d numerical imaging and nondestructive testing experiments",
            "venue": "Ultrasonics, 82:357\u2013370,",
            "year": 2018
        },
        {
            "authors": [
                "Alireza Kordjazi",
                "Joseph .T Coe",
                "Michael Afanasiev"
            ],
            "title": "The use of the spectral element method for modeling stress wave propagation in non-destructive testing applications for drilled shafts",
            "venue": "Geo-Congress",
            "year": 2020
        },
        {
            "authors": [
                "Llu\u0131\u0301s Guasch",
                "Oscar Calder\u00f3n Agudo",
                "Meng- Xing Tang",
                "Parashkev Nachev",
                "Michael Warner"
            ],
            "title": "Full-waveform inversion imaging of the human brain",
            "venue": "NPJ digital medicine,",
            "year": 2020
        },
        {
            "authors": [
                "Patrick Marty",
                "Christian Boehm",
                "Andreas Fichtner"
            ],
            "title": "Acoustoelastic full-waveform inversion for transcranial ultrasound computed tomography",
            "venue": "In Medical Imaging",
            "year": 2021
        },
        {
            "authors": [
                "Max Rietmann",
                "Peter Messmer",
                "Tarje Nissen- Meyer",
                "Daniel Peter",
                "Piero Basini",
                "Dimitri Komatitsch",
                "Olaf Schenk",
                "Jeroen Tromp",
                "Lapo Boschi",
                "Domenico Giardini"
            ],
            "title": "Forward and adjoint simulations of seismic wave propagation on emerging large-scale GPU architectures",
            "venue": "In SC\u201912: Proceedings of the International Conference on High Performance Computing,",
            "year": 2012
        },
        {
            "authors": [
                "Imre Kiss",
                "Szabolcs Gyimothy",
                "Zsolt Badics",
                "Jozsef Pavo"
            ],
            "title": "Parallel realization of the element-by-element FEM technique by CUDA",
            "venue": "IEEE Transactions on magnetics,",
            "year": 2012
        },
        {
            "authors": [
                "Jack W. Davidson",
                "Sanjay Jinturkar"
            ],
            "title": "Memory access coalescing: A technique for eliminating redundant memory accesses",
            "venue": "In Proceedings of the ACM SIGPLAN 1994 Conference on Programming Language Design and Implementation,",
            "year": 1994
        }
    ],
    "sections": [
        {
            "text": "In this work, we show how the M series chips can be leveraged using the Metal Shading Language (MSL) to accelerate typical array operations in C++. More importantly, we show how the usage of MSL avoids the typical complexity of CUDA or OpenACC memory management, by allowing the central processing unit (CPU) and GPU to work in unified memory. We demonstrate how performant the M series chips are on standard onedimensional and two-dimensional array operations such as array addition, SAXPY and finite difference stencils, with respect to serial and OpenMP accelerated CPU code. The reduced complexity of implementing MSL also allows us to accelerate an existing elastic wave equation solver (originally based on OpenMP accelerated C++) using MSL, with minimal effort, while retaining all CPU and OpenMP functionality.\nThe resulting performance gain of simulating the wave equation is near an order of magnitude for specific settings. This gain attained from using MSL is similar to other GPU-accelerated wave-propagation codes with respect to their CPU variants, but does not come at much increased programming complexity that prohibits the typical scientific programmer to leverage these accelerators. This result shows how unified processing units can be a valuable tool to seismologists and computational scientists in general, lowering the bar to writing performant codes that leverage modern GPUs."
        },
        {
            "heading": "1 Introduction",
            "text": "Scientific computing has always been at the forefront of technological developments in computing. On the point of the latest ARM chips produced by Apple, the M series, the scientific community should act no different. This series of chips is a relatively new component of MacBooks produced by Apple. These are a ARM based chips, which means that its instruction set is fundamentally different from typical notebook, workstation and HPC suited processors from Intel or AMD. According to Apple, the usage of ARM chips in high performance notebooks and workstations has seen yielded increased performance and power efficiency[1].\nMore importantly, however, is that the design of the M series chips are such that they combine the central and graphical processing units (CPU and GPU) onto a single chip, creating a \u2019Unified\u2019 Processing Unit, something atypical for modern systems. This means that both processing units can communicate with the same memory, simplifying many operations. The differences in design are schematically illustrated in Figure 1.\nThe Metal Shading Language (MSL), originally\ndeveloped for the iPhone iOS operating system, is a programming language exclusive tailored to address GPU hardware present on Apple ARM chips in general. Although it has been in development for approximately a decade, it only saw its first stable release in 2019, and it\u2019s first non-mobile usage in 2020, when Apple released their first ARM-based MacBooks, fitted with M1 chips. Prior to this, MSL\nar X\niv :2\n20 6.\n01 79\n1v 3\n[ ph\nys ic\ns. co\nm p-\nph ]\n2 8\nJu n\n20 22\nwas only used to perform GPU operations on mobile devices. Although MSL is also able to control various GPUs from other manufacturers, we do not focus on systems with these GPUs in this work.\nAlthough MSL is mostly focused on enabling graphics-oriented operations on Apple ARM GPUs, it also possesses functionality to instruct these chips for mathematical operations. This aspect of the M chip has, as it currently seems, not received much attention in the computational sciences, outside of the development of an MSL version of the popular TensorFlow software. This might be in part due to the fact that most documentation provided by Apple is focused on the Objective-C and Swift programming languages, typically favoured for developing general-purpose software on MacOS. The focus of these documentations is of course not the computational sciences, and as such, the exposure of the community to the MSL has so far been limited.\nIn this work, we illustrate the usage of MSL in general C++ array operations, and analyse how and when MSL provides benefit over running \u201cplain\u201d (multithreaded) CPU code. Special care will be given on how to enable the GPU operations in existing scientific C++ codes, specifically for numerical simulations of partial differential equations (PDEs) using finite differences. A case study focusing on accelerating elastic wave propagation in two dimensions using the M1 GPU illustrates the potential performance and ease of use of the M chip and unified processing units in general. All our simulations are run using single-precision decimal numbers (floats). Additionally, we provide a web portal that both links to our research codes as well as material helpful to the computational scientist to get started using MSL in C++ [2]."
        },
        {
            "heading": "2 MSL execution and memory model",
            "text": "MSL code itself compiles to instructions that are purely run on the GPU. These compiled functions are called shaders, or kernels. To orchestrate the execution of these kernels from the CPU, the instructions need to be fed from CPU code, i.e., any program we write in, e.g., C++. The communication of these instructions, as well as the scheduling of multiple operations and other \u201csteering\u201d tasks, are performed by the Metal Framework, an ObjectiveC library callable directly from C++ using \u2018metalcpp\u2018 [3]. Performing operations on data with the Metal Framework on a GPU follows a set collection of steps, after [4]:\n1. Create a command buffer and encoder. These objects respectively receive instructions (buffer), and encode them into machine language for the appropriate GPU (encoder);\n2. Place instructions and data addresses in encoder;\n3. Encode instructions with the encoder;\n4. Execute instructions.\nDuring the execution of the command buffer, the CPU can resume operation and synchronize with GPU execution at a later stage. Although one does need to instruct the GPU which data to use for these operations, the data itself does not need to be communicated to the GPU, by virtue of the unified design. The Metal Framework allows one to create this shared data in existing applications, and then simply get a standard C++ pointer to the underlying data such that it can be used in existing CPU code. This greatly simplifies exposing existing codes\u2019 array to new GPU operations, compared to using e.g. NVidia\u2019s CUDA[5]. Trying to access the data from both the CPU and GPU simultaneously creates race conditions, and should be avoided.\nAlthough encoding the commands and data in the buffer is more intricate than typical C++ operations, we provide a simple interface to these operations for both one-dimensional and twodimensional data on our web portal."
        },
        {
            "heading": "3 Basic array operations",
            "text": "To introduce how performant the M GPU is with respect to (M) CPU configurations, we benchmark various \u201dsingle instruction, multiple data\u201d, or SIMD, operations on both processing units for onedimensional data. We test the performance of the SAXPY operation (Single-Precision a \u00b7 x+ y) and a 3- point central differencing scheme. By testing these operations on arrays of different sizes, we can thoroughly demonstrate the overhead required for using both OpenMP and MSL configurations. To generate these numbers, we ran the operations repeatedly on a 2021 MacBook Pro equipped with the M1 Max 10 core CPU (of which 8 performance cores) and a 32 core GPU.\nFigure 2 shows how operations in GPU configurations, as well as operations in multithreaded CPU configuration, only provide benefit when the data is relatively large. Both array operations show fastest performance in serial mode up to an approximate data size of 216 (= 65536). The exact speed-up of using various configurations at largest data size is summarized in Table 1. These sepeed-ups demonstrate that even for simple operations, the usage of the GPU does accelerate one-dimensional array operations. The benefit of using the GPU for these relatively simple array operations in one dimension however only manifests itself at large data sizes, i.e. at 222 elements and up."
        },
        {
            "heading": "4 Multidimensional operations",
            "text": "One staple of physical modelling are spatial derivatives, especially when solving PDEs (e.g. the wave equation) in two or three dimensions. MSL enables one to execute kernels specifically with twoor three-dimensional layouts, greatly simplifying the implementation of kernels that operate on twodimensional or three-dimensional arrays.\nWe detail the performance in CPU and GPU configurations by again performing benchmark of the operations on a 2021 M1 Max chip. The runtimes of these benchmarks are given in Figure 3. The operations again show dominance of the scheduling overhead (for both CPU and GPU) for multithreaded configurations at small data sizes compared to the serial configurations. The crossover point (for all operations) where GPU becomes the most performant configuration seems to be around a data size of 210 \u00d7 210 elements, close to the same number of effective elements for the crossover points in one dimension.\nAs the number of instructions per operation increases (i.e. top to bottom in Figure 3), so does the benefit of both the multithreaded CPU and GPU configurations with respect to serial configuration. Table 1 summarizes the speedup of the MSL, serial and optimal OpenMP configurations, showing how for large data sizes, MSL becomes more than an order of magnitude faster than OpenMP if the operation is complex."
        },
        {
            "heading": "5 Accelerating existing codes: elastic wave propagation on the M series GPU",
            "text": "Possibly the biggest strength of using MSL is its drop-in capabilities. Because the CPU and GPU have unified memory, data created in this unified pool can be readily accessed by both processing units. Thus, MSL allows a user to readily modify existing C++ to be GPU-capable.\nSpecifically, when creating an MSL buffer (i.e. an array that can be seen by the CPU and GPU), one can easily obtain a plain C++ pointer to the underlying data. This way, integrating MSL into an existing C++ application simply requires two additional lines per array: the declaration of the buffer and retrieving the raw C++ pointer (per Listing 1).\nTo demonstrate this capability, we took an existing two-dimensional elastic wave propagation code [6, 7] based on Virieux\u2019s seminal paper [8]. This wave propagation code was developed to perform Full-Waveform Inversion (FWI), an approach to fit recorded vibrations to interior structure of materials. This method finds applications in for example seismology when imaging the entire Earth [9, 10, 11], in non-destructive testing when imaging\nman-made structures[12, 13], and in medical tomography when imaging the human body [14, 15].\nWe implement the integration of the dynamic fields (material velocity in x and z direction, i.e. vx, vz, and vertical, horizontal and shear strain \u03c4xx, \u03c4zz, \u03c4xz) in MSL, but perform the rest of the operations required for FWI on CPU. These operations include recording the entire forward dynamical wavefields (for later use in the computation of sensitivity kernels), injecting point sources, recording wavefields at receivers, and the crosscorrelation between forward and adjoint dynamical fields.\nFigure 4 demonstrates a surprising result with respect to our preceding results. At all domain sizes, the GPU configuration outperforms the 8-threaded OpenMP configuration. Where we were seeing cross-over points between the two configurations only at larger data sizes for simpler array operations, it now seems that the complexity of integrating wavefields means that MSL always outperforms OpenMP.\nAs domain size grows, so does the speed-up of MSL with respect to OpenMP. The simulation code fails at domain sizes above approximately 2000 \u00d7 2000, as system RAM unpredictably runs out on our benchmarking machine. We were able to run a limited number of benchmarks at a domain size of 5000 \u00d7 5000, where the speed-up of MSL with respect to 8-threaded OpenMP was approximately a factor 10."
        },
        {
            "heading": "6 Discussion",
            "text": "Although the acceleration of computational physics by graphical processing units has long been ac-\nknowledged to yield performant codes, the complexity of implementation does create a barrier to its actual usage. The M chip series might herald the start of a paradigm-shift in computational sciences towards usage of unified chips and their programming languages. Although the M1 chip is not the first unified chip to be commercially available, nor is our implementation the first GPU-accelerated physical simulation, its ease of implementation does demonstrate the first implementation on a widely available chip using minimal effort. The upcoming release of the M2 chip promises increased unified memory bandwidth, thus further enlarging the potential for M series chips [16].\nAlthough for simple operations the speed-up of MSL with respect to OpenMP only becomes apparent at relatively large data sizes, in practical use for computational physics this threshold is much lower, due to the amount of instructions of the operations. This means that practically for our studied example (elastic wave propagation), it is typically worthwhile to use MSL over OpenMP. It is, however, not limited to our specific physics or numerical solver; these concepts of acceleration translate well to other PDEs solved with finite differences as well as to other numerical methods such as the finite element method[17, 18]."
        },
        {
            "heading": "6.1 ThreadGroupSize dimensions",
            "text": "When using MSL shader functions, one needs to define in what shape the GPU traverses the operation. This access pattern is designated ThreadGroupSize in MSL, and is defined by a one-, two- or threedimensional vector, depending on the input data. These vectors define in what pattern the multiple parallel cores on the GPU operate on the input data.\nAs an example, we show how we define the traversing of two-dimensional array operations for arrays with nx rows and ny columns. C++ arrays have linear memory layout and are indexed in two dimensions using the following linear index i:\ni = ix \u00b7 ny + iy (1)\nwhere ix is the index of the row, iy is the index of the column of the array. The ThreadGroupSize to launch two-dimensional kernels is defined by (tx, ty), where tx and ty indicate the dimensions of the tile of cores working on the data. The total amount of cores operating on the data in this tile is tx \u00b7 ty.\nThe usage of ThreadGroupSize is similar to CUDA scheduling of thread blocks. The access patterns of the data influence the performance of MSL and CUDA code alike. As an example, consider finite differencing schemes accessing neighboring elements. In these cases it is often beneficial to process multidimensional data in the order it is laid out in memory. Specifically, our two-dimensional data is laid out with strides of 1 element in the second dimension (y), and strides of ny in the first dimension (x). Therefore, we launch our kernels with ThreadGroupSizes of (tx, ty), where ty >> tx. We find that this way our MSL kernels are the most performant. These optimal memory access patterns are well-known for general CPU and GPU programming, where the practice of optimizing access is typically known as memory coalescing [19, 20]."
        },
        {
            "heading": "6.2 Asynchronous operation",
            "text": "As the CPU and GPU on the M series chips can operate asynchronously, there exists a potential further speed-up of array operations and computational tasks in general. We implemented this hybrid configuration for the two-dimensional elastic wave propagation.\nIn this configuration, approximately half the workload is shifted from the GPU back to the CPU on the M chip by letting the CPU integrate the vertical velocity field and the shear strain field, while the other 3 fields are integrated by the GPU. As the strain fields depend on the velocity fields and viceversa, synchronization between the GPU and CPU is performed twice per time-step.\nThe results, however, are disappointing. For any configuration, the runtime of the hybrid configuration is approximately half that of the CPU configuration. This is because the M1 GPU significantly\noutperforms the M1 CPU for the elastic wave propagation, and spends most of the time waiting for synchronization. To actually attain a practical speedup, the workload needs to be divided proportionally to the performance of both processing units, i.e. a larger part of the compute task needs to be allocated to the GPU. This was not implemented for our work."
        },
        {
            "heading": "7 Acknowledgements",
            "text": "We would like to thank the various members of the Seismology and Wave Physics Group for providing valuable feedback on and accommodating a proving ground for our MSL implementation."
        }
    ],
    "title": "Seamless GPU acceleration for C++ based physics with the Metal Shading Language on Apple\u2019s M series unified chips",
    "year": 2022
}