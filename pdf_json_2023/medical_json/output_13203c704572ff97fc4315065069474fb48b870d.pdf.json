{
    "authors": [
        {
            "affiliations": [],
            "name": "Guangting Mai"
        },
        {
            "affiliations": [],
            "name": "Zhizhao Jiang"
        },
        {
            "affiliations": [],
            "name": "Xinran Wang"
        },
        {
            "affiliations": [],
            "name": "Ilias Tachtsidis"
        },
        {
            "affiliations": [],
            "name": "Peter Howell"
        }
    ],
    "id": "SP:1155c6b1f63a88acf05d0d5cb5c6f6502e83f7d2",
    "references": [
        {
            "authors": [
                "C. 630 Alain",
                "Y. Du",
                "L.J. Bernstein",
                "T. Barten",
                "K. Banai"
            ],
            "title": "Listening under difficult",
            "year": 2018
        },
        {
            "authors": [
                "J. Alho",
                "F.H. Lin",
                "M. Sato",
                "H. Tiitinen",
                "M. Sams",
                "I.P. J\u00e4\u00e4skel\u00e4inen"
            ],
            "title": "Enhanced neural synchrony",
            "year": 2014
        },
        {
            "authors": [
                "C.A. Anderson",
                "I.M. Wiggins",
                "P.T. Kitterick",
                "D.E. Hartley"
            ],
            "title": "Adaptive benefit of cross-modal",
            "year": 2017
        },
        {
            "authors": [
                "C.A. Anderson",
                "I.M. Wiggins",
                "P.T. Kitterick",
                "D.E. Hartley"
            ],
            "title": "Pre-operative brain imaging",
            "year": 2019
        },
        {
            "authors": [
                "M. 644 Balconi",
                "E. Grippa",
                "M.E. Vanutelli"
            ],
            "title": "What hemodynamic (fNIRS), electrophysiological (EEG",
            "year": 2015
        },
        {
            "authors": [
                "M. 646 Balconi",
                "M.E. Vanutelli"
            ],
            "title": "Hemodynamic (fNIRS) and EEG (N200) correlates of emotional",
            "year": 2016
        },
        {
            "authors": [
                "M. Balconi",
                "Vanutelli",
                "M.E.M. E"
            ],
            "title": "Empathy in negative and positive interpersonal interactions",
            "year": 2017
        },
        {
            "authors": [
                "M.L. 651 Barren\u00e4s",
                "I. Wikstr\u00f6m"
            ],
            "title": "The influence of hearing and age on speech recognition scores in noise",
            "year": 2000
        },
        {
            "authors": [
                "G.J. 653 Basura",
                "X.S. Hu",
                "J.S. Juan",
                "A.M. Tessier",
                "I. Kovelman"
            ],
            "title": "Human central auditory plasticity",
            "year": 2018
        },
        {
            "authors": [
                "R.E. Bieber",
                "S. Gordon-Salant"
            ],
            "title": "Improving older adults\u2019 understanding of challenging speech",
            "year": 2021
        },
        {
            "authors": [
                "G.A. 658 Blackman",
                "D.A. Hall"
            ],
            "title": "Reducing the effects of background noise during auditory functional",
            "year": 2011
        },
        {
            "authors": [
                "A. 661 Blasi",
                "S. Lloyd-Fox",
                "M.H. Johnson",
                "C. Elwell"
            ],
            "title": "Test\u2013retest reliability of functional near infrared",
            "year": 2014
        },
        {
            "authors": [
                "D.A. Boas",
                "C.E. Elwell",
                "M. Ferrari",
                "G. Taga"
            ],
            "title": "Twenty years of functional near-infrared",
            "year": 2014
        },
        {
            "authors": [
                "J. Campbell",
                "A. Sharma"
            ],
            "title": "Cross-modal re-organization in adults with early stage hearing loss",
            "year": 2014
        },
        {
            "authors": [
                "D.P. 670 Corina",
                "S. Blau",
                "T. LaMarr",
                "L.A. Lawyer",
                "S. Coffey-Corina"
            ],
            "title": "Auditory and visual",
            "year": 2017
        },
        {
            "authors": [
                "H.B. 673 Coslett",
                "M.F. Schwartz"
            ],
            "title": "The parietal lobe and language",
            "venue": "Handbook of Clinical Neurology,",
            "year": 2018
        },
        {
            "authors": [
                "J. 677 Defenderfer",
                "A. Kerr-German",
                "M. Hedrick",
                "A.T. Buss"
            ],
            "title": "Investigating the role of temporal lobe",
            "year": 2017
        },
        {
            "authors": [
                "R.S. 683 Dewey",
                "D.E. Hartley"
            ],
            "title": "Cortical cross-modal plasticity following deafness",
            "year": 2015
        },
        {
            "authors": [
                "B. Efron",
                "R.J. Tibshirani"
            ],
            "title": "An introduction to the bootstrap",
            "venue": "CRC press",
            "year": 1994
        },
        {
            "authors": [
                "M. C"
            ],
            "title": "Cross-modal functional connectivity supports speech understanding in cochlear implant",
            "year": 2022
        },
        {
            "authors": [
                "B.R. Buchsbaum",
                "M. D'Esposito"
            ],
            "title": "Repetition suppression and reactivation in auditory\u2013verbal",
            "year": 2009
        },
        {
            "authors": [
                "N. 693 Gaab",
                "J.D. Gabrieli",
                "G.H. Glover"
            ],
            "title": "Assessing the influence of scanner background noise on",
            "year": 2007
        },
        {
            "authors": [
                "B. 696 Gopinath",
                "E. Rochtchina",
                "J.J. Wang",
                "J. Schneider",
                "S.R. Leeder",
                "P. Mitchell"
            ],
            "title": "Prevalence of age",
            "year": 2009
        },
        {
            "authors": [
                "M. G\u00f6ttlich",
                "Z. Ye",
                "A. Rodriguez-Fornells",
                "T.F. M\u00fcnte",
                "U.M. Kr\u00e4mer"
            ],
            "title": "Viewing socio-affective",
            "year": 2017
        },
        {
            "authors": [
                "Y. Grodzinsky",
                "P. Pieperhoff",
                "C. Thompson"
            ],
            "title": "Stable brain loci for the processing of complex syntax",
            "year": 2021
        },
        {
            "authors": [],
            "title": "Sparse\u201d temporal sampling in auditory fMRI",
            "venue": "Human Brain Mapping,",
            "year": 1999
        },
        {
            "authors": [
                "G. 710 Hickok",
                "D. Poeppel"
            ],
            "title": "The cortical organization of speech processing",
            "venue": "Nature Reviews",
            "year": 2007
        },
        {
            "authors": [
                "L.E. 712 Humes"
            ],
            "title": "Speech understanding in the elderly",
            "venue": "Journal-American Academy of Audiology,",
            "year": 1996
        },
        {
            "authors": [
                "L.E. Humes"
            ],
            "title": "Aging and speech communication: Peripheral, central-auditory, and cognitive factors",
            "year": 2008
        },
        {
            "authors": [
                "L.E. 716 Humes",
                "D. Kewley-Port",
                "D. Fogerty",
                "D. Kinney"
            ],
            "title": "Measures of hearing threshold and temporal",
            "year": 2010
        },
        {
            "authors": [
                "T.J. 718 Huppert",
                "S.G. Diamond",
                "M.A. Franceschini",
                "D.A. Boas"
            ],
            "title": "HomER: a review of time-series",
            "year": 2009
        },
        {
            "authors": [
                "I.S. Johnsrude",
                "A.L. Giraud",
                "R.S. Frackowiak"
            ],
            "title": "Functional imaging of the auditory system: the use",
            "year": 2002
        },
        {
            "authors": [
                "A. MacLeod",
                "Q. Summerfield"
            ],
            "title": "A procedure for measuring auditory and audiovisual speech-reception",
            "year": 1990
        },
        {
            "authors": [
                "M.C. Mutlu",
                "S.B. Erdo\u011fan",
                "O.C. \u00d6zt\u00fcrk",
                "R. Canbeyli",
                "H. Sayba\u015f\u03b9l\u03b9"
            ],
            "title": "Functional Near-Infrared",
            "year": 2020
        },
        {
            "authors": [
                "M. 739 Nahum",
                "H. Lee",
                "M.M. Merzenich"
            ],
            "title": "Principles of neuroplasticity-based rehabilitation",
            "year": 2013
        },
        {
            "authors": [
                "J.E. 746 Peelle",
                "V. Troiani",
                "M. Grossman",
                "A. Wingfield"
            ],
            "title": "Hearing loss in older adults",
            "year": 2011
        },
        {
            "authors": [
                "J.E. 750 Peelle",
                "A. Wingfield"
            ],
            "title": "The neural consequences of age-related hearing loss",
            "year": 2016
        },
        {
            "authors": [
                "E.B. Petersen",
                "M. W\u00f6stmann",
                "J. Obleser",
                "T. Lunner"
            ],
            "title": "Neural tracking of attended versus ignored",
            "year": 2017
        },
        {
            "authors": [
                "L. 760 Pollonini",
                "C. Olds",
                "H. Abaya",
                "H. Bortfeld",
                "M.S. Beauchamp",
                "J.S. Oghalai"
            ],
            "title": "Auditory cortex",
            "year": 2014
        },
        {
            "authors": [
                "J. Rissman",
                "A. Gazzaley",
                "M. D'Esposito"
            ],
            "title": "Measuring functional connectivity during distinct stages",
            "year": 2004
        },
        {
            "authors": [
                "J. 765 Rouger",
                "S. Lagleyre",
                "J.F. D\u00e9monet",
                "B. Fraysse",
                "O. Deguine",
                "P. Barone"
            ],
            "title": "Evolution of crossmodal",
            "year": 2012
        },
        {
            "authors": [
                "J. 767 Rovetti",
                "H. Goy",
                "M.K. Pichora-Fuller",
                "F.A. Russo"
            ],
            "title": "Functional near-infrared spectroscopy",
            "year": 2019
        },
        {
            "authors": [
                "J. Saliba",
                "H. Bortfeld",
                "D.J. Levitin",
                "J.S. Oghalai"
            ],
            "title": "Functional near-infrared spectroscopy",
            "year": 2016
        },
        {
            "authors": [
                "C.J. 771 Scarff",
                "J.C. Dort",
                "J.J. Eggermont",
                "B.G. Goodyear"
            ],
            "title": "The effect of MR scanner noise on",
            "year": 2004
        },
        {
            "authors": [
                "M. 773 Schecklmann",
                "A.C. Ehlis",
                "M.M. Plichta",
                "A.J. Fallgatter"
            ],
            "title": "Functional near-infrared spectroscopy",
            "year": 2008
        },
        {
            "authors": [
                "T. 775 Schoof",
                "S. Rosen"
            ],
            "title": "The role of auditory and cognitive factors in understanding speech in noise",
            "year": 2014
        },
        {
            "authors": [
                "S.K. 777 Scott",
                "C.C. Blank",
                "S. Rosen",
                "R.J. Wise"
            ],
            "title": "Identification of a pathway for intelligible speech",
            "year": 2000
        },
        {
            "authors": [
                "S.K. 779 Scott",
                "S. Rosen",
                "C.P. Beaman",
                "J.P. Davis",
                "R.J. Wise"
            ],
            "title": "The neural processing of masked",
            "year": 2009
        },
        {
            "authors": [
                "J.I. 785 Skipper",
                "J.T. Devlin",
                "D.R. Lametti"
            ],
            "title": "The hearing ear is always found close to the speaking",
            "year": 2017
        },
        {
            "authors": [
                "K. 787 Slade",
                "C.J. Plack",
                "H.E. Nuttall"
            ],
            "title": "The effects of age-related hearing loss on the brain and cognitive",
            "year": 2020
        },
        {
            "authors": [
                "P.E. Souza",
                "C.W. Turner"
            ],
            "title": "Masking of speech in young and elderly listeners with hearing loss",
            "year": 1994
        },
        {
            "authors": [
                "F. Strand",
                "H. Forssberg",
                "T. Klingberg",
                "F. Norrelgen"
            ],
            "title": "Phonological working memory with auditory",
            "year": 2008
        },
        {
            "authors": [
                "M. 793 Stropahl",
                "J. Besser",
                "S. Launer"
            ],
            "title": "Auditory training supports auditory rehabilitation: a state-of-the",
            "year": 2020
        },
        {
            "authors": [
                "S. Tak",
                "M. Uga",
                "G. Flandin",
                "I. Dan",
                "W.D. Penny"
            ],
            "title": "Sensor space group analysis for fNIRS data",
            "year": 2016
        },
        {
            "authors": [
                "K.I. 797 Vaden",
                "S.E. Kuchinsky",
                "J.B. Ahlstrom",
                "J.R. Dubno",
                "M.A. Eckert"
            ],
            "title": "Cortical activity predicts",
            "year": 2015
        },
        {
            "authors": [
                "M.S. 802 Vafaee",
                "A. Gjedde"
            ],
            "title": "Model of blood-brain transfer of oxygen explains nonlinear flow",
            "year": 2000
        },
        {
            "authors": [
                "M. 805 Vogelzang",
                "C.M. Thiel",
                "S. Rosemann",
                "J.W. Rieger",
                "E. Ruigendijk"
            ],
            "title": "Effects of age-related",
            "year": 2021
        },
        {
            "authors": [
                "P. 807 Wijayasiri",
                "D.E. Hartley",
                "I.M. Wiggins"
            ],
            "title": "Brain activity underlying the recovery of meaning",
            "year": 2017
        },
        {
            "authors": [
                "I.M. Wiggins",
                "C.A. Anderson",
                "P.T. Kitterick",
                "D.E. Hartley"
            ],
            "title": "Speech-evoked activation in adult",
            "year": 2016
        },
        {
            "authors": [
                "C.J. Wild",
                "A. Yusuf",
                "D.E. Wilson",
                "J.E. Peelle",
                "M.H. Davis",
                "I.S. Johnsrude"
            ],
            "title": "Effortful listening",
            "year": 2012
        },
        {
            "authors": [
                "P.C. Wong",
                "J.X. Jin",
                "G.M. Gunasekera",
                "R. Abel",
                "E.R. Lee",
                "S. Dhar"
            ],
            "title": "Aging and cortical",
            "year": 2009
        },
        {
            "authors": [
                "D. Wong",
                "R.T. Miyamoto",
                "D.B. Pisoni",
                "M. Sehgal",
                "G.D. Hutchins"
            ],
            "title": "PET imaging of cochlear",
            "year": 1999
        },
        {
            "authors": [
                "T. Yamada",
                "S. Umeyama",
                "K. Matsuda"
            ],
            "title": "Separation of fNIRS signals into functional and systemic",
            "year": 2012
        },
        {
            "authors": [
                "Z. 821 Ye",
                "A. Hammer",
                "E. Camara",
                "T.F. M\u00fcnte"
            ],
            "title": "Pramipexole modulates the neural network of reward",
            "year": 2011
        },
        {
            "authors": [
                "E. 823 Yorgancigil",
                "F. Yildirim",
                "B.A. Urgen",
                "S.B. Erdogan"
            ],
            "title": "An exploratory analysis of the neural",
            "year": 2022
        },
        {
            "authors": [
                "R.J. 826 Zatorre",
                "E. Meyer",
                "A. Gjedde",
                "A.C. Evans"
            ],
            "title": "PET studies of phonetic processing",
            "year": 1996
        },
        {
            "authors": [
                "R.J. Zatorre"
            ],
            "title": "Neural specializations for tonal processing",
            "venue": "Annals of the New York Academy",
            "year": 2001
        },
        {
            "authors": [
                "X. 830 Zhou",
                "E. Burg",
                "A. Kan",
                "R.Y. Litovsky"
            ],
            "title": "Investigating effortful speech perception using fNIRS",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "Functional near-infrared spectroscopy (fNIRS), a non-invasive optical neuroimaging technique that is portable 29 and acoustically silent, has become a promising tool for evaluating auditory brain functions in hearing-30 vulnerable individuals. This study, for the first time, used fNIRS to evaluate neuroplasticity of speech-in-noise 31 processing in older adults. Ten older adults, most of whom had moderate-to-mild hearing loss, participated in a 32 4-week speech-in-noise training. Their speech-in-noise performances and fNIRS brain responses to speech 33 (auditory sentences in noise), non-speech (spectrally-rotated speech in noise) and visual (flashing chequerboards) 34 stimuli were evaluated pre- (T0) and post-training (immediately after training, T1; and after a 4-week retention, 35 T2). Behaviourally, speech-in-noise performances were improved after retention (T2 vs. T0) but not 36 immediately after training (T1 vs. T0). Neurally, we intriguingly found brain responses to speech vs. non-speech 37 decreased significantly in the left auditory cortex after retention (T2 vs. T0 and T2 vs. T1) for which we 38 interpret as suppressed processing of background noise during speech listening alongside the significant 39 behavioural improvements. Meanwhile, functional connectivity within and between multiple regions of 40 temporal, parietal and frontal lobes was significantly enhanced in the speech condition after retention (T2 vs. 41 T0). We also found neural changes before the emergence significant behavioural improvements. Compared to 42 pre-training, responses to speech vs. non-speech in the left frontal/prefrontal cortex were decreased significantly 43 both immediately after training (T1 vs. T0) and retention (T2 vs. T0), reflecting possible alleviation of listening 44 efforts. Finally, connectivity was significantly decreased between auditory and higher-level non-auditory 45 (parietal and frontal) cortices in response to visual stimuli immediately after training (T1 vs. T0), indicating 46 decreased cross-modal takeover of speech-related regions during visual processing. The results thus showed that 47 neuroplasticity can be observed not only at the same time, but also before behavioural changes in speech-in-48 noise perception. To our knowledge, this is the first fNIRS study to evaluate speech-based auditory 49 neuroplasticity in older adults. It thus provides important implications for current research by illustrating the 50 promises of detecting neuroplasticity using fNIRS in hearing-vulnerable individuals. 51\n52\nKey words 53\nfunctional near-infrared spectroscopy (fNIRS), auditory neuroplasticity, older adults, speech-in-noise perception 54\n55\n56\n57\n58\n59\n60\n61\n1 Introduction 62\nHow the brain processes speech is an important topic in auditory cognitive neuroscience. A long-standing 63 focus is to study the brain functions in hearing-vulnerable populations such as older adults and hearing-impaired 64 listeners who experience challenges in speech and language perception (see reviews: Peelle and Wingfield, 2016; 65 Slade et al., 2020). This current study asks questions on how contemporary sophisticated functional 66 neuroimaging techniques can help us practically study this essential topic. Over the years, studies have used 67 techniques, such as functional magnetic resonance imaging (fMRI) and positron emission tomography (PET), to 68 illustrate the breakdown of brain processing of speech and language in older and hearing-impaired listeners 69 (Wong et al., 1999; Wong et al., 2009; Peelle et al., 2011; Vaden et al., 2015, 2016; Vogelzang et al., 2021). 70 Both fMRI and PET detects dynamics of cerebral haemoglobin (haemodynamic responses) at different regions 71 of the brain capturing neural responses with high special resolution and has been used in auditory research (e.g., 72 Zatorre et al., 1996; Zatorre, 2001; Hall et al., 1999, 2009; Peelle, 2014). Using these techniques, studies have 73 observed altered neural sensitivity to speech signals in the auditory cortices (Wong et al., 1999; Wong et al., 74 2009; Peelle et al., 2011) as well as abnormal neural responses at higher-level non-auditory, cognitive regions in 75 these individuals compared to normal-hearing young adult listeners (Wong et al., 1999; Wong et al., 2009; 76 Vaden et al., 2015, 2016; Vogelzang et al., 2021). While both fMRI and PET have been widely used, they also 77 face limitations in auditory research. For example, both techniques can be expensive and may not be always 78 easy to use for large-scale studies in clinical populations (Boas et al., 2014; Pinti et al., 2020). Also, fMRI 79 generates loud extraneous scanning noise that can cause problems for assessing auditory functions (Scarff et al., 80 2004; Gaab et al., 2007). Furthermore, hearing protheses, like hearing aids and cochlear implants, can have 81 intensive magnetic interference with MRI scanning (Saliba et al., 2016; Basura et al., 2018; Harrison et al., 2021) 82 such that hearing aid and cochlear implant users are largely excluded from fMRI research. For PET, although it 83 is noise-free and does not have magnetic interactions with hearing protheses, it requires an invasive procedure, 84 i.e., injection of radioactive isotopes (Johnsrude et al., 2002), making it unsuitable for repetitive use in clinical 85 populations. Besides fMRI and PET, functional near-infrared spectroscopy (fNIRS) is another promising 86 technique to study the neural processes of auditory and speech perception (Pollonini et al., 2014; Wiggins et al., 87 2016; Defenderfer et al., 2017, 2021; Wijayasiri et al., 2017; Lawrence et al., 2018; Mushtaq et al., 2021; Zhou 88 et al., 2022). fNIRS is an optical imaging technique that illuminates scalp of the brain using near-infrared light 89 and measures the intensity of light returning from cortical areas through which haemodynamic responses are 90 estimated (Boas et al., 2014; Pinti et al., 2020). Nowadays it has become more advantageous and practical to use 91 in hearing-vulnerable populations to study their auditory brain functions (Boas et al., 2014; Pinti et al., 2020). 92 Here, we focus on how fNIRS may be feasible and show promises of measuring changes in neural processing of 93 speech in hearing-vulnerable populations. 94\nCompared to fMRI or PET, fNIRS is more portable and relatively less expensive hence easier to use in 95 laboratory environments for clinical populations (Boas et al., 2014; Pinti et al., 2020). Also, compared to fMRI, 96 fNIRS is acoustically silent which is crucial for auditory experiments in those who face challenges in hearing 97 and speech. Furthermore, unlike PET, fNIRS is non-invasive, making it more suitable for repeated 98 measurements, e.g., in longitudinal studies, for clinical populations (Saliba et al., 2016; Basura et al., 2018; 99 Harrison et al., 2021). Lastly, fNIRS is compatible with people who wear hearing protheses like hearing aids 100\nand cochlear implants which can have intensive magnetic interference with MRI scanning (Saliba et al., 2016; 101 Basura et al., 2018; Harrison et al., 2021). Recent research has successfully used fNIRS to illustrate the neural 102 processes of hearing and speech perception in hearing-vulnerable populations. For instance, using fNIRS, Olds 103 et al. (2016) showed that cochlear implant patients with good speech perception exhibited greater auditory 104 cortical activations in response to intelligible than unintelligible speech whilst those with poor perception did 105 not show distinguishable activations, revealing the association between speech perception and cortical activities. 106 Previous studies have also shown successes in detecting listening efforts using fNIRS in older and hearing-107 impaired listeners. Rovetti et al., (2019) showed that reduction of fNIRS prefrontal cortical activations 108 (reflecting alleviation in listening effort) during an auditory N-back task is associated with the use of hearing 109 aids in older adults with hearing loss. Sherafati et al. (2022) showed greater fNIRS prefrontal cortical activations 110 in cochlear implant patients than normal-hearing controls during spoken word listening tasks, reflecting greater 111 listening efforts in the implanted patients. fNIRS also demonstrated promises in detecting cross-modal 112 activations in relation to speech perception in the hearing-impaired. For instance, Anderson et al. (2017) showed 113 that better speech perception in cochlear implant patients is associated with enhanced fNIRS cross-modal 114 activations (auditory cortical responses to visual speech). Fullerton et al. (2022) further showed better speech 115 perception is associated with functional connectivity between auditory and visual cortices in response to visual 116 speech in implanted patients. 117\nDespite these successes of the use of fNIRS and its unique advantages, previous research also confronted 118 limitations of this technique. For example, compared to neuroelectromagnetic methods like 119 electroencephalography (EEG) and magnetoencephalography (MEG), fNIRS measures haemodynamic 120 responses that are sluggish, so it is unable to capture fine-grained timing information of the neural signals (Pinti 121 et al., 2020). Also, its restricted depth of optode penetration makes it only detects neural activations occurred in 122 the outer cortices with a relatively sparse spatial resolution compared to fMRI and PET which can further 123 capture activities within sulci and deep into medial cortices (Pinti et al., 2020). Hence, it is worth noticing these 124 limitations due to which some brain functions may not be easily detected through fNIRS. Therefore, evaluating 125 the feasibility of this technique as discussed above is an important step to confirm its great promises in auditory 126 research. However, most of these efforts so far have focused on cross-sectional experiments and it is unclear 127 how changes in brain functions over time could be feasibly detected by fNIRS. Such changes are referred as 128 \u2018neuroplasticity\u2019, which reflects the capacity of the brain to undergo functional reorganization across time 129 (Innocenti, 2022). Observing this plasticity is important because it should pave the way for future research into 130 the neural mechanisms underlying the behavioural changes, especially in older adults and hearing-impaired 131 populations who have shown the potential to improve their speech perception after proper speech-based training 132 interventions (Stropahl et al., 2020; Bieber and Gordon-Salant, 2021). Clinically, it can help identify those who 133 have strong potentials for positive neuroplastic changes so that individualized treatments can be properly 134 designed (Cramer et al., 2011; Nahum et al., 2013). 135\nThe current study aimed to assess the promises of using fNIRS to detect auditory neuroplasticity through a 136 longitudinal experiment in older adults, most of whom had mild-to-moderate hearing loss. Participants received 137 a 4-week home-based speech-in-noise training and their brain activities were measured by fNIRS over the 138 speech- and language-related cortical areas (temporal, parietal and frontal regions, see Poeppel and Hickock, 139\n2007) both before and after training. Neural responses at various brain regions of interest as well as functional 140 connectivity were examined during an auditory and a visual test and were compared between sessions before 141 and after training. In the auditory test, participants listened to speech (spoken sentences) and non-speech stimuli 142 (spectrally-rotated versions of speech controlling for acoustic complexity to examine speech specificity) 143 presented in noisy backgrounds. We expect increased auditory cortical activities reflecting greater auditory 144 sensitivity after training as well as decreased left frontal/prefrontal cortical activities reflecting reduced listening 145 efforts (Wild et al., 2012; Wijayasiri et al., 2017; Rovetti et al., 2019; Sherafati et al., 2022). We also expect 146 enhancements in brain connectivity reflecting better coordination between language-related areas (Poeppel and 147 Hickock, 2007). In the visual test, participants were exposed to speech-unrelated visual stimuli (flashing 148 chequerboards). Previous research has reported that such stimuli can elicit greater auditory cortical activities in 149 hearing-impaired people reflecting cross-modal maladaptation associated with poorer speech perception 150 (Campbell and Sharma, 2014; Chen et al., 2015; Corina et al., 2017). We expect that this maladaptation would 151 be reduced after training (i.e., reduced auditory cortical activities and/or reduced connectivity between auditory 152 cortex and higher-order parietal and frontal speech-related areas in response to visual stimuli). We anticipate 153 that the observed longitudinal changes should provide new insights into possible underlying mechanisms for 154 changes in speech-in-noise perception over time. 155\n156\n2 Methods and Materials 157\nThis study was approved by the UCL Research Ethics Committee. All participants were consent and 158 reimbursed for their participation. 159\n2.1 Participants 160\nTen right-handed, healthy adult participants (two males) aged between 63 and 78 years (mean = 70, SD = 161 4.5) were recruited. They were all native British English speakers with no reported histories of neurological, 162 cognitive or language disorders. Their pure-tone audiograms (PTAs) were measured for each ear before the 163 speech-in-noise training using a MAICO MA41 Audiometer (MAICO Diagnostics, Germany) at 0.25, 0.5, 1, 2, 164 3, 4, 6 and 8 kHz. Two participants had normal hearing (\u2264 25 dB HL) at all frequencies \u2264 6 kHz in both ears. 165 The other eight showed a general pattern of mild-to-moderate hearing loss (30\u201360 dB HL) especially at high 166 frequencies (> 2 kHz) (see Figure 1). This therefore matches the real-life scenario where majority of healthy 167 ageing populations suffer from high-frequency mild-to-moderate hearing loss (Gopinath et al., 2009; Humes et 168 al., 2010). 169\n174\n2.2 Design 175\nParticipants received a home-based speech-in-noise training through a participant-/patient-friendly App 176 developed by Green et al., (2019). With proper instructions, participants were able to complete the training 177 process by themselves via controlling the Matlab Graphical User Interfaces using a computer tablet at their own 178 home. Training data were saved in an online UCL Research Dropbox in a daily basis so that experimenters 179 could make sure the training was gone through smoothly. During the training, participants listened to storybooks 180 (in British English) spoken by a male and a female speaker sentence-by-sentence presented in background noise 181 and they were asked to identify words within each sentence through multiple-choice word tasks. The 182 background noises were multiple-talker babbles (4, 8 and 16 talker-babbles presented throughout the training in 183 intermixed orders; half males and half females). An adaptive procedure was adopted where the signal-to-noise 184 ratio (SNR) increased/decreased following the decreases/increases in participants\u2019 accuracies over time to keep 185 their attention. The training lasted for 4 weeks, 6 days per week, ~30 minutes per day. 186\nTheir speech-in-noise performances and brain responses to auditory and visual stimuli were measured both 187 before (a day or two before the training as the baseline, T0) and after training (the next day after the training 188 ended, T1; and after an additional 4-week retention period, T2). Figure 2A illustrates the study procedure. 189\n200\n2.3 Speech-in-noise tasks 201\nThe speech-in-noise performances were measured as participants\u2019 speech reception thresholds (SRT) when 202 they listened to short sentences in noisy backgrounds. The sentences were chosen from the Adaptive Sentence 203 List (ASL), each of which consists of three key (content) words (e.g., \u2018He wiped the table\u2019 with key words \u2018he\u2019, 204 \u2018wiped\u2019 and \u2018table\u2019) spoken by a male native British English speaker (MacLeod and Summerfield, 1990). 205 Participants were seated in a quiet room listening to 30 sentences under an 8-talker babble noise (the same 8-206 talker babbles as in the training) via inserted earphones (ER-3 insert earphone, Intelligent Hearing Systems, 207 USA). They were required to verbally report as many words as they could for each sentence. The signal-to-noise 208 ratio (SNR) was initially set at 6 dB for the first sentence (for which all participants were able to recognize all 209 key words) and was decreased by 4 dB for subsequent sentences until < 50% words (i.e., < 2 words) were 210\ncorrectly reported. SNR was then increased/decreased by 2 dB when word correctness was smaller/greater than 211 50% for each of the following sentences. The SRT was measured as the mean SNR across all reversals at the 212 step size of 2 dB (Schoof and Rosen, 2014). Therefore, lower SRT reflects better speech-in-noise performance. 213 The overall sound level (sentence plus noise) was calibrated and fixed at 75 dB SL. The procedure was 214 controlled using Matlab 2016a (Mathworks, USA) with key words for each sentence appearing on the computer 215 screen seen only by the experimenters. The \u2018loose keyword scoring\u2019 approach was followed, meaning that a 216 reported word was considered correct as long as it matched the root of a key word (e.g., \u2018stand\u2019 was considered 217 correct for the keyword \u2018stood\u2019) (Macleod and Summerfield, 1990). There were 6 practice sentences prior to 218 each formal test. 219\n2.4 fNIRS experiment 220\n2.4.1 Optode placements 221\nBrain haemodynamic responses were recorded by a continuous-wave fNIRS system (ETG-4000, Hitachi 222 Medical, Japan; sample rate of 10 Hz) that uses two wavelengths of light at 695 and 830 nm to allow the 223 estimates of changes in both oxy- (HbO) and deoxy-haemoglobin (HbR). The haemodynamics were measured 224 using two 5-by-3 optode probe sets (8 sources and 7 detectors with a fixed source-detector distance of 3 cm on 225 both hemispheres), hence 44 channels covering much of the temporal, parietal and frontal areas (see Figure 2C). 226 These areas are consistent with the some of the most important cortical regions that contribute to human 227 processing of speech and language (Hickok and Poeppel, 2007). To ensure that the channels are in largely the 228 same positions across participants, the probe sets were fitted on a specific cap based on the international 10-20 229 system (channel 7/29 corresponds to T7/T8 near the left/right primary auditory cortex). All participants wore the 230 same cap. The vertex position and the nasion-vertex-inion midline were aligned across participants. To fit the 231 channel positions on the cortical anatomy, the optodes and anatomical surface landmarks (nasion, vertex, inion, 232 left and right ears) were registered using a 3D digitizer provided by the EGT-4000. In practice, it had shown 233 difficult to appropriately register the landmarks in many of our participants (e.g., very small dislocations of 234 digital sensors can cause greatly spurious head shape). Therefore, we used the most successful digitization result 235 in one of the participants as the representative for channel positioning over the anatomical areas for all 236 participants. Since a fixed cap was used, the standardized alignment procedure should not lead to large 237 interindividual variability of channel positions with pronounced effects on the neural measurements. 238\nEfforts were taken by the experimenters to maximize the good optode contacts with the scalp. With 239 participants who had hair, a thin stick was used to help pull out the hair out of the way between the optodes and 240 the scalp. General good contacts were ensured with waveforms having clear cardiac elements monitored by 241 ETG-4000 in real-time. Formal tests started when better contacts could no longer be achieved after every effort 242 was taken. Channels with poor signal quality were further detected and excluded for subsequent analyses (see 243 fNIRS data analyses). 244\n2.4.2 Paradigms 245\nThe fNIRS experiments included an auditory and a visual test. The auditory test used speech and non-246 speech stimuli. The speech stimuli were ASL sentences spoken by the same male speaker as in the speech-in-247\nnoise tasks while non-speech stimuli were spectrally-rotated versions of the speech (Scott et al., 2000, 2009). 248 The spectrally-rotated speech preserves some of the acoustic properties of the original speech, including similar 249 wideband amplitude modulations, harmonic complexity and intonations, but they were highly unintelligible 250 (Scott et al., 2000, 2009). This thus controlled for the auditory processing of acoustic properties that enabled us 251 to study how neuroplasticity may be related to speech-specific factors such as intelligibility. All stimuli were 252 presented via ER-3 earphones under an 8-talker babble noise with the overall sound level (sentence plus noise) 253 calibrated at 75 dB as in the speech-in-noise tasks. The SNR was fixed across all sessions at the SRT obtained 254 from the speech-in-noise task at T0 on a participant-by-participant basis. This ensured that speech stimuli were 255 partly intelligible (~50% word recognition at T0) which thus required similar listening efforts across participants 256 and that neural responses to the speech/non-speech stimuli can be statistically compared across different 257 sessions. 258\nA block design was adopted in which participants sat in front of a computer screen with a grey background 259 and a black cross in the middle for them to keep their eyes on and listened to 12 speech and 12 non-speech 260 blocks presented in a randomized order (see Figure 2B). Each block consisted of 4 sentence trials. All sentences 261 were ~2 seconds long and each sentence plus noise was set to a fixed duration of 2.5 seconds that allowed the 262 babble noise to start before sentence onset and extend after it ended. Another 2.5 seconds silent interval 263 followed each sentence before the next during which participants were required to gently press a button (1, 2 or 264 3) to indicate how many key words they could recognize from the sentence. Each block thus lasted 20 seconds. 265 Silent resting blocks were interleaved between the speech and non-speech blocks, each of which had a duration 266 set randomly at 15, 17, 19 or 21 seconds. This was to reduce the possibility of participants being able to predict 267 when the next speech/non-speech block would happen. The auditory test lasted for ~15 minutes. 268\nFor the visual test, participants were exposed to a flashing radial chequerboard with black and white patches 269 (the two colours reversed at the rate of 8 Hz, see Vafaee and Gjedde, 2000) on the computer screen against a 270 grey background. Similar to the auditory test, a block design was used (see Figure 2B). There were 10 271 chequerboard blocks, each with a duration of 20 seconds. In addition to the chequerboard, a white cross appear 272 in the middle of the screen and was set to change to red and then back to white (timings for the changes were set 273 at random but occurred no earlier than 4 seconds after the block onset). To ensure participants\u2019 engagement, 274 they were asked to focus on the cross and gently press a button whenever the colour changed. Resting blocks 275 were interleaved between stimulus blocks, each with a duration randomly set at 17, 20, or 23 seconds. The 276 visual test lasted for ~7 minutes. 277\nA two to three minutes\u2019 practice run was provided before formally starting each test so that participants 278 were familiarized with the paradigms. Across the entire test period, participants were asked to restrain their 279 body and head movements and consistently keep their eyes on the cross in the middle of the screen. 280\n292\n2.5 fNIRS data analyses 293\nThe signal processing procedure includes preprocessing, data processing of functional activations and 294 connectivity, and statistical analyses. Figure 3 shows the flow charts of this procedure. 295\n2.5.1 Preprocessing 296\nAll signal processing and analyses of fNIRS were conducted using Matlab 2019b (Mathworks, USA) 297 combining customized codes and the HOMER2 (Huppert et al., 2009) (homer-fnirs.org) and SPM-fNIRS 298 toolbox (Tak et al., 2016) (www.nitrc.org/projects/spm_fnirs). We followed the signal processing procedure 299 which was reported to result in high test-retest reliability of speech-evoked responses by fNIRS (Wiggins et al., 300 2016). 301\nThe raw fNIRS intensity signals were first converted to changes in optical density (via the HOMER2 302 function hmrIntensity2OD). Then motion artefacts were corrected using wavelet filtering (via the HOMER2 303 function hmrMotionCorrectWavelet). This removed wavelet coefficients lying more than 0.719 times the inter-304 quantile range below the first or above the third quartiles (Lawrence et al., 2018; Mushtaq et al., 2021). The 305 optical density signals were then bandpass filtered between 0.015 and 0.08 Hz using a zero-phase 3rd-order 306 Butterworth filter (hence covering the presentation frequency of ~0.025 Hz in the block design) which 307 attenuated the low-frequency drifts and changes in arterial blood pressure, respiratory and cardiac activities. The 308 signals were then converted to changes in HbO and HbR concentrations via the modified Beer-Lambert Law 309 (Huppert et al., 2009). The haemodynamic modality separation (HMS) algorithm (Yamada et al., 2012) was 310 finally applied to further minimize the possible remaining systemic physiological noise and motion artefacts 311 (e.g., slow head and body motions) (Wiggins et al., 2016). HMS is based on the fact that changes in HbO and 312 HbR are negatively correlated in the functional responses but positively correlated in the motion and 313 physiological noises. Accordingly, it returned separate estimates of the functional and noise components. We 314 used the functional components for the changes in HbO as the final pre-processed measurements (due to the 315 negative correlation with HbO, functional components for the changes in HbR were thus redundant after 316 applying HMS, see Yamada et al., 2012). 317\nAs well as the pre-processing, channels with poor signal quality were detected despite the efforts to 318 optimize optode contacts with the scalp. The scalp coupling index (SCI), which can effectively identify poor 319 fNIRS signals in speech perception experiments (Pollonini et al., 2014; Mushtaq et al., 2019, 2021; Lawrence et 320 al., 2021), were adopted. The signals with the two wavelengths were first bandpass filtered into 0.5\u20132.5 Hz that 321 represents the cardiac elements captured by fNIRS and were correlated with each other. The higher correlation 322 indicates better optode contacts. Following the criteria used in previous speech perception studies using fNIRS 323 (Mushtaq et al., 2019, 2021; Lawrence et al., 2021), the worst 5% of channels (across all participants and 324 sessions) were excluded for subsequent analyses. This threshold was set to ensure as many channels as possible 325 (i.e., 95% of all channels) were preserved for statistical analyses (Mushtaq et al., 2019, 2021; Lawrence et al., 326 2021) especially when relative low number of participants (i.e., 10) were recruited in the current study. 327\n2.5.2 Data processing of functional activations and connectivity 328\nThe pre-processed fNIRS activations were analysed to measure: (1) functional activation levels; and (2) 329 functional connectivity during both auditory and visual tests. We examined activation levels using block 330 averaging across channels within several regions of interests (ROIs). This approach was employed because test-331 retest reliability in previous studies have shown that fNIRS activations are more reliably estimated when signals 332 are averaged across small number of channels within a given ROI compared to when signals are analysed on a 333 single-channel basis (Plichta et al., 2006; Schecklmann et al., 2008; Blasi et al., 2014; Wiggins et al., 2016). For 334 the auditory tests, we focused on four ROIs of the bilateral auditory cortices (left: Channels 2, 3 and 7; right: 335 Channels 24, 25 and 29), left inferior parietal lobule (Channels 11, 15, 16 and 20) and left frontal/prefrontal 336 cortex (Channels 13, 17, 18 and 22). For the visual tests, we focused on two ROIs of the bilateral auditory 337 cortices. Auditory cortices were chosen as we wanted to assess the functional neuroplasticity in auditory 338 sensitivity in the auditory test and cross-modal maladaptation in the visual test. The other two ROIs (left inferior 339 parietal lobule and frontal/prefrontal cortex) were chosen for the auditory test since they reflect higher-order 340\nspeech and language processing dominant in the left hemisphere (Hickok and Poeppel, 2007). The left inferior 341 parietal lobule is specifically associated with speech-in-noise perception (Alain et al., 2018) as well as semantic 342 processing (Coslett and Schwartz, 2018), whilst left frontal/prefrontal cortex is associated with listening effort 343 (Wild et al., 2012; Wijayasiri et al., 2017; Rovetti et al., 2019; Sherafati et al., 2022). The fNIRS waveforms 344 were temporally averaged across channels within each given ROI for each trial. The averaged waveform was 345 then baseline-corrected by subtracting the mean of the 10-second pre-stimulus period and normalized by 346 dividing the pre-stimulus\u2019 standard deviation (Balconi et al., 2015; Balconi and Vanutelli, 2016, 2017; Mutlu et 347 al., 2020; Yorgancigil et al., 2022). The waveforms were then averaged across trials for each condition in each 348 session. Because the haemodynamic responses peak at ~5 seconds after the stimulus presentation, the response 349 amplitude for a given condition was measured as the mean amplitude across the 5\u201325 seconds\u2019 period 350 (according to the 20 seconds block duration) after stimulus onset. 351\nFunctional connectivity was also quantified following the approach developed by Rissman et al. (2004) 352 which measures correlations of beta-weight series across individual blocks (obtained via General Linear Model, 353 GLM) between different channels. Specifically, design matrices were first created for the three experiment 354 sessions (T0, T1 and T2) and for the auditory and visual tests, respectively. In each matrix, a boxcar regressor 355 was created for every single block. The resting state was not included as a regressor based on the assumption 356 that it did not actively trigger the haemodynamic responses and its activation level approximated to the global 357 intercept. The canonical haemodynamic response function (HRF) was then convolved with the design matrix 358 and the corresponding fNIRS signals were fitted using the convolved matrix via GLM (using the SPM-fNIRS 359 toolbox) to obtain channel-wise beta weights. As such, a beta weight was obtained for every single block that 360 reflected the level of activations of that block in each channel. This thus generated a beta-weight series for each 361 condition (e.g., there were 12 blocks for the speech condition, hence giving a series of 12 beta values) for each 362 channel. Pearson correlations of the beta-weight series were then calculated between individual channels 363 (followed by Fisher-transform) as the values of connectivity between them. Such an approach has been 364 successfully applied to quantify effective haemodynamic functional connectivity (Rissman et al., 2004; Ye et al., 365 2011; Gottlich et al., 2017; Antonucci et al., 2020; Pang et al., 2022). 366\n2.6 Statistical analyses 367\nFollowing acquirement of the behavioural (SRTs) and fNIRS data (activation levels and functional 368 connectivity), statistically analyses were conducted to compare how these data changed between different 369 experiment sessions (T1 vs T0, T2 vs T0 and T2 vs T1). Due to the relatively small number of participants, we 370 applied bootstrapping instead of ANOVAs to avoid requirement for assumptions of specific data distributions 371 (e.g., normality). Specifically, data were resampled with replacement in each replication and a bootstrap 372 distribution was obtained after 10,000 replications. The confidence intervals were measured using the bias-373 corrected and accelerated (BCa) approach (using the Matlab function \u2018bootci\u2019) which corrected the confidence 374 limits by accounting for deviations of the bootstrapped mean from the sample mean and skewness of the 375 distributions (Efron, 1987; Efron and Tibshirani, 1994). An effect was considered as statistically significant if 376 the value of zero fell outside the [1\u2013\u03b1] (\u03b1 as the significance level set at 0.05) confidence interval of a given 377 distribution. For the SRTs and fNIRS activation levels in each ROI, \u03b1 was set at 0.05/3 to correct for the number 378\nof sessions (i.e., 3). For the functional connectivity, \u03b1 was set at 0.05/(946*3) to correct for the total number of 379 connectivity between all 44 channels (i.e., 946) and the number of sessions (i.e., 3). 380\n381\n3 Results 382\n3.1 Behavioural results 383\nBehavioural speech-in-noise performances were measured as SRTs. We found significantly lower SRT (i.e., 384 better speech-in-noise performance) at T2 than at T0, but no significant differences between T1 and T0 or 385 between T2 and T1 (Figure 4 and Table 1). This thus shows that speech-in-noise performance improved after 386 retention (T2) but not immediately after training (T1). 387\n394\n3.2 Neural results 399\n3.2.1 Auditory tests 400\nFunctional activation levels connectivity in response to auditory stimuli were compared between the three 401 sessions. We conducted the comparisons separately for the speech and non-speech conditions, as well as for 402 speech vs. non-speech. 403\nFor the activation levels, we found significantly changes in response magnitudes in the ROIs of left auditory 404 cortex and left frontal/prefrontal cortex (see Figure 5B and Table 2). Specifically, response magnitudes were 405 significantly increased at post-training (T1) than the baseline (T0) in the left auditory cortex for the non-speech 406 condition and significant decreases in responses after retention (T2 vs. T0 and T2 vs. T1) for speech than non-407 speech. In the left frontal/prefrontal cortex, responses amplitudes were significantly reduced at post-training 408 than baseline (T1 vs. T0 and T2 vs. T0) in the left frontal/prefrontal cortex for the speech but not the non-speech 409 condition. In addition, such decreases were also significantly greater for speech than for non-speech. No 410 significant differences were found between sessions in right auditory cortex or left inferior parietal lobule. 411\nFor the functional connectivity, we found significant enhancements of connectivity for both speech and 412 non-speech at T1 and T2 compared to T0 (as well as several decreases, see Figure 5C). Importantly, however, 413 these enhancements were dominant in the speech condition after retention (i.e., T2). There were 14 pairs of 414 channels for T2 vs. T0 and 9 pairs of channels for T2 vs. T1 for the speech condition as opposed to no more than 415 4 pairs of channels in any other comparison for speech/non-speech where significant enhancements were found. 416 These enhancements include intra- and inter-hemispheric connectivity between auditory (channels 2, 3, 7, 23, 24 417 and 29) and non-auditory (parietal and frontal) channels. For speech vs. non-speech, significant enhancements 418 were found between non-auditory channels (posterior temporal lobe, parietal and frontal lobes) for T2 vs T0 and 419 T2 vs. T1 (Figure 5C). These changes in functional connectivity thus corresponded to the behavioural changes 420 where speech-in-noise performances improved after retention (T2) but not immediately after training (T1). 421\nFigure 5. Changes in functional activation levels and connectivity during the auditory test across sessions (T1 422 vs. T0, T2 vs. T0 and T2 vs. T1) for the speech, non-speech and (speech - non-speech) conditions. See Methods 423 and Materials for details of determining statistical significance. (A) Left: ROIs for calculating functional 424 activation levels indicated by red circles. ROIs include the bilateral auditory cortices (left: Channels 2, 3 and 7; 425 right: Channels 24, 25 and 29), left inferior parietal lobule (Channels 11, 15, 16 and 20) and left 426 frontal/prefrontal cortices (Channels 13, 17, 18 and 22). Right: changes in response amplitude in the ROI of the 427 left auditory cortex and left frontal/prefrontal cortex which showed significant changes (no significant changes 428 between sessions were found for the right auditory cortex and left parietal lobule, hence patterns for these two 429 ROIs were not shown here). The left auditory cortex had significant increases in amplitudes in T1 vs. T0 for 430 non-speech and decreases after retention (T2 vs. T0 and T2 vs. T1). The left frontal/prefrontal cortex had 431 significant decreases in amplitude after training for speech and speech vs. non-speech (T1 vs. T0 and T2 vs. T0). 432 Averaged normalised amplitudes for all three sessions (upper panels) and changes across sessions with mean 433 values indicated by circles in the middle and the error bars indicating 95% confidence intervals (significance 434 level \u03b1 corrected at 0.05/3, lower panels). Single, double, and triple asterisk(s) indicate that zeros are outside the 435 95%, 99% and 99.9% confidence intervals, respectively. (B) Changes in functional connectivity. In each panel, 436 significant changes (\u03b1 corrected at 0.05/(964*3)) in intra- (within left and right hemispheres respectively) and 437 inter-hemispheric connectivity are shown respectively (from left to right). The red and blue lines indicate the 438 enhancement/increases and decreases in connectivity, respectively, showing that major enhancement occurred 439 for speech after retention (T2 vs. T0 and T2 vs. T1). 440\n441\n442\n3.2.2 Visual tests 448\nSame as the auditory test, brain activation levels (response amplitudes in ROIs) and functional connectivity 449 for the visual tests were compared between sessions. For the activation levels, we did not find any significant 450 differences in beta-weights in any channel or response amplitudes in either ROI (the left or right auditory cortex) 451 between sessions. 452\nFor the functional connectivity, changes were mainly found in T1 where significant decreases in 453 connectivity were found between 14 pairs of channels for T1 vs. T0, where only one pair was found for T2 vs. 454 T0 (see Figure 6). Out of these 14 pairs for T1 vs. T0, only two pairs were those unrelated to auditory cortices 455 (connectivity between channels 13 and 35 and between 5 and 36); the other 12 pairs were all between auditory 456 cortices (10 pairs at channels 2, 3 and 7 on the left and 2 pairs at channel 24 on the right) and non-auditory 457 regions in the parietal and frontal areas and temporo-parietal junctions. Therefore, the results show that brain 458 connectivity between auditory cortices (especially the left auditory cortex) and higher-level non-auditory 459 regions in response to the visual stimuli were significantly decreased immediately after training, but then such 460 decreases vanished after retention. 461\n469\n4 Discussion 470\n4.1 Neuroplasticity for speech-in-noise processing in noise older adults 471\ndetected by fNIRS 472\nFunctional neuroimaging techniques, such as fMRI and PET, often face limitations in auditory research. 473 These include loud scanning noise (e.g., fMRI) that requires careful design of paradigms in auditory 474 experiments assuming that responses to the noise are the same across different experimental conditions (Hall et 475 al., 1999, 2009; Blackman and Hall, 2011; Peelle, 2014). This could be tricky for hearing-impaired participants 476 who often struggle with hearing in noisy backgrounds and when using speech stimuli who themselves are 477 designed to be presented under noise. PET, on the other hand, does not have such caveat, but it is invasive 478 requiring injection of radioactive isotopes, hence limiting its feasibility of repetitive use for longitudinal studies 479 (Saliba et al., 2016; Basura et al., 2018; Harrison et al., 2021). Compared to fMRI/PET, fNIRS is non-invasive, 480 acoustic silent/low noise and feasibly used longitudinally. In the current study, we used fNIRS to conduct a 481 longitudinal study to examine auditory neuroplasticity in older adults. To our knowledge, there is the first study 482 using fNIRS to examine neuroplasticity in terms of speech-in-noise perception. Most of our older adults (eight 483 out of ten) had mild-to-moderate hearing loss, especially at high-frequencies (> 2 kHz), consistent with the real-484 life patterns of sensorineural hearing loss during normal ageing (Gopinath et al., 2009; Humes et al., 2010). 485\nOlder adults often face challenges in listening to speech under noisy environments (Humes, 1996), especially for 486 those who have hearing loss (Souza and Turner, 1994; Barren\u00e4s and Wikstr\u00f6m, 2000; Humes, 2008) and 487 speech-based training has been provided aiming to improve their speech-in-noise perception (Stropahl et al., 488 2020; Bieber and Gordon-Salant, 2021). Our results showed both behavioural and neural changes after training. 489\nWe showed significant behavioural improvements (i.e., speech-in-noise performances) after the retention 490 period (T2), but not immediately after training (T1) compared to the pre-training baseline (T0) (Figure 4). This 491 corresponded to enhancements in functional connectivity during the auditory tests. Significant enhancements in 492 connectivity were predominantly observed for the speech condition at T2 (T2 vs. T0 and T2 vs. T1), but not T1 493 (T1 vs. T0) (Figure 5B). Such enhancements include greater intra- and inter-hemispheric connectivity between 494 channels across bilateral temporal and parietal and frontal regions. This may indicate that changes in wide-495 spread functional connectivity could be potential indices for behavioural changes in speech-in-noise perception. 496 This is also consistent with arguments that speech perception involves functioning of large-scale neural 497 networks encompassing multiple wide-spread cortical regions that wire together rather than functioning of a 498 single hub (Hickok and Poeppel, 2007). As indicated in our results, such networks whose enhancements were 499 observed include not only lower-order auditory/temporal regions, but also higher-order non-auditory (parietal 500 and frontal) regions. It has been reported that parietal cortices are involved with short-term phonological storage 501 (Buchsbaum and D'Esposito, 2009), sensorimotor speech integration (Alho et al., 2014; Skipper et al., 2017) and 502 semantic processing (Coslett and Schwartz, 2018), whilst frontal cortices are related to effortful listening (Wild 503 et al., 2012; Wijayasiri et al., 2017), phonological working memory maintenance (Strand et al., 2008; Liebenthal 504 et al., 2013) and syntactic processing (Grodzinsky et al., 2021) during speech perception. Also, the 505 enhancements of inter-hemispheric connectivity indicate the potential importance of coordination and 506 cooperation between the two hemispheres for speech-in-noise perception, which is a result, to our knowledge, 507 that has not been reported previously. 508\nWe also found neural changes in the ROI of the left auditory cortex in the auditory tests that correspond to 509 the behavioural changes. Intriguingly, we found significant decreases in functional activations in the left 510 auditory cortex comparing speech with non-speech at T2 (T2 vs. T0 and T2 vs. T1) (Figure 5A). This is, 511 however, inconsistent with our hypothesis predicting that auditory sensitivity, especially that to speech stimuli, 512 should increase after training. A possible reason may be that the auditory stimuli consisted of not only target 513 stimuli (speech/non-speech sentences), but also background noise (multi-talker babbles, see 2.4.2). The 514 decreased activations may thus be explained by suppression in neural responses to background noise in the left 515 auditory cortex. It is noticeable that these significant decreases (speech vs. non-speech) were due to decreased 516 responses in speech, while at the same time, increased responses to non-speech (see Table 2). We thus suggest 517 that this can be interpreted as overall combined effects of neural suppression of background noise during speech 518 listening (leading to decreased responses in the speech condition) and increases in general auditory sensitivity 519 (leading to increased responses in the non-speech condition). This interpretation is consistent with previous 520 studies showing that neural suppression of background noise could be more important than neural enhancement 521 of target speech to achieve successful speech-in-noise perception in older adults with hearing loss (Peterson et 522 al., 2017). Interestingly, these effects were significant only on the left, but not right, auditory cortex, further 523 stressing the hemispheric specificity of speech processing (Hickok and Poeppel, 2007). To our knowledge, this 524\nis the very first finding of possible background suppression observed by haemodynamic responses to speech in 525 noise according to longitudinal changes. 526\nFurthermore, we also observed neural changes can occur before the significant changes in behavioural 527 performances. Specifically, functional activation decreased in the left frontal/prefrontal cortex during the 528 auditory tests at both T1 and T2 compared to T0, hence taking place before the behavioural improvements that 529 only emerged at T2. These decreases occurred for the speech condition and were significantly greater for speech 530 than non-speech (see Figure 5A and Table 2). This thus indicates that such effects were not merely driven by 531 acoustics, but also higher-level speech-specific features like intelligibility. Previous research has demonstrated 532 that activations in the left frontal/prefrontal regions reflect listening efforts during auditory and speech 533 perception in populations with various hearing status, including young normal-hearing adults (Wild et al., 2012; 534 Wijayasiri et al., 2017), older adults with normal hearing (Wong et al., 2009) and mild-to-moderate hearing loss 535 (Rovetti et al., 2019), and cochlear implant patients who have severe hearing impairment (Sherafati et al., 2022). 536 Therefore, this result demonstrated reduced listening effort during speech-in-noise perception even before the 537 occurrence of behavioural improvement and such reduction persisted after the retention period. 538\nWe also observed significant decreases in functional connectivity between auditory cortices and non-539 auditory parietal and frontal regions during the visual (checkerboard flashing) test (T1 vs. T0), which also 540 occurred before the significant behavioural changes. Previous studies have shown greater auditory cortical 541 activities in hearing-impaired people when they process non-auditory (e.g., visual) stimuli possibly reflecting 542 functional takeover of the auditory functions (Rouger et al., 2012; Campbell and Sharma, 2014; Chen et al., 543 2015; Dewey and Hartley, 2015; Corina et al., 2017) associated with worsened speech perception (Campbell 544 and Sharma, 2014). The current result may thus reflect decreases in cross-modal takeover after training. Also, 545 this result should be the first time to indicate the possible takeover effects reflected by functional connectivity 546 between auditory and higher-order speech-related areas. This may also reflect greater suppression of activities in 547 auditory-related areas during visual stimulations. However, such decreases did not persist after retention and 548 thus did not correspond to the changes in speech-in-noise performances. We argue that this may be because 549 older participants in the current study had either normal hearing or mild-to-moderate hearing loss, while the 550 takeover effects shown in the previous studies were reported in those with severe hearing loss (Campbell and 551 Sharma, 2014; Chen et al., 2015; Dewey and Hartley, 2015; Corina et al., 2017). It is thus possible that, with 552 less impaired hearing, our participants may have lower potentials for cross-modal neuroplastic changes. 553 Therefore, while these decreases were observed immediately after training, they may be harder to persist, 554 especially when the training had stopped during the retention period. Nonetheless, we demonstrated these 555 longitudinal changes in cross-modal activations in healthy older participants that have not been reported in 556 previous studies, hence illustrating the promises of using fNIRS to study such changes in more hearing-557 vulnerable populations in the future. 558\nTaken together, our results demonstrated the auditory neuroplasticity using fNIRS where longitudinal 559 changes in brain functions in response to auditory and visual stimuli occurred along with changes in behavioural 560 (i.e., speech-in-noise) performances. Specifically, we found that large-scale functional connectivity in response 561 to speech in noise was enhanced corresponding to the behavioural improvements. We also found corresponding 562 decreases in left auditory cortical responses to speech vs. non-speech, possibly reflecting neural suppression of 563\nbackground noise that contributes the behavioural improvements. Crucially, we also demonstrated that neural 564 changes, i.e., decreased left frontal/prefrontal responses to speech (reflecting reduced listening efforts) and 565 decreased visual-elicited connectivity between auditory cortices and higher-order speech-related non-auditory 566 areas (reflecting reduced cross-modal takeover and/or greater cross-modal suppression), occurred before the 567 emergence of behavioural improvements. These changes can thus be seen as neural precursors that would not be 568 detected solely through behavioural measurements, hence indicating predictive/prognostic potentials for 569 treatments of speech-in-noise perception in hearing-impaired populations. 570\n4.2 Limitations and future research 571\nThe current finding that speech-in-noise performance was improved only after retention (T2) rather than 572 immediately after training (T1) indicates that the training may have resulted in a longer/medium-term rather 573 than an immediate behavioural effect. Alternatively, this may be due to learning effects of multiple experiment 574 sessions. This would also apply to changes in neural activities observed here. Future studies including a control 575 group without receipt of training would help to disentangle the training and learning effects. Nonetheless, an 576 important goal of our study was to assess the promises of fNIRS to study auditory neuroplasticity alongside 577 behavioural changes without much concerning about the exact driver of this plasticity. In this sense, it is less 578 important to clarify the training and learning effects, whereas the speech-based training can be seen as a tool that 579 helped facilitate the emergence of neuroplastic changes. 580\nAnother limitation was the small sample size. More participants would be recruited to have greater 581 statistical power in the future and to allow for better estimation of how neural changes are associated with 582 behavioural changes. We suggest that the bootstrapping approach applied here has mitigated this potential 583 concern for sample size, but a larger sample size would be needed to validate our results by future studies. Also, 584 future research would apply fNIRS in those who have more severe hearing impairment and/or those with 585 hearing protheses (e.g., hearing aids and cochlear implants) to further prove the promises of fNIRS in wider 586 hearing-vulnerable populations. 587\n4.3 Conclusion 588\nTo our knowledge, the current study is the first to use the optical neuroimaging technique of fNIRS to test 589 and observed longitudinal changes in auditory functions in older adults. fNIRS is a tool that has unique 590 advantages to assess and monitor functional brain activities in hearing-vulnerable populations over other 591 neuroimaging techniques like fMRI and PET. Here, we demonstrated evidence for detecting neuroplasticity for 592 speech-in-noise perception using fNIRS. Novel findings of functional neural changes were illustrated along with 593 behavioural changes in longitudinal experiments in older adults after speech-in-noise training. Corresponding to 594 improvements in speech-in-noise performances, we observed increased functional connectivity across wide-595 spread speech- and language-related regions reflecting enhancement of inter-regional coordination/cooperation 596 to process speech, as well as decreased left auditory cortical responses to speech in noise possibly reflecting 597 neural suppression of background noise. More interestingly, neural changes not only occurred at the same time 598 as behavioural improvements, but also emerged as neural precursors before these improvements took place. 599 Specifically, listening effort to speech and cross-modal takeover was reduced (decreased activations in the left 600\nprefrontal/frontal cortex during speech listening and decreased connectivity between auditory cortices and 601 higher-order non-auditory areas during exposure of visual stimuli, respectively) before speech-in-noise 602 performances could be improved. To our knowledge, these novel findings have not been previously reported. 603 The findings thus open up new opportunities for future studies to base on to further investigate neuro-markers 604 for functional changes during speech processing in older adults. We also argue that the current study should lay 605 the ground for evaluating auditory neuroplasticity in wider hearing-impaired populations in the future, such as 606 those who wear hearing protheses (e.g., hearing-aid and cochlear implant users). 607\n608\n5 Acknowledgement 609\nAll experiments were conducted at the neuroimaging laboratories based at UCL Department of Speech, 610 Hearing and Phonetic Sciences. We thank the lab\u2019s experimental officer Mr Andrew Clark for the technical 611 assistance. We thank Dr Tim Green and Prof Stuart Rosen (UCL) for providing the speech-in-noise training 612 program and the guidance for its application, and Prof Rosen for providing the Matlab scripts that create the 613 spectrally-rotated speech. We also thank Dr Paola Pinti for useful advice on fNIRS signal processing. Parts of 614 the Matlab scripts for signal preprocessing was provided by Dr Ian Wiggins (University of Nottingham). 615\n616\n6 Declarations 617\nFunding: The ETG-4000 fNIRS equipment was purchased and managed through a Wellcome Trust Multi-User 618 Equipment Grant (108453/Z/15/Z) awarded to PH. The study was financially supported by a UCL Graduate 619 Scholarship for Cross-disciplinary Training programme awarded to GM. 620\nCompeting interests: The authors have no competing interests to declare relevant to the content of this article. 621\nEthics approval: The current study was approved by the UCL Research Ethics Committee. All participants 622 were consent and reimbursed for their participation. 623\nData availability: data will be available upon request. 624\nAuthors\u2019 contributions: GM - conceptualization, funding acquisition, data collection, data analyses, original 625 paper drafting, paper review and editing. ZJ \u2013 data collection, data analyses, paper review. XW \u2013 data collection, 626 data analyses. IT \u2013 conceptualization, funding acquisition, paper review and editing, supervision. PH \u2013 627 conceptualization, funding acquisition, paper review and editing, supervision. 628\n629\n7 References 630\nAlain, C., Du, Y., Bernstein, L. J., Barten, T., & Banai, K. (2018). Listening under difficult conditions: An 631 activation likelihood estimation meta analysis. Human brain mapping, 39(7), 2695-2709. 632\nAlho, J., Lin, F. H., Sato, M., Tiitinen, H., Sams, M., & J\u00e4\u00e4skel\u00e4inen, I. P. (2014). Enhanced neural synchrony 633 between left auditory and premotor cortex is associated with successful phonetic categorization. Frontiers in 634 Psychology, 5, 394. 635\nAnderson, C. A., Wiggins, I. M., Kitterick, P. T., & Hartley, D. E. (2017). Adaptive benefit of cross-modal 636 plasticity following cochlear implantation in deaf adults. Proceedings of the National Academy of Sciences, 637 114(38), 10256-10261. 638\nAnderson, C. A., Wiggins, I. M., Kitterick, P. T., & Hartley, D. E. (2019). Pre-operative brain imaging using 639 functional near-infrared spectroscopy helps predict cochlear implant outcome in deaf adults. Journal of the 640 Association for Research in Otolaryngology, 20(5), 511-528. 641\nAntonucci, L. A., Penzel, N., Pergola, G., Kambeitz-Ilankovic, L., Dwyer, D., Kambeitz, J., ... & Koutsouleris, 642 N. (2020). Multivariate classification of schizophrenia and its familial risk based on load-dependent attentional 643 control brain functional connectivity. Neuropsychopharmacology, 45(4), 613-621. 644\nBalconi, M., Grippa, E., & Vanutelli, M. E. (2015). What hemodynamic (fNIRS), electrophysiological (EEG) 645 and autonomic integrated measures can tell us about emotional processing. Brain and Cognition, 95, 67-76. 646\nBalconi, M., & Vanutelli, M. E. (2016). Hemodynamic (fNIRS) and EEG (N200) correlates of emotional inter-647 species interactions modulated by visual and auditory stimulation. Scientific reports, 6(1), 1-11. 648\nBalconi, M., & Vanutelli, M. E. M. E. (2017). Empathy in negative and positive interpersonal interactions. What 649 is the relationship between central (EEG, fNIRS) and peripheral (autonomic) neurophysiological responses?. 650 Advances in Cognitive Psychology, 13(1), 105. 651\nBarren\u00e4s, M. L., & Wikstr\u00f6m, I. (2000). The influence of hearing and age on speech recognition scores in noise 652 in audiological patients and in the general population. Ear and hearing, 21(6), 569-577. 653\nBasura, G. J., Hu, X. S., Juan, J. S., Tessier, A. M., & Kovelman, I. (2018). Human central auditory plasticity: a 654 review of functional near infrared spectroscopy (fNIRS) to measure cochlear implant performance and tinnitus 655 perception. Laryngoscope investigative otolaryngology, 3(6), 463-472. 656\nBieber, R. E., & Gordon-Salant, S. (2021). Improving older adults\u2019 understanding of challenging speech: 657 Auditory training, rapid adaptation and perceptual learning. Hearing Research, 402, 108054. 658\nBlackman, G. A., & Hall, D. A. (2011). Reducing the effects of background noise during auditory functional 659 magnetic resonance imaging of speech processing: qualitative and quantitative comparisons between two image 660 acquisition schemes and noise cancellation. Journal of Speech, Language and Hearing Research, 54(2):693-704. 661\nBlasi, A., Lloyd-Fox, S., Johnson, M. H., & Elwell, C. (2014). Test\u2013retest reliability of functional near infrared 662 spectroscopy in infants. Neurophotonics, 1(2), 025005. 663\nBoas, D. A., Elwell, C. E., Ferrari, M., & Taga, G. (2014). Twenty years of functional near-infrared 664 spectroscopy: introduction for the special issue. Neuroimage, 85, 1-5. 665\nCampbell, J., & Sharma, A. (2014). Cross-modal re-organization in adults with early stage hearing loss. PloS 666 one, 9(2), e90594. 667\nChen, L. C., Sandmann, P., Thorne, J. D., Bleichner, M. G., & Debener, S. (2016). Cross-modal functional 668 reorganization of visual and auditory cortex in adult cochlear implant users identified with fNIRS. Neural 669 plasticity, 2016. 670\nCorina, D. P., Blau, S., LaMarr, T., Lawyer, L. A., & Coffey-Corina, S. (2017). Auditory and visual 671 electrophysiology of deaf children with cochlear implants: Implications for cross-modal plasticity. Frontiers in 672 Psychology, 8, 59. 673\nCoslett, H. B., & Schwartz, M. F. (2018). The parietal lobe and language. Handbook of Clinical Neurology, 151, 674 365-375. 675\nCramer, S. C., Sur, M., Dobkin, B. H., O'Brien, C., Sanger, T. D., Trojanowski, J. Q., ... & Vinogradov, S. 676 (2011). Harnessing neuroplasticity for clinical applications. Brain, 134(6), 1591-1609. 677\nDefenderfer, J., Kerr-German, A., Hedrick, M., & Buss, A. T. (2017). Investigating the role of temporal lobe 678 activation in speech perception accuracy with normal hearing adults: An event-related fNIRS study. 679 Neuropsychologia, 106, 31-41. 680\nDefenderfer, J., Forbes, S., Wijeakumar, S., Hedrick, M., Plyler, P., & Buss, A. T. (2021). Frontotemporal 681 activation differs between perception of simulated cochlear implant speech and speech in background noise: An 682 image-based fNIRS study. NeuroImage, 240, 118385. 683\nDewey, R. S., & Hartley, D. E. (2015). Cortical cross-modal plasticity following deafness measured using 684 functional near-infrared spectroscopy. Hearing Research, 325, 55-63. 685\nEfron, B. (1987). Better bootstrap confidence intervals. Journal of the American statistical Association, 82(397), 686 171-185. 687\nEfron, B., & Tibshirani, R. J. (1994). An introduction to the bootstrap. CRC press. 688\nFullerton, A. M., Vickers, D. A., Luke, R., Billing, A. N., McAlpine, D., Hernandez-Perez, H., ... & McMahon, 689 C. M. (2022). Cross-modal functional connectivity supports speech understanding in cochlear implant users. 690 Cerebral Cortex. 691\nBuchsbaum, B. R., & D'Esposito, M. (2009). Repetition suppression and reactivation in auditory\u2013verbal short-692 term recognition memory. Cerebral Cortex, 19(6), 1474-1485. 693\nGaab, N., Gabrieli, J. D., & Glover, G. H. (2007). Assessing the influence of scanner background noise on 694 auditory processing. I. An fMRI study comparing three experimental designs with varying degrees of scanner 695 noise. Human brain mapping, 28(8), 703-720. 696\nGopinath, B., Rochtchina, E., Wang, J.J., Schneider, J., Leeder, S.R., & Mitchell, P. (2009). Prevalence of age-697 related hearing loss in older adults: Blue Mountains Study. Archives of Internal Medicine, 169:415-418. 698\nG\u00f6ttlich, M., Ye, Z., Rodriguez-Fornells, A., M\u00fcnte, T. F., & Kr\u00e4mer, U. M. (2017). Viewing socio-affective 699 stimuli increases connectivity within an extended default mode network. NeuroImage, 148, 8-19. 700\nGrodzinsky, Y., Pieperhoff, P., & Thompson, C. (2021). Stable brain loci for the processing of complex syntax: 701 A review of the current neuroimaging evidence. Cortex, 142, 252-271. 702\nHall, D. A., Haggard, M. P., Akeroyd, M. A., Palmer, A. R., Summerfield, A. Q., Elliott, M. R., ... & Bowtell, R. 703 W. (1999). \u201cSparse\u201d temporal sampling in auditory fMRI. Human Brain Mapping, 7(3), 213-223. 704\nHall, D. A., Chambers, J., Akeroyd, M. A., Foster, J. R., Coxon, R., & Palmer, A. R. (2009). Acoustic, 705 psychophysical, and neuroimaging measurements of the effectiveness of active cancellation during auditory 706 functional magnetic resonance imaging. The Journal of the Acoustical Society of America, 125(1), 347-359. 707\nHarrison, S. C., Lawrence, R., Hoare, D. J., Wiggins, I. M., & Hartley, D. E. (2021). Use of Functional Near-708 Infrared Spectroscopy to Predict and Measure Cochlear Implant Outcomes: A Scoping Review. Brain Sciences, 709 11(11), 1439. 710\nHickok, G., & Poeppel, D. (2007). The cortical organization of speech processing. Nature Reviews 711 Neuroscience, 8(5), 393-402. 712\nHumes, L. E. (1996). Speech understanding in the elderly. Journal-American Academy of Audiology, 7, 161-713 167. 714\nHumes, L. E. (2008). Aging and speech communication: Peripheral, central-auditory, and cognitive factors 715 affecting the speech-understanding problems of older adults. The ASHA Leader, 13(5), 10-33. 716\nHumes, L.E., Kewley-Port, D., Fogerty, D., & Kinney, D. (2010) Measures of hearing threshold and temporal 717 processing across the adult lifespan. Hearing Research 264:30-40. 718\nHuppert, T. J., Diamond, S. G., Franceschini, M. A., & Boas, D. A. (2009). HomER: a review of time-series 719 analysis methods for near-infrared spectroscopy of the brain. Applied optics, 48(10), D280-D298. 720\nJohnsrude, I. S., Giraud, A. L., & Frackowiak, R. S. (2002). Functional imaging of the auditory system: the use 721 of positron emission tomography. Audiology and Neurotology, 7(5), 251-276. 722\nLawrence, R. J., Wiggins, I. M., Anderson, C. A., Davies-Thompson, J., & Hartley, D. E. (2018). Cortical 723 correlates of speech intelligibility measured using functional near-infrared spectroscopy (fNIRS). Hearing 724 research, 370, 53-64. 725\nLiebenthal, E., Sabri, M., Beardsley, S. A., Mangalathu-Arumana, J., & Desai, A. (2013). Neural dynamics of 726 phonological processing in the dorsal auditory stream. Journal of Neuroscience, 33(39), 15414-15424. 727\nMacLeod, A., & Summerfield, Q. (1990). A procedure for measuring auditory and audiovisual speech-reception 728 thresholds for sentences in noise: Rationale, evaluation, and recommendations for use. British Journal of 729 Audiology, 24(1), 29-43. 730\nMutlu, M. C., Erdo\u011fan, S. B., \u00d6zt\u00fcrk, O. C., Canbeyli, R., & Sayba\u015f\u03b9l\u03b9, H. (2020). Functional Near-Infrared 731 Spectroscopy Indicates That Asymmetric Right Hemispheric Activation in Mental Rotation of a Jigsaw Puzzle 732 Decreases With Task Difficulty. Frontiers in Human Neuroscience, 14, 252. 733\nMushtaq, F., Wiggins, I. M., Kitterick, P. T., Anderson, C. A., & Hartley, D. E. (2019). Evaluating time-734 reversed speech and signal-correlated noise as auditory baselines for isolating speech-specific processing using 735 fNIRS. PLoS One, 14(7), e0219927. 736\nMushtaq, F., Wiggins, I. M., Kitterick, P. T., Anderson, C. A., & Hartley, D. E. (2021). Investigating cortical 737 responses to noise-vocoded speech in children with normal hearing using functional near-infrared spectroscopy 738 (fNIRS). Journal of the Association for Research in Otolaryngology, 22(6), 703-717. 739\nNahum, M., Lee, H., & Merzenich, M. M. (2013). Principles of neuroplasticity-based rehabilitation. Progress in 740 brain research, 207, 141-171. 741\nOlds, C., Pollonini, L., Abaya, H., Larky, J., Loy, M., Bortfeld, H., ... & Oghalai, J. S. (2016). Cortical 742 activation patterns correlate with speech understanding after cochlear implantation. Ear and hearing, 37(3), e160. 743\nPang, J., Guo, H., Tang, X., Fu, Y., Yang, Z., Li, Y., An, N., Luo, J., Yao, Z., & Hu, B. (2022). Uncovering the 744 global task-modulated brain network in chunk decomposition with Chinese characters. NeuroImage, 247, 745 118826. 746\nPeelle, J. E., Troiani, V., Grossman, M., & Wingfield, A. (2011). Hearing loss in older adults affects neural 747 systems supporting speech comprehension. Journal of Neuroscience, 31(35), 12638-12643. 748\nPeelle, J. E. (2014). Methodological challenges and solutions in auditory functional magnetic resonance imaging. 749 Frontiers in Neuroscience, 8, 253. 750\nPeelle, J. E., & Wingfield, A. (2016). The neural consequences of age-related hearing loss. Trends in 751 Neurosciences, 39(7), 486-497. 752\nPetersen, E. B., W\u00f6stmann, M., Obleser, J., & Lunner, T. (2017). Neural tracking of attended versus ignored 753 speech is differentially affected by hearing loss. Journal of Neurophysiology, 117(1), 18-27. 754\nPinti, P., Tachtsidis, I., Hamilton, A., Hirsch, J., Aichelburg, C., Gilbert, S., & Burgess, P. W. (2020). The 755 present and future use of functional near infrared spectroscopy (fNIRS) for cognitive neuroscience. Annals of 756 the New York Academy of Sciences, 1464(1), 5-29. 757\nPlichta, M. M., Herrmann, M. J., Baehne, C. G., Ehlis, A. C., Richter, M. M., Pauli, P., & Fallgatter, A. J. 758 (2006). Event-related functional near-infrared spectroscopy (fNIRS): are the measurements reliable?. 759 Neuroimage, 31(1), 116-124. 760\nPollonini, L., Olds, C., Abaya, H., Bortfeld, H., Beauchamp, M. S., & Oghalai, J. S. (2014). Auditory cortex 761 activation to natural speech and simulated cochlear implant speech measured with functional near-infrared 762 spectroscopy. Hearing Research, 309, 84-93. 763\nRissman, J., Gazzaley, A., and D'Esposito, M. (2004) Measuring functional connectivity during distinct stages 764 of a cognitive task. Neuroimage, 23:752-763. 765\nRouger, J., Lagleyre, S., D\u00e9monet, J. F., Fraysse, B., Deguine, O., & Barone, P. (2012). Evolution of crossmodal 766 reorganization of the voice area in cochlear implanted deaf patients. Human Brain Mapping, 33(8), 1929-1940. 767\nRovetti, J., Goy, H., Pichora-Fuller, M. K., & Russo, F. A. (2019). Functional near-infrared spectroscopy as a 768 measure of listening effort in older adults who use hearing aids. Trends in Hearing, 23, 2331216519886722. 769\nSaliba, J., Bortfeld, H., Levitin, D. J., & Oghalai, J. S. (2016). Functional near-infrared spectroscopy for 770 neuroimaging in cochlear implant recipients. Hearing Research, 338, 64-75. 771\nScarff, C. J., Dort, J. C., Eggermont, J. J., & Goodyear, B. G. (2004). The effect of MR scanner noise on 772 auditory cortex activity using fMRI. Human Brain Mapping, 22(4), 341-349. 773\nSchecklmann, M., Ehlis, A. C., Plichta, M. M., & Fallgatter, A. J. (2008). Functional near-infrared spectroscopy: 774 a long-term reliable tool for measuring brain activity during verbal fluency. Neuroimage, 43(1), 147-155. 775\nSchoof, T., & Rosen, S. (2014). The role of auditory and cognitive factors in understanding speech in noise by 776 normal-hearing older listeners. Frontiers in Aging Neuroscience, 6, 307. 777\nScott, S. K., Blank, C. C., Rosen, S., & Wise, R. J. (2000). Identification of a pathway for intelligible speech in 778 the left temporal lobe. Brain, 123(12), 2400-2406. 779\nScott, S. K., Rosen, S., Beaman, C. P., Davis, J. P., & Wise, R. J. (2009). The neural processing of masked 780 speech: Evidence for different mechanisms in the left and right temporal lobes. The Journal of the Acoustical 781 Society of America, 125(3), 1737-1743. 782\nSherafati, A., Dwyer, N., Bajracharya, A., Hassanpour, M. S., Eggebrecht, A. T., Firszt, J. B., Culver, J. P. & 783 Peelle, J. E. (2022). Prefrontal cortex supports speech perception in listeners with cochlear implants. eLife, 11, 784 e75323. 785\nSkipper, J. I., Devlin, J. T., & Lametti, D. R. (2017). The hearing ear is always found close to the speaking 786 tongue: Review of the role of the motor system in speech perception. Brain and Language, 164, 77-105. 787\nSlade, K., Plack, C. J., & Nuttall, H. E. (2020). The effects of age-related hearing loss on the brain and cognitive 788 function. Trends in Neurosciences, 43(10), 810-821. 789\nSouza, P. E., & Turner, C. W. (1994). Masking of speech in young and elderly listeners with hearing loss. 790 Journal of Speech, Language, and Hearing Research, 37(3), 655-661. 791\nStrand, F., Forssberg, H., Klingberg, T., & Norrelgen, F. (2008). Phonological working memory with auditory 792 presentation of pseudo-words\u2014an event related fMRI Study. Brain Research, 1212, 48-54. 793\nStropahl, M., Besser, J., & Launer, S. (2020). Auditory training supports auditory rehabilitation: a state-of-the-794 art review. Ear and Hearing, 41(4), 697-704. 795\nTak, S., Uga, M., Flandin, G., Dan, I., & Penny, W. D. (2016). Sensor space group analysis for fNIRS data. 796 Journal of Neuroscience Methods, 264, 103-112. 797\nVaden, K. I., Kuchinsky, S. E., Ahlstrom, J. B., Dubno, J. R., & Eckert, M. A. (2015). Cortical activity predicts 798 which older adults recognize speech in noise and when. Journal of Neuroscience, 35(9), 3929-3937. 799\nVaden, K. I., Kuchinsky, S. E., Ahlstrom, J. B., Teubner-Rhodes, S. E., Dubno, J. R., & Eckert, M. A. (2016). 800 Cingulo-opercular function during word recognition in noise for older adults with hearing loss. Experimental 801 Aging Research, 42(1), 67-82. 802\nVafaee, M. S., & Gjedde, A. (2000). Model of blood-brain transfer of oxygen explains nonlinear flow-803 metabolism coupling during stimulation of visual cortex. Journal of Cerebral Blood Flow & Metabolism, 20(4), 804 747-754. 805\nVogelzang, M., Thiel, C. M., Rosemann, S., Rieger, J. W., & Ruigendijk, E. (2021). Effects of age-related 806 hearing loss and hearing aid experience on sentence processing. Scientific Reports, 11(1), 1-14. 807\nWijayasiri, P., Hartley, D. E., & Wiggins, I. M. (2017). Brain activity underlying the recovery of meaning from 808 degraded speech: A functional near-infrared spectroscopy (fNIRS) study. Hearing Research, 351, 55-67. 809\nWiggins, I. M., Anderson, C. A., Kitterick, P. T., & Hartley, D. E. (2016). Speech-evoked activation in adult 810 temporal cortex measured using functional near-infrared spectroscopy (fNIRS): Are the measurements reliable?. 811 Hearing Research, 339, 142-154. 812\nWild, C. J., Yusuf, A., Wilson, D. E., Peelle, J. E., Davis, M. H., & Johnsrude, I. S. (2012). Effortful listening: 813 the processing of degraded speech depends critically on attention. Journal of Neuroscience, 32(40), 14010-814 14021. 815\nWong, P. C., Jin, J. X., Gunasekera, G. M., Abel, R., Lee, E. R., & Dhar, S. (2009). Aging and cortical 816 mechanisms of speech perception in noise. Neuropsychologia, 47(3), 693-703. 817\nWong, D., Miyamoto, R. T., Pisoni, D. B., Sehgal, M., & Hutchins, G. D. (1999). PET imaging of cochlear-818 implant and normal-hearing subjects listening to speech and non-speech. Hearing Research, 132(1-2), 34-42. 819\nYamada, T., Umeyama, S., & Matsuda, K. (2012). Separation of fNIRS signals into functional and systemic 820 components based on differences in hemodynamic modalities. PloS One, 7(11), e50271. 821\nYe, Z., Hammer, A., Camara, E., & M\u00fcnte, T. F. (2011). Pramipexole modulates the neural network of reward 822 anticipation. Human Brain Mapping, 32(5), 800-811. 823\nYorgancigil, E., Yildirim, F., Urgen, B. A., & Erdogan, S. B. (2022). An exploratory analysis of the neural 824 correlates of human-robot interactions with functional near infrared spectroscopy. Frontiers in Human 825 Neuroscience, 465. 826\nZatorre, R. J., Meyer, E., Gjedde, A., & Evans, A. C. (1996). PET studies of phonetic processing of speech: 827 review, replication, and reanalysis. Cerebral Cortex, 6(1), 21-30. 828\nZatorre, R. J. (2001). Neural specializations for tonal processing. Annals of the New York Academy of 829 Sciences, 930(1), 193-210. 830\nZhou, X., Burg, E., Kan, A., & Litovsky, R. Y. (2022). Investigating effortful speech perception using fNIRS 831 and pupillometry measures. Current Research in Neurobiology, 3, 100052. 832"
        }
    ],
    "title": "Neuroplasticity of speech-in-noise processing in older adults assessed by functional near-infrared spectroscopy (fNIRS)",
    "year": 2023
}