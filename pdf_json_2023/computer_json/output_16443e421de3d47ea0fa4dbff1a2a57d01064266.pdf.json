{
    "abstractText": "We examine how the choice of data-side attributes for two important visual tasks of image classification and object detection can aid in the choice or design of lightweight convolutional neural networks. We show by experimentation how four data attributes \u2013 number of classes, object color, image resolution, and object scale affect neural network model size and efficiency. Intraand inter-class similarity metrics, based on metric learning, are defined to guide the evaluation of these attributes toward achieving lightweight models. Evaluations made using these metrics are shown to require 30\u00d7 less computation than running full inference tests. We provide, as an example, applying the metrics and methods to choose a lightweight model for a robot path planning application and achieve computation reduction of 66% and accuracy gain of 3.5% over the pre-method model.",
    "authors": [
        {
            "affiliations": [],
            "name": "Bryan Bo Cao"
        },
        {
            "affiliations": [],
            "name": "Lawrence O\u2019Gorman"
        },
        {
            "affiliations": [],
            "name": "Michael Coss"
        },
        {
            "affiliations": [],
            "name": "Shubham Jain"
        }
    ],
    "id": "SP:bfcbdd1adc427212b1426c8666db27f023821295",
    "references": [
        {
            "authors": [
                "Olga Russakovsky",
                "Jia Deng",
                "Hao Su",
                "Jonathan Krause",
                "Sanjeev Satheesh",
                "Sean Ma",
                "Zhiheng Huang",
                "Andrej Karpathy",
                "Aditya Khosla",
                "Michael Bernstein",
                "Alexander C. Berg",
                "Li Fei-Fei"
            ],
            "title": "ImageNet Large Scale Visual Recognition Challenge",
            "venue": "International Journal of Computer Vision (IJCV),",
            "year": 2015
        },
        {
            "authors": [
                "Hung Nguyen",
                "Sarah J. Maclagan",
                "Tu Dinh Nguyen",
                "Thin Nguyen",
                "Paul Flemons",
                "Kylie Andrews",
                "Euan G. Ritchie",
                "Dinh Phung"
            ],
            "title": "Animal recognition and identification with deep convolutional neural networks for automated wildlife monitoring",
            "venue": "IEEE International Conference on Data Science and Advanced Analytics (DSAA),",
            "year": 2017
        },
        {
            "authors": [
                "Yingfeng Cai",
                "Tianyu Luan",
                "Hongbo Gao",
                "Hai Wang",
                "Long Chen",
                "Yicheng Li",
                "Miguel Angel Sotelo",
                "Zhixiong Li"
            ],
            "title": "Yolov4-5d: An effective and efficient object detector for autonomous driving",
            "venue": "IEEE Transactions on Instrumentation and Measurement,",
            "year": 2021
        },
        {
            "authors": [
                "Fisher Yu",
                "Haofeng Chen",
                "Xin Wang",
                "Wenqi Xian",
                "Yingying Chen",
                "Fangchen Liu",
                "Vashisht Madhavan",
                "Trevor Darrell"
            ],
            "title": "Bdd100k: A diverse driving dataset for heterogeneous multitask learning, 2018",
            "year": 2018
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Ilya Sutskever",
                "Geoffrey E Hinton"
            ],
            "title": "Imagenet classification with deep convolutional neural networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2012
        },
        {
            "authors": [
                "Karen Simonyan",
                "Andrew Zisserman"
            ],
            "title": "Very deep convolutional networks for large-scale image recognition",
            "venue": "arXiv preprint arXiv:1409.1556,",
            "year": 2014
        },
        {
            "authors": [
                "Soumya Sudhakar",
                "Vivienne Sze",
                "Sertac Karaman"
            ],
            "title": "Data centers on wheels: Emissions from computing onboard autonomous vehicles",
            "venue": "IEEE Micro,",
            "year": 2023
        },
        {
            "authors": [
                "Yu Cheng",
                "Duo Wang",
                "Pan Zhou",
                "Tao Zhang"
            ],
            "title": "Model compression and acceleration for deep neural networks: The principles, progress, and challenges",
            "venue": "IEEE Signal Processing Magazine,",
            "year": 2018
        },
        {
            "authors": [
                "Forrest N Iandola",
                "Song Han",
                "Matthew W Moskewicz",
                "Khalid Ashraf",
                "William J Dally",
                "Kurt Keutzer"
            ],
            "title": "Squeezenet: Alexnet-level accuracy with 50x fewer parameters and< 0.5 mb model size",
            "venue": "arXiv preprint arXiv:1602.07360,",
            "year": 2016
        },
        {
            "authors": [
                "Andrew G. Howard",
                "Menglong Zhu",
                "Bo Chen",
                "Dmitry Kalenichenko",
                "Weijun Wang",
                "Tobias Weyand",
                "Marco Andreetto",
                "Hartwig Adam"
            ],
            "title": "Mobilenets: Efficient convolutional neural networks for mobile vision applications",
            "year": 2017
        },
        {
            "authors": [
                "Philipp Gysel",
                "Jon Pimentel",
                "Mohammad Motamedi",
                "Soheil Ghiasi"
            ],
            "title": "Ristretto: A framework for empirical study of resource-efficient inference in convolutional neural networks",
            "venue": "IEEE transactions on neural networks and learning systems,",
            "year": 2018
        },
        {
            "authors": [
                "Song Han",
                "Huizi Mao",
                "William J Dally"
            ],
            "title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding",
            "venue": "arXiv preprint arXiv:1510.00149,",
            "year": 2015
        },
        {
            "authors": [
                "Cong Leng",
                "Zesheng Dou",
                "Hao Li",
                "Shenghuo Zhu",
                "Rong Jin"
            ],
            "title": "Extremely low bit neural network: Squeeze the last bit out with admm",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "Yu Cheng",
                "Duo Wang",
                "Pan Zhou",
                "Tao Zhang"
            ],
            "title": "A survey of model compression and acceleration for deep neural networks",
            "venue": "arXiv preprint arXiv:1710.09282,",
            "year": 2017
        },
        {
            "authors": [
                "Davis Blalock",
                "Jose Javier Gonzalez Ortiz",
                "Jonathan Frankle",
                "John Guttag"
            ],
            "title": "What is the state of neural network pruning",
            "venue": "Proceedings of machine learning and systems,",
            "year": 2020
        },
        {
            "authors": [
                "Hao Li",
                "Asim Kadav",
                "Igor Durdanovic",
                "Hanan Samet",
                "Hans Peter Graf"
            ],
            "title": "Pruning filters for efficient convnets",
            "venue": "arXiv preprint arXiv:1608.08710,",
            "year": 2016
        },
        {
            "authors": [
                "Maying Shen",
                "Hongxu Yin",
                "Pavlo Molchanov",
                "Lei Mao",
                "Jianna Liu",
                "Jose M Alvarez"
            ],
            "title": "Structural pruning via latency-saliency knapsack",
            "venue": "arXiv preprint arXiv:2210.06659,",
            "year": 2022
        },
        {
            "authors": [
                "Serena Yeung",
                "Olga Russakovsky",
                "Greg Mori",
                "Li Fei-Fei"
            ],
            "title": "End-to-end learning of action detection from frame glimpses in videos",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Michael Maire",
                "Serge Belongie",
                "James Hays",
                "Pietro Perona",
                "Deva Ramanan",
                "Piotr Doll\u00e1r",
                "C Lawrence Zitnick"
            ],
            "title": "Microsoft coco: Common objects in context",
            "venue": "In European conference on computer vision,",
            "year": 2014
        },
        {
            "authors": [
                "Mark Everingham",
                "Luc Van Gool",
                "Christopher KI Williams",
                "John Winn",
                "Andrew Zisserman"
            ],
            "title": "The pascal visual object classes (voc) challenge",
            "venue": "International journal of computer vision,",
            "year": 2009
        },
        {
            "authors": [
                "Joseph Redmon",
                "Santosh Divvala",
                "Ross Girshick",
                "Ali Farhadi"
            ],
            "title": "You only look once: Unified, real-time object detection",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Glenn Jocher et. al"
            ],
            "title": "ultralytics/yolov5: v6.0 - YOLOv5n \u2019Nano\u2019 models, Roboflow integration, TensorFlow export, OpenCV DNN support, oct 2021",
            "year": 2021
        },
        {
            "authors": [
                "Chien-Yao Wang",
                "Alexey Bochkovskiy",
                "Hong-Yuan Mark Liao"
            ],
            "title": "Yolov7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors",
            "venue": "arXiv preprint arXiv:2207.02696,",
            "year": 2022
        },
        {
            "authors": [
                "Daniel Kang",
                "John Emmons",
                "Firas Abuzaid",
                "Peter Bailis",
                "Matei Zaharia"
            ],
            "title": "Noscope: optimizing neural network queries over video at scale",
            "venue": "arXiv preprint arXiv:1703.02529,",
            "year": 2017
        },
        {
            "authors": [
                "M. Fayyaz"
            ],
            "title": "Deep-anomaly: Fully convolutional neural network for fast anomaly detection in crowded scenes",
            "venue": "Computer Vision and Image Understanding,",
            "year": 2018
        },
        {
            "authors": [
                "Shanwei Liu",
                "Weimin Kong",
                "Xingfeng Chen",
                "Mingming Xu",
                "Muhammad Yasir",
                "Limin Zhao",
                "Jiaguo Li"
            ],
            "title": "Multi-scale ship detection algorithm based on a lightweight neural network for spaceborne sar images",
            "venue": "Remote Sensing,",
            "year": 2022
        },
        {
            "authors": [],
            "title": "Koenigkan. A study on the detection of cattle in uav images using deep",
            "venue": "learning. Sensors,",
            "year": 2019
        },
        {
            "authors": [
                "Z. Han J. Huang W. Wang Y. Li",
                "H. Wei"
            ],
            "title": "Deep learning-based safety helmet detection in engineering management based on convolutional neural networks",
            "venue": "Advances in Civil Engineering,",
            "year": 2020
        },
        {
            "authors": [
                "X. Sun"
            ],
            "title": "Printed circuit boards defect detection method based on improved fully convolutional networks",
            "venue": "IEEE Access,",
            "year": 2022
        },
        {
            "authors": [
                "Zhenfeng Shao",
                "Linggang Wang",
                "Zhongyuan Wang",
                "Wan Du",
                "Wenjing Wu"
            ],
            "title": "Saliency-aware convolution neural network for ship detection in surveillance video",
            "venue": "IEEE Transactions on Circuits and Systems for Video Technology,",
            "year": 2020
        },
        {
            "authors": [
                "S. Foucher"
            ],
            "title": "Multispecies detection and identification of african mammals in aerial imagery using convolutional neural networks",
            "venue": "Remote Sensing in Ecology and Conservation,",
            "year": 2022
        },
        {
            "authors": [
                "Gregory Koch",
                "Richard Zemel",
                "Ruslan Salakhutdinov"
            ],
            "title": "Siamese neural networks for one-shot image recognition",
            "venue": "In ICML deep learning workshop,",
            "year": 2015
        },
        {
            "authors": [
                "Mat Kelcey"
            ],
            "title": "Metric learning for image similarity search (accessed: 2022/10/02)",
            "year": 2022
        },
        {
            "authors": [
                "Andreas Veit",
                "Serge Belongie",
                "Theofanis Karaletsos"
            ],
            "title": "Conditional similarity networks",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Mariya I Vasileva",
                "Bryan A Plummer",
                "Krishna Dusad",
                "Shreya Rajpal",
                "Ranjitha Kumar",
                "David Forsyth"
            ],
            "title": "Learning type-aware embeddings for fashion compatibility",
            "venue": "In Proceedings of the European conference on computer vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Reuben Tan",
                "Mariya I Vasileva",
                "Kate Saenko",
                "Bryan A Plummer"
            ],
            "title": "Learning similarity conditions without explicit supervision",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2019
        },
        {
            "authors": [
                "Andrew Zhai",
                "Hao-Yu Wu"
            ],
            "title": "Classification is a strong baseline for deep metric learning",
            "venue": "arXiv preprint arXiv:1811.12649,",
            "year": 2018
        },
        {
            "authors": [
                "Raia Hadsell",
                "Sumit Chopra",
                "Yann LeCun"
            ],
            "title": "Dimensionality reduction by learning an invariant mapping",
            "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR\u201906),",
            "year": 2006
        },
        {
            "authors": [
                "Konstantin Kobs",
                "Michael Steininger",
                "Andrzej Dulny",
                "Andreas Hotho"
            ],
            "title": "Do different deep metric learning losses lead to similar learned features",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Samarth Mishra",
                "Zhongping Zhang",
                "Yuan Shen",
                "Ranjitha Kumar",
                "Venkatesh Saligrama",
                "Bryan A Plummer"
            ],
            "title": "Effectively leveraging attributes for visual similarity",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Geoffrey Hinton"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "year": 2009
        },
        {
            "authors": [
                "Mark Sandler",
                "Andrew Howard",
                "Menglong Zhu",
                "Andrey Zhmoginov",
                "Liang-Chieh Chen"
            ],
            "title": "Mobilenetv2: Inverted residuals and linear bottlenecks",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Nermin Samet",
                "Samet Hicsonmez",
                "Emre Akbas"
            ],
            "title": "Houghnet: Integrating near and long-range evidence for bottom-up object detection",
            "venue": "In European Conference on Computer Vision (ECCV),",
            "year": 2020
        },
        {
            "authors": [
                "Hieu Minh Bui",
                "Margaret Lech",
                "Eva Cheng",
                "Katrina Neville",
                "Ian S. Burnett"
            ],
            "title": "Using grayscale images for object recognition with convolutional-recursive neural network",
            "venue": "In 2016 IEEE Sixth International Conference on Communications and Electronics (ICCE),",
            "year": 2016
        },
        {
            "authors": [
                "Yue Meng",
                "Chung-Ching Lin",
                "Rameswar Panda",
                "Prasanna Sattigeri",
                "Leonid Karlinsky",
                "Aude Oliva",
                "Kate Saenko",
                "Rogerio Feris"
            ],
            "title": "Ar-net: Adaptive frame resolution for efficient action recognition",
            "venue": "In European Conference on Computer Vision,",
            "year": 2020
        },
        {
            "authors": [
                "Le Yang",
                "Yizeng Han",
                "Xi Chen",
                "Shiji Song",
                "Jifeng Dai",
                "Gao Huang"
            ],
            "title": "Resolution adaptive networks for efficient inference",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Xueyang Wang",
                "Xiya Zhang",
                "Yinheng Zhu",
                "Yuchen Guo",
                "Xiaoyun Yuan",
                "Liuyu Xiang",
                "Zerun Wang",
                "Guiguang Ding",
                "David Brady",
                "Qionghai Dai"
            ],
            "title": "Panda: A gigapixel-level human-centric video dataset",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Lawrence O'Gorman"
            ],
            "title": "Activity analytics from fixed cameras for robot path planning",
            "venue": "TechRxiv, Jun 2022",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "Keywords Efficient Neural Network \u00b7 Convolutional Neural Network \u00b7 Image Classification \u00b7 Object Detection"
        },
        {
            "heading": "1 Introduction",
            "text": "Traditionally for computer vision applications, an algorithm designer with domain expertise would begin by identifying handcrafted features to help recognize objects of interest. More recently, end-to-end learning (E2E) has supplanted that expert by training a deep neural network to learn important features on its own. Besides the little forethought required about data features, there is usually only basic preprocessing done on the input data; an image is often downsampled and converted to a vector, and an audio signal is often transformed to a spectogram. In this paper, we use the term \u201cdata-side\u201d to include operations that are performed on the data before input to the neural network. Our proposal is that a one-time analysis of data-side attributes can aid the design of more efficient convolutional neural networks (CNNs) for the many-times that they are used to perform inferences.\nOn the data side of the neural network, we examine four independent image attributes and two dependent attributes, the latter which we use as metrics. The independent attributes are number of classes, object color, image resolution and object scale. The metrics are intra- and inter-class similarity. Our goal is to optimize the metrics by choice of the independent variables \u2013 specifically to maximize intra-class similarity and minimize inter-class similarity \u2013 to obtain the most computationally efficient model.\nUnlike benchmark competitions such as ImageNet [1], practical applications involve a design stage that can include adjustment of input specifications. In Section 2, we tabulate a selection of applications. The \u201cwildlife\u201d application reduced the number of animal and bird classes from 18 to 6 in the Wildlife Spotter dataset [2]. In the \u201cdriving\u201d application [3], the 10 classes of the BDD dataset [4] were reduced to 7 by eliminating the \u201ctrain\u201d class due to few labeled instances and combining the similar classes of rider, motor, and bike into rider.\nThe main contributions of this paper are:\n1. Four data-side attributes are identified, and experiments are run to show their effects on the computational efficiency of lightweight CNNs.\n\u2217Work done during an internship at Nokia Bell Labs.\nar X\niv :2\n30 8.\n13 05\n7v 1\n[ cs\n.C V\n] 2\n4 A\n2. Intra- and inter-class similarity metrics are defined to aid evaluation of the four independent attribute values. Use of these metrics is shown to be about 30\u00d7 faster than evaluation by full inference testing. 3. Procedures are described using the similarity metrics to evaluate how changing attribute values can reduce model computation while maintaining accuracy. 4. Starting with the EfficientNet-B0 model, we show how our methods can guide the application designer to smaller \u201csub-EfficientNets\u201d with greater efficiency and similar or higher accuracy.\nWe describe related work in Section 2. Each of the attributes is defined in Section 3, procedures are described to apply these toward more efficient models in Section 4, and experimental evidence is shown in Section 5. We conclude in Section 6."
        },
        {
            "heading": "2 Related Work",
            "text": "The post-AlexNet [5] era (2012-) of convolutional neural networks brought larger and larger networks with the understanding that a larger model yielded higher accuracy (e.g., VGGNet-16 [6] in 2014 with 144M parameters). But the need for more efficiency, especially for embedded systems [7], encouraged the design of lightweight neural networks [8], such as SqueezeNet [9] in 2017 with 1.2M parameters. Models were reduced in size by such architectures as depthwise separable convolution filters [10]. More efficient handling of data was incorporated by using quantization [11\u201313], pruning [14\u201317], and data saliency [18]. This model-side efficiency quest is a common research trend where new models are evaluated for general purpose classification and object detection on public benchmarks such as ImageNet [1], COCO [19], and VOC [20]. Orthogonal and complementary to these model-side efficiencies [21\u201323], we examine efficiencies that can be gained before the model by understanding and adjusting the data attributes within the confines of the application specifications. Early work [24] optimizes models specialized to the target video only for binary-classification. Our work extends to multi-class classification and object detection.\nIn Table 1, we list a selection of 9 applications whose data attributes are far less complex than common benchmarks. For these applications, class number is often just 2. The largest number of classes, 7, is for the \u201cdriving\u201d [4] application. Compare these with 80 classes for the COCO dataset and 1000 for ImageNet. For 2 applications, color is not used. For the \u201ccrowd\u201d application, it is not deemed useful and for the \u201cship, SAR\u201d application, the input data is inherently not color. The resolution range is not broad in this sampling, likely due to matching image size to model input width. Many papers did not describe the scale range; for these, we approximated from the given information or images in the paper. The broadest scale range (as a fraction of image size) is the \u201cdriving\u201d application (1/32 to 1/2), and the narrowest is for the \u201cmammals\u201d application, using aerial image capture, with scale from 1/30 to 1/20.\nWe use a measure of class similarity to efficiently examine data attributes, based on neural network metric learning. This term describes the use of learned, low-dimensional representations of discrete variables (images in our case). The distance between two instances in the latent space can be measured by L1 [32], L2, or cosine similarity [33]. Previous studies [34\u201336] focus on learning a single similarity latent space. Differences between classification [37] and ranking based losses [38] have been studied in [39]. PAN incorporates attributes to visual similarity learning [40]. In Sections 3.6 and 3.7 we extend this line of research by adapting the metric from [33] to measure intra- and inter-class similarity to serve efficiency purposes."
        },
        {
            "heading": "In contrast to research to improve model performance on public benchmarks, our goal is to develop an empirical understanding of the effects of these attributes on common CNNs, and from this to provide practical guidelines to",
            "text": "obtain lightweight CNNs in real-world scenarios. Our use of intra- and inter-class similarity metrics enables an efficient\nmethodology toward this goal. Practical, low-complexity applications as in Table 1 can benefit from our investigation and method."
        },
        {
            "heading": "3 Data Attributes and Metrics",
            "text": "This work is centered on the hypothesis that, the easier the input data is to classify, the more computationally efficient the model can be at a fixed or chosen lower accuracy threshold. In this section, we describe each of the data attributes and their relationships to the hypothesis. We define metrics to obtain the dependent variables. And we describe procedures to adjust the independent variables to obtain metrics that guide the design of an efficient model.\nWe first introduce the term, Ease of Classification EoC and hypothesize the following relationship exists with the data-side attributes,\nEoC\u2190 (S1, 1 S2 )\u2190 ( 1 NCl , 1 NCo , 1 RE , 1 SC ). (1)\nThe symbol (\u2190) is used to describe the direct relationships in which the left expressions are related to the right. EoC increases with intra-class similarity S1 and decreases with inter-class similarity S2. The dependent variable S1 is related to the reciprocals of the independent variables, number of classes NCl, number of color channels NCo, image resolution RE , and object scale SC . The dependent variable S2 is directly related to these independent variables. The model designer follows an approach to adjust these independent variables to obtain similarity measurements that achieve high EoC. Note that we will sometimes simplify NCl to CL for readability in figures.\nIn Section 5 we perform experiments on a range of values for each attribute to understand how they affect model size and accuracy. However, these experiments cannot be done independently for each attribute because some have dependencies on others. We call these interdependencies because they are 2-way. We discuss interdependencies below for two groups, {SC , RE} and {S1, S2, NCl}.\n3.1 Number of Classes, NCl\nThe NCl attribute is the number of classes being classified in the dataset. Experimental results with different numbers of classes are shown in Section 5.2. In Section 5.6 we present results of changing the number of classes for a robot path planning application.\n3.2 Object Colors, NCo\nThe NCo attribute is the number of color axes, either 1 for grayscale or 3 for tristimulus color models such as RGB (red, green, blue). When the data has a color format, the first layer of the neural model has 3 channels. For grayscale data input, the model has 1 input channel. In Section 5.3, we show the efficiency gain for 1 versus 3 input channels."
        },
        {
            "heading": "3.3 Image Resolution, RE",
            "text": "Image resolution, measured in pixels, has the most direct relationship to model size and computation. Increasing the image size by a multiple m (rows Ir to m \u00d7 Ir/2 and columns Ic to m \u00d7 Ic) increases the computation by at least one-to-one. In Figure 1, MobileNet computation increases proportionally to image size. The lower sized EfficientNet-B0 and -B1 models also increase proportionally, but rise faster with larger models B2, B3, and B4.\nIt is not the case that higher resolution always yields better accuracy. It usually plateaus, and that plateau is different for different classes involved. The objective is to choose the minimum image resolution before which accuracy drops unacceptably. Note that resolution and scale are dependent attributes, and neither can be adjusted without considering the effect on the other. Experimental results with different resolutions are shown in Section 5.5."
        },
        {
            "heading": "3.4 Object Scale, SC",
            "text": "For a CNN, the image area, or receptive field, of a 3 \u00d7 3 filter kernel expands as the following sequence for layers L = 1, 2, 3, ..., L,\n3\u00d7 3, 7\u00d7 7, 15\u00d7 15, . . . , (2L+1 \u2212 1)\u00d7 (2L+1 \u2212 1). (2)\nFor object detection, this means that, if the maximum object size in an image is, for example, 15\u00d7 15, the network needs at least 3 layers to yield features that convolve with (or overlap) the full size of these objects. In practice, from the\nsequence in equation 2 the minimum number of layers can be found from the maximum sidelength bmax of bounding boxes of objects over all image instances,\nL \u2265 \u2308log2(bmax + 1)\u2309 \u2212 1. (3)\nwhere \u2308x\u2309 is a ceiling operator, which rounds the x value to the higher integer. For example, if bmax is 250, the minimum number of layers needed is \u23087.966\u2309 \u2212 1 = 7. In practice, the maximum object size is approximated as the longer sidelength of the bounding box of the object. In terms of model size and minimizing computation, by measuring the bounding box size of objects in a dataset, one can discover the maximum number of layers needed for full-object feature extraction. Furthermore, using filter kernels that are larger than the maximum object size increases extractions of inter-object features. When generalized to a sequence of rectangular filters, these tend toward Gaussian filters with increasing convolution layers, so the filter edge versus middle magnitude lowers as well.\nTo decouple scale from resolution, we define the scale of an object as a fraction of the longer image dimension. With this definition, one can measure the scale of an object in an image without knowing the resolution. The size of the object is measured in pixels; it is the product of scale times the longer dimension of the resolution. Figure 2 shows the range of scales for objects in the COCO datase. Although 50% of instances account for \u2264 10% of image size, still the sizes range from very small to full image size. Conversely, in many applications, scale range is known and much more contained as for some applications in Table 1. Experimental results with different scales are shown in Section 5.5."
        },
        {
            "heading": "3.5 Resolution and Scale Interdependency",
            "text": "The interdependence between the attributes {SC , RE } is common to any image processing application, and can be described by two cases. For case 1, if the scale of objects within a class in an image is large enough to reduce the image size and still recognize the objects with high accuracy, then there is a resulting benefit of lower computation. However, reducing the image size also reduces resolution, and if there is another class that depends upon a high spatial frequency\nfeature such as texture, then reducing the image would reduce overall accuracy. For case 2, if resolution of the highest frequency features of a class is more than adequate to support image reduction, then computational efficiency can be gained by reducing the image size. However, because that reduction also reduces scale, classes that are differentiated by scale will become more similar, and this situation may cause lower accuracy. We can write these relationships as follows,\n(SC \u221d RE) \u2192 1\nEoC , (4)\nwhere the expression within parentheses is commutative (a change in SC will cause a proportional change in RE , and vice versa), and these are inversely related to EoC."
        },
        {
            "heading": "3.6 Intra-Class Similarity, S1",
            "text": "Intra-class similarity is a measure of visual similarity between members of the same class as measured with vectors in the embedding space of a deep neural network. It is described by the average and variance of pairwise similarities of instances in the class,\nS1(C1) = 1\nN \u2211 i,j\u2208C1 cos(ZiZj), (5)\n\u03c32S1(C1) = 1\nN \u2211 i,j\u2208C1 (Sij \u2212 S1)2, (6)\nwhere C1 is a class containing a set of instances, i and j are indices in set C1, i \u0338= j, and N is the total number of similarity pairs Sij of two different instances in the class. Z is the latent vector in the embedding space from a neural network trained by metric learning on instances of the class as well as other classes. (That is, it is the same model trained on the same instances as for inter-class similarity.) This metric is adapted from [33]. We show the use of S1 and \u03c32S1 in Section 4.1."
        },
        {
            "heading": "3.7 Inter-Class Similarity, S2",
            "text": "Inter-class similarity is a measure of visual similarity between classes as determined by the closeness of instances of two classes in the embedding space of a deep neural network. For 2 classes, it is defined as the average of pairwise similarities of instances between classes,\nS2(C1, C2) = 1\nN \u2211 i\u2208C1,j\u2208C2 cos(ZiZj), (7)\nwhere C1 and C2 are instance sets of two different classes, i and j are indices in sets C1 and C2 respectively, N is the total number of pairs of two instances in two different classes, and Z is the latent vector in the embedding space from a neural network trained by metric learning on instances that include both classes as well as other classes if there are more than 2.\nFor an application involving more than two classes, we choose the inter-class similarity measure to be the maximum of inter-class similarity measures over all class pairs,\nS\u03022({CK}) = max{S2(Cm, Cn)}, (8)\nwhere {CK} is the set of all classes for 0 <= k < K, and {(Cm, Cn)} is all class pairs for 0 <= m,n < K, m \u0338= n. We choose the maximum of similarity results of class pairs because maximum similarity represents the worst case \u2013 most difficult to distinguish \u2013 between two classes. Alternatively, one could calculate the average of inter-class similarities for each pair, however the operation of averaging will hide the effect of one pair of very similar classes among other dissimilar classes, and this effect is worse for larger NCl. We show the use of S\u03022 in Section 4.2.\nWe also use a measure that is the normalized difference between the maximum and the average,\n\u2206S2({CK}) = S\u03022 \u2212 S2\nS2 . (9)\nA larger \u2206S2 indicates a higher value of worst case S\u03022, so we seek low \u2206S2 in the methods described in Sections 4.1 and 4.3."
        },
        {
            "heading": "3.8 Intra- and Inter-Class Interdependency",
            "text": "There is a strong interdependence between S1 and S2 with a secondary dependence upon NCl, as we will describe. With other factors fixed, smaller intra-class similarity increases inter-class similarity because more heterogeneous classes with wider attribute ranges are being compared, thus reducing EoC. As NCl is increased, this effect is exacerbated because there is higher probability of low S1 and high S2 pairs, and (similarly) because we use worst-case maximum in equation 8. We can write these dependent relationships as follows,\n(S1 \u221d 1/S2) \u2192 EoC , forNCl \u2265 2, (10)\nwhere the expression within brackets is commutative (either S1 or S2 can cause inverse change in the other, and the relationship becomes stronger as NCl increases."
        },
        {
            "heading": "4 Method",
            "text": "The general approach toward finding the most efficient model is to select ranges of the independent attribute values {NCl, NCo, RE , SC}, calculate the similarity measures {S1, S2} for selected values in the range, and choose those independent attribute values that minimize S2 and maximize S1. Step-by-step procedures for selecting each of the independent attributes are described below. These procedures should be followed in the order of the sections below.\n4.1 Selection of NCl\nIf the application permits adjustment of the number of classes, then the following procedure can help finding class groupings and an associated NCl that supports a more efficient model. Initialize \u2206S2 =\u221e, and follow these steps,\ni Choose class groupings. ii Calculate \u2206S2 from equation 9. If this is less than the current smallest \u2206S2 then this grouping is the most\nefficient so far. iii If \u2206S2 is low, one can choose to exit, or continue. iv If one decides to continue, calculate S1 from equation 5 and \u03c32S1 from equation 6 for each class. The value of\nS1 is for guidance to help understand why the current S2 is good or bad (low or high) to give guidance on choosing the next groupings. In general, class grouping with high S1 and low \u03c32S1 will yield higher accuracy. Repeat these steps.\nWe want to stop when inter-class similarity is low, indicated by a low value of \u2206S2. However there is subjectivity in this step due to the manual choice of groupings and because different applications will have different levels of intra-class homogeneity and inter-class distinguishability. In practice, the procedure is usually run for a few iterations to understand relative \u2206S2 values for different groupings, and the lowest is chosen.\n4.2 Selection of NCo\nThe application can gain computational advantage if all classes can be reduced to grayscale; if all classes cannot be reduced, then the model must handle color and there is no computational advantage. Following are the steps to choose grayscale or color,\ni For all classes in color and grayscale, calculate S\u03022. ii If S\u03022 for grayscale is less than or equal to S\u03022 for color, choose a grayscale model and grayscale for all instances\nof all classes.\nIf the procedure above does not result in the choice of grayscale, there is a second option. This requires more testing and does not directly yield computation reduction, but may improve accuracy.\ni For each class, calculate S\u03022 against every other class for these four combinations of the (C1, C2) pairs: (grayscale, grayscale), (grayscale, color), (color, grayscale), and (color, color).\nii For each class C1 whose S\u03022 for (grayscale, grayscale) and (grayscale, color) is smaller than for (color, grayscale) and (color, color), choose to use grayscale for the C1 class."
        },
        {
            "heading": "4.3 Selection of RE and SC",
            "text": "We first adjust the attribute RE to find the lower bound of resolution with respect to acceptable accuracy. Initialize \u2206S2 =\u221e, and follow these steps,\ni Calculate \u2206S2 from equation 9. If this is less than the current smallest \u2206S2 then this resolution is the most efficient so far.\nii If \u2206S2 is low, one can choose to exit, or continue. iii Reduce the resolution by half and repeat these steps.\nIn practice, a few iterations is run to see when \u2206S2 rises, then choose the resolution where it is lowest.\nAfter the resolution has been chosen, maximum object scale is multiplied by resolution to find the maximum object size in pixels. Equation 3 is used to find an estimate of the lower bound of number of layers needed in the model."
        },
        {
            "heading": "5 Experiments",
            "text": "In this section, we perform experiments on the data attributes to show how their values relate to model efficiency and accuracy. Note that our level of experimentation is not even across all attributes. We believe that the experiments upon number of classes, intra- and inter-class similarities, and resolution are sufficient to support the Ease of Classification relationship of equation 1. For color, we give a quantitative comparison of color versus grayscale computation, and then because the difference is small, leave further investigation of accuracy and efficiency to cited literature. For scale, one aspect of this attribute is covered by its interdependency in the resolution experiments. However another aspect, the relationship of scale to model levels (equation 2), we leave to future work. This is because scale is less easily separated from particular applications \u2013 specifically the size range of objects in an application \u2013 than for other attributes. Our plan is to investigate this in a more application-oriented paper in the future."
        },
        {
            "heading": "5.1 Similarity Metric Efficiency",
            "text": "We could discover the effects of data attributes by training and inference testing all combinations of attribute values. For NCL classes, binary classification of pairs would require ( NCL 2 ) training and inference operations. In comparison, for similarity metrics, we need to train once for all combinations of binary classifications. During testing, the similarity model (SM) caches the latent space, thus only indices of each instance need to be paired with other instances to obtain their cosine similarities.\nFor the CIFAR10 dataset for example, there is a one-time task of feeding all test images into SM, which takes 0.72 seconds, and then caching, which takes 0.63 seconds. In contrast, we feed each image into the CNN to obtain its prediction in order to calculate its accuracy in the conventional pipeline. We show the runtime in Table 2 .\n5.2 Number of Classes, NCl\nIt is well known that accuracy is reduced when more classes are involved since more visual features are needed for a CNN to learn in the feature extractor backbone, as well as more complex decision boundary to learn in the fully connected layers for classification. However, we perform experiments here, first to confirm this relationship with test data, but secondly to gain any extra insight into the relationship between number of classes and accuracy.\nWe performed three sets of experiments. The first was for object detection using the YOLOv5-nano [22] backbone upon increasing-size class groupings of the COCO dataset [19]. Ten groups with Cl of {1, 2, 3, 4, 5, 10, 20, 40, 60, 80} were prepared. For each group, we trained a separate YOLOv5-nano model from scratch. As seen in Figure 3 (left), accuracy decreases with number of classes. An added insight is that the accuracy decrease is steep for very few classes, say 5-10 or fewer, and flattens beyond 10.\nThe second set of experiments was for image classification on the CIFAR-10 dataset. With many fewer classes in CIFAR-10 [41] than COCO (10 versus 80), we expect to see how the number of classes and accuracy relate for this smaller range. We extracted subsets of classes \u2014 which we call groups \u2013 from CIFAR-10 with NCl ranging from 2 to 9.\nFor example, a group with NCl = 4 might contain airplane, cat, automobile, and ship classes. We trained classifiers from scratch for each group.\nResults of the image classification experiments are shown in Figure 3 (middle). The three classifiers used for testing, EfficientNet-B0, VGG19 [6], and MobileNetV2 [42] showed the expected trend of accuracy reduction as NCl per group increased. However, the trend was not as monotonic as might be expected. We hypothesized that this might be due to the composition of each group, specifically if the group classes were largely similar or dissimilar. This insight led to the experiments on class similarity in the next section.\nThe third set of experiments involved reducing model size for different numbers of classes. We prepared 90 class groupings extracted from the COCO minitrain dataset [43]. There are 80 datasets for NCl = 1, each containing a single class from 80 classes. There are 8 datasets for NCl = 10 combining the 8 single-class datasets. The final dataset is the original COCO minitrain with NCl = 80.\nWe scale YOLOv5 layers and channels with the depth and width multiples already used for scaling the family between nano and x-large. Starting with depth and width multiples of 0.33 and 0.25 for YOLOv5-nano, we reduce these in step sizes of 0.04 for depth and 0.03 for width. In this way, we design a monotonically decreasing sequence of sub-YOLO models denoted as SY1 to SY8. We train each model separately for each of the six datasets.\nResults of sub-YOLO detection are shown in Figure 3 (right). There are three lines where each point of mAP@.5 is averaged across all models in all datasets for a specific NCl. An overall trend is observed that fewer-class models (upper-left blue star) achieve higher efficiency than many-class models. Another finding of interest here is that, whereas the accuracies for 80 classes drops steadily from the YOLOv5-nano size, accuracy for 10 classes is fairly flat down to SY2, which corresponds to a 36% computation reduction, and for 1 class down to SY4, which corresponds to a 72% computation reduction.\n5.3 Color, NCo\nBecause reduction from color to grayscale only affects the number of multiplies in the first layer, the efficiency gain depends upon the model size. For a large model such as VGG-19, percentage efficiency gain will be much smaller than for a small model such as EfficientNet-B0, as shown in Table 3. However, even for small networks, the effect of reducing from color to grayscale processing is small relative to effects of other attributes, so we perform experiments on these more impactful attributes. For further investigation of color computation, refer to previous work on this topic [44].\n5.4 Intra- and Inter-Class Similarity, S1, S2\nIn equation 1, we hypothesized that accuracy is lower if inter-class similarity is higher. Table 4 shows accuracy and inter-class similarity results for groups of 2 and 4 classes from the CIFAR-10 dataset that we have subjectively designated similar (S) or dissimilar (D). \u201cS4\u201d indicates a similar group of 4 classes, and \u201cD2\u201d indicates a dissimilar group of 2 classes. The results indicate that our subjective labels correspond to our hypothesis, and that our objective measure of inter-class similarity in equation 7 both corresponds to the subjective, and is consistent with the hypothesis.\nTo further support the objectivity of our similarity measures, we compare similarity scores to accuracies for all pairwise classifications in CIFAR-10. The Pearson correlation coefficient between these two matrices is -0.77, showing strong correlation; it is negative because high accuracy corresponds with low similarity. All (similarity, accuracy) pairs are plotted on Figure 4 (right) clearly indicating the inverse relationship between similarity and accuracy. Notably, the lowest similarity data point is for the (automobile, deer) pair, and the highest similarity data point is for the (cat, dog) pair.\nIntra-class similarity scores are shown for the same CIFAR-10 classes as in Table 5. All classes show high similarity scores with some differences. One can subjectively justify these numbers by visual examination. Classes 6 and 7 (dog and frog) have the highest similarity scores because foreground and background are similar for most images. Classes 0 and 8 (airplane and ship) have lower similarity scores because there is more diversity in foreground object and background."
        },
        {
            "heading": "5.5 Resolution and Scale, RE and SC",
            "text": "In Table 1, the applications often downsize images to much smaller than the originals. Designers of these applications likely determined that accuracy didn\u2019t suffer when images were reduced to the degree that they chose. This is also the case for our experiment of YOLOv5 nano on the COCO dataset in Figure 5 (left). One can see that accuracies drop slowly for image reduction from 6402 to 4482 \u2013 and further depending upon the degree of accuracy reduction that can be tolerated. Because scale drops with resolution, this plot includes the effects on accuracy reduction for both. In Figure 5 (right) we see the object detection performance almost flattens when reducing RE from 4k down to 1080p, and SY1-3 models\u2019 accuracies are very close to nano but with about half the required computation (4.2 GFLOPs for nano versus 2.2 GFLOPs for SY3.\nWe also verify by experiment that RE has a direct impact on inference runtime per image [45, 46]. Experiments on YOLOv5 nano in the COCO validation set are conducted, and results are shown in Table 6. One observation is that runtime almost doubles from 7.4ms for 12802 to 13.4ms for 25602 on a GPU, while it rises dramatically from 64.1 for 6402 to 4396.7 for 12802 on a CPU."
        },
        {
            "heading": "5.6 Robot Path Planning Application",
            "text": "We briefly mention results of methods from this paper used for efficient model design for our own application. The application is to detect times, locations, and densities of human activity on a factory floor for robot path planning. Initial labelling comprised 5 object classes: 2 of humans engaged in different activities, and 3 of different types of robots. Similarity metrics guided a reduction to 2 classes enabling a smaller model with computation reduction of 66% and accuracy gain of 3.5%. See [48] for a more complete description of this application."
        },
        {
            "heading": "6 Conclusion",
            "text": "We conclude that for applications requiring lightweight CNNs, data attributes can be examined and adjusted to obtain more efficient models. We examined four independent data-side variables, and results from our experiments indicate the following ranking upon computation reduction. Resolution has the greatest effect on computation. Most practitioners already perform resolution reduction, but many simply to fit the model of choice. We show that, for small (few-class) applications the model size can be reduced (to sub-YOLO models) to achieve more efficiency with similar accuracy, and this can be done efficiently using similarity metrics. Number of classes is second in rank. This is dependent on the application, but our methods using similarity metrics enable the application designer to compare different class groupings efficiently. We showed that the choice of color or grayscale had a relatively small (1-2%) effect on computation for small models. We don\u2019t rank scale because scale was only tested with interdependency to resolution."
        }
    ],
    "title": "DATA-SIDE EFFICIENCIES FOR LIGHTWEIGHT CONVOLUTIONAL NEURAL NETWORKS",
    "year": 2023
}