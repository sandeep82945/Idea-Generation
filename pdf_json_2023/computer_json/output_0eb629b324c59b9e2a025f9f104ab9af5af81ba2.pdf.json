{
    "abstractText": "We seek to extract a small number of representative scenarios from large and highdimensional panel data that are consistent with sample moments. Among two novel algorithms, the first identifies scenarios that have not been observed before, and comes with a scenario-based representation of covariance matrices. The second proposal picks important data points from states of the world that have already realized, and are consistent with higher-order sample moment information. Both algorithms are efficient to compute, and lend themselves to consistent scenario-based modeling and high-dimensional numerical integration. Extensive numerical benchmarking studies and an application in portfolio optimization favor the proposed algorithms.",
    "authors": [
        {
            "affiliations": [],
            "name": "MICHAEL MULTERER"
        }
    ],
    "id": "SP:9b314d5d35eb82989a7cca6a7a6e58625a5dbf2c",
    "references": [
        {
            "authors": [
                "Christian Bayer",
                "Josef Teichmann"
            ],
            "title": "The proof of Tchakaloff\u2019s theorem",
            "venue": "Proceedings of the American Mathematical Society,",
            "year": 2006
        },
        {
            "authors": [
                "Amir Beck"
            ],
            "title": "First-Order Methods in Optimization. Society for Industrial and Applied Mathematics, Philadelphia, PA, 2017",
            "year": 2017
        },
        {
            "authors": [
                "Claudia Bittante",
                "Stefano De Marchi",
                "Giacomo Elefante"
            ],
            "title": "A new quasi-Monte Carlo technique based on nonnegative least squares and approximate Fekete points",
            "venue": "Numerical Mathematics: Theory, Methods and Applications,",
            "year": 2016
        },
        {
            "authors": [
                "Len Bos",
                "Stefano De Marchi",
                "Alvise Sommariva",
                "Marco Vianello"
            ],
            "title": "Computing multivariate Fekete and Leja points by numerical linear algebra",
            "venue": "SIAM Journal on Numerical Analysis,",
            "year": 1984
        },
        {
            "authors": [
                "Len Bos",
                "Stefano De Marchi",
                "Alvise Sommariva",
                "Marco Vianello"
            ],
            "title": "Weakly Admissible Meshes and Discrete Extremal Sets",
            "venue": "Numerical Mathematics: Theory, Methods and Applications,",
            "year": 2011
        },
        {
            "authors": [
                "Jean-Luc Bouchot",
                "Simon Foucart",
                "Pawel Hitczenko"
            ],
            "title": "Hard thresholding pursuit algorithms: Number of iterations",
            "venue": "Applied and Computational Harmonic Analysis,",
            "year": 2016
        },
        {
            "authors": [
                "Stephen Boyd",
                "Neal Parikh",
                "Eric Chu",
                "Borja Peleato",
                "Jonathan Eckstein"
            ],
            "title": "Distributed optimization and statistical learning via the alternating direction method of multipliers",
            "venue": "Foundations and Trends in Machine Learning, 3:1\u2013122,",
            "year": 2011
        },
        {
            "authors": [
                "Ra\u00fal Curto",
                "Lawrence Fialko"
            ],
            "title": "Recursiveness, positivity, and truncated moment problems",
            "venue": "Houston Journal of Mathematics,",
            "year": 1991
        },
        {
            "authors": [
                "David L. Donoho",
                "Jared Tanner"
            ],
            "title": "Sparse nonnegative solution of underdetermined linear equations by linear programming",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2005
        },
        {
            "authors": [
                "Robert Engle",
                "Guillaume Roussellet",
                "Emil"
            ],
            "title": "Siriwardane. Scenario generation for long run interest rate risk assessment",
            "venue": "Journal of Econometrics,",
            "year": 2017
        },
        {
            "authors": [
                "H. Engler"
            ],
            "title": "The behavior of the QR-factorization algorithm with column pivoting",
            "venue": "Applied Mathematics Letters,",
            "year": 1997
        },
        {
            "authors": [
                "S. Foucart",
                "H. Rauhut"
            ],
            "title": "A Mathematical Introduction to Compressive Sensing",
            "venue": "Applied and Numerical Harmonic Analysis. Springer New York,",
            "year": 2013
        },
        {
            "authors": [
                "Simon Foucart"
            ],
            "title": "Hard thresholding pursuit: An algorithm for compressive sensing",
            "venue": "SIAM Journal on Numerical Analysis,",
            "year": 2011
        },
        {
            "authors": [
                "Gene H. Golub",
                "Charles F. Van Loan"
            ],
            "title": "Matrix Computations",
            "venue": "(Cited on pages",
            "year": 2013
        },
        {
            "authors": [
                "H. Harbrecht",
                "M. Peters",
                "R. Schneider"
            ],
            "title": "On the low-rank approximation by the pivoted Cholesky decomposition",
            "venue": "Applied Numerical Mathematics,",
            "year": 2012
        },
        {
            "authors": [
                "J. William Helton",
                "Jiawang Nie"
            ],
            "title": "A semidefinite approach for truncated k-moment problems",
            "venue": "Foundations of Computational Mathematics,",
            "year": 2012
        },
        {
            "authors": [
                "Alston S. Householder"
            ],
            "title": "The Theory of Matrices in Numerical Analysis",
            "venue": "Blaisdell Publishing Company, New York-Toronto-London,",
            "year": 1965
        },
        {
            "authors": [
                "Stefan Kunis",
                "Thomas Peter",
                "Tim Romer",
                "Ulrich von der Ohe"
            ],
            "title": "A multivariate generalization of Prony\u2019s method",
            "venue": "Linear Algebra and its Applications,",
            "year": 2016
        },
        {
            "authors": [
                "Jean Bernard Lasserre"
            ],
            "title": "Moments, Positive Polynomials and Their Applications",
            "venue": "(Cited on pages 1,",
            "year": 2010
        },
        {
            "authors": [
                "Monique Laurent"
            ],
            "title": "Sums of squares, moment matrices and optimization over polynomials",
            "venue": "Emerging Applications of Algebraic Geometry,",
            "year": 2009
        },
        {
            "authors": [
                "Alexander J. McNeil",
                "R\u00fcdiger Frey",
                "Paul Embrechts"
            ],
            "title": "Quantitative Risk Management: Concepts, Techniques and Tools Revised edition. Number 10496 in Economics Books",
            "year": 2015
        },
        {
            "authors": [
                "Alena Miftakhova",
                "Kenneth L. Judd",
                "Thomas S. Lontzek",
                "Karl Schmedders"
            ],
            "title": "Statistical approximation of high-dimensional climate models",
            "venue": "Journal of Econometrics,",
            "year": 2020
        },
        {
            "authors": [
                "Jens Oettershagen"
            ],
            "title": "Construction of Optimal Cubature Algorithms with Applications to Econometrics and Uncertainty Quantification",
            "venue": "PhD dissertation, Rheinischen Friedrich-Wilhelms-Universita\u0308t Bonn,",
            "year": 2017
        },
        {
            "authors": [
                "Neal Parikh",
                "Stephen Boyd"
            ],
            "title": "Proximal algorithms. Found",
            "venue": "Trends Optim.,",
            "year": 2014
        },
        {
            "authors": [
                "Maryam Pazouki",
                "Robert Schaback"
            ],
            "title": "Bases for kernel-based spaces",
            "venue": "Journal of Computational and Applied Mathematics,",
            "year": 2011
        },
        {
            "authors": [
                "Ernest K. Ryu",
                "Stephen P. Boyd"
            ],
            "title": "Extensions of Gauss Quadrature Via Linear Programming",
            "venue": "Foundations of Computational Mathematics,",
            "year": 2015
        },
        {
            "authors": [
                "Konrad Schm\u00fcdgen"
            ],
            "title": "The Moment Problem",
            "venue": "Graduate Texts in Mathematics. Springer International Publishing,",
            "year": 2017
        },
        {
            "authors": [
                "V P"
            ],
            "title": "The above variant of the pivoted Cholesky algorithm is tailored to the extraction algorithm from [Lasserre, 2010, Section 4.3], as the choice of pivots is restricted to the principal block of size mq \u00d7mq",
            "year": 2010
        },
        {
            "authors": [
                "B. Appendix"
            ],
            "title": "Greedy maximum-volume scenarios A quasi-Monte Carlo method to construct good quadrature rules, with a small number of nodes, to approximate integrals is the so-called Approximate Fekete points (AFP)",
            "venue": "cf. Bos et al",
            "year": 2011
        },
        {
            "authors": [
                "MICHAEL MULTERER",
                "PAUL SCHNEIDER",
                "ROHAN SEN see Foucart",
                "Rauhut"
            ],
            "title": "It performs sparse recovery in a stable and robust manner, and in particular, it is used to obtain sparse solutions to linear underdetermined systems. The HTP is a combination of greedy and thresholding methods, namely the compressive sampling matching pursuit (CoSaMP) and the iterative hard thresholding pursuit (IHT) respectively, see Foucart and",
            "venue": "Foucart",
            "year": 2013
        },
        {
            "authors": [
                "Bouchot"
            ],
            "title": "therefore, either set the maximum number of iterations as proportional to the sparsity level or run the iteration with an input tolerance level that is some factor times the error level. In our numerical experiments, we take both into consideration for the loop. Herein, we treat the maximum number of iterations as a hyper-parameter",
            "year": 2016
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Applications across many diverse fields are faced with high-dimensional data that has to be processed efficiently to make decisions informed by both sample moments, but also the realizations themselves. For instance, portfolio optimization necessitates covariance matrices, or even higher-order and/or nonlinear moments, while the corresponding risk management is oftentimes scenario-based. The factor structure in interest rates also suggests a scenario-based approach, see Engle et al. [2017]. Likewise, climate models often are complex and high-dimensional, necessitating low-dimensional representations for generative modeling, cp. Miftakhova et al. [2020]. In this paper, we are concerned with finding scenarios induced by high-dimensional and large data sets, that exploit the availability of panel data. The theoretical basis of our problem lies within the truncated moment problem (TMP), that underlies the theory of Gaussian quadrature, see Lasserre [2010], Schm\u00fcdgen [2017]. The TMP asks the question whether a sequence {y\u03b1}\u03b1\u22650 can be identified as the moments of an underlying probability measure, and if so, what is the representing discrete probability measure with the smallest number of atoms that has the same moment sequence. We think of these atoms as salient states of the world that may serve within a scenario-based representation of the underlying distribution. Keeping in mind data-driven applications within high-dimensional settings, we introduce the concept of empirical moment problems (EMP), in which moments are derived from the sample distribution induced by the data. The EMP is an empirical version of the TMP, connecting the same principles with the availability of sample realizations. We propose two suitable algorithms for scenario extraction, where the first one can be applied to the TMP as well as the EMP, while the second one is solely for the purpose of the EMP. The first algorithm yields scenarios that have possibly not been observed before, while the second approach picks eminent scenarios from data that have been observed before. The former is a specialization of the TMP to the uniform measure. It is the fastest algorithm considered in this paper and reveals a set of uniformly distributed covariance scenarios that perfectly match sample covariance matrices. The second lies within the greedy orthogonal matching pursuit (OMP) family, and reflects higher-order moment information, if so desired. A similar approach for the construction of optimal quadrature weights, rather than scenarios, for integrands lying in reproducing kernel Hilbert spaces, has been suggested in Oettershagen [2017]. Another canonical candidate for scenario extraction is basis pursuit, induced by the \u21131-regularized least squares, a standard approach in compressive sensing and machine learning literature. We prove, however, that basis pursuit does not lend itself to recovering optimal quadrature rules in the context of the EMP. We numerically demonstrate that both proposals are computationally efficient, robust, and precise. Furthermore, we contrast our proposals with existing approaches in the literature. In the context of the\nWe gratefully acknowledge support from the SNF grant 100018_189086 \u201cScenarios\u201d. 1\nar X\niv :2\n30 7.\n03 92\n7v 1\n[ st\nat .M\nL ]\n8 J\nul 2\n02 3\nEMP, a linear program approach, cp. Ryu and Boyd [2015], and Lasserre\u2019s algorithm, cp. Lasserre [2010], which in its prototypical form is akin to the multivariate Prony\u2019s method cp. Kunis et al. [2016], can be used to recover such quadratures also in higher dimensions. However, as we substantiate below, the approach from Ryu and Boyd [2015] does not possess optimality guarantees, and is not suited for higher dimensions. Likewise, the approach outlined in Lasserre [2010] becomes numerically prohibitive for higher dimensions and larger data sets. A viable construction of quadrature rules for higher dimensions and general domains have been suggested in Bittante et al. [2016], Bos et al. [2011, 2010], Sommariva and Vianello [2009] using approximate Fekete points which are extracted by sub-sampling weakly admissible meshes. In numerical simulations, we thus consider the greedy maximum volume algorithm from Bittante et al. [2016] that is also applicable in high dimensions, along with hard thresholding pursuit, cp. Foucart and Rauhut [2013]. We perform extensive numerical studies with data in up to 100 dimensions generated by different multimodal distributions with complex dependency structures. Comparisons with the aforementioned methods suggest superior accuracy with fewer scenarios and lower computation times of our proposals. We find that the multivariate Gaussian quadrature algorithm from Lasserre [2010] cannot efficiently be used for dimensions higher than ten. It furthermore suffers from significant computation times, as well as numerical non-robustness issues. The greedy maximum volume approach, as well as the OMP, is applicable even for 100 dimensions. The OMP has better computational efficiency, as well as consistency in the number of scenarios, recovered, especially when matching higher-order moments. Covariance scenarios reflect moments up to the second degree, and can also be deployed for 100 dimensions and beyond. The paper is organized as follows: In Section 2, we formally introduce the TMP and the notion of moment matrices, on the basis of which we propose covariance scenarios. In Section 3, we present the empirical version, the EMP, and propose the OMP algorithm. In Section 4, we prove why basis pursuit cannot work in the context of the EMP. Section 5 addresses numerical experiments, where we benchmark our proposed algorithms against extant approaches. In Section 6, we conclude and identify areas for future research."
        },
        {
            "heading": "2. The truncated moment problem (TMP)",
            "text": "In this section, we review the truncated moment problem (TMP), as the underlying mathematical foundation. Moreover, we present an algorithm for the direct extraction of scenarios from covariance matrices. Let \u2126 \u2282 Rd. We consider the measurable space (\u2126,B), where B denotes the Borel \u03c3-field on \u2126 and we denote by \u00b5 : B \u2192 [0,\u221e) a Borel measure on B. We aim at recovering \u00b5 from a known truncated moment sequence, y = {y\u03b1}|\u03b1|\u2264q, \u03b1 \u2208 Nd, q \u2208 N,\n(1) y\u03b1 = \u222b \u2126 x\u03b1 d\u00b5 for |\u03b1| \u2264 q.\nHere and in what follows, \u03b1 \u2208 Nd always denotes a multi index with modulus |\u03b1| := \u2211d\ni=1 \u03b1i. If a measure \u00b5 satisfies (1), we call \u00b5 a representing measure for the moment sequence y. We start by recalling a condition under which such a measure exists. To this end, let Pq be the space of all polynomials of total degree q, i.e., Pq := span{x\u03b1 : |\u03b1| \u2264 q}. It is well known that the dimension of this space is\n(2) mq := ( q + d\nd\n) .\nLetting (3) \u03c4 q(x) := [ 1, x1, . . . , xd, x 2 1, x1x2, . . . x 2 d, . . . , x q 1, . . . , x q d ] denote the monomial basis in Pq, we may express every p \u2208 Pq according to p(x) = \u03c4 q(x)p :=\u2211\n|\u03b1|\u2264q p\u03b1x \u03b1 for a suitable coefficient vector p = {p\u03b1}|\u03b1|\u2264q \u2208 Rmq . Moreover, we introduce the ring of polynomials R[x] := \u222aq\u2208NPq. Given the sequence y = {y\u03b1}\u03b1\u2208Nd , we finally introduce the linear form Ly : R[x] \u2192 R, p(x) = \u2211 \u03b1\u22650 p\u03b1x \u03b1 7\u2192 Ly(p) = \u2211 \u03b1\u22650 p\u03b1y\u03b1. There holds the following theorem, cp. Lasserre [2010].\nTheorem 2.1 (Riesz-Haviland). Let y = {y\u03b1}\u03b1\u22650 and C \u2282 Rd be a closed set. There exists a finite Borel measure \u00b5 on C such that \u222b\nC\nx\u03b1 d\u00b5 = y\u03b1 for all \u03b1 \u2265 0\niff Ly(p) \u2265 0 for all p \u2208 R[x] that satisfy p(x) \u2265 0 for all x \u2208 C.\nFor the actual computation of the measure \u00b5, we introduce the notion of a moment matrix and its associated flat extension.\nDefinition 2.2. Let y \u2208 Rm2q , cp. (2). We define the moment matrix\n(4) My := Ly(\u03c4\u22a4q \u03c4 q) = [y\u03b1+\u03b2]|\u03b1|,|\u03b2|\u2264q \u2208 Rmq\u00d7mq ,\nwhere the action of Ly on \u03c4\u22a4q \u03c4 q has to be understood element-wise, cp. (3). If there exists a vector y\u0303 \u2208 Rm2(q+1) such that y\u0303i = yi for i = 1, . . . ,m2q and M y\u0303 is positive semi-definite with rankM y\u0303 = rankMy, we call M y\u0303 a flat extension of My.\nFlat extensions play an important role in the determination of \u00b5, given y, see Laurent [2009]. For truncated moment sequences, the existence of a representing measure necessarily implies the existence of a discrete representing measure with finite support as shown by Bayer and Teichmann [2006]. The result is known as Tchakaloff\u2019s theorem for non-compact support which gives the existence of quadrature formulas of degree q for Borel measures with moments up to order q. From the flat extension, we can recover these discrete measures, as described in Lasserre [2010], (5) \u00b5 = r\u2211\ni=1\n\u03bbi\u03b4\u03bei , \u03bb1, . . . , \u03bbr > 0,\nwhere \u03b4x(y) denotes the Dirac measure supported at x. The points \u03bei \u2208 Rd are called the atoms of \u00b5, and \u00b5 in (5) is called an r-atomic measure. In our context, we refer to atoms as scenarios. There holds the following result from Curto and Fialko [1991] which gives the characterization of truncated sequences having finite atomic representing measures.\nTheorem 2.3. Let y \u2208 Rm2q with positive semi-definite moment matrix My \u2208 Rmq\u00d7mq with rankMy = r. Then, there exists an r-atomic representing measure \u00b5 with support on Rd iff My has a flat extension.\nThe above theorem relates the problem of finding atoms of the finite-atomic representing measure to the construction of optimal quadrature rules with a minimal number of nodes in numerical integration, known in the literature as Gaussian quadrature in the case of one spatial dimension. Lasserre\u2019s algorithm, see Lasserre [2010], for the extraction of nodes relies on the above theorem, wherein the assumption is that a flat extension of the moment matrix is available. A particular consequence of Theorem 2.3 is that any moment matrix My \u2208 Rmq\u00d7mq associated to an r-atomic measure \u00b5 can be represented in Vandermonde form. (6) My = r\u2211\ni=1\n\u03bbi\u03c4 \u22a4 q (\u03bei)\u03c4 q(\u03bei) = V q(\u03be1, . . . , \u03ber) \u22a4\u039bV q(\u03be1, . . . , \u03ber),\nwith \u03be1, . . . , \u03ber the scenarios and their probabilities \u039b := diag(\u03bb1, . . . , \u03bbr), see Lasserre [2010], Laurent [2009], Schm\u00fcdgen [2017]. Therein, the matrix V (\u03be1, . . . , \u03ber) is the Vandermonde matrix\n(7) V q(\u03be1, . . . , \u03ber) := \u03c4 q(\u03be1)... \u03c4 q(\u03ber)  \u2208 Rr\u00d7mq . While multivariate Gaussian quadrature would be highly desirable, Lasserre\u2019s algorithm requires a flat extension, cf. Definition 2.2, of the moment matrix under consideration, which is difficult to obtain, in particular, in high dimensions and for q > 1. For the case My \u2208 Rm1\u00d7m1 ,y \u2208 Rm2 , there nevertheless exists a simple approach to compute the Vandermonde form as in (6) by means of Householder reflections, cp. Householder [1965].\nTheorem 2.4. Let R \u2208 Rm1\u00d7r be a matrix root of the covariance matrix My \u2208 Rm1\u00d7m1 , that is My = RR \u22a4. Then, the Vandermonde form of My reads My = V \u22a4\u039bV with V = \u221a rHvR\n\u22a4 and \u039b = 1rI, where Hv := I \u2212 2 vv\u22a4 v\u22a4v for v := r \u2212 1\u221a r 1. Herein, r\u22a4 is the first row of R and 1 \u2208 Rr is the vector of all 1\u2019s.\nProof. There holds Hvw = w for w \u22a5 v and Hvv = \u2212v. Hence, choosing v = r \u2212 \u03bb1, and \u03bb := \u2225r\u22252/\u22251\u22252 = \u2225r\u22252/ \u221a r, one readily verifies Hvr = \u03bb1.\nMoreover, it is straightforward to see that Hv is orthogonal. Therefore, we arrive at the Vandermonde form My = RH\u22a4vHvR\n\u22a4 = V \u22a4\u039bV with \u039b = \u03bb2I = \u2225r\u222522/rI. Finally, we obtain the assertion by noticing that \u2225r\u222522 = r\u22a4r =My,1,1 = 1. \u25a1\nThe reason why Theorem 2.4 yields unique scenarios without a flat extension of the moment matrix lies in the uniform distribution of the weights. The restriction to q = 1 is due to the preservation of the Vandermonde form, which could otherwise not be guaranteed. We list the computational steps of Theorem 2.4 in Algorithm 1. We refer to the unique scenarios obtained from this algorithm as covariance scenarios.\nAlgorithm 1 Covariance scenarios\ninput: symmetric and positive semidefinite moment matrix My \u2208 Rm1\u00d7m1 , tolerance \u03b5 > 0\noutput: the r scenarios \u039e := { \u03be1, \u00b7 \u00b7 \u00b7 , \u03ber } 1: compute a matrix root My = RR\u22a4 2: set r := R\u22a4e1, v := r \u2212 (1/ \u221a r)1, \u03b3 := 2/\u2225v\u222522, \u03bb := 1/ \u221a r 3: set Hv := I \u2212 \u03b3vv\u22a4 and V := 1\u03bbHvR \u22a4\n4: set \u03bej = [ V \u22a4ej ]d+1 i=2 for j = 1, . . . , r\nIn the next section, we present alternative approaches for the case when the underlying probability measure is known empirically from i.i.d. draws as is typically the case in data-driven applications."
        },
        {
            "heading": "3. The empirical moment problem (EMP)",
            "text": "Let X = {x1, \u00b7 \u00b7 \u00b7 ,xN} \u2282 \u2126 \u2286 Rd, denote a set of samples. The associated empirical measure is given by P\u0302 := 1N \u2211N i=1 \u03b4xi . It satisfies P\u0302(X) = 1, and is hence a probability measure. With the empirical measure at hand, it is straightforward to compute the associated empirical moment sequence (8) y\u0302 = [ y\u0302\u03b1 ] |\u03b1|\u22642q = [\u222b \u2126 x\u03b1 dP\u0302 ] = 1 N N\u2211 i=1 \u03c4\u22a42q(xi) = 1 N V \u22a42q(x1, . . . ,xN )1 \u2208 Rm2q .\nInspired by the TMP, our goal is to determine a small number of representative scenarios \u039e := { \u03be1, . . . , \u03ber } with r \u226a N . We are interested in deriving a compressed version P\u22c6 = \u2211r i=1 \u03bbi\u03b4\u03bei , \u03bbi \u2265 0, \u2211r i=1 \u03bbi = 1, of the sample measure with the associated moment sequence\n(9) y\u22c6 = [ y\u22c6\u03b1 ] |\u03b1|\u22642q = [ \u222b \u2126 x\u03b1 dP\u22c6 ] |\u03b1|\u22642q = V \u22a42q(\u03be1, . . . , \u03ber)\u039b, \u039b := [\u03bb1, . . . , \u03bbr] \u22a4\nBy analogy to the TMP, we refer to this task as the empirical moment problem (EMP). In one dimension, Gaussian quadrature rules are efficient for the moment-matching problem since they use the optimal number of scenarios that match moments for the highest polynomial degree. In multiple dimensions, however, generalizing Gaussian quadrature is difficult and thus, retrieving the optimal number of scenarios for the EMP is rather challenging.\nRemark 3.1. In the case of matching sample moments, the following formulation is suggested in Ryu and Boyd [2015]:\n(10)\nFind a probability measure P\u22c6 which solves\nminimize P\u22c6 \u222b \u2126 p(x) dP\u22c6\nsubject to: \u222b \u2126 x\u03b1 dP\u22c6 = \u222b \u2126 x\u03b1 dP\u0302 for |\u03b1| \u2264 2q,\nwhere p is a polynomial that is linearly independent of the monomials in \u03c4 2q and can be thought of as a sensitivity term. The above linear programming (LP) approach (10), while convex, is NP-Hard in general, see Ryu and Boyd [2015]. Moreover, the LP approach is effective only when d is small, say 2 or 3, and does not possess optimality guarantees with regard to the number of scenarios.\nAnother possible solution for the EMP is via a flat extension of M y\u0302 and using Lasserre\u2019s extraction algorithm, see Lasserre [2010]. However, as there are no guarantees on termination and efficiency, it may easily become infeasible. The approaches presented in this section are based on the assumption that we have at our disposal a candidate set \u039e, which may, or may not be a subset of the data points X. For instance, the case where \u039e contains elements of a low-discrepancy sequence was considered by Bittante et al. [2016]. In our numerical tests, we focus on \u039e \u2282 X. We consider a relaxed version of the EMP and determine the vector y\u22c6 such that\n(11) \u2225\u2225y\u22c6 \u2212 y\u0302\u2225\u2225 \u2264 \u03b5\nfor a given tolerance \u03b5 > 0 and a given norm \u2225 \u00b7 \u2225 on Rm2q . With \u2225 \u00b7 \u2225 = \u2225 \u00b7 \u22252 denoting the Euclidean norm, recasting (11) using (9) would lead to the optimization problem\n\u039b = argmin w\u2208RN \u2225w\u22250\nsubject to: \u2225\u2225V \u22a42q(x1, . . . ,xN )w \u2212 y\u0302\u2225\u22252 \u2264 \u03b5, w \u2265 0.(12)\nDue to the non-convexity of the \u21130-seminorm, problem (12), which seeks the sparsest non-negative approximate solution of the linear underdetermined system, is NP-Hard in general. In Donoho and Tanner [2005], the following convex optimization approach to solve for sparse non-negative approximations to linear underdetermined systems is proposed:\n\u039b = argmin w\u2208RN\n1\u22a4w\nsubject to: \u2225\u2225V \u22a42q(x1, . . . ,xN )w \u2212 y\u0302\u2225\u22252 \u2264 \u03b5, w \u2265 0.(13) However, the atomic decomposition of P\u22c6 requires the weights to sum to 1. The consequent redundancy due to this prevents us from obtaining sparse solutions using the above optimization problem. Moreover, as we show in Theorem 4.1 below, neither \u21131-regularization used in basis pursuit is applicable in our setting. To simultaneously maintain feasibility and to ensure that the weights are normalized and positive, we solve (13) in two stages. First, we extract the scenarios by solving the unconstrained least-squares problem\nminimize w\u2208RN \u2225\u2225V \u22a42q(x1, . . . ,xN )w \u2212 y\u0302\u2225\u22252.(14) While problem (14) is trivially solved at the constant vector with element 1/N , it nevertheless serves as objective in identifying the scenarios through the particular properties of V 2q and y\u0302. We describe this step in Section 3.2. Second, given the scenarios, we retrieve the corresponding probabilities by enforcing the simplex constraints in Section 3.3 with a standard algorithm. In the next section, we introduce a computational framework to resolve both steps, that is suited also for high-dimensional and ill-conditioned settings.\n3.1. Problem reformulation. The scenario extraction problem (14) is potentially underdetermined with respect to the number of samples considered and the degree to which the moments are observed.\nThus, to solve the problem, we reformulate it in terms of the reproducing kernel Hilbert space (RKHS) of polynomials with respect to the sample measure.\nDefinition 3.2. Let ( H, \u27e8\u00b7, \u00b7\u27e9H ) be a Hilbert space of real-valued functions on a non-empty set \u2126. A function K : \u2126\u00d7 \u2126 \u2212\u2192 R is a reproducing kernel of the Hilbert space H iff \u2200 x \u2208 \u2126, K(x, \u00b7) \u2208 H,\n\u2200 \u03c6 \u2208 H, \u2200 x \u2208 \u2126, \u2329 K(x, \u00b7), \u03c6 \u232a H = \u03c6(x) (15) The last condition of Equation (15) is called the reproducing property. If these two properties hold, then( H, \u27e8\u00b7, \u00b7\u27e9H ) is called a reproducing kernel Hilbert space (RKHS).\nIn order to define the polynomial moments of the samples, we examine the polynomial space P2q endowed with the L2\nP\u0302 -inner product. To define the reproducing kernel, we introduce the Gramian, which coincides\nwith the moment matrix cp. (2.2). For the sake of a lighter notation, we will henceforth always write V := V 2q(x1, . . . ,xN ) \u2208 RN\u00d7m2q and set\nM := \u2329 \u03c4\u22a42q, \u03c4 2q \u232a L2\nP\u0302 = [\u222b \u2126 x\u03b1+\u03b2 dP\u0302 ] |\u03b1|,|\u03b2|\u22642q = 1 N V \u22a4V \u2208 Rm2q\u00d7m2q\nWe have the following result.\nTheorem 3.3. The function K : \u2126\u00d7\u2126 \u2212\u2192 R, K(x, z) := \u03c4 2q(x)M \u2020\u03c4\u22a42q(z) is a symmetric and positive type function. Herein, M \u2020 denotes the Moore-Penrose inverse of M . Moreover, K has the reproducing property on X, i.e.,\n(16) \u2329 K(xi, \u00b7), p \u232a L2\nP\u0302 = p(xi) for all p \u2208 P2q, xi \u2208 X. If M is invertible, the kernel K is the reproducing kernel of ( P2q, \u27e8\u00b7, \u00b7\u27e9L2\nP\u0302\n) .\nProof. We introduce the kernel matrix K := [ K(xi,xj) ]N i,j=1 = VM \u2020V \u22a4 \u2208 RN\u00d7N . The matrix K is\nsymmetric which follows from the fact that M \u2020 is symmetric. There holds M \u2020 = NV \u2020 ( V \u22a4 )\u2020 . Note that rankK = rankM . Furthermore, we have\na\u22a4Ka = Na\u22a4V V \u2020 ( V \u22a4 )\u2020 V \u22a4a = N \u2225\u2225(V \u22a4)\u2020V \u22a4a\u2225\u22252 2 \u2265 0 for all a \u2208 RN\nHence, the matrixK is symmetric and positive semi-definite, i.e., the kernel K is a symmetric and positive type function. In order to prove the reproducing property, we first recall that the Moore-Penrose inverse satisfies AA\u2020A = A, (AA\u22a4)\u2020 = (A\u22a4)\u2020A\u2020 as well as (AA\u2020)\u22a4 = AA\u2020 for any matrix A. From this, we directly infer\n(17) V ( M \u2020M ) = V (( V \u22a4V )\u2020 V \u22a4V ) = (( V V \u2020 )( V V \u2020 )\u22a4) V = ( V V \u2020 )( V V \u2020 ) V = V .\nLet p \u2208 P2q, i.e., p(x) = \u03c4 2q(x)p for any coefficient vector p \u2208 Rm2q . Hence, we have\u2329 K(xi, \u00b7), p \u232a L2\nP\u0302 = \u03c4 2q(xi)M \u2020\u2329\u03c4\u22a42q, \u03c4 2q\u232aL2 P\u0302 p = \u03c4 2q(xi)M \u2020Mp = \u03c42q(xi)p = p(xi),\nwhere we use (17) and that [V i,j ] m2q j=1 = \u03c4 2q(xi). In particular, if M is invertible, there holds\u2329 K(x, \u00b7), p \u232a L2\nP\u0302 = p(x) for all x \u2208 \u2126, which makes K the reproducing kernel of\n( P2q, \u27e8\u00b7, \u00b7\u27e9L2\nP\u0302\n) . \u25a1\nEmpirically, the Vandermonde matrix, and hence, the moment matrix may be rank-deficient. Applying the pivoted Cholesky factorization, as in Algorithm (2) with sufficiently low tolerance \u03b5 on the positive semi-definite moment matrix M yields M = LL\u22a4, with matrix L \u2208 Rm2q\u00d7r with r = rankM . Furthermore, the algorithm concurrently computes a bi-orthogonal basis B \u2208 Rm2q\u00d7r such that B\u22a4L = I \u2208 Rr\u00d7r. The basis transformation is an essential byproduct of efficiently solving the low-rank approximation of the constrained optimization problem at a large scale. Algorithm 2 iteratively reduces the largest entry of the Schur complement in a greedy fashion, see Harbrecht et al. [2012]. We particularly haveM \u2020 = BB\u22a4, and \u03c82q(x) := \u03c4 2q(x)B forms an L2P\u0302-orthonormal\nAlgorithm 2 Pivoted Cholesky Decomposition\ninput: symmetric and positive semidefinite matrix M \u2208 Rm2q\u00d7m2q , \u03b5 \u2265 0 output: low-rank approximation M \u2248 LL\u22a4\nand biorthogonal basis B such that B\u22a4L = Ir\n1: Initialization: set r := 1, d := diag(M), L := [ ], B := [ ], err := \u2225d\u22251 2: while err > \u03b5 3: determine j := argmax1\u2264i\u2264m2q di 4: compute\n\u2113r := 1\u221a dj\n( M \u2212LL\u22a4 ) ej and br :=\n1\u221a dj\n( I \u2212BL\u22a4 ) ej\n5: set L := [L, \u2113r], B := [B, br] 6: set d := d\u2212 \u2113r \u2299 \u2113r, where \u2299 denotes the Hadamard product 7: set err := \u2225d\u22251, r := r + 1\nsystem consisting of r polynomials since\u2329 \u03c8\u22a42q,\u03c82q \u232a L2\nP\u0302 = B\u22a4\n\u2329 \u03c4\u22a42q, \u03c4 2q \u232a L2\nP\u0302 B = B\u22a4MB = B\u22a4LL\u22a4B = I \u2208 Rr\u00d7r.\nThus, the matrix\n(18) Q := 1\u221a N\n[ \u03c82q(xi) ]N i=1 = 1\u221a N V B \u2208 RN\u00d7r\nsatisfies Q\u22a4Q = Ir. The matrix K can be decomposed as K = NQQ\u22a4. Especially, we have that ImV = ImQ. Using the Euclidean norm, there holds\n\u03c3min(M \u2020) N \u00b7 \u2225\u2225V \u22a4w \u2212 y\u0302\u2225\u22252 2 \u2264 \u2225\u2225\u2225Q\u22a4w \u2212 y\u0303\u2225\u2225\u22252 2 \u2264 \u03c3max(M \u2020) N \u00b7 \u2225\u2225V \u22a4w \u2212 y\u0302\u2225\u22252 2\nwhere y\u0303 = 1\u221a N B\u22a4y\u0302. Thus, using the decomposition of the moment matrixM = (V \u22a4V )/N , we have that \u2225V \u22a4\u222522 = N\u03c3max(M) = N/\u03c3min(M \u2020). Therefore, up to the condition of the matrix V \u22a4V , minimization problem (14) is equivalent to the problem\n(19) minimize w\u2208RN \u2225\u2225Q\u22a4w \u2212 y\u0303\u2225\u2225\u2225 2 ,\nwhich we will henceforth consider for the extraction of scenarios. In the next result, we collect additional properties of Q, in particular in connection with the above optimization problem.\nLemma 3.4. Matrix Q satisfies the relation\n(20) Qy\u0303 = 1\nN 1.\nFrom the normal equations QQ\u22a4w = Qy\u0303, the vector w = 1N 1 is a solution of (19).\nProof. To show (20), we first note that y\u0302 can be identified with the first column of the moment matrix, i.e., y\u0302 =Me1 = 1NV \u22a4V e1. Hence, we can write\n(21) Qy\u0303 = 1\nN V BB\u22a4Me1 =\n1\nN VM \u2020Me1 =\n1\nN V e1 =\n1\nN 1,\nwhere we exploit the reproducing property of the kernel on the sample set, cp. (17), and the identity M \u2020 = BB\u22a4. We have as a corollary Qy\u0303 = 1N 1 = QQ \u22a4 1 N 1, again by (17) and the fact that V e1 = 1. Hence, the vector w = 1N 1 is a solution to the normal equations. \u25a1\n3.2. Step 1: Algorithm for empirical scenario extraction. In this section, we present our main algorithm for the computation of scenarios for panel data sets, a greedy orthogonal matching pursuit algorithm (OMP), based on the pivoted Cholesky decomposition, cp. Pazouki and Schaback [2011], of the matrix K. In Oettershagen [2017], a similar approach is used to construct a greedy approximation to the sample moment (which lies in an RKHS) that minimizes the worst-case quadrature error. With regard to the optimization problem (19), the matrix K = NQQ\u22a4 appears as the objective interest. Particularly, the vector Qy\u0303 belongs to the linear span of the columns of K. We thus seek a greedy choice\nof a subsample of the columns of K that can approximate the whole column space within a certain error tolerance. With this objective in mind, we adapt the notion of the decomposition of K into the setting of OMP as in Algorithm 3.\nAlgorithm 3 Orthogonal matching pursuit (OMP)\ninput: symmetric and positive semi-definite matrix K \u2208 RN\u00d7N , h \u2208 RN , \u03b5 > 0 output: index set ind corresponding to the sparse scenarios,\nlow-rank approximation K \u2248 LL\u22a4 and bi-orthogonal basis B such that B\u22a4L = Ir\n1: Initialization: set r := 1, L := [ ], B := [ ], ind := [ ],d0 = diag(K),h0 = h, err = 1 2: while err > \u03b5 3: determine j := argmax1\u2264i\u2264N |hr\u22121,i| 4: ind := [ind, j] 5: compute\n\u2113r := 1\u221a dj\n( M \u2212LL\u22a4 ) ej and br :=\n1\u221a dj\n( I \u2212BL\u22a4 ) ej\n6: set L := [L, \u2113r], B := [B, br] 7: set dr := dr\u22121 \u2212 \u2113r \u2299 \u2113r, where \u2299 denotes the Hadamard product 8: set hr := hr\u22121 \u2212 (br\u22a4h)\u2113r 9: set err := \u2225hr\u22252 / \u2225h\u22252, r := r + 1\nWe refer to the procedure underlying Algorithm 2 to justify Algorithm 3. The only changes lie in the pivoting strategy in line 3, which now seeks to eliminate the largest entry of the vector h at each step. In particular, the algorithm iteratively constructs a bi-orthogonal basis B. We remark that for contextual convenience, we reuse the variables B and L, however they are related to the matrix K now. If we set h = Qy\u0303, we have in the case r = rankK that h = K 1N 1 = LL \u22a4 1 N 1 \u2208 span{\u21131, . . . , \u2113r} . By\nthe definition of the bi-orthogonal basis issued from line 5, we have that h = \u2211r\ni=1(bi \u22a4h)\u2113i. Algorithm\n(3) greedily constructs the basis expansion such that the absolute error after m \u2264 r steps is given by hm := h\u2212 \u2211m i=1(bi \u22a4h). In our numerical experiments, we find it computationally more robust to consider the relative error as in line 9 of Algorithm 3. It is worth remarking that {\u21131, . . . , \u2113r} is the so called Newton basis evaluated at the sample points, see Pazouki and Schaback [2011].\n3.3. Step 2: Retrieval of probability weights. In order to enforce the probability simplex constraints, we use the alternating direction method of multipliers (ADMM), see Boyd et al. [2011], Parikh and Boyd [2014]. This approach can be used for projecting least-squares solutions of linear systems onto convex sets. Denote the set of the two simplex constraints as\n\u2206+ := { w \u2208 Rr : w \u2265 0 } ,(22)\n\u22061 := { w \u2208 Rr : 1\u22a4w = 1 } (23)\nIn particular, we proceed to obtain the probability weights by solving the following constrained optimization problem.\n\u039b = argmin w\u2208Rr \u2225\u2225V \u22a42q(\u03be1, . . . , \u03ber)w \u2212 y\u0302\u2225\u22252 subject to: w \u2208 \u2206. (24)"
        },
        {
            "heading": "4. Basis pursuit in the EMP",
            "text": "A related natural way to solve scenario extraction would be the basis pursuit, i.e., \u21131-regularized least squares. However, in our particular setting of finding optimal representation using scenarios, it does not lead to sparsity, as given by the following theorem.\nTheorem 4.1. The minimizer of the basis pursuit problem\nw\u22c6 = argmin w\u2208RN\n1\n2 \u2225\u2225\u2225Q\u22a4w \u2212 y\u0303\u2225\u2225\u22252 2 + \u03bb \u2225w\u22251 = argmin\nw\u2208RN\n1\n2 \u2225\u2225\u2225V \u22a4w \u2212 y\u0302\u2225\u2225\u22252 2 + \u03bb \u2225w\u22251, \u03bb > 0,\nhas the constant form w\u22c6 = c1 with\nc =\n{ 1\u2212N\u03bb\nN , if 1 N > \u03bb,\n0, otherwise.\nProof. We define the two respective cost functions g\u03bb, h\u03bb : RN \u2212\u2192 R as\ng\u03bb(w) := 1\n2 \u2225\u2225\u2225Q\u22a4w \u2212 y\u0303 \u2225\u2225\u22252 2 + \u03bb\u2225w\u22251, and h\u03bb(w) := 1 2 \u2225\u2225\u2225V \u22a4w \u2212 y\u0302 \u2225\u2225\u22252 2 + \u03bb\u2225w\u22251.\nClearly, for given \u03bb > 0, both g\u03bb and h\u03bb are convex functions. Therefore, g\u03bb and h\u03bb are subdifferentiable over RN cp. [Beck, 2017, Corollary 3.15]. We can compute them as, cf. Beck [2017],\n\u2202g\u03bb(w) = QQ \u22a4w \u2212Qy\u0303 + \u03bb\u2202\u2225w\u22251, and \u2202h\u03bb(w) = V V \u22a4 ( w \u2212 1\nN 1\n) + \u03bb\u2202\u2225w\u22251,\nwhere\n(\u2202\u2225w\u22251)i = { signwi, wi \u0338= 0, x \u2208 [\u22121, 1], wi = 0.\nFrom Lemma 3.4, there holds Qy\u0303 = QQ\u22a41 1N = 1 N 1. Hence, the subgradient \u2202g\u03bb reads\n\u2202g\u03bb ( c N 1 ) = ( c\u2212 1 N + \u03bb sign(c) ) 1.\nSince 0 \u2208 \u2202g\u03bb describes the optimality for this strictly convex problem, we can exclude c < 0, since then the subgradient will be strictly element-wise negative. However, suppose that 1/N > \u03bb and c > 0, then, from the equation c\u22121N + \u03bb sign(c) = 0, c = 1\u2212N\u03bb proves to be optimal. For 1/N \u2264 \u03bb, the subgradient evaluated at 0 becomes\n\u2202g\u03bb (0) = ( \u2212 1 N + \u03bb|x| ) 1,\nwhere we can again deduce 0 \u2208 \u2202g\u03bb(0) with x = 1/(N\u03bb). For h\u03bb,\n\u2202h\u03bb ( c N 1 ) = V V \u22a4 ( c\u2212 1 N + \u03bb sign(c) ) 1, and \u2202h\u03bb (0) = V V \u22a4 ( \u2212 1 N + \u03bb|x| ) 1,\nthe cases are analogous. \u25a1\nThe statement shows that basis pursuit does not induce sparsity in the EMP setting. Note that in the above proof, we do not assume any positivity of the weight vector w. Therefore, the above proof also shows that it is not possible to obtain optimal quadrature rules with high accuracy with respect to the total degree polynomial class of functions, using \u21131-minimization. This fact restricts the pool of candidate algorithms that can be deployed for efficient recovery of the sparse scenarios from the empirical moment sequences."
        },
        {
            "heading": "5. Numerical experiments",
            "text": "In this section we perform numerical experiments to investigate the behavior of the proposed algorithms with respect to the dimension, the number of data points, and the order of the maximum degree of the polynomial basis generating the moment matrix in question. We test the algorithm by Lasserre [2010], the maximum volume algorithm, see Bittante et al. [2016], Bos et al. [2010, 2011], Golub and Van Loan [2013], Sommariva and Vianello [2009], the graded hard thresholding pursuit (GHTP), see Bouchot et al. [2016], the OMP Algorithm 3, and finally the covariance scenarios described in Algorithm 1. Algorithm 1 is only attempted for q = 1 since it was developed solely with the purpose of matching covariance. All algorithms have been implemented on a single Intel Xeon E5-2560 core with 3 GB of RAM, with the exception of Lasserre\u2019s algorithm which is run on an 18-core Intel i9-10980XE machine with 64 GB RAM.\n5.1. Multivariate Gaussian mixture distributions. We first examine the different algorithms on simulated data sampled from a Gaussian mixture model with a different number of dimensions d and having a different number of clusters c, components of the mixture distribution, that induce multiple modes into the joint distribution. We test for d = 2, 5, 10, 20, 50, 100 and c = 5, 10, 20, 25, 50, 100, 150, 200. In order to investigate the efficacy of the algorithms, we do not use a higher c for relatively lower d and vice-versa. The means of the different clusters are taken to be random vectors of uniformly distributed numbers in the interval (\u221250, 50). The variance-covariance matrices for the different clusters are either (i) randomly generated positive definite matrices with N (1, 1)-distributed eigenvalues, or (ii) the identity matrix. Except in the case of unit variance-covariance, the different clusters have different variancecovariance matrices. The mixing proportions for the different clusters are taken to be either (i) random or, (ii) equal. We can categorize them as\n(1) random variance-covariance and random mixing proportion (2) random variance-covariance and equal mixing proportion (3) unit variance-covariance and random mixing proportion (4) unit variance-covariance and equal mixing proportion\nFor each of the above cases, 100 data sets are randomly constructed, each containing 10 000 samples. For each data set containing the samples, we calculate the sample moments y\u0302 up to order 2q with q = 1, 2 which give rise to the empirical moment matricesM y\u0302 of orders 1 and 2 respectively. The 100-dimensional and 50-dimensional data sets are computed for q = 1 only, still resulting in 5 151-dimensional, and 1 326- dimensional moment matrices, respectively. The biggest moment matrix for q = 2 is computed for twenty dimensions, resulting in a 10 262-dimensional moment matrix. It is noteworthy to point out that there is, theoretically speaking, no bound on the degree to which we can calculate the moments. However, the computational cost of obtaining higher-order moment information is evidently high. We compare the performance of the different algorithms with respect to the relative errors, computation times, and the number of scenarios extracted, across different dimensions and clusters, as shown subsequently.\n5.1.1. Relative errors. We compare the behavior of the different algorithms with regard to how well the sample moments are matched using the scenarios. For a fair comparison of the relative errors for the different algorithms, we define\n(25) relative error := \u2225\u2225M y\u0302 \u2212 V \u22a4\u039bV \u2225\u2225F\u2225\u2225M y\u0302\u2225\u2225F where V is the Vandermonde matrix of order q generated from the samples, and \u039b is the (sparse) diagonal matrix containing the respective probability weights of the samples. We plot the relative errors (25) against the dimensions and number of clusters in Figure 1 and Figure 2 respectively for q = 1 and q = 2. In both figures, the y-axis represents the relative error in the log scale. The x-axis contains the different dimensions d which is a categorical variable. The color bar depicts the different number of clusters and is taken as a categorical variable. The performance of the different algorithms is shown in the respective tiles. Lasserre. The algorithm from Lasserre [2010] is computationally feasible only until dimension 10 for q = 1 and d = 5 for q = 2. It breaks down thereafter since the algorithm involves finding flat extensions requiring the resolution of large semidefinite relaxations. For each dimension, the relative errors vary considerably from the order of 10\u22128 to 10\u22121 for q = 1, and 10\u22126 to 10\u22121 for q = 2. There is no particular pattern in their distribution with the number of clusters. The results suggest that Lasserre\u2019s algorithm is not well suited for large and high-dimensional data sets. Maximum volume. We find that, unlike Lasserre\u2019s algorithm, the maximum volume algorithm is applicable up to d = 100 for q = 1, and d = 20 for q = 2. The relative errors range from the order of 10\u221214 to 10\u22122 for q = 1, and 10\u22124 to 10\u22123 for q = 2. They exhibit a slight decrease with the number of dimensions, but across dimensions, and a maximum increase by a factor of 10 with the number of clusters. This is a considerable improvement from Lasserre\u2019s algorithm insofar as the relative errors are more stable across\ndimensions and the number of clusters. For d = 20 and q = 2, they exhibit a sharp decrease and range in the order of 10\u221210, however. GHTP. This algorithm is easily applicable up to d = 100 for q = 1, and d = 20 for q = 2. The relative errors range from the order of 10\u221213 to 100 for q = 1. Except for the case of d = 2, the relative errors mostly range from the order of 10\u22124 to 100. For q = 2, the relative errors range from the order of 10\u22124 to 100. In terms of the stability of the relative errors, although the GHTP fares better than the maximum volume algorithm, nevertheless, it is a deterioration when considering the range of the errors. OMP. We find that this algorithm is easily applicable until d = 100 for q = 1, and d = 20 for q = 2. The relative errors range from the order of 10\u221214 to 10\u22122 for q = 1, and 10\u221210 to 10\u22123 for q = 2, with most in the range of 10\u22124 to 10\u22123. Not only are the errors overall much more stable across the different dimensions, but their orders are also comparable to that of the maximum volume algorithm. Except for the case of d = 2, the relative errors mostly range from the order of 10\u22124 to 10\u22122. While the range of the relative errors for the OMP is similar to that of the maximum volume algorithm, there is a difference in that we do not observe any significant noticeable pattern in the distribution of the relative errors across dimensions. Covariance scenarios. We find that this algorithm is not only applicable until d = 100, but the relative errors are considerably smaller than for all the above algorithms, ranging in the order of 10\u221217 to 10\u221215. Hence, as stipulated in the earlier section, in the special case of covariance matrices, i.e., when q = 1, the covariance algorithm performs the best since it matches exactly all the sample moments until the second moment. By its construction, covariance scenarios are not applicable for q = 2.\n5.1.2. Number of scenarios. For practical purposes when working with large data sets, given a relative error, sparsity is preferred in the number of scenarios extracted when solving the EMP. This is analogous to quadrature, which is deemed to be efficient if it matches the highest number of polynomial moments\nusing as few nodes as possible. To that end, we compare the number of scenarios extracted from the different algorithms. Except for the case of covariance scenarios, that have uniform weights, for the remaining methods, we retrieve the weights using the ADMM algorithm described in Section 3.3. For each probability vector, we set the entries less than 10\u22128 to zero. As such, we consider the number of scenarios to be the number of non-zero entries of the modified probability vector. We then plot the number of scenarios against the dimensions and number of clusters for q = 1, 2 in Figure 3 and Figure 4 respectively. In both figures, the y-axis denotes the number of scenarios and is taken in the log scale. The x-axis contains the different dimensions which is a categorical variable. The color bar denotes the number of clusters, and it is also taken as a categorical variable. The performance of the different algorithms is shown in the respective tiles. Lasserre. Lasserre\u2019s algorithm depends on the flat extension of the moment matrix, with the precise rank of the flat extension determining the number of scenarios. Accordingly, we find that the number of scenarios, while not too high for each dimension, still shows variability for d = 5, 10. For q = 2, the number of scenarios is also not exceedingly high. Maximum volume. We first observe that the number of scenarios is considerably high across the dimensions, ranging from 4 000 to 5 000 for d = 100. Except for d = 100, there is no considerable difference in the number of scenarios retrieved with the number of clusters. Similar to the case for q = 1, the number of scenarios is considerably high across the dimensions and is equal to 10 000 for d = 20, i.e., it does not give sparse scenarios and considers all the sample points. There is also no variation with different clusters for either dimension. Nevertheless, it cannot be used for sparse recovery of scenarios from large samples. GHTP. For this algorithm, we find that the number of scenarios is much less compared to that of the maximum volume algorithm, and similar to that of Lasserre\u2019s algorithm for d = 2, 5, 10. This result is not surprising, given that the algorithm constructs the index set whose size is equal to a prior input maximum number of iterations (see Bouchot et al. [2016]). For d = 20, 50, 100, however, the number of scenarios varies considerably with the number of clusters. For q = 2, the number of scenarios is much\nless, with the maximum being 250, compared to that of the maximum volume algorithm, nevertheless, it varies considerably with the number of clusters for each dimension. OMP. The number of scenarios retrieved is somewhat similar to that of the GHTP. This results from the fact that we set the maximum number of iterations for the OMP to be the same as that of the GHTP. Unlike the GHTP however, we find that in this case, there is no considerable variation in the number of scenarios with the number of clusters, for each dimension. Therefore, the OMP outperforms the above algorithms, with regard to the number as well the consistency in the retrieval of scenarios. The same interpretation holds true for q = 2. Covariance scenarios. The range for the number of scenarios is the least among all the algorithms for all the dimensions. Furthermore, there is no variation in the case of the different clusters for each dimension. This reaffirms that the covariance scenarios perform the best with regard to the number of scenarios retrieved, keeping in mind that they are applicable only in the case q = 1.\n5.1.3. Computation time. Finally, we test how fast the algorithms are in computing the scenarios from large empirical data sets, keeping practical applications in mind. To that end, we plot the CPU run times of the different algorithms against the different dimensions and number of clusters for q = 1, 2 in Figure 5 and Figure 6 respectively. We take the y-axis to be the computation times in the log scale. The x-axis contains the different dimensions which is a categorical variable. The color bar depicts the different number of clusters and is also taken as a categorical variable. Lasserre. Computation times range from 10\u22121 to 103 seconds just the until d = 10 only. For d = 5, the run times increase with the number of clusters, by an order of 10. Therefore, Lasserre\u2019s algorithm becomes computationally costly in the face of higher dimensions. For q = 2, We find that the times range from 10\u22121 to 103 seconds until d = 5 only. For d = 5, the run times increase with the number of clusters, by an order of 100.\nMaximum volume. The run times range from 10\u22122 to 103 seconds across the different dimensions and clusters. A noticeable aspect of this algorithm is that while the run-time increases, the variation among them, however, decreases with an increase in the dimension. For q = 2, the run times range from 10\u22121 to 103 seconds. Similar to q = 1, the run times of the algorithm for this case increase, and the variation among them, decreases with an increase in the dimension. However, a particular drawback remains that the run times show sharp increases with the number of dimensions. GHTP. For the GHTP algorithm, the times range from 10\u22121 to 102 seconds. Except for the case of d = 2, overall the run times of the GHTP algorithm lie in the range of 102, i.e., it is fairly similar across the different dimensions and clusters and does not increase with an increase in either. For q = 2, the times range from 102 to 103 seconds. Unlike in the case for q = 1, here, we observe that in general, the times increase with the number of dimensions, with a sharp increase for d = 20. OMP. The run times of the OMP range from 10\u22121 to 102 seconds across different dimensions and clusters. Similar to the maximum volume algorithm, the OMP also shows a gradual increase in the computation times with an increase in the dimensions. Moreover, in general, it fares better than the GHTP, whose run time is at least higher by a factor of 10. For q = 2, we observe that the run times range from 10\u22121 to 102 seconds. Despite the increase in the run times with the number of dimensions, nevertheless, we can conclude that it is still considerably faster than all of the above algorithms. Note that except for d = 20 for all the other dimensions, it is still below 10 seconds, which further reinforces the efficiency of the algorithm. Covariance scenarios. We see that the run times for the covariance scenarios range from 10\u22122 to 101 seconds, which is considerably better than that of all the above algorithms.\n5.2. Portfolio optimization with CVaR constraints. In this section, we discuss an application of the proposed OMP algorithm in finance using portfolio excess return data from Fama-French. The data set contains more than 25 000 daily excess returns of 25 portfolios. An important aspect of portfolio risk management is to address extreme outcomes, wherein investors are concerned by the multivariate nature\nof risk and the scale of the portfolios under consideration. For the purpose of decision-making, any sensible strategy involves dimension reduction and modeling the key features of the overall risk landscape. To this end, we consider here the problem of maximizing expected portfolio returns with expected shortfall constraints as a proxy for the entailing risk. Expected shortfall, also known as conditional value-at-risk (CVaR) is a coherent risk measure that quantifies the tail risk an investment portfolio has, see McNeil et al. [2015]. Precisely, we consider the following optimization problem:\nw\u2217 = argmin w\u2208Rd \u2212w\u22a4\u00b5\nsubject to: CVaR\u03b1 \u2264 \u03b4, w \u2208 W. (26)\nwhere \u00b5 is the expected return of the individual assets, CVaR\u03b1 denotes the conditional value-at-risk, at the \u03b1 level with \u03b1 \u2208 (0, 1), and \u03b4 represents the maximum portfolio loss that is acceptable. Specifically, the CVaR or expected shortfall is the expected loss conditional on the event that the loss exceeds the quantile associated with the probability \u03b1. Further, W can capture constraints like relative portfolio weights, short-sale restrictions, limits on investment in a particular asset, etc. We use the reformulation of the CVaR as in Uryasev and Rockafellar [2001] to find a global solution. While generally for the optimization approach to work, simulations using a Monte-Carlo approach or bootstrapping of historical data are required, we use our generated scenarios, using the OMP, to showcase the applicability in the case of non-smooth optimization as well. To that end, we split the portfolio data set into a training set of 10 000 observations and a testing set of the rest of the observations. We first extract the scenarios and the corresponding probabilities from the training data set using the OMP algorithm, see Algorithm 3. We then solve problem (26) using the scenarios and the approach suggested in Uryasev and Rockafellar [2001], taking the conditional value-at-risk constraint to be at 95% confidence level, the CVaR\u03b1 for \u03b1 = 0.95, 0.98, 0.99 for the returns of the naive portfolio rule w = (1/d)1. We perform back testing using the optimized portfolio weights on the test sample observations. We perform 100 simulations and\nobserve the distribution of the expected daily returns versus the CVaR or the expected shortfall for both the training and testing samples in Figure 7. All the observations are considered as percentages, the color bar indicates the \u03b1 level considered for the optimization problem. The figure shows that the optimized portfolios perform well on the testing set as well, hence, our realized scenarios can still capture the behavior of the underlying asset returns considerably well, even when trying to match moments of non-smooth functions of polynomials."
        },
        {
            "heading": "6. Conclusion and future work",
            "text": "We introduce two algorithms to find scenarios representing large and high-dimensional panel data. The first converts estimated sample covariance matrices to a set of uniformly distributed scenarios that possibly have not been observed before. The second picks a particular subset of realized data points, considering higher-order moment information present in the sample. Both algorithms perform well with respect to computational efficiency and accuracy relative to extant proposals, such as the maximum volume approach by Bittante et al. [2016], greedy hard thresholding, and Lasserre [2010] multivariate Gaussian quadrature in particular in higher dimensions. Our framework allows for extensions with non-uniform weighting of the sample points, and expectations of a bigger class of functions rather than polynomials. We expect this to be beneficial for non-smooth scenario-based problems."
        },
        {
            "heading": "Appendix A. Lasserre\u2019s algorithm",
            "text": "To make the paper self-contained, we include in this section a method to find flat extensions of input moment matrices and Lasserre\u2019s algorithm for scenario extraction cp. [Lasserre, 2010, Chapter 4.3] with a modification for handling the rank-deficient case as well. We note that the multivariate generalization of Prony\u2019s method applied in the context of the reconstruction of measures supported on the unit circle, from its moments up to a certain degree, cp. Kunis et al. [2016], also resembles Lasserre\u2019s algorithm in its prototypical form.\nA.1. Finding flat extensions. Theorem 2.3 reveals that the computation of scenarios can be performed via the flat extension of the moment matrix. Except for specialized cases, when the moment matrix is singular or the associated variety is finite, etc., for the general case when the moment matrix is positive definite, methods to find flat extensions are relatively unexplored. A practical method for finding flat extensions via a semi-definite program (SDP) has been proposed in Helton and Nie [2012]. We describe here a variant that involves solving a sequence of trace minimization problems for finding flat extensions through semidefinite programming (SDP). We start from a rank-r input moment matrix My \u2208 Rmq\u00d7mq with the goal to find its flat extension M y\u0303 \u2208 Rmq+l\u00d7mq+l for some l \u2265 1. Introducing the block matrix representation\nM y\u0303 = [ Ay\u0303 By\u0303 B\u22a4y\u0303 Cy\u0303 ] with\nAy\u0303 \u2208 Rmq+l\u22121\u00d7mq+l\u22121 , By\u0303 \u2208 Rmq+l\u22121\u00d7(mq+l\u2212mq+l\u22121), C y\u0303 \u2208 R(mq+l\u2212mq+l\u22121)\u00d7(mq+l\u2212mq+l\u22121).\nClearly there holds Ay\u0303 =My if y\u0303\u03b1 = y\u03b1 for |\u03b1| \u2264 q. We subsequently solve a convex relaxation of the rank minimization problem by minimizing the trace of the bottom-right block.\nAlgorithm 4 SDP Flat Extension\n(1) Initialize k := 1 (2)\nminimize y\u0303\u2208Rm2(q+k) trCy\u0303 subject to y\u0303\u03b1 = y\u03b1, |\u03b1| \u2264 mq M y\u0303 \u2265 0\n(3) If rankM y\u0303 \u2264 r terminate, otherwise increment k := k + 1 and return to (2)\nHowever, there is no guarantee that this algorithm converges after a finite number of steps. Furthermore, the size of the SDP grows exponentially fast with k, and the termination criterion of the algorithm necessitates the numerical computation of rankM y\u0303, as described below in Algorithm 5.\nA.2. Lasserre\u2019s extraction algorithm specialized to pivoted Cholesky decomposition. Given a flat extensionM y\u0303 ofMy, we consider a modified version of the extraction algorithm suggested in Lasserre [2010]. To handle the rank deficiency of the input moment matrix, we start from the pivoted Cholesky decomposition, cp. Harbrecht et al. [2012], of M y\u0303. Through the permutation matrix P determined from the pivot selection in line 3 of Algorithm 5, the algorithm yields\nPM y\u0303P \u22a4 = PV \u22a4\u039bV P\u22a4 = V\u0303 \u22a4 \u039bV\u0303 = LL\u22a4\nAlgorithm 5 Pivoted Cholesky decomposition (specialization to flat extensions)\ninput: symmetric and positive semidefinite moment matrix M \u2208 Rmq+l\u00d7mq+l tolerance \u03b5 > 0 output: low-rank approximation M \u2248 LL\u22a4 1: Initialization: set m := 1, d := diag(M), L := [ ], err := \u2225d\u22251 2: while err > \u03b5 3: determine j := argmax1\u2264i\u2264mq+\u2113\u22121 di 4: compute\n\u2113m := 1\u221a dj\n( M \u2212LL\u22a4 ) ej\n5: set L := [L, \u2113m], 6: set d := d\u2212 \u2113m \u2299 \u2113m, where \u2299 denotes the Hadamard product 7: set err := \u2225d\u22251 8: set m := m+ 1\nwith V\u0303 := V P\u22a4. The above variant of the pivoted Cholesky algorithm is tailored to the extraction algorithm from [Lasserre, 2010, Section 4.3], as the choice of pivots is restricted to the principal block of size mq \u00d7mq. Note that this amounts to a total pivoting within this block. We remark that L and V\u0303 \u22a4 have the same span, and L can be permuted such that is upper triangular with non-vanishing diagonal entries. In particular, L exhibits a column echelon form, where the upper block is an identity matrix, i.e.\nL\u0303 =  1 . . . 1 \u22c6 \u00b7 \u00b7 \u00b7 \u22c6 ... . . . ...\n\u22c6 \u00b7 \u00b7 \u00b7 \u22c6\n .\nIn complete analogy to Lasserre [2010], there exists now a set of monomials x\u03b1j , j = 1, . . . , r, where the indices \u03b1j correspond to the r indices selected by the pivoted Cholesky decomposition in their order of selection. In particular, there holds now\n(27) P\u03c4\u22a4(\u03bej) = L\u0303w(\u03bej) for j = 1, . . . , r,\nwhere w(x) := [x\u03b11 , . . . ,x\u03b1r ]\u22a4. Next, as the pivots are only selected in the upper mq \u00d7mq block, for each index \u03b1j , the index \u03b1j + ei, i = 1, . . . , d is also contained in \u03c4 . In view of (27), forming for i = 1, . . . , d the submatricesN i containing the rows of L\u0303 corresponding to the indices \u03b1j + ei, leads to d eigenvalue problems\nN iw(x) = xiw(x) for i = 1, . . . , d.\nThe sought scenarios \u03bei are now the joint eigenvalues of these problems, i.e.,"
        },
        {
            "heading": "N iw(\u03bej) = \u03bej,iw(\u03bej) for i = 1, . . . , d, j = 1, . . . , r.",
            "text": "As w(\u03bej) for j = 1, . . . , r are the joint eigenvectors of all multiplication matrices N i, they may now be computed from a random convex combination of N i,\nN = d\u2211 i=1 \u03c1iN i.\nFinally, letting N = QTQ\u22a4 denote the ordered Schur decomposition of N , we arrive at\n\u03bej(i) = q \u22a4 j N iqj for i = 1, . . . , d, j = 1, . . . , r,\nwhere qj is the j-th column of Q. If Algorithm 5 is run with \u03b5 = 0, we retrieve the Vandermonde form of the flat extension M y\u0303 with the preceding procedure. Algorithm 6 summarizes the computational steps. For a given Vandermonde decomposition V \u22a4(\u03be1, . . . , \u03ber)\u039bV (\u03be1, . . . , \u03ber) with scenarios \u03be1, . . . , \u03ber, it remains to compute their probabilities \u03bb1, . . . , \u03bbr in accordance with the input moment matrix My.\nAlgorithm 6 Lasserre\u2019s extraction algorithm specialized to pivoted Cholesky decomposition\ninput: symmetric and positive semi-definite moment matrix My \u2208 Rmq\u00d7mq , rank My = r\noutput: r scenarios \u039e := { \u03be1, \u00b7 \u00b7 \u00b7 , \u03ber } 1: Perform pivoted Cholesky decomposition to obtain\nPMyP \u22a4 = PV \u22a4\u039bV P\u22a4 = V\u0303 \u22a4 \u039bV\u0303 = LL\u22a4\n2: Reduce L to an echelon form L\u0303. 3: Extract from L\u0303 the multiplication matrices N i, i = 1, \u00b7 \u00b7 \u00b7 , d. 4: Compute N := \u2211d i=1 \u03c1iN i with random convex combination.\n5: Compute the Schur decomposition N = QTQ\u22a4 with Q = [ q1 . . . qr ] . 6: Extract \u03bej(i) = q\u22a4j N iqj , i = 1, . . . , d, j = 1, . . . , r."
        },
        {
            "heading": "Appendix B. Greedy maximum-volume scenarios",
            "text": "A quasi-Monte Carlo method to construct good quadrature rules, with a small number of nodes, to approximate integrals is the so-called Approximate Fekete points (AFP), cf. Bos et al. [2011], Sommariva and Vianello [2009], Bittante et al. [2016]. The computation of these points, which give us the scenarios from (14) is done via a greedy selection of the maximum volume submatrix of the transpose of the Vandermonde matrix. It turns out to be numerically equivalent to a column-pivoted QR factorization of V \u22a4, cp. Bittante et al. [2016], Bos et al. [2010, 2011], Golub and Van Loan [2013], Sommariva and Vianello [2009] which tries to give a computational solution to the basis choice problem. A natural application of the AFP is the construction of quadrature rules. We use this notion to obtain the indices for the sparse scenarios from the choice of the column indices of V \u22a4. An important fact is that this algorithm is dependent on the polynomial basis chosen. Since Vandermonde matrices, as constructed from the usual monomial basis, are susceptible to being ill-conditioned, an iterative refinement based on successive changes of the basis by QR decomposition has been suggested to improve numerical stability, cp. Bos et al. [2011], Sommariva and Vianello [2009]. This amounts to a change of the monomial basis to an orthonormal basis with respect to the L2\nP\u0302 -inner product on P2q, i.e.,\n\u27e8p1, p2\u27e9L2 P\u0302 := \u222b Rd p1p2 dP\u0302 for any p1, p2 \u2208 P2q.\nThe set \u039e is now given by the pivot columns selected by a column-pivoted QR factorization. The procedure is summarized in Algorithm 7. The core of the algorithm is given lines 3-5 and can be efficiently\nAlgorithm 7 Maximum volume scenarios\ninput: Matrix Q \u2208 RN\u00d7r output: index set ind corresponding to the sparse scenarios 1: Initialization: set W := Q\u22a4, wm := \u2225Wm\u222522 for m \u2208 {1, . . . , N}, ind := [ ] 2: for k = 1, . . . , r (greedy algorithm for maximum volume submatrix) 3: set mk := argmax{wm : m /\u2208 ind} 4: set ind := [ind, mk] 5: set wm := \u2225Wm\u222522 \u2212 (W\u22a4mWmk) 2\n\u2225Wmk\u222522 for m /\u2208 ind\n6: end\nimplemented by the QR factorization with column pivoting, see Engler [1997]. We remark that we can take the number of iterations to be less than r in line 2 of Algorithm (7) for a prior estimated sparsity level of the number of scenarios."
        },
        {
            "heading": "Appendix C. Greedy thresholding method",
            "text": "We discuss herein an iterative greedy method that is used to solve problems of the form (14). The algorithm called the hard thresholding pursuit algorithm (HTP), is popular in compressive sensing literature,\nsee Foucart and Rauhut [2013], Foucart [2011]. It performs sparse recovery in a stable and robust manner, and in particular, it is used to obtain sparse solutions to linear underdetermined systems. The HTP is a combination of greedy and thresholding methods, namely the compressive sampling matching pursuit (CoSaMP) and the iterative hard thresholding pursuit (IHT) respectively, see Foucart and Rauhut [2013]. Unlike the IHT which does not require the computation of an orthogonal projection, the HTP, in fact, has an additional step that computes a vector with the same support as the iterate that best fits the measurement. We apply a variant of the HTP, called the graded hard thresholding pursuit (GHTP), cp. Bouchot et al. [2016], where the size of the index set increases with the number of iterations. The stopping criterion for the GHTP algorithm requires a prior estimation of either the sparsity level or the error level, cp. Bouchot et al. [2016]. We can, therefore, either set the maximum number of iterations as proportional to the sparsity level or run the iteration with an input tolerance level that is some factor times the error level. In our numerical experiments, we take both into consideration for the loop. Herein, we treat the maximum number of iterations as a hyper-parameter, which has to be validated. We finally put the steps of GHTP applied to Q\u22a4 and y\u0303 in Algorithm 8. Considering the\nAlgorithm 8 Graded hard thresholding pursuit\ninput: matrix A \u2208 Rm\u00d7n, vector y \u2208 Rm, tolerance \u03b5 > 0, and maximum number of iterations J output: index set ind corresponding to a sparse solution of min x\n\u2225Ax\u2212 y\u22252 1: Initialization: set S0 := \u2205, x0 := 0, ind := [ ] 2: while err > \u03b5 and k < J 3: set Sk := index set of k largest absolute entries of xk\u22121 \u2212A\u22a4(Axk\u22121 \u2212 y) 4: set xk := argmin { \u2225Az \u2212 y\u22252, supp(z) \u2286 Sk\n} 5: set err := \u2225Axk \u2212 y\u22252 6: set k := k + 1 7: end 8: ind = indices of non-zero components of xk\nsquare system QQ\u22a4w = Qy\u0303, Algorithm 8 iteratively computes a sparse solution of the above system using the fixed-point equation, see Foucart and Rauhut [2013], Foucart [2011] with\nw = (I \u2212QQ\u22a4)w +Qy\u0303.\nas in Line 3. Since the target is to obtain a sparse solution, hence it considers the k largest absolute entries as in Line 3, according to the iteration count k, which is different from the usual HTP. Finally, the algorithm computes the orthogonal projection step, as in Line 4, which is the most costly step of the algorithm.\nMichael Multerer, Euler Institute, USI Lugano, Via la Santa 1, 6962 Lugano, Svizzera.\nEmail address: michael.multerer@usi.ch\nPaul Schneider, USI Lugano and SFI, Via Buffi 6, 6900 Lugano, Svizzera.\nEmail address: paul.schneider@usi.ch\nRohan Sen, Euler Institute, USI Lugano, Via la Santa 1, 6962 Lugano, Svizzera.\nEmail address: rohan.sen@usi.ch"
        }
    ],
    "title": "FAST EMPIRICAL SCENARIOS",
    "year": 2023
}