{
    "abstractText": "Federated learning allows clients to collaboratively train a global model without uploading raw data for privacy preservation. This feature, i.e., the inability to review participants\u2019 datasets, has recently been found responsible for federated learning\u2019s vulnerability in the face of backdoor attacks. Existing defense methods fall short from two perspectives: 1) they consider only very specific and limited attacker models and thus are unable to cope with advanced backdoor attacks, such as distributed backdoor attacks, which break down the global trigger into multiple distributed triggers. 2) they conduct detection based on model granularity thus their performance gets impacted by the model size. To address these challenges, we propose Federated Layer Detection (FLD), a novel model filtering approach to effectively defend against backdoor attacks. FLD examines the models on layer granularity to capture the complete model details and effectively detect potential backdoor models regardless of model size. We provide theoretical analysis and proof for the convergence of FLD. Extensive experiments demonstrate that FLD effectively mitigates state-of-the-art (SOTA) backdoor attacks with negligible impact on the accuracy of the primary task, outperforming SOTA defense methods. Impact Statement\u2014Federated learning (FL), which enables distributed clients to collaboratively train a global model without data centralization, preserves user privacy and has become increasingly prominent. Despite its advancement, FL\u2019s vulnerability to adversarial attacks, particularly backdoor threats, has become a critical concern. In FL, the aggregation server does not have direct access to the data, which makes it harder to find these sneaky attacks. This makes it very hard to keep the model\u2019s integrity without losing accuracy. To address this, we introduce Federation Layer Detection (FLD), an innovative model filtering approach designed to counteract backdoor attacks in FL. Our experiments across various datasets demonstrate that FLD effectively mitigates such attacks with minimal impact on the main task\u2019s performance, highlighting its potential as an effective defense mechanism in secure FL systems.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yihang Lin"
        },
        {
            "affiliations": [],
            "name": "Zhiqian Wu"
        },
        {
            "affiliations": [],
            "name": "Yong Liao"
        },
        {
            "affiliations": [],
            "name": "Pengyuan Zhou"
        }
    ],
    "id": "SP:0615d5171e125f501eee54993c6ad4dec3df7595",
    "references": [
        {
            "authors": [
                "P. Voigt",
                "A. Von dem Bussche"
            ],
            "title": "The eu general data protection regulation (gdpr),",
            "venue": "A Practical Guide, 1st Ed., Cham: Springer International Publishing,",
            "year": 2017
        },
        {
            "authors": [
                "L. S"
            ],
            "title": "Pardau, \u201cThe california consumer privacy act: Towards a europeanstyle privacy regime in the united states,",
            "venue": "J. Tech. L. & Pol\u2019y, vol. 23,",
            "year": 2018
        },
        {
            "authors": [
                "S. Chesterman"
            ],
            "title": "After privacy: The rise of facebook",
            "venue": "the fall of wikileaks, and singapore\u2019s personal data protection act 2012,\u201d Sing. J. Legal Stud., p. 391",
            "year": 2012
        },
        {
            "authors": [
                "B. McMahan",
                "E. Moore",
                "D. Ramage",
                "S. Hampson"
            ],
            "title": "and B",
            "venue": "A. y Arcas, \u201cCommunication-efficient learning of deep networks from decentralized data,\u201d in Artificial intelligence and statistics. PMLR",
            "year": 2017
        },
        {
            "authors": [
                "T. Gu",
                "K. Liu",
                "B. Dolan-Gavitt"
            ],
            "title": "and S",
            "venue": "Garg, \u201cBadnets: Evaluating backdooring attacks on deep neural networks,\u201d IEEE Access, vol. 7, pp. 47 230\u201347 244",
            "year": 2019
        },
        {
            "authors": [
                "M. Fang",
                "X. Cao",
                "J. Jia"
            ],
            "title": "and N",
            "venue": "Gong, \u201cLocal model poisoning attacks to {Byzantine-Robust} federated learning,\u201d in 29th USENIX Security Symposium (USENIX Security 20)",
            "year": 2020
        },
        {
            "authors": [
                "Y. Li",
                "Y. Jiang"
            ],
            "title": "Z",
            "venue": "Li, and S.-T. Xia, \u201cBackdoor learning: A survey,\u201d IEEE Transactions on Neural Networks and Learning Systems",
            "year": 2022
        },
        {
            "authors": [
                "C. Xie",
                "K. Huang",
                "P.-Y. Chen"
            ],
            "title": "and B",
            "venue": "Li, \u201cDba: Distributed backdoor attacks against federated learning,\u201d in International Conference on Learning Representations",
            "year": 2020
        },
        {
            "authors": [
                "M. Baruch",
                "G. Baruch"
            ],
            "title": "and Y",
            "venue": "Goldberg, \u201cA little is enough: Circumventing defenses for distributed learning,\u201d in NeurIPS",
            "year": 2019
        },
        {
            "authors": [
                "J. Tang",
                "Z. Chen",
                "A.W.-C. Fu"
            ],
            "title": "and D",
            "venue": "W. Cheung, \u201cEnhancing effectiveness of outlier detections for low density patterns,\u201d in Advances in Knowledge Discovery and Data Mining: 6th Pacific-Asia Conference, PAKDD 2002 Taipei, Taiwan, May 6\u20138, 2002 Proceedings 6. Springer",
            "year": 2002
        },
        {
            "authors": [
                "T. Crosby"
            ],
            "title": "How to detect and handle outliers,",
            "year": 1994
        },
        {
            "authors": [
                "B. McMahan",
                "E. Moore",
                "D. Ramage",
                "S. Hampson"
            ],
            "title": "and B",
            "venue": "A. y Arcas, \u201cCommunication-efficient learning of deep networks from decentralized data,\u201d in Artificial intelligence and statistics. PMLR",
            "year": 2017
        },
        {
            "authors": [
                "E. Bagdasaryan",
                "A. Veit",
                "Y. Hua",
                "D. Estrin"
            ],
            "title": "and V",
            "venue": "Shmatikov, \u201cHow to backdoor federated learning,\u201d in International Conference on Artificial Intelligence and Statistics. PMLR",
            "year": 2020
        },
        {
            "authors": [
                "P. Blanchard",
                "E.M. El Mhamdi",
                "R. Guerraoui"
            ],
            "title": "and J",
            "venue": "Stainer, \u201cMachine learning with adversaries: Byzantine tolerant gradient descent,\u201d Advances in Neural Information Processing Systems, vol. 30",
            "year": 2017
        },
        {
            "authors": [
                "P. Kairouz",
                "H.B. McMahan",
                "B. Avent",
                "A. Bellet",
                "M. Bennis",
                "A.N. Bhagoji",
                "K. Bonawitz",
                "Z. Charles",
                "G. Cormode"
            ],
            "title": "R",
            "venue": "Cummings et al., \u201cAdvances and open problems in federated learning,\u201d Foundations and Trends\u00ae in Machine Learning, vol. 14, no. 1\u20132, pp. 1\u2013210",
            "year": 2021
        },
        {
            "authors": [
                "C. Fung",
                "C.J.M. Yoon"
            ],
            "title": "and I",
            "venue": "Beschastnikh, \u201cThe Limitations of Federated Learning in Sybil Settings,\u201d in Symposium on Research in Attacks, Intrusion, and Defenses, ser. RAID",
            "year": 2020
        },
        {
            "authors": [
                "T.D. Nguyen",
                "P. Rieger",
                "R. De Viti",
                "H. Chen",
                "B.B. Brandenburg",
                "H. Yalame",
                "H. M\u00f6llering",
                "H. Fereidooni",
                "S. Marchal"
            ],
            "title": "M",
            "venue": "Miettinen et al., \u201c{FLAME}: Taming backdoors in federated learning,\u201d in 31st USENIX Security Symposium (USENIX Security 22)",
            "year": 2022
        },
        {
            "authors": [
                "H.B. McMahan",
                "D. Ramage",
                "K. Talwar"
            ],
            "title": "and L",
            "venue": "Zhang, \u201cLearning differentially private recurrent language models,\u201d arXiv preprint arXiv:1710.06963",
            "year": 2017
        },
        {
            "authors": [
                "K. Pillutla",
                "S.M. Kakade"
            ],
            "title": "and Z",
            "venue": "Harchaoui, \u201cRobust Aggregation for Federated Learning,\u201d IEEE Transactions on Signal Processing, vol. 70, pp. 1142\u20131154",
            "year": 2022
        },
        {
            "authors": [
                "C. Xie",
                "O. Koyejo"
            ],
            "title": "and I",
            "venue": "Gupta, \u201cGeneralized byzantine-tolerant sgd,\u201d arXiv preprint arXiv:1802.10116",
            "year": 2018
        },
        {
            "authors": [
                "S.U. Stich",
                "J.-B. Cordonnier"
            ],
            "title": "and M",
            "venue": "Jaggi, \u201cSparsified sgd with memory,\u201d Advances in Neural Information Processing Systems, vol. 31",
            "year": 2018
        },
        {
            "authors": [
                "H. Yu",
                "S. Yang"
            ],
            "title": "and S",
            "venue": "Zhu, \u201cParallel restarted sgd with faster convergence and less communication: Demystifying why model averaging works for deep learning,\u201d in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 33, no. 01",
            "year": 2019
        },
        {
            "authors": [
                "X. Li",
                "K. Huang",
                "W. Yang",
                "S. Wang"
            ],
            "title": "and Z",
            "venue": "Zhang, \u201cOn the convergence of fedavg on non-iid data,\u201d arXiv preprint arXiv:1907.02189",
            "year": 2019
        },
        {
            "authors": [
                "L. Mu\u00f1oz-Gonz\u00e1lez",
                "K.T. Co"
            ],
            "title": "and E",
            "venue": "C. Lupu, \u201cByzantine-robust federated machine learning through adaptive model averaging,\u201d arXiv preprint arXiv:1909.05125",
            "year": 2019
        },
        {
            "authors": [
                "T.D. Nguyen",
                "P. Rieger"
            ],
            "title": "M",
            "venue": "Miettinen, and A.-R. Sadeghi, \u201cPoisoning attacks on federated learning-based iot intrusion detection system,\u201d in Proc. Workshop Decentralized IoT Syst. Secur.(DISS)",
            "year": 2020
        },
        {
            "authors": [
                "X. Gong",
                "Y. Chen",
                "Q. Wang",
                "W. Kong"
            ],
            "title": "Backdoor attacks and defenses in federated learning: State-of-the-art",
            "venue": "taxonomy, and future directions,\u201d IEEE Wireless Communications",
            "year": 2022
        },
        {
            "authors": [
                "R. Guerraoui"
            ],
            "title": "S",
            "venue": "Rouault et al., \u201cThe hidden vulnerability of distributed learning in byzantium,\u201d in International Conference on Machine Learning. PMLR",
            "year": 2018
        },
        {
            "authors": [
                "Z. Sun",
                "P. Kairouz",
                "A.T. Suresh"
            ],
            "title": "and H",
            "venue": "B. McMahan, \u201cCan you really backdoor federated learning?\u201d arXiv preprint arXiv:1911.07963",
            "year": 2019
        },
        {
            "authors": [
                "M. Liu",
                "J. Shi",
                "Z. Li",
                "C. Li",
                "J. Zhu"
            ],
            "title": "and S",
            "venue": "Liu, \u201cTowards better analysis of deep convolutional neural networks,\u201d IEEE transactions on visualization and computer graphics, vol. 23, no. 1, pp. 91\u2013100",
            "year": 2016
        },
        {
            "authors": [
                "F. Pukelsheim"
            ],
            "title": "The three sigma rule,",
            "venue": "The American Statistician,",
            "year": 1994
        },
        {
            "authors": [
                "D.G. Zill"
            ],
            "title": "Advanced engineering mathematics",
            "venue": "Jones & Bartlett Publishers",
            "year": 2020
        },
        {
            "authors": [
                "X. Yi",
                "R. Paulet",
                "E. Bertino",
                "X. Yi",
                "R. Paulet",
                "E. Bertino"
            ],
            "title": "Homomorphic encryption",
            "venue": "Springer",
            "year": 2014
        },
        {
            "authors": [
                "O. Goldreich"
            ],
            "title": "Secure multi-party computation,",
            "venue": "Manuscript. Preliminary version,",
            "year": 1998
        },
        {
            "authors": [
                "X. Liu",
                "H. Li",
                "G. Xu",
                "Z. Chen",
                "X. Huang"
            ],
            "title": "and R",
            "venue": "Lu, \u201cPrivacy-enhanced federated learning against poisoning adversaries,\u201d IEEE Transactions on Information Forensics and Security, vol. 16, pp. 4574\u20134588",
            "year": 2021
        },
        {
            "authors": [
                "Q. Li",
                "Y. Diao",
                "Q. Chen"
            ],
            "title": "and B",
            "venue": "He, \u201cFederated learning on non-iid data silos: An experimental study,\u201d in 2022 IEEE 38th International Conference on Data Engineering (ICDE). IEEE",
            "year": 2022
        },
        {
            "authors": [
                "H. Wang",
                "K. Sreenivasan",
                "S. Rajput",
                "H. Vishwakarma"
            ],
            "title": "S",
            "venue": "Agarwal, J.y. Sohn, K. Lee, and D. Papailiopoulos, \u201cAttack of the tails: Yes, you really can backdoor federated learning,\u201d Advances in Neural Information Processing Systems, vol. 33, pp. 16 070\u201316 084",
            "year": 2020
        },
        {
            "authors": [
                "V. Tolpegin",
                "S. Truex",
                "M.E. Gursoy"
            ],
            "title": "and L",
            "venue": "Liu, \u201cData poisoning attacks against federated learning systems,\u201d in European Symposium on Research in Computer Security. Springer",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "To address these challenges, we propose Federated Layer Detection (FLD), a novel model filtering approach to effectively defend against backdoor attacks. FLD examines the models on layer granularity to capture the complete model details and effectively detect potential backdoor models regardless of model size. We provide theoretical analysis and proof for the convergence of FLD. Extensive experiments demonstrate that FLD effectively mitigates state-of-the-art (SOTA) backdoor attacks with negligible impact on the accuracy of the primary task, outperforming SOTA defense methods.\nImpact Statement\u2014Federated learning (FL), which enables distributed clients to collaboratively train a global model without data centralization, preserves user privacy and has become increasingly prominent. Despite its advancement, FL\u2019s vulnerability to adversarial attacks, particularly backdoor threats, has become a critical concern. In FL, the aggregation server does not have direct access to the data, which makes it harder to find these sneaky attacks. This makes it very hard to keep the model\u2019s integrity without losing accuracy. To address this, we introduce Federation Layer Detection (FLD), an innovative model filtering approach designed to counteract backdoor attacks in FL. Our experiments across various datasets demonstrate that FLD effectively mitigates such attacks with minimal impact on the main task\u2019s performance, highlighting its potential as an effective defense mechanism in secure FL systems.\nIndex Terms\u2014Federated learning; backdoor attack; backdoor defense\nI. INTRODUCTION\nThe rapidly developing artificial intelligence technologies have been applied to numerous fields, such as computer vision, natural language processing, data mining, and so on. Many AI services are supported by cloud services to collect big data from large numbers of distributed users. However, privacy protection regulations released in recent years, such as GDPR [1], CCPA [2] and PDPA [3], have presented serious challenges for user data collection. Therefore, privacy-preserving data analysis technologies are critical.\nFederated learning [4], proposed by Google in 2016, allows distributed clients to jointly train a global model without\nuploading user private data and thus has attracted wide attention. In federated learning, a central parameter server sends an initial global model to the clients. The clients then train local models using local datasets and send the updated model parameters to the server after training. The server updates the global model via parameter aggregation and distributes the updated global model to the participants in the next round. The process iterates until the model is convergent or the predefined period ends.\nAs a trade-off, federated learning limits the central server\u2019s access to users\u2019 data. However, this feature, as uncovered by recent studies, makes federated learning vulnerable to adversarial attacks, especially backdoor attacks [5], [6]. Backdoor attacks embed backdoor into the model without impacting the primary task performance, thus being much more stealthy than other attacks such as untargeted poisoning attacks [7].\nThe defenses against centralized backdoor attacks fall into three broad categories: pre-training, during-training, and posttraining [8]. Pre-training requires pre-processing of training data and post-training requires model fine-tuning using users\u2019 datasets, both of which require access to local data and thus cannot be applied to federated learning. As a result, backdoor defenses for federated learning mostly use robustness aggregation or differential privacy perturbation during the training stage. However, the state-of-the-art (SOTA) defense methods are either only applicable to independent and identically distributed (i.i.d) datasets, or can only defend against conventional backdoor attacks but fail at defending SOTA attacks like DBA [9] and A Little Is Enough [10].\nTo address these challenges, we propose Federated Layer Detection (FLD), which measures the fine-grained layer-level differences across models. FLD consists of two major modules, namely Layer Scoring and Anomaly Detection, which can reliably identify and remove malicious clients to guarantee system performance. Layer Scoring assigns an outlier score to each layer of the models uploaded by the clients based on the concept of density instead of commonly used distance which sometimes causes wrong detection results [11]. Anomaly Detection uses the median absolute deviation (MAD) [12] of layer scores to determine if a model is anomalous. FLD overcomes three major limitations of existing defense methods, namely, simplified assumptions of data distribution, degraded accuracy of the primary task, and only functioning against specific backdoor attacks. Our contributions are mainly threefold, as follows: \u2022 We propose FLD, an innovative backdoor defense scheme\nfor federated learning. FLD is the first fine-grained defense scheme that assesses models based on layer level, to our\nar X\niv :2\n30 3.\n00 30\n2v 2\n[ cs\n.L G\n] 1\n8 D\nec 2\n02 3\n2 best knowledge. \u2022 Layer Scoring module captures the fine-grained model\ndetails to improve the generalizability of FLD to deeper models. Anomaly Detection employs MAD to avoid impactful mean shifts caused by extreme outlier scores of anomalous models. As such, FLD can effectively detect potential backdoored models regardless of model size. \u2022 We theoretically prove the convergence guarantee of FLD in both i.i.d and non-i.i.d data distributions. We also prove the correctness of FLD in homomorphic encryption scenarios. \u2022 Extensive experiments on several well-known datasets can show that FLD effectively defends against a wide range of SOTA federated backdoor attacks in different scenarios without compromising the accuracy of the primary task, demonstrating FLD\u2019s robustness and generalizability."
        },
        {
            "heading": "II. RELATED WORK",
            "text": ""
        },
        {
            "heading": "A. Federated Learning",
            "text": "Federated learning is a popular strategy for training models on distributed data while preserving client privacy. Assuming there are N clients, C = {C1, C2, C3, \u00b7 \u00b7 \u00b7 , CN}, each of which has a local training dataset Di, i \u2208 {1, \u00b7 \u00b7 \u00b7 , N}, that can communicate with the central parameter server to collaboratively train a global model. The standard federated learning process generally follows these phases: 1) In round t, the central parameter server randomly selects\nn clients that satisfy predefined conditions to participate in the training and sends these clients the latest global model Gt\u22121. 2) Each selected client Ci executes \u03c4i iterations to update its local model: wti = w t\u22121 i \u2212 \u03b7tigi ( wt\u22121i ; \u03be t\u22121 i,k ) , where \u03b7ti is\nthe learning rate, \u03bet\u22121i,k is a batch of uniformly chosen data samples. After the local training, Ci sends updated wti to the central parameter server. 3) The parameter server aggregates the received models to update the global model. We choose the classical FedAvg algorithm [13] for aggregation: Gt = \u2211n i=1 mi m w t i , where\nmi = \u2225Di\u2225 ,m = \u2211n\ni=1 mi. Previous works [14], [9], [15] typically used the same weights ( mi m = 1 n ) to average client contributions. For simplicity, we follow this setup, i.e., Gt = \u2211n i=1 1 nw t i ."
        },
        {
            "heading": "B. Backdoor Attacks",
            "text": "Backdoor attacks impact the model training with \u201ccarefully crafted\u201d poisoned data (mixing triggers into a small portion of data) to get a backdoor-poisoned model. The corrupted model outputs original labels for clean samples but target labels for poisoned samples. Thus, it does not impact primary task performance and is difficult to detect. Besides conventional centralized backdoor attacks, distributed backdoor attacks have emerged in recent years. A formal description of the attacker\u2019s purpose is\n\u2200 (x, y) \u2208 D, f (G, x) = y \u2227 f (G, x\u2217) = ybackdoor, (1)\nwhere D is the clean dateset, ybackdoor is the attacker\u2019s target label, x\u2217 is the clean sample x combined with backdoor\ntriggers. Backdoor Attacks on Federated Learning can be divided into two categories: data poisoning and model poisoning [16]: \u2022 Data poisoning: The attacker can only modify the dataset\nof the compromised clients but cannot control their training process or modify the data uploaded by the clients to the parameter server. The common methods are label flipping (e.g., making that picture of a cat labeled as a dog) and adding triggers to the image samples (e.g., adding a hat to the face images). To avoid being detected, the attackers often control the Poisoned Data Rate (PDR) to restrict the poisoned model\u2019s deviation from the benign model. Let Di denote the poisoned training dataset of the compromised client i and Dpoii denote the poisoned data, then the PDR of Di is\nPDR = \u2223\u2223\u2223Dpoii \u2223\u2223\u2223 |Di| . (2)\n\u2022 Model poisoning is more powerful than data poisoning because the attacker can manipulate the compromised client\u2019s training and directly modify their uploaded data to maximize the impact, driving the global model closer to the backdoor model without being noticed by the anomaly detection mechanism running in the parameter server. A classic model poisoning attack is the model-replacement attack [14] which scales the uploaded model parameters. Model poisoning attacks can be divided into two types, namely scaling and evasion. Scaling drives the global model close to the backdoor model by scaling up the weights of the uploaded model. Evasion constrains model variation during training to reduce the malicious model\u2019s deviation from the benign model in order to avoid anomaly detection. A common method is to modify the target loss function by adding an anomaly detection term Lano as follows:\nLmodel = \u03b1Lclass + (1\u2212 \u03b1)Lano, (3)\nwhere Lclass captures the accuracy of the primary and backdoor tasks, the hyperparameter \u03b1 controls the importance of evading anomaly detection. Lano functions as the anomaly detection penalty, e.g., the Euclidean distance between the local models and global model. Model poisoning can directly control the training process and modify the model weights. Therefore, model poisoning can bypass the anomaly detection mechanism and robust aggregation mechanism deployed on the parameter server. It scales the model weights to satisfy the bound defined by the anomaly detection mechanism."
        },
        {
            "heading": "C. Backdoor Defenses",
            "text": "Works on defending against backdoor attacks in federated learning can be broadly categorized into two directions: robust aggregation and anomaly model detection. Robust aggregation optimizes the aggregation function to mitigate the impact of contaminated updates sent by attackers. One common approach is to apply a threshold to limit the impact of updates from all clients on the global model, such as by constraining the l2 norm of the updates (referred to as clipping) [14],\n3 [17], [18], [19]. Other approaches explore new global model estimators, like Robust Federated Averaging (RFA) [20] and Trimmed Mean [21], to enhance the robustness of the aggregation process. However, a major drawback of these approaches is that contaminated updates may still persist in the global model, resulting in reduced model accuracy and incomplete mitigation of backdoor effects. Additionally, applying update constraints to all clients, including benign ones, reduces the magnitude of updates and thus slows down the convergence. The second direction, anomaly model detection, aims to identify and remove malicious updates from the aggregation process. This task is challenging due to the non-independent and heterogeneous distribution of client data and the uncertainty of the number of malicious clients. Previous methods have typically utilized clustering directly for detection."
        },
        {
            "heading": "III. PROBLEM SETUP",
            "text": "First, we describe the assumptions behind the convergence analysis of FLD, then we introduce the concepts that are vital to the algorithm design and analysis.\nLet Fi denotes the local model of the i-th client, i = 1, 2, \u00b7 \u00b7 \u00b7 , N . Let F denotes the global model in the central parameter server. Suppose our models satisfy Lipschitz continuous gradient, we make Assumptions 1 and 2.\nAssumption 1. (L-smooth). F1, \u00b7 \u00b7 \u00b7 , FN are all L-smooth:\n\u2200x, y, Fi(x) \u2264 Fi(y) + (x\u2212 y)T\u2207Fi(y) + L\n2 ||x\u2212 y||22.\nAssumption 2. (\u00b5-strongly convex). F1, \u00b7 \u00b7 \u00b7 , FN are all \u00b5strongly convex:\n\u2200x, y, Fi(x) \u2265 Fi(y) + (x\u2212 y)T\u2207Fi(y) + \u00b5\n2 ||x\u2212 y||22.\nWe also follow the assumption made by [22], [23], [24] as follows.\nAssumption 3. The expected squared norm of stochastic gradients is uniformly bounded, i.e., \u2203U > 0, E||\u2207Fi(\u00b7)||2 \u2264 U2 for all i = 1, \u00b7 \u00b7 \u00b7 , N .\nWe make Assumption 4 to bound the expectation of ||wti ||2, where wti denotes the parameters of Fi in t-round.\nAssumption 4. (Bounding the expectation of ||wti ||2). The expected squared norm of i-th client\u2019s local model parameters is bounded: \u2203M > 0, E||wti ||2 \u2264 M2 for all i = 1, \u00b7 \u00b7 \u00b7 , N and t = 1, \u00b7 \u00b7 \u00b7 , T .\nAdversary Capability. Following previous works [14], [15], [25], [26], we assume the attacker controls a portion (less than 50%) of the clients, called \u201ccompromised clients\u201d. We assume that the attacker can possess the strongest attack capability, i.e., both data poisoning and model poisoning. The attacker can send arbitrary gradient contributions to the aggregator in any iteration according to its observation of the global model state. Compromised clients can collude in an intrinsic and coordinated fashion by sharing states and updates with each other. The attacker has no control over the benign clients or the server\u2019s aggregation process.\nAdversarial Target. To ensure the effectiveness of the attack, the attack should: 1) be stealthy, i.e., injecting the backdoor should not lead to accuracy fluctuation of the model\u2019s primary task 2) have a high attack success rate (ASR), i.e., the success rate that the model identifies the samples containing backdoor triggers as target label should be as high as possible.\nThe adversarial objective of the compromised client i in round t can be denoted as follows:\nwti =argmax wi\n( \u2211\nj\u2208Dpoii\nP [Gt+1(R(xij , \u03d5)) = ybackdoor]+\n\u2211 j\u2208Dclei P [Gt+1(xij) = y i j ]).\n(4)\nwhere Dpoii and D cle i denote the poisoned dataset and clean dataset respectively, Dpoii \u2229Dclei = \u2205 and D poi i \u222aDclei = Di. Function R transforms clean data xij into poisoned data R(xij , \u03d5) by embedding the trigger \u03d5. An attacker trains its local model to find the optimal parameters wti so that G t+1 identifies poisoned data R(xij , \u03d5) as backdoor target label ybackdoor and identifies clean data xij as ground truth label yij . Defense Goal.Our defense goal is to mitigate backdoor attacks in federated learning. Specifically, an efficient defense algorithm needs to: 1) ensure the performance of the global model in terms of the main task\u2019s accuracy, 2) minimize the possibility of outputting backdoor target labels, and 3) defend against a wide range of SOTA federated backdoor attacks without prior knowledge of the proportion of compromised clients."
        },
        {
            "heading": "IV. METHODOLOGY",
            "text": ""
        },
        {
            "heading": "A. Motivation",
            "text": "According to the attack analysis in constrain-and-scale [14], the federated backdoor attacks are divided into two attack scenarios: single-shot attacks (Attack A-S) and multi-shot attacks (Attack A-M) [9], [27]. Attack A-S: the attacker successfully embeds its backdoor trigger in only one round. The attacker performs parameter scaling on the compromised clients\u2019 updates to substitute the global model Gt with a backdoor model X in Equation 5:\nX = n\u2211 i=1 1 n wti . (5)\nTo achieve this goal, the attacker can scale the model parameters as follows:\nw\u0303tn = nX \u2212 n\u22121\u2211 i=1 wti \u2248 nX \u2212 n\u22121\u2211 i=1 Gt\u22121\n= n ( X \u2212Gt\u22121 ) +Gt\u22121.\n(6)\nAs the global model converges, wti \u2248 Gt\u22121. In other words, the attacker scales up the model weights X by n to prevent the malicious updates from being mitigated by the aggregation. Attack A-M lets compromised clients accumulate malicious updates over multiple rounds, instead of directly scaling the uploaded parameters, to avoid being detected by the defense\nalgorithm. We have thoroughly reviewed the SOTA backdoor defense works in federated learning [17], [15], [21], [28], [20], [29] and found that existing methods focus on defending against Attack A-S while overlooking Attack A-M. Unsurprisingly, we found through empirical experiments that SOTA defense algorithms fail at defending against Attack A-M.\nTo address the challenges mentioned above, we propose Federated Layer Detection (FLD), an innovative defense method for effectively detecting backdoor attacks in federated learning that overcomes the deficiencies of previous works. As depicted in Fig. 1, FLD consists of two components, namely Layer Scoring and Anomaly Detection. Layer Scoring assigns scores to the local models uploaded by the clients according to the concept of isolation. Anomaly Detection checks the outlier scores given by Layer Scoring to determine if an uploaded model is compromised. The overall process is as follows: 1) The server receives the local models from the clients participating in the current round. 2) The server assigns each layer of each model an outlier score using Layer Scoring. 3) The server labels each layer as abnormal or not according to its outlier score, and excludes from the aggregation the anomalous models that contain more anomalous layers than the predefined threshold, as summarized in Algorithm 1."
        },
        {
            "heading": "B. Layer Scoring",
            "text": "In round t, the parameter server sends the global model Gt\u22121 to the selected clients i \u2208 Ct, each of which trains Gt\u22121 using its local data Di and sends the model parameters wti back to the server after local training is completed.\nExisting backdoor defense methods assess uploaded models based on either similarity or distance metrics. They usually flatten the parameters of each model layer and then stitch them into a vector to perform the assessment. However, different layers of the neural network have heterogeneous parameter value distribution spaces due to their different functions. For example, in a CNN, the lower layers learn to detect simple features such as stripes, the middle layers learn to detect a part of an object, and the higher layers learn to detect a concept (e.g., a person) [30]. As a result, directly flattening and splicing the parameters of each layer easily leads to the loss of important information and hence the escape of malicious models. Therefore, finer-grained detection is demanded.\nAlgorithm 1 Overview Input: Set of clients C = {C1, C2, C3, \u00b7 \u00b7 \u00b7 , CN}, local\ndatasets D = {D1, D2, D3, \u00b7 \u00b7 \u00b7 , DN}, the number of training iterations T , the percentage of participating clients per round K.\nOutput: Global model GT 1: Initialize the global model G0\n2: for t in [1, T ] do 3: n\u2190 max (K \u00b7N, 1) 4: Ct \u2190 (random set of n clients ) 5: for each client i \u2208 Ct in parallel do 6: The server sends Gt\u22121 to client i 7: wti \u2190 ClientUpdate ( Di, G\nt\u22121) 8: Client i sends wti back to the server 9: end for\n10: (S1, \u00b7 \u00b7 \u00b7 , Sn)\u2190 LayerScoring (wt1, \u00b7 \u00b7 \u00b7 , wtn) 11: Ctb \u2190 AnomalyDetection (S1, \u00b7 \u00b7 \u00b7 , Sn) 12: m\u2190 len (Ctb) 13: Gt = \u2211 i\u2208Cb 1 mw t i 14: end for\nTo address this issue, we have devised a hierarchical detection method, Layer Scoring, to measure fine-grained model differences, as shown in Algorithm 2. Layer Scoring examines and assigns an outlier score to each layer of the uploaded models in turn. To provide accurate scores, the outlier scoring method is crucial and faces the following challenges: C1: The proportion of compromised clients is unknown. Many existing works on outlier detection reply to the impractical assumption of knowing the proportion of compromised clients in advance, which severely limits their applicability in reality. To address this limitation, in this work, we propose an algorithm without requiring such prior knowledge. As such, conventional outlier detection methods such as K-Nearest Neighbors (KNN) and One-Class SVM, which rely on prior knowledge of the number of neighbors, are not feasible. C2: Identifying backdoored models in dynamic scenarios. In each round, the number of injected backdoors is unknown and may vary. Hence, it is important to have a stable backdoored model identification method that can effectively handle dynamic attacks. Otherwise, many false positives may be generated, failing the backdoor defenses and impacting the\n5 main task\u2019s accuracy. To address the above challenges, we chose Connectivitybased Outlier Factor (COF) [11] as our outlier detection algorithm. COF is a density-based outlier detection algorithm that measures the degree of connectivity of a data point to its neighboring points. COF calculates the outlier score of each data point by comparing its average reachability distance to that of its neighbors. COF is advantageous over other distancebased outlier detection algorithms as it is less sensitive to the number of dimensions of the data and can effectively detect outliers in high-dimensional data. It is also able to detect outliers in non-uniform density data sets and is less affected by the presence of noise in the data. Additionally, COF does not require any assumptions about the underlying data distribution, making it more robust to different types of data. Therefore, COF is a better choice for identifying backdoored models in a dynamic federated learning setting where the proportion of compromised clients is unknown and may vary over time.\nAlgorithm 2 Layer Scoring\nInput: The local model wi uploaded by each client i \u2208 Ct Output: The set of Layer Scoring Si for each client i \u2208 Ct\n1: initialize n\u2190 len (Ct) 2: for layer j in [1, total] do \u25b7 total is the number of\nlayers of the model 3: ( sj1, \u00b7 \u00b7 \u00b7 , sjn ) \u2190 COF ( wj1, \u00b7 \u00b7 \u00b7 , wjn ) 4: for i \u2208 [n] do 5: Add sji to the set of Layer Scoring Si 6: end for 7: end for 8: return S1, \u00b7 \u00b7 \u00b7 , Sn"
        },
        {
            "heading": "C. Anomaly Detection",
            "text": "Layer Scoring assigns each layer of each local model an outlier score. Then, Anomaly Detection uses the scores to identify the anomalous clients to safeguard the model from backdoor attacks. Anomaly Detection checks the scores layer by layer and increments a model\u2019s flag count by one upon finding an abnormal layer score. In the end, the clients with the higher flag counts are marked as anomalies. In this paper, we mark clients with more than 50% of the layer count as anomalies. The algorithm for determining layer anomalies needs to be carefully designed to achieve the three defense goals as mentioned in Section III.\nWe follow the common assumption that less than 50% of clients are compromised. We argue that the commonly employed Three Sigma Rule [31] and Z-score [32] can not identify anomalous clients well, because these algorithms assess clients using the mean value, which can be strongly influenced and shifted towards the location of the outliers in the presence of extreme outliers, resulting in failed outlier identifications. To solve this problem, we use MAD for anomaly detection because: 1) it tolerates extreme values since MAD uses the median which is not affected by extreme values, and 2) it can be applied to any data distribution,\nunlike Three Sigma Rule and Z-score which are only applicable to normally distributed data. The Anomaly Detection processes include: 1) Calculate the median of all the outlier scores. 2) Calculate the absolute deviation value of the outlier scores from the median. 3) Assign the median of all the absolute deviation values to MAD. 4) A layer whose outlier score deviates from the median by larger than \u00b5 MAD is classified as anomalous and the model\u2019s flag (Outlierflag) is incremented by one. \u00b5 is the hyperparameter which we set to 3 by default in the experiments. In each round, the server gets all the uploaded models and checks all their layers to get Outlierflagi,\u2200i \u2208 [n]. FLD classifies the models of which at least half of the layers are marked as anomalies as anomalous models and aggregates only the other models that are classified as benign models.\nAlgorithm 3 Anomaly Detection Input: : The set of Layer Scoring from each client i \u2208 Ct are\nregarded as Si Output: The benign clients set Ctb\n1: initialize n\u2190 len (Ct) 2: initialize Outlierflagi \u2190 0,\u2200i \u2208 [n] 3: for layer j in [1, total] do \u25b7 total is the number of\nlayers of the model 4: Me\u2190MEDIAN ( Sj1, \u00b7 \u00b7 \u00b7 , Sjn ) 5: MAD \u2190MEDIAN(\n\u2223\u2223\u2223Sj1 \u2212Me\u2223\u2223\u2223 , \u00b7 \u00b7 \u00b7 , \u2223\u2223Sjn \u2212Me\u2223\u2223) 6: for i \u2208 [n] do 7: flag1\u2190 ( Sji >= Me+ \u00b5 \u2217MAD ) ?1 : 0\n8: flag2\u2190 ( Sji <= Me\u2212 \u00b5 \u2217MAD ) ?1 : 0\n9: Outlierflagi \u2190 Outlierflagi + flag1 + flag2 10: end for 11: end for 12: for i \u2208 [n] do 13: if Outlierflagi < total/2 then 14: Add i to the benign clients set Ctb 15: end if 16: end for 17: return Ctb"
        },
        {
            "heading": "D. Private FLD",
            "text": "Many attacks on federated learning have been proposed besides backdoor attacks, such as membership inference attack and attribute inference attack. These attacks all demonstrate the necessity of enhancing the privacy protection of federated learning to prohibit access to local model plaintext updates. In general, there are two approaches to protect the privacy of customer data: differential privacy and encryption techniques such as homomorphic encryption [33] or multi-party secure computation [34]. Differential privacy is a statistical and simple-to-implement method, but with impacts on the model performance, while encryption provides strong privacy guarantees and protection, but at the cost of reduced efficiency. Specifically, homomorphic encryption is a cryptographic primitive that allows computations to be performed on encrypted\n6 Algorithm 4 Private FLD\nInput: : The local model JwiK uploaded by each client i \u2208 Ct Output: The set of Layer Scoring Si for each client i \u2208 Ct Server:\n1: Randomly select m nonzero integer ri for j in [1,m] \u25b7 m is the length of wi 2: for j in [1,m] do \u25b7 n is 3: cij \u2190 J\u03c9ijK \u00b7 JrjK 4: end for 5: send {cij}j=nj=1 to CP CP: : 1: for j in [1,m] do \u25b7 n is 2: \u03c9 \u2032\nij \u2190 Dec(skc, cij) 3: end for 4: (S1, \u00b7 \u00b7 \u00b7 , Sn)\u2190 LayerScoring ( w \u2032 1, \u00b7 \u00b7 \u00b7 , w \u2032 n ) 5: Send (S1, \u00b7 \u00b7 \u00b7 , Sn) to PS\ndata without revealing the underlying plaintext. The basic idea is to encrypt the plaintext first to obtain the ciphertext and continue the calculation operation on the ciphertext, decrypt the final ciphertext result to obtain the plaintext, to keep the result consistent with the calculation on the plaintext. For example, Paillier cryptosystem is a representative additive homomorphic encryption that has been commonly used in federated learning. It has the following two homomorphic properties:\n\u2022 Homomorphic addition of plaintexts: Jx1K \u00b7 Jx2K = Jx1 + x2K, where x1 and x2 represent plaintexts, J K is an encryption operation. \u2022 Homomorphic multiplication of plaintexts: JxKr = Jr \u00b7 xK, where x represents plaintext, J K is an encryption operation, r is a constant.\nNext we illustrate the applicability of FLD in federated learning homomorphic encryption scenarios. First, we follow the federated setup of [35]:\n\u2022 Server is responsible for receiving the gradients submitted by all participants and conducting aggregation to obtain a new global model. \u2022 Cloud Platform (CP) performs homomorphic encryption calculations together with the server. The CP holds a (private-key, public-key) pair generated by a trusted authority for encryption and decryption.\nOur algorithm is summarized in Algorithm 4. Correctness: To ensure that FLD can effectively identify malicious gradients, we need to prove that homomorphic encryption does not affect the calculation of COF anomaly detection. According to the properties of homomorphic encryption, we have\ncij = J\u03c9ijK \u00b7 JrjK = J\u03c9ij + rjK.\n(7)\nso \u03c9 \u2032 ij = \u03c9ij + rj , for \u03c9 \u2032 x and \u03c9 \u2032\ny the Euclidean distance is\u2225\u2225\u2225\u03c9\u2032x \u2212 \u03c9\u2032y\u2225\u2225\u2225 = \u221a\u221a\u221a\u221a n\u2211\nj=1\n( \u03c9\n\u2032 xj \u2212 \u03c9 \u2032 yj )2 =\n\u221a\u221a\u221a\u221a n\u2211 j=1 (\u03c9xj + rj \u2212 (\u03c9yj + rj))2\n= \u221a\u221a\u221a\u221a n\u2211 j=1 (\u03c9xj \u2212 \u03c9yj)2\n= \u2225\u03c9x \u2212 \u03c9y\u2225 .\n(8)\nWhen the distance metric is Euclidean distance, the COF anomaly detection algorithm can still function in the homomorphic encryption scenario and the results are consistent with the plaintext."
        },
        {
            "heading": "E. Convergence Analysis",
            "text": "To analyze the convergence of FLD, we propose the theorem of convergence and prove it.\nTheorem 1. Let Assumptions 1 to 4 hold and L, \u00b5, U , M be defined therein. Choose the learning rate \u03b7t = \u03b8t+\u03f5 , \u03f5 > 0, \u03b8 > 1\u00b5 , we define \u03bb = max{ \u03b8A \u03b8\u00b5\u22121 , (\u03f5 + 1)Z1}. Then FLD satisfies\nE[F (Gt)]\u2212 F \u2217 \u2264 L 2 Zt \u2264 L 2\n\u03bb\n(t+ \u03f5) 1 2\nt\u2192\u221e\u2212\u2192 0, (9)\nwhere\nA = 4U2 +M2 + 2\u0393, Zt = E||Gt \u2212G\u2217||2. (10)\nProof: Let Gt+1 denote the global model\u2019s parameters in the central server in (t + 1)-round and G\u2217 be the optimal parameters in the central server. Additionally, gt =\n\u2211 i\u2208Ctb pi\u2207Fi(wti , \u03beti), where gt denotes the gradient\nupdates uploaded by the clients in t-round and pi denotes the weight of the i-client\u2019s gradient during aggregation. g\u0304t =\u2211 i\u2208Ctb pi\u2207Fi(wti) and Gt+1 = Gt\u2212\u03b7tgt, where Ctb denotes the collection of benign clients chosen by FLD in t-round. Then, we have\n||Gt+1 \u2212G\u2217||2 = ||Gt \u2212 \u03b7tgt \u2212G\u2217 \u2212 \u03b7tg\u0304t + \u03b7tg\u0304t|| = ||Gt \u2212G\u2217 \u2212 \u03b7tg\u0304t||2\ufe38 \ufe37\ufe37 \ufe38\nP1\n+ 2\u03b7t < Gt \u2212G\u2217 \u2212 \u03b7tg\u0304t, g\u0304t \u2212 gt >\ufe38 \ufe37\ufe37 \ufe38 P2 + (\u03b7t)2||g\u0304t \u2212 gt||2.\n(11)\nSince Egt = g\u0304t, we see EP2 = 0. Now we split P1 into three terms:\nP1 = ||Gt \u2212G\u2217 \u2212 \u03b7tg\u0304t||2\n= ||Gt \u2212G\u2217||2\u22122\u03b7t < Gt \u2212G\u2217, g\u0304t >\ufe38 \ufe37\ufe37 \ufe38 P3 +(\u03b7t)2||g\u0304t||2\ufe38 \ufe37\ufe37 \ufe38 P4 .\n(12)\n7 Focusing on the last term in the above equation, according to Assumption 3, we have\nEP4 = E[(\u03b7t)2||g\u0304t||2] \u2264 (\u03b7t)2 \u2211 i\u2208Ctb p2iE||\u2207Fi(wti)||2\n\u2264 (\u03b7t)2U2. Consider P3, it follows:\nP3 = \u22122\u03b7t < Gt \u2212G\u2217, g\u0304t > = \u22122\u03b7t \u2211 i\u2208Ctb pi < G t \u2212 wti ,\u2207Fi(wti) >\n\u2212 2\u03b7t \u2211 i\u2208Ctb pi < w t i \u2212G\u2217,\u2207Fi(wti) > .\n(13)\nIt is well known that \u22122ab \u2264 a2 + b2, so \u2212 2 < Gt \u2212 wti ,\u2207Fi(wti) > \u2264 ||Gt \u2212 wti ||2 + ||\u2207Fi(wti)||2.\n(14)\nAccording to Assumption 2, it follows:\n\u2212 < wti \u2212G\u2217,\u2207Fi(wti) >\n\u2264 \u2212(Fi(wti)\u2212 Fi(G\u2217))\u2212 \u00b5\n2 ||wti \u2212G\u2217||2.\n(15)\nUse Equation 12 and Inequalities 13, 14, 15, we obtain the following formula\nP1 = ||Gt \u2212G\u2217 \u2212 \u03b7tg\u0304t||2\n\u2264 ||Gt \u2212G\u2217||2 + (\u03b7t)2||\u2207Fi(wti)||2 + \u03b7t \u2211 i\u2208Ctb pi(||Gt \u2212 wti ||2 + ||\u2207Fi(wti)||2)\n\u2212 2\u03b7t \u2211 i\u2208Ctb pi(Fi(w t i)\u2212 Fi(G\u2217) + \u00b5 2 ||wti \u2212G\u2217||2) \u2264 (1\u2212 \u03b7t\u00b5)||Gt \u2212G\u2217||2 + ((\u03b7t)2 + \u03b7t)||\u2207Fi(wti)||2\n+ \u03b7t \u2211 i\u2208Ctb pi||Gt \u2212 wti ||2\n\u22122\u03b7t \u2211 i\u2208Ctb pi(Fi(w t i)\u2212 Fi(G\u2217))\ufe38 \ufe37\ufe37 \ufe38\nP5\n.\nMotivated by [24], we define \u0393 = F \u2217\u2212 \u2211\ni\u2208Ctb\npiF \u2217 i . \u0393 is used to\nmeasure the degree of heterogeneity between the local models and the global model, in i.i.d data distributions, E\u0393 = 0. We have\nP5 = \u22122\u03b7t \u2211 i\u2208Ctb pi(Fi(w t i)\u2212 Fi(G\u2217))\n= \u22122\u03b7t \u2211 i\u2208Ctb pi(Fi(w t i)\u2212 F \u2217i + F \u2217i \u2212 Fi(G\u2217))\n\u2264 2\u03b7t \u2211 i\u2208Ctb pi(F \u2217 \u2212 F \u2217i ) = 2\u03b7t\u0393,\nHence,\nP1 \u2264 (1\u2212 \u03b7t\u00b5)||Gt \u2212G\u2217||2 + ((\u03b7t)2 + \u03b7t)||\u2207Fi(wti)||2 + \u03b7t \u2211 i\u2208Ctb pi||Gt \u2212 wti ||2 + 2\u03b7t\u0393.\nUtilize the above results, we have\nE|Gt+1 \u2212G\u2217||2 \u2264 (1\u2212 \u03b7t\u00b5)E||Gt \u2212G\u2217||2\n+ ((\u03b7t)2 + \u03b7t)E||\u2207Fi(wti)||2 + \u03b7t \u2211 i\u2208Ctb piE||Gt \u2212 wti ||2 + 2\u03b7t\u0393 + (\u03b7t)2E||g\u0304t \u2212 gt||2. (16)\nAccording to Assumption 3, it follows: E||gt \u2212 g\u0304t||2 = E|| \u2211 i\u2208Ctb pi\u2207Fi(wti , \u03beti)\u2212\u2207Fi(wti)||2\n\u2264 \u2211 i\u2208Ctb p2i (E||\u2207Fi(wti , \u03beti)||2 + E||\u2207Fi(wti)||2)\n\u2264 2 \u2211 i\u2208Ctb p2iU 2.\n(17)\nAccording to Assumption 4, it follows:\u2211 i\u2208Ctb pi||Gt \u2212 wti ||2 = \u2211 i\u2208Ctb pi|| \u2211 i\u2208Ctb piw t i \u2212 wti ||2\n\u2264 \u2211 i\u2208Ctb pi||wti ||2 \u2264M2.\n(18)\nSo far, we have all the preparations ready to prove the final conclusion. Let Zt = E||Gt \u2212G\u2217||2, \u03b7t = \u03b8t+\u03f5 , \u03f5 > 0, \u03b8 > 1 \u00b5 , \u03bb = max{ \u03b8A\u03b8\u00b5\u22121 , (\u03f5+1)Z1}, our goal of proving Zt \u2264 \u03bb\n(t+\u03f5) 1 2\ncan be achieved as follows. For t = 1, it holds. Suppose that the conclusion establishes for some t and use Inequalities 16, 17, 18, we have Zt+1 as follows:\nZt+1 \u2264 (1\u2212 \u03b7t\u00b5)Zt + ((\u03b7t)2 + \u03b7t)U2 + \u03b7tM2 + 2(\u03b7t)2 \u2211 i\u2208Ctb p2iU 2 + 2\u03b7t\u0393\n\u2264 (1\u2212 \u03b7t\u00b5)Zt + \u03b7tA\n= (t+ \u03f5)\n1 2 \u2212 1\n(t+ \u03f5) \u03bb+ (\n\u03b8A t+ \u03f5 \u2212 \u03b8\u00b5\u2212 1 t+ \u03f5 \u03bb)\n\u2264 \u03bb (t+ \u03f5+ 1) 1 2 ,\n(19)\nwhere A = 4U2 + M2 + 2\u0393. Then, from Assumption 1, we get\nE[F (Gt)]\u2212 F \u2217 \u2264 L 2 Zt \u2264 L 2\n\u03bb\n(t+ \u03f5) 1 2\nt\u2192\u221e\u2212\u2192 0. (20)"
        },
        {
            "heading": "V. EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "A. Experiments Setup",
            "text": "Datasets and Models. To evaluate the effectiveness of FLD, we tested two different federated learning use cases, namely word prediction used by [14], and image classification used by DBA [9].\n8 \u2022 Word Prediction: We followed the experiment setup of [14]. We use the Reddit public dataset from November 2017 and filter out users with fewer than 150 or more than 500 posts. We assume that every remaining Reddit user is a participant in federated learning and treat each post as a sentence in the training data and use a model consisting of two LSTM layers and a linear output layer.\n\u2022 Image Classification: We conducted experiments on three classic image datasets: MNIST, CIFAR10, and Tinyimagenet. MNIST is a classic handwritten digital image set containing 60,000 training examples and 10,000 test examples. CIFAR10 consists of 10 classes of 32x32 color images, including a total of 50,000 training examples and 10,000 test examples. The Tiny-imagenet [9] consists of 200 classes each of which has 500 training images and 50 test images. To simulate the non-i.i.d environment, we divide the datasets using Dirichlet distribution, a commonly used prior distribution in Bayesian statistics [36].\nBaselines. We choose the following defense approaches as baselines: FoolsGold [17], Robust Federated Aggregation (RFA) [20], Differential Privacy (DP) [29], Krum [15], Trimmed Mean [21], Bulyan [28], and Flame [18]. Please refer to the Appendix for details. Evaluation Metrics Based on the characteristics of federated learning and backdoor attacks, we consider the following metrics for evaluating the effectiveness of backdoor attacks and defense techniques. \u2022 Backdoor Accuracy (BA) refers to the model accuracy for\nbackdoor tasks, where the attacker\u2019s target is to maximize BA, and an effective defense algorithm minimizes it. \u2022 Main Accuracy (MA) refers to the model accuracy for the primary task. Note that both attackers and defenders aim to minimize the attack\u2019s impact on MA."
        },
        {
            "heading": "B. Results",
            "text": "As shown in Table II, compared to SOTA defenses: FoolsGold, RFA, DP, Krum, Trimmed Mean, Bulyan and Flame, FLD is the most effective on all 4 datasets facing Attack A-M of constrain-and-scale [14]. On the MNIST dataset, RFA, Krum, Bulyan, Flame, and FLD perform fairly well, while FoolsGold, DP, and Trimmed Mean have worse performance with the Backdoor Accuracy getting close to 100%. However, on CIFAR and Tiny-imagenet datasets, RFA and Bulyan also failed at defending against backdoors. Krum albeit successfully defended against the backdoor attack, performed significantly reduced MA. This is because CIFAR and Tinyimagenet use ResNets neural networks, which are much more complex than the 2-layer convolutional networks deployed on MNIST. In the complex and larger models, the backdoor is\nhidden more deeply and thus difficult to detect or defuse with these model-granularity defensive methods. Flame has slightly better performance but is still outperformed by FLD. The reason mainly is that FLD is based on layer granularity and thus allows for fine-grained detection of anomalous models, resulting in the best defense performances on all datasets. Specifically, FLD presents 88.97% MA on CIFAR, 25.26% MA on Tiny-imagenet and 0.00% BA on both datasets. In the NLP task, FLD effectively defended against backdoor attacks, with a BA of 0.00% and MA of 19.26% on Reddit dataset.\nTo further demonstrate the effectiveness of FLD, we tested FLD against several other defenses on the DBA attack, which breaks down global trigger patterns into separate local patterns and embeds them separately into the training sets of different adversaries. Compared to centralized backdoor attacks, DBA take better advantage of the distributed nature of federated learning and are therefore more stealthy and difficult to detect for federated learning.\nAs shown in Table III, RFA and Bulyan cannot effectively defend against DBA attack on MNIST, while Krum can effectively defend against the DBA attack but at the cost of a considerable drop on MA, with an accuracy of 54.12% on CIFAR10 and 9.35% on Tiny-imagenet. Flame achieves better defense effects on the MNIST and Tiny-imagenet, but worse performance on CIFAR10. In a word, SOTA defense methods either fail to effectively defend against DBA or endure a huge impact on the accuracy of the primary task, whereas FLD achieves effective defense on MNIST, CIFAR and Tinyimagenet with backdoor success rates of only 0.03%, 0.60%, and 0.00%, respectively, with negligible drops in MA, i.e., 0.15%, 3.18%, and 0.10%.\nGeneralizability To demonstrate the generalizability of FLD, we extend our evaluation to various backdoors such as constrain-and-scale, DBA, Edge-Case [37], Little Is Enough [10], PGD [37] and Flip attack [38], on CIFAR10. As summarized in Table IV, FLD effectively mitigated all the attacks with negligible impact on MA. Therefore, FLD achieves robust performances in the face of various attacks and thus shows great generalizability. It is worth clarifying that the target of Flip attack is to decrease the accuracy of specific labels, so the MA of Flip refers to the accuracy of the attacked class. In our experiments, we set the attacked class\n9\nto be \u201cairplane\u201d, and the attacker succeeded to decrease the prediction accuracy of \u201cairplane\u201d to 0.5%. FLD effectively defended against Flip attack and present the prediction accuracy of \u201cairplane\u201d at 96.5%."
        },
        {
            "heading": "C. Ablation Tests",
            "text": "Proportion of Compromised Clients. We use PMR to represent the proportion of compromised clients, PMR = kn , where k represents the number of compromised clients and n represents the number of all clients. We evaluate FLD for different PMR values, i.e., 0, 0.1, 0.2, 0.3, 0.4. An effective backdoor defense method should have minimum impact on the primary task in an environment with different proportions of attackers, also when no attackers exist.\nWe followed the DBA [9] setup and randomly selected 10 clients for each round of federated learning training, where 0, 1, 2, 3, 4 clients were malicious clients representing PMR values 0, 0.1, 0.2, 0.3, 0.4, respectively. The results of the 10 rounds of the constrain-and-scale attack are shown in Figure 2. In CIFAR10, the success rates of BA after 10 rounds of attacks were 69.05%, 82.24%, 83.88% and 85.54%. Higher PMR leads to a faster BA growth rate. FLD can successfully identify the poisoned clients in scenarios with PMR of 0.1, 0.2, 0.3, and 0.4. On MNIST, when PMR = 0.1, A-M attack needs more rounds to successfully inject the backdoor, hence the\nBA with No Defense is also 0. With PMR of 0.2, 0.3, and 0.4, the backdoor was embedded smoothly. Nevertheless, FLD still presented a good defense against malicious clients. With PMR = 0, FLD has a negligible effect on MA. Therefore, FLD is robust to different poisoning rates (proportion of compromised clients).\nDegree of Data Heterogeneity. Since FLD is based on detecting the difference between benign and backdoor models, the different data distribution among clients may affect its effectiveness. To test FLD under diverse data distribution assumptions, we vary the degree of data heterogeneity across clients by adjusting the parameter of Dirichlet distribution to 0.1, 0.5, 1, 10, and 100, on the CIFAR10 dataset. Figure 3 showcases the data distribution of clients when the Dirichlet alpha is 0.1 and 100. It can be seen that when Dirichlet alpha = 0.1, the data distribution is extremely heterogeneous between clients, with each client having a different number of samples and labels. When Dirichlet alpha = 100, the data distribution is close to i.i.d. As shown in Fig. 4a , MA decreases as the degree of data heterogeneity increases. FLD is effective in identifying and removing malicious clients in both extreme non-i.i.d and near-i.i.d scenarios."
        },
        {
            "heading": "VI. CONCLUSIONS",
            "text": "In this work, we show that the existing defense methods can not effectively defend against SOTA backdoor attacks in federated learning. To tackle this challenge, we propose an innovative robust federated learning backdoor defense algorithm, Federated Layer Detection (FLD). FLD is the first algorithm that assesses model outliers based on the granularity of layer. We theoretically prove the convergence guarantee of FLD on both i.i.d and non-i.i.d data distributions. Extensive experimental results prove the superiority of FLD over SOTA defense methods against various attacks in different scenarios, demonstrating the robustness and generalizability of FLD.\n10"
        },
        {
            "heading": "A. Hyperparameters",
            "text": "Reddit datasets are collected from distributed clients and therefore do not require manual division [14]. We use Dirichlet distribution to partition the image datasets (MNIST, CIFAR10, Tiny-imagenet). The distribution hyperparameter is 0.5, 0.9, and 0.5 for MNIST, CIFAR10, and Tiny-imagenet. Each client uses SGD as the optimizer and a default batch size of 64. \u00b5 is set as 3 by default. For the semantic backdoor, we follow the experimental setup used by [14], where the trigger sentence is \u201cpasta from Astoria is\u201d and the target word is \u201cdelicious\u201d. After the model was trained for 5000 rounds with 100 randomly selected clients in each round, the adversary used 10 malicious clients to inject the backdoor. For the pixel pattern backdoor, we set specific pixels (same as the ones selected by DBA) to white, and then modify the label of the sample with the trigger to backdoor label. The backdoor label is \u201cdigit 2\u201d in MNIST, \u201cbird\u201d in CIFAR10, and \u201cbullfrog\u201d in Tiny-imagenet. The default PMR is 20/64 for MNIST, 10/64 for CIFAR, and 20/64 for Tiny-imagenet, to be consistent with DBA [9]. All participants train the global model, 10 of which are selected in each round to submit local SGD updates for aggregation. The adversary used 2 malicious clients to inject a backdoor in Attack A-M of constrain-and-scale, 4 malicious clients to inject a backdoor in Attack DBA, and 1 malicious client to inject a backdoor in Attack A-S."
        },
        {
            "heading": "B. Baselines",
            "text": "\u2022 FoolsGold argues that in federated learning, malicious clients tend to upload updates with higher similarities than benign clients, since each benign client has a unique data distribution while malicious clients share the same target. FoolsGold leverages this assumption to adapt the learning rate of each client during each iteration. The objective is to preserve the learning rates of the clients uploading distinct gradient updates, while decreasing the learning rates of the clients that consistently contribute similar gradient updates. \u2022 Robust Federated Aggregation (RFA) replaces the weighted arithmetic mean with the geometric median for federated learning aggregation. Specifically, RFA aggregates the local model parameters by finding the point that minimizes the sum of the distances to all the other points, where the distance is measured using a suitable metric such as the Euclidean distance. This point is known as the geometric median, representing the \u201ccenter\u201d of the distribution of the local models. \u2022 Differential Privacy (DP) is a privacy technique designed to ensure that the output does not reveal individual data records of participants. DP can be applied to machine learning to protect the privacy of training data or model updates. DPbased backdoor defense mitigates the impact of poisoned model updates on the global model by adding random noise to the uploaded parameters during aggregation. The random noise dilutes the malicious information injected by the malicious participants to mitigate their impact on the final global model.\n\u2022 Krum selects one of the n local models that is similar to the others as the global model by calculating the Euclidean distance between two of the local models. \u2022 Trimmed Mean, also known as truncated mean, is a statistical method for calculating the average of a dataset while eliminating outliers. To compute the trimmed mean, a certain percentage of the highest and lowest values are removed or trimmed, and the mean is then calculated based on the remaining values. Specifically, trimmed mean aggregates each model parameter independently. The server ranks the j-th parameters of the n local models. The largest and smallest k parameters are removed, and the average of the remaining n\u2212 2k parameters are calculated as the j-th parameters of the global model. \u2022 Bulyan first iteratively applies Krum to select the local models, then aggregates these local models using a variant of the trimmed mean. In other words, Bulyan is a combination of Krum and Trimmed Mean. \u2022 Flame employs the cosine distance metric to measure the dissimilarity between locally uploaded models from various clients. It leverages HDBSCAN density clustering to distinguish between benign and malicious clients. Subsequently, Flame applies a clipping operation to limit the extent of modifications to the local models and introduces noise into the global model."
        },
        {
            "heading": "C. Effectiveness of Layer Scoring",
            "text": "To test the necessity and effectiveness of Layer Scoring, we compared the performance between FLD with and without Layer Scoring under different attacker\u2019s poisoned data rate (PDR) on the CIFAR10 dataset. FLD without Layer Scoring uses the same approach as the existing defense methods, i.e., splicing the model weights directly and using COF and Anomaly Detection to detect anomalous models.\nAs shown in Figure 4b, as the PDR increases, the BA of the FLD without Layer Scoring rises and then falls. This is due to the fact that higher PDR leads to faster backdoor injection. But, higher PDR also makes the attacks easier to detect. Hence, the impact of the attack increases in the beginning and then decreases as the defense algorithm starts to identify the attack.\nIn Section Experiments, we focus on the more insidious and difficult-to-detect Attack A-M. Here we evaluate the effectiveness of FLD against Attack A-S. Table V shows that FLD is effective for all 4 datasets on Attack A-S of constrain-and-scale.\n12"
        }
    ],
    "title": "Mitigating Backdoors in Federated Learning with FLD",
    "year": 2023
}