{
    "abstractText": "The recent success of hybrid methods in monocular odometry has led to many attempts to generalize the performance gains to hybrid monocular SLAM. However, most attempts fall short in several respects, with the most prominent issue being the need for two different map representations (local and global maps), with each requiring different, computationally expensive, and often redundant processes to maintain. Moreover, these maps tend to drift with respect to each other, resulting in contradicting pose and scene estimates, and leading to catastrophic failure. In this paper, we propose a novel approach that makes use of descriptor sharing to generate a single inverse depth scene representation. This representation can be used locally, queried globally to perform loop closure, and has the ability to re-activate previously observed map points after redundant points are marginalized from the local map, eliminating the need for separate and redundant map maintenance processes. The maps generated by our method exhibit no drift between each other, and can be computed at a fraction of the computational cost and memory footprint required by other monocular SLAM systems. Despite the reduced resource requirements, the proposed approach maintains its robustness and accuracy, delivering performance comparable to state-of-the-art SLAM methods (e.g ., LDSO, ORB-SLAM3) on the majority of sequences from well-known datasets like EuRoC, KITTI, and TUM VI. The source code is available at: https://github.com/AUBVRL/fslam_ros_docker.",
    "authors": [
        {
            "affiliations": [],
            "name": "Georges Younes"
        },
        {
            "affiliations": [],
            "name": "John Zelek"
        },
        {
            "affiliations": [],
            "name": "Daniel Asmar"
        }
    ],
    "id": "SP:0499a9032c8f748bd94b62c90889f823b51d9681",
    "references": [
        {
            "authors": [
                "G. Younes",
                "D. Asmar",
                "J. Zelek"
            ],
            "title": "A unified formulation for visual odometry",
            "venue": "in: 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)",
            "year": 2019
        },
        {
            "authors": [
                "X. Gao",
                "R. Wang",
                "N. Demmel",
                "D. Cremers"
            ],
            "title": "Ldso: Direct sparse odometry with loop closure",
            "venue": "in: 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)",
            "year": 2018
        },
        {
            "authors": [
                "S.H. Lee",
                "J. Civera"
            ],
            "title": "Loosely-coupled semi-direct monocular slam",
            "venue": "IEEE Robotics and Automation Letters 4 (2) ",
            "year": 2019
        },
        {
            "authors": [
                "H. Luo",
                "C. Pape",
                "E. Reithmeier"
            ],
            "title": "Hybrid monocular slam using double window optimization",
            "venue": "IEEE Robotics and Automation Letters 6 (3) ",
            "year": 2021
        },
        {
            "authors": [
                "R. Mur-Artal",
                "J.M.M. Montiel",
                "J.D. Tardos"
            ],
            "title": "ORB-SLAM: A Versatile and Accurate Monocular SLAM System",
            "venue": "IEEE Transactions on Robotics PP (99) ",
            "year": 2015
        },
        {
            "authors": [
                "G. Klein",
                "D. Murray"
            ],
            "title": "Parallel Tracking and Mapping for Small AR Workspaces",
            "venue": "6th IEEE and ACM International Symposium on Mixed and Augmented Reality ",
            "year": 2007
        },
        {
            "authors": [
                "C. Forster",
                "M. Pizzoli",
                "D. Scaramuzza"
            ],
            "title": "Svo : Fast semi-direct monocular visual odometry",
            "venue": "in: Robotics and Automation (ICRA), IEEE International Conference on",
            "year": 2014
        },
        {
            "authors": [
                "J. Engel",
                "T. Sch\u00f6ps",
                "D. Cremers"
            ],
            "title": "LSD-SLAM: Large-Scale Direct Monocular SLAM",
            "venue": "in: D. Fleet, T. Pajdla, B. Schiele, T. Tuytelaars (Eds.), Computer Vision \u2013 ECCV 2014 SE - 54, Vol. 8690 of Lecture Notes in Computer Science, Springer International Publishing",
            "year": 2014
        },
        {
            "authors": [
                "J. Engel",
                "V. Koltun",
                "D. Cremers"
            ],
            "title": "Direct sparse odometry",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence PP (99) ",
            "year": 2017
        },
        {
            "authors": [
                "J. Engel",
                "J. Sturm",
                "D. Cremers"
            ],
            "title": "Semi-dense Visual Odometry for a Monocular Camera",
            "venue": "in: Computer Vision (ICCV), IEEE International Conference on, IEEE",
            "year": 2013
        },
        {
            "authors": [
                "J. Civera",
                "A. Davison",
                "J. Montiel"
            ],
            "title": "Inverse Depth Parametrization for Monocular SLAM",
            "venue": "IEEE Transactions on Robotics 24 (5) ",
            "year": 2008
        },
        {
            "authors": [
                "G. Younes",
                "D.C. Asmar",
                "J. Zelek"
            ],
            "title": "FDMO: feature assisted direct monocular odometry",
            "venue": "in: Proceedings of the 14th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications, INSTICC",
            "year": 2019
        },
        {
            "authors": [
                "M. Agrawal",
                "K. Konolige",
                "M.R. Blas"
            ],
            "title": "Censure: Center surround extremas for realtime feature detection and matching",
            "venue": "in: D. Forsyth, P. Torr, A. Zisserman (Eds.), Computer Vision \u2013 ECCV 2008, Springer Berlin Heidelberg, Berlin, Heidelberg",
            "year": 2008
        },
        {
            "authors": [
                "H. Bay",
                "A. Ess",
                "T. Tuytelaars",
                "L. Van Gool"
            ],
            "title": "Speeded-up robust features (surf)",
            "venue": "Comput. Vis. Image Underst. 110 (3) ",
            "year": 2008
        },
        {
            "authors": [
                "A. Glover",
                "W. Maddern",
                "M. Warren",
                "S. Reid",
                "M. Milford",
                "G. Wyeth"
            ],
            "title": "OpenFABMAP: An open source toolbox for appearance-based loop closure detection",
            "venue": "in: 2012 IEEE International Conference on Robotics and Automation (ICRA), IEEE",
            "year": 2012
        },
        {
            "authors": [
                "J. Shi",
                "C. Tomasi"
            ],
            "title": "Good features to track",
            "venue": "in: Computer Vision and Pattern Recognition, 1994. Proceedings CVPR \u201994., 1994 IEEE Computer Society Conference on",
            "year": 1994
        },
        {
            "authors": [
                "E. Rublee",
                "V. Rabaud",
                "K. Konolige",
                "G. Bradski"
            ],
            "title": "ORB: An efficient alternative to SIFT or SURF",
            "venue": "in: International Conference on Computer Vision (ICCV)",
            "year": 2011
        },
        {
            "authors": [
                "D. Galvez-L\u00f3pez",
                "J.D. Tardos"
            ],
            "title": "Bags of Binary Words for Fast Place Recognition in Image Sequences",
            "venue": "Robotics, IEEE Transactions on 28 (5) ",
            "year": 2012
        },
        {
            "authors": [
                "R. Mur-Artal",
                "J.D. Tardos"
            ],
            "title": "Orb-slam2: An open-source slam system for monocular",
            "venue": "stereo, and rgb-d cameras, IEEE Transactions on Robotics 33 (5) ",
            "year": 2017
        },
        {
            "authors": [
                "C. Campos",
                "R. Elvira",
                "J.J.G. Rod\u0155\u0131guez",
                "J.M.M. Montiel",
                "J.D. Tard\u00f3s"
            ],
            "title": "Orb-slam3: An accurate open-source library for visual",
            "venue": "visual\u2013inertial, and multimap slam, IEEE Transactions on Robotics 37 (6) ",
            "year": 2021
        },
        {
            "authors": [
                "E. Rosten",
                "T. Drummond"
            ],
            "title": "Machine Learning for High-speed Corner Detection",
            "venue": "in: 9th European Conference on Computer Vision - Volume Part I, Proceedings of the, ECCV\u201906, Springer-Verlag, Berlin, Heidelberg",
            "year": 2006
        },
        {
            "authors": [
                "B.K.P. Horn"
            ],
            "title": "Closed-form solution of absolute orientation using unit quaternions",
            "venue": "Journal of the Optical Society of America A 4 (4) ",
            "year": 1987
        },
        {
            "authors": [
                "J. \u201dZubizarreta",
                "I. Aguinaga",
                "J.M.M. Montiel"
            ],
            "title": "Direct sparse mapping",
            "venue": "IEEE Transactions on Robotics",
            "year": 2020
        },
        {
            "authors": [
                "S. Leutenegger",
                "S. Lynen",
                "M. Bosse",
                "R. Siegwart",
                "P. Furgale"
            ],
            "title": "Keyframebased visual\u2013inertial odometry using nonlinear optimization",
            "venue": "The International Journal of Robotics Research 34 (3) ",
            "year": 2015
        },
        {
            "authors": [
                "J. Engel",
                "V. Usenko",
                "D. Cremers"
            ],
            "title": "A photometrically calibrated benchmark for monocular visual odometry",
            "venue": "in: arXiv:1607.02555",
            "year": 2016
        },
        {
            "authors": [
                "M. Burri",
                "J. Nikolic",
                "P. Gohl",
                "T. Schneider",
                "J. Rehder",
                "S. Omari",
                "M.W. Achtelik",
                "R. Siegwart"
            ],
            "title": "The euroc micro aerial vehicle datasets",
            "venue": "The International Journal of Robotics Research ",
            "year": 2016
        },
        {
            "authors": [
                "A. Geiger",
                "P. Lenz",
                "R. Urtasun"
            ],
            "title": "Are we ready for autonomous driving? the kitti vision benchmark suite",
            "venue": "in: Conference on Computer Vision and Pattern Recognition (CVPR)",
            "year": 2012
        },
        {
            "authors": [
                "J. Engel",
                "V.C. Usenko",
                "D. Cremers"
            ],
            "title": "A photometrically calibrated benchmark for monocular visual odometry",
            "venue": "CoRR abs/1607.02555 ",
            "year": 2016
        },
        {
            "authors": [
                "D. Schubert",
                "T. Goll",
                "N. Demmel",
                "V. Usenko",
                "J. St\u00fcckler",
                "D. Cremers"
            ],
            "title": "The TUM VI benchmark for evaluating visual-inertial odometry",
            "venue": "CoRR abs/1804.06120 ",
            "year": 2018
        }
    ],
    "sections": [
        {
            "text": "The recent success of hybrid methods in monocular odometry has led to many attempts to generalize the performance gains to hybrid monocular SLAM. However, most attempts fall short in several respects, with the most prominent issue being the need for two different map representations (local and global maps), with each requiring different, computationally expensive, and often redundant processes to maintain. Moreover, these maps tend to drift with respect to each other, resulting in contradicting pose and scene estimates, and leading to catastrophic failure. In this paper, we propose a novel approach that makes use of descriptor sharing to generate a single inverse depth scene representation. This representation can be used locally, queried globally to perform loop closure, and has the ability to re-activate previously observed map points after redundant points are marginalized from the local map, eliminating the need for separate and redundant map maintenance processes. The maps generated by our method exhibit no drift between each other, and can be computed at a fraction of the computational cost and memory footprint required by other monocular SLAM systems. Despite the reduced resource requirements, the proposed approach maintains its robustness and accuracy, delivering performance comparable to state-of-the-art SLAM methods (e.g ., LDSO, ORB-SLAM3) on the majority of sequences from well-known datasets like EuRoC, KITTI, and TUM VI. The source code is available at: https://github.com/AUBVRL/fslam_ros_docker.\nKeywords: Monocular, SLAM, Hybrid, Direct, Indirect methods.\nar X\niv :2\n30 6.\n07 36\n3v 1\n[ cs\n.R O\n] 1\n2 Ju"
        },
        {
            "heading": "1. Introduction",
            "text": "Recently, hybrid methods have emerged as Visual Odometry (VO) solutions that harness the strengths of both direct and indirect (feature-based) formulations, to yield superior performance compared to each individual formulation. For instance, the sub-pixel accuracy and robustness to texturedeprived environments of direct methods, combined with the resilience of indirect methods to large motions, have been shown to outperform current state-of-the-art methods in both direct and indirect VO methods [1].\nThe success of hybrid VO systems led to many methods that aim to extend this symbiotic relationship from an odometry formulation to a Simultaneous Localization and Mapping (SLAM) one. However, most attempts fall short in one or several aspects. For example, LDSO [2] makes use of indirect features to efficiently query global keyframes for loop closure, but does not utilize these features for neither tracking nor mapping. On the other hand, [3] deploy a cascaded approach where a direct odometry is run at frame rate. The resulting output is subsequently fed into an indirect framework for additional processing and global mapping. This approach results in two disjoint map representations, having completely different and redundant map maintenance and optimization processes, which can lead to decreased accuracy due to the potential drift between the two maps.\nMore recently, [4] proposed a double window approach that leverages both direct and indirect formulations in a SLAM framework, where three sequential optimizations, namely, photometric Bundle Adjustment (BA), double window optimization, and structure-only optimization, are used to maintain the local and global maps separately. While this work is the most promising approach for implementing a hybrid SLAM system to date, it still processes and maintains two separate map representations.\nHaving two distinct map representations introduces several limitations into a SLAM system; for example, the two representations can drift with respect to each other, yielding contradictory pose and scene structure estimates, which can in turn lead to catastrophic failure. While this is typically mitigated by separate, often redundant, and computationally expensive map maintenance processes, such catastrophic failure is inevitable due to the suboptimal nature of these maintenance methods. In contrast, our proposed method uses the same process to build, maintain, and update both local and global map representations. In essence, they are the same map representation. The only difference between a local and global map point is that a\nglobal map point is not included in the local photometric bundle adjustment, but is added again as soon as it is re-observed. This allows us to build a coherent and re-usable SLAM map that remains immune to drift, and does not require a separate and computationally expensive maintenance processes.\nOn another note, most of the aforementioned SLAM systems employ Pose-Graph Optimization (PGO) for loop closure. While PGO is computationally efficient, it is a sub-optimal optimization that neglects all 3D observations. In contrast, we propose to perform a full global inverse depth BA on the entire map, taking into account the 3D observation constraints. While this is typically not possible in systems similar to [2], it is possible in ours as we keep track of the connectivity information of the map points across keyframes.\nMoreover, it is common in most PGO implementations to utilize only one type of constraint. Typically, either co-visibility constraints (as described in [5]) or pose-pose constraints (as described in [2]) are employed. Each representation offers its own advantages and disadvantages. It is rare for systems to use both types concurrently, resulting in compromised performance when re-observing previous scenes with methods relying on pose-pose constraints, and facing catastrophic failures when traversing feature-deprived environments with methods relying on co-visibility constraints. To tackle this challenge, we propose a hybrid connectivity graph that keeps track of both pose-pose and co-visibility constraints. By adopting this approach, our system effectively handles temporal constraints in feature deprived environments, while maintaining the ability to establish long range constraints when revisiting past scenes.\nTo address all of the aforementioned issues, we leverage our descriptor sharing formulation presented in [1] and extend its capabilities to maintain a global map. The proposed hybrid SLAM system performs a joint and tightly-coupled (direct-indirect) optimization for tracking. It then uses the same inverse depth parametrization for mapping, where global map points are excluded from the local photometric BA and reintegrated whenever they are re-observed (i.e., the same map representation can be concurrently used locally and globally). Furthermore, we capitalize on the advantages of this joint formulation to generate hybrid connectivity graphs without incurring any additional computational cost. These graphs contain both co-visibility and pose-pose constraints, which can in turn be used either for PGO or to produce optimal results using a full inverse-depth BA over the keyframe poses and associated landmarks.\nThe key aspects of the proposed system are:\n1. The ability to leverage the advantages of both direct and indirect formulations to achieve a robust camera pose estimation. The achieved accuracy is comparable to existing state-of-the-art methods, and in some cases, surpasses them in both direct and indirect monocular SLAM/Odometry systems.\n2. The ability to compute both local and global map representations at a fraction of the time required by indirect methods to build and maintain a global map alone, while introducing a mere increase of 13 ms per keyframe1 when compared to strictly local direct odometry systems such as DSO.\n3. The ability to identify and re-use global map points efficiently.\n4. A reduced memory consumption, where the proposed unified globallocal map requires less than half of the memory typically consumed by other SLAM methods.\n5. The proposed approach keeps track of both pose-pose and co-visibility constraints, utilizing them strategically during loop closure for added robustness in feature-deprived environments. Additionally, our method enables the ability to re-populate the co-visibility constrains after loop closure is performed, allowing for optimal subsequent optimization processes such as full inverse depth BA and efficient re-use of global map points."
        },
        {
            "heading": "2. Related work",
            "text": "Before delving into our proposed solution, it is important to highlight the various limitations of current Visual SLAM (VSLAM) systems as a prelude to the significance of our contributions, particularly in the areas of local and global mapping, as well as loop closure."
        },
        {
            "heading": "2.1. Local and global mapping: a parametrization issue",
            "text": "Over the years, the research community has adopted several formulations to represent local and global maps, grounded by the data type (direct vs indirect) available to the localization and mapping systems. For example,\n1Reported timings in the paper are subject to the hardware used. More in Sec. 4.1.2.\nmost indirect methods such as [6, 5] employ traditional multi-view geometry to perform triangulation across large baselines. This is possible because their features are resilient to viewpoint changes. The triangulation method entails representing the map as an (X, Y, Z) point cloud. However, it suffers from the instability of multi-view geometry under small parallax. In contrast, direct methods like [7, 8, 9] require small motion between frames for brightness constancy to be preserved, making traditional multi-view triangulation methods unsuitable. Instead, the map is updated via sequential small-baseline observations using a filter-based formulation such as [7] and [10]. This triangulation method entails an inverse depth parametrization (u, v, 1\nd ) [11] that can transition smoothly from infinity under no parallax to\na triangulated depth value as more observations come through. Converting between these two parametrizations is fairly trivial. However, the associated triangulation methods differ, making it difficult to use both concurrently. For example, one method requires the feature match locations across all observed images, while the other requires a consistent depth variance estimate across observations. Although this is not an issue when a system adopts a single representation, it becomes problematic in hybrid methods such as [3, 12, 4] where the two representations are used. The noninterchangeability of the triangulating methods forces these systems to adopt separate map maintenance processes (e.g. structure-only optimization) to ensure no drift occurs between both representations.\nIn our previous work [1], we circumvented the need for several map maintenance processes by operating within a pure odometry system. Redundant map points that were marginalized out of the local window were completely removed and not subsequently used in a global framework. In this paper, we rely on the same unified formulation, but extend its capabilities to maintain both representations using a single process. This means that instead of having separate maps, we employ a unified map that can be used interchangeably as either an (X, Y, Z) point cloud or as a (u, v, 1\nd ) inverse depth\nparametrization."
        },
        {
            "heading": "2.2. Loop closure essentials",
            "text": "As an odometry system integrates new measurements with old ones, accumulated numerical errors can grow unbounded, resulting in a drift between the actual camera trajectory, the 3D scene, and their corresponding system estimates. In fact, a monocular odometry system can drift along the scale,\nrotation, and translation axes, leading to an ill-representation of the environment and eventual failure.\nLoop closure is a mechanism used to identify and correct drift. It can be split into three main tasks: (1) loop closure detection, (2) error estimation, and (3) correction.\n(1) Loop closure detection is the process of flagging a candidate keyframe from a set of previously observed keyframes that exhibit similar visual cues to the most recent keyframe (i.e., the camera has returned to a previously observed scene). While most SLAM systems perform loop closure by following the above three tasks, the implementation differs according to the type of information at their disposal. For example, low level features (e.g ., patches of pixels) are susceptible to viewpoint and illumination changes, and are inefficient at encoding and querying an image. Therefore, most direct odometry systems discard them once they go out of view (e.g . DSO [9]). To perform loop closure, direct systems require auxiliary features; for example, LSD SLAM [8] extracts STAR features [13] from keyframes and associates them with SURF descriptors [14], which are in turn parsed through OpenFABMAP [15] (an independent appearance-only SLAM system) to detect loop closure candidates. Similarly, LDSO [2] extracts corner features [16] with their associated ORB descriptors [17], then uses them to encode and query the keyframes for loop closure candidate detection using a Bags of Visual Words (BoVW) model [18].\nIn contrast, ORB-SLAM (1, 2 and 3) [5, 19, 20] does not require separate processes; the same features used for odometry (FAST features [21] and ORB descriptors [17]) are parsed in a BoVW [18] to compute a global keyframe descriptor, which is then used to query a database of previously added keyframes for potential loop closure candidates.\n(2) Error estimation: once a candidate keyframe is detected, its corresponding map point measurements are matched to the most recently added 3D map points, then a similarity transform T \u2208 Sim(3) := [sR|t] that minimizes the error between the two 3D sets is computed. This process is slightly different between the two commonly used loop closure methods, but can have a significant impact on the resulting accuracy. In particular, ORBSLAM establishes 3D-3D map point matches between the loop-ends, which are then used in a RANSAC implementation of [22] to recover a similarity transform. The new similarity is used to search for additional map point\nmatches across other keyframes connected to both loop ends, resulting in a relatively large set of 3D map point matches. The entire set is eventually employed to refine the estimation of the similarity transformation.\nIn contrast, LDSO only establishes 3D map point matches between the currently active keyframe and one loop candidate keyframe. The loop candidate keyframe is also found through BoVW; however, it has one single requirement, which is to be outside the currently active window. As a consequence, the number of used matches is considerably lower compared to ORBSLAM, often resulting in a non-reliable similarity transformation. Moreover, since LDSO only requires a loop closure candidate to exist outside the current active window, it often performs consecutive loop closures whenever the camera transitions away from a scene and returns to it few seconds later. These recurrent loop closures, along with the non-reliable similarity transforms, can introduce significant errors into the 3D map.\n(3) Path correction. The error estimation process corrects the loop-end keyframes; however, it does not correct the accumulated drift throughout the entire path. For that, a SLAM system must keep track of the connectivity information between keyframes along the traversed path in the form of a graph, to subsequently correct the entire trajectory. Most systems employ a PGO, which is a sub-optimal optimization that distributes the accumulated error along the path by considering frame-to-frame constraints exclusively, and disregarding the 3D observations.\nTo be able to perform PGO, a notion of connectivity between the keyframes must be established. To that end, ORB-SLAM employs several representations of such connections. Notably, the co-visibility graph is a byproduct of ORB-SLAM\u2019s feature matching on every keyframe, where a connectivity edge is inserted between keyframes that observe the same 3D map points. ORB-SLAM also uses a spanning tree graph, made from a minimal number of connections, where each keyframe is connected to its reference keyframe and to one child keyframe. Finally, ORB-SLAM also keeps track of an essential graph, which contains the spanning tree and all edges between keyframes that share more than 100 feature matches. Note that the spanning tree \u2286 essential graph \u2286 co-visibility graph; and while the full co-visibility graph can be used for PGO, ORB-SLAM relies on the essential graph and cites the computational efficiency as a reason for this choice, since the former might introduce a large number of constraints.\nUnlike ORB-SLAM, which establishes connectivity graphs by finding fea-\nture matches between keyframes, LDSO lacks access to such information, as it does not perform feature matching between keyframes nor does it keep track of feature matches. Instead, it considers all currently active keyframes in the local window to be connected, and accordingly adds an edge between them in its connectivity graph. While this works well in feature-deprived environments, where not enough feature matches can be established, it has several drawbacks when compared to its ORB-SLAM counterpart. Specifically, whenever a loop closure takes place in ORB-SLAM, new connections between keyframes that were previously disconnected due to drift are updated based on their mutually observed map points; such update is not possible within LDSO\u2019s model.\nThis is where our proposed hybrid connectivity graph plays an important role; by maintaining both connectivity information, a hybrid graph enables the addition of connections between keyframes based on their shared 3D map points. This is particularly useful in situations involving loop closure or when previous scenes are re-observed. Additionally, the hybrid graph maintains the capability to establish connections in feature-deprived environments by considering the temporal proximity of the added keyframes. Furthermore, adding both types of information allows for more optimal optimization methods such as full BA on both the path and reconstructed scene; albeit, at an extra computational cost."
        },
        {
            "heading": "2.3. Global map memory management",
            "text": "For proper operation, direct systems typically add a relatively large number of keyframes per second. While this is generally not an issue for pure odometry methods, where the memory consumption remains constant, the unbounded memory growth becomes an issue for SLAM systems that maintain and re-use a global map representation. To preserve a reasonable memory consumption and keep the compute-time low, ORB-SLAM invokes a keyframe culling strategy that removes keyframes whose 90% of map points are visible in other keyframes. This, however, has a negative impact on the final result\u2019s accuracy since culled keyframes are completely removed from the system, whereas their poses could have been used to better constrain the estimated trajectory.\nOn the other hand, while LDSO does not utilize its global map for pose estimation and mapping, it still needs to store all keyframes in memory along with their associated indirect features and depth estimates in order to perform loop closure. Since it also does not perform feature matching to\ndetect redundant keyframes, it cannot invoke a keyframe culling strategy, resulting in a relatively large memory consumption.\nFinally, despite the fact that DSM [23] does not have a loop closure module, it blurs the line between what can be classified as an odometry system vs. a SLAM system: it is a direct method that keeps track of a global map, and attempts to re-use it for both pose estimation and mapping, without performing loop closure. However, this implies the need to store a large number of information per keyframe, and retrieve them whenever a new keyframe is added. As a consequence, the resulting global map requires a relatively significant amount of memory, and the computational cost increases with the addition of each new keyframe. DSM addresses these challenges by adding fewer keyframes than other direct methods.\nWe mitigate the issues of frequent keyframe addition, along with its associated memory growth and increased map querying time, by using descriptor sharing. In our formulation, a map point can have several representations such as ORB descriptors, patch of pixels, and more. This enables efficient querying of landmarks from both the local and global map. We also maintain connectivity information between keyframes, which allows us to perform keyframe culling. While keyframes are removed, their poses are retained within the pose-pose constraints graph, enabling us to refine the trajectory in the future. These memory management routines allow us to maintain both local and global representations of a scene at a smaller memory footprint compared to state-of-the-art direct, indirect, and other hybrid methods."
        },
        {
            "heading": "3. Proposed system",
            "text": "The proposed system architecture (shown in Fig. 1) follows a fairly standard Visual SLAM architecture with three parallel threads, namely, pose estimation, mapping, and loop closure. However, as will be detailed in this section, we introduce several key modifications to generate both local and global maps concurrently, i.e., with no separate map representations. This approach allows us to efficiently perform the joint pose optimization, to maintain the current moving window and the set of redundant features that were marginalized within the same process, and to perform loop closure all within the same framework. The key concept that enables us to achieve this is descriptor sharing, which was initially proposed in our previous work [1]."
        },
        {
            "heading": "3.1. Descriptor sharing",
            "text": "Descriptor sharing is the idea of associating several types of descriptors with the same feature. To illustrate, one could detect corners and simultaneously associate them with both an ORB descriptor [17] and a patch of pixels as shown in Fig. 2. This enables the use of each descriptor in its favorable conditions to perform various SLAM tasks. For instance, the patch of pixels can be employed for low-parallax triangulation, while the ORB descriptor can be used to perform large-baseline feature-matching. This matching can be leveraged for tasks such as pose estimation, constructing connectivity graphs, maintaining a global map for landmark re-use, and more.\nTherefore, within the context of this paper, it is crucial to establish clear definitions of the different features, their corresponding residuals, and their respective uses, as well as what we mean by local and global maps. This will ensure a common understanding before proceeding with further details.\nFeatures: in this work, a blend of feature types is extracted: first, we detect corners using the FAST detector [21], then augment them with pixel locations whose gradient surpasses a dynamically adjusted cutoff threshold, as described in [9]. There are two primary reasons for using a combination of features. Firstly, FAST corners are repeatable and stable; in contrast, pixel locations with high gradients (gradient-based features) are typically detected\nalong edges, which makes them unstable as they tend to drift along the edge directions, thereby reducing the overall VSLAM performance. Secondly, a texture-deprived environment can cause a significant decrease in detected corners and may lead to tracking failure. In contrast, gradient-based features can be abundantly extracted along any gradient in the image. Having some information in such scenarios is better than having no information. Once the two types of features are extracted, they are treated equally in the subsequent steps, i.e., they are associated with both ORB descriptors and patches of pixels. The ORB descriptors will be used to do feature matching, establish a geometric re-projection error, maintain a global co-visibility graph among keyframes, and perform loop closures. On the other hand, the patch of pixels will contribute towards computing the photometric residuals, estimating the depth of the features as described in [10], and performing the photometric BA.\nLocal vs. global maps: while we adopt the conventional naming of local and global maps used in hybrid approaches, our implementation introduces a slight but fundamental difference. We represent landmarks in both definitions using the same inverse depth parametrization, effectively allowing us to re-activate the global landmarks in the local window when re-observed. This is not feasible in previous hybrid approaches as they convert the marginalized redundant map points to an (X, Y, Z) representation, consequently requiring separate map maintenance processes to mitigate the drift between the two representations."
        },
        {
            "heading": "3.2. Pose estimation",
            "text": "We estimate the camera pose in two sequential steps: (1) we perform feature matching and relative pose estimation with respect to the last frame; (2) we proceed with a joint multi-objective pose optimization over both photometric and geometric residuals.\nWhile we could solely rely on the joint multi-objective optimization, we have found that performing step (1) beforehand can help establish more reliable (i.e., less outlier) feature matches in the subsequent joint optimization. This is primarily because we can narrow down the search window for matches when matching sequential frames. With fewer outliers, the overall computational cost remains fairly similar to using the joint optimization alone. In order to address situations where an insufficient number of matches are found in the previous steps, we implemented a failure recovery mechanism\nusing BoVW. This process is summarized in the green block of Fig. 1.\nJoint multi-objective pose optimization. We compute the camera pose by concurrently minimizing photometric and geometric residuals. Since both types of descriptors have different but complementary properties, a utility function is introduced to analyze the current observations and accordingly modify the weights of each residual type as the optimization progresses. This scalarized multi-objective optimization is summarized as:\nargmin \u03be e(\u03be) = argmin \u03be [ \u2225ep(\u03be)\u2225\u03b3 np\u03c32p +K \u2225eg(\u03be)\u2225\u03b3 ng\u03c32g ] , (1)\nwhere K is the utility function\u2019s output, n is the count of each feature type, \u03c32 is the residual\u2019s variance2, \u2225 \u00b7 \u2225\u03b3 is the Huber norm, and the energy per feature type is defined as:\nep(\u03be) = (r TWr)p, and eg(\u03be) = (r TWr)g, (2)\nr is the vector of stacked residuals per feature type, i.e., rp represents the photometric residuals (pixel intensity differences) and rg represents the geometric re-projection residuals (pixel distance differences). Finally,\nW = 1  1\u03c32d max ( 1 \u03c32d )  , (3)\nis a weight matrix that dampens the effect of landmarks with large depth variance on the pose estimation process.\nLogistic utility function. The logistic utility function provides a mechanism to steer the multi-objective optimization as it progresses, allowing us to incorporate prior information on the behaviour of the different descriptor types. For example, pixel-based residuals have a small convergence basin [12, 8],\n2Residual balancing is a very important aspect of multi-objective optimization that is often neglected by VSLAM practitioners, leading to fallacies such as the joining of posepose constraints with geometric re-projection errors. In this work we balance the residuals by normalizing against their variance and number of measurements.\nwhereas geometry-based residuals are better behaved when starting the optimization from a relatively far initializing point. As such, the utility function gives higher weights to the geometric residuals in the early stages of the optimization then gradually shifts the weight towards the pixel-based residuals. Similarly, geometric residuals are prone to outliers in texture-deprived environments and under motion blur. The proposed logistic utility function decreases the influence of the geometry-based residuals when the number of feature matches is low. Both of these effects are captured by:\nK = 5e\u22122l\n1 + e 30\u2212Ng 4\n, (4)\nwhere l is the pyramid level at which the optimization is taking place, and Ng is the number of current inlier geometric matches."
        },
        {
            "heading": "3.3. Mapping",
            "text": "Local map (Fig. 1 - red block). This map follows a similar definition of direct VO systems [9]; it consists of a moving window that contains 7 keyframes. As new keyframes are added, the oldest keyframes are marginalized, ensuring a fixed-size local map as described in [24]. The map also contains a set of landmarks (features) associated with both patches of pixels and ORB descriptors, and whose depth estimate can be modified within a local photometric BA. Since keyframe addition and removal is performed through marginalization, the local map also has prior factors that encode the probability distribution of the remaining states in the local map given the marginalized redundant data. This process makes local maps difficult to modify as any edits or subsequent post-processing like loop closures would render the prior factors meaningless, and introduce significant errors into the local BA.\nIn our previous work [1], we extended the local map beyond the currently active set of features to include recently marginalized landmarks that can still be matched to the latest keyframe using their ORB descriptors. This allows recently marginalized landmarks to contribute towards the pose estimation. However, since these landmarks have a different paramterization than their local counterparts, we could not re-activate them within the local window, nor could we maintain them for future re-use. Instead, keyframes in the extended local map along with their features were completely dropped whenever all of the corresponding features were no longer observed in the latest keyframe.\nIn this work, we leverage this extended set of keyframes and landmarks beyond assisting the local active map, to build a global and queryable map that enables loop closure and allows for future feature re-use within the local window.\nGlobal map. To clarify, we do not maintain two separate representations. Both global and local maps are made of the same keyframes and landmarks using the same inverse depth representation. The difference lies in the fact that the global map only comprises keyframes and landmarks that have been marginalized from the local map and are no longer part of the local photometric BA. Once marginalized, the patch of pixels descriptors associated with the features are removed to minimize memory usage. However, we retain their inverse depth and variance estimates, which are held fixed until their ORB descriptor matches a feature from the active map. At this point, we propose two different mechanisms to re-use and update the global features.\n\u2022 Early adoption: before adding new map points to the local map, we perform feature matching between the global map and the newly added keyframe. If a match is found, the corresponding global map point is then re-activated in the local map by assigning a local patch of pixels descriptor extracted from the new keyframe. Additionally, its depth estimate and variance are initialized using the last estimates obtained before the map point was marginalized from the local map.\n\u2022 Late fusion: during map maintenance (the update local map block in Fig. 1), we check for matches between the currently active map points and the global ones. If a match is found, we check if the projected depth estimate of the global point is in close proximity of the local one (i.e., the local depth \u22122\u03c3d \u2264 projected global depth \u2264 local depth +2\u03c3d). If this condition is met, the global map point is re-activated and assigned to the local map point by fusing the information from their observed keyframes, as well as by combining their depth estimates and variance using [10]-Eq. (13).\nOnce a global map point is re-activated, its depth estimate and variance are maintained using the local photometric BA until it is marginalized again, thereby eleminating the need for separate map maintenance processes."
        },
        {
            "heading": "3.4. Loop closure with hybrid graphs",
            "text": "The availability of temporally connected and co-visibility information provides a valuable set of constraints that relate the keyframes and their observed landmarks. This can be leveraged to perform various types of optimizations (e.g. PGO, full BA, etc.) using various techniques (discussed in sec. 2.2).\nLoop closure starts by parsing the newly added keyframe into a BoVW model [18] to generate a global keyframe descriptor, which is then used to query the global database of keyframes for potential matches. To prevent spurious detections (which are common in LDSO [2]), candidates connected to the latest keyframe in the co-visibility graph are discarded. If no loop candidates are found, which is the common case for most keyframes, the loop closure thread terminates. However, if a loop candidate is detected, we perform a 3D-3D map point matching between the loop candidate keyframe and the most recently added one, then use the matches to compute a corrective Sim(3) in a RANSAC implementation of [22]. The corrective Sim(3) is used to establish more 3D-3D map point matches from keyframes connected to both sides of the loop. In other words, we query the co-visibility graphs of both sides of the loop to build a set of keyframes, from which we compute more 3D-3D matches and use them to further refine the Sim(3) estimate. This typically returns a much larger number of inlier matches compared to\nsystems like LDSO, and eventually reduces the risks of incorporating erroneous transforms into the map. Moreover, if an insufficient number of matches are found, the loop closure thread rejects the candidate keyframe; otherwise, it uses the corrective Sim(3) to correct the poses of all keyframes that contributed feature matches from one side of the loop. Since we rely on a moving active window to explore the scene, we follow a similar approach to [2] by fixing the more recent side of the loop and correcting the old observations. Aside from not breaking the priors in the local window, this has the advantage of running the loop closure correction without the need to lock the mapping thread. Thus, regular pose estimation and mapping processes can continue normally in their respective threads while the loop closure correction takes place in the third parallel thread.\nThe corrected keyframes are then considered fixed, and a PGO is used to correct the remainder of the path as described in [5]-Eq. (9). The key differentiating factor between this approach and ORB-SLAM\u2019s lies in the type of constraints fed into the system. While ORB-SLAM can only use covisibility constraints, we employ several sources. In particular, the temporally connected keyframes provide pose-pose constraints in feature deprivedenvironments, allowing the optimization to smooth the path in these locations. We also include the poses of removed keyframes as part of the posepose constraints; these can be thought of as control points that can further help constrain the traversed trajectory.\nPGO is relatively fast to compute, requiring approximately 400 ms to correct a path made of 700 keyframes. It achieves this speed by discarding 3D observations during its path correction, which makes the results sub-optimal. To achieve optimal results, we further refine the loop closure by performing a full inverse depth BA using all connectivity and 3D map point observations. The connected graph of the full inverse depth BA is shown in Fig. 3. Note that this step is not possible in odometry methods or direct SLAM methods like LDSO as the necessary information is not tracked. It is also not possible in previously proposed hybrid methods as they maintain different map representations for the local and global maps. Although the computation for this step is relatively slow, it can be executed without interfering with other thread operations. This is because it considers all keyframes from or newer than the active window at the time of loop closure detection as fixed, and only modifies marginalized keyframes and map points."
        },
        {
            "heading": "4. Evaluation",
            "text": "We evaluate our proposed SLAM system qualitatively and quantitatively on EuRoC [26], KITTI [27], TUM mono [28], and TUM VI [29] datasets. We further split the evaluation into odometry and SLAM modes. We then assess the performance of our system in each mode of operation using applicable dataset sequences."
        },
        {
            "heading": "4.1. Quantitative results",
            "text": ""
        },
        {
            "heading": "4.1.1. Trajectory error",
            "text": "In this section, we evaluate the accuracy of our proposed system in both VO and SLAM modes against state-of-the-art methods, namely LDSO [2], DSO [9], ORB-SLAM3 [20], and DSM [23]. We repeat each sequence from EuRoC, KITTI, and TUM VI datasets 10 times, and we report on the median absolute trajectory error (in meters). The results are summarized in Tab. 1.\nAs for the TUM mono dataset [28], it cannot be used to evaluate SLAM systems; despite the dataset\u2019s wide ranging scenarios, it does not contain ground truth information for the entire paths, as it only provides localization information for the beginning and end of each sequence. Evaluating a system that can perform loop closure on this dataset will subsequently yield overoptimistic results that does not reflect the true performance. For this reason, we disable loop closures on this dataset and evaluate the odometry systems while enabling global map point re-use. The alignment errors are reported in Tab. 2"
        },
        {
            "heading": "4.1.2. Computational cost",
            "text": "The computational cost and memory footprint analysis was performed on the same CPU Intel core i9-8950 @ 2.9GHz with 32 GB RAM; no GPU acceleration was used. To gauge tractability of the SLAM systems we consider the following metrics:\n\u2022 Tracking time (ms): the time required for the system to process an input frame and generate the corresponding camera pose as output. Tracking is performed every frame.\n\u2022 Mapping time (ms): the time for the mapping thread to finish integrating a new keyframe into the map. Mapping is run whenever a new keyframe is added.\n\u2022 Loop closure time (ms): the duration required to perform loop closure error estimation and correction. This process runs fairly infrequently, i.e., whenever a loop closure is detected.\n\u2022 Memory cost: the amount of memory (MB) consumed by the system per keyframe.3\n\u2022 Loop closure found: The number of independent sequences per dataset in which at least 1 loop closure is detected.\nSimilarly, we run each sequence 10 times and compute the median of the stats per sequence; we then report on the mean performance of each system across all different sequences per dataset. The results are shown in Tab. 3. Note that our system, LDSO, and ORB-SLAM3 use a BoVW dictionary to detect loop closure candidates. The dictionary must be loaded in memory and has a constant size (not included in the table). In contrast, DSM does not perform loop closure, and therefore does not require the dictionary; however, we don\u2019t report on the numbers for DSM in the table as its performance was sub-realtime on the majority of the datasets we tested and failed on a significant amount of sequences outside EuRoC. For those interested, on the datasets where it actually succeeded, the average tracking time was 10 ms, the mapping time 660 ms and the memory consumed was about 9.26 MB per keyframe."
        },
        {
            "heading": "4.2. Qualitative Results",
            "text": "Fig. 4 shows the constraints between the keyframes available for optimization. We plot our proposed hybrid graph vs LDSO\u2019s pose-pose graph vs ORB-SLAM\u2019s co-visibility graph. The use of both co-visibility and pose-pose\n3We don\u2019t report on the total memory cost as it can vary significantly across datasets and sequences. Instead, we rely on a metric that can be consolidated over several sequences.\ngraphs allows our system to generate a relatively dense network of constraints when compared to ORB-SLAM or LDSO (as shown in Fig. 4). This is because the hybrid co-visibility graph keeps track of both temporally connected keyframes and those that are connected by common observations. The former allows the system to establish a prior on keyframe motion in featuredeprived environments, while the latter allows it to establish connections based on re-observed features, which can be especially helpful after performing loop closure (encircled areas in Fig. 4). This allows us to recognize and add new connections between keyframes that were once unconnected due to large drift, and accordingly re-use global points in the local odometry window post a loop closure event. On the other hand, LDSO\u2019s pose-pose constraints cannot be updated to account for new constraints when loop closure takes place, and global map points cannot be re-used.\nNote that ORB-SLAM employs a keyframe culling strategy by removing redundant keyframes from its map, resulting in a fairly pruned set of covisibility constraints. We employ a similar strategy, except we keep the culled keyframes to act as temporal pose-pose constraints."
        },
        {
            "heading": "5. Discussion",
            "text": ""
        },
        {
            "heading": "5.1. Processing time",
            "text": "The average tracking time per frame for our system is 13 ms, during which an indirect optimization first takes place, followed by a joint multi-objective optimization. In contrast, LDSO being a direct system, only requires around 10.6 ms to perform direct image alignment. On the other hand, ORB-SLAM3 needs an average of 21.7 ms to extract features from multiple pyramid levels and compute the frame\u2019s pose.\nThe entire mapping process of our approach requires 75 ms per keyframe on average to generate and maintain both the direct and indirect global maps. In comparison, the mapping thread of LDSO requires 117 ms to process a keyframe, while ORB-SLAM3 requires 230 ms. Note that ORBSLAM\u2019s mapping process performs a local BA every time a new keyframe is added; while this results in improved accuracy, it comes at the cost of increased mapping time, requiring almost three times as much as our system to maintain its global map. Finally DSM requires about 660 ms per keyframe. This significant increase in computational cost is mainly due to the use of a photometric global map that DSM attempts to query when a new keyframe is added. Additionally, DSM performs pyramidal photometric BA on three\nlevels, contributing to the higher time requirements. In contrast, our method, along with LDSO, perform the local photometric BA on a single pyramid level. It\u2019s worth noting that our system\u2019s speed in maintaining both local and global maps is comparable to pure odometry methods like DSO, which only maintains a local map at a cost of 62 ms per keyframe.\nAs for loop closure, LDSO is typically the fastest as it only performs a PGO, whereas ORB-SLAM3 and our system perform a full BA. However, since this process runs on a third parallel thread, the longer processing time does not significantly impact the overall SLAM\u2019s performance. This is especially true in our case since we adopt a similar strategy to LDSO, i.e., we update the old trajectory and keep the latest keyframes fixed in order to maintain coherence in the local photometric optimization priors."
        },
        {
            "heading": "5.2. Memory Cost",
            "text": "We employ several strategies to keep our memory consumption low. By maintaining a co-visbility graph, it is possible to apply a pruning strategy similar to that used in ORB-SLAM. We remove redundant keyframes from the global map that have a high degree of overlap (more than 90%) with other keyframes in terms of shared map points. However, unlike ORB-SLAM, our system will still utilize pruned keyframes in the pose-pose constraints of our hybrid graph, which can provide better constraints to the overall trajectory for long range loop closures. Moreover, the use of descriptor sharing directly translates to reduced memory cost as we do not need to keep track of separate landmark information for local-global representations. Compared to our proposed approach, LDSO consumes about 6% more memory (MB) per keyframe, despite not maintaining a global reusable 3D map. ORB-SLAM3, on the other hand, consumes 120% more. These numbers highlight the significant impact of descriptor sharing and the use of a single representation for both local and global maps in reducing memory overhead in our system."
        },
        {
            "heading": "5.3. Trajectory accuracy",
            "text": "Our proposed SLAM system achieves better performance than LDSO on most sequences, even though both systems use a local moving window to explore the scene. The performance improvement is attributed to three reasons: (1) the improved accuracy of the hybrid pose estimation process, (2) the improved connectivity graph that contains both co-visibility and posepose constraints, and (3) the global BA that optimizes the global map. On\nthe other hand, our system scores between ORB-SLAM3 and DSM, outperforming them on few sequences, while being a close second candidate on others.\nThe mixed results can be attributed to several factors, the most prominent one being the small sized environments of the datasets themselves: all the sequences of EuRoC and TUM VI that have ground truth poses take place in small rooms. In such scenarios, ORB-SLAM\u2019s local BA encompasses the entire map and behaves like a global BA, optimizing over the total map for every keyframe added. This, however, results in a significantly increased computational cost per keyframe, requiring more than 240 ms in EuRoC and more than 300 ms in TUM VI. Whereas in exploratory datasets like KITTI, the computational cost drops to 130 ms, and so does their trajectory performance.\nIn contrast, our system maintains a strictly fixed seven-keyframe window for performing local BA. A global BA is only triggered at loop closure, which happens fairly infrequently in small rooms as we identify and re-use previously observed global map points without loop closure. This is shown in Tab. 3, where loop closure is only triggered in 3 out of the 11 sequences of the EuRoC dataset. In the remaining sequences, full BA is never performed and our system operates as a pure odometry system with the ability to re-use global points. The impact of this ability is contrasted when comparing the results of our system in strictly VO mode, where the use of global points is disabled (shown in Tab. 1 under \u201dours (strict VO)\u201d).\nAnother reason for the performance difference in exploratory sequences could be the lack of photometric calibration in the KITTI dataset, which puts our system at a disadvantage. The impact of this issue is mitigated in the TUM mono dataset where we evaluate our system in odometry mode (no loop closure) but with global map points re-use enabled, and compare it to the same setup in ORB-SLAM3 and DSO. The results shown in Tab. 2 demonstrate that our system outperforms both of them on the majority of the sequences."
        },
        {
            "heading": "5.4. Limitations and further improvements",
            "text": "While the proposed system can theoretically perform at least as good as ORB-SLAM3, our current implementation under-performs in small roomlike environments (reasons discussed in 5.3). The underlying problem in our current implementation is that we don\u2019t re-activate previously marginalized keyframes. Instead, we add new keyframes and re-activate old map points\nwithin them. This was done to cater to the needs of the local moving window photometric BA, where we aim to refresh the global map points using recent photometric pixel patches. DSM, on the other hand, addresses this issue by re-activating previously marginalized keyframes, not just map points, through a pyramidal photometric BA that can handle large baselines and intensity changes. However, this is computationally very expensive and makes their system sub-realtime, which contradicts our overall goal of achieving high performance at low computational cost. In our system, we have observed the potential of using the presence of indirect residuals in the map to enhance the resilience of the local photometric BA to large baselines and intensity changes. Similar to what we have done in joint multi-objective tracking, leveraging these indirect residuals could enable us to re-activate old keyframes. This remains an open problem that we plan to address in our future work."
        },
        {
            "heading": "6. Conclusion",
            "text": "In this work, we have demonstrated the advantages of combining direct and indirect formulations through a descriptor sharing approach. This integration enables us to create a unified representation for both local and global maps, introducing several key capabilities that are typically absent in individual frameworks or other hybrid methods. These capabilities include the ability to perform global Bundle Adjustment on a shared map representation, to re-activate previously observed map points, to perform keyframe culling, and to extract and maintain hybrid connectivity graphs. By incorporating both temporally connected and co-visibility constraints, our approach allows for loop closure using concurrent Pose Graph Optimization over both types of constraints. Additionally, the sub-optimal results of the PGO are further refined in a global inverse depth BA, a process not typically feasible in other hybrid methods. We have validated the system\u2019s capabilities through its computational and memory efficiency, showcasing competitive performance compared to other methods on most tested sequences."
        },
        {
            "heading": "Acknowledgment",
            "text": "The authors would like to thank the Ontario Centres of Excellence (OCE), the University Research Board (URB) at the American University of Beirut, and the Natural Sciences and Engineering Research Council of Canada\n(NSERC) for their support in conducting this research. The work was further supported by the Didymos Horizon Europe project under grant number 101092875\u2013DIDYMOS-XR."
        }
    ],
    "title": "H-SLAM: Hybrid Direct-Indirect Visual SLAM",
    "year": 2023
}