{
    "abstractText": "The recent success of the CLIP model has shown its potential to be applied to a wide range of vision and language tasks. However this only establishes embedding space relationship of language to images, not to the video domain. In this paper, we propose a novel approach to map video embedding space to natural langugage. We propose a two-stage approach that first extracts visual features from each frame of a video using a pre-trained CNN, and then uses the CLIP model to encode the visual features for the video domain, along with the corresponding text descriptions. We evaluate our method on two benchmark datasets, UCF101 and HMDB51, and achieve state-of-the-art performance on both tasks.",
    "authors": [
        {
            "affiliations": [],
            "name": "Phani Krishna Uppala"
        },
        {
            "affiliations": [],
            "name": "Abhishek Bamotra Shriti"
        },
        {
            "affiliations": [],
            "name": "Priya Vaidehi Joshi"
        }
    ],
    "id": "SP:83a5831ccf6462dffbbc3f2f436bf560cc920639",
    "references": [
        {
            "authors": [
                "Tom B. Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell",
                "Sandhini Agarwal",
                "Ariel Herbert-Voss",
                "Gretchen Krueger",
                "Tom Henighan",
                "Rewon Child",
                "Aditya Ramesh",
                "Daniel M. Ziegler",
                "Jeffrey Wu",
                "Clemens Winter",
                "Christopher Hesse",
                "Mark Chen",
                "Eric Sigler",
                "Mateusz Litwin",
                "Scott Gray",
                "Benjamin Chess",
                "Jack Clark",
                "Christopher Berner",
                "Sam McCandlish",
                "Alec Radford",
                "Ilya Sutskever",
                "Dario Amodei"
            ],
            "title": "Language models are few-shot learners",
            "year": 2020
        },
        {
            "authors": [
                "J. Deng",
                "W. Dong",
                "R. Socher",
                "L.-J. Li",
                "K. Li",
                "L. Fei-Fei"
            ],
            "title": "ImageNet: A Large-Scale Hierarchical Image Database",
            "venue": "In CVPR09,",
            "year": 2009
        },
        {
            "authors": [
                "Christoph Feichtenhofer",
                "Haoqi Fan",
                "Jitendra Malik",
                "Kaiming He"
            ],
            "title": "Slowfast networks for video recognition, 2019",
            "year": 2019
        },
        {
            "authors": [
                "Raghav Goyal",
                "Samira Ebrahimi Kahou",
                "Vincent Michalski",
                "Joanna Materzy\u0144ska",
                "Susanne Westphal",
                "Heuna Kim",
                "Valentin Haenel",
                "Ingo Fruend",
                "Peter Yianilos",
                "Moritz Mueller-Freitag",
                "Florian Hoppe",
                "Christian Thurau",
                "Ingo Bax",
                "Roland Memisevic"
            ],
            "title": "The \"something something\" video database for learning and evaluating visual common sense, 2017",
            "year": 2017
        },
        {
            "authors": [
                "Neeraj Kumar",
                "Phanikrishna Uppala",
                "Karthik Duddu",
                "Hari Sreedhar",
                "Vishal Varma",
                "Grace Guzman",
                "Michael Walsh",
                "Amit Sethi"
            ],
            "title": "Hyperspectral tissue image segmentation using semi-supervised nmf and hierarchical clustering",
            "venue": "IEEE Transactions on Medical Imaging,",
            "year": 2019
        },
        {
            "authors": [
                "Jogendra Nath Kundu",
                "Maharshi Gor",
                "Phani Krishna Uppala",
                "Venkatesh Babu Radhakrishnan"
            ],
            "title": "Unsupervised feature learning of human actions as trajectories in pose embedding manifold",
            "venue": "IEEE Winter Conference on Applications of Computer Vision (WACV),",
            "year": 2019
        },
        {
            "authors": [
                "Jogendra Nath Kundu",
                "Jay Patravali",
                "Venkatesh Babu RADHAKRISHNAN"
            ],
            "title": "Unsupervised cross-dataset adaptation via probabilistic amodal 3d human pose completion",
            "venue": "In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision,",
            "year": 2020
        },
        {
            "authors": [
                "Jogendra Nath Kundu",
                "Phani Krishna Uppala",
                "Anuj Pahuja",
                "R. Venkatesh Babu"
            ],
            "title": "Adadepth: Unsupervised content congruent adaptation for depth estimation",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2018
        },
        {
            "authors": [
                "Jie Lei",
                "Linjie Li",
                "Luowei Zhou",
                "Zhe Gan",
                "Tamara L. Berg",
                "Mohit Bansal",
                "Jingjing Liu"
            ],
            "title": "Less is more: Clipbert for video-and-language learning via sparse sampling, 2021",
            "year": 2021
        },
        {
            "authors": [
                "Konda Reddy Mopuri",
                "Phani Krishna Uppala",
                "R. Venkatesh Babu"
            ],
            "title": "Ask, acquire, and attack: Data-free uap generation using class impressions",
            "venue": "In Proceedings of the European Conference on Computer Vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark",
                "Gretchen Krueger",
                "Ilya Sutskever"
            ],
            "title": "Learning transferable visual models from natural language supervision, 2021",
            "year": 2021
        },
        {
            "authors": [
                "Motoki Sato",
                "Jun Suzuki",
                "Hiroyuki Shindo",
                "Yuji Matsumoto"
            ],
            "title": "Interpretable adversarial perturbation in input embedding space for text",
            "venue": "arXiv preprint arXiv:1805.02917,",
            "year": 2018
        },
        {
            "authors": [
                "Ramprasaath R. Selvaraju",
                "Michael Cogswell",
                "Abhishek Das",
                "Ramakrishna Vedantam",
                "Devi Parikh",
                "Dhruv Batra"
            ],
            "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization, Oct 2019",
            "year": 2019
        },
        {
            "authors": [
                "Lucas Smaira",
                "Jo\u00e3o Carreira",
                "Eric Noland",
                "Ellen Clancy",
                "Amy Wu",
                "Andrew Zisserman"
            ],
            "title": "A short note on the kinetics-700-2020 human action",
            "year": 2020
        },
        {
            "authors": [
                "Monorama Swain",
                "Aurobinda Routray",
                "P Kabisatpathy",
                "Jogendra N Kundu"
            ],
            "title": "Study of prosodic feature extraction for multidialectal odia speech emotion recognition",
            "venue": "IEEE Region 10 Conference (TENCON),",
            "year": 2016
        },
        {
            "authors": [
                "Pedro Tabacof",
                "Eduardo Valle"
            ],
            "title": "Exploring the space of adversarial images",
            "venue": "In 2016 International Joint Conference on Neural Networks (IJCNN),",
            "year": 2016
        },
        {
            "authors": [
                "Phani Krishna Uppala"
            ],
            "title": "Exemplar-free video retrieval",
            "year": 2021
        },
        {
            "authors": [
                "Phani Krishna Uppala",
                "Abhishek Bamotra",
                "Raj Kolamuri"
            ],
            "title": "Dynamic object removal for effective slam",
            "year": 2023
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need, 2017",
            "year": 2017
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Text query based image retrieval systems have recently shown drastic improvement [11, 9, 17]. These approaches compute textual embedding for the given sentence using a language models like GPT-3 [1] and compute visual embedding using a pre-trained Imagenet [2]. It is then trained for a matching function, which can be thought of as a learned similarity (ie. weighted - dot product). Since we are using dot product to find the maximum match, for any given text embedding there will exist a sub-space of visual embedding which maximizes the dot product. Do all samples in this visual embedding sub-space correspond to the coherent images? Or do some samples in this sub-space form an adversarial image? We will explore this direction with a goal of decoding the visual embedding space of a given text query embedding. Further, consecutive images in a video have both similar semantic and pixel-value distribution. Does this translate to videos being represented by continuous path walks in the visual embedding sub-space?\nIn order to answer the above questions, we want to use multiple tools including visualizations. Visualizations reveal what the network is looking at. For a given sentence embedding, we first find sub-space of visual embedding that match with the sentence embedding. Using these visual embedding and visualization techniques, we find the images that will produce a visual embedding corresponding to the textual embedding within the sub-space of interest.\nContent Based Video Retrieval [egs: YouTube] is desirable because searches that rely purely on metadata are dependent on annotation quality and completeness. The presented architecture provides an efficient way of Text based Video Retrieval. Also, our work is an extension of CLIP Model Action Recognition [RN50x16], where they have used Mid-frame level approach to get an accuracy of 53.4%, while our method gives an improvement of 83.9% [LSTM] and 85.5% [Transformer] comparing to the SOTA on Kinetics 400 dataset 84.8%.\nState of the art computer vision methods [5, 7] are usually structured for a fixed set of output categories or a fixed training data distribution on which the model is trained on. This imposes a restricted form of supervision on learning. This restriction indicates that the model is only going to recognize and classify into one of the output classes on which it has learned on or on which it has been trained on. This form of restricted supervision does not prove to be helpful in a real world scenario wherein there could be visual concepts from all sorts of classes on which the model has not been trained on previously. Thus, the restricted form of supervision can often limit the generality and usability of the SOTA methods for performing well on other visual concepts. In such a scenario, in order to learn new visual concepts, some form of context can help the model to perform a valid inference at test time and thus learn new visual concepts simultaneously with some help from the context information. With respect to context, zero-shot transfer, natural language supervision and\nar X\niv :2\n30 3.\n14 58\n4v 2\n[ cs\n.C V\n] 8\nA pr\n2 02\n3\nmultimodal learning are capable of providing this extra information or context required for the model to learn from and to accurately infer from. Open set recognition also proves to be helpful and has been proven to work well with real life examples or scenarios. In open set recognition, incomplete knowledge of the world is present at training time and unknown classes can be submitted to an algorithm during testing. Open set recognition handles these unknown and unseen classes of images efficiently at test time based on certain statistical modelling. We aim to explore more into the Open set recognition approach through our project."
        },
        {
            "heading": "2 Related Works",
            "text": ""
        },
        {
            "heading": "2.1 Open Set",
            "text": "Deep neural networks have made breakthroughs in a wide range of visual understanding tasks. A typical challenge that hinders their real-world applications is that unknown samples may be fed into the system during the inference phase, but traditional deep neural networks or SOTA computer vision methods will wrongly recognize these unknown samples as one of the known classes. Open set recognition (OSR) is a potential solution to overcome this problem, where the open set classifier should have the flexibility to handle unknown samples and meanwhile maintain high classification accuracy in known classes. Consequently, there has been a lot of relevant prior work in this area. Open Set Recognition with Conditional Probabilistic Generative Models [2, 10] proposes an open set classifier which has the flexibility to reject unknown samples posed to the model at test time. In this paper, a novel framework, called Conditional Probabilistic Generative Models (CPGM), for open set recognition is proposed. The core insight of this work is to add discriminative information into the probabilistic generative models, such that the proposed models can not only detect unknown samples but also classify known classes by forcing different latent features to approximate conditional Gaussian distributions. Unified Probabilistic Deep Continual Learning through Generative Replay and Open Set Recognition [3] introduces a probabilistic approach to unify open set recognition with the prevention of catastrophic forgetting in deep continual learning, based on variational Bayesian inference[8]. In order to successfully distinguish unseen unknown data from trained known tasks, the paper proposes to bound the class specific approximate posterior by fitting regions of high density on the basis of correctly classified data points. These bounds are further used to significantly alleviate catastrophic forgetting by avoiding samples from low density areas in generative replay. There is much more prior work which involves rejecting or avoiding the unseen or unknown input samples (that are out of the training data distribution or are from a different output class category) at inference time.\nWe aim to propose an approach where we perform an Open Set Recognition (OSR) on the unknown or unseen samples but instead of avoiding or rejecting them, we find a way to automatically handle these samples at the inference time. In order to explore this approach more in the open set conditions, we also aim to perform experiments for Video Retrieval from textual input using an image based CLIP model, but on a Video input dataset."
        },
        {
            "heading": "2.2 Image Retrieval",
            "text": "The neural network will be trained on text and visual embedding pairs to make it learn a mapping between text and corresponding images[6]. Based on a supervised learning approach, labels, Image and text embedding will be used to match the unseen text data examples to a corresponding image. Likewise, we will achieve Image retrieval."
        },
        {
            "heading": "2.3 Neuron visualizations",
            "text": "As mentioned in the Introduction, we aim to use visualizations as a tool to understand what exactly a network is looking at. For this purpose, we need to find a match or correlation between the sentence embedding and a certain subspace of visual embedding[15, 18]. We propose to do so by using different neuron visualization techniques. Some of them include visualizing receptive fields or by visualizing activation through backpropagation. Both of these can lead us to finding the subspace of visual embedding that we are looking for. Visual embedding subspace or visual explanations of what a network sees at every stage of the learning process can also be achieved using Gradient-Weighted\nClass Activation Mapping [13] which can prove in extrapolating semantic sense from the network\u2019s learning process."
        },
        {
            "heading": "2.4 Adversarial space",
            "text": "CLIP architecture recognizes a wide variety of visual concepts in images and associate them with their names. Since, there is a large association between the image embedding and visual embedding space, a slight variation of the input image, can potentially cause the architecture to produce adversarial output. As discussed in [16], we want to explore the space of adversarial images in this architecture. Also, if possible and feasible, the idea of \"restricting the directions of perturbations toward the existing words in the input embedding space\" as mentioned in the paper [12] could also be explored for the CLIP architecture."
        },
        {
            "heading": "2.5 Video Understanding/Action recognition/Video retrieval",
            "text": "Action recognition is one of the actively researched areas in computer vision. This has led to collection of large sized datasets like kinetics [14], something-something [4], AVA etc. Most of the work is focused on classifying the actions given temporal and spatial localized clips as an input [3]. A different stream of research that works on non localized input is also being explored. This works by first generating proposals from the input videos and later classifying the proposals. Our work focuses on a different way of exploring video understanding, by looking at video as a continuous path in an image embedding space."
        },
        {
            "heading": "3 Approach",
            "text": "NLP models are usually trained on all the available text on the internet, we want to take advantage of this for open set video retrieval. We do this by projecting videos into an visual embedding, which lie in the same joint embedding space as of text, Following an approach similar to what CLIP did for images. We first extend the this clip image based model to videos. Given a video our approach projects it into the joint embedding space. We evaluate the quality of this model using action recognition on kinetics. Then we remap the video classification model into retrieval setup. Given a textual query, our setup retrieves all the videos in the video database that match the textual query."
        },
        {
            "heading": "3.1 Baseline",
            "text": "Towards the first contribution of converting image based CLIP to video classification. We establish the following baselines In first visual embeddings are extracted individually for the frames, followed\nby max pooling. In second each image is individually classified, followed by majority vote to make the final prediction."
        },
        {
            "heading": "3.2 Temporal fusion using LSTM",
            "text": "In further experimentation, we want to project an entire video as an embedding in the joint embedding space. Towards this we used a LSTM module that takes in series of image embeddings from the video and projects into a single embedding in the joint embedding space of GPT-3. Dot product of textual embedding with visual embedding to get classification logits. Trained this on kinetics, and will show results in the upcoming slides."
        },
        {
            "heading": "3.3 Temporal fusion using Transfomer",
            "text": "We improved upon the previous approach by using a multi headed attention blocks to extract a visual embedding representing the input video."
        },
        {
            "heading": "3.4 Retrieval",
            "text": "Finally we remapped the video classification model for retrieval. Using this pipeline given any textual query we retrieve all the videos that match the textual query. To do this we use GPT-3 to first convert text query into an embedding. Then compute the visual embedding for all the videos in the database using our classification model.(This step only needs to be done once and can reused across queries). And use the dot product between the video embeddings and text embedding to get similarity score. And returns the samples with highest similarity, dot product score."
        },
        {
            "heading": "4 Results",
            "text": "The first task towards Video Retrieval requires the best action recognition model. Thus, we tried four different extensions of Image based CLIP Model to work for action recognition on videos. For the baseline we did a maxpool over all the frames in a video, and used CLIP Architecture to give the top voted class. Another, baseline architecture is taking majority votes over all the frames in a video. This gave a performance of 55.02\nHowever, the above models did not take into account the temporal relationships between the frames in a video. We achieved that by adding LSTM/Transformer[19] to the existing CLIP Architecture, which gave a performance of 83.9\nThe original CLIP paper [1], takes a mid frame approach (taking middle frame in video and performing image-based classification on that) for action recognition and achieves an accuracy of only 53.4 percent. Thus, adding temporal information improved the performance on video action recognition by around 32 percent.\nThe above model has been tested on only 25 classes of kinetics400 dataset, due to limited resources available on AWS in terms of video storage, frame storage and training. Also, we downsampled the video 100 frames per video, which might also have reduced the performance. In future, we would keep video length as a hyperparameter too.\nWe can see the loss curve of training and validation loss. The loss curve indicates overfitting, and the possible reason could be less number of classes and examples in the train dataset. We see similar behavior in both CLIP + LSTM and CLIP + Transformer Architecture."
        },
        {
            "heading": "4.1 Action Recognition",
            "text": "4.2 Retrieval\n4.3 Emeddings"
        },
        {
            "heading": "5 Conclusion",
            "text": "Conclusion We extended the above trained models to Video Retrieval Task and got a remarkable result on image retrieval as indicated in the pictures. The way it works is that we calculate the text embedding of the entered query and get the embeddings of all the videos calculated using the above trained models (variations of CLIP) and get all the videos similar to the text embedding, by calculating the cosine similarity between text and video in the embedding space.\nAll the frames for a video cluster together as evident in the embedding space visualization (t-SNE - reduced from 1024 dimension to 2 dimension) of the video embedding corresponding to selected 25 classes.\nThe generalisation and performance of the model was done on two different datasets - Kinetics400 and VIRAT dataset as shown in Figures. Content Based Video Retrieval [egs: YouTube] is desirable because searches that rely purely on metadata are dependent on annotation quality and completeness. The presented architecture provides an efficient way of Text based Video Retrieval.\nAlso, our work is an extension of CLIP Model Action Recognition [RN50x16], where they have used Mid-frame level approach to get an accuracy of 53.4%, while our method gives an improvement of 83.9% [LSTM] and 85.5% [Transformer] comparing to the SOTA on Kinetics 400 dataset 84.8%."
        },
        {
            "heading": "6 Future Work",
            "text": "Our current work is based on classifying an action over the entire video. Using the same LSTM /Transformer model, we also wish to caption/add subtitle to each frame in the video with changing action. For proof of concept, we have worked on 25 classes of Kinetics 400/700 dataset. In the future, we would be extending our work on the entire dataset with 400/700 classes. We wish to see the performance on other complex datasets like MSR Action 3D and datasets like RareAct which contain unusual actions like \u201chammering a phone\u201d and \u201cdrilling an egg\u201d. This will give us true performance of all the model over unusual/unseen actions too, which will validate the generalization of the model.\nWe also wish to explore the adversarial space of CLIP Models as different attacks like text patching and adversarial perturbation."
        }
    ],
    "title": "Learning video embedding space with Natural Language Supervision",
    "year": 2023
}