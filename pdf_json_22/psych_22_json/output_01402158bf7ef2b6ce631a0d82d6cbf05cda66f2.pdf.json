{
    "abstractText": "In the last decade, soundscapes have become one of the most active topics in Acoustics, providing a holistic approach to the acoustic environment, which involves human perception and context. Soundscapes-elicited emotions are central and substantially subtle and unnoticed (compared to speech or music). Currently, soundscape emotion recognition is a very active topic in the literature. We provide an exhaustive variable selection study (i.e., a selection of the soundscapes indicators) to a well-known dataset (emo-soundscapes). We consider linear soundscape emotion models for two soundscapes descriptors: arousal and valence. Several ranking schemes and procedures for selecting the number of variables are applied. We have also performed an alternating optimization scheme for obtaining the best sequences keeping fixed a certain number of features. Furthermore, we have designed a novel technique based on Gibbs sampling, which provides a more complete and clear view of the relevance of each variable. Finally, we have also compared our results with the analysis obtained by the classical methods based on p-values. As a result of our study, we suggest two simple and parsimonious linear models of only 7 and 16 variables (within the 122 possible features) for the two outputs (arousal and valence), respectively. The suggested linear models provide very good and competitive performance, with R > 0.86 and R > 0.63 (values obtained after a cross-validation procedure), respectively.",
    "authors": [
        {
            "affiliations": [],
            "name": "R. San Mill\u00e1n-Castillo"
        },
        {
            "affiliations": [],
            "name": "L. Martino"
        },
        {
            "affiliations": [],
            "name": "E. Morgado"
        },
        {
            "affiliations": [],
            "name": "F. Llorente"
        }
    ],
    "id": "SP:bcd2fccdcb602a9541f5e503396c48cc28cf488a",
    "references": [
        {
            "authors": [
                "Faranak Abri",
                "Luis Felipe Guti\u00e9rrez",
                "Prerit Datta",
                "David RW Sears",
                "Akbar Siami Namin",
                "Keith S Jones"
            ],
            "title": "A comparative analysis of modeling and predicting perceived and induced emotions in sonification",
            "year": 2021
        },
        {
            "authors": [
                "Faranak Abri",
                "Luis Felipe Guti\u00e9rrez",
                "Akbar Siami Namin",
                "David RW Sears",
                "Keith S Jones"
            ],
            "title": "Predicting emotions perceived from sounds",
            "venue": "IEEE International Conference on Big Data (Big Data),",
            "year": 2020
        },
        {
            "authors": [
                "Francesco Aletta",
                "Jian Kang",
                "\u00d6sten Axelsson"
            ],
            "title": "Soundscape descriptors and a conceptual framework for developing predictive soundscape models",
            "venue": "Landscape and Urban Planning,",
            "year": 2016
        },
        {
            "authors": [
                "Francesco Aletta",
                "Jieling Xiao"
            ],
            "title": "What are the current priorities and challenges for (urban) soundscape research? Challenges",
            "year": 2018
        },
        {
            "authors": [
                "Pierre Aumond",
                "Arnaud Can",
                "Bert De Coensel",
                "Dick Botteldooren",
                "Carlos Ribeiro",
                "Catherine Lavandier"
            ],
            "title": "Modeling soundscape pleasantness using perceptual assessments and acoustic measurements along paths in urban context",
            "venue": "Acta Acustica united with Acustica,",
            "year": 2017
        },
        {
            "authors": [
                "\u00d6sten Axelsson",
                "Mats E Nilsson",
                "Birgitta Berglund"
            ],
            "title": "A principal components model of soundscape perception",
            "venue": "The Journal of the Acoustical Society of America,",
            "year": 2010
        },
        {
            "authors": [
                "William J Davies",
                "Neil S Bruce",
                "Jesse E Murphy"
            ],
            "title": "Soundscape reproduction and synthesis",
            "venue": "Acta Acustica United with Acustica,",
            "year": 2014
        },
        {
            "authors": [
                "M.A. Efroymson"
            ],
            "title": "Multiple regression analysis. Mathematical methods for digital computers, pages 191\u2013203",
            "year": 1960
        },
        {
            "authors": [
                "Jianyu Fan",
                "Miles Thorogood",
                "Philippe Pasquier"
            ],
            "title": "Emosoundscapes: A dataset for soundscape emotion recognition",
            "year": 2017
        },
        {
            "authors": [
                "Jianyu Fan",
                "Fred Tung",
                "William Li",
                "Philippe Pasquier"
            ],
            "title": "Soundscape emotion recognition via deep learning",
            "venue": "Proceedings of the Sound and Music Computing,",
            "year": 2018
        },
        {
            "authors": [
                "Andr\u00e9 Fiebig",
                "Pamela Jordan",
                "Cleopatra Christina Moshona"
            ],
            "title": "Assessments of acoustic environments by emotions\u2013the application of emotion theory in soundscape",
            "venue": "Frontiers in Psychology,",
            "year": 2020
        },
        {
            "authors": [
                "Eduardo Fonseca",
                "Jordi Pons Puig",
                "Xavier Favory",
                "Frederic Font Corbera",
                "Dmitry Bogdanov",
                "Andres Ferraro",
                "Sergio Oramas",
                "Alastair Porter",
                "Xavier Serra"
            ],
            "title": "Freesound datasets: a platform for the creation of open audio datasets",
            "venue": "Proceedings of the 18th ISMIR Conference;",
            "year": 2017
        },
        {
            "authors": [
                "E.J. Hannan",
                "B.G. Quinn"
            ],
            "title": "The determination of the order of an autoregression",
            "venue": "Journal of the Royal Statistical Society. Series B (Methodological),",
            "year": 1979
        },
        {
            "authors": [
                "G. Heinze",
                "C. Wallisch",
                "D. Dunkler"
            ],
            "title": "Variable selection - a review and recommendations for the practicing statistician",
            "venue": "Biometrical journal,",
            "year": 2018
        },
        {
            "authors": [
                "Karmele Herranz-Pascual",
                "Igone Garc\u0131\u0301a",
                "Itziar Aspuru",
                "Itxasne D\u0131\u0301ez",
                "\u00c1lvaro Santander"
            ],
            "title": "Progress in the understanding of soundscape: objective variables and objectifiable criteria that predict acoustic comfort in urban places",
            "venue": "Noise Mapping,",
            "year": 2016
        },
        {
            "authors": [
                "R.R. Hocking"
            ],
            "title": "The analysis and selection of variables in linear regression",
            "year": 1976
        },
        {
            "authors": [
                "Xinchen Hong",
                "Yu Jiang",
                "Shuting Wu",
                "Linying Zhang",
                "Siren Lan"
            ],
            "title": "Study on evaluation model of soundscape in urban park based on radial basis function neural network: A case study of shiba park and kamogawa park, japan",
            "venue": "In IOP Conference Series: Earth and Environmental Science,",
            "year": 2019
        },
        {
            "authors": [
                "Olivier Lartillot",
                "Petri Toiviainen",
                "Tuomas Eerola"
            ],
            "title": "A matlab toolbox for music information retrieval. In Data analysis, machine learning and applications, pages 261\u2013268",
            "year": 2008
        },
        {
            "authors": [
                "Matteo Lionello",
                "Francesco Aletta",
                "Jian Kang"
            ],
            "title": "A systematic review of prediction models for the experience of urban soundscapes",
            "venue": "Applied Acoustics,",
            "year": 2020
        },
        {
            "authors": [
                "F. Llorente",
                "L. Martino",
                "D. Delgado",
                "J. Lopez-Santiago"
            ],
            "title": "Marginal likelihood computation for model selection and hypothesis testing: an extensive review",
            "year": 2005
        },
        {
            "authors": [
                "Phil Lopes",
                "Antonios Liapis",
                "Georgios N Yannakakis"
            ],
            "title": "Modelling affect for horror soundscapes",
            "venue": "IEEE Transactions on Affective Computing,",
            "year": 2017
        },
        {
            "authors": [
                "D. Luengo",
                "L. Martino",
                "M. Bugallo",
                "V. Elvira",
                "Sarkka S"
            ],
            "title": "A survey of monte carlo methods for parameter estimation",
            "venue": "EURASIP J. Adv. Signal Process.,",
            "year": 2020
        },
        {
            "authors": [
                "Peter Lund\u00e9n",
                "Malin Hurtig"
            ],
            "title": "On urban soundscape mapping: A computer can predict the outcome of soundscape assessments",
            "venue": "In INTER-NOISE and NOISE-CON Congress and Conference Proceedings,",
            "year": 2016
        },
        {
            "authors": [
                "Weiyi Ma",
                "William Forde Thompson"
            ],
            "title": "Human emotions track changes in the acoustic environment",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2015
        },
        {
            "authors": [
                "L. Martino",
                "V. Elvira",
                "G. Camps-Valls"
            ],
            "title": "The recycling Gibbs sampler for efficient learning",
            "venue": "Digital Signal Processing,",
            "year": 2018
        },
        {
            "authors": [
                "L. Martino",
                "F. Llorente",
                "E. Curbelo",
                "Javier Lopez-Santiago",
                "J. Miguez"
            ],
            "title": "Automatic tempered posterior distributions for bayesian inversion problems",
            "year": 2021
        },
        {
            "authors": [
                "L. Martino",
                "J. Read",
                "D. Luengo"
            ],
            "title": "Independent doubly adaptive rejection Metropolis sampling within Gibbs sampling",
            "venue": "IEEE Transactions on Signal Processing,",
            "year": 2015
        },
        {
            "authors": [
                "L. Martino",
                "H. Yang",
                "D. Luengo",
                "J. Kanniainen",
                "J. Corander"
            ],
            "title": "A fast universal self-tuned sampler within Gibbs sampling",
            "venue": "Digital Signal Processing,",
            "year": 2015
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014Soundscape emotion, variable selection, ranking methods, best sequence search, MCMC algorithms, Gibbs sampling.\nI. INTRODUCTION\nEnvironmental noise is one of the most critical risks for population health and well-being. The World Health Organization has recently remarked that noise affects at least 100 million people, only in the European Union [41]. Generally, sound level monitoring and control are the common tools for managing the acoustic environment and sound quality remains dismissed. However, noise abatement is often unavailable or unsuitable in certain scenarios like cities, or does not necessarily result in an approving appraisal of final soundscapes [39]. Hence, \u201cquiet areas\u201d are a new perspective that focuses on the acoustic quality more than on the sound level, and which are being even regulated in the European Union [3]. This vision is limited since it is not accountable for people\u2019s experiences in different acoustic environments. Soundscapes provide an alternative and holistic approach to assess human perception, acoustic environments, and context, beyond\nthe concept of noise [33]. Thus, this subjective evaluation depends on physical, psychological, social, and even cultural estimators and their complex interactions.\nIn the last decade, soundscapes have become one of the most active topics in acoustics. In fact, the number of related research projects and scientific articles grows exponentially [5]. Research requires a sizeable sample of participants in surveys and a considerable amount of locations. These intensive and time-consuming resources may limit the soundscape approach [24]. Soundscape modeling might predict people\u2019s perception of acoustic environments at lower expenses [20]. In urban planning and environmental acoustics, the procedure consists of (a) soundscapes recording, (b) calculation of acoustic and psychoacoustic indicators of the signals, (c) collecting other context indicators (e.g. visual information [7]), and (d) ranking of soundscapes audio signals employing emotional descriptors. Finally, the model can be developed. Soundscape-elicited emotions are substantially different from those related to music or speech because they are more subtle and unnoticed. Thus, soundscape emotion recognition (SER) requires further research to support perception and context descriptors [12], [25]. Additionally to environmental acoustics and urban planning, there is an increasing research interest in SER for certain domains like sound design in films and digital games [22], or sonification in the Internet of Things (IoT) [1]. Soundscapes descriptors are identified with perceived emotions and SER becomes a relatively new sub-field of research in affective computing [10]. Rusell\u2019s circumplex model can be applied to soundscapes [7], [8], [10], [32] by scaling the perception of soundscapes. Russell's affect representation can be modeled with two main factors: arousal represents the eventfulness of the acoustic environment, and valence is the pleasantness ratio. Currently, arousal and valence are accepted as the principal and sufficient affective descriptors in research [8], [40], but there are different proposals to enhance soundscapes emotions evaluation with additional or different descriptors [4], and even to include emotion appraisal in procedures of the soundscapes standards [12].\nSoundscape modeling has been extensively and recently\nar X\niv :2\n20 7.\n12 74\n3v 1\n[ cs\n.S D\n] 2\n6 Ju\nl 2 02\n2 reviewed in [20]. Soundscapes indicators (i.e. features), soundscapes descriptors (i.e. outputs), and employed prediction models and their performances are presented. Researchers have been approaching SER from a variety of perspectives, and the results are roughly comparable. However, the published literature shows some trends. Firstly, a large dataset leads to stable and well-performing models. Indicators that include psychoacoustic and perceptual information contribute to improving model performance. Finally, nonlinear models (NLMs) seem to result in (slightly) more accurate models than linear models (LMs). However, NLMs approaches remain complex and challenging for researchers since the model development and the hyperparameters tuning might become demanding. Hence, LMs are usually the preferred choice although they could be often outperformed by NLMs strategies. Some of the predictive LMs provide poor performance (R2 = 0.18) [16], whereas other LMs achieve very good performance (R2 = 0.9) [6]. On the other hand, reported NLMs use sophisticated machine learning techniques such as support vector machines (SVM), artificial neural networks (ANN), or random forests (RF) to name a few. They outperform slightly LMs in terms of prediction, e.g., regarding scores (R2 = 0.91) [18]. Nevertheless, LMs still appear as prevalent in this field, while research with NLMs seems to be just promising, so far. A wide range of descriptors is modeled by a large array of indicators in a variety of scenarios. Thus, a general framework for comparison seems not to be established. One of the reasons is the scarcity of SER datasets that are publicly available. Emo-soundscapes database (EMO) [10] sets up a free and available dataset for SER comparison from 2017, which is focused on arousal and valence. Thus, other researchers have been exploring EMO as a reference. Firstly, [10] presents a baseline for EMO based on two independent SVMs, in order to to model both arousal and valence, selecting 39 features by a variance threshold. In [2], a comparison of four LM and four NLM is explored and a dimension reduction is performed by a principal components analysis (PCA).1 Furthermore, the authors in [10] also show that a RF model outperforms the rest of the models with only 25 features. In [1], a fine-tuned RF model with 14 features overcomes the previous RF model, and convolution neural networks (CNNs). Deep learning techniques have been also applied to SER through CNN and 23 simplified mel-frequency cepstral coefficients (MFCC) in [31], and the combination with SVM (Transfer learning) in [11]. Promising results use up to 54 features by heuristic methods despite the limited samples of EMO. Thus, EMO is the selected dataset for our study, because it is a suitable and relevant reference. The goal of this work is to design simple and interpretable SER linear models. Additionally, this study offers an\n1In order to avoid confusions, it is important to remark that the dimension reduction obtained by a PCA is different from a dimension reduction obtained by applying a variable selection scheme. The dimension reduction by PCA is obtained by suitable linear combinations of variables. These linear combinations can be considered as \u201cnew variables\u201d (and/or meta-features). A variable selection technique just selects some of existing variables trying to removing useless redundancy (without creating new features).\nexhaustive feature selection framework that helps researchers adjust their model errors with the features importance and their relationships. Namely, we provide an exhaustive variable selection study for LM, considering classical and novel methodologies. Hence, the required resources for SER modeling become less laborious and the research community can employ the designed models to improve the knowledge about SER. A wide range of applications such as urban planning, noise monitoring, and sonification production might improve their performance based on SER models. Moreover, variable selection may lead to less significant computing resources. This helps to bring SER models to devices with real-time responsivity, such as the IoT framework. First of all, we divide the methods into two main parts: ranking of variables and selection of the effective number of variables. This approach yields several benefits: (a) allows a better understanding of the different techniques, (b) allows the combination of different ranking and number selection schemes, and (c) produces a more complete view of the variable selection problem. We consider five different ranking methods and also compare the results to the classical ranking method based on p-values [9], [17]. Moreover, we apply the best sequence search (keeping fixed the number of variables). For this purpose, we perform an alternating optimization method that allows us finding easily (at least) local modes. We repeat the procedure for several different runs for obtaining the global mode. Last but not least, we design a pseudo-target density and a Gibbs sampling scheme which allows us having a complete view of the importance of the variables in terms of prediction error. The results of the Gibbs analysis support and clarify the results obtained previously by the ranking methods and the best sequence search. Some other considerations are only remarked by the Gibbs analysis. This novel technique can be applied for general variable selection purposes (not just for the specific database analyzed). The overall study allows us to propose (a) parsimonious, (b) interpretable, and (c) robust linear models. Namely, we can focus on a few very relevant variables that are highlighted in all the performed analyses. We believe that these variables keep their relevance also in different databases (as also suggested by the cross-validation results). Moreover, focusing on a few variables helps the interpretability of the resulting model. In summary, we aim at developing well-performing, simple, robust and interpretable SER linear models. In order to achieve this objective, the contributions of the work are the following:\n\u2022 We apply five different ranking methods to analyze the relevance of the variables in terms of prediction error in arousal and valence. \u2022 Additionally, we apply two more sophisticated methods to find the most relevant variables: 1) the best sequence search; and 2) a Gibbs sampling approach building a suitable target density based on prediction error (a technique developed for the first time in this work but which can be applied for general variable selection purposes). \u2022 Regarding the selection of the effective number of vari-\n3 ables, we apply several information criteria (such as the well-known AIC and BIC [21]) and a classical p\u2212value based approach. The obtained results are also compared with the Gibbs analysis.\n\u2022 Based on the complete and exhaustive analysis of the previous methods, we offer two different linear models to predict arousal and valence from a very reduced number of variables and with low prediction error.\nThe rest of the work is organized as follows. Section II-A describes the dataset which is the object of our study. Section II-B presents some background material describing the LM. Section III describes the different techniques that we will use for our analysis: ranking methods, the algorithm for best sequence search, and the Gibbs sampling analysis. Then, Sections IV and V show the results applied to the EMO database for the first output (arousal) and the second output (valence), respectively. Finally, in Section VI, we discuss some conclusions. The detailed results for both outputs are given in the Supplementary material."
        },
        {
            "heading": "II. DATABASE AND MODEL DESCRIPTIONS",
            "text": ""
        },
        {
            "heading": "A. Dataset Description",
            "text": "This research explores the EMO that might be considered as the largest publicly available soundscape database with annotations of emotion labels, and the most bench-marked up to now [10]. EMO contains 1213 audio clips which are released under Creative Commons license from the Freesound collaborative audio platform [13]. The Schafer's taxonomy classifies the selected clips into six categories due to their generality and simplicity . The Schafer's categories consider both the identification of the source and the listening context [10], [33] : natural sounds (e.g. birds, wind), human sounds (e.g. laugh, shouts), sounds and society (e.g. party, store), mechanical sounds (e.g. engine, factory), quiet and silence (e.g. quiet park, silent forest), and sounds as indicators (e.g. clock, church bells). EMO consists of 100 audio clips per category within a first subset (i.e. 600 audio clips) and 613 manually mixed sound of two or three categories of the first subset. A crowd-sourcing procedure provided data annotations of perceived arousal and valence, by a ranking-based questionnaire of a two clips pairwise comparison. Eventually, 1182 trusted annotators performed the required tasks with reasonable inter-subject reliability. Audio clips are monophonic and the sample rate was 44100 Hz, , which is widely considered a high quality standard for audio files. Monophonic recordings are sufficient to evaluate the eventfulness (arousal) and pleasantness (valence) of acoustic environments, among many other soundscapes indicators, according to [42]. EMO has employed both YAAFE [30] and MIRToolbox [19] for the extraction of 122 normalized audio features, applying a 23 ms Hanning window with 50% overlapping. There are three main groups of features of the audio signals: \u2022 Psychoacoustic features: They are indexed 4, from 24 to\n49, and 113, 114, 117, 118, 119. These features represent perceptual (i.e., subjective) attributes of sounds such as level (i.e., loudness for overall level and MFCC for\nband-limited levels), spectrum (i.e., sharpness for high frequencies), and temporal and spectrum modulation (i.e., fluctuation). \u2022 Time-domain features: They are indexed from 1 to 7, and 22, 23, 52, 115, 116. These features represent the signal dynamics, such as classical estimators based on samples of the audio signal (i.e., energy, entropy of energy, root mean square (RMS), or zero-crossing), the ratio between the magnitude difference at the beginning and the ending of a decay period (i.e., decrease slope), and the percentage of frames showing less energy than the average energy (i.e., low energy). \u2022 Frequency-domain features: They are represented by the remainder indexes, i.e., the rest of features. These features represent the shape of the spectrum and the harmonic structure of sounds such as the fundamental frequency of the audio signal (i.e., pitch), the proportion of frequencies that are not multiple of the fundamental frequency (i.e., inharmonicity), the spectral representation based on the 12 equal-tempered pitches of Western music (i.e., chromagrams), the spectral statistical moments (i.e., centroid, the first one; spread, the second one; skewness, the third one), the ratio between the geometric mean and the arithmetic mean (i.e., flatness), the spectral changes between two successive frames (i.e., flux), and the estimation of the amount of high frequency (i.e., roll-off).\nFor further information, the complete database can be found at https://metacreation.net/emo-soundscapes/."
        },
        {
            "heading": "B. Multiple Linear Regression Model",
            "text": "Let us consider a set of R variables x = [x1, ..., xR]> (input vector) and a related variable y (output). In several real-world applications, we observe a dataset of N pairs {xn, yn}Nn=1. In this work, we consider the case that R \u2264 N . The relationship between inputs and outputs is then studied. A linear observation model is usually used,\nyn = \u03b20 + \u03b21xn,1 + \u03b22xn,2 + ...\u03b2Rxn,R + n, (1) yn = \u03b20 + x > n\u03b2 + n,\nwhere \u03b2 = [\u03b20, \u03b21, ..., \u03b2R]> is a vector of coefficients and n is a Gaussian noise with zero mean and variance \u03c32 , i.e., n \u223c N ( |0, \u03c32 ). Defining the vectors y = [y1, ..., yN ]> and = [ 1, ..., N ]\n>, and the N \u00d7 (R+ 1) design matrix defined as\nX = 1 x > 1 ... ...\n1 x>N  , the previous model can be rewritten in the following way,\ny = X\u03b2 + . (2)\nThe least squares (LS) estimator (which coincides with the maximum likelihood estimator, in this setting) is\n\u03b2\u0302 = ( X>X )\u22121 X>y. (3)\n4 Hence, the vector of output predictions according to the model is y\u0302 = X\u03b2\u0302 = X ( X>X )\u22121 X>y, and the error vector is\ne\u0302 = y \u2212 y\u0302 = ( I\u2212X ( X>X )\u22121 X> ) y, (4)\nwhere e\u0302 = [e\u03021, ..., e\u0302N ]>, and I is a N \u00d7 N unit (diagonal) matrix. The mean absolute error (MAE) and the mean squared error (MSE) - in one specific realization2 - are defined as MAE = 1N \u2211N n=1 |e\u0302n| and MSE = 1 N \u2211N n=1 e\u0302 2 n, respectively. We also denote as y1, y2 the output 1 (arousal) and the output 2 (valence), respectively."
        },
        {
            "heading": "III. ROBUST VARIABLE SELECTION ANALYSIS",
            "text": "Variable selection is one of the most important task in machine learning, signal processing and statistics. The main goal of a variable selection approach is to remove the redundancy contained in the data, ideally without any loss of information or, at least, without incurring in a sensible loss of information. Variable selection methods are conceptually formed by two main parts. The first stage consists in ranking the variables for their importance, measured with some suitable criterion. In a second stage, based on the previous ranking, a selection of the number of relevant variables is performed. This last part can be considered a dimension reduction step, and it is also strongly connected to the model selection problem in nested models (in this case, the model selection problem is often called order selection) [21], [36], [37]. Generally, in the literature, these two stages are jointly presented within a unique technique, including the second part as a stopping rule in the ranking procedure (e.g., using a threshold value and stopping the rank at some position once the threshold value is reached) [15]. Here, we describe separately these two stages: the ranking methods in the next subsection and the selection of the number of features in Section III-D. Hence, we can combine different ranking schemes with different procedures for selecting the number of variables. In this work, we describe five different ranking methods (RMs). The first four ranking procedures are based, in a different way, on the prediction error. To the best of our knowledge, the procedures RM3 and RM4 described below present also some degree of novelty. They allow us to perform a more robust analysis, as discussed in Section IV. As an additional final check on the obtained results, we also apply a classical ranking method based on p-values [9], [17]. In Section III-B, we also describe the best sequence search (keeping fixed the number M of variables in the sequence) and an alternating optimization technique for obtaining the optimal sequence. Furthermore, we introduce a target density based on the prediction error, and employ a Gibbs sampling scheme for drawing samples from it. This analysis allows to have a complete view of the importance of the variables.\n2The MAE and MSE are theoretically defined as the expectation of the (abosolute or squared) error over different realizations of the data y(`) with ` = 1, 2, 3,..., where the index ` denotes a different realization. Here, we consider a fixed vector of data y (i.e., the observed data) and compute the error vector in one realization e\u0302 = y\u2212 y\u0302. Moreover, we average the absolute value (or the squared values) of error components in order to obtain a unique error value."
        },
        {
            "heading": "A. Ranking methods",
            "text": "In this section, we briefly describe the ranking methods (RMs) that we have applied to our dataset. Some of them are well-known techniques, whereas others contain some degree of novelty [15]. We list them below.\nRM1 - Forward Selection (FS): adding variables \u201cforward\u201d minimizing the error. The method starts searching for the most significant univariable model (in terms of the error in prediction), i.e., considering the linear regression model in Eq. (1) with only one component (namely, one column in the matrix X in Eq. (2)). We repeat then considering a model with two variables (re-estimating the model for each pair), including (and keeping) the previously selected variable. We iterate the procedure until considering a complete model of R variables. This procedure provides a sequence of included variables that will be the final ranking. RM2 - Backward Elimination (BE): removing variables \u201cbackward\u201d minimizing the error. The method starts considering the complete model. Then, we remove the most insignificant variable in terms of the error in prediction, considering models with R \u2212 1 variables (clearly, removing a different variable and we re-estimate the coefficients for each model). We repeat the procedure considering always smaller models and removing one variable at each iteration. The procedure provides an inverse ranking where the first variable is the worst one and the last is the best one. RM3: removing variables maximizing the error. The method starts again considering the complete model. Then, we remove one variable and compute the error in prediction. Hence, we select the best variable, i.e., which (when removed) produces the higher increase of the error in prediction. This variable will be the first in our ranking (the most relevant variable). We repeat the procedure considering the rest of variables. RM4: adding variables maximizing the error. Here, we create a sequence of variables from the worst to the best variable (i.e., increasing their relevance), starting with a univariable model as in RM1, but selecting the worst variable (i.e., the variable which maximizes the prediction error). Then, we consider a model with two variables (keeping the previous select one) and select the second variable which maximizes the prediction error. We repeat the procedure, obtaining a final inverse ranking of the variable, i.e., the last one will be the most relevant variable. RM5 - based on the correlation coefficient: We compute the Pearson correlation coefficients between one single variable and the output y. Then we rank the features in decreasing order according to the module of correlation coefficients. This procedure is similar to the often so-called univariable selection [15].\nThe joint use of these different ranking procedures allows to perform a robust analysis, obtaining a more complete view of our variable selection problem. Indeed, some ranking methods, although yield sequences far from the smallest possible error, detect relevant variables that appear also by the Gibbs sampling analysis (described below). More specifically,\n5 although we will see that RM1 and RM2 provide the best performance in terms of prediction error, but the results of RM3, RM4 and RM5 reveal other important aspects shown by the rest of our analyses below. Moreover, for completing our view, we will also apply a classical ranking method based on p-values [9], [17], and show the results in Table IV."
        },
        {
            "heading": "B. Best sequence search",
            "text": "Let us define the vector of M different indices\nvM = [k1, ..., kM ], M \u2264 R,\nwhere ki \u2208 {1, 2, ..., R} but ki 6= kj for i 6= j. Considering only the M variables in vM , we can build a smaller N \u00d7 (M + 1) design matrix VM , and consider a smaller (M + 1)\u00d7 1 vector of coefficients \u03b2\u0302M = [\u03b2\u03021, ..., \u03b2\u0302M+1]>, which is obtained as\n\u03b2\u0302M = ( V>MVM )\u22121 V>My. (5)\nMoreover, we define the cost function\nC(vM ) = ||y \u2212 y\u0302||\u03b1p , = ||y \u2212VM \u03b2\u0302M ||\u03b1p , (6)\nwhere || \u00b7 ||p is the Lp norm with p > 0 and \u03b1 > 0. Note that C(vM ) is defined in the discrete space of M possible different indices. We desire to find the vector of indices such that\nv\u2217M = argmin v C(vM ). (7)\nNote that an exhaustive search is only possible when M is small (typically it is suitable only for M \u2264 4). Moreover, a random search in the entire space (as with a simulated annealing approach [23]) can be very costly and to reach the global minimum (or a \u201cgood\u201d local minimum) is very difficult. For this reason, we employ an alternating optimization approach that, at least, ensures a fast convergence to a local minimum. Furthermore, we perform the alternating optimization scheme several times (500 runs) with different initializations, and compare the minimum obtained at each run [23]. We finally consider the solution v\u0302M with the smallest associate cost value C(v\u0302M ), i.e., v\u0302M is our estimator v\u2217M . Below, we describe the alternating optimization method.\nAlternating optimization. Choose M < R + 1, a maximum number of iterations Titer \u2265 1, and start with v(0)M .\nFor t = 1, ..., Titer (or until convergence) repeat: 1) For j = 1, ...,M :\na) Keeping fixed the rest of M \u2212 1 variable, work only on the j-th variable, i.e., given\nbj = [k (t) 1 , k (t) 2 , ...k (t) j\u22121, kj , k (t\u22121) j+1 ..., k (t\u22121) M ],\nfind k\u2217j = argmin\nv C(bj).\nThe optimization above can be solved in an exhaustive way since it is a one-dimensional problem.\nb) Set k(t)j = k \u2217 j , and\nbj+1 = [k (t) 1 , k (t) 2 , ...k (t) j\u22121, k (t) j , kj+1, k (t\u22121) j+2 ..., k (t\u22121) M ].\n2) Set v (t) M = bM , and b1 = v (t) M ."
        },
        {
            "heading": "C. A Gibbs sampling approach",
            "text": "We generalize the optimization scheme considering a Markov Chain Monte Carlo (MCMC) sampling approach. More specifically, we consider a Gibbs sampler which is the counterpart of the alternating optimization in the Monte Carlo sampling world [23]. The sampling approach (applied in this context) can provide the probability that each variable is contained in the best subset of M elements. Let us recall the vector of M different elements\nvM = [v1, ..., vM ],\nwhere vi \u2208 {1, 2, ..., R} but vi 6= vj for i 6= j. In this section, we consider the target density\np(vM ) \u221d exp (\u2212\u03b7C(vM )) , \u03b7 > 0,\nwhere C(vM ) is the cost function previously considered in Eq. (6). The constant \u03b7 can be used and set to provide a tempering effect [21], [23], [27]. The variables that belong to sequences with smaller errors acquire more value according to p(vM ). Thus, by drawing samples from p(vM ), we can obtain the proportion of times that a feature provides a sequence with yields a small error. This is a very important information that helps us to yield a more robust analysis, in the sense that we can avoid overfitting at this specific set of data. The overfitting can occur performing the variable selection only considering the best sequence, for instance. Note that this idea can be employed in any problem where a cost function (as function of the parameters of interest) is available.\nOn the choice of \u03b7. We can observe that, as \u03b7 \u2192 0, the density p(vM ) becomes closer and closer to a delta function around the best sequence v\u2217M in Eq. (7), which is the global minimum of C(vM ). As \u03b7 > 0 grows, more and more local modes appear in p(vM ). These local modes contain relevant information for our study. As \u03b7 \u2192 \u221e, the difference among the values of the modes become smaller and smaller, and p(vM ) tends to a uniform density in support domain. It is important to remark that there is a range of suitable values of \u03b7 such that the analysis can be performed. These suitable values are all the values of \u03b7 such that all the possible local modes appear. The interested user can perform some preliminary runs for choosing a proper value of \u03b7.\nA Gibbs sampling algorithm is a type of Markov Chain Monte Carlo (MCMC) method for drawing samples from general distribution as p(vM ) above [26], [28], [29]. An MCMC algorithm generates a Markov chain with invariant density exactly the target density, that in our case is p(vM ). A Gibbs sampler works at each step in a one dimensional space [23], simplifying the multivariate sampling problem drawing from simpler one-dimensional densities. Before describing\n6 the Gibbs sampling method, we have to recall that the j-th full-conditional density is\npj(vj |v1, ..., vj\u22121, vj+1, ..., vM ) \u221d p(vM ) = p(v1, ..., vj\u22121, vj , vj+1, ..., vM ), (8)\nwhere all the variables are fixed with the exception of vj , and the normalizing constant is p(v1, ..., vj\u22121, vj+1, ..., vM ) that does not depend on vj . For simplicity, we use the more compact notation\npj(vj |v1, ..., vj\u22121, vj+1, ..., vM ) = pj(vj |v1:j\u22121, vj+1:M ),\nfor denoting the j-th full-conditional density. A detailed description of the Gibbs sampling algorithm is given below.\nGibbs sampler. Choose M < R, a maximum number of iterations Titer \u2265 1, and start with v(0)M .\nFor t = 1, . . . , Titer:\n1) For j = 1, . . . ,M : a) Draw v(t)j \u223c pj ( vj | v(t)1:j\u22121, v (t\u22121) j+1:M ) .\n2) Set v(t)M = [ v (t) 1 , v (t) 2 , . . . , v (t) M ] ."
        },
        {
            "heading": "D. Selection of the number of relevant variables",
            "text": "Several selection procedures (also denoted as stopping rules during the ranking process) can be applied. Clearly, a naive method could be just to set a threshold value (or a percentage) for the prediction error, or by a simple visual inspection of the error curve, i.e., the so-called elbow method [36], [38]. In the classical variables selection analysis, practitioners and researchers often consider statistical tests (e.g., F-test and t-test), employed sequentially to decide whether individual variables should be included in the model, and a stopping rule based on p-values [9], [17]. Other approaches rely on the so called information criteria methods [21], [37], which are based on the following cost function\nC(M) = \u22122 log p(y|\u03b2\u0302M )\ufe38 \ufe37\ufe37 \ufe38 fitting + 2\u03beM\ufe38\ufe37\ufe37\ufe38 penalization , (9)\nwhere \u03be > 0 is a constant that specifies the criterion. The first term is a fitting term (based on the maximum likelihood value), whereas the second term is a penalty for the model complexity. The expression (9) encompasses several wellknown information criteria proposed in the literature and shown in Table I, which differ for the choice of \u03b7 [21], [37]."
        },
        {
            "heading": "IV. RESULTS FOR THE OUTPUT 1 - AROUSAL",
            "text": "In this section, we describe the results obtained for the first output (arousal). A more complete discussion is provided in the Supplementary material.\nTo carry out the analysis and selection of variables with which we propose our final linear model for the prediction of the arousal, we will follow the next steps:\n1) First of all, we apply five ranking techniques (from RM1 to RM5) and analyze the variables that appear classified as the most significant (Section IV-A). 2) We apply the best sequence search (Section IV-B) and the Gibbs sampling analysis (Section IV-C) and analyze the variables that appear as the most relevant according to these methods that start from a fixed number of variables. We compare and discuss the obtained results with the results previously provided by the ranking techniques. 3) We summarize all the previous results by classifying the variables into three levels according to their level of global relevance (Section IV-D). 4) We propose the linear model for the prediction of the arousal including only the most relevant variables according to our analysis and we evaluate their prediction error (Section IV-E).\nNote that these same steps are briefly replicated for the second output (valence) throughout Section V."
        },
        {
            "heading": "A. Results of the Ranking Methods",
            "text": "Figure 1 shows the MAE in the estimation of the first output considering models with M \u2264 122 variables. The variables are ordered according to the different ranking criteria. At each M , we consider the first M variables in each ranking and compute the MAE. Clearly, when M = 122 (i.e., we are using all the variables) all the curves reach the same point. The black solid line corresponds to the MAE curves without ordering the variables (i.e., keeping the order in the data matrix). Note that, even in this curve with unordered variables, we can observe the importance of the variables \u201c12-th chromagram standard deviation\u201d (indexed as 112), \u201cloudness mean\u201d (indexed as 113) and \u201cloudness standard deviation\u201d (indexed as 114). Indeed, there is a relevant drop in MAE at the variable 112, the decrease continues with the variable 113, and the derivative seems to be null after the variable 114. Moreover, even in this curve with unordered variables, we can observe in Figure 1(a) that there is already an elbow within the interval between the 15-th variable and the 20-th variable [38]. The cyan and blue solid lines correspond to RM1 and RM2 which provides the best results in terms of MAE. Namely, RM1 and RM2 provide two orders of variables which produce the fastest decays in terms of MAE. Figure 1(b) provides the same information of Figure 1(a) but in log-log-scale. Both curves, corresponding to RM1 and RM2, seem to present a clear elbow between the 7-th and 8-th ordered variables. The curves corresponding to RM3, RM4, RM5 are depicted with dashed lines. Although these rankings do not achieve the\n7 best results in this figure, they provide interesting information regarding the importance of the variables, as we discussed below. For instance, note that the error with RM4 has a big drop (reaching the best performance given by RM2) when the third variable is considered, which is feature 1. This feature appears in 8-th position of RM5 but is not considered relevant by RM1, RM2, RM3 and by the best sequence search, as we will see later on. Moreover, the Gibbs analysis confirms its relevance (as we will see below).\nThe rankings of the variables obtained by RM1, RM2, RM3, RM4, RM5, from the first one to the 20-th one are given in Table II. We highlight with boxes the variables that are within the most important twenty variables in all the ranking methods; these variables are five and are labeled as:\n113 (\u201cloudness mean\u201d), 114 (\u201cloudness standard deviation\u201d), 14 (\u201cspread mean\u201d), 13 (\u201ccentroid standard deviation\u201d), and 3 (\u201cdecrease slope mean\u201d),\nalthough, variable 3 is never contained within the most important ten variables within the different rankings."
        },
        {
            "heading": "B. Results of the best sequence search",
            "text": "In Table III, we can observe the best sequences for M = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12}, obtained with the alternative optimization procedure (after 103 independent runs with different random initializations). See also the decrease of the error in Figure 2(a). All the best sequences are given just in ascending order of the labels. Indeed, unlike with the Gibbs approach, we cannot discriminate among the variables within a best sequence. In Table III, each new entry in the best sequence (as M grows - with respect to the previous shorter sequence), is highlighted with a box. We remark especially the best sequence with M = 7, i.e.,\n4, (\u201cmaximum fluctuation\u201d), 8 (\u201croll-off mean\u201d), 14 (\u201cspread mean\u201d), 56 (\u201cpitch standard deviation\u201d), (10) 113 (\u201cloudness mean\u201d), 114 (\u201cloudness standard deviation\u201d), and 115 (\u201cenergy mean\u201d),\n(shown here in ascending order of the labels).\nThey exactly coincide with the ranking given by RM2 of the first seven most important variables, i.e.,\n113, 14, 8, 114, 115, 56, and 4, (11)\nshown here in decreasing order of importance by RM2."
        },
        {
            "heading": "C. Gibbs sampling analysis",
            "text": "In the Gibbs sampling analysis, the sequences of length M , vM = [v1, ..., vM ] (with vi \u2208 {1, 2, ..., R + 1}, vi 6= vj for i 6= j), are weighted according to error C(vM ) or, more specifically, according to\np(vM ) \u221d exp (\u2212\u03b7C(vM )) , \u03b7 > 0. (12)\nIn our simulation, we set \u03b7 = 100. Moreover, for defining C(\u00b7), we consider the L1 norm (and \u03b1 = 1). The variables that belongs to sequences with smaller errors acquire more weight/importance. In some sense, the Gibbs analysis provides the probability that a feature provides a sequence of small error. In Figure 3, we show the results of Gibbs sampling analysis for M \u2208 {2, 6, 10, 20}. The dashed line depicts the equiprobability (uniform) distribution with probability 1/122. Clearly, probabilities bigger than 1/122 denote the most important variables. We can observe that the results are coherent with the previous results above. For instance, the variable 113 is clear the most important and also the features 114, 4, 115, 56, 8 are quite relevant. The Gibbs analysis also confirms that the feature 39 seems relevant for small M but, as M grows, the importance of this feature disappears. However, by the Gibbs analysis, we can obtain more interesting information. The features 4 and 56 are quite relevant even from small values of M , confirming also the results of the best sequence search. The features 14 seems to have a relevance very similar to 114. As we have also previously stated, the Gibbs analysis clearly shows that the variable 115 is the fourth most important feature (as we expect after a care look of the rankings). Surprisingly, the feature 1 seems to be equally relevant than the feature 115: we provide an explanation below. The variable 13 seems also to be some relevance by the Gibbs analysis specially as M grows. However, as we expected for the previous study, its relevance is moderate. Furthermore, we can also observe the importance of other variables whose relevance was not clear from the previous analysis above. This is the case of the following features:\n1, (\u201cRMS mean\u201d), 20, (\u201cflatness mean\u201d), 50, (\u201cflux mean\u201d), 16, (\u201cskewness mean\u201d), and 22 (\u201centropy mean\u201d).\nThe feature 1 deserves some specific comments (see Supplementary Material). The feature 20 is in the fourth position of RM3, in the 10-th position of RM4, and in the 6-th position of RM5. The variable 50 appears also in the fifth position of RM4, and in the 10-th position of RM5. The variable 16 appears in the best sequences for M \u2265 9 and in 9-th position of RM1. The feature 22 has not been detected by the previous studies: it does not appear either in the rankings, or in the best sequence search (at least for M \u2264 12 as in Table III).\n8"
        },
        {
            "heading": "D. Summary for the output 1",
            "text": "Here, we classify the features into four different classes: very relevant (Level 1), relevant (Level 2), and relevant but maybe only for the specific dataset (Level 3), and the rest of variables belong to the class non-relevant (Level 4). Level 1. After all the studies, we can assert at least 7 variables are very relevant:\n113, 114, 14, 115, 4, 8, and 56, (ordered by Gibbs analysis),\nwhich are shown in decreasing order of importance considering the Gibbs sampling analysis. This is also the best sequence for M = 7, as shown in Table III and includes also the first 7 elements in the ranking obtained by RM2 but with a different order,\n113, 14, 8, 114, 115, 56, and 4 (ordered by RM2).\nLevel 2. Other important variables are\n1, 22, 20, 50, 13, 16, and 3 (ordered by Gibbs analysis),\nbut the features 1 (\u201cRMS mean\u201d) and 50 (\u201cflux mean\u201d) are highly correlated to the variable 115 (\u201cenergy mean\u201d), as shown by Figure 2(b) and as we could intuitively expect. The variable 13 (\u201ccentroid standard deviation\u201d) appears in some of the first twelve best sequences. However, the Gibbs analysis reveals (that in terms of robustness) other variables such as 20 (\u201cflatness mean\u201d) and 22 (\u201centropy mean\u201d) are more or a similar relevance than 13. The feature 20 is particularly important in RM3 (4-th position) and RM5 (6-th position). The feature 16 (\u201cskewness mean\u201d) does not appears in the best first twenty variables in the rankings, but appears in the best sequences permanently for M \u2265 9. In the Gibbs analysis, the feature 16 acquires some relevance as M grows. Finally, the feature 3 (\u201cdecrease slope mean\u201d) does not appear in the first twelve best sequences and it does not seems relevant by the Gibbs analysis. However, it appears within the first twenty more relevant variables in all the ranking methods.3 Level 3. Other possibly important variables, which appear in the best sequence search and in the rankings RM1 and RM2, are\n43 (\u201c7th MFCC stand. deviation\u201d) and 107 (\u201c7th chromagram stand. deviation\u201d).\nThe variable 43 appears in 8-th position in RM2 and 13-th position in RM1. Moreover, it appears in the best sequences for M \u2265 8. The feature 43 appears in 11-th position in RM2 and 14-th position in RM1. Moreover, it appears in the best sequences for M \u2265 10. On the other hand, the Gibbs analysis does not associate any particular relevance to these variables.\n3More surprisingly, for the output 2 - valence -, only three features are included within the first twenty more relevant variables in all the ranking methods: they are the variables 113, 114 and again 3. Namely the feature 3 (as 113 and 114) is included within the first twenty more relevant variables in all the ranking methods for both outputs."
        },
        {
            "heading": "E. Selection of the number of variables and suggested model for the output 1",
            "text": "First of all, we have applied different information criteria, such as the AIC and BIC, shown in Table I [21]. The more adequate results have been provided the Bayesian information criterion (BIC) which suggests to use 17 variables whereas AIC suggests the use of 40 variables. We have also tried the classical analysis based on p-values which suggests 71 variables [9], [17]. The results of the corresponding ranking method is given in Table IV. The first most relevant 7 variables are again 113, 14, 8, 4, 56, 115 and 114, i.e., the very relevant features that we have found after our analysis. However, after our exhaustive study, we believe that a more parsimonious model can be suggested. The most parsimonious LM after all the consideration in our study, is the model which includes at least the seven very relevant variables (described above),\n113, 114, 14, 115, 4, 8, and 56. (13)\nMore precisely, the suggested linear model for the output 1 is\ny1 =\u2212 0.5293 + 3.6494 x113 + 1.8080 x114+ \u2212 1.5534 x14 \u2212 3.8491 x115+ (14) + 1.5056 x4 + 1.1714 x8 \u2212 0.3450 x56,\nobtaining a MAE of 0.1593, MSE of 0.0432 (i.e, RMSE of 0.2078), and R2 = 0.8703. Considering a Monte Carlo crossvalidation procedure (with 80% of the data in the train-set and the rest of 20% of data in the test-set, chosen randomly in each 2 \u00b7 104 independent runs), we obtain MAE of 0.1611, MSE of 0.0450 (i.e, RMSE of 0.2118), and R2 = 0.8641. Namely, we have a very slight increase of MAE and MSE (or a slight decrease of R2), proving the robustness of our proposed model."
        },
        {
            "heading": "V. RESULTS FOR THE OUTPUT 2 - VALENCE",
            "text": "In this section, we analyze briefly the results the output 2 of the dataset (valence). The decreases of the error for the RMs are shown in Figure 4. A complete discussion is provided in the Supplementary Material. Here, we also point out the the variables which seem relevant for both outputs (1 and 2) and some features just relevant for output 2."
        },
        {
            "heading": "A. Ranking, best sequences and Gibbs analysis",
            "text": "The most important features for output 2 are\n114 (\u201cloudness standard deviation\u201d), 113 (\u201cloudness mean\u201d), 14 (\u201cspread mean\u201d), and 3 (\u201cdecrease slope mean\u201d).\nThey are also relevant for the output 1 (as we can see in the main body of the work). The variable 114 seems to increase its relevance with respect the output 1, whereas the variable 3 is much more relevant for the output 2. The importance of these features is confirmed by the Gibbs analysis in Fig. 5, specially for the feature 14. Indeed, the case of feature 14 is very interesting and reveals also the importance of the Gibbs\n9 analysis. The variable 14 does not seem relevant following RM1 and RM2 (which provides the sequences with the smaller errors) and does not appears in the best sequences in Table VII. However, it is the third more important variables for RM3, RM4 and RM5, and it is the third more relevant variable for the Gibbs analysis (see Fig. 6). Furthermore, it acquires more relevance as M grows (see again Fig. 6). The variables\n1 (\u201cRMS mean\u201d), and 115 (\u201cenergy mean\u201d),\n(which are highly correlated, also with the feature 50; see the main body of the paper for further details) appear relevant for the second output as well. The variable 1 is contained in the first twenty more important features in RM2, RM4 and RM5. Moreover, the variable 1 appears in the best sequences playing the role of the variable 115, i.e., when the feature 115 does not appear in those sequences. Namely, due to their correlation in the rankings and in the best sequences, the presence of one of them (1 or 115) avoids the presence of the other one. The feature 115 appears in the best sequences almost in a stable way for M \u2265 3. The Gibbs analysis confirms the relevance of both 1 and 115, and they seem even more relevant than the variable 3. Furthermore, the following variables\n50, (\u201cflux mean\u201d), 20, (\u201cflatness mean\u201d), 13, (\u201ccentroid standard deviation\u201d), and 8 (\u201croll-off mean\u201d),\nare also important for the output 2. The variable 50 seems relevant bit is highly correlated with the features 1 and 115. The variables above have certain relevance also for the output 1. Now, we discuss some features that seem to have importance only for the output 2 (i.e., valence). A careful look to the results reveals that the following features\n88 (\u201cinharmonicity standard deviation\u201d), 31 (\u201c8th MFCC mean\u201d), 40 (\u201c4th MFCC standard deviation\u201d), 42 (\u201c6th MFCC standard deviation\u201d), 52 (\u201cLow Energy\u201d), 79 (\u201c11th chromagram center stand. deviation\u201d), 109 (\u201c9th chromagram stand. deviation\u201d), and 110 (\u201c10th chromagram standard deviation\u201d),\nare relevant, and appear in the best sequences for the output 2. Moreover, the feature 88 is the second more important in RM1 and appears in the 13-th position of RM3. The importance of 88 is confirmed (and is even more clear) by the Gibbs sampling analysis shown in Figure 5. The feature 31 seems to be relevant for RM1, RM5 and the Gibbs analysis. Moreover, it appears in the best sequences for M \u2265 11. The variable 40 is within the first twenty most important variables of RM1 and RM2. It also appears in the ranking based on p-values in the 10-th position (see Table VI). Moreover, it starts to appear in the best sequences for M \u2265 10. The importance of the feature 40 increases as M grows, following the Gibbs analysis. The\nfeature 42 seems to have also the same importance of the variable 40 for the Gibbs analysis, and appears in the 16- th position of the GR and in the 15-th position of RM1. The variable 52 takes the 9-th position in RM2, appears in the best sequences for M \u2265 10 and seems relevant according to the Gibbs analysis. The feature 79 appears within the first twenty more important variables in RM1, RM2 and RM3. From the Gibbs analysis, it seems clear that its importance grows with M . In the best sequences, the feature 79 appears in a stable way for all the best sequences with M \u2265 8. The feature 110 is contained within the first twenty more important variables in RM1, RM2 and RM3. Following the Gibbs analysis, the feature 109 is similar or more relevant than 110. It also appears in the best sequence with M = 7 and in the 6-th position of the classical ranking based on p-values. The feature 50 seems relevant by the Gibbs analysis but it is very correlated to 1 and 115."
        },
        {
            "heading": "B. Selection of the number of variables and suggested model for the output 2",
            "text": "From the results, we can notice that the output 2 (valence) is less linear correlated with the variables x, compared with the first output (arousal). The BIC suggests the use of 22 variables, whereas the AIC suggests the use of 68 variables. The classical p-values method suggests the use of 83 variables. However, all the considerations in our study above, we believe that a more parsimonious model can be proposed. In our opinion, The most parsimonious linear model that we can suggest is the model which includes at least the six very relevant variables which are (see the considerations above)\n114 (\u201cloudness standard deviation\u201d), 113 (\u201cloudness mean\u201d), 14 (\u201cspread mean\u201d), 88 (\u201cinharmonicity standard deviation\u201d), 115 (\u201cenergy mean\u201d), 3 (\u201cdecrease slope mean\u201d), (ordered by the Gibbs analysis),\nand the ten relevant variables,\n8, (\u201croll-off mean\u201d), 20, (\u201cflatness mean\u201d), 79 (\u201c11th chromagram center stand. deviation\u201d),\n4 , (\u201cmaximum fluctuation\u201d), 109 (\u201c9th chromagram stand. deviation\u201d), 110 (\u201c10th chromagram standard deviation\u201d), 40, (\u201c4th MFCC standard deviation\u201d), 31, (\u201c8th MFCC mean\u201d), 42, (\u201c6th MFCC standard deviation\u201d), and 52 (\u201cLow Energy\u201d),\n(ordered by the Gibbs analysis),\nwhere we have included the feature 4 due to the Gibbs analysis. It appears also in the first twenty positions of RM3,\n10\nRM4 and RM5: in the 12-th position, in the 18-th position, and in the 15-th position, respectively. The variable 72 has been excluded due to the Gibbs analysis, as well. More precisely, the suggested model for the output 2 is\ny2 =0.2831\u2212 2.4741 x114 \u2212 1.0919 x113 + 0.8070 x14+ 0.2538 x88 + 2.8482 x115 \u2212 0.6448x3+ \u2212 1.4867 x8 + 1.1290 x20 \u2212 0.2003 x79+ (15) \u2212 0.7192 x4 + 0.5182 x109 + 0.1642 x110+ 0.3312 x40 + 0.2978 x31 + 0.4621 x42 \u2212 0.5342 x52,\nobtaining a MAE of 0.2799, MSE of 0.1182 (i.e, an RMSE of 0.3437), and an R2 = 0.6452. Considering a Monte Carlo cross-validation procedure (with 80% of the data in the trainset and the rest of 20% of data in the test-set, chosen randomly in each 2 \u00b7 104 independent runs), we obtain MAE of 0.2849, MSE of 0.1233 (i.e, RMSE of 0.3509), and R2 = 0.6311. As for the output 1, we have a very slight increase of MAE and MSE (and a slight decrease of R2), proving the robustness of our proposed model."
        },
        {
            "heading": "VI. FINAL DISCUSSION AND CONCLUSIONS",
            "text": "From the previous section, we observe that within the most important features for both outputs, 1 and 2, are\n114, 113, 14, 115, and 3, (ordered by Gibbs analysis).\nTherefore, we can conclude that the psychoacoustic features 113, 114 (\u201cloudness mean\u201d and \u201cloudness standard deviation\u201d, respectively), the frequency-domain feature 14 (\u201cspread mean\u201d), and the time-domain feature 115 ( \u201cenergy mean\u201d) are the most relevant variables in our study. They have been included in both suggested models. The frequency-domain feature 3 (\u201cdecrease slope mean\u201d), although has not been included in the suggested model for the output 1, appears within the first twenty more relevant variables in all the rankings, for both outputs. The relevant features reveal the importance of the psychoacoustic indicators in SER. However, time and frequency-domain features have been also included into the suggested models. These results are in line with other studies which also highlight that subjective perception and time-dynamics of the signals (jointly embedded in indicators) lead to better model scores [1], [20]. The valence model provides worse performance than arousal one, even involving more features. This outcome agrees with the literature and it seems to be due to the prevalence of neutral annotations of valence in some soundscape categories [11]. We remark that the suggested LMs are very cheap and parsimonious models (including only 7 variables for arousal and 16 for valence, over the 122 possible features) and provide quite high R2 coefficients (R2 = 0.8703 for arousal and R2 = 0.6452 for valence), and small MSEs (0.045 for arousal and 0.118 for valence), compared with the results previously obtained in the literature, even using non-linear models and including more variables. Indeed, our results are competitive with respect to the EMO baseline that employs a non-linear SVR and many more features (exactly 39 variables), both in arousal (with an MSE of 0.048) and valence (with an MSE\nof 0.128) [10]. In [2], the authors suggest also linear models with EMO obtaining worse results: specifically MSE \u2248 0.090 for arousal and MSE \u2248 0.160 for valence) using also more features in their models (exactly 25). Recent studies with EMO have shown that more sophisticated nonlinear models (such as RF) can reach good scores with 15 features for arousal (MSE \u2248 0.050) and 14 features for valence (MSE \u2248 0.140). Finally, other authors using other complex nonlinear models, such as CNNs and data augmentation techniques, obtain slightly better metrics (MSE \u2248 0.035 for arousal, and MSE \u2248 0.078 for valence), but also including substantially more variables in their models: from 23 up to 54 features [11], [31]. All these considerations confirm the quality of our suggested models. Due to the exhaustive study that we have performed, we believe that the suggested LMs are robust and allow good prediction in different databases. Thus, the obtained parsimonious models can help the design of SER methods, and its practical applications by remarking the most relevant features. As future research lines, we plan to extend our variable selection study (including the proposed Gibbs analysis) for nonlinear models, and then judge if this non-linearity is strongly required with the EMO dataset, since the proposed LMs provides already very good performance. Furthermore, we plan to design novel schemes for selecting automatically a reasonable number of variables, when the priority is to obtain the simplest (and hence cheapest) possible model. Indeed, at least with these soundscapes data, the current benchmark techniques often seem to widely overestimate the adequate number of relevant variables."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "The authors acknowledge support by the Agencia Estatal de Investigacio\u0301n AEI (project SPGRAPH, ref. num. PID2019-105032GB-I00), by Young Researchers R&D Project with ref. num. F861 (AUTO-BA-GRAPH) funded by Community of Madrid and Rey Juan Carlos University and F. Llorente acknowledges support by Spanish government via grant FPU19/00815."
        }
    ],
    "title": "An exhaustive variable selection study for linear models of soundscape emotions: rankings and Gibbs analysis",
    "year": 2022
}