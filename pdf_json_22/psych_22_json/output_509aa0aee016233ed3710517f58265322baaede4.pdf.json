{
    "abstractText": "Conversational agents (CAs) embodied in speakers or chatbots are becoming very popular in some countries, and despite their adult -centred design, they have become part of children\u2019s lives, generating a need for children-centric trustworthy systems. This paper presents a literature rev iew t o identify the main opportunities, challenges and risks brought by CAs when used by children. We then consider relevant ethical guidelines for AI and adapt them to this particular system and population, using a Delphi methodology with a set of experts from different disciplines. From this analysis, we propose specific guidelines to help CAs developers improve their design towards trustworthiness and children.",
    "authors": [
        {
            "affiliations": [],
            "name": "Escobar-Planas"
        },
        {
            "affiliations": [],
            "name": "Marina"
        }
    ],
    "id": "SP:05dd440459a4381da9090534202ca9829e5d8a50",
    "references": [
        {
            "authors": [
                "V. Braun",
                "V. Clarke"
            ],
            "title": "Using thematic analysis in psychology",
            "year": 2006
        },
        {
            "authors": [
                "F. Catania",
                "P. Crovari",
                "E. Beccaluva",
                "G. De Luca",
                "E. Colombo",
                "N. Bombaci",
                "F. Garzotto"
            ],
            "title": "Boris: a Spoken Conversational Agent for Music Production for People with Motor Disabilities",
            "venue": "14th Biannual Conference of the Italian SIGCHI Chapter (pp. 1-5)",
            "year": 2021
        },
        {
            "authors": [
                "Y. Cheng",
                "K. Yen",
                "Y. Chen",
                "S. Chen",
                "A. Hiniker"
            ],
            "title": "Why doesn't it work? voice driven interfaces and young children's communication repair strategies",
            "venue": "17th ACM Conference on Interaction Design and Children (pp. 337-348)",
            "year": 2018
        },
        {
            "authors": [
                "V. Dignum",
                "M. Penagos",
                "K. Pigmans",
                "S. Vosloo"
            ],
            "title": "Policy guidance on AI for children",
            "venue": "Communications of UNICEF. https://www.unicef.org/globalinsight/reports/policy-guidance-ai-children",
            "year": 2021
        },
        {
            "authors": [
                "J. Fraser",
                "I. Papaioannou",
                "O. Lemon"
            ],
            "title": "Spoken conversational ai in video games: Emotional dialogue management increases user engagement",
            "venue": "International Conference on Intelligent Virtual Agents (pp. 179-184)",
            "year": 2018
        },
        {
            "authors": [
                "Y.M. Hopf",
                "C.B. Bond",
                "J.J. Francis",
                "J. Haughney",
                "P.J. Helms"
            ],
            "title": "Linked health data for pharmacovigilance in children: perceived legal and ethical issues for stakeholders",
            "year": 2014
        },
        {
            "authors": [
                "S. Shen"
            ],
            "title": "Robovie, you'll have to go into the closet now\u201d: Children's social",
            "year": 2012
        },
        {
            "authors": [
                "S.M. Kelly"
            ],
            "title": "Growing up with Alexa: A child\u2019s relationship with Amazo n ",
            "year": 2018
        },
        {
            "authors": [
                "N. Kova\u010devi\u0107",
                "A. Stojiljkovi\u0107",
                "M. Kova\u010d"
            ],
            "title": "Application of the matrix approach in risk",
            "year": 2019
        },
        {
            "authors": [
                "M. Landoni",
                "E. Murgia",
                "Huibers",
                "M.S. Pera"
            ],
            "title": "You've Got a Friend in Me",
            "year": 2020
        },
        {
            "authors": [
                "M Lavechin",
                "R Bousbib",
                "H Bredin",
                "E Dupoux"
            ],
            "title": "Cristia A (2020), An open -source voice type",
            "year": 2020
        },
        {
            "authors": [
                "N. Dillen"
            ],
            "title": "Curiosity Notebook: A Platform for Learning by Teaching Conversational",
            "year": 2020
        },
        {
            "authors": [
                "M. McTear"
            ],
            "title": "Conversational AI: Dialogue",
            "year": 2020
        },
        {
            "authors": [
                "L.A. Petitto"
            ],
            "title": "Multimodal dialogue management for multiparty interaction with infants",
            "year": 2018
        },
        {
            "authors": [
                "A. Pradhan",
                "K. Mehta",
                "L. Findlater"
            ],
            "title": "Accessibility Came by Accident\" Use",
            "year": 2018
        },
        {
            "authors": [
                "A. Sala",
                "Y. Punie",
                "V. Garkov",
                "M. Cabrera"
            ],
            "title": "LifeComp: The European framework for personal, social and learning to learn key competence (No",
            "venue": "Joint Research Centre (Seville site)",
            "year": 2020
        },
        {
            "authors": [
                "G. Noritz",
                "A. Elek",
                "K. Conkol",
                "S. Rust",
                "M. Bailey",
                "R. Strouse",
                "A. Chandawarkar",
                "V. Sadovszky",
                "S. Lin",
                "Y. Huang"
            ],
            "title": "Capturing at -home health and care information for children with medical complexity using voice interactive technologies: multi-stakeholder viewpoint",
            "venue": "Journal of medical Internet research ,",
            "year": 2020
        },
        {
            "authors": [
                "C.L.V. Straten",
                "J. Peter",
                "R. K\u00fchne",
                "A. Barco"
            ],
            "title": "Transparency about a robot 's lack of human psychological capacities: effects on child-robot perception and relationship formation",
            "venue": "ACM Transactions on Human-Robot Interaction (THRI),",
            "year": 2020
        },
        {
            "authors": [
                "S. von Struensee"
            ],
            "title": "Eye on Developments in Artificial Intelligence and Children's Rights: Artificial Intelligence in Education (AIEd), EdTech, Surveillance, and Harmful Content. EdTech, Surveillance, and Harmful",
            "venue": "Content (June",
            "year": 2021
        },
        {
            "authors": [
                "Y. Xu",
                "M. Warschauer"
            ],
            "title": "Elinor Is Talking to Me on the Screen!\" Integrating Conversational Agents into Children's Television Programming",
            "venue": "Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems (pp",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "becoming very popular in some countries, and despite their adult -centred design, they have become part of children\u2019s lives, generating a need for children-centric trustworthy systems. This paper presents a literature rev iew t o identify the main opportunities, challenges and risks brought by CAs when used by children. We then consider relevant ethical guidelines for AI and adapt them to this particular system and population, using a Delphi methodology with a set of experts from different disciplines. From this analysis, we propose specific guidelines to help CAs developers improve their design towards trustworthiness and children.\nKeywords: Conversational Agents, Children, Ethical Guidelines"
        },
        {
            "heading": "1 Introduction",
            "text": ""
        },
        {
            "heading": "1.1 Motivation",
            "text": "Dialogue Systems, Virtual Assistants, Chatbots ... Conversational agents (CAs) have many different names, but they all refer to a computer program that supports conversational interactions with humans (McTear, 2020). Nowadays, CAs have become very popular, and recent developments allow people to in teract with s mall computers placed in handy gadgets through voice: Google assistant can guide you in a car, Siri can send your messages in a smartphone, or Alexa can play music on a smart speaker. CAs are gaining popularity as a greater number of people are using them in their daily life. Traditional CAs are composed of five different modules as illustrated in Figure 1: ASR, an automatic speech recognition engine that transforms audio speech inputs into text; NLU, a natural language understanding system that semantically in terprets an input text; DM, a dialogue manager that manages the CA actions at the communication level and at the action level; NLG, a natural language generator that translates the computer intent into text; TTS, a text to speech system that transforms\ntext into audio output. Nowadays many systems use machine learning techniques to fulfil the task of one or several modules.\nDespite the adult centric design of CAs, accessibility and popularity among children should be carefully considered, since even the younger ones can interact with CAs through voice. For instance, Sciuto found out that children make a more extensive use of these devices and explore further capabilities than adu lts (Sciu to, 2018). This explains the huge impact CAs can have on the little ones. As an example, in a CNN article, Kelly quotes: \u201cThe first four words my toddler understood were \u2018mom\u2019, \u2018dad\u2019, \u2018cat\u2019 and \u2018Alexa\u2019\u201d (Kelly, 2018). There is then a need to research the impact of CAs on children, being a vulnerable population widely exposed to these technologies. In addition, we need some ethical guidelines for the development o f CAs that can be trusted by children."
        },
        {
            "heading": "1.2 Goals and structure of the paper",
            "text": "The goal of this paper is to provide some practical ethical guidance to CAs developers, considering children as a target audience. These guidelines are intended to enhance the opportunities of CAs while minimising the risks they may bring to th is vulnerable population. Our study has then two main goals: (1) Identify opportunit ies vs risks of CAs for children and the main ethical considerations documented in the research literature; and (2) Adapt existing ethical guidelines for Artificial Intelligence (AI) to these particular systems (CAs) and target population (children).\nThese two goals are addressed as follows. Section 2 presents a literature review on the opportunities, risks and challenges of developing CAs for children, and an\nanalysis of relevant ethical guidelines in the context of AI and children\u2019s rights. Section 3 presents our approach for adapting the two considered ethical guidelines to our particular context. Section 4 presents the obtained results, fu rther d is cussed in Section 5. Section 6 summarises our main conclusions and steps for future research."
        },
        {
            "heading": "2 Literature Review",
            "text": ""
        },
        {
            "heading": "2.1 Opportunities, risks and learned lessons",
            "text": "The research literature has identified opportunities brought by CAs to children, summarised as follows:\n Improvement of accessibility: CAs can facilitate the interaction with computers to children too young to write, with dyslexia or physical\ndisabilities (Catania et al., 2021; Pradhan et al., 2018).\n Engagement of learning: CAs can support information search (Landoni et al., 2020) language learning (Nasihati et al., 2018), or school material\nlearning (Law et al., 2020; Xu and Warschauer, 2020).\n Promotion of social behaviour: CAs that requires the user to use persuasive strategies in games with them (Fraser et al., 2018), or help\nautistic children with their social skills (Zhang et al., 2020).\n Support of health at homes : CAs have been used to help recording treatments and track certain diseases (Sezgin et al., 2020).\nRecent studies have also identified some risks brought by CAs and challenges that\nneed to be considered, mainly bias due to adult-centric design of CAs, children\u2019s overtrust and potential unexpected impact. In fact, different issues have been identified depending on different modules (Table 1).\nWe have identified the following main suggestions to overcome the mentioned risks:\n Communication abilities . Children\u2019s speech and understanding s hould be considered in the interaction to improve children's inclusivity. Even with the current technology it is difficult to overcome these biases, however, s ome researchers try to improve CA\u2019s performance for children. Lavechin\ndeveloped a speech identifier for babies (Lavechin et al., 2020), and o ther researchers identified good strategies to follow when a system does not understand a child (Cheng et al., 2018).\n Dialogue management. User\u2019s age should influence certain decisions of the system. Verbal and not verbal responses should also be appropriate.  Transparency. Another relevant risk of CAs is to generate overtrust in children. Children tend to perceive CAs as friends, so CAs might in fluence them, e.g. in terms of data disclosure (Druga et al., 2020; Kahn et al., 2012). Straten taught us that transparent information helps to fight overtrust (Straten\net al., 2020).\n Continuous evaluation. CAs are new in our lives, and the impact of their use on our society and children are still to be discovered. An example o f an\nunexpected problem is parents asking Amazon to change the wake-up word \u201cAlexa\u201d in their CA product because their daughters, named Alexa, were suffering bullying at school due to this name coincidence (Johns, T., 2021). In consequence, evaluation and oversight of these devices have become highly relevant for the early detection of potential risks, and to implement the needed intervention practices."
        },
        {
            "heading": "2.2 Ethical guidelines for trustworthy AI and children",
            "text": "In recent years, several organisations have paid special attention to the ethical development of AI systems. Their aim is to generate awareness on AI systems, contribute to their understanding and evaluation, and study how to minimise the ris ks they can bring, while maximising their benefits. In this study, we focus on two main initiatives: the Ethical Guidelines for Trustworthy AI and UNICEF policy gu idance on AI for children.\nThe High Level Expert Group (HLEG) of the European Commission developed the Ethical Guidelines for Trustworthy AI (AI HLEG, 2020), motivated by the need to protect people\u2019s fundamental rights in different contexts where AI systems are used. These ethical guidelines include seven requirements and are complemented by an assessment list for trustworthy AI (ALTAI), designed as a practical too l to help organisations self-assess the trustworthiness of their AI systems. ALTAI is a lis t o f sixty-nine self-evaluation questions, grouped in the mentioned seven requirements as follows:\n1. Human agency and oversight (11 questions). AI systems s hould respect human autonomy and decision-making, and should be supervised by\nhumans. 2. Technical robustness and safety (21 questions). AI systems should be\naccurate, reliable and safe, having a preventative approach to risks.\n3. Privacy and data governance (6 questions). AI systems should pro tect our\nprivacy and have legitimate access to our data. 4. Transparency (5 questions). AI systems should have clear documentat ion\nand inform users about its decisions, capabilities and limitations. 5. Diversity, non-discrimination and fairness (10 questions). AI systems\nshould ensure inclusion through all the AI system\u2019s life cycle. 6. Societal and environmental well-being (8 questions). AI s ystems should\nbenefit the world and society. 7. Accountability (8 questions). AI systems should have mechanisms to ensure\nresponsibility for development, deployment and use of AI systems.\nThe UNICEF policy guidance on AI for children is a guide intended to help\npolicy makers and businesses by raising awareness of children\u2019s rights in the context of AI systems. It is proposed to complement existing work, guided by nine requirements that are presented in Table 2 (Digdum et al., 2021).\nWe performed a qualitative mapping between HLEG ALTAI and UNICEF AI fo r children requirements to understand the suitability of ALTAI having in account UNICEF consideration for AI and children\u2019s rights. From the detailed defin itions, we related the 9 UNICEF requirements to the 7 requirements of HLEG. For instance, UNICEF requirement 9 refers to oversight, digital divide, and ethical development o f AI from governments, that we connect to HLEG requirements on oversight, agency , fairness, and societal well/being. More details about the procedure to obtain this matrix is shared in https://github.com/mescpla/CAs4Children-ETHICOMP22.git.\nWe observe that ALTAI has a strong focus on the development and evaluat ion o f specific AI devices. However, some UNICEF AI for children requirements have a strong focus on policies. Nevertheless, most requirements from UNICEF AI for children are connected to at least one major and some additional requirements of HLEG ALTAI, except for the educational aspect of requirement (8) which focuses on policies and is missing in ALTAI, which only refers to work and skills in HLEG requirement 6. In addition, all requirements present in HLEG ALTAI are covered by UNICEF AI for children guidance.\nFrom this analysis we consider in the rest of our study the ALTAI list as a starting point, with a focus on CAs, complemented by the UNICEF guidelines on AI for children, as a complementary framework connected to children's rights, e.g. incorporating educational aspects."
        },
        {
            "heading": "3 Proposed methodology",
            "text": "From the previous literature, we propose a methodology to adapt existing ethical guidelines to the use of CAs and children and incorporate identified considerations. For that purpose, we identified prioritisation and action points from ALTAI by performing a risk level analysis for every ALTAI item (question), following the metric below:\nRisk = Likelihood x Impact (1)\nIn order to obtain these measures (Likelihood and Impact), we have followed the Delphi method (Linstone and Turoff, 1975), conducting a survey among four experts. Then, once the experts concluded the Likelihood and Impact of every ALTAI item, we assessed the risk by the matrix approach (Kova\u010devi\u0107et al., 2019), identifying critical points."
        },
        {
            "heading": "3.1 Delphi method",
            "text": "To follow the Delphi method, we designed a questionnaire that asks experts to rate, for each ALTAI question, how relevant it is for children vs general population and for CAs vs AI systems in general. We use two main criteria: the likelihood or frequency of application (i.e. if the question would apply to all situations or only to certain) and the impact or relevance, using a 3 point likert scale for simplicity, given that the questionnaire has 69 questions, with 4 ratings per question (Table 3). In addition , we provided some space for experts to comment on their specificities and reasoning behind the rating when needed.\nWe explained the questionnaire to experts from different disciplines and areas (AI ethics, CAs, education) who independently filled it using their own criteria. Individual answers were then analysed in order to identify disagreements (the mean of s imilar answers was 74%) and critical points. An expert meeting was later organised to discuss the identified critical points and disagreements and arrive at a common\ncriteria and consensus. After the meeting, experts had the chance to review and refine their individual responses and submit their final version (84% similar answers)."
        },
        {
            "heading": "3.2 Risk assessment",
            "text": "For each of the ALTAI items (i.e. questions) we computed partial risk levels for children and CAs, as follows:\nRisk value for ALTAI questions . For each item of ALTAI, we performed the arithmetic mean to combine the four reported Likelihood and Impact measures from the experts related to children and CAs in a separate way. Later on, we bu ilt the Child Risk and the CA Risk (called partial risks from now on) by using Formula (1) with their respective Likelihood and Impact means. In addition, a risk assessment matrix was built to measure the level of risk of our results (Fig.2.a).\nRisk value for HLEG ALTAI requirements . We computed the arithmetic mean for every question inside a given requirement (e.g. Human Agency and Oversight requirement is composed of eleven questions), to calculate the Likelihood and Impact of the requirement. We did it separately to calc u late the partial risks for children and CAs. Later on, the Child Risk and the CA Risk o f a particular requirement was calculated following Formula (1).\nFrom individual partial risks (Child Risk and CA Risk) per question and requirement , we calculate the Total Risk of every question and requirement by the following formula:\nTotal Risk = Child Risk x CA Risk (2)\nFinally, we calculate a risk assessment matrix for the Total Risk (Figure 2.b) in order to understand the severity of the risk levels. Detailed results can be found in https://github.com/mescpla/CAs4Children-ETHICOMP22.git."
        },
        {
            "heading": "3.3 Risk assessment",
            "text": "In order to complement the quantitative assessments, we combined all notes provided by the experts in the questionnaire and the critical points discussed during the Delph i meeting. We carried out a thematic analysis (Braun & Clarke, 2006). First, we compiled all the annotated comments, identifying initial ideas. Secondly, we grouped the ideas in possible themes that we discussed and refined unt il our final vers ion . Finally, we selected the examples that we would use in the report."
        },
        {
            "heading": "4 Results",
            "text": ""
        },
        {
            "heading": "4.1 Ordered assessment list",
            "text": "Table 4 shows the ratings for likelihood, impact and risk for children and CAs dimensions as well as total values. We first observe that, in general, the obtained Impact values are higher for children than for CAs. However, the Likelihood is much larger for CAs than for children. This explains why the CA Risk is higher than Children Risk for every category but for: Human oversight, Accuracy, Explainability, and Environmental well-being (Table 4).\nConsidering the partial risk assessment matrix (Fig.2.a), our experts have identified Human agency and oversight, Privacy and data governance and Transparency as the main critical points for children, while Privacy and data governance, Human agency and oversight, and Diversity, non -discriminat ion and fairness are the main critical requirements for CAs. Regarding the Total Risk , and considering the combined risk assessment matrix (Fig.2.b), we identify Privacy and data governance and Human agency and oversight (with a critical po in t on Human agency) as the main critical requirements to be considered when developing CAs for\nchildren. The only requirement which values are over the matrix diagonal is Societa l and environmental well-being, with the lowest partial and combined risk levels scores."
        },
        {
            "heading": "4.2 Thematic Analysis",
            "text": "During the Delphi meeting, our experts (R1, R2, R3 and R4) discussed some relevan t topics, and other critical considerations were pointed out as questionnaire notes (Table 3). As mentioned in the methodology (Section 3), we performed a thematic analysis and identified critical considerations for the ethical design of ch ild -cent ric CAs. These considerations are visually presented in Figure 3 and Table 5, and summarised as follows: Involve children stakeholders. Many of the experts commented on the relevance o f involving children stakeholders (children, teachers, parents \u2026) in the design, use, and test of the system. R1 wrote: \u201cInclude children, tutors and teachers as stakeholders\u201d, R2 mentioned: \u201cMultiple stakeholders need to be involved\u201d. Furthermore, it was mentioned the need to \u201cteach stakeholders\u201d (R2), so they can help to oversee the system. In addition the experts agreed that -as children must not work- their collaboration in the design, use, and test of the system needs to be done in a meaningful and entertaining way. As R4 said: \u201cWe need to involve ch i ldren in the design, but in a meaningful way as this participation should be far from job conditions\u201d.\nAI awareness. The experts expressed many considerations regarding the fact that people -particularly children- should be aware of what a CA is, how it works and it s limitations. In particular:\nNot-human nature. The experts remarked that CAs communicate in a natural way, which can lead to confusion about their nature (\u201cNaturalness o f CAs might create confusion\u201d (R2)). Therefore, there is a risk of developing attachment to the device, especially for children developing their social abilities and with a particular way to understand the world. R4 wrote \u201cCareful with human attachment as children are developing their cogni tive and emotional abilities\u201d, R1 also shared: \u201cChildren can think that something that is not alive has alive characteristics such as feelings\u201d. In addit ion, the social role of the device should be regarded, as mentioned by R4: \u201cChildren might understand what the system is and that has no authority\u201d. The experts also discussed the positive points of being consistent with the in format ion provided by the CA \u201cIf we want a child to understand that a CA has no feelings, maybe it is better to avoid sentences such as \u2018I am happy\u2019 \u201d.\nInfluence. Another critical consideration is the influence a system might have on the user. Firstly, R3 remarked \u201cCAs can provide information critical to make decisions. People don't usually double check information \u201d. Additionally, R4 shared concerns about children\u2019s critical thinking \u201cChildren need to learn to be critical, to look further from the provided information, to develop creativity\u201d. Secondly, regarding CA\u2019s influence, the experts\nexpressed their concerns about children's vulnerability: \u201cCheck overreliance. Consider children's vulnerability\u201d (R2); \u201cSpecial attention to k ids over-reliance\u201d (R3). Children are highly impressionable -even more provided that we consider the previous point- and their judgement and awareness, on things such as data disclosure, are still in development. Following this, R4 pointed out: \u201cKids don't yet have good judgement, careful with influence and over-reliance\u201d. Last but not least, R2 highlighted the relevance of checking a CA\u2019s influence, as their impact is magnified by the number of people able to use the device: \u201cCAs can influence a large number of people, so the social impact of CAs should be considered\u201d.\nAccess to the system information. From a general point of view, all our experts advised developers to be transparent about the nature, funct ion ing and limitations of the system. This information promotes AI awareness, and can minimise some of the above mentioned risks. R3 also commented: \u201cUnderstanding CAs nature and interactions is important to avoid frustration\u201d. In addition, during the experts\u2019 meeting, they discussed that this information should be always accessible to the user, and provided in a proactive manner by the system in accordance with the duration and risks o f the interaction. R1 mentioned: \u201cRegarding explainability it should a lso fi t the purpose of the CA. It doesn\u2019t make sense to explain all the limitations o f a system that will interact with you for a few seconds (e.g. to ask you on the phone what department you want to contact).\u201d R3 added that this would be necessary in bank transactions or high risk interactions.\nTransparency (e.g. informing on how an AI system works, and what are its limitations) is a transversal topic mentioned in different requirements. It is considered as a tool to fight different risks brought by AI, by raising AI awareness and enhancing critical thinking. The access to information about the system (i.e. working princip les and limitations), is then essential. Two clear examples of the use of information to fight data disclosure are the \u201crelevance to inform about recordings\u201d (R3); and being \u201cimportant to know if the system learns about them or not\u201d (R4). Moreover, all the experts highlighted the need to communicate with children in a child -friendly language (\u201cQuestionnaires or explanations must be adapted to children\" (R1)).\nRisk Management. Regarding the novelty of CAs in people\u2019s lives, it is paramount to detect potential risks and ensure the accountability of the developed systems . It is also essential to understand which personal data a CA is using and storing, as related to the privacy requirement. Some identified critical aspects of risk management:\nTest, evaluation and reporting. Many CAs tend to have a low accuracy with small children and other vulnerable groups. In addition, risks for children require continuous vigilance. Defining metrics and levels to hand le risks can help with this problem; but, in addition, the experts suggested some strategies to minimise these risks . On the one hand, developers can evaluate the system in a controlled environment, involving children and\nother minorities (\u201cRelevance of testing and detect if a CA is having problems with children\u201d (R3); \u201cTest the system for different children considering vulnerable groups\u201d (R4)). On the other hand, developers can provide users - including children- with mechanisms to report issues that appear after extensive use in the real world scenarios (\u201cFlag issues in a ch ild-friendly way\u201d (R4); \u201cChildren should be able to report issues\u201d (R2)).\nAccountability. The experts shared some concerns about the vulnerability of children, and the relevance of people taking responsibility fo r the s ystem. For instance, R2 wrote: \u201cChildren are a vulnerable population in developmental stage\u201d, and R3 shared: \u201cAs CAs use biometrical data, and children are a vulnerable population, a special attention to accountability is needed\u201d. Audits may help to keep track of this problem.\nData. Regarding data, the experts raised some concerns. First of all, they emphasise on the recording of biometric data by CAs (\u201cCAs data storage contain biometrics and personal data\u201d (R2); \u201cCAs use biometric data\u201d (R3); \u201cVoice is personal data\u201d (R4)) and children\u2019s vulnerability (\u201cChildren\nhave the right to be protected\u201d (R1); \u201cExtra protection for vu lnerab il ity, careful with children overtrust\u201d (R4)). Therefore, not only should we make an extra effort for personal data protection, (compliance with European regulations such as \u201cGDPR (General Data Protection Regulat ion )\u201d (R4)), but we should also pay a special attention, if the data is shared or transferred to 3rd parties (\u201cCareful about selling data to 3rd parties\u201d (R4)), and protect the system from cyberattacks (\u201cCAs have critical data and must be protected from cyberattacks\u201d (R3)). Secondly, the consulted experts highlighted that some CAs use information from large data sources (e.g. CAs can search fo r information on the internet, or can use large datasets for training using deep learning). Among the risks of using these data sources, they ment ioned the lack of control of possible risks that can later appear in the s ystem (\u201cCAs with untrusted data sources may cause more damage\u201d (R3)). For ins tance, the experts mentioned in the meeting that, last summer, a game displayed on a CA that took information from the web, told a child to put a coin on a connected plug. The experts recommend a better risk management for those systems.\nAge appropriate behaviour. During the study our experts also identified the need to recognise children, and act appropriately, i.e specific considerations/behaviours when interacting with children:\nInclusivity. CAs bring opportunities for inclusivity of illiterate people, or people with disabilities (\u201cCAs help with inclusivity and should take special care on this point, special attention to disabilities\u201d (R3)). That is why developers should emphasise on the inclusivity of the device, setting an example to children (\u201cChildren can internalise bias, so it i s importan t for\nthem\u201d (R3); \u201cChildren inclusivity for all culture, language, age, ..\u201d (R2)). Nonetheless, these systems face some challenges understanding little children and underrepresented groups (\u201cBiases linked to not available d ata, which is a challenge for all EU languages, dialects and children\u201d (R4); \u201cConsider limitations of CAs understanding different people\u201d (R1)). Consequently, our experts remarked the importance of recognizing child ren as users, and to keep working against this bias (\u201cSpecial a t tention to b ias towards children\u201d (R1); \u201cDiscrimination by age\u201d, (R2)). In the meantime, a good strategy to fix conversations when the system cannot identify the user might help to mitigate this risk (\u201cImportant to use a good recovery strategy\u201d (R1)).\nGuardians. Children have a particular autonomy, as they need the supervision of their guardians. This should be taken into consideration when designing a device that can interact with a child. The experts highlighted their presence in different points. They advise to consider them in o rder to meet consent obligations (\u201cNeed of tutor consent\u201d (R4); \u201cTutors and children must give their consent\u201d (R1)), but also to take advantage o f their presence during interactions (\u201cRely on adult supervision when low confidence\u201d (R2); \u201cChildren are not aware, so an adult should supervise \u201d (R3)). During the experts\u2019 meeting, they discussed the supervision of guardians, identifying their presence, but bearing in mind that the s ystem should be safe enough for the child to not require constant supervision. They agreed that the system can try to use them/call them in s pecific moments , although the security of the system cannot be just based on the guardians' supervision.\nEducational and personal development. All the experts missed a section on education and children development. For instance R1 commented : \u201cWe need to consider CAs in education\u201d, R3 mentioned: \u201cNeed for educat ional consideration\u201d, and R2 wrote: \u201cWe should consider adding to ALTAI education and development questions\u201d, referring not just to school education, but also to personal development such as self-regulation (\u201cConsider children addictive behaviour\u201d, R3).\nThe experts\u2019 recommendations cover critical points for all the seven requirements from HLEG ALTAI (Table 5). Being Societal and environmental well-being the requirement with more critical themes pointed out by the experts, followed by Human agency and oversight and Technical robustness and safety. Experts also covered all the learned lessons from the literature review."
        },
        {
            "heading": "5 Discussion and recommendations for child-centric CA",
            "text": "From the results presented in Section 4, we recommend developers to consider the ALTAI assessment list, in the following order of priority (Fig.3): Privacy and data governance, Human agency and oversight, Diversity, non-discrimination and fairness, Transparency, Accountability, Technical robustness and safety, and Societal and environmental well-being. In addition, developers should pay special attention to the considerations outlined by the experts (Fig.6): Involve children stakeholders , AI awareness, Transparency, Risk management and Age appropriate behaviour . These recommendations will help developers to maximise CAs opportunities fo r ch ild ren while minimising risks, creating more accessible CAs, supporting educational activities, social behaviour and safety.\nAnother interesting conclusion from our work is the identification of a subsect ion that could enrich current ALTAI guidelines for children: an \u201ceducation and selfdevelopment\u201d set of questions in the Societal and environmental well-being requirement.\nFurthermore, in our risk assessment analysis, the main critical point detected is Privacy and data governance. This point was also covered by our experts\u2019 crit ical\nconsiderations in the identified topics of Risk management and Age appropriate behaviour where they highlighted the presence of children\u2019s guardians. These considerations are aligned with previous studies (von Struensee, 2021). Nevertheless, while the use of data protection regulations is well established, we found little research on the application of data privacy regulations considering AI, children autonomy, and guardians. Therefore, we recommend integrating research outcomes\nfrom existing medical studies that use biometric data from children (Hopf et al., 2014).\nBesides, our results bring special attention to Human agency and oversight, with a special focus on the not-human nature of the system. This was also reflected on the thematic analysis in AI awareness using Transparency as a tool. These recommendations are in accordance with existing work (Straten et al., 2020).\nWe also identified some limitations of our study. Firstly, our study comes from an european-centric perspective, e.g. focus on Ethical guidelines for trustworthy AI and involving an EU expert group. Therefore, these results cannot then be generalis ed to different cultures such as Asian or African. Henceforth, we suggest complementary research in larger and more diverse groups with different cultural backgrounds.\nSecondly, regarding our metrics, Children Risk was generally lower than CAs Risk, mainly because of a high rated CAs Likelihood. We recognize that our metrics pu t at the same level children and CAs considerations, when it might be more adequate to highlight children\u2019s considerations, e.g. through weights. We encourage complementary studies with alternative metrics.\nFinally, HLEG ALTAI Societal and environmental well-being requirement had the lowest risk level - surprisingly low considering it is also a fundamental requ irement on other studies on AI and children\u2019s rights (Charisi, et al., 2022). This might be due to the inclusion of a work impact section (with a low Likelihood and Impact on children), and the lack of an Education and self-development section. This new - and needed- section would change our results of the risk assessment. We encourage further research to build new items for ALTAI on this topic, and suggest the use o f LifeComp competences for its development (Sala, et al., 2020): personal (selfregulation, flexibility, well-being), social (empathy, communication, collaboration), and learning to learn (growth mindset, critical thinking, managing learning)."
        },
        {
            "heading": "6 Conclusions",
            "text": "We have performed a literature review on conversational agents (CAs), ident ify ing opportunities, challenges and risks for their use with children. In add it ion , we have consulted a group of experts to measure the risk of all the items of the assessment lis t for trustworthy Artificial Intelligence (ALTAI) with a focus on children and CAs. With our results, we adapt ALTAI for this specific use, defining priorities on the requirements and adding additional considerations. We hope this res earch can help CA developers to build trustworthy child-centric systems that can respect fundamental and children rights, ensuring a future where children can also take the most from CAs. We may have safer and better-informed citizens with critical thinking in the future."
        },
        {
            "heading": "7 Acknowledgements",
            "text": "We thank our experts for their patience and dedication to our experiment, particularly Riina Vuorikari. We thank Isabelle Hupont-Torres and Marta Rivera for their guidance regarding the analysis and assessment of risk. Finally, we want to thank the HUMAINT team for their constant support and useful comments to the text."
        }
    ],
    "title": "Guidelines to Develop Trustworthy Conversational Agents for Children",
    "year": 2022
}