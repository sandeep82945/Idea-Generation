{
    "abstractText": "Graph Neural Networks (GNN) have recently gained popularity in the forecasting domain due to their ability to model complex spatial and temporal patterns in tasks such as traffic forecasting and region-based demand forecasting. Most of these methods require a predefined graph as input, whereas in real-life multivariate time series problems, a well-predefined dependency graph rarely exists. This requirement makes it harder for GNNs to be utilised widely for multivariate forecasting problems in other domains such as retail or energy. In this paper, we propose a hybrid approach combining neural networks and statistical structure learning models to self-learn the dependencies and construct a dynamically changing dependency graph from multivariate data aiming to enable the use of GNNs for multivariate forecasting even when a well-defined graph does not exist. The statistical structure modeling in conjunction with neural networks provides a well-principled and efficient approach by bringing in causal semantics to determine dependencies among the series. Finally, we demonstrate significantly improved performance using our proposed approach on real-world benchmark datasets without a pre-defined dependency graph.",
    "authors": [
        {
            "affiliations": [],
            "name": "Abishek Sriramulua"
        },
        {
            "affiliations": [],
            "name": "Nicolas Fourrier"
        },
        {
            "affiliations": [],
            "name": "Christoph Bergmeir"
        }
    ],
    "id": "SP:d021c22f99c4d6a20e6c64e8e2f1f883cc150df2",
    "references": [
        {
            "authors": [
                "A. Ali",
                "Y. Zhu",
                "M. Zakarya"
            ],
            "title": "Exploiting dynamic spatio-temporal correlations for citywide traffic flow prediction using attention based neural networks",
            "venue": "Information Sciences 577,",
            "year": 2021
        },
        {
            "authors": [
                "D. Bahdanau",
                "K. Cho",
                "Y. Bengio"
            ],
            "title": "Neural machine translation by jointly learning to align and translate",
            "year": 2015
        },
        {
            "authors": [
                "S. Bai",
                "J.Z. Kolter",
                "V. Koltun"
            ],
            "title": "An empirical evaluation of generic convolutional and recurrent networks for sequence modeling",
            "year": 2018
        },
        {
            "authors": [
                "K. Bandara",
                "H. Hewamalage",
                "Y.H. Liu",
                "Y. Kang",
                "C. Bergmeir"
            ],
            "title": "Improving the accuracy of global forecasting models using time series data augmentation",
            "year": 2021
        },
        {
            "authors": [
                "G. Box",
                "G.M. Jenkins"
            ],
            "title": "Time Series Analysis: Forecasting and Control. Holden-Day",
            "year": 1976
        },
        {
            "authors": [
                "W. Chen",
                "M. Jiang",
                "W.G. Zhang",
                "Z. Chen"
            ],
            "title": "A novel graph convolutional feature based convolutional neural network for stock trend prediction",
            "venue": "Information Sciences",
            "year": 2021
        },
        {
            "authors": [
                "C. Fan",
                "Y. Zhang",
                "Y. Pan",
                "X. Li",
                "C. Zhang",
                "R. Yuan",
                "D. Wu",
                "W. Wang",
                "J. Pei",
                "H. Huang"
            ],
            "title": "Multi-horizon time series forecasting with temporal attention learning",
            "venue": "in: Proceedings of the 25th ACM SIGKDD,",
            "year": 2019
        },
        {
            "authors": [
                "J. Friedman",
                "T. Hastie",
                "R. Tibshirani"
            ],
            "title": "Sparse inverse covariance estimation with the graphical lasso",
            "venue": "Biostatistics",
            "year": 2008
        },
        {
            "authors": [
                "M. Guan",
                "A.P. Iyer",
                "T. Kim"
            ],
            "title": "Dynagraph: dynamic graph neural networks at scale, in: Proceedings of the 5th ACM SIGMOD Joint International Workshop on Graph Data Management Experiences & Systems (GRADES) and Network Data Analytics (NDA)",
            "year": 2022
        },
        {
            "authors": [
                "T. Guo",
                "T. Lin",
                "N. Antulov-Fantulin"
            ],
            "title": "Exploring interpretable LSTM neural networks over multi-variable data",
            "year": 2019
        },
        {
            "authors": [
                "H. Hartle",
                "B. Klein",
                "S. McCabe",
                "A. Daniels",
                "G. St-Onge",
                "C. Murphy",
                "L. H\u00e9bert-Dufresne"
            ],
            "title": "Network comparison and the within-ensemble graph distance",
            "venue": "Proceedings of the Royal Society A 476,",
            "year": 2020
        },
        {
            "authors": [
                "S. Hochreiter",
                "J. Schmidhuber"
            ],
            "title": "Long short-term memory",
            "venue": "Neural computation",
            "year": 1997
        },
        {
            "authors": [
                "S. Huang",
                "D. Wang",
                "X. Wu",
                "A. Tang"
            ],
            "title": "Dsanet: Dual self-attention network for multivariate time series forecasting",
            "venue": "in: ACM CIKM,",
            "year": 2019
        },
        {
            "authors": [
                "Z. Huang",
                "W. Zhang",
                "D. Wang",
                "Y. Yin"
            ],
            "title": "A GAN frameworkbased dynamic multi-graph convolutional network for origin-destinationbased ride-hailing demand prediction",
            "venue": "Information Sciences",
            "year": 2022
        },
        {
            "authors": [
                "T.V. Jensen",
                "P. Pinson"
            ],
            "title": "Re-europe, a large-scale dataset for modeling a highly renewable european electricity system",
            "venue": "Scientific data",
            "year": 2017
        },
        {
            "authors": [
                "W. Jiang",
                "J. Luo"
            ],
            "title": "Graph neural network for traffic forecasting: A survey",
            "year": 2021
        },
        {
            "authors": [
                "D. Jin",
                "Z. Liu",
                "W. Li",
                "D. He",
                "W. Zhang"
            ],
            "title": "Graph convolutional networks meet markov random fields: Semi-supervised community detection in attribute networks",
            "venue": "in: Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "J. Klicpera",
                "A. Bojchevski",
                "S. G\u00fcnnemann"
            ],
            "title": "Predict then propagate: Graph neural networks meet personalized pagerank",
            "year": 2018
        },
        {
            "authors": [
                "J. Klicpera",
                "A. Bojchevski",
                "S. G\u00fcnnemann"
            ],
            "title": "Combining neural networks with personalized pagerank for classification on graphs, in: ICLR",
            "year": 2019
        },
        {
            "authors": [
                "S. Kolassa"
            ],
            "title": "Evaluating predictive count data distributions in retail sales forecasting",
            "venue": "International Journal of Forecasting",
            "year": 2016
        },
        {
            "authors": [
                "S. Li",
                "X. Jin",
                "Y. Xuan",
                "X. Zhou",
                "W. Chen",
                "Y.X. Wang",
                "X. Yan"
            ],
            "title": "Enhancing the locality and breaking the memory bottleneck of transformer 28 on time series forecasting",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2019
        },
        {
            "authors": [
                "X. Li",
                "M. Zhang",
                "S. Wu",
                "Z. Liu",
                "L. Wang",
                "S.Y. Philip"
            ],
            "title": "Dynamic graph collaborative filtering, in: 2020",
            "venue": "IEEE International Conference on Data Mining (ICDM),",
            "year": 2020
        },
        {
            "authors": [
                "Y. Li",
                "K. Li",
                "C. Chen",
                "X. Zhou",
                "Z. Zeng"
            ],
            "title": "Modeling temporal patterns with dilated convolutions for time-series forecasting",
            "venue": "ACM Transactions on Knowledge Discovery from Data (TKDD)",
            "year": 2021
        },
        {
            "authors": [
                "Z. Li",
                "F. Nie",
                "X. Chang",
                "Y. Yang",
                "C. Zhang",
                "N. Sebe"
            ],
            "title": "Dynamic affinity graph construction for spectral clustering using multiple features",
            "venue": "IEEE transactions on neural networks and learning systems",
            "year": 2018
        },
        {
            "authors": [
                "B. Lim",
                "S.O. Arik",
                "N. Loeff",
                "T. Pfister"
            ],
            "title": "Temporal fusion transformers for interpretable multi-horizon time series forecasting arXiv:1912.09363",
            "year": 1912
        },
        {
            "authors": [
                "J.T. Lizier"
            ],
            "title": "Jidt: An information-theoretic toolkit for studying the dynamics of complex systems",
            "venue": "Frontiers in Robotics and AI",
            "year": 2014
        },
        {
            "authors": [
                "M. Luo",
                "X. Chang",
                "L. Nie",
                "Y. Yang",
                "A.G. Hauptmann",
                "Q. Zheng"
            ],
            "title": "An adaptive semisupervised feature analysis for video semantic recognition",
            "venue": "IEEE transactions on cybernetics",
            "year": 2017
        },
        {
            "authors": [
                "T. Luong",
                "H. Pham",
                "C.D. Manning"
            ],
            "title": "Effective approaches to attention-based neural machine translation",
            "venue": "in: Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2015
        },
        {
            "authors": [
                "R.N. Mantegna"
            ],
            "title": "Hierarchical structure in financial markets",
            "venue": "The European Physical Journal B-Condensed Matter and Complex Systems",
            "year": 1999
        },
        {
            "authors": [
                "H. Peng",
                "B. Du",
                "M. Liu",
                "S. Ji",
                "S. Wang",
                "X. Zhang",
                "L. He"
            ],
            "title": "Dynamic graph convolutional network for long-term traffic flow prediction with reinforcement learning",
            "venue": "Information Sciences 578,",
            "year": 2021
        },
        {
            "authors": [
                "H. Peng",
                "H. Wang",
                "B. Du",
                "M.Z.A. Bhuiyan",
                "H. Ma",
                "J. Liu",
                "L. Wang",
                "Z. Yang",
                "L. Du",
                "S Wang"
            ],
            "title": "Spatial temporal incidence dynamic graph neural networks for traffic flow forecasting",
            "venue": "Information Sciences",
            "year": 2020
        },
        {
            "authors": [
                "M. Qu",
                "Y. Bengio",
                "J. Tang"
            ],
            "title": "Gmnn: Graph markov neural networks, in: International conference on machine learning, PMLR",
            "year": 2019
        },
        {
            "authors": [
                "M. Qu",
                "J. Tang"
            ],
            "title": "Probabilistic logic neural networks for reasoning. Advances in neural information processing systems",
            "year": 2019
        },
        {
            "authors": [
                "S. Roberts",
                "M. Osborne",
                "M. Ebden",
                "S. Reece",
                "N. Gibson",
                "S. Aigrain"
            ],
            "title": "Gaussian processes for time-series modelling",
            "venue": "Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences",
            "year": 2013
        },
        {
            "authors": [
                "M. Scutari"
            ],
            "title": "Learning bayesian networks with the bnlearn r package",
            "year": 2009
        },
        {
            "authors": [
                "S.Y. Shih",
                "F.K. Sun",
                "H.Y. Lee"
            ],
            "title": "Temporal pattern attention for multivariate time series forecasting",
            "year": 2019
        },
        {
            "authors": [
                "Z. Sun",
                "B. Wu",
                "Y. Wang",
                "Y. Ye"
            ],
            "title": "Sequential graph collaborative filtering",
            "venue": "Information Sciences",
            "year": 2022
        },
        {
            "authors": [
                "I. Sutskever",
                "O. Vinyals",
                "Q.V. Le"
            ],
            "title": "Sequence to sequence learning with neural networks, in: Advances in neural information processing",
            "year": 2014
        },
        {
            "authors": [
                "A. Vaswani",
                "N. Shazeer",
                "N. Parmar",
                "J. Uszkoreit",
                "L. Jones",
                "A.N. Gomez",
                "Kaiser",
                "L.u",
                "I. Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "in: Advances in Neural Information Processing Systems",
            "year": 2017
        },
        {
            "authors": [
                "Z. Wu",
                "S. Pan",
                "F. Chen",
                "G. Long",
                "C. Zhang",
                "P.S. Yu"
            ],
            "title": "A comprehensive survey on graph neural networks",
            "venue": "IEEE TNNLS ,",
            "year": 2020
        },
        {
            "authors": [
                "Z. Wu",
                "S. Pan",
                "G. Long",
                "J. Jiang",
                "X. Chang",
                "C. Zhang"
            ],
            "title": "Connecting the dots: Multivariate time series forecasting with graph neural networks, in: Proceedings of the 26th ACM SIGKDD",
            "year": 2020
        },
        {
            "authors": [
                "K. Xu",
                "J. Ba",
                "R. Kiros",
                "K. Cho",
                "A. Courville",
                "R. Salakhudinov",
                "R. Zemel",
                "Y. Bengio"
            ],
            "title": "Show, attend and tell: Neural image caption generation with visual attention, PMLR, Lille, France",
            "year": 2015
        },
        {
            "authors": [
                "S. Yan",
                "Y. Xiong",
                "D. Lin"
            ],
            "title": "Spatial temporal graph convolutional networks for skeleton-based action recognition",
            "year": 2018
        },
        {
            "authors": [
                "J. You",
                "T. Du",
                "J. Leskovec"
            ],
            "title": "Roland: graph learning framework for dynamic graphs",
            "venue": "in: Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,",
            "year": 2022
        },
        {
            "authors": [
                "H.L. Zeng",
                "M. Alava",
                "E. Aurell",
                "J. Hertz",
                "Y. Roudi"
            ],
            "title": "Maximum likelihood reconstruction for ising models with asynchronous updates",
            "venue": "Physical review",
            "year": 2013
        },
        {
            "authors": [
                "G.P. Zhang"
            ],
            "title": "Time series forecasting using a hybrid arima and neural network model",
            "venue": "Neurocomputing",
            "year": 2003
        },
        {
            "authors": [
                "Y. Zhang",
                "X. Chen",
                "Y. Yang",
                "A. Ramamurthy",
                "B. Li",
                "Y. Qi",
                "L. Song"
            ],
            "title": "Efficient probabilistic logic reasoning with graph neural networks",
            "year": 2020
        },
        {
            "authors": [
                "R. Zhou",
                "X. Chang",
                "L. Shi",
                "Y.D. Shen",
                "Y. Yang",
                "F. Nie"
            ],
            "title": "Person reidentification via multi-feature fusion with adaptive graph learning",
            "venue": "IEEE transactions on neural networks and learning systems",
            "year": 2019
        },
        {
            "authors": [
                "E. Zivot",
                "J. Wang"
            ],
            "title": "Vector autoregressive models for multivariate time series, in: Modeling Financial Time Series with S-Plus\u00ae",
            "year": 2003
        }
    ],
    "sections": [
        {
            "text": "Graph Neural Networks (GNN) have recently gained popularity in the forecasting domain due to their ability to model complex spatial and temporal patterns in tasks such as traffic forecasting and region-based demand forecasting. Most of these methods require a predefined graph as input, whereas in real-life multivariate time series problems, a well-predefined dependency graph rarely exists. This requirement makes it harder for GNNs to be utilised widely for multivariate forecasting problems in other domains such as retail or energy. In this paper, we propose a hybrid approach combining neural networks and statistical structure learning models to self-learn the dependencies and construct a dynamically changing dependency graph from multivariate data aiming to enable the use of GNNs for multivariate forecasting even when a well-defined graph does not exist. The statistical structure modeling in conjunction with neural networks provides a well-principled and efficient approach by bringing in causal semantics to determine dependencies among the series. Finally, we demonstrate significantly improved performance using our proposed approach on real-world benchmark datasets without a pre-defined dependency graph.\nKeywords: Multivariate Forecasting, Graph Neural Networks, Dynamic Graph Learning, Time Series\n\u2217Postal Address: Faculty of Information Technology, P.O. Box 63 Monash University, Victoria 3800, Australia. E-mail address: abishek.sriramulu@monash.edu\nPreprint submitted to Elsevier December 8, 2023\nar X\niv :2\n31 2.\n03 90\n3v 1\n[ cs\n.L G\n] 6\nD ec"
        },
        {
            "heading": "1. Introduction",
            "text": "In the present day, various communities in the world are positively impacted by understanding and modeling the series of data recorded over time such as the price of products in the retail industry, energy consumption, and movement of traffic. Frequently, multiple series of data are observed together at the same point in time and these series might exhibit relationships among themselves. Recent studies on Global Forecasting Models (GFM) have proven that many forecasting problems benefit hugely by generalising over multiple (related) time series, rather than modeling them isolated [4]. Even though GFMs model global information across multiple time series, they assume that each series is recorded in isolation and that there exists no direct influence among them.\nIn situations with multivariate data, considering the fact that all the series have been recorded together at the same point in time can provide advantages while modeling the data. Modeling these inter-dependencies from multivariate data is significant but difficult as they are often complex and dynamic in nature. For example, an increase in the sales of one product may cause an increase or decrease in the sales of another product in a retail dataset due to several reasons, and this relationship may change over time.\nTraditionally forecasts are made based on the historical values of the series. The Gaussian Process and different vector autoregression (VAR) models are a few of the most widely used forecasting methods for multivariate time series [50]. These models work under the assumption that there exists a linear relationship between every variable to every other variable in the system, which makes a direct interpretation of the estimated coefficients difficult. Furthermore, these models tend to over-fit while handling large numbers of variables [42].\nThe ability to capture complex relationships makes neural network models a successful alternative to solve multivariate forecasting problems [21, 10, 13]. Lai et al. [21] introduced a new variant of the recurrent neural network model called LSTNet which encodes the locality in series using 1D convolutional layers to a low dimensional representation. Then, the network decodes the output of the\nencoder using recurrent neural network blocks, however, the interdependencies between the series are not learned distinctly in LSTNet.\nAttention mechanisms have gained popularity in the machine learning space due to their ability to model dependencies without regard to their distance in the input or output sequences [2]. For time series forecasting, attention mechanisms have proven to improve modeling when dealing with longer sequences and also aid in improvising feature selection [26]. The IMV-LSTM model proposed by Guo et al. [10] has the ability to exploit the structure of Long Short-Term Memory (LSTM) to learn both the inter-series and intra-series dependencies simultaneously. The attention-based LSTM used in IMV-LSTM aids in learning the inner dynamics of the series. IMV-LSTM is observed to perform the best when the number of series is small but does not scale well when the series are large in number. Huang et al. [13] proposed DSANet which combines an autoregressive component with a dual self- attention network where each of the series in a multivariate time series is modeled independently in two convolutional components simultaneously to capture both the global and local temporal patterns in the data. These learned time series representations from the convolutional components are passed into individual attention modules to capture the inner dynamics of the series.\nGraph Neural Networks (GNN) have shown state-of-the-art performance in multivariate forecasting problems where a known graph structure is available such as traffic forecasting [16]. GNNs are preferred in settings such as traffic forecasting or financial time series where hierarchical or inter-series dependencies have significant influence [6]. In the context of traffic forecasting, spatial dependency can naturally be represented through graphs making GNN an excellent candidate for modeling, where road intersections are the nodes and road connections the edges [16]. However, many multivariate forecasting problems in the real world do not have a prior well-defined graph, which hinders the use of GNNs in these cases. To overcome this, Wu et al. [42] propose constructing a graph structure to enable the use of GNNs in problems where a well-defined graph does not exist. Consider the graph shown in Fig. 1. Here, the nodes\nrepresent series in a multivariate time series dataset where the edges connecting them represent the relationships among the series. In this way, any multivariate time series data can be represented as graphs by estimating the relationship among the series. The limitation of the graph construction method presented in [42] is that the complexity is high as the method involves training the model to learn the weights of two N \u00d7N matrices where N is the number of series. To the best of our knowledge, the work of Wu et al. [42] is the only neural network approach to constructing graph structures from multivariate time series data for time series forecasting application, which is the base of our research work.\nIn multivariate forecasting problems, it is important to model the intraseries temporal dependencies along with the inter-series spatial dependencies that exist within the series history. The following are the contributions of our work:\n\u2022 We introduce the Adaptive Dependency Learning Neural Network (ADLNN)\nthat has the ability to learn the dynamically changing dependencies among the series by bringing in inter-series causal semantics along with modeling the intra-series dependencies to improve the forecasting accuracy.\n\u2022 We introduce a well-principled approach for initialising the adjacency ma-\ntrix when a pre-defined graph structure is not provided.\n\u2022 We introduce the Dynamic Graph Construction Block that converts the\nstatic adjacency matrix into a dynamic matrix that helps model the changes in the relationship between time series over time.\n\u2022 We study and compare the performance in terms of accuracy and time\ncomplexity of our methods against the existing state-of-art models on real-world datasets.\nSome approaches in other areas that combine GNNs and statistical modeling have been proposed in the literature [48, 34, 17, 34, 33]. Our approach is to the best of our knowledge the first that combines Statistical modeling and neural network for graph construction in a multivariate time series setting.\nIn our proposed model, we initialise the adjacency matrix using various computationally efficient statistical methods which are then fed into a convolutional attention mechanism to dynamically weight the matrix in order to model the dynamic changes in dependency. We include alternating blocks of graph convolution and temporal convolution to model both the spatial and temporal patterns simultaneously. Furthermore, we combine a spatio-temporal convolutional attention forecasting model along with the GNN to improve the temporal modeling ability. Our code and models are made publicly available at https://github.com/AbishekSriramulu/ADLGNN.git."
        },
        {
            "heading": "2. Related Works",
            "text": ""
        },
        {
            "heading": "2.1. Multivariate time series forecasting",
            "text": "Some of the major challenges of multivariate time series forecasting are nonlinearity and complex dynamic inter-series and intra-series dependencies. The prominently used traditional methods for multivariate forecasting are vector autoregression (VAR) models and Gaussian Processes (GP). The VAR model is a statistical model that captures linear dependencies among multiple variables that change over time [50]. VAR models are linear autoregressive models, which are a general class of stochastic models used for forecasting time series which\ncan be transformed into stationary series [5]. In VAR, each series has an equation to model its evolution over time and this equation includes several lags of the series along with the lags of other series in the multivariate dataset. GPs are a type of Bayesian model used to model the multivariate distribution of inputs, so they can be easily used on multivariate data. However, these models tend to overfit when handling datasets with many variables. Moreover, they also involve high computational costs so they do not scale well. Many deep learning models have been proposed for the last few years to overcome these issues. Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN) have been widely applied in time series forecasting to capture complex nonlinear dependencies. Some noteworthy models are long short-term memory (LSTM) [12], LSTNet [21] and Temporal Convolution Network (TCN) [3]. However, these models fail to capture the inter-series relationships."
        },
        {
            "heading": "2.2. Graph Neural Networks",
            "text": "A graph is a data structure that defines dependencies between objects or entities. A graph is mathematically represented as G = (V,E) where V represents all nodes, and all edges are in the set E. The total number of nodes in the graph is denoted by n. Node v \u2208 V is said to be in the neighborhood of node u \u2208 V in a graph G if there exists an edge from node v to u or vice versa. A graph structure can be defined as an adjacency matrix. Adjacency matrices are mathematically denoted as A \u2208 Rn\u00d7n where A[i, j] > 0 if (vi, vj) \u2208 E and A[i, j] = 0 if (vi, vj) /\u2208 E [42].\nGraph neural networks have been successfully used in many machine learning applications involving graphs due to their ability to directly handle graph data. GNNs are based on a concept of information propagation, nodes in a graph exchange information among their neighbors [19]. The state of each node in a graph depends on the state of its neighbors. This ability of GNNs allows the model to capture spatial dependencies in a network [42]. Convolutional GNNs, graph autoencoders, recurrent GNNs, and spatio-temporal GNNs are the most widely used variants of GNN [41]. Spatio-temporal GNNs are more suitable for multivariate time series forecasting due to their ability to capture the temporal patterns in the spatial domain. In spatio-temporal GNNs, the graph convolution module models the spatial dependencies while 1D convolution layers model the temporal patterns in the data [44]. The graph structure that exists in the data needs to be explicitly represented in an adjacency matrix which can be fed into the graph convolutions as input. Dynamic graphs were used in [32], with external time-based features to change the traffic flow graph in a pre-specified way. To introduce this dynamicity in the graph Huang et al. [14] introduced a weighted matrix based on irregularities in the relationships between regions. Even though these methods can be effective in the specific applications they are developed for, they cannot easily be transferred to other applications. The use of an attention mechanism to introduce dynamicity allows the dynamic graph approach to be scaled to any application but at O(N2) complexity, where N is the number of nodes [1, 38, 23]."
        },
        {
            "heading": "2.3. Graph Structure Construction",
            "text": "Multivariate time series data are arguably best represented as graphs. An increased understanding of the structural characteristics of the data enables better analysis and prediction of such systems. However, most of the multivariate data observed in nature do not exist with an explicit graph structure. In such cases, network science concepts can be used to infer the underlying graph structure from the data. The most widely used graph structure construction methods are the Bayesian network structure learning methods such as Grow-Shrink (GS), Incremental, Association Markov Blanket (IAMB), Fast Incremental Association (Fast-IAMB), Interleaved Incremental Association (Inter-IAMB), Incremental Association with FDR Correction (IAMB-FDR), Max-Min Parents and Children (MMPC), Semi-Interleaved Hiton-PC (SI-HITON-PC) and Hybrid Parents and Children (HPC) [36]. However, these methods are not suitable for high dimensional data due to their high computation costs [11]. In [25], an approach involving the learning of affinity graph and feature fusion, for resulting in better clustering results was proposed. This method also assigns the affinity weights for data points on a per-data-pair basis to avoid the explicit specification of the size of the neighborhood. In [28], the similarity matrix is initialised according to the gaussian function, this matrix is updated with a network that performs semi-supervised joint feature selection. In [49], The initial adjacency matrix is either a similarity matrix or a pre-defined matrix which is then added to a sparse constraint in order to model the evolving node-to-node associations in a graph. Moreover, the complexity of these methods is observed to be at O(N2) where N is the number of nodes. In [32, 31], an incidence dynamic graph structure called a traffic flow probability graph is obtained from historical data by calculating the normalised probability score of travel from one station to another while modeling for traffic flow forecasting. In [9], an efficient dynamic GNN training framework that reduces the overhead by cross-layer optimizations across GNN and RNN operations is introduced. In [45], A multi-hop mechanism is used to capture node information from the neighborhood to update the embedding states over time. This approach can easily enable any static GNNs to handle\ndynamic graphs. However, this also increases the number of parameters of the model. Since in most real-life cases, we deal with large numbers of series and it is more practical to use cost-effective methods. Hence, in this paper we use the following simple concepts from information theory to infer dependencies among the series:\n\u2022 Correlation Matrix (CM): The adjacency matrix is constructed by\nusing the pairwise Pearson\u2019s correlation coefficient across the series.\n\u2022 Granger Causality (GC): The graph structure is inferred by the effect\nof one series over another. The effect of series Y2 on series Y1 can be mathematically formulated as log(e1/e12) where e1 is the error from an autoregressive model trained on data of Y1 and e12 is the error from an autoregressive model trained on data from Y1 and Y2.\n\u2022 Correlation Spanning Tree (CST): The graph is constructed by build-\ning a minimum spanning tree connecting all the series in the data using the distance matrix computed from Pearson\u2019s correlation matrix [30].\n\u2022 Graphical Lasso (GL): The graph structure is constructed using the\ninverse covariance matrix of the multivariate data estimated using the graphical lasso method proposed by Friedman et al. [8]\n\u2022 Maximum Likelihood Reconstruction (MLE): The graph structure\nis inferred using the coupling matrix obtained from the MLR method proposed by Zeng et al. [46].\n\u2022 Mutual Information (MI): The adjacency matrix is obtained by es-\ntimating the mutual information between the probability distributions of each pair of series in the data. Mutual information measures by how much knowledge of one variable reduces uncertainty about the other.\n\u2022 Transfer Entropy (TE): The adjacency matrix is obtained from the\npairwise transfer entropy estimation of each series with the others in the dataset. Transfer entropy is a measure of information contained about the\nfuture states of one series by knowing the past states of that series along with the other series [27]."
        },
        {
            "heading": "2.4. Attention Mechanism",
            "text": "Transformer models have become quite popular in recent years to solve many problems in computer vision (CV), natural language processing (NLP), and in time series forecasting. One of the most fundamental blocks of these kinds of models is the attention mechanism. Originally, it was developed to solve problems in Seq2seq model architectures used for neural machine translation, which encode input to a representation form, and pass it to a decoder to get predictions [39]. For learning larger sequences, one of the biggest problems with this kind of architecture is the first state of the decoder, which only takes the output of the last state of the encoder. This has a bottleneck problem as none of the previous encoder states are taken into consideration while decoder weights are learned. An attention mechanism can solve this problem by creating the context vector from all states of the encoder to pass it to the decoder. During context vector learning, the attention weights are learned and are multiplied with the output of each state of the encoder. These weights represent the contribution made by each state in generating a final representation to pass to the output.\nBahdanau et al. [2] first introduced the additive attention mechanism for neural machine translation which allowed the model to align towards segments of a sentence which are useful to detect the target word. Luong et al. [29] proposed a global multiplicative attention concept which takes all hidden states of the encoder into consideration to compute the representation of the context vector. Xu et al. [43] also proposed soft and hard attention mechanisms to focus on certain parts of the image for image caption generation. Soft attention calculates the weighted sum of each state of the encoder, while hard attention picks the encoder state having the highest weight to build the context vector. Vaswani et al. [40] revolutionized the NLP space by proposing scaled dot product attention and multi-head attention. They proposed an architecture that completely\nreplaces recurrence and convolution blocks. This network is highly parallelizable, so it can be trained faster [40]. Scaled dot product attention is comprised of the dot product of inputs, which are queries and keys. This dot product is multiplied with a values vector and the final output is passed through a softmax function. Parallel calculations of such attention functions form multi-head attention blocks where the output of each dot product attention is concatenated and passed through a linear transformation function to calculate the output.\nFan et al. [7] introduce a neural network model that feeds univariate time series into an attention mechanism to capture temporal patterns. This model generates predictions for multiple horizons at once based on the latent variables. Lim et al. [26] introduced a new variant of the transformer model called temporal fusion transformer which uses self-attention layers to capture long-term dependencies. These models build attention on hidden states from the inputs, so the attention weights\u2019 direction of correlation with the target is not taken into consideration. All these variants of attention are insensitive to the local context as they use a point-wise dot product self-attention. To overcome this, Li et al. [22] present convolutional self-attention where the queries and keys are\npassed through a causal convolution layer to incorporate the local context into the attention. Figure 2 shows the architecture of causal convolution, properly padded convolution of kernel size k with stride 1 is used to transform the inputs into queries and keys. In causal convolutions, the similarities are computed by their local shapes, instead of point-wise values to provide the local context."
        },
        {
            "heading": "3. Framework",
            "text": ""
        },
        {
            "heading": "3.1. Problem Formulation",
            "text": "Let Y \u2208 RT\u00d7N be a multivariate variable of dimension N for times 1 . . . T . Let Yt \u2208 RN be the values of Y at time t, and Y(i,t) \u2208 R be the value of variable i at time t. Thus, Y = {Y1, Y2, ..., YT }. Our goal is to predict the next values for the horizon of length H for the N variables. Hence, we predict the future values of sequence Y as Y\u0302 = {Y\u0302T+1, Y\u0302T+2, . . . , Y\u0302T+H}. Our goal is to create a function that maps from Y to Y\u0302 such that loss is minimized with L2 regularization."
        },
        {
            "heading": "3.2. Model Architecture",
            "text": "As shown in Figure 3, the model architecture consists of a dynamic graph construction block, several spatio-temporal blocks which contain a temporal convolution module and graph convolution module, a spatio-temporal convolutional attention and output module. The residual connections are included to avoid vanishing gradient problems and skip connections are used to avoid performance degradation due to the depth of the network."
        },
        {
            "heading": "3.3. Dynamic graph construction block",
            "text": "A multivariate time series is a complex system with inter-series dependencies that mostly change over time. To achieve a good performance, it is important to model these inter-series dynamics. A dynamic graph construction block takes in the multivariate time series as input and extracts the dependency graph that represents the inter-series dependencies at each time step. The output of this block is an adjacency matrix for each time step in the data, where the strength\nof relationships between series is specified with a value between 0 and 1 as mentioned in Section 2.2. The process of graph extraction is challenging for large and long datasets due to its complexity, so this block is carefully designed with well-principled techniques that require less computation. As shown in Figure 4,\nfirstly we extract adjacency matrices from the training data using the following methods: Correlation Matrix, Granger Causality, Correlation Spanning Tree, Graphical Lasso , Maximum Likelihood Estimation, Mutual Information, and Transfer Entropy (refer to Section 2.3 for details). Each of these methods outputs an adjacency matrix Ax and these matrices are normalised to have values in the range 0 to 1. The resultant matrices are then reduced to a single matrix by selecting the maximum value of each cell across all the matrices as shown in Equation 1 where S is a hyper-parameter that limits the neighborhood to the most influential neighbors. To relax the computation strain we only consider the latest 10% of the training data to compute this static graph structure. Moreover, this initialised structure will also be updated as we train the network over the entire training set.\nA(i, j) = maxx(Ax(i, j)), \u2200 i, j \u2208 {1, 2, ..., N}\nwhere Ax \u2208 {ACM , AGC , ACST , AGL, AMLE , AMI , ATE}\nA[i, k] = 0,\u2200k /\u2208 argtopS(A[i, :]), \u2200i \u2208 {1, 2, ..., N}\n(1)\nAs mentioned in Figure 4, a single adjacency matrix is obtained by selecting the maximum value of each cell across the matrices and then the topK neighbors are selected for each node to create a sparse matrix in order to reduce the complexity of the model. From this reduced sparse adjacency matrix we create a binary adjacency matrix where A[i, j] > 0 is set to 1.m nhkppjkpjkk,kj.\u201d In Vaswani et al. [40] and Lim et al. [26], the attention mechanism has been used to estimate the importance of a variable at a given time to predict the future state of a target variable. We use the same idea here to estimate the influence of one variable over the other at each time step.\nIn order to make this static graph into a dynamic one, we use sparse convolutional attention which takes in the binary adjacency matrix ABi along with the input time series at each time step as input to learn an N \u00d7K sparse dynamic weight matrix. These weights determine the strength of dependency of a series over the other. This weight is then summed with the static adjacency matrix obtained previously to construct a dynamic adjacency matrix. The static adjacency matrix provides a well-principled approach to reducing the complexity of the model from O(N2) to O(N \u2217K). Moreover, self-attention is used instead of multi-head attention to reduce the complexity of the model. The operation of convolutional attention in this block can be defined as in Equation 2.\nAttention(q, k, v) = softmax ( qkT\u221a dk \u00b7ABi ) v (2)\nHere, dk is the input time series that is transformed into query vector q, a key vector k, and a value vector v using causal convolutions, and ABi is the binary adjacency matrix that is used to mask the attention mechanism to reduce complexity."
        },
        {
            "heading": "3.4. Spatio-temporal convolutional attention",
            "text": "Figure 5 shows the shape of the input data that is fed into the model. In a spatio-temporal convolutional attention, convolutional attention transforms the\ninputs at each time step into queries and keys. We use causal convolutions to ensure that at any point in time the future states are not accessible. The input shape of the data ensures that the model attends to both the time and spatial domains simultaneously. Moreover, using a large kernel size would increase the local context awareness of the model and reduce the impact of anomalies or change points. As shown in Figure 2, the input dk is transformed into query vector q, a key vector k, and a value vector v of the same size. A score is estimated for each value in the input against each other. This learned attention score is then normalised using a softmax function, as in Equation 3.\nAttention(q, k, v) = softmax ( qkT\u221a dk ) v (3)\nEquations 2 and 3 are the same except for the masking. The spatio-temporal\nconvolutional attention does not have any masks. The attention score is a measure of the influence each value in the input has on the other. This score is then fed into skip connections which are a standard convolution layer of shape 1 \u00d7 LW where LW is the length of input windows. The skip connection standardises the length of intermediate layer outputs that are fed into the output module."
        },
        {
            "heading": "3.5. Graph Convolution Module",
            "text": "The graph convolution module that we utilise in this model is based on\nthe work of Wu et al. [42]. This module handles two main operations - information propagation and information selection. The adjacency matrix is fed into this module as input and the graph convolutions use the spatial information obtained from the adjacency matrix to propagate the information over the graph structure and select the information. This information flow is handled by two mix-hop propagation modules as shown in Figure 6. When using many graph convolution layers, the propagation of information happens recursively and this causes the node\u2019s hidden states to converge to a single point. Studies have recorded that when using many layers, the hidden states of nodes reach a random walk distribution [18]. To overcome this, we retain a fraction of the node\u2019s initial states. The information propagation operation in the mix-hop propagation module is defined as shown in Equations 4 and 5.\nHk = \u03b2Hin + (1\u2212 \u03b2)A\u0302Hk\u22121 (4)\nHout = \u03b2HkWk (5)\nHere, \u03b2 is a hyper-parameter that controls how much of the initial state to retain, k is the depth of propagation, Hin represents the initial hidden states, and Hout represents the output hidden states. Equation 4 represents how the information propagates horizontally across nodes and Equation 5 shows how the MLP attaches a learnable parameter matrix to the current hidden state that selects the important information and disregards noise."
        },
        {
            "heading": "3.6. Temporal convolution module",
            "text": "In order to capture the intra-series temporal patterns, we employ a temporal convolution module. we choose temporal convolutions over LSTM or RNN architectures since they do not backpropagate over the temporal dimension of the sequence. This avoids exploding or vanishing gradient problems. Moreover, temporal convolutions have proven to perform well on longer sequences [13]. Since temporal convolutions use causal convolution, information leakage is prevented as a state can only access the previous states and not the future. In our architecture, the temporal convolution module consists of multiple causal\nconvolution filters of different sizes with dilation. This allows the module to extract patterns at various ranges and improves the ability to model long sequences. This removes the need to choose the right kernel size for convolutional networks. We use filter sizes of 1 \u00d7 2, 1 \u00d7 3, 1 \u00d7 6, and 1 \u00d7 7, to capture both short-term and long-term patterns. The outputs of these four convolution filters are first truncated to the length of the largest filter and concatenated over the channel dimension. For z \u2208 RT as a 1D input sequence, and 4 causal convolutional filters of sizes f1\u00d72, f1\u00d73, f1\u00d76, and f1\u00d77, the output of the temporal convolution module is given as shown in Equation 6, where z \u22c6 f1\u00d7k denotes the dilated convolution, defined in Equation 7, and d denotes the factor for dilation.\nz = concat(z \u22c6 f1\u00d72, z \u22c6 f1\u00d73, z \u22c6 f1\u00d76, z \u22c6 f1\u00d77) (6)\nz \u22c6 f1\u00d7k(t) = k\u22121\u2211 s=0 f1\u00d7k(s)z(t\u2212 d\u00d7 s) (7)"
        },
        {
            "heading": "3.7. Output Module",
            "text": "The output module is used to transform the channel dimension of the current state to output dimensions. To achieve this, we use two convolution layers of size 1\u00d7 1."
        },
        {
            "heading": "4. Experiments and results",
            "text": ""
        },
        {
            "heading": "4.1. Datasets and performance metrics",
            "text": "We use three benchmark datasets for all the experiments to evaluate the performance of the model against the state-of-the-art and baseline models. These benchmark datasets were used by Lai et al. [21] and Wu et al. [42] which allows us to make straightforward comparisons with those works. Information about dataset statistics is provided in Table 2.\nSolar energy dataset. This dataset comprises of solar energy production data from a solar power plant collected in 2006. These data were logged every 10 minutes from different power plants situated in the state of Alabama, USA [21].\nElectricity dataset. This dataset consists of electricity consumption records measured in kWh from 2012 to 2014 for 321 clients. We use the hourly data converted from the original data which was logged every 15 minutes [21].\nTraffic dataset. This dataset is a collection of 48 months of data from the Department of Transportation of California collected in the period from 2015 to 2016 on an hourly basis. The dataset contains the log of the road occupancy rates, which is estimated by multiple sensors on freeways located in the San Francisco Bay area.\nFor performance evaluation, following Wu et al. [42], we use the following\nevaluation metrics.\nRoot Relative Squared Error (RSE): This metric is a modified scaled variant of the popular Root Mean Squared Error (RMSE), which is developed to ensure a meaningful evaluation, irrespective of the size of the data. As per Lai et al. [21], it is defined as in Equation 8.\nRSE =\n\u221a\u2211 (i,t)\u2208\u2126Test (Y(i,t) \u2212 Y\u0302(i,t))\n2\u221a\u2211 (i,t)\u2208\u2126Test (Y(i,t) \u2212mean(Y )) 2 (8)\nCorrelation Coefficient (CORR): It is an estimate of the intensity of the relation between the relative variations between two variables. As per Lai et al. [21], the formula of CORR is given as in Equation 9.\nCORR = 1\nn n\u2211 i=1 \u2211 t (Y(i,t) \u2212mean(Y(i,)))(Y\u0302(i,t) \u2212mean(Y\u0302(i,)))\u221a\u2211 t (Y(i,t) \u2212mean(Y(i,))) 2 (Y\u0302(i,t) \u2212mean(Y\u0302(i,))) 2 (9)\nHere, Y, Y\u0302 \u2208 Rn\u00d7T , where Y denotes ground truth values and Y\u0302 denotes the values predicted by the model. For performance, lower values of RSE are\nbetter, while for CORR, higher values are better. While these measures may not be generally applicable in a forecasting context due to non-stationarities (e.g., if there is a strong trend in the series, calculating a mean is essentially meaningless), there doesn\u2019t exist a general consensus about how to evaluate forecasts, and these measures are applicable to our datasets and make our results comparable to results published in the literature [20]."
        },
        {
            "heading": "4.2. Experimental Setup",
            "text": "For consistency in evaluation, we follow the literature of the state-of-the-art models we compare against. The datasets are divided into training set (60%), validation set (20%), and test set (20%) in a sequential manner [21].\nThree major techniques are used for model training:\n\u2022 Increase batch size on the plateau: The model takes in the two\nhyper-parameters initial batch size and maximum batch size. The dataset\nis batched at the initial batch size when the training starts and every time a plateau in validation loss is observed over 3 epochs, the batch size is increased by a factor of 2 until it reaches the maximum batch size.\n\u2022 Reduce learning rate on the plateau: This function is active only\nafter the maximum batch size is reached. The learning rate is reduced by a factor of 0.75 every time the validation loss plateaus for over 3 epochs.\n\u2022 Early stopping: The training stops when there is no improvement ob-\nserved in the validation metrics for over 10 epochs.\nThe hyper-parameters are selected following Lai et al. [21] except for the learning rate which is initially set to a larger value of 0.003, due to the use of larger batch sizes and dynamically adjusted by monitoring the validation loss. The initial batch size is set to be 4 and the maximum batch size is set to 32 except for the traffic dataset which uses a maximum batch size of 16 (we use a low maximum batch size due to size of the dataset and the associated hardware resource limitations). We use L2 regularization with a penalty of 1\u00d7 10\u22124 and dropout with 0.3 to prevent overfitting. The depth k of the graph convolution module is set to 2, the \u03b2 hyper-parameter is set to 0.05, S is set to 20, and the number of spatio-temporal blocks is set to 5 following Wu et al. [42]. The kernel size for convolutional attention is set to 6. The input window size is set to 168 following Lai et al. [21] and Wu et al. [42]."
        },
        {
            "heading": "4.3. Comparison with State-Of-the-Art Methods",
            "text": "We perform evaluations for our proposed model Adaptive Dependency Learning Neural Network (ADLNN), two static graph variants of our proposed model, namely Static Dependency Learning Neural Network (SDLNN) and SDLNNCorr, the state-of-the-art multivariate forecasting models LSTNet [21], TPALSTM [37], MTGNN [42], HyDCNN [24], and the baseline models AR and VAR-MLP. These models are outlined as follows:\n\u2022 AR: An auto-regressive linear model.\n\u2022 VAR-MLP: A composite model of a linear VAR and an MLP [47].\n\u2022 GP: A Gaussian Process Model [35].\n\u2022 RNN-GRU: An RNN model with gated recurrent units (GRU).\n\u2022 LSTNet: A neural network model with temporal convolution layers in\nrecurrent fashion with skip connections [21].\n\u2022 TPA-LSTM: An RNN model that uses temporal attention [37].\n\u2022 MTGNN: This GNN model constructs the graph from data. To the best\nof our knowledge, this is the only work in GNN for multivariate forecasting that constructs graphs from the data [42].\n\u2022 HyDCNN: This model uses a position-aware fully dilated CNNs to model\nthe non-linear patterns followed by an autoregressive model which models the sequential linear dependencies [24].\n\u2022 SDLGNN-Corr: A variant of our proposed model, which uses pairwise\nPearson\u2019s correlation to obtain the adjacency matrix which is fed to the graph convolutions as input.\n\u2022 SDLGNN: A static variant of our proposed model, where we use the out-\nput from a static graph structure extraction module mentioned in Figure 4 as adjacency matrix which is fed to the graph convolutions as input.\n\u2022 ADLGNN: Our proposed model."
        },
        {
            "heading": "4.4. Additional experiments",
            "text": "We perform some additional experiments on a much larger electricity load dataset [15] recorded at 1494 nodes across Europe with a series length of 26304. This experiment compares our method against MTGNN as the source code for HyDCNN is not available publicly. We also record the runtime for these experiments to compare the time complexity of both these methods. The experiments use the same hyperparameters listed in section 4.2. The Experiments were all conducted on a machine with 20vCPUs, 200GiB RAM, 512TiB SSD, and 1 \u00d7 A100 (40GB SXM4) GPU.\ncreasing the RSE by 13.383%."
        },
        {
            "heading": "4.5. Results",
            "text": "In figure 7, the critical difference diagram compares the accuracy of the models considered in our experiments. The lower the rank, the better the model is at producing more accurate forecasts. The horizontal lines in the diagram connecting the models indicate that the connected models are not significantly different from each other. Figure 7, shows that there exists a significant difference in accuracy recorded in our experiments between the ADLGNN and the other models.\nTable 3 shows the experimental results of the proposed models along with other existing methods. From the results, we can observe that our proposed models SDLGNN and ADLGNN have outperformed all other models.\nTable 4 shows the percentage decrease in error metric RSE that the proposed models achieved over the best among the baseline and benchmark models. For all the tasks, our proposed models surpassed the performance of benchmark methods. The best of all the proposed model variants is ADLGNN which has recorded the lowest RSE across all tasks. From Table 6, we can see that introducing statistically initialised dynamic graphs in ADLGNN improved forecasting accuracy by 4.9% over MTGNN which is an existing state-of-the-art GNN\nmodel that similarly uses a trainable graph.\nIt can be observed that we have recorded maximum improvement over the traffic & solar datasets in relation to the other. The nature of these datasets makes dependency learning more important. The time series in the traffic dataset are more complex and have strong inter-series dependencies. Moreover, the performance of the MTGNN model seems to be degrading with an increase in the number of variables, which ADLGNN and SDLGNN are able to overcome."
        },
        {
            "heading": "5. Conclusions",
            "text": "We have proposed a hybrid approach for combining neural networks and statistical structure learning models to learn the dependencies across multivariate data and construct a dynamically changing dependency graph. Therewith, we are able to enable the use of GNNs even where no pre-defined graph is available. The statistical structure modeling together with neural networks provides an efficient approach as it brings causal semantics to determine the dependencies across the series. We have been able to demonstrate significantly improved performance on several real-world datasets, outperforming all the competitor methods."
        },
        {
            "heading": "Acknowledgment",
            "text": "This research was supported by the Australian Research Council under grant DE190100045, a Facebook Statistics for Improving Insights and Decisions research award, Monash University Graduate Research funding, and the MASSIVE - High performance computing facility, Australia."
        }
    ],
    "title": "Adaptive Dependency Learning Graph Neural Networks",
    "year": 2023
}