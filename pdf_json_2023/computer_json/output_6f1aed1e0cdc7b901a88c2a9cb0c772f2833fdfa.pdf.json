{
    "abstractText": "Making personalized recommendation for cold-start users, who only have a few interaction histories, is a challenging problem in recommendation systems. Recent works leverage hypernetworks to directly map user interaction histories to user-specific parameters, which are then used to modulate predictor by feature-wise linear modulation function. These works obtain the state-of-the-art performance. However, the physical meaning of scaling and shifting in recommendation data is unclear. Instead of using a fixedmodulation function and deciding modulation position by expertise, we propose a modulation framework called ColdNAS for user cold-start problem, where we look for proper modulation structure, including function and position, via neural architecture search. We design a search space which covers broad models and theoretically prove that this search space can be transformed to a much smaller space, enabling an efficient and robust one-shot search algorithm. Extensive experimental results on benchmark datasets show that ColdNAS consistently performs the best. We observe that different modulation functions lead to the best performance on different datasets, which validates the necessity of designing a searching-based method. Codes are available at https://github.com/LARS-research/ColdNAS.",
    "authors": [
        {
            "affiliations": [],
            "name": "Shiguang Wu"
        },
        {
            "affiliations": [],
            "name": "Yaqing Wang"
        },
        {
            "affiliations": [],
            "name": "Qinghe Jing"
        },
        {
            "affiliations": [],
            "name": "Daxiang Dong"
        },
        {
            "affiliations": [],
            "name": "Dejing Dou"
        },
        {
            "affiliations": [],
            "name": "Quanming Yao"
        }
    ],
    "id": "SP:21d15e1658bd99384ab48ca1874717cf0da800f0",
    "references": [
        {
            "authors": [
                "Bowen Baker",
                "Otkrist Gupta",
                "Nikhil Naik",
                "Ramesh Raskar"
            ],
            "title": "Designing neural network architectures using reinforcement learning",
            "venue": "In International Conference on Learning Representations",
            "year": 2017
        },
        {
            "authors": [
                "James Bergstra",
                "Yoshua Bengio"
            ],
            "title": "Random search for hyper-parameter optimization",
            "venue": "Journal of Machine Learning Research 13,",
            "year": 2012
        },
        {
            "authors": [
                "Marc Brockschmidt"
            ],
            "title": "GNN-FiLM: Graph neural networks with feature-wise linear modulation",
            "venue": "In International Conference on Machine Learning",
            "year": 2020
        },
        {
            "authors": [
                "Heng-Tze Cheng",
                "Levent Koc",
                "Jeremiah Harmsen",
                "Tal Shaked",
                "Tushar Chandra",
                "Hrishi Aradhye",
                "Glen Anderson",
                "Greg Corrado",
                "Wei Chai",
                "Mustafa Ispir"
            ],
            "title": "Wide & deep learning for recommender systems",
            "venue": "In Workshop on Deep Learning for Recommender Systems",
            "year": 2016
        },
        {
            "authors": [
                "Manqing Dong",
                "Feng Yuan",
                "Lina Yao",
                "Xiwei Xu",
                "Liming Zhu"
            ],
            "title": "MAMO: Memory-augmented meta-optimization for cold-start recommendation",
            "venue": "In ACM SIGKDD Conference on Knowledge Discovery and Data",
            "year": 2020
        },
        {
            "authors": [
                "Xidong Feng",
                "Chen Chen",
                "Dong Li",
                "Mengchen Zhao",
                "Jianye Hao",
                "Jun Wang"
            ],
            "title": "CMML: Contextual modulation meta learning for cold-start recommendation",
            "venue": "In ACM International Conference on Information and Knowledge Management",
            "year": 2021
        },
        {
            "authors": [
                "Chelsea Finn",
                "Pieter Abbeel",
                "Sergey Levine"
            ],
            "title": "Model-agnostic metalearning for fast adaptation of deep networks",
            "venue": "In International Conference on Machine Learning",
            "year": 2017
        },
        {
            "authors": [
                "Chen Gao",
                "Yinfeng Li",
                "Quanming Yao",
                "Depeng Jin",
                "Yong Li"
            ],
            "title": "Progressive feature interaction search for deep sparse network",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Chen Gao",
                "Quanming Yao",
                "Depeng Jin",
                "Yong Li"
            ],
            "title": "Efficient Data-specific Model Search for Collaborative Filtering",
            "venue": "In ACM SIGKDD Conference on Knowledge Discovery and Data",
            "year": 2021
        },
        {
            "authors": [
                "F Maxwell Harper",
                "Joseph A Konstan"
            ],
            "title": "The MovieLens datasets: History and context",
            "venue": "ACM Transactions on Interactive Intelligent Systems",
            "year": 2015
        },
        {
            "authors": [
                "Xiangnan He",
                "Lizi Liao",
                "Hanwang Zhang",
                "Liqiang Nie",
                "Xia Hu",
                "Tat-Seng Chua"
            ],
            "title": "Neural collaborative filtering",
            "venue": "In International Conference on World Wide Web",
            "year": 2017
        },
        {
            "authors": [
                "Kurt Hornik",
                "Maxwell Stinchcombe",
                "Halbert White"
            ],
            "title": "Multilayer feedforward networks are universal approximators",
            "venue": "Neural Networks 2,",
            "year": 1989
        },
        {
            "authors": [
                "Cheng-Kang Hsieh",
                "Longqi Yang",
                "Yin Cui",
                "Tsung-Yi Lin",
                "Serge Belongie",
                "Deborah Estrin"
            ],
            "title": "Collaborative metric learning",
            "venue": "In International Conference on World Wide Web",
            "year": 2017
        },
        {
            "authors": [
                "Frank Hutter",
                "Lars Kotthoff",
                "Joaquin Vanschoren"
            ],
            "title": "Automated machine learning: methods, systems, challenges",
            "year": 2019
        },
        {
            "authors": [
                "Haifeng Jin",
                "Qingquan Song",
                "Xia Hu"
            ],
            "title": "Auto-Keras: An efficient neural architecture search system",
            "venue": "In ACM SIGKDD Conference on Knowledge Discovery and Data",
            "year": 2019
        },
        {
            "authors": [
                "Yehuda Koren"
            ],
            "title": "Factorization meets the neighborhood: A multifaceted collaborative filtering model",
            "venue": "In ACM SIGKDD Conference on Knowledge Discovery and Data",
            "year": 2008
        },
        {
            "authors": [
                "Hoyeop Lee",
                "Jinbae Im",
                "Seongwon Jang",
                "Hyunsouk Cho",
                "Sehee Chung"
            ],
            "title": "MeLU: Meta-learned user preference estimator for cold-start recommendation",
            "venue": "In ACM SIGKDD Conference on Knowledge Discovery and Data",
            "year": 2019
        },
        {
            "authors": [
                "Jovian Lin",
                "Kazunari Sugiyama",
                "Min-Yen Kan",
                "Tat-Seng Chua"
            ],
            "title": "Addressing cold-start in app recommendation: latent user models constructed from twitter followers",
            "venue": "In International ACM SIGIR Conference on Research and Development in Information Retrieval",
            "year": 2013
        },
        {
            "authors": [
                "Xixun Lin",
                "Jia Wu",
                "Chuan Zhou",
                "Shirui Pan",
                "Yanan Cao",
                "Bin Wang"
            ],
            "title": "Task-adaptive neural process for user cold-start recommendation",
            "venue": "In The Web Conference",
            "year": 2021
        },
        {
            "authors": [
                "Bin Liu",
                "Chenxu Zhu",
                "Guilin Li",
                "Weinan Zhang",
                "Jincai Lai",
                "Ruiming Tang",
                "Xiuqiang He",
                "Zhenguo Li",
                "Yong Yu"
            ],
            "title": "AutoFIS: Automatic feature interaction selection in factorization models for click-through rate prediction",
            "venue": "In ACM SIGKDD Conference on Knowledge Discovery and Data Mining",
            "year": 2020
        },
        {
            "authors": [
                "Hanxiao Liu",
                "Karen Simonyan",
                "Yiming Yang"
            ],
            "title": "DARTS: Differentiable architecture search",
            "venue": "In International Conference on Learning Representations",
            "year": 2019
        },
        {
            "authors": [
                "Yuanfu Lu",
                "Yuan Fang",
                "Chuan Shi"
            ],
            "title": "Meta-learning on heterogeneous information networks for cold-start recommendation",
            "venue": "InACMSIGKDDConference on Knowledge Discovery and Data",
            "year": 2020
        },
        {
            "authors": [
                "Renqian Luo",
                "Fei Tian",
                "Tao Qin",
                "Enhong Chen",
                "Tie-Yan Liu"
            ],
            "title": "Neural architecture optimization",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Haoyu Pang",
                "Fausto Giunchiglia",
                "Ximing Li",
                "Renchu Guan",
                "Xiaoyue Feng"
            ],
            "title": "PNMTA: A pretrained network modulation and task adaptation approach for user cold-start recommendation",
            "venue": "In The Web Conference",
            "year": 2022
        },
        {
            "authors": [
                "Yoon-Joo Park",
                "Alexander Tuzhilin"
            ],
            "title": "The long tail of recommender systems and how to leverage it",
            "venue": "In ACM Conference on Recommender Systems",
            "year": 2008
        },
        {
            "authors": [
                "Ethan Perez",
                "Florian Strub",
                "Harm De Vries",
                "Vincent Dumoulin",
                "Aaron Courville"
            ],
            "title": "FiLM: Visual reasoning with a general conditioning layer",
            "venue": "In AAAI Conference on Artificial Intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "Hieu Pham",
                "Melody Guan",
                "Barret Zoph",
                "Quoc Le",
                "Jeff Dean"
            ],
            "title": "Efficient neural architecture search via parameters sharing",
            "venue": "In International Conference on Machine Learning",
            "year": 2018
        },
        {
            "authors": [
                "Esteban Real",
                "Alok Aggarwal",
                "Yanping Huang",
                "Quoc V Le"
            ],
            "title": "Regularized evolution for image classifier architecture search",
            "venue": "In AAAI Conference on Artificial Intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "James Requeima",
                "Jonathan Gordon",
                "John Bronskill",
                "Sebastian Nowozin",
                "Richard E Turner"
            ],
            "title": "Fast and flexible multi-task classification using conditional neural adaptive processes",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Andrew I Schein",
                "Alexandrin Popescul",
                "Lyle H Ungar",
                "David M Pennock"
            ],
            "title": "Methods and metrics for cold-start recommendations",
            "venue": "In International ACM SIGIR Conference on Research and Development in Information Retrieval",
            "year": 2002
        },
        {
            "authors": [
                "Suvash Sedhain",
                "Aditya Krishna Menon",
                "Scott Sanner",
                "Lexing Xie"
            ],
            "title": "AutoRec: Autoencoders meet collaborative filtering",
            "venue": "In International Conference on World Wide Web",
            "year": 2015
        },
        {
            "authors": [
                "Qingquan Song",
                "Dehua Cheng",
                "Hanning Zhou",
                "Jiyan Yang",
                "Yuandong Tian",
                "Xia Hu"
            ],
            "title": "Towards automated neural interaction discovery for click-through rate prediction",
            "venue": "In ACM SIGKDD Conference on Knowledge Discovery and Data Mining",
            "year": 2020
        },
        {
            "authors": [
                "Maksims Volkovs",
                "Guangwei Yu",
                "Tomi Poutanen"
            ],
            "title": "DropoutNet: Addressing cold start in recommender systems",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Li Wang",
                "Binbin Jin",
                "Zhenya Huang",
                "Hongke Zhao",
                "Defu Lian",
                "Qi Liu",
                "Enhong Chen"
            ],
            "title": "Preference-adaptive meta-learning for cold-start recommendation",
            "venue": "In International Joint Conference on Artificial Intelligence",
            "year": 2021
        },
        {
            "authors": [
                "YaqingWang",
                "Quanming Yao",
                "James T Kwok",
                "Lionel M Ni"
            ],
            "title": "Generalizing from a few examples: A survey on few-shot learning",
            "venue": "Comput. Surveys 53,",
            "year": 2020
        },
        {
            "authors": [
                "Yuexiang Xie",
                "ZhenWang",
                "Yaliang Li",
                "Bolin Ding",
                "Nezihe Merve G\u00fcrel",
                "Ce Zhang",
                "Minlie Huang",
                "Wei Lin",
                "Jingren Zhou"
            ],
            "title": "FIVES: Feature interaction via edge search for large-scale tabular data",
            "venue": "InACM SIGKDDConference on Knowledge Discovery and Data Mining",
            "year": 2021
        },
        {
            "authors": [
                "Yibo Yang",
                "Hongyang Li",
                "Shan You",
                "Fei Wang",
                "Chen Qian",
                "Zhouchen Lin"
            ],
            "title": "ISTA-NAS: Efficient and consistent neural architecture search by sparse coding",
            "venue": "In Cross attention network for few-shot classification,",
            "year": 2020
        },
        {
            "authors": [
                "Quanming Yao",
                "Xiangning Chen",
                "James T Kwok",
                "Yong Li",
                "Cho-Jui Hsieh"
            ],
            "title": "Efficient neural interaction function search for collaborative filtering",
            "venue": "In The Web Conference",
            "year": 2020
        },
        {
            "authors": [
                "Quanming Yao",
                "Ju Xu",
                "Wei-Wei Tu",
                "Zhanxing Zhu"
            ],
            "title": "Efficient neural architecture search via proximal iterations",
            "venue": "In AAAI Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Kaicheng Yu",
                "Christian Sciuto",
                "Martin Jaggi",
                "Claudiu Musat",
                "Mathieu Salzmann"
            ],
            "title": "Evaluating the search phase of neural architecture search",
            "venue": "In International Conference on Learning Representations",
            "year": 2019
        },
        {
            "authors": [
                "Runsheng Yu",
                "Yu Gong",
                "Xu He",
                "Bo An",
                "Yu Zhu",
                "Qingwen Liu",
                "Wenwu Ou"
            ],
            "title": "Personalized adaptive meta learning for cold-start user preference prediction",
            "venue": "In AAAI Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Yongqi Zhang",
                "Quanming Yao",
                "James T Kwok"
            ],
            "title": "Bilinear scoring function search for knowledge graph learning",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 45,",
            "year": 2023
        },
        {
            "authors": [
                "Xiangyu Zhao",
                "Haochen Liu",
                "Wenqi Fan",
                "Hui Liu",
                "Jiliang Tang",
                "Chong Wang"
            ],
            "title": "AutoLoss: Automated loss function search in recommendations",
            "venue": "In ACM SIGKDD Conference on Knowledge Discovery and Data",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "CCS CONCEPTS \u2022 Information systems\u2192 Recommender systems; \u2022 Computing methodologies\u2192 Supervised learning; Neural networks.\nKEYWORDS User-Cold Start Recommendation, Neural Architecture Search, FewShot Learning, Meta-Learning, Hypernetworks\n\u2217Corresponding author.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. WWW \u201923, May 1\u20135, 2023, Austin, TX, USA \u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-9416-1/23/04. . . $15.00 https://doi.org/10.1145/3543507.3583344\nACM Reference Format: Shiguang Wu, Yaqing Wang, Qinghe Jing, Daxiang Dong, Dejing Dou, and Quanming Yao. 2023. ColdNAS: Search to Modulate for User ColdStart Recommendation. In Proceedings of the ACM Web Conference 2023 (WWW \u201923), May 1\u20135, 2023, Austin, TX, USA. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3543507.3583344"
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Recommendation systems (RSs) [32] target at providing suggestions of items that are most pertinent to a particular user, such as movie recommendation [12] and book recommendation [47]. Nowadays, RSs are abundant online, offering enormous users convenient ways to shopping regardless of location and time, and also providing intimate suggestions according to their preferences. However, user cold-start recommendation [33] remains a severe problem in RSs. On the one hand, the users in RSs follow long tail effect [27], some users just have a few interaction histories. On the other hand, new users are continuously emerging, who naturally have rated a few items in RSs. Such a problem is even more challenging as modern RSs are mostly built with over-parameterized deep networks, which needs a huge amount of training samples to get good performance and can easily overfit for cold-start users [36].\nUser cold-start recommendation problem can naturally be modeled as a few-shot learning problem [38], which targets at quickly generalize to new tasks (i.e. personalized recommendation for coldstart users) with a few training samples (i.e. a few interaction histories). A number of works [6, 19, 24, 37, 44] adopt the classic gradient-based meta-learning strategy called model-agnostic metalearning (MAML) [8], which learns a good initialized parameter from a set of tasks and adapts it to a new task by taking a few steps of gradient descent updates on a limited number of labeled samples. This line of models has demonstrated high potential of alleviating user cold-start problem. However, gradient-based meta-learning strategy require expertise to tune the optimization procedure to avoid over-fitting. Besides, the inference time can be long.\nInstead of adapting to each user by fine-tuning via gradient descent, another line of works uses hypernetworks [11] to directly map user interaction history to user-specific parameters [6, 7, 21, 26]. These modulation-based methods consist of embedding layer,\nar X\niv :2\n30 6.\n03 38\n7v 1\n[ cs\n.A I]\n6 J\nun 2\nadaptation network and predictor. The adaptation network generates user-specific parameters, which are used to modulate the predictor in the form of a modulation function. Particularly, they all adopt feature-wise linear modulation function (FiLM) [28], which modulates the representation via scaling and shifting based on the conditioning information, to modulate the user cold-start models to obtain user-specific representation. Although FiLM has been proved to be highly effective in on images [31] and graphs such molecules and protein-protein interaction graphs [4], applying scaling and shifting on user interaction history has rather obscure physical meaning. Moreover, choosing where to modulate is hard to decide. Existing works modulate different parts of the model, such as last layers of the decoder [21] and most layers in the model [26]. How to modulate well for different users, and how to choose the right functions at the right positions to modulate, are still open questions.\nIn this paper, we propose ColdNAS to find appropriate modulation structure for user cold-start problem by neural architecture search (NAS). Although NAS methods have been applied in RSs [9, 39], the design of NAS methods are problem-specific. For user cold-start problem, it is still unknown how to (i) design a search space that can cover effective cold-start models with good performance for various datasets, and (ii) design an efficient and robust search algorithm. To solve the above challenges, we design a search space of modulation structure, which can cover not only existing modulation-based user cold-start models, but also contain more flexible structures. We theoretically prove that the proposed search space can be transformed to an equivalent space, where we search efficiently and robustly by differentiable architecture search. Our main contributions are summarized as follows:\n\u2022 We propose ColdNAS, a modulation framework for user coldstart problem. We use a hypernetwork to map each user\u2019s history interactions to user-specific parameters which are then used to modulate the predictor, and formulate how to modulate and where to modulate as a NAS problem. \u2022 We design a search space of modulation structure, which can cover not only existing modulation-based user cold-start models, but also contain more expressive structures. As this search space can be large to search, we conduct search space transformation to transform the original space to an equivalent but much smaller space. Theoretical analysis is provided to validate its correctness. Upon the transformed space, we then can search efficiently and robustly by differentiable architecture search algorithm. \u2022 We perform extensive experiments on benchmark datasets for user cold-start problem, and observe that ColdNAS consistently obtains the state-of-the-art performance. We also validate the design consideration of search spaces and algorithms, demonstrating the strength and reasonableness of ColdNAS."
        },
        {
            "heading": "2 RELATEDWORKS",
            "text": ""
        },
        {
            "heading": "2.1 User Cold-Start Recommendation",
            "text": "Making personalized recommendation for cold-start users is particular challenging, as these users only have a few interaction histories [33]. In the past, collaborative filtering (CF)-based [13, 18, 34] methods, which make predictions by capturing interactions among users and items to represent them in low-dimensional space, obtain leading performance in RSs. However, these CF-based methods make inferences only based on the user\u2019s history. They cannot handle user cold-start problem. To alleviate the user cold-start problem, content-based methods leverage user/item features [33] or even user social relations [20] to help to predict for cold-start users. The recent deep model DropoutNet [36] trains a neural network with dropout mechanism applied on input samples and infers themissing data. However, it is hard to generalize these content-based methods to new users, which usually requires model retraining.\nA recent trend is to model user cold-start problem as a few-shot learning problem [38]. The resultant models learn the ability to quickly generalize to recommend for new users with a few interaction histories. Most works mainly follow the classic gradient-based meta-learning strategy MAML [8], which first learns a good initialized parameter from training tasks, then locally update the parameter on the provided interaction history by gradient descent. In particular, existing works consider different directions to improve the performance: MeLU [19] selectively adapts model parameters to the new task in the local update stage, MAMO [6] introduces external memory to guide the model to adapt, MetaHIN [24] uses heterogeneous information networks to leverage the rich semantics between users and items, REG-PAML [44] proposes to use userspecific learning rate during local update, and PAML [37] leverages social relations to share information among similar users. While these approaches can adapt models to training data, they are computationally inefficient at test-time, and usually require expertise to tune the optimization procedure to avoid over-fitting."
        },
        {
            "heading": "2.2 Neural Architecture Search",
            "text": "Neural architecture search (NAS) targets at finding an architecture with good performance without human tuning [16]. Recently, NAS methods have been applied in RSs. SIF [41] searches for interaction function in collaborative filtering, AutoCF [10] further searches for basic components including input encoding, embedding function, interaction function, and prediction function in collaborative filtering. AutoFIS [22], AutoCTR [35] and FIVES [39] search for effective feature interaction in click-through rate prediction. AutoLoss [46] searches for loss function in RSs. Due to different problem settings, search spaces needs to problem-specific and cannot be shared or transferred. Therefore, none of these works can be applied for user cold-start recommendation problem. To search efficiently on the search space, one can choose reinforcement learning methods [1], evolutionary algorithms [30], and one-shot differentiable architecture search algorithms [23, 29, 42]. Among them, one-shot differentiable architecture search algorithms have demonstrated higher efficiency. Instead of training and evaluating different models like classical methods, they optimize only one supernet where themodel parameters are shared across the search space and co-adapted."
        },
        {
            "heading": "3 PROPOSED METHOD",
            "text": "In this section, we present the details of ColdNAS, whose overall architecture is shown in Figure 1. In the sequel, we first provide the formal problem formulation of user cold-start problem (Section 3.1). Then, we present our search space and theoretically show how to transform it (Section 3.2). Finally, we introduce the search algorithm to search for a good user cold-start model (Section 3.3)."
        },
        {
            "heading": "3.1 Problem Formulation",
            "text": "Let user set be denotedU = {\ud835\udc62\ud835\udc56 }, where each user \ud835\udc62\ud835\udc56 is associated with user features. The feature space is shared across all users. Let item set be denotedV = {\ud835\udc63 \ud835\udc57 } where each item \ud835\udc63 \ud835\udc57 is also associated with item features. When a user \ud835\udc62\ud835\udc56 rates an item \ud835\udc63 \ud835\udc57 , the rating is denoted \ud835\udc66\ud835\udc56, \ud835\udc57 . In user cold-start recommendation problem, the focus is to make personalized recommendation for user \ud835\udc62\ud835\udc56 who only has rated a few items.\nFollowing recent works [3, 21, 24], we model the user cold-start recommendation problem as a few-shot learning problem. The target is to learn a model from a set of training user cold-start tasks T train and generalize to provide personalized recommendation for new tasks. Each task\ud835\udc47\ud835\udc56 corresponds to a user \ud835\udc62\ud835\udc56 , with a support set S\ud835\udc56 = {(\ud835\udc63 \ud835\udc57 , \ud835\udc66\ud835\udc56, \ud835\udc57 )}\ud835\udc41\ud835\udc57=1 containing existing interaction histories and a query set Q\ud835\udc56 = {(\ud835\udc63 \ud835\udc57 , \ud835\udc66\ud835\udc56, \ud835\udc57 )}\ud835\udc40\ud835\udc57=1 containing interactions to predict. \ud835\udc41 and\ud835\udc40 are the number of interactions in S\ud835\udc56 . Q\ud835\udc56 and \ud835\udc41 are small."
        },
        {
            "heading": "3.2 Search Space",
            "text": "Existing modulation-based user cold-start works can be summarized into the following modulation framework consisting of three parts, i.e., embedding layer, adaptation network and predictor, as plotted in Figure 1.\n\u2022 The embedding layer \ud835\udc38 with parameter \ud835\udf3d\ud835\udc38 embeds the categorical features from users and items into dense vectors, i.e., (\ud835\udc96\ud835\udc56 , \ud835\udc97 \ud835\udc57 ) = \ud835\udc38 (\ud835\udc62\ud835\udc56 , \ud835\udc63 \ud835\udc57 ;\ud835\udf3d\ud835\udc38 ).\n\u2022 The adaptation network \ud835\udc34 with parameter \ud835\udf3d\ud835\udc34 takes the support set S\ud835\udc56 for a specific user \ud835\udc62\ud835\udc56 as input and generates user-specific adaptive parameters, i.e.,\n\ud835\udebd\ud835\udc56 = {\ud835\udf53\ud835\udc58\ud835\udc56 } \ud835\udc36 \ud835\udc58=1 = \ud835\udc34(S\ud835\udc56 ;\ud835\udf3d\ud835\udc34), (1)\nwhere \ud835\udc36 is the number of adaptive parameter groups for certain modulation structure. \u2022 The predictor \ud835\udc43 with parameter \ud835\udf3d\ud835\udc43 takes user-specific parameters \ud835\udf53\ud835\udc56 and \ud835\udc63\ud835\udc5e \u2208 Q\ud835\udc56 from the query set as input, and generate predictions by\n\ud835\udc66\ud835\udc56, \ud835\udc57 = \ud835\udc43 ((\ud835\udc96\ud835\udc56 , \ud835\udc97\ud835\udc5e),\ud835\udebd\ud835\udc56 ;\ud835\udf3d\ud835\udc43 ). (2)\nComparing with classical RS models, the extra adaptation network is introduced to handle cold-start users. To make personalized recommendation, for each \ud835\udc62\ud835\udc56 , the support set S\ud835\udc56 is first mapped to user-specific parameter \ud835\udf53\ud835\udc56 by (1). Then taking the features of target item \ud835\udc63\ud835\udc5e \u2208 Q\ud835\udc56 , user features \ud835\udc62\ud835\udc56 , and the \ud835\udf53\ud835\udc56 , prediction is made as (2). Subsequently, how to use the user-specific parameter \ud835\udf53\ud835\udc56 to change the prediction process in (2) can be crucial to the performance. Usually, a multi-layer perception (MLP) is used as \ud835\udc43 [5, 13, 21]. Assume a \ud835\udc3f-layer MLP is used and \ud835\udc89\ud835\udc59 denotes its output from the \ud835\udc59th layer, and let \ud835\udc890 = (\ud835\udc96\ud835\udc56 , \ud835\udc97\ud835\udc5e) for notation simplicity. For the \ud835\udc56th user, \ud835\udc89\ud835\udc59+1 is modulated as\n\ud835\udc89\ud835\udc59\ud835\udc56 = \ud835\udc40 \ud835\udc59 (\ud835\udc89\ud835\udc59 ,\ud835\udebd\ud835\udc56 ), (3)\n\ud835\udc89\ud835\udc59+1 = ReLU(\ud835\udc7e\ud835\udc59\ud835\udc43\ud835\udc89 \ud835\udc59 \ud835\udc56 + \ud835\udc83 \ud835\udc59 \ud835\udc43 ), (4)\nwhere \ud835\udc40\ud835\udc59 is the modulation function, \ud835\udc7e\ud835\udc59 and \ud835\udc83\ud835\udc59 are learnable weights at the \ud835\udc59th layer. This \ud835\udc40\ud835\udc59 controls how \ud835\udc89\ud835\udc59 is personalized w.r.t. the \ud835\udc56th user. The recent TaNP [21] directly lets\ud835\udc40\ud835\udc59 in (3) adopt the form of FiLM for all \ud835\udc3f MLP layers:\n\ud835\udc89\ud835\udc59\ud835\udc56 = \ud835\udc89 \ud835\udc59 \u2299 \ud835\udf531\ud835\udc56 + \ud835\udf53 2 \ud835\udc56 . (5)\nFiLM applies a feature-wise affine transformation on the intermediate features, and has been proved to be highly effective in other domains [4, 31]. However, users and items can have diverse interaction patterns. For example, both the inner product and summation have been used in RS to measure the preference of user over items [15, 18].\nIn order to find the appropriate modulation function, we are motivated to search \ud835\udc40\ud835\udc59 for different recommendation tasks. We design the following space for\ud835\udc40\ud835\udc59 :\n\ud835\udc89\ud835\udc59\ud835\udc56 = \ud835\udc89 \ud835\udc59 \u25e6op1 \ud835\udf531\ud835\udc56 \u25e6op2 \ud835\udf53 2 \ud835\udc56 \u00b7 \u00b7 \u00b7 \u25e6op\ud835\udc36 \ud835\udf53 \ud835\udc36 \ud835\udc56 , (6)\nwhere op\ud835\udc56 \u2019 are defined as \u25e6op\ud835\udc56 \u2208 O \u2261 { max,min, \u2299, /, +,\u2212 } .\nThey are all commonly used simple dimension-preserving binary operations. We choose them to avoid the resultant \ud835\udc40\ud835\udc59 being too complex, which can easily overfit for cold-start users.\nThe search space in (6) can be viewed as a binary tree, as shown in Figure 2(a). Since\ud835\udc40\ud835\udc59 can be different for each layer, the size of this space is 6\ud835\udc36\u00d7\ud835\udc3f . A larger \ud835\udc36 leads to a larger search space, which has higher potential of containing appropriate modulation function but is also more challenging to search effectively."
        },
        {
            "heading": "3.3 Search Strategy",
            "text": "We aim to find an efficient and practicable search algorithm on the proposed search space. However, the search space can be very large, and differentiable NAS methods are known to be fragile on large spaces [43]. In the sequel, we propose to first transform the original search space to an equivalent but much smaller space, as shown in Figure 2(a), where the equivalence is inspired by some similarities between the operations and the expressiveness of deep neural networks. On the transformed space, we design a supernet structure to conduct efficient and robust differentiable search.\n3.3.1 Search Space Transformation. Though the aforementioned search space can be very large, we can transform it to an equivalent space of size 24\u00d7\ud835\udc3f which is invariant with \ud835\udc36 , as proved in Proposition 3.11 below.\nProposition 3.1 (Search Space Transformation). Assume the adaptation network \ud835\udc34 is expressive enough. Any\ud835\udc40\ud835\udc59 with a form of (6) where \u25e6op\ud835\udc58 \u2208 O, \ud835\udc36 is any non-negative integer, and \ud835\udf53\ud835\udc58\ud835\udc56 \u2208 \ud835\udebd\ud835\udc56 = \ud835\udc34(S\ud835\udc56 , \ud835\udf3d\ud835\udc34), can be represented as\n\ud835\udc89\ud835\udc59\ud835\udc56 = min(max(\ud835\udc89 \ud835\udc59 , ?\u0302?1\ud835\udc56 ), ?\u0302? 2 \ud835\udc56 ) \u2299 ?\u0302? 3 \ud835\udc56 + ?\u0302? 4 \ud835\udc56 , (7)\nand the above four operations are permutation-invariant.\nThe intuitions are as follows. First, operations inO can be divided into four groups: \ud835\udc3a1 = {max}, \ud835\udc3a2 = {min}, \ud835\udc3a3 = {+,\u2212}, \ud835\udc3a4 = {\u2299, /}. Then, with mild assumption on adaptation network, we can prove two important properties: (i) inner-group consistence: operations that in the same group can be associated; and (ii) inter-group permutation-invariance: operations that are not in the same group are permutation-invariant which means any two operations can switch with another. Thus, we can recurrently commute operations until operations in the same group are neighbors, and associate operations in the four groups respectively.\nRemark 1. To better understand Proposition 3.1, let us check two examples.\n(1) min(max(\ud835\udc89\ud835\udc59 , \ud835\udf531 \ud835\udc56 ) + \ud835\udf532 \ud835\udc56 \u2212 \ud835\udf533 \ud835\udc56 , \ud835\udf534 \ud835\udc56 ) \u2299 \ud835\udf535 \ud835\udc56 equals to (7) where\n?\u0302?1 \ud835\udc56 = \ud835\udf531 \ud835\udc56 , ?\u0302?2 \ud835\udc56 = \ud835\udf534 \ud835\udc56 \u2212 \ud835\udf532 \ud835\udc56 + \ud835\udf533 \ud835\udc56 , ?\u0302?3 \ud835\udc56 = \ud835\udf535 \ud835\udc56 , ?\u0302?4 \ud835\udc56 = (\ud835\udf532 \ud835\udc56 \u2212 \ud835\udf533 \ud835\udc56 ) \u2299 \ud835\udf535 \ud835\udc56 ;\nand (2) max(min(\ud835\udc89\ud835\udc59 + \ud835\udf531\n\ud835\udc56 , \ud835\udf532 \ud835\udc56 ), \ud835\udf533 \ud835\udc56 ) \u2299 \ud835\udf534 \ud835\udc56 also equals to (7) where\n?\u0302?1 \ud835\udc56 = \ud835\udf533 \ud835\udc56 \u2212 \ud835\udf531 \ud835\udc56 , ?\u0302?2 \ud835\udc56 = \ud835\udf532 \ud835\udc56 \u2212 \ud835\udf531 \ud835\udc56 , ?\u0302?3 \ud835\udc56 = \ud835\udf534 \ud835\udc56 , ?\u0302?4 \ud835\udc56 = \ud835\udf531 \ud835\udc56 \u2299 \ud835\udf534 \ud835\udc56 .\nNote that due to the universal approximation ability of deep network [14], the assumption in this proposition can be easily satisfied. For example, we implement \ud835\udc34 based on two-layer MLP in experiments (see Appendix A.2), which can already ensure a good 1The proof is in Appendix B.\nperformance (see Section 4.3.3). After the transformation, the space in (7) also spans a permutation-invariant binary tree, as plotted in Figure 2(b). Such a space can be significantly smaller than the original one, as explained in Remark 2 below.\nRemark 2. The space transformation plays an essential role in ColdNAS, Table 1 helps to better understand to what extent the proposition can help reduce the search space, we take layer number \ud835\udc3f = 4 and the ratio is calculated as original spacetransformed space = 6\ud835\udc36\u00d74 24\u00d74 .\nNote that when \ud835\udc36 = 1, the transformation will not lead to a reduction on the space. However, such a case is not meaningful as it suffers from poor performance due to lack of flexibility for the modulation function (see Section 4.3.2). The transformed spaces enable efficient and robust differentiable search, via reducing the number of architecture parameter from 6 \u00d7\ud835\udc36 \u00d7 \ud835\udc3f to 4 \u00d7 \ud835\udc3f. Meanwhile, the space size is transformed from 6\ud835\udc36\u00d7\ud835\udc3f to 24\u00d7\ud835\udc3f , which is a reduction for any \ud835\udc36 > 1.\n3.3.2 Construction of the Supernet. For each \ud835\udc40\ud835\udc59 , since there are 4 operations at most and they are permutation-invariant, we only need to decide whether to take the operation or not by any order. We search by introducing differentiable parameters to weigh the operations and optimize the weights. For the \ud835\udc59th layer of the predictor, we have\n?\u0302?\ud835\udc59,\ud835\udc58+1=\ud835\udefc\ud835\udc59,\ud835\udc58+1 (?\u0302?\ud835\udc59,\ud835\udc58 \u25e6op\ud835\udc58+1 \ud835\udf53 \ud835\udc58+1 \ud835\udc56 )+(1\u2212\ud835\udefc \ud835\udc59,\ud835\udc58+1)?\u0302?\ud835\udc59,\ud835\udc58 , (8)\nwhere \ud835\udefc\ud835\udc59,\ud835\udc58+1 is a weight to measure operation \u25e6op\ud835\udc58+1 in \ud835\udc40\ud835\udc59 , \ud835\udc58 \u2208 {0, 1, 2, 3} and {\u25e6op\ud835\udc58+1 }3\ud835\udc58=0 = {max,min, \u2299, +}. For notation simplicity, we let ?\u0302?\ud835\udc59,0 = \ud835\udc89\ud835\udc59 , \ud835\udc89\ud835\udc59\n\ud835\udc56 = ?\u0302?\ud835\udc59,4. We construct the supernet by\nreplacing (3) with (8), i.e., replacing every\ud835\udc40\ud835\udc59 in red dotted lines in Figure 1 with the structure shown in Figure 2 (c).\n3.3.3 Complete Algorithm. The complete algorithm is summarized in Algorithm 1. We first optimize the supernet and make selection to determine the structure, then reconstruct the model with determined structure and retrain it to inference.\nBenefited from the great reduction brought by the space transformation, while conventional differentiable architecture search [23] optimizes the supernet w.r.t. the bilevel objective with the upperlevel variable \ud835\udf36 and lower-level variable \ud835\udf3d :\nmin \ud835\udf36 Lval (\ud835\udf3d \u2217 (\ud835\udf36 ),\ud835\udf36 ), s.t. \ud835\udf3d \u2217 (\ud835\udf36 ) = argmin \ud835\udf3d Ltrain (\ud835\udf3d ,\ud835\udf36 ), (9)\nAlgorithm 1 Training procedure of ColdNAS.\nInput: Learning rate \ud835\udefd , number of operations to keep \ud835\udc3e . 1: Construct the supernet by (8) and randomly initialize all pa-\nrameters \ud835\udeaf = {{\ud835\udefc\ud835\udc59,\ud835\udc58 }4,\ud835\udc3f\u22121 \ud835\udc58=1,\ud835\udc59=0, \ud835\udf3d\ud835\udc38 , \ud835\udf3d\ud835\udc34, \ud835\udf3d\ud835\udc43 }.\n2: while Not converge do 3: for Every \ud835\udc47\ud835\udc56 \u2208 T train do 4: Calculate \ud835\udebd\ud835\udc56 by (1). 5: Calculate \ud835\udc66\ud835\udc56, \ud835\udc57 for every \ud835\udc63 \ud835\udc57 in Q\ud835\udc56 by (2). 6: Calculate loss L\ud835\udc56 by (10). 7: end for 8: Ltrain = 1| Ttrain | \u2211 | Ttrain | \ud835\udc56=1 L\ud835\udc56\n9: Update all parameters \ud835\udeaf\u2190 \ud835\udeaf \u2212 \ud835\udefd\u2207\ud835\udeafLtrain. 10: end while 11: Determine the modulation structure by keeping operations\ncorresponding to Top-\ud835\udc3e \ud835\udefc\ud835\udc59,\ud835\udc58 and remove the others. 12: Construct the model with determined modulation structure\nand randomly initialize all parameters \ud835\udeaf = {\ud835\udf3d\ud835\udc38 , \ud835\udf3d\ud835\udc34, \ud835\udf3d\ud835\udc43 }. 13: Train the model in the same way as Step 2 \u223c 10. 14: Return: The trained model.\nwhere Lval and Ltrain represent the loss obtained on validation set and training set respectively, we only need to optimize the supernet w.r.t. objective Ltrain only, in an end-to-end manner by episodic training. For every task \ud835\udc47\ud835\udc56 \u2208 T train, we first input S\ud835\udc56 to the adaptation network \ud835\udc34 to generate \ud835\udebd\ud835\udc56 by (1), and then for every item \ud835\udc63 \ud835\udc57 \u2208 Q\ud835\udc56 , we take (\ud835\udc62\ud835\udc56 , \ud835\udc63 \ud835\udc57 ) and\ud835\udebd\ud835\udc56 as input of the predictor \ud835\udc43 and make prediction by (2). Then, we use mean squared error (MSE) between the prediction \ud835\udc66\ud835\udc56, \ud835\udc57 and true label \ud835\udc66\ud835\udc56, \ud835\udc57 as loss function:\nL\ud835\udc56 = 1 \ud835\udc40 \u2211\ufe01\ud835\udc40 \ud835\udc57=1 (\ud835\udc66\ud835\udc56, \ud835\udc57 \u2212 \ud835\udc66\ud835\udc56, \ud835\udc57 )2, (10)\nand Ltrain = \u2211\ud835\udc47\ud835\udc56 \u2208Ttrain L\ud835\udc56 . We update all parameters by gradient descent. Once the supernet converges, we determine all\ud835\udc40\ud835\udc59 s jointly by keeping the operation corresponding to the Top-\ud835\udc3e largest values among the 4\u00d7\ud835\udc3f values in {\ud835\udefc\ud835\udc59,\ud835\udc58+1}4,\ud835\udc3f\u22121\n\ud835\udc58=1,\ud835\udc59=0. We then retrain the model to obtain the final user cold-start model with searched modulation structure. During inference, a new set of tasks T test is given, which is disjoint from T train. For \ud835\udc47\ud835\udc56 \u2208 T test, we take the whole S\ud835\udc56 and (\ud835\udc62\ud835\udc56 , \ud835\udc63 \ud835\udc57 ) as input to the trained model to obtain prediction \ud835\udc66\ud835\udc56, \ud835\udc57 for each item \ud835\udc63 \ud835\udc57 \u2208 Q\ud835\udc56 ."
        },
        {
            "heading": "3.4 Discussion",
            "text": "Existing works in space transformation can be divided into three types. One is using greedy search strategy [9]. Methods of this kind explore different groups of architectures in the search space greedily. Thus, they fail to explore the full search space and can easily fall into bad local optimal. Another is mapping the search space into a low-dimensional one. For examples, using auto-encoder [25] or sparse coding [40]. However, these types of methods do not consider special properties of the search problem, e.g., the graph structure of the supernet. Thus, the performance ranking of architectures may suffer from distortion in the low-dimensional space. ColdNAS belongs to the third type, which is to explore architecture equivalence in the search space. The basic idea is that if we can find a group of architectures that are equivalent with each other,\nthen evaluation any one of them is enough for all architectures in the same group. This type of methods is problem-specific. For example, perturbation equivalence for matrices is explored in [45], morphism of networks is considered in [17]. The search space of ColdNAS is designed for modulation structures in cold-start problem, which has not been explored before. We theoretically prove the equivalence between the original space and the transformed space, which then significantly reduces the space size (Remark 2). Other methods for general space reduction cannot achieve that."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "We perform experiments on three benchmark datasets with the aim to answer the following research questions:\n\u2022 RQ1: What is the modulation structure selected by ColdNAS and how does ColdNAS perform in comparison with the state-of-theart cold-start models? \u2022 RQ2: How can we understand the search space and algorithm of ColdNAS? \u2022 RQ3: How do hyperparameters affect ColdNAS? Results are averaged over five runs."
        },
        {
            "heading": "4.1 Datasets",
            "text": "We use three benchmark datasets (Table 4): (i)MovieLens [12]: a dataset containing 1 million movie ratings of users collected from MovieLens, whose features include gender, age, occupation, Zip code, publication year, rate, genre, director and actor; (ii)BookCrossing [47]: a collection of users\u2019 ratings on books in BookCrossing community, whose features include age, location, publish year, author, and publisher; and (iii) Last.fm: a collection of user\u2019s listening count of artists from Last.fm online system, whose features only consist of user and item IDs. Following Lin et al. [21], we generate negative samples for the query sets in Last.fm.\nData Split. Following Lin et al. [21], the ratio of T train : T val : T test is set as 7 : 1 : 2. T val is used to judge the convergence of supernet. T train,T val,T test contain no overlapping users. For MovieLens and Last.fm, we keep any user whose interaction history length lies in [40, 200]. Each support set contains \ud835\udc41 = 20 randomly selected interactions of a user, and query set contains the rest interactions of the same user. As for BookCrossing with severe longtail distribution of user-item interactions, we particularly put any user whose interaction history length lies in [50, 1000) into T train. Then, we divide users with interaction history length in [2, 50) into 70%, 10% and 20% to be put in T train, T val, T test respectively. The proportion of cold users in each dataset is also shown in Table 4. Then, we randomly select half of each user\u2019s interaction history as support set and take the rest as query set.\nEvaluation Metric. Following [19, 26], we evaluate the performance by mean average error (MAE), mean squared Error (MSE), normalized discounted cumulative gain nDCG3 and nDCG5. MAE and MSE evaluate the numerical gap between the prediction and the ground-truth rating, lower value is better. For nDCG3 and nDCG5, the higher value is better, representing the proportion between the discounted cumulative gain of the predicted item list and the ground-truth list."
        },
        {
            "heading": "4.2 Performance Comparison (RQ1)",
            "text": "We compare ColdNAS with the following representative user cold-start methods: (i) traditional deep cold-start model DropoutNet [36] and (ii) FSL based methods includeMeLU [19],MetaCS [3], MetaHIN [24], MAMO [6], and TaNP [21]. We run the public codes provided by the respective authors. PNMTA [26], CMML [7], PAML [37] and REG-PAML [44] are not compared due to the lack of public codes. We choose a 4-layer predictor, more details of our model and parameter setting are provided in Appendix C.1. We also compare with a variant of ColdNAS called ColdNAS-Fixed, which uses the fixed FiLM function in (5) at every layer rather than our searched modulation function.\nTable 2 shows the overall user-cold start recommendation performance for all methods. We can see that ColdNAS significantly outperforms the others on all the datasets and metrics. Among all compared baselines, DropoutNet performs the worst as it is not a\nfew-shot learning method that the model has no ability to adapt to different users. Among meta-learning based methods, MeLU, MetaCS, MetaHIN and MAMO adopt gradient-based meta-learning strategy, which may suffer from overfitting during local-updates. In contrast, TaNP and ColdNAS learn to generate user-specific parameters to guide the adaptation. TaNP uses a fixed modulation structure which may not be optimal for different datasets, while ColdNAS automatically finds the optimal structure. Further, the consistent performance gain of ColdNAS over ColdNAS-Fixed validates the necessity of searching modulation structure to fit datasets instead of using a fixed one.\nTable 3 shows the searched modulation structures. We find that the searched modulation structures are different across the datasets. No modulation should be taken at the last layer, as directly use user-specific preference possibly leads to overfitting. Note that all the operations in the transformed search space have been selected at least once, which means all of them are helpful.\nComparing time consumption to other cold-start models, ColdNAS only additionally optimizes a supernet with the same objective and comparable size. Table 5 shows the clock time taken by ColdNAS and TaNP which obtains the second-best in Table 2. As can be observed, the searching in ColdNAS is very efficient, as it is only slightly more expensive than retraining."
        },
        {
            "heading": "4.3 Searching in ColdNAS (RQ2)",
            "text": "4.3.1 Choice of Search Strategy. In ColdNAS, we optimize (10) by gradient descent. In this section, we compare ColdNAS with other search strategies to search the modulation structure, includingRandom (\ud835\udc6a = 4) which conducts random search [2] on the original space with operation number\ud835\udc36 = 4 in each\ud835\udc40\ud835\udc59 ,Random (T)which conducts random search [2] on the transformed space, ColdNASBilevel which optimizes the supernet with the bilevel objective 9. For random search, we record its training and evaluation time. For ColdNAS and ColdNAS-bilevel, we sample the architecture parameter of the supernet and retrain from scratch, and exclude the time for retraining and evaluation. Figure 3 shows the performance of the best searched architecture. One can observe that searching on the transformed space allows higher efficiency and accuracy, comparing Random (T) to Random (\ud835\udc36 = 4). In addition, differentiable search on the supernet structure has brought more efficiency, as both ColdNAS and ColdNAS-Bilevel outperform random search. ColdNAS and ColdNAS-Bilevel converge to similar testing MSE, but ColdNAS is faster. This validates the efficacy of directly using gradient descent on all parameters in ColdNAS.\n4.3.2 Necessity of Search Space Transformation. Now that the search algorithm has been chosen, we now particularly examine the necessity of search space transformation in terms of time and performance. As shown in Table 1, a larger\ud835\udc36 which constrains the number of operations will lead to a larger original space. And the space reduction ratio grows exponentially along with the original space size. Figure 4 plots the testing MSE vs clock time of random search on original spaces of different size (Random (\ud835\udc36 = \ud835\udc50)), random search on transformed space (Random (T)), and our ColdNAS. When \ud835\udc36 is small, modulation functions would not be flexible enough, though it is easy to find the optimal candidate in this small search space. When \ud835\udc36 is large, the search space is large, where random search would be too time-consuming to find a good modulation structure. In contrast, the transformed search space has a consistent small size, and is theoretically proved to be equal to the space with any \ud835\udc36 . As can be seen, both Random (T) and ColdNAS can find good structure more effective than Random (\ud835\udc36 = 4), and ColdNAS searches the fastest via differentiable search.\n4.3.3 Understanding Proposition 3.1. Here, we first show that the assumption of adaptation network \ud835\udc34 being expressive enough can be easily satisfied. Generally, deeper networks are more expressive. Figure 6 plots the effect of changing the depth of the neural network in \ud835\udc34 on Last.fm. As shown in Figure 6 (a-d), although different number of layers in\ud835\udc34 are used, the searched modulation operations are the same. Consequently, the performance difference is small, as shown in Figure 6 (d). Thus, we use 2 layers which is expressive enough and has smaller parameter size.\nFurther, we validate the inner-group consistence and permutationinvariance properties proved in Proposition 3.1. Comparing Figure 6\n(a) with Figure 7 (a-b), one can see that changing \u2299 to / and + to \u2212 obtains the same results, which validates inner-group consistency. Finally, comparing Figure 6 (a) with Figure 7 (c), one can observe that inter-group permutation invariant also holds.\n(a) 2 layers (chosen). (b) 3 layers. (c) 4 layers."
        },
        {
            "heading": "4.4 Sensitivity Analysis (RQ3)",
            "text": "Finally, we conduct sensitivity analysis of ColdNAS on Last.fm.\nEffects of\ud835\udc3e . Recall that we keep modulation operations corresponding to the Top-\ud835\udc3e largest \ud835\udefc\ud835\udc59,\ud835\udc58 s in modulation structure. Figure 5(a) plots the effect of changing \ud835\udc3e . As shown, \ud835\udc3e cannot be too small that the model is unable to capture enough user-specific preference,\nnor too large that the model can overfit to the limited interaction history of cold-start users.\nEffect of the Depth of Predictor. Figure 5(b) plots the effect of using predictors with different \ud835\udc3f number of layers. One can observe that choosing different \ud835\udc3f in a certain range has low impact on the performance, while choosing \ud835\udc3f = 4 is already good enough to obtain the state-of-the-art results as shown in Table 2.\nEffects of Support Set Size. Figure 5(c) shows the performance of users with different length of history. During inference, we randomly sample only a portion of interactions from the original support set S\ud835\udc56 of each \ud835\udc47\ud835\udc56 in \ud835\udc47 test. As can be seen, prediction would be more accurate given interaction history, which may alleviate the bias in representing the user-specific preference."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "We propose a modulation framework called ColdNAS for user coldstart recommendation. In particular, we use a hypernetwork to map each user\u2019s history interactions to user-specific parameters, which are then used to modulate the predictor. We design a search space for modulation functions and positions, which not only covers existing modulation-based models but also has the ability to find more effective structures. We theoretically prove the space could be transformed to a smaller space, where we can search for modulation structure efficiently and robustly. Extensive experiments show that ColdNAS performs the best on benchmark datasets. Besides, ColdNAS can efficiently find proper modulation structure for different data, which make it easy to be deployed in recommendation systems."
        },
        {
            "heading": "ACKNOWLEDGMENT",
            "text": "Q. Yao was in part supported by NSFC (No. 92270106) and CCFBaidu Open Fund."
        },
        {
            "heading": "A DETAILS OF MODEL STRUCTURE",
            "text": "As mentioned in Section 3.2, existing models [6, 21, 26] share an architecture consisting of three components: embedding layer \ud835\udc38, adaptation network \ud835\udc34, and predictor \ud835\udc43 . Here we provide the details of these components used in ColdNAS.\nA.1 Embedding Layer We first embed user and item\u2019s one-hot categorical features into dense vectors through the embedding layer. Taking \ud835\udc62\ud835\udc56 for example, we generate a content embedding for each categorical content feature and concatenate them together to obtain the initial user embedding. Given \ud835\udc35 user contents, the embedding is defined as:\n\ud835\udc96\ud835\udc8a = [\ud835\udc7e1\ud835\udc38 \ud835\udc84 1 \ud835\udc56 | \ud835\udc7e 2 \ud835\udc38 \ud835\udc84 2 \ud835\udc56 | \u00b7 \u00b7 \u00b7 | \ud835\udc7e \ud835\udc35 \ud835\udc38 \ud835\udc84 \ud835\udc35 \ud835\udc56 ] (11)\nwhere [ \u00b7 | \u00b7 ] is the concatenation operation, \ud835\udc84\ud835\udc4f \ud835\udc56 is the one-hot vector of the \ud835\udc4fth categorical content of \ud835\udc62\ud835\udc56 , and\ud835\udc7e\ud835\udc4f\ud835\udc38 represents the embedding matrix of the corresponding feature in the shared user feature space. The parameters of embedding layer are collectively denoted as \ud835\udf3d\ud835\udc38 = {\ud835\udc7e\ud835\udc4f\ud835\udc38 } \ud835\udc35 \ud835\udc4f=1.\nA.2 Adaptation Network Wefirst use a fully connected (FC) layer to get hidden representation of interaction history \ud835\udc93\ud835\udc56, \ud835\udc57 , which is calculated as\n\ud835\udc93\ud835\udc56, \ud835\udc57 = ReLU(\ud835\udc7e1\ud835\udc34 [ \ud835\udc96\ud835\udc56 | \ud835\udc97 \ud835\udc57 | \ud835\udc66\ud835\udc56, \ud835\udc57 ] + \ud835\udc83 1 \ud835\udc34) . (12)\nWe use mean pooling to aggregate the interactions in S\ud835\udc56 as the task context information of \ud835\udc62\ud835\udc56 , i.e.,preference, \ud835\udc84\ud835\udc56 = 1\ud835\udc41 \u2211\ud835\udc41 1 \ud835\udc93\ud835\udc56, \ud835\udc57 . Then, we use another FC layer to generate the user-specific parameters as \ud835\udf53\ud835\udc56 =\ud835\udc7e2\ud835\udc34\ud835\udc84\ud835\udc56 + \ud835\udc83 2 \ud835\udc34 . The parameters of adaptation network is denoted as \ud835\udf3d\ud835\udc34 = {\ud835\udc7e1\ud835\udc34, \ud835\udc83 1 \ud835\udc34 ,\ud835\udc7e2 \ud835\udc34 , \ud835\udc832 \ud835\udc34 }.\nA.3 Predictor We use a \ud835\udc3f-layer MLP as the predictor. Denote output of the \ud835\udc59th layer hidden units as \ud835\udc89\ud835\udc59 , the modulation function at the \ud835\udc59th layer as \ud835\udc40\ud835\udc59 , we obtain prediction by\n\ud835\udc89\ud835\udc59\ud835\udc56 = \ud835\udc40\ud835\udc61 (\ud835\udc89 \ud835\udc59 ,\ud835\udebd\ud835\udc56 ),\n\ud835\udc89\ud835\udc59+1 = ReLU(\ud835\udc7e\ud835\udc59\ud835\udc43\ud835\udc89 \ud835\udc59 \ud835\udc56 + \ud835\udc83 \ud835\udc59 \ud835\udc43 ), (13)\nwhere \ud835\udc59 \u2208 [0, 1, \u00b7 \u00b7 \u00b7 \ud835\udc3f \u2212 1], \ud835\udc890 = [\ud835\udc96\ud835\udc56 | \ud835\udc97 \ud835\udc57 ] and \ud835\udc66\ud835\udc56, \ud835\udc57 = \ud835\udc89\ud835\udc3f . The parameters of predictor are denoted as \ud835\udf3d\ud835\udc43 = {\ud835\udc7e\ud835\udc59\ud835\udc43 , \ud835\udc83 \ud835\udc59 \ud835\udc43 }\ud835\udc3f\u22121 \ud835\udc59=0 ."
        },
        {
            "heading": "B PROOF OF THE PROPOSITION 3.1",
            "text": "Proof. To prove Proposition 3.1, we need to use the two conditions below: Condition 1: If \ud835\udf19\ud835\udc58\n\ud835\udc56 is the input of operation \u2299 or /, then every\nelement in \ud835\udf19\ud835\udc58 \ud835\udc56 is non-negative. We implement this as ReLU function. Condition 2: The adaptation network \ud835\udc34 is expressive enough, that it can approximate\ud835\udf19\ud835\udc56 = \ud835\udf19 \ud835\udc5d\n\ud835\udc56 \u25e6op1\ud835\udf19\n\ud835\udc5e \ud835\udc56 , where \u25e6op1 \u2208 O and\ud835\udf19 \ud835\udc5d \ud835\udc56 ,\ud835\udf19\ud835\udc5e \ud835\udc56 \u2208 \ud835\udebd\ud835\udc56\nlearned from the data. This naturally holds due to the assumption. Divide \u25e6op into 4 groups: \ud835\udc3a1 = {max}, \ud835\udc3a2 = {min}, \ud835\udc3a3 = {+,\u2212}, \ud835\udc3a4 = {\u2299, /}. We can prove two important properties below.\nProperty 1: Inner-group consistence. .We can choose an operation in each group as the group operation \u25e6\ud835\udc54\ud835\udc56 , e.g. \u25e6\ud835\udc541 = max, \u25e6\ud835\udc542 = min, \u25e6\ud835\udc543 = +, \u25e6\ud835\udc544 = \u2299. Then any number of successive operations that belong to the same group \ud835\udc3a\ud835\udc56 can be associated into one operation \u25e6\ud835\udc54\ud835\udc56 , i.e.,\n\ud835\udc99 \u25e6op\ud835\udc58 \ud835\udf53 \ud835\udc58 \ud835\udc56 \u25e6op\ud835\udc58+1 \ud835\udf53 \ud835\udc58+1 \ud835\udc56 \u00b7 \u00b7 \u00b7 \u25e6op\ud835\udc58+\ud835\udc5a \ud835\udf53 \ud835\udc58+\ud835\udc5a \ud835\udc56 = \ud835\udc99 \u25e6\ud835\udc54\ud835\udc56 ?\u0302? \ud835\udc58 \ud835\udc56 ,\n\ud835\udc60 .\ud835\udc61 . \u25e6op\ud835\udc58 , \u25e6op\ud835\udc58+1 \u00b7 \u00b7 \u00b7 \u25e6op\ud835\udc58+\ud835\udc5a \u2208 \ud835\udc3a\ud835\udc56 It\u2019s trivial to prove with condition 2, e.g., \ud835\udc99 + \ud835\udf531\n\ud835\udc56 \u2212 \ud835\udf532 \ud835\udc56 + \ud835\udf533 \ud835\udc56 = \ud835\udc99 + ?\u0302?1 \ud835\udc56 ,\nwhere ?\u0302?1 \ud835\udc56 = \ud835\udf531 \ud835\udc56 \u2212 \ud835\udf532 \ud835\udc56 + \ud835\udf533 \ud835\udc56 .\nProperty 2: Inter-group permutation-invariance. The operations in different groups are permutation-invariant, i.e.,\n\ud835\udc99 \u25e6\ud835\udc54\ud835\udc56 \ud835\udf53\ud835\udc58\ud835\udc56 \u25e6\ud835\udc54\ud835\udc57 \ud835\udf53 \ud835\udc58+1 \ud835\udc56 = \ud835\udc99 \u25e6\ud835\udc54\ud835\udc57 ?\u0302? \ud835\udc58 \ud835\udc56 \u25e6\ud835\udc54\ud835\udc56 ?\u0302? \ud835\udc58+1 \ud835\udc56\nIt is also trivial to prove with condition 1 and condition 2, e.g.,\nmax(\ud835\udc99, \ud835\udf531\ud835\udc56 ) + \ud835\udf53 2 \ud835\udc56 = max(\ud835\udc99 + ?\u0302? 1 \ud835\udc56 , ?\u0302? 2 \ud835\udc56 ),\nwhere ?\u0302?1 \ud835\udc56 = \ud835\udf532 \ud835\udc56 and ?\u0302?2 \ud835\udc56 = \ud835\udf531 \ud835\udc56 + \ud835\udf532 \ud835\udc56 .\nWith the above two properties, we can recurrently commute operations until operations in the same group are gathered, and merge operations in the four groups respectively. So\n\ud835\udc89\ud835\udc59\ud835\udc56 = \ud835\udc89 \ud835\udc59 \u25e6op1 \ud835\udf531\ud835\udc56 \u25e6op2 \ud835\udf53 2 \ud835\udc56 \u00b7 \u00b7 \u00b7 \u25e6op\ud835\udc36 \ud835\udf53 \ud835\udc36 \ud835\udc56\nequals to \ud835\udc89\ud835\udc59\ud835\udc56 = \ud835\udc89 \ud835\udc59 \u25e6\ud835\udc541 ?\u0302?1\ud835\udc56 \u25e6\ud835\udc542 ?\u0302? 2 \ud835\udc56 \u25e6\ud835\udc543 ?\u0302? 3 \ud835\udc56 \u25e6\ud835\udc544 ?\u0302? 4 \ud835\udc56 , and the four group operations are permutation-invariant. Note that since the identity element (0 or 1) of each group operation can be learned from condition 2, a modulation function that doesn\u2019t cover all groups can also be represented in the same form. \u25a1"
        },
        {
            "heading": "C EXPERIMENTS C.1 Experiment Setting",
            "text": "Experiments were conducted on a 24GB NVIDIA GeForce RTX 3090 GPU, with Python 3.7.0, CUDA version 11.6.\nC.1.1 Hyperparameter Setting. We find hyperparameters using the T val via grid search for existing methods. In ColdNAS, the batch size is 32,\ud835\udc7e\ud835\udc4f\n\ud835\udc38 in (11) has size 32 \u00d7 |\ud835\udc50\ud835\udc4f \ud835\udc56 | where |\ud835\udc50\ud835\udc4f \ud835\udc56 | is the length of\n\ud835\udc50\ud835\udc4f \ud835\udc56 . The dimension of hidden units in (13) is set as \ud835\udc891 = 128,\ud835\udc892 = 64,\ud835\udc893 = 32 for all three datasets. The learning rate \ud835\udefd is chosen from {5\u00d7 10\u22126, 1\u00d7 10\u22125, 5\u00d7 10\u22125, 1\u00d7 10\u22124} and the dimension of \ud835\udc93\ud835\udc56, \ud835\udc57 in (12) is chosen from {128, 256, 512, 1024}. The final hyperparameters chosen on the three benchmark datasets are shown in Table 6.\nC.1.2 URLs of Datasets and Baselines. We use three benchmark datasets (Table 4): (i) MovieLens2 [12]: a dataset containing 1 million movie ratings of users collected from MovieLens, whose features include gender, age, occupation, Zip code, publication year, rate, genre, director and actor; (ii)BookCrossing3 [47]: a collection of users\u2019 ratings on books in BookCrossing community, whose features include age, location, publish year, author, and publisher; and (iii) Last.fm4: a collection of user\u2019s listening count of artists 2https://grouplens.org/datasets/movielens/1m/ 3http://www2.informatik.uni-freiburg.de/~cziegler/BX/ 4https://grouplens.org/datasets/hetrec-2011/\nfrom Last.fm online system, whose features only consist of user and item IDs. In experiments, we compare ColdNAS with the following representative user cold-start methods: (i) traditional\ndeep cold-start modelDropoutNet5 [36] and (ii) FSL basedmethods include MeLU6 [19], MetaCS [3], MetaHIN7 [24], MAMO8 [6], and TaNP9 [21]. MetaCS is very similar to MeLU, except that it updates all parameters during meta-learning. Hence, we implement MetaCS based on the codes of MeLU.\nC.2 More Experimental Results Here, we show empirical results of Section 4.4 of MovieLens and BookCrossing in Figure 8 and 9. The results of all three datasets show similar patterns and the same analysis and conclusions in Section 4.4 are applicable.\nC.3 Complexity Analysis Denote the layer number of adaptation network as \ud835\udc3f\ud835\udc34 , the layer number of predictor as \ud835\udc3f\ud835\udc43 , the support set size as \ud835\udc41 , and the query set size as \ud835\udc40 . For notation simplicity, we denote the hidden size of each layer is the same as \ud835\udc37 . In the search phase (Algorithm 1 step 2\u223c10), the calculation of \ud835\udebd\ud835\udc56 of a task is \ud835\udc42 (\ud835\udc41\ud835\udc3f\ud835\udc34\ud835\udc373), and predicting each item costs \ud835\udc42 (\ud835\udc3f\ud835\udc43\ud835\udc373), so the average complexity is \ud835\udc42 ((\ud835\udc3f\ud835\udc43 + \ud835\udc41\ud835\udc3f\ud835\udc34\ud835\udc40 )\ud835\udc37\n3). Similarly, in the retrain (Algorithm 1 step 12\u223c13), the time complexity is also \ud835\udc42 ((\ud835\udc3f\ud835\udc43 + \ud835\udc41\ud835\udc3f\ud835\udc34\ud835\udc40 )\ud835\udc37\n3). In total, the time complexity is \ud835\udc42 ((\ud835\udc3f\ud835\udc43 + \ud835\udc41\ud835\udc3f\ud835\udc34\ud835\udc40 )\ud835\udc37 3).\n5https://github.com/layer6ai-labs/DropoutNet 6https://github.com/hoyeoplee/MeLU 7https://github.com/rootlu/MetaHIN 8https://github.com/dongmanqing/Code-for-MAMO 9https://github.com/IIEdm/TaNP"
        }
    ],
    "title": "ColdNAS: Search to Modulate for User Cold-Start Recommendation",
    "year": 2023
}