{
    "abstractText": "There has been significant progress in using diffusion models for large-scale text-to-image synthesis. This has led to versatile downstream applications such as 3D object synthesis, image editing, and customized image generation. In this paper, we present a generic approach that uses latent diffusion models as powerful image priors for various visual synthesis tasks. Existing methods that use these priors do not fully exploit the models\u2019 capabilities. To address this issue, we propose 1) a feature matching loss that provides detailed guidance by comparing features from different decoder layers and 2) a KL divergence loss that regularizes predicted latent features to stabilize the training process. We apply our method to three applications: text-to3D, StyleGAN adaptation, and layered image editing, and demonstrate its efficacy through extensive experimentation. Our results show that our method performs favorably compared to baseline methods. ar X iv :2 30 2. 08 51 0v 2 [ cs .C V ] 3 A pr 2 02 3",
    "authors": [
        {
            "affiliations": [],
            "name": "Ting-Hsuan Liao"
        },
        {
            "affiliations": [],
            "name": "Songwei Ge"
        },
        {
            "affiliations": [],
            "name": "Yiran Xu Yao-Chih"
        },
        {
            "affiliations": [],
            "name": "Lee Badour AlBahar"
        },
        {
            "affiliations": [],
            "name": "Jia-Bin Huang"
        }
    ],
    "id": "SP:289c24fb3956a093bb0a905ebfad7e094715ec18",
    "references": [
        {
            "authors": [
                "Yogesh Balaji",
                "Seungjun Nah",
                "Xun Huang",
                "Arash Vahdat",
                "Jiaming Song",
                "Karsten Kreis",
                "Miika Aittala",
                "Timo Aila",
                "Samuli Laine",
                "Bryan Catanzaro",
                "Tero Karras",
                "Ming-Yu Liu"
            ],
            "title": "ediff-i: Text-to-image diffusion models with ensemble of expert denoisers",
            "venue": "arXiv preprint arXiv:2211.01324,",
            "year": 2022
        },
        {
            "authors": [
                "Omer Bar-Tal",
                "Dolev Ofri-Amar",
                "Rafail Fridman",
                "Yoni Kasten",
                "Tali Dekel"
            ],
            "title": "Text2live: Text-driven layered image and video editing",
            "year": 2022
        },
        {
            "authors": [
                "Tim Brooks",
                "Aleksander Holynski",
                "Alexei"
            ],
            "title": "A Efros. Instructpix2pix: Learning to follow image editing instructions",
            "venue": "arXiv preprint arXiv:2211.09800,",
            "year": 2022
        },
        {
            "authors": [
                "Yunjey Choi",
                "Youngjung Uh",
                "Jaejun Yoo",
                "Jung-Woo Ha"
            ],
            "title": "Stargan v2: Diverse image synthesis for multiple domains",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Oran Gafni",
                "Adam Polyak",
                "Oron Ashual",
                "Shelly Sheynin",
                "Devi Parikh",
                "Yaniv Taigman"
            ],
            "title": "Make-a-scene: Scenebased text-to-image generation with human",
            "year": 2022
        },
        {
            "authors": [
                "Rinon Gal",
                "Or Patashnik",
                "Haggai Maron",
                "Amit H Bermano",
                "Gal Chechik",
                "Daniel Cohen-Or"
            ],
            "title": "Stylegan-nada: Clipguided domain adaptation of image generators",
            "venue": "ACM Transactions on Graphics (TOG),",
            "year": 2022
        },
        {
            "authors": [
                "Alexandros Graikos",
                "Nikolay Malkin",
                "Nebojsa Jojic",
                "Dimitris Samaras"
            ],
            "title": "Diffusion models as plug-and-play priors",
            "venue": "In NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Martin Heusel",
                "Hubert Ramsauer",
                "Thomas Unterthiner",
                "Bernhard Nessler",
                "Sepp Hochreiter"
            ],
            "title": "Gans trained by a two time-scale update rule converge to a local nash equilibrium",
            "venue": "NeurIPS,",
            "year": 2017
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "arXiv preprint arxiv:2006.11239,",
            "year": 2020
        },
        {
            "authors": [
                "Jonathan Ho",
                "Tim Salimans"
            ],
            "title": "Classifier-free diffusion guidance, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Ajay Jain",
                "Ben Mildenhall",
                "Jonathan T Barron",
                "Pieter Abbeel",
                "Ben Poole"
            ],
            "title": "Zero-shot text-guided object generation with dream fields",
            "year": 2022
        },
        {
            "authors": [
                "Nikolay Jetchev"
            ],
            "title": "Clipmatrix: Text-controlled creation of 3d textured",
            "venue": "meshes. ArXiv,",
            "year": 2021
        },
        {
            "authors": [
                "Tero Karras",
                "Samuli Laine",
                "Timo Aila"
            ],
            "title": "A style-based generator architecture for generative adversarial networks",
            "year": 2019
        },
        {
            "authors": [
                "Tero Karras",
                "Samuli Laine",
                "Miika Aittala",
                "Janne Hellsten",
                "Jaakko Lehtinen",
                "Timo Aila"
            ],
            "title": "Analyzing and improving the image quality of stylegan",
            "year": 2020
        },
        {
            "authors": [
                "Bahjat Kawar",
                "Michael Elad",
                "Stefano Ermon",
                "Jiaming Song"
            ],
            "title": "Denoising diffusion restoration models",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Bahjat Kawar",
                "Shiran Zada",
                "Oran Lang",
                "Omer Tov",
                "Huiwen Chang",
                "Tali Dekel",
                "Inbar Mosseri",
                "Michal Irani"
            ],
            "title": "Imagic: Text-based real image editing with diffusion models",
            "venue": "arXiv preprint arXiv:2210.09276,",
            "year": 2022
        },
        {
            "authors": [
                "Gyeongman Kim",
                "Hajin Shim",
                "Hyunsu Kim",
                "Yunjey Choi",
                "Junho Kim",
                "Eunho Yang"
            ],
            "title": "Diffusion video autoencoders: Toward temporally consistent face video editing via disentangled video encoding",
            "venue": "arXiv preprint arXiv:2212.02802,",
            "year": 2022
        },
        {
            "authors": [
                "Gihyun Kwon",
                "Jong Chul Ye"
            ],
            "title": "Clipstyler: Image style transfer with a single text condition",
            "year": 2022
        },
        {
            "authors": [
                "Bowen Li",
                "Xiaojuan Qi",
                "Thomas Lukasiewicz",
                "Philip HS Torr"
            ],
            "title": "Manigan: Text-guided image manipulation",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Yijun Li",
                "Richard Zhang",
                "Jingwan Lu",
                "Eli Shechtman"
            ],
            "title": "Few-shot image generation with elastic weight consolidation",
            "venue": "In NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Chen-Hsuan Lin",
                "Jun Gao",
                "Luming Tang",
                "Towaki Takikawa",
                "Xiaohui Zeng",
                "Xun Huang",
                "Karsten Kreis",
                "Sanja Fidler",
                "Ming-Yu Liu",
                "Tsung-Yi Lin"
            ],
            "title": "Magic3d: Highresolution text-to-3d content creation",
            "venue": "arXiv preprint arXiv:2211.10440,",
            "year": 2022
        },
        {
            "authors": [
                "Bingchen Liu",
                "Yizhe Zhu",
                "Kunpeng Song",
                "Ahmed Elgammal"
            ],
            "title": "Towards faster and stabilized gan training for highfidelity few-shot image synthesis",
            "venue": "In ICLR,",
            "year": 2020
        },
        {
            "authors": [
                "Cheng Ma",
                "Yongming Rao",
                "Yean Cheng",
                "Ce Chen",
                "Jiwen Lu",
                "Jie Zhou"
            ],
            "title": "Structure-preserving super resolution with gradient guidance",
            "year": 2020
        },
        {
            "authors": [
                "Gal Metzer",
                "Elad Richardson",
                "Or Patashnik",
                "Raja Giryes",
                "Daniel Cohen-Or"
            ],
            "title": "Latent-nerf for shape-guided generation of 3d shapes and textures",
            "year": 2023
        },
        {
            "authors": [
                "Sangwoo Mo",
                "Minsu Cho",
                "Jinwoo Shin"
            ],
            "title": "Freeze the discriminator: a simple baseline for fine-tuning gans",
            "venue": "In CVPR AI for Content Creation Workshop,",
            "year": 2020
        },
        {
            "authors": [
                "Nasir Mohammad Khalid",
                "Tianhao Xie",
                "Eugene Belilovsky",
                "Tiberiu Popa"
            ],
            "title": "Clip-mesh: Generating textured meshes from text using pretrained image-text models",
            "venue": "In SIGGRAPH Asia 2022 Conference Papers,",
            "year": 2022
        },
        {
            "authors": [
                "Ron Mokady",
                "Amir Hertz",
                "Kfir Aberman",
                "Yael Pritch",
                "Daniel Cohen-Or"
            ],
            "title": "Null-text inversion for editing real images using guided diffusion models",
            "venue": "arXiv preprint arXiv:2211.09794,",
            "year": 2022
        },
        {
            "authors": [
                "Seonghyeon Nam",
                "Yunji Kim",
                "Seon Joo Kim"
            ],
            "title": "Textadaptive generative adversarial networks: manipulating images with natural language",
            "venue": "NeurIPS,",
            "year": 2018
        },
        {
            "authors": [
                "Alex Nichol",
                "Prafulla Dhariwal",
                "Aditya Ramesh",
                "Pranav Shyam",
                "Pamela Mishkin",
                "Bob McGrew",
                "Ilya Sutskever",
                "Mark Chen"
            ],
            "title": "GLIDE: towards photorealistic image generation and editing with text-guided diffusion models",
            "year": 2021
        },
        {
            "authors": [
                "Atsuhiro Noguchi",
                "Tatsuya Harada"
            ],
            "title": "Image generation from small datasets via batch statistics adaptation",
            "venue": "In ICCV,",
            "year": 2019
        },
        {
            "authors": [
                "Dong Huk Park",
                "Samaneh Azadi",
                "Xihui Liu",
                "Trevor Darrell",
                "Anna Rohrbach"
            ],
            "title": "Benchmark for compositional text-toimage synthesis",
            "venue": "In NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Or Patashnik",
                "Zongze Wu",
                "Eli Shechtman",
                "Daniel Cohen-Or",
                "Dani Lischinski"
            ],
            "title": "Styleclip: Text-driven manipulation of stylegan imagery",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Justin NM Pinkney",
                "Doron Adler"
            ],
            "title": "Resolution dependent gan interpolation for controllable image synthesis between domains",
            "venue": "arXiv preprint arXiv:2010.05334,",
            "year": 2020
        },
        {
            "authors": [
                "Ben Poole",
                "Ajay Jain",
                "Jonathan T. Barron",
                "Ben Mildenhall"
            ],
            "title": "Dreamfusion: Text-to-3d using 2d diffusion",
            "venue": "In ICLR,",
            "year": 2023
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In International Conference on Machine Learning. PMLR,",
            "year": 2021
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In International Conference on Machine Learning. PMLR,",
            "year": 2021
        },
        {
            "authors": [
                "Aditya Ramesh",
                "Mikhail Pavlov",
                "Gabriel Goh",
                "Scott Gray",
                "Chelsea Voss",
                "Alec Radford",
                "Mark Chen",
                "Ilya Sutskever"
            ],
            "title": "Zero-shot text-to-image generation",
            "venue": "In International Conference on Machine Learning. PMLR,",
            "year": 2021
        },
        {
            "authors": [
                "Esther Robb",
                "Wen-Sheng Chu",
                "Abhishek Kumar",
                "Jia- Bin Huang"
            ],
            "title": "Few-shot adaptation of generative adversarial networks",
            "venue": "arXiv preprint arXiv:2010.11943,",
            "year": 2020
        },
        {
            "authors": [
                "Robin Rombach",
                "Andreas Blattmann",
                "Dominik Lorenz",
                "Patrick Esser",
                "Bj\u00f6rn Ommer"
            ],
            "title": "High-resolution image synthesis with latent diffusion models",
            "year": 2022
        },
        {
            "authors": [
                "Nataniel Ruiz",
                "Yuanzhen Li",
                "Varun Jampani",
                "Yael Pritch",
                "Michael Rubinstein",
                "Kfir Aberman"
            ],
            "title": "Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation",
            "venue": "arXiv preprint arXiv:2208.12242,",
            "year": 2022
        },
        {
            "authors": [
                "Chitwan Saharia",
                "William Chan",
                "Saurabh Saxena",
                "Lala Li",
                "Jay Whang",
                "Emily Denton",
                "Seyed Kamyar Seyed Ghasemipour",
                "Burcu Karagol Ayan",
                "S Sara Mahdavi",
                "Rapha Gontijo Lopes"
            ],
            "title": "Photorealistic text-to-image diffusion models with deep language understanding",
            "venue": "arXiv preprint arXiv:2205.11487,",
            "year": 2022
        },
        {
            "authors": [
                "Aditya Sanghi",
                "Hang Chu",
                "Joseph G Lambourne",
                "Ye Wang",
                "Chin-Yi Cheng",
                "Marco Fumero"
            ],
            "title": "Clip-forge: Towards zero-shot text-to-shape generation",
            "year": 2022
        },
        {
            "authors": [
                "Jiaming Song",
                "Chenlin Meng",
                "Stefano Ermon"
            ],
            "title": "Denoising diffusion implicit models",
            "venue": "ICLR,",
            "year": 2021
        },
        {
            "authors": [
                "Kunpeng Song",
                "Ligong Han",
                "Bingchen Liu",
                "Dimitris Metaxas",
                "Ahmed Elgammal"
            ],
            "title": "Diffusion guided domain adaptation of image generators",
            "venue": "arXiv preprint arXiv:2212.04473,",
            "year": 2022
        },
        {
            "authors": [
                "Hung-Yu Tseng",
                "Lu Jiang",
                "Ce Liu",
                "Ming-Hsuan Yang",
                "Weilong Yang"
            ],
            "title": "Regularizing generative adversarial networks under limited data",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Rotem Tzaban",
                "Ron Mokady",
                "Rinon Gal",
                "Amit Bermano",
                "Daniel Cohen-Or"
            ],
            "title": "Stitch it in time: Gan-based facial editing of real videos",
            "venue": "In SIGGRAPH Asia 2022 Conference Papers,",
            "year": 2022
        },
        {
            "authors": [
                "Dani Valevski",
                "Matan Kalman",
                "Yossi Matias",
                "Yaniv Leviathan"
            ],
            "title": "Unitune: Text-driven image editing by fine tuning an image generation model on a single image",
            "venue": "arXiv preprint arXiv:2210.09477,",
            "year": 2022
        },
        {
            "authors": [
                "Haochen Wang",
                "Xiaodan Du",
                "Jiahao Li",
                "Raymond A. Yeh",
                "Greg Shakhnarovich"
            ],
            "title": "Score jacobian chaining: Lifting pretrained 2d diffusion models for 3d generation",
            "year": 2023
        },
        {
            "authors": [
                "Ting-Chun Wang",
                "Ming-Yu Liu",
                "Jun-Yan Zhu",
                "Andrew Tao",
                "Jan Kautz",
                "Bryan Catanzaro"
            ],
            "title": "High-resolution image synthesis and semantic manipulation with conditional gans",
            "year": 2018
        },
        {
            "authors": [
                "Yaxing Wang",
                "Chenshen Wu",
                "Luis Herranz",
                "Joost van de Weijer",
                "Abel Gonzalez-Garcia",
                "Bogdan Raducanu"
            ],
            "title": "Transferring gans: generating images from limited data",
            "year": 2018
        },
        {
            "authors": [
                "Jay Zhangjie Wu",
                "Yixiao Ge",
                "Xintao Wang",
                "Weixian Lei",
                "Yuchao Gu",
                "Wynne Hsu",
                "Ying Shan",
                "Xiaohu Qie",
                "Mike Zheng Shou"
            ],
            "title": "Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation",
            "venue": "arXiv preprint arXiv:2212.11565,",
            "year": 2022
        },
        {
            "authors": [
                "Yiran Xu",
                "Badour AlBahar",
                "Jia-Bin Huang"
            ],
            "title": "Temporally consistent semantic video editing",
            "year": 2022
        },
        {
            "authors": [
                "Zipeng Xu",
                "Tianwei Lin",
                "Hao Tang",
                "Fu Li",
                "Dongliang He",
                "Nicu Sebe",
                "Radu Timofte",
                "Luc Van Gool",
                "Errui Ding"
            ],
            "title": "Predict, prevent, and evaluate: Disentangled text-driven image manipulation empowered by pre-trained vision-language model",
            "year": 2022
        },
        {
            "authors": [
                "Ceyuan Yang",
                "Yujun Shen",
                "Yinghao Xu",
                "Bolei Zhou"
            ],
            "title": "Data-efficient instance generation from instance discrimination",
            "venue": "NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Jiahui Yu",
                "Yuanzhong Xu",
                "Jing Yu Koh",
                "Thang Luong",
                "Gunjan Baid",
                "Zirui Wang",
                "Vijay Vasudevan",
                "Alexander Ku",
                "Yinfei Yang",
                "Burcu Karagol Ayan",
                "Ben Hutchinson",
                "Wei Han",
                "Zarana Parekh",
                "Xin Li",
                "Han Zhang",
                "Jason Baldridge",
                "Yonghui Wu"
            ],
            "title": "Scaling autoregressive models for content-rich text-to-image generation, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Richard Zhang",
                "Phillip Isola",
                "Alexei A Efros",
                "Eli Shechtman",
                "Oliver Wang"
            ],
            "title": "The unreasonable effectiveness of deep features as a perceptual metric",
            "year": 2018
        }
    ],
    "sections": [
        {
            "text": "There has been significant progress in using diffusion models for large-scale text-to-image synthesis. This has led to versatile downstream applications such as 3D object synthesis, image editing, and customized image generation. In this paper, we present a generic approach that uses latent diffusion models as powerful image priors for various visual synthesis tasks. Existing methods that use these priors do not fully exploit the models\u2019 capabilities. To\naddress this issue, we propose 1) a feature matching loss that provides detailed guidance by comparing features from different decoder layers and 2) a KL divergence loss that regularizes predicted latent features to stabilize the training process. We apply our method to three applications: text-to3D, StyleGAN adaptation, and layered image editing, and demonstrate its efficacy through extensive experimentation. Our results show that our method performs favorably compared to baseline methods."
        },
        {
            "heading": "1. Introduction",
            "text": "Diffusion models have shown impressive image generation capabilities in terms of photorealism and compositionality [1, 30, 40, 42]. Through guidance control and embedding techniques, text-to-image diffusion models have been applied to various visual editing and processing tasks, e.g. customized image generation [41], video generation [52], image and video editing [17, 18], text-to-3D generation [22, 35], and image generator adaptation [45]. With a free-form text prompt, these methods utilize a pre-trained text-to-image diffusion model to guide the synthesis tasks. However, specialized methods are required for each task to produce good results. While these methods may share similar objectives, a unified approach is underexplored.\nIn this paper, we leverage a pretrained diffusion model as a generic image prior for various visual synthesis applications. This is similar to the recent line of work that uses a CLIP model [36]. CLIP models are trained to encode a paired image and its caption and maximize their cosine similarity. Several works have used a pretrained CLIP model as a prior to facilitate different text-guided synthesis tasks, e.g., image and video editing [2, 33, 47, 53], generator adaptation [6], text-to-3D synthesis [12, 27]. However, as CLIP models are not generative models, their contrastive objective may not preserve the visual information useful for synthesis-oriented tasks.\nDiffusion models have recently shown great potential in these synthesis tasks [17, 18, 22, 35, 45]. They can achieve competitive or even better performance compared to CLIPbased approaches. However, unlike CLIP models where the original CLIP objective was uniformly adopted, these works have applied diffusion models differently under different tasks. In this work, we present a framework to utilize diffusion models for various tasks.\nWe build our framework on Score Distillation Sampling [35], which uses an image diffusion model [42] as prior to train a NeRF model without costly backpropagating through the diffusion model itself. Other works [22, 25, 45, 49] follow a similar approach but are based on latent diffusion models [40]. The score distillation loss is computed in the latent space, which we term \u201clatent score distillation (LSD)\u201d. Although the methods using score distillation with trained diffusion models have shown promising results, the loss is computed in a limited spatial resolution (e.g., 64\u00d7 64) and thus cannot provide sufficient detailed guidance.\nTo achieve detailed guidance, we propose a new Feature Matching Loss (FM) loss that uses features in multiple decoder layers of the latent diffusion model to guide the optimization process.\nAnother limitation of recent works utilizing LSD is the lack of regularization over the optimized latent code. While minimizing the optimization loss in an unconstraint man-\nner, these methods are likely to produce out-of-distribution latent code that is not seen by the decoder during the training, resulting in a lower-quality output. To mitigate this issue, we propose to use a KL divergence loss to regularize the optimized latent so that they stay close to the prior distribution during the training.\nWe evaluate our method on three text-driven synthesis applications, text-to-3D generation, layered image editing, and StyleGAN adaptation (as shown in Fig. 1). Our approach achieves competitive results over baselines using the CLIP model as the image prior and diffusion models with latent score distillation.\nWe make the following contributions:\n\u2022 We propose a feature matching loss to extract detailed information from the decoder to guide text-based visual synthesis tasks.\n\u2022 We propose a KL divergence loss to regularize the optimized latent, stabilizing the optimization process.\n\u2022 We extensively evaluate our method on three downstream tasks and show competitive results with strong baselines."
        },
        {
            "heading": "2. Related Work",
            "text": "Text-to-image diffusion models. Diffusion models [10, 16, 44] synthesize images by denoising independent noises drawn from the standard Gaussian diffusion. Impressive progress has been made in photorealistic and zero-shot textto-image generation [1, 5, 30, 38, 40, 42, 56]. When combined with tricks such as classifier-free guidance [11] and gradient guidance [24], the flexibility of the diffusion model makes it readily adapt to different conditional generation tasks that are not part of its training objectives, such as image editing [17, 48], and personalized image generation [41]. Our work capitalizes on the recent progress textto-image diffusion model and uses it as image priors for various visual synthesis tasks. Text-driven 3D generative models. With the guidance from a Contrastive Language-Image Pre-training (CLIP) model [36], text-guided 3D generation becomes possible together with a differentiable renderer [12, 13, 27, 43]. By optimizing the 3D representation through the CLIP objective computed on the rendered image, they can explicitly control the pose of generated 3D shapes, and generate creative appearances and shapes from the text freely. More recently, DreamFusion [35] has brought diffusion models to this task and shown impressive results. In contrast to [7] backpropagating through the pre-trained diffusion model itself, DreamFusion leverages noise residual predicted by the pre-trained diffusion model as gradients for efficient backpropagation. Follow-up works [25, 49] adapt the method\nto latent diffusion models [40]. However, the 3D models resulting from these methods still lack details since the latent score distillation is computed in the latent space with a limited spatial resolution (e.g., 64\u00d7 64 latent space for a 512\u00d7512 image). In this work, we leverage the knowledge embedded in the latent diffusion decoder to provide more detailed guidance. We show better details can be achieved in the synthesized 3D models.\nImage generator domain adaptation. Some works aim to fine-tune a pre-trained generator with few-shot or textguided zero-shot domain adaptation to reduce the cost of training an image generator. Few-shot adaption aims to use fewer data (typically hundreds or fewer) to train an image generator. Previous works control the learnable parameters [26, 31, 39, 51], design regularizers [21, 34, 46], or use auxiliary tasks [23, 55] to improve the quality of the generator. With the advancement of large fundamental models, some works use them as guidance to achieve zero-shot adaptation. StyleGAN-NADA [6] uses pre-trained CLIP model [36] as guidance and shows diverse domain adaption results on StyleGAN [14]. StyleGANFusion [45] leverages pre-trained StableDiffusion [40] and follow the score distillation approach proposed by DreamFusion [35] to achieve even better results. Building upon StyleGANFusion [45],\nwe show that using our proposed method produces a significantly better FID score and a competitive CLIP score. Text-driven image editing. Pioneering image manipulation methods [20, 29] utilize GANs to achieve editing of appearances while preserving the shape. However, the training image domains and text expression constraints often restrict the GAN-based approaches. The advanced methods have leveraged embeddings from a pretrained CLIP [36]. These embeddings can be applied to update the generative models with test-time optimization [2, 6, 19, 33, 54]. Recently, text-to-image diffusion models have shown exceptional success in manipulation tasks [1, 3, 17, 28, 30, 41]. We demonstrate the application of our diffusion prior to the image editing task. Unlike existing diffusion-based editors, our method manipulates images using test-time optimization with the proposed diffusion guidance. Our method produces more detailed results than the latent diffusion-guided baselines and the CLIP-guided method, Text2LIVE [2]."
        },
        {
            "heading": "3. Proposed Method",
            "text": ""
        },
        {
            "heading": "3.1. Background",
            "text": "Score distillation. DreamFusion [35] first proposes using the Imagen [42] diffusion model for a text-driven 3D\ngeneration. Imagen can generate high-resolution images by cascade diffusion. However, the base model only operates on a low-resolution 64\u00d7 64 image space. They build upon the formulation introduced by [7] and propose a more stable training process. Their approach optimizes the underlying 3D representation model by using the training objective of the diffusion model. The resulting approach, score distillation sampling, involves perturbing the rendered image with a random noise \u03b5 , and then using the pretrained diffusion model to predict the noise \u03b5\u0302 . The training is based on the gradient computed from the noise residual between the added random noise and the predicted one: \u03b5\u0302 \u2212 \u03b5 . Note that the Imagen model [42] only operates on low-resolution image space.\nLatent score distillation. Jacobian NeRF [49], Latent NeRF [25] and StyleGANFusion [45] have recently incorporated score distillation into the latent diffusion models [40]. A latent diffusion model generally contains two components, an autoencoder with encoder E\u03c6enc and decoder G\u03c6dec , and a diffusion model \u03b5\u0302\u03c6UNet operating in the latent space. However, the decoder is not utilized in the latent score distillation of [25, 45, 49], which can lead to inferior results since the knowledge of converting low-resolution latent to high-resolution RGB images is not used. To address this, we propose using feature matching and KL losses to reintroduce the decoder into the optimization procedure."
        },
        {
            "heading": "3.2. Feature matching Loss",
            "text": "We propose a feature matching loss to leverage the generative capacity of the decoder G\u03c6dec and provide finergrained guidance to the differentiable renderer. We use the decoder of the stable diffusion autoencoder as shown in Figure 2. We feed the original latent code v and the updated latent code v\u2032 = v+(\u03b5\u0302 \u2212 \u03b5) to the decoder G\u03c6dec(\u00b7), and then compute the feature matching loss:\nLFM = \u2211 \u2113 \u2225G\u2113\u03c6dec (v)\u2212G \u2113 \u03c6dec (v+ \u03b5\u0302 \u2212 \u03b5)\u22251,\nwhere \u2113 is the specific level in the decoder G\u03c6dec . Our proposed feature matching loss, denoted as LFM , is inspired by the GAN discriminator feature matching loss proposed in pix2pixHD [50], which aims to align the features of multiple discriminator layers of real and synthetic images. Our feature matching loss LFM operates similarly to the latent score distillation loss LLSD, but its signal is amplified through the use of the decoder G\u03c6dec . Our approach differs from the feature matching loss in pix2pixHD in two major aspects. First, we measure the similarity of the extracted features from the pretrained (and fixed) decoder, not from an additional trainable discriminator. Second, we use latent code with added noise residual and the clean latent as the decoder input, as opposed to real/fake images.\nWe denote \u03b8 as the target parameters in the renderer\ng\u03b8 (\u00b7) to optimize, e.g. NeRF model for text-to-3D generation and the StyleGAN model for generator adaptation. The direct gradient computation involves calculating the UNet Jacobian, which is computationally expensive [35]. We therefore consider the gradient backpropagation to the generator dLFMd\u03b8 using the chain rule:\ndLFM d\u03b8 = \u2202v \u2202\u03b8 \u2202LFM \u2202v\n= \u2202v \u2202\u03b8\n( \u03b1t\n\u2202 \u03b5\u0302 \u2202zt\n\u2202x\u2032\n\u2202v\u2032 \u2202LFM \u2202x\u2032 +\n\u2202x\u2032\n\u2202v\u2032 \u2202LFM \u2202x\u2032 + \u2202x \u2202v \u2202LFM \u2202x\n) ,\n(1) where \u2202 \u03b5\u0302\u2202zt refers to the UNet Jacobian term, which we ignore due to the high optimization cost. x and x\u2032 denote the latent feature within the decoder obtained by the original v and updated latent code v\u2032. zt refers to the perturbed latent code defined as zt = \u03b1tv+\u03c3t , where \u03b1t and \u03c3t are timedependent constant defined in DDPM [10].\nThe final gradient can be derived as follows:\ndLFM d\u03b8 = \u2202v \u2202\u03b8\n( (1+\u03b1t) \u2202x\u2032\n\u2202v\u2032 \u2202LFM \u2202x\u2032 + \u2202x \u2202v \u2202LFM \u2202x\n) . (2)"
        },
        {
            "heading": "3.3. Kullback-Leibler divergence regularizer.",
            "text": "When conducting optimization through the latent code in an unconstrained way [25, 35, 49], the resulting latent code likely drifts away from the original distribution. Consequently, the decoder must handle an unseen input during the training and produces poor quality. Our feature matching loss mitigates this issue by incorporating the gradient from the decoder. However, it is insufficient in practice as we still observe artifacts or unrealistic styles in the decoded image. Therefore, we propose further regularizing the latent with a KL divergence loss.\nStable Diffusion and VAE models both use a KL penalty to regularize the variance of the latent space. However, in the text-to-3D task, a latent radiance field is built to directly predict the latent code without an encoder E\u03c6enc . Thus, different from the training process, we do not compute the KL penalty on the mean and variance output by the encoder but directly on the latent sample v as below:\nLKL = 1 2 ( \u00b52v +\u03c3 2 v \u2212 log ( \u03c32v ) +1 ) , (3)\nwhere \u00b5v = 1N \u2211i vi and \u03c3 2 v = 1 N \u2211i(vi \u2212 \u00b5v) 2 represent the mean and variance of the latent code v, respectively. And N denotes the number of elements of the latent code v. We find this to be effective in improving the stability of the optimization process."
        },
        {
            "heading": "3.4. Training procedure",
            "text": "The diffusion prior includes three parts, latent score distillation, feature matching loss, and KL regularizer.\nDuring the optimization, the latent code v is first perturbed following the DDPM [10] scheduler at a random time step t, such that perturbed latent code zt = \u03b1tv+\u03c3t\u03b5 . This perturbed latent code zt is then passed to the UNet to generate the predicted noise \u03b5\u0302 . We then use the predicted noise \u03b5\u0302 to derive the latent score distillation gradient. We define the latent score distillation loss as LLSD in this paper. To compute the feature matching loss, we input the latent code v and updated latent code v+(\u03b5\u0302 \u2212 \u03b5) into the decoder G\u03c6dec(\u00b7). We use the decoded features at three different layers from the decoder, to compute the feature matching loss.\nWe compute the gradient partial to the latent code v, and propagate back to optimize g\u03b8 using the final loss L f inal :\nL f inal = \u03bb1LFM +\u03bb2LKL +\u03bb3LLSD, (4)\nwhere \u03bb1,\u03bb2,\u03bb3 are the balancing factor for each loss."
        },
        {
            "heading": "4. Experiments",
            "text": "We evaluated three applications using Stable Diffusion [40] v1.4 for Text-to-3D (Latent NeRF) and StyleGAN adaptation, and Stable Diffusion v1.5 for Text-to-3D (Jacobian NeRF) and layered image editing as our pretrained diffusion model. Fig. 3 depicts the overall pipeline for each application and demonstrates how we acquire the latent code v from each differentiable generator used in the three applications. We include the implementation details and more results in the supplementary material and we will make the source code publicly available."
        },
        {
            "heading": "4.1. Applications",
            "text": "Text-to-3D. The text-to-3D task aims to generate a 3D model from a free-form text description. We evaluate our method on two text-guided 3D generative models, Jacobian NeRF [49], and Latent NeRF [25] using the Stable Diffusion as guidance. Both methods learn and predict the latent code within each viewing point and optimize with the latent score distillation. We applied the proposed feature matching loss LFM and KL regularizer LKL to both meth-\nods. We also compare our results with two other baselines, CLIP-Mesh [27] and DreamFields [12], which leverage CLIP [36] as guidance instead. As shown in Fig. 4, Ours (Latent NeRF) and Ours (Jacobian NeRF) using our proposed losses upon Latent NeRF and Jacobian NeRF offer better quality with more details.\nWe further evaluate the generated 3D model with the CLIP-R precision score [32], comparing with DreamFields [12], CLIP-Mesh [27], Latent NeRF [25] and Jacodian NeRF [49]. We follow the same experiment setup outline in CLIP-Mesh [27], which involved generating one 3D model per prompt in 153 text prompts. We take a rendered image from a random camera pose during the evaluation to compute the CLIP score with 99 random prompts and the generated text prompt. We test two different-sized CLIP models for computing the precision. As shown in Table 1, our method consistently improves over both baseline methods [25, 49]. Note that both DreamFields and CLIPMesh [12, 27] optimize directly with CLIP loss, which can lead to potential overfitting issues as discussed in [12]. Our focus is not on achieving state-of-the-art text-to-3D results but showing how our proposed losses complement the commonly used latent score distillation. The SOTA methods [22, 35] rely on proprietary image-based diffusion models (Imagen [42] and e-Diff [1]) that are not publicly available. Magic3D [22], however, does use a latent diffusion model in their second stage of training. We believe that our proposed method can also be applied to improve the results.\nStyleGAN adaptation. We also apply our proposed method to image generator adaptation. We conduct experiments on StyleGAN2 [15] with our feature matching loss LFM and KL loss LKL. To demonstrate the effectiveness of our method, we compare our approach StyleGANFusion [45] and StyleGAN-NADA [6]. We follow the three metrics used in [45] to evaluate different methods: FID score [9], CLIP score [37], and LPIPS score [57]. For our method, we apply the feature matching loss LFM to StyleGANFusion as additional guidance with KL loss LKL.\nWe generate 2,000 samples from the adapted generator after the training for the quantitative evaluations. We first show FID scores of different approaches on various target domains in Table 2. Groundtruth images are extracted from AFHQ dataset [4]. Some of the labels are provided by [45]. Following StyleGANFusion [45], we compare the\nFID scores on \u201cCat \u2192 Dog/Fox/Lion/Tiger/Wolf\u201d. Our FID scores outperform other baselines in most cases. This indicates that our method can help gain a more similar distribution to the target domain and improve the quality.\nCLIP score is used to evaluate the similarity between the text prompt and the output image. LPIPS score measures all the pairs in the samples to examine the diversity of the generated samples. Our method achieves competitive CLIP and LPIPS scores compared to the baselines. For some domains, e.g., Dog, Hamster, Badger, Otter, and Pig, our LPIPS score outperforms other baselines by a large margin.\n1\u201cA high quality photo of a delicious burger.\u201d 2\u201cA high quality photo of a jug made of blue and white porcelain.\u201d\nNote that the results shown here have discrepancies with\n3\u201c3d cute cat, closeup cute and adorable, cute big circular reflective eyes, long fuzzy fur, Pixar render, unreal engine cinematic smooth, intri-\ncate detail, cinematic\u201d 4\u201c3d human face, closeup cute and adorable, cute big circular reflective eyes, Pixar render, unreal engine cinematic smooth, intricate detail,\nthe reported results in StyleGANFusion [45]. We consulted with the authors and were informed that the reported results were obtained from per-dataset tuning. We run the experiments with the default parameters provided by the authors. The qualitative comparisons of human faces and cats are illustrated in Fig. 6. As reported in [45], StyleGAN-NADA cannot handle long prompts well but achieves competitive CLIP scores for short prompts. We show results with short prompts, like \u201ca photo of X\u201d, and long prompts. Other baselines tend to have over-smoothed texture (StyleGANFusion) or lower fidelity (StyleGAN-NADA). Our method can generate sharper textures and better overall image quality.\nLayered image editing. We demonstrate the application on text-driven image editing tasks. We follow the layeredediting approach in Text2LIVE [2], which utilizes a CNN generator to output an RGBA editing layer blended with the input image. The generator is trained with CLIP [36] guidance via test-time optimization. We adopt the same CNN generator \u03b8CNN and introduce an additional trainable latent code \u03b8latent , allowing to generate richer details. The trainable parameter set is \u03b8 = {\u03b8CNN ,\u03b8latent}. The parameters are updated using a combination of latent score distillation LLSD and the proposed feature matching loss LFM along with KL loss LKL. Besides, we use object mask supervision for the blending alpha map to improve editing quality.\nWe compare our method with Text2LIVE, and the latentscore-distillation-only (i.e., LLSD) baseline with the same configuration as our model in Fig. 5. Text2LIVE fails to manipulate the appearance effectively and produces noticeable artifacts, particularly in the \u201czebra\u201d case. On the other\ncinematic\u201d 5\u201cphoto of a face [SEP] A realistic detailed portrait, single face, science fiction, artstation, volumetric lighting, octane render\u201d\nhand, while the LLSD-only baseline can guide the editing effectively, it fails to generate fine details. It cannot adequately synthesize the zebra stripes in Example 2 and the tiger\u2019s face in Example 3. In contrast, our proposed method can generate more detailed results than the LLSD-only baseline, such as the feathers of the white swan in Example 1. Overall, our proposed method produces visually better results and richer details than the baselines."
        },
        {
            "heading": "4.2. Ablation",
            "text": "We conduct an ablation study in Fig. 7 to assess the effectiveness of the feature matching loss LFM and the KL regularizer LKL, along with the baseline latent score distillation loss LLSD in a simple image-synthesis task. The results demonstrate that incorporating LFM and LKL improved the overall image quality performance. Specifically, the feature matching loss LFM is able to bring more detail to the image, while the KL regularizer mitigates the color over-saturation problem. For more quantitative and qualitative results of ablation studies on the three applications, please refer to the supplementary material."
        },
        {
            "heading": "5. Implementation Details",
            "text": "In this section, we elaberate our implemantation detail for each application."
        },
        {
            "heading": "5.1. Text-to-3D.",
            "text": "We evaluate our method on two text-guided 3D generative models, Latent NeRF [25] and Jacobian NeRF [49]. We follow the original setup of using Stable Diffusion [40] v1.4 for Latent NeRF and Stable Diffusion v1.5 for Jacobian NeRF. We integrate their method with our LFM and KL regularizer. For Jacobian NeRF, we run experiments for\n10,000 iterations, and for Latent NeRF, we run experiments for 5,000 iterations with an additional 2,000 refined iterations, following the default setting. The objective after integrating our method will be L = \u03bb1LFM +\u03bb2LKL +\u03bb3LLSD, where \u03bb1 = 10\u22121,\u03bb2 = 10\u22121,\u03bb3 = 1.0 for Jacobian NeRF and \u03bb1 = 10\u22122,\u03bb2 = 10\u22121,\u03bb3 = 1.0 for Latent NeRF."
        },
        {
            "heading": "5.2. StyleGAN adaptation.",
            "text": "We implement our method on StyleGAN2 [15] with our feature matching loss LFM . Our method is built upon StyleGANFusion [45]. We add our LFM and KL regularizer. We run the experiments for 2,000 training iterations and a learning rate of 0.002. We generate 2,000 samples from the adapted generator after the training for the quantitative evaluations. The objective after integrating our method will be L = \u03bb1LFM +\u03bb2LKL+\u03bb3LLSD, where \u03bb1 = 3.0,\u03bb2 = 0,5\u00d710\u22121,\u03bb3 = 1.0."
        },
        {
            "heading": "5.3. Layered image editing",
            "text": "We follow the layered image editing approach of Text2LIVE [2]. As illustrated in Fig. 9, we first feed the input image I to the CNN generator of Text2LIVE to produce an initial editing layer and alpha map (I\u2032CNN ,\u03b1). The initial edited image can be synthesized by alpha blending I\u2032 = I\u2032CNN \u00b7\u03b1 + I \u00b7 (1\u2212\u03b1). We then input the initial edited image I\u2032 into the frozen encoder E\u03c6enc of the latent diffusion model [40] to obtain the initial latent v\u2032. We exploit an additional learnable residual latent \u03b8latent to add to the initial latent v\u2032\u2032 = v\u2032+\u03b8latent to obtain richer details. Lastly, the frozen decoder G\u03c6dec generates the final edited image I\u2032\u2032 from the latent v\u2032\u2032 followed by the same alpha blending. The parameter set {\u03b8CNN ,\u03b8latent} (i.e., the parameters of Text2LIVE\u2019s CNN generator and the additional residual latent) is trained by the diffusion prior, either the LLSD-only baseline or our proposed LFM + LKL + LLSD full loss. We compute the diffusion-guided losses on v\u2032\u2032 and I\u2032\u2032. In addition, to enhance the editing quality, we use an additional mask supervision loss on learning the alpha map \u03b1 . The final loss is L f inal = \u03bb1LFM +\u03bb2LKL +\u03bb3LLSD +\u2225\u03b1 \u2212M\u22251, where \u03bb1 = 10\u22125,\u03bb2 = 10\u22127,\u03bb3 = 10\u22126, and M is the mask obtained from MaskRCNN [8]."
        },
        {
            "heading": "5.4. Image Synthesis",
            "text": "We conduct an ablation study of the two primary components of our proposed method, the feature matching loss\nLFM and the KL regularizer LKL on a straightforward image synthesis task. We begin by initializing a 64\u00d7 64\u00d7 4 random noise map and then optimizing it with a combination of different losses for 1000 iterations. The final results are rendered from the decoder of the pretrained diffusion model. We use the AdamW optimizer with a learning rate of 10\u22121 for optimization. The objective of the full model is L = \u03bb1LFM + \u03bb2LKL + \u03bb3LLSD, where \u03bb1 = 3.0,\u03bb2 = 10\u22121,\u03bb3 = 1.0."
        },
        {
            "heading": "6. Limitations",
            "text": "In Text-to-3D task, the learned 3D objects sometimes encounter the multiple faces Janus problem (Fig. 8 first case), in which the model will appear with more than one front face. The problem also comes up in DreamFusion, which might be caused by less knowledge of 3D geometry in the diffusion model. Or as mentioned in DreamFusion and Latent NeRF, the text prompt often interprets from the canonical views and might not be a good condition for sampling other viewpoints. Besides, with the guidance of diffusion score distillation, we found that few cases would suffer from the issue of color over-saturation and out-of-focus despite the use of our KL loss LKL (Fig. 8 second and third case). The over-saturation issue can also be found in previous score distillation works [25, 35]."
        },
        {
            "heading": "7. Conclusions",
            "text": "In this paper, we proposed a framework that uses a diffusion model as a prior for visual synthesis tasks. Our core contributions are a feature-matching loss to extract detailed information and a KL loss for regularization. Through extensive experimental evaluations, we show that our method improves the quality compared to strong baselines on textto-3D, StyleGAN adaptation, and layered image editing tasks."
        }
    ],
    "title": "Text-driven Visual Synthesis with Latent Diffusion Prior",
    "year": 2023
}