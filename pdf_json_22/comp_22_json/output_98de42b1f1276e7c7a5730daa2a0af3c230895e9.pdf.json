{
    "abstractText": "A recommender system aims to understand the users\u2019 inclination towards the different items and provide better experiences by recommending candidate items for future interactions. These personalized recommendations can be of various forms, such as e-commerce products, pointsof-interest (POIs), music, social connections, etc. Traditional recommendation systems, such as content-based and collaborative filtering models, calculate the similarity between items and users and then recommending similar items to similar users. However, these approaches utilize the user-item interactions in a static way, i.e., without any time-evolving features. This assumption significantly limits their applicability in real-world settings, as a notable fraction of data generated via human activities can be represented as a sequence of events over a continuous time. These continuous-time event sequences or CTES 1 are pervasive across a wide range of domains such as online purchases, health records, spatial mobility, social networks, etc. Moreover, these sequences can implicitly represent the time-sensitive properties of events, the evolving relationships between events, and the temporal patterns within and across sequences. For example, (i) event sequences derived from the purchases records in e-commerce platforms can help in monitoring the users\u2019 evolving preferences towards products; and (ii) sequences derived from spatial mobility of users can help in identifying the geographical preferences of users, their check-in category interests, and the physical activity of the population within the spatial region. Therefore, we represent the user-item interactions as temporal sequences of discrete events, as understanding these patterns is essential to power accurate recommender systems. With the research directions described in this thesis, we seek to address the critical challenges in designing recommender systems that can understand the dynamics of continuous-time event sequences. We follow a ground-up approach, i.e., first, we address the problems that may arise due to the poor quality of CTES data being fed into a recommender system. Later, we handle the task of designing accurate recommender systems. To improve the quality of the CTES data, we address a fundamental problem of overcoming missing events in temporal sequences. Moreover, to provide accurate sequence modeling frameworks, we design solutions for points1We use the acronym CTES to denote a single and well as multiple continuous-time event sequences.",
    "authors": [
        {
            "affiliations": [],
            "name": "VINAYAK GUPTA"
        }
    ],
    "id": "SP:3d271aef5b8c431ee6da34fa94021934af5b43da",
    "references": [
        {
            "authors": [
                "Wang-Cheng Kang",
                "Julian McAuley"
            ],
            "title": "Self-Attentive Sequential Recommendation",
            "venue": "In ICDM",
            "year": 2018
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A Method for Stochastic Optimization",
            "venue": "In ICLR",
            "year": 2015
        },
        {
            "authors": [
                "Durk P Kingma",
                "Prafulla Dhariwal"
            ],
            "title": "Glow: Generative flow with invertible 1x1 convolutions",
            "venue": "NeurIPS",
            "year": 2018
        },
        {
            "authors": [
                "Durk P Kingma",
                "Tim Salimans",
                "Rafal Jozefowicz",
                "Xi Chen",
                "Ilya Sutskever",
                "Max Welling"
            ],
            "title": "Improved variational inference with inverse autoregressive flow",
            "venue": "NeurIPS",
            "year": 2016
        },
        {
            "authors": [
                "Thomas N. Kipf",
                "Max Welling"
            ],
            "title": "Semi-supervised classification with graph convolutional networks. In ICLR",
            "year": 2017
        },
        {
            "authors": [
                "Ivan Kobyzev",
                "Simon JD Prince",
                "Marcus A Brubaker"
            ],
            "title": "Normalizing flows: An introduction and review of current methods",
            "venue": "IEEE transactions on pattern analysis and machine intelligence 43,",
            "year": 2020
        },
        {
            "authors": [
                "Adit Krishnan",
                "Mahashweta Das",
                "Mangesh Bendre",
                "Hao Yang",
                "Hari Sundaram"
            ],
            "title": "Transfer Learning via Contextual Invariants for One-to-Many Cross-Domain Recommendation",
            "year": 2020
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Ilya Sutskever",
                "Geoffrey E Hinton"
            ],
            "title": "Imagenet classification with deep convolutional neural networks",
            "venue": "Commun. ACM 60,",
            "year": 2017
        },
        {
            "authors": [
                "H. Kuehne",
                "A.B. Arslan",
                "T. Serre"
            ],
            "title": "The Language of Actions: Recovering the Syntax and Semantics of Goal-Directed Human Activities",
            "year": 2014
        },
        {
            "authors": [
                "Srijan Kumar",
                "Xikun Zhang",
                "Jure Leskovec"
            ],
            "title": "Predicting dynamic embedding trajectory in temporal interaction",
            "year": 2019
        },
        {
            "authors": [
                "Matt Kusner",
                "Yu Sun",
                "Nicholas Kolkin",
                "Kilian Weinberger"
            ],
            "title": "From word embeddings to document distances",
            "year": 2015
        },
        {
            "authors": [
                "Tian Lan",
                "Tsung-Chuan Chen",
                "Silvio Savarese"
            ],
            "title": "A hierarchical representation for future action prediction",
            "year": 2014
        },
        {
            "authors": [
                "Hoyeop Lee",
                "Jinbae Im",
                "Seongwon Jang",
                "Hyunsouk Cho",
                "Sehee Chung"
            ],
            "title": "MeLU: Meta-Learned User Preference Estimator for Cold-Start Recommendation",
            "year": 2019
        },
        {
            "authors": [
                "Jaekoo Lee",
                "Hyunjae Kim",
                "Jongsun Lee",
                "Sungroh Yoon"
            ],
            "title": "Transfer Learning for Deep Learning on Graph-Structured Data",
            "year": 2017
        },
        {
            "authors": [
                "Junhyun Lee",
                "Inyeop Lee",
                "Jaewoo Kang"
            ],
            "title": "Self-attention graph pooling",
            "venue": "In ICML",
            "year": 2019
        },
        {
            "authors": [
                "Hui Li",
                "Ke Deng",
                "Jiangtao Cui",
                "Zhenhua Dong",
                "Jianfeng Ma",
                "Jianbin Huang"
            ],
            "title": "Hidden Community Identification in Location-Based Social Network via Probabilistic Venue Sequences",
            "venue": "Inf. Sci",
            "year": 2018
        },
        {
            "authors": [
                "Jiacheng Li",
                "Yujie Wang",
                "Julian McAuley"
            ],
            "title": "Time Interval Aware Self-Attention for Sequential Recommendation",
            "venue": "In WSDM",
            "year": 2020
        },
        {
            "authors": [
                "Pan Li",
                "Alexander Tuzhilin"
            ],
            "title": "DDTCDR: Deep Dual Transfer Cross Domain Recommendation",
            "venue": "In WSDM",
            "year": 2020
        },
        {
            "authors": [
                "Ruirui Li",
                "Jyunyu Jiang",
                "Chelsea Ju",
                "Wei Wang"
            ],
            "title": "CORALS: Who are My Potential New Customers? Tapping into the Wisdom of Customers Decisions",
            "venue": "In WSDM",
            "year": 2019
        },
        {
            "authors": [
                "Ranzhen Li",
                "Yanyan Shen",
                "Yanmin Zhu"
            ],
            "title": "Next point-of-interest recommendation with temporal and multi-level context attention",
            "venue": "In ICDM",
            "year": 2018
        },
        {
            "authors": [
                "Ruirui Li",
                "Xian Wu",
                "Xiusi Chen",
                "Wei Wang"
            ],
            "title": "Few-Shot Learning for New User Recommendation inLocation-based Social Networks",
            "year": 2020
        },
        {
            "authors": [
                "Ruirui Li",
                "Xian Wu",
                "Wei Wang"
            ],
            "title": "Adversarial Learning to Compare: Self-Attentive Prospective Customer Recommendation in Location based Social Networks. In WSDM",
            "year": 2020
        },
        {
            "authors": [
                "Shuang Li",
                "Shuai Xiao",
                "Shixiang Zhu",
                "Nan Du",
                "Yao Xie",
                "Le Song"
            ],
            "title": "Learning Temporal Point Processes via Reinforcement Learning",
            "venue": "NeurIPS",
            "year": 2018
        },
        {
            "authors": [
                "Tong Li",
                "Mingyang Zhang",
                "Hancheng Cao",
                "Yong Li",
                "Sasu Tarkoma",
                "Pan Hui"
            ],
            "title": "What Apps Did You Use?\": Understanding the Long-term Evolution of Mobile App Usage",
            "year": 2020
        },
        {
            "authors": [
                "Yaguang Li",
                "Rose Yu",
                "Cyrus Shahabi",
                "Yan Liu"
            ],
            "title": "Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting",
            "venue": "In ICLR",
            "year": 2018
        },
        {
            "authors": [
                "Youru Li",
                "Zhenfeng Zhu",
                "Deqiang Kong",
                "Hua Han",
                "Yao Zhao"
            ],
            "title": "EA-LSTM: Evolutionary Attention-based LSTM for Time Series Prediction",
            "year": 2018
        },
        {
            "authors": [
                "Defu Lian",
                "Yongji Wu",
                "Yong Ge",
                "Xing Xie",
                "Enhong Chen"
            ],
            "title": "Geography-Aware Sequential Location Recommendation",
            "year": 2020
        },
        {
            "authors": [
                "Ankita Likhyani",
                "Srikanta Bedathur",
                "Deepak P"
            ],
            "title": "LoCaTe: Influence Quantification for Location Promotion in Location-based Social Networks",
            "year": 2017
        },
        {
            "authors": [
                "Ankita Likhyani",
                "Vinayak Gupta",
                "PK Srijith",
                "P Deepak",
                "Srikanta Bedathur"
            ],
            "title": "Modeling Implicit Communities from Geo-tagged Event Traces using Spatio-Temporal Point Processes",
            "year": 2020
        },
        {
            "authors": [
                "Thomas Josef Liniger"
            ],
            "title": "Multivariate hawkes processes",
            "venue": "Ph. D. Dissertation. ETH Zurich",
            "year": 2009
        },
        {
            "authors": [
                "Roderick JA Little",
                "Donald B Rubin"
            ],
            "title": "Statistical analysis with missing data",
            "year": 2019
        },
        {
            "authors": [
                "Qiang Liu",
                "Shu Wu",
                "Liang Wang",
                "Tieniu Tan"
            ],
            "title": "Predicting the Next Location: A Recurrent Model with Spatial and Temporal Contexts",
            "year": 2016
        },
        {
            "authors": [
                "Lars Lorch",
                "Abir De",
                "Samir Bhatt",
                "William Trouleau",
                "Utkarsh Upadhyay",
                "Manuel Gomez-Rodriguez"
            ],
            "title": "Stochastic Optimal Control of Epidemic Processes in Networks",
            "year": 2018
        },
        {
            "authors": [
                "Yuanfu Lu",
                "Yuan Fang",
                "Chuan Shi"
            ],
            "title": "Meta-learning on Heterogeneous Information Networks for Cold-start Recommendation",
            "year": 2020
        },
        {
            "authors": [
                "Zhongqi Lu",
                "Zhicheng Dou",
                "Jianxun Lian",
                "Xing Xie",
                "Qiang Yang"
            ],
            "title": "Content-based collaborative filtering for news topic recommendation",
            "year": 2015
        },
        {
            "authors": [
                "Zheng Lu",
                "Yunhe Feng",
                "Wenjun Zhou",
                "Xiaolin Li",
                "Qing Cao"
            ],
            "title": "Inferring Correlation between User Mobility and App Usage in Massive Coarse-Grained Data Traces",
            "venue": "Proc. ACM Interact. Mob. Wearable Ubiquitous Technol",
            "year": 2018
        },
        {
            "authors": [
                "Chuxu Zhang Lu Yu",
                "Shangsong Liang",
                "Xiangliang Zhang"
            ],
            "title": "Multi-Order Attentive Ranking Model for Sequential Recommendation",
            "year": 2019
        },
        {
            "authors": [
                "Yonghong Luo",
                "Xiangrui Cai",
                "Ying Zhang",
                "Jun Xu",
                "Yuan Xiaojie"
            ],
            "title": "Multivariate Time Series Imputation with Generative Adversarial Networks",
            "venue": "NeurIPS",
            "year": 2018
        },
        {
            "authors": [
                "Yue Luo",
                "Sarah M Coppola",
                "Philippe C Dixon",
                "Song Li",
                "Jack T Dennerlein",
                "Boyi Hu"
            ],
            "title": "A database of human gait performance on irregular and uneven surfaces collected by wearable sensors",
            "venue": "Scientific Data",
            "year": 2020
        },
        {
            "authors": [
                "Yonghong Luo",
                "Ying Zhang",
                "Xiangrui Cai",
                "Xiaojie Yuan"
            ],
            "title": "E2GAN: End-to-End Generative Adversarial Network for Multivariate Time Series Imputation",
            "venue": "In IJCAI",
            "year": 2019
        },
        {
            "authors": [
                "Shugao Ma",
                "Leonid Sigal",
                "Stan Sclaroff"
            ],
            "title": "Learning Activity Progression in LSTMs for Activity Detection and Early Detection",
            "year": 2016
        },
        {
            "authors": [
                "Tahmida Mahmud",
                "Mahmudul Hasan",
                "Amit K. Roy-Chowdhury"
            ],
            "title": "Joint prediction of activity labels and starting times in untrimmed videos",
            "year": 2017
        },
        {
            "authors": [
                "Jarana Manotumruksa",
                "Craig Macdonald",
                "Iadh Ounis"
            ],
            "title": "A Contextual Attention Recurrent Architecture for Context-Aware Venue Recommendation",
            "year": 2018
        },
        {
            "authors": [
                "Jarana Manotumruksa",
                "Dimitrios Rafailidis",
                "Craig Macdonald",
                "Iadh Ounis"
            ],
            "title": "On Cross-Domain Transfer in Venue Recommendation",
            "venue": "In ECIR",
            "year": 2019
        },
        {
            "authors": [
                "Geoffrey McLachlan",
                "David Peel"
            ],
            "title": "Finite Mixture Models",
            "year": 2004
        },
        {
            "authors": [
                "Nazanin Mehrasa",
                "Akash Abdu Jyothi",
                "Thibaut Durand",
                "Jiawei He",
                "Leonid Sigal",
                "Greg Mori"
            ],
            "title": "A Variational Auto-Encoder Model for Stochastic Point Processes",
            "year": 2019
        },
        {
            "authors": [
                "Nazanin Mehrasa",
                "Yatao Zhong",
                "Frederick Tung",
                "Luke Bornn",
                "Greg Mori"
            ],
            "title": "Learning person trajectory representations for team activity analysis",
            "year": 2017
        },
        {
            "authors": [
                "Hongyuan Mei",
                "Jason M Eisner"
            ],
            "title": "The neural hawkes process: A neurally self-modulating multivariate point process",
            "venue": "NeurIPS",
            "year": 2017
        },
        {
            "authors": [
                "Hongyuan Mei",
                "Guanghui Qin",
                "Jason Eisner"
            ],
            "title": "Imputing Missing Events in Continuous-Time Event Streams",
            "year": 2019
        },
        {
            "authors": [
                "Denis Metev"
            ],
            "title": "41+ Must Know Foursquare Statistics in 2020",
            "year": 2021
        },
        {
            "authors": [
                "Tomas Mikolov",
                "Ilya Sutskever",
                "Kai Chen",
                "Greg S Corrado",
                "Jeff Dean"
            ],
            "title": "Distributed Representations of Words and Phrases and their Compositionality",
            "venue": "NeurIPS",
            "year": 2013
        },
        {
            "authors": [
                "Christopher Morris",
                "Martin Ritzert",
                "Matthias Fey",
                "William L Hamilton",
                "Jan Eric Lenssen",
                "Gaurav Rattan",
                "Martin Grohe"
            ],
            "title": "Weisfeiler and leman go neural: Higher-order graph neural networks",
            "year": 2019
        },
        {
            "authors": [
                "Abdullah Mueen",
                "Eamonn Keogh"
            ],
            "title": "Extracting optimal performance from dynamic time warping",
            "venue": "In KDD",
            "year": 2016
        },
        {
            "authors": [
                "Abdullah Mueen",
                "Yan Zhu",
                "Michael Yeh",
                "Kaveh Kamgar",
                "Krishnamurthy Viswanathan",
                "Chetan Gupta",
                "Eamonn Keogh"
            ],
            "title": "The Fastest Similarity Search Algorithm for Time Series Subsequences under Euclidean Distance. http://www.cs.unm.edu/ mueen/FastestSimilaritySearch.html",
            "year": 2017
        },
        {
            "authors": [
                "David Murray",
                "Lina Stankovic",
                "Vladimir Stankovic"
            ],
            "title": "An electrical load measurements dataset of United Kingdom households from a two-year longitudinal study",
            "venue": "Scientific data",
            "year": 2017
        },
        {
            "authors": [
                "A. Nagrani",
                "J.S. Chung",
                "A. Zisserman"
            ],
            "title": "VoxCeleb: a large-scale speaker identification dataset. In INTERSPEECH",
            "year": 2017
        },
        {
            "authors": [
                "Zahra Nazari",
                "Christophe Charbuillet",
                "Johan Pages",
                "Martin Laurent",
                "Denis Charrier",
                "Briana Vecchione",
                "Ben Carterette"
            ],
            "title": "Recommending podcasts for cold-start users based on music listening and taste",
            "year": 2020
        },
        {
            "authors": [
                "Jianmo Ni",
                "Jiacheng Li",
                "Julian McAuley"
            ],
            "title": "Justifying recommendations using distantly-labeled reviews and fine-grained aspects",
            "venue": "EMNLP-IJCNLP",
            "year": 2019
        },
        {
            "authors": [
                "Alex Nichol",
                "Joshua Achiam",
                "John Schulman"
            ],
            "title": "On First-Order Meta-Learning Algorithms",
            "venue": "arXiv preprint arXiv",
            "year": 2018
        },
        {
            "authors": [
                "Anastasios Noulas",
                "Salvatore Scellato",
                "Cecilia Mascolo",
                "Massimiliano Pontil"
            ],
            "title": "Exploiting semantic annotations for clustering geographic areas and users in location-based social networks",
            "year": 2011
        },
        {
            "authors": [
                "Y. Ogata"
            ],
            "title": "On Lewis\u2019 Simulation Method for Point Processes",
            "venue": "IEEE Trans. Inf. Theor. 27,",
            "year": 1981
        },
        {
            "authors": [
                "Adam J Oliner",
                "Anand P Iyer",
                "Ion Stoica",
                "Eemil Lagerspetz",
                "Sasu Tarkoma"
            ],
            "title": "Carat: Collaborative energy diagnosis for mobile devices. In SenSys",
            "year": 2013
        },
        {
            "authors": [
                "Takahiro Omi",
                "Naonori Ueda",
                "Kazuyuki Aihara"
            ],
            "title": "Fully Neural Network based Model for General Temporal Point Processes",
            "venue": "NeurIPS",
            "year": 2019
        },
        {
            "authors": [
                "Kun Ouyang",
                "Reza Shokri",
                "David S Rosenblum",
                "Wenzhuo Yang"
            ],
            "title": "Non-Parametric Generative Model for Human Trajectories",
            "year": 2018
        },
        {
            "authors": [
                "Sinno Jialin Pan",
                "Qiang Yang"
            ],
            "title": "A Survey on Transfer Learning",
            "venue": "IEEE Transactions on Knowledge and Data Engineering 22,",
            "year": 2010
        },
        {
            "authors": [
                "Zheyi Pan",
                "Yuxuan Liang",
                "Weifeng Wang",
                "Yong Yu",
                "Yu Zheng",
                "Junbo Zhang"
            ],
            "title": "Urban Traffic Prediction from Spatio-Temporal Data Using Deep Meta Learning",
            "year": 2019
        },
        {
            "authors": [
                "John Paparrizos",
                "Luis Gravano"
            ],
            "title": "k-shape: Efficient and accurate clustering of time series",
            "year": 2015
        },
        {
            "authors": [
                "Niki J. Parmar",
                "Ashish Vaswani",
                "Jakob Uszkoreit",
                "Lukasz Kaiser",
                "Noam Shazeer",
                "Alexander Ku",
                "Dustin Tran"
            ],
            "title": "Image Transformer",
            "year": 2018
        },
        {
            "authors": [
                "Jeffrey Pennington",
                "Richard Socher",
                "Christopher D. Manning"
            ],
            "title": "GloVe: Global Vectors for Word Representation",
            "year": 2014
        },
        {
            "authors": [
                "Zongyue Qin",
                "Yunsheng Bai",
                "Yizhou Sun"
            ],
            "title": "GHashing: Semantic Graph Hashing for Approximate Similarity Search in Graph Databases",
            "year": 2020
        },
        {
            "authors": [
                "Thanawin Rakthanmanon",
                "Bilson Campana",
                "Abdullah Mueen",
                "Gustavo Batista",
                "Brandon Westover",
                "Qiang Zhu",
                "Jesin Zakaria",
                "Eamonn Keogh"
            ],
            "title": "Searching and Mining Trillions of Time Series Subsequences under Dynamic Time Warping",
            "year": 2012
        },
        {
            "authors": [
                "Jakob Gulddahl Rasmussen"
            ],
            "title": "Bayesian Inference for Hawkes Processes",
            "venue": "Method. Comput. Appl. Prob. 15,",
            "year": 2013
        },
        {
            "authors": [
                "Jakob Gulddahl Rasmussen"
            ],
            "title": "Lecture Notes: Temporal Point Processes and the Conditional Intensity Function",
            "venue": "arXiv preprint arXiv:1806.00221",
            "year": 2018
        },
        {
            "authors": [
                "Steffen Rendle"
            ],
            "title": "Factorization machines. In ICDM",
            "year": 2010
        },
        {
            "authors": [
                "Steffen Rendle",
                "Christoph Freudenthaler",
                "Lars Schmidt-Thieme"
            ],
            "title": "Factorizing personalized Markov chains for next-basket recommendation",
            "year": 2010
        },
        {
            "authors": [
                "Danilo Rezende",
                "Shakir Mohamed"
            ],
            "title": "Variational inference with normalizing flows",
            "venue": "In ICML",
            "year": 2015
        },
        {
            "authors": [
                "Marian-Andrei Rizoiu",
                "Swapnil Mishra",
                "Quyu Kong",
                "Mark Carman",
                "Lexing Xie"
            ],
            "title": "SIR-Hawkes: on the Relationship Between Epidemic Models and Hawkes Point Processes",
            "year": 2018
        },
        {
            "authors": [
                "Marian-Andrei Rizoiu",
                "Lexing Xie",
                "Scott Sanner",
                "Manuel Cebrian",
                "Honglin Yu",
                "Pascal Van Hentenryck"
            ],
            "title": "Expecting to be hip: Hawkes intensity processes for social media popularity",
            "year": 2017
        },
        {
            "authors": [
                "Indradyumna Roy",
                "Abir De",
                "Soumen Chakrabarti"
            ],
            "title": "Adversarial Permutation Guided Node Representations for Link Prediction",
            "year": 2021
        },
        {
            "authors": [
                "David E Rumelhart",
                "Geoffrey E Hinton",
                "Ronald J Williams"
            ],
            "title": "Learning representations by backpropagating errors",
            "venue": "Nature 323,",
            "year": 1986
        },
        {
            "authors": [
                "M. Ryoo"
            ],
            "title": "Human activity prediction: Early recognition of ongoing activities from streaming videos",
            "year": 2011
        },
        {
            "authors": [
                "Salvatore Scellato",
                "Anastasios Noulas",
                "Cecilia Mascolo"
            ],
            "title": "Exploiting place features in link prediction on location-based social networks",
            "year": 2011
        },
        {
            "authors": [
                "Martin Sewell"
            ],
            "title": "The fisher kernel: a brief review",
            "venue": "RN 11,",
            "year": 2011
        },
        {
            "authors": [
                "Karishma Sharma",
                "Yizhou Zhang",
                "Emilio Ferrara",
                "Yan Liu"
            ],
            "title": "Identifying Coordinated Accounts on Social Media through Hidden Influence and Group Behaviours",
            "year": 2021
        },
        {
            "authors": [
                "Peter Shaw",
                "Jakob Uszkoreit",
                "Ashish Vaswani"
            ],
            "title": "Self-Attention with Relative Position Representations",
            "year": 2018
        },
        {
            "authors": [
                "Oleksandr Shchur",
                "Marin Bilo\u0161",
                "Stephan G\u00fcnnemann"
            ],
            "title": "Intensity-Free Learning of Temporal Point Processes",
            "venue": "In ICLR",
            "year": 2020
        },
        {
            "authors": [
                "Oleksandr Shchur",
                "Nicholas Gao",
                "Marin Bilo\u0161",
                "Stephan G\u00fcnnemann"
            ],
            "title": "Fast and Flexible Temporal Point Processes with Triangular Maps",
            "venue": "NeurIPS",
            "year": 2020
        },
        {
            "authors": [
                "Oleksandr Shchur",
                "Ali Caner T\u00fcrkmen",
                "Tim Januschowski",
                "Stephan G\u00fcnnemann"
            ],
            "title": "Neural Temporal Point Processes: A Review",
            "venue": "In IJCAI. Survey Track",
            "year": 2021
        },
        {
            "authors": [
                "Christian R. Shelton",
                "Zhen Qin",
                "Chandini Shetty"
            ],
            "title": "2018. Hawkes Process Inference with Missing Data",
            "year": 2018
        },
        {
            "authors": [
                "Yilin Shen",
                "Yanping Chen",
                "Eamonn Keogh",
                "Hongxia Jin"
            ],
            "title": "Accelerating time series searching with large uniform scaling",
            "year": 2018
        },
        {
            "authors": [
                "Ajit P Singh",
                "Geoffrey J Gordon"
            ],
            "title": "Relational learning via collective matrix factorization",
            "year": 2008
        },
        {
            "authors": [
                "Marek Smieja",
                "Lukasz Struski",
                "Jacek Tabor",
                "Bartosz Zielinski",
                "Przemyslaw Spurek"
            ],
            "title": "Processing of Missing Data by Neural Networks",
            "venue": "NeurIPS",
            "year": 2018
        },
        {
            "authors": [
                "Han Su",
                "Shuncheng Liu",
                "Bolong Zheng",
                "Xiaofang Zhou",
                "Kai Zheng"
            ],
            "title": "A survey of trajectory distance measures and performance evaluation",
            "venue": "The VLDB Journal 29,",
            "year": 2020
        },
        {
            "authors": [
                "Fei Sun",
                "Jun Liu",
                "Jian Wu",
                "Changhua Pei",
                "Xiao Lin",
                "Wenwu Ou",
                "Peng Jiang"
            ],
            "title": "BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer",
            "venue": "In CIKM",
            "year": 2019
        },
        {
            "authors": [
                "Richard S. Sutton",
                "Andrew G. Barto"
            ],
            "title": "Reinforcement Learning: An Introduction",
            "year": 2018
        },
        {
            "authors": [
                "Jiaxi Tang",
                "Ke Wang"
            ],
            "title": "Personalized top-n sequential recommendation via convolutional sequence embedding",
            "venue": "In WSDM",
            "year": 2018
        },
        {
            "authors": [
                "E. Triantafillou",
                "T. Zhu",
                "V. Dumoulin",
                "P. Lamblin",
                "U. Evci",
                "K. Xu",
                "R. Goroshin",
                "C. Gelada",
                "K. Swersky",
                "P. Manzagol",
                "H. Larochelle"
            ],
            "title": "Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples",
            "venue": "In ICLR",
            "year": 2020
        },
        {
            "authors": [
                "Zhen Tu",
                "Yali Fan",
                "Yong Li",
                "Xiang Chen",
                "Li Su",
                "Depeng Jin"
            ],
            "title": "From Fingerprint to Footprint: Cold-Start Location Recommendation by Learning User Interest from App Data",
            "venue": "Proc. ACM Interact. Mob. Wearable Ubiquitous Technol",
            "year": 2019
        },
        {
            "authors": [
                "Zhen Tu",
                "Runtong Li",
                "Yong Li",
                "Gang Wang",
                "Di Wu",
                "Pan Hui",
                "Li Su",
                "Depeng Jin"
            ],
            "title": "Your Apps Give You Away: Distinguishing Mobile Users by Their App Usage Fingerprints",
            "venue": "Proc. ACM Interact. Mob. Wearable Ubiquitous Technol",
            "year": 2018
        },
        {
            "authors": [
                "Ali Caner T\u00fcrkmen",
                "Yuyang Wang",
                "Alexander J Smola"
            ],
            "title": "Fastpoint: Scalable deep point processes. In ECML-PKDD",
            "year": 2020
        },
        {
            "authors": [
                "Utkarsh Upadhyay",
                "Abir De",
                "Manuel Gomez-Rodriguez"
            ],
            "title": "Reinforcement Learning of Marked Temporal Point Processes",
            "venue": "NeurIPS",
            "year": 2018
        },
        {
            "authors": [
                "Dmitry Ustalov",
                "Natalia Fedorova",
                "Nikita Pavlichenko"
            ],
            "title": "Improving Recommender Systems with Human-in-the-Loop",
            "venue": "In RecSys",
            "year": 2022
        },
        {
            "authors": [
                "Isabel Valera",
                "Manuel Gomez-Rodriguez",
                "Krishna Gummadi"
            ],
            "title": "Modeling Diffusion of Competing Products and Conventions in Social Media",
            "year": 2014
        },
        {
            "authors": [
                "Aaron Van den Oord",
                "Sander Dieleman",
                "Benjamin Schrauwen"
            ],
            "title": "Deep content-based music recommendation",
            "venue": "NeurIPS",
            "year": 2013
        },
        {
            "authors": [
                "Manasi Vartak",
                "Arvind Thiagarajan",
                "Conrado Miranda",
                "Jeshua Bratman",
                "Hugo Larochelle"
            ],
            "title": "A Meta-Learning Perspective on Cold-Start Recommendations for Items",
            "venue": "NeurIPS",
            "year": 2017
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "NeurIPS",
            "year": 2017
        },
        {
            "authors": [
                "Huandong Wang",
                "Yong Li",
                "Sihan Zeng",
                "Gang Wang",
                "Pengyu Zhang",
                "Pan Hui",
                "Depeng Jin"
            ],
            "title": "Modeling Spatio-Temporal App Usage for a Large User Population",
            "venue": "Proc. ACM Interact. Mob. Wearable Ubiquitous Technol",
            "year": 2019
        },
        {
            "authors": [
                "Leye Wang",
                "Xu Geng",
                "Xiaojuan Ma",
                "Feng Liu",
                "Qiang Yang"
            ],
            "title": "Cross-city transfer learning for deep spatio-temporal prediction",
            "year": 2019
        },
        {
            "authors": [
                "Pengfei Wang",
                "Yanjie Fu",
                "Guannan Liu",
                "Wenqing Hu",
                "Charu Aggarwal"
            ],
            "title": "Human Mobility Synchronization and Trip Purpose Detection with Mixture of Hawkes Processes",
            "year": 2017
        },
        {
            "authors": [
                "Xiang Wang",
                "Xiangnan He",
                "Liqiang Nie",
                "Tat-Seng Chua"
            ],
            "title": "Item silk road: Recommending items from information domains to social users",
            "year": 2017
        },
        {
            "authors": [
                "Xiang Wang",
                "Xiangnan He",
                "Meng Wang",
                "Fuli Feng",
                "Tat-Seng Chua"
            ],
            "title": "Neural Graph Collaborative Filtering",
            "year": 2019
        },
        {
            "authors": [
                "Xinxi Wang",
                "Ye Wang"
            ],
            "title": "Improving Content-Based and Hybrid Music Recommendation Using Deep Learning",
            "venue": "In MM",
            "year": 2014
        },
        {
            "authors": [
                "Zhu Wang",
                "Daqing Zhang",
                "Xingshe Zhou",
                "Dingqi Yang",
                "Zhiyong Yu",
                "Zhiwen Yu"
            ],
            "title": "Discovering and profiling overlapping communities in location-based social networks",
            "venue": "IEEE Transactions on Systems, Man, and Cybernetics: Systems 44,",
            "year": 2013
        },
        {
            "authors": [
                "Antoine Wehenkel",
                "Gilles Louppe"
            ],
            "title": "Unconstrained monotonic neural networks. In NeurIPS",
            "year": 2019
        },
        {
            "authors": [
                "Ying Wei",
                "Yu Zheng",
                "Qiang Yang"
            ],
            "title": "Transfer knowledge between cities",
            "venue": "In KDD",
            "year": 2016
        },
        {
            "authors": [
                "Chuhan Wu",
                "Fangzhao Wu",
                "Mingxiao An",
                "Jianqiang Huang",
                "Yongfeng Huang",
                "Xing Xie"
            ],
            "title": "NPA: Neural News Recommendation with Personalized Attention",
            "year": 2019
        },
        {
            "authors": [
                "Felix Wu",
                "Angela Fan",
                "Alexei Baevski",
                "Yann N. Dauphin",
                "Michael Auli"
            ],
            "title": "Pay Less Attention with Lightweight and Dynamic Convolutions",
            "venue": "In ICLR",
            "year": 2019
        },
        {
            "authors": [
                "Hao Wu",
                "Ziyang Chen",
                "Weiwei Sun",
                "Baihua Zheng",
                "Wei Wang"
            ],
            "title": "Modeling trajectories with recurrent neural networks",
            "year": 2017
        },
        {
            "authors": [
                "Le Wu",
                "Yong Ge",
                "Qi Liu",
                "Enhong Chen",
                "Bai Long",
                "Zhenya Huang"
            ],
            "title": "Modeling users preferences and social links in social networking services: A joint-evolving perspective",
            "year": 2016
        },
        {
            "authors": [
                "Qitian Wu",
                "Hengrui Zhang",
                "Xiaofeng Gao",
                "Peng He",
                "Paul Weng",
                "Han Gao",
                "Guihai Chen"
            ],
            "title": "Dual Graph Attention Networks for Deep Latent Representation of Multifaceted Social Effects in Recommender Systems",
            "year": 2019
        },
        {
            "authors": [
                "Xian Wu",
                "Baoxu Shi",
                "Yuxiao Dong",
                "Chao Huang",
                "Louis Faust",
                "Nitesh V Chawla"
            ],
            "title": "Restful: Resolution-aware forecasting of behavioral time series data",
            "venue": "In CIKM",
            "year": 2018
        },
        {
            "authors": [
                "Yongji Wu",
                "Defu Lian",
                "Shuowei Jin",
                "Enhong Chen"
            ],
            "title": "Graph Convolutional Networks on User Mobility Heterogeneous Graphs for Social Relationship Inference",
            "venue": "In IJCAI",
            "year": 2019
        },
        {
            "authors": [
                "Shuai Xiao",
                "Mehrdad Farajtabar",
                "Xiaojing Ye",
                "Junchi Yan",
                "Le Song",
                "Hongyuan Zha"
            ],
            "title": "Wasserstein Learning of Deep Generative Point Process Models. In NeurIPS",
            "year": 2017
        },
        {
            "authors": [
                "Shuai Xiao",
                "Junchi Yan",
                "Stephen Chu",
                "Xiaokang Yang",
                "Hongyuan Zha"
            ],
            "title": "Modeling the Intensity function of Point Process via Recurrent Neural Networks",
            "year": 2017
        },
        {
            "authors": [
                "Hongteng Xu",
                "Lawrence Carin",
                "Hongyuan Zha"
            ],
            "title": "Learning registered point processes from idiosyncratic observations",
            "year": 2018
        },
        {
            "authors": [
                "Hongteng Xu",
                "Dixin Luo",
                "Hongyuan Zha"
            ],
            "title": "Learning Hawkes processes from short doublycensored event sequences",
            "year": 2017
        },
        {
            "authors": [
                "Dingqi Yang",
                "Bingqing Qu",
                "Jie Yang",
                "Philippe Cudre-Mauroux"
            ],
            "title": "Revisiting User Mobility and Social Relationships in LBSNs: A Hypergraph Embedding Approach",
            "year": 2019
        },
        {
            "authors": [
                "Guolei Yang",
                "Ying Cai",
                "Chandan K. Reddy"
            ],
            "title": "Recurrent Spatio-Temporal Point Process for Checkin Time Prediction",
            "venue": "In CIKM",
            "year": 2018
        },
        {
            "authors": [
                "Huaxiu Yao",
                "Yiding Liu",
                "Ying Wei",
                "Xianfeng Tang",
                "Zhenhui Li"
            ],
            "title": "Learning from Multiple Cities: A Meta-Learning Approach for Spatial-Temporal Prediction",
            "year": 2019
        },
        {
            "authors": [
                "Huaxiu Yao",
                "Ying Wei",
                "Junzhou Huang",
                "Zhenhui Li"
            ],
            "title": "Hierarchically Structured Meta-learning",
            "venue": "In ICML",
            "year": 2019
        },
        {
            "authors": [
                "Huaxiu Yao",
                "Chuxu Zhang",
                "Ying Wei",
                "Meng Jiang",
                "Suhang Wang",
                "Junzhou Huang",
                "Nitesh V. Chawla"
            ],
            "title": "Graph Few-shot Learning via Knowledge Transfer",
            "year": 2020
        },
        {
            "authors": [
                "M. Yao",
                "S. Zhao",
                "S. Sahebi",
                "R. Feyzi Behnagh"
            ],
            "title": "Stimuli-Sensitive Hawkes Processes for Personalized Student Procrastination Modeling",
            "year": 2021
        },
        {
            "authors": [
                "Serena Yeung",
                "Olga Russakovsky",
                "Ning Jin",
                "Mykhaylo Andriluka",
                "Greg Mori",
                "Li Fei-Fei"
            ],
            "title": "Every moment counts: Dense detailed labeling of actions in complex videos",
            "venue": "International Journal of Computer Vision 126,",
            "year": 2018
        },
        {
            "authors": [
                "Rex Ying",
                "Ruining He",
                "Kaifeng Chen",
                "Pong Eksombatchai",
                "William L Hamilton",
                "Jure Leskovec"
            ],
            "title": "Graph convolutional neural networks for web-scale recommender systems",
            "year": 2018
        },
        {
            "authors": [
                "Rex Ying",
                "Jiaxuan You",
                "Christopher Morris",
                "Xiang Ren",
                "William L Hamilton",
                "Jure Leskovec"
            ],
            "title": "Hierarchical graph representation learning with differentiable pooling",
            "venue": "NeurIPS",
            "year": 2018
        },
        {
            "authors": [
                "Jinsung Yoon",
                "Daniel Jarrett",
                "Mihaela van der Schaar"
            ],
            "title": "Time-series Generative Adversarial Networks",
            "venue": "NeurIPS",
            "year": 2019
        },
        {
            "authors": [
                "Jinsung Yoon",
                "William R. Zame",
                "Mihaela van der Schaar"
            ],
            "title": "Estimating Missing Data in Temporal Data Streams Using Multi-Directional Recurrent Neural Networks",
            "venue": "IEEE Transactions on Biomedical Engineering",
            "year": 2019
        },
        {
            "authors": [
                "Donghan Yu",
                "Yong Li",
                "Fengli Xu",
                "Pengyu Zhang",
                "Vassilis Kostakos"
            ],
            "title": "Smartphone App Usage Prediction Using Points of Interest",
            "venue": "Proc. ACM Interact. Mob. Wearable Ubiquitous Technol",
            "year": 2018
        },
        {
            "authors": [
                "Quan Yuan",
                "Gao Cong",
                "Zongyang Ma",
                "Aixin Sun",
                "Nadia Magnenat-Thalmann"
            ],
            "title": "Time-aware point-of-interest recommendation",
            "year": 2013
        },
        {
            "authors": [
                "Sergey Zagoruyko",
                "Nikos Komodakis"
            ],
            "title": "Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer",
            "venue": "In ICLR",
            "year": 2017
        },
        {
            "authors": [
                "Dell Zhang",
                "Jun Wang",
                "Deng Cai",
                "Jinsong Lu"
            ],
            "title": "Self-taught hashing for fast similarity search",
            "year": 2010
        },
        {
            "authors": [
                "Qiang Zhang",
                "Aldo Lipani",
                "Omer Kirnap",
                "Emine Yilmaz"
            ],
            "title": "Self-attentive Hawkes processes",
            "venue": "In ICML",
            "year": 2020
        },
        {
            "authors": [
                "Yunchao Zhang",
                "Yanjie Fu",
                "Pengyang Wang",
                "Yu Zheng",
                "Xiaolin Li"
            ],
            "title": "Unifying Inter-Region Autocorrelations and Intra-Region Structure for Spatial Embedding via Collective Adversarial Learning",
            "year": 2019
        },
        {
            "authors": [
                "Pengpeng Zhao",
                "Haifeng Zhu",
                "Yanchi Liu",
                "Jiajie Xu",
                "Zhixu Li",
                "Fuzhen Zhuang",
                "Victor S Sheng",
                "Xiaofang Zhou"
            ],
            "title": "Where to go next: a spatio-temporal gated network for next poi recommendation",
            "year": 2019
        },
        {
            "authors": [
                "Qingyuan Zhao",
                "Murat A Erdogdu",
                "Hera Y He",
                "Anand Rajaraman",
                "Jure Leskovec"
            ],
            "title": "Seismic: A self-exciting point process model for predicting tweet popularity",
            "year": 2015
        },
        {
            "authors": [
                "Sha Zhao",
                "Zhiling Luo",
                "Ziwen Jiang",
                "Haiyan Wang",
                "Feng Xu",
                "Shijian Li",
                "Jianwei Yin",
                "Gang Pan"
            ],
            "title": "AppUsage2Vec: Modeling Smartphone App Usage for Prediction",
            "venue": "In ICDE",
            "year": 2019
        },
        {
            "authors": [
                "Yu Zheng"
            ],
            "title": "Location-Based Social Networks: Users",
            "venue": "In Computing with Spatial Trajectories",
            "year": 2011
        },
        {
            "authors": [
                "Chenyi Zhuang",
                "Qiang Ma"
            ],
            "title": "Dual Graph Convolutional Networks for Graph-Based SemiSupervised Classification",
            "year": 2018
        },
        {
            "authors": [
                "Jiancang Zhuang",
                "Ting Wang",
                "Koji Kiyosugi"
            ],
            "title": "Detection and replenishment of missing data in marked point processes",
            "venue": "Statistica Sinica 30,",
            "year": 2020
        },
        {
            "authors": [
                "Vinayak Gupta",
                "Abir De",
                "Sourangshu Bhattacharya",
                "Srikanta Bedathur"
            ],
            "title": "Learning Temporal Point Processes with Intermittent Observations",
            "venue": "In Proc. of the 24th International Conference on Artificial Intelligence and Statistics (AISTATS),",
            "year": 2021
        },
        {
            "authors": [
                "Vinayak Gupta",
                "Srikanta Bedathur"
            ],
            "title": "Region Invariant Normalizing Flows for Mobility Transfer",
            "venue": "In Proc. of the 30th ACM Intl. Conference on Information and Knowledge Management (CIKM),",
            "year": 2021
        },
        {
            "authors": [
                "Vinayak Gupta",
                "Abir De",
                "Srikanta Bedathur"
            ],
            "title": "Learning Temporal Point Processes for Efficient Retrieval of Continuous Time Event Sequences",
            "venue": "In Proc. of the 36th AAAI Conference on Artificial Intelligence (AAAI),",
            "year": 2022
        },
        {
            "authors": [
                "Vinayak Gupta",
                "Srikanta Bedathur"
            ],
            "title": "ProActive: Self-Attentive Temporal Point Process Flows for Activity Sequences",
            "venue": "In Proc. of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD),",
            "year": 2022
        },
        {
            "authors": [
                "Vinayak Gupta",
                "Srikanta Bedathur"
            ],
            "title": "Doing More with Less: Overcoming Data Scarcity for POI Recommendation via Cross-Region Transfer",
            "venue": "ACM Transactions on Intelligent Systems and Technology (ACM TIST),",
            "year": 2022
        },
        {
            "authors": [
                "Vinayak Gupta",
                "Abir De",
                "Sourangshu Bhattacharya",
                "Srikanta Bedathur"
            ],
            "title": "Modeling Continuous Time Sequences with Intermittent Observations using Marked Temporal Point Processes",
            "venue": "ACM Transactions on Intelligent Systems and Technology (ACM TIST),",
            "year": 2022
        },
        {
            "authors": [
                "Vinayak Gupta",
                "Srikanta Bedathur"
            ],
            "title": "Modeling Spatial Trajectories using CoarseGrained Smartphone Logs",
            "venue": "IEEE Transactions on Big Data (IEEE TBD),",
            "year": 2022
        },
        {
            "authors": [
                "Ankita Likhyani",
                "Vinayak Gupta",
                "P.K. Srijith",
                "Deepak",
                "Srikanta Bedathur"
            ],
            "title": "Modeling Implicit Communities from Geo-tagged Event Traces using Spatio-Temporal Point Processes",
            "venue": "In Proc. of the 21st International Conference on Web Information Systems Engineering (WISE),",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "MODELING TIME-SERIES AND SPATIAL",
            "text": ""
        },
        {
            "heading": "DATA FOR RECOMMENDATIONS AND",
            "text": ""
        },
        {
            "heading": "OTHER APPLICATIONS",
            "text": ""
        },
        {
            "heading": "VINAYAK GUPTA",
            "text": ""
        },
        {
            "heading": "DEPARTMENT OF COMPUTER SCIENCE & ENGINEERING",
            "text": "INDIAN INSTITUTE OF TECHNOLOGY DELHI\n2022\nar X\niv :2\n21 2.\n13 25\n9v 1\n[ cs\n.I R\n] 2\n5 D\nec 2\n02 2\n\u00a9Vinayak Gupta - 2022\nAll rights reserved."
        },
        {
            "heading": "MODELING TIME-SERIES AND SPATIAL",
            "text": ""
        },
        {
            "heading": "DATA FOR RECOMMENDATIONS AND",
            "text": "OTHER APPLICATIONS\nby"
        },
        {
            "heading": "VINAYAK GUPTA",
            "text": "Department of Computer Science and Engineering\nSubmitted\nin fulfillment of the requirements of the degree of Doctor of Philosophy\nto the\nIndian Institute of Technology Delhi 2022"
        },
        {
            "heading": "Certificate",
            "text": "This is to certify that the thesis titled MODELING TIME-SERIES AND SPATIAL DATA FOR RECOMMENDATIONS AND OTHER APPLICATIONS being submitted by Mr. VINAYAK GUPTA for the award of Doctor of Philosophy in Computer Science and Engineering is a record of bona fide work carried out by him under my guidance and supervision at the Department of Computer Science and Engineering, Indian Institute of Technology Delhi. The work presented in this thesis has not been submitted elsewhere, either in part or full, for the award of any other degree or diploma.\nSrikanta Bedathur Associate Professor\nDepartment of Computer Science and Engineering Indian Institute of Technology Delhi\nNew Delhi- 110016\ni"
        },
        {
            "heading": "Acknowledgements",
            "text": "I am highly indebted to my guide, Prof. Srikanta Bedathur, a brilliant advisor and an invaluable friend over the years. His \u2018what are we trying to achieve here\u2019 approach to all aspects of research and life will continue to inspire me. I will greatly cherish our discussions on topics ranging from sophisticated neural models trained on tens of GPUs to casual gigs on academic life. I am beyond belief grateful that he took me under his guidance and withstanding my never-ending tantrums around \u2018what am I here?\u2019. This thesis is a tiny aspect of the immense knowledge I have gained by working under him.\nA special thanks must go to Prof. Abir De for his invaluable encouragement and constant update meetings that always motivated me to work on my projects. Without his support, a significant part of the research presented in this thesis would have never begun. Never had I thought that a small interaction at CODS-COMAD 2019 would result in a collaboration of many years and multiple top-tier publications. I am also grateful to my research committee members Prof. Parag Singla and Prof. Rahul Garg, and Dr. L. V. Subramaniam, for their critical evaluation and suggestions that helped me shape most of the work in this thesis.\nSpecial shout-outs to my SIT 309 gang \u2013 Dishant Goyal, Sandeep Kumar, Omais Shafi, Dilpreet Kaur, Ovia Seshadri, and Arindam Bhattacharya for their constant support via poker nights, cricket games, food hunting trips, and innumerable coffee breaks. To Dishant, I would like to know if I\u2019ll ever be able to pay him back for being my go-to guy for many years. I am also thankful to have immense support from Apala Shankar Garg and Garima Gaur during my degree\u2019s initial and final phases, respectively. I\u2019m also in massive debt to my friends from my IIIT days, specially Vijendra Singh, Ayush Srivastava, Mayur Mishra, Avashesh Singh, Ayushi Jain, and Ovais Malik, for enduring my never-ending cribs about grad school. Lastly, I thank my parents, my brother Kartik, and my cousins Surabhi and Ketan, for standing by me.\nI am the author of this thesis, but all of them are surely the authors of me.\nVinayak Gupta\niii"
        },
        {
            "heading": "Abstract",
            "text": "A recommender system aims to understand the users\u2019 inclination towards the different items and provide better experiences by recommending candidate items for future interactions. These personalized recommendations can be of various forms, such as e-commerce products, pointsof-interest (POIs), music, social connections, etc. Traditional recommendation systems, such as content-based and collaborative filtering models, calculate the similarity between items and users and then recommending similar items to similar users. However, these approaches utilize the user-item interactions in a static way, i.e., without any time-evolving features. This assumption significantly limits their applicability in real-world settings, as a notable fraction of data generated via human activities can be represented as a sequence of events over a continuous time. These continuous-time event sequences or CTES 1 are pervasive across a wide range of domains such as online purchases, health records, spatial mobility, social networks, etc. Moreover, these sequences can implicitly represent the time-sensitive properties of events, the evolving relationships between events, and the temporal patterns within and across sequences. For example, (i) event sequences derived from the purchases records in e-commerce platforms can help in monitoring the users\u2019 evolving preferences towards products; and (ii) sequences derived from spatial mobility of users can help in identifying the geographical preferences of users, their check-in category interests, and the physical activity of the population within the spatial region. Therefore, we represent the user-item interactions as temporal sequences of discrete events, as understanding these patterns is essential to power accurate recommender systems.\nWith the research directions described in this thesis, we seek to address the critical challenges in designing recommender systems that can understand the dynamics of continuous-time event sequences. We follow a ground-up approach, i.e., first, we address the problems that may arise due to the poor quality of CTES data being fed into a recommender system. Later, we handle the task of designing accurate recommender systems. To improve the quality of the CTES data, we address a fundamental problem of overcoming missing events in temporal sequences. Moreover, to provide accurate sequence modeling frameworks, we design solutions for points-\n1We use the acronym CTES to denote a single and well as multiple continuous-time event sequences.\nv\nof-interest recommendation, i.e., models that can handle spatial mobility data of users to various POI check-ins and recommend candidate locations for the next check-in. Lastly, we highlight that the capabilities of the proposed models can have applications beyond recommender systems, and we extend their abilities to design solutions for large-scale CTES retrieval and human activity prediction.\nTo summarize, this thesis includes three directions: (i) Temporal Sequences with Missing Events; (ii) Recommendation in Spatio-Temporal Settings; and (iii) Applications of Modeling Temporal Sequences. In the first part, we present an unsupervised model and inference method for learning neural sequence models in the presence of CTES with missing events. This framework has many downstream applications, such as imputing missing events and forecasting future events. In the second part, we design point-of-interest recommender systems that utilize the geographical features associated with the spatial check-ins to recommend future locations to a user. Here, we propose solutions for static POI recommendation, sequential recommendations, and recommendations models that utilize the similarity between physical mobility and the smartphone activities of users. Lastly, in the third part, we highlight the strengths of the proposed frameworks to design solutions for two tasks: (i) retrieval systems, i.e., retrieving relevance sequences for a given query CTES from a large corpus of CTES data; and (ii) understanding that different users take different times to perform similar actions in activity videos. Moreover, in each chapter, we highlight the drawbacks of current deep learning-based models, design better sequence modeling frameworks, and experimentally underline the efficacy of our proposed solutions over the state-of-the-art baselines. Lastly, we report the drawbacks and the possible extensions for every solution proposed in this thesis.\nA significant part of this thesis uses the idea of modeling the underlying distribution of CTES via neural marked temporal point processes (MTPP). Traditional MTPP models are stochastic processes that utilize a fixed formulation to capture the generative mechanism of a sequence of discrete events localized in continuous time. In contrast, neural MTPP combine the underlying ideas from the point process literature with modern deep learning architectures. The ability of deep-learning models as accurate function approximators has led to a significant gain in the predictive prowess of neural MTPP models. In this thesis, we utilize and present several neural network-based enhancements for the current MTPP frameworks for the aforementioned real-world applications.\n\u0938\u093e\u0930\n\u090f\u0915 \u0905\u0928\u0936\u0941\u0938\u0902\u093e\u0915\u0924\u093e \u0923\u093e\u0932 \u0915\u093e \u0909 \u0926\u0947 \u092f \u0935 \u092d \u0928 \u0935 \u0924\u0913\u0941\u0902 \u0915\u0947 \u0924 \u0909\u092a\u092f\u094b\u0917\u0915\u0924\u093e \u0913\u0902 \u0915\u0947 \u091d\u0915\u0941\u093e\u0935 \u0915\u094b \u0938\u092e\u091d\u0928\u093e \u0914\u0930 \u092d \u0935 \u092f \u0915 \u092c\u093e\u0924\u091a\u0940\u0924 \u0915\u0947 \u0932\u090f \u0909 \u092e\u0940\u0926\u0935\u093e\u0930 \u0935 \u0924\u0913\u0941\u0902 \u0915 \u0938\u092b\u093e \u0930\u0936 \u0915\u0930\u0915\u0947 \u092c\u0947\u0939\u0924\u0930 \u0905\u0928\u092d\u0941\u0935 \u0926\u093e\u0928 \u0915\u0930\u0928\u093e \u0939\u0948\u0964 \u092f\u0947 \u0935\u092f\u0948\u093f \u0924\u0915\u0943\u0924 \u0905\u0928\u0936\u0941\u0938\u0902\u093e\u090f\u0901 \u0935 \u092d \u0928 \u092a \u0915 \u0939\u094b \u0938\u0915\u0924\u0940 \u0939 , \u091c\u0938\u0947\u0948 \u0908-\u0915\u0949\u092e\u0938 \u0909 \u092a\u093e\u0926, \u091a \u0915\u0947 \u092c\u0926\u0902\u0941 (\u092a\u0940\u0913\u0906\u0908), \u0938\u0917\u0902\u0940\u0924, \u0938\u093e\u092e\u093e\u093f\u091c\u0915 \u0938\u092a\u0902\u0915 \u0906 \u0926\u0964 \u092a\u093e\u0930\u0902\u092a \u0930\u0915 \u0905\u0928\u0936\u0941\u0938\u0902\u093e \u0923\u093e\u0932 , \u091c\u0938\u0947\u0948 \u0938\u093e\u092e \u0940-\u0906\u0927\u093e \u0930\u0924 \u0914\u0930 \u0938\u0939\u092f\u094b\u0917\u0940 \u095e \u091f \u0930\u0917\u0902 \u092e\u0949\u0921\u0932, \u0935 \u0924\u0913\u0941\u0902 \u0914\u0930 \u0935 \u0924\u0913\u0941\u0902 \u0915\u0947 \u092c\u0940\u091a \u0938\u092e\u093e\u0928\u0924\u093e \u0915 \u0917\u0923\u0928\u093e \u0915\u0930\u0924\u0947 \u0939 \u0964 \u0909\u092a\u092f\u094b\u0917\u0915\u0924\u093e \u0913\u0902 \u0914\u0930 \u092b\u0930 \u0938\u092e\u093e\u0928 \u0909\u092a\u092f\u094b\u0917\u0915\u0924\u093e \u0913\u0902 \u0915\u094b \u0938\u092e\u093e\u0928 \u0935 \u0924\u0913\u0941\u0902 \u0915 \u0938\u092b\u093e \u0930\u0936 \u0915\u0930\u0928\u093e\u0964 \u0939\u093e\u0932\u093e\u0901 \u0915, \u092f\u0947 \u093f \u091f\u0915\u094b\u0923 \u0909\u092a\u092f\u094b\u0917\u0915\u0924\u093e -\u0906\u0907\u091f\u092e \u0907\u0902\u091f\u0930\u0948 \u0936\u0928 \u0915\u094b \u093f \u0925\u0930 \u0924\u0930 \u0915\u0947 \u0938\u0947 \u0909\u092a\u092f\u094b\u0917 \u0915\u0930\u0924\u0947 \u0939 , \u0905\u0925\u093e \u0924, \u092c\u0928\u093e \u0915\u0938\u0940 \u0938\u092e\u092f- \u0935\u0915 \u0938\u0924 \u0938\u0941\u0935\u0927\u093e\u0913\u0902 \u0915\u0947\u0964 \u092f\u0939 \u0927\u093e\u0930\u0923\u093e \u0935\u093e \u0924 \u0935\u0915 \u0926\u0941\u0928\u092f\u093e \u0915 \u0938\u0947 \u091f\u0902\u0938 \u092e \u0909\u0928\u0915 \u092f\u094b \u092f\u0924\u093e \u0915\u094b \u092e\u0939 \u0935\u092a\u0923\u0942 \u092a \u0938\u0947 \u0938\u0940 \u092e\u0924 \u0915\u0930\u0924\u0940 \u0939\u0948, \u092f \u0915 \u092e\u093e\u0928\u0935 \u0917 \u0924 \u0935 \u0927\u092f \u0915\u0947 \u092e\u093e \u092f\u092e \u0938\u0947 \u0909 \u092a \u0928 \u0921\u091f\u0947\u093e \u0915\u093e \u090f\u0915 \u0909 \u0932\u0947\u0916\u0928\u0940\u092f \u0905\u0936\u0902 \u0928\u0930\u0902\u0924\u0930 \u0938\u092e\u092f \u092e \u0918\u091f\u0928\u093e\u0913\u0902 \u0915\u0947 \u0905\u0928\u0941\u092e \u0915\u0947 \u092a \u092e \u0926 \u0936 \u0924 \u0915\u092f\u093e \u091c\u093e \u0938\u0915\u0924\u093e \u0939\u0948\u0964 \u092f\u0947 \u0928\u0930\u0902\u0924\u0930-\u0938\u092e\u092f \u0918\u091f\u0928\u093e \u0905\u0928\u0941\u092e \u092f\u093e \u0938\u0940\u091f \u0908\u090f\u09381 \u0911\u0928\u0932\u093e\u0907\u0928 \u0916\u0930 \u0926, \u0935\u093e \u092f \u0930\u0915\u0949\u0921 , \u0925\u093e \u0928\u0915 \u0917 \u0924\u0936\u0940\u0932\u0924\u093e, \u0938\u093e\u092e\u093e\u093f\u091c\u0915 \u0928\u0947\u091f\u0935\u0915 \u0907 \u092f\u093e \u0926 \u091c\u0938\u0947\u0948 \u0921\u094b\u092e\u0947\u0928 \u0915 \u090f\u0915 \u0935 \u0924\u0924\u0943 \u0943\u0916\u0902\u0932\u093e \u092e \u092f\u093e\u092a\u0915 \u0939 \u0964 \u0907\u0938\u0915\u0947 \u0905\u0932\u093e\u0935\u093e, \u092f\u0947 \u0905\u0928\u0941\u092e \u0918\u091f\u0928\u093e\u0913\u0902 \u0915\u0947 \u0938\u092e\u092f-\u0938\u0935\u0947\u0902\u0926 \u0917\u0923\u0941 \u0915\u093e \u0924 \u0928 \u0927 \u0935 \u0915\u0930 \u0938\u0915\u0924\u0947 \u0939 , \u0935\u0915 \u0938\u0924 \u0918\u091f\u0928\u093e\u0913\u0902 \u0915\u0947 \u092c\u0940\u091a \u0938\u092c\u0902\u0927\u0902, \u0914\u0930 \u0905\u0928\u0941\u092e \u0915\u0947 \u092d\u0940\u0924\u0930 \u0914\u0930 \u092a\u093e\u0930 \u0932\u094c \u0915\u0915 \u092a\u091f\u0948\u0928 \u0964 \u0909\u0926\u093e\u0939\u0930\u0923 \u0915\u0947 \u0932\u090f, (i) \u0908-\u0915\u0949\u092e\u0938 \u0932\u0947\u091f\u092b\u0949\u092e \u092e \u0916\u0930 \u0926\u093e\u0930 \u0915\u0947 \u0930\u0915\u0949\u0921 \u0938\u0947 \u093e \u0924 \u0918\u091f\u0928\u093e \u092e \u0938\u0947 \u0909 \u092a\u093e\u0926 \u0915\u0947 \u0924 \u0909\u092a\u092f\u094b\u0917\u0915\u0924\u093e \u0913\u0902 \u0915 \u092c\u095d\u0924\u0940 \u093e\u0925 \u092e\u0915\u0924\u093e\u0913\u0902 \u0915 \u0928\u0917\u0930\u093e\u0928\u0940 \u092e \u092e\u0926\u0926 \u092e\u0932\u0947\u0917\u0940; \u0914\u0930 (ii) \u0909\u092a\u092f\u094b\u0917\u0915\u0924\u093e \u0913\u0902 \u0915 \u0925\u093e \u0928\u0915 \u0917 \u0924\u0936\u0940\u0932\u0924\u093e \u0938\u0947 \u093e \u0924 \u0905\u0928\u0941\u092e \u0909\u092a\u092f\u094b\u0917\u0915\u0924\u093e \u0913\u0902 \u0915 \u092d\u094c\u0917\u094b \u0932\u0915 \u093e\u0925 \u092e\u0915\u0924\u093e\u0913,\u0902 \u0909\u0928\u0915\u0947 \u091a\u0947\u0915-\u0907\u0928 \u0947\u0923\u0940 \u0915\u0947 \u0939\u0924 \u0914\u0930 \u0925\u093e \u0928\u0915 \u0947 \u0915\u0947 \u092d\u0940\u0924\u0930 \u091c\u0928\u0938\u0902\u092f\u093e \u0915 \u092d\u094c \u0924\u0915 \u0917 \u0924 \u0935 \u0927 \u0915 \u092a\u0939\u091a\u093e\u0928 \u0915\u0930\u0928\u0947 \u092e \u092e\u0926\u0926 \u0915\u0930\u0947\u0917\u093e\u0964 \u0907\u0938 \u0932\u090f, \u0939\u092e \u0905\u0938\u0924\u0924 \u0918\u091f\u0928\u093e\u0913\u0902 \u0915\u0947 \u0905 \u0925\u093e\u092f\u0940 \u0905\u0928\u0941\u092e \u0915\u0947 \u092a \u092e \u0909\u092a\u092f\u094b\u0917\u0915\u0924\u093e -\u0906\u0907\u091f\u092e \u0907\u0902\u091f\u0930\u0948 \u0936\u0928 \u0915\u093e \u0924 \u0928 \u0927 \u0935 \u0915\u0930\u0924\u0947 \u0939 , \u092f \u0915 \u0907\u0928 \u092a\u091f\u0948\u0928 \u0915\u094b \u0938\u092e\u091d\u0928\u093e \u0938\u091f \u0915 \u0905\u0928\u0936\u0941\u0938\u0902\u093e\u0915\u0924\u093e \u0938 \u091f\u092e \u0915\u094b \u0936\u093f \u0924 \u0926\u0947\u0928\u0947 \u0915\u0947 \u0932\u090f \u0906\u0935 \u092f\u0915 \u0939\u0948\u0964\n\u0907\u0938 \u0925\u0940 \u0938\u0938 \u092e \u0935 \u0923 \u0924 \u0905\u0928\u0938\u0941\u0927\u0902\u093e\u0928 \u0926\u0936\u093e\u0913\u0902 \u0915\u0947 \u0938\u093e\u0925, \u0939\u092e \u0905\u0928\u0936\u0941\u0938\u0902\u093e\u0915\u0924\u093e \u0923\u093e \u0932\u092f \u0915\u094b \u0921\u091c\u093e\u0907\u0928 \u0915\u0930\u0928\u0947 \u092e \u092e\u0939 \u0935\u092a\u0923\u0942 \u091a\u0928\u0941\u094c \u0924\u092f \u0915\u093e \u0938\u092e\u093e\u0927\u093e\u0928 \u0915\u0930\u0928\u093e \u091a\u093e\u0939\u0924\u0947 \u0939 \u091c\u094b \u0928\u0930\u0902\u0924\u0930-\u0938\u092e\u092f \u0915 \u0918\u091f\u0928\u093e\u0913\u0902 \u0915\u0947 \u0905\u0928\u0941\u092e \u0915 \u0917 \u0924\u0936\u0940\u0932\u0924\u093e \u0915\u094b \u0938\u092e\u091d \u0938\u0915\u0924\u0947 \u0939 \u0964 \u0939\u092e \u093e\u0909\u0902\u0921-\u0905\u092a \u093f \u091f\u0915\u094b\u0923 \u0915\u093e \u092a\u093e\u0932\u0928 \u0915\u0930\u0924\u0947 \u0939 , \u092f\u093e\u0928\u0940, \u092a\u0939\u0932\u0947, \u0939\u092e \u0909\u0928 \u0938\u092e \u092f\u093e\u0913\u0902 \u0915\u093e \u0938\u092e\u093e\u0927\u093e\u0928 \u0915\u0930\u0924\u0947 \u0939 \u091c\u094b \u0938\u0940\u091f \u0908\u090f\u0938 \u0921\u091f\u0947\u093e \u0915 \u0916\u0930\u093e\u092c \u0917\u0923\u0941\u0935 \u093e \u0915\u0947 \u0915\u093e\u0930\u0923 \u0909 \u092a \u0928 \u0939\u094b \u0938\u0915\u0924\u0940 \u0939 , \u091c\u094b \u090f\u0915 \u0905\u0928\u0936\u0941\u0938\u0902\u093e\u0915\u0924\u093e \u0923\u093e\u0932 \u092e \u0926\u091c \u0915 \u091c\u093e\u0924\u0940 \u0939 \u0964 \u092c\u093e\u0926 \u092e , \u0939\u092e \u0938\u091f \u0915 \u0905\u0928\u0936\u0941\u0938\u0902\u093e \u0923\u093e\u0932 \u0915\u094b \u0921\u091c\u093e\u0907\u0928 \u0915\u0930\u0928\u0947 \u0915\u093e \u0915\u093e\u092f \u0938\u092d\u0902\u093e\u0932\u0924\u0947 \u0939 \u0964 \u0938\u0940\u091f \u0908\u090f\u0938 \u0921\u091f\u0947\u093e \u0915 \u0917\u0923\u0941\u0935 \u093e \u092e \u0938\u0927\u0941\u093e\u0930 \u0915\u0930\u0928\u0947 \u0915\u0947 \u0932\u090f, \u0939\u092e \u0932\u094c \u0915\u0915 \u0905\u0928\u0941\u092e \u092e \u0932\u093e\u092a\u0924\u093e \u0918\u091f\u0928\u093e\u0913\u0902 \u092a\u0930 \u0915\u093e\u092c\u0942 \u092a\u093e\u0928\u0947 \u0915 \u092e\u0932\u0942\u092d\u0924\u0942 \u0938\u092e \u092f\u093e \u0915\u093e \u0938\u092e\u093e\u0927\u093e\u0928 \u0915\u0930\u0924\u0947 \u0939 \u0964 \u0907\u0938\u0915\u0947 \u0905\u0932\u093e\u0935\u093e, \u0938\u091f \u0915 \u0905\u0928\u0941\u092e \u092e\u0949\u0921 \u0932\u0917\u0902 \u0922\u093e\u0902\u091a\u093e \u0926\u093e\u0928 \u0915\u0930\u0928\u0947 \u0915\u0947 \u0932\u090f, \u0939\u092e \u092f\u093e\u091c \u0915\u0947 \u092c\u0926\u0902\u0913\u0941\u0902 \u0915\u0947 \u0932\u090f \u0938\u092e\u093e\u0927\u093e\u0928 \u0924\u092f\u0948\u093e\u0930 \u0915\u0930\u0924\u0947 \u0939 , \u092f\u093e\u0928\u0940 \u092e\u0949\u0921\u0932 \u091c\u094b \u0935 \u092d \u0928 \u092a\u0940\u0913\u0906\u0908 \u091a\u0947\u0915-\u0907\u0928 \u0915\u0947 \u0932\u090f \u0909\u092a\u092f\u094b\u0917\u0915\u0924\u093e \u0913\u0902 \u0915\u0947 \u0925\u093e \u0928\u0915 \u0917 \u0924\u0936\u0940\u0932\u0924\u093e \u0921\u091f\u0947\u093e \u0915\u094b \u0938\u092d\u0902\u093e\u0932 \u0938\u0915\u0924 \u0947\u0939 \u0914\u0930 \u0905\u0917\u0932\u0947 \u091a\u0947\u0915-\u0907\u0928 \u0915\u0947 \u0932\u090f \u0909 \u092e\u0940\u0926\u0935\u093e\u0930 \u0925\u093e\u0928 \u0915 \u0938\u092b\u093e \u0930\u0936 \u0915\u0930 \u0938\u0915\u0924 \u0947\u0939 \u0964\n1\u0939\u092e \u092a \u0930\u0935\u0923 \u0936 \u0926 CTES \u0915\u093e \u0909\u092a\u092f\u094b\u0917 \u090f\u0915\u0932 \u0914\u0930 \u0905 \u091b \u0924\u0930\u0939 \u0938\u0947 \u090f\u0915\u093e \u0927\u0915 \u0928\u0930\u0902\u0924\u0930-\u0938\u092e\u092f \u0918\u091f\u0928\u093e \u0905\u0928\u0941\u092e \u0915\u094b \u0928 \u092a\u0924 \u0915\u0930\u0928\u0947 \u0915\u0947 \u0932\u090f \u0915\u0930\u0924\u0947 \u0939 \u0964\n\u0905\u0924\u0902 \u092e , \u0939\u092e \u0907\u0938 \u092c\u093e\u0924 \u092a\u0930 \u0915\u093e\u0936 \u0921\u093e\u0932\u0924\u0947 \u0939 \u0915 \u0924\u093e \u0935\u0924 \u092e\u0949\u0921\u0932 \u0915 \u092e\u0924\u093e\u0913\u0902 \u092e \u0905\u0928\u0936\u0941\u0938\u0902\u093e\u0915\u0924\u093e \u0923\u093e \u0932\u092f \u0938\u0947 \u092a\u0930\u0947 \u0905\u0928\u0941\u092f\u094b\u0917 \u0939\u094b \u0938\u0915\u0924\u0947 \u0939 , \u0914\u0930 \u0939\u092e \u092c\u095c\u0947 \u092a\u092e\u0948\u093e\u0928\u0947 \u092a\u0930 \u0938\u0940\u091f \u0908\u090f\u0938 \u092a\u0928\u0941 \u093e \u093f \u0924 \u0914\u0930 \u092e\u093e\u0928\u0935 \u0917 \u0924 \u0935 \u0927 \u0915 \u092d \u0935 \u092f\u0935\u093e\u0923\u0940 \u0915\u0947 \u0932\u090f \u0938\u092e\u093e\u0927\u093e\u0928 \u0924\u092f\u0948\u093e\u0930 \u0915\u0930\u0928\u0947 \u0915\u0947 \u0932\u090f \u0909\u0928\u0915 \u092e\u0924\u093e\u0913 \u0902\u0915\u093e \u0935 \u0924\u093e\u0930 \u0915\u0930\u0924 \u0947\u0939 \u0964\n\u0938\u0947\u0902\u092a \u092e , \u0907\u0938 \u0925\u0940 \u0938\u0938 \u092e \u0924\u0940\u0928 \u0926\u0936\u093e\u090f\u0901 \u0936\u093e \u092e\u0932 \u0939 : (i) \u0932\u093e\u092a\u0924\u093e \u0918\u091f\u0928\u093e\u0913\u0902 \u0915\u0947 \u0938\u093e\u0925 \u091f\u0947 \u092a\u094b\u0930\u0932 \u0938\u0940 \u0935 \u0938; (ii) \u0925\u093e\u0928-\u0905 \u0925\u093e\u092f\u0940 \u0938\u0947 \u091f\u0902\u0938 \u092e \u0938\u092b\u093e \u0930\u0936; \u0914\u0930 (iii) \u092e\u0949\u0921 \u0932\u0917\u0902 \u091f\u0947 \u092a\u094b\u0930\u0932 \u0938\u0940 \u0935 \u0938 \u0915\u0947 \u0905\u0928\u0941\u092f\u094b\u0917\u0964 \u092a\u0939\u0932\u0947 \u092d\u093e\u0917 \u092e , \u0939\u092e \u0932\u093e\u092a\u0924\u093e \u0918\u091f\u0928\u093e\u0913\u0902 \u0915\u0947 \u0938\u093e\u0925 \u0938\u0940\u091f \u0908\u090f\u0938 \u0915 \u0909\u092a\u093f \u0925 \u0924 \u092e \u0924\u0902 \u0915\u093e \u0905\u0928\u0941\u092e \u092e\u0949\u0921\u0932 \u0938\u0940\u0916\u0928\u0947 \u0915\u0947 \u0932\u090f \u090f\u0915 \u0905\u0928\u092a\u0941\u092f\u094b\u0917\u0940 \u092e\u0949\u0921\u0932 \u0914\u0930 \u0905\u0928\u092e\u0941\u093e\u0928 \u0935 \u0927 \u0924\u0924\u0941 \u0915\u0930\u0924\u0947 \u0939 \u0964 \u0907\u0938 \u0922\u093e\u0902\u091a\u0947 \u092e \u0915\u0908 \u0921\u093e\u0909\u0928 \u092e \u0905\u0928\u0941\u092f\u094b\u0917 \u0939 , \u091c\u0938\u0947\u0948 \u0932\u093e\u092a\u0924\u093e \u0918\u091f\u0928\u093e\u0913\u0902 \u0915\u094b \u0932\u093e\u0917\u0942 \u0915\u0930\u0928\u093e \u0914\u0930 \u092d \u0935 \u092f \u0915 \u0918\u091f\u0928\u093e\u0913\u0902 \u0915 \u092d \u0935 \u092f\u0935\u093e\u0923\u0940 \u0915\u0930\u0928\u093e\u0964 \u0926\u0938\u0942\u0930\u0947 \u092d\u093e\u0917 \u092e , \u0939\u092e \u091a \u0915\u0947 \u092c\u0926\u0902\u0941 \u0905\u0928\u0936\u0941\u0938\u0902\u093e\u0915\u0924\u093e \u0938 \u091f\u092e \u0921\u095b\u093e\u0907\u0928 \u0915\u0930\u0924\u0947 \u0939 \u091c\u094b \u0909\u092a\u092f\u094b\u0917\u0915\u0924\u093e \u0915\u094b \u092d \u0935 \u092f \u0915\u0947 \u0925\u093e\u0928 \u0915 \u0905\u0928\u0936\u0941\u0938\u0902\u093e \u0915\u0930\u0928\u0947 \u0915\u0947 \u0932\u090f \u0925\u093e \u0928\u0915 \u091a\u0947\u0915-\u0907\u0928 \u0938\u0947 \u091c\u095c\u0941\u0940 \u092d\u094c\u0917\u094b \u0932\u0915 \u0935\u0936\u0937\u0947\u0924\u093e\u0913\u0902 \u0915\u093e \u0909\u092a\u092f\u094b\u0917 \u0915\u0930\u0924\u0947 \u0939 \u0964 \u092f\u0939\u093e\u0902, \u0939\u092e \u0925\u0948\u0924\u0915 POI \u0905\u0928\u0936\u0941\u0938\u0902\u093e, \u0905\u0928\u0941 \u092e\u0915 \u0905\u0928\u0936\u0941\u0938\u0902\u093e \u0914\u0930 \u0905\u0928\u0936\u0941\u0938\u0902\u093e \u092e\u0949\u0921\u0932 \u0915\u0947 \u0938\u092e\u093e\u0927\u093e\u0928 \u0924\u093e \u0935\u0924 \u0915\u0930\u0924\u0947 \u0939 \u091c\u094b \u092d\u094c \u0924\u0915 \u0917 \u0924\u0936\u0940\u0932\u0924\u093e \u0914\u0930 \u0909\u092a\u092f\u094b\u0917\u0915\u0924\u093e \u0913\u0902 \u0915 \u092e\u093e\u091f \u095e\u094b\u0928 \u0917 \u0924 \u0935 \u0927\u092f \u0915\u0947 \u092c\u0940\u091a \u0938\u092e\u093e\u0928\u0924\u093e \u0915\u093e \u0909\u092a\u092f\u094b\u0917 \u0915\u0930\u0924\u0947 \u0939 \u0964 \u0905\u0924\u0902 \u092e , \u0924\u0940\u0938\u0930\u0947 \u092d\u093e\u0917 \u092e , \u0939\u092e \u0926\u094b \u0915\u093e\u092f \u0915\u0947 \u0932\u090f \u0938\u092e\u093e\u0927\u093e\u0928 \u0921\u091c\u093e\u0907\u0928 \u0915\u0930\u0928\u0947 \u0915\u0947 \u0932\u090f \u0924\u093e \u0935\u0924 \u0922\u093e\u0902\u091a\u0947 \u0915 \u0924\u093e\u0915\u0924 \u092a\u0930 \u0915\u093e\u0936 \u0921\u093e\u0932\u0924\u0947 \u0939 : (i) \u092a\u0928\u0941 \u093e \u093f \u0924 \u0923\u093e\u0932 , \u092f\u093e\u0928\u0940, \u0938\u0940\u091f \u0908\u090f\u0938 \u0921\u091f\u0947\u093e \u0915\u0947 \u090f\u0915 \u092c\u095c\u0947 \u0915\u094b\u0937 \u0938\u0947 \u0915\u0938\u0940 \u0926\u090f \u0917\u090f \u0928 \u0938\u0940\u091f \u0908\u090f\u0938 \u0915\u0947 \u0932\u090f \u093e\u0938\u0902\u0917\u0915\u0924\u093e \u0905\u0928\u0941\u092e \u092a\u0928\u0941 \u093e \u0924 \u0915\u0930\u0928\u093e; \u0914\u0930 (ii) \u0935 \u092d \u0928 \u0909\u092a\u092f\u094b\u0917\u0915\u0924\u093e \u0913\u0902 \u0935\u093e\u0930\u093e \u0917 \u0924 \u0935 \u0927 \u0935\u0940 \u0921\u092f\u094b \u092e \u092f\u093e\u0913\u0902 \u0915\u0947 \u0905\u0928\u0941\u092e \u0915\u0947 \u0938\u092e\u093e\u0928 \u0915\u093e\u092f \u0915\u0930\u0928\u0947 \u0915\u0947 \u0932\u090f \u0906\u0935 \u092f\u0915 \u0938\u092e\u092f \u0915\u094b \u0938\u092e\u091d\u0928\u093e\u0964 \u0907\u0938\u0915\u0947 \u0905\u0932\u093e\u0935\u093e, \u092f\u0947\u0915 \u0905 \u092f\u093e\u092f \u092e , \u0939\u092e \u0935\u0924 \u092e\u093e\u0928 \u0917\u0939\u0928 \u0936 \u0923-\u0906\u0927\u093e \u0930\u0924 \u092e\u0949\u0921\u0932 \u0915 \u0915 \u092e\u092f \u0915\u094b \u0909\u091c\u093e\u0917\u0930 \u0915\u0930\u0924\u0947 \u0939 , \u092c\u0947\u0939\u0924\u0930 \u0905\u0928\u0941\u092e \u092e\u0949\u0921 \u0932\u0917\u0902 \u0922\u093e\u0902\u091a\u0947 \u0915\u094b \u0921\u091c\u093e\u0907\u0928 \u0915\u0930\u0924\u0947 \u0939 , \u0914\u0930 \u0905 \u092f\u093e\u0927\u0941\u0928\u0915 \u092c\u0947\u0938\u0932\u093e\u0907\u0928 \u092a\u0930 \u0939\u092e\u093e\u0930\u0947 \u0924\u093e \u0935\u0924 \u0938\u092e\u093e\u0927\u093e\u0928 \u0915 \u092d\u093e\u0935\u0915\u093e \u0930\u0924\u093e \u0915\u094b \u092f\u094b\u0917\u093e \u092e\u0915 \u092a \u0938\u0947 \u0930\u0947\u0916\u093e\u0902 \u0915\u0924 \u0915\u0930\u0924\u0947 \u0939 \u0964 \u0905\u0924\u0902 \u092e , \u0939\u092e \u0907\u0938 \u0925\u0940 \u0938\u0938 \u092e \u0924\u093e \u0935\u0924 \u0939\u0930 \u0938\u092e\u093e\u0927\u093e\u0928 \u0915 \u0915 \u092e\u092f \u0914\u0930 \u0938\u092d\u0902\u093e \u0935\u0924 \u0935 \u0924\u093e\u0930 \u0915 \u0930\u092a\u094b\u091f \u0915\u0930\u0924 \u0947\u0939 \u0964\n\u0907\u0938 \u0925\u0940 \u0938\u0938 \u0915\u093e \u090f\u0915 \u092e\u0939 \u0935\u092a\u0923\u0942 \u0939 \u0938\u093e \u0938\u0940\u091f \u0908\u090f\u0938 \u0915\u0947 \u0905\u0924\u0902 \u0928 \u0939\u0924 \u0935\u0924\u0930\u0923 \u0915\u094b \u0924\u0902 \u0915\u093e \u091a\u093f \u0928\u0924 \u0905 \u0925\u093e\u092f\u0940 \u092c\u0926\u0902\u0941 \u092f\u093e\u0913\u0902 (\u090f\u092e\u091f \u092a\u0940\u092a\u0940) \u0915\u0947 \u092e\u093e \u092f\u092e \u0938\u0947 \u092e\u0949\u0921 \u0932\u0917\u0902 \u0915\u0947 \u0935\u091a\u093e\u0930 \u0915\u093e \u0909\u092a\u092f\u094b\u0917 \u0915\u0930\u0924\u093e \u0939\u0948\u0964 \u092a\u093e\u0930\u0902\u092a \u0930\u0915 \u090f\u092e\u091f \u092a\u0940\u092a\u0940 \u092e\u0949\u0921\u0932 \u091f\u094b\u091a\u093f\u0948 \u091f\u0915 \u092f\u093e\u090f\u0902 \u0939 \u091c\u094b \u0928\u0930\u0902\u0924\u0930 \u0938\u092e\u092f \u092e \u0925\u093e\u0928\u0940\u092f\u0915\u0943\u0924 \u0905\u0938\u0924\u0924 \u0918\u091f\u0928\u093e\u0913\u0902 \u0915\u0947 \u0905\u0928\u0941\u092e \u0915\u0947 \u091c\u0928\u0930\u0947 \u091f\u0935 \u0924\u0902 \u0915\u094b \u092a\u0915\u095c\u0928\u0947 \u0915\u0947 \u0932\u090f \u090f\u0915 \u0928\u093f \u091a\u0924 \u092b\u0949\u092e\u0942 \u0932\u0947\u0936\u0928 \u0915\u093e \u0909\u092a\u092f\u094b\u0917 \u0915\u0930\u0924\u0940 \u0939 \u0964 \u0907\u0938\u0915\u0947 \u0935\u092a\u0930 \u0924, \u0924\u0902 \u0915\u093e \u090f\u092e\u091f \u092a\u0940\u092a\u0940 \u0906\u0927\u0941\u0928\u0915 \u0917\u0939\u0928 \u0936 \u0923 \u0906 \u0915 \u091f\u0947 \u091a\u0930 \u0915\u0947 \u0938\u093e\u0925 \u092c\u0926\u0902\u0941 \u092f\u093e \u0938\u093e \u0939 \u092f \u0938\u0947 \u0905\u0924\u0902 \u0928 \u0939\u0924 \u0935\u091a\u093e\u0930 \u0915\u094b \u091c\u094b\u095c\u0924\u0940 \u0939\u0948\u0964 \u0921\u0940\u092a-\u0932 \u0928 \u0917 \u092e\u0949\u0921\u0932 \u0915 \u0938\u091f \u0915 \u095e\u0902 \u0936\u0928 \u0938\u093f \u0928\u0915\u091f\u0928 \u0915\u0947 \u092a \u092e \u092e\u0924\u093e \u0928\u0947 \u0924\u0902 \u0915\u093e \u0938\u092c\u0902\u0927\u0902\u0940 \u092d \u0935 \u092f \u0915\u0939\u0928\u0947\u0935\u093e\u0932\u093e \u0915\u094c\u0936\u0932 \u092e \u090f\u0915 \u092e\u0939 \u0935\u092a\u0923\u0942 \u0932\u093e\u092d \u093e \u0924 \u0915\u092f\u093e \u0939\u0948\u0964"
        },
        {
            "heading": "Contents",
            "text": ""
        },
        {
            "heading": "Certificate i",
            "text": ""
        },
        {
            "heading": "Acknowledgements iii",
            "text": ""
        },
        {
            "heading": "Abstract v",
            "text": ""
        },
        {
            "heading": "1 Introduction 1",
            "text": "1.1 Our Research Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n1.2 Organization of Thesis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7"
        },
        {
            "heading": "2 Background 9",
            "text": "2.1 Marked Temporal Point Process . . . . . . . . . . . . . . . . . . . . . . . . . 9\n2.1.1 Conditional Intensity Function of an MTPP . . . . . . . . . . . . . . . 10\n2.1.2 Normalizing Flows . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n2.2 Neural Temporal Point Process . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n2.2.1 Intensity-free formulation of Temporal Point Process . . . . . . . . . . 13\n2.3 Graph Convolution/Attention Networks . . . . . . . . . . . . . . . . . . . . . 14\n2.3.1 Graphs in Recommendation Systems . . . . . . . . . . . . . . . . . . 14\n2.4 Self-Attention . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\nix\nx CONTENTS"
        },
        {
            "heading": "I Temporal Sequences with Missing Events 17",
            "text": ""
        },
        {
            "heading": "3 Overcoming Missing Events in Continuous-Time Sequences 19",
            "text": "3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n3.1.1 Our Contribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n3.2 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n3.2.1 Missing Data Models for Discrete-Time Series . . . . . . . . . . . . . 21\n3.2.2 Missing Data Models for Temporal Point Process . . . . . . . . . . . . 22\n3.3 Problem Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n3.3.1 Preliminaries and Notations . . . . . . . . . . . . . . . . . . . . . . . 23\n3.3.2 Overcoming Missing Events . . . . . . . . . . . . . . . . . . . . . . . 23\n3.4 Components of IMTPP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\n3.5 Architecture of IMTPP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n3.5.1 High-level Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n3.5.2 Parameterization of p\u03b8 . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n3.5.3 Parameterization of q\u03c6 . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n3.5.4 Prior MTPP model pprior . . . . . . . . . . . . . . . . . . . . . . . . . 30\n3.5.5 Training \u03b8 and \u03c6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n3.5.6 Optimal Position for Missing Events . . . . . . . . . . . . . . . . . . . 31\n3.5.7 Salient Features of IMTPP . . . . . . . . . . . . . . . . . . . . . . . . 31\n3.6 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n3.6.1 Experimental Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n3.6.2 Implementation Details . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n3.6.3 Event Prediction Performance . . . . . . . . . . . . . . . . . . . . . . 36"
        },
        {
            "heading": "CONTENTS xi",
            "text": "3.6.4 Ablation Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\n3.6.5 Forecasting Future Events . . . . . . . . . . . . . . . . . . . . . . . . 40\n3.6.6 Performance with Missing Data . . . . . . . . . . . . . . . . . . . . . 40\n3.6.7 Scalability Analysis with PFPP [148] . . . . . . . . . . . . . . . . . . 41\n3.6.8 Imputation Performance . . . . . . . . . . . . . . . . . . . . . . . . . 43\n3.6.9 Evaluating the Performance of IMTPP++ . . . . . . . . . . . . . . . . 44\n3.7 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45"
        },
        {
            "heading": "II Recommendation in Spatio-Temporal Settings 47",
            "text": ""
        },
        {
            "heading": "4 Cross-Region Transfer for Spatial Features 49",
            "text": "4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\n4.1.1 Our Contribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\n4.2 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n4.2.1 Mobility Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n4.2.2 Graph based Recommendation . . . . . . . . . . . . . . . . . . . . . . 52\n4.2.3 Clustering in Spatial Datasets . . . . . . . . . . . . . . . . . . . . . . 52\n4.2.4 Transfer Learning and Mobility . . . . . . . . . . . . . . . . . . . . . 53\n4.3 Problem Formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53\n4.3.1 User-Location Graph Construction . . . . . . . . . . . . . . . . . . . . 54\n4.4 AXOLOTL Framework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\n4.4.1 Basic Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\n4.4.2 Model Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\n4.4.3 AXOLOTL: Information Transfer . . . . . . . . . . . . . . . . . . . . . 59\nxii CONTENTS\n4.5 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\n4.5.1 Experimental Settings . . . . . . . . . . . . . . . . . . . . . . . . . . 64\n4.5.2 Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65\n4.5.3 Performance Comparison . . . . . . . . . . . . . . . . . . . . . . . . . 67\n4.5.4 Ablation Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68\n4.5.5 Transfer of Weights across Regions . . . . . . . . . . . . . . . . . . . 69\n4.6 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70"
        },
        {
            "heading": "5 Sequential Recommendation using Spatio-Temporal Sequences 71",
            "text": "5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\n5.1.1 Our Contribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72\n5.2 Related Works . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\n5.2.1 Sequential POI Recommendation . . . . . . . . . . . . . . . . . . . . 73\n5.2.2 Marked Temporal Point Processes with Spatial Data . . . . . . . . . . 73\n5.3 Problem Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\n5.4 Model Description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74\n5.4.1 Region-Specific MTPP . . . . . . . . . . . . . . . . . . . . . . . . . . 74\n5.4.2 Flow-based Transfer . . . . . . . . . . . . . . . . . . . . . . . . . . . 76\n5.5 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77\n5.5.1 Experimental Settings . . . . . . . . . . . . . . . . . . . . . . . . . . 77\n5.5.2 Baselines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78\n5.5.3 Prediction Performance . . . . . . . . . . . . . . . . . . . . . . . . . . 79\n5.5.4 Qualitative Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\n5.5.5 Advantages of Transfer . . . . . . . . . . . . . . . . . . . . . . . . . . 81"
        },
        {
            "heading": "CONTENTS xiii",
            "text": "5.5.6 Product Recommendation . . . . . . . . . . . . . . . . . . . . . . . . 81\n5.6 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82"
        },
        {
            "heading": "6 Learning Spatial Behaviour using User Traces on Smartphone Apps 83",
            "text": "6.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83\n6.1.1 Limitations of Prior Works . . . . . . . . . . . . . . . . . . . . . . . . 84\n6.1.2 Our Contribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85\n6.2 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86\n6.2.1 Modeling Smartphone and Mobility . . . . . . . . . . . . . . . . . . . 86\n6.2.2 Sequential Recommendation . . . . . . . . . . . . . . . . . . . . . . . 87\n6.2.3 Relative Positional Encodings and Self-Attention . . . . . . . . . . . . 87\n6.3 Problem Formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88\n6.4 REVAMP Framework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89\n6.4.1 High-level Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . 89\n6.4.2 Embedding Initiator (EI) . . . . . . . . . . . . . . . . . . . . . . . . . 91\n6.4.3 Relative or Inter-check-in Variations . . . . . . . . . . . . . . . . . . . 93\n6.4.4 Sequential Recommender (SR) . . . . . . . . . . . . . . . . . . . . . . 95\n6.4.5 REVAMP: Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98\n6.5 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98\n6.5.1 Experimental Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99\n6.5.2 Performance Comparison . . . . . . . . . . . . . . . . . . . . . . . . . 102\n6.5.3 Ablation Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104\n6.5.4 App and Location Prediction Category . . . . . . . . . . . . . . . . . . 105\n6.5.5 Scalability of REVAMP . . . . . . . . . . . . . . . . . . . . . . . . . . 106\nxiv CONTENTS\n6.6 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107"
        },
        {
            "heading": "III Applications of Modeling Temporal Sequences 109",
            "text": ""
        },
        {
            "heading": "7 Large-Scale Retrieval of Temporal Sequences 111",
            "text": "7.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\n7.1.1 Our Contribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112\n7.2 Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113\n7.2.1 Notations and MTPP . . . . . . . . . . . . . . . . . . . . . . . . . . . 113\n7.2.2 Problem setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114\n7.3 NEUROSEQRET Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115\n7.3.1 Components of NEUROSEQRET . . . . . . . . . . . . . . . . . . . . . 115\n7.3.2 Neural Parameterization of NEUROSEQRET . . . . . . . . . . . . . . . 117\n7.3.3 Parameter estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . 120\n7.4 Scalable Retrieval with Hashing . . . . . . . . . . . . . . . . . . . . . . . . . 120\n7.4.1 Random hyperplane based hashing method . . . . . . . . . . . . . . . 121\n7.4.2 Trainable Hashing for Retrieval . . . . . . . . . . . . . . . . . . . . . 121\n7.5 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123\n7.5.1 Experimental Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124\n7.5.2 Results on retrieval accuracy . . . . . . . . . . . . . . . . . . . . . . . 127\n7.5.3 Results on Retrieval Efficiency . . . . . . . . . . . . . . . . . . . . . . 129\n7.5.4 Analysis at a Query Level . . . . . . . . . . . . . . . . . . . . . . . . 131\n7.5.5 Runtime Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131\n7.5.6 Query Length . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132"
        },
        {
            "heading": "CONTENTS xv",
            "text": "7.5.7 Qualitative Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . 132\n7.6 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132"
        },
        {
            "heading": "8 Modeling Human Action Sequences 135",
            "text": "8.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135\n8.1.1 Our Contribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136\n8.2 Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137\n8.2.1 Activity Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137\n8.2.2 Problem Formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . 137\n8.3 PROACTIVE Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138\n8.3.1 High Level Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . 138\n8.3.2 Neural Parameterization . . . . . . . . . . . . . . . . . . . . . . . . . 139\n8.3.3 Early Goal Detection and Action Hierarchy . . . . . . . . . . . . . . . 142\n8.3.4 Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143\n8.3.5 Sequence Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . 144\n8.4 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145\n8.4.1 Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146\n8.4.2 Baselines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146\n8.4.3 Evaluation Criteria . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147\n8.4.4 Experimental Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147\n8.4.5 Action Prediction Performance . . . . . . . . . . . . . . . . . . . . . . 148\n8.4.6 Goal Prediction Performance . . . . . . . . . . . . . . . . . . . . . . . 150\n8.4.7 Sequence Generation Performance . . . . . . . . . . . . . . . . . . . . 151\n8.5 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152\nxvi CONTENTS"
        },
        {
            "heading": "9 Conclusion and Future Work 153",
            "text": ""
        },
        {
            "heading": "List of Publications 175",
            "text": "Biography 177"
        },
        {
            "heading": "List of Figures",
            "text": "3.1 The overall neural architecture of IMTPP. The figure illustrates the notations, the observed, and missing point processes in IMTPP. The components concerning observed events and missing events are marked with blue and red, respectively. The figure also illustrates the generation process for events ek+1 and r. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\n3.2 Architecture of different processes in IMTPP. Panel (a) shows the neural architecture of the MTPP of observations p\u03b8. Panel (b) shows the neural architecture of the posterior MTPP of missing events q\u03c6. The information of ek+1 is truncated to the lognormal distribution for missing data generation, whereas the lognormal distribution for observed is non-truncated. . . . . . . . . . . . . . . 26\n3.3 Real life examples of true and predicted inter-arrival times \u2206t,k of different events ek, against k for k \u2208 {k+ 1, . . . , N}. Panels (a) and (b) show the results for Movies and Toys datasets, respectively. . . . . . . . . . . . . . . . . . . . . 38\n3.4 Performance gain in terms of AE(baseline) \u2212 AE(IMTPP)\u2014 the gain (above x-axis) or loss (below x-axis) of the average error per event E[|tk \u2212 t\u0302k|] of IMTPP\u2014 with respect to two competitive baselines: RMTPP and PFPP. Events in the test set are sorted by decreasing gain of IMTPP along x-axis. Panels (a) and (b) show the results for Movies and Toys datasets, respectively. . . . . . . . 38\n3.5 Variation of the forecasting performance of IMTPP and RMTPP in terms of MAE and MPA at predicting next i-th event, against i for Movies and Toys dataset. Panels (a\u2013b) show the variation of MAE, while panels (c\u2013d) show the variation of MPA. They show that as n increases, the performance deteriorates for both the metrics and both datasets as the prediction task becomes more and more difficult. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\nxvii\nxviii LIST OF FIGURES\n3.6 Impact of missing observations on model performance for Movies and Toys dataset. We randomly delete 40% and 60% events from the observed sequence and then train and test IMTPP and best performing baselines on the rest of the observed events. Panels (a\u2013b) show the results for time prediction, while panels (c\u2013d) show the results for mark prediction. . . . . . . . . . . . . . . . . . . . . 41\n3.7 Runtime performance of PFPP and IMTPP for the largest dataset, i.e., Movies, with complete sequences. Panel (a) shows the time vs. length of the training sequence, and panel (b) shows the time vs. the number of epochs. . . . . . . . 42\n3.8 Missing event imputation performance of IMTPP and PFPP for Movies and Toys datasets. Panels (a\u2013b) show the results for time prediction while panels (c\u2013d) show the results for mark prediction. . . . . . . . . . . . . . . . . . . . . 43\n3.9 Predicting observed events using IMTPP++ across different values of N . The results show that as we increase N , the performance gap between IMTPP++ and IMTPP is decreased. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\n3.10 Compared the imputation performance of IMTPP++ and PFPP across different numbers of missing events. Note that this setting differs from Figure 3.8, as here the events are missing at random positions. . . . . . . . . . . . . . . . . . 44\n4.1 Skew in the volume of mobility data across different states in the US (Figure 4.1a) and the large variation in region-specific density of mobility data between California and Washington in Figures 4.1b and 4.1c respectively (based on Gowalla dataset [182]). . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\n4.2 Different node and edge types in our graph model (users are in yellow and locations are in blue). The dashed arrows represent various influences and corresponding GATs (\u03a6i, i = 1, 2, 3, 4) in AXOLOTL. . . . . . . . . . . . . . . . . 56\n4.3 System architecture of AXO-basic with level-wise embedding computation and affinity prediction. User-latent (U l) and location conditioned (U s) embeddings are combined to a final user embedding (U f ), and similarly for location,Ll and Ls are combined to Lf . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58"
        },
        {
            "heading": "LIST OF FIGURES xix",
            "text": "4.4 The difference between MAML(fig 4.4a) and our SSML (fig 4.4b) is that the latter optimizes parameters in a hierarchy, i.e., user and location parameters are updated for neighborhood prediction and then combined for POI recommendation. (Best viewed in color). . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\n4.5 Ablation Study across different graph architectures possible in AXOLOTL along with a GCN variant. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69\n4.6 Contribution of novel spatio-social meta-learning in AXOLOTL and the corresponding gains over MeLU. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69\n4.7 Bucket-wise Mean Attention Transfer Weights between Source and Target regions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70\n5.1 Architecture of REFORMD with flow-based transfer between source region (red) and target region (blue). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75\n5.2 Real life true and predicted inter-arrival times \u2206t,k of different events ek for (a) Virginia and (b) Aichi. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\n5.3 Training curves of REFORMD and RMTPP for time prediction with best MAE for (a) Virginia (VI) and (b) Aichi (AI). . . . . . . . . . . . . . . . . . . . . . 81\n6.1 The probability of a smartphone-app category \u2013 among \u2018Social\u2019, \u2018Travel\u2019, \u2018Shopping\u2019, \u2018Navigation\u2019, and \u2018Music\u2019 \u2013 to be used at ten most popular locations from Shanghai-Telecom dataset. The plot indicates that the smartphone app usage depends on the check-in location. . . . . . . . . . . . . . . . . . . . . . . . . . 84\n6.2 REVAMP learns the dynamics of a check-in sequence via inter-check-in time, app, and POI variations. Here, \u2206a\u2022,\u2022 and \u2206 d \u2022,\u2022 denote the smartphone-app and\nPOI-based differences respectively. . . . . . . . . . . . . . . . . . . . . . . . . 85\n6.3 Overview of the neural architecture of REVAMP. . . . . . . . . . . . . . . . . . 89\n6.4 Architecture of different components in REVAMP. Panel (a) illustrates the setup of the Embedding Initiator (EI) that learns the category representations. Panel (b) shows the self-attention architecture in Sequential Recommender (SR). Note that the input to the self-attention is an aggregation of all past events and relative positional encodings. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89\nxx LIST OF FIGURES\n6.5 Word-cloud for POI Categories for both Shanghai-Telecom and TalkingData datasets. The larger the font size indicates a larger frequency of location of the category. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100\n6.6 POI recommendation performance of REVAMP with different methods for obtaining the relative positional encodings, i.e., the inter-check-in differences between app- and location embeddings. Here, the time-based representations are kept consistent across all the models. . . . . . . . . . . . . . . . . . . . . . . . 103\n6.7 Ablation study with different relative positional encodings used in REVAMP and their comparison with SASRec [99] and TiSRec [115]. . . . . . . . . . . . 105\n6.8 Root-mean squared distance between the user-preference vector estimated by the model and the mean of the app- and location-category vectors of events in the test set. It shows that REVAMP is the best performer for all the datasets. . . 105\n6.9 Epoch-wise recommendation performance of REVAMP for both datasets in terms of Hits@1, 5, 10. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106\n7.1 Effect of unwarping on a relevant query-corpus pair in Audio. U\u03c6(\u00b7) learns to transformHq in order to capture a high value of its latent similarity withHc. . . 129\n7.2 Tradeoff between NDCG@10 vs. Reduction factor, i.e., % reduction in number of comparisons between query-corpus pairs w.r.t. the exhaustive comparisons for different hashing methods. The point marked as ? indicates the case with exhaustive comparisons on the set of corpus sequences. . . . . . . . . . . . . . 130\n7.3 Query-wise performance comparison between NEUROSEQRET and best baseline methods \u2013 Rank-THP, Rank-SAHP. Queries are sorted by the decreasing gain in AP. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131\n7.4 Qualitative examples of inter-event times of events in a query sequence and the top-search results by NEUROSEQRET for all datasets. . . . . . . . . . . . . . . 133\n8.1 Real life true and predicted inter-arrival times \u2206t,k of different events ek for (a) Multi-THUMOS and (b) Activity-Net datasets. The results show that the true arrival times match with the times predicted by PROACTIVE. . . . . . . . . . . 147"
        },
        {
            "heading": "LIST OF FIGURES xxi",
            "text": "8.2 Sequence goal prediction performance of PROACTIVE, its variants \u2013 PROACTm and PROACT-\u03b3, and other baseline models. The results show that PROACTIVE can effectively detect the CTAS goal even with smaller test sequences as input. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149\n8.3 Sequence Generation results for PROACTIVE and other baselines in terms of APA for action prediction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151\n8.4 Sequence Generation results for PROACTIVE and other baselines in terms of MAE for time prediction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151\nxxii LIST OF FIGURES"
        },
        {
            "heading": "List of Tables",
            "text": "1.1 List of first-author research contributions made in this thesis. . . . . . . . . . . 4\n2.1 Drawbacks and advantages of neural MTPP frameworks. . . . . . . . . . . . . 13\n3.1 Statistics of all real datasets used in this chapter. . . . . . . . . . . . . . . . . . 34\n3.2 Performance of all the methods in terms of mean absolute error across all datasets on the 20% test set. Numbers with bold font (boxes) indicate the best (second best) performer. Results marked \u2020 are statistically significant (twosided Fisher\u2019s test with p \u2264 0.1) over the best baseline. . . . . . . . . . . . . . 37\n3.3 Performance of all the methods in terms of mark prediction accuracy (MPA). Numbers with bold font (boxes) indicate the best (second best) performer. Results marked \u2020 are statistically significant (two-sided Fisher\u2019s test with p \u2264 0.1) over the best baseline. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n3.4 Mark prediction performance of Markov Chains and IMTPP across all datasets. We use MC of orders 1,2, and 3 and report results for the best-performing model. 39\n3.5 Time prediction performance of IMTPP and its variants \u2013 IMTPPSand IMTPPRin terms of MAE on the 20% test set. Numbers with bold font indicate the best performer. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n3.6 Mark prediction performance of IMTPP and its variants in terms of MPA on the 20% test set. Numbers with bold font indicate the best performer. . . . . . . 39\n3.7 Runtime comparison between IMTPP and PFPP in a streaming setting. Here, DNF indicates that the code did not finish within 24:00hrs. . . . . . . . . . . . 42\nxxiii\nxxiv LIST OF TABLES\n4.1 Summary of Notations Used. . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\n4.2 Statistics of datasets used in this chapter. The source region columns are highlighted, followed by target regions. The datasets are further partitioned based on the country of origin (US, Japan, and Germany). . . . . . . . . . . . . . . . 65\n4.3 Performance comparison between state-of-the-art baselines, AXOLOTL and its variants (AXO-f and AXO-m). The first column represents the source and the corresponding target regions. The grouping is done based on baseline details in Section 4.5.2. Numbers with bold font indicate the best-performing model. All results marked \u2020 are statistically significant (i.e., two-sided Fisher\u2019s test with p \u2264 0.1) over the best baseline. . . . . . . . . . . . . . . . . . . . . . . . . . . 67\n5.1 Statistics of datasets used in this chapter. The source region columns are highlighted, followed by target regions, and are partitioned based on the country of source (the US and Japan). . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77\n5.2 Performance of all the methods in terms of mean absolute error (MAE) on the 20% test set. Numbers with bold font (boxes) indicate the best (second best) performer. Results marked \u2020 are statistically significant (two-sided Fisher\u2019s test with p \u2264 0.1) over the best baseline. . . . . . . . . . . . . . . . . . . . . . . . 79\n5.3 Performance in terms of mark prediction accuracy (MPA) on the 20% test set. Numbers with bold font (boxes) indicate the best (second best) performer. Results marked \u2020 are statistically significant (two-sided Fisher\u2019s test with p \u2264 0.1) over the best baseline. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79\n5.4 Prediction performance of all the methods for product recommendation in Amazon datasets. Results marked \u2020 are statistically significant as in Tables 5.2 and 5.3. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\n6.1 Summary of Notations Used. . . . . . . . . . . . . . . . . . . . . . . . . . . . 91\n6.2 Statistics of all datasets used in this chapter. . . . . . . . . . . . . . . . . . . . 99"
        },
        {
            "heading": "LIST OF TABLES xxv",
            "text": "6.3 Next check-in recommendation performance of REVAMP and state-of-the-art baselines for the Shanghai-Telecom dataset. Here, we exclude a comparison with STGN [245] as it requires the precise geographical coordinates for checkin locations, which we lack in the Shanghai-Telecom dataset. All results are statistically significant (i.e., two-sided Fisher\u2019s test with p \u2264 0.1) over the best baseline. Numbers with bold font (boxes) indicate the best (second best) performer. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102\n6.4 Next check-in performance comparison between REVAMP and state-of-the-art baselines for the TalkingData dataset. All results are statistically significant over the best baseline as in Table 6.3. . . . . . . . . . . . . . . . . . . . . . . . 102\n6.5 Run-time Statistics of training REVAMP in minutes on a 32GB Tesla V100 GPU with 256 batch-size. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106\n7.1 Statistics of the search corpus for all datasets. |Cq+|/|C| denotes the ratio of positive corpus sequences to the total sequences sampled for training. The ratio is kept the same for all queries. . . . . . . . . . . . . . . . . . . . . . . . . . . 124\n7.2 Hyper-parameter values used for different datasets. The values are determined by fine-tuning the performance on the validation set. . . . . . . . . . . . . . . 125\n7.3 Retrieval quality in terms of Mean Average Precision (MAP in %) of all the methods across five datasets on the test set. Numbers with bold font (underlined) indicate the best (second best) performer. Boxed numbers indicate the best-performing state-of-the-art baseline. Results marked \u2020 are statistically significant (two-sided Fisher\u2019s test with p \u2264 0.1) over the best performing stateof-the-art baseline (Rank-THP or Sharp). The standard deviation for MASS and UDTW are zero since they are deterministic retrieval algorithms. . . . . . . 127\n7.4 Retrieval quality in terms of NDCG@10 (in %) of all the methods. . . . . . . . 127\n7.5 Retrieval quality in terms of mean reciprocal rank (MRR in %). . . . . . . . . . 128\n7.6 Retrieval quality in terms of NDCG@20 (in %) of all the methods. . . . . . . . 128\n7.7 Ablation study of CROSSATTN-NEUROSEQRET and its variants in terms of Mean Average Precision (MAP) in %. . . . . . . . . . . . . . . . . . . . . . . 129\n7.8 Training-times of NEUROSEQRET for all datasets. . . . . . . . . . . . . . . . 131\nxxvi LIST OF TABLES\n7.9 Retrieval quality in terms of mean average precision (MAP) for query sequence lengths sampled between 10 and 50. . . . . . . . . . . . . . . . . . . . . . . . 132\n7.10 Retrieval quality in terms of mean average precision (MAP) for query sequence lengths sampled between 50 and 100. . . . . . . . . . . . . . . . . . . . . . . 132\n8.1 Performance of all the methods in terms of action prediction accuracy (APA). Bold (underline) fonts indicate the best performer (baseline). Results marked \u2020\nare statistically significant (i.e. two-sided Fisher\u2019s test with p \u2264 0.1) over the best baseline. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148\n8.2 Performance of all the methods in terms of mean absolute error (MAE). . . . . 148\nChapter 1"
        },
        {
            "heading": "Introduction",
            "text": "Recommender systems have become pervasive across various applications, including finance, social networks, healthcare, and spatial mobility. These recommendations can be from a wide range of sources, such as e-commerce products [99, 115, 212, 221], music [156, 204, 213], news articles [133, 217], points-of-interest [54, 72, 126], etc. With the widespread use of applications on the web, it is easier for people to provide feedback about their likes or dislikes to their application providers. These providers can use the collected feedback to understand the customers\u2019 preferences and provide better recommendations. These recommendations can enhance the customer experience, as the recommended content is generally preferable to the suggestions given at random [3]. In the terminology of recommender systems, the entities to which the recommendations are provided are called users, and the products that can be recommended are referred to as items. The working of a recommender system can be divided into two phases \u2013 understanding the preferences of the users and recommending candidate items for future interactions. Traditional recommendation systems, such as content-based and collaborative filtering models, generate relationships between users and items based on their past interactions [3]. Specifically, content-based filtering uses the similarity between item features to recommend similar items to the users based on the items the users have interacted with in the past. In the same context, collaborative filtering uses similarities between users and items simultaneously to provide recommendations. Thus, in a collaborative filtering-based recommender system, it is possible to recommend an item to user A based on the interests of a similar user B. The similarity between users and items is calculated based on the features available for each entity. However, a significant drawback of these approaches is that they utilize the user-item interactions in a static way, i.e., they cannot calculate similarities between users and items when the features evolve with time. This limitation drastically reduces their applicability in real-world applications, as most user-generated data is in the form of temporal sequences.\n1"
        },
        {
            "heading": "2 Introduction",
            "text": "In detail, the data extracted from a majority of online activities, physical actions, and natural phenomena can be represented as sequences of discrete events localized in continuous time, i.e., continuous-time event sequences (CTES). Thus, to design accurate recommender systems, it is necessary to understand the rich information encoded in these temporal sequences. To highlight a few examples, to recommend the future purchases of a user, we must capture the time-sensitive relationship between products of different types; to advertise meet-up places in the neighborhood, we must understand how user preferences change with time \u2013 \u2018coffee houses\u2019 during the day and \u2018social joints\u2019 in the night; and in the particular case of recommending actions that people should take to prepare breakfast, we must capture the time they might take to complete independent actions, such whisking an egg or slicing vegetables.\nIn recent years, deep learning frameworks have shown unmatched prowess in understanding visual data [81, 82, 106], natural language text [42, 206], and audio [88, 155]. Thus, it is hardly surprising that designing recommender systems using deep learning tools has attracted significant attention and research efforts [95, 99, 115, 174, 194, 212, 221, 235]. However, due to the rich information encoded in these CTES, it is challenging to learn the dynamics of these sequences with standard deep-learning models. The rich information encoded in every CTES data may include the ever-changing time intervals between interactions, high variance in the length of sequences for different users, additional attributes such as spatial features with every event, and the influence structure between events \u2013 within and across different sequences. Encapsulating these features in a recommender system is necessary to perform a wide range of downstream tasks, such as forecasting future interactions for users and identifying the mostlikely time when the user will interact with an item.\nIn this thesis, we seek to address critical challenges in designing neural recommender systems that can understand the dynamics of continuous-time event sequences. To better address these challenges, we follow a ground-up approach, i.e., first, we address the problems that may arise due to the poor quality of sequential data being fed into the systems. Then we address the task of designing systems that accurately understand these sequences. In the context of poor data quality, we address a real-world problem with CTES data, of overcoming missing events, i.e., the loss in data quality as the data-collection procedure could not record all events in the sequences due to constraints such as crawling or privacy restrictions. This data loss is a crucial problem as the performance of any recommender system, or deep-leaning in general is conditioned on the availability of high-quality data. Moreover, since most of the sequential models have considered only the settings where the training data is completely observed, i.e., there are no missing observations, their predictive performance deteriorates significantly as the sequence quality further degrades [24, 69, 238]. Later, we address the problems associated with\nIntroduction 3\ndesigning accurate sequence modeling frameworks that can better understand the users\u2019 preferences and recommend candidate items. For this, we consider the task of points-of-interests (or POI) recommendation, i.e., given the past mobility records of a user via her check-ins, the goal is to recommend the most likely POIs that the user will visit in her future check-ins. Addressing this problem is necessary as recent research has shown that accurate advertisements on POI networks, such as Foursquare and Instagram, can achieve up to 25 times the return-oninvestment [149]. However, since deep learning models require large quantities of training data, designing a POI recommender system can be challenging if the spatial data for the underlying region is insufficient to train large neural networks. The skew in geographical distribution is a result of the variability in the quantity of mobility data across spatial regions, i.e., a majority of the human population is located in urban and suburban regions [32]. This variability in data makes it challenging to design POI recommendation systems for regions with limited data. Thus, we overcome problems associated with designing POI systems with limited training data.\nAcross different chapters of this thesis, we propose robust yet scalable sequence modeling frameworks. However, the ability to understand the dynamics of CTES better can have applications beyond recommender systems. Therefore, in the last part of this thesis, we utilize the predictive prowess of the proposed frameworks and design solutions for two real-world applications of large-scale sequence retrieval and human activity prediction. In detail, we show that the current approaches for both applications have a limited ability to model the temporal relationships between events in a sequence. To this extent, we propose deep-learning frameworks that understand the dynamics of CTES data and outperform the current approaches for the two applications and a wide range of downstream tasks.\nFor a significant part of this thesis, we model the dynamics of CTES using neural marked temporal point processes (MTPP1) [45, 147, 162, 186, 225, 243, 251] \u2013 probabilistic generative models that learn the latent interaction between the current and the past events in a CTES. Neural MTPP models bridge the gap between the universal approximation ability of deep learning models and the probabilistic formulation of point processes. In detail, the traditional MTPP models, like self-exciting processes, use a fixed mathematical function to model the interaction between events in a CTES [36, 80]. Thus, capturing complex relationships within the CTES requires the knowledge to represent these patterns using mathematical functions while being scalable for large datasets. In contrast, the neural MTPP combine the underlying ideas from the point process literature with deep-learning models\u2019 ability to be effective function approximators. Thus, these models have the flexibility of a neural network and can benefit from the sequence modeling ability of neural recurrent layers [64] and transformers [206].\n1In this and the following chapters, we use MTPP to denote a single and well as multiple point processes."
        },
        {
            "heading": "4 Introduction",
            "text": ""
        },
        {
            "heading": "1.1 Our Research Contributions",
            "text": "In this thesis, we address the problems associated with designing recommender systems that can understand user-item interactions in the form of temporal sequences. Firstly, we address problems associated with missing events that degrade the quality of the CTES data. Later, we design accurate POI recommendation frameworks that utilize temporal data and understand the physical mobility of users. We demonstrate that our proposed architectures can effectively overcome the drawbacks of the data-collection process and simultaneously outperform the state-of-the-art models for predicting and forecasting future events. Moreover, we highlight the strengths of the proposed models and propose solutions that are tailored to address a few real-world CTES applications that have been overlooked in the past \u2013 large-scale CTES retrieval and human activity prediction. The research publications that constitute this thesis are listed in Table 1.1, and can be organized into the following three parts:\n1. Temporal Sequences with Missing Events. In the first part of the thesis, we address a data-related problem of overcoming missing events in temporal sequences and the limitations of traditional approaches in modeling these sequences. Addressing this problem is crucial as most of the existing models and inference methods in the neural MTPP literature consider only the complete observation scenario. In a complete observation setting, the underlying event sequence is assumed to be observed entirely with no missing events \u2013 an ideal setting that is rarely applicable in real-world applications. A recent line of work that considers missing events while training MTPP utilizes supervised learning techniques that require additional knowledge of missing or observed labels for each event in a sequence. The need for additional knowledge further restricts their practicability in real-world settings, as in several scenarios, the details of the missing events are not known apriori. To address these problems, in Chapter 3, we propose IMTPP (Intermittently-observed Marked Temporal Point Processes), a novel unsupervised model and inference method for learning MTPP in the presence of event sequences with missing\nIntroduction 5\nevents. Specifically, we first model the generative processes of observed and missing events using two MTPP, representing the missing events as latent random variables. Then, we devised an unsupervised training method that jointly learns the MTPP through variational inference. Such a formulation can impute the missing among the observed events, which enhances its predictive prowess and identify the optimal position of missing events.\n2. Recommendations in Spatio-Temporal Settings. In the second part of the thesis, we design recommender systems for datasets with geographical features. In detail, we analyze the physical mobility data of users worldwide and highlight the problems associated with recommending spatial points of interest (POI) for users to visit in the future. These problems primarily arise due to the high variance in the volume of data collected across different spatial regions. In contrast to the concept of incomplete sequences in Chapter 3, we regard limited data as the problem of data scarcity, i.e., the available data is assumed to be complete but in short supply. However, the volume is insufficient to train a deep neural network-based recommender system effectively. Thus, in this part, we focus on the task of points-of-interest (POI) recommendation, i.e., recommending candidate locations to a user based on her past visits. Unlike standard item recommendation, the task of POI recommendation is more challenging as a user\u2019s preference in the network is influenced by its geo-location and the distribution of nearby POIs. We address three real-world recommendation scenarios that are affected by limited data:\n2 (a). Top-k POI Recommendation with Limited Data. Firstly, we address the problems arising from limited data in designing systems for top-k recommendations, i.e., estimating the probability of a specific user checking into a candidate POI using their past checkins. Therefore, in Chapter 4, we present AXOLOTL (Automated cross Location-network Transfer Learning), a novel method aimed at transferring location preference models learned in a data-rich region to boost the quality of recommendations in a data-scarce region significantly. Precisely, we deploy two channels for information transfer, (i) a meta-learning based procedure learned using location recommendation as well as social predictions, and (ii) a lightweight unsupervised cluster-based transfer across users and locations with similar preferences. Both of these work together synergistically to achieve improved accuracy of recommendations in data-scarce regions without any prerequisite of overlapping users and with minimal fine-tuning. Our model is built on top of an twin graph-attention neural network, which captures user- and location-conditioned influences in a user-mobility graph for each region. 2 (b). Sequential POI Recommendation with Limited Data. In addition to top-k recommendations, we design neural models to overcome regional data scarcity for sequential POI recommendations, i.e., continuous-time recommendations for the following spatial"
        },
        {
            "heading": "6 Introduction",
            "text": "location. In contrast to top-k recommendations, here, our goal is to recommend specifically the next candidate POI and the probable check-in time using the past check-in sequence of the user. To overcome data scarcity for sequential POI recommendation, in Chapter 5, we propose REFORMD (Reusable Flows for Mobility Data), a novel transfer learning framework for continuous-time location prediction for regions with sparse check-in data. Specifically, we model user-specific check-in sequences in a region using a marked temporal point process with normalizing flows to learn the inter-check-in time and geo-distributions [143, 175]. Later, we transfer the model parameters of spatial and temporal flows trained on a data-rich source region for the next check-in and time prediction in a target region with scarce check-in data. We capture the evolving region-specific check-in dynamics for MTPP and spatial-temporal flows by maximizing the joint likelihood of next check-in with three channels \u2013 (i) check-in category prediction, (ii) check-in time prediction, and (iii) travel distance prediction.\n2 (c). User App-Usage to Physical Mobility. Here, we overcome the problems associated with limited data by using the different features associated with users\u2019 physicla mobility. Specifically, we aim to capture the relationship between users\u2019 mobility and smartphone usage. This task is based on the intuition that as every user carries their smartphone wherever they go \u2013 a crucial aspect ignored by the current models for spatial recommendations. Thus, in Chapter 6, we present REVAMP (Relative position Vector for Appbased Mobility Prediction), a sequential POI recommendation approach that uses smartphone app-usage logs to identify the mobility preferences of a user. This work aligns with the recent psychological studies of online urban users that show that the activity of their smartphone apps largely influences their spatial mobility behavior. Specifically, our proposal for using coarse-grained data refers to data logs collected in a privacy-conscious manner consisting only of the following: (i) category of the smartphone app used, such as \u2018retail\u2019,\u2018social\u2019, etc.; and (ii) category of the check-in location. Thus, REVAMP is not privy to precise geo-coordinates, social networks, or the specific app used. Buoyed by the efficacy of self-attention models, we learn the POI preferences of a user using two forms of positional encodings \u2013 absolute and relative \u2013 with each extracted from the inter-check-in dynamics in the mobility sequence of a user.\n3. Applications In the third part of the thesis, we address the limitations of the current sequence models that restrict their modeling ability in many real-world applications. In detail, we highlight that the sequence modeling propose ability of MTPP models proposed in the previous chapters can have applications beyond recommender systems, and can be used to better learn the embeddings of CTES in the specific application settings. These embeddings are then tailored to outperform the existing approaches in two downstream tasks \u2013 large-scale retrieval\nIntroduction 7\nof CTES and modeling actions performed by humans in activity sequences.\n3 (a). Large Scale CTES Retrieval. The recent developments in MTPP frameworks have enabled an accurate characterization of temporal sequences for a wide range of applications. However, the problem of retrieving such sequences still needs to be addressed in the literature. Specifically, given a large corpus of temporal sequences and a query sequence, our goal is to retrieve all the sequences from the corpus that are relevant to the query. Therefore, in Chapter 7, we propose NEUROSEQRET, a family of MTPP models to retrieve and rank a relevant set of continuous-time event sequences for a given query sequence from a large corpus of sequences. More specifically, we first apply a trainable unwarping function on the query sequence, which makes it comparable with corpus sequences, especially when a relevant query-corpus pair has individually different attributes. Next, we feed the unwarped query and corpus sequences into MTPP-guided neural relevance models. We develop two variants of the relevance model, which offer a tradeoff between accuracy and efficiency. We also propose an optimization framework to learn binary sequence embeddings from the relevance scores, suitable for locality-sensitive hashing, leading to a significant speedup in returning top-k results for a given query sequence. 3 (b). Actions by Humans in Activity Sequences. Unlike machine-made time series, the sequences of actions done by different humans are highly disparate, as the time required to finish a similar activity might vary between people. Therefore, understanding the dynamics of these sequences is essential for many downstream tasks such as activity length prediction, goal prediction, etc. Existing neural approaches that model an activity sequence are either limited to visual data or are task-specific, i.e., limited to the next action or goal prediction. In Chapter 8, we present PROACTIVE (Point Process flows for Activity Sequences), a neural MTPP framework for modeling the continuous-time distribution of actions in an activity sequence while simultaneously addressing three real-world applications \u2013 next action prediction, sequence-goal prediction, and end-to-end sequence generation. Specifically, we utilize a self-attention module with temporal normalizing flows to model the influence and the inter-arrival times between actions in a sequence. For time-sensitive prediction, we perform a constrained margin-based optimization to predict the goal of the sequence with a limited number of actions."
        },
        {
            "heading": "1.2 Organization of Thesis",
            "text": "We organize the rest of the thesis as follows: In Chapter 2, we review some background on marked temporal point processes, graph neural networks, and self-attention models. Part 1 of the thesis presents a method for addressing missing data problems in temporal sequences"
        },
        {
            "heading": "8 Introduction",
            "text": "(Chapter 3). In Part 2, we design POI recommendation systems that overcome limited data problems \u2013 in a top-k setting (Chapter 4) and for sequential recommendation (Chapter 5) \u2013 and identify the influence of app usage on the physical mobility (Chapter 6). In Part 3, we present two novel applications of neural MTPP models in large-scale sequence retrieval (Chapter 7) and modeling human actions in activity sequences (Chapter 8). Finally, in Chapter 9 we summarize our contributions and discuss future avenues of research that this thesis offers.\nChapter 2"
        },
        {
            "heading": "Background",
            "text": "In this chapter, we provide a detailed overview of marked temporal point processes (MTPP), a crucial element of most approaches proposed in this work. In addition, we offer a brief introduction to graph neural networks (GNN). These neural network-based approaches constitute the necessary background for understanding this work\u2019s subsequent chapters. Specifically, MTPP are essential to understand the contributions made in Chapters 3, 5, 7, and 8 and GNN are necessary for Chapter 4."
        },
        {
            "heading": "2.1 Marked Temporal Point Process",
            "text": "Marked temporal point processes [36, 80, 172] are probabilistic generative models for continuoustime event sequences. In recent years, MTPP have emerged as a powerful tool to model asynchronous events localized in continuous time [36, 80], which have a wide variety of applications, e.g., information diffusion, disease modeling, finance, etc. Driven by these motivations, in recent years, MTPP have appeared in a wide range of applications in healthcare [131, 176], traffic [45, 68], web and social networks [45, 46, 51, 70, 75, 108, 127], and finance [7, 13]. Moreover, MTPP models are even applied to many applications, including seismology and neuroscience [188].\nAn MTPP represents an event using two quantities: (i) the time of its occurrence and (ii) the associated mark, where the latter indicates the category of the event and therefore bears different meanings for different applications. For example, in a social network setting, the marks may indicate users\u2019 likes, topics, and opinions on the posts; in finance, they may correspond to the stock prices and the category of sales; in healthcare, they may indicate the state of the disease of an individual. Mathematically, MTPP can be represented as a probability distribution over\n9"
        },
        {
            "heading": "10 Background",
            "text": "sequences of variable lengths belonging to a closed time interval [0, T ], and can be realized as an event sequence Sk = {ei = (ci, ti)|i \u2208 [k]}, where k is the number of events. Here, the times are ever-increasing, i.e., 0 < t1 < t2 < \u00b7 \u00b7 \u00b7 < tN \u2264 T and ci \u2208 C is the corresponding mark with C as the set of all categorical marks. Note that across the different chapters in this thesis, we denote the mark in an event ei using ci as well as xi. However, both these notations mean the same and are used interchangeably. We also denote \u2206t,i = ti\u2212 ti\u22121 as the inter-event time difference between events ei and ei\u22121."
        },
        {
            "heading": "2.1.1 Conditional Intensity Function of an MTPP",
            "text": "For an MTPP, the time of each event is a random variable. Therefore, given the times of past events, {t1, \u00b7 \u00b7 \u00b7 ti \u2212 1}, we can determine ti using the following functions [38]:\n\u2022 Conditional probability density function f \u2217(t) = f(t|S(t)), that determines that the next event will occur in interval [t, t+ dt).\n\u2022 Cumulative distribution function F \u2217(t) = F (t|S(t)) = \u222b t ti\u22121\nf \u2217(\u03c4)d\u03c4 , that determines the probability that the next event will occur before time t. \u2022 A secondary cumulative distribution function, called survival function represented as, S\u2217(t) = S(t|S(t)) = 1 \u2212 F \u2217(t). This function represents the probability that the next event will not occur before time t.\nUsing these functions, we can determine the characteristics of future events in a sequence. However, a major drawback of representing an MTPP using these functions is that we cannot combine multiple MTPP models together. Therefore, we resort to characterizing the event times of an MTPP using a conditional intensity function (CIF) or hazard function, denoted by \u03bb\u2217(t) = \u03bb(t|S(t)), that represents the conditional probability that next event in a sequence has not happened before time t and will happen during the interval [t, t + dt). Mathematically, we can define the relationship between \u03bb\u2217(t)dt, f \u2217(t), and S\u2217(t) as:\n\u03bb\u2217(t)dt = f \u2217(t)dt\nS\u2217(t) , (2.1)\nIntuitively, \u03bb\u2217(t) is an instantaneous rate of events per unit of time. Using the conditional intensity function, we can easily combine multiple MTPP models. Specifically, for two MTPP models SA(t) and SB(t) with intensities \u03bb\u22171(t) and \u03bb\u22172(t) respectively, we can characterize the joint history S(t) = SA(t) \u222a SB(t) as:\n\u03bb\u2217(t) = \u03bb\u22171(t) + \u03bb \u2217 2(t), (2.2)\nBackground 11\nRelationship between \u03bb\u2217(t) and f \u2217(t). By definition, we have the following:\n\u03bb\u2217(t) = f \u2217(t)\nS\u2217(t) = \u2212 1 S\u2217(t)\ndS\u2217(t)\ndt = \u2212d logS\n\u2217(t)\ndt , (2.3)\nThus, if we integrate the left and right-hand sides in the above equation and consider that dS\u2217(t) = \u2212f \u2217(t)dt, we obtain the relationship between \u03bb\u2217(t) and f \u2217(t) as:\nf \u2217(t) = \u03bb\u2217(t) exp ( \u2212 \u222b t ti\u22121 \u03bb\u2217(\u03c4)d\u03c4 ) , (2.4)\nRelationship between \u03bb\u2217(t) and Log-likelihood. We can compute the log-likelihood that\n\u03bb\u2217(t) will generate the sequence S(t) with parameters \u03b8 as:\nL = ( n\u2211 i=1 log \u03bb\u2217\u03b8(ti)\u2212 \u222b ti ti\u22121 \u03bb\u2217\u03b8(\u03c4)d\u03c4 ) \u2212 \u222b T tn \u03bb\u2217\u03b8(\u03c4), (2.5)\nThis can be resolved in the following:\nL = n\u2211 i=1 log \u03bb\u2217\u03b8(ti)\u2212 \u222b T 0 \u03bb\u2217\u03b8(\u03c4)d\u03c4, T \u2265 tn. (2.6)"
        },
        {
            "heading": "2.1.2 Normalizing Flows",
            "text": "Normalizing flows [144, 175] (NF) are generative models used for density estimation and event sampling. They work by mapping simple distributions to complex ones using multiple bijective, i.e., reversible functions. For e.g., the function r(x) = x + 1 is a reversible function because, for each input, a unique output exists and vice-versa, whereas the function r(x) = x2 is not a reversible function. In detail, let Z \u2208 RD be a random variable with a known probability density function pZ : RD \u2192 R. Let g be an invertible function and Y = g(Z). Then, via the change of variables formula [104], the probability density function of Y is:\npY (y) = pZ ( f(y) ) \u2223\u2223\u2223detDf(y)\u2223\u2223\u2223, (2.7) where f(\u00b7) is the inverse of g and Df(y) = \u03b4f\n\u03b4y is the Jacobian of f . Here, the above function\ng(\u00b7) (a generator) projects the base density p(Z) to a more complex density, and this projection is considered to be in the generative direction. Whereas the inverse function f(\u00b7) moves from a complicated distribution towards the simpler one of p(Z), referred to as the normalizing direction. Since in generative models, the base density p(Z) is considered as Normal distribution, this formulation gives rise to the name normalizing flows. To sample a point y, one can sam-"
        },
        {
            "heading": "12 Background",
            "text": "ple a point z and then apply the generator y = g(z). Such a procedure supports closed-form sampling. Moreover, modern approaches for normalizing flows approximate the above functions using a neural network [101, 102, 175]. Normalizing flows have been increasingly used to define flexible and theoretically sound models for marked temporal point processes [144, 186]."
        },
        {
            "heading": "2.2 Neural Temporal Point Process",
            "text": "Buoyed by the predictive prowess of deep-learning models in modeling the dynamics of temporal sequences, modern MTPP models utilize a neural network with the probabilistic modeling ability of MTPP to enhance its predictive power [45, 70, 147, 162, 184, 186, 243, 251]. Specifically, they combine the continuous-time approach from the point process literature with modern deep learning approaches such as RNNs and transformers. Thus, these models can better capture the complex relationships between future events and historical events. The most popular approaches [45, 147, 243, 251] use different variants of neural networks to model the time- and mark distribution.\nRecurrent Marked Temporal Point Process. RMTPP is the first-ever neural network-based MTPP model [45]. The underlying model of RMTPP is a two-step procedure that embeds the event sequence using a recurrent neural network (RNN) and then derives the formulation of CIF using this embedding. Specifically, given the sequence Sk = {e1, \u00b7 \u00b7 \u00b7 , ek}, an RNN determines its vector representation denoted by hk. Later, RMTPP uses this representation of the sequence over an exponential function to formulate the CIF.\n\u03bb\u2217(t) = exp(whhk +w\u2206\u2206t,k + b), (2.8)\nwhere, w\u2022 and b are trainable parameters. Due to this formulation, the modeling prowess of RMTPP is limited by the expressive power of the exponential function.\nNeural Hawkes Process. NHP modified the LSTM architecture to model the continuous time of events in a sequence. Later, it uses the embedding from the LSTM to determine the CIF using a softplus function [147]. Such a formulation is more expressive; however, it does not have a closed-form for the likelihood.\nFully Neural Point Process. FNP is a fully neural network-based intensity function for TPP. The underlying framework idea of their approach is to model the cumulative conditional intensity function, i.e., \u2212 \u222b t ti\u22121\n\u03bb\u2217(\u03c4)d\u03c4 , using a neural network. Later, the CIF can be obtained by differentiating this w.r.t time. This approach allows the model to compute the log-likelihood efficiently.\nBackground 13\nPoint Process and Self-Attention. Transformer Hawkes process (THP) [251] and self-attentive Hawkes Process [243] combine the transformer architecture to formulate a point process. In detail, these architectures obtain the embedding of the sequence using a transformer and then formulate the CIF using the obtained embeddings."
        },
        {
            "heading": "2.2.1 Intensity-free formulation of Temporal Point Process",
            "text": "Here, we describe an intensity-free formulation for MTPP that estimates the temporal distribution of events using normalizing flows. MTPP models with a neural network-based intensity function have shown incredible prowess in learning the dynamics of CTES. However, these models face many constraints while sampling future events in a sequence. Du et al. [45] and Zuo et al. [251] are limited by the design choice for their intensity function, Mei and Eisner [147] requires approximating the integral using Monte Carlo and thus lacks closed-form sampling for future events. Lastly, Omi et al. [162] lacks a defined formulation for a proper density function and uses an expensive sampling procedure. We summarize these drawbacks of existing MTPP models in Table 2.1.\nTo overcome these drawbacks, Shchur et al. [186] propose a simple yet efficient intensityfree formulation for modeling the inter-event arrival times in an MTPP. Specifically, they use temporal normalizing flows [175] over an RNN layer that performs on par with the state-of-theart method. In particular, they use a log-normal distribution for inter-event arrival times:\nti \u2212 ti\u22121 \u223c LOGNORMAL (\u00b5(hi), \u03c3(hi)) , (2.9)\nwhere \u00b5 and \u03c3 represent the mean and the variance of the log-normal distribution, respectively. The mean and variance are derived from the recurrent neural network layer with output h\u2022. Thus, the probability density function of the inter-arrival times will be:\np (\u2206t|\u00b5, \u03c3) = 1\u221a\n2\u03c0\u2206t\u03c3 exp\n( \u2212(log \u2206t \u2212 \u00b5) 2\n2\u03c32\n) (2.10)\nSuch a log-normal distribution for inter-arrival times facilitates faster and closed sampling with stable convergence [186], i.e., we can sample the inter-arrival times simply using a normaldistribution based sampling. Standard intensity-driven MTPP models rely on Ogata\u2019s thinning [160] or an inverse sampling [201], which operates using iterative acceptance-rejection"
        },
        {
            "heading": "14 Background",
            "text": "protocol and, therefore, can be expensive."
        },
        {
            "heading": "2.3 Graph Convolution/Attention Networks",
            "text": "In some applications, data cannot be represented in a Euclidean space and is, thus, represented as graphs with complex relationships and interdependency between nodes. Modeling this complex nature has led to several developments in neural graph networks that bridge the gap between deep learning and spectral graph theory [40, 103]. The most popular GNN models are graph-convolution networks (GCN) [103] and graph-attention networks (GAT) [207]. Both these approaches work on the principle of neighborhood aggregation; however, GCN explicitly assigns a non-parametric weight during the aggregation process and implicitly captures the weight via an end-to-end neural network architecture to better model the importance between neighborhood nodes. However, GAT uses a learnable attention weight for each node in the neighborhood [8]."
        },
        {
            "heading": "2.3.1 Graphs in Recommendation Systems",
            "text": "Traditional graph embedding approaches focused on incorporating the node neighborhood proximity in a classical graph in their embedding learning process [66, 228]. In detail, He et al. [85] adopts a label propagation mechanism to capture the inter-node influence and hence the collaborative filtering effect. Later, it determines the most probable purchases for a user via the items she has interacted with based on the structural similarity between the historical purchases and the new target item. However, the performance of these approaches is inferior to model-based CF methods since they do not optimize a recommendation-specific loss function. The recently proposed graph convolutional networks (GCNs) [103] have shown an incredible prowess for recommendation tasks in user-item graphs. The attention-based variant of GCNs, graph attention networks (GATs) [207] are used for recommender systems in information networks [49, 212], traffic networks [69, 123] and social networks [235, 244]. Furthermore, the heterogeneous nature of these information networks comprises multi-faceted influences that led to approaches with dual-GCNs across both user and item domains [49, 249]. However, these models have limited ability to learn highly heterogeneous data, e.g., a POI network with disparate weights, location-category as node feature, and varied sizes. Thus, limited research has been done on utilizing these models for spatial recommendations."
        },
        {
            "heading": "2.4 Self-Attention",
            "text": "Attention in deep learning is a widely used technique to get a weighted aggregation of different components of a model [8]. Vaswani et al. [206] proposed an attention-based sequence-\nBackground 15\nto-sequence method that achieved state-of-the-art performance in machine translation. Thus, there have been several applications of such models in domains including product recommendations [99, 115], modeling spatial mobility preferences of users [125], image generation [167], etc. Here, we present a detailed description of the underlying sequence encoder-decoder module in Vaswani et al. [206]. Here, we provide an overview of the self-attention framework used in this thesis.\nA self-attention mechanism requires a fixed length input sequence, say R = [r1, r2, \u00b7 \u00b7 \u00b7 , rn], where ri \u2208 RD denotes the embedding of the event in the i-th position and n denotes the sequence length. The process for obtaining these embeddings can vary as per the modeling problem. For e.g., in NLP tasks, the embeddings are representations of words in a sentence [42, 206], and in recommender systems, they represent the items bought by a user [99, 115].\nUsing the input embedding sequence, a self-attention model first injects a position encoding ri \u2192 ri + pi, to every event embedding. Later, at every index, it calculates the weighted sum of all embeddings using three linear transformations as below:\nR = Attention(RWQ,RWK ,RW V ), (2.11)\nwhere, WQ,WK ,W V \u2208 RD\u00d7D denote the projection matrices for queries, keys, and values respectively. R denotes the sequence of output embeddings and the function Attention(\u00b7) is defined as:\nAttention(Q,K,V ) = softmax ( QK>\u221a D ) V , (2.12)\nIn addition, it introduces a causality between events in the sequence by forbidding all links betweenQi andKj where j > i.\nTo introduce a non-linearity into the present formulation, the self-attention procedure includes a point-wise feed-forward layer described below:\nV i = PFFN(Ri) = ReLU(RiW 1 + b1)W 2 + b2, (2.13)\nLastly, based on the design choice, a sequence-to-sequence mechanism based on self-attention may include multiple stacked attention blocks, residual connections, layer normalization, and dropout between layers [206]. The final output embedding at index i, in this case, V i, represents a weighted aggregation of the history, i.e., all the events that have occurred before the current index. In recent years, neural MTPP models have used self-attention as the underlying mechanism to capture the dynamics of a sequence [74, 77, 184, 243, 251]."
        },
        {
            "heading": "16 Background",
            "text": "Thus, in this chapter, we have a detailed overview of the techniques necessary for understanding the subsequent chapters of this thesis. In the following chapters, we describe the key technical contributions of this thesis.\nPart I"
        },
        {
            "heading": "Temporal Sequences with Missing Events",
            "text": "17\nChapter 3"
        },
        {
            "heading": "Overcoming Missing Events in",
            "text": ""
        },
        {
            "heading": "Continuous-Time Sequences",
            "text": ""
        },
        {
            "heading": "3.1 Introduction",
            "text": "Designing an accurate recommender system is conditioned on the availability of high-quality sequential data with no missing events. In this chapter, we study the problems associated with missing events in continuous time sequences and propose our solution to model and impute these missing events. In recent years, marked temporal point processes (MTPP) have shown an outstanding potential to characterize asynchronous events localized in continuous time. However, most of the MTPP models [45, 92, 203, 210, 243, 251] \u2014 with a few recent exceptions [148, 189] \u2014 have considered only the settings where the training data is completely observed or, in other words, there is no missing observation at all. While working with fully observed data is ideal for understanding any dynamical system, this is not possible in many practical scenarios. We may miss observing events due to constraints such as crawling restrictions by social media platforms; privacy restrictions (certain users may disallow collection of certain types of data); budgetary factors such as data collection for exit polls; or other practical factors, e.g., a patient may not be available at a certain time. This results in the poor predictive performance of MTPP models [45, 243, 251] that skirt this issue.\nStatistical analysis in the presence of missing data has been widely researched in literature in various contexts [24, 69, 192, 238]. Little and Rubin [129] offer a comprehensive survey. It provides three models that capture missing data mechanisms in increasing order of complexity, viz., MCAR (missing completely at random), MAR (missing at random), and MNAR (missing not at random). Recently, Shelton et al. [189] and Mei et al. [148] proposed novel methods\n19"
        },
        {
            "heading": "20 Overcoming Missing Events in Continuous-Time Sequences",
            "text": "to impute missing events in continuous-time sequences via MTPP from the viewpoint of the MNAR mechanism. However, they focus on imputing missing data in between prior available observed events rather than predicting observed events in the face of missing events. Moreover, they deploy expensive learning and sampling mechanisms, which make them often intractable in practice, especially in the case of learning from a sequence of streaming events. For example, Shelton et al. [189] applies an expensive MCMC sampling procedure to draw missing events between the observation pairs, which requires several simulations of the sampling procedure upon the arrival of a new sample. On the other hand, Mei et al. [148] uses a bi-directional RNN, which re-generates all missing events by making a completely new pass over the backward RNN whenever one new observation arrives. As a consequence, it suffers from quadratic complexity with respect to the number of observed events. On the other hand, the proposal of Shelton et al. [189] depends on a predefined influence structure among the underlying events, which is available in linear multivariate parameterized point processes. In more complex point processes with neural architectures, such a structure is not explicitly defined, which further limits their applicability in real-world settings."
        },
        {
            "heading": "3.1.1 Our Contribution",
            "text": "In this chapter, we present our solution to overcome the above limitations via a novel modeling framework for point processes called IMTPP (Intermittently-observed Marked Temporal Point Processes) [75], which characterizes the dynamics of both observed and missing events as two coupled MTPP, conditioned on the history of previous events. In our setup, the generation of missing events depends both on the previously occurring missing events as well as the previously observed events. Therefore, they are MNAR (missing not at random), in the context of the literature on missing data [129]. In contrast to the prior models [148, 189], IMTPP aims to learn the dynamics of both observed and missing events, rather than simply imputing missing events in between the known observed events, which is reflected in its superior predictive power over those existing models.\nPrecisely, IMTPP represents the missing events as latent random variables, which, together with the previously observed events, seed the generative processes of the subsequent observed and missing events. Then it deploys three generative models\u2014 MTPP for observed events, prior MTPP for missing events, and posterior MTPP for missing events using recurrent neural networks (RNN) that capture the nonlinear influence of the past events. We also show that such a formulation can be easily extended to imputation tasks and still achieve significant performance gains over other models. IMTPP includes several technical innovations over other models that significantly boost its modeling and prediction accuracy. In detail, our contributions are:"
        },
        {
            "heading": "Overcoming Missing Events in Continuous-Time Sequences 21",
            "text": "\u2022 In a notable departure from almost all existing MTPP models [39, 45, 148] which rely strongly on conditional intensity functions, we use a lognormal distribution to sample the arrival times of the events. As suggested by Shchur et al. [186], such distribution allows efficient sampling as well as a more accurate prediction than the standard intensity function-based models. \u2022 The built-in RNNs in our model are designed to make forward computations. Therefore, they incrementally update the dynamics upon the arrival of a new observation. Consequently, unlike the prior models, it does not require us to re-generate all the missing events in response to the arrival of an observation, which significantly boosts the efficiency of both learning and prediction as compared to both the previous approaches [148, 189].\nOur modeling framework allows us to train IMTPP using an efficient variational inference method, that maximizes the evidence lower bound (ELBO) of the likelihood of the observed events. Such a formulation highlights the connection of our model with the variational autoencoders (VAEs) [30]. However, in sharp contrast to traditional VAEs, where the random noises or seeds often do not have immediate interpretations, our random variables bear concrete physical explanations, i.e., they are missing events, which renders our model more explainable than an off-the-shelf VAE. In addition, an extension to IMTPP, called IMTPP++ can identify the optimal positions of missing events in a sequence. Finally, we perform exhaustive experiments with eight diverse real-world datasets across different domains to show that IMTPP can model missing observations within a stream of observed events and enhance the predictive power of the original generative process for a full observation scenario."
        },
        {
            "heading": "3.2 Related Work",
            "text": "Our work is broadly related to the literature on (i) missing data models for discrete-time series; and (ii) missing data models for temporal point processes."
        },
        {
            "heading": "3.2.1 Missing Data Models for Discrete-Time Series",
            "text": "Our current work is also related to existing missing data models for discrete-time series, which do not necessarily consider MTPP. In principle, training sequential models in the presence of missing data is essential for robust predictions across a wide range of applications e.g., traffic networks [69], modeling disease propagation [9] and wearable sensor data [222]. Motivated by these applications, there has been a significant effort in recent years in designing learning tools for sequence models with missing data [24, 136, 238]. In particular, the proposal by Che et al. [24] compensates for a missing event by applying a time decay factor to the previous hidden state in a GRU before calculating the new hidden state. Yoon et al. [238] captures the effect"
        },
        {
            "heading": "22 Overcoming Missing Events in Continuous-Time Sequences",
            "text": "of missing data by incorporating future information using bidirectional-RNNs. While these approaches do not provide explicit generative models of missing events, a few other models generate them by imputing them in between available observations. For example, Cao et al. [21] proposed a method of imputing missing events using a bi-directional RNN; Luo et al. [136] employs a generative adversarial approach for generating missing events conditioned on the observed events. Luo et al. [138] and Li et al. [124] are used for imputing in time-series. However, they cannot be used to sample marks of missing events and, thus, cannot be extended to imputation in continuous-time event sequences. Thus, these models are complementary to our proposal as they do not work with temporal point processes."
        },
        {
            "heading": "3.2.2 Missing Data Models for Temporal Point Process",
            "text": "Very recently, there has been a growing interest in modeling MTPP in the presence of missing observations. However, they deploy expensive learning and sampling mechanisms on an apriori-known complete sequence of observations. More specifically, Shelton et al. [189] proposed a way of incorporating missing data by generating children events for the observed events. They rely strongly on an expensive MCMC sampling procedure to draw missing events between the observation pairs. In order to adapt to such a protocol, we need to run the entire sampling routine several times whenever a new observation arrives. Such a method is extremely time-consuming and often intractable in practice. Moreover, they require an underlying multivariate parenthood structure that is not available in a complicated neural setting. Our work is closely related to the proposal by Mei et. al. [148]. It employs two RNNs, in which the forward RNN\u2014 initialized on t = 0 \u2013 models the observation sequence, and the backward RNN \u2013 initialized on t = T \u2013 models the missing observations. To operate a backward RNN in an online setting, we need to pass the entire sequence of observations into it whenever a new sample arrives, which in turn makes it super expensive in practice. While re-running these methods after batch arrivals\u2014 instead of re-running after every single arrival\u2014 may appear as a compromised solution; however, that is ineffective in practice. Other approaches include the proposal by Xu et al. [227], which proposes a training method for MTPP when the future and past events of a sequence window are censored; the work by Rasmussen [171], which assumes certain characteristics of missing data, and Zhuang et al. [250] is limited to spatial modeling."
        },
        {
            "heading": "3.3 Problem Setup",
            "text": "In this section, we first introduce the notations and then the setup of our problem of learning marked temporal point processes with observed and missing events over continuous time."
        },
        {
            "heading": "Overcoming Missing Events in Continuous-Time Sequences 23",
            "text": ""
        },
        {
            "heading": "3.3.1 Preliminaries and Notations",
            "text": "We characterize an MTPP using the sequence of observed events Sk = {ei = (xi, ti)|i \u2208 [k], ti < ti+1}, where the details of the notations is given in chapter 2. As highlighted in Section 3.1, there may be instances where an event has actually taken place but not recorded with the observed event sequence S. To this end, we introduce the MTPP for missing events\u2014 a latent MTPP\u2014 which is characterized by a sequence of hidden eventsMr = { j = (yj, \u03c4j)|j \u2208 [r], \u03c4j < \u03c4j+1}where \u03c4j \u2208 R+ and yj \u2208 C are the times and the marks of the j-th missing events. Therefore,Mr defines the set of first r missing events. Moreover, we denote the inter-arrival times of the missing events as, \u2206\u03c4,r = \u03c4r \u2212 \u03c4r\u22121. Note that \u03c4\u2022, y\u2022,M\u2022 and \u2206\u03c4,\u2022 for the MTPP of missing events share similar meanings with t\u2022, x\u2022, S\u2022 and \u2206t,\u2022 respectively for the MTPP of observed events. Here we further define two critical notations k and k as follows:\nk = argmin r {\u03c4r | tk < \u03c4r < tk+1}, (3.1)\nk = argmax r {\u03c4r | tk < \u03c4r < tk+1} (3.2)\nHere, k and k are the indices of the first and the last missing events respectively, among those which have arrived between k-th and k + 1-th observed events. Figure 3.2 (a) illustrates our setup. In practice, the arrival times (t and \u03c4 ) of both observed and missing events are continuous random variables, whereas the marks (x and y) are discrete random variables. Therefore, following the state-of-the-art MTPP models [45, 147], we model a density function to draw the event timings and a probability mass function to draw marks."
        },
        {
            "heading": "3.3.2 Overcoming Missing Events",
            "text": "Our goal via IMTPP is to design an MTPP model which can generate the subsequent observed (ek+1) and missing events ( r+1) in a recursive manner, conditioned on the history of all events Sk \u222a Mr that have occurred thus far. Given the input sequence of observations SK consisting of the first K observed events {e1, e2, ..., eK}, we first train our generative model and then recursively predict the next observed event eK+1. Though IMTPP can also predict the missing events, we evaluate the predictive performance only on observed events since the missing events are not available in practice. We also evaluate the imputation performance of our model by predicting synthetically deleted events. Note that this setting is in contrast to the proposal of [148] that aims to impute the missing events based on the entire observation sequence SK using a bi-directional RNN. Specifically, whenever one new observation arrives, it re-generates all missing events by making a completely new pass over the backward RNN. As a result, such an imputation method not only suffers from quadratic complexity with respect to the number"
        },
        {
            "heading": "24 Overcoming Missing Events in Continuous-Time Sequences",
            "text": "of observed events, but it also has limited practicability, as future events are not available beyond the current timestamp in a streaming or online setting. Furthermore, their approach is tailored towards imputing missing events based on complete observations and is not well suited to predicting observed events in the face of missing observations. In contrast, our proposal is designed to generate subsequent observed and missing events in between previously observed events. Therefore, it does not require to re-generate all missing events whenever a new observation arrives, which allows it to enjoy a linear complexity with respect to the number of observed events and can be easily extended to online settings."
        },
        {
            "heading": "3.4 Components of IMTPP",
            "text": "At the very outset, IMTPP, our proposed generative model, connects two stochastic processes \u2013 one for the observed events, which samples the observed, and the other for the missing events \u2013 based on the history of previously generated missing and observed events. Note, that the sequence of training events that are given as input to IMTPP consists of only the observed events. We model the missing event sequence through latent random variables, which, along with the previously observed events, drive a unified generative model for the complete (observed and missing) event sequence. The overall neural architecture of IMTPP, including the different processes for observed and missing events, is given in Figure 3.1.\nMore specifically, given a stream of observed events denoted as SK = {e1 = (x1, t1), e2 = (x2, t2), . . . , eK = (xK , tK)}, if we use the maximum likelihood principle to train IMTPP, then we should maximize the marginal log-likelihood of the observed stream of events, i.e., log p(SK). However, computation of log p(SK) demands marginalization with respect to the"
        },
        {
            "heading": "Overcoming Missing Events in Continuous-Time Sequences 25",
            "text": "set of latent missing events MK\u22121, which is typically intractable. Therefore, we resort to maximizing a variational lower bound or evidence lower bound (ELBO) of the log-likelihood of the observed stream of events SK . Mathematically, we note that:\np(SK) = K\u22121\u220f k=0 \u222b Mk p(ek+1 | Sk,Mk) p(Mk) d\u03c9(Mk)\n= Eq(MK\u22121 | SK) K\u22121\u220f k=0\np(ek+1 | Sk,Mk) k\u220f r=k p( r | Sk,Mr\u22121)\nk\u220f r=k q( r | ek+1,Sk,Mr\u22121) (3.3)\nwhere \u03c9(M) is the measure of the setM, q is an approximate posterior distribution that aims to interpolate missing events r within the interval (tk, tk+1), based on the knowledge of the next observed event ek, along with all previous events Sk \u222aMr\u22121, and k, k. Recall that k (k) is the index r of the first (last) missing event r among those which have arrived between k-th and k + 1-th observed events, i.e., k = argminr{\u03c4r | tk < \u03c4r < tk+1} and k = argmaxr{\u03c4r | tk < \u03c4r < tk+1}. Next, by applying Jensen inequality1 over the likelihood, log p(SK) is at-least: Eq(MK\u22121 | SK ) K\u22121\u2211 k=0 log p(ek+1 | Sk,Mk)\u2212 K\u22121\u2211 k=0 k\u2211 r=k KL [ q( r | ek+1,Sk,Mr\u22121)||p( r | Sk,Mr\u22121) ] , (3.4) While the above inequality holds for any distribution q, the quality of this lower bound depends on the expressivity of q, which we would model using a deep recurrent neural network. Moreover, the above lower bound suggests that our model consists of the following components.\n\u2022 MTPP for observed events. The distribution p(ek+1 | Sk,Mk) models the MTPP for observed events, which generates the (k + 1)-th event, ek+1, based on the history of all k observed events Sk and all missing eventsMk generated so far. \u2022 Prior MTPP for missing events. The distribution p( r | Sk,Mr\u22121) is the prior model of the MTPP for missing events. It generates the r-th missing event r after the observed event ek, based on the prior information\u2014 the history with all k observed events Sk and all missing eventsMr\u22121 generated so far. \u2022 Posterior MTPP for missing events. Given the set of observed events represented by Sk+1 = {e1, e2, . . . , ek+1}, the distribution q( r | ek+1,Sk,Mr\u22121) generates the r-th missing event r, after the knowledge of the subsequent observed event ek+1 is taken into account, along with information about all previously observed events Sk and all missing eventsMr\u22121 generated so far.\n1https://en.wikipedia.org/wiki/Jensen\u2019s_inequality\n26 Overcoming Missing Events in Continuous-Time Sequences\n<latexit sha1_base64=\"IvLSn1C3yERO2JDv2jZEDDCcnrM=\">AAACAXicbVDLSgMxFM3UV62vUTeCm2ARXJQyIxVdFt24rGAf0A5DJk3b0CQzJJlKGcaNv+LGhSJu/Qt3/o3pdBbaeuDCyTn3kntPEDGqtON8W4WV1bX1jeJmaWt7Z3fP3j9oqTCWmDRxyELZCZAijArS1FQz0okkQTxgpB2Mb2Z+e0KkoqG419OIeBwNBR1QjLSRfPuoF/EgeUj9RFcmaQVmT5T6E98uO1UnA1wmbk7KIEfDt796/RDHnAiNGVKq6zqR9hIkNcWMpKVerEiE8BgNSddQgThRXpJdkMJTo/ThIJSmhIaZ+nsiQVypKQ9MJ0d6pBa9mfif14314MpLqIhiTQSefzSIGdQhnMUB+1QSrNnUEIQlNbtCPEISYW1CK5kQ3MWTl0nrvOrWqhd3tXL9Oo+jCI7BCTgDLrgEdXALGqAJMHgEz+AVvFlP1ov1bn3MWwtWPnMI/sD6/AHmApcv</latexit>\n<latexit sha1_base64=\"7nWGp6aX6bd1MBEULKoQHavVwBA=\">AAAB9HicbVBNSwMxEJ2tX7V+VT16CRbBg5Rdqeix6MVjBfsB7VKyadqGJtk1yVbLsr/DiwdFvPpjvPlvTNs9aOuDgcd7M8zMCyLOtHHdbye3srq2vpHfLGxt7+zuFfcPGjqMFaF1EvJQtQKsKWeS1g0znLYiRbEIOG0Go5up3xxTpVko780kor7AA8n6jGBjJb8TiSB5TLvJ09k47RZLbtmdAS0TLyMlyFDrFr86vZDEgkpDONa67bmR8ROsDCOcpoVOrGmEyQgPaNtSiQXVfjI7OkUnVumhfqhsSYNm6u+JBAutJyKwnQKboV70puJ/Xjs2/Ss/YTKKDZVkvqgfc2RCNE0A9ZiixPCJJZgoZm9FZIgVJsbmVLAheIsvL5PGedmrlC/uKqXqdRZHHo7gGE7Bg0uowi3UoA4EHuAZXuHNGTsvzrvzMW/NOdnMIfyB8/kDXf+Sfw==</latexit>\n<latexit sha1_base64=\"3DnS3vImeIO9jz1IyYjxDR65oEU=\">AAAB6HicbVDLSgNBEOyNrxhfUY9eBoMgCGFXInoMevGYgHlAsoTZSW8yZnZ2mZkVQsgXePGgiFc/yZt/4yTZgyYWNBRV3XR3BYng2rjut5NbW9/Y3MpvF3Z29/YPiodHTR2nimGDxSJW7YBqFFxiw3AjsJ0opFEgsBWM7mZ+6wmV5rF8MOME/YgOJA85o8ZK9YteseSW3TnIKvEyUoIMtV7xq9uPWRqhNExQrTuemxh/QpXhTOC00E01JpSN6AA7lkoaofYn80On5MwqfRLGypY0ZK7+npjQSOtxFNjOiJqhXvZm4n9eJzXhjT/hMkkNSrZYFKaCmJjMviZ9rpAZMbaEMsXtrYQNqaLM2GwKNgRv+eVV0rwse5XyVb1Sqt5mceThBE7hHDy4hircQw0awADhGV7hzXl0Xpx352PRmnOymWP4A+fzB3QdjLc=</latexit>\n<latexit sha1_base64=\"10zWS9OWZHJUzLGj6ZpET3V8+Co=\">AAACBnicbVDLSgMxFM3UV62vUZciBIvgopQZqeiy6MZlBfuAdhgyaaYNTTJDkimUYVZu/BU3LhRx6ze4829M21nU1gMXTs65l9x7gphRpR3nxyqsrW9sbhW3Szu7e/sH9uFRS0WJxKSJIxbJToAUYVSQpqaakU4sCeIBI+1gdDf122MiFY3Eo57ExONoIGhIMdJG8u3TXsyDtJ35qaqorAIXnuOs4ttlp+rMAFeJm5MyyNHw7e9eP8IJJ0JjhpTquk6svRRJTTEjWamXKBIjPEID0jVUIE6Ul87OyOC5UfowjKQpoeFMXZxIEVdqwgPTyZEeqmVvKv7ndRMd3ngpFXGiicDzj8KEQR3BaSawTyXBmk0MQVhSsyvEQyQR1ia5kgnBXT55lbQuq26tevVQK9dv8ziK4AScgQvggmtQB/egAZoAgyfwAt7Au/VsvVof1ue8tWDlM8fgD6yvXzYzmPY=</latexit>\n<latexit sha1_base64=\"I0BAguJoKmtzIpAGE8TBlv3UxhU=\">AAACAnicbVDLSsNAFL3xWesr6krcDBbBRSmJVHRZdOOygn1AG8JkOm2HTiZhZqKUENz4K25cKOLWr3Dn3zhts9DWAxfOnHMvc+8JYs6Udpxva2l5ZXVtvbBR3Nza3tm19/abKkokoQ0S8Ui2A6woZ4I2NNOctmNJcRhw2gpG1xO/dU+lYpG40+OYeiEeCNZnBGsj+fYh6sZhkD5kfqrKo6w8e+LMV75dcirOFGiRuDkpQY66b391exFJQio04VipjuvE2kux1IxwmhW7iaIxJiM8oB1DBQ6p8tLpCRk6MUoP9SNpSmg0VX9PpDhUahwGpjPEeqjmvYn4n9dJdP/SS5mIE00FmX3UTzjSEZrkgXpMUqL52BBMJDO7IjLEEhNtUiuaENz5kxdJ86ziVivnt9VS7SqPowBHcAyn4MIF1OAG6tAAAo/wDK/wZj1ZL9a79TFrXbLymQP4A+vzByd0l0o=</latexit>\n<latexit sha1_base64=\"iSFQSzdozAoOLDhNGsPV8dHO2Kg=\">AAACBHicbVDLSgMxFM3UV62vUZfdBIvgxjIjFV0W3bisYB/QGYZMmmnDJJkhyQhl6MKNv+LGhSJu/Qh3/o1pO4K2HggczrmXm3PClFGlHefLKq2srq1vlDcrW9s7u3v2/kFHJZnEpI0TlsheiBRhVJC2ppqRXioJ4iEj3TC+nvrdeyIVTcSdHqfE52goaEQx0kYK7KqX8jBXkyCPT90J9HQCf5Q4sGtO3ZkBLhO3IDVQoBXYn94gwRknQmOGlOq7Tqr9HElNMSOTipcpkiIcoyHpGyoQJ8rPZyEm8NgoAxgl0jyh4Uz9vZEjrtSYh2aSIz1Si95U/M/rZzq69HMq0kwTgeeHooxBE3XaCBxQSbBmY0MQltT8FeIRkghr01vFlOAuRl4mnbO626if3zZqzauijjKogiNwAlxwAZrgBrRAG2DwAJ7AC3i1Hq1n6816n4+WrGLnEPyB9fEN0BqYNg==</latexit>\n<latexit sha1_base64=\"1iHBemziWrvYqeHh4k4f2dVDMaI=\">AAACEHicbVDLSgMxFM3UV62vqks3wSK6KGVGKrosunFZwT6gHYZMmmlDk5khuSOUYT7Bjb/ixoUibl26829MH0JtPRA495x7ubnHjwXXYNvfVm5ldW19I79Z2Nre2d0r7h80dZQoyho0EpFq+0QzwUPWAA6CtWPFiPQFa/nDm7HfemBK8yi8h1HMXEn6IQ84JWAkr3jajaWftjIvhbLOyniulL8lyTzwiiW7Yk+Al4kzIyU0Q90rfnV7EU0kC4EKonXHsWNwU6KAU8GyQjfRLCZ0SPqsY2hIJNNuOjkowydG6eEgUuaFgCfq/ERKpNYj6ZtOSWCgF72x+J/XSSC4clMexgmwkE4XBYnAEOFxOrjHFaMgRoYQqrj5K6YDoggFk2HBhOAsnrxMmucVp1q5uKuWatezOPLoCB2jM+SgS1RDt6iOGoiiR/SMXtGb9WS9WO/Wx7Q1Z81mDtEfWJ8/CcydOg==</latexit>\n<latexit sha1_base64=\"U7AW9mQbAr7EmhBz7XjX3Ce7fVE=\">AAAB9HicdVDLSsNAFJ3UV62vqks3g0VwY5iERNtd0Y3LCvYBbSiT6aQdOnk4MymWkO9w40IRt36MO//G6UNQ0QMXDufcy733+AlnUiH0YRRWVtfWN4qbpa3tnd298v5BS8apILRJYh6Ljo8l5SyiTcUUp51EUBz6nLb98dXMb0+okCyObtU0oV6IhxELGMFKS14vCf3sPu9n4zMr75cryER21XVsiEzbRTWrpomLrNq5Ay0TzVEBSzT65ffeICZpSCNFOJaya6FEeRkWihFO81IvlTTBZIyHtKtphEMqvWx+dA5PtDKAQSx0RQrO1e8TGQ6lnIa+7gyxGsnf3kz8y+umKqh6GYuSVNGILBYFKYcqhrME4IAJShSfaoKJYPpWSEZYYKJ0TiUdwten8H/Ssk3LMd0bp1K/XMZRBEfgGJwCC1yAOrgGDdAEBNyBB/AEno2J8Wi8GK+L1oKxnDkEP2C8fQJMS5J3</latexit>\n<latexit sha1_base64=\"56uHtmYdlptV3mBeu9A0ZPepG0Q=\">AAAB/HicbVDLSgNBEJz1GeNrNUcvg0HwIGFXInoM6sFjBPOAbAizk04yZPbBTK8QlvVXvHhQxKsf4s2/cZLsQRMLGoqqbrq7/FgKjY7zba2srq1vbBa2its7u3v79sFhU0eJ4tDgkYxU22capAihgQIltGMFLPAltPzxzdRvPYLSIgofcBJDN2DDUAwEZ2iknl1K46yXejgCZGfeLUhkWc8uOxVnBrpM3JyUSY56z/7y+hFPAgiRS6Z1x3Vi7KZMoeASsqKXaIgZH7MhdAwNWQC6m86Oz+iJUfp0EClTIdKZ+nsiZYHWk8A3nQHDkV70puJ/XifBwVU3FWGcIIR8vmiQSIoRnSZB+0IBRzkxhHElzK2Uj5hiHE1eRROCu/jyMmmeV9xq5eK+Wq5d53EUyBE5JqfEJZekRu5InTQIJxPyTF7Jm/VkvVjv1se8dcXKZ0rkD6zPHx7tlRQ=</latexit>\n<latexit sha1_base64=\"VD6Be3xssqIavdN04myoL1ym8ZA=\">AAACAnicbVDLSsNAFJ34rPUVdSVuBovgQkoiFV0W3bisYNpCE8JkOmmHzkzCzEQoIbjxV9y4UMStX+HOv3HaZqGtBy73cM69zNwTpYwq7Tjf1tLyyuraemWjurm1vbNr7+23VZJJTDycsER2I6QIo4J4mmpGuqkkiEeMdKLRzcTvPBCpaCLu9TglAUcDQWOKkTZSaB/6KY9yL8z9KGOM6LOyF0Vo15y6MwVcJG5JaqBEK7S//H6CM06Exgwp1XOdVAc5kppiRoqqnymSIjxCA9IzVCBOVJBPTyjgiVH6ME6kKaHhVP29kSOu1JhHZpIjPVTz3kT8z+tlOr4KcirSTBOBZw/FGYM6gZM8YJ9KgjUbG4KwpOavEA+RRFib1KomBHf+5EXSPq+7jfrFXaPWvC7jqIAjcAxOgQsuQRPcghbwAAaP4Bm8gjfryXqx3q2P2eiSVe4cgD+wPn8AEj+X4A==</latexit>\n<latexit sha1_base64=\"pFjsGzuG926XMggqRe4IEatf5tM=\">AAAB/HicdVDJSgNBEO1xjXGL5uilMQiCMvSERJNbUA8eI5gFMkPo6XSSJj0L3TVCGOKvePGgiFc/xJt/Y2cRVPRBweO9Kqrq+bEUGgj5sJaWV1bX1jMb2c2t7Z3d3N5+U0eJYrzBIhmptk81lyLkDRAgeTtWnAa+5C1/dDn1W3dcaRGFtzCOuRfQQSj6glEwUjeXd6+4BNpN4XR04kywq0XQzRWITYqVcqmIiV0sk6pTNaRMnOpZCTs2maGAFqh3c+9uL2JJwENgkmrdcUgMXkoVCCb5JOsmmseUjeiAdwwNacC1l86On+Ajo/RwP1KmQsAz9ftESgOtx4FvOgMKQ/3bm4p/eZ0E+hUvFWGcAA/ZfFE/kRgiPE0C94TiDOTYEMqUMLdiNqSKMjB5ZU0IX5/i/0mzaDslu3xTKtQuFnFk0AE6RMfIQeeohq5RHTUQQ2P0gJ7Qs3VvPVov1uu8dclazOTRD1hvnzN2lII=</latexit>\n<latexit sha1_base64=\"yNCJ+sRnZa97gTxjQuiqQETHxlA=\">AAAB83icdVDLSsNAFJ3UV62vqks3g0UQhDApiba7ohuXFewDmlAm00k7dCYJMxOxhP6GGxeKuPVn3Pk3Th+Cih64cDjnXu69J0w5UxqhD6uwsrq2vlHcLG1t7+zulfcP2irJJKEtkvBEdkOsKGcxbWmmOe2mkmIRctoJx1czv3NHpWJJfKsnKQ0EHsYsYgRrI/n3/Xx85kyhr5jolyvIRtWa51Yhsqseqjt1Qzzk1M9d6NhojgpYotkvv/uDhGSCxppwrFTPQakOciw1I5xOS36maIrJGA9pz9AYC6qCfH7zFJ4YZQCjRJqKNZyr3ydyLJSaiNB0CqxH6rc3E//yepmOakHO4jTTNCaLRVHGoU7gLAA4YJISzSeGYCKZuRWSEZaYaBNTyYTw9Sn8n7SrtuPa3o1baVwu4yiCI3AMToEDLkADXIMmaAECUvAAnsCzlVmP1ov1umgtWMuZQ/AD1tsn466RnQ==</latexit>\n<latexit sha1_base64=\"r+hbt0vdq3NjOIxREhydzj8suzo=\">AAAB83icdVDJSgNBEO2JW4xb1KOXxiB4ydAzTDS5Bb14jGAWSELo6XSSZnoWumuEMOQ3vHhQxKs/482/sbMIKvqg4PFeFVX1/EQKDYR8WLm19Y3Nrfx2YWd3b/+geHjU0nGqGG+yWMaq41PNpYh4EwRI3kkUp6EvedsPrud++54rLeLoDqYJ74d0HImRYBSM1INBgMswyIKyMxsUS8QmbrXiuZjYboXUnJohFeLULjzs2GSBElqhMSi+94YxS0MeAZNU665DEuhnVIFgks8KvVTzhLKAjnnX0IiGXPezxc0zfGaUIR7FylQEeKF+n8hoqPU09E1nSGGif3tz8S+vm8Ko2s9ElKTAI7ZcNEolhhjPA8BDoTgDOTWEMiXMrZhNqKIMTEwFE8LXp/h/0nJtx7Mrt16pfrWKI49O0Ck6Rw66RHV0gxqoiRhK0AN6Qs9Waj1aL9brsjVnrWaO0Q9Yb5+HiZFh</latexit>\n<latexit sha1_base64=\"mz8jU2InqNC3zJVRfwt/hqDygJA=\">AAAB/nicbVDLSsNAFJ34rPUVFVdugkVwISWRii6LblxWsA9oQphMJ+3QySTM3IglBPwVNy4Ucet3uPNvnLRZaOuBgcM593LPnCDhTIFtfxtLyyura+uVjerm1vbOrrm331FxKgltk5jHshdgRTkTtA0MOO0lkuIo4LQbjG8Kv/tApWKxuIdJQr0IDwULGcGgJd88dCMMoyDIWrmfuTCigM8ec9+s2XV7CmuROCWpoRIt3/xyBzFJIyqAcKxU37ET8DIsgRFO86qbKppgMsZD2tdU4IgqL5vGz60TrQysMJb6CbCm6u+NDEdKTaJATxZh1bxXiP95/RTCKy9jIkmBCjI7FKbcgtgqurAGTFICfKIJJpLprBYZYYkJ6MaqugRn/suLpHNedxr1i7tGrXld1lFBR+gYnSIHXaImukUt1EYEZegZvaI348l4Md6Nj9noklHuHKA/MD5/AMUelgQ=</latexit>\n<latexit sha1_base64=\"s9Vo1MH8DLHX39G5frcLWVtJ3aQ=\">AAACAXicdVDLSgNBEJz1bXxFvQheBoPgKWziLiY30YsHDxFMIiQhzE4mccg8lpleMSzx4q948aCIV//Cm3/j5CGoaEFDUdVNd1cUC27B9z+8mdm5+YXFpeXMyura+kZ2c6tmdWIoq1IttLmKiGWCK1YFDoJdxYYRGQlWj/qnI79+w4zlWl3CIGYtSXqKdzkl4KR2dqcZyyhtArsFS9Nz3VPaSCKGw3Y25+f9w3JYKmFHwjDwwxEpBkG5jAt5f4wcmqLSzr43O5omkimggljbKPgxtFJigFPBhplmYllMaJ/0WMNRRSSzrXT8wRDvO6WDu9q4UoDH6veJlEhrBzJynZLAtf3tjcS/vEYC3VIr5SpOgCk6WdRNBAaNR3HgDjeMghg4Qqjh7lZMr4khFFxoGRfC16f4f1Ir5gtBPrwIcscn0ziW0C7aQweogI7QMTpDFVRFFN2hB/SEnr1779F78V4nrTPedGYb/YD39glSoJgg</latexit>\n<latexit sha1_base64=\"REot61VLIHrAokEBYNyt6oFsEVc=\">AAAB/3icdVDNS8MwHE39nPOrKnjxEhyCp9LJZrfb0IvHCe4D1lLSLNvCkrYkqTBqD/4rXjwo4tV/w5v/jelWQUUfBB7vvV/yywtiRqWy7Q9jaXlldW29tFHe3Nre2TX39rsySgQmHRyxSPQDJAmjIekoqhjpx4IgHjDSC6aXud+7JULSKLxRs5h4HI1DOqIYKS355qEb8yDlmZ+6kc7l16TTLPPNim05jUbdbkLbsufIybntOE1YLZQKKND2zXd3GOGEk1BhhqQcVO1YeSkSimJGsrKbSBIjPEVjMtA0RJxIL53vn8ETrQzhKBL6hArO1e8TKeJSznigkxypifzt5eJf3iBRo4aX0jBOFAnx4qFRwqCKYF4GHFJBsGIzTRAWVO8K8QQJhJWurKxL+Pop/J90z6xqzapf1yqti6KOEjgCx+AUVIEDWuAKtEEHYHAHHsATeDbujUfjxXhdRJeMYuYA/IDx9gnTQ5dL</latexit>\n<latexit sha1_base64=\"3DnS3vImeIO9jz1IyYjxDR65oEU=\">AAAB6HicbVDLSgNBEOyNrxhfUY9eBoMgCGFXInoMevGYgHlAsoTZSW8yZnZ2mZkVQsgXePGgiFc/yZt/4yTZgyYWNBRV3XR3BYng2rjut5NbW9/Y3MpvF3Z29/YPiodHTR2nimGDxSJW7YBqFFxiw3AjsJ0opFEgsBWM7mZ+6wmV5rF8MOME/YgOJA85o8ZK9YteseSW3TnIKvEyUoIMtV7xq9uPWRqhNExQrTuemxh/QpXhTOC00E01JpSN6AA7lkoaofYn80On5MwqfRLGypY0ZK7+npjQSOtxFNjOiJqhXvZm4n9eJzXhjT/hMkkNSrZYFKaCmJjMviZ9rpAZMbaEMsXtrYQNqaLM2GwKNgRv+eVV0rwse5XyVb1Sqt5mceThBE7hHDy4hircQw0awADhGV7hzXl0Xpx352PRmnOymWP4A+fzB3QdjLc=</latexit>\n<latexit sha1_base64=\"hsoMvyWDtqYhPDBy61dyHySSc0A=\">AAAB/3icdVDLSsNAFJ34rPUVFdy4GSyCm4akb3dFNy4r2Ae0IUymk3bo5MHMRCgxC3/FjQtF3Pob7vwbJ32Aih64cDjnXu69x40YFdI0P7WV1bX1jc3cVn57Z3dvXz847Igw5pi0cchC3nORIIwGpC2pZKQXcYJ8l5GuO7nK/O4d4YKGwa2cRsT20SigHsVIKsnRjwcSxU7Ci1YKi0teSh29YBr1RqNcK0PTMGfISK1yUa9Ca6EUwAItR/8YDEMc+ySQmCEh+pYZSTtBXFLMSJofxIJECE/QiPQVDZBPhJ3M7k/hmVKG0Au5qkDCmfp9IkG+EFPfVZ0+kmPx28vEv7x+LL2GndAgiiUJ8HyRFzMoQ5iFAYeUEyzZVBGEOVW3QjxGHGGpIsurEJafwv9Jp2RYFaN6Uyk0Lxdx5MAJOAXnwAJ10ATXoAXaAIN78AiewYv2oD1pr9rbvHVFW8wcgR/Q3r8AZKCVuQ==</latexit>\n<latexit sha1_base64=\"u0hrz+0nD91QkA9CsvS+KiZ/bFY=\">AAACDnicbZDLSsNAFIYn9VbrLerSzWApuCglkYoui25cVrAXaEI4mU7boTNJmJkIJeQJ3Pgqblwo4ta1O9/G6WWhrT8M/HznHM6cP0w4U9pxvq3C2vrG5lZxu7Szu7d/YB8etVWcSkJbJOax7IagKGcRbWmmOe0mkoIIOe2E45tpvfNApWJxdK8nCfUFDCM2YAS0QYFd8RIRZsM8yDwNadUbghCQV/EMh3kwB4FddmrOTHjVuAtTRgs1A/vL68ckFTTShINSPddJtJ+B1Ixwmpe8VNEEyBiGtGdsBIIqP5udk+OKIX08iKV5kcYz+nsiA6HURISmU4AeqeXaFP5X66V6cOVnLEpSTSMyXzRIOdYxnmaD+0xSovnEGCCSmb9iMgIJRJsESyYEd/nkVdM+r7n12sVdvdy4XsRRRCfoFJ0hF12iBrpFTdRCBD2iZ/SK3qwn68V6tz7mrQVrMXOM/sj6/AHm9Zym</latexit>\n<latexit sha1_base64=\"pFDMHO4bOfKma37Dwzkg7hy+GzI=\">AAAB/3icbVBNS8NAEN34WetXVPDiZbEIHqQkUtFj0YvHCvYDmlAm2226dDcJuxuhxBz8K148KOLVv+HNf+O2zUFbHww83pthZl6QcKa043xbS8srq2vrpY3y5tb2zq69t99ScSoJbZKYx7ITgKKcRbSpmea0k0gKIuC0HYxuJn77gUrF4uhejxPqCwgjNmAEtJF69iH2EhFkYd7LPA3pmReCEJD37IpTdabAi8QtSAUVaPTsL68fk1TQSBMOSnVdJ9F+BlIzwmle9lJFEyAjCGnX0AgEVX42vT/HJ0bp40EsTUUaT9XfExkIpcYiMJ0C9FDNexPxP6+b6sGVn7EoSTWNyGzRIOVYx3gSBu4zSYnmY0OASGZuxWQIEog2kZVNCO78y4ukdV51a9WLu1qlfl3EUUJH6BidIhddojq6RQ3URAQ9omf0it6sJ+vFerc+Zq1LVjFzgP7A+vwBIsKWMQ==</latexit>\n<latexit sha1_base64=\"FngvNzVJMxVFqaVzfyjqdi063y8=\">AAACCnicbVC7TsMwFHXKq5RXgJHFUCExVFWCimCsYICxSPQhNVHkuG5r1XYi20Gqosws/AoLAwix8gVs/A1um6G0HOlKx+fcK997wphRpR3nxyqsrK6tbxQ3S1vbO7t79v5BS0WJxKSJIxbJTogUYVSQpqaakU4sCeIhI+1wdDPx249EKhqJBz2Oic/RQNA+xUgbKbCPvZiH6W0WpLzCswqce3oDxDnKArvsVJ0p4DJxc1IGORqB/e31IpxwIjRmSKmu68TaT5HUFDOSlbxEkRjhERqQrqECcaL8dHpKBk+N0oP9SJoSGk7V+YkUcaXGPDSdHOmhWvQm4n9eN9H9Kz+lIk40EXj2UT9hUEdwkgvsUUmwZmNDEJbU7ArxEEmEtUmvZEJwF09eJq3zqlurXtzXyvXrPI4iOAIn4Ay44BLUwR1ogCbA4Am8gDfwbj1br9aH9TlrLVj5zCH4A+vrF00smqk=</latexit>\n<latexit sha1_base64=\"cNPEoHsRSELjwZcVuMBY6VsI1Wg=\">AAACCHicbVC7TsMwFHV4lvIKMDJgUSGxUCVVEYwVLIxFog+piSLHdVqrthPZDlIVZWThV1gYQIiVT2Djb3DbDKXlSJaOzrlX1+eECaNKO86PtbK6tr6xWdoqb+/s7u3bB4dtFacSkxaOWSy7IVKEUUFammpGuokkiIeMdMLR7cTvPBKpaCwe9DghPkcDQSOKkTZSYJ94CQ8zngeZvKjl0NMxnFPcPLArTtWZAi4TtyAVUKAZ2N9eP8YpJ0JjhpTquU6i/QxJTTEjedlLFUkQHqEB6RkqECfKz6ZBcnhmlD6MYmme0HCqzm9kiCs15qGZ5EgP1aI3Ef/zeqmOrv2MiiTVRODZoShl0MSdtAL7VBKs2dgQhCU1f4V4iCTC2nRXNiW4i5GXSbtWdevVy/t6pXFT1FECx+AUnAMXXIEGuANN0AIYPIEX8AberWfr1fqwPmejK1axcwT+wPr6BZvumbc=</latexit>\n<latexit sha1_base64=\"DV7kuerZAkVNuchtuNE0l5TlO/U=\">AAAB8nicdVDLSsNAFJ3UV62vqks3g0VwFSah0XZXdOOygn1AGspkOmmHTiZhZiKUkM9w40IRt36NO//G6UNQ0QMXDufcy733hClnSiP0YZXW1jc2t8rblZ3dvf2D6uFRVyWZJLRDEp7IfogV5UzQjmaa034qKY5DTnvh9Hru9+6pVCwRd3qW0iDGY8EiRrA2kj9I4zBXxTCfFsNqDdnIbXh1FyLb9VDTaRriIad5UYeOjRaogRXaw+r7YJSQLKZCE46V8h2U6iDHUjPCaVEZZIqmmEzxmPqGChxTFeSLkwt4ZpQRjBJpSmi4UL9P5DhWahaHpjPGeqJ+e3PxL8/PdNQIcibSTFNBlouijEOdwPn/cMQkJZrPDMFEMnMrJBMsMdEmpYoJ4etT+D/purZTt73beq11tYqjDE7AKTgHDrgELXAD2qADCEjAA3gCz5a2Hq0X63XZWrJWM8fgB6y3T2L6kgA=</latexit>\n<latexit sha1_base64=\"n+IOPmm4XmDc+E2IQ4krum+ZYLc=\">AAAB+HicdVDLSsNAFJ3UV62PRl26GSyCCwlJ7GtZ1IXLCvYBTQiT6aQdOnk4MxFq6Je4caGIWz/FnX/jpK2gogcuHM65l3vv8RNGhTTND62wsrq2vlHcLG1t7+yW9b39rohTjkkHxyzmfR8JwmhEOpJKRvoJJyj0Gen5k4vc790RLmgc3chpQtwQjSIaUIykkjy9fOtlTjKmp84lYRLNPL1iGuaZbTbrUJGaWbetnNi1RrUOLcOcowKWaHv6uzOMcRqSSGKGhBhYZiLdDHFJMSOzkpMKkiA8QSMyUDRCIRFuNj98Bo+VMoRBzFVFEs7V7xMZCoWYhr7qDJEci99eLv7lDVIZNN2MRkkqSYQXi4KUQRnDPAU4pJxgyaaKIMypuhXiMeIIS5VVSYXw9Sn8n3Rtw6oatetqpXW+jKMIDsEROAEWaIAWuAJt0AEYpOABPIFn7V571F6010VrQVvOHIAf0N4+AQVmk1k=</latexit>\n<latexit sha1_base64=\"O82aKMQ7V2t4lWSCGzzjdf0Ocpw=\">AAAB/HicbVDLSsNAFL3xWesr2qWbwSK4kJJIRZdFNy5bsA9oQphMJ+3QyYOZiRBC/BU3LhRx64e482+ctllo64GBwzn3cs8cP+FMKsv6NtbWNza3tis71d29/YND8+i4J+NUENolMY/FwMeSchbRrmKK00EiKA59Tvv+9G7m9x+pkCyOHlSWUDfE44gFjGClJc+sOSFWE9/PO4WXO8mEXWSFZ9athjUHWiV2SepQou2ZX84oJmlII0U4lnJoW4lycywUI5wWVSeVNMFkisd0qGmEQyrdfB6+QGdaGaEgFvpFCs3V3xs5DqXMQl9PzqLKZW8m/ucNUxXcuDmLklTRiCwOBSlHKkazJtCICUoUzzTBRDCdFZEJFpgo3VdVl2Avf3mV9C4bdrNx1WnWW7dlHRU4gVM4BxuuoQX30IYuEMjgGV7hzXgyXox342MxumaUOzX4A+PzByrxlR0=</latexit>\n<latexit sha1_base64=\"D7WLE+j/etF66sbp45HBdkJhYag=\">AAACAXicbVBNS8NAEN34WetX1IvgJVgETyWRih6LXjyIVLAf0ISy2W7bpbvZsDsRS4gX/4oXD4p49V9489+4bXPQ1gcDj/dmmJkXxpxpcN1va2FxaXlltbBWXN/Y3Nq2d3YbWiaK0DqRXKpWiDXlLKJ1YMBpK1YUi5DTZji8HPvNe6o0k9EdjGIaCNyPWI8RDEbq2Pt+LMLUB/oAmqTXsn8jlcA8yzp2yS27EzjzxMtJCeWodewvvytJImgEhGOt254bQ5BiBYxwmhX9RNMYkyHu07ahERZUB+nkg8w5MkrX6UllKgJnov6eSLHQeiRC0ykwDPSsNxb/89oJ9M6DlEVxAjQi00W9hDsgnXEcTpcpSoCPDMFEMXOrQwZYYQImtKIJwZt9eZ40TspepXx6WylVL/I4CugAHaJj5KEzVEVXqIbqiKBH9Ixe0Zv1ZL1Y79bHtHXBymf20B9Ynz+te5ew</latexit>\n<latexit sha1_base64=\"5afmuY5LTvc9ChZ+ecxRuveTuEo=\">AAAB/3icbVBNS8NAEN3Ur1q/ooIXL8EieCqJVPRY9OKxQr+gDWWznbZLN5uwOxFLzMG/4sWDIl79G978N24/Dtr6YODx3gwz84JYcI2u+23lVlbX1jfym4Wt7Z3dPXv/oKGjRDGos0hEqhVQDYJLqCNHAa1YAQ0DAc1gdDPxm/egNI9kDccx+CEdSN7njKKRuvZRJw6DtIPwgGlNJdLo0Muyrl10S+4UzjLx5qRI5qh27a9OL2JJCBKZoFq3PTdGP6UKOROQFTqJhpiyER1A21BJQ9B+Or0/c06N0nP6kTIl0ZmqvydSGmo9DgPTGVIc6kVvIv7ntRPsX/kpl3GCINlsUT8RDkbOJAynxxUwFGNDKFPc3OqwIVWUoYmsYELwFl9eJo3zklcuXdyVi5XreRx5ckxOyBnxyCWpkFtSJXXCyCN5Jq/kzXqyXqx362PWmrPmM4fkD6zPHzwBluU=</latexit>\n<latexit sha1_base64=\"wK04djhaYV0BE5IR6vS1KgTHGrc=\">AAAB/HicdVDJSgNBEO1xjXEbzdFLYxA8SJjRLOYW1IPHCGaBTAg9nZ6kSc9Cd40QhvgrXjwo4tUP8ebf2JOMoKIPCh7vVVFVz40EV2BZH8bS8srq2npuI7+5tb2za+7tt1UYS8paNBSh7LpEMcED1gIOgnUjyYjvCtZxJ5ep37ljUvEwuIVpxPo+GQXc45SAlgZmwbliAsggcYDEJ3LmKO4PzKJVqtXPKlUbWyVrjpRULbtewXamFFGG5sB8d4YhjX0WABVEqZ5tRdBPiAROBZvlnVixiNAJGbGepgHxmeon8+Nn+EgrQ+yFUlcAeK5+n0iIr9TUd3WnT2Csfnup+JfXi8E77yc8iGJgAV0s8mKBIcRpEnjIJaMgppoQKrm+FdMxkYSCziuvQ/j6FP9P2qclu1yq3JSLjYssjhw6QIfoGNmohhroGjVRC1E0RQ/oCT0b98aj8WK8LlqXjGymgH7AePsEOhSVKw==</latexit>\n<latexit sha1_base64=\"4kC6Y4YPWSdTLzlwx1AZuMIigvA=\">AAACAnicbVDLSsNAFL3xWesr6krcDBbBhZREKrosunFZwT6gCWEynbZDZ5IwMxFKCG78FTcuFHHrV7jzb5y2WWjrgcs9nHMvM/eECWdKO863tbS8srq2Xtoob25t7+zae/stFaeS0CaJeSw7IVaUs4g2NdOcdhJJsQg5bYejm4nffqBSsTi61+OE+gIPItZnBGsjBfahl4gwa+VB5oUp51SfFT0P7IpTdaZAi8QtSAUKNAL7y+vFJBU00oRjpbquk2g/w1Izwmle9lJFE0xGeEC7hkZYUOVn0xNydGKUHurH0lSk0VT9vZFhodRYhGZSYD1U895E/M/rprp/5WcsSlJNIzJ7qJ9ypGM0yQP1mKRE87EhmEhm/orIEEtMtEmtbEJw509eJK3zqlurXtzVKvXrIo4SHMExnIILl1CHW2hAEwg8wjO8wpv1ZL1Y79bHbHTJKnYO4A+szx8VVZfh</latexit>\n<latexit sha1_base64=\"t4m4aMlz5oXUR+/f1IhWp1HKHuo=\">AAACBHicbVDLSsNAFJ3UV62vqMtuBovgopREKrosunFZwT6gCWEynbZDZyZhZiKUkIUbf8WNC0Xc+hHu/BunaRbaeuDC4Zx7ufeeMGZUacf5tkpr6xubW+Xtys7u3v6BfXjUVVEiMengiEWyHyJFGBWko6lmpB9LgnjISC+c3sz93gORikbiXs9i4nM0FnREMdJGCuyqF/MwHWdByuueRklWh7kSZgEP7JrTcHLAVeIWpAYKtAP7yxtGOOFEaMyQUgPXibWfIqkpZiSreIkiMcJTNCYDQwXiRPlp/kQGT40yhKNImhIa5urviRRxpWY8NJ0c6Yla9ubif94g0aMrP6UiTjQReLFolDCoIzhPBA6pJFizmSEIS2puhXiCJMLa5FYxIbjLL6+S7nnDbTYu7pq11nURRxlUwQk4Ay64BC1wC9qgAzB4BM/gFbxZT9aL9W59LFpLVjFzDP7A+vwBD/GYXg==</latexit>\n<latexit sha1_base64=\"hPTM4VPV+xBFLD8chs9ef202tdU=\">AAAB+HicdVDLSsNAFJ34rPXRqEs3g0VwY0js013RjcsK9gFtCJPppB06eTAzEWLIl7hxoYhbP8Wdf+OkjaCiBy4czrmXe+9xI0aFNM0PbWV1bX1js7RV3t7Z3avo+wd9EcYckx4OWciHLhKE0YD0JJWMDCNOkO8yMnDnV7k/uCNc0DC4lUlEbB9NA+pRjKSSHL0yjnw3TTInTfmZlWWOXjWNVrtda9agaZgL5KRZv2g1oFUoVVCg6+jv40mIY58EEjMkxMgyI2mniEuKGcnK41iQCOE5mpKRogHyibDTxeEZPFHKBHohVxVIuFC/T6TIFyLxXdXpIzkTv71c/MsbxdJr2ykNoliSAC8XeTGDMoR5CnBCOcGSJYogzKm6FeIZ4ghLlVVZhfD1Kfyf9M8Nq240burVzmURRwkcgWNwCizQAh1wDbqgBzCIwQN4As/avfaovWivy9YVrZg5BD+gvX0ClW2Ttg==</latexit>\n<latexit sha1_base64=\"pxLij24zbOEvFr/TSM/WefIUseY=\">AAAB+3icdVDLSgMxFM3UV62vsS7dBIvgqkzt013RjcsK9gFtGTJp2oYmmSHJiMMwv+LGhSJu/RF3/o2ZdgQVPXDhcM693HuPFzCqtON8WLm19Y3Nrfx2YWd3b//APiz2lB9KTLrYZ74ceEgRRgXpaqoZGQSSIO4x0vcWV6nfvyNSUV/c6iggY45mgk4pRtpIrl0cBdyLo8SNY5kkcKQod+2SU262WtVGFTplZ4mUNGoXzTqsZEoJZOi49vto4uOQE6ExQ0oNK06gxzGSmmJGksIoVCRAeIFmZGioQJyocby8PYGnRpnAqS9NCQ2X6veJGHGlIu6ZTo70XP32UvEvbxjqaWscUxGEmgi8WjQNGdQ+TIOAEyoJ1iwyBGFJza0Qz5FEWJu4CiaEr0/h/6R3Xq7UyvWbWql9mcWRB8fgBJyBCmiCNrgGHdAFGNyDB/AEnq3EerRerNdVa87KZo7AD1hvnzpulTs=</latexit>\n<latexit sha1_base64=\"yUTDIdUUCYeX6z9GKX/2Q7e5bso=\">AAACGHicdZDLSsNAFIYn9VbrrerSzWARXJSahLZ2WXShywr2Ak0Jk+m0HTqThJmJUEIew42v4saFIm67822cpBWs6A8DP985hzPn90JGpTLNTyO3tr6xuZXfLuzs7u0fFA+POjKIBCZtHLBA9DwkCaM+aSuqGOmFgiDuMdL1ptdpvftAhKSBf69mIRlwNPbpiGKkNHKLF07IvfgmcWNHoajMk/IqkEkZZsRL3BS4xZJZMat2w7SgNpZdszNjNurVOrS0SVUCS7Xc4twZBjjixFeYISn7lhmqQYyEopiRpOBEkoQIT9GY9LX1ESdyEGeHJfBMkyEcBUI/X8GM/pyIEZdyxj3dyZGayN+1FP5V60dq1BjE1A8jRXy8WDSKGFQBTFOCQyoIVmymDcKC6r9CPEECYaWzLOgQvi+F/5uOXbGqldpdtdS8WsaRByfgFJwDC1yCJrgFLdAGGDyCZ/AK3own48V4Nz4WrTljOXMMVmTMvwAWMKET</latexit>"
        },
        {
            "heading": "3.5 Architecture of IMTPP",
            "text": "We first present a high-level overview of deep neural network parameterization of different components of the IMTPP model and then describe component-wise architecture in detail. Finally, we briefly present the salient features of our proposal."
        },
        {
            "heading": "3.5.1 High-level Overview",
            "text": "We parameterize different components of IMTPP, introduced in the previous section using deep neural networks. More specifically, we approximate the MTPP for observed events, p(ek+1 | Sk,Mk) using p\u03b8 and the posterior MTPP for missing events q( r | ek+1,Sk,Mr\u22121) using q\u03c6, both implemented as neural networks with parameters \u03b8 and \u03c6 respectively. We set the prior MTPP for missing events p( r | Sk,Mr\u22121) as a known distribution pprior using the history of all the events it is conditioned on. In this context, we design two recurrent neural networks (RNNs) which embed the history of observed events S into the hidden vectors s and the missing eventsM into the hidden vector m, similar to several state-of-the art MTPP models [45, 147, 148]. In particular, the embeddings sk and mr encode the influence of the arrival time and the mark of the first k observed events from Sk and first r missing events fromMr respectively. Therefore, we can represent the model for predicting the next observed event as:\np(ek+1 | Sk,Mk) = p\u03b8(ek+1 | sk,mk). (3.5)\nFollowing the above MTPP model for observed events, both the prior MTPP model and the posterior MTPP model for missing events offer similar conditioning with respect to s\u2022 andm\u2022. Identical to other MTPP models [45, 147], the RNN for the observed events updates sk\u22121 to sk"
        },
        {
            "heading": "Overcoming Missing Events in Continuous-Time Sequences 27",
            "text": "by incorporating the effect of ek. Similarly, the RNN for the missing events updates mr\u22121 to mr by taking into account the event r.\nEach event has two components, its mark and the arrival-time, which are discrete and continuous random variables respectively. Therefore, we characterize the event distribution as a density function which is the product of the density function (p\u03b8,\u2206, q\u03c6,\u2206, pprior,\u2206) of the inter-arrival time and the probability distribution (P\u03b8,x,Q\u03c6,y,Pprior,y) of the mark, i.e.,\np\u03b8(ek+1 = (xk+1, tk+1) | Sk,Mk) = P\u03b8,x(xk+1 |\u2206t,k+1, sk,mk) \u00b7 p\u03b8,\u2206(\u2206t,k+1 | sk,mk), (3.6)\nq\u03c6( r = (yr, \u03c4r) | ek+1,Sk,Mr\u22121) = Q\u03c6,y(yr |\u2206\u03c4,r, ek+1, sk,mr\u22121) \u00b7q\u03c6,\u2206(\u2206\u03c4,r | ek+1, sk,mr\u22121), (3.7)\npprior( r = (yr, \u03c4r) | Sk,Mr\u22121) = Pprior,y(yr |\u2206\u03c4,r, sk,mr\u22121) \u00b7 pprior,\u03c4 (\u2206\u03c4,r | sk,mr\u22121), (3.8)\nwhere, as mentioned, the inter-arrival times \u2206t,k and \u2206\u03c4,r are given as \u2206t,k = tk \u2212 tk\u22121 and \u2206r = \u03c4r \u2212 \u03c4r\u22121. Moreover, p\u03b8,\u2206, q\u03c6,\u2206, and pprior,\u2206 denote the density of the inter-arrival times for the observed events, posterior density and the prior density of the inter-arrival times of the missing events, and P\u03b8,x, Q\u03c6,y, and Pprior,y denote the corresponding probability mass functions of the mark distributions. Figure 3.2 denotes the neural architecture of the MTPP for observed events and the posterior MTPP for missing events in IMTPP. The Prior MTPP for missing events has a similar architecture as standard MTPP models."
        },
        {
            "heading": "3.5.2 Parameterization of p\u03b8",
            "text": "Given k observed events and r = k missing events, the generative model p\u03b8 samples the next event ek+1 based on Sk andMr. To this aim, the underlying neural network takes the embedding vectors h and s as input and provides the density p\u03b8,\u2206 and P\u03b8,x as output, which in turn are used to draw the event ek+1. More specifically, we realize p\u03b8 in Eq. 3.6 as:\n(1) Input layer: The first level is the input layer, which takes the last event as input and represents it through a suitable vector. In particular, upon arrival of ek, it computes the corresponding vector vk as:\nvk = wt,vtk +wx,vxk +wt,\u2206(tk \u2212 tk\u22121) + av, (3.9)\nwhere w\u2022,\u2022 and av are trainable parameters. (2) Hidden layer: The next level is the hidden layer that embeds the sequence of observa-\ntions into finite-dimensional vectors s\u2022, computed using RNN. Such a layer takes vi as"
        },
        {
            "heading": "28 Overcoming Missing Events in Continuous-Time Sequences",
            "text": "input and feeds it into an RNN to update its hidden states in the following way.\nsk = tanh(W s,ssk\u22121 +W s,vvk + (tk \u2212 tk\u22121)ws,k + as), (3.10)\nwhereW s,\u2022 and as are trainable parameters. This hidden state sk can also be considered as a sufficient statistic of Sk, the sequence of the first k observations. (3) Output layer: The next level is the output layer which computes both p\u03b8,\u2206(\u00b7) and P\u03b8,x(\u00b7) based on sk andmk. To this end, we have the density of inter-arrival times as:\np\u03b8,\u2206(\u2206t,k+1 | sk,mk) = LOGNORMAL ( \u00b5e(sk,mk), \u03c3 2 e(sk,mk) ) , (3.11)\nwith [\u00b5e(sk,mk), \u03c3e(sk,mk)] = W > t,ssk +W > t,mmk +at; and, the mark distribution as,\nP\u03b8,x(xk+1 = x |\u2206t,k+1, sk,mk) = exp(U>x,ssk +U > x,mmk)\u2211\nx\u2032\u2208C exp(U > x\u2032,ssk +U > x\u2032,mmk)\n, (3.12)\nThe distributions are finally used to draw the inter-arrival time \u2206t,k+1 and the mark xk+1 for the event ek+1. The sampled inter-arrival time \u2206t,k+1 gives tk+1 = tk + \u2206t,k. Here, the mark distribution is independent of \u2206t,k+1.\nFinally, we note that \u03b8 = {W \u2022,\u2022,w\u2022,\u2022,U \u2022,\u2022,a\u2022} are trainable parameters. We would like to highlight that, the proposed lognormal distribution of inter-arrival times \u2206t,k allows an easy re-parameterization trick\u2014 LOGNORMAL(\u00b5e, \u03c3e) = exp(\u00b5e + \u03c3e \u00b7 NORMAL(0, 1))\u2014which mitigates variance of estimated parameters and facilitates faster training."
        },
        {
            "heading": "3.5.3 Parameterization of q\u03c6",
            "text": "At the very outset, q\u03c6(\u2022 | ek, sk,mr\u22121) (Eq. 3.7) generates missing events that are likely to be omitted during the interval (tk, tk+1) after the knowledge of the subsequent observed event ek+1 is taken into account. To ensure that missing events are generated within desired interval, (tk, tk+1), whenever an event is drawn with \u03c4r > tk+1, then q\u03c6(\u2022 | ek+1, sk,mr\u22121) is set to zero and k is set to r \u2212 1. Otherwise, k is flagged as k. Note that, q\u03c6(\u2022 | sk,mr\u22121) generates all potential missing events in this interval. That said, it generates multiple events sequentially in one single run in contrast to the p\u03b8. Similar to p\u03b8, it has also a three level architecture.\n(1) Input layer: Given the subsequent observed event tk+1 along with Sk and r\u22121 = (yr\u22121, \u03c4r\u22121) arrives with \u03c4r\u22121 < tk+1 or equivalently if r \u2212 1 6= k, then we first con-"
        },
        {
            "heading": "Overcoming Missing Events in Continuous-Time Sequences 29",
            "text": "vert \u03c4r\u22121 into a suitable representation as follows:\n\u03b3r\u22121 = g\u03c4,\u03b3\u03c4r\u22121 + gy,\u03b3yr\u22121 + g\u2206,\u03b3(\u03c4r\u22121 \u2212 \u03c4r\u22122) + b\u03b3, (3.13)\nwhere g\u2022,\u2022 and b\u03b3 are trainable parameters. (2) Hidden layer: Similar to the hidden layer used in the p\u03b8 model, the hidden layer here\ntoo embeds the sequence of missing events into finite-dimensional vectorsm\u2022, computed using RNN in a recurrent manner. Such a layer takes \u03b3r\u22121 as input and feeds it into an RNN to update its hidden states in the following way:\nmr\u22121 = tanh ( Gm,mmr\u22122 +Gm,\u03b3\u03b3r\u22121 + (\u03c4r\u22121 \u2212 \u03c4r\u22122)gm,\u03c4 + bm ) , (3.14)\nwhereG\u2022,\u2022, g\u2022,\u2022 and bm are trainable parameters. (3) Output layer: The next level is the output layer which computes both q\u03c6,\u2206(\u00b7) and Q\u03c6,y(\u00b7)\nbased on mr and sk. To compute these quantities, it takes five signals as input: (i) the current update of the hidden state mr for the RNN in the previous layer, (ii) the current update of the hidden state sk that embeds the history of observed events, (iii) the timing of the last observed event, tk, (iv) the timing of the last missing event, \u03c4r\u22121, and (v) the timing of the next observation, tk+1. To this end, we have the density of inter-arrival times as:\nq\u03c6,\u2206(\u2206\u03c4,r | ek+1, sk,mr\u22121) = LOGNORMAL ( \u00b5 (mr\u22121, sk), \u03c3 2 (mr\u22121, sk) ) J\u03c4r\u22121 + \u2206\u03c4,r < tk+1K,\nwith [\u00b5 (mr\u22121, sk), \u03c3 (mr\u22121, sk)] = G>\u03c4,mmr\u22121+G > \u03c4,ssk+b\u03c4 ; and, the mark distribution as,\nP\u03b8,x(yr = y |\u2206\u03c4,r, ek+1, sk,mr\u22121) = J\u03c4r\u22121 + \u2206\u03c4,r < tk+1K exp(V >y,ssk + V >y,mmr\u22121)\u2211\ny\u2032\u2208C exp(V > y\u2032,ssk + V > y\u2032,mmr\u22121)\n,\n(3.15) Here, J\u00b7K denotes the indicator function of whether the sampled times of missing events are within the current observed time-interval."
        },
        {
            "heading": "30 Overcoming Missing Events in Continuous-Time Sequences",
            "text": "Hence, we have:\n\u2206\u03c4,r \u223c q\u03c6,\u2206(\u2022 | ek+1, sk,mr\u22121) If \u2206\u03c4,r < tk+1 \u2212 \u03c4r\u22121 :\n\u03c4r = \u03c4j + \u2206\u03c4, yr \u223c P\u03b8,x(yr = y |\u2206\u03c4,r, ek+1, sk,mr\u22121) k =\u221e (Allow more missing events)\nOtherwise:\nk = r \u2212 1.\nHere, note that the mark distribution depends on \u2206\u03c4,r. \u03c6 = {G\u2022,\u2022, g\u2022,\u2022,V \u2022,\u2022, b\u2022} are trainable parameters. The distribution in Eq. 3.15 ensure that given the first k + 1 observations, q\u03c6 generates the missing events only for (tk, tk+1) and not for further subsequent intervals.\n3.5.4 Prior MTPP model pprior We model the prior density (Eq. 3.8) of the arrival times of the missing events as,\npprior,\u2206(\u2206\u03c4,r | sk,mr\u22121) = LOGNORMAL ( \u00b5(sk,mr\u22121), \u03c3 2(sk,mr\u22121) ) , (3.16)\nwith [\u00b5(sk,mr\u22121), \u03c32(sk,mr\u22121] = q>\u00b5,mmr\u22121 + q > \u00b5,ssk + c; and, the mark distribution of the missing events as,\nPprior,y(yr = y |\u2206\u03c4,r, sk,mr\u22121) = exp(Q>y,ssk +Q > y,mmr\u22121)\u2211\ny\u2032\u2208C exp(Q > y\u2032,ssk +Q > y\u2032,mmr\u22121)\n, (3.17)\nAll parameters Q\u2022,\u2022, q\u2022,\u2022 and c are scaled a-priori using a hyper-parameter \u00b5. Thus, \u00b5 determines the importance of the pprior in the missing event sampling procedure of IMTPP. We specify the optimal value for \u00b5 based on the prediction performance in the validation set."
        },
        {
            "heading": "3.5.5 Training \u03b8 and \u03c6",
            "text": "Note that the trainable parameters for observed and posterior MTPPs are \u03b8 = {w\u2022,\u2022,W \u2022,\u2022,a\u2022,U \u2022,\u2022} and \u03c6 = {g\u2022,\u2022,G\u2022,\u2022, b\u2022,V \u2022,\u2022} respectively. Given a history SK of observed events, we aim to learn \u03b8 and \u03c6 by maximizing ELBO, as defined in Eq. 3.4, i.e.,\nmax \u03b8,\u03c6 ELBO(\u03b8, \u03c6). (3.18)"
        },
        {
            "heading": "Overcoming Missing Events in Continuous-Time Sequences 31",
            "text": "We compute optimal parameters \u03b8\u2217 and \u03c6\u2217 that maximizes ELBO(\u03b8, \u03c6) using stochastic gradient descent (SGD) [179]. More details regarding the hyper-parameter values are given in Section 3.6."
        },
        {
            "heading": "3.5.6 Optimal Position for Missing Events",
            "text": "To better explain the missing event modeling procedure of IMTPP while simultaneously enhancing its practicability, we present a novel application of IMTPP++, a novel variant that offers a trade-off between the number of missing events and the model scalability [76]. In sharp contrast to the original problem setting of generating missing events between observed events, IMTPP++ is designed to impute a fixed number of events in a sequence. Specifically, given an input sequence and a user-determined parameter of the number of missing events to be imputed (denoted by N ), IMTPP++ determines the optimal time and mark of N events that when included with the observed MTPP achieve superior event prediction prowess. Note that these events may be missing at random positions that are not considered while training IMTPP++. IMTPP++ achieves this by constraining the missing event sampling procedure of the posterior MTPP (q\u03c6,\u2206(\u2022)) to limited iterations while simultaneously maximizing the likelihood of observed MTPP. Mathematically, it optimizes the following objective:\nmax qimp,\u2206 Eqimp,\u2206 K\u22121\u2211 k=0 log p(ek+1 | Sk,MN), where \u222b T 0 qimp,\u2206dt = N, (3.19)\nwhere qimp,\u2206 and p(ek+1) denote the constrained posterior MTPP and the observed MTPP. However, determining the optimal position of missing events is a challenging task as while imputing events, the generator must consider the dynamics of future events in the sequence. Therefore, IMTPP++ includes a two-step training procedure: (i) training observed and missing MTPP using the training set with unbounded missing events (as in Section 3.5.5); and then (ii) finetuning the parameters of the constrained posterior MTPP and observed MTPP by maximizing the objective in Eq. 3.19. For the latter stage, we use the optimal positions of N missing events sampled from the posterior MTPP determined by their occurrence probabilities. Later, we assume these events represent all missing (MN ), followed by a fine-tuning using Eq. 3.19."
        },
        {
            "heading": "3.5.7 Salient Features of IMTPP",
            "text": "It is worth noting the similarity of our modeling and inference framework to variational autoencoders [30], with q\u03c6 and p\u03b8 playing the roles of encoder and decoder, respectively, while pprior plays the role of the prior distribution of latent events. However, the random seeds in our model are not simply noise as they are interpreted in autoencoders. They can be concretely interpreted"
        },
        {
            "heading": "32 Overcoming Missing Events in Continuous-Time Sequences",
            "text": "in IMTPP as missing events, making our model physically interpretable.\nSecondly, note that the proposal of [148] aims to impute the missing events based on the entire observation sequence SK , rather than to predict observed events in the face of missing events. For this purpose, it uses a bi-directional RNN and, whenever a new observation arrives, it regenerates all missing events by making a completely new pass over the backward RNN. As a consequence, such an imputation method suffers from quadratic complexity with respect to the number of observed events.\nIn contrast, our proposal is designed to generate subsequent observed and missing events rather than imputing missing events in between observed events2. To that aim, we only make forward computations, and therefore, it does not require us to re-generate all missing events whenever a new observation arrives, which makes it much more efficient than [148] in terms of both learning and prediction. Through our experiments, we also show the exceptionally time-effective operation of IMTPP over other missing-data models.\nFinally, unlike most of the prior works [45, 147, 148, 189, 243, 251] we model our distribution for inter-arrival times using lognormal. Such a modeling procedure has major advantages over intensity-based models \u2013 (i) scalable sampling during prediction as opposed to Ogata\u2019s thinning/inverse sampling; and (ii) efficient training via re-parametrization. Moreover, our generative procedure for missing events requires iterative sampling in the absence of new observed events and such an unsupervised procedure can largely benefit from the prowess of intensityfree models in forecasting future events in a sequence [41].\nWhile Shchur et al. [186] also uses model inter-arrival times using lognormal; they do not focus on predicting observations in the face of missing events. However, it is important to reiterate (see Shchur et al. [186] for details) that this modeling choice offers significant advantages over intensity-based models in terms of providing ease of re-parameterization trick for efficient training, allowing a closed-form expression for expected arrival times and usability for supervised training as well.\nImportance of IMTPP++. On a broader level, IMTPP++ may be similar to IMTPP, however, they vary significantly. Specifically, the main distinctions are: (i) IMTPP++ offers higher practicability as it can be used for predicting future events and for imputing a fixed number of missing events; (ii) IMTPP cannot achieve the latter as it involves an unconstrained procedure for generating missing events; and (iii) IMTPP++ has an added feature to identify the optimal position of missing events in a sequence. Moreover, as the training procedure of IMTPP++\n2However, note that we also use the posterior distribution q\u03c6 to impute missing events between already occurred events."
        },
        {
            "heading": "Overcoming Missing Events in Continuous-Time Sequences 33",
            "text": "involves a pre-training step, the missing event generator has the knowledge of future events in a sequence. This is a sharp contrast to IMTPP which only involves forward temporal computations. To the best of our knowledge, IMTPP++ is the first-of-its-kind application of neural point process models that can solve several real-world problems, ranging from smooth learning curves to extending the sequence lengths."
        },
        {
            "heading": "3.6 Experiments",
            "text": "In this section, we report a comprehensive empirical evaluation of IMTPP along with its comparisons with several state-of-the-art approaches. Our code uses Tensorflow3 v.1.13.1 and Tensorflow-Probability v0.6.04. Through these experiments, we aim to answer the following research questions.\nRQ1 What is the mark and time prediction performance of IMTPP in comparison to the stateof-the-art baselines? Where are the gains and losses? RQ2 How does IMTPP perform in presence of limited data? RQ3 How does the efficiency of IMTPP compare with the proposal of Mei et al. [148]?"
        },
        {
            "heading": "3.6.1 Experimental Setup",
            "text": "Here we present the details of all datasets, baselines, and the hyperparameter values used. Datasets. For our experiments, we use eight real datasets from different domains: Amazon\nmovies (Movies) [157], Amazon toys (Toys) [157], NYC-Taxi (Taxi), Twitter [246], Stackoverflow (SO) [45], Foursquare [228], Celebrity [155], and Health [10]. The statistics of all datasets are summarized in Table 3.1 and we describe them as follows:\n\u2022 Amazon Movies [157]. For this dataset, we consider the reviews given to items under the category \"Movies\" on Amazon. For each item, we consider the time of the written review as the time of the event in the sequence and the rating (1 to 5) as the corresponding mark. \u2022 Amazon Toys [157]. Similar to Amazon Movies, but here we consider the reviews given to items under the category \"Toys\". \u2022 NYC Taxi5. Here, each sequence corresponds to a series of timestamped pick-up and drop-off events of a taxi in New York City, and location IDs are considered event marks. \u2022 Twitter [246]. Similar to [147], we group retweeting users into three classes based on their connectivity: an ordinary user (degree lower than the median), a popular user (degree lower than 95-percentile), and influencers (degree higher than 95-percentile). Each\n3https://www.tensorflow.org/ 4https://www.tensorflow.org/probability 5https://chriswhong.com/open-data/foil_nyc_taxi/"
        },
        {
            "heading": "34 Overcoming Missing Events in Continuous-Time Sequences",
            "text": ""
        },
        {
            "heading": "Dataset Movies Toys Taxi Twitter SO Foursquare Celebrity Health",
            "text": "Baselines. We compare IMTPP with the following state-of-the-art baselines for modeling continuous-time event sequences:\n\u2022 HP [80]. A conventional Hawkes process or self-exciting multivariate point process model with an exponential kernel i.e., the past events raise the intensity of the next event of the same type. \u2022 SMHP [128]. A self-modulating Hawkes process wherein the intensity of the next event is not ever-increasing as in standard Hawkes but learned based on past events. \u2022 RMTPP [45]. A state-of-the-art neural point process that embeds sequence history using inter-event time differences and event marks using a recurrent neural network. \u2022 SAHP [243]. A self-attention-based Hawkes process that learns the embedding for the temporal dynamics using a weighted aggregation of all historical events. \u2022 THP [251]. The transformer Hawkes process extends the transformer model [206] to"
        },
        {
            "heading": "Overcoming Missing Events in Continuous-Time Sequences 35",
            "text": "include time and mark influences between events to calculate the conditional intensity function for the arrival of future events in the sequence. \u2022 PFPP [148]. A particle filtering process for MTPP that learns the sequence dynamics using a bi-directional recurrent neural network. \u2022 HPMD [189]. Models the sequences using linear multivariate parameterized point processes and learns the inter-event influence using a predefined structure.\nWe omit the comparisons with other MTPP models [147, 162, 186, 224, 225] as they have already been outperformed by these approaches. Moreover, recent research [121] has shown that the performance of other RNN-based models such as Mei and Eisner [147] is comparable to RMTPP [45].\nEvaluation protocol. Given a stream of N observed events SN , we split them into training SK and test set SN\\SK , where the training set (test set) consists of first 80% (last 20%) events, i.e., K = d0.8Ne. We train IMTPP and the baselines on SK and then evaluate the trained models on the test set SN\\SK in terms of (i) mean absolute error (MAE) of predicted times, and (ii) mark prediction accuracy (MPA).\nMAE = 1 |SN\\SK | \u2211\nei\u2208SN\\SK\nE[|ti \u2212 t\u0302i|], MPA = 1 |SN\\SK | \u2211\nei\u2208SN\\SK\nP(xi = x\u0302i), (3.20)\nHere t\u0302i and x\u0302i are the predicted time and mark the i-th event in the test set. Note that such predictions are made only on observed events in real datasets. For time prediction, given the varied temporal distribution across the datasets, we normalize event times across each dataset [45]. We report results and confidence intervals based on three independent runs."
        },
        {
            "heading": "3.6.2 Implementation Details",
            "text": "Parameter Settings. For our experiments, we set dim(v\u2022) = 16, and dim(\u03b3\u2022) = 32, where v\u2022 and \u03b3\u2022 are the output of the first layers in p\u2217\u03b8 and q \u2217 \u03c6 respectively; the sizes of hidden states as dim(h\u2022) = 64 and dim(z\u2022) = 128; batch-size B = 64. In addition, we set an l2 regularizer over the parameters with regularizing coefficient of 0.001.\nSystem Configuration. All our experiments were done on a server running Ubuntu 16.04. CPU: Intel(R) Xeon(R) Gold 5118 CPU @ 2.30GHz, RAM: 125GB and GPU: NVIDIA Tesla T4 16GB DDR6.\nBaseline Details. Since HP and SMHP [128] generate a sequence of events of a specified length from the weights learned over the training set, we generate |N | sequences as per the data"
        },
        {
            "heading": "36 Overcoming Missing Events in Continuous-Time Sequences",
            "text": "as S = {s1, s2, \u00b7 \u00b7 \u00b7 sN} each with maximum sequence length. For evaluation, we consider the first li set of events for each sequence i. For RMTPP, we set hidden dimension and BPTT is selected among {32, 64} and {20, 50} respectively. For THP, and SAHP, we set the number of attention heads as 2, hidden key-matrix, and value-matrix dimensions are selected among {32, 64}. If applicable, for each model we use a dropout of 0.1. For PFPP, we set \u03b3 = 1 and use a similar procedure to calculate the embedding dimension as in the THP. All other parameter values are the ones recommended by the authors of the corresponding models."
        },
        {
            "heading": "3.6.3 Event Prediction Performance",
            "text": "To address RQ1, we evaluate the event prediction ability of IMTPP. More specifically, we compare the performance of IMTPP with all the baselines introduced above across all six datasets. Tables 3.2 and 3.3 summarizes the results, which sketches the comparative analysis in terms of mean absolute error (MAE) on time and mark prediction accuracy (MPA), respectively. From the results, we make the following observations:\n\u2022 IMTPP exhibits steady improvement over all the baselines in most of the datasets in the case of both time and mark prediction. However, for Stackoverflow and Foursquare datasets, THP outperforms all other models, including IMTPP, in terms of MPA. \u2022 RMTPP is the second-best performer in terms of MAE of time prediction almost in all datasets. In fact, in Stackoverflow (SO) dataset, it shares the lowest MAE together with IMTPP. However, there is no consistent second-best performer in terms of MPA. Notably, PFPP and IMTPP, which take into account missing events, are the second-best performers for four datasets. \u2022 Both PFPP [148] and HPMD [189] fare poorly with respect to IMTPP in terms of both MAE and MPA. This is because PFPP focuses on imputing missing events based on complete observations and is not well suited to predict observed events in the face of missing observations. In fact, PFPP does not offer a joint training mechanism for the MTPP for observed events and the imputation model. Rather it trains an imputation model based on the observation model learned a-priori. On the other hand, HPMD only assumes a linear Hawkes process with a known influence structure. Therefore it shows poor performance with respect to IMTPP.\nQualitative Analysis. In addition, we also perform a qualitative analysis to identify if IMTPP can model the inter-event time-intervals in a sequence. Figure 3.3 provides some real-life event sequences taken from Movies and Toys datasets and the time-intervals predicted by IMTPP. The results qualitatively show that the predicted inter-arrival times closely match the true interarrival times. Moreover, the results also show that IMTPP can event efficiently model the large"
        },
        {
            "heading": "Overcoming Missing Events in Continuous-Time Sequences 37",
            "text": "spikes in inter-event time intervals.\nDrill-down Analysis. Next, we provide a comparative analysis of the time prediction performance at the level of every event in the test set. To this end, for each observed event ei in the test set, we compute the gain (or loss) IMTPP achieves in terms of the time-prediction error per event E[|tk \u2212 t\u0302k|], i.e., AE(baseline) \u2212 AE(IMTPP) for two competitive baselines, e.g., RMTPP and PFPP for Movies and Toys datasets. Figure 3.4 summarizes the results, which shows that IMTPP outperforms the most competitive baseline i.e., RMTPP for more than 70% events across both Movies and Toys datasets. It also shows that the performance gain of IMTPP over PFPP is ever more prominent.\nPerformance Comparison with Markov Chains. Previous research [45] has shown that the mark prediction performance of neural MTPP models is comparable to Markov Chains (MCs). Therefore, in addition to the mark prediction experiments with MTPP models in Table 3.3,"
        },
        {
            "heading": "38 Overcoming Missing Events in Continuous-Time Sequences",
            "text": "we also report the results for the comparison between IMTPP and MCs of orders 1 to 3 in Table 3.4. Note that we only report the results of the best-performing MC. The results show that the Markov models perform quite well in terms of mark prediction accuracy across all datasets. A careful investigation revealed that the datasets exhibit significant repetitive characteristics of marks in a small window. Thus for some datasets with large repetitions within a short history window, using a deep point process-based model is overkill. On the other hand, for NYC Taxi, the mobility distribution clearly shows long-term dependencies, thus severely hampering the performance of Markov Chains. In these cases, point process-based models show better performance by being able to model the inter-event complex dependencies more efficiently."
        },
        {
            "heading": "3.6.4 Ablation Study",
            "text": "We also conduct an ablation study for two key contributions in IMTPP: missing event MTPP and the intensity-free modeling of time intervals. We denote IMTPPSas the variants of IMTPP"
        },
        {
            "heading": "Overcoming Missing Events in Continuous-Time Sequences 39",
            "text": "without the missing MTPP and IMTPPRas the variant without the lognormal distribution for inter-event arrival times. More specifically, for IMTPPRwe follow [45] to determine an intensity function \u03bbpk for observed events using the output of the RNN, sk.\n\u03bb\u2217p(tk) = exp(w\u03bb,ssk +w\u03bb,mmk +w\u03bb,\u2206(tk \u2212 tk\u22121) + b\u03bb), (3.21)\nLater, we use the intensity function at a given timestamp to estimate the probability distribution of future events as:\np\u03b8,t(tk+1) = \u03bb \u2217 p(tk) exp ( \u2212 \u222b t tk \u03bb\u2217p(\u03c4) d\u03c4 ) , (3.22)\nWe report the performance of IMTPP and its variants in terms of MAE and MPA in Tables 3.5 and 3.6 respectively. The results show that IMTPP outperforms IMTPPSand IMTPPRacross all metrics. The performance gain of IMTPP over IMTPPSsignifies the importance of including missing events for modeling event sequences. We also note that the performance gain of IMTPP over IMTPPRreinforces our modeling design of using an intensity-free model."
        },
        {
            "heading": "40 Overcoming Missing Events in Continuous-Time Sequences",
            "text": ""
        },
        {
            "heading": "3.6.5 Forecasting Future Events",
            "text": "To highlight the forecasting prowess of IMTPP against its competitors, we design a difficult event prediction task, where we predict the next n events given only the current event as input. To do so, we keep sampling events using the trained model p\u03b8\u0302 and q\u03c6\u0302 till n-th prediction. Such an evaluation protocol effectively requires accurate inference of the missing data distribution since, unlike during the training phase, the future observations are not fed into the missing event model. To this end, we compare the forecasting performance of IMTPP against RMTPP, the most competitive baseline. Figure 3.5 summarizes the results for Movies and Toys datasets, which shows that (i) the performances of all the algorithms deteriorate as n increases and; (ii) IMTPP achieves 5.5% improvements in MPA and significantly better 10.12% improvements in MAE than RMTPP across both datasets. The results further reinforce the ability of IMTPP to model the long-term distribution of events in a sequence."
        },
        {
            "heading": "3.6.6 Performance with Missing Data",
            "text": "For RQ2 and to emphasize the applicability of IMTPP in the presence of missing data, we perform event prediction on sequences with limited training data. Specifically, we synthetically\ndelete events from a sequence i.e., we randomly (via a normal distribution) delete 40% (and 60%) of events from the original sequence and then train and test our model on the rest 60% events(40%). Figure 3.6 summarizes the results across Movies and Toys datasets. From the results, we note that with synthetic data deletion, the performance improvement of IMTPP over best-performing baselines \u2013 RMTPP, THP, and PFPP\u2013 is significant even after 40% of events are deleted. This is because IMTPP is trained to capture the missing events, and as a result, it can exploit the underlying setting with data deletion more effectively than the other models. Though this performance gains saturate with a further increase in missing data as the added noise in the datasets severely hampers the learning of both models. Interestingly, we note that RMTPP outperforms THP and PFPP even in situations with limited data."
        },
        {
            "heading": "3.6.7 Scalability Analysis with PFPP [148]",
            "text": ""
        },
        {
            "heading": "3.6.7.1 With Complete Sequences",
            "text": "To highlight the time-effective learning ability of IMTPP, we compare the runtimes of IMTPP and PFPP across no. of training epochs as well as the length of training sequence |SK |. Figure 3.7 summarizes the results, which shows that IMTPP enjoys a better latency than PFPP. In particular, we observe that the runtime of PFPP increases quadratically with respect to |SK |,"
        },
        {
            "heading": "42 Overcoming Missing Events in Continuous-Time Sequences",
            "text": "whereas the runtime of IMTPP increases linearly. The quadratic complexity of PFPP is due to the presence of a backward RNN, which requires a complete pass whenever a new event arrives. The larger runtimes of both models can be attributed to the massive size of Movies dataset with 1.4 million events."
        },
        {
            "heading": "3.6.7.2 Streaming-based Runtime",
            "text": "Our setting differs significantly from PFPP [148] as the latter requires the complete data distribution of missing and observed events. However, we evaluate if their model can be extended to our setting i.e., a streaming setting wherein complete sequences are not available upfront rather, they arrive as we progress with time along a sequence. In a streaming setting, the model is trained as per the arrival of events, and the only way for PFPP to be extended in this setting is to update the parameters at each arrival. This repetitive training is expensive and can withdraw the practicability of the model. To further assert our proposition, we evaluate the runtime of their model across the different datasets and compare it with IMTPP. We report the results for training across only a few epochs (10) in a streaming setting in Table 3.7. We note that across all datasets PFPP fails to scale as expected. This delay in training could be attributed to the two-phase training of their model; (i) particle filtering, where they learn the underlying complete data distribution, and (ii) particle smoothing, in which they filter out the inadequate events generated during particle filtering. Secondly, since PFPP requires complete data during training, an online setting would require repetitive parameter optimization on the arrival of each"
        },
        {
            "heading": "Overcoming Missing Events in Continuous-Time Sequences 43",
            "text": "event. Thus, their model cannot be extended to such a scenario while maintaining its practicality. Thus, IMTPP is the singular practical approach for learning MTPPs in an online setting with intermittent observational data."
        },
        {
            "heading": "3.6.8 Imputation Performance",
            "text": "Here, we evaluate the ability of IMTPP and PFPP to impute missing events in a sequence. Specifically, we evaluate the ability of both models to generate the missing events that were not present during training. Thus, for Movies and Toys datasets where we synthetically remove all the ratings between the first month and the third month for all the entities in both datasets. We evaluate across the imputed events for test sequences. One important thing to note is that IMTPP only takes into account the history, but PFPP uses both, history and future events. We report the results across the Movies and Toys datasets in Figure 3.8. To summarize, our results show that even with limited historical information, IMTPP outperforms PFPP in time-prediction, whereas both models perform competitively for the mark prediction of missing events."
        },
        {
            "heading": "44 Overcoming Missing Events in Continuous-Time Sequences",
            "text": "1 3 5 7 10 13 15 N \u2192 0.0\n0.1\n0.2\nM A\nE \u2192\nPFPP IMTPP++\n(a) Movies\n1 3 5 7 10 13 15 N \u2192 0.0\n0.1\n0.2\nM A\nE \u2192\nPFPP IMTPP++\n(b) Toys\nFigure 3.10: Compared the imputation performance of IMTPP++ and PFPP across different numbers of missing events. Note that this setting differs from Figure 3.8, as here the events are missing at random positions."
        },
        {
            "heading": "3.6.9 Evaluating the Performance of IMTPP++",
            "text": "Observed Event Prediction using IMTPP++. Here, we evaluate the ability of IMTPP++ to predict the observed events in a sequence. Specifically, we report the time prediction performance of IMTPP++ across a different number of permitted missing events (N ) and compare them with IMTPP i.e., with an unbounded number of missing events. Figure 3.9 summarizes our results which show that as we increase N , the time prediction performance for IMTPP++ increases, and it narrows the performance gap with IMTPP. However, IMTPP still performs better than IMTPP++. From the results, we conclude that IMTPP++ acts as a trade-off between the number of missing events and the prediction quality. This is a significant improvement over IMTPP as sampling missing events can be an expensive procedure. Moreover, as IMTPP++ involves fine-tuning over pre-trained IMTPP, it has an added advantage of fine-tuning at amounts of missing events. From our experiments, we found that fine-tuning IMTPP++ took less than 15 minutes across all values of N . We also note that with small N , IMTPP is comparable to RMTPP i.e., the best performer for time prediction.\nImputing Missing Events using IMTPP++. Our main contribution via IMTPP++ is to predict the missing events located randomly in a sequence. We evaluate this by performing an additional experiment using synthetic deletion. Specifically, we randomly sample N events from each sequence and tag them to be missing. Later, we evaluate the ability of IMTPP++ and PFPP in imputing these missing events. Figure 3.10 summarizes the results and we note that IMTPP++ easily outperforms PFPP across all values of N . Moreover, we note that as we increase N , the imputation performance becomes better. Naturally, it can be attributed to relatively lesser variance in position of missing events with largeN . We reiterate that all confidence intervals are calculated using three independent runs."
        },
        {
            "heading": "Overcoming Missing Events in Continuous-Time Sequences 45",
            "text": ""
        },
        {
            "heading": "3.7 Conclusion",
            "text": "Modeling continuous-time events with irregular observations is a non-trivial task that requires learning the distribution of both \u2013 observed and missing events. Standard MTPP models ignore this aspect and assume that the underlying data is complete with no missing events \u2013 an ideal assumption that is not practicable in many settings. In order to solve these shortcomings, in Gupta et al. [75], we provide a method for incorporating missing events for training marked temporal point processes that simultaneously sample missing as well as observed events across continuous time. The proposed model IMTPP uses a coupled MTPP approach with its parameters optimized via variational inference. We further improve on IMTPP and propose IMTPP++ that has an added feature to identify the optimal position of missing events in a sequence. Experiments on several real datasets from diverse application domains show that our proposal outperforms other state-of-the-art approaches for predicting the dynamics of observed events. We also evaluate the ability of our models to impute synthetically deleted missing events within observed events. In this setting as well, our models outperform other alternatives along with better scalability and guaranteed convergence. Since including missing data, improves over standard learning procedures, this observation opens avenues for further research that includes modeling or sampling missing data.\n46 Overcoming Missing Events in Continuous-Time Sequences\nPart II"
        },
        {
            "heading": "Recommendation in Spatio-Temporal",
            "text": ""
        },
        {
            "heading": "Settings",
            "text": "47\nChapter 4"
        },
        {
            "heading": "Cross-Region Transfer for Spatial",
            "text": ""
        },
        {
            "heading": "Features",
            "text": ""
        },
        {
            "heading": "4.1 Introduction",
            "text": "In this chapter, we present our solution to overcome the drawbacks of data scarcity for top-k recommendations in spatial mobility networks. As POI (Points-of-Interest) gathering services such as Foursquare, Yelp, and Google Places are becoming widespread, there is significant research in extracting location preferences of users to predict their mobility behavior and recommend the next POIs that users are likely to visit [27, 54, 117, 126, 141, 165]. However, the quality of POI recommendations for users in regions where there is a severe scarcity of mobility data is much poorer in comparison to those from data-rich regions. This is a critical problem affecting the state-of-the-art approaches [84, 209, 230]. The situation is further exacerbated in recent times due to the advent of various restrictions on collecting personal data and growing awareness (in some geopolitical regions) about the need for personal privacy [11, 216]. It not only means that there is an overall reduction in the high-quality (useful) data1, but, even more importantly, it introduces a high-skew in the mobility data across different regions (see Figure 4.1) \u2013 primarily due to varying views towards personal privacy across these regions.\nTherefore, the current state-of-the-art methods struggle in low-data regions and the approaches that attempt to incorporate data from external sources suffer from the following limitations: (i) limited to cold-start users from within a city [119, 163, 219], (ii) focused only on using traffic network ignoring the use of social network of users and location dynamics [165, 209, 230], (iii) generate trajectories using a model learned on traffic-network images of source region [50,\n1Nearly 80% of the data generated by Foursquare users is discarded [61].\n49\n84], thus vulnerable to recalibration on noisy images, (iv) operating only for users who are common across locations [44, 116], or (v) adopting a limited level of transfer through domaininvariant features [105] \u2013like the spending capacity or the users\u2019 age, thus constrained by the feature unavailability in public datasets. Unfortunately, none of these approaches, especially those based on visual data (i.e., traffic and location images), can be easily fine-tuned for a target POI network due to the varied spatial density, location category, and lack of user-specific features in POI datasets."
        },
        {
            "heading": "4.1.1 Our Contribution",
            "text": "In this chapter, we present AXOLOTL(Automated cross Location-network Transfer Learning), a novel meta learning-based approach for POI recommendation in limited-data regions while transferring model parameters learned at a data-rich region without any prerequisite of interregion user overlap [72]. Specifically, we use a hierarchical multi-channel learning procedure with a novel meta-learning [57, 111] extension for spatial mobility networks, called spatiosocial meta-learning (SSML), that learns the model parameters by jointly minimizing the region-specific social as well as location recommendation losses, and a cross-region transfer via clusters [209, 230] of user and locations with similar preferences by minimizing an alignment loss [94, 241], to achieve high performance even in extremely limited-data regions. We represent the POI network of each region via a heterogeneous graph with users and locations as nodes and capture the user-location inter-dependence and their neighborhood structure via a twin graph attention model [221, 249]. The graph-attention model aggregates all four aspects of user and location influences [49, 220] namely, (i) users\u2019 social neighborhood and their location affinity to construct user-specific and location-conditioned representations, and (ii) similarly for each location, its neighboring locations and associated user affinity to obtain a locality-specific and user-conditioned representations. We combine this multi-faceted information to determine"
        },
        {
            "heading": "Cross-Region Transfer for Spatial Features 51",
            "text": "the final user and location representations that are used for POI recommendation. We highlight the region-size invariant performance of AXOLOTL by using regions with different spatial granularities, i.e., states for Germany and US, respectively, and prefectures for Japan.\nIn summary, our key contributions via AXOLOTL are three-fold:\n\u2022 Region-wise Transfer: We address the problems associated with POI recommendation in limited data regions and propose AXOLOTL, a cross-region model transfer approach for POI recommendation that does not require common users and their traces across regions. It utilizes a novel spatio-social meta-learning-based transfer and minimizes the divergence between user-location clusters with similar characteristics. \u2022 User-Location Influences: Our twin graph attention-based model combines user and location influences in heterogeneous mobility graphs. This is the first approach to combine these aspects for addressing the data scarcity problem with POI recommendations. AXOLOTL is robust to network size, trajectory spread, and check-in category variance making it more suitable for transfer across geographically distant regions (and even across different networks). \u2022 Detailed Empirical Evaluation: We conduct thorough experiments over 12 real-world points-of-interest datasets from the US, Japan, and Germany, at different region-wise granularity. They highlight the superior recommendation performance of AXOLOTL over state-of-the-art methods across all metrics."
        },
        {
            "heading": "4.2 Related Work",
            "text": "In this section, we introduce key related works. They mainly falls into the following categories: 1) Mobility Prediction; 2) Graph-based Recommendations; 3) Clustering in Spatial Datasets; and 4) Transfer Learning and Mobility."
        },
        {
            "heading": "4.2.1 Mobility Prediction",
            "text": "Understanding the mobility dynamics of a user is widely studied using different data sources [28, 248]. Early efforts relied on taxi datasets to study individual trajectories [69, 123]. However, these approaches are limited by the underlying datasets as it excludes two critical aspects of a mobility network; social friendships and location categories. The social network is used to model the influence dynamics across different users [126, 141], and the POI categories capture the different preferences of an individual [27, 127]. We utilize user POI social networks for our model as these datasets provide both: a series of social dynamics for different users and location-specific interest patterns for a user. These are essential for tasks such"
        },
        {
            "heading": "52 Cross-Region Transfer for Spatial Features",
            "text": "as location-specific advertisements and personalized recommendations. Standard POI models that utilize an RNN [22, 54, 130, 141, 248] or a temporal point process [71, 75, 127] are prone to making mistakes due to irregularities in the trajectories. These irregularities arise due to uneven data distributions, missing check-ins, and social links. Moreover, these approaches consider the check-in trajectory for each user as a sequence of events and thus have limited power to capture the user-location inter-dependence through their spatial neighborhood, i.e., the location-sensitive information that influences all neighborhood events. Some of the recent approaches [209, 230], harness the spatial characteristics by generating an image corresponding to each user trajectory and then utilizing a CNN as an underlying model. Such approaches based on visual data, and all CNN-based approaches, are limited by the image characteristics such as its resolution and scale of capture. Modern POI recommendation approaches such as [228] model the spatial network as a graph and utilize a random-walk-based model, with [223] proposing a graph-based neural network-based model to incorporate structural information of the network. Unfortunately, none of these approaches are designed for mobility prediction in limited data regions."
        },
        {
            "heading": "4.2.2 Graph based Recommendation",
            "text": "Existing graph embedding approaches focus on incorporating the node neighborhood proximity in a classical graph in their embedding learning process [66, 228]. For e.g., [85] adopts a label propagation mechanism to capture the inter-node influence and hence the collaborative filtering effect. Later, it determines the most probable purchases for a user via her interacted items based on the structural similarity between the historical purchases and the new target item. However, these approaches perform inferior to model-based CF methods since they do not optimize a recommendation-specific loss function. The recently proposed graph convolutional networks (GCNs) [103] have shown significant promise for recommendation tasks in useritem graphs. The attention-based variant of GCNs, graph attention networks (GATs) [207] are used for recommender systems in information networks [49, 212], traffic networks [69, 123] and social networks [235, 244]. Furthermore, the heterogeneous nature of these information networks comprises of multi-faceted influences that led to approaches with dual-GCNs across both user and item domains [49, 249]. However, these models cannot be generalized for spatial graphs due to the disparate weights, location-category as node feature, and varied sizes."
        },
        {
            "heading": "4.2.3 Clustering in Spatial Datasets",
            "text": "Due to the disparate features in our spatial graph, identifying the optimal number of clusters for the source and target region is a challenging task. Thus, we highlight a few key related works for clustering POIs and users in a spatial graph. Standard community-detection algorithms"
        },
        {
            "heading": "Cross-Region Transfer for Spatial Features 53",
            "text": "for spatial datasets [114, 127, 159, 214] are not suitable for grouping POIs as they ignore the graph structure, POI-specific features such as categories, geographical distances, and the order of check-ins in a user trajectory.\nRecent approaches [1, 58, 113, 151, 236] can automatically identify the number of clusters in a graph by capturing higher-order semantics between graph nodes, however, their application to graphs in spatial and mobility domains has certain challenges. In detail, (i) Ying et al. [236] can learn differentiable clusters for POIs and users for each region, however, these assignments are soft, i.e., without definite boundaries between distant POI clusters and, moreover, have a quadratic storage complexity; (ii) Gao and Ji [58] ignores the topology of the underlying spatial graph; (iii) Lee et al. [113] can be extended to spatial graphs, however, has limited scalability due to its self-attention [206] based procedure; (iv) Morris et al. [151] can incorporate higherorder structure in a POI graph using multi-dimensional Weisfeiler-Leman graph isomorphism; and (v) Abu-El-Haija et al. [1] can learn inter-user and inter-POI relationships by mixing feature representations of neighbors at various distances. However, due to the presence of two types of graph nodes \u2013 user and POI \u2013 identifying higher-order relationships by solely considering POI or user nodes is challenging. In addition, Chen et al. [25] uses a differentiable grouping network to discover the latent dependencies in a spatial network but is limited to air-quality forecasting."
        },
        {
            "heading": "4.2.4 Transfer Learning and Mobility",
            "text": "Transfer learning has long been addressed for tasks involving sparse data [57, 94] with applications to recommender systems as well [111, 116, 205]. Transfer-based spatial applications deploy CNNs across regions and achieve significant improvements in limited datasettings [209, 230]. However, these approaches are restricted to non-structural data and a graphbased approach has not been explored by the previous literature. [119] extends meta-learning to enhance recommendations in a POI setting, but is limited to a specific region. Information transfer across graphs is not a trivial task [105, 112, 232], and recent mobility models that incorporate graphs with meta-learning in [142, 165] are either limited to traffic datasets and do not incorporate the social network or are limited to new trajectories [50, 84]. From our experiments, we prove that a simple fine-tuning of the target data is susceptible to large cross-data variances, and thus, fine-tuning a generative model is not a trivial task in mobility-based networks."
        },
        {
            "heading": "4.3 Problem Formulation",
            "text": "We consider POI data for two regions, a source and a target denoted by Dsrc and Dtgt. We denote the users and locations in source and target networks as U src,Psrc \u2208 Dsrc and"
        },
        {
            "heading": "54 Cross-Region Transfer for Spatial Features",
            "text": "U tgt,P tgt \u2208 Dtgt correspondingly with no common entries U src \u2229 U tgt = Psrc \u2229 P tgt = \u2205. In other words, we do not need a common user between two regions to perform a cross-region mobility knowledge transfer. With a slight abuse of notation, the network for any region \u2013either target or source\u2013 is assumed to consist of users |U| = M , locations |P| = N and an affinity matrix R = {r}M\u00d7N . We populate entries in R as row-normalized number of check-ins made by a user to a location (i.e., multiple check-ins mean higher value). This can be further weighed by the user-location ratings, if available. We denote a pair of users as ui, uj and locations as la, lb. We also assume that for each location la we have one (or more) category label (such as Jazz Club, Cafe, etc.).\nProblem Statement (Target Region POI Recommendation). Given the mobility data of source and target regions,Dsrc andDtgt, our aim is to transfer the rich dynamics in the source region to improve POI recommendation for users in the target region. Specifically, maximize the following probability:\nP \u2217 = arg max{E[rtgtui,la |Dsrc,Dtgt]}, ui \u2208 U tgt, la \u2208 P tgt, (4.1)\nwhere E[rtgtu,l ] calculates the expectation of location la in the target region being visited by the user ui, thus r tgt ui,la \u2208 Rtgt, given the mobility data of users from both source and target regions. Simultaneously for a region, our objective as personalized location recommendation is to retrieve, for each user, a ranked list of candidate locations that are most likely to be visited by her based on the past check-ins available in the training set."
        },
        {
            "heading": "4.3.1 User-Location Graph Construction",
            "text": "For each region, we construct a heterogeneous user-location graph Gsrc and Gtgt but we describe generically as G = {U \u222a P , E} with each user and location as a node. The disparate edges Eu, El, Er \u2208 E determine the user-user, location-location and user-location relationships respectively. The structure of the graph is as follows:\n\u2022 An edge, eui,uj \u2208 Eu, between two users, ui and uj is denotes a social network friendship. \u2022 We form an edge, ela,lb \u2208 El, between two locations when any user has consecutive check-\nins between them \u2013 i.e., a check in at la (or lb) followed immediately by lb (or la) with edge weight based on the geographical distance [123, 244] between the two locations. Specifically, we use non-linear decay with distance as:\nw(ela,lb) = exp ( \u2212d(la,lb) \u03c32 ) , if d(la, lb) \u2264 \u03ba,\n0 otherwise,"
        },
        {
            "heading": "Cross-Region Transfer for Spatial Features 55",
            "text": "We also use the following notations to define different neighborhoods that will be used in our model description later:\n\u2022 Nui = {uk : eui,uk \u2208 Eu}: the social neighborhood of user ui, \u2022 Nla = {uk : euk,la \u2208 Er}: the user neighborhood of location la, \u2022 Sui = {lk : eui,lk \u2208 Er}: the location neighborhood of user ui, and, finally, \u2022 Sla = {lk : ela,lk \u2208 El}: locations in the spatial vicinity of la.\nNote that the graph G can also be enriched with all edges as weighted [223] conditioned on the availability of different features. However, we present a general framework that can be easily extended to such settings."
        },
        {
            "heading": "4.4 AXOLOTL Framework",
            "text": "In this section, we first describe in detail the basic model of AXOLOTL along with its training procedure. Then we present the key feature of AXOLOTL, viz., its ability to transfer model parameters learned from a data-rich region to a data-scarce region. For a specific region, we embed all users, U , and all locations, P , through matrices U = {ui}M\u00d7D and L = {la}N\u00d7D respectively with D as the embedding dimension. A summary of all notations is given in Table 4.1."
        },
        {
            "heading": "56 Cross-Region Transfer for Spatial Features",
            "text": ""
        },
        {
            "heading": "4.4.1 Basic Model",
            "text": "In the graph model of AXOLOTL, we capture the four aspects of influence propagation \u2013namely, user-latent embeddings (U l \u2208 RM\u00d7D), location-conditioned user embeddings (U s \u2208 RM\u00d7D), location-latent embeddings (Ll \u2208 RN\u00d7D), and user-conditioned location embeddings (Ls \u2208 RN\u00d7D)\u2013 illustrated in Figure 4.2. The basic model of AXOLOTL captures these four aspects using different graph attention networks, resulting in a twin-graph architecture as shown in Figure 4.3. In the rest of this section, we first describe each of the graph attention components and the prediction model in the basic model of AXOLOTL. Subsequently, we delineate the information transfer component that operates over this basic model.\nGAT for User Latent Embedding(\u03a61): In each iteration, using the available user embeddings, U , we aggregate each user\u2019s social neighborhood to obtain a new latent representation of the user. We denote this embedding as U l and calculate as follows:\nul,i = \u03c3 ( \u2211 uk\u2208Nui \u03b1\u03a61ui,uk (W \u03a61uk + b\u03a61) ) , ul,i \u2208 U l, (4.2)\nwhere ui \u2208 U ,Nui , \u03c3,W \u03a61 and b\u03a61 are the user node ui, nodes in the social neighborhood of ui, the activation function, the weight matrix and the bias vector respectively. \u03b1\u03a61ui,uk determines the attention weights between user embeddings uk and ui given by:\n\u03b1\u03a61ui,uk = exp\n( \u03c81(ui,uk) )\u2211 uj\u2208Nui exp ( \u03c81(ui,uj)\n) , (4.3) where, \u03c81(ui,uj) = LeakyReLU(G\u03a61 \u2297 (ui \u2016 uj)) calculates the inter-user attention weights with learnable parameterG\u03a61 .\nGAT for Location-conditioned User Embeddings (\u03a62): To encapsulate the influence on a user based on her check-ins as well as those by her social neighborhood, our embeddings must include location information for all check-ins made by different users in her social proximity. For this purpose, we first need to aggregate the location embeddings for every check-in made"
        },
        {
            "heading": "Cross-Region Transfer for Spatial Features 57",
            "text": "by a user (ui) as her location-based embedding, Q = {qi}M\u00d7D. We note that the category of a check-in location is arguably the root cause for a user to visit the location, and thus, to capture the location-specific category and the user-category affinity in these embeddings, we populate Q using a max-pooling aggregator across each location embedding weighted by the probability of a category to be in a user\u2019s check-in locations. That is,\nqi = MaxPool [ \u2211 lk\u2208Sui pui(lk) \u00b7 lk ] , \u2200qi \u2208 Q, (4.4)\nwhere Sui , pui(lk) respectively denote the location neighborhood of ui and the probability of a POI-category to be present in the past check-ins of ui. We calculate pui(lk) as the fraction of check-ins made by the user to POIs of the specific category with the total number of her checkins. Mathematically,\npui(lk) = Number of check-ins by ui at POIs with category same as lk\nTotal number of check-ins by ui , (4.5)\nWe calculate these probabilities for every POI category and these values are user-specific. Moreover, the values of pui(lk) can be considered as the explicit category preferences of a user ui. Later, to get the influence of neighborhood locations on a user, we aggregate the location-based neighbor embeddingsQ for each user (ui) based on her social network as:\nus,i = \u03c3 ( \u2211 uk\u2208Nui \u03b1\u03a62ui,uk (W \u03a62qk + b\u03a62) ) , us,i \u2208 U s, (4.6)\nwhere \u03b1\u03a62ui,uk is the attention weight for quantifying the influence a user has on another through its check-ins and is formulated using \u03c82 similar to \u03c81 (Eqn 4.3). The resulting embedding U s is the location-conditioned user embedding.\nGAT for Location Latent Embedding (\u03a63): Similar to \u03a61, we aggregate the neighborhood of each location, la \u2208 L, to get a latent representation of each location. To factor the inter-location edge-weight in our embeddings, we sample locations from the neighborhood with probability proportional to w(ela,lb), i.e., the closer the locations higher their repetitive sampling. These sampled locations represent the vicinity of the check-in and we encapsulate them to get the latent location representation.\nll,a = \u03c3 ( \u2211 lk\u2208Sla \u03b1\u03a63la,lk (W \u03a63lk + b\u03a63) ) , \u2200la \u2208 P , ll,a \u2208 Ll, (4.7)\nwhere \u03b1\u03a63la,lk is again formulated as in Equation 4.3, using \u03c83(la, lk)."
        },
        {
            "heading": "58 Cross-Region Transfer for Spatial Features",
            "text": "GAT for User-conditioned Location Embedding(\u03a64): Similar to \u03a62, we need to capture the influence of different users with check-ins nearby to the current location la. Thus we use a max-pool aggregator to capture the user neighborhood of each location weighted by its affinity towards the location category. Through this, we aim to encapsulate the locality-specific user preferences, i.e., the counter-influence ofQ in \u03a62, denoted as Y = {ya}N\u00d7D.\nya = MaxPool [ \u2211 uk\u2208Nla pla(uk) \u00b7 uk ] , \u2200ya \u2208 Y , (4.8)\nwhere, pla(uk) denotes the category affinity of all users in the neighborhood Nla of a location la. We calculate pla(uk) for each user as the fraction of check-ins of a user uk with the total check-ins for all users in Nla at the POIs with category same as la. Mathematically,\npla(uk) = Number of check-ins by uk with category \u2018cat\u2019\nNumber of check-ins by users in Nla with category \u2018cat\u2019 , (4.9)\nwhere \u2018cat\u2019 denotes the category of POI la. Moreover, these probabilities are specific to each POI and can be interpreted as the affinity of nearby users towards the POI category. Similar to \u03a63, we aggregate the location neighborhood using an edge-weight based sampling on Y to get user-conditioned location embedding Ls with parametersW \u03a64 , b\u03a64 and \u03b1 \u03a64 and \u03c84(ya,yb)."
        },
        {
            "heading": "4.4.2 Model Prediction",
            "text": "We combine the four representations of users and locations developed above using fully-connected layers, with a concatenated input of U l and U s for final user embedding U f = \u03d51 (U l \u2016U s);"
        },
        {
            "heading": "Cross-Region Transfer for Spatial Features 59",
            "text": "and similarly for locations Ll and Ls to obtain final location embedding Lf = \u03d52 (Ll \u2016Ls). Finally, we estimate a user\u2019s affinity to check-in at a location by a Hadamard (element-wise) product between the corresponding representations. Formally,\nr\u0302ul = \u03d53 (U f \u2297 Lf ) , (4.10)\nwhere \u03d51(\u00b7), \u03d52(\u00b7) and \u03d53(\u00b7) represent fully-connected neural layers. The parameters are optimized using a mean-squared error that considers the difference between the user\u2019s predicted and the actual affinity towards a location, with L1 regularization over the trainable parameters.\nLp = \u2211 (\u2200u,l) \u2225\u2225r\u0302ul \u2212 rul\u2225\u22252 + \u03bbp\u2225\u2225\u0398pred\u2225\u2225. (4.11) \u0398pred refers to all the trainable parameters in AXOLOTL for a region-specific prediction including the weights for all attention networks."
        },
        {
            "heading": "4.4.3 AXOLOTL: Information Transfer",
            "text": "We now turn our attention to the central theme of AXOLOTL, namely, the training procedure for AXOLOTL along with its cluster-wise transfer approach. We reiterate that we do not expect any common users/POIs between source and target regions, and thus the only feasible way to transfer mobility knowledge using the trained model parameters and the user-POI embeddings. Specifically, there are two channels of learning for AXOLOTL, (i) Spatio-Social Meta-learning based optimization, and (ii) Region-wise cluster alignment loss."
        },
        {
            "heading": "60 Cross-Region Transfer for Spatial Features",
            "text": ""
        },
        {
            "heading": "4.4.3.1 Spatio-Social Meta-Learning (SSML)",
            "text": "Meta-learning has long been proposed to alleviate data scarcity problems in spatial datasets [119, 165, 230]. Specifically, in meta-learning, we aim to learn a joint parameter initialization for multiple tasks by simultaneously optimizing the prediction loss for each task. However, there is a high variance in data-quality between the regions Dsrc and Dtgt and thus a vanilla metalearning \u2014also called a model agnostic meta-learning (MAML) [57]\u2014 will not be sufficient as the target region is expected to have nodes with limited interactions. Such nodes, due to their low contribution to the loss function, may get neglected during the meta-procedure. We overcome this via a hierarchical learning procedure that not only considers the location recommendation performance, but also the social neighborhood of all user and location nodes. We call the resulting learning procedure as spatio-social meta-learning (SSML) and the contrast between this approach and standard model agnostic meta-learning approach (MAML) [57] is schematically given in Figure 4.4.\nSpecifically, we consider the two tasks of (i) optimizing the POI recommendation loss function Lp across both source and target regions, and, (ii) neighborhood prediction for each node in both the networks [231].We initialize the parameters for the recommender system with global initial values (\u03b8st) shared across both source and target. Note that by \u03b8st we mean the parameters for all GATs (\u03a61\u00b7\u00b7\u00b74) and prediction MLPs (\u03d51\u00b7\u00b7\u00b73) and thus exclude region-specific, user and location embedding matrices, U src,Lsrc,U tgt and Ltgt. We describe them here:\nNeighborhood Prediction: For any region target or source, consider a user ui and her neighbor uj \u2208 Nui , we obtain the probability of them being connected on the social network as v\u0302ui,uj = ui \u00b7uTj where u\u2022 \u2208 U f represents the final representations of a user. Thus, for all regions,Dsrc andDtgt, we optimize the following cross-entropy loss:\nLUs (D\u2022) = \u2212 \u2211 ui\u2208U\u2022 \u2211 uj\u2208Nui u\u2032j /\u2208Nui [ log ( \u03c3(v\u0302ui,uj) ) + log ( 1\u2212 \u03c3(v\u0302ui,u\u2032j) )] , (4.12)\nwhere, v\u0302ui,uj , v\u0302ui,u\u2032j , \u03c3 denote the estimated link probability between two users connected by their social networks, with a negatively sampled user u\u2032j , i.e., a user not inNui , and the sigmoid function. Similarly, we calculate the probability of a location lb being in the spatial neighborhood of a location la as v\u0302la,lb = la \u00b7 lTb and denote the neighborhood loss as LPs (D\u2022). For the region as a whole, say target, the net neighborhood loss is defined as:\nLtgts = LUs (Dtgt) + LPs (Dtgt), (4.13)\nSimilarly, for source regions, we denote social loss as Lsrcs ."
        },
        {
            "heading": "Cross-Region Transfer for Spatial Features 61",
            "text": "POI Recommendation: Since AXOLOTL is designed for limited data regions, we purposely incline the meta-procedure towards improved target-region predictions. Specifically, we alter the meta-learning procedure by optimizing the parameters with the recommendation loss for the target region (Ltgtp ) for a predefined number of updates (Nu) and then optimize for the source region prediction loss(Lsrcp ).\nTarget : \u03b8tgtk+1 \u2190 \u03b8 tgt k \u2212 \u03c91\u2207\u03b8tgtk L tgt p (f\u03b8tgtk\n), \u2200 1 \u2264 k \u2264 Nu, \u03b8st \u2212 \u03c91\u2207\u03b8st Ltgtp (f\u03b8st), otherwise,\nSource : \u03b8src \u2190 \u03b8st \u2212 \u03c91\u2207\u03b8st Lsrcp (f\u03b8st), (4.14)\nwhere \u03b8st, \u03b8tgt, \u03b8src,Ltgtp ,Lsrcp , f\u03b8 are the global model-independent parameters, parameters for target and source region, prediction loss for target and source, and AXOLOTL output respectively.\nFinal Update: The final update to global parameters is done by: (i) optimizing the regionspecific social loss, Ltgts and Lsrcs , and (ii) minimizing the POI recommendation loss Ltgtp ,Lsrcp for both source and target regions. \u03b8st \u2190 \u03b8st\u2212\u03c92 \u00b7 [ \u2207\u03b8st Lsrcp (f\u03b8st) +\u2207\u03b8tgtNu L tgt p (f\u03b8tgtNu ) ] \u2212\u03c93 \u00b7 [ \u2207\u03b8st Lsrcs (f\u03b8st) +\u2207\u03b8st Ltgts (f\u03b8st) ] , (4.15) where, \u03c92, \u03c93 denote the region-wise learning rates."
        },
        {
            "heading": "4.4.3.2 Cluster Alignment Loss",
            "text": "Recent research [209, 230] has shown that enforcing similar patterns across specific POI clusters between source and target domains, e.g. from one university campus to another, facilitates better knowledge transfer. Unfortunately, obtaining the necessary semantic information to align similar clusters across regions that these techniques require is not always practical in large-scale settings. For a POI network, a rudimentary approach to identify clusters would be to traverse across the categories associated with each location \u2013 which may be quite expensive to compute and will neglect the user dynamics as well. We avoid these approaches and use a lightweight Euclidean-distance based k-means clustering to identify a set of users and locations in source as well as target regions (separately) that have displayed similar characteristics till the current iteration. Such a dynamic clustering mechanism over the contemporary GAT embeddings prevents the need for additional hand-crafting. In contrast to previous approaches [126, 231], we utilize a hard-assignment as unlike online product purchases, the mobility of a user is bounded by geographical distance [28], and thus checkins to distant locations are very unlikely. We denote U tgtc , L tgt c ,U src c , L src c \u2208 RK\u00d7D and C as the cluster embedding matrices for target region-users,"
        },
        {
            "heading": "62 Cross-Region Transfer for Spatial Features",
            "text": "Algorithm 1: Training Algorithm for AXOLOTL Input: Dsrc: Source-Region Training Data,Dtgt: Target-Region Training Data Mt: Epoch-based checkpoint, C: Clustering function Output: \u03b8tgt: Trained AXOLOTL Parameters for Target Region 1 \u03b8st, \u03b8tgt, \u03b8src \u2190 Randomly initialize all parameters 2 while epoch < Max_Epoch do 3 Parameter update via target-region social prediction: \u03b8tgt0 \u2190 \u03b8st \u2212 \u03c9\u2207\u03b8st Ltgts (f\u03b8st) 4 Parameter update via source-region social prediction: \u03b8src0 \u2190 \u03b8st \u2212 \u03c9\u2207\u03b8st Lsrcs (f\u03b8st) 5 Calculate the target-region recommendation loss: Ltgtp \u2190 PredictionLoss(Dtgt) 6 Initial update-before iterations: \u03b8tgt1 \u2190 \u03b8st \u2212 \u03c91\u2207\u03b8st Ltgtp (f\u03b8st) 7 for k < Nu do 8 Iterative updates: \u03b8tgtk+1 \u2190 \u03b8 tgt k \u2212 \u03c91\u2207\u03b8tgtk L tgt p (g\u03b8tgtk )\n9 Calculate the source-region recommendation loss: Lsrcp \u2190 PredictionLoss(Dsrc) 10 Update for source parameters \u03b8src \u2190 \u03b8st \u2212 \u03c91\u2207\u03b8st Lsrcp (f\u03b8st(Rt)) 11 Joint update for global parameters: As in Eqn. 4.15 12 if epoch mod Mt then 13 Lc \u2190 ClusterLoss(C,Dsrc,Dsrc) 14 Initialize cluster-based transfer \u03b8src, \u03b8tgt \u2190 minLc 15 epoch + + 16 Fine-tune for target region: \u03b8tgt \u2190 FineTune(Ltgtp ) 17 Return model parameters: return \u03b8tgt\nlocations, source region-users, locations, and the clustering algorithm respectively. We calculate the cluster embedding by mean-pooling the embeddings of elements in the cluster. For example, we calculate embeddings for user clusters in the target region as:\nutgtc = MeanPool [ui \u00b7\u03a8(ui, c)] \u2200ui \u2208 U tgt,utgtc \u2208 U tgtc , (4.16)\nwhere, \u03a8(ui, c) is the indicator function denoting whether user ui belongs to cluster c. Similarly we calculate Ltgtc , U src c and L src c . For minimizing the divergence between the similar users and POIs across regions, instead of engineering an explicit alignment between clusters, we use an attention-based approach to identify and align clusters with similar patterns and minimize the corresponding weighted L2 loss [94, 241].\nLc = \u2211\n\u2200ctu\u2208U tgt c \u2211 \u2200ctl\u2208L tgt c \u2225\u2225ctu \u2212 \u2211 csu\u2208Usrcc \u03b2uctu,csuc s u \u2225\u22252 + \u2225\u2225ctl \u2212 \u2211 csl\u2208L src c \u03b2lctl ,csl csl \u2225\u22252, (4.17)\nwhere \u03b2u,\u03b2l \u2208 RK\u00d7K , are the attention matrices for user and location clusters, respectively. Each index in \u03b2u,\u03b2l denotes the weight for a target and source cluster for users and location, respectively. A key modeling distinction between \u03b1 and \u03b2 is that the latter includes self -"
        },
        {
            "heading": "Cross-Region Transfer for Spatial Features 63",
            "text": "contribution of the node under consideration and in \u03b2 we only aim to capture the contribution by the source-cluster on the particular target-cluster. Therefore \u03b2 is calculated similarly as \u03b1 (Equation 4.3) after restricting to only the inter-cluster interactions."
        },
        {
            "heading": "4.4.3.3 Overcoming the Curse of Pre-Training",
            "text": "Transfer learning, by definition, requires the transfer source to be pre-trained, i.e., for the information propagation across clusters of users and locations, the set of weights for the source should be trained before initiating the transfer. In our setting, we do not extensively train the source parameters separately, as these parameters are jointly learned via meta-learning. This could be a severe bottleneck for the cluster-based transfer as it may lead to inaccurate information sharing across clusters as the source parameters are also simultaneously being learned. We reconcile these two by adopting a checkpoint-based transfer approach by performing transfer based on the number of epochs for parameter optimization of the model. Specifically, we optimize the region-specific model parameters for Mt epochs with each epoch across the entire source and target data. We checkpoint this model state and consider the user-location cluster embeddings to initialize transfer by optimizing the all-region parameters through clusteralignment loss, Lc [94]. Optimizing Lc updates the user and POI embedding by backpropagating the difference between similar source and target clusters. We minimize Lc via stochastic gradient descent(SGD) [179]. This checkpoint-based optimize-transfer cycle continues for fixed iterations, and then weights are later fine-tuned [57]. This learning procedure is described between line #12 and line #15 in Algorithm 1."
        },
        {
            "heading": "4.4.3.4 Significance of using SSML with Cluster Loss",
            "text": "Here we highlight the importance of the two tasks in SSML \u2013 neighborhood prediction and POI recommendation that are achieved by minimizing the loss functions LUs , LPs , and Lp respectively for each region. Particularly, the task of neighborhood prediction of each region is a combination of predicting the spatial neighbors of a POI (via LPs ) and the social network of a user (via LUs ). The former ensures that the embeddings of POIs located within a small geographical area can capture the latent features of the particular area [126, 230]. Such a feat is not achievable by minimizing the difference between POIs in a common cluster, as the clusters are determined explicitly from these embeddings, whereas the spatial graph is constructed using the distance between POIs, and thus is a better estimate of neighborhoods within a region. Similarly, minimizing LUs ensures that user embeddings capture the flow of POI-preferences between socially connected users [212] that cannot be captured via an embedding-based clustering. Thus, the task of predicting the neighborhood of a node can lead to better POI recommendations for users closer to a locality."
        },
        {
            "heading": "64 Cross-Region Transfer for Spatial Features",
            "text": ""
        },
        {
            "heading": "4.5 Experiments",
            "text": "We perform check-in recommendations in the test data to evaluate AXOLOTL across three geotagged activity streams from different countries. With our experiments, we aim to answer the following research questions:\nRQ1 Can AXOLOTL outperform state-of-the art baselines for location recommendation in sparse regions? RQ2 What are the contributions of different modules in AXOLOTL? RQ3 How are the weights in AXOLOTL transferred across regions?\nAll our models are implemented in Tensorflow on a server running Ubuntu 16.04. CPU: Intel(R) Xeon(R) Gold 5118 CPU @ 2.30GHz, RAM: 125GB, and GPU: NVIDIA V100 GPU."
        },
        {
            "heading": "4.5.1 Experimental Settings",
            "text": "Dataset Description. For our experiments, we combine POI data from two popular datasets, Gowalla [182] and Foursquare [228], across 12 different regions of varied granularities from the United States(US), Japan(JP), and Germany(DE). For each country, we construct 4 datasets: one with large check-in data and three with limited data. We adopt a commonly followed data cleaning procedure [126, 141] \u2014for source datasets, we filter out locations with less than 10 check-ins, users with less than 10 check-ins and less than 5 connections. For target datasets, these thresholds are set at 5, 5, and 2 respectively. Higher criteria are used for source datasets to minimize the effects of noisy data during transfer. The statistics of the twelve datasets is given in Table 4.2 with each acronym denoting the following region: (i) CA: California(US), (ii) WA: Washington(US), (iii) MA: Massachusetts(US), (iv) OH: Ohio(US), (v) TY: Tokyo(JP), (vi) HY: Hyogo(JP), (vii) KY: Kyoto(JP), (viii) AI: Aizu(JP) (ix) NR: North-Rhine Westphalia(DE), (x) BW: Baden-W\u00fcrttemberg(DE), (xi) BE: Berlin(DE), and (xii) BV: Bavaria(DE). We consider CA, TY, and NR as the source regions and WA, NY, MA and KY, HY, AI, and BV, BW, and BE as the corresponding target regions.\nEvaluation Protocol: For each region, we consider the first 70% data, based on the time of check-in as training, 10% as validation, and the rest as test data for both Gowalla and Foursquare. For each region, we use the training data to get a list of the top-k most probable check-in locations for each user and compare it with ground-truth check-ins in the test. Note that there is no user or location overlap between source and target regions and for each user we only recommend check-ins located in the specific region. For evaluation, we use: Precision@k and NDCG@k, with k = 1, 5, 10, and report confidence intervals based on three independent runs."
        },
        {
            "heading": "Cross-Region Transfer for Spatial Features 65",
            "text": "Parameter Settings: For all experiments, we adopt a three layer architecture for MLPs with dimensions \u03d51, \u03d52 = {32\u2192 32\u2192 D} and \u03d53 = {32\u2192 32\u2192 1}. Other variations for the MLP had insignificant differences. We keep Mt = {4, 6}, Nu = {4, 8}, K = {20, 50, 100}, \u03bbp = 0.01, \u03ba = 50km,D = 16 and batch-size in {16, 32}. Unless otherwise mentioned, we use these parameters in all our experiments. For meta- and cluster-based transfer, we set \u03c91, \u03c92, \u03c93 = 0.001 and learning-rate as 0.01, as recommended for training a meta-learning algorithm [57]."
        },
        {
            "heading": "4.5.2 Methods",
            "text": "We compare AXOLOTL with the state-of-the-art methods based on their architectures below:\n(1) Methods based on Random Walks: Node2Vec [66] Popular random-walks-based embedding approach that uses parameterized breadth- and depth-first search to capture representations. Lbsn2Vec [228] State-of-the-art random-walk-based POI recommendation, it uses a randomwalk-with-stay scheme to jointly sample user check-ins and social relationships to learn node embeddings. (2) Graph-based POI Recommendation Reline [29] State-of-the-art multi-graph based POI recommendation algorithm. Traverses across location and user graphs to generate individual embeddings. (3) Methods based on Matrix Factorization: GMF [173] Standard matrix factorization, which is optimized using a personalized prediction loss for users. NMF [86] Collaborative filtering-based model that applies MLPs above the concatenation of user and item embeddings to capture their interactions. (4) Methods based on Graph Neural Networks: NGCF [212] State-of-the-art graph neural network recommendation framework that encodes the collaborative signal with connectivity in the user-item bipartite graph. DANSER [221] Uses dual graph-attention networks across the item and user networks and predicts using a reinforcement policy-based algorithm."
        },
        {
            "heading": "66 Cross-Region Transfer for Spatial Features",
            "text": "(5) Methods using Transfer Learning: MDNN [16] An MLP based meta-learning model that performs global as well as local updates together. MCSM [205] A neural architecture with parameters learned through an optimization-based meta-learning. MeLU [111] State-of-the-art meta-learning based recommendation system. Estimates user preferences in a data-limited query set by using a data-rich support set across the concatenation of user and item representations. MAMO [44] Modifies MAML [57] to incorporate a heterogeneous information network by item content information and memory-based mechanism. PGN [78] A state-of-the-art meta-learning procedure for pre-training neural graph models to better capture the user and item embeddings. Specifically, it includes a three-step procedure \u2013 a neighborhood sampler, a GNN-based aggregator, and meta-learning-based updates. For our experiments, we apply PGN over graph attention networks [207].\nAs mentioned in Section 4.1, other techniques either collectively learn parameters across common users in both domains[116, 191] or either utilize to meta-path based approach [132], and thus are not suitable for our setting. Furthermore, to demonstrate the drawbacks of traditional transfer learning, we report results for the following variants of AXOLOTL:\nAXO-f: We train AXO-basic on the source data and fine-tune the weights for the target data as a standard transfer-learning setting. AXO-m: For this variant AXO-basic model is trained on both regions using only the proposed spatio-social meta-learning, i.e., without cluster-based optimization.\nThe main contribution including both SSML and cluster-based transfer is termed AXOLOTL.\nBaseline Implementations: Here, we present the implementation details for each of our baselines. Specifically, for region-specific models, we follow a standard practice of optimizing their parameters on the training set of the target region and then predicting for users in the corresponding test set. For MDNN and MCSM, train the parameters on both regions using their standard meta-learning procedure. For MeLU, we modify the training protocol to perform global-updates using the user-POI pairs for the target regions and local-updates using the source-region parameters. For more details, please refer to Section 3.2 in [111]. A similar training procedure is followed for MAMO. Lastly, for PGN we pre-train the model parameters on the source-region check-ins and then fine-tune on the target region as per their three-step optimizing procedure."
        },
        {
            "heading": "Cross-Region Transfer for Spatial Features 67",
            "text": ""
        },
        {
            "heading": "4.5.3 Performance Comparison",
            "text": "To address RQ1, we report on the performance of location recommendation of different methods across all our target datasets in Table 4.3. From these results, we make the following key observations:\n\u2022 AXOLOTL, and its variant AXO-m that employ meta-learning, consistently yield the best performance on all the datasets. In particular, the complete AXOLOTL improves over the strongest baselines by 5-18% across the metrics. These results signify the importance of meta-learning with external data to design solutions for limited-data regions. \u2022 AXO-f does not perform on par with other approaches. This observation further cements the advantage of a joint-learning over traditional fine-tuning. The performance gain by AXOLOTL over AXO-m highlights the importance of minimizing the divergence between"
        },
        {
            "heading": "68 Cross-Region Transfer for Spatial Features",
            "text": "the embeddings across the two regions. \u2022 Among meta-learning-based models, we note that MeLU [111] and PGN [78] perform\nbetter than other baseline models, however, they are easily outperformed by AXOLOTL. We also note that though AXOLOTL and PGN are graph-based meta-learning models, the performance difference can be attributed to the ability of AXOLOTL to include node features. Specifically, PGN only leverages the graph structure and cannot thus incorporate any heterogeneous auxiliary information about the entities, such as POI category and distances, whereas AXOLOTL captures all features of a spatial network. \u2022 The characteristic of MeLU [111] to include samples from the data-rich network into its meta-learning-based procedure leads to significant improvements over other baselines even with its MLP-based architecture. These improvements are more noteworthy for smaller datasets like Bavaria (BV). However, with the inclusion of graph attention networks, AXOLOTL captures the complex user-location dynamics better than MeLU. \u2022 Danser [221] and NGCF [212] perform comparable to meta-learning based baselines in some datasets \u2013 e.g. MA and WA. This is due to a sufficiently moderate dataset size to fuel their dual graph neural networks. Danser [221] particularly incorporates a reinforcement learning-based policy optimization which particularly leads to better modeling power albeit at the cost of more computation. However, for extremely limited-data regions and Precision@1 predictions, the input-data size alone is not sufficient to accurately train all parameters. \u2022 Despite Reline [29] being the state-of-the-art multi-graph-based model for location recommendation, other methods that incorporate complex structures using dual-GCNs or meta-learning are able to easily outperform it, even under sparse data conditions.\nTo sum up, our empirical analysis suggests the following: (i) the state-of-the-art models, including fine-tuning based transfer approaches, are not suitable for location recommendation in a limited-data region, (ii) AXOLOTL is a powerful recommender system not only for mobility networks with limited-data, but also in general, and (iii) for data-scarce regions, forcing embeddings to adapt with similar source-region clusters has significant performance gains."
        },
        {
            "heading": "4.5.4 Ablation Study",
            "text": "To address RQ2, we conduct an ablation study for two key contributions in AXOLOTL: userGATs (\u03a61 and \u03a62) and location-GATs (\u03a63 and \u03a64). For estimating the contribution of userGATs, we use meta-learning and alignment loss-based training only for \u03a61 and \u03a62. We denote this variant as AXOLOTL-\u03a61,2. Similarly, for location-GATs, we use AXOLOTL-\u03a63,4. We also include a GCN [103] based implementation of AXOLOTL denoted as AXOLOTL-gcn. From the results in Figure 4.5, we observe that AXOLOTL with joint training of user and location\nGATs has better prediction performance than AXOLOTL-\u03a61,2 and AXOLOTL-\u03a63,4. Interestingly, transferring across users leads to better prediction performance than transferring across locations. This could be attributed to a larger difference in the number of locations between source and target regions in comparison to the number of users. AXOLOTL-gcn has significant performance improvements over the preceding approaches, with AXOLOTL further having modest improvements due to weighted neighborhood aggregation.\nContribution of SSML: To further assert the importance of our SSML (Section 4.4.3.1), we compare the state-of-the-art MAML-based model, viz., MeLU, with our proposed learning procedure, and also include results after training AXO-basic with MAML. From the results given in Figure 4.6, we note that MeLU, when trained with SSML, easily outperforms its MAML-based counterpart. It demonstrates not only the effectiveness of the proposed learning method but also its versatility to be incorporated with other baselines. This claim is substantiated further by the poorer performance of AXO-basic with MAML over the complete AXOLOTL model."
        },
        {
            "heading": "4.5.5 Transfer of Weights across Regions",
            "text": "Another important contribution we make is the cross-region transfer via cluster-based alignment loss. To address RQ3, we show that AXOLOTL encapsulates the cluster-wise analogy by"
        },
        {
            "heading": "70 Cross-Region Transfer for Spatial Features",
            "text": "plotting the attention-weights corresponding to the similarity between location clusters (Ltgtc and Lsrcc ). We quantify the similarity using Damerau-Levenshtein distance [31] across category distribution for all clusters in source and target regions. Later, we group them into five equal buckets as per their similarity score, i.e., bucket-5 will have clusters with higher similarity as compared to other buckets. Figure 4.7 shows the mean attention value across each bucket for all datasets. We observe that AXOLOTL is able to capture the increase in inter-cluster similarity by increasing its attention weights. This feature is more prominent for Aizu and comparable for Berlin."
        },
        {
            "heading": "4.6 Conclusion",
            "text": "In conclusion, we developed a novel architecture called AXOLOTL that incorporates mobility data from other regions to design a location recommendation system for data-scarce regions. We also propose a novel procedure called the spatio-social meta-learning approach that captures the regional mobility patterns as well as the graph structure. We address the problems associated with an extremely data-scarce region and devise a suitable cluster-based alignment loss that enforces similar embeddings for communities of users and locations with similar dynamics. Experiments over diverse mobility datasets revealed that AXOLOTL is able to significantly improve over the state-of-the-art baselines for POI recommendation in limited data regions and even performs considerably better across datasets and data-rich source regions.\nChapter 5"
        },
        {
            "heading": "Sequential Recommendation using",
            "text": ""
        },
        {
            "heading": "Spatio-Temporal Sequences",
            "text": ""
        },
        {
            "heading": "5.1 Introduction",
            "text": "There exists a high variability in mobility data volumes across different regions, which deteriorates the performance of spatial recommender systems that rely on region-specific data. In addition, the existing techniques to overcome spatial data-scarcity are limited by their inability to model the sequentially in POI visits by a user [72, 84, 209, 230]. Addressing this problem is necessary as recent research has shown that accurate advertisements on Points-of-Interest (POI) networks, such as Foursquare and Instagram, can achieve up to 25 times the return-oninvestment [149]. Consequently, predicting the time-evolving mobility of users, i.e., where and when, is of utmost importance to power systems relying on spatial data.\nIn this chapter, we present our solution to overcome the drawbacks of data scarcity for sequential POI recommendations. Current approaches [28, 126, 240] overlook the temporal aspect of a recommender system as it involves modeling continuous-time check-in sequences \u2013 which is challenging with standard neural architectures [45, 147, 251]. The problem is further aggravated by the variation in volumes of mobility data across regions due to the growing awareness of personal data privacy [11, 216]. As highlighted in Chapter 4, there exists a high variability in mobility data volumes across different regions, which deteriorates the performance of spatial recommender systems that rely on region-specific data. Consequently, this scarcity of data affects the performance of all neural models.\nIn recent years, neural marked temporal point processes (MTPP) models have outperformed other neural architectures for characterizing asynchronous events localized in continuous time\n71"
        },
        {
            "heading": "72 Sequential Recommendation using Spatio-Temporal Sequences",
            "text": "with applications including healthcare [176], finance [7, 243], and social networks [75, 147, 251]. These models have also been used in spatial networks and for predicting user mobility patterns. However, these models are either: (i) limited to predicting the time of userlocation interactions, i.e., the time when the user will check-in next and do not predict the precise location [229], (ii) restricted to one dataset without a foreseeable way to easily utilize external information and disregard the opportunity to reuse trained parameters from external datasets [26, 127]. Thus, none of these approaches can be used for designing spatial mobility prediction models for limited data regions."
        },
        {
            "heading": "5.1.1 Our Contribution",
            "text": "We present REFORMD (Reusable Flows for Mobility Data), a novel transfer learning framework that learns spatial and temporal distribution of check-ins using normalizing flows(NFs) on a check-in-rich source region and transfers them for efficient prediction in a check-in-scarce target region [71]. Specifically, we consider the series of check-ins made by a user as her check-in sequence and model these sequences for all users from a region using a neural MTPP and learn the inter-check-in time interval and spatial-distance distributions as two independent NFs [143, 175]. To make the learned spatial and temporal NFs invariant of the underlying region, we restrict our model to learn the distribution of inter-check-in time intervals and spatial distance. These features are unaffected by the network characteristics that vary across regions \u2013 POI categories and user affinities towards these POIs. Therefore, these NFs can be easily extended for prediction in other mobility regions. The ability of NFs to provide faster sampling and closed-form training for continuous-time event sequences [186] makes them a perfect medium to transfer mobility information. Moreover, for transferring across regions, we cluster the check-in sequences of each region, with each cluster containing check-in sequences with similar spatial and temporal check-in patterns and only transfer the parameters across these clusters. In summary, the key contributions we make via REFORMD are three-fold:\n\u2022 We propose REFORMD, a transfer-learning model for predicting mobility dynamics in check-in-scarce datasets by incorporating mobility parameters trained on a check-in-rich region. \u2022 We present a novel NF-based transfer over the MTPP that not only enables a faster sampling of time and distance features of next check-in but also achieves high performance even with limited fine-tuning on the target region. \u2022 Finally, we empirically show that REFORMD outperforms the state-of-the-art models by up to 20% and 23% for check-in-category and time prediction and can easily be extended to product recommendation datasets."
        },
        {
            "heading": "Sequential Recommendation using Spatio-Temporal Sequences 73",
            "text": ""
        },
        {
            "heading": "5.2 Related Works",
            "text": "In this section, we introduce key related works. It mainly falls into the following categories: 1) Sequential POI Recommendation; and 2) Temporal Point Processes with Spatial Data."
        },
        {
            "heading": "5.2.1 Sequential POI Recommendation",
            "text": "Recent sequential POI prediction models consider the check-in trajectory for each user as a sequence of events and utilize an RNN-based learning [27, 141, 240] with some variants that incorporate the spatial features as well [54, 130]. Another approach [56] is a generic model for predicting user trajectories as well as the next product recommendation. Recent approaches for check-in time prediction are limited to a single dataset [26, 127, 229]. They also model event times as random variables rather than sequential flows and, thus, cannot be used for transfer across regions."
        },
        {
            "heading": "5.2.2 Marked Temporal Point Processes with Spatial Data",
            "text": "MTPPs have emerged as a powerful tool to model asynchronous events localized in continuous time [36, 80], which have a wide variety of applications, for e.g., information diffusion, disease modeling, finance, etc. Driven by these motivations, in recent years, there has been a surge of works on TPPs [51, 75, 176]. Modeling the event sequences via a neural network led to further developments, including neural Hawkes process [147] and several other neural models of TPPs [162, 224, 225], but cannot incorporate heterogeneous features as in spatial networks. Moreover, recent works that deploy MTPP for predicting user mobility patterns are either: (i) limited to predicting the time of user-location interactions rather than actual locations [229], (ii) restricted to one dataset without a foreseeable way to easily utilize external information [127], or (iii) disregard the opportunity to reuse trained parameters from external datasets by jointly embedding the check-in and time distributions [26]. Thus, none of these approaches can be used for designing mobility prediction models for limited data regions. The approach most similar to our model is [186] that learns the inter-event time intervals using NFs, but ignores the spatial dynamics and is limited to a single data source."
        },
        {
            "heading": "5.3 Problem Setup",
            "text": "We consider the mobility records for two regions with non-overlapping locations and users, source and target asDsrc andDtgt respectively. These notations are consistent with Chapter 4. For any region, we represent a user trajectory as a sequence of check-ins represented by Sk = {ei = (ci, ti, di)|i \u2208 [k], ti < ti+1, di < di+1}, where ti \u2208 R+ is the check-in time, di \u2208 R+ is the total distance traveled, and ci \u2208 C is a discrete category of the i-th check-in with C as"
        },
        {
            "heading": "74 Sequential Recommendation using Spatio-Temporal Sequences",
            "text": "the set of all categories, and Sk denotes the first k check-ins. We represent the inter-check-in times and distances as, \u2206t,k = tk \u2212 tk\u22121 and \u2206d,k = dk \u2212 dk\u22121 respectively and model their distribution using NFs. Our goal is to capture these region invariant dynamics in the source region for mobility prediction in the target region, i.e.given the check-in sequence for the target region, S tgtK and the MTPP trained on source region, we aim to predict the time and category of the next check-in, etgtK+1."
        },
        {
            "heading": "5.4 Model Description",
            "text": "We divide the working of REFORMD into two parts: (i) the neural MTPP to capture mobility dynamics specific to a region, and (ii) the transfer of NFs trained on the source region to the target region. The underlying framework of REFORMD is given in Figure 5.1."
        },
        {
            "heading": "5.4.1 Region-Specific MTPP",
            "text": "We model the check-in sequences using an MTPP that we build on a recurrent neural network (RNN). The RNN is used to obtain time-conditioned vector representation of sequences, as in [45, 147, 162]. Later, via these embeddings, we estimate the mark distribution and interevent time and space densities using a three-stage architecture:\n(1) Input stage: In this stage, we represent the incoming check-in at index k, ek using a suitable vector embedding, vk as:\nvk = wcck +wt\u2206t,k +wd\u2206d,k + bv, (5.1)\nwhere w\u2022, b\u2022 are trainable parameters, vk denotes the vector embedding for check-in ek, and ck denotes the category of the k-th check-in respectively. Here, \u2206t,k = tk \u2212 tk\u22121 and \u2206d,k = dk \u2212 dk\u22121 denote the difference between check-ins in terms of time and distance respectively. (2) Update stage: In this stage, we update the hidden state representation of the RNN to include the current check-in ek as:\nsk = tanh(Gssk\u22121 +Gvvk + gt\u2206t,k + gd\u2206d,k + bs), (5.2)\nwhere G\u2022, g\u2022, b\u2022 are trainable parameters and sk denotes the RNN hidden state, i.e., a cumulative embedding for all previous check-ins till the current time tk. (3) Output stage: Given the trajectory embedding sk, we predict the next check-in time and the check-in category. Unlike [45, 147] that learn the time distribution using an intensitybased formulation using the RNN hidden state, we model the density of arrival times"
        },
        {
            "heading": "Sequential Recommendation using Spatio-Temporal Sequences 75",
            "text": "using a lognormal [186] flow denoted as pt(\u2206t,k+1) conditioned on sk. More details are given in Chapter 2.\npt(\u2206t,k+1|sk) = LogNormal ( \u00b5t(sk), \u03c3 2 t (sk) ) , (5.3)\nwith [\u00b5t(sk), \u03c32t (sk)] = [W \u00b5sk + \u00b5t,W \u03c32sk + \u03c3 2 t] denote the mean and variance of the time distribution. Such a formulation reduces model complexity and facilitates faster training and sampling in a closed-form [186]. To predict the time of the next check-in, we sample the probable time difference between the current and the next check-in as \u2206t,k+1 \u223c LNt ( \u00b5t(sk), \u03c3 2 t (sk) ) , where LNt denotes the learned lognormal parameters. The time of the next check-in is the sum of the sampled time difference and the current check-in time, t\u0302k+1 = tk+\u2206t,k+1. Similar to the temporal flow, we also model the inter-check-in density of spatial distances using a lognormal denoted as pd(\u2206d,k+1|sk). We interpret this distribution as the spatial flow for a region.\nThe inter-location spatial distance plays a crucial role in determining the next POI [28, 141]. Unlike time, the distances between two check-in locations are unchanged throughout the data. Previous approaches [45, 147] ignore these spatial features and rely solely on the past checkin-categories. This affects the prediction accuracy as the travel distance to the location is a critical element for determining POI recommendations. Moreover, in a sequential setting, the distance that the user will travel for her next check-in is not known. Our MTPPs, being generative models, and spatial flows overcome this drawback as we can sample the probable travel distance for the next check-in from the spatial flow as \u2206d,k+1 \u223c LNd ( \u00b5d(sk), \u03c3 2 d(sk) ) . Then, for predicting the next check-in, we use the sampled distance \u2206d,k+1 and RNN hidden state sk via attention-weighted embedding [8].\ns\u2217k = sk + \u03b1 \u00b7wf\u2206d,k+1, (5.4)"
        },
        {
            "heading": "76 Sequential Recommendation using Spatio-Temporal Sequences",
            "text": "where \u03b1,wf denote the attention weight, a trainable parameter, and s\u2217k denotes the updated hidden state. We then predict the next check-in category as:\nP(ck+1 = c|s\u2217k) = exp(V s,cs\n\u2217 k + bs,c)\u2211\n\u2200c\u2032\u2208C exp(V s,c\u2032s \u2217 k + bs,c\u2032)\n, (5.5)\nwhere V s,\u2022, bs,\u2022 are trainable parameters and \u2022 denotes the entry corresponding to a category. P(ck+1 = c|s\u2217k) denotes the probability of next check-in being of category c with c \u2208 C.\nOptimization: Given the set of all sequences S for a region D, we maximize the joint likelihood for the next check-in, the lognormal density distribution of spatial and temporal normalizing flows.\nL = \u2211 \u2200S |S|\u2211 k=1 log ( P(ck+1|s\u2217k) \u00b7 pt(\u2206t,k+1|sk) \u00b7 pd(\u2206d,k+1|sk) ) . (5.6)\nwhere L denotes the joint likelihood, which we represent as the sum of the likelihoods for all user sequences. We learn the parameters of REFORMD using Adam [100] optimizer."
        },
        {
            "heading": "5.4.2 Flow-based Transfer",
            "text": "For transferring the mobility parameters across the regions, we follow the standard transfer learning procedure of training exclusively on the source region and then fine-tuning for the target region [142, 164]. However, the affinity of a user towards a POI evolves with time [28, 126]. For example, a POI with frequent user check-ins during the summer season might not be an attractive option in winter. We include these insights by training multiple independent normalizing flows, each for spatial and temporal densities. Specifically, we cluster the checkin sequences in the source region intoM perfectly equal clusters based on the median of the occurrence times for all the check-ins. Later, for each cluster of sequences, we train spatial and temporal flows independently. Here, our net likelihood changes to include the sum of all M likelihoods, L = \u2211M i=1 Li, where Li is the joint likelihood for trajectories in clustersM.\nAs in the source region, we divide the user trajectories in the target region as well into M clusters, and for trajectories in target-cluster mtgti , we attentively factor the spatial and temporal flows corresponding to source-cluster msrci . Mathematically, for the temporal flows in the target region, our density of arrival times changes to:\n[\u00b5t(sk), \u03c3 2 t (sk)] tgt = [W \u00b5sk + b\u00b5 + \u03c6t\u00b5 src t ,W \u03c3sk + b\u03c3 + \u03c6t\u03c3 src t ], (5.7)\nwhere sk, \u03c6t,\u00b5srct ,\u03c3 src t denotes the hidden state representation for the target region, attention"
        },
        {
            "heading": "Sequential Recommendation using Spatio-Temporal Sequences 77",
            "text": "parameter for temporal flow and the learned flow parameters of mean and variance for cluster mi in the source region. Similarly, our spatial flows for the target region include the source flow parameters with an attention parameter \u03c6d. For faster convergence, we share \u03c6t and \u03c6d across all M. Other model components are the same as in the source region and we maximize the joint likelihood for the target region as in Equation 5.6.\nWe highlight that our choice to divide the sequences based on median of user trajectories rather than the individual check-in locations is driven by the following technical point: in the latter case, the net flow \u2013be it spatial or temporal\u2013 would be the sum of lognormal flows for each set. Such a formulation is undesirable since the result is neither closed nor does it remain a lognormal [47], thus requiring involved techniques to approximate them [12], which we would like to explore in future work. However, with the current formulation, we can learn the parameters of different flows independently."
        },
        {
            "heading": "5.5 Evaluation",
            "text": "In this section, we conduct an empirical evaluation of REFORMD. Specifically, we address the following research questions.\nRQ1 Can REFORMD outperform state-of-the-art baselines for time and check-in prediction? RQ2 What is the advantage of transferring via normalizing flows? RQ3 Can we extend REFORMD for non-spatial datasets?\nFor evaluating mobility prediction, we consider six POI datasets from the US and Japan [228]. All our models are implemented in Tensorflow on an NVIDIA Tesla V100 GPU."
        },
        {
            "heading": "5.5.1 Experimental Settings",
            "text": "Dataset Description:. We use POI data from Foursquare [228] in the United States(US) and Japan(JP). For each country, we construct four datasets: one with large check-in data and three with limited data. The statistics of all datasets are given in Table 5.1 with each acronym denoting the following region: (i) NY: New York(US), (ii) MI: Michigan(US), (iii) NV: Nevada(US),"
        },
        {
            "heading": "78 Sequential Recommendation using Spatio-Temporal Sequences",
            "text": "(iv) VI: Virginia(US), (v) TY: Tokyo(JP), (vi) CH: Chiba(JP), (vii) SA: Saitama(JP) and (viii) AI: Aichi(JP). We consider NY and TY as the source regions and MI, NV, VI, CH, SA, and AI as the corresponding target regions. For each region, we consider the time of check-in and category as event time and mark and normalize the times based on the minimum and maximum event times. We set the embedding and RNN hidden dimension to 64 andM = 3 for all our experiments.\nEvaluation Protocol:. We split each stream of, say N check-ins SN into training and test set, where the training set (test set) consists of the first 80% (last 20%) check-in. We evaluate models using standard metrics [45] of (i) mean absolute error (MAE) of predicted and actual check-in times, 1|S| \u2211 ei\u2208S [|ti\u2212t\u0302i|] and (ii) mark (check-in category) prediction accuracy (MPA),\ni.e., 1|S| \u2211\nei\u2208S #(ci = c\u0302i). Here t\u0302i and c\u0302i are the predicted time and category of the i-th checkin. Moreover, the clustering of sequences into different sets is done based solely on the training data, and using these thresholds, we assign clusters to sequences in the test data.\nBaseline Implementation Details. We use similar baseline model implementations as in Chapter 3. For RMTPP, we set hidden dimension, and the back-propagation through time parameter (BPTT) is selected among {32, 64} and {20, 50}, respectively. For THP, and SAHP, we set the number of attention heads as 2, hidden, key-matrix and value-matrix dimensions are selected among {32, 64}. If applicable, for each model, we use a dropout of 0.1. For PFPP, we set \u03b3 = 1 and use a similar procedure to calculate the embedding dimension as in the THP. All other parameter values are the ones recommended by the respective authors."
        },
        {
            "heading": "5.5.2 Baselines",
            "text": "We compare the performance of REFORMD with the following state-of-the-art methods:\n\u2022 NHP [147]: Models an MTPP using continuous-time LSTMs for capturing the temporal evolution of sequences. \u2022 RMTPP [45]: A recurrent neural network that models time differences to learn a representation of the past events. \u2022 SAHP [243]: A self-attention model to learn the temporal dynamics using an aggregation of historical events. \u2022 THP [251]: Extends the transformer model [206] to include the conditional intensity of event arrival and the inter-mark influences.\nWe omit comparison with other continuous-time models [80, 162, 186, 224, 225] as they have already been outperformed by these approaches."
        },
        {
            "heading": "Sequential Recommendation using Spatio-Temporal Sequences 79",
            "text": ""
        },
        {
            "heading": "5.5.3 Prediction Performance",
            "text": "Here we address RQ1, i.e., the ability of REFORMD to model the dynamics of a check-in sequence. Tables 5.2 and 5.3 summarize the results, which sketches the comparative analysis in terms of mean absolute error (MAE) on time and mark prediction accuracy (MPA) of the next check-in, respectively. Through the results, we make the following observations:\n\u2022 REFORMD consistently yields the best performance on all the datasets. In particular, it improves over the strongest baselines by 10% and 19% for category and time prediction respectively. These results indicate the importance of spatial and temporal flow-based transfer from external data for prediction in limited-data regions. RMTPP [45] is the second-best performer in terms of MAE of time prediction almost for all the datasets. \u2022 As per the results, we also establish that the enhancement in terms of performance gain due to transfer learning is more significant for mark prediction rather than time prediction. This is due to the consistent performance gains over all baselines in MPA; however, for MAE, the performance gap narrows."
        },
        {
            "heading": "80 Sequential Recommendation using Spatio-Temporal Sequences",
            "text": "We also highlight that there exists a performance gap between NHP and RMTPP. Recent research [121] has shown that these models show similar performances. However, these performances can be obtained after extensive parameter tuning for NHP, which we consider beyond the scope of this work."
        },
        {
            "heading": "5.5.4 Qualitative Analysis",
            "text": "We also perform a qualitative analysis to demonstrate how REFORMD is able to model the check-in time distribution. For this, we plot the actual inter-check-in time differences and the difference time predicted by REFORMD in Figure 5.2 for Virginia and Aichi datasets. From the results, we note that the predicted inter-arrival times closely match with the true inter-arrival times, and REFORMD is even able to capture large time differences (peaks). This reinforces the ability of REFORMD to learn the temporal dynamics of a check-in sequence. Moreover, it demonstrates our claim that using external data can enhance recommendation performance. For other datasets, we noted that they displayed similar trends.\nRuntime: Here, we report on the run-time of REFORMD to verify its applicability in real-world settings. Specifically, for all datasets, the times for training on the source and, later on, target regions are within 3 hours, thus within the range for practical deployment. In addition to shorter"
        },
        {
            "heading": "Sequential Recommendation using Spatio-Temporal Sequences 81",
            "text": "training times that are feasible for deployment, we further highlight that these values are mainly due to the inefficient CPU-based batch sampling. With GPU-based sampling alternatives, this run-time can be significantly improved, which we consider for future work."
        },
        {
            "heading": "5.5.5 Advantages of Transfer",
            "text": "To address RQ2, we report that REFORMD outperforms other baselines and also exhibits a key feature of transfer learning, i.e., quick parameter learning [164]. Exhibiting this property is necessary for transfer-learning models as it highlights the effect of source-region data and whether a transfer procedure was necessary. We highlight this characteristic by plotting the time prediction error (MAE) corresponding to the epochs trained on the target region for REFORMD and the best time prediction model, i.e., RMTPP. Figure 5.3 summarizes the results for Virginia and Aichi. From the results, we note that REFORMD exhibits faster convergence than RMTPP for both datasets. More specifically, the flow-based transfer procedure of REFORMD can outperform most baselines even with a fine-tuning of a few epochs. Thus, validating the effectiveness of the transfer-learning method. Moreover, the results also highlight the stable learning procedure of REFORMD. Specifically, even after using the tuned parameters on a new region. REFORMD displays consistently improving performance with an increase in epochs."
        },
        {
            "heading": "5.5.6 Product Recommendation",
            "text": "To address RQ3, we further evaluate the performance of REFORMD in product recommendation, i.e., without spatial coordinates. Consequently, we use purchase records for three item categories from Amazon [157], namely Digital Music (DM), Appliances (AP), and Beauty (BY). For each item, we use the user reviews as the events in a sequence with the time of the written review as the event time and the rating (1 to 5) as the corresponding mark. As in this case, we do not have a spatial density function pd(\u2206d,k+1), we change the fusion equation 5.4"
        },
        {
            "heading": "82 Sequential Recommendation using Spatio-Temporal Sequences",
            "text": "to include the predicted time of next purchase as:\ns\u2217k = sk + \u03b1 \u00b7wf\u2206t,k+1, (5.8)\nWe consider Digital Music(|S| = 12k) as source and Appliances(|S| = 7k) and Beauty(|S| = 6k) as target. From the results in Table 5.4 we make the following observations:\n\u2022 Even in the absence of spatial flows, REFORMD outperforms other baselines across all metrics. \u2022 For product recommendations as well, we see a similar trend as in spatial datasets with RMTPP outperforming other baseline models in terms of time prediction. \u2022 Lastly, we note that THP performs competitively with REFORMD for mark prediction.\nThe results show that the performance gain due to REFORMD is more significant for POI recommendation than product recommendation. Thus, highlighting the scope for improvement with transfer-learning methods for the product recommendations, which is beyond the current formulation of REFORMD."
        },
        {
            "heading": "5.6 Conclusion",
            "text": "In conclusion, we developed a novel architecture, called REFORMD, a novel method for sequential POI recommendation that incorporates mobility data from other regions to design a location recommendation system for data-scarce regions. Specifically, it transfers mobility knowledge across regions by sharing the spatial and temporal NFs for continuous-time check-in prediction. Experiments over diverse mobility datasets revealed that REFORMD is able to significantly improve over the state-of-the-art baselines for POI recommendation in limited data regions and even performs better than other models for product recommendation.\nChapter 6"
        },
        {
            "heading": "Learning Spatial Behaviour using User",
            "text": ""
        },
        {
            "heading": "Traces on Smartphone Apps",
            "text": ""
        },
        {
            "heading": "6.1 Introduction",
            "text": "The rapid advancements in the smartphone industry and ubiquitous internet access have led to exponential growth in the number of available users and internet-based applications. These smartphones have become increasingly prevalent across the entire human population, with up to 345 million units sold in the first quarter of 20211. Consequently, the online footprint of a user spans multiple applications with an average smartphone owner accessing 10 applications (or apps) in a day and 30 apps in each month2. These footprints can be perceived as the digitized nature of the user\u2019s proclivity in different domains. Recent research [2, 97] has shown that the online web activity of a user exhibits re-visitation patterns, i.e., a user is likely to visit certain apps repetitively with similar time intervals between corresponding visits. Gonzalez et al. [63] and Li et al. [122] have shown that these online re-visitation patterns are analogous to their spatial mobility preferences, i.e., the current geographical location can influence the web-browsing activities of a user. Moreover, such cross-domain information of app preferences of a user can be collected without using any personally identifiable information (PII), and thus, maintain the privacy of a user [53, 211, 239]. Therefore, to enhance the performance of a points-of-interest (POI) recommendation system, it is crucial to model the app re-visitation users along with their location preferences.\n1https://www.canalys.com/newsroom/canalys-worldwide-smartphone-market-Q1-2021 (Accessed October 2022) 2https://buildfire.com/app-statistics/ (Accessed October 2022)\n83"
        },
        {
            "heading": "84 Learning Spatial Behaviour using User Traces on Smartphone Apps",
            "text": ""
        },
        {
            "heading": "6.1.1 Limitations of Prior Works",
            "text": "Modern POI recommendation approaches [54, 72, 228] utilize the standard features specific to a user and a POI \u2013 a social network, geo-coordinates, and the category classifications of POIs \u2013 to learn the mobility patterns of a user. The situation has been exacerbated in recent times due to the advent of restrictions on personal data collection and a growing awareness (in some geopolitical regions) about the need for personal privacy [11, 216]. Moreover, current approaches overlook two crucial aspects of urban computing \u2013 the exponential growth of online platforms and the widespread use of smartphones. Undeniably, everyone carries and simultaneously uses their smartphones wherever they go. To highlight the importance of the relationship between POI and the apps being used, in Figure 6.1, we plot the category of the app used by all users at the ten most popular locations from our Shanghai-Telecom dataset [239]. The plot shows that the check-in locations can influence a user to visit apps of certain categories more than other apps. We note that this influence of a POI over the category of the app is applicable to multiple users.\nThe correlation between spatial mobility and smartphone use is essential to address the problems related to user demographics [161, 199], trajectory analysis [134], app recommendation [247], and to identify hotspots for network operators [122]. However, utilizing smartphone usage for sequential POI recommendations is not addressed in the past literature. The approaches most similar to our work are by Wang et al. [208] and Tu et al. [198]. Wang et al. [208] utilizes a Dirichlet process to determine the next user location but it completely disregards the user\u2019s privacy, i.e., requires precise geo-coordinates. Lastly, Tu et al. [198] is limited to the cold-start recommendation."
        },
        {
            "heading": "Learning Spatial Behaviour using User Traces on Smartphone Apps 85",
            "text": ""
        },
        {
            "heading": "6.1.2 Our Contribution",
            "text": "In this chapter, we present REVAMP(Relative position Vector for App-based Mobility Prediction), a sequential POI recommendation model that learns the location and app affinities of smartphone users while simultaneously maintaining their privacy needs [73]. Specifically, we consider each check-in as an event involving a smartphone activity and the physical presence at a POI and REVAMP models the correlation between the smartphone-app preferences and the spatial mobility preferences of a user. Parallelly, to preserve the privacy restrictions, it solely utilizes two aspects of urban mobility: (a) the types of smartphone apps used during a check-in and (b) the category of the check-in location. Thus, the proposed approach is not privy to any kind of identifiable information (PII) related features such as the precise smartphone app being accessed, e.g., \u2018Facebook\u2019, \u2018Amazon\u2019, etc., the accurate geo-location, inter-check-in distance, or the social network of a user. Buoyed by the success of self-attention [206] models in sequence modeling, REVAMP encodes the dynamic check-in preferences in the user trajectory as a weighted aggregation of all past check-ins. Moreover, to better capture the evolving POI and app preferences, it models the variation between each check-in in the sequence using absolute and relative positional encodings [17, 185]. Specifically, we embed three properties associated with each check-in \u2013 the smartphone app category, POI-category, and the time of check-in\u2013 and model the temporal evolution as the inter-check-in embedding differences independently. Figure 6.2 demonstrates how REVAMP embeds and adaptively learns the inter-check-in dynamics between the app and POI categories to determine the next check-in location for a user. Moreover, REVAMP grants the flexibility to predict the category of the most likely smartphone app to be accessed and the POI category at the next check-in. Predicting the app preferences of a user has limitless applications, ranging from smartphone app recommendation and bandwidth modeling by cellular network providers [198, 199, 247]. To summarize, the key contributions we make via REVAMP are three-fold:"
        },
        {
            "heading": "86 Learning Spatial Behaviour using User Traces on Smartphone Apps",
            "text": "\u2022 We propose a self-attention-based approach, called REVAMP, to learn the POI preferences of a user via the coarse-grained smartphone usage logs. REVAMP returns a ranked list of candidate POIs and the most likely app and POI category for the next check-in. \u2022 We preserve the privacy needs of a user by learning a personalized sequence encoding for every user. In detail, we force our model to learn the evolving spatial preferences using the variations between each check-in in the sequence based on app category, POI category, and time of the check-in. Thus, our approach is not privy to accurate geolocations and social networks. \u2022 Exhaustive experiments over two large-scale datasets from China show that REVAMP outperforms other state-of-the-art methods for sequential POI recommendation, next app, and location-category prediction tasks. Moreover, we perform a detailed analysis of each component of REVAMP and a convergence analysis."
        },
        {
            "heading": "6.2 Related Work",
            "text": "In this section, we highlight some relevant works to our work. It mainly falls into \u2013 smartphone and mobility, sequential recommendation, and positional encodings for self-attention."
        },
        {
            "heading": "6.2.1 Modeling Smartphone and Mobility",
            "text": "Understanding the mobility dynamics of a user has wide applications ranging from locationsensitive advertisements, social community of user, and disease propagation [28, 127, 134]. Traditional mobility prediction models utilized function-based learning for spatial preferences but were highly susceptible to irregular events in the user trajectory [27, 126]. Therefore, modern approaches [54, 120, 141] utilize a neural network to model the complex user-POI relationships, geographical features, travel distances, and category distribution. These approaches consider the user trajectory as a check-in sequence and train their model parameters by capturing the influences across different sequences. Other approaches [71, 75, 240] include the continuous-time contexts for modeling the time-evolving preferences of a user. However, prior research has shown that users exhibit re-visitation patterns on their web activities [2, 97] and these re-visitation patterns resonate with the mobility preferences of a user [20, 208]. As per the permissions given by a user to an app, leading corporations, such as Foursquare, utilize smartphone activities to better understand the likes and dislikes of a user to give better POI recommendations [37]. The correlation between spatial mobility and smartphone use is essential to address the problems related to user demographics [161, 199], trajectory analysis [134], app recommendation [247], and to identify hotspots for network operators [122]. However, utilizing smartphone usage for sequential POI recommendations is not addressed in the past lit-"
        },
        {
            "heading": "Learning Spatial Behaviour using User Traces on Smartphone Apps 87",
            "text": "erature. The works most similar to our work are by Wang et al. [208] and Tu et al. [198]. Wang et al. [208] utilizes a Dirichlet process to determine the next user\u2019s location, but it completely disregards the user\u2019s privacy, i.e., requires precise geo-coordinates, and Tu et al. [198] is limited to cold-start POI recommendation rather than sequential recommendations."
        },
        {
            "heading": "6.2.2 Sequential Recommendation",
            "text": "Standard collaborative filtering (CF) and matrix factorization (MF) based recommendation approaches [86, 199] return a list of most likely items that a user will purchase in the future. However, these approaches ignore the temporal context associated with the preferences, i.e., it evolves with time. The task of a sequential recommender system is to continuously model the user-item interactions in the past purchases (or check-ins) and predict future interactions. Traditional sequence modeling approaches such as personalized Markov chains [174] combine matrix factorization with inter-item influences to determine the time-evolving user preferences. However, it has limited expressivity and cannot model complex functions. Neural models such as GRU4Rec [90] utilize a recurrent neural network (RNN) to embed the timeconditioned user preferences which led to multiple developments like GRU4Rec+ [89]. Recent research has shown that including attention [8] within the RNN architecture achieved better prediction performances than standard RNN models even in the case of POI recommendations [118, 130, 245]. However, all these approaches were outperformed by the self-attentionbased sequential recommendation models [99, 115]. In detail, the underlying model of Kang and McAuley [99] is a transformer architecture [206] that embeds user preferences using a weighted aggregation of all past user-item interactions. However, due to largely the heterogeneous nature of data in spatial datasets, e.g., POI category, geographical distance, etc., extending such models for sequential POI recommendation is a challenging task."
        },
        {
            "heading": "6.2.3 Relative Positional Encodings and Self-Attention",
            "text": "The self-attention models are oblivious to the position of events in the sequence, and thus, the original proposal to capture the order of events used fixed function-based encodings [206]. However, recent research on positional encodings [17, 185] has shown that modeling the position as a relative pairwise function between all events in a sequence, in addition to the fixedfunction encodings, achieves significant improvements over the standard method. Thus, such relative encodings have been used in a wide range of applications \u2013 primarily for determining the relative word order in natural language tasks [48, 67] and image order in computer vision problems [14, 218]. Such relative encodings have also been incorporated in item-based recommender systems [115] through time-interval based inter-event relevance and in POI recommendation [125] through geographical distance-based variances. However, the former ap-"
        },
        {
            "heading": "88 Learning Spatial Behaviour using User Traces on Smartphone Apps",
            "text": "proach cannot be extended to model the heterogeneous nature of smartphone mobility data, and the latter requires precise geographical coordinates. Moreover, including the app-category and POI-category-based relevance is challenging because these are context-dependent, i.e., two categories such as \u2018Burger Joint\u2019 and \u2018Sushi Restaurant\u2019 differ in terms of the semantic meaning of the category term. Such differences are not explicit and must be learned via natural language embeddings."
        },
        {
            "heading": "6.3 Problem Formulation",
            "text": "We consider a setting with a set of users as U and a set of locations (or POIs), P . We embed each POI using a D dimensional vector and denote the embedding matrix as L \u2208 R|P|\u00d7D. We represent the mobile trajectory of a user ui as a sequence of check-ins, euik \u2208 Eui , with each check-in comprising of the smartphone app and the POI details. For a better understanding of our model, let us consider a toy sequence with five check-ins to POIs with categories, \u2013 \u2018Bar\u2019, \u2018Cafe\u2019, \u2018Burger-Joint\u2019, \u2018Cafe\u2019, and \u2018Sushi Restaurant\u2019, while using smartphone apps categories \u2013 \u2018Social\u2019, \u2018Shopping\u2019, \u2018Game\u2019, \u2018Social\u2019, and \u2018Travel\u2019, respectively. Thus, for this example, REVAMP will use the details of the first four check-ins to predict the last check-in.\nDefinition 1 (Check-ins). We define a check-in as a timestamped activity of a user with her smartphone and location details. Specifically, we represent the k-th check-in in E as ek = {lk, tk,Ak,Sk}where lk and tk denote the POI and check-in time respectively. Here,Ak denotes the categories set of the smartphone app accessed by a user, and Sk denotes the set of POI categories.\nWith a slight abuse of notation, we denote a check-in sequence as E and the set of all appand location categories till a k-th check-in as A\u2217k = \u22c3k i=1Ak and S\u2217k = \u22c3k i=1 Sk respectively. Now, we formally define the problem of sequential POI recommendations. For our example,A will consist of \u2018Shopping\u2019, \u2018Game\u2019, \u2018Social\u2019, and \u2018Travel\u2019, while S will include \u2018Bar\u2019, \u2018BurgerJoint\u2019, \u2018Cafe\u2019, and \u2018Sushi Restaurant\u2019 respectively.\nProblem Statement (Personalized Sequential Recommendation). Using the user\u2019s past checkin records consisting of app and POI categories, we aim to get a ranked list of the most likely locations the user is expected to visit in her next check-in. Specifically, we learn the time-evolving variation in smartphone and physical mobility to estimate her future preference towards different locations in her vicinity.\nMathematically, given the first k check-ins in a sequence as Ek, we aim to identify the set of candidate POI for the next check-in, i.e., ek+1, conditioned on the app- and location-categories"
        },
        {
            "heading": "Learning Spatial Behaviour using User Traces on Smartphone Apps 89",
            "text": "of all check-ins in the history. Specifically, we aim to maximize the following probability:\nP\u2217 = arg max \u0398 {E[ek+1|Ek,A\u2217k,S\u2217k ]} (6.1)\nwhere E[ek+1] calculates the expectation of ek+1 being in the sequence of the user, Ek given the past check-ins of a user. Here, \u0398 denotes the REVAMP model parameters."
        },
        {
            "heading": "6.4 REVAMP Framework",
            "text": "In this section, we first present a high-level overview of the deep neural network architecture of REVAMP and then describe the component-wise architecture in detail."
        },
        {
            "heading": "6.4.1 High-level Overview",
            "text": "REVAMP comprises two components \u2013 (i) an embedding initiator (EI) and (ii) a sequential recommender (SR). Figure 6.3 shows the overall architecture of REVAMP with different com-"
        },
        {
            "heading": "90 Learning Spatial Behaviour using User Traces on Smartphone Apps",
            "text": "ponents, and the schematic diagrams for both of them are given in Figure 6.4. The workflow of REVAMP includes three steps: (i) determining the embeddings of all POI and app categories using the EI module; (ii) calculating the relative positional encodings in terms of app category, POI category, and time of check-in, and determining embedding matrices for each; and (iii) using the category embeddings from EI module and the newly derived relative embeddings to determine the mobility preferences of a user via the SR module.\nAs we model the differences between the category of POI and smartphone apps across checkins, we must capture the semantic meaning associated with each category, for e.g., the difference between a \u2018Sushi restaurant\u2019 and a \u2018Cafe\u2019. Accordingly, EI takes the check-in sequence of a user as input, learns the representations of all smartphone app- and POI-categories, and calculates the evolving user preferences as the variation between check-ins.\nA,S = GEI(Ek,A\u2217k,S\u2217k), (6.2)\nwhere A,S denote the learned embeddings for app and POI categories, respectively, and GEI(\u2022) denotes the Embedding Initiator. Moreover, REVAMP works by modeling the variations between different check-ins in a sequence. Specifically, it learns how the mobility preference of a user has evolved based on the difference in the current and past check-ins. Capturing and feeding these differences to our self-attention model is a challenging task as we denote each check-in via POI and app category embeddings. Therefore, we derive these differences and simultaneously embed them to be fed into a self-attention model. These variations are used to assign relative positional encodings to the check-ins in the stacked self-attention architecture in SR.\nJ ,K,T = fRE(Ek,A,S), (6.3)\nwhere J ,K,T are the relative positional encodings for app categories, POI categories, and time respectively. Here, fRE(\u2022) denotes the function to calculate these relative encodings. Note that these encodings are personalized, i.e., they are calculated independently for each user. SR then combines these relative encodings with absolute positional encodings to model the sequential POI preference of a user. Through this, we aim to get a ranked list of the most probable candidate POIs for the next check-in of a user.\nl\u0302k+1 = GSR(Ek,J ,K,T ), (6.4)\nwhere l\u0302k+1 is the candidate POI for the k + 1-th check-in of a user and GSR(\u2022) denotes the sequential recommender. Figure 6.4 shows a schematic diagram of REVAMP architecture. The training process of REVAMP is divided into two steps \u2013 train the category embeddings using EI"
        },
        {
            "heading": "Learning Spatial Behaviour using User Traces on Smartphone Apps 91",
            "text": "and then use them for sequential recommendation in the SR section. More details are given in Section 6.4.5.\nPreserving the user\u2019s privacy via REVAMP. Here, we highlight the privacy-conscious nature of our underlying framework. In detail, the existing techniques that model the relationship between the app usage and the physical mobility of a user utilize the precise geo-coordinates, the precise apps used, and the details of all background apps [20, 198, 239]. Thus, these approaches have two major drawbacks: (i) they are a serious violation of the privacy of a user; and (ii) collecting accurate data of this granularity makes the problem highly synthetic in nature. Therefore, in REVAMP, we do not incorporate any of this information that can compromise the privacy of a user. Specifically, we only use the category of the visited POI and the category of the only active app. Therefore, in our setting, it is difficult to identify individual users based on these coarse-grained records. Moreover, such cross-app data can be collected while simultaneously preserving the user\u2019s privacy [53]."
        },
        {
            "heading": "6.4.2 Embedding Initiator (EI)",
            "text": "An important ability of REVAMP is that it learns the mobility preferences conditioned only on the categories of smartphone apps and POI rather than the exact location coordinates and app preferences. Learning from such coarse data is a challenging task and training with randomlyinitialized embeddings may not capture the category semantics. For e.g., if \u2018Burger-Joints\u2019 and \u2018Asian-Restaurants\u2019 are frequently visited, then a training process with random initialization will lead to similar the trained embeddings. Therefore, our category embeddings must simultaneously capture the user preferences towards each category and the category semantics via pre-trained word embeddings. We highlight this through an example \u2013 a user checks a"
        },
        {
            "heading": "92 Learning Spatial Behaviour using User Traces on Smartphone Apps",
            "text": "smartphone app of category \u2018Social\u2019 frequently at two separate locations, say \u2018Cafe\u2019 and \u2018Sushi Restaurant\u2019, then the category embeddings should capture the POI influence that persuaded a user to use apps of a similar category(\u2018Social\u2019 in this case) as well as the semantic difference between a coffee joint and an Asian restaurant. Therefore, we use a two-channel training procedure, wherein we use pre-trained embeddings to extract the semantic meaning of all app and location categories and learn user preferences towards these categories via a lightweight matrix factorization. Specifically, given a check-in sequence ek \u2208 E we follow a four-layer architecture:\n(1) Input Layer. We initially embed the app and location categories, A\u2217 and S\u2217, as A \u2208 R|A\u2217|\u00d7D and S \u2208 R|S\u2217|\u00d7D respectively. Each row ai \u2208 A represents a D-dimension representation of a smartphone app category. Similarly, si \u2208 S is a representation for a POI category. (2) MF Layer. To learn the interaction between the app and POI categories, we follow a lightweight collaborative filtering approach, wherein we concatenate the entries inA and S that appear together in a check-in ek \u2208 E . Specifically, we concatenate the app and POI category embeddings for a check-in and then use a feed-forward network.\nv\u0302ai,sj = ReLU (wv(ai||sj) + bv) , (6.5)\nwhere v\u0302ai,sj denotes the probability of an app of category ai to be accessed at a POI of category sj , || denotes the concatenation operator, and w\u2022, b\u2022 are trainable parameters. We train our embeddings via a cross-entropy loss:\nLMF = \u2212 |E|\u2211 k=1 \u2211 ai\u2208Ak sj\u2208Sk [ log ( \u03c3(v\u0302ai,sj) ) + log ( 1\u2212 \u03c3(v\u0302ai,s\u2032j) ) + log ( 1\u2212 \u03c3(v\u0302a\u2032i,sj) )] , (6.6)\nwhere v\u0302\u2022,\u2022 denotes the estimated access probability (i) v\u0302ai,sj between a true app- and location-category, i.e., ai \u2208 Ak, sj \u2208 Sk, (ii) v\u0302a\u2032i,sj for a negatively sampled app-category with a true location-category, i.e., ai /\u2208 Ak, sj \u2208 Sk, and (iii) v\u0302ai,s\u2032i for a negatively sampled location-category with a true app-category, i.e., ai \u2208 Ak, sj /\u2208 Sk. (3) BERT Layer. To capture the real-world semantics of a category, we use a pre-trained BERT [42] model with over 110M parameters. Specifically, we extract the embeddings for each smartphone app and POI category from the pre-trained model. Later, we maximize the similarity between these embeddings and our category representations, A and"
        },
        {
            "heading": "Learning Spatial Behaviour using User Traces on Smartphone Apps 93",
            "text": "S, by optimizing a mean squared loss.\nLBert = 1\n|E| |E|\u2211 k=1 \u2211 ai\u2208Ak sj\u2208Sk [ ||ai \u2212 \u03a61(ai)||2 + ||si \u2212 \u03a62(si)||2 ] , (6.7)\nwhere, ai \u2208 A and sj \u2208 S are our trainable embedding for categories ai and sj respectively, and \u03a6\u2022 denotes a two-step function that extracts pre-trained embeddings for all categories and uses a feed-forward network to normalize the embedding dimension to D. Specifically,\n\u03a61(ai) = ReLU ( w1 \u00b7 B(ai) + b1 ) , (6.8)\n\u03a62(si) = ReLU ( w2 \u00b7 B(si) + b2 ) , (6.9)\nwhere B denotes the set of all pre-trained embeddings, B(ai) and B(si) denote the extracted app and location category embedding, and w\u2022, b\u2022 are trainable parameters. (4) Optimization. We train our embeddings using a two-channel learning procedure consisting of app-location interaction loss, LMF, and pre-trained embedding loss, LBert, by optimizing a weighted joint loss.\nLEI = \u03b3LMF + (1\u2212 \u03b3)LBert, (6.10)\nwhere \u03b3 denotes a scaling parameter. Later, we useA and S to identify the inter-check-in differences and model the POI preferences of a user."
        },
        {
            "heading": "6.4.3 Relative or Inter-check-in Variations",
            "text": "Buoyed by the efficacy of relative encodings for self-attention models [17, 185], REVAMP captures the evolving preferences of a user as relative encodings based on three inter-check-in differences: (i) Smartphone App-based dynamics, (ii) Location category distribution, and (iii) Time-based evolution across the event sequence.\nSmartphone App-based Variation. Recent research [17, 185] has shown that users\u2019 preferences towards smartphone apps are influenced by their geo-locations and other POI-based semantics. Seemingly, it is more likely for a user to be active on a multiplayer game at a social joint rather than at her workplace. We quantify the differences in the app preferences of a user via the differences in the embeddings of the smartphone-app category being used at a check-in. However, in our datasets, every smartphone app is associated with at least one category, i.e., an app can belong to multiple categories, for e.g., Amazon belongs to only one category of \u2018Retail\u2019, but PUBG (a popular mobile game) may belong to categories \u2018Game\u2019 and \u2018Action Game\u2019."
        },
        {
            "heading": "94 Learning Spatial Behaviour using User Traces on Smartphone Apps",
            "text": "Calculating the variation based on different embedding is a challenging task. Therefore, we first calculate a \u201cnet app-category\u201d embedding to denote the representation of all categories an app belongs to. Specifically, for each check-in ek, we calculate the net app-category as a mean of all category embeddings.\n\u00b5ak = 1 |Ak| \u2211 ai\u2208Ak ai, (6.11)\nwhere \u00b5ak, ai \u2208 Ak,ai \u2208 A represent the net app-category embedding for a check-in ek, the app-category used in the check-in and the corresponding embedding learned in the EI (see Section 6.4.2). Such an embedding allows us to simplify the input given to the self-attention mechanism in our recommender system. Following [185], we use these embeddings to calculate a inter-check-in variance matrix J \u2208W|E|\u00d7|E| for each check-in sequence. Specifically, the i-th row in matrix J denotes the difference between the mean app-category embedding of check-in ei with all other check-ins in the sequence and is calculated as:\nJ i,j =\n\u230a fcos(\u00b5 a i ,\u00b5 a j )\u2212minf (E) maxf (E)\u2212minf (E) \u00b7 Ia \u230b , (6.12)\nwhere fcos(\u2022, \u2022),minf (E),maxf (E) denote the function for normalized cosine-distance, the minimum and maximum cosine distance between the mean category embedding for any two check-ins in a sequence. We use Ia as a clipping constant and a floor operator to discretize the entries in J . Such discretization makes it convenient to extract positional encodings for the self-attention model in SR.\nPOI-based Variation. We derive the inter-check-in differences between POI categories using a similar procedure for app-based differences. However, POI can belong to multiple categories. Therefore, similar to our procedure for calculating app-based variations, we calculate a net POI category embedding, \u00b5lk for each check-in as \u00b5 l k = 1 |Sk| \u2211\nsi\u2208Sk si. Later, as in Eqn 6.12, we calculate the POI-based inter-check-in variance matrix K \u2208W|E|\u00d7|E| using a clipping constant Il. Here, the i-th row in matrix K denotes the difference between the mean POI-category embedding of check-in ei with all other check-ins in the sequence.\nTime-based Variation. Ostensibly, there may be irregularities in the smartphone app usage of a user. For e.g., a user browsing \u2018Amazon\u2019 may receive a message \u2018Twitter\u2019 that she immediately checks and then later continues her shopping on Amazon. Notably, the \u2018Amazon\u2019 app did not influence the user to access \u2018Twitter\u2019 and vice-versa, as such a change between apps was coincidental. To model these nuances in REVAMP we use the time interval between accessing different smartphone apps. Specifically, similar to app- and POI-category based inter-check-in differences, we derive a time-based variations matrix, T \u2208 W|E|\u00d7|E|, using the absolute time-"
        },
        {
            "heading": "Learning Spatial Behaviour using User Traces on Smartphone Apps 95",
            "text": "difference between each check-in.\nT i,j = \u230a |ti \u2212 tj| tmin \u00b7 It \u230b , (6.13)\nwhere ti, tj, tmin, and It denote the time of check-in ei and ej , minimum time-interval between check-ins of a user and the normalizing constant for time respectively."
        },
        {
            "heading": "6.4.4 Sequential Recommender (SR)",
            "text": "In this section, we elaborate on the sequential recommendation procedure of REVAMP that is responsible for modeling the app and POI preferences of a user and then recommending a candidate POI for the next check-in. Specifically, it uses a self-attention architecture consisting of five layers:\n(1) Input Layer. The SR model takes the check-in sequence of a user (E), relative app, POI, and time encodings, (K,J , and T respectively), and the mean app and location category representations (\u00b5a\u2022,\u00b5 l \u2022) as input to the self-attention model. Since the self-\nattention models require a fixed input sequence, we limit our training to a fixed number of check-ins, i.e., we consider the N most recent check-ins in E for training our model, and if the number of check-ins is lesser than N , we repeatedly add a [pad] vector for the initial check-ins within the sequence. (2) Embedding Retrieval Layer. Since the self-attention models are oblivious of the position of each check-in in the sequence, we use a trainable positional embedding for each check-in [99, 115]. Specifically, we initialize two distinct vectors denoted by P key \u2208 RN\u00d7D and P val \u2208 RN\u00d7D where the i-th rows, pkeyi and pvali , denote the positional encoding for the check-in ei in the sequence. Similarly, we embed the relative positional matricesK, J , and T into encoding matrices Jkey,Jval \u2208 RN\u00d7N\u00d7D,Kkey,Kval \u2208 RN\u00d7N\u00d7D, and T key,T val \u2208 RN\u00d7N\u00d7D respectively.\nKkey =  kkey1,1 \u00b7 \u00b7 \u00b7 kkey1,N ... ...\n... kkeyN,1 \u00b7 \u00b7 \u00b7 kkeyN,N\n , Kval =  kval1,1 \u00b7 \u00b7 \u00b7 kval1,N ... ...\n... kvalN,1 \u00b7 \u00b7 \u00b7 kvalN,N  , (6.14) We use two separate matrices to avoid any further linear transformations [185]. Each entry in Kkey and Kval denotes a D dimensional vector representation of corresponding value in inK. We follow a similar procedure to initialize Jkey,Jval,T key and T val for J and T respectively. (3) Self-Attention Layer. Given the check-in sequence of a user, the self-attention archi-"
        },
        {
            "heading": "96 Learning Spatial Behaviour using User Traces on Smartphone Apps",
            "text": "tecture learns the sequential preference of a user towards POIs. Specifically, for an input sequence consisting of POI embeddings of locations visited by a user, LE = (le1 , le2 , \u00b7 \u00b7 \u00b7 leN ) where lei \u2208 ei and lei \u2208 L are the location visited in check-in ei the POI embedding for lei respectively, we compute a new sequence Z = (z1, z2, \u00b7 \u00b7 \u00b7 zN), where z\u2022 \u2208 RD. Each output embedding is calculated as a weighted aggregation of embeddings of all the POIs visited in the past.\nzi = N\u2211 j=1 \u03b1i,j ( wv,jlej + \u00b5j + p val j + j val i,j + k val i,j + t val i,j ) , (6.15)\nwhere lej is the POI embedding, \u00b5j = \u00b5 a j + \u00b5 l j is the sum of the smartphone app and POI category mean embeddings, andwv,j is a trainable parameter. The attention weights \u03b1\u2022,\u2022 are calculated using a soft-max over other input embeddings as:\n\u03b1i,j = exp ( xi,j )\u2211N\nk=1 exp ( xi,k ) , (6.16)\nwhere xi,j denotes the compatibility between two check-ins\u2013 ei and ej \u2013 and is computed using both \u2013 relative- as well as absolute-positional encodings.\nxi,j = wq,ilei\n( wk,jlej + p key j + j key i,j + k key i,j + t key i,j )> \u221a D , (6.17)\nwhere wq,\u2022,wk,\u2022 and D denote the input query projection, key projection, and the embedding dimension respectively. We use the denominator as a scaling factor to control the dot-product gradients. As our task is to recommend a candidate POI for future checkins and should only consider the first k check-ins to predict the (k + 1)-th check-in, we introduce a causality over the input sequence. Specifically, we modify the procedure to attention in Eqn.(6.17) and remove all links between the future check-ins and the current check-in. (4) Point-wise Layer. As the self-attention lacks any non-linearity, we apply a feed-forward layer with two linear transformations with ReLU activation.\nPFFN(zk) = ReLU (zkwp,1 + b1)wp,2 + b2, (6.18)\nwhere wp,\u2022, b\u2022 are trainable layer parameters. The combination of a self-attention layer and the point-wise layer is referred to as a self-attention block and stacking self-attention blocks gives the model more flexibility to learn complicated dynamics [206]. Thus, we stack Mb such blocks, and to stabilize the learning process, we add a residual connection"
        },
        {
            "heading": "Learning Spatial Behaviour using User Traces on Smartphone Apps 97",
            "text": "between each such block.\nz (r) k = z (r\u22121) k + PFFN ( fln(z (r\u22121) k ) ) , (6.19)\nwhere 1 \u2264 r \u2264 Mb, fln(\u2022) denote the level of the current self-attention block and layernormalization function respectively. The latter is used to further accelerate the training of self-attention and is defined as follows:\nfln(zk) = \u03b2 zk \u2212 \u00b5z\u221a \u03c32z + + \u03b3, (6.20)\nwhere , \u00b5z, \u03c3z, \u03b2, \u03b3, denote the element-wise product, mean of all the input embeddings, the variance of all input embeddings, learned scaling factor, bias term, and the Laplace smoothing constant respectively. (5) Prediction Layer. A crucial distinction between REVAMP and the standard self-attention model is that REVAMP not only predicts the candidate POIs for the next check-in, but also the category of the smartphone app and the POI category to be used in the next check-in. Here, we describe the prediction procedure for each of them. POI Recommendation: We predict the next POI to be visited by a user in the checkin sequence using a matrix-factorization [86] based approach between the transformer output Z(Mb) = (z1, z2, \u00b7 \u00b7 \u00b7 zk) and the embeddings of the POIs visited by the user, (le2 , le3 , \u00b7 \u00b7 \u00b7 lek+1)\nv\u0302ui,lek = zk\u22121l > ek , (6.21)\nwhere v\u0302ui,lek is the calculated probability of user, ui, to visit the POI, lek , for her next check-in. We learn the model parameters by minimizing the following cross-entropy loss.\nLRec = \u2212 \u2211 ui\u2208U N\u2211 k=1 [ log ( \u03c3(v\u0302ui,lek ) + log ( 1\u2212 \u03c3(v\u0302ui,l\u2032ek ) )] + \u03bb||\u0398||2F , (6.22)\nwhere v\u0302ui,l\u2032ek denotes the check-in probability for a negatively sampled POI, i.e., a randomly sampled location that will not be visited by a user. \u03bb, \u03c3,\u0398 denote regularization parameter, sigmoid function, and the trainable parameters respectively. Predicting App Categories: Predicting the next smartphone app to be accessed by a user has numerous applications ranging from smartphone system optimization, resource management in mobile operating systems, and battery optimization [161, 247]. Therefore, to predict the category of the next app to be used, we follow a matrix-factorization approach to calculate the relationship between the user preference embedding, zk, and the mean of"
        },
        {
            "heading": "98 Learning Spatial Behaviour using User Traces on Smartphone Apps",
            "text": "smartphone app embeddings for the next check-in.\nq\u0302ui,Ak = zk\u22121\u00b5 a k >, (6.23)\nwhere q\u0302ui,Ak ,\u00b5 a k denote the usage probability of apps of categories in Ak and the mean embedding for all apps used in check-in ek. Later, we minimize a cross-entropy loss with negatively sampled apps, i.e., apps that were not used by the user, denoted as LApp. Predicting Location Categories: As in app-category prediction, we calculate the preference towards a POI-category using the mean of POI category embedding \u00b5lk and learn the parameters by optimizing a similar cross-entropy loss denoted as LPOI. The net loss for sequential recommendation is a weighted combination of POI recommendation loss, app-category loss, and location-category loss.\nLSR = LRec + \u03ba(LApp + LPOI), (6.24)\nHere, \u03ba is a tunable hyper-parameter for determining the contribution of category prediction losses. All the parameters of REVAMP, including the weight matrices, relativeposition weights, and embeddings are learned using an Adam optimizer[100]."
        },
        {
            "heading": "6.4.5 REVAMP: Training",
            "text": "As mentioned in Section 6.4.1, REVAMP involves a two-step training procedure. Specifically, it consists of the following steps: (i) learning the app and POI category embeddings using the embedding initiator(EI) and (ii) training the self-attention model in SR to recommend candidate POI to the user. In detail, we first train the parameters of EI by minimizing the LEI loss for multiple epochs and later use the trained category embeddings in SR and recommend candidate POI by minimizing the recommendation loss, LSR.\nWe highlight that a joint training of both EI and SR is not suitable in the presence of relative positional encodings, as they are conditioned on the category embeddings learned in EI. Therefore, during joint training, an update in the category embedding will make the trained parameters of SR across the previous epochs unsuitable for prediction in the future. Moreover, these encodings are calculated relatively, i.e., conditioned on the embedding of other categories in a sequence, and thus any change in the category embedding will affect the category embeddings."
        },
        {
            "heading": "6.5 Experiments",
            "text": "In this section, we report a comprehensive empirical evaluation of REVAMP and compare it with other state-of-the-art approaches. We evaluate the POI recommendation performance of"
        },
        {
            "heading": "Learning Spatial Behaviour using User Traces on Smartphone Apps 99",
            "text": "REVAMP using two real-world datasets from China. These datasets vary significantly in terms of data sparsity, the no. of app categories, and POI categories. With our experiments, we aim to answer the following research questions:\nRQ1 How does REVAMP compare to cutting-edge models for sequential POI recommendation? What are the gains and losses? RQ2 What is the contribution of relative positional encodings? RQ3 What is the scalability of REVAMP and the stability of the learning procedure?\nAll our algorithms are implemented in Tensorflow on a server with Ubuntu 16.04. CPU: Intel (R) Xeon (R) Gold 5118 processor at 2.30GHz, RAM: 125GB, GPU: NVIDIA V100 32GB."
        },
        {
            "heading": "6.5.1 Experimental Setup",
            "text": "Dataset Description. As our goal is to recommend POIs to a user based on her smartphone usage, the mobility datasets used in our experiments must contain the user trajectory data, i.e., geographical coordinates, time of a check-in, as well as the smartphone-usage statistics \u2013 applications used across different locations, the categories of different apps based on online app-stores, etc. Therefore we consider two popular large-scale datasets \u2013 Shanghai-Telecom and TalkingData and their statistics are given in Table 6.2. Moreover, we highlight the high variance between the category semantics of both datasets by plotting the location category word clouds in Figure 6.5. The variance across the datasets is due to the different sources used for extracting location categories \u2013 the at-hand location categories for Shanghai-Telecom and Foursquare-based categories for TalkingData.\n(1) Shanghai-Telecom: This smartphone usage and the physical-mobility dataset was collected by a major network operator in China [239]. The trajectories were collected from Shanghai in April 2016. It contains the details of a user\u2019s physical mobility and time-andgeo-stamped smartphone app usage records. More specifically, for each user, we have the timestamped records of the smartphone apps being used and the different cellularnetwork base stations to which the smartphone was connected during the data collection procedure. For the region covered by each cellular network base station, we also have the details of the internal POIs and their corresponding categories. For our experiments, we consider each user\u2192base-station entry as a check-in and all the apps and their categories"
        },
        {
            "heading": "100 Learning Spatial Behaviour using User Traces on Smartphone Apps",
            "text": "associated with that check-in as the events in the sequence E . We adopt a commonly followed data cleaning procedure [126, 141] and filter out users and POI with less the five check-ins. (2) TalkingData: A large-scale public app-usage dataset that was released by TalkingData3, a leading data intelligence solution provider based in China. The original dataset released by the company [98] consists of location- and timestamped records of smartphone app usage and physical trajectories of a user. However, in this dataset, we lack the categories associated with each POIs. We overcome this by extracting location categories and geocoordinates from publicly available check-in records [228] for users in Foursquare \u2013 a leading social mobility network, and mapping each check-in location in Foursquare to a location in the TalkingData based on geographical coordinates. For our experiments using this dataset, we restrict our check-in records to only the locations situated in mainland China. As in the Shanghai-Telecom dataset, here as well we filter out the users and POI with lesser than five check-ins.\nEvaluation Metric. We evaluate REVAMP and the other sequential recommendation baselines using a widely used leave-one-out evaluation, i.e., next check-in prediction task. Specifically, for each user, we consider the last check-in of the trajectory sequence as the test check-in, the second last check-in for validation, and all preceding events as the training set [99, 115]. For a fair evaluation, we also follow a common testing strategy wherein we pair each ground truth check-in in the test set with 100 randomly sampled negative events, i.e., the check-ins not associated to the user [86, 99]. Therefore, the task becomes to rank the negative check-ins with the ground truth check-in. To evaluate the effectiveness of all approaches, we use Hits@k and NDCG@k, with k \u2208 {1, 5, 10}, and report the confidence intervals based on five independent runs.\n3www.talkingdata.com/ (Accessed October 2022)"
        },
        {
            "heading": "Learning Spatial Behaviour using User Traces on Smartphone Apps 101",
            "text": "Parameter Settings. For all results in Section 6.5.2 and 6.5.4, we set N = 200 and N = 100 for Shanghai-Telecom and TalkingData respectively. We set Ia = Il = It = 64, D = 64, and \u03bb = 0.002, We search the batch-size in {128, 256}, the no of attention-heads in {1, 2, 4, 8}, \u03ba, \u03b3 are searched in {0.2, 0.5, 0.8}, and the dropout probability is set to 0.2.\nBaselines. We compare REVAMP with the state-of-the-art methods based on their architectures below:\n(1) Standard Recommendation Systems. FPMC [174] FPMC utilizes a combination of factorized first-order Markov chains and matrix factorization for recommendation and encapsulates a user\u2019s evolving long-term preferences as well as short-term purchase-to-purchase transitions. TransRec [83] A first-order sequential recommendation model that captures the evolving item-to-item preferences of a user through a translation vector. (2) POI Recommendation Systems. STGN [245] Uses a modified LSTM network that captures the spatial and temporal dynamic user preferences between successive check-ins using spatio-temporal gates. Hence, it requires the exact location coordinates as input to the model. (3) Smartphone App-based. AUM [208] Models the user\u2019s mobility as well as app-usage dynamics using a Dirichlet process to predict the next successive check-in locations. (4) Recurrent and Convolutional Neural Network. GRU4Rec+ [89] A RNN-based approach that models the user action sequences for a sessionbased recommendation. It is an improved version of GRU4Rec [90] with changes in the loss function and the sampling techniques. Caser [196] A state-of-the-art CNN-based sequential recommendation method that applies convolution operations on the N -most recent item embeddings to capture the higherorder Markov chains. (5) Self-Attention. Bert4Rec [194] A bi-directional self-attention [42] based sequential recommendation model that learns user preferences using a Cloze-task loss function, i.e., predicts the artificially masks events form a sequence. SASRec [99] A self-attention [206] based sequential recommendation method that attentively captures the contribution of each product towards a user\u2019s preference embedding. TiSRec [115] A recently proposed enhancement to SASRec that uses relative-position embeddings using the difference in the time of consecutive purchases made by the user.\nWe omit comparisons across other approaches for sequential recommendations, such as GRU4Rec [90]"
        },
        {
            "heading": "102 Learning Spatial Behaviour using User Traces on Smartphone Apps",
            "text": "and MARank [135], as they have already been outperformed by the current baselines. We calculate the confidence intervals based on the results obtained after three independent runs."
        },
        {
            "heading": "6.5.2 Performance Comparison",
            "text": "In this section, we address RQ1 and report the location recommendation performance of different methods across both mobility datasets. The results for Shanghai-Telecom and TalkingData datasets are given in Table 6.3 and Table 6.4, respectively. From these results, we make the following observations.\n\u2022 REVAMP consistently outperforms all other baselines for sequential mobility prediction across\nboth datasets. The superior performance signifies the importance of including the smartphone usage pattern of a user to determine her mobility preferences. We also note that the performance gains over other self-attention-based models \u2013 Bert4Rec [194], SASRec [99], and TiSRec [115] further reinforce our claim that including relative positional encodings based on the smartphone, spatial and temporal characteristics can better learn the mobility preferences. \u2022 We also note that the self-attention-based models, such as Bert4Rec, SASRec, TiSRec, and\nREVAMP consistently yield the best performance on all the datasets and easily outperform CNN and RNN-based models, namely Caser [196] and GRU4Rec+ [89]. This further signifies the unequaled proficiency of the transformer [206] architecture to capture the evolution of user preferences across her trajectory sequence. More importantly, it outperforms the stateof-the-art location recommendation model STGN [245] that uses the additional information of precise geographical coordinates of each POI location. \u2022 REVAMP also outperforms the other smartphone-activity-based approach, AUM [208] by up\nto 34% across different metrics. \u2022 We also note that neural baselines such as Caser [196], GRU4Rec+ [89] achieve better results\nas compared to FPMC [174] and TransRec [83]. It asserts the utmost importance of designing modern recommender systems using neural architectures. Moreover, GRU4Rec+ achieves a similar performance compared to Caser.\nTo sum up, our empirical analysis suggests the following: (i) the state-of-the-art models, including self-attention and standard neural models, are not suitable for modeling mobile-user trajectories, and (ii) REVAMP achieves better recommendation performance as it captures the mobility dynamics as well as the smartphone-activity of a user."
        },
        {
            "heading": "104 Learning Spatial Behaviour using User Traces on Smartphone Apps",
            "text": ""
        },
        {
            "heading": "6.5.3 Ablation Study",
            "text": "To address RQ2, we perform an ablation study to estimate the efficacy of different components in the REVAMP architecture. More specifically, we aim to calculate the contribution of (i) the embedding initiator and (ii) relative positional embeddings.\nAnalysis of Embedding Initiator. We reiterate that EI, defined in Section 6.4.2, is used to learn the semantic meaning of each app- and POI category as well as the influence between these embeddings in a mobility sequence. We accomplish this via a joint loss that consists of \u2013 minimizing the divergence between the category vector and the pre-trained BERT[42] vectors and a collaborative-filtering (CF) loss. These trained embeddings are later used to learn the inter-check-in differences through relative positional encodings. To emphasize its importance, we compare the prediction performances of REVAMP with different procedures to learn category embeddings and thus the relative embeddings. Specifically, we consider: (i) word-movers-distance(WMD) [109] between the word2vec [150] representations of each category, (ii) WMD on Glove [168] based representations, (iii) WMD based on BERT [42] initialized vectors, (iv) simple collaborative filtering based parameter training, (v) using pretrained BERT, and (vi) the proposed EI model. From the results in Figure 6.6, we note that our proposed EI achieves the best prediction performance compared to other approaches. We also note that standard pre-trained BERT vectors outperform other WMD-based approaches.\nRelative Positional Encodings. Relative positional embeddings are a crucial element in our model. We calculate the performance gains due to the different relative encodings \u2013 app-, time- and location-based by estimating the recommendation performance of the following approaches: SASRec [99]; (i) TiSRec [115]; (ii) REVAMP with time-based relative positional encoding called REVAMP-t; (iii) REVAMP with app-based encodings, denoted as REVAMP-a; (iv) REVAMP with location-based encodings, denoted as REVAMP-l; and (v) the complete REVAMP model with all relative encodings.\nFigure 6.7 summarizes our results where we observe that including relative positional encodings of any form, whether app-based or location-based, leads to better prediction performances. Interestingly, the contribution of location-based relative positional embeddings is more significant than the app-based and could be attributed to larger variations in location-category than the app-category across an event sequence. For example, the difference between location categories of a university region and an office space will capture larger dynamics than the differences in smartphone app usage across these two regions. However, jointly learning all positional encoding leads to the best performance over both datasets. The improvements of REVAMP-t over TiSRec [115] could be due to the inclusion of absolute event encodings (both app and location)."
        },
        {
            "heading": "6.5.4 App and Location Prediction Category",
            "text": "Our goal via REVAMP is to understand the smartphone activity of a user and correlate it with her mobile trajectories. Therefore, we perform an additional experiment to evaluate how effectively is REVAMP able to predict the app- and the location category for the next user check-in. We also introduce an additional state-of-the-art smartphone-activity modeling baseline, Appusage2Vec [247] which considers the category of the app and the time spent on the app by the user to learn an app-preference embedding for a user. We also compare with the state-of-the-art transformer-based models \u2013 SASRec [99] and TiSRec [115]. For an even comparison, we rank the models using the root-mean-squared (RMS) distance between the final user preference embedding obtained after learning on N consecutive events of a user and the mean of location and category embeddings of the N + 1 event in the sequence. Accordingly, we also modify the architectures of SASRec and TiSRec to predict user affinity across the location and app category affinities. From the results in Figure 6.8, we make the following observations: (i) REVAMP easily outperforms all other baselines for both apps and location category prediction. This illustrates the better user-preference modeling power of REVAMP over other approaches, (ii) For"
        },
        {
            "heading": "106 Learning Spatial Behaviour using User Traces on Smartphone Apps",
            "text": "app-category prediction, Appusage2Vec also outperforms both SASRec and TiSRec; however, REVAMP easily outperforms Appusage2Vec across both the datasets."
        },
        {
            "heading": "6.5.5 Scalability of REVAMP",
            "text": "To address RQ3, i.e., determine the scalability of REVAMP with different positional encodings \u2013 absolute and relative, we present the epoch-wise time taken for training REVAMP in Table 6.5. Note that these running times exclude the time for pre-processing, where we calculate the inter-event app and location category-based differences. We note that the runtime of REVAMP is linear with the number of users, and secondly, even for a large-scale dataset, like TalkingData, we can optimize all parameters in REVAMP well within 170 minutes. These run times are well within the range for designing recommender systems.\nConvergence of REVAMP Training. As we propose the first-ever application of the selfattention model for smartphones and human mobility, we also perform a convergence analysis during training REVAMP. To emphasize the stability of REVAMP training procedure, we plot the epoch-wise best prediction performance of REVAMP across both datasets in Fig 6.9. From the results, we note that despite the multi-variate nature of the data and the disparate positional encodings, REVAMP converges only in a few training iterations. It is also important to note that the REVAMP significantly outperforms other RNN-based baselines even with limited training of 40 iterations."
        },
        {
            "heading": "Learning Spatial Behaviour using User Traces on Smartphone Apps 107",
            "text": ""
        },
        {
            "heading": "6.6 Conclusion",
            "text": "In this chapter, we highlighted the drawbacks of modern POI recommender systems that ignore the smartphone usage characteristics of users. We also proposed a novel sequential POI recommendation model, called REVAMP, that incorporates the smartphone usage details of a user while simultaneously maintaining user privacy. Inspired by the success of relative positional encodings and self-attention models, REVAMP uses relative as well as absolute positional encodings determined by the inter-check-in variances in the smartphone app category, POI category, and time over the check-ins in the sequence. Our experiments over two diverse datasets from China show that REVAMP significantly outperforms other state-of-the-art baselines for POI recommendation. Moreover, we also show the contribution of each component in the REVAMP architecture and analyze the learning stability of the model.\n108 Learning Spatial Behaviour using User Traces on Smartphone Apps\nPart III"
        },
        {
            "heading": "Applications of Modeling Temporal",
            "text": ""
        },
        {
            "heading": "Sequences",
            "text": "109\nChapter 7"
        },
        {
            "heading": "Large-Scale Retrieval of Temporal",
            "text": ""
        },
        {
            "heading": "Sequences",
            "text": ""
        },
        {
            "heading": "7.1 Introduction",
            "text": "Recent developments in predictive modeling using marked temporal point processes (MTPP) have enabled an accurate characterization of several real-world applications involving continuoustime event sequences (CTESs). However, the retrieval problem of such sequences remains largely unaddressed in literature. In this chapter, we address the problem of retrieving temporal event sequences from a large corpus using neural MTPP models. This is a first-of-its-kind application of MTPP as the earlier developments in MTPP models focused on improving the predictive analytics in several real-world applications\u2014 from information diffusion in social networks to healthcare\u2014 by characterizing them with continuous-time event sequences (CTESs) [36, 39, 45, 46, 51, 68, 70, 95, 108, 177, 203, 210]. Therefore, in this context, given a query sequence, retrieval of relevant CTESs from a corpus of sequences is a challenging problem having a wide variety of search-based applications. For example, in audio or music retrieval, one may like to search sequences having different audio or music signatures; the retrieval of ECG sequences relevant to one pathological query ECG sequence can help in the early detection of cardiac disease; in social networks, retrieval of trajectories of information diffusion, relevant to a given trajectory can assist in viral marketing, fake news detection, etc. Despite having a rich literature on searching similar time-series [4, 18, 19, 35, 62, 166, 190, 237], the problem of designing retrieval models specifically for CTES has largely been unaddressed in the past. Moreover, as shown in our experiments, the existing search methods for time sequences are largely ineffective for a CTES retrieval task since the underlying characterization of the sequences varies across these two domains.\n111"
        },
        {
            "heading": "112 Large-Scale Retrieval of Temporal Sequences",
            "text": ""
        },
        {
            "heading": "7.1.1 Our Contribution",
            "text": "In this chapter, we first introduce NEUROSEQRET, a family of supervised retrieval models for continuous-time event sequences, and then develop a trainable locality-sensitive hashing (LSH) based method for efficient retrieval over very large datasets [77]. Specifically, our contributions are as follows:\nQuery unwarping. The notion of relevance between two sequences varies across applications. A relevant sequence pair can share very different individual attributes, which can mislead the retrieval model if the sequences are compared as-it-is. In other words, an observed sequence may be a warped transformation of a hidden sequence [59, 226]. To tackle this problem, NEUROSEQRET first applies a trainable unwarping function on the query sequence before the computation of a relevance score. Such an unwarping function is a monotone transformation, which ensures that the chronological order of events across the observed and the unwarped sequences remains the same [226].\nNeural relevance scoring model. In principle, the relevance score between two sequences depends on their latent similarity. We measure such similarity by comparing the generative distribution between the query-corpus sequence pairs. In detail, we feed the unwarped query sequence and the corpus sequence into a neural MTPP-based relevance scoring model, which computes the relevance score using a Fisher kernel [93] between the corpus and the unwarped query sequences. Such a kernel offers two key benefits over other distribution similarity measures, e.g., KL divergence or Wasserstein distance: (i) it computes a natural similarity score between query-corpus sequence pairs in terms of the underlying generative distributions; and, (ii) it computes a dot product between the gradients of log-likelihoods of the sequence pairs, which makes it compatible with locality-sensitive hashing for certain design choices and facilitates efficient retrieval. In this context, we provide two MTPP models, leading to two variants of NEUROSEQRET, which allows a nice tradeoff between accuracy and efficiency.\nSELFATTN-NEUROSEQRET: Here, we use transformer Hawkes process [251] which computes the likelihood of corpus sequences independently of the query sequence. Such a design admits precomputable corpus likelihoods, which in turn allows for prior indexing of the corpus sequences before observing the unseen queries. This setup enables us to apply LSH for efficient retrieval.\nCROSSATTN-NEUROSEQRET: Here, we propose a novel cross attention-based neural MTPP model to compute the sequence likelihoods. Such a cross-attention mechanism renders the likelihood of corpus sequence dependent on the query sequence, making it a more powerful retrieval model. While CROSSATTN-NEUROSEQRET is not directly compatible with such a"
        },
        {
            "heading": "Large-Scale Retrieval of Temporal Sequences 113",
            "text": "hashing-based retrieval, it can be employed in a telescopic manner\u2014 where a smaller set of relevant candidates are first retrieved using LSH applied on top of SELFATTN-NEUROSEQRET, and then reranked using CROSSATTN-NEUROSEQRET. Therefore, from the design perspective, these two models provide a tradeoff between accuracy and efficiency.\nHaving computed the relevance scores, we learn the unwarping function and the MTPP model by minimizing a pairwise ranking loss based on the ground truth relevance labels.\nScalable retrieval. Next, we use the predictions made by SELFATTN-NEUROSEQRET to develop a novel hashing method that enables efficient sequence retrieval. More specifically, we propose an optimization framework that compresses the learned sequence embeddings into binary hash vectors, while simultaneously limiting the loss due to compression. Then, we use locality-sensitive hashing [60] to bucketize the sequences into hash buckets so that sequences with similar hash representations share the same bucket. Finally, given a query sequence, we consider computing relevance scores only with the sequences within its bucket. Such a hashing mechanism combined with high-quality sequence embeddings achieves fast sequence retrieval with no significant loss in performance. Finally, our experiments with real-world datasets from different domains show that both variants of NEUROSEQRET outperform several baselines including the methods for continuous-time series retrieval. Moreover, we observe that our hashing method applied on SELFATTN-NEUROSEQRET can make a tradeoff between the retrieval accuracy and efficiency more effectively than baselines based on random hyperplanes as well as exhaustive enumeration."
        },
        {
            "heading": "7.2 Preliminaries",
            "text": ""
        },
        {
            "heading": "7.2.1 Notations and MTPP",
            "text": "An event e in an MTPP is realized using a tuple (t, x), where t \u2208 R+ and x \u2208 C are the arrival time and the mark of the event e. Then, we use H(t) to denote a continuous time event sequence (CTES) where each event has arrived until and excluding time t, i.e., H(t) := {ei = (ti, xi) | ti\u22121 < ti < t}. Moreover we use T (t) and M(t) to denote the sequence of arrival times {ti | ei \u2208 H(t)} and the marks {xi | ei \u2208 H(t)}. Finally, we denote the counting process N(t) as counts of the number of events that happened until and excluding time t, encapsulating the generative mechanism of the arrival times.\nGenerative model for CTES. The underlying MTPP model consists of two components \u2013 (i) the dynamics of the arrival times and (ii) the dynamics of the distribution of marks. Most existing works [45, 147, 148, 189, 243, 251] model the first component using an intensity"
        },
        {
            "heading": "114 Large-Scale Retrieval of Temporal Sequences",
            "text": "function which explicitly models the likelihood of an event in the infinitesimal time window [t, t + dt), i.e., \u03bb(t) = Pr(dN(t) = 1|H(t)). In contrast, we use an intensity-free approach following the proposal by Shchur et al. [186], where we explicitly model the distribution of the arrival time t of the next event e. Specifically, we denote the density \u03c1 of the arrival time and the distribution m of the mark of the next event as follows:\n\u03c1(t)dt = Pr(e in [t, t+ dt) |H(t)), (7.1) m(x) = Pr(x |H(t)) (7.2)\nAs discussed by Shchur et al. [186], such an intensity-free MTPP model enjoys several benefits over its intensity-based counterparts in terms of facilitating efficient training, scalable prediction, computation of expected arrival times, etc. Given a sequence of observed events H(T ) collected during the time interval (0, T ], the likelihood function is given by: p(H(T )) = \u220fei=(ti,xi)\u2208H(T ) \u03c1(ti)\u00d7m(xi) (7.3)"
        },
        {
            "heading": "7.2.2 Problem setup",
            "text": "Next, we set up our problem of retrieving a ranked list of sequences from a corpus of continuoustime event sequences (CTESs) which are relevant to a given query CTES.\nQuery and corpus sequences, relevance labels. We operate on a large corpus of sequences {Hc(Tc) | c \u2208 C}, where Hc(Tc) = {(t(c)i , x(c)i ) | t(c)i < Tc}. We are given a set of query sequences {Hq(Tq) | q \u2208 Q} withHq(Tq) = {(t(q)i , x(q)i ) | t(q)i < Tq}, as well as a query-specific relevance label for the set of corpus sequences. That is, for a given query sequence Hq, we have: y(Hq,Hc) = +1 ifHc is marked as relevant toHq and y(Hq,Hc) = \u22121 otherwise.\nWe define Cq+ = {c \u2208 C | y(Hq,Hc) = +1}, and, Cq\u2212 = {c \u2208 C | y(Hq,Hc) = \u22121}, with C = Cq+ \u222a Cq\u2212. Finally, we denote T = max{Tq, Tc | q \u2208 Q, c \u2208 C} as the maximum time of the data collection.\nOur Goal. We aim to design an efficient CTES retrieval system, which would return a list of sequences from a known corpus of sequences, relevant to a given query sequenceHq. Therefore, we can view a sequence retrieval task as an instance of the ranking problem. Similar to other information retrieval algorithms, a CTES retrieval algorithm first computes the estimated relevance s(Hq,Hc) of the corpus sequenceHc for a given query sequenceHq and then provides a ranking of C in the decreasing order of their scores."
        },
        {
            "heading": "Large-Scale Retrieval of Temporal Sequences 115",
            "text": ""
        },
        {
            "heading": "7.3 NEUROSEQRET Model",
            "text": "In this section, we describe NEUROSEQRET family of MTPP-based models that we propose for the retrieval of continuous-time event sequences (CTES). We begin with an outline of its two key components."
        },
        {
            "heading": "7.3.1 Components of NEUROSEQRET",
            "text": "NEUROSEQRET models the relevance scoring function between query and corpus sequence pairs. However, the relevance of a corpus sequence to the query is latent and varies widely across applications. To accurately characterize this relevance measure, NEUROSEQRET works in two steps. First, it unwarps the query sequences to make them compatible for comparison with the corpus sequences. Then, it computes the pairwise relevance score between the query and corpus sequences using neural MTPP models.\nUnwarping query sequences. Direct comparison between a query and a corpus sequence can provide misleading outcomes, since they also contain their own individual idiosyncratic factors in addition to sharing some common attributes. In fact, a corpus sequence can be highly relevant to the query, despite greatly varying in timescale, initial time, etc. In other words, it may have been generated by applying a warping transformation on a latent sequence [59, 226]. Thus, a direct comparison between a relevant sequence pair may give a poor relevance score. To address this challenge, we first apply a trainable unwarping function [226] U(\u00b7) on the arrival times of a query sequence, which enhances its compatibility for comparing it with the corpus sequences1. More specifically, we define U(Hq) := {(U(t(q)i ), x(q)i )}. In general, U satisfies two properties [59, 226]: unbiasedness, i.e., having a small value of \u2016U(t)]\u2212 t\u2016 and monotonicity, i.e., dU(t)/dt \u2265 0. These properties ensure that the chronological order of the events across both the warped observed sequence and the unwarped sequence remains the same.\nSuch a sequence transformation learns to capture the similarity between two sequences, even if it is not apparent due to different individual factors, as we shall later in our experiments (Figure 7.1).\nComputation of relevance scores. Given a query sequence Hq and a corpus sequence Hc, we compute the relevance score s(Hq,Hc) using two similarity scores, e.g., (i) a model independent sequence similarity score and (ii) a model based sequence similarity score.\n\u2014Model independent similarity score: Computation of model-independent similarity score between two sequences is widely studied in literature [152, 193, 224]. They are computed using\n1We only apply the unwarping function on the times. Since marks belong to a fixed discrete set, we believe marks are directly comparable."
        },
        {
            "heading": "116 Large-Scale Retrieval of Temporal Sequences",
            "text": "different distance measures between two sequences, e.g., DTW, Wasserstein distance, etc., and therefore, can be immediately derived from data without using the underlying MTPP model. In this work, we compute the model-independent similarity score, SIMU(Hq,Hc), between Hq andHc as follows:\nSIMU(Hq,Hc) = \u2212\u2206t(U(Hq),Hc)\u2212\u2206x(Hq,Hc) (7.4)\nwhere, \u2206t and \u2206x are defined as:\n\u2206t(U(Hq),Hc) = Hmin\u2211 i=0 \u2223\u2223\u2223U(t(q)i )\u2212 t(c)i \u2223\u2223\u2223+ \u2211 ti\u2208Hc\u222aHq i>|Hmin| (T \u2212 ti), (7.5)\n\u2206x(Hq,Hc) = Hmin\u2211 i=0 I[x(q)i 6= x(c)i ] + \u2223\u2223|Hc| \u2212 |Hq|\u2223\u2223. (7.6)\nHere, Hmin = min{|Hq|, |Hc|}, T = max{Tq, Tc} where the events ofHq andHc are gathered until time Tq and Tc respectively; \u2206t(U(Hq),Hc) is the Wasserstein distance between the unwarped arrival time sequence U(Hq) and the corpus sequence [224] and, \u2206x(Hq,Hc) measures the matching error for the marks, wherein the last term penalizes the marks of last |Hc| \u2212 |Hq| events of |Hc|.\n\u2014Model-based similarity score using Fisher kernel: We hypothesize that the relevance score s(Hq,Hc) also depends on a latent similarity that may not be immediately evident from the observed query and corpus sequences even after unwarping. Such similarity can be measured by comparing the generative distributions of the query-corpus sequence pairs. To this end, we first develop an MTPP-based generative model p\u03b8(H) parameterized by \u03b8 and then compute a similarity score using the Fisher similarity kernel between the unwarped query and corpus sequence pairs (U(Hq),Hc) [93]. Specifically, we compute the relevance score between the unwarped query sequence U(Hq) and the corpus sequenceHc as follows:\n\u03bap\u03b8(Hq,Hc) = vp\u03b8(U(Hq))>vp\u03b8(Hc), (7.7)\nwhere \u03b8 is the set of trainable parameters; vp\u03b8(\u00b7) is given by\nvp(H) = I\u22121/2\u03b8 \u2207\u03b8 log p\u03b8(H)/||I \u22121/2 \u03b8 \u2207\u03b8 log p\u03b8(H)||2, (7.8)\nI\u03b8 is the Fisher information matrix [93], i.e., I\u03b8 = EH\u223cp\u03b8(\u2022) [ \u2207\u03b8 log p\u03b8(H)\u2207\u03b8 log p\u03b8(H)> ] . We would like to highlight that \u03bap\u03b8(Hq,Hc) in Eq. (7.7) is a normalized version of Fisher kernel"
        },
        {
            "heading": "Large-Scale Retrieval of Temporal Sequences 117",
            "text": "since ||vp\u03b8(\u00b7)|| = 1. Thus, \u03bap\u03b8(Hq,Hc) measures the cosine similarity between vp\u03b8(U(Hq)) and vp\u03b8(Hc).\nNote that, KL divergence or Wasserstein distance could also serve our purpose of computing the latent similarity between the generative distributions. However, we choose the Fisher similarity kernel because of two reasons: (i) it is known to be a natural similarity measure that allows us to use the underlying generative model in a discriminative learning task [93, 183]; and, (ii) unlike KL divergence or other distribution (dis)similarities, it computes the cosine similarity between vp\u03b8(U(Hq)) and vp\u03b8(Hc), which makes it compatible with locality-sensitive hashing [23].\n\u2014Net relevance score: Finally, we compute the relevance score as:\nsp,U (Hq,Hc) = \u03bap (Hq,Hc) + \u03b3SIMU (Hq,Hc) (7.9)\nwhere \u03b3 is a hyperparameter."
        },
        {
            "heading": "7.3.2 Neural Parameterization of NEUROSEQRET",
            "text": "Here, we first present the neural architecture of the unwarping function and then describe the MTPP models used to compute the model-based similarity score in Eq. (7.7). As we describe later, we use two MTPP models with different levels of modeling sophistication, viz., SELFATTN-NEUROSEQRET and CROSSATTN-NEUROSEQRET. In SELFATTN-NEUROSEQRET, the likelihood of a corpus sequence is computed independently of the query sequence using self attention-based MTPP model, e.g., Transformer Hawkes Process [251]. As a result, we can employ a locality-sensitive hashing-based efficient retrieval-based SELFATTN-NEUROSEQRET. In CROSSATTN-NEUROSEQRET, on the other hand, we propose a more expressive and novel cross-attention MTPP model, where the likelihood of a corpus sequence is dependent on the query sequence. Thus, our models can effectively tradeoff between accuracy and efficiency.\nNeural architecture of U(\u00b7). As discussed in Section 7.3.1, the unwarping function U(\u00b7) should be unbiased, i.e., a small value of \u2016U(t)\u2212 t\u2016 and monotonic, i.e., dU(t)/dt > 0. To this end, we model U(\u00b7) \u2248 U\u03c6(\u00b7) using a nonlinear monotone function which is computed using an unconstrained monotone neural network (UMNN) [215],i.e.,\nU\u03c6(t) = \u222b t 0 u\u03c6(\u03c4)d\u03c4 + \u03b7, (7.10)\nwhere \u03c6 is the parameter of the underlying neural network u\u03c6(\u00b7), \u03b7 \u2208 N (0, \u03c3) and u\u03c6 : R\u2192 R+ is a non-negative non-linear function. Since the underlying monotonicity can be achieved only"
        },
        {
            "heading": "118 Large-Scale Retrieval of Temporal Sequences",
            "text": "by enforcing the non-negativity of the integrand u\u03c6, UMNN admits an unconstrained, highly expressive parameterization of monotonic functions. Therefore, any complex unwarping function U\u03c6(\u00b7) can be captured using Eq. (7.10) by integrating a suitable neural model augmented with ReLU(\u00b7) in the final layer. In other words, if u\u03c6 is a universal approximator for a positive function, then U\u03c6 can capture any differentiable unwarping function. We impose an additional regularizer 1\n\u03c32 \u222b T 0 \u2016u\u03c6(t)\u2212 1\u20162 dt on our training loss which ensures that \u2016U(t)\u2212 t\u2016 remains\nsmall.\nNeural architecture of MTPP model p\u03b8(\u00b7). We provide two variants of p\u03b8(\u00b7), which leads to two retrieval models, viz., SELFATTN-NEUROSEQRET and CROSSATTN-NEUROSEQRET. These two models offer a nice tradeoff between accuracy and efficiency.\nSELFATTN-NEUROSEQRET: Here, we use the Transformer Hawkes process [251], which applies a self-attention mechanism to model the underlying generative process. In this model, the gradient of corpus sequences v\u03b8(Hc) = \u2207\u03b8 log p\u03b8(Hc) are computed independently of the query sequence Hq. Once we train the retrieval model, v\u03b8(Hc) can be precomputed and bucketized before observing the test query. Such a model, together with the Fisher kernel-based cosine similarity scoring model, allows us to apply locality-sensitive hashing for efficient retrieval.\nCROSSATTN-NEUROSEQRET: The above self-attention-based mechanism models a query agnostic likelihood of the corpus sequences. Next, we introduce a cross attention based MTPP model which explicitly takes into account the underlying query sequence while modeling the likelihood of the corpus sequence. Specifically, we measure the latent relevance score betweenHq andHc via a query-induced MTPP model built using the cross-attention between the generative process of both the sequences.\nGiven a query sequence Hq and the first r events of the corpus sequence Hc, we parameterize the generative model for (r + 1)-th event, i.e., p(e(c)r+1 |H(tr)) as p\u03b8(\u00b7), where p\u03b8(e(c)r+1) = \u03c1\u03b8(t (c) r+1)m\u03b8(x (c) r+1), where \u03c1 and m are the density and distribution functions for the arrival time and the mark respectively, as described in Eq. (7.1).\n\u2014Input Layer: For each event e(q)i in the query sequence Hq and each event e(c)j in the first r events in the corpus sequenceHc, the input layer computes the initial embeddings y(q)i and y(c)j as follows:\ny (q) i = wy,xx (q) i +wy,tU(t (q) i ) +wy,\u2206t\n( U(t\n(q) i )\u2212 U(t(q)i\u22121) ) + by,\u2200i \u2208 [|Hq|], (7.11)\ny (c) j = wy,xx (c) j +wy,tt (c) j +wy,\u2206t ( t (c) j \u2212 t(c)j\u22121 ) + by, \u2200j \u2208 [|Hc(tr)| \u2212 1] (7.12)"
        },
        {
            "heading": "Large-Scale Retrieval of Temporal Sequences 119",
            "text": "where w\u2022,\u2022 and by are trainable parameters.\n\u2014Attention layer: The second layer models the interaction between all the query events and the past corpus events, i.e., Hq and Hc(tr) using an attention mechanism. Specifically, following the existing attention models [99, 115, 206] it first adds a trainable position embedding p with y\u2014 the output from the previous layer. As compared to fixed positional encodings [206], these learnable encodings demonstrate better performances [99, 115]. More specifically, we have the updates: y(q)i \u2190 y(q)i + pi and y(c)j \u2190 y(c)j + pj . Where, p\u2022 \u2208 RD. Next, we apply two linear transformations on the vectors [y(q)i ]i\u2208[|Hq |] and one linear transformation on [y\n(c) j ]j\u2208[r], i.e., sj = W Sy (c) j ,ki = W Ky (q) i ,vi = W V y (q) i . The state-of-the-art works on attention models [99, 115, 206, 251] often refer s\u2022, k\u2022 and v\u2022 as query,2, key and value vectors respectively. Similarly, we call the trainable weightsW S,WK andW V as the Query, Key and Value matrices, respectively. Finally, we use the standard attention recipe [206] to compute the final embedding vector h(c,q)j for the event e (c) j , induced by queryHq. Such a recipe adds the values weighted by the outputs of a softmax function induced by the query and key, i.e.,\nh (c,q) j = \u2211 i\u2208[|Hq |]\nexp ( s>j ki/ \u221a D )\n\u2211 i\u2032\u2208[|Hq |] exp ( s>j ki\u2032/ \u221a D )vi, (7.13)\n\u2014Output layer: Given the vectors h(c,q)j provided by the attention mechanism (7.13), we first apply a feed-forward neural network on them to compute h (c,q)\nr as follows:\nh (c,q) r = r\u2211 j=1 [ wh ReLU(h (c,q) j wh,f + bh,o) + bh ] ,\nwhere w\u2022,\u2022, w\u2022 and bv. Finally, we use these vectors to compute the probability density of the arrival time of the next event e(c)r+1, i.e., \u03c1\u03b8(tr+1) and the mark distribution m\u03b8(xr+1). In particular, we realize \u03c1\u03b8(tr+1) using a lognormal distribution of inter-arrival times, i.e.,\nt (c) r+1 \u2212 t(c)r \u223c LOGNORMAL ( \u00b5e ( h (c,q) r ) , \u03c32e ( h (c,q) r )) ,\nwhere, [ \u00b5e ( h (c,q)\nr\n) , \u03c3e ( h (c,q)\nr\n)] = W t,qh (c,q) r +bt,q. Similarly, we model the mark distribution\n2The term query in this attention model is different from \u201cquery\u201d sequence\n120 Large-Scale Retrieval of Temporal Sequences\nas,\nm\u03b8(xr+1) = exp\n( w>x,mh (c,q) + bx,m ) \u2211\nx\u2032\u2208C exp ( w>x\u2032,mh (c,q) + bx\u2032,m ) , (7.14) where W \u2022,\u2022 are the trainable parameters. Therefore the set of trainable parameters for the underlying MTPP models is \u03b8 = {W \u2022,W \u2022,\u2022,w\u2022,w\u2022,\u2022, b\u2022, b\u2022,\u2022}.\nTradeoff between accuracy and efficiency. Note that the likelihoods of corpus sequences in CROSSATTN-NEUROSEQRET depend on the query sequence, making it more expressive than SELFATTN-NEUROSEQRET. However, these query-dependent corpus likelihoods cannot be precomputed before observing the query sequences and thus cannot be directly used for hashing. However, it can be deployed on top of SELFATTN-NEUROSEQRET, where a smaller set of relevant candidates is first selected using LSH applied on SELFATTN-NEUROSEQRET and then reranked using this SELFATTN-NEUROSEQRET. Thus, our proposal offers a nice tradeoff between accuracy and efficiency."
        },
        {
            "heading": "7.3.3 Parameter estimation",
            "text": "Given the query sequences {Hq}, the corpus sequences {Hc} along with their relevance labels {y(Hq,Hc)}, we seek to find \u03b8 and \u03c6 which ensure that:\nsp\u03b8,U\u03c6(Hq,Hc+) sp\u03b8,U\u03c6(Hq,Hc\u2212)\u2200 c\u00b1 \u2208 Cq\u00b1. (7.15)\nTo this aim, we minimize the following pairwise ranking loss [96] to estimate the parameters \u03b8, \u03c6:\nmin \u03b8,\u03c6 \u2211 q\u2208Q \u2211 c+\u2208Cq+, c\u2212\u2208Cq\u2212 [ sp\u03b8,U\u03c6(Hq,Hc\u2212)\u2212 sp\u03b8,U\u03c6(Hq,Hc+) + \u03b4 ] + ,\nwhere, \u03b4 is a tunable margin."
        },
        {
            "heading": "7.4 Scalable Retrieval with Hashing",
            "text": "Once we learn the model parameters \u03b8 and \u03c6, we can rank the set of corpus sequences Hc in the decreasing order of sp\u03b8,U\u03c6(Hq\u2032 ,Hc) for a new query Hq\u2032 and return top\u2212K sequences. Such an approach requires |C| comparisons per each test query, which can take a huge amount of time for many real-life applications where |C| is high. However, for most practical query"
        },
        {
            "heading": "Large-Scale Retrieval of Temporal Sequences 121",
            "text": "sequences, the number of relevant sequences is a very small fraction of the entire corpus of sequences. Therefore, the number of comparisons between query-corpus sequence pairs can be reduced without significantly impacting the retrieval quality by selecting a small number of candidate corpus sequences that are more likely to be relevant to a query sequence. We first briefly describe the traditional random hyperplane-based hashing method and highlight its limitations before presenting our retrieval approach that uses a specific trainable hash code with desirable properties."
        },
        {
            "heading": "7.4.1 Random hyperplane based hashing method",
            "text": "Since the relevance between the query and corpus sequence pairs (Hq,Hc) is measured using the cosine similarity between the gradient vectors, i.e., \u03bap\u03b8(Hq,Hc), one can use random hyperplane based locality sensitive hashing method for hashing the underlying gradient vectors vp\u03b8(Hc) [23]. Towards this goal, after training SELFATTN-NEUROSEQRET we generate R unit random vectors ur \u2208 RD from i.i.d. Normal distributions and then compute a binary hash code, \u03b6c = [sign(u>1 vp\u03b8(Hc)), . . . , sign(u>Rvp\u03b8(Hc))] for each c \u2208 C. This leads to 2R possible hash buckets {B}, where each corpus sequence is assigned to one hash bucket using the algorithm proposed by Gionis et al. [60].\nWhen we encounter an unseen test query Hq, we compute the corresponding hash code \u03b6q, assign it to a bucket B and finally return only those sequences Hc which were assigned to this bucket B. Thus, for each query, the number of comparisons is reduced from |C| to |B|, i.e., the number of corpus sequences in the bucket B. Thus, if the corpus sequences are assigned uniformly across the different buckets, then the expected number of comparisons becomes |C|/2R, which provides a significant improvement for R > 2.\nLimitations of Binary Hash Codes. In practice, binary hash codes are not trained from data, and consequently, they are not optimized to be uniformly distributed across different hash buckets. Consequently, the assignment of corpus sequences across different buckets may be quite skewed, leading to inefficient sequence retrieval."
        },
        {
            "heading": "7.4.2 Trainable Hashing for Retrieval",
            "text": "Responding to the limitations of the random hyperplane-based hashing method, we propose to learn the hash codes from data so that they can be optimized for performance.\nComputation of a trainable hash code. We first apply a trainable nonlinear transformation \u039b\u03c8 with parameter \u03c8 on the gradients vc = vp\u03b8(Hc) and then learn the binary hash vectors \u03b6c = sign (\u039b\u03c8 (v c)) by solving the following optimization, where we use tanh (\u039b\u03c8(\u00b7)) as a"
        },
        {
            "heading": "122 Large-Scale Retrieval of Temporal Sequences",
            "text": "Algorithm 2: Efficient retrieval with hashing Require: Trained corpus embeddings {vc = vp\u03b8(Hc)} using SELFATTN-NEUROSEQRET;\nnew query sequences {Hq\u2032}, K: # of corpus sequences to return; trained models for SELFATTN-NEUROSEQRET and CROSSATTN-NEUROSEQRET.\n1: Output: {Lq\u2032}: top-K relevant sequences from {Hc}. 2: \u03c8 \u2190 TRAINHASHNET (\u039b\u03c8, [vc]c\u2208C) 3: INITHASHBUCKETS(\u00b7) 4: for c \u2208 C do 5: \u03b6c \u2190 COMPUTEHASHCODE (vc; \u039b\u03c8) 6: B \u2190 ASSIGNBUCKET(\u03b6c) 7: end for 8: for each new queryHq\u2032 do 9: vq\n\u2032 \u2190 SELFATTN-NEUROSEQRET(Hq\u2032) 10: \u03b6q\n\u2032 \u2190 COMPUTEHASHCODE(vq\u2032 ; \u039b\u03c8) 11: B \u2190 ASSIGNBUCKET(\u03b6q\u2032) 12: for c \u2208 B do 13: vq \u2032 cross,v c cross \u2190 CROSSATTN-NEUROSEQRET(Hq\u2032 ,Hc) 14: sp\u03b8,U\u03c6(Hq\u2032 ,Hc)\u2190 SCORE(vq \u2032 cross,v c cross,Hq,Hc) 15: end for 16: Lq\u2032 \u2190 RANK({sp\u03b8,U\u03c6(Hq\u2032 ,Hc)},K) 17: end for 18: Return {Lq\u2032}\nsmooth approximation of sign (\u039b\u03c8(\u00b7)).\nmin \u03c8 \u03b71 |C| \u2211 c\u2208C \u2223\u22231> tanh (\u039b\u03c8 (vc))\u2223\u2223+ \u03b72|C|\u2211 c\u2208C \u2016|tanh (\u039b\u03c8 (vc))| \u2212 1\u20161 (7.16)\n+ 2\u03b73( D 2 ) \u00b7 \u2223\u2223\u2223\u2223 \u2211 c\u2208C\ni 6=j\u2208[D]\ntanh (\u039b\u03c8 (v c) [i]) \u00b7 tanh (\u039b\u03c8 (vc) [j]) \u2223\u2223\u2223\u2223\nHere, \u22113\ni=1 \u03b7i = 1. Moreover, different terms in Eq. (7.16) allow the hash codes, \u03b6 c, to have\na set of four desired properties: (i) the first term ensures that the numbers of +1 and \u22121 are evenly distributed in the hash vectors \u03b6c = tanh (\u039b\u03c8 (vc)); (ii) the second term encourages the entries of \u03b6c to become as close to \u00b11 as possible so that tanh(\u00b7) gives an accurate approximation of sign(\u00b7) ; (iii) the third term ensures that the entries of the hash codes, \u03b6c, contain independent information and therefore they have no redundant entries. Trainable hashing has been used in other domains of information retrieval, including graph hashing [169, 178], document retrieval [181, 242]. However, to the best of our knowledge, such an approach has never been proposed for continuous-time sequence retrieval.\nOutline of our retrieval method. We summarize our retrieval procedure in Algorithm 2. We"
        },
        {
            "heading": "Large-Scale Retrieval of Temporal Sequences 123",
            "text": "are given gradient vectors vc = vp\u03b8(Hc) obtained by training SELFATTN-NEUROSEQRET. Next, we train an additional neural network \u039b\u03c8 parameterized by \u03c8 (TRAINHASHNET(), line 2), which is used to learn a binary hash vector \u03b6c for each sequence Hc. Then these hash codes are used to arrange corpus sequences in different hash buckets (for-loop in lines 4\u20137) using the algorithm proposed by Gionis et al. [60], so that two sequences Hc,Hc\u2032 lying in the same hash bucket have a very high value of cosine similarity cos(vc,vc\u2032). Finally, once a new query Hq\u2032 comes, we first compute vq\u2032 using the trained SELFATTN-NEUROSEQRET model and then compute the binary hash codes \u03b6q \u2032 using the trained hash network \u039b\u03c8 (lines 9\u201310). Next, we assign an appropriate bucket B to it (line 11) and finally compare it with only the corpus sequences in the same bucket, i.e.,Hc \u2208 B (lines 12\u201315) using our model.\nBucket Assignment. As suggested in [60], we design multiple hash tables and assign a bucket to the hashcode of a sequence using only a set of bits selected randomly. More specifically, let the number of hash tables be M . Given a query sequence, we calculate its hashcode using the procedure described in Algorithm 2, \u03b6q \u2032 = sign ( \u039b\u03c8(v q\u2032) ) . The hash code is a R dimension vector with \u03b6q \u2032 \u2208 {\u22121, 1}R and from this vector, we consider L bits at random positions to determine the bucket to be assigned to the sequence. Here, \u03b6q \u2032\nrepresents the numbers between {0, 2L \u2212 1}, i.e., one of the 2L different buckets in a hash table. Correspondingly, we assign \u03b6q \u2032\ninto a bucket. However, such a procedure is dependent on the specific set of bits \u2013that were selected randomly\u2013 used for deciding the bucket ID. Therefore, we use M hash-tables and repeat the procedure of sampling L bits and bucket assignment for each table. We follow a similar bucket assignment procedure for corpus sequences. As described in Algorithm 2, for an incoming query sequence in the test set, we use the above bucket assignment procedure and compute the relevance score for only the corpus sequences within the same buckets. For all our experiments, we set H the same as the hidden dimension D, M = 10, and L = 12.\nRecall that we must use SELFATTN-NEUROSEQRET to compute gradient vectors for subsequent hash code generation (in lines 9\u201310). However, at the last stage for final score computation and ranking, we can use any variant of NEUROSEQRET (in line 13), preferably CROSSATTNNEUROSEQRET, since the corpus sequences have already been indexed by our LSH method."
        },
        {
            "heading": "7.5 Experiments",
            "text": "In this section, we provide a comprehensive evaluation of NEUROSEQRET and our hashing method."
        },
        {
            "heading": "124 Large-Scale Retrieval of Temporal Sequences",
            "text": ""
        },
        {
            "heading": "7.5.1 Experimental Setup",
            "text": "Datasets. We use five real-world datasets containing event sequences from various domains: (i) Audio [34], (ii) Sports [234], (iii) Celebrity [155], (iv) Electricity [154], and (v) Health [10]. The statistics of all datasets are given in Table 7.1. Across all datasets, |Hq| = 5K and |Hc| = 200K. We partition the set of queries into 50% training, 10% validation, and the rest as test sets. During training, we negatively sample 100 corpus sequences for each query.\n\u2022 Audio: The dataset contains audio files for spoken commands to a smart-light system and the demographics(age, nationality) of the speaker. Here, a query corpus sequence pair is relevant if they are from an audio file with a common speaker. \u2022 Sports: The dataset contains actions (e.g., run, pass, shoot) taken while playing different sports. We consider the time of action and action class as time and mark of sequence, respectively. Here, a query corpus sequence pair is relevant if they are from a common sport. \u2022 Celebrity: In this dataset, we consider the series of frames extracted from youtube videos of multiple celebrities as event sequences where event-time denotes the video-time, and the mark is decided upon the coordinates of the frame where the celebrity is located. Here, a query corpus sequence pair is relevant if they are from a video file having a common celebrity. \u2022 Electricity: This dataset contains the power-consumption records of different devices across smart homes in the UK. We consider the records for each device as a sequence with an event mark as the normalized change in the power consumed by the device and the time of recording as event time. Here, a query corpus sequence pair is relevant if they are from a similar appliance. \u2022 Health: The dataset contains ECG records for patients suffering from heart-related problems. Since the length of the ECG record for a single patient can be up to 10 million, we generate smaller individual sequences of length 10,000 and consider each such sequence as an independent sequence. The marks and times of events in a sequence are determined using a similar procedure as in Electricity. Here, a query corpus sequence pair is relevant if they are from a common patient."
        },
        {
            "heading": "Large-Scale Retrieval of Temporal Sequences 125",
            "text": "For Health, Celebrity, and Electricity datasets, we lack the true ground-truth labeling of relevance between sequences. Therefore, we adopt a heuristic in which, given a dataset D, from each sequence seqq \u2208 D with q \u2208 [|D|], we first sample a set of sub-sequences Uq = {H \u2282 seqq} with |Uq| \u223c Unif [200, 300]. For each such collection Uq, we draw exactly one queryHq uniformly at random from Uq, i.e.,Hq \u223c Uq. Then, we define C = \u222aq\u2208[|D|]Uq\\Hq, Cq+ = Uq\\Hq and Cq\u2212 = \u222ac6=q ( Uc\\Hc ) .\nSystem Configuration. All our models were implemented using Pytorch v1.6.0 3. We conducted all our experiments on a server running Ubuntu 16.04, CPU: Intel(R) Xeon(R) Gold 6248 2.50GHz, RAM: 377GB, and GPU: NVIDIA Tesla V100.\nHyperparameters setup. We set the hyper-parameters values of NEUROSEQRET as follows: (i) contribution of model-independent similarity score in Eq. (7.9), \u03b3 = 0.1; (ii) margin parameters for parameter estimation, \u03b4 \u2208 {0.1, 0.5, 1} and weight for constraint violations, \u03bb \u2208 {0.1, 0.5, 1}; (iii) weight parameters for hashing objective (7.16) \u03b71, \u03b72, \u03b73 \u2208 {0.1, 0.2, 0.25} and correspondingly \u03b74 \u2208 {0.25, 0.4, 0.7}.\nMoreover, the values of training specific parameter values are: (i) batch-size, B is selected from {16, 32}, i.e., for each batch we select B query sequences and all corresponding corpus sequences; (ii) hidden-layer dimension for cross-attention model, D \u2208 {32, 48, 64}; (iii) number of attention blocks Nb = 2; (iv) number of attention heads Nh = 1 and (v) UMNN network as a two-layer feed-forward network with dimension {128, 128}. We also add a dropout after each attention layer with probability p = 0.2 and an l2 regularizer over the trainable parameters with the coefficient set to 0.001. All our parameters are learned using the Adam optimizer. We summarize the details of hyperparameters across different datasets in Table 7.2.\nEvaluation metrics. We evaluate NEUROSEQRET and the baselines using mean average precision (MAP), NDCG@k, and mean reciprocal rank (MRR). We calculate these metrics as\n3https://pytorch.org/\n126 Large-Scale Retrieval of Temporal Sequences\nfollows:\nMAP = 1 |Hq\u2032| \u2211 q\u2032\u2208Hq\u2032 APq\u2032 , NDCG@k = DCGk IDCGk , MRR = 1 |Hq\u2032 | \u2211 q\u2032\u2208Hq\u2032 1 rq\u2032 , (7.17)\nwhere APq\u2032 ,DCGk, IDCGk, and rq\u2032 denote the average precision, discounted cumulative gain at top-k position, ideal discounted cumulative gain (at top-k), and the topmost rank of a related corpus sequence respectively. For all our evaluations, we follow a standard evaluation protocol [99, 115] for our model and all baselines wherein for each query sequence in the test set, we rank all relevant corpus sequences and 1000 randomly sampled non-relevant sequences. All confidence intervals and standard deviations are calculated after five independent runs. For all metrics \u2013 MAP, NDCG, and MRR, we report results in terms of percentages with respect to maximum possible value i.e., 1.\nBaseline Implementations. For all the baselines, we use the official python implementations released by the authors of MASS, UDTW, and Sharp. The other implementations are same as in Chapter 3. For MASS and UDTW, we report the results using the default parameter values. For Sharp, we tune the hyper-parameter \u2018gamma\u2019 (for more details see [18]) based on the validation set. In RMTPP, we set the BPTT length to 50, the RNN hidden layer size to 64, and the event embedding size 16. These are the parameter values recommended by the authors. For SAHP and THP, we set the dimension to 128 and the number of heads to 2. The values for all other transformer parameters are similar to the ones we used for the attention part in NEUROSEQRET.\nBaselines. We consider three continuous time-series retrieval models: (i) MASS [153], (ii) UDTW [170], and (iii) Sharp [18]; and, three MTPP models (iv) RMTPP [45], (v) SAHP [243], and (vi) THP [251]. For sequence retrieval with MTPP models, we first train them across all the sequences using maximum likelihood estimation. Then, given a test query Hq\u2032 , this MTPP method ranks the corpus sequences {Hc} in decreasing order of their cosine similarity CosSim(emb(q\n\u2032),emb(c)), where emb(\u2022) is the corresponding sequence embedding provided by the underlying MTPP model. In addition, we build supervised ranking models over these approaches, viz., Rank-RMTPP, Rank-SAHP and Rank-THP corresponding to RMTPP, SAHP, and THP. Specifically, Rank-MTPP formulates a ranking loss on the query-corpus pairs based on the cosine similarity scores along with the likelihood function to get the final training objective. Therefore, the vanilla MTPP models are used as unsupervised models and the corresponding Rank-MTPP models work as supervised models.\nEvaluation protocol. We partition the set of queries Q into 50% training, 10% validation, and the rest as test sets. First, we train a retrieval model using the set of training queries."
        },
        {
            "heading": "Large-Scale Retrieval of Temporal Sequences 127",
            "text": "Then, for each test query q\u2032, we use the trained model to obtain a top-K ranked list from the corpus sequences. Next, we compute the average precision (AP) and discounted cumulative gain (DCG) of each top-K list, based on the ground truth. Finally, we compute the mean average precision (MAP) and NDCG@K by averaging AP and DCG values across all test queries. We set K \u2208 {10, 20}."
        },
        {
            "heading": "7.5.2 Results on retrieval accuracy",
            "text": "Comparison with baselines. First, we compare the retrieval performance of our model against the baselines. Tables 7.3, 7.4, 7.5, and 7.6 summarizes the results in terms of MAP, NDCG@10,"
        },
        {
            "heading": "128 Large-Scale Retrieval of Temporal Sequences",
            "text": "MRR, and NDCG@20 respectively. From the results, we make the following observations:\n\u2022 Both CROSSATTN-NEUROSEQRET and SELFATTN-NEUROSEQRET outperform all the baselines by a substantial margin. \u2022 CROSSATTN-NEUROSEQRET outperforms SELFATTN-NEUROSEQRET, since the former has a higher expressive power. \u2022 The variants of baseline MTPP models trained for sequence retrieval, i.e., Rank-RMTPP, Rank-SAHP, and Rank-THP outperform the vanilla MTPP models. \u2022 The performances of vanilla MTPPs and the time series retrieval models (MASS, UDTW, and Sharp) are comparable.\nAblation Study. Next, we compare the retrieval performance across four model variants: (i) our model with only model-independent score, i.e., sp\u03b8,U\u03c6(Hq,Hc) = \u2212\u2206x(Hq,Hc) \u2212 \u2206t(U\u03c6(Hq),Hc); (ii) our model with only model-dependent score, i.e., sp\u03b8,U\u03c6(Hq,Hc) = \u03bap\u03b8(Hq,Hc); (iii) our model without any model-independent time similarity, i.e., sp\u03b8,U\u03c6(Hq,Hc) = \u03bap\u03b8(Hq,Hc)\u2212"
        },
        {
            "heading": "Large-Scale Retrieval of Temporal Sequences 129",
            "text": "\u03b3\u2206x(Hq,Hc); (iv) our model without any model-independent mark similarity, i.e., sp\u03b8,U\u03c6(Hq,Hc) = \u03bap\u03b8(Hq,Hc)\u2212\u03b3\u2206t(U\u03c6(Hq),Hc); (v) our model without unwarping function U\u03c6(\u00b7); and (vi) the complete design of our model.\nIn all cases, we used CROSSATTN-NEUROSEQRET. Table 7.7 shows that the complete design of our model (variant (vi)) achieves the best performance. We further note that removing \u03bap\u03b8 from the score (variant (i)) leads to significantly poor performance. Interestingly, our model without any mark-based similarity (variant (iv)) leads to better performance than the model without time similarity (variant (iii))\u2014 this could be attributed to the larger variance in querycorpus time distribution than the distribution of marks.\nEffect of U\u03c6(\u00b7). Finally, we observe that the performance deteriorates if we do not use an unwarping function U\u03c6(\u00b7) (variant (v)). Figure 7.1 illustrates the effect of U\u03c6(\u00b7). It shows that U\u03c6(\u00b7) is able to learn a suitable transformation of the query sequence, which encapsulates the high value of latent similarity with the corpus sequence."
        },
        {
            "heading": "7.5.3 Results on Retrieval Efficiency",
            "text": "We compare our efficient sequence retrieval method given in Algorithm 2 against the random hyperplane (RH) method and three variants of our proposed training problem in Eq. (7.16)."
        },
        {
            "heading": "130 Large-Scale Retrieval of Temporal Sequences",
            "text": "(i) Our (\u03b72, \u03b73) which sets \u03b71 = 0 and thus does not enforce even distribution of \u00b11 in \u03b6c; (ii) Our (\u03b71, \u03b73) which sets \u03b72 = 0 and thus tanh does not accurately approximate sign; (iii) Our (\u03b71, \u03b72) which sets \u03b73 = 0 and thus does not enforce \u03b6c to be compact and free of redundancy. Our (\u03b71, \u03b72, \u03b73) is the complete design that includes all trainable components. Figure 7.2 summarizes the results.\nComparison with random hyperplane. Figure 7.2 shows that our method (Our(\u03b71, \u03b72, \u03b73)) demonstrates better Pareto efficiency than RH. This is because RH generates hash code in a data-oblivious manner, whereas our method learns the hash code on top of the trained embeddings.\nAblation study on different components of Eq. (7.16). Figure 7.2 summarizes the results, which shows that (i) the first three variants are outperformed by Our(\u03b71, \u03b72, \u03b73); (ii) the first term having \u03b71 6= 0, which enforces an even distribution of\u00b11, is the most crucial component for the loss function\u2014 as the removal of this term causes significant deterioration of the performance."
        },
        {
            "heading": "Large-Scale Retrieval of Temporal Sequences 131",
            "text": ""
        },
        {
            "heading": "7.5.4 Analysis at a Query Level",
            "text": "Next, we compare the performance between NEUROSEQRET and other state-of-the-art methods at a query level. Specifically, for each query Hq we compute the advantage of using NEUROSEQRET in terms of gain in average precision, i.e., AP(NEUROSEQRET) \u2212 AP(baseline) for two most competitive baselines \u2013 Rank-SAHP and Rank-THP. We summarize the results in Figure 7.3, which show that for at least 70% of the queries, NEUROSEQRET outperforms or fares competitively with these baselines across all datasets."
        },
        {
            "heading": "7.5.5 Runtime Analysis",
            "text": "Next, we calculate the run-time performance of NEUROSEQRET. With this experiment, our goal is to determine if the training times of NEUROSEQRET are suitable for designing solutions for real-world problems. From the results in Table 7.8, we note that even for datasets with up to 60 million events, the training times are well within the feasible range for practical deployment."
        },
        {
            "heading": "132 Large-Scale Retrieval of Temporal Sequences",
            "text": ""
        },
        {
            "heading": "7.5.6 Query Length",
            "text": "We perform an additional experiment of sequence retrieval with varying query lengths. Specifically, we sample queries of lengths |Hq| \u223c Unif(10, 50) and |Hq| \u223c Unif(50, 100) and report the sequence retrieval results in Table 7.9 and Table 7.10 respectively. The results show that the performance of all models deteriorates significantly as we reduce the length of query sequences. They also show that even with smaller query lengths, CROSSATTN-NEUROSEQRET significantly outperforms the other state-of-the-art baseline Rank-THP."
        },
        {
            "heading": "7.5.7 Qualitative Analysis",
            "text": "To get deeper insights into the working of our model, we perform a qualitative analysis between a query sequence from the dataset and the sequence retrieved by NEUROSEQRET. More specifically, we aim to understand the similar patterns between query and corpus sequences that NEUROSEQRET searches for in the corpus and plot the query sequence and the corresponding top-ranked relevant corpus sequence retrieved by NEUROSEQRET. The results across all datasets in Figure 7.4 show that the inter-arrival times of the CTES retrieved by NEUROSEQRET closely matches the query inter-arrival times."
        },
        {
            "heading": "7.6 Conclusion",
            "text": "In this chapter, we proposed a novel supervised continuous-time event sequence retrieval system called NEUROSEQRET using neural MTPP models. To achieve efficient retrieval over a very large corpus of sequences, we also propose a trainable hash-coding of corpus sequences which can be used to narrow down the number of sequences to be considered for similarity score computation. Our experiments with real-world datasets from a diverse range of domains show that our retrieval model is more effective than several baselines. Our work opens several avenues for future work, including the design of generative models for relevance sequences and counterfactual explanations for relevance label predictions."
        },
        {
            "heading": "Large-Scale Retrieval of Temporal Sequences 133",
            "text": "134 Large-Scale Retrieval of Temporal Sequences\nChapter 8"
        },
        {
            "heading": "Modeling Human Action Sequences",
            "text": ""
        },
        {
            "heading": "8.1 Introduction",
            "text": "A majority of the data generated via human activities such as running, playing basketball, cooking, etc., can be represented as a sequence of actions over a continuous time. These actions denote a step taken by a user towards achieving a certain goal and vary in their start and completion times, depending on the user and the surrounding environment [107, 145, 146]. Therefore, unlike synthetic time series, these continuous-time action sequences (CTAS) can vary significantly even if they consist of the same set of actions. For e.g., one person making omelets may take a longer time to cook eggs while another may prefer to cook for a short time1; or in a football game, Xavi may make a faster pass than Pirlo, even though the goals and the sequence of actions are the same. In addition, modeling the dynamics of CTAS becomes increasingly challenging due to the limited ability of the current neural frameworks, recurrent or self-attention-based, in capturing the continuous nature of action times [99, 206]. This situation is further exacerbated due to the large variance in action-times and types. Therefore, the problem of modeling a CTAS has been overlooked by the past literature.\nIn recent years, neural marked temporal point processes (MTPP) have shown significant promise in modeling a variety of continuous-time sequences in healthcare [176, 177], finance [7, 243], education [233], and social networks [147, 246, 251]. However, standard MTPP have a limited modeling ability for CTAS as: (i) they assume a homogeneity among sequences, i.e., they cannot distinguish between two sequences of similar actions but with different time duration; (ii) in a CTAS, an action may finish before the start of the next action and thus, to model this empty time interval an MTPP must introduce a new action type, e.g., NULL or end-action, which may\n1https://bit.ly/3F5aEwX (Accessed October 2022)\n135"
        },
        {
            "heading": "136 Modeling Human Action Sequences",
            "text": "lead to an unwarranted increase in the types of actions to be modeled; and (iii) they cannot encapsulate the additional features associated with an action, for e.g., the minimum time for completion, necessary previous actions, or can be extended to sequence generation."
        },
        {
            "heading": "8.1.1 Our Contribution",
            "text": "In this chapter, we present PROACTIVE (Point Process flows for Activity Sequences), a normalizing flow-based neural MTPP framework, designed specifically to model the dynamics of a CTAS [74]. Specifically, PROACTIVE addresses three challenging problems \u2013 (i) action prediction, (ii) goal detection, and (iii) the first-of-its-kind task of end-to-end sequence generation. We learn the distribution of actions in a CTAS using temporal normalizing flows (NF) [144, 175] conditioned on the dynamics of the sequence as well as the action features (e.g., minimum completion time, etc.). Such a flow-based formulation provides PROACTIVE flexibility over other similar frameworks [184, 186] to better model the inter-action dynamics within and across the sequences. Moreover, our model is designed for early detection of the sequence goal, i.e., identifying the result of a CTAS without traversing the complete sequence. We achieve this by using a time-bounded optimization procedure, i.e., by incrementally increasing the probability of identifying the true goal via a margin-based and a weighted factor-based learning [91, 139]. Such an optimization procedure allows PROACTIVE to model the goal-action hierarchy within a sequence, i.e., the necessary set of actions in a CTAS towards achieving a particular goal, and simultaneously, the order of actions in CTAS with similar goals.\nTo the best of our knowledge, in this paper, we present the first-ever application of MTPP via end-to-end action sequence generation. Specifically, given the resultant goal, PROACTIVE can generate a CTAS with the necessary set of actions and their occurrence times. Such a novel ability for MTPP models can reinforce their usage in applications related to bio-signals [79], sensor-data [5], etc., and overcome the modeling challenge due to scarcity of activity data [43, 72, 137]. Buoyed by the success of attention models in sequential applications [206], we use a self-attention architecture in PROACTIVE to model the inter-action influences in a CTAS. In summary, the key contributions we make in this paper via PROACTIVE are:\n\u2022 We propose PROACTIVE, a novel temporal flow-based MTPP framework designed specifically for modeling human activities with a time-bounded optimization framework for early detection of CTAS goal. \u2022 Our normalizing flow-based modeling framework incorporates the sequence and individual action dynamics along with the action-goal hierarchy. Thus, PROACTIVE introduces the first-of-its-kind MTPP application of end-to-end action CTAS generation with just the sequence-goal as input."
        },
        {
            "heading": "Modeling Human Action Sequences 137",
            "text": "\u2022 Finally, we empirically show that PROACTIVE outperforms the state-of-the-art models for all three tasks \u2013 action prediction, goal detection, and sequence generation."
        },
        {
            "heading": "8.2 Preliminaries",
            "text": "In this section, we present a background on activity prediction and the problems addressed in this chapter."
        },
        {
            "heading": "8.2.1 Activity Prediction",
            "text": "Activity modeling in videos is a widely used application with recent approaches focusing on frame-based prediction. Lan et al. [110] predicts the future actions via hierarchical representations of short clips, Mahmud et al. [140] jointly predicts future activity ad the starting time by capturing different sequence features, and a similar procedure is adopted by [52] that predicts the action categories of a sequence of future activities as well as their starting and ending time. Ma et al. [139] propose a method for early classification of a sequence of frames extracted from a video by maximizing the margin-based loss between the correct and the incorrect categories, however, it is limited to visual data and cannot incorporate the action times. This limits its ability for use in CTAS, and sequence generation. A recent approach [145] proposed to model the dynamics of action sequences using a variational auto-encoder built on top of a temporal point process. We consider their work as most relevant to PROACTIVE as it also addressed the problem of CTAS modeling. However, as shown in our experiments PROACTIVE was able to easily outperform it across all metrics. This could be attributed to the limited modeling capacity of VAE over normalizing flows. Moreover, their sampling procedure could not be extended to sequence generation. Therefore, in contrast to the past literature, PROACTIVE is the first application of MTPP models for CTAS modeling and end-to-end sequence generation."
        },
        {
            "heading": "8.2.2 Problem Formulation",
            "text": "As mentioned earlier, we represent an activity via a continuous-time action sequence, i.e., a series of actions undertaken by users and their corresponding time of occurrences. We derive each CTAS from annotated frames of videos consisting of individuals performing certain activities. Specifically, for every video, we have a sequence of activity labels being performed in the video along with timestamps for each activity. Therefore, each CTAS used in our experiments is derived from these sequences of a video. Formally, we provide a detailed description of a CTAS in Definition 2.\nDefinition 2 (Continuous Time Action Sequence). We define a continuous-time action sequence (CTAS) as a series of action events taken by a user to achieve a particular goal. Specifically,"
        },
        {
            "heading": "138 Modeling Human Action Sequences",
            "text": "we represent a CTAS as an MTPP Sk = {ei = (ci, ti)|i \u2208 [k], ti < ti+1}, where ti \u2208 R+ is the start time of the action, ci \u2208 C is the discrete category or mark of the i-th action, and C is the set of all categories. Each CTAS has an associated result, g \u2208 G, that signifies the goal of the CTAS. Here, G denotes the set of all possible sequence goals.\nTo highlight the relationship between sequence goal and actions, consider the example of a CTAS with the goal of \u2018making-coffee\u2019, would comprise of actions \u2013 \u2018take-a-cup\u2019, \u2018pour-milk\u2019, \u2018add-coffee-powder\u2019, \u2018add-sugar\u2019, and \u2018stir\u2019 \u2013 at different time intervals. Given the aforementioned definitions, we formulate the tasks of next action, sequence goal prediction, and sequence generation as:\nInput. A CTAS of all actions, Sk, consisting of categories and times of different actions that lead to a goal g.\nOutput. A probabilistic prediction model with three distinct tasks \u2013 (i) to estimate the likelihood of the next action ek+1 along with the action category and occurrence time; (ii) to predict the goal of the CTAS being modeled, i.e., g\u0302; and (iii) a generative model to sample a sequence of actions, S\u0302 given the true sequence goal, g."
        },
        {
            "heading": "8.3 PROACTIVE Model",
            "text": "In this section, we first present a high-level overview of the PROACTIVE model and then describe the neural parameterization of each component in detail. Lastly, we provide a detailed description of its optimization and sequence generation procedure."
        },
        {
            "heading": "8.3.1 High Level Overview",
            "text": "We use an MTPP denoted by p\u03b8(\u00b7), to learn the generative mechanism of a continuous-time action sequence. Moreover, we design the sequence modeling framework of p\u03b8(\u00b7) using a self-attention-based encoder-decoder model [206]. Specifically, we embed the actions in a CTAS, i.e., Sk, to a vector embedding, denoted by sk, using a weighted aggregation of all past actions. Therefore, sk signifies a compact neural representation of the sequence history, i.e., all actions till the k-th index and their marks and occurrence times. Recent research [243, 251] has shown that an attention-based modeling choice can better capture the long-term dependencies as compared to RNN-based MTPP models [45, 147, 162, 186]. A detailed description of the embedding procedure is given in Section 8.3.2.\nWe use our MTPP p\u03b8(\u00b7) to estimate the generative model for the (k + 1)-th action conditioned"
        },
        {
            "heading": "Modeling Human Action Sequences 139",
            "text": "on the past, i.e., p(ek+1) as:\np\u03b8(ek+1|sk) = P\u03b8(ck+1|sk) \u00b7 \u03c1\u03b8(\u2206t,k+1|sk), (8.1)\nwhere P\u03b8(\u00b7) and \u03c1\u03b8(\u00b7) denote the probability distribution of marks and the density function for inter-action arrival times, respectively. Note that both the functions are conditioned on sk and thus PROACTIVE requires a joint optimizing procedure for both \u2013 action time and mark prediction. Next, we describe the mechanism used in PROACTIVE to predict the next action and goal detection in a CTAS.\nNext Action Prediction. We determine the most probable mark and time of the next action, using p\u03b8(\u00b7) via standard sampling techniques over P\u03b8(\u00b7) and \u03c1\u03b8(\u00b7) respectively [45, 186].\ne\u0302k+1 \u223c p\u03b8(ek+1|sk), (8.2)\nIn addition, to keep the history embedding up-to-date with all past actions, we iteratively update sk to sk+1 by incorporating the details of action ek+1.\nGoal Detection. Since the history embedding, sk, represents an aggregation of all past actions in a sequence, it can also be used to capture the influences between actions and thus, can be extended to detect the goal of the CTAS. Specifically, to detect the CTAS goal, we use a nonlinear transformation over sk as:\ng\u0302 \u223c Pg\u2032\u2208G(\u03a6(sk)), (8.3)\nwhere P\u2022 denotes the distribution over all sequence goals and \u03a6(\u00b7) denotes the transformation via a fully-connected MLP layer."
        },
        {
            "heading": "8.3.2 Neural Parameterization",
            "text": "Here, we present a detailed description of the architecture of our MTPP, p\u03b8(\u00b7), and the optimization procedure in PROACTIVE. Specifically, we realize p\u03b8(\u00b7) via a three-layer architecture:\n(1) Input Layer As mentioned in Section 8.2, each action ei \u2208 Sk is represented by a mark ci and time ti. Therefore, we embed each action as a combination of all these features as:\nyi = wy,cci +wy,tti +wy,\u2206\u2206t,i + by, (8.4)\nwhere w\u2022,\u2022, b\u2022 are trainable parameters and yi \u2208 RD denotes the vector embedding for the action ei respectively. In other sections as well, we denote weight and bias as w\u2022,\u2022 and b\u2022,\u2022 respectively."
        },
        {
            "heading": "140 Modeling Human Action Sequences",
            "text": "(2) Self-Attention Layer We use a masked self-attention layer to embed the past actions to sk and to interpret the influence between the past and the future actions. In detail, we follow the standard attention procedure [206] and first add a trainable positional encoding, pi, to the action embedding, i.e., yi \u2190 yi + pi. Such trainable encodings are shown to be more scalable and robust for long sequence lengths as compared to those based on a fixed function [99, 115]. Later, to calculate an attentive aggregation of all actions in the past, we perform three independent linear transformations on the action representation to get the query, key, and value embeddings, i.e.,\nqi = W Qyi, ki = W Kyi, vi = W V yi, (8.5)\nwhere q\u2022,k\u2022,v\u2022 denote the query, key, and value vectors, respectively. Following standard self-attention model, we representWQ,WK andW V as trainable Query, Key, and Value matrices respectively. Finally, we compute sk conditioned on the history as:\nsk = k\u2211 i=1\nexp ( q>k ki/ \u221a D )\n\u2211k i\u2032=1 exp ( q>k ki\u2032/ \u221a D )vi, (8.6)\nwhere D denotes the number of hidden dimensions. Here, we compute the attention weights via a softmax over the interactions between the query and key embeddings of each action in the sequence and perform a weighted sum of the value embeddings. Now, given the representation sk, we use the attention mechanism in Eqn. (8.6) and apply a feed-forward neural network to incorporate the necessary non-linearity to the model as:\nsk \u2190 k\u2211 i=1 [ ws,m RELU(si ws,n + bs,n) + bs,m ] ,\nwhere ws,m, bs,m and ws,n, bs,n are trainable parameters of the outer and inner layer of the point-wise feed-forward layer. To support faster convergence and training stability, we employ (i) layer normalization; (ii) stacking multiple self-attention blocks; and (iii) multi-head attention. Since these are standard techniques [6, 206], we omit their mathematical descriptions. (3) Output Layer At every index k, PROACTIVE outputs the next action and the most probable goal of the CTAS. We present the prediction procedure for each of them as follows: Action Prediction: We use the output of the self-attention layer, sk to estimate the mark distribution and time density of the next event, i.e., P\u03b8(ek+1) and \u03c1\u03b8(ek+1) respectively."
        },
        {
            "heading": "Modeling Human Action Sequences 141",
            "text": "Specifically, we model the P\u03b8(\u00b7) as a softmax over all other marks as:\nP\u03b8(ck+1) = exp\n( w>c,ssi + bc,s )\u2211|C| c\u2032=1 exp ( w>c\u2032,ssi + bc\u2032,s\n) , (8.7) where, w\u2022,\u2022 and b\u2022,\u2022 are trainable parameters.\nIn contrast to standard MTPP approaches that rely on an intensity-based model [45, 147, 243, 251], we capture the inter-action arrival times via a temporal normalizing flow (NF). In detail, we use a lognormal flow to model the temporal density \u03c1\u03b8(\u2206t,k+1). Moreover, standard flowbased approaches [144, 186] utilize a common NF for all events in a sequence, i.e., the arrival times of each event are determined from a single or mixture of flows trained on all sequences. We highlight that such an assumption restricts the ability to model the dynamics of a CTAS, as unlike standard events, an action has three distinguishable characteristics \u2013 (i) every action requires a minimum time for completion; (ii) the time taken by a user to complete an action would be similar to the times of another user; and (iii) similar actions require similar times to complete. For example, the time taken to complete the action \u2018add-coffee\u2019 would require a certain minimum time of completion, and these times would be similar for all users. Intuitively, the time for completing the action \u2018add-coffee\u2019 would be similar to those for the action \u2018addsugar\u2019.\nTo incorporate these features in PROACTIVE, we identify actions with similar completion times and model them via independent temporal flows. Specifically, we cluster all actions ci \u2208 C into M non-overlapping clusters based on the mean of their times of completion, and for each cluster, we define a trainable embedding zr \u2208 RD \u2200r \u2208 {1, \u00b7 \u00b7 \u00b7 ,M}. Later, we sample the start-time of the future action by conditioning our temporal flows on the cluster of the current action as:\n\u2206\u0302t,k+1 \u223c LOGNORMAL ( \u00b5k,\u03c3 2 k ) , (8.8)\nwhere, [\u00b5k,\u03c32k], denote the mean and variance of the lognormal temporal flow and are calculated via the sequence embedding and the cluster embedding as:\n\u00b5k = M\u2211 r=1 R(ek, r) ( w\u00b5 (sk zc,i) + b\u00b5 ) , (8.9) \u03c32k = M\u2211 r=1 R(ek, r) ( w\u03c3 (sk zc,i) + b\u03c3 ) , (8.10)\nwherew\u2022, b\u2022 are trainable parameters,R(ek, r) is an indicator function that determines if event ek belongs to the cluster r and zr denotes the corresponding cluster embedding. Such a cluster-"
        },
        {
            "heading": "142 Modeling Human Action Sequences",
            "text": "based formulation facilitates the ability of the model to assign similar completion times for events in the same cluster. To calculate the time of the next action, we add the sampled time difference to the time of the previous action ek, i.e.,\nt\u0302k+1 = tk + \u2206\u0302t,k+1 (8.11)\nwhere, t\u0302k+1 denotes the predicted time for the action ek+1.\nGoal Detection: In contrast to other MTPP approaches [45, 147, 186, 243, 251], an important feature of PROACTIVE is identifying the goal of a sequence, i.e., a hierarchy on top of the actions in a sequence, based on the past sequence dynamics. To determine the goal of a CTAS, we utilize the history embedding sk as it encodes the inter-action relationships of all actions in the past. Specifically, we use a non-linear transformation via a feed-forward network, denoted as \u03a6(\u00b7) over sk and apply a softmax over all possible goals.\n\u03a6(sk) = RELU(w\u03a6,ssk + b\u03a6,s), (8.12)\nwhere, w\u2022,\u2022, b\u2022,\u2022 are trainable parameters. We sample the most probable goal as in Eqn. (8.3).\nWe highlight that we predict the CTAS goal at each interval, though a CTAS has only one goal. This is to facilitate early goal detection in comparison to detecting the goal after traversing the entire CTAS. More details are given in Section 8.3.3 and Section 8.3.4."
        },
        {
            "heading": "8.3.3 Early Goal Detection and Action Hierarchy",
            "text": "Here, we highlight the two salient features of PROACTIVE\u2013 early goal detection and modeling the goal-action hierarchy.\nEarly Goal Detection. Early detection of sequence goals has many applications ranging from robotics to vision [91, 180]. To facilitate early detection of the goal of a CTAS in PROACTIVE, we devise a ranking loss that forces the model to predict a non-decreasing detection score for the correct goal category. Specifically, the detection score of the correct goal at the k-th index of the sequence, denoted by pk(g|sk,\u03a6), must be more than the scores assigned the correct goal in the past. Formally, we define the ranking loss as:\nLk,g = max ( 0, p\u2217k(g)\u2212 pk(g|sk,\u03a6) ) , (8.13)\nwhere p\u2217k(g) denotes the maximum probability score given to the correct goal in all past predic-\nModeling Human Action Sequences 143\ntions. p\u2217k(g) = max\nj\u2208{1,k\u22121} pj(g|sj,\u03a6), (8.14)\nwhere pk(g) denotes the probability score for the correct goal at index k. Intuitively, the ranking lossLk,g would penalize the model for predicting a smaller detection score for the correct CTAS goal than any previous detection score for the same goal.\nAction Hierarchy. Standard MTPP approaches assume the category of marks as independent discrete variables, i.e., the probability of an upcoming mark is calculated independently [45, 147, 243, 251]. Such an assumption restricts the predictive ability while modeling CTAS, as in the latter case, there exists a hierarchy between goals and actions that lead to the specific goal. Specifically, actions that lead to a common goal may have similar dynamics and it is also essential to model the relationships between the actions of different CTAS with a common goal. We incorporate this hierarchy in PROACTIVE along with our next action prediction via an action-based ranking loss. In detail, we devise a loss function similar to Eqn. 8.13 where we restrict the model to assign non-decreasing probabilities to all actions leading to the goal of CTAS under scrutiny.\nLk,c = \u2211 c\u2032\u2208C\u2217g max ( 0, p\u2217k(c \u2032)\u2212 pk(c\u2032|sk) ) , (8.15)\nwhere C\u2217g , pk(c\u2032|sk) denote a set of all actions in CTAS with the goal g and the probability score for the action c\u2032 \u2208 C\u2217g at index k respectively. Here, p\u2217k(c\u2032) denotes the maximum probability score given to action c\u2032 in all past predictions and is calculated similar to Eqn. 8.14. We regard Lk,g and Lk,c as margin losses, as they aim to increase the difference between two prediction probabilities."
        },
        {
            "heading": "8.3.4 Optimization",
            "text": "We optimize the trainable parameters in PROACTIVE, i.e., the weight and bias tensors (w\u2022,\u2022 and b\u2022,\u2022) for our MTPP p\u03b8(\u00b7), using a two channels of training consisting of action and goal prediction. Specifically, to optimize the ability of PROACTIVE for predicting the next action, we maximize the joint likelihood for the next action and the lognormal density distribution of the temporal flows.\nL = |S|\u2211 k=1 log ( P\u03b8(ck+1|sk) \u00b7 \u03c1\u03b8(\u2206t,k+1|sk) ) , (8.16)\nwhere L denotes the joint likelihood, which we represent as the sum of the likelihoods for all CTAS. In addition to action prediction, we optimize the PROACTIVE parameters for early goal detection via a temporally weighted cross entropy (CE) loss over all sequence goals. Specifi-"
        },
        {
            "heading": "144 Modeling Human Action Sequences",
            "text": "cally, we follow a popular reinforcement recipe of using a time-varying discount factor over the prediction loss as:\nLg = |S|\u2211 k=1 \u03b3k \u00b7 LCE ( pk(g|sk) ) , (8.17)\nwhere \u03b3 \u2208 [0, 1],LCE ( pk(g|sk) ) denote the decaying factor and a standard softmax-crossentropy loss respectively. Such a recipe is used exhaustively for faster convergence of reinforcement learning models [195]. Here, the discount factor penalizes the model for taking longer times to detect the CTAS goal by decreasing the gradient updates to the loss.\nMargin Loss. In addition, we minimize the margin losses given in Section 8.3.3 with the current optimization procedure. Specifically, we minimize the following loss:\nLm = |S|\u2211 k=1 Lk,g + Lk,c, (8.18)\nwhere Lk,g and Lk,c are margin losses defined in Eqn. 8.13 and Eqn. 8.15 respectively. We learn the parameters of PROACTIVE using an Adam [100] optimizer for both likelihood and prediction losses."
        },
        {
            "heading": "8.3.5 Sequence Generation",
            "text": "An important ability of PROACTIVE is the end-to-end generation of action sequences. Specifically, given the CTAS goal as input, we can generate a most probable sequence of actions that may lead to that specific goal. Such a feature has a range of applications from sports analytics [146], forecasting [41], identifying the duration of an activity [145], etc.\nA standard approach for training a sequence generator is to sample future actions in a sequence and then compare them with the true actions [237]. However, such a procedure has multiple drawbacks as it is susceptible to noises during training and deteriorates the scalability of the model. Moreover, we highlight that such sampling-based training cannot be applied to a selfattention-based model as it requires a fixed-sized sequence as input [206]. Therefore, we resort to a two-step generation procedure that is defined below:\n1 Pre-Training: The first step requires training all PROACTIVE parameters for action prediction and goal detection. This step is necessary to model the relationships between actions and goals and we represent the set of optimized parameters as \u03b8\u2217 and the corresponding MTPP as p\u03b8\u2217(\u00b7) respectively. 2 Iterative Sampling: We iteratively sample events and update parameters via our trained"
        },
        {
            "heading": "Modeling Human Action Sequences 145",
            "text": "Algorithm 3: Sequence Generation with PROACTIVE 1 Input: g: Goal of CTAS 2 e1: First Action 3 p\u03b8\u2217(\u00b7): Trained MTPP 4 Output: S\u0302: Generated CTAS S1 \u2190 e1 5 k = 1 6 while k < max_len do 7 Sample the mark of next action: c\u0302k+1 \u223c P\u03b8\u2217(sk) 8 Sample the time of next action: t\u0302k+1 \u223c \u03c1\u03b8\u2217(sk) 9 Add to CTAS: Sk+1 \u2190 Sk + ek+1\n10 Update the MTPP parameters sk+1 \u2190 p(sk, ek+1) 11 Calculate most probable goal: g\u0302k = max\u2200g\u2032 ( pk(g \u2032|sk) ) 12 if g\u0302i! = g or c\u0302k+1 == < EOS > then 13 Add EOS mark: S\u0302 \u2190 Sk+1 +< EOS > 14 Exit the sampling procedure: BREAK 15 Increment iteration: k \u2190 k + 1 16 Return generated CTAS: return S\u0302\nMTPP till the model predicts the correct goal for CTAS or we encounter an <EOS> action. Specifically, using p\u03b8\u2217(\u00b7) and the first real action (e1) as input, we calculate the detection score for the correct goal, i.e., p1(g|sk) and while its value is highest among all probable goals, we sample the mark and time of next action using Eqn. 8.7 and Eqn. 8.8 respectively.\nSuch a generation procedure harnesses the fast sampling of temporal normalizing flows and simultaneously is conditioned on the action and goal relationships. A detailed pseudo-code of the sequence generation procedure used in PROACTIVE is given in Algorithm 3."
        },
        {
            "heading": "8.4 Experiments",
            "text": "In this section, we present the experimental setup and the empirical results to validate the efficacy of PROACTIVE. Through our experiments, we aim to answer the following research questions:\nRQ1 What is the action-mark and time prediction performance of PROACTIVE in comparison to the state-of-the-art baselines? RQ2 How accurately and quickly can PROACTIVE identify the goal of an activity sequence? RQ3 How effectively can PROACTIVE generate an action sequence?"
        },
        {
            "heading": "146 Modeling Human Action Sequences",
            "text": ""
        },
        {
            "heading": "8.4.1 Datasets",
            "text": "To evaluate PROACTIVE, we need timestamped action sequences and their goals. Therefore, we derive CTAS from three activity modeling datasets sourced from different real-world applications \u2013 cooking, sports, and collective activity. The datasets vary significantly in terms of origin, sparsity, and sequence lengths. We highlight the details of each of these datasets below:\n\u2022 Breakfast [107]. This dataset contains CTAS derived from 1712 videos of different people preparing breakfast. The actions in a CTAS and sequence goals can be classified into 48 and 9 classes, respectively. These actions are performed by 52 different individuals in 18 different kitchens. \u2022 Multi-THUMOS [234]. A sports activity dataset that is designed for action recognition in videos. We derive the CTAS using 400 videos of individuals involved in different sports such as discus throw, baseball, etc. The actions and goals can be classified into 65 and 9 classes, respectively and on average, there are 10.5 action class labels per video. \u2022 Activity-Net [87]. This dataset comprises activity categories collected from 591 YouTube videos with a total of 49 action labels and 14 goals.\nWe highlight that in Activity-Net, many of the videos are shot by amateurs in many uncontrolled environments, the variances within the CTAS of the same goal are often large, and the lengths of CTAS vary and are often long and complex."
        },
        {
            "heading": "8.4.2 Baselines",
            "text": "We compare the action prediction performance of PROACTIVE with the following state-of-theart methods:\n\u2022 NHP [147]: Models an MTPP using continuous-time LSTMs for capturing the temporal evolution of sequences. \u2022 RMTPP [45]: A recurrent neural network that models time differences to learn a representation of past events. \u2022 AVAE [145]: A variational autoencoder-based MTPP framework designed specifically for activities in a sequence. \u2022 SAHP [243]: A self-attention model to learn the temporal dynamics using an aggregation of historical events. \u2022 THP [251]: Extends the transformer model [206] to include the conditional intensity of event arrival and the inter-mark influences.\nWe omit comparison with other continuous-time models [80, 162, 186, 224, 225] as they have already been outperformed by these approaches."
        },
        {
            "heading": "Modeling Human Action Sequences 147",
            "text": ""
        },
        {
            "heading": "8.4.3 Evaluation Criteria",
            "text": "Given the datasetD ofN action sequences, we split them into training and test sets based on the goal of the sequence. Specifically, for each goal g \u2208 G, we consider 80% of the sequences as the training set and the other last 20% as the test set. We evaluate PROACTIVE and all baselines on the test set in terms of (i) mean absolute error (MAE) of predicted times of action, and (ii) action prediction accuracy (APA) described as:\nMAE = 1 |S| \u2211 ei\u2208S [|ti \u2212 t\u0302i|], APA = 1 |S| \u2211 ei\u2208S #(ci = c\u0302i), (8.19)\nwhere, t\u0302i and c\u0302i are the predicted time and type the i-th action in test set. Moreover, we follow a similar protocol to evaluate the sequence generation ability of PROACTIVE and other models. For goal prediction, we report the results in terms of accuracy (ratio) calculated across all sequences. We calculate confidence intervals across five independent runs."
        },
        {
            "heading": "8.4.4 Experimental Setup",
            "text": "System Configuration. All our experiments were done on a server running Ubuntu 16.04. CPU: Intel(R) Xeon(R) Gold 5118 CPU @ 2.30GHz , RAM: 125GB and GPU: NVIDIA Tesla T4 16GB DDR6.\nParameter Settings. For our experiments, we set D = 16, M = 8, \u03b3 = 0.9 and weigh the margin loss Lm by 0.1. In addition, we set a l2 regularizer over the parameters with coefficient value 0.001."
        },
        {
            "heading": "148 Modeling Human Action Sequences",
            "text": ""
        },
        {
            "heading": "8.4.5 Action Prediction Performance",
            "text": "To address RQ1, we report on the performance of action prediction of different methods in terms of APA and MAE in Table 8.1 and Table 8.2, respectively. In addition, we include two variants of PROACTIVE\u2013 (i) PROACT-c, which represents our model without the goal-action hierarchy loss and cluster-based flows, and (ii) PROACT-t, which represents our model without cluster-based flows. From the results, we note the following:\n\u2022 PROACTIVE consistently yields the best prediction performance on all the datasets. In particular, it improves over the strongest baselines by 8-27% for time prediction and by 2- 7% for action prediction. These results signify the drawbacks of using standard sequence approaches for modeling a temporal action sequence. \u2022 RMTPP [45] is the second-best performer in terms of MAE of time prediction in almost all the datasets. We also note that for Activity-Netdataset, THP [251] outperforms RMTPP for action category prediction. However, PROACTIVE still significantly outperforms these models across all metrics."
        },
        {
            "heading": "Modeling Human Action Sequences 149",
            "text": "\u2022 Neural MTPP methods that deploy a self-attention for modeling the distribution of action \u2013 namely THP, SAHP, and PROACTIVE, achieve better performance in terms of category prediction. \u2022 Despite AVAE [145] being a sequence model designed specifically for activity sequences, other neural methods that incorporate complex structures using self-attention or normalizing flows easily outperform it.\nTo sum up, our empirical analysis suggests that PROACTIVE can better model the underlying dynamics of a CTAS as compared to all other baseline models. Moreover, the performance gain over PROACT-c and PROACT-t highlights the need for modeling action hierarchy and\n150 Modeling Human Action Sequences\ncluster-based flows.\nQualitative Assessment. We also perform a qualitative analysis to highlight the ability of PROACTIVE to model the inter-arrival times for action prediction. Specifically, we plot the actual inter-action time differences and the time-difference predicted by PROACTIVE in Figure 8.1 for Multi-THUMOS and Activity-Net datasets. From the results, we note that the predicted inter-arrival times closely match with the true inter-arrival times, and PROACTIVE is even able to capture large time differences (peaks)."
        },
        {
            "heading": "8.4.6 Goal Prediction Performance",
            "text": "To address RQ2, we evaluate the goal detection performance of PROACTIVE along with other baselines. To highlight the early goal detection ability of our model, we report the results across different variants of the test set, i.e., with the initial 30% and 60% of the actions in the CTAS in terms of goal prediction accuracy (GPA). In addition, we introduce two novel baselines, LSTM-c, and LSTM-t, that detect the CTAS goal using just the types and the times of actions, respectively. We also compare with the two best-performing MTPP baselines \u2013 RMTPP and THP which we extend for the task of goal detection by a k-means clustering algorithm. In detail, we obtain the sequence embedding, say sk using the MTPP models and then cluster them into |G| clusters based on their cosine similarities and perform a maximum polling across each cluster, i.e., predict the most common goal for each cluster as the goal for all CTAS in the same cluster. In addition, we introduce two new variants of our model to analyze the benefits of early goal detection procedures in PROACTIVE\u2013 (i) PROACTIVE-m, which represents our model without the goal-based margin loss given in Eqn. 8.13 and (ii) PROACTIVE-\u03b3, is our model without the discount-factor weight in Eqn. 8.17. We also report the results for the complete model PROACTIVE.\nThe results for goal detection in Figure 8.2, show that the complete design of PROACTIVE achieves the best performance among all other models. We also note that the performance of MTPP-based models deteriorates significantly for this new task which shows the unilateral nature of the prediction prowess of MTPP models, unlike PROACTIVE. Interestingly, the variant of PROACTIVE without the margin loss PROACTIVE-m performs poorly as compared to the one without the discount factor, PROACTIVE-\u03b3. This could be attributed to better convergence guarantees with a margin-based loss over the latter. Finally, we observe that standard LSTM models are easily outperformed by our model, thus reinforcing the need for joint training of types and action times."
        },
        {
            "heading": "8.4.7 Sequence Generation Performance",
            "text": "To address RQ3, we evaluate the sequence generation ability of PROACTIVE. Specifically, we generate all the sequences in the test set by giving the true goal of the CTAS and the first action as input to the procedure described in Section 8.3.5. However, there may be differences in the lengths of the generated and true sequences, i.e., the length of generated sequences is usually greater than the true CTAS. Therefore, we compare the actions in the true sequence with the initial |S| generated actions. Such an evaluation procedure provides us the flexibility of comparing with other MTPP models such as RMTPP [45] and THP [251]. As these models cannot be used for end-to-end sequence generation, we alter their underlying model for forecasting future actions given the first action and then iteratively update and sample from the MTPP parameters. We report the results in terms of APA and MAE for action and time prediction in Figure 8.3 and Figure 8.4, respectively. The results show that PROACTIVE can better capture the generative dynamics of a CTAS in comparison to other MTPP models. We also note that the prediction performance deteriorates significantly in comparison to the results given in Tables 8.1 and 8.2. This could be attributed to the error that gets compounded in further predictions made by the model. Interestingly, the performance advantage that PROACTIVE has over THP and RMTPP is further widened during sequence generation."
        },
        {
            "heading": "152 Modeling Human Action Sequences",
            "text": "Length Comparison. Here, we report the results for the length comparison of the generated sequence and the true sequence. Specifically, we identify the count of instances where PROACTIVE was able to effectively capture the generative mechanism of a sequence as:\nCL = 1\nN \u2211 \u2200S #(|S| = |S\u0302|), (8.20)\nwhere CL denotes the Correct-Length ratio with values 0.21, 0.11, and 0.16 for datasets Breakfast, Multi-THUMOS, and Activity-Net respectively. On a coarse level, these results might seem substandard; however, given the difficulty associated with the problem of sequence generation using just the CTAS goal, we believe these values are satisfactory. Moreover, we believe that the sequence generation procedure of PROACTIVE opens up new frontiers for generating action sequences.\nScalability. For all datasets, the run-times for training PROACTIVE are within 1 hour and thus are within the practical range for deployment in real-world scenarios. These running times further reinforce our design choice of using a neural MTPP due to their faster learning and closed-form sampling [144, 186]."
        },
        {
            "heading": "8.5 Conclusion",
            "text": "Standard deep-learning models are not designed for modeling sequences of actions localized in continuous time. However, neural MTPP models overcome this drawback but have limited ability to model the events performed by a human. Therefore, we developed a novel point process flow-based architecture called PROACTIVE for modeling the dynamics of a CTAS. PROACTIVE solves the problems associated with action prediction and goal prediction, and for the first time, we extend MTPP for end-to-end CTAS generation. Our experiments on three large-scale, diverse datasets reveal that PROACTIVE can significantly improve over the stateof-the-art baselines across all metrics. Moreover, the results also reinforce the novel ability of PROACTIVE to generate a CTAS. We hope that such an application will open many horizons for using MTPP in a wide range of tasks.\nChapter 9"
        },
        {
            "heading": "Conclusion and Future Work",
            "text": "To conclude, traditional recommender systems have limited ability to model user-item interactions with changing dynamics. In this thesis, we present the research directions, possible data-related problems, and solutions for learning recommender systems that utilize user-item interactions in the form of continuous-time event sequences (CTES). This thesis has been divided into three parts based on the issues addressed and the type of solutions proposed. We also identify the sequence modeling prowess of our proposed models and design solutions for applications beyond recommender systems. Here, we summarize the contribution of each part of this thesis:\nIn Chapter 3, we address the problem of modeling missing events in temporal sequences. The presence of missing events can significantly deteriorate the quality of the CTES data and, consecutively, the performance of recommender systems trained on these datasets. The traditional sequence models assume that the underlying sequence is complete \u2013 an ideal setting. Thus, we proposed IMTPP, a novel MTPP model that learns the generative processes of observed events and missing events with missing events as latent random variables. Then, we devise an unsupervised training method to jointly learn the parameters of the observed and missing MTPP models using variational inference. IMTPP can effectively impute the missing data among the observed events, which in turn enhances its predictive prowess. Later, its enhanced version, IMTPP++, can impute and identify the optimal position of missing events in a sequence. However, one unaddressed aspect of the missing data problem is partially missing events i.e., events with either the time or mark missing. This is a challenging problem, as standard MTPP models are not designed to handle such sequences or functions in the absence of event times. In addition, the constrained optimization procedure in IMTPP++ can be improved by using Lagrangian multipliers. This would prevent the two-step procedure mentioned in IMTPP++\n153"
        },
        {
            "heading": "154 Conclusion and Future Work",
            "text": "while simultaneously facilitating faster convergence.\nIn the second part of the thesis, we address the problems associated with designing better sequence modeling frameworks for the application of spatial recommendations. Specifically, given the continuous-time mobility records of a user, we use external data or features to overcome the data scarcity in the region and better recommend better POI candidates to a user in the spatial network. This part is divided into three chapters based on the three different real-world applications: (i) top-k POI recommendation; (ii) sequential POI recommendation and check-in time prediction; and (iii) using smartphone usage to predict the mobility of a user.\nIn Chapter 4, we design solutions for the top-k POI recommendation and propose AXOLOTL (Automated cross Location-network Transfer Learning), a graph neural network-based framework that transfers mobility knowledge across regions via \u2013 (i) a novel meta-learning procedure derived from spatial as well as a social network, and (ii) a lightweight unsupervised cluster-based transfer technique. We also address the problems associated with an extremely data-scarce region and devise a suitable cluster-based alignment loss that enforces similar embeddings for communities of users and locations with similar dynamics. In future work, we plan to modify the transfer procedure of AXOLOTL to incorporate novel meta-learning approaches, namely ProtoMAML [197] and Reptile [158]. Additionally, we plan to experiment with approaches [25, 113, 236] that automatically identify the number of clusters within a region while simultaneously maintaining the scalability of our model.\nIn Chapter 5, we design solutions to overcome the problems due to data scarcity for developing sequential POI recommendation frameworks. Thus, we propose REFORMD (Reusable Flows for Mobility Data), a point process framework that learns the dynamics of user-specific check-in sequences in a region using normalizing flows to learn the inter-check-in time and geo-distributions. REFORMD achieved better performance in data-scarce regions without any prerequisite of overlapping users. In future work, we plan to explore the relationship between MTPP-based transfer for product recommendation and incorporate complex transfer techniques such as meta-learning [57].\nIn Chapter 6, we propose REVAMP (Relative position Vector for App-based Mobility Prediction. This sequential POI recommendation approach uses smartphone app-usage logs to identify the mobility preferences of a user while simultaneously maintaining the user\u2019s privacy. Inspired by the success of relative positional encodings and self-attention models, REVAMP uses relative and absolute positional encodings determined by the inter-check-in variances in the smartphone app category, POI category, and time over the check-ins in the sequence. Through REVAMP, we aim to understand the relationship between the smartphone usage of users and their mobil-"
        },
        {
            "heading": "Conclusion and Future Work 155",
            "text": "ity preferences. A drawback of the current formulation is that it requires the entire dataset to train its parameters. However, modern privacy-conscious techniques use a federated learning approach to train the model parameters with decentralized data. In future work, we plan to expand REVAMP using such an efficient architecture.\nIn the third part, we highlight that the sequence modeling ability of the proposed models can have applications beyond recommender systems. Thus, we extend these models for two novel applications \u2013 (i) efficient retrieval of event sequences; and (ii) human activity prediction. In Chapter 7, we propose NEUROSEQRET, a novel MTPP model to retrieve and rank a relevant set of continuous-time event sequences for a given query sequence from a large corpus of sequences. NEUROSEQRET has many advantages over the previous models, such as a trainable unwarping function that makes it easier to compare query-corpus sequence pairs, especially when they have individually different attributes. Moreover, it has a learnable hashing for efficient retrieval of sequences with millions of events. Currently, in high-impact applications like healthcare, our method requires additional care since an incorrect prediction made by our model would have an adverse impact. In the end, one may consider a human-in-loop retrieval system that will mitigate such risk via human intervention. Moreover, event sequences from some domains, e.g., mobility records, can contain user-specific data and their use may lead to privacy violations. To that aim, it would be interesting to consider designing a privacy-preserving retrieval system for continuous-time event sequences. Empirically we show that our hash-codebased retrieval is Pareto-efficient, offering a sweet spot in trading off the computational cost and the retrieval performance.\nIn Chapter 8, we address the problem of modeling action sequences and first highlight the dissimilarities between standard time series and human-performed action sequences. To efficiently model these action sequences, we propose PROACTIVE (Point Process flows for Activity Sequences), a neural MTPP framework for modeling the continuous-time distribution of actions in activity sequences while simultaneously addressing three real-world applications \u2013 next action prediction, sequence-goal prediction, and end-to-end sequence generation. PROACTIVE brings forward the first-of-its-kind application of MTPP models via sequence generation, i.e., given the goal of an action sequence, it can generate a sequence with actions towards achieving that particular goal. In future work, we plan to incorporate a generative adversarial network [65, 224] with action sampling and train it simultaneously with the MTPP.\nTherefore, this thesis addresses some critical limitations of the current approaches for modeling continuous-time event sequences. To achieve this, we proposed robust and scalable neural network-based models that show their prowess in their respective applications. In addition, we design MTPP frameworks for two novel and high-impact applications. However, the research"
        },
        {
            "heading": "156 Conclusion and Future Work",
            "text": "directions addressed in this have led to several open questions and future works. Here, we list out a few of them that were unaddressed in this thesis and, to the best of our knowledge, have been overlooked by the past literature as well.\n\u2022 Evaluating Time Prediction Performance. A significant advantage of the neural MTPP over traditional sequence modeling frameworks is their ability to effectively understand the time of a future event. To evaluate the time-prediction performance of an MTPP model, the current procedures utilize one of the two popular metrics \u2013 mean absolute error (MAE) or root mean squared error (RMSE) [45, 243, 251]. However, both of these metrics evaluate the time-prediction ability in an event-wise setting, i.e., perform a onevs-one matching between the predicted and the actual data and then take mean overall events. This metric provides limited insights into the generative ability of MTPP models and the ability to results for the entire sequence. An interesting future direction for this thesis will be to utilize sequence-to-sequence comparison metrics such as dynamic time warping (DTW) to evaluate the prediction performance, or if possible, use a trainable DTW [35] as a loss in the optimization procedure. Such an optimization procedure can give deeper insights into the generative and long-tail forecasting ability of MTPP models, which can be further used to design better MTPP frameworks. \u2022 Repetitiveness in Recommendations. Recent research has shown that the purchase records of users on an e-commerce platform may have repetitive patterns, i.e., there is a temporal dependency between the consecutive purchase of similar items [15]. Incorporating this dependency into a continuous-time model has been studied with traditional MTPP models. However, there has been limited research in exploring periodicity in a neural MTPP model. This is a challenging task as the neural MTPP models do not have an explicit influence function, and these influences are approximated by a neural layer. Thus, an interesting future work could be to incorporate the repetitive purchasing nature of a user into the mathematical framework of a neural MTPP. A possible solution could be to optimize the ability of the model to predict similar purchases in the next time window, i.e., the likelihood that a user will re-buy the same product in the near future. \u2022 Learning Neural MTPP at Scale. A drawback of using MTPP for modeling the dynamics of a CTES is their larger inference and sampling times for long sequences. This problem arises due to the inherently sequential nature of MTPP, which restricts their scalability even in the presence of highly-parallel modern hardware. However, recent research has shown that the conditional intensity function of an MTPP can be replaced by a normalizing flow based on triangular maps that allows parallel sampling and likelihood computation [187]. In a similar context, researchers have also decoupled the neural MTPP to gain better scalability, i.e., rather than a joint neural MTPP, they use deep re-"
        },
        {
            "heading": "Conclusion and Future Work 157",
            "text": "current neural networks to capture complex temporal dependency patterns, and the selfexcitation dynamics are modeled with a traditional Hawkes processes [200]. These techniques can be used to design scalable MTPP models, however, a detailed analysis of how these additions result in the overall improvement in scalability in the context of recommender systems has not been performed in the past. Thus, another interesting future work for this thesis will be a detailed study that compares the training and inference times of MTPP models while plugging-in several heuristics that will be a breakthrough in scaling these techniques and their applicability in large-scale recommender systems. \u2022 Human in the Loop. Lastly, we highlight the limited ability of sequence modeling frameworks to incorporate feedback given by users along with the training data. Incorporating additional feedback given by a user, such as a ranked list of desired items, is necessary for modern recommender systems as recent research has shown that the recommendations produced by a majority of ML models do not correlate with actual human preferences [55, 202]. This is a major drawback of the current models and highlights their limited practicability even when they consist of sophisticated ML techniques. Thus, a crucial problem that needs to be addressed will be training neural models over CTES data with additional aids from other domains, such as crowd-sourcing, with the goal of incorporating real human feedback on the ranked recommendations.\nTo summarize, in this thesis, we have designed solutions for a few real-world problems associated with recommender systems, and have discussed some open questions and directions for improvements.\n158 Conclusion and Future Work"
        },
        {
            "heading": "160 BIBLIOGRAPHY",
            "text": "[13] Luc Bauwens and Nikolaus Hautsch. 2009. Modelling Financial High Frequency Data Using Point Processes.\n[14] Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens, and Quoc V. Le. 2019. Attention Augmented Convolutional Networks. In ICCV.\n[15] Rahul Bhagat, Srevatsan Muralidharan, Alex Lobzhanidze, and Shankar Vishwanath. 2018. Buy it again: Modeling repeat purchase recommendations. In KDD.\n[16] Homanga Bharadhwaj. 2019. Meta-Learning for User Cold-Start Recommendation. In IJCNN.\n[17] Ivan Bilan and Benjamin Roth. 2018. Position-aware Self-attention with Relative Positional Encodings for Slot Filling. arXiv preprint arXiv:1807.03052 (2018).\n[18] Mathieu Blondel, Arthur Mensch, and Jean-Philippe Vert. 2021. Differentiable Divergences Between Time Series. In AISTATS.\n[19] Xingyu Cai, Tingyang Xu, Jinfeng Yi, Junzhou Huang, and Sanguthevar Rajasekaran. 2019. DTWNet: a dynamic time warping network. In NeurIPS.\n[20] Hancheng Cao, Zhilong Chen, Fengli Xu, Yong Li, and Vassilis Kostakos. 2018. Revisitation in Urban Space vs. Online: A Comparison across POIs, Websites, and Smartphone Apps. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 2, 4 (2018).\n[21] Wei Cao, Dong Wang, Jian Li, Hao Zhou, Lei Li, and Yitan Li. 2018. BRITS: Bidirectional Recurrent Imputation for Time Series. In NeurIPS.\n[22] Anirban Chakraborty, Debasis Ganguly, and Owen Conlan. 2020. Relevance Models for Multi-Contextual Appropriateness in Point-of-Interest Recommendation. In SIGIR.\n[23] Moses S Charikar. 2002. Similarity estimation techniques from rounding algorithms. In STOC.\n[24] Zhengping Che, Sanjay Purushotham, Kyunghyun Cho, David Sontag, and Yan Liu. 2018. Recurrent Neural Networks for Multivariate Time Series with Missing Values. Scientific reports 8, 1 (2018).\n[25] Ling Chen, Jiahui Xu, Binqing Wu, Yuntao Qian, Zhenhong Du, Yansheng Li, and Yongjun Zhang. 2021. Group-Aware Graph Neural Network for Nationwide City Air Quality Forecasting. arXiv preprint arXiv:2108.12238 (2021).\n[26] Yile Chen, Cheng Long, Gao Cong, and Chenliang Li. 2020. Context-aware Deep Model for Joint Mobility and Time Prediction. In WSDM.\n[27] Chen Cheng, Haiqin Yang, Michael R Lyu, and Irwin King. 2013. Where you like to go next: Successive point-of-interest recommendation. In IJCAI.\n[28] Eunjoon Cho, Seth A. Myers, and Jure Leskovec. 2011. Friendship and mobility: User movement in location-based social networks. In KDD.\nBIBLIOGRAPHY 161\n[29] Giannis Christoforidis, Pavlos Kefalas, Apostolos N. Papadopoulos, and Yannis Manolopoulos. 2021. RELINE: Point-of-Interest Recommendations Using Multiple Network Embeddings. Knowl. Inf. Syst. 63, 4 (2021).\n[30] Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron C Courville, and Yoshua Bengio. 2015. A recurrent latent variable model for sequential data. In NeurIPS.\n[31] Wikipedia Contributors. 2022. Damerau Levenshtein distance. Available At: https://en.wikipedia.org/wiki/Damerau_Levenshtein_distance.\n[32] Wikipedia Contributors. 2022. Demographics of the world. Available At: https://en.wikipedia.org/wiki/Demographics_of_the_world.\n[33] Wikipedia Contributors. 2022. Haversine formula. Available At: https://en.wikipedia.org/wiki/Haversine_formula.\n[34] Alice Coucke, Alaa Saade, Adrien Ball, Th\u00e9odore Bluche, Alexandre Caulier, David Leroy, Cl\u00e9ment Doumouro, Thibault Gisselbrecht, Francesco Caltagirone, Thibaut Lavril, et al. 2018. Snips voice platform: an embedded spoken language understanding system for private-by-design voice interfaces. arXiv preprint arXiv:1805.10190 (2018).\n[35] Marco Cuturi and Mathieu Blondel. 2017. Soft-dtw: a differentiable loss function for time-series. In ICML.\n[36] Daryl J Daley and David Vere-Jones. 2007. An introduction to the theory of point processes: volume II: general theory and structure. Springer Science & Business Media.\n[37] Nield David. 2017. All the Ways Your Smartphone and Its Apps Can Track You. Available At: https://gizmodo.com/all-the-ways-your-smartphone-and-its-apps-can-track-you-1821213704.\n[38] Abir De, Utkarsh Upadhyay, and Manuel Gomez-Rodriguez. 2019. Temporal point processes. Technical Report. Tech. Rep., Technical report, Saarland University.\n[39] Abir De, Isabel Valera, Niloy Ganguly, Sourangshu Bhattacharya, and Manuel Gomez-Rodriguez. 2016. Learning and Forecasting Opinion Dynamics in Social Networks. In NeurIPS.\n[40] Micha\u00ebl Defferrard, Xavier Bresson, and Pierre Vandergheynst. 2016. Convolutional neural networks on graphs with fast localized spectral filtering. In NeurIPS.\n[41] Prathamesh Deshpande, Kamlesh Marathe, Abir De, and Sunita Sarawagi. 2021. Long Horizon Forecasting With Temporal Point Processes. In WSDM.\n[42] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018).\n[43] Seth R Donahue, Li Jin, and Michael E Hahn. 2020. User Independent Estimations of Gait Events With Minimal Sensor Data. IEEE Journal of Biomedical and Health Informatics 25, 5 (2020).\n[44] Manqing Dong, Feng Yuan, Lina Yao, Xiwei Xu, and Liming Zhu. 2020. MAMO: Memory-Augmented Meta-Optimization for Cold-Start Recommendation. In KDD."
        },
        {
            "heading": "162 BIBLIOGRAPHY",
            "text": "[45] Nan Du, Hanjun Dai, Rakshit Trivedi, Utkarsh Upadhyay, Manuel Gomez-Rodriguez, and Le Song. 2016. Recurrent marked temporal point processes: Embedding event history to vector. In KDD.\n[46] Nan Du, Mehrdad Farajtabar, Amr Ahmed, Alexander J Smola, and Le Song. 2015. Dirichlet-hawkes processes with applications to clustering continuous-time document streams. In KDD.\n[47] Daniel Dufresne. 2004. The Log-Normal Approximation in Financial and Other Computations. Advances in Applied Probability 36, 3 (2004).\n[48] Sergey Edunov, Myle Ott, Michael Auli, and David Grangier. 2018. Understanding Back-Translation at Scale. In EMNLP.\n[49] Wenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, and Dawei Yin. 2019. Graph Neural Networks for Social Recommendation. In WWW.\n[50] Zipei Fan, Xuan Song, Ryosuke Shibasaki, Tao Li, and Hodaka Kaneda. 2016. CityCoupling: Bridging Intercity Human Mobility. In UbiComp.\n[51] Mehrdad Farajtabar, Jiachen Yang, Xiaojing Ye, Huan Xu, Rakshit Trivedi, Elias Khalil, Shuang Li, Le Song, and Hongyuan Zha. 2017. Fake news mitigation via point process based intervention. In ICML.\n[52] Yazan Abu Farha, Alexander Richard, and Juergen Gall. 2018. When Will You Do What? - Anticipating Temporal Occurrences of Activities. In CVPR.\n[53] Aleksandr Farseev, Ivan Samborskii, Andrey Filchenkov, and Tat-Seng Chua. 2017. Cross-Domain Recommendation via Clustering on Multi-Layer Graphs. In SIGIR.\n[54] Jie Feng, Yong Li, Chao Zhang, Funing Sun, Fanchao Meng, Ang Guo, and Depeng Jin. 2018. DeepMove: Predicting Human Mobility with Attentional Recurrent Networks. In WWW.\n[55] Maurizio Ferrari Dacrema, Paolo Cremonesi, and Dietmar Jannach. 2019. Are we really making much progress? A worrying analysis of recent neural recommendation approaches. In RecSys.\n[56] Flavio Figueiredo, Bruno Ribeiro, Jussara M Almeida, and Christos Faloutsos. 2016. TribeFlow: mining & predicting user trajectories. In WWW.\n[57] Chelsea Finn, Pieter Abbeel, and Sergey Levine. 2017. Model-agnostic meta-learning for fast adaptation of deep networks. In ICML.\n[58] Hongyang Gao and Shuiwang Ji. 2019. Graph U-Nets. In ICML.\n[59] Daniel Gervini and Theo Gasser. 2004. Self-modelling warping functions. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 66, 4 (2004), 959\u2013971.\n[60] Aristides Gionis, Piotr Indyk, and Rajeev Motwani. 1999. Similarity Search in High Dimensions via Hashing. In VLDB.\n[61] Jeff Glueck. 2019. The Future of Location Technology. Available At: https://bit.ly/2VWrABc.\n[62] Anna Gogolou, Theophanis Tsandilas, Karima Echihabi, Anastasia Bezerianos, and Themis Palpanas. 2020. Data series progressive similarity search with probabilistic quality guarantees. In SIGMOD.\nBIBLIOGRAPHY 163\n[63] Marta C Gonzalez, Cesar A Hidalgo, and Albert-Laszlo Barabasi. 2008. Understanding individual human mobility patterns. Nature 453, 7196 (2008).\n[64] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning. MIT Press. http://www.deeplearningbook.org.\n[65] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative Adversarial Networks. In NeurIPS.\n[66] Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable Feature Learning for Networks. In KDD.\n[67] Jiatao Gu, Qi Liu, and Kyunghyun Cho. 2019. Insertion-based Decoding with Automatically Inferred Generation Order. Transactions of the Association for Computational Linguistics 7 (2019).\n[68] Ruocheng Guo, Jundong Li, and Huan Liu. 2018. INITIATOR: Noise-contrastive Estimation for Marked Temporal Point Process. In IJCAI.\n[69] Shengnan Guo, Youfang Lin, Ning Feng, Chao Song, and Huaiyu Wan. 2019. Attention based spatialtemporal graph convolutional networks for traffic flow forecasting. In AAAI.\n[70] Vinayak Gupta. 2021. Learning Neural Models for Continuous-Time Sequences. arXiv preprint arXiv:2111.07189 (2021).\n[71] Vinayak Gupta and Srikanta Bedathur. 2021. Region Invariant Normalizing Flows for Mobility Transfer. In CIKM.\n[72] Vinayak Gupta and Srikanta Bedathur. 2022. Doing More with Less: Overcoming Data Scarcity for POI Recommendation via Cross-Region Transfer. ACM Trans. Intell. Syst. Technol. 13, 3, Article 50 (2022).\n[73] Vinayak Gupta and Srikanta Bedathur. 2022. Modeling Spatial Trajectories using Coarse-Grained Smartphone Logs. IEEE Transactions on Big Data (2022).\n[74] Vinayak Gupta and Srikanta Bedathur. 2022. ProActive: Self-Attentive Temporal Point Process Flows for Activity Sequences. In KDD.\n[75] Vinayak Gupta, Srikanta Bedathur, Sourangshu Bhattacharya, and Abir De. 2021. Learning Temporal Point Processes with Intermittent Observations. In AISTATS.\n[76] Vinayak Gupta, Srikanta Bedathur, Sourangshu Bhattacharya, and Abir De. 2022. Modeling Continuous Time Sequences with Intermittent Observations Using Marked Temporal Point Processes. ACM Trans. Intell. Syst. Technol. 13, 6, Article 103 (2022).\n[77] Vinayak Gupta, Srikanta Bedathur, and Abir De. 2022. Learning Temporal Point Processes for Efficient Retrieval of Continuous Time Event Sequences. In AAAI.\n[78] Bowen Hao, Jing Zhang, Hongzhi Yin, Cuiping Li, and Hong Chen. 2021. Pre-Training Graph Neural Networks for Cold-Start Users and Items Representation. In WSDM.\n[79] Shota Haradal, Hideaki Hayashi, and Seiichi Uchida. 2018. Biosignal data augmentation based on generative adversarial networks. In EMBC."
        },
        {
            "heading": "164 BIBLIOGRAPHY",
            "text": "[80] Alan G Hawkes. 1971. Spectra of some self-exciting and mutually exciting point processes. Biometrika 58, 1 (1971).\n[81] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In ICCV.\n[82] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In CVPR.\n[83] Ruining He, Wang-Cheng Kang, and Julian McAuley. 2018. Translation-based Recommendation: A Scalable Method for Modeling Sequential Behavior. In IJCAI.\n[84] Tianfu He, Jie Bao, Ruiyuan Li, Sijie Ruan, Yanhua Li, Li Song, Hui He, and Yu Zheng. 2020. What is the Human Mobility in a New City: Transfer Mobility Knowledge Across Cities. In WWW.\n[85] Xiangnan He, Ming Gao, Min-Yen Kan, and Dingxian Wang. 2017. BiRank: Towards Ranking on Bipartite Graphs. arXiv preprint arXiv:1708.04396 (2017).\n[86] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. 2017. Neural Collaborative Filtering. In WWW.\n[87] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. 2015. ActivityNet: A Large-Scale Video Benchmark for Human Activity Understanding. In CVPR.\n[88] Shawn Hershey, Sourish Chaudhuri, Daniel PW Ellis, Jort F Gemmeke, Aren Jansen, R Channing Moore, Manoj Plakal, Devin Platt, Rif A Saurous, Bryan Seybold, et al. 2017. CNN architectures for large-scale audio classification. In ICASSP.\n[89] Bal\u00e1zs Hidasi and Alexandros Karatzoglou. 2018. Recurrent neural networks with top-k gains for sessionbased recommendations. In CIKM.\n[90] Bal\u00e1zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. 2016. Session-based Recommendations with Recurrent Neural Networks. In ICLR.\n[91] Minh Hoai and Fernando De la Torre. 2012. Max-margin early event detectors. In CVPR.\n[92] Hengguang Huang, Hao Wang, and Brian Mak. 2019. Recurrent poisson process unit for speech recognition. In AAAI.\n[93] Tommi S Jaakkola, David Haussler, et al. 1998. Exploiting generative models in discriminative classifiers. In NeurIPS.\n[94] Yunhun Jang, Hankook Lee, Sung Ju Hwang, and Jinwoo Shin. 2019. Learning What and Where to Transfer. In ICML.\n[95] How Jing and Alexander J Smola. 2017. Neural survival recommender. In WSDM.\n[96] Thorsten Joachims. 2002. Optimizing Search Engines Using Clickthrough Data. In KDD.\n[97] Simon L. Jones, Denzil Ferreira, Simo Hosio, Jorge Goncalves, and Vassilis Kostakos. 2015. Revisitation analysis of smartphone app use. In UbiComp.\nBIBLIOGRAPHY 165\n[98] Kaggle. 2018. TalkingData Mobile User Demographics. Available At: https://www.kaggle.com/c/talkingdata-mobile-user-demographics.\n[99] Wang-Cheng Kang and Julian McAuley. 2018. Self-Attentive Sequential Recommendation. In ICDM.\n[100] Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimization. In ICLR.\n[101] Durk P Kingma and Prafulla Dhariwal. 2018. Glow: Generative flow with invertible 1x1 convolutions. In NeurIPS.\n[102] Durk P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. 2016. Improved variational inference with inverse autoregressive flow. In NeurIPS.\n[103] Thomas N. Kipf and Max Welling. 2017. Semi-supervised classification with graph convolutional networks. In ICLR.\n[104] Ivan Kobyzev, Simon JD Prince, and Marcus A Brubaker. 2020. Normalizing flows: An introduction and review of current methods. IEEE transactions on pattern analysis and machine intelligence 43, 11 (2020).\n[105] Adit Krishnan, Mahashweta Das, Mangesh Bendre, Hao Yang, and Hari Sundaram. 2020. Transfer Learning via Contextual Invariants for One-to-Many Cross-Domain Recommendation. In SIGIR.\n[106] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2017. Imagenet classification with deep convolutional neural networks. Commun. ACM 60, 6 (2017).\n[107] H. Kuehne, A. B. Arslan, and T. Serre. 2014. The Language of Actions: Recovering the Syntax and Semantics of Goal-Directed Human Activities. In CVPR.\n[108] Srijan Kumar, Xikun Zhang, and Jure Leskovec. 2019. Predicting dynamic embedding trajectory in temporal interaction networks. In KDD.\n[109] Matt Kusner, Yu Sun, Nicholas Kolkin, and Kilian Weinberger. 2015. From word embeddings to document distances. In ICML.\n[110] Tian Lan, Tsung-Chuan Chen, and Silvio Savarese. 2014. A hierarchical representation for future action prediction. In ECCV.\n[111] Hoyeop Lee, Jinbae Im, Seongwon Jang, Hyunsouk Cho, and Sehee Chung. 2019. MeLU: Meta-Learned User Preference Estimator for Cold-Start Recommendation. In KDD.\n[112] Jaekoo Lee, Hyunjae Kim, Jongsun Lee, and Sungroh Yoon. 2017. Transfer Learning for Deep Learning on Graph-Structured Data. In AAAI.\n[113] Junhyun Lee, Inyeop Lee, and Jaewoo Kang. 2019. Self-attention graph pooling. In ICML.\n[114] Hui Li, Ke Deng, Jiangtao Cui, Zhenhua Dong, Jianfeng Ma, and Jianbin Huang. 2018. Hidden Community Identification in Location-Based Social Network via Probabilistic Venue Sequences. Inf. Sci. 422, C (2018).\n[115] Jiacheng Li, Yujie Wang, and Julian McAuley. 2020. Time Interval Aware Self-Attention for Sequential Recommendation. In WSDM."
        },
        {
            "heading": "166 BIBLIOGRAPHY",
            "text": "[116] Pan Li and Alexander Tuzhilin. 2020. DDTCDR: Deep Dual Transfer Cross Domain Recommendation. In WSDM.\n[117] Ruirui Li, Jyunyu Jiang, Chelsea Ju, and Wei Wang. 2019. CORALS: Who are My Potential New Customers? Tapping into the Wisdom of Customers Decisions. In WSDM.\n[118] Ranzhen Li, Yanyan Shen, and Yanmin Zhu. 2018. Next point-of-interest recommendation with temporal and multi-level context attention. In ICDM.\n[119] Ruirui Li, Xian Wu, Xiusi Chen, and Wei Wang. 2020. Few-Shot Learning for New User Recommendation inLocation-based Social Networks. In WWW.\n[120] Ruirui Li, Xian Wu, and Wei Wang. 2020. Adversarial Learning to Compare: Self-Attentive Prospective Customer Recommendation in Location based Social Networks. In WSDM.\n[121] Shuang Li, Shuai Xiao, Shixiang Zhu, Nan Du, Yao Xie, and Le Song. 2018. Learning Temporal Point Processes via Reinforcement Learning. In NeurIPS.\n[122] Tong Li, Mingyang Zhang, Hancheng Cao, Yong Li, Sasu Tarkoma, and Pan Hui. 2020. \"What Apps Did You Use?\": Understanding the Long-term Evolution of Mobile App Usage. In WWW.\n[123] Yaguang Li, Rose Yu, Cyrus Shahabi, and Yan Liu. 2018. Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting. In ICLR.\n[124] Youru Li, Zhenfeng Zhu, Deqiang Kong, Hua Han, and Yao Zhao. 2018. EA-LSTM: Evolutionary Attention-based LSTM for Time Series Prediction. arXiv preprint arXiv:1811.03760 (2018).\n[125] Defu Lian, Yongji Wu, Yong Ge, Xing Xie, and Enhong Chen. 2020. Geography-Aware Sequential Location Recommendation. In KDD.\n[126] Ankita Likhyani, Srikanta Bedathur, and Deepak P. 2017. LoCaTe: Influence Quantification for Location Promotion in Location-based Social Networks. In IJCAI.\n[127] Ankita Likhyani, Vinayak Gupta, PK Srijith, P Deepak, and Srikanta Bedathur. 2020. Modeling Implicit Communities from Geo-tagged Event Traces using Spatio-Temporal Point Processes. In WISE.\n[128] Thomas Josef Liniger. 2009. Multivariate hawkes processes. Ph. D. Dissertation. ETH Zurich.\n[129] Roderick JA Little and Donald B Rubin. 2019. Statistical analysis with missing data. John Wiley & Sons.\n[130] Qiang Liu, Shu Wu, Liang Wang, and Tieniu Tan. 2016. Predicting the Next Location: A Recurrent Model with Spatial and Temporal Contexts. In AAAI.\n[131] Lars Lorch, Abir De, Samir Bhatt, William Trouleau, Utkarsh Upadhyay, and Manuel Gomez-Rodriguez. 2018. Stochastic Optimal Control of Epidemic Processes in Networks. arXiv preprint arXiv:1810.13043 (2018).\n[132] Yuanfu Lu, Yuan Fang, and Chuan Shi. 2020. Meta-learning on Heterogeneous Information Networks for Cold-start Recommendation. In KDD.\nBIBLIOGRAPHY 167\n[133] Zhongqi Lu, Zhicheng Dou, Jianxun Lian, Xing Xie, and Qiang Yang. 2015. Content-based collaborative filtering for news topic recommendation. In AAAI.\n[134] Zheng Lu, Yunhe Feng, Wenjun Zhou, Xiaolin Li, and Qing Cao. 2018. Inferring Correlation between User Mobility and App Usage in Massive Coarse-Grained Data Traces. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 1, 4 (2018).\n[135] Chuxu Zhang Lu Yu, Shangsong Liang, and Xiangliang Zhang. 2019. Multi-Order Attentive Ranking Model for Sequential Recommendation. In AAAI.\n[136] Yonghong Luo, Xiangrui Cai, Ying Zhang, Jun Xu, and Yuan Xiaojie. 2018. Multivariate Time Series Imputation with Generative Adversarial Networks. In NeurIPS.\n[137] Yue Luo, Sarah M Coppola, Philippe C Dixon, Song Li, Jack T Dennerlein, and Boyi Hu. 2020. A database of human gait performance on irregular and uneven surfaces collected by wearable sensors. Scientific Data 7, 1 (2020).\n[138] Yonghong Luo, Ying Zhang, Xiangrui Cai, and Xiaojie Yuan. 2019. E\u00b2GAN: End-to-End Generative Adversarial Network for Multivariate Time Series Imputation. In IJCAI.\n[139] Shugao Ma, Leonid Sigal, and Stan Sclaroff. 2016. Learning Activity Progression in LSTMs for Activity Detection and Early Detection. In CVPR.\n[140] Tahmida Mahmud, Mahmudul Hasan, and Amit K. Roy-Chowdhury. 2017. Joint prediction of activity labels and starting times in untrimmed videos. In ICCV.\n[141] Jarana Manotumruksa, Craig Macdonald, and Iadh Ounis. 2018. A Contextual Attention Recurrent Architecture for Context-Aware Venue Recommendation. In SIGIR.\n[142] Jarana Manotumruksa, Dimitrios Rafailidis, Craig Macdonald, and Iadh Ounis. 2019. On Cross-Domain Transfer in Venue Recommendation. In ECIR.\n[143] Geoffrey McLachlan and David Peel. 2004. Finite Mixture Models. John Wiley & Sons.\n[144] Nazanin Mehrasa, Ruizhi Deng, Mohamed Osama Ahmed, Bo Chang, Jiawei He, Thibaut Durand, Marcus Brubaker, and Greg Mori. 2019. Point process flows. arXiv preprint arXiv:1910.08281 (2019).\n[145] Nazanin Mehrasa, Akash Abdu Jyothi, Thibaut Durand, Jiawei He, Leonid Sigal, and Greg Mori. 2019. A Variational Auto-Encoder Model for Stochastic Point Processes. In CVPR.\n[146] Nazanin Mehrasa, Yatao Zhong, Frederick Tung, Luke Bornn, and Greg Mori. 2017. Learning person trajectory representations for team activity analysis. arXiv preprint arXiv:1706.00893 (2017).\n[147] Hongyuan Mei and Jason M Eisner. 2017. The neural hawkes process: A neurally self-modulating multivariate point process. In NeurIPS.\n[148] Hongyuan Mei, Guanghui Qin, and Jason Eisner. 2019. Imputing Missing Events in Continuous-Time Event Streams. In ICML."
        },
        {
            "heading": "168 BIBLIOGRAPHY",
            "text": "[149] Denis Metev. 2021. 41+ Must Know Foursquare Statistics in 2020. Available At: https://review42.com/resources/foursquare-statistics/.\n[150] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed Representations of Words and Phrases and their Compositionality. In NeurIPS.\n[151] Christopher Morris, Martin Ritzert, Matthias Fey, William L Hamilton, Jan Eric Lenssen, Gaurav Rattan, and Martin Grohe. 2019. Weisfeiler and leman go neural: Higher-order graph neural networks. In AAAI.\n[152] Abdullah Mueen and Eamonn Keogh. 2016. Extracting optimal performance from dynamic time warping. In KDD.\n[153] Abdullah Mueen, Yan Zhu, Michael Yeh, Kaveh Kamgar, Krishnamurthy Viswanathan, Chetan Gupta, and Eamonn Keogh. 2017. The Fastest Similarity Search Algorithm for Time Series Subsequences under Euclidean Distance. http://www.cs.unm.edu/ mueen/FastestSimilaritySearch.html.\n[154] David Murray, Lina Stankovic, and Vladimir Stankovic. 2017. An electrical load measurements dataset of United Kingdom households from a two-year longitudinal study. Scientific data 4, 1 (2017).\n[155] A. Nagrani, J. S. Chung, and A. Zisserman. 2017. VoxCeleb: a large-scale speaker identification dataset. In INTERSPEECH.\n[156] Zahra Nazari, Christophe Charbuillet, Johan Pages, Martin Laurent, Denis Charrier, Briana Vecchione, and Ben Carterette. 2020. Recommending podcasts for cold-start users based on music listening and taste. In SIGIR.\n[157] Jianmo Ni, Jiacheng Li, and Julian McAuley. 2019. Justifying recommendations using distantly-labeled reviews and fine-grained aspects. In EMNLP-IJCNLP.\n[158] Alex Nichol, Joshua Achiam, and John Schulman. 2018. On First-Order Meta-Learning Algorithms. arXiv preprint arXiv 1803.02999 (2018).\n[159] Anastasios Noulas, Salvatore Scellato, Cecilia Mascolo, and Massimiliano Pontil. 2011. Exploiting semantic annotations for clustering geographic areas and users in location-based social networks. In The Social Mobile Web, ICWSM Workshop.\n[160] Y. Ogata. 1981. On Lewis\u2019 Simulation Method for Point Processes. IEEE Trans. Inf. Theor. 27, 1 (1981).\n[161] Adam J Oliner, Anand P Iyer, Ion Stoica, Eemil Lagerspetz, and Sasu Tarkoma. 2013. Carat: Collaborative energy diagnosis for mobile devices. In SenSys.\n[162] Takahiro Omi, Naonori Ueda, and Kazuyuki Aihara. 2019. Fully Neural Network based Model for General Temporal Point Processes. In NeurIPS.\n[163] Kun Ouyang, Reza Shokri, David S Rosenblum, and Wenzhuo Yang. 2018. Non-Parametric Generative Model for Human Trajectories. In IJCAI.\n[164] Sinno Jialin Pan and Qiang Yang. 2010. A Survey on Transfer Learning. IEEE Transactions on Knowledge and Data Engineering 22, 10 (2010).\nBIBLIOGRAPHY 169\n[165] Zheyi Pan, Yuxuan Liang, Weifeng Wang, Yong Yu, Yu Zheng, and Junbo Zhang. 2019. Urban Traffic Prediction from Spatio-Temporal Data Using Deep Meta Learning. In KDD.\n[166] John Paparrizos and Luis Gravano. 2015. k-shape: Efficient and accurate clustering of time series. In SIGMOD.\n[167] Niki J. Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. 2018. Image Transformer. In ICML.\n[168] Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation. In EMNLP.\n[169] Zongyue Qin, Yunsheng Bai, and Yizhou Sun. 2020. GHashing: Semantic Graph Hashing for Approximate Similarity Search in Graph Databases. In KDD.\n[170] Thanawin Rakthanmanon, Bilson Campana, Abdullah Mueen, Gustavo Batista, Brandon Westover, Qiang Zhu, Jesin Zakaria, and Eamonn Keogh. 2012. Searching and Mining Trillions of Time Series Subsequences under Dynamic Time Warping. In KDD.\n[171] Jakob Gulddahl Rasmussen. 2013. Bayesian Inference for Hawkes Processes. Method. Comput. Appl. Prob. 15, 3 (2013).\n[172] Jakob Gulddahl Rasmussen. 2018. Lecture Notes: Temporal Point Processes and the Conditional Intensity Function. arXiv preprint arXiv:1806.00221 (2018).\n[173] Steffen Rendle. 2010. Factorization machines. In ICDM.\n[174] Steffen Rendle, Christoph Freudenthaler, and Lars Schmidt-Thieme. 2010. Factorizing personalized Markov chains for next-basket recommendation. In WWW.\n[175] Danilo Rezende and Shakir Mohamed. 2015. Variational inference with normalizing flows. In ICML.\n[176] Marian-Andrei Rizoiu, Swapnil Mishra, Quyu Kong, Mark Carman, and Lexing Xie. 2018. SIR-Hawkes: on the Relationship Between Epidemic Models and Hawkes Point Processes. In WWW.\n[177] Marian-Andrei Rizoiu, Lexing Xie, Scott Sanner, Manuel Cebrian, Honglin Yu, and Pascal Van Hentenryck. 2017. Expecting to be hip: Hawkes intensity processes for social media popularity. In WWW.\n[178] Indradyumna Roy, Abir De, and Soumen Chakrabarti. 2021. Adversarial Permutation Guided Node Representations for Link Prediction. In AAAI.\n[179] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. 1986. Learning representations by backpropagating errors. Nature 323, 6088 (1986).\n[180] M. Ryoo. 2011. Human activity prediction: Early recognition of ongoing activities from streaming videos. In ICCV.\n[181] Ruslan Salakhutdinov and Geoffrey Hinton. 2009. Semantic hashing. International Journal of Approximate Reasoning 50, 7 (2009)."
        },
        {
            "heading": "170 BIBLIOGRAPHY",
            "text": "[182] Salvatore Scellato, Anastasios Noulas, and Cecilia Mascolo. 2011. Exploiting place features in link prediction on location-based social networks. In KDD.\n[183] Martin Sewell. 2011. The fisher kernel: a brief review. RN 11, 06 (2011), 06.\n[184] Karishma Sharma, Yizhou Zhang, Emilio Ferrara, and Yan Liu. 2021. Identifying Coordinated Accounts on Social Media through Hidden Influence and Group Behaviours. In KDD.\n[185] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-Attention with Relative Position Representations. In NAACL-HLT.\n[186] Oleksandr Shchur, Marin Bilo\u0161, and Stephan G\u00fcnnemann. 2020. Intensity-Free Learning of Temporal Point Processes. In ICLR.\n[187] Oleksandr Shchur, Nicholas Gao, Marin Bilo\u0161, and Stephan G\u00fcnnemann. 2020. Fast and Flexible Temporal Point Processes with Triangular Maps. In NeurIPS.\n[188] Oleksandr Shchur, Ali Caner T\u00fcrkmen, Tim Januschowski, and Stephan G\u00fcnnemann. 2021. Neural Temporal Point Processes: A Review. In IJCAI. Survey Track.\n[189] Christian R. Shelton, Zhen Qin, and Chandini Shetty. 2018. Hawkes Process Inference with Missing Data. In AAAI.\n[190] Yilin Shen, Yanping Chen, Eamonn Keogh, and Hongxia Jin. 2018. Accelerating time series searching with large uniform scaling. In SDM.\n[191] Ajit P Singh and Geoffrey J Gordon. 2008. Relational learning via collective matrix factorization. In KDD.\n[192] Marek Smieja, Lukasz Struski, Jacek Tabor, Bartosz Zielinski, and Przemyslaw Spurek. 2018. Processing of Missing Data by Neural Networks. In NeurIPS.\n[193] Han Su, Shuncheng Liu, Bolong Zheng, Xiaofang Zhou, and Kai Zheng. 2020. A survey of trajectory distance measures and performance evaluation. The VLDB Journal 29, 1 (2020).\n[194] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. 2019. BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer. In CIKM.\n[195] Richard S. Sutton and Andrew G. Barto. 2018. Reinforcement Learning: An Introduction.\n[196] Jiaxi Tang and Ke Wang. 2018. Personalized top-n sequential recommendation via convolutional sequence embedding. In WSDM.\n[197] E. Triantafillou, T. Zhu, V. Dumoulin, P. Lamblin, U. Evci, K. Xu, R. Goroshin, C. Gelada, K. Swersky, P. Manzagol, and H. Larochelle. 2020. Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples. In ICLR.\n[198] Zhen Tu, Yali Fan, Yong Li, Xiang Chen, Li Su, and Depeng Jin. 2019. From Fingerprint to Footprint: Cold-Start Location Recommendation by Learning User Interest from App Data. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 3, 1 (2019).\nBIBLIOGRAPHY 171\n[199] Zhen Tu, Runtong Li, Yong Li, Gang Wang, Di Wu, Pan Hui, Li Su, and Depeng Jin. 2018. Your Apps Give You Away: Distinguishing Mobile Users by Their App Usage Fingerprints. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 2, 3 (2018).\n[200] Ali Caner T\u00fcrkmen, Yuyang Wang, and Alexander J Smola. 2020. Fastpoint: Scalable deep point processes. In ECML-PKDD.\n[201] Utkarsh Upadhyay, Abir De, and Manuel Gomez-Rodriguez. 2018. Reinforcement Learning of Marked Temporal Point Processes. In NeurIPS.\n[202] Dmitry Ustalov, Natalia Fedorova, and Nikita Pavlichenko. 2022. Improving Recommender Systems with Human-in-the-Loop. In RecSys.\n[203] Isabel Valera, Manuel Gomez-Rodriguez, and Krishna Gummadi. 2014. Modeling Diffusion of Competing Products and Conventions in Social Media. arXiv preprint arXiv:1406.0516 (2014).\n[204] Aaron Van den Oord, Sander Dieleman, and Benjamin Schrauwen. 2013. Deep content-based music recommendation. In NeurIPS.\n[205] Manasi Vartak, Arvind Thiagarajan, Conrado Miranda, Jeshua Bratman, and Hugo Larochelle. 2017. A Meta-Learning Perspective on Cold-Start Recommendations for Items. In NeurIPS.\n[206] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In NeurIPS.\n[207] Petar Velic\u030ckovic\u0301, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, and Yoshua Bengio. 2018. Graph Attention Networks. In ICLR.\n[208] Huandong Wang, Yong Li, Sihan Zeng, Gang Wang, Pengyu Zhang, Pan Hui, and Depeng Jin. 2019. Modeling Spatio-Temporal App Usage for a Large User Population. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 3, 1 (2019).\n[209] Leye Wang, Xu Geng, Xiaojuan Ma, Feng Liu, and Qiang Yang. 2019. Cross-city transfer learning for deep spatio-temporal prediction. In IJCAI.\n[210] Pengfei Wang, Yanjie Fu, Guannan Liu, Wenqing Hu, and Charu Aggarwal. 2017. Human Mobility Synchronization and Trip Purpose Detection with Mixture of Hawkes Processes. In KDD.\n[211] Xiang Wang, Xiangnan He, Liqiang Nie, and Tat-Seng Chua. 2017. Item silk road: Recommending items from information domains to social users. In SIGIR.\n[212] Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. 2019. Neural Graph Collaborative Filtering. In SIGIR.\n[213] Xinxi Wang and Ye Wang. 2014. Improving Content-Based and Hybrid Music Recommendation Using Deep Learning. In MM.\n[214] Zhu Wang, Daqing Zhang, Xingshe Zhou, Dingqi Yang, Zhiyong Yu, and Zhiwen Yu. 2013. Discovering and profiling overlapping communities in location-based social networks. IEEE Transactions on Systems, Man, and Cybernetics: Systems 44, 4 (2013)."
        },
        {
            "heading": "172 BIBLIOGRAPHY",
            "text": "[215] Antoine Wehenkel and Gilles Louppe. 2019. Unconstrained monotonic neural networks. In NeurIPS.\n[216] Ying Wei, Yu Zheng, and Qiang Yang. 2016. Transfer knowledge between cities. In KDD.\n[217] Chuhan Wu, Fangzhao Wu, Mingxiao An, Jianqiang Huang, Yongfeng Huang, and Xing Xie. 2019. NPA: Neural News Recommendation with Personalized Attention. In KDD.\n[218] Felix Wu, Angela Fan, Alexei Baevski, Yann N. Dauphin, and Michael Auli. 2019. Pay Less Attention with Lightweight and Dynamic Convolutions. In ICLR.\n[219] Hao Wu, Ziyang Chen, Weiwei Sun, Baihua Zheng, and Wei Wang. 2017. Modeling trajectories with recurrent neural networks. In IJCAI.\n[220] Le Wu, Yong Ge, Qi Liu, Enhong Chen, Bai Long, and Zhenya Huang. 2016. Modeling users preferences and social links in social networking services: A joint-evolving perspective. In AAAI.\n[221] Qitian Wu, Hengrui Zhang, Xiaofeng Gao, Peng He, Paul Weng, Han Gao, and Guihai Chen. 2019. Dual Graph Attention Networks for Deep Latent Representation of Multifaceted Social Effects in Recommender Systems. In WWW.\n[222] Xian Wu, Baoxu Shi, Yuxiao Dong, Chao Huang, Louis Faust, and Nitesh V Chawla. 2018. Restful: Resolution-aware forecasting of behavioral time series data. In CIKM.\n[223] Yongji Wu, Defu Lian, Shuowei Jin, and Enhong Chen. 2019. Graph Convolutional Networks on User Mobility Heterogeneous Graphs for Social Relationship Inference. In IJCAI.\n[224] Shuai Xiao, Mehrdad Farajtabar, Xiaojing Ye, Junchi Yan, Le Song, and Hongyuan Zha. 2017. Wasserstein Learning of Deep Generative Point Process Models. In NeurIPS.\n[225] Shuai Xiao, Junchi Yan, Stephen Chu, Xiaokang Yang, and Hongyuan Zha. 2017. Modeling the Intensity function of Point Process via Recurrent Neural Networks. In AAAI.\n[226] Hongteng Xu, Lawrence Carin, and Hongyuan Zha. 2018. Learning registered point processes from idiosyncratic observations. In ICML.\n[227] Hongteng Xu, Dixin Luo, and Hongyuan Zha. 2017. Learning Hawkes processes from short doublycensored event sequences. In ICML.\n[228] Dingqi Yang, Bingqing Qu, Jie Yang, and Philippe Cudre-Mauroux. 2019. Revisiting User Mobility and Social Relationships in LBSNs: A Hypergraph Embedding Approach. In WWW.\n[229] Guolei Yang, Ying Cai, and Chandan K. Reddy. 2018. Recurrent Spatio-Temporal Point Process for Checkin Time Prediction. In CIKM.\n[230] Huaxiu Yao, Yiding Liu, Ying Wei, Xianfeng Tang, and Zhenhui Li. 2019. Learning from Multiple Cities: A Meta-Learning Approach for Spatial-Temporal Prediction. In WWW.\n[231] Huaxiu Yao, Ying Wei, Junzhou Huang, and Zhenhui Li. 2019. Hierarchically Structured Meta-learning. In ICML.\nBIBLIOGRAPHY 173\n[232] Huaxiu Yao, Chuxu Zhang, Ying Wei, Meng Jiang, Suhang Wang, Junzhou Huang, and Nitesh V. Chawla. 2020. Graph Few-shot Learning via Knowledge Transfer. In AAAI.\n[233] M. Yao, S. Zhao, S. Sahebi, and R. Feyzi Behnagh. 2021. Stimuli-Sensitive Hawkes Processes for Personalized Student Procrastination Modeling. In WWW.\n[234] Serena Yeung, Olga Russakovsky, Ning Jin, Mykhaylo Andriluka, Greg Mori, and Li Fei-Fei. 2018. Every moment counts: Dense detailed labeling of actions in complex videos. International Journal of Computer Vision 126, 2 (2018).\n[235] Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, and Jure Leskovec. 2018. Graph convolutional neural networks for web-scale recommender systems. In KDD.\n[236] Rex Ying, Jiaxuan You, Christopher Morris, Xiang Ren, William L Hamilton, and Jure Leskovec. 2018. Hierarchical graph representation learning with differentiable pooling. In NeurIPS.\n[237] Jinsung Yoon, Daniel Jarrett, and Mihaela van der Schaar. 2019. Time-series Generative Adversarial Networks. In NeurIPS.\n[238] Jinsung Yoon, William R. Zame, and Mihaela van der Schaar. 2019. Estimating Missing Data in Temporal Data Streams Using Multi-Directional Recurrent Neural Networks. IEEE Transactions on Biomedical Engineering 66, 5 (2019).\n[239] Donghan Yu, Yong Li, Fengli Xu, Pengyu Zhang, and Vassilis Kostakos. 2018. Smartphone App Usage Prediction Using Points of Interest. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 1, 4 (2018).\n[240] Quan Yuan, Gao Cong, Zongyang Ma, Aixin Sun, and Nadia Magnenat-Thalmann. 2013. Time-aware point-of-interest recommendation. In SIGIR.\n[241] Sergey Zagoruyko and Nikos Komodakis. 2017. Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer. In ICLR.\n[242] Dell Zhang, Jun Wang, Deng Cai, and Jinsong Lu. 2010. Self-taught hashing for fast similarity search. In SIGIR.\n[243] Qiang Zhang, Aldo Lipani, Omer Kirnap, and Emine Yilmaz. 2020. Self-attentive Hawkes processes. In ICML.\n[244] Yunchao Zhang, Yanjie Fu, Pengyang Wang, Yu Zheng, and Xiaolin Li. 2019. Unifying Inter-Region Autocorrelations and Intra-Region Structure for Spatial Embedding via Collective Adversarial Learning. In KDD.\n[245] Pengpeng Zhao, Haifeng Zhu, Yanchi Liu, Jiajie Xu, Zhixu Li, Fuzhen Zhuang, Victor S Sheng, and Xiaofang Zhou. 2019. Where to go next: a spatio-temporal gated network for next poi recommendation. In AAAI.\n[246] Qingyuan Zhao, Murat A Erdogdu, Hera Y He, Anand Rajaraman, and Jure Leskovec. 2015. Seismic: A self-exciting point process model for predicting tweet popularity. In KDD.\n[247] Sha Zhao, Zhiling Luo, Ziwen Jiang, Haiyan Wang, Feng Xu, Shijian Li, Jianwei Yin, and Gang Pan. 2019. AppUsage2Vec: Modeling Smartphone App Usage for Prediction. In ICDE.\n[248] Yu Zheng. 2011. Location-Based Social Networks: Users. In Computing with Spatial Trajectories. 243\u2013 276.\n[249] Chenyi Zhuang and Qiang Ma. 2018. Dual Graph Convolutional Networks for Graph-Based SemiSupervised Classification. In WWW.\n[250] Jiancang Zhuang, Ting Wang, and Koji Kiyosugi. 2020. Detection and replenishment of missing data in marked point processes. Statistica Sinica 30, 4 (2020).\n[251] Simiao Zuo, Haoming Jiang, Zichong Li, Tuo Zhao, and Hongyuan Zha. 2020. Transformer Hawkes Process. In ICML.\n174"
        },
        {
            "heading": "List of Publications",
            "text": "Here we list out all first-author articles accepted/published during the Ph.D tenure.\nPublications based on this Thesis.\n1. Vinayak Gupta, Abir De, Sourangshu Bhattacharya, and Srikanta Bedathur. \u201cLearning Temporal Point Processes with Intermittent Observations\u201d, In Proc. of the 24th International Conference on Artificial Intelligence and Statistics (AISTATS), 2021.\n2. Vinayak Gupta and Srikanta Bedathur. \u201cRegion Invariant Normalizing Flows for Mobility Transfer\u201d, In Proc. of the 30th ACM Intl. Conference on Information and Knowledge Management (CIKM), 2021.\n3. Vinayak Gupta, Abir De, and Srikanta Bedathur. \u201cLearning Temporal Point Processes for Efficient Retrieval of Continuous Time Event Sequences\u201d, In Proc. of the 36th AAAI Conference on Artificial Intelligence (AAAI), 2022.\n4. Vinayak Gupta and Srikanta Bedathur. \u201cProActive: Self-Attentive Temporal Point Process Flows for Activity Sequences\u201d, In Proc. of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD), 2022.\n5. Vinayak Gupta and Srikanta Bedathur. \u201cDoing More with Less: Overcoming Data Scarcity for POI Recommendation via Cross-Region Transfer\u201d, ACM Transactions on Intelligent Systems and Technology (ACM TIST), 2022.\n6. Vinayak Gupta, Abir De, Sourangshu Bhattacharya, and Srikanta Bedathur. \u201cModeling Continuous Time Sequences with Intermittent Observations using Marked Temporal Point Processes\u201d, ACM Transactions on Intelligent Systems and Technology (ACM TIST), 2022.\n175\n7. Vinayak Gupta and Srikanta Bedathur. \u201cModeling Spatial Trajectories using CoarseGrained Smartphone Logs\u201d, IEEE Transactions on Big Data (IEEE TBD), 2022."
        },
        {
            "heading": "Other Publications.",
            "text": "1. Ankita Likhyani*, Vinayak Gupta*, P. K. Srijith, Deepak, and Srikanta Bedathur. \u201cModeling Implicit Communities from Geo-tagged Event Traces using Spatio-Temporal Point Processes\u201d, In Proc. of the 21st International Conference on Web Information Systems Engineering (WISE), 2020.\nThis work received the Outstanding Doctoral Paper award at The First International Conference on AI-ML-Systems 2021.\n176"
        },
        {
            "heading": "Biography",
            "text": "Vinayak Gupta is a Ph.D. student at the Department of Computer Science and Engineering, Indian Institute of Technology (IIT) Delhi. He obtained a Bachelors\u2019 degree in Computer Science and Engineering from the Indian Institute of Information Technology (IIIT) Jabalpur. His research interests are in data mining. Specifically, his research addresses the problems related to \u2013 limited training data, asynchronous data collection in sequences, event-imputation, end-to-end sequence generation, activity recognition, and spatial recommendation for users \u2013 in temporal sequences using point processes and graph neural networks.\n177\n178"
        }
    ],
    "title": "MODELING TIME-SERIES AND SPATIAL DATA FOR RECOMMENDATIONS AND OTHER APPLICATIONS",
    "year": 2022
}