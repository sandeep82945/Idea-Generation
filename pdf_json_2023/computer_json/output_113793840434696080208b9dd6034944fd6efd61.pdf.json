{
    "abstractText": "Attention-based vision models, such as Vision Transformer (ViT) and its variants, have shown promising performance in various computer vision tasks. However, these emerging architectures suffer from large model sizes and high computational costs, calling for efficient model compression solutions. To date, pruning ViTs has been well studied, while other compression strategies that have been widely applied in CNN compression, e.g., model factorization, is little explored in the context of ViT compression. This paper explores an efficient method for compressing vision transformers to enrich the toolset for obtaining compact attention-based vision models. Based on the new insight on the multi-head attention layer, we develop a highly efficient ViT compression solution, which outperforms the state-ofthe-art pruning methods. For compressing DeiTsmall and DeiT-base models on ImageNet, our proposed approach can achieve 0.45% and 0.76% higher top-1 accuracy even with fewer parameters. Our finding can also be applied to improve the customization efficiency of text-to-image diffusion models, with much faster training (up to 2.6\u00d7 speedup) and lower extra storage cost (up to 1927.5\u00d7 reduction) than the existing works. The code and models are publicly available at https: //github.com/jinqixiao/ComCAT.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jinqi Xiao"
        },
        {
            "affiliations": [],
            "name": "Miao Yin"
        },
        {
            "affiliations": [],
            "name": "Yu Gong"
        },
        {
            "affiliations": [],
            "name": "Xiao Zang"
        },
        {
            "affiliations": [],
            "name": "Jian Ren"
        },
        {
            "affiliations": [],
            "name": "Bo Yuan"
        }
    ],
    "id": "SP:286d752b1aae192f0ffad7618a97b24a1e74282f",
    "references": [
        {
            "authors": [
                "A. Aghajanyan",
                "L. Zettlemoyer",
                "S. Gupta"
            ],
            "title": "Intrinsic dimensionality explains the effectiveness of language model fine-tuning",
            "venue": "arXiv preprint arXiv:2012.13255,",
            "year": 2020
        },
        {
            "authors": [
                "N. Carion",
                "F. Massa",
                "G. Synnaeve",
                "N. Usunier",
                "A. Kirillov",
                "S. Zagoruyko"
            ],
            "title": "End-to-end object detection with transformers",
            "venue": "In European conference on computer vision,",
            "year": 2020
        },
        {
            "authors": [
                "M. Caron",
                "H. Touvron",
                "I. Misra",
                "H. J\u00e9gou",
                "J. Mairal",
                "P. Bojanowski",
                "A. Joulin"
            ],
            "title": "Emerging properties in self-supervised vision transformers",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "B. Chen",
                "T. Dao",
                "E. Winsor",
                "Z. Song",
                "A. Rudra",
                "C. R\u00e9"
            ],
            "title": "Scatterbrain: Unifying sparse and low-rank attention",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "T. Chen",
                "Y. Cheng",
                "Z. Gan",
                "L. Yuan",
                "L. Zhang",
                "Z. Wang"
            ],
            "title": "Chasing sparsity in vision transformers: An end-to-end exploration",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Chen",
                "Y.-H",
                "J. Emer",
                "V. Sze"
            ],
            "title": "Eyeriss: A spatial architecture for energy-efficient dataflow for convolutional neural networks",
            "venue": "ACM SIGARCH Computer Architecture News,",
            "year": 2016
        },
        {
            "authors": [
                "K. Choromanski",
                "V. Likhosherstov",
                "D. Dohan",
                "X. Song",
                "A. Gane",
                "T. Sarlos",
                "P. Hawkins",
                "J. Davis",
                "A. Mohiuddin",
                "L Kaiser"
            ],
            "title": "Rethinking attention with performers",
            "year": 2009
        },
        {
            "authors": [
                "J. Deng",
                "W. Dong",
                "R. Socher",
                "Li",
                "L.-J",
                "K. Li",
                "L. Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "IEEE conference on computer vision and pattern recognition,",
            "year": 2009
        },
        {
            "authors": [
                "R. Gal",
                "Y. Alaluf",
                "Y. Atzmon",
                "O. Patashnik",
                "A.H. Bermano",
                "G. Chechik",
                "D. Cohen-Or"
            ],
            "title": "An image is worth one word: Personalizing text-to-image generation using textual inversion",
            "venue": "arXiv preprint arXiv:2208.01618,",
            "year": 2022
        },
        {
            "authors": [
                "S. Goyal",
                "A.R. Choudhury",
                "S. Raje",
                "V. Chakaravarthy",
                "Y. Sabharwal",
                "A. Verma"
            ],
            "title": "Power-bert: Accelerating bert inference via progressive word-vector elimination",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "S. Han",
                "H. Mao",
                "W.J. Dally"
            ],
            "title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding",
            "venue": "arXiv preprint arXiv:1510.00149,",
            "year": 2015
        },
        {
            "authors": [
                "M. Heusel",
                "H. Ramsauer",
                "T. Unterthiner",
                "B. Nessler",
                "S. Hochreiter"
            ],
            "title": "Gans trained by a two time-scale update rule converge to a local nash equilibrium",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "G. Hinton",
                "O. Vinyals",
                "J. Dean"
            ],
            "title": "Distilling the knowledge in a neural network",
            "venue": "arXiv preprint arXiv:1503.02531,",
            "year": 2015
        },
        {
            "authors": [
                "Z. Hou",
                "Kung",
                "S.-Y"
            ],
            "title": "Multi-dimensional vision transformer compression via dependency guided gaussian process search",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Hsu",
                "Y.-C",
                "T. Hua",
                "S. Chang",
                "Q. Lou",
                "Y. Shen",
                "H. Jin"
            ],
            "title": "Language model compression with weighted low-rank factorization",
            "venue": "arXiv preprint arXiv:2207.00112,",
            "year": 2022
        },
        {
            "authors": [
                "E. Jang",
                "S. Gu",
                "B. Poole"
            ],
            "title": "Categorical reparameterization with gumbel-softmax",
            "venue": "arXiv preprint arXiv:1611.01144,",
            "year": 2016
        },
        {
            "authors": [
                "Kim",
                "Y.-D",
                "E. Park",
                "S. Yoo",
                "T. Choi",
                "L. Yang",
                "D. Shin"
            ],
            "title": "Compression of deep convolutional neural networks for fast and low power mobile applications",
            "venue": "arXiv preprint arXiv:1511.06530,",
            "year": 2015
        },
        {
            "authors": [
                "N. Kumari",
                "B. Zhang",
                "R. Zhang",
                "E. Shechtman",
                "Zhu",
                "J.-Y"
            ],
            "title": "Multi-concept customization of text-to-image diffusion",
            "venue": "arXiv preprint arXiv:2212.04488,",
            "year": 2022
        },
        {
            "authors": [
                "Y. Li",
                "G. Yuan",
                "Y. Wen",
                "J. Hu",
                "G. Evangelidis",
                "S. Tulyakov",
                "Y. Wang",
                "J. Ren"
            ],
            "title": "Efficientformer: Vision transformers at mobilenet speed",
            "venue": "arXiv preprint arXiv:2206.01191,",
            "year": 2022
        },
        {
            "authors": [
                "L. Liebenwein",
                "A. Maalouf",
                "D. Feldman",
                "D. Rus"
            ],
            "title": "Compressing neural networks: Towards determining the optimal layer-wise decomposition",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "H. Liu",
                "K. Simonyan",
                "Y. Yang"
            ],
            "title": "Darts: Differentiable architecture search",
            "venue": "arXiv preprint arXiv:1806.09055,",
            "year": 2018
        },
        {
            "authors": [
                "L. Liu",
                "Y. Ren",
                "Z. Lin",
                "Z. Zhao"
            ],
            "title": "Pseudo numerical methods for diffusion models on manifolds",
            "venue": "arXiv preprint arXiv:2202.09778,",
            "year": 2022
        },
        {
            "authors": [
                "Z. Liu",
                "Y. Lin",
                "Y. Cao",
                "H. Hu",
                "Y. Wei",
                "Z. Zhang",
                "S. Lin",
                "B. Guo"
            ],
            "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2021
        },
        {
            "authors": [
                "Z. Liu",
                "J. Ning",
                "Y. Cao",
                "Y. Wei",
                "Z. Zhang",
                "S. Lin",
                "H. Hu"
            ],
            "title": "Video swin transformer",
            "venue": "arXiv preprint arXiv:2106.13230,",
            "year": 2021
        },
        {
            "authors": [
                "B. Pan",
                "R. Panda",
                "Y. Jiang",
                "Z. Wang",
                "R. Feris",
                "A. Oliva"
            ],
            "title": "Ia-red 2: Interpretability-aware redundancy reduction for vision transformers",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Z. Pan",
                "B. Zhuang",
                "J. Liu",
                "H. He",
                "J. Cai"
            ],
            "title": "Scalable vision transformers with hierarchical pooling",
            "venue": "In Proceedings of the IEEE/cvf international conference on computer vision,",
            "year": 2021
        },
        {
            "authors": [
                "A. Parashar",
                "P. Raina",
                "Y.S. Shao",
                "Chen",
                "Y.-H",
                "V.A. Ying",
                "A. Mukkara",
                "R. Venkatesan",
                "B. Khailany",
                "S.W. Keckler",
                "J. Emer"
            ],
            "title": "Timeloop: A systematic approach to dnn accelerator evaluation",
            "year": 2019
        },
        {
            "authors": [
                "G. Parmar",
                "R. Zhang",
                "Zhu",
                "J.-Y"
            ],
            "title": "On buggy resizing libraries and surprising subtleties in fid calculation",
            "venue": "arXiv preprint arXiv:2104.11222,",
            "year": 2021
        },
        {
            "authors": [
                "A. Ramesh",
                "P. Dhariwal",
                "A. Nichol",
                "C. Chu",
                "M. Chen"
            ],
            "title": "Hierarchical text-conditional image generation with clip latents",
            "venue": "arXiv preprint arXiv:2204.06125,",
            "year": 2022
        },
        {
            "authors": [
                "Y. Ren",
                "B. Wang",
                "L. Shang",
                "X. Jiang",
                "Q. Liu"
            ],
            "title": "Exploring extreme parameter compression for pre-trained language models",
            "venue": "arXiv preprint arXiv:2205.10036,",
            "year": 2022
        },
        {
            "authors": [
                "R. Rombach",
                "A. Blattmann",
                "D. Lorenz",
                "P. Esser",
                "B. Ommer"
            ],
            "title": "High-resolution image synthesis with latent diffusion models",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "N. Ruiz",
                "Y. Li",
                "V. Jampani",
                "Y. Pritch",
                "M. Rubinstein",
                "K. Aberman"
            ],
            "title": "Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation",
            "venue": "arXiv preprint arXiv:2208.12242,",
            "year": 2022
        },
        {
            "authors": [
                "C. Saharia",
                "W. Chan",
                "S. Saxena",
                "L. Li",
                "J. Whang",
                "E. Denton",
                "S.K.S. Ghasemipour",
                "B.K. Ayan",
                "S.S. Mahdavi",
                "Lopes",
                "R. G"
            ],
            "title": "Photorealistic text-to-image diffusion models with deep language understanding",
            "venue": "arXiv preprint arXiv:2205.11487,",
            "year": 2022
        },
        {
            "authors": [
                "D. Shi",
                "C. Tao",
                "Y. Jin",
                "Z. Yang",
                "C. Yuan",
                "J. Wang"
            ],
            "title": "Upop: Unified and progressive pruning for compressing vision-language transformers",
            "venue": "arXiv preprint arXiv:2301.13741,",
            "year": 2023
        },
        {
            "authors": [
                "M. Tan",
                "Q. Le"
            ],
            "title": "Efficientnet: Rethinking model scaling for convolutional neural networks",
            "venue": "In International conference on machine learning,",
            "year": 2019
        },
        {
            "authors": [
                "Y. Tang",
                "Y. Wang",
                "Y. Xu",
                "D. Tao",
                "C. Xu"
            ],
            "title": "Scop: Scientific control for reliable neural network pruning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Y. Tang",
                "K. Han",
                "Y. Wang",
                "C. Xu",
                "J. Guo",
                "D. Tao"
            ],
            "title": "Patch slimming for efficient vision transformers",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "H. Touvron",
                "M. Cord",
                "M. Douze",
                "F. Massa",
                "A. Sablayrolles",
                "H. J\u00e9gou"
            ],
            "title": "Training data-efficient image transformers & distillation through attention",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "S. Wang",
                "B.Z. Li",
                "M. Khabsa",
                "H. Fang",
                "H. Ma"
            ],
            "title": "Linformer: Self-attention with linear complexity",
            "venue": "arXiv preprint arXiv:2006.04768,",
            "year": 2020
        },
        {
            "authors": [
                "T. Wolf",
                "L. Debut",
                "V. Sanh",
                "J. Chaumond",
                "C. Delangue",
                "A. Moi",
                "P. Cistac",
                "T. Rault",
                "R. Louf",
                "M Funtowicz"
            ],
            "title": "Huggingface\u2019s transformers: State-of-the-art natural language processing",
            "year": 1910
        },
        {
            "authors": [
                "J. Xiao",
                "C. Zhang",
                "Y. Gong",
                "M. Yin",
                "Y. Sui",
                "L. Xiang",
                "D. Tao",
                "B. Yuan"
            ],
            "title": "Haloc: Hardware-aware automatic low-rank compression for compact neural networks",
            "year": 2023
        },
        {
            "authors": [
                "E. Xie",
                "W. Wang",
                "Z. Yu",
                "A. Anandkumar",
                "J.M. Alvarez",
                "P. Luo"
            ],
            "title": "Segformer: Simple and efficient design for semantic segmentation with transformers",
            "venue": "arXiv preprint arXiv:2105.15203,",
            "year": 2021
        },
        {
            "authors": [
                "M. Yin",
                "Y. Sui",
                "S. Liao",
                "B. Yuan"
            ],
            "title": "Towards efficient tensor decomposition-based dnn model compression with optimization framework",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2021
        },
        {
            "authors": [
                "M. Yin",
                "H. Phan",
                "X. Zang",
                "S. Liao",
                "B. Yuan"
            ],
            "title": "Batude: Budget-aware neural network compression based on tucker decomposition",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "M. Yin",
                "Y. Sui",
                "W. Yang",
                "X. Zang",
                "Y. Gong",
                "B. Yuan"
            ],
            "title": "Hodec: Towards efficient high-order decomposed convolutional neural networks",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "M. Yin",
                "B. Uzkent",
                "Y. Shen",
                "H. Jin",
                "B. Yuan"
            ],
            "title": "Gohsp: A unified framework of graph and optimization-based heterogeneous structured pruning for vision transformer",
            "venue": "arXiv preprint arXiv:2301.05345,",
            "year": 2023
        },
        {
            "authors": [
                "H. Yu",
                "J. Wu"
            ],
            "title": "Compressing transformers: Features are low-rank, but weights are not",
            "year": 2023
        },
        {
            "authors": [
                "J. Yu",
                "Y. Xu",
                "J.Y. Koh",
                "T. Luong",
                "G. Baid",
                "Z. Wang",
                "V. Vasudevan",
                "A. Ku",
                "Y. Yang",
                "Ayan",
                "B. K"
            ],
            "title": "Scaling autoregressive models for content-rich text-to-image generation",
            "venue": "arXiv preprint arXiv:2206.10789,",
            "year": 2022
        },
        {
            "authors": [
                "S. Yu",
                "T. Chen",
                "J. Shen",
                "H. Yuan",
                "J. Tan",
                "S. Yang",
                "J. Liu",
                "Z. Wang"
            ],
            "title": "Unified visual transformer compression",
            "venue": "arXiv preprint arXiv:2203.08243,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Recently the attention-based vision models have achieved comparable or superior performance than the convolutioncentered architecture in various computer vision tasks, demonstrating the promising benefit brought by using an attention mechanism (Liu et al., 2021b; Caron et al., 2021;\n1Rutgers University 2Snap Inc. Correspondence to: Jinqi Xiao <jinqi.xiao@rutgers.edu>.\nProceedings of the 40 th International Conference on Machine Learning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright 2023 by the author(s).\nXie et al., 2021; Carion et al., 2020). On the downside, these emerging architectures, e.g., vision transformer (ViT) and its variants (Touvron et al., 2021; Liu et al., 2021a; Li et al., 2022), suffer from even larger model sizes and higher computational costs than convolutional neural networks (CNNs), hindering their efficient deployment in many practical resource-constrained scenarios.\nAn attractive solution to this challenge is to perform model compression, a strategy that can reduce network size without affecting task performance. Motivated by the huge prior success of compressing CNNs (Hinton et al., 2015; Han et al., 2015), several recent studies (Yin et al., 2023; Yu et al., 2022b; Hou & Kung, 2022) have proposed to apply one (e.g., pruning) or combining several (e.g., pruning and knowledge distillation) compression methods for vision transformers, bringing considerable reduction in model size and/or FLOPs.\nDifferent from the existing works, this paper aims to address the above analyzed efficiency challenge from another perspective \u2013 exploring the low-rankness of the attentionbased vision models. To date, a rich set of low-rank compression techniques for CNNs have been proposed in the literature (Kim et al., 2015; Yin et al., 2021; Liebenwein et al., 2021; Yin et al., 2022b;a; Xiao et al., 2023; Xiang et al., 2023). However, consider 1) there exists a substantial difference on network architecture and operation mechanism, e.g., multi-head attention in ViTs vs. channel-wise convolution in CNNs; and 2) as indicated in (Yu & Wu, 2023) and verified by our analysis, many weight matrices in the vision transformers do not exhibit low rankness, it is not clear that whether low-rank compression would bring the satisfied improvement on model efficiency. From the perspective of practical deployment, a question naturally arises: For compressing attention-based vision models, can exploring model low-rankness provide comparable or even better performance than other methods such as pruning?\nTo answer this question and fully unleash the potential of low-rank compression for ViTs and attention-based models, this paper first investigates the low-rankness in the multiahead attention layer, and proposes that the head-level lowrankness, instead of weight matrix-level, should be explored. Based on this new insight, we then develop a highly efficient\nar X\niv :2\n30 5.\n17 23\n5v 2\n[ cs\n.C V\n] 9\nJ un\n2 02\n3\nlow-rank ViT compression solution with automatic rank selection. Compared with the state-of-the-art ViT pruning methods, the proposed approach can achieve 0.45% and 0.76% higher top-1 accuracy even with fewer parameters, for compressing DeiT-small and DeiT-base models on ImageNet dataset, respectively. Furthermore, our finding can also be applied to improve the efficiency for customizing text-to-image diffusion modules (Ruiz et al., 2022; Kumari et al., 2022), a recent emerging and important computer vision task, with much faster training (up to 2.6\u00d7 speedup) and lower extra storage cost (up to 1927.5\u00d7 reduction) than the state-of-the-art customization solutions."
        },
        {
            "heading": "2. Related Works",
            "text": "Pruning for Vision Transformers. To reduce the model size and achieve practical speedup, structured pruning on different substructures of ViT models, e.g., attention heads, blocks, and rows of weight matrices, have been studied in the literature (Hou & Kung, 2022; Yu et al., 2022b; Zhu et al., 2021; Chen et al., 2021b). In addition, another research direction proposes to improve model processing speed via using dynamic or static token pruning (Bolya et al., 2022; Pan et al., 2021b; Tang et al., 2022; Goyal et al., 2020; Pan et al., 2021a). Recently, Yu et al. (Yu et al., 2022b) develop a unified framework to jointly perform pruning, knowledge distillation and block skipping, achieving state-of-the-art ViT compression performance.\nLow-rank Compression of Weight Matrices (W ) in NLP Transformers. Most of the existing studies on low-rank compressed transformers are concentrated in the NLP field. Noach et al. (Noach & Goldberg, 2020) decompose the weight matrices of the pre-trained language models (PLMs) using SVD and perform feature distillation to improve model performance. Ren et al. (Ren et al., 2022) adopt tensor decomposition to compress PLMs and achieve practical inference speedups. Hsu et al. (Hsu et al., 2022) introduce Fisher information to measure the importance of parameters to factorize task-specific PLMs.\nLow-Rank Approximation for Attention Matrices (Q, K, V ). Another line of works is to perform low-rank approximation for the attention matrices, the intermediate results from the attention mechanism. Different types of approximation schemes, including adding additional projection matrices and sparse approximation, have been investigated (Wang et al., 2020; Choromanski et al., 2020; Chen et al., 2021a). As the orthogonal effort from our approach, these methods do not reduce the model sizes of transformers.\nPersonalized Text-to-Image Diffusion Models. Recently released text-to-image diffusion models (Rombach et al., 2022; Ramesh et al., 2022; Saharia et al., 2022; Yu et al., 2022a) have shown impressive content-generation capabil-\nity. A very emerging and practical demand is to make these pre-trained models customized for a user-provided specific concept. To that end, some efforts leverage transfer learning via fine-tuning all the parameters or introducing a word vector for the new concept (Ruiz et al., 2022; Gal et al., 2022). However, the large sizes of the diffusion models bring costly training time and high extra storage requirements during the fine-tuning. To improve the efficiency of customization, Kumari et al. (Kumari et al., 2022) propose to only fine-tune the key and value mapping from text to latent features in the cross-attention layers; while freezing other parts."
        },
        {
            "heading": "3. Method",
            "text": ""
        },
        {
            "heading": "3.1. Preliminaries",
            "text": "The attention operation can be viewed as the mapping from a query and a set of key-value pairs to an output. To better extract and learn the information from different representation subspace and spatial regions, the state-of-the-art attentionbased vision models adopt multi-head attention (MHA) as:\nMHA(XQ, XK , XV ) = Concat(head1, . . . , headh)W O, (1)\nwhere XQ, XK , XV \u2208 Rn\u00d7dm are the input embedding matrices, n is the sequence length, dm is the embedding dimension, and h is the number of heads. For each headi, it performs attention operation as follows:\nheadi = Attention(XQW Q i , XKW K i , XV W V i )\n= Softmax( XQW\nQ i (XKW K i ) T\n\u221a dk\n)XV W V i ,\n(2)\nwhere WQi , W K i \u2208 Rdm\u00d7dk , WVi \u2208 Rdm\u00d7dv , WO \u2208 Rhdv\u00d7dm are the weight matrices, and dk and dv are the dimension of XQ and XK , respectively. Since dk = dv = dm h , for simplicity we use d instead of dk and dv ."
        },
        {
            "heading": "3.2. Exploring Low-Rankness in MHA Layer",
            "text": "In this subsection, we describe our proposed low-rank MHA for efficient vision models. We first demonstrate the lowrankness of the weight matrices in each attention head, and analyze the limitation when only leveraging the matrix-level low-rankness. Built on these observation and analysis, we then propose to explore the head-level low-rankness and formulate the mechanism. We further detail the procedure of using this finding to improve model efficiency in two important scenarios: compressing vision transformers and customization of diffusion models.\nLow-Rankness of WQi , WKi , WVi , WO. Low-rankness of the weight matrices has been widely observed in many types of deep learning architectures including CNN and NLP transformers (Kim et al., 2015; Ren et al., 2022), inspiring us to explore its potential existence in the attention-based\n0 4 8 12 16 20 24 28 32 36 40 44 48 52 56 60 64\nSingular Value (SV) index\n{WQi }\n{WKi }\n{WVi } De co\nm po\nse d\nm at\nric es\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 1. The heatmap of the cumulative singular values after performing SVD for each weight matrices in the MHA layers of the pre-trained DeiT-base model. It is seen that some matrices in some layers exhibit weaker low-rankness than others, implying that directly factorizing individual weight matrix may not be efficient.\n0 20 40 60 80 100 120 140 Matrix index\n0.2 0.4 0.6 0.8 1.0 # Pa ra m s\n\u00d7105\nWQi , WKi WQKi\n0 20 40 60 80 100 120 140 Matrix index\n0.4\n0.6\n0.8\n1.0\n# Pa\nra m\ns\n\u00d7105\nWVi , WOi WVOi\nFigure 2. The number of parameters when the ratio of the cumulative singular values reaches 90% with factorizing WQ,WK ,WV ,WO , and the corresponding WQK ,WV O of the pre-trained DeiT-base. It is seen that exploring head-level lowrankness is more parameter efficient than matrix-level.\nvision models. To that end, we analyze the distributions of the singular values of the weight matrices (WQi , W K i , WVi ,W O) in the pre-trained DeiT-base model (Touvron et al., 2021). Figure 1 shows the heatmap of the cumulative singular values after applying Singular Value Decomposition (SVD) into each weight matrices. It is seen that the phenomenon that most information is concentrated in part of singular values (the largest ones) indeed exist in the weight matrices across different heads and layers, indicating the potential of exploring low rankness of attention-based models.\nLimitation of Matrix-Level Low-Rankness. Based on the above observation, a natural idea is to construct each weight matrix (WQi , W K i , W V i ,W O i ) in its own low-rank format. However, we argue that the benefit brought this straightforward strategy would not be significant. As shown in Figure 1, some types of weight matrices, e.g., WVi , do not exhibit sufficient low-rankness, a phenomenon that is also observed in the matrices of the higher layers, thereby limiting the overall potential performance improvement brought by low-rank factorization.\nExploring Head-Level Low-Rankness. To better leverage the low rankness in the MHA layer and fully unleash the potential benefits, we propose to explore the low-rank property at the head level for efficient multi-head attention. Our idea is motivated by the observation that there exists consecutive linear transformations in the attention head, e.g., XQW Q i (XKW K i ) T = XQ(W Q i W K i T )XTK in Eq. 2, opening up the opportunities of constructing the\ncombinations of weight matrices, e.g., WQi W K i T , instead of the individual matrix, in the low-rank format. According to linear algebra, such reformulation brings two benefits. First, it provides more parameter-efficient low-rank solution. More specifically, for two full-rank matrices A \u2208 Rn\u00d7d and B \u2208 Rd\u00d7n, the total number of parameters of their rank-r approximations A\u2032 \u2208 Rn\u00d7r and B\u2032 \u2208 Rr\u00d7n is 2nr; while the same rank-r approximation C \u2032 \u2208 Rn\u00d7r for C = AB only contains nr parameters. Second, it relaxes the constraints of applying low-rankness approximation. Since the low-rankness of A or B, instead of both, is sufficient to lead to the low-rank C, it indicates that the head-level lowrankness is a more common and feasible opportunity when aiming to leverage low-rankness in the MHA layer.\nMotivated by these benefits, now we formulate our idea in the context of multi-head attention mechanism. First Eq. 1 and Eq. 2 can be reformulated as:\nMHA(XQ, XK , XV ) = h\u2211 i=1 headiW O i\n= h\u2211 i=1 Softmax( XQ(W Q i W K i T )XTK\u221a dk )XV (W V i W O i ), (3)\nwhere WOi \u2208 Rd\u00d7dm and WO = Concat(WO1 , . . . ,WOh ). Recall that a matrix W \u2208 Rin\u00d7out can be low-rank approximated by performing SVD as W \u2248 W \u2032 = U\u03a3S\u2032 = US, where U \u2208 Rin\u00d7r, S\u2032 \u2208 Rr\u00d7out, diagonal matrix \u03a3 \u2208 Rr\u00d7r, S = \u03a3S\u2032 \u2208 Rr\u00d7out, and r is the rank value. Then the entire multi-head attention (Eq. 3) can be constructed in the low-rank format as:\nMHA(XQ, XK , XV )\n\u2248 h\u2211\ni=1\nSoftmax( XQ(U\nQ i S K i T )XTK\u221a\ndk )XV (U\nV i S O i )\n= Concat(head\u20321, . . . , head \u2032 h)W \u2032O, where\nhead\u2032i = Attention(XQU Q i , XKS K i , XV U V i )\n= Softmax( XQU\nQ i (XKS K i ) T\n\u221a dk\n)XV U V i ,\nSO = Concat(SO1 , . . . , S O h ).\n(4)\nHere WQi W K i T \u2248 UQi SKi T (rank = r1), WVi W O i \u2248 UVi S O i (rank = r2), U Q i , S K i \u2208 Rdm\u00d7r1 , UVi , SOi \u2208 Rdm\u00d7r2 , and SO \u2208 Rhr2\u00d7dm .\nNotice that as shown in Eq. 4, in addition to exploring the low-rankness of WQKi = W Q i W K i\nT , the inter-matrix correlation between WVi and W O i is also considered, bringing the low-rank construction for WV Oi = W V i W O i . In Figure 2, we compare the direct low-rank decomposition of WQi and W K i with the decomposition of W Q i W K i\nT and report the required number of parameters after performing low-rank factorization in the case that the ratio of the cumulative singular values reaches 90%. By comparing the required number of parameters to reach this threshold, we can evaluate and compare the low-rankness of the original matrices, i.e., fewer parameters indicate better low-rankness. This is because to preserve the same amount of information, e.g., 90% cumulative singular values, the matrix with better low-rankness requires fewer parameters. As shown in this figure, the number of parameters required for factorizing WQi W K i\nT (blue line) to reach the 90% threshold is always smaller than that for WQi and W K i (red line), indicating that the combination matrix shows better low-rankness. Table 1 illustrates the benefit of such head-level low-rank MHA mechanism, with its application for fine-tuning-free ViT compression as example. Compared to directly applying SVD to the individual weight matrices, our approach brings much higher model accuracy with the same compression ratio, verifying the two benefits (parameter efficiency and relaxed low-rank constraint) indicated in our prior analysis.\nLow-Rank MHA for Vision Transformer Compression. A direct application of our proposed low-rank MHA is to compress vision transformers. In general, for a b-block ViT with one MHA layer and one 2-layer feedforward network (FFN) per block, the corresponding compression task using low-rank MHA can be formulated as follows:\nmin {WQKi,j ,W V O i,j ,W FFN k,j }h,b,2 i=1,j=1,k=1\nL({WQKi,j ,W V O i,j ,W FFN k,j })\ns.t. b\u2211\nj=1\n( h\u2211 i=1 C(R(WQKi,j )) + C(R(W V O i,j ))\n+ 2\u2211 k=1 C(R(WFFNk,j ))) \u2264 \u03b5,\n(5)\nwhere L(\u00b7), C(\u00b7) and R(\u00b7) are the functions that return the loss, cost (e.g., model size or FLOPs) and rank, respectively. WQKi,j = W Q i,jW K i,j T , WV Oi,j = W V i,jW O i,j and W FFN k,j are the combination matrices of the i-th attention head and weight matrix in the FFN in the j-th block. It is seen that given the target cost budget (\u03b5) of the compressed ViTs, rank is an important type of hyperparameter that directly determines the accuracy and complexity. In practice, because the huge range of the possible rank values, the proper rank selection for all the blocks and layers of vision transformers is challenging. For instance, there exist 4.53\u00d7 10188 rank combinations when performing low-rank compression for DeiT-small model, making manual selection impracticable.\nTo address this challenge, we propose an automatic rank selection approach to efficiently incorporate low-rank MHA into ViT compression. Our key idea is to interpret the automatic rank selection of low-rank compression as a specialized neural architecture search (NAS), considering the fact that the choice of the rank essentially decides the final structure of the compressed ViT. From this insight, the proper rank value can be identified via differentiable samplingbased search, a strategy has been well studied in NAS literature (Liu et al., 2018; Wu et al., 2019; Tan & Le, 2019).\nFigure 4 illustrates the overall framework for automatic rank search for low-rank ViT. Here for simple notation, we use W \u2217j \u2208 Rinj\u00d7outj to denote the matrices that need to be decomposed in the j-th layer, i.e., WQKi,j , WV Oi,j and W FFN k,j , with the candidate rank set as Rj = {r1j , r2j , raj , . . . rmaxj }. Assume that r\u2217j \u2208 Rj is the currently selected rank for Wj \u2248 W \u2217j = U\u2217j S\u2217j (U\u2217j \u2208 Rinj\u00d7r \u2217 j and S\u2217j \u2208 Rr \u2217 j\u00d7outj ). Then we alternately update the selection probability Pj = {p1j , p2j , paj , . . . pmaxj } for rank candidates and the parameters of W \u2217j . To be specific, because Pj is calculated via GumbelSoftmax (Jang et al., 2016), i.e., Pj = GumbelSoftmax(\u03b1j) with learnable vector \u03b1j, Pj can be updated via minimizing the following loss (with the frozen weight parameters):\nLProb = LCE(Y , Y ) \u00b7 ( \u2211b j=1 \u2211 paj C(raj )\n\u03b5 )\u03b2 , (6)\nwhere LCE(\u00b7) is the cross-entropy loss, Y is the final output of the entire model, Y is the ground truth, and \u03b2 is the hyper-parameter controlling overall search process. Here as shown in Figure 4, the calculation of Y = Yb is based on considering all the decomposition candidates (Uaj , S a j ) with different rank settings and their selection probabilities. After finishing the probability update, W \u2217j is first factorized via using the rank that corresponds to the largest selection probability, and then updated via minimizing the cross-entropy loss with the frozen rank selection probabilities. The rank settings can be then finally determined after multiple rounds of such alternated update of probabilities and weights.\nLow-Rank MHA for Personalized Text-to-Image Diffusion. Our proposed low-rank MHA can also be used for efficiently customizing text-to-image diffusion, an emerging computer vision task that the pre-trained diffusion model can quickly synthesize high-quality visual instantiations of user-defined concepts with few examples of images and guided text prompt. More specifically, given a pre-trained diffusion model {Wdiff} that has been well trained on image set {x} and the condition vector set {c} obtained from text prompt, we aim to minimize the following loss:\nE\u03f5,xnew,cnew,t[wt\u2225{Wnew}(\u03b1txnew + \u03c3t\u03f5, cnew) \u2212 xnew\u222522], (7)\nwhere \u03b1t, \u03c3t and wt are the function of timestep t controlling diffusion process, and xnew and cnew denote the user-provided images and text prompts, respectively, with |{xnew}| \u226a |{x}| and |{cnew}| \u226a |{c}|. Notice that here {Wnew} is initialized as f({Wdiff}), where f(\u00b7) can be either an identity function, meaning that the personalized model {Wnew} is directly initialized as the pre-trained {Wdiff}, or a transformation function, indicating that the initialization for {Wnew} is the modification of {Wdiff}.\nAs indicated in (Kumari et al., 2022), updating the entire {Wnew} is very computationally inefficient and easily\ncauses overfitting, due to the large size of {Wdiff} and small size of {xnew}. Inspired by the insights that 1) only changing a few parameters is sufficiently to make the diffusion models learn the user-defined concept (Kumari et al., 2022); and 2) adding the low-rank component is an efficient fine-tuning strategy for large-size language models (LLMs) in NLP tasks (Aghajanyan et al., 2020), we propose to use the low-rank MHA to improve the deployment efficiency of personalized text-to-image diffusion. Figure 5 illustrates the overall framework. More specifically, the MHA layer of the personalized model is initialized as:\nMHA(XQ, XK , XV ) =\nh\u2211 i=1 Softmax( XQ(W Q i W K i T + UQi S K i T )XTK\u221a dk )\nXV (W V i W O i + (U V i S O i ))\n(8)\nwhere {WQi ,WKi ,WVi ,WOi } are obtained from the pretrained diffusion model {Wdiff}, and {UQi , SKi , UVi , SOi } are the randomly initialized low-rank components. As shown in Figure 5, in the model customization process all the parameters of the pre-trained model {Wdiff} are frozen; while their computation follows the mechanism described in our proposed low-rank MHA. Meanwhile, the added lowrank component UQi , S K i , U V i and S O i are updated to make Wnew adapt for the user-provided new concepts."
        },
        {
            "heading": "4. Experiments",
            "text": ""
        },
        {
            "heading": "4.1. Image Classification on ImageNet-1K",
            "text": "Setting. We first validate our approach on the ImageNet-1K dataset (Deng et al., 2009) for the image classification task. The dataset includes 1.2M training images and 50K validation samples. We adopt the baseline models, i.e., dense networks without compression, and training recipe from DeiT (Touvron et al., 2021) since they show promising results on training the transformer models by only using\nthe ImageNet-1K without other large-scale datasets for pretraining and thus are widely adopted. We compare our approach with previous state-of-the-art ViT compression methods, including low-rank (Yu & Wu, 2023), model pruning (Hou & Kung, 2022; Yu et al., 2022b; Zhu et al., 2021; Tang et al., 2020), sparse training (Chen et al., 2021b) and token pruning (Bolya et al., 2022; Pan et al., 2021b; Tang et al., 2022; Goyal et al., 2020; Pan et al., 2021a).\nImplementation Details. The whole process of our method consists of two steps: searching ranks and fine-tuning. We conduct the automatic rank selection algorithm to the pretrained DeiT model to produce the low-rank model under the given constraint. In the fine-tuning process, the initial learning rate is set as 0.0001 and decreases to the minimum learning rate of 0.000001 with the Cosine scheduler. The weight decay for training the compressed DeiT-small is set as 0.005. The rest of the training hyper-parameters are consistent with DeiT (Touvron et al., 2021).\nComparison Results. Table 2 shows the performance of different compression methods on the ImageNet dataset. Compared with the previous state-of-the-art automatic pruning method UVC (Yu et al., 2022b), our compressed models enjoy 0.45% and 1.69% higher top-1 accuracy with much larger FLOPs reduction on DeiT-small and DeiT-base, respectively. Additionally, our approach can significantly reduce the model parameters, i.e., 49.98% reduction for DeiTsmall and 61.06% for DeiT-base, while the compressed model from UVC (Yu et al., 2022b) can not. Compared with low-rank work of CT-GFM (Yu & Wu, 2023), our method achieves 0.98% accuracy increase with much fewer number of parameters. Compared with sparse training work of S2ViTE (Chen et al., 2021b), with similar top-1 accuracy, we achieve much larger FLOPs reduction as 19.58% and 28.55% and parameters reduction as 16.04% and 26.65% on DeiT-small and DeiT-base, respectively. Compared with\nTable 3. Measured speedup for the low-rank compressed DeiT-small and DeiT-base models on different computing platforms.\nModel #Params (M) FLOPs (G) Top-1 (%) Throughput (images/s)\nNvidia V100 Snapdragon 855 Nvidia JetsonTX2 ASIC Eyeriss FPGA\nDeiT-small 21.96 4.24 79.8 974.46 7.26 27.37 24.38 4.02 COMCAT (Ours) 10.98 2.07 79.27 1512.91 11.00 40.36 39.34 6.66 DeiT-base 86.38 16.85 81.8 301.34 1.73 9.36 6.14 0.95 COMCAT (Ours) 33.63 6.46 82.26 602.51 4.37 17.80 14.87 2.09\nthe work of PS-ViT (Tang et al., 2022) for token reduction, our method also achieves higher top-1 accuracy as 0.18% and 0.76% with much fewer FLOPs and model parameters on DeiT-small and DeiT-base, respectively.\nPractical Speedups on Various Hardware Platforms. We further measure the practical speedups of our compressed models on various computing hardware platforms, including Nvidia Tesla V100, Nvidia Jetson TX2, Android mobile phone (Snapdragon 855, 4 Cortex-A76 + 4 Cortex-A55), ASIC accelerator Eyeriss (Chen et al., 2016), and FPGA (PYNQ Z1) in Table 3. Here the performance of Eyeriss is reported via using Timeloop (Parashar et al., 2019) with 45nm CMOS technology setting. Our compressed DeiTsmall and DeiT-base models achieve significant speedups across different platforms. For example, on Snapdragon 855, our compressed DeiT-base obtains 2.52\u00d7 speedup than the baseline model with even higher top-1 accuracy on ImageNet. Such results demonstrate the practical effectiveness of our low-rank compression solution."
        },
        {
            "heading": "4.2. Ablation Analysis for ViT Low-Rank Compression",
            "text": "Automatic Rank Selection vs. Manual Rank Selection. To demonstrate the superiority of our automated rank se-\nlection method, we compare it with the fixed rank method. Figure 6 shows the variation curves of the top-1 accuracy on ImageNet-1K and the number of parameters for both methods during training. Our method has a smaller loss of accuracy, and the number of parameters of the model gradually converges to the target value (10.5M). In contrast, the model based on fixed-rank decomposition loses more accuracy from the beginning, resulting in poor performance. Therefore, we can conclude that our method can search for a better rank combination under the constraints.\nHyper-Parameter for Searching Ranks. We also explore the effect of hyper-parameter \u03b2 mentioned by Eq. 6 on searching rank process. Figure 7 shows the convergence of the searching process with respect to different \u03b2. It can be seen that when \u03b2 \u2265 1, the number of parameters of the model can converge to the target value quickly, and the final accuracy of the models is basically the same. However, when \u03b2 = 1.5, the accuracy curve of the model is relatively smooth, therefore, we think 1.5 is a relatively better value for \u03b2. The final rank distribution of DeiT-small and DeiT-base are shown in Figure 8 and Figure 9.\nSVD vs. Higher-order Tensor Decomposition. For ViT compression, in addition to the MHA, we apply the lowrank compression to the FFN (Feed-Forward Network), and the rank selection for FFN is also included in our proposed automatic rank determination mechanism. In order to find the optimal low-rank decomposition method from various low-rank decomposition methods such as SVD, Tucker decomposition, Tensor Train decomposition, we evaluate the Top-1 accuracy and throughput on GPU for the compressed DeiT-Small on ImageNet dataset when compressing FFNs using different low-rank methods. As shown in Table 4, with the same compression rate, using SVD brings higher accuracy (without fine-tuning) than using Tucker decomposition and Tensor Train decomposition with better throughput on GPU.\n*The extra storage means that, given a pre-trained model, the extra storage requirement when the pre-trained model is further fine-tuned to adapt to a new customized concept. Notice that here we follow the same definition for extra storage cost used in Customize Diffusion (Kumari et al., 2022). That is, in customization scenario, because the pre-trained model needs to be always preserved for future more new concepts, any modification on the pre-trained model for the current new concept is viewed as extra storage cost. The amount of extra storage is obtained via direct measurement of file size."
        },
        {
            "heading": "4.3. Personalized Text-to-Image Diffusion Models",
            "text": "Setting. We fine-tune the cross-attention layer of the pretrained Stable Diffusion (Rombach et al., 2022) (model weights obtained from HuggingFace Hub1 (Wolf et al., 2019)) by using our proposed low-rank MHA mechanism to enable the model to learn a new concept. For quantitative evaluation, we use the six objects included in the dataset released by CustomDiffusion (Kumari et al., 2022) and one newly collected object, with the number of images contained\n1https://huggingface.co/CompVis/stable-diffusion-v1-4\nin each one ranging from 4 to 12.\nTraining Cost Comparison. We first present the training cost required by all approaches in Table 5. We train all the approaches on one Nvidia RTX A6000 GPU with the batch size as 1 and the number of training steps as 500. Compared with CustomDiffusion and DreamBooth, our approach reduces the training time by 18.6% and 61.6%, respectively (see Training Time in Table 5), and decreases the extra storage space for each concept by 12.5\u00d7 and 1927.5\u00d7, respectively (see Extra Storage in Table 5). The order of magnitude reduction of extra storage for each new concept is extremely important for the broad adoption of personalized text-to-image diffusion models, where users can prepare their diffusion models without the burden of model storage.\nImage Quality Comparison. We then evaluate the quality of the synthesized images for all approaches. We generate 20 images for each learned target image (concept) by using the same text prompt for all approaches and calculate the FID (Heusel et al., 2017; Parmar et al., 2021) between the\nsynthesized and real images. The lower FID score indicates the smaller difference between the generated and the real images. As shown in Table 5, our approach achieves the lowest FID than the existing works. We further provide the qualitative comparison in Figure 10. In addition to generating the corresponding scenes accurately from text prompts, the V* Goldendoodle generated by our method is the closest to the real image, while the V* Goldendoodle synthesized by DreamBooth (Ruiz et al., 2022) has obvious differences from the real one, and images synthesized from CustomDiffusion (Kumari et al., 2022) contain less natural mouth as it has been deformed. In Figure 11, we further show that even with much fewer computation resources, i.e., less training time and fewer number of GPUs for model fine-tuning, we can still generate high-quality images.\nMS-COCO Evaluation. Lastly, we perform the experiments to understand if the fine-tuned models can generate images that are unrelated to the learned target subject (V*). We use the prompted text of 5, 000 images from the MSCOCO 2017 (Lin et al., 2014) validation set to generate images and calculate the FID. As shown in Table 6, the FID from the personalized models are similar to the pre-trained text-to-image model, indicating that they can synthesize high-quality images for unrelated concepts. Thus, the model fine-tuned by our method still holds the distribution of the synthesized images as the pre-trained model."
        },
        {
            "heading": "5. Conclusion",
            "text": "This paper fundamentally investigates the low-rankness in the multi-ahead attention layer of the emerging vision models and proposes that the head-level low-rankness should be explored for efficient model design, bringing highly efficient low-rank ViT compression solution. Our method\nnot only outperforms existing compression approaches by providing higher performance but also brings faster practical speedup. Particularly, our finding is further applied for efficient customization of text-to-image diffusion models, outperforming the state-of-the-art solutions."
        },
        {
            "heading": "6. Acknowledgements",
            "text": "This work was partially supported by National Science Foundation under Grant CCF-1937403 and CCF-1955909."
        }
    ],
    "title": "COMCAT: Towards Efficient Compression and Customization of Attention-Based Vision Models",
    "year": 2023
}