{
    "abstractText": "In this paper, we study the lattice linearity of multiplication and modulo operations. We demonstrate that these operations are lattice linear and the parallel processing algorithms that we study for both these operations are able to exploit the lattice linearity of their respective problems. This implies that these algorithms can be implemented in asynchronous environments, where the nodes are allowed to read old information from each other. These algorithms also exhibit snapstabilizing properties, i.e., starting from an arbitrary state, the sequence of state transitions made by the system strictly follows its specification.",
    "authors": [
        {
            "affiliations": [],
            "name": "Sandeep S Kulkarni"
        }
    ],
    "id": "SP:9cb07f16e59194f386c36dc247ca8bc1caf47957",
    "references": [
        {
            "authors": [
                "A. Bui",
                "A.K. Datta",
                "F. Petit",
                "V. Villain"
            ],
            "title": "State-optimal snap-stabilizing pif in tree networks",
            "venue": "In Proceedings 19th IEEE International Conference on Distributed Computing Systems,",
            "year": 1999
        },
        {
            "authors": [
                "J.T. Butler",
                "T. Sasao"
            ],
            "title": "Fast hardware computation of x mod z",
            "venue": "IEEE International Symposium on Parallel and Distributed Processing Workshops and Phd Forum,",
            "year": 2011
        },
        {
            "authors": [
                "Giovanni Cesari",
                "Roman Maeder"
            ],
            "title": "Performance analysis of the parallel karatsuba multiplication algorithm for distributed memory architectures",
            "venue": "Journal of Symbolic Computation,",
            "year": 1996
        },
        {
            "authors": [
                "Vijay Garg"
            ],
            "title": "A lattice linear predicate parallel algorithm for the dynamic programming problems",
            "venue": "In 23rd International Conference on Distributed Computing and Networking, ICDCN",
            "year": 2022
        },
        {
            "authors": [
                "Vijay K. Garg"
            ],
            "title": "Predicate detection to solve combinatorial optimization problems",
            "venue": "ACM Symposium on Parallelism in Algorithms and Architectures,",
            "year": 2020
        },
        {
            "authors": [
                "Vijay K. Garg"
            ],
            "title": "A lattice linear predicate parallel algorithm for the housing market problem",
            "venue": "Stabilization, Safety, and Security of Distributed Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Arya Tanmay Gupta",
                "Sandeep S. Kulkarni"
            ],
            "title": "Extending lattice linearity for self-stabilizing algorithms",
            "venue": "Stabilization, Safety, and Security of Distributed Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Arya Tanmay Gupta",
                "Sandeep S. Kulkarni"
            ],
            "title": "Brief announcement: Fully lattice linear algorithms. In Stabilization, Safety, and Security of Distributed Systems: 24th International Symposium, SSS 2022",
            "year": 2022
        },
        {
            "authors": [
                "A. Karatsuba",
                "Y. Ofman"
            ],
            "title": "Multiplication of many-digital numbers by automatic computers",
            "venue": "Doklady Ak- ademii Nauk SSSR,",
            "year": 1962
        },
        {
            "authors": [
                "R.L. Rivest",
                "A. Shamir",
                "L. Adleman"
            ],
            "title": "A method for obtaining digital signatures and public-key cryptosystems",
            "venue": "Commun. ACM,",
            "year": 1978
        },
        {
            "authors": [
                "Thomas Zeugmann"
            ],
            "title": "Highly parallel computations modulo a number having only small prime factors",
            "venue": "Information and Computation,",
            "year": 1992
        }
    ],
    "sections": [
        {
            "text": "ar X\niv :2\n30 2.\n07 20\n7v 4\n[ cs\n.D C\n] 2\n4 Ju\nl 2 02\nKeywords: lattice linear \u00b7 modulo \u00b7 multiplication \u00b7 self-stabilization \u00b7 asynchrony"
        },
        {
            "heading": "1 Introduction",
            "text": "Development of parallel processing algorithms to solve problems is increasingly gaining interest. This is because computing machines are manufactured with multiprocessor chips as we face a bound on the rate of architectural development of individual microprocessors. Such algorithms, in general, require synchronization among their processing nodes. Without such synchronization, the nodes perform executions based on inconsistent values possibly resulting in substantial delay or potentially incorrect computation.\nIn this context, lattice theory has provided with very useful concepts. In lattice linear systems, a partial order is induced in the state space, which allows the nodes to perform executions asynchronously. Lattice linearity was utilized in modelling the problems (where there exists a predicate, naturally describing the problem, under which the global states form a lattice, called lattice linear problems) [5] and in developing algorithms (called lattice linear algorithms) which impose a lattice structure in problems (which are not lattice linear) [7,8]. Thus, the algorithms that traverse such a state transition system can allow the nodes to perform executions asynchronously while preserving correctness.\nLattice linearity allows inducing single or multiple lattices among the global states. If self-stabilization is required, then for every induced lattice, its supremum must be an optimal state. This way, it can be ensured that the system can\n\u22c6 Technical report of the paper to appear in 25th International Symposium on Stabilization, Safety, and Security of Distributed Systems (SSS 2023).\ntraverse to an optimal state from an arbitrary state. We introduced eventually lattice linear algorithms [7] and fully lattice linear algorithms [8] for non-lattice linear problems (we discuss more on this in Section 2 for the sake of completeness, but non-lattice linear problems are not the focus of this paper). These algorithms are self-stabilizing.\nMany lattice linear problems do not allow self-stabilization [5,6,4]. While developing new algorithms for lattice linear and non-lattice linear problems is very interesting, it is also worthwhile to study if algorithms already present in the literature exploit lattice linearity of problems or if lattice linearity is present in existing algorithms. For example, lattice linearity was found to be exploited by Johnson\u2019s algorithm for shortest paths in graphs [5] and by Gale\u2019s top trading cycle algorithm for housing market [6].\nIn this paper, we study parallel implementations of two fundamental operations in mathematics: multiplication and modulo. We study algorithms for n\u00d7m and n mod m where n and m are large integers represented as binary strings.\nThe applications of integer multiplication include the computation of power, matrix products which has applications in a plethora of fields including artificial intelligence and game theory, the sum of fractions and coprime base. Modular arithmetic has applications in theoretical mathematics, where it is heavily used in number theory and various topics (groups, rings, fields, knots, etc.) in abstract algebra. Modular arithmetic also has applications in applied mathematics, where it is used in computer algebra, cryptography, chemistry and the visual and musical arts. In many of these applications, the value of the divisor is fixed.\nA crucial observation is that these operations are lattice linear. Also, the algorithms that we study in this paper are capable of computing the correct answer even if the nodes are initialized in an arbitrary state. These properties are present in these operations as opposed to many other lattice linear problems where the lattice structure does not allow self-stabilization."
        },
        {
            "heading": "1.1 Contributions of the paper",
            "text": "\u2013 We study the lattice linearity of modulo and multiplication operations. We demonstrate that these problems satisfy the contstraints of lattice linear problems [5]. This implies that some of the algorithms for them are capable of benefiting from the property of lattice linearity. \u2013 We also show that these algorithms exhibit properties that are similar to snap-stabilizing algorithms [1], i.e., starting from an arbitrary state, the sequence of state transitions made by the system strictly follows its specification, i.e., initializing in an arbitrary state, the nodes immediately start obtaining the values as expected with each action they execute."
        },
        {
            "heading": "1.2 Organization of the paper",
            "text": "In Section 2, we describe the definitions and notations that we use in this paper. In Section 3, we discuss the related work. We study lattice linearity of the multiplication operation in Section 4. Then, in Section 5, we study the lattice linearity of the modulo operation. Finally, we conclude in Section 6."
        },
        {
            "heading": "2 Preliminaries",
            "text": "This paper focuses on multiplication and modulo operations, where the operands are n and m. In the computation n \u00d7m or n mod m, n and m are the values of these numbers respectively, and |n| and |m| are the length of the bitstrings required to represent n and m respectively. If n is a bitstring, then n[i] is the ith bit of n (indices start from 1). For a bitstring n, n[1], for example, is the most significant bit of n and n[|n|] is the least significant bit of n. n[i : j] is the bitstring from ith to jth bit of n. For simplicity, we stipulate that n and m are of lengths in some power of 2. Since size of n and m may be substantially different, we provide complexity results that are of the form O(f(n,m)) in all cases.\nWe use the following string operations: (1) append(a, b), appends b to the end of a in O(1) time, (2) rshift(a, k), deletes rightmost k bits of a in O(k) time, and (3) lshift(a, k), appends k zeros to the right of a in O(k) time."
        },
        {
            "heading": "2.1 Modeling distributed programs",
            "text": "n\u00d7m or n mod m are typically thought of as arithmetic operations. However, when n and m are large, we view them as algorithms. In this paper, we view them as parallel/distributed algorithms where the nodes collectively perform computations to converge to the final output. Next, we provide the relevant definitions of a parallel/distributed program that we utilize in this paper.\nThe parallel/distributed program consists of nodes where each node is associated with a set of variables. A global state, say s, is obtained by assigning each variable of each node a value from its respective domain. s is represented as a vector, where s[i] itself is a vector of the variables of node i. S denotes the state space, which is the set of all global states that a given system can obtain.\nEach node is associated with actions. Each action at node i checks the values of other nodes and updates its own variables. An action is of the form g \u2212\u2192 ac, where g is the guard (a Boolean predicate involving variables of i and other nodes) and ac is an instruction that updates the variables of i. We assume all actions to be executed atomically.\nAn algorithmA is self-stabilizing for the subset So of S where S is the set of all global states, iff (1) convergence. starting from an arbitrary state, any sequence of computations of A reaches a state in So, and (2) closure. any computation of A starting from So always stays in So. We assume So to be the set of optimal states: the system is deemed converged once it reaches a state in So. An algorithm is snap-stabilizing iff starting from an arbitrary state, it makes the system follow a sequence of state transitions as per the specification of that system."
        },
        {
            "heading": "2.2 Execution without synchronization.",
            "text": "Typically, we view the computation of an algorithm as a sequence of global states \u3008s0, s1, \u00b7 \u00b7 \u00b7 \u3009, where st+1, t \u2265 0, is obtained by executing some action by one or more nodes in st. For the sake of discussion, assume that only node i executes in state st. The computation prefix uptil st is \u3008s0, s1, \u00b7 \u00b7 \u00b7 , st\u3009. The state that\nthe system traverses to after st is st+1. Under proper synchronization, i would evaluate its guards on the current local states of its neighbours in st.\nTo understand the execution in asynchrony, let x(s) be the value of some variable x in state s. If i executes in asynchrony, then it views the global state that it is in to be s\u2032, where x(s\u2032) \u2208 {x(s0), x(s1), \u00b7 \u00b7 \u00b7 , x(st)}. In this case, st+1 is evaluated as follows. If all guards in i evaluate to false, then the system will continue to remain in state st, i.e., st+1 = st. If a guard g evaluates to true then i will execute its corresponding action ac. Here, we have the following observations: (1) st+1[i] is the state that i obtains after executing an action in s\u2032, and (2) \u2200j 6= i, st+1[j] = st[j].\nThe model described above allows nodes to read old values of other nodes arbitrarily. In this paper, however, we require that the values of variables of other nodes are read/received in the order in which they were updated/sent.\n2.3 Embedding a <-lattice in global states.\nLet s denote a global state, and let s[i] denote the state of node i in s. First, we define a total order <l; all local states of a node i are totally ordered under <l. Using <l, we define a partial order <g among global states as follows:\nWe say that s <g s \u2032 iff (\u2200i : s[i] = s\u2032[i] \u2228 s[i] <l s \u2032[i]) \u2227 (\u2203i : s[i] <l s \u2032[i]). Also, s = s\u2032 iff \u2200i s[i] = s\u2032[i]. For brevity, we use < to denote <l and <g: < corresponds to <l while comparing local states, and < corresponds to <g while comparing global states. We also use the symbol \u2018>\u2019 which is such that s > s\u2032 iff s\u2032 < s. Similarly, we use symbols \u2018\u2264\u2019 and \u2018\u2265\u2019; e.g., s \u2264 s\u2032 iff s = s\u2032 \u2228 s < s\u2032. We call the lattice, formed from such partial order, a <-lattice.\nDefinition 1. <-lattice. Given a total relation <l that orders the states visited by i (for each i) the <-lattice corresponding to <l is defined by the following partial order: s < s\u2032 iff (\u2200i s[i] \u2264l s \u2032[i]) \u2227 (\u2203i s[i] <l s \u2032[i]).\nA <-lattice constraints how global states can transition among one another: state s can transition to state s\u2032 iff s < s\u2032. In the <-lattice discussed above, we can define the meet and join of two states in the standard way: the meet (respectively, join), of two states s1 and s2 is a state s3 where \u2200i, s3[i] is equal to min(s1[i], s2[i]) (respectively, max(s1[i], s2[i])), where min(x, y) = min(y, x) = x iff (x <l y \u2228 x = y), and max(x, y) = max(y, x) = y iff (y >l x \u2228 y = x). For s1 and s2, their meet (respectively, join) has a path to (respectively, is reachable from) both s1 and s2.\nBy varying <l that identifies a total order among the states of a node, one can obtain different lattices. A <-lattice, embedded in the state space, is useful for permitting the algorithm to execute asynchronously. Under proper constraints on the structure of <-lattice, convergence can be ensured."
        },
        {
            "heading": "2.4 Lattice Linear problems",
            "text": "Next, we discuss lattice linear problems, i.e., the problems where the problem statement creates the lattice structure automatically. Such problems can be represented by a predicate that induces a lattice among the states in S.\nIn lattice linear problems, a problem P can be represented by a predicate P such that for any node i, if it is violating P in some state s, then it must change its state. Otherwise, the system will not satisfy P . Let P(s) be true iff state s satisfies P . A node violating P in s is called an impedensable node (an impediment to progress if does not execute, indispensable to execute for progress). Formally,\nDefinition 2. [5] Impedensable node. Impedensable(i, s,P) \u2261 \u00acP(s) \u2227 (\u2200s\u2032 > s : s\u2032[i] = s[i] =\u21d2 \u00acP(s\u2032)).\nDefinition 2 implies that if a node i is impedensable in state s, then in any state s\u2032 such that s\u2032 > s, if the state of i remains the same, then the algorithm will not converge. Thus P induces a total order among the local states visited by a node, for all nodes. Consequently, the discrete structure that gets induced among the global states is a <-lattice, as described in Definition 1. Thus, any <-lattice among the global states is induced by a predicate P that satisfies Definition 2.\nThere can be multiple arbitrary lattices that can be induced among the global states. A system cannot guarantee convergence while traversing an arbitrary lattice. To resolve this, we design the predicate P such that it fulfils some properties, and guarantees reachability to an optimal state. P is used by the nodes to determine if they are impedensable, using Definition 2. Thus, P induces a <l relation among the local states, and as a result, a <-lattice among the global states. We say that P is lattice linear with respect to that <-lattice. Consequently, in any suboptimal global state, there will be at least one impedensable node. Formally,\nDefinition 3. [5]Lattice Linear Predicate. P is a lattice linear predicate with respect to a <-lattice induced among the global states iff \u2200s \u2208 S : \u00acP(s) \u21d2 \u2203i : Impedensable(i, s,P).\nNow we complete the definition of lattice linear problems. In a lattice linear problem P , given any suboptimal global state, we can identify all and the only nodes which cannot retain their state. In this paper, we observe that the algorithms that we study exploit this nature of their respective problems. P is thus designed conserving this nature of the subject problem P .\nDefinition 4. Lattice linear problems. A problem P is lattice linear iff there exists a predicate P and a <-lattice such that\n\u2013 P is deemed solved iff the system reaches a state where P is true, and \u2013 P is lattice linear with respect to the <-lattice induced in S, i.e., \u2200s :\n\u00acP(s) \u21d2 \u2203i : Impedensable(i, s,P). \u2013 \u2200s : (\u2200i : Impedensable(i, s,P) \u21d2 (\u2200s\u2032 : P(s\u2032) \u21d2 s\u2032[i] 6= s[i])).\nRemark: Certain problems are non-lattice linear problems. In such problems, there are instances in which the impedensable nodes cannot be distinctly determined, i.e., in those instances \u2203s : \u00acP(s) \u21d2 (\u2203s\u2032 : \u2200i : P(s\u2032) \u2227 s[i] = s\u2032[i]). Minimal dominating set (MDS) and several other graph theoretic problems are examples of such problems. (This can be illustrated through a simple instance\nof a 2 node connected network with nodes A and B, initially both in the dominating set. Here, MDS can be obtained without removing A. Thus, A is not impedensable. The same applies to B.) For such problems, <-lattices are induced algorithmically as an impedensable node cannot be distinctly determined naturally. Eventually lattice linear algorithms (introduced in [7]) and fully lattice linear algorithms (introduced in [8]) were developed for many such problems.\nProblems such as stable marriage, job scheduling and market clearing price, as studied in [5], are lattice linear problems. In this paper, we study lattice structures that can be induced in multiplication and modulo: we show that multiplication and modulo are lattice linear problems. All the lattice structures that we study in this paper allow self-stabilization: the supremum of the lattice induced in the state space is the optimal state.\nDefinition 5. Self-stabilizing lattice linear predicate. Continuing from Definition 4, P is a self-stabilizing lattice linear predicate if and only if the supremum of the lattice that P induces is an optimal state, i.e., P(supremum(S)) = true.\nNote that P can also be true in states other than the supremum of the <-lattice. Remark: A <-lattice, induced under P , allows asynchrony because if a node, reading old values, reads the current state as s, then for the current state s\u2032, s < s\u2032. So \u00acP(s) \u21d2 \u00acP(s\u2032) because Impedensable(i, s,P) and s[i] = s\u2032[i]."
        },
        {
            "heading": "3 Related Work",
            "text": "Lattice linear problems: The notion of representing problems through a predicate under which the states form a lattice was introduced in [5]. We call the problems for which such a representation is possible lattice linear problems. Lattice linear problems are studied in [5,6,4], where the authors have studied lattice linearity in, respectively, housing market problem and several dynamic programming problems. Many of these problems are not self-stabilizing. Snap-stabilization: The notion of snap-stabilization was introduced in [1]. The algorithms that we study in this paper make the system follow a sequence of states that are deterministically predictable because of the underlying lattice structure in the state space. Thus, they exhibit snap-stabilization. In general, self-stabilizing algorithms where lattice linearity is utilized are snap stabilizing. Modulo: In [11], the authors have presented parallel processing algorithms for inverse, discrete roots, or a large power modulo a number that has only small prime factors. A hardware circuit implementation for mod is presented in [2].\nIn this paper, we present several parallel processing algorithms which are selfstabilizing. Some require critical preprocessing, and some do not. The general algorithm for modulo (Algorithm 2) utilizes the power of (sequential or parallel) modulo and multiplication operations on smaller operands. Multiplication: In [3], the authors presented three parallel implementations of the Karatsuba algorithm for long integer multiplication on a distributed memory architecture. Two of the implementations have time complexity of O(n) on nlg 3 processors. The third algorithm has complexity O(n lg n) on n processors. In this paper, we study lattice linearity of parallelized Karatsuba algorithm."
        },
        {
            "heading": "4 Parallelized Karatsuba\u2019s Multiplication Operation",
            "text": "In this section, we study the lattice linearity of this algorithm that was presented in [3]. First we discuss the idea behind the sequential Karatsuba\u2019s algorithm, and then we elaborate on its lattice linearity."
        },
        {
            "heading": "4.1 Key Idea of the Sequential Karatsuba\u2019s Algorithm [9]",
            "text": "The input is a pair of bitstrings n and m. This algorithm is recursive in nature. As the base case, if the length of n and m equals 1 then, the multiplication result is trivial. When the length is greater than 1, we let m = ab and n = cd, where a and b are half the length of m, and c and d are half the length of n. Here, ab, for example, represents concatenation of a and b, which equals m.\nLet z = 2|b|. n\u00d7m can be computed as ac\u00d7 z2 + (ad+ bc)\u00d7 z + bd. ad+ bc can be computed as (a + b) \u00d7 (c + d) \u2212 ac \u2212 bd. Thus, to compute m \u00d7 n, it suffices to compute 3 multiplications a \u00d7 c, b \u00d7 d and (a + b) \u00d7 (c + d). Hence, we can eliminate one of the multiplications. In the following section, we analyse the lattice linearity of the parallelization of this algorithm as described in [3]."
        },
        {
            "heading": "4.2 The CM parallelization [3] for Karatsuba\u2019s algorithm",
            "text": "The Karatsuba multiplication algorithm involves dividing the input string into substrings and use them to evaluate the multiplication recursively. In the parallel version of this algorithm, the recursive call is replaced by utilizing another children nodes to treat those substrings. We elaborate more on this in the following paragraphs. Consequently, this algorithm induces a tree among the computing nodes. Every non-leaf node has three children. This algorithm works in two phases, top-down and bottom-up. This algorithm uses four variables to represent the state of each node i: n.i, m.i, ans.i and shift.i respectively.\nIn the sequential Karatsuba\u2019s algorithm, both of the input strings n and m are divided into two substrings each and the algorithm then runs recurively on three different input pairs computed from those excerpt bitstrings. In the parallel version, those recursive calls are replaced by activating three children nodes [3]. As a result of such parallelization, if there is no carry-forwarding due to addition, we require lg |n| levels, for which a total of |n|lg 3 nodes are required. However, if there is carry-forwarding due to additions, then we require 2 lg |n| levels, for which a total of |n|2 lg 3 nodes are required.\nIn the top-down phase, if |m.i| > 1 or |n.i| > 1, then i writes (1) a and c to its left child, node 3i\u2212 1 (m.(3i\u2212 1) = a.i and n.(3i\u2212 1) = c.i), (2) b and d to its middle child, node 3i (m.(3i) = b.i and n.(3i) = d.i), and (3) a+ b and c+ d to its right child, node 3i+ 1 (m.(3i+ 1) = a.i + b.i and n.(3i+ 1) = c.i+ d.i). If |m.i| = |n.i| = 1, i.e., in the base case, the bottom-up phase begins and node i sets ans.i = m.i\u00d7 n.i that can be computed trivially since |m.i| = |n.i| = 1.\nIn the bottom-up phase, node i sets ans.i = ans.(3i \u2212 1) \u00d7 z2 + (ans.(3i + 1) \u2212 (ans.(3i \u2212 1) + ans.(3i)) \u00d7 z + ans.(3i). Notice that multiplication by z\nand z2 corresponds to bit shifts and does not need an actual multiplication. Consequently, the product of m\u00d7 n for node i is computed by this algorithm.\nWith some book-keeping (storing the place values of most significant bits of\na+ b and c+d), a node i only needs to write the rightmost |m.i| 2 and |n.i| 2\nbits to its children. Thus, we can safely assume that when a node writes m and n to any of its children, then m and n of that child are of equal length and are of length in some power of 2. (If we do not do the book-keeping, the required number of nodes increases, this number is upper bounded by |n|2 lg 3 as the number of levels is upper bounded by 2 lg |n|; this observation was not made in [3].) However, we do not show the same in the algorithm for brevity. Thus this algorithm would require 2 lg |n| levels, i.e., |n|2 lg 3 nodes. Computation of shift.i : This algorithm utilizes shift to compute z. A node i updates shift.i by doubling the value of shift from its children. A node i evaluates that it is impedensable because of an incorrect value of shift.i by evaluating the following macro.\nImpedensable-Multiplication-Karatsuba-Shift(i) \u2261 \n \n \n|m.i| = 1 \u2227 |n.i| = 1 \u2227 shift.i 6= 0 OR\nshift.(3i) = shift.(3i\u2212 1) = 0 \u2264 shift.(3i+ 1) \u2227 shift.i 6= 1 OR\n0 < shift.(3i) = shift.(3i\u2212 1) \u2264 shift.(3i+ 1) \u2227 shift.i 6= shift.(3i) \u2217 2.\nComputation of m.i and n.i : To ensure that the data flows down correctly, we declare a node i to be impedensable as follows.\nImpedensable-Multiplication-Karatsuba-TopDown(i) \u2261 \n              \n              \ni = 1 \u2227 (m.i 6= m \u2228 n.i 6= n) OR\n((|m.i| > 1 \u2227 |n.i| > 1)\u2227 (m.(3i\u2212 1) 6= m.i [\n1 : |m.i| 2\n]\nOR\nn.(3i\u2212 1) 6= n.i [\n1 : |n.i| 2\n]\nOR\nm.(3i) 6= m.i [\n|m.i| 2\n+ 1 : |m.i| ] OR\nn.(3i) 6= n.i [\n|n.i| 2\n+ 1 : |n.i| ] OR\nm.(3i+ 1) 6= m.i [\n1 : |m.i| 2\n] +m.i [\n|m.i| 2\n+ 1 : |m.i| ] OR\nn.(3i+ 1) 6= n.i [\n1 : |n.i| 2\n] + n.i [\n|n.i| 2\n+ 1 : |n.i| ] )).\nComputation of ans.i : To determine if a node i has stored ans.i incorrectly, it evaluates to be impedensable as follows.\nImpedensable-Multiplication-Karatsuba-BottomUp(i) \u2261 \n   \n   \n|m.i| = 1 \u2227 |n.i| = 1 \u2227 ans.i 6= m.i\u00d7 n.i OR\n|m.i| > 1 \u2227 |n.i| > 1 \u2227 (ans.i 6= lshift(ans.(3i\u2212 1), shift.i)\n+lshift(ans.(3i+ 1)\u2212 ans.(3i\u2212 1)\u2212 ans.(3i), shift.(3i))\n+ans.(3i+ 1))\nThus, Algorithm 1 is described as follows:\nAlgorithm 1 Parallel processing version of Karatsuba\u2019s algorithm.\nRules for node i.\nImpedensable-Multiplication-Karatsuba-Shift(i) \u2212\u2192 \n   \n   \nshift.i = 0 if |m.i| = 1 \u2227 |n.i| = 1 \u2227 shift.i 6= 0.\nshift.i = 1 if shift.(3i) = shift.(3i\u2212 1) = 0\n\u2264 shift.(3i+ 1) \u2227 shift.i 6= 1\nshift.i = shift.(3i)\u00d7 2 otherwise\nImpedensable-Multiplication-Karatsuba-TopDown(i) \u2212\u2192 \n                     \n                     \nm.i = m,n.i = n if i = 1 \u2227 (m.i 6= m \u2228 n.i 6= n). m.(3i\u2212 1) = m.i [\n1 : |m.i| 2\n]\nif m.(3i\u2212 1) 6= m.i [\n1 : |m.i| 2\n]\n.\nn.(3i\u2212 1) = n.i [\n1 : |n.i| 2\n]\nif n.(3i\u2212 1) 6= n.i [\n1 : |n.i| 2\n]\n.\nm.(3i) = m.i [\n|m.i| 2\n+ 1 : |m.i| ] if m.(3i) 6= m.i [\n|m.i| 2\n+ 1 : |m.i| ] .\nn.(3i) = n.i [\n|n.i| 2\n+ 1 : |n.i| ]\nif n.(3i) 6= n.i [\n|n.i| 2\n+ 1 : |n.i| ] .\nm.(3i+ 1) = m.i [\n1 : |m.i| 2\n]\n+m.i [\n|m.i| 2\n+ 1 : |m.i| ]\nif m.(3i+ 1) 6= m.i [\n1 : |m.i| 2\n]\n.\n+m.i [\n|m.i| 2\n+ 1 : |m.i| ] .\nn.(3i+ 1) = n.i [\n1 : |n.i| 2\n]\n+n.i [\n|n.i| 2\n+ 1 : |n.i| ]\notherwise\nImpedensable-Multiplication-Karatsuba-BottomUp(i) \u2212\u2192 \n   \n   \nans.i = m.i\u00d7 n.i if |m.i| = 1 \u2227 |n.i| = 1\nans.i = lshift(ans.(3i\u2212 1), shift.i)+\nlshift(ans.(3i+ 1)\u2212 ans.(3i\u2212 1)\n\u2212ans.(3i), shift.(3i)) + ans.(3i+ 1)) otherwise.\nAlgorithm 1 converges in O(|n|) time [3], and its work complexity is O(nlg 3), which is the time complexity of the sequential Karatsuba\u2019s algorithm [3].\nExample 1. Figure 1 evaluates 100\u00d7 100 following Algorithm 1. \u2293\u2294"
        },
        {
            "heading": "4.3 Lattice linearity",
            "text": "Theorem 1. Given the input bitstrings n and m, the predicate\n\u2200i\u00ac(Impedensable-Multiplication-Karatsuba-Shift(i)\u2228 Impedensable-Multiplication-Karatsuba-TopDown(i)\u2228 Impedensable-Multiplication-Karatsuba-BottomUp(i))\nis lattice linear on |n|2 lg 3 computing nodes.\nProof. For the global state to be optimal, in this problem, we require node 1 to store the correct multiplication result in ans.1. To achieve this, each node i must have the correct value of n.i and m.i, and their children must store correct values of n, m and i according to their n.i and m.i values. This in turn requires all nodes to store the correct shift.i values.\nLet us assume for contradiction that node 1 does not have the correct value of ans.1 = n\u00d7m. This implies that (1) node 1 does not have an updated value in n.1 or m.1, or (2) node 1 has a non-updated value of ans.1, (3) node 1 has not written the updated values to n.2 & m.2 or n.3 & m.3 or n.4 & m.4, (4) node 1 has a non-updated value in shift.1, or (5) nodes 2, 3 or 4 have incorrect values in their respective n, m, ans or shift variables. In cases (1),...,(4), node 1 is impedensable.\nRecursively, this can be extended to any node i. Let node i has stored an incorrect value in ans.i or shift.i. Let i > 1. Then (1) node i has a non-updated value in shift.i, ans.i, n.i or m.i, or (2) (if m.i > 1 or n.i > 1) node i has not written updated values to n.(3i\u22121) & m.(3i\u22121) or n.(3i) & m.(3i) or n.(3i+1) & m.(3i+ 1), in which case node i is impedensable. In both these cases, node i is impedensable. It is also possible that at least one of the children of node i has incorrect values in its respective n, m, ans or shift variables. From these cases, we have that given a global state s, where s = \u3008\u3008n.1, m.1, ans.1\u3009, \u3008n.2, m.2,\nans.2\u3009, ..., \u3008n.(|n|2 lg 3), m.(|n|2 lg 3), ans.(|n|2 lg 3)\u3009\u3009, if s is impedensable, there is at least one node which is impedensable. This shows that if the global state is impedensable, then there exists some node i which is impedensable.\nNext, we show that if some node is impedensable, then node 1 will not store the correct answer. Node 1 is impedensable if it has not read the correct value m.1 and n.1. Additionally, \u2200i : i \u2208 [1 : n2 lg 3] node i is impedensable if (1) it has non-updated values in ans.i or shift.i, (2) i has not written the correct values to ans.(3i\u22121) or ans.(3i) or ans.(3i+1). This implies that the parent of i will also store incorrect value in its ans or shift variable. Recursively, we have that node 1 stores an incorrect value in ans.1. Thus, the global state is impedensable. \u2293\u2294\nCorollary 1. Algorithm 1 computes multiplication of two numbers with n2 lg 3 nodes without synchronization.\nIn Algorithm 1, we allow nodes to change values of other nodes, where parents update their children, which is generally not allowed in a distributed system. It can be transformed such that all nodes update themselves only, and will stay lattice linear, as follows. Nodes will copy n and m from their parents and update their own n and m values. With this change, the definition of ImpedensableMultiplication-Karatsuba-TopDown(i) will change accordingly."
        },
        {
            "heading": "5 Parallel processing modulo operation",
            "text": "In this section, we present a parallel processing algorithm to compute n mod m using 4\u00d7|n|/|m|\u22121 computing nodes. It induces a binary tree among the nodes based on their ids; there are 2\u00d7 |n|/|m| nodes in the lowest level (level 1).\nThis algorithm starts from the leaves where all leaves compute, contiguously, a substring of n of length |m|/2 under modulo m. In the induced binary tree, the computed modulo result by sibling nodes at level \u2113 is sent to the parent. Consecutively, those parents at level \u2113+1, contiguously, store a larger substring of n (double the bits as their children cover) under modulo m. We elaborate this procedure in this subsection. This algorithm uses three variables to represent the state of each node i: shift.i, pow.i and ans.i. Computation of shift.i : The variable shift stores the required power of 2. At any node at level 1, shift is 0. At level 2, the value of shift at any node is |m|/2. At any higher level, the value of shilft is twice the value of shift of its children. Impedensable-Log-Modulo-Shift, in this context, is defined below.\nImpedensable-Log-Modulo-Shift(i) \u2261 \n \n \nshift.i 6= 0 if i \u2265 2\u00d7 |n|/|m|\nshift.i 6= |m|/2 if shift.(2i) = shift.(2i+ 1) = 0\nshift.i 6= 2\u00d7 shift.(2i) if shift.(2i) = shift.(2i+ 1) \u2265 |m|\nComputation of pow.i : The goal of this computation is to set pow.i to be 2shift.i mod m, whenever level.i \u2265 2. This can be implemented using the following definition for Impedensable-Log-Modulo-Pow.\nImpedensable-Log-Modulo-Pow(i) \u2261 \n \n \npow.i 6= 1 if shift.i = 0 pow.i 6= 2 |m| 2 if shift.i = |m|/2\npow.i 6= (pow.(2i))2 mod m otherwise\nBy definition, pow.i is less than m. Also, computation of pow requires multiplication of two numbers that are bounded by |m|. Hence, this calculation can benefit from parallelization of Algorithm 1. However, as we will see later, the complexity of this algorithm (for modulo) is dominated by the modulo operation happening in individual nodes which is O(|m|2), we can use the sequential version of Karatsuba\u2019s algorithm for multiplication as well, without affecting the order of the time complexity of this algorithm. Computation of ans.i : We split n into strings of size |m| 2\n, the number representing this substring is less than m. At the lowest level (level 1), ans.i is set to the corresponding substring. At higher levels, ans.i is set to (pow.i\u00d7 ans.(2i)+ ans.(2i+1)) mod m. This computation also involves multiplication of two numbers whose size is upper bounded by |m|. An impedensable node i from a nonupdated ans.i can be evaluated using Impedensable-Log-Modulo-Ans(i).\nImpedensable-Log-Modulo-Ans(i) \u2261 \n\n\nans.i 6= n[(i\u2212 2\u00d7 |n|\n|m| )\u00d7\n|m|\n2 + 1 : (i\u2212 2\u00d7\n|n| |m| + 1)\u00d7 |m| 2 ] if shift.i = 0\nans.i 6= Mod(Sum(Mul(ans.(2i), pow.i), ans.(2i+ 1)),m) otherwise\nWe describe the algorithm as Algorithm 2.\nAlgorithm 2 Modulo computation by inducing a tree among the nodes.\nRules for node i.\nImpedensable-Log-Modulo-Shift(i) \u2212\u2192 \n  \n  \nshift.i = 0 if i \u2265 2\u00d7 |n|/|m| shift.i = |m|\n2 if shift.(2i) = shift.(2i+ 1) = 0\nshift.i = 2\u00d7 shift.(2i) if shift.(2i) = shift.(2i+ 1) \u2265 |m|\nImpedensable-Log-Modulo-Pow(i) \u2212\u2192 \n \n \npow.i = 1 if shift.i = 0 pow.i = 2 |m| 2 if shift.i = |m|/2\npow.i = Mod(Mul(pow.i, pow.i),m) otherwise\nImpedensable-Log-Modulo-Ans(i) \u2212\u2192 \n\n\nans.i = n[(i\u2212 2\u00d7 |n|\n|m| )\u00d7\n|m|\n2 + 1 : (i\u2212 2\u00d7\n|n| |m| + 1)\u00d7 |m| 2 ] if shift.i = 0\nans.i = Mod(Sum(Mul(ans.(2i), pow.i), ans.(2i+ 1)),m) otherwise\nExample 2. Figure 2 shows the computation of 11011 mod 11 as performed by Algorithm 2. \u2293\u2294"
        },
        {
            "heading": "5.1 Lattice Linearity",
            "text": "Theorem 2. Given the input bitstrings n and m, the predicate\n\u2200i\u00ac(Impedensable-Log-Modulo-Shift(i)\u2228 Impedensable-Log-Modulo-Pow(i)\u2228 Impedensable-Log-Modulo-Ans(i))\nis lattice linear on 4|n|/|m| \u2212 1 computing nodes.\nProof. For the global state to be optimal, in this problem, we require node 1 to store the correct modulo result in ans.1. To achieve this, each node i must have the correct value of ans.i. This in turn requires all nodes to store the correct shift.i and pow.i values.\nLet us assume for contradiction that node 1 has incorrect value in ans.1. This implies that (1) node 1 has a non-updated value in shift.i or pow.i, or (2) node 1 does not have an updated value in ans.i. in both these cases node 1 is impedensable. It is also possible that node 2 or node 3 have an incorrect value in their variables.\nRecursively, this can be extended to any node i. Let node i has stored an incorrect value in ans.i. If i < 2(|n|/|m|), then (1) node i has a non-updated value in ans.i pow.i or shift.i, in which case node i is impedensable or (2) node 2i or node 2i + 1 have incorrect value in their respective shift, pow or ans variables. If i \u2265 2(|n|/|m|), then i do not have values shift.i = 0, pow.i = 1 or a correct ans.i value, in which case i is impedensable. From these cases, we have that given a global state s, where s = \u3008\u3008shift.1, pow.1, ans.1\u3009, \u3008shift.2, pow.2, ans.2\u3009, ..., \u3008shift.(4|n|/|m| \u2212 1), pow.(4|n|/|m| \u2212 1), ans.(4|n|/|m| \u2212 1)\u3009\u3009, if s is impedensable, there is at least one node which is impedensable.\nThis shows that if the global state is impedensable, then there exists some node i which is impedensable.\nNext, we show that if some node is impedensable, then node 1 will not store the correct answer. \u2200i : i \u2208 [1 : 4|n|/|m|\u22121] node i is impedensable if it has nonupdated values in ans.i, pow.i or shift.i. This implies that the parent of node i also stores incorrect value in its ans variable. Recursively, we have that node 1 stores an incorrect value in ans.1, and thus the global state is impedensable. \u2293\u2294\nCorollary 2. Algorithm 2 computes multiplication of two numbers with 4 \u00d7 |n|/|m| nodes without synchronization."
        },
        {
            "heading": "5.2 Time complexity analysis",
            "text": "Algorithm 2 is a general algorithm that uses the Mod( Mul(\u00b7 \u00b7 \u00b7 )) and Mod( Sum(\u00b7 \u00b7 \u00b7 )). For some given x, y and z values, Mod(Mul(x, y),z) (resp., Mod (Sum(x, y),z)) involves first the multiplication (resp., addition) of two input values x and y and then evaluating the resulting value under modulo z. These functions can be implemented in different ways. Choices for these implementations affect the time complexity. We consider two approaches for this as follows.\nModulo via Long Division. First, we consider the standard approach for computing Mod(Mul(\u00b7 \u00b7 \u00b7 )) and Mod(Sum(\u00b7 \u00b7 \u00b7 )). Observe that in Algorithm 2, if we compute Mod(Mul(x, y)) then x, y < m. Hence, we can use Karatsuba\u2019s parallelized algorithm from Section 4 where the input numbers are less than m. Using the analysis from Section 4, we have that each multiplication operation has a time complexity of O(|n|).\nSubsequently, to compute the mod operation, we need to compute xy mod m where xy is upto 2|m| digits long. Using the standard approach of long division, we will need |m| iterations where in each iteration, we need to do a subtraction operation with numbers that |m| digits long. Hence, the complexity of this approach is O(|m|2) per modulo operation. Since this complexity is higher than the cost of multiplication, the overall time complexity is O(|m|2 \u00d7 lg(|n|/|m|)).\nModulo by Constructing Transition Tables. The above approach uses m and n as inputs. Next, we consider the case where m is hardcoded. The preprocessing required in this method makes it impractical. However, we present this method to show lower bounds on the complexity of the modulo operation when m is hardcoded, and to show that there is a potential to reduce the complexity.\nThis approach is motivated by algorithms such as RSA [10] where the value of n changes based on the message to be encrypted/decrypted, but the value of m is fixed once the keys are determined. Thus, some pre-processing can potentially improve the performance of the modulo operation; we observe that certain optimizations are possible. While the time and space complexities required for preprocessing in this algorithm are high, thereby making it impractical, it demonstrates a gap between the lower and upper bound in the complexity.\nIf m is fixed, we can create a table \u03b4sum of size m \u00d7 m where an entry at location (i, j) represents i+ j mod m in O(m2) time. Using \u03b4sum, we can create another transition table \u03b4mul of size m \u00d7 m where an entry at location (i, j) represents i \u00d7 j mod m in O(m2) time. Using \u03b4mul, the time complexity of a Mod(Mul(\u00b7 \u00b7 \u00b7 )) operation becomes O(1). Hence, the overall complexity of the modulo operation becomes O(lg(|n|/|m|))."
        },
        {
            "heading": "6 Conclusion",
            "text": "Multiplication and modulo operations are among the fundamental mathematical operations. Fast parallel processing algorithms for such operations reduce the execution time of the applications which they are employed in. In this paper, we showed that these problems are lattice linear. In this context, we studied an algorithm by Cesari and Maeder [3] which is a parallelization of Karatsuba\u2019s algorithm for multiplication. We showed how to correctly implement this algorithm using |n|lg 3 nodes. In addition, we studied a parallel processing algorithm for the modulo operation.\nThe presence of lattice linearity in problems and algorithms allows nodes to execute asynchronously. This is specifically valuable in parallel algorithms where synchronization can be removed as is. These algorithms are snap-stabilizing, which means that even if the initial states of the nodes are arbitrary, the state transitions of the system strictly follow its specification. These are also selfstabilizing, i.e., the supremum states in the lattices induced under the respective predicates are the respective optimal states.\nUtilizing these algorithms, a virtual machine, e.g., Java or Python, can utilize the available GPU power to compute the multiplication and modulo operations on big-number inputs. In this case, a synchronization primitive also does not need to be deployed. Thus a plethora of applications will benefit from the observations presented in this paper."
        }
    ],
    "year": 2023
}