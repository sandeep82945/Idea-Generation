{
    "abstractText": "Computed tomography (CT) is usually accompanied by a long scanning time and substantial patient radiation exposure. Sinograms are the basis for constructing CT scans; however, continuous sinograms may highly overlap, resulting in extra radiation exposure. This paper proposes a deep learning model to inpaint a sparse-view sinogram sequence. Because a sinogram sequence around the human body is circular in nature, we propose a circular LSTM (CirLSTM) architecture that feeds position-relevant information to our model. To evaluate the performance of our proposed method, we compared the results of our inpainted sinograms with ground truth sinograms using evaluation metrics, including the peak signal-tonoise ratio (PSNR) and structural similarity index measure (SSIM). The SSIM values for both our proposed method and the state-of-the-art method range from 0.998 to 0.999, indicating that the prediction of structures is not challenging for either method. Our proposed CirLSTM achieves PSNR values ranging from 49 to 52, outperforming all the other compared methods. These results demonstrate the feasibility of using only interleaved sinograms to construct a complete sinogram sequence and to generate high-quality CT images. Furthermore, we validated the proposed model across different body portions and CT machine models. The results show that CirLSTM outperforms all other methods in both the across-body segment validation and across-machine validation scenarios. INDEX TERMS computed tomography (CT), deep learning, inpainting, LSTM (Long Short-Term Memory), medical imaging, sinogram",
    "authors": [
        {
            "affiliations": [],
            "name": "CHIN KUO"
        },
        {
            "affiliations": [],
            "name": "TZU-TI WEI"
        },
        {
            "affiliations": [],
            "name": "JEN-JEE CHEN"
        },
        {
            "affiliations": [],
            "name": "YU-CHEE TSENG"
        }
    ],
    "id": "SP:8f5ce423247ca001c6c9e897da5a158ba1c560f1",
    "references": [
        {
            "authors": [
                "Dominik Fleischmann",
                "F Edward Boas"
            ],
            "title": "Computed tomography\u2014old ideas and new technology",
            "venue": "European radiology,",
            "year": 2011
        },
        {
            "authors": [
                "Sajid Abbas",
                "Taewon Lee",
                "Sukyoung Shin",
                "Rena Lee",
                "Seungryong Cho"
            ],
            "title": "Effects of sparse sampling schemes on image quality in low-dose ct",
            "venue": "Medical physics,",
            "year": 2013
        },
        {
            "authors": [
                "Kyungsang Kim",
                "Jong Chul Ye",
                "William Worstell",
                "Jinsong Ouyang",
                "Yothin Rakvongthai",
                "Georges El Fakhri",
                "Quanzheng Li"
            ],
            "title": "Sparse-view spectral ct reconstruction using spectral patch-based low-rank penalty",
            "venue": "IEEE transactions on medical imaging,",
            "year": 2014
        },
        {
            "authors": [
                "James A Brink",
                "Jay P Heiken",
                "Ge Wang",
                "Kevin W McEnery",
                "Francis J Schlueter",
                "MW Vannier"
            ],
            "title": "Helical ct: principles and technical considerations",
            "year": 1994
        },
        {
            "authors": [
                "Hiroyuki Kudo",
                "Taizo Suzuki",
                "Essam A Rashed"
            ],
            "title": "Image reconstruction for sparse-view ct and interior ct\u2014introduction to compressed sensing and differentiated backprojection",
            "venue": "Quantitative imaging in medicine and surgery,",
            "year": 2013
        },
        {
            "authors": [
                "Yoseob Han",
                "Jong Chul Ye"
            ],
            "title": "Framing u-net via deep convolutional framelets: Application to sparse-view ct",
            "venue": "IEEE transactions on medical imaging,",
            "year": 2018
        },
        {
            "authors": [
                "Hu Chen",
                "Yi Zhang",
                "Mannudeep K Kalra",
                "Feng Lin",
                "Yang Chen",
                "Peixi Liao",
                "Jiliu Zhou",
                "Ge Wang"
            ],
            "title": "Low-dose ct with a residual encoderdecoder convolutional neural network",
            "venue": "IEEE transactions on medical imaging,",
            "year": 2017
        },
        {
            "authors": [
                "Hongming Shan",
                "Atul Padole",
                "Fatemeh Homayounieh",
                "Uwe Kruger",
                "Ruhani Doda Khera",
                "Chayanin Nitiwarangkul",
                "Mannudeep K Kalra",
                "Ge Wang"
            ],
            "title": "Competitive performance of a modularized deep neural network compared to commercial algorithms for low-dose ct image reconstruction",
            "venue": "Nature Machine Intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Huizhuo Yuan",
                "Jinzhu Jia",
                "Zhanxing Zhu"
            ],
            "title": "Sipid: A deep learning framework for sinogram interpolation and image denoising in low-dose ct reconstruction",
            "venue": "IEEE 15th International Symposium on Biomedical Imaging (ISBI",
            "year": 2018
        },
        {
            "authors": [
                "Martti Kalke",
                "Samuli Siltanen"
            ],
            "title": "Sinogram interpolation method for sparse-angle tomography",
            "venue": "Applied Mathematics,",
            "year": 2014
        },
        {
            "authors": [
                "AN Van Daatselaar",
                "PF Van der Stelt",
                "J Weenen"
            ],
            "title": "Effect of number of projections on image quality of local ct",
            "venue": "Dentomaxillofacial Radiology,",
            "year": 2004
        },
        {
            "authors": [
                "Xu Dong",
                "Swapnil Vekhande",
                "Guohua Cao"
            ],
            "title": "Sinogram interpolation for sparse-view micro-ct with deep learning neural network",
            "venue": "In Medical Imaging 2019: Physics of Medical Imaging,",
            "year": 2019
        },
        {
            "authors": [
                "Atul Padole",
                "Ranish Deedar Ali Khawaja",
                "Mannudeep K Kalra",
                "Sarabjeet Singh"
            ],
            "title": "Ct radiation dose and iterative reconstruction techniques",
            "venue": "AJR Am J Roentgenol,",
            "year": 2015
        },
        {
            "authors": [
                "Oliver S Grosser",
                "Juri Ruf",
                "Dennis Kupitz",
                "Damian Czuczwara",
                "David Loewenthal",
                "Markus Thormann",
                "Christian Furth",
                "Jens Ricke",
                "Timm Denecke",
                "Maciej Pech"
            ],
            "title": "Iterative ct reconstruction in abdominal lowdose ct used for hybrid spect-ct applications: effect on image quality, image noise, detectability, and reader\u2019s confidence",
            "venue": "Acta radiologica open,",
            "year": 2019
        },
        {
            "authors": [
                "Koichi Sugisawa",
                "Katsuhiro Ichikawa",
                "Atsushi Urikura",
                "Kazuya Minamishima",
                "Shota Masuda",
                "Takashi Hoshino",
                "Akiko Nakahara",
                "Yoshitake Yamada",
                "Masahiro Jinzaki"
            ],
            "title": "Spatial resolution compensation by adjusting the reconstruction kernels for iterative reconstruction images of computed tomography",
            "venue": "Physica Medica,",
            "year": 2020
        },
        {
            "authors": [
                "Thomas Henzler",
                "Christian Fink",
                "Stefan O Schoenberg",
                "U Joseph Schoepf"
            ],
            "title": "Dual-energy ct: radiation dose aspects",
            "venue": "AJR-American Journal of Roentgenology,",
            "year": 2012
        },
        {
            "authors": [
                "Ahmed Hosny",
                "Chintan Parmar",
                "John Quackenbush",
                "Lawrence H Schwartz",
                "Hugo JWL Aerts"
            ],
            "title": "Artificial intelligence in radiology",
            "venue": "Nature Reviews Cancer,",
            "year": 2018
        },
        {
            "authors": [
                "Eunhee Kang",
                "Junhong Min",
                "Jong Chul Ye"
            ],
            "title": "A deep convolutional neural network using directional wavelets for low-dose x-ray ct reconstruction",
            "venue": "Medical physics,",
            "year": 2017
        },
        {
            "authors": [
                "Kyong Hwan Jin",
                "Michael T McCann",
                "Emmanuel Froustey",
                "Michael Unser"
            ],
            "title": "Deep convolutional neural network for inverse problems in imaging",
            "venue": "IEEE Transactions on Image Processing,",
            "year": 2017
        },
        {
            "authors": [
                "Huaizu Jiang",
                "Deqing Sun",
                "Varun Jampani",
                "Ming-Hsuan Yang",
                "Erik Learned-Miller",
                "Jan Kautz"
            ],
            "title": "Super slomo: High quality estimation of multiple intermediate frames for video interpolation",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Fitsum A Reda",
                "Deqing Sun",
                "Aysegul Dundar",
                "Mohammad Shoeybi",
                "Guilin Liu",
                "Kevin J Shih",
                "Andrew Tao",
                "Jan Kautz",
                "Bryan Catanzaro"
            ],
            "title": "Unsupervised video interpolation using cycle consistency",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Yu-Lun Liu",
                "Yi-Tung Liao",
                "Yen-Yu Lin",
                "Yung-Yu Chuang"
            ],
            "title": "Deep video frame interpolation using cyclic frame generation",
            "venue": "AAAI Conference on Artificial Intelligence,",
            "year": 2023
        },
        {
            "authors": [
                "Wenbo Bao",
                "Wei-Sheng Lai",
                "Xiaoyun Zhang",
                "Zhiyong Gao",
                "Ming- Hsuan Yang"
            ],
            "title": "Memc-net: Motion estimation and motion compensation driven neural network for video interpolation and enhancement",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Michael Strobel",
                "Julia Diebold",
                "Daniel Cremers"
            ],
            "title": "Flow and color inpainting for video completion",
            "venue": "In German Conference on Pattern Recognition,",
            "year": 2014
        },
        {
            "authors": [
                "Ryan Szeto",
                "Ximeng Sun",
                "Kunyi Lu",
                "Jason J Corso"
            ],
            "title": "A temporallyaware interpolation network for video frame inpainting",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Thien Huynh-The",
                "Cam-Hao Hua",
                "Quoc-Viet Pham",
                "Dong-Seong Kim"
            ],
            "title": "Mcnet: An efficient cnn architecture for robust automatic modulation classification",
            "venue": "IEEE Communications Letters,",
            "year": 2020
        },
        {
            "authors": [
                "Ting-Hui Chiang",
                "Yun-Tang Lin",
                "Jaden Chao-Ho Lin",
                "Yu-Chee Tseng"
            ],
            "title": "Trapezoid-structured lstm with segregated gates and bridge joints for video frame inpainting",
            "venue": "The Visual Computer,",
            "year": 2023
        },
        {
            "authors": [
                "Sheng-Yang Chiu",
                "Yu-Chee Tseng",
                "Jen-Jee Chen"
            ],
            "title": "Low-resolution thermal sensor-guided image synthesis",
            "venue": "In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) Workshops,",
            "year": 2023
        },
        {
            "authors": [
                "Wei-An Lin",
                "Haofu Liao",
                "Cheng Peng",
                "Xiaohang Sun",
                "Jingdan Zhang",
                "Jiebo Luo",
                "Rama Chellappa",
                "Shaohua Kevin Zhou"
            ],
            "title": "Dudonet: Dual domain network for ct metal artifact reduction",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Fengyuan Jiao",
                "Zhiguo Gui",
                "Kunpeng Li",
                "Hong Shangguang",
                "Yanling Wang",
                "Yi Liu",
                "Pengcheng Zhang"
            ],
            "title": "A dual-domain cnn-based network for ct reconstruction",
            "venue": "IEEE Access,",
            "year": 2021
        },
        {
            "authors": [
                "Ao Zheng",
                "Hewei Gao",
                "Li Zhang",
                "Yuxiang Xing"
            ],
            "title": "A dual-domain deep learning-based reconstruction method for fully 3d sparse data helical ct",
            "venue": "Physics in Medicine & Biology,",
            "year": 2020
        },
        {
            "authors": [
                "Chi-Fu Huang",
                "Yu-Chee Tseng",
                "Li-Chu Lo"
            ],
            "title": "The coverage problem in three-dimensional wireless sensor networks",
            "venue": "Journal of Interconnection Networks,",
            "year": 2007
        },
        {
            "authors": [
                "Richard Yi-Chia Tsai",
                "Hans Ting-Yuan Ke",
                "Kate Ching-Ju Lin",
                "Yu- Chee Tseng"
            ],
            "title": "Enabling identity-aware tracking via fusion of visual and inertial features",
            "venue": "In 2019 International Conference on Robotics and Automation (ICRA),",
            "year": 2019
        },
        {
            "authors": [
                "Sheng-Yang Chiu",
                "Yu-Ting Huang",
                "Chieh-Ting Lin",
                "Yu-Chee Tseng",
                "Jen- Jee Chen",
                "Meng-Hsuan Tu",
                "Bo-Chen Tung",
                "YuJou Nieh"
            ],
            "title": "Privacypreserving video conferencing via thermal-generative images",
            "venue": "In IEEE Int\u2019l Conf. on Robotics and Automation,",
            "year": 2023
        },
        {
            "authors": [
                "Lan-Da Van",
                "Ling-Yan Zhang",
                "Chun-Hao Chang",
                "Kit-Lun Tong",
                "Kun- Ru Wu",
                "Yu-Chee Tseng"
            ],
            "title": "Things in the air: tagging wearable iot information on drone",
            "venue": "videos. Discov. Internet Things,",
            "year": 2021
        },
        {
            "authors": [
                "S Anbukkarasi",
                "S Varadhaganapathy"
            ],
            "title": "Analyzing sentiment in tamil tweets using deep neural network",
            "venue": "Fourth International Conference on Computing Methodologies and Communication (ICCMC),",
            "year": 2020
        },
        {
            "authors": [
                "Alaa Abu-Srhan",
                "Mohammad AM Abushariah",
                "Omar S Al-Kadi"
            ],
            "title": "The effect of loss function on conditional generative adversarial networks",
            "venue": "Journal of King Saud University-Computer and Information Sciences,",
            "year": 2022
        },
        {
            "authors": [
                "Qi Wang",
                "Yue Ma",
                "Kun Zhao",
                "Yingjie Tian"
            ],
            "title": "A comprehensive survey of loss functions in machine learning",
            "venue": "Annals of Data Science,",
            "year": 2022
        },
        {
            "authors": [
                "Yunbo Wang",
                "Zhifeng Gao",
                "Mingsheng Long",
                "Jianmin Wang",
                "S Yu Philip"
            ],
            "title": "Predrnn++: Towards a resolution of the deep-in-time dilemma in spatiotemporal predictive learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Kenneth Clark",
                "Bruce Vendt",
                "Kirk Smith",
                "John Freymann",
                "Justin Kirby",
                "Paul Koppel",
                "Stephen Moore",
                "Stanley Phillips",
                "David Maffitt",
                "Michael Pringle"
            ],
            "title": "The cancer imaging archive (tcia): maintaining and operating a public information repository",
            "venue": "Journal of digital imaging,",
            "year": 2013
        },
        {
            "authors": [
                "Zijun Zhang"
            ],
            "title": "Improved adam optimizer for deep neural networks",
            "venue": "IEEE/ACM 26th International Symposium on Quality of Service (IWQoS),",
            "year": 2018
        },
        {
            "authors": [
                "Paul Suetens"
            ],
            "title": "Fundamentals of medical imaging",
            "venue": "Cambridge university press,",
            "year": 2017
        }
    ],
    "sections": [
        {
            "text": "INDEX TERMS computed tomography (CT), deep learning, inpainting, LSTM (Long Short-Term Memory), medical imaging, sinogram\nI. INTRODUCTION\nACOMPUTED TOMOGRAPHY (CT) is performed us-ing a sequence of sinograms taken circularly from a slice of the human body. CT is usually accompanied by a long scanning time and substantial radiation exposure to patients [1]. Solutions to low-dose CT include hardware approaches, such as using dynamic beam blockers and alternating voltage switching, and software approaches, such as strengthening the construction process with fewer inputs [2], [3]. The process of constructing a CT involves back-projection from multi-angle radiographic projections, commonly referred to as sinograms. However, a sequence of sinograms often contains many overlapping regions among adjacent frames [4]. Therefore, constructing a CT from fewer projection views, known as sparse-view CT [5], holds substantial potential for reducing patients\u2019 radiation exposure and deserves further investigation.\nRecent studies have applied deep neural networks as a cost-effective solution for implementing sparse-view CT. A\nmulti-frame U-net was proposed in [6]. The model improves the noise robustness and orientation information processing, thus enabling sparse-view CT. The residual encoder-decoder CNN [7] was designed to denoise low-quality and noisy CT images constructed from sparse views. A deep-learning approach was developed in [8] for CT construction and compared with commercial algorithms. An alternative is to use neural networks to compute intermediate sinograms from existing sinograms [9], [10]. The results are then used to construct a high-quality CT with fewer artifacts during backprojection. It has been demonstrated that sinogram interpolation is an effective way to implement sparse-view CT [11], [12]. In [12], a combination of U-Net and residual learning was proposed for sinogram interpolation. It was shown that both the Root Mean Square Error (RMSE) and Structural Similarity Index (SSIM) were significantly improved.\nThe problem to be addressed in this study is formally defined as follows: A slice of CT is constructed by a sequence of L sinograms, denoted as v0, v1, . . . , vL\u22121, taken from 360\nVOLUME , 2023 1\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nChin Kuo et al.: Circular LSTM for Low-dose Sinograms Inpainting\ndegrees of a human body segment in a circular manner. To reduce the radiation dose to patients, our goal was to only take sinograms at interleaving angles with a step size of s. Specifically, let L = ns, where n is an integer. Only sinograms vks, k = 0..n, are actually obtained from a patient. Note that because of the circular acquisition of sinograms from a 360-degree human body segment, the sinograms v0 and vns are identical. The remaining L \u2212 n sinograms were to be synthesized. The goal was to develop a model for synthesizing those missing sinograms as close to the ground truth as possible. With these synthesized sinograms, the CT produced is expected to be of higher quality, as illustrated in Fig. 1. Fig. 1 shows how our proposed model incorporates into the clinical process, and the proposed neural network is called Circular LSTM (CirLSTM).\nII. RELATED WORKS In the literature, various post-image reconstruction methods have been applied to reduce noise in sparse-view CT images, thereby potentially increasing their diagnostic value [13]. Commercially available reconstruction techniques, such as iterative reconstruction (IR), have been shown to be effective in improving the diagnostic value for various clinical indications [14], [15]. However, IR techniques are limited in terms of dose reduction beyond filtered back projection (FBP), especially for low-contrast diagnostic tasks. Dualenergy CT (DECT) [16] offers the ability to acquire multiphase images and thus provides more detailed diagnostic information. However, this could significantly increase the radiation dose. These observations highlight the need for innovative alternatives such as convolutional neural networks (CNN) [17]. Our work explores a new possibility of using sinogram inpainting in sparse-view CT. The results show promising PSNR and SSIM performance indices.\nRecent research has attempted to optimize sparse-view CT using deep learning techniques. To reduce the severe global streaking and blurring artifacts in a CT constructed from non-overlapping projections, [18] proposed the use of an ori-\nented wavelet DNN. A neural network for residual learning using U-Net with a large receptive field was proposed in [19], which demonstrated an effective streak artifact removal capability. In [6], a U-net-based structure was proposed to overcome the blurring artifacts. In [12], a combination of U-Net and residual learning was proposed for sinogram interpolation. It significantly improved the PSNR and SSIM of sparse-view CT and confirmed that sinogram interpolation is a feasible method.\nThe sinogram inpainting problem is similar to the video frame interpolation (VFI) problem, which has been widely studied in computer vision. These previous studies motivated us to study the sinogram inpainting problem for constructing high-quality CT. The goal of VFI is to predict missing middle frames between the given preceding and succeeding frames. However, the main differences are that the media are natural RGB images and that they do not have a circular interleaving structure, as studied in this work. Reference [20] proposed bidirectional optical flows for VFI. However, optical flows cannot handle small objects, sudden changes in brightness, or motion blur well. In [21], edge information and two loss functions, namely cyclic consistency loss and motion linear loss, were introduced to enhance the quality of synthesized frames. The model in [22] works on a smaller amount of training data; however, the motion approximation is assumed to be linear. An adaptive deformation method that integrates optical flow and interpolation kernels was proposed in [23] to synthesize the target frames. Flow and color inpainting to complete a video was addressed in [24]. In [25], MCnet [26] was applied first to predict forward and backward video frames and then the temporally-aware interpolation (TAI) network was applied to combine the two outputs for final frame restoration. A Trapezoid-structured LSTM for VFI is proposed in [27]. Sensor-guided image synthesis was also studied in [28] and [29].\nRecently, there have been significant interests in the development of dual-domain methods for computed tomography (CT) reconstruction and other fields. Several CT methods"
        },
        {
            "heading": "2 VOLUME , 2023",
            "text": "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nChin Kuo et al.: Circular LSTM for Low-dose Sinograms Inpainting\nhave explored both the image domain and sinogram domain to improve the quality and accuracy of reconstructed CT images. For instance, [30] proposed a dual domain network for CT metal artifact reduction. Similarly, [31] introduced a dual-domain CNN-based network for CT reconstruction. Furthermore, [32] investigated a dual-domain deep learningbased reconstruction method for fully 3D sparse data helical CT. These researches on dual-domain methods for CT reconstruction have shown promising advancements in improving image quality, reducing artifacts, and enhancing computational efficiency, thus overcoming traditional methods\u2019 limitations. On the other hand, with the advance of sensors [33], cross-domain fusion of video data with sensing data has been explored in [34]\u2013[36].\nIII. METHODS A. CIRCULAR LSTM In this study, we propose an unsupervised learning approach. Our model utilizes a bidirectional long short-term memory (LSTM) as its backbone. There were three design ideas. First, to balance the computation efficiency and synthesis quality, we avoided using a lengthy LSTM. The length of the inputs to the LSTM is set to 2s + 1 such that the first, sth, and last inputs are known sinograms and the other inputs are to be inpainted (to recap, we only take sinograms at interleaved angles with a step size of s, while the intermediate s \u2212 1 sinograms are to be synthesized using our proposed model). Second, the model is designed to be \u201cposition-aware\u201d in the sense that the model knows which sinogram acquisition angle it is trying to inpaint. Third, a straightforward approach to fully reconstruct 360-degree sinograms is to employ a lengthy LSTM with L input cells. The typical number of L was 1152. However, this approach is significantly complex. By contrast, the input length of the proposed Circular LSTM is fixed at 2s+1, which is shorter than the required sinogram length (L). Consequently, the Circular LSTM must be run in a continuous and patchy manner to repair the missing sinogram.\nThe model was designed to have K layers of bidirectional LSTMs stacked together, each with two LSTMs working in opposite directions. Each layer uses the prediction results of the previous layer as inputs and makes predictions. The inputs consisted of four sequences stacked together, all of the same dimensions, IW\u00d7H\u00d7(2s+1). The first sequence was a sinogram sequence, and the other three were position-related sequences. The first sequence contains 2s + 1 sinograms [v(j\u22121)s, \u00b7 \u00b7 \u00b7 , vjs, \u00b7 \u00b7 \u00b7 , v(j+1)s] such that v(j\u22121)s, vjs, and v(j+1)s are known, and the other 2s\u22122 contain random noise, where 0 \u2264 j < n. The second sequence contained the CT slice information. Specifically, we computed H(ID1, ID2), where H is a hash function, ID1 is the CT slice index, and ID2 is the patient\u2019s unique identifier. Then H(ID1, ID2) are duplicated to all W \u00d7H\u00d7(2s+1) elements of the sequence. The third and fourth sequences contain sinogram angle information. Angle of the ith sinogram, i = (j \u2212 1)s..(j + 1)s, is \u03b8i = 2\u03c0i/L. The angle is represented by (cos \u03b8i, sin \u03b8i). As\nfor the angle information of vi, cos \u03b8i is duplicated W \u00d7 H times to the block indexed by i of the third sequence, and sin \u03b8i is duplicated W \u00d7 H times to the block indexed by i of the fourth sequence. These four sequences were stacked together in one sequence and fed into the LSTM. This model is illustrated in Fig. 2.\nWe chose bi-directional LSTM as the backbone of CirLSTM. The working mechanism of such LSTM is not only to propagate inter-frame temporal and spatial states along the time direction but also to extract deep features and analyze the changes in time and space through its multi-layer structure. LSTM has been successfully used for filling in missing words in a sentence, making predictions, and understanding sequential data [37]. In this study, we apply LSTM to CT, which has a special circular property and demonstrates its excellent inpainting capability when given sufficient location and angle information.\nTo train the model, the loss function was set to a combination of L1 loss, VGG perceptual loss, and gradient difference loss (GDL) [38], [39]. L1 loss minimizes the error by summing the absolute differences between the ground truth and predicted results. The VGG perceptual loss was obtained from the pre-trained 19-th ReLU activation layer of the VGG network; this loss aims to approximate the perceptual similarity. GDL penalizes the differences in image gradients and can be used to sharpen image quality. These losses have been used in color and gray images. Through our experiments, we found that a combination of L1, VGG, and GDL loss functions yielded better results than using any single loss function alone. Therefore, the following results are presented based on this combined loss function.\nIn our implementation, we adopted causal LSTM (csLSTM) [40] as the basic unit. csLSTM has two sets of LSTM gates in its memory unit to generate temporal and spatial hidden states separately. To train the model, we feed in each sinogram sequence [v(j\u22121)s, \u00b7 \u00b7 \u00b7 , vjs, \u00b7 \u00b7 \u00b7 , v(j+1)s] in the training set in a circular manner such that j = 1..n \u2212 1. Note that when the sequence loops back to the origin, we still assume the first, middle, and last sinograms as known, and the others as unknown. This setup also works when L is not a multiple of s."
        },
        {
            "heading": "B. DATASETS AND PRE-PROCESSING",
            "text": "We trained our model based on the LDCT-and-projection dataset [41] and tested it on the following three datasets: (i) the non-training part of the LDCT-and-projection dataset [41], (ii) NCKUH Siemens CT dataset, and (iii) Alisa Medical Imaging dataset for external verification. The LDCT-andprojection dataset is specially designed for CT reconstruction research and includes original sinogram images and reconstructed CT images. The data were obtained using Siemens CT machines and followed an extended DICOM format called DICOM-CT-PD, which includes sinogram data, acquisition geometry, patient information, and pathology recognition. The NCKUH Siemens CT dataset is a private deidentified dataset containing CT data of the head, chest,\nVOLUME , 2023 3\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nChin Kuo et al.: Circular LSTM for Low-dose Sinograms Inpainting\nand abdomen (access approval to this dataset by NCKUH is required). The Alisa Medical Imaging dataset is public. It includes, but is not limited to, CT performed by GE and Toshiba scanners.\nFor the LDCT-and-projection dataset, the original raw sinograms in the dataset cannot be directly used because they require pre-processing, such as denoising and filtering of artifacts, before they can be used for CT reconstruction. However, the dataset did not provide preprocessed sinograms. Since our focus is on studying the low-dose sinograms inpainting problem rather than pre-processing algorithms, we employed a filter-based back-projection (FBP) algorithm to decompose the CT images from the LDCT-and-projection dataset into continuous sinograms spanning 360 degrees. Subsequently, we divided the 360-degree continuous sinograms into 1152 sinogram patches based on the acquisition angles, and these decomposed sinogram patches were used as inputs for training the CirLSTM. Since our goal is to study all body regions, CT scans of the head, chest, and abdomen were all decomposed into sinograms. Finally, we obtained 359 CT slices of one patient\u2019s abdomen, 394 CT slices of three patients\u2019 chests, and 319 CT slices of nine patients\u2019 heads to train CirLSTM. After decomposition, all sinograms were shuffled into a training dataset. The size of the sinograms was the same as the original size, that is, 88*512 pixels. Each CT slice had L = 1152 sinograms.\nAs for the testing phase, we utilized multiple datasets. The LDCT-and-projection dataset was used for both training and\ntesting, specifically employing the non-training portion of the LDCT-and-projection dataset. For each of the three body segments (chest, abdomen, and head), we selected one patient to test all the slices in that particular segment, resulting in 303 slices for the chest, 168 slices for the abdomen, and 40 slices for the head. Furthermore, to conduct the model\u2019s ablation study, we randomly selected 5 slices from 5 patients for each of the three body segments, amounting to a total of 75 slices for testing. In addition to the aforementioned datasets, we performed tests on two additional datasets for external validation. The first is the NCKUH Siemens CT dataset, from which we randomly selected 8 slices from 3 patients for each of the three body segments, resulting in a total of 72 slices. The second dataset used is the Alisa Medical Imaging dataset, which consists of CT scans performed using GE and Toshiba scanners. We chose 24 slices from a single patient for each scanner, leading to a total of 48 slices for external validation."
        },
        {
            "heading": "C. EXPERIMENT SETUP AND COMPLEXITY ANALYSIS",
            "text": "In our experiment, we used two layers of LSTM: K = 2. The number of hidden nodes was 64. The training took a total of 150 epochs, and the batch size was 8. The Adam optimizer with a learning rate of 0.001 was adopted, and the decay rate was set to \u03b21 = 0.5 and \u03b22 = 0.999 [42]. We explore s = 4, 8, 12, and 16 for CirLSTM.\nTo understand the sinogram generation time required by CirLSTM, we analyzed using two NVIDIA 2080 Ti GPUs."
        },
        {
            "heading": "4 VOLUME , 2023",
            "text": "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nChin Kuo et al.: Circular LSTM for Low-dose Sinograms Inpainting\nThe time required to generate one sinogram slice is 24.59, 23.19, 22.84, and 22.79 seconds when s = 4, 8, 12, and 16, respectively. It is worth noting that the inference times for s = 4, 8, 12, and 16 are similar. The reason for this is as follows: a sinogram is composed of sinogram patches taken from 1152 angles, with each angle considered as a separate patch. Therefore, there are a total of 1152 sinogram patches for 360 degrees. In our experiments, we utilize the CirLSTM structure to inpaint the missing sinogram patches. For instance, with s = 4, only 289 sinogram patches are actually taken, and the remaining sinogram patches are generated through inpainting using a smaller CirLSTM model, which requires a total of 94 iterations. Similarly, for s = 16, only 73 sinogram patches are actually taken, while the other sinogram patches are generated through inpainting using a larger CirLSTM model with fewer inpainting iterations, specifically 36. Inpainting with fewer patches requires more iterations, resulting in similar inference times across different values of s.\nAlthough a larger s implies a larger model, the computation cost per sinogram is averaged out by a larger s due to the parallel processing capability of our model. This means that the ability of our model to generate multiple sinograms simultaneously offsets the increase in model complexity. However, the overall time required for computing one CT is still larger than that required in typical clinical settings (normally ranging from 665 \u2212 726 seconds for generating one head CT, 5, 352\u22126, 201 seconds for generating one chest CT, and 3, 079\u22123, 287 seconds for generating one abdominal CT).\nIV. RESULTS A. CIRCULAR VS. NON-CIRCULAR ARCHITECTURE By our definition of \u201ccircular\u201d, we feed angular information (0 \u223c 2\u03c0) as inputs to CirLSTM. Table 1 demonstrates the importance of making CirLSTM angle-aware during sinogram inpainting. We compared different combinations of inputs to CirLSTM. The evaluation was conducted on the LDCTand-projection dataset, which consisted of Siemens CT scans of the head, chest, and abdomen and their corresponding sinograms. We used 359 abdominal CT slices from a single patient, 394 chest CT slices from three patients, and 319 head CT slices from nine patients were used for model training. For testing, we used five CT slices from five patients in each of three body segments (head, chest, and abdomen).\nNoInp represents the baseline where no inpainting is conducted and only sparse-view sinograms are used to construct\nthe CT. CirLSTM(1) is our reduced model, which only takes the first (sinogram) sequence as input. CirLSTM(4ind) is our full model, that takes four sequences as the input, but the second sequence is replaced with a duplicated CT index number (instead of a hashing result). The difference is that the CT scans of two patients may have the same index. CirLSTM is our full (angle-aware) model (Sec. III-A). The results validated the effectiveness of our circular design for obtaining high-quality CT images. There were significant margins when the RMSE and PSNR were considered."
        },
        {
            "heading": "B. SINOGRAM INPAINTING EVALUATION",
            "text": "We compared the inpainting results with those of several state-of-the-art methods, such as SuperSloMo [20] and Unet [12]. Bilinear interpolation [43] was used as the baseline. This experiment was also conducted on the LDCT-andprojection dataset, as described in Sec IV-A.\nIn Table 2, we evaluate the quality of inpainted sinograms against the ground truth, in terms of mean RMSE, PSNR, and SSIM, under different settings of s. A smaller s generally leads to better results, because the inpainting task is relatively easier. The SSIM of our proposed CirLSTM all ranges from 0.998-0.999, indicating that structure prediction is not difficult for the proposed method. Despite no significant differences in SSIM among the compared algorithms, we believe that it is still valuable to provide such evaluations for future references. The PSNR of CirLSTM ranges from 49 to 52. Under different settings of s, CirLSTM always performs the best in all schemes. Overall, CirLSTM outperformed the other methods for all indices. We found that CirLSTM has a good generalization ability. Fig. 3 shows the visualization results for the different body parts, along with side-by-side\nVOLUME , 2023 5\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nChin Kuo et al.: Circular LSTM for Low-dose Sinograms Inpainting\ncomparisons of the post-algorithm sinograms and the ground truth using the absolute error map to visualize the discrepancies. As can be seen, the generalization ability of our model is as good as that of the IR model [44] when applied to different body regions. Models commonly trained using data from a specific site, such as the head, cannot be applied directly to different sites. To address this issue, it is necessary to employ transfer learning or to retrain the model using images from the target site. On the other hand, our proposed model has the advantage of direct applicability to multiple sites, thereby significantly reducing the number of models required in the clinical workflow.\nFig. 4 compares the mean PSNR of the inpainted sinograms according to their indices. In general, sinograms that are closer to the known ones are easier to inpaint (and thus have higher PSNR) than those farther from the known ones. This trend is consistently shown across all settings of s. Fig. 5 displays the visualization results of these inpainted\nsinograms."
        },
        {
            "heading": "C. ACROSS-BODY SEGMENT VALIDATION",
            "text": "The training data included different body parts. This experiment aimed to validate the performance of our model across these parts. In addition to the LDCT-and-projection dataset, this validation was conducted using a private NCKUH1 dataset. The NCKUH dataset includes Siemens CT images of the head, chest, and abdomen with four randomly selected CT slices of each body segments. Table 3 lists the experimental results for the two datasets with length s = 4. The results indicated that CirLSTM consistently outperformed the other solutions across all body parts. The trend for the other length settings remains similar and we omit the details.\n1National Cheng Kung University Hospital (NCKUH) is a universityowned research-oriented hospital."
        },
        {
            "heading": "6 VOLUME , 2023",
            "text": "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nChin Kuo et al.: Circular LSTM for Low-dose Sinograms Inpainting\nD. ACROSS-MACHINE VALIDATION To investigate the generalizability of CirLSTM, we conducted a challenging across-machine validation study. The task is to train CirLSTM on sinograms obtained from one source machine and to attempt to inpaint sinograms from known sinograms obtained from other target machines. The training dataset was LDCT obtained from a Siemens CT scanner (source machine). The test set includes 25 CT slices using the GE scanner and 25 CT slices using the Toshiba scanner. Both GE and Toshiba scanners served as the target machines. Table 4 lists the experimental results. The performance indices remained similar to those obtained from the source machine, indicating that CirLSTM can generalize well to different scanner models.\nV. CONCLUSIONS This study introduced CirLSTM, a deep learning model that effectively inpaints sparse-view sinogram sequences to construct high-quality CT images. By leveraging CirLSTM, radiation exposure to patients and scanning time can be reduced while maintaining the image quality. The circular LSTM architecture employed in this model incorporates position-relevant information, which enables the generation of a complete (or full) sinogram sequence from the interleaved sinograms. In contrast to the lengthy LSTM with\nL input cells, CirLSTM offers greater feasibility for future clinical applications. The model was tested on different body parts and CT machine models, and it was found that CirLSTM consistently performed well across all scenarios. The results indicate that the proposed CirLSTM model possesses comprehensive capabilities and can serve as a valuable tool for reducing radiation exposure to patients, while enhancing CT image quality in clinical settings. CirLSTM is specifically designed to reconstruct circular sinograms of 360 angles. Limited-angle CT presents unique challenges and distinctive features attributed to a sector of sinograms and deserves future research."
        },
        {
            "heading": "VI. ACKNOWLEDGEMENTS",
            "text": "This study received no funding. We are grateful to the National Cheng Kung University Hospital for generously providing us with the NCKUH de-identification dataset."
        },
        {
            "heading": "VII. CONFLICT OF INTEREST STATEMENT",
            "text": "The authors have no relevant conflicts of interest to disclose."
        },
        {
            "heading": "8 VOLUME , 2023",
            "text": "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nChin Kuo et al.: Circular LSTM for Low-dose Sinograms Inpainting\nAAAI Conference on Artificial Intelligence, volume 33, pages 8794\u20138802, 2019. [23] Wenbo Bao, Wei-Sheng Lai, Xiaoyun Zhang, Zhiyong Gao, and MingHsuan Yang. Memc-net: Motion estimation and motion compensation driven neural network for video interpolation and enhancement. IEEE transactions on pattern analysis and machine intelligence, 43(3):933\u2013948, 2019. [24] Michael Strobel, Julia Diebold, and Daniel Cremers. Flow and color inpainting for video completion. In German Conference on Pattern Recognition, pages 293\u2013304. Springer, 2014. [25] Ryan Szeto, Ximeng Sun, Kunyi Lu, and Jason J Corso. A temporallyaware interpolation network for video frame inpainting. IEEE Transactions on Pattern Analysis and Machine Intelligence, 42(5):1053\u20131068, 2019. [26] Thien Huynh-The, Cam-Hao Hua, Quoc-Viet Pham, and Dong-Seong Kim. Mcnet: An efficient cnn architecture for robust automatic modulation classification. IEEE Communications Letters, 24(4):811\u2013815, 2020. [27] Ting-Hui Chiang, Yun-Tang Lin, Jaden Chao-Ho Lin, and Yu-Chee Tseng. Trapezoid-structured lstm with segregated gates and bridge joints for video frame inpainting. The Visual Computer, Apr 2023. [28] Sheng-Yang Chiu, Yu-Chee Tseng, and Jen-Jee Chen. Low-resolution thermal sensor-guided image synthesis. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) Workshops, 2023. [29] Jia-Yan Li, Jaden Chao-Ho Lin, Kun-Ru Wu, and Yu-Chee Tseng. Sensepred: Guiding video prediction by wearable sensors. IEEE Internet of Things Journal, to appear. [30] Wei-An Lin, Haofu Liao, Cheng Peng, Xiaohang Sun, Jingdan Zhang, Jiebo Luo, Rama Chellappa, and Shaohua Kevin Zhou. Dudonet: Dual domain network for ct metal artifact reduction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10512\u201310521, 2019. [31] Fengyuan Jiao, Zhiguo Gui, Kunpeng Li, Hong Shangguang, Yanling Wang, Yi Liu, and Pengcheng Zhang. A dual-domain cnn-based network for ct reconstruction. IEEE Access, 9:71091\u201371103, 2021. [32] Ao Zheng, Hewei Gao, Li Zhang, and Yuxiang Xing. A dual-domain deep learning-based reconstruction method for fully 3d sparse data helical ct. Physics in Medicine & Biology, 65(24):245030, 2020. [33] Chi-Fu Huang, Yu-Chee Tseng, and Li-Chu Lo. The coverage problem in three-dimensional wireless sensor networks. Journal of Interconnection Networks, 08(03):209\u2013227, 2007. [34] Richard Yi-Chia Tsai, Hans Ting-Yuan Ke, Kate Ching-Ju Lin, and YuChee Tseng. Enabling identity-aware tracking via fusion of visual and inertial features. In 2019 International Conference on Robotics and Automation (ICRA), pages 2260\u20132266, 2019. [35] Sheng-Yang Chiu, Yu-Ting Huang, Chieh-Ting Lin, Yu-Chee Tseng, JenJee Chen, Meng-Hsuan Tu, Bo-Chen Tung, and YuJou Nieh. Privacypreserving video conferencing via thermal-generative images. In IEEE Int\u2019l Conf. on Robotics and Automation, 2023. [36] Lan-Da Van, Ling-Yan Zhang, Chun-Hao Chang, Kit-Lun Tong, KunRu Wu, and Yu-Chee Tseng. Things in the air: tagging wearable iot information on drone videos. Discov. Internet Things, 1(1), 2021. [37] S Anbukkarasi and S Varadhaganapathy. Analyzing sentiment in tamil tweets using deep neural network. In 2020 Fourth International Conference on Computing Methodologies and Communication (ICCMC), pages 449\u2013453. IEEE, 2020. [38] Alaa Abu-Srhan, Mohammad AM Abushariah, and Omar S Al-Kadi. The effect of loss function on conditional generative adversarial networks. Journal of King Saud University-Computer and Information Sciences, 2022. [39] Qi Wang, Yue Ma, Kun Zhao, and Yingjie Tian. A comprehensive survey of loss functions in machine learning. Annals of Data Science, 9(2):187\u2013 212, 2022. [40] Yunbo Wang, Zhifeng Gao, Mingsheng Long, Jianmin Wang, and S Yu Philip. Predrnn++: Towards a resolution of the deep-in-time dilemma in spatiotemporal predictive learning. In International Conference on Machine Learning, pages 5123\u20135132. PMLR, 2018. [41] Kenneth Clark, Bruce Vendt, Kirk Smith, John Freymann, Justin Kirby, Paul Koppel, Stephen Moore, Stanley Phillips, David Maffitt, Michael Pringle, et al. The cancer imaging archive (tcia): maintaining and operating a public information repository. Journal of digital imaging, 26(6):1045\u2013 1057, 2013.\n[42] Zijun Zhang. Improved adam optimizer for deep neural networks. In 2018 IEEE/ACM 26th International Symposium on Quality of Service (IWQoS), pages 1\u20132. Ieee, 2018. [43] Paul Suetens. Fundamentals of medical imaging. Cambridge university press, 2017. [44] Bing Guan, Cailian Yang, Liu Zhang, Shanzhou Niu, Minghui Zhang, Yuhao Wang, Weiwen Wu, and Qiegen Liu. Generative modeling in sinogram domain for sparse-view ct reconstruction. arXiv preprint arXiv:2211.13926, 2022.\nCHIN KUO is an MD specializing in radiation oncology, having graduated from National Taiwan University. With a wealth of clinical experience, she previously served as an attending physician in the Department of Oncology at Cheng Kung University Hospital. With a strong dedication to oncology clinical studies and the application of medical artificial intelligence, Chin Kuo actively participates in significant research projects focused on healthcare AI. Currently pursuing a Ph.D. degree\nat the College of AI, National Yang Ming Chiao Tung University, Chin Kuo aims to utilize her expertise in oncology and AI to advance patient care. Her objective is to merge the fields of oncology and artificial intelligence, contributing to the development of innovative approaches and solutions for improved diagnosis and treatment in oncology.\nTZU-TI WEI is a software engineer specializing in artificial intelligence. He is currently pursuing a Ph.D. degree in the College of Artificial Intelligence at National Yang Ming Chiao Tung University, focusing on researching video interpolation for generating high-quality medical images. TzuTi has worked on various projects involving the development of deep learning models, including tasks such as inpainting missing sinogram frames and implementing AI-based automatic patrol sys-\ntems. With a Bachelor\u2019s degree in Electrical Engineering from National University of Tainan, TzuTi brings a robust educational background and expertise in software development and AI technologies.\nJEN-JEE CHEN is an Associate Professor and the Director at the Institute of Intelligent Systems, College of Artificial Intelligence, National Yang Ming Chiao Tung University (NCTU), Taiwan. He received his B.S. and M.S. degrees in computer science and information engineering from NCTU and obtained his Ph.D. in computer science from the same institution. He has an extensive academic background, including being a Visiting Scholar at the University of Illinois, Urbana-Champaign, and\na Postdoctoral Research Fellow at the Department of Electrical Engineering, NCTU. Prior to joining NCTU, Dr. Chen worked at the Department of Electrical Engineering, National University of Tainan, Taiwan. Throughout his career, he has served as a reviewer for numerous computer and communication journals and has been a Technical Program Committee (TPC) member for various computer and communication conferences. He has received several prestigious awards, including Best Paper Awards at APNOMS, MC, WASN, and ITAOI, as well as a Best Poster Award at MC. His research interests encompass AI, 5G V2X and IoT, Robotics and Assistive Applications, and mobile computing. Dr. Chen is a member of IEEE and the Phi Tau Phi Society.\nVOLUME , 2023 9\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nChin Kuo et al.: Circular LSTM for Low-dose Sinograms Inpainting\nYU-CHEE TSENG is a highly accomplished researcher and academic. He obtained his Ph.D. in Computer and Information Science from the Ohio State University in 1994. Throughout his career, he has held several esteemed positions, including Chairman and Dean of the College of Computer Science at National Chiao-Tung University (NCTU), Taiwan. Currently, he serves as the Director of the Microelectronics and Information Research Center at NCTU, as well as the Director\nof the Pervasive AI Research Labs under the Ministry of Science and Technology. Dr. Tseng has received numerous accolades for his contributions to the field. He has been honored with titles such as NCTU Chair Professor and Y. Z. Hsu Scientific Chair Professor. He has also been recognized with Outstanding Research Awards from the National Science Council, Academic Awards from the Ministry of Education, and Best Paper Awards from various conferences. Additionally, he has received the Distinguished Alumnus Award from the Ohio State University and the TWAS Prize. As an IEEE Fellow, Dr. Tseng actively serves on the editorial boards of several prestigious journals in the field of computer science. His research interests revolve around mobile computing, wireless communication, and the Internet of Things, and he has a notable h-index of over 60."
        },
        {
            "heading": "10 VOLUME , 2023",
            "text": "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/"
        }
    ],
    "title": "Circular LSTM for Low-dose Sinograms Inpainting",
    "year": 2023
}