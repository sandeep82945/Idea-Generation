{
    "abstractText": "We propose a novel image registration method based on implicit neural representations that addresses the challenging problem of registering a pair of brain images with similar anatomical structures, but where one image contains additional features or artifacts that are not present in the other image. To demonstrate its effectiveness, we use 2D microscopy in situ hybridization gene expression images of the marmoset brain. Accurately quantifying gene expression requires image registration to a brain template, which is difficult due to the diversity of patterns causing variations in visible anatomical brain structures. Our approach uses implicit networks in combination with an image exclusion loss to jointly perform the registration and decompose the image into a support and residual image. The support image aligns well with the template, while the residual image captures individual image characteristics that diverge from the template. In experiments, our method provided excellent results and outperformed other registration techniques.",
    "authors": [
        {
            "affiliations": [],
            "name": "Michal Byra"
        },
        {
            "affiliations": [],
            "name": "Charissa Poon"
        },
        {
            "affiliations": [],
            "name": "Tomomi Shimogori"
        },
        {
            "affiliations": [],
            "name": "Henrik Skibbe"
        }
    ],
    "id": "SP:b060f82f41becbe560abfdf7866547a32db9762f",
    "references": [
        {
            "authors": [
                "B.B. Avants",
                "N.J. Tustison",
                "G. Song",
                "P.A. Cook",
                "A. Klein",
                "J.C. Gee"
            ],
            "title": "A reproducible evaluation of ants similarity metric performance in brain image registration",
            "venue": "Neuroimage 54(3), 2033\u20132044",
            "year": 2011
        },
        {
            "authors": [
                "M. Corrales",
                "B.T. Cocanougher",
                "A.B. Kohn",
                "J.D. Wittenbach",
                "X.S. Long",
                "A. Lemire",
                "A. Cardona",
                "R.H. Singer",
                "L.L. Moroz",
                "M. Zlatic"
            ],
            "title": "A single-cell transcriptomic atlas of complete insect nervous systems across multiple life stages",
            "venue": "Neural Development 17(1), 8",
            "year": 2022
        },
        {
            "authors": [
                "Y. Fu",
                "Y. Lei",
                "T. Wang",
                "W.J. Curran",
                "T. Liu",
                "X. Yang"
            ],
            "title": "Deep learning in medical image registration: a review",
            "venue": "Physics in Medicine & Biology 65(20), 20TR01",
            "year": 2020
        },
        {
            "authors": [
                "Y. Gandelsman",
                "A. Shocher",
                "M. Irani"
            ],
            "title": " double-dip\u201d: unsupervised image decomposition via coupled deep-image-priors",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 11026\u201311035",
            "year": 2019
        },
        {
            "authors": [
                "M. Hoffmann",
                "B. Billot",
                "D.N. Greve",
                "J.E. Iglesias",
                "B. Fischl",
                "A.V. Dalca"
            ],
            "title": "Synthmorph: learning contrast-invariant registration without acquired images",
            "venue": "IEEE transactions on medical imaging 41(3), 543\u2013558",
            "year": 2021
        },
        {
            "authors": [
                "Y. Kita",
                "H. Nishibe",
                "Y. Wang",
                "T. Hashikawa",
                "S.S. Kikuchi",
                "M. U",
                "A.C. Yoshida",
                "C. Yoshida",
                "T. Kawase",
                "S Ishii"
            ],
            "title": "Cellular-resolution gene expression profiling in the neonatal marmoset brain reveals dynamic species-and region-specific differences",
            "venue": "Proceedings of the National Academy of Sciences 118(18), e2020125118",
            "year": 2021
        },
        {
            "authors": [
                "E.S. Lein",
                "M.J. Hawrylycz",
                "N. Ao",
                "M. Ayres",
                "A. Bensinger",
                "A. Bernard",
                "A.F. Boe",
                "M.S. Boguski",
                "K.S. Brockway",
                "Byrnes",
                "E.J"
            ],
            "title": "Genome-wide atlas of gene expression in the adult mouse brain",
            "venue": "Nature 445(7124), 168\u2013176",
            "year": 2007
        },
        {
            "authors": [
                "I. Loshchilov",
                "F. Hutter"
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "arXiv preprint arXiv:1711.05101",
            "year": 2017
        },
        {
            "authors": [
                "I. Mehta",
                "M. Gharbi",
                "C. Barnes",
                "E. Shechtman",
                "R. Ramamoorthi",
                "M. Chandraker"
            ],
            "title": "Modulated periodic activations for generalizable local functional representations",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 14214\u201314223",
            "year": 2021
        },
        {
            "authors": [
                "S. Nam",
                "M.A. Brubaker",
                "M.S. Brown"
            ],
            "title": "Neural image representations for multiimage fusion and layer separation",
            "venue": "Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part VII. pp. 216\u2013232. Springer",
            "year": 2022
        },
        {
            "authors": [
                "H. Qiu",
                "C. Qin",
                "A. Schuh",
                "K. Hammernik",
                "D. Rueckert"
            ],
            "title": "Learning diffeomorphic and modality-invariant registration using b-splines",
            "venue": "Medical Imaging with Deep Learning",
            "year": 2021
        },
        {
            "authors": [
                "T. Shimogori",
                "A. Abe",
                "Y. Go",
                "T. Hashikawa",
                "N. Kishi",
                "S.S. Kikuchi",
                "Y. Kita",
                "K. Niimi",
                "H. Nishibe",
                "M Okuno"
            ],
            "title": "Digital gene atlas of neonate common marmoset brain",
            "venue": "Neuroscience research 128, 1\u201313",
            "year": 2018
        },
        {
            "authors": [
                "V. Sitzmann",
                "J. Martel",
                "A. Bergman",
                "D. Lindell",
                "G. Wetzstein"
            ],
            "title": "Implicit neural representations with periodic activation functions",
            "venue": "Advances in Neural Information Processing Systems 33, 7462\u20137473",
            "year": 2020
        },
        {
            "authors": [
                "S. Sun",
                "K. Han",
                "D. Kong",
                "C. You",
                "X. Xie"
            ],
            "title": "Mirnf: medical image registration via neural fields",
            "venue": "arXiv preprint arXiv:2206.03111",
            "year": 2022
        },
        {
            "authors": [
                "M. Tancik",
                "P. Srinivasan",
                "B. Mildenhall",
                "S. Fridovich-Keil",
                "N. Raghavan",
                "U. Singhal",
                "R. Ramamoorthi",
                "J. Barron",
                "R. Ng"
            ],
            "title": "Fourier features let networks learn high frequency functions in low dimensional domains",
            "venue": "Advances in Neural Information Processing Systems 33, 7537\u20137547",
            "year": 2020
        },
        {
            "authors": [
                "J.M. Wolterink",
                "J.C. Zwienenberg",
                "C. Brune"
            ],
            "title": "Implicit neural representations for deformable image registration",
            "venue": "International Conference on Medical Imaging with Deep Learning. pp. 1349\u20131359. PMLR",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "Keywords: brain \u00b7 deep learning \u00b7 gene expression \u00b7 implicit neural representations \u00b7 registration"
        },
        {
            "heading": "1 Introduction",
            "text": "Image registration is a crucial prerequisite for image comparison, data integration, and group studies in contemporary medical and neuroscience research. In research and clinical settings, pairs of images often show similar anatomical structures but may contain additional features or artifacts, such as specific staining, electrodes, or lesions, that are not present in the other image. This difficulty of finding corresponding structures for automatically aligning images complicates image registration. In this work, we address the challenging problem of the gene expression image registration in the marmoset brain. Brain atlases of gene\n\u22c6 Corresponding author.\nar X\niv :2\n30 8.\n04 03\n9v 1\n[ cs\n.C V\n] 8\nexpression, created using images of brain tissue processed through in situ hybridization (ISH), offer single-cell resolution of spatial gene expression patterns across the entire brain [2,7]. However, accurately quantifying gene expression requires brain image registration to spatially align ISH images to a common atlas space. The diversity of gene expression patterns in ISH images causes variations in visible anatomical brain structures with respect to the template image. ISH microscopy images are also susceptible to tissue processing artifacts, resulting in non-specific staining and tissue deformations.\nTraditional pair-wise image registration methods use optimization algorithms to find the deformation field that maximizes the similarity between a pair of images. While several deep learning methods based on convolutional neural networks (CNNs) have been proposed for calculating the deformation field between two images [3], such models typically require large training sets and may suffer from generalization issues when applied to images presenting texture patterns that diverge from the training data. Therefore, classic algorithms, such as Advanced Normalization Tools (ANTs) [1], are still preferred as off-the-shelf tools for image registration in neuroscience due to scarce experimental data and the diversity of data acquisition protocols and registration tasks. Recently, implicit neural representations (INRs) have been utilized for image registration in MRI and CT [14,16], offering a hybrid approach that connects modern deep learning techniques with per-case optimization as used in classical approaches. INRs are defined on continuous coordinate spaces, making them suitable for registration of images that differ in geometry.\nIn this work, we propose a novel INR-based framework well-suited to address the challenging problem of gene expression brain image registration. We associate the registration problem with an image decomposition task. We utilize implicit neural networks to decompose the ISH image into two separate images: a support image and a residual image. The support image corresponds to the part of the ISH image that is well-aligned with the registration template image in respect to the texture. On the contrary, the residual image presents features of the ISH image, such as artifacts or texture patterns (e.g. gene expression), which presumably undermine the registration procedure. The support image is used to improve the deformation field calculations. We also introduce an exclusion loss to encourage clearer separation of the support and residual images. The usefulness of the proposed method is demonstrated using 2D ISH gene expression images of the marmoset brain."
        },
        {
            "heading": "2 Methods",
            "text": ""
        },
        {
            "heading": "2.1 Registration with implicit networks",
            "text": "The goal of the pairwise image registration is to determine a spatial transformation that maximizes the similarity between the moving image M and the target fixed template image F . INRs serve as a continuous, coordinate based approximation of the deformation field obtained through a fully connected neural network. In this study, as the backbone for our method, we utilized the standard\napproach to registration with INRs, as described in [14,16]. We used a single implicit deformation network D to map 2D spatial coordinates x\u0304 \u2208 [\u22121, 1]2 of the moving image M to a displacement vector \u2206x\u0304 \u2208 R2. Next, the transformation field was determined as \u03a6(x\u0304) = x\u0304+\u2206x\u0304 and the bilinear interpolation algorithm was applied to obtain the corresponding moved image T\u03a6(M).\nTo train the deformation network, the following loss function based on correlation coefficients was applied to assess the similarity between the moved image T\u03a6(M) and the fixed template image F :\nLcc(F, T\u03a6(M)) = 1\n2N \u2211 x\u0304 ( NCC(F, T\u03a6(M)) + LNCC(F, T\u03a6(M)) ) , (1)\nwhere NCC and LNCC stand for the normalized cross-correlation and local normalized cross-correlation based loss functions averaged over the entire image domain consisting of N elements. NCC was used to stabilize the training of the network, while LNCC ensured good local registration results. Additionally, following the standard approach to INR based registration, we regularized the deformation field based on the Jacobian matrix determinant |J\u03a6(x\u0304)| using following equation [16]:\nLreg(\u03a6(x\u0304)) = 1\nN \u2211 x\u0304 |1\u2212 |J\u03a6(x\u0304)||. (2)"
        },
        {
            "heading": "2.2 Registration guided image decomposition",
            "text": "Our aim is to improve the registration performance associated with the implicit deformation network D. The proposed framework is presented in Fig. 1. We assume that the moving image M can be decomposed with separate implicit networks, S and R, into two images: the support image MS and the residual image MR. Ideally, the support image should correspond to the part of the moving image that contributes to the registration performance. On the contrary, we expect the residual image to include image artifacts and texture patterns (e.g. ISH gene expression patterns) that diverge from the fixed template image and undermine the registration procedure. We impose the following condition based on the mean squared error loss function for the decomposition of the moving image:\nLrec(M,MS +MR) = 1\nN \u2211 x\u0304 (M \u2212MS \u2212MR)2, (3)\nstating that the support MS and residual MR images should sum up to the moving image M . To ensure that the support image MS contributes to the registration with respect to the fixed image F , we utilize the cross-correlation based loss function Lcc(F, T\u03a6(MS)) (eq. 1), where T\u03a6(MS) stands for the transformed support image MS . Therefore, the deformation network is trained to provide the transformation field \u03a6(x) both for the moving image and the support image\nusing two cross-correlation based loss functions. This way the training of the deformation network is guided to provide a more detailed transformation field for the contents of the moving image that actually correspond to the fixed template image. Moving image texture patterns that do not correspond to the fixed image have lower impact on the training of the deformation network.\nIn practice, it might be beneficial, following INR based methods for obstruction and rain removal, to additionally constrain the image decomposition procedure to obtain more clearly separated support MS and residual MR images [10]. For this, we utilize the following exclusion loss to encourage the gradient structure of the implicit networks S and R to be decorrelated [4]:\nLexcl(MS ,MR) = 1\nN \u2211 x\u0304 \u2211 i,j |\u0393 (JS(x\u0304), JR(x\u0304))| (4)\nwhere \u0393 (JMS (x\u0304), JMR(x\u0304)) = tanh(JS(x\u0304)) \u2297 tanh(JR(x\u0304)), \u2297 indicates elementwise multiplication and indices i, j go over all elements of the matrix \u0393 .\nIn our framework, we jointly optimize all three implicit networks (D, S and R) using the following composite loss function:\nLoss = \u03b11Lcc(F, T\u03a6(M)) + \u03b12Lcc(F, T\u03a6(MS)) + \u03b13Lreg(\u03a6(x\u0304)) + \u03b14Lrec(M,MS +MR) + \u03b15Lexcl(MS ,MR).\n(5)\nThe first row of eq. 5 can be perceived as a standard registration loss, while the second row stands for a regularized image reconstruction loss."
        },
        {
            "heading": "2.3 Evaluation",
            "text": "We designed the proposed method with the aim to address the problem of ISH gene expression image registration. For the evaluation, we used neonate marmoset brain ISH images collected at the Laboratory for Molecular Mechanisms of Brain Development, RIKEN Center for Brain Science, Wako, Japan (geneatlas.brainminds.jp) [6,12]. We prepared manual annotations for 2D images from 50 gene expression datasets. Atlas template images were created using ANTs [1], based on semi-automatically aligned sets of 2D ISH images from 1942 gene expression datasets. ISH images used to generate the template were converted to gray-scale to meet ANTs requirements and better highlight brain tissue interfaces.\nPerformance of the proposed approach was compared to the SynthMorph network and the ANTs SyN registration algorithm based on mutual information metric, as these two methods do not require pre-training and can serve as off-the-shelf registration tools for neuroscience [1,5]. We conducted an ablation study to assess the effectiveness of the proposed representation decomposition approach with and without the exclusion loss. Registration methods were evaluated quantitatively based on Dice scores using manual 2D segmentations prepared for the following five brain structures ranging in size and shape complexity: aqueduct (AQ, 95 masks), hippocampus area (HA, 570 masks), dorsal lateral geniculate (DLG, 370 masks), inferior colliculus (IC, 70 masks) and visual cortex area (VCA, 68 masks). Segmentations were outlined both for the template and ISH 2D images, resulting in 1114 image pairs corresponding to the same brain regions. We also calculated the percentage of the non-positive Jacobian determinant values to assess the deformation field folding. Moreover, we determined the structural similarity index (SSIM) between the moved images and the template fixed images."
        },
        {
            "heading": "2.4 Implementation",
            "text": "We utilized sinusoidal representation networks to determine the implicit representations [13]. Each network contained five fully connected hidden layers with 256 neurons. We used the Fourier mapping with six frequencies to encode the input coordinates [15]. The coordinates and the encoded coordinates were additionally concatenated within the middle layer of the network. Weights of the networks were initialized following the original paper except for the last linear layer of the deformation network D, for which we uniformly sampled the weights from [-0.0001, 0.0001] interval to ensure small deformations at initial epochs. Additional details about the network architecture can be found in the supplementary materials. Networks were trained for 1000 epochs using AdamW optimizer with learning rate of 0.0001 on a server equipped with several NVIDIA A100 GPUs [8]. ISH images of size 360x420 were downsampled to 256x256. Each epoch corresponded to a batch of all image pixel coordinates [13]. After some initial experiments, we set the composite loss function weights (eq. 5) to \u03b11=\u03b12=\u03b13=\u03b15=1 and \u03b14=100, partially following the previous studies on INRs\n[10,14,16]. The window size for the LNCC loss was set to [32, 32]. Our PyTorch implementation of the proposed INR based registration method is available at https://github.com/BrainImageAnalysis/ImpRegDec."
        },
        {
            "heading": "3 Results",
            "text": ""
        },
        {
            "heading": "3.1 Qualitative results",
            "text": "Support and residual images generated with the proposed method are shown in Fig.2. The support images retain the main style and content of the fixed template image, while the residual images include the remaining image contents, along with gene expression patterns not present in the template image. Utilization of the exclusion loss resulted in a clearer and more visually plausible separation between the support and residual images, particularly for gene expression patterns. Fig. 3 further highlights the usefulness of the proposed registration guided image decomposition technique. First, our method can be applied to extract microscopy image artifacts, and therefore mitigate their impact on the registration. Second, the proposed method is general and can also be applied to register an ISH gene expression image to a Nissl image. In this case, the color distribution of the support image corresponds to that of a Nissl image, while the residual image presents the local contents of the gene expression image. We also used the proposed method to register an ISH brain image to another ISH image with a different gene expression. For this example, the residual image highlighted the gene expression patterns of the moving image, while the support image showed the gene expression patterns of the fixed image.\nFig. 4 visually compares the registration performance of the proposed technique, equipped with the exclusion loss, to ANTs. We found that the proposed\nmethod provided good results both in respect to the image registration and the transformation of the manual segmentations."
        },
        {
            "heading": "3.2 Quantitative results",
            "text": "Table 1 shows Dice scores obtained for the selected marmoset brain regions. Registration techniques based on INRs outperformed the other methods on four\nout of five brain regions. ANTs achieved better registration results for only one structure, the VCA, which was the largest among the annotated brain regions and already similar in unregistered images with an initial Dice score of 0.848. Additionally, the Dice score for the VCA was high and comparable across all investigated registration methods. Our approach achieved significantly better Dice scores compared to the standard INRs for AQ, HA, DLG and IC (t-test\u2019s p-values<0.05). Furthermore, incorporating the exclusion loss slightly improved the Dice scores for three structures.\nSSIM values in Table 2 show that the registration based on implicit networks provided the most structurally similar results to the template images. With respect to the SSIM metric, our method significantly outperformed other approaches (t-test\u2019s p-values<0.05). ANTs and SynthMorph provided smoother deformation fields compared to the implicit networks, with significantly lower percentage of folding (t-test\u2019s p-values<0.05). However, the percentage of the folding obtained for the implicit networks was small and acceptable, as defined by folds in 0.5% of all pixels [11]. The main disadvantage of the proposed approach was the relatively long optimization time of about 90 seconds for a single pairwise registration, resulting from the requirement to jointly train three implicit networks."
        },
        {
            "heading": "4 Conclusion",
            "text": "Our approach based on implicit networks and registration-guided image decomposition has demonstrated excellent performance for the challenging task of registering ISH gene expression images of the marmoset brain. The results show that our approach outperformed pairwise registration methods based on ANTs and SynthMorph CNN, highlighting the potential of INRs as versatile off-the-shelf tools for image registration. Moreover, the proposed registration-guided image decomposition mechanism not only improved the registration performance, but also could be used to effectively separate the patterns that diverge from the target fixed image. In the future, we plan to investigate the possibility of using image decomposition for simultaneous registration and pattern segmentation, and methods to speed up the training [9]. We also plan to extend our technique to 3D and test it on medical images that include pathologies.\nAcknowledgement. The authors do not have any conflicts of interest. This work was supported by the program for Brain Mapping by Integrated Neurotechnologies for Disease Studies (Brain/MINDS) from the Japan Agency for Medical Research and Development AMED (JP15dm0207001) and the Japan Society for the Promotion of Science (JSPS, Fellowship PE21032)."
        },
        {
            "heading": "A Network architecture",
            "text": "Implicit sinusoidal representation network (SIREN) utilized in our work is depicted in Fig. 5 and has the following form:\nz(0) = [x\u0304,FE(x\u0304)],\nz(l) =\n{ \u03c1 ( W (l)z(l\u22121) \u2212 b(l) ) , l \u2208 {1, ..., L\u2212 1} \\ lmid[\n\u03c1 ( W (l)z(l\u22121) \u2212 b(l) ) , z(0) ] , l = lmid\n\u2206x\u0304 = W (L)z(L\u22121) \u2212 b(L),\n(6)\nwhere x\u0304 and \u2206x\u0304 stand the moving image coordinate and the corresponding displacement vector, respectively. W (l), b(l) and z(l) correspond to network\u2019s weights, bias and post-activation for the l-th layer, l = 1, ..., lmid, ..., L, with lmid indicating the middle layer. Number of the hidden layers was equal to 5 in our work, each consisting of 256 units. Network utilized sine activation function \u03c1(y) = sin(\u03c9y) with \u03c9 standing for the frequency related parameter and set to 30 [13]. FE(x\u0304) indicates the positional Fourier encoding that was concatenated with the input coordinate x\u0304. Additionally, we formed a residual connection by concatenating the image coordinate x\u0304 and the FE(x\u0304) within the middle layer of the network. The positional encoding FE(x\u0304) had the following form [15]:\nFE(x\u0304) = [..., cos(2\u03c0\u03c3j x\u0304), sin(2\u03c0\u03c3j x\u0304), ...] (7)\nfor j = 0, ..., N \u2212 1 and N and \u03c3 equal to 6 and 2 in our work, respectively. Regarding the weight initialization, we followed the SIREN paper [13]. Weights\nof the network were sampled from the uniform distribution U ( \u2212 \u221a\nc n\u03c92 ,\n\u221a c\nn\u03c92\n) ,\nwith c, \u03c9 and n equal to 6, 30 and 256, respectively. However, for the last layer of the network we sampled the weights from U(\u22120.0001, 0.0001) to ensure small displacement vectors \u2206x\u0304 at the initial training epochs."
        }
    ],
    "title": "Implicit neural representations for joint decomposition and registration of gene expression images in the marmoset brain",
    "year": 2023
}