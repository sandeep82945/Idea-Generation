{
    "abstractText": "The demonstrated success of transfer learning has popularized approaches that involve pretraining models from massive data sources and subsequent finetuning towards a specific task. While such approaches have become the norm in fields such as natural language processing, implementation and evaluation of transfer learning approaches for chemistry are in the early stages. In this work, we demonstrate finetuning for downstream tasks on a graph neural network (GNN) trained over a molecular database containing 2.7 million water clusters. The use of Graphcore IPUs as an AI accelerator for training molecular GNNs reduces training time from a reported 2.7 days on 0.5M clusters to 1.2 hours on 2.7M clusters. Finetuning the pretrained model for downstream tasks of molecular dynamics and transfer to a different potential energy surface took only 8.3 hours and 28 minutes, respectively, on a single GPU.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jenna A. Bilbrey"
        },
        {
            "affiliations": [],
            "name": "Kristina M. Herman"
        }
    ],
    "id": "SP:4b06c50f4c48f222ca6863da3dbe14d6086f8ec4",
    "references": [
        {
            "authors": [
                "Sheng Wang",
                "Yuzhi Guo",
                "Yuhong Wang",
                "Hongmao Sun",
                "Junzhou Huang"
            ],
            "title": "SMILES-BERT: large scale unsupervised pre-training for molecular property prediction",
            "venue": "In Proceedings of the 10th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics,",
            "year": 2019
        },
        {
            "authors": [
                "Seyone Chithrananda",
                "Gabriel Grand",
                "Bharath Ramsundar"
            ],
            "title": "ChemBERTa: large-scale self-supervised pretraining for molecular property prediction",
            "venue": "arXiv preprint arXiv:2010.09885,",
            "year": 2020
        },
        {
            "authors": [
                "Walid Ahmad",
                "Elana Simon",
                "Seyone Chithrananda",
                "Gabriel Grand",
                "Bharath Ramsundar"
            ],
            "title": "ChemBERTa- 2: Towards chemical foundation models",
            "venue": "arXiv preprint arXiv:2209.01712,",
            "year": 2022
        },
        {
            "authors": [
                "Josep Ar\u00fas-Pous",
                "Thomas Blaschke",
                "Silas Ulander",
                "Jean-Louis Reymond",
                "Hongming Chen",
                "Ola Engkvist"
            ],
            "title": "Exploring the GDB-13 chemical space using deep generative models",
            "venue": "Journal of Cheminformatics,",
            "year": 2019
        },
        {
            "authors": [
                "Vijil Chenthamarakshan",
                "Payel Das",
                "Samuel Hoffman",
                "Hendrik Strobelt",
                "Inkit Padhi",
                "Kar Wai Lim",
                "Benjamin Hoover",
                "Matteo Manica",
                "Jannis Born",
                "Teodoro Laino"
            ],
            "title": "Cogmol: target-specific and selective drug design for covid-19 using deep generative models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Jerret Ross",
                "Brian Belgodere",
                "Vijil Chenthamarakshan",
                "Inkit Padhi",
                "Youssef Mroueh",
                "Payel Das"
            ],
            "title": "Do large scale molecular language representations capture important structural information",
            "venue": "arXiv preprint arXiv:2106.09553,",
            "year": 2021
        },
        {
            "authors": [
                "Fang Wu",
                "Qiang Zhang",
                "Dragomir Radev",
                "Jiyu Cui",
                "Wen Zhang",
                "Huabin Xing",
                "Ningyu Zhang",
                "Huajun Chen"
            ],
            "title": "Molformer: Motif-based transformer on 3d heterogeneous molecular graphs",
            "venue": "arXiv preprint arXiv:2110.01191,",
            "year": 2021
        },
        {
            "authors": [
                "Sutanay Choudhury",
                "Jenna A. Bilbrey",
                "Logan T. Ward",
                "Sotiris S. Xantheas",
                "Ian T. Foster",
                "Joseph P. Heindel",
                "Ben Blaiszik",
                "Marcus E. Schwarting"
            ],
            "title": "HydroNet: Benchmark tasks for preserving intermolecular interactions and structural motifs in predictive and generative models for molecular data",
            "venue": "Machine Learning and the Physical Sciences Workshop at the 34th Conference on Neural Information Processing Systems (NeurIPS),",
            "year": 2020
        },
        {
            "authors": [
                "Jenna A Bilbrey",
                "Joseph P Heindel",
                "Malachi Schram",
                "Pradipta Bandyopadhyay",
                "Sotiris S Xantheas",
                "Sutanay Choudhury"
            ],
            "title": "A look inside the black box: Using graph-theoretical descriptors to interpret a continuous-filter convolutional neural network (CF-CNN) trained on the global and local minimum energy structures of neutral water clusters",
            "venue": "The Journal of Chemical Physics,",
            "year": 2020
        },
        {
            "authors": [
                "Kaiyuan Guo",
                "Shulin Zeng",
                "Jincheng Yu",
                "Yu Wang",
                "Huazhong Yang"
            ],
            "title": "A survey of FPGA-based neural network accelerator",
            "venue": "arXiv preprint arXiv:1712.08934,",
            "year": 2017
        },
        {
            "authors": [
                "Maurizio Capra",
                "Beatrice Bussolino",
                "Alberto Marchisio",
                "Muhammad Shafique",
                "Guido Masera",
                "Maurizio Martina"
            ],
            "title": "An updated survey of efficient hardware architectures for accelerating deep convolutional neural networks",
            "venue": "Future Internet,",
            "year": 2020
        },
        {
            "authors": [
                "Nathan C Frey",
                "Siddharth Samsi",
                "Joseph McDonald",
                "Lin Li",
                "Connor W Coley",
                "Vijay Gadepally"
            ],
            "title": "Scalable geometric deep learning on molecular graphs",
            "venue": "NeurIPS",
            "year": 2021
        },
        {
            "authors": [
                "KT Sch\u00fctt",
                "Pan Kessel",
                "Michael Gastegger",
                "KA Nicoli",
                "Alexandre Tkatchenko",
                "K-R M\u00fcller"
            ],
            "title": "SchNet- Pack: A deep learning toolbox for atomistic systems",
            "venue": "Journal of Chemical Theory and Computation,",
            "year": 2018
        },
        {
            "authors": [
                "Kristof T Sch\u00fctt",
                "Huziel E Sauceda",
                "P-J Kindermans",
                "Alexandre Tkatchenko",
                "K-R M\u00fcller"
            ],
            "title": "SchNet\u2013a deep learning architecture for molecules and materials",
            "venue": "The Journal of Chemical Physics,",
            "year": 2018
        },
        {
            "authors": [
                "Avijit Rakshit",
                "Pradipta Bandyopadhyay",
                "Joseph P Heindel",
                "Sotiris S"
            ],
            "title": "Xantheas. Atlas of putative minima and low-lying energy networks of water clusters n=3\u201325",
            "venue": "The Journal of Chemical Physics,",
            "year": 2019
        },
        {
            "authors": [
                "George S Fanourgakis",
                "Sotiris S Xantheas"
            ],
            "title": "The flexible, polarizable, Thole-type interaction potential for water (TTM2-F) revisited",
            "venue": "The Journal of Physical Chemistry A,",
            "year": 2006
        },
        {
            "authors": [
                "Volodymyr Babin",
                "Claude Leforestier",
                "Francesco Paesani"
            ],
            "title": "Development of a \u201cfirst principles\u201d water potential with flexible monomers: Dimer potential energy surface, VRT spectrum, and second virial coefficient",
            "venue": "Journal of Chemical Theory and Computation,",
            "year": 2013
        },
        {
            "authors": [
                "Volodymyr Babin",
                "Gregory R Medders",
                "Francesco Paesani"
            ],
            "title": "Development of a \u201cfirst principles\u201d water potential with flexible monomers. II: Trimer potential energy surface, third virial coefficient, and small clusters",
            "venue": "Journal of Chemical Theory and Computation,",
            "year": 2014
        },
        {
            "authors": [
                "Johannes Moe",
                "Konstantin Pogorelov",
                "Daniel Thilo Schroeder",
                "Johannes Langguth"
            ],
            "title": "Implementating spatio-temporal graph convolutional networks on Graphcore IPUs",
            "venue": "IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW),",
            "year": 2022
        },
        {
            "authors": [
                "Matthias Fey",
                "Jan E. Lenssen"
            ],
            "title": "Fast graph representation learning with PyTorch Geometric",
            "venue": "In ICLR Workshop on Representation Learning on Graphs and Manifolds,",
            "year": 2019
        },
        {
            "authors": [
                "Weihua Hu",
                "Bowen Liu",
                "Joseph Gomes",
                "Marinka Zitnik",
                "Percy Liang",
                "Vijay Pande",
                "Jure Leskovec"
            ],
            "title": "Strategies for pre-training graph neural networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Stefan Chmiela",
                "Alexandre Tkatchenko",
                "Huziel E Sauceda",
                "Igor Poltavsky",
                "Kristof T Sch\u00fctt",
                "Klaus- Robert M\u00fcller"
            ],
            "title": "Machine learning of accurate energy-conserving molecular force fields",
            "venue": "Science Advances,",
            "year": 2017
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Pretraining models on massive datasets followed by finetuning towards specific downstream tasks is commonplace in natural language processing and computer vision approaches. The uptake of similar approaches for chemistry is lagging due to the limited number of large datasets and long training times involved. Training atomistic property prediction models from massive scientific datasets is a compute-intensive task, and much of the focus in recent literature has been on transformer-based models [1, 2]. The reduction of atomic positions to character strings via the SMILES notation has aided the generation of large datasets. For example, Wang et al. trained the BERT architecture on 18.7M SMILES strings from the ZINC database [1], while Chithrananda et al. trained a network based on the RoBERTa architecture, called ChemBERTa, on 77M SMILES strings obtained from PubChem [2]. ChemBERTa was later shown to perform well on downstream property prediction\nPreprint. Under review.\nar X\niv :2\n21 1.\n04 59\n8v 1\ntasks [3]. RNN-based generative models have been trained on a 1M-molecule subset of GDB-13 [4] and a 1.6M-molecule subset of ZINC [5]. More recently, Ross et al. developed a transformer-based encoder that uses linear attention, called Molformer, to efficiently train on >1000M SMILES strings, outperforming several graph- and geometry-based baselines on regression and classification tasks from benchmark datasets [6]. Subsequent work has explicitly incorporated spatial properties into Molformer for training on comparatively smaller datasets [7].\nSMILES strings describe atom compositions and bonding configurations, but neglect information about the 3D geometry and long-range interactions, such as interactions between molecules. Here, we answer a benchmark challenge set forth in our prior work [8] of generating a predictive model that preserves intermolecular interactions. This benchmark is supported by an open-source dataset containing 4.95M unique hydrogen bonded clusters of water molecules [9]. All clusters in the dataset are minima on the potential energy surface (PES) computed using the TTM2.1-F potential, making them useful for static property prediction. For example, Bilbrey et al. [10] trained the SchNet neural network on a subset of 500,000 clusters and obtained a mean absolute error per water molecule of 0.002 kcal/mol on a 10,500-sample test set, which included cluster sizes outside of the range of those included in the training subset, indicating the ability of the network to extrapolate. Training on \u223c10% of the full benchmark dataset took 2.7 days scaled over four NVIDIA V100 GPUs [10], making it impractical to train on larger subsets, much less the complete dataset, with traditional hardware. The computing power required for training neural networks on very large datasets restricts exploration of this area to researchers with ample access to GPU clusters or AI hardware accelerators [11, 12, 13].\ndatabase that contains millions of 3D geometries and (2) the use of a novel AI accelerator that dramatically reduces training times."
        },
        {
            "heading": "2 Methods",
            "text": "Data Collection. The dataset of water cluster minima used for pretraining was obtained from an existing database generated using Monte Carlo temperature basin-paving (MC-TBP) simulations driven by the TTM2.1-F potential [16, 17]. Each cluster is associated with 3D coordinates r and energy E. The dataset of non-minima water clusters for downstream MD via data space expansion was obtained from MD simulations performed at 260K and 300K. Each non-minima includes atomic forces F, r, and E. To demonstrate transfer of the PES, E for a subset of 5,000 minima were obtained using the MB-pol interatomic potential [18, 19].\nHardware Accelerators. Accelerators designed specifically for AI/ML applications show improved processing speed, scalability, and energy efficiency, allowing faster training on larger datasets. In particular, Graphcore\u2019s IPU accelerators show a 4\u00d7 speedup over NVIDIA V100 GPUs for training of GNNs [20]. Graphcore\u2019s high-level development framework PopTorch was used to implement the Pytorch Geometric (PyG) library [21], which includes the SchNet framework. We trained SchNet on a dataset containing 2,726,710 water clusters, using hyperparameters reported in previous work [10].\nFinetuning. Pretraining molecular GNNs on large datasets have been shown to improve generalization in downstream tasks [22]. Use of the PyG implementation of SchNet allows the model to be easily transferred between IPUs, GPUs, and CPUs. The saved model weights from the IPU-trained model constitute the pretrained network, which we update to obtain (1) drive MD simulations via\naccurate predictions of E and F on non-minima and (2) accurate predictions of E on a different PES. Inference using trained models was performed on a CPU, while finetuning was performed on a single NVIDIA V100 GPU.\nActive Sampling. We adapt an active sampling strategy when finetuning for non-minima to minimize bias when compiling the small dataset. The training set was divided into a small training subset and large reserve set. During training, the mean absolute error (\u00b5\u03b5) and standard deviation of errors (\u03c3\u03b5) in F were computed over the validation set. Then, \u03b5s was calculated for a subset of reserve samples. A sample was moved from the reserve set to the training subset if 1 \u2212 erf ((\u03b5s \u2212 \u00b5\u03b5)/\u03c3\u03b5) < ptol, where ptol is a chosen tolerance and erf(x) is the Gaussian error function."
        },
        {
            "heading": "3 Results and discussion",
            "text": "We present performance between three model variations: SchNet trained on 2.7M water cluster minima (PRETRAINED), the pretrained model further finetuned using a much smaller dataset (FINETUNED), and a SchNet model trained from scratch using only the smaller dataset (SCRATCH) \u2013 on two downstream tasks \u2013 data space expansion and PES transfer.\nModel Pretraining. The SchNet model was trained on 2.7M water clusters of size N=3\u201325 with a 0.8:0.1:0.1 train-validation-test split. Training took 4.2, 2.2, or 1.2 hours scaled across 16, 32, or 64 IPUs, respectively. The validation loss increased with the number of IPUs to 0.0017, 0.0020, and 0.0030, respectively; therefore, we used the model trained over 16 IPUs to finetune for subsequent downstream tasks. This model showed a similarly low test-set error of 0.0018 kcal/mol.\nData Space Expansion. MD simulations explore non-minima on the PES and can be driven by neural network potentials (NNPs). The forces F used to drive atomic motions are obtained as\nthe negative of the NNP gradient. A force term is typically added to the loss function to improve prediction accuracy [23]. Training without the force term, as was done for the pretrained model, leads to poor prediction of F, even through the accuracy in E predictions on minima configurations is high. Moreover, the pretrained model produces poor E predictions on non-minima, necessitating the need for the data space covered by the model to be expanded. Finetuning the pretrained model on a much smaller subset of non-minima (>1% of the minima dataset) and including a force term in the loss produces a NNP with good predictions of non-minima F and E. Because of the roughly three-order-of-magnitude decrease in the size of the training set when using the pretrained model, training was accomplished on a single NVIDIA V100 GPU in 8.3 hours.\nTable 1 shows the test-set accuracy of the pretrained model and models trained on non-minima with and without pretraining. The test set corresponds to the specific training set, i.e., the pretrained model is tested on minima, while the models trained on non-minima are tested on non-minima. Following Chmiela et al. [23], we quantify the topological accuracy of atomic force predictions by the magnitude error Fmag = \u2016F\u0302\u2016 \u2212 \u2016F\u2016, which describes the extent to which the slope of the predicted and reference PES differ, and the angular error Fang = cos\u22121(F\u0302/\u2016F\u0302\u2016 \u00b7 F/\u2016F\u2016)/\u03c0, which describes the orientation of the predicted force direction relative to the reference force direction and ranges between 0 (aligned) and 1 (inverted). Figure 2 shows static errors for the three models on minima and non-minima. Training from scratch on non-minima greatly improves static predictions of E and provides some improvement in F predictions, while finetuning from the pretrained model greatly improves predictions of F and E for both minima and non-minima.\nWe then performed MD simulations driven by the NNPs (Fig. 2). Berendsen NVT dynamics of three clusters (N = 10, 20, 30) at 300K were simulated 10 times each with different random seeds. The mean (bold lines) and individual simulations (dashed lines) are shown for TTM2.1-F and the NNPs trained on non-minima. MD simulations using the pretrained model are not shown, as the predicted energies were several orders of magnitude below those from TTM2.1-F. The mean E of the finetuned model aligns for all cluster sizes, while that of the model without pretraining aligns only for N = 10. We then calculated the TTM2.1-F E on each point in the NNP-generated trajectories to validate the resulting molecular structures. Notably, only the finetuned model produced valid dynamics (E < 0 kcal/mol). Though the model without pretraining predicted E of similar magnitude to TTM2.1-F, the generated structures were calculated by TTM2.1-F to be highly unstable. In fact, the model trained from scratch did not outperform even the pretrained model in generating stable dynamics.\ndataset, the model was trained in under 28 minutes on 1 NVIDIA V100 GPU. Figure 3 shows EH2O distributions for the TTM2.1-F and MB-pol test sets, with errors shown in Table 2. A shift in EH2O towards higher values is seen for the MB-pol potential and is reproduced for both models trained on MB-pol data. However, the finetuned model showed a \u223c17% lower error and more accurately reproduced the EH2O distribution."
        },
        {
            "heading": "4 Conclusions",
            "text": "We demonstrate that pretraining with a very large dataset of molecular structures improves downstream tasks such as driving MD simulations as well as transfer learning to a different PES. This paper is the first to demonstrate the effectiveness of the fine-grained parallelism of the Graphcore IPU architecture for training molecular GNNs. Initial training was accomplished over 16 IPUs in 4.2 hours, while finetuning with a small set of non-minima was accomplished in 8.3 hours on a single NVIDIA V100 GPU and transfer learning with a very small set of minima computed by a different method was accomplished in only 28 minutes. Pretaining was shown to decrease the amount of data and time required, as well as reduce hardware requirements, increasing training throughput and improving accessibility to researchers with limited resources. The pretrained model was not finetuned for downstream tasks on molecules other than water; moving to a separate area of chemical space, for example, organic small molecules, could be accomplished by following our workflow, i.e., training on an open dataset of minima and finetuning for the desired downstream task on a small bespoke dataset."
        },
        {
            "heading": "Data Availability",
            "text": "The full database of water cluster minima computed with the TTM2.1-F potential is available for download at https://data.pnnl.gov/group/nodes/dataset/33224. The preprocessed databases for training, including the database of nonminima computed with the TTM2.1-F potential and the database of minima computed with the MB-pol potential, dataset split files, trained model state dictionaries, ASE databases used for MD simulations, and results of data space expansion and PES transfer analyses are available for download at https://data.pnnl.gov/group/nodes/ dataset/33283."
        },
        {
            "heading": "Code Availability",
            "text": "The codebase used to finetune the pretrained model for the tasks of data space expansion and PES transfer, along with hyperparameters used in this work, is available at https://github.com/pnnl/ downstream_mol_gnn."
        },
        {
            "heading": "Impact statement",
            "text": "IPUs show reduced energy consumption compared with GPUs when training neural networks. In addition, pretraining reduces data and hardware requirements, leading to reduced energy consumption and increased accessibility to researchers with limited resources."
        },
        {
            "heading": "Acknowledgments and Disclosure of Funding",
            "text": "The authors thank Dr. Logan Ward for fruitful discussions on neural network potentials. J.A.B., K.M.H., H.S., S.S.X., and S.C. were supported by the DOE Exascale Computing Project, ExaLearn Co-design Center. The research was performed using resources available through Research Computing at Pacific Northwest National Laboratory (PNNL). PNNL is operated by Battelle for the U.S. Department of Energy under Contract DE-AC05-76RL01830."
        }
    ],
    "title": "Reducing Down(stream)time: Pretraining Molecular GNNs using Heterogeneous AI Accelerators",
    "year": 2022
}