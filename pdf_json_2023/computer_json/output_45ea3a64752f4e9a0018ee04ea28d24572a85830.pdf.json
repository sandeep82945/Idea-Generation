{
    "abstractText": "In this paper, we propose a novel approach to address the challenges of printed Urdu text recognition using high-resolution, multiscale semantic feature extraction. Our proposed UTRNet architecture, a hybrid CNN-RNN model, demonstrates state-of-the-art performance on benchmark datasets. To address the limitations of previous works, which struggle to generalize to the intricacies of the Urdu script and the lack of sufficient annotated real-world data, we have introduced the UTRSet-Real, a large-scale annotated real-world dataset comprising over 11,000 lines and UTRSet-Synth, a synthetic dataset with 20,000 lines closely resembling real-world and made corrections to the ground truth of the existing IIITH dataset, making it a more reliable resource for future research. We also provide UrduDoc, a benchmark dataset for Urdu text line detection in scanned documents. Additionally, we have developed an online tool for end-to-end Urdu OCR from printed documents by integrating UTRNet with a text detection model. Our work not only addresses the current limitations of Urdu OCR but also paves the way for future research in this area and facilitates the continued advancement of Urdu OCR technology. The project page with source code, datasets, annotations, trained models, and online tool is available at abdur75648.github.io/UTRNet.",
    "authors": [
        {
            "affiliations": [],
            "name": "Abdur Rahman"
        },
        {
            "affiliations": [],
            "name": "Chetan Arora"
        }
    ],
    "id": "SP:b5ff950f6869c169ea568112d1f742902661d63c",
    "references": [
        {
            "authors": [
                "S.B. Ahmed",
                "S. Naz",
                "S. Swati",
                "M.I. Razzak"
            ],
            "title": "Handwritten urdu character recognition using 1-dimensional blstm classifier (2017)",
            "year": 2017
        },
        {
            "authors": [
                "M.U. Akram",
                "S. Hussain"
            ],
            "title": "Word segmentation for urdu ocr system",
            "year": 2010
        },
        {
            "authors": [
                "J.M. Alghazo",
                "G. Latif",
                "L. Alzubaidi",
                "A. Elhassan"
            ],
            "title": "Multi-language handwritten digits recognition based on novel structural features",
            "venue": "Journal of Imaging Science and Technology",
            "year": 2019
        },
        {
            "authors": [
                "A. Ali",
                "M. Pickering"
            ],
            "title": "Urdu-text: A dataset and benchmark for urdu text detection and recognition in natural scenes",
            "venue": "2019 International Conference on Document Analysis and Recognition (ICDAR) pp. 323\u2013328",
            "year": 2019
        },
        {
            "authors": [
                "H. Althobaiti",
                "C. Lu"
            ],
            "title": "A survey on arabic optical character recognition and an isolated handwritten arabic character recognition algorithm using encoded freeman chain code",
            "venue": "2017 51st Annual Conference on Information Sciences and Systems (CISS). pp. 1\u20136",
            "year": 2017
        },
        {
            "authors": [
                "T. Anjum",
                "N. Khan"
            ],
            "title": "An attention based method for offline handwritten urdu text recognition",
            "venue": "2020 17th International Conference on Frontiers in Handwriting Recognition (ICFHR). pp. 169\u2013174",
            "year": 2020
        },
        {
            "authors": [
                "R. Atienza"
            ],
            "title": "Vision transformer for fast and efficient scene text recognition (2021)",
            "venue": "https://arxiv.org/abs/",
            "year": 2021
        },
        {
            "authors": [
                "J. Baek",
                "G. Kim",
                "J. Lee",
                "S. Park",
                "D. Han",
                "S. Yun",
                "S.J. Oh",
                "H. Lee"
            ],
            "title": "What is wrong with scene text recognition model comparisons? dataset and model analysis (2019)",
            "year": 1906
        },
        {
            "authors": [
                "D. Bahdanau",
                "K. Cho",
                "Y. Bengio"
            ],
            "title": "Neural machine translation by jointly learning to align and translate",
            "year": 2014
        },
        {
            "authors": [
                "D. Bautista",
                "R. Atienza"
            ],
            "title": "Scene text recognition with permuted autoregressive sequence models (2022)",
            "year": 2022
        },
        {
            "authors": [
                "F. Borisyuk",
                "A. Gordo",
                "V. Sivakumar"
            ],
            "title": "Rosetta: Large scale system for text detection and recognition in images",
            "venue": "Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. ACM",
            "year": 2018
        },
        {
            "authors": [
                "H. Butt",
                "M.R. Raza",
                "M. Ramzan",
                "M.J. Ali",
                "M. Haris"
            ],
            "title": "Attention-based cnnrnn arabic text recognition from natural scene images",
            "venue": "Forecasting 3, 520\u2013540",
            "year": 2021
        },
        {
            "authors": [
                "W. Byeon",
                "M. Liwicki",
                "T.M. Breuel"
            ],
            "title": "Texture classification using 2d lstm networks",
            "venue": "2014 22nd International Conference on Pattern Recognition. pp. 1144\u2013 1149",
            "year": 2014
        },
        {
            "authors": [
                "E. Chammas",
                "C. Mokbel"
            ],
            "title": "Fine-tuning handwriting recognition systems with temporal dropout",
            "venue": "ArXiv abs/2102.00511",
            "year": 2021
        },
        {
            "authors": [
                "A.A. Chandio",
                "M. Asikuzzaman",
                "M. Pickering",
                "M. Leghari"
            ],
            "title": "Cursivetext: A comprehensive dataset for end-to-end urdu text recognition in natural scene images",
            "venue": "Data in Brief 31, 105749",
            "year": 2020
        },
        {
            "authors": [
                "K. Cho",
                "B. van Merrienboer",
                "D. Bahdanau",
                "Y. Bengio"
            ],
            "title": "On the properties of neural machine translation: Encoder-decoder approaches",
            "year": 2014
        },
        {
            "authors": [
                "P. Choudhary",
                "N. Nain"
            ],
            "title": "A four-tier annotated urdu handwritten text image dataset for multidisciplinary research on urdu script",
            "venue": "ACM Trans. Asian LowResour. Lang. Inf. Process. 15(4)",
            "year": 2016
        },
        {
            "authors": [
                "S. Djaghbellou",
                "A. Bouziane",
                "A. Attia",
                "Z. Akhtar"
            ],
            "title": "A survey on arabic handwritten script recognition systems",
            "venue": "International Journal of Artificial Intelligence and Machine Learning 11, 1\u201317",
            "year": 2021
        },
        {
            "authors": [
                "S. Fang",
                "H. Xie",
                "Y. Wang",
                "Z. Mao",
                "Y. Zhang"
            ],
            "title": "Read like humans: Autonomous, bidirectional and iterative language modeling for scene text recognition",
            "year": 2021
        },
        {
            "authors": [
                "M. Fasha",
                "B.H. Hammo",
                "N. Obeid",
                "J. Widian"
            ],
            "title": "A hybrid deep learning model for arabic text recognition",
            "venue": "ArXiv abs/2009.01987",
            "year": 2020
        },
        {
            "authors": [
                "R. Girshick",
                "J. Donahue",
                "T. Darrell",
                "J. Malik"
            ],
            "title": "Rich feature hierarchies for accurate object detection and semantic segmentation",
            "year": 2013
        },
        {
            "authors": [
                "A. Graves",
                "S. Fern\u00e1ndez",
                "F. Gomez",
                "J. Schmidhuber"
            ],
            "title": "Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks",
            "venue": "Proceedings of the 23rd International Conference on Machine Learning. p. 369\u2013376. ICML \u201906",
            "year": 2006
        },
        {
            "authors": [
                "A. Graves",
                "J. Schmidhuber"
            ],
            "title": "Offline arabic handwriting recognition with multidimensional recurrent neural networks",
            "venue": "pp. 545\u2013552",
            "year": 2008
        },
        {
            "authors": [
                "K. Han",
                "Y. Wang",
                "H. Chen",
                "X. Chen",
                "J. Guo",
                "Z. Liu",
                "Y. Tang",
                "A. Xiao",
                "C. Xu",
                "Y. Xu",
                "Z. Yang",
                "Y. Zhang",
                "D. Tao"
            ],
            "title": "A survey on vision transformer",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 45(1), 87\u2013110",
            "year": 2023
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "year": 2015
        },
        {
            "authors": [
                "M. Husnain",
                "M.M. Saad Missen",
                "S. Mumtaz",
                "M. Coustaty",
                "M. Luqman",
                "J.M. Ogier"
            ],
            "title": "Urdu handwritten text recognition: a survey",
            "venue": "IET Image Processing 14(11), 2291\u20132300",
            "year": 2020
        },
        {
            "authors": [
                "S. Hussain"
            ],
            "title": "A survey of ocr in arabic language: Applications, techniques, and challenges",
            "venue": "Applied Sciences 13, 27",
            "year": 2023
        },
        {
            "authors": [
                "M. Jain",
                "M. Mathew",
                "C.V. Jawahar"
            ],
            "title": "Unconstrained scene text and video text recognition for arabic script",
            "venue": "2017 1st International Workshop on Arabic Script Analysis and Recognition (ASAR) pp. 26\u201330",
            "year": 2017
        },
        {
            "authors": [
                "M. Jain",
                "M. Mathew",
                "C. Jawahar"
            ],
            "title": "Unconstrained ocr for urdu using deep cnnrnn hybrid networks",
            "venue": "2017 4th IAPR Asian Conference on Pattern Recognition (ACPR). pp. 747\u2013752. IEEE",
            "year": 2017
        },
        {
            "authors": [
                "M. Kashif"
            ],
            "title": "Urdu handwritten text recognition using resnet18 (2021)",
            "year": 2021
        },
        {
            "authors": [
                "A.M. Kassem",
                "O. Mohamed",
                "A. Ashraf",
                "A. Elbehery",
                "S. Jamal",
                "G. Khoriba",
                "A.S. Ghoneim"
            ],
            "title": "Ocformer: A transformer-based model for arabic handwritten text recognition",
            "venue": "2021 International Mobile, Intelligent, and Ubiquitous Computing Conference (MIUCC) pp. 182\u2013186",
            "year": 2021
        },
        {
            "authors": [
                "K. Khan",
                "R. Ullah",
                "N. Ahmad",
                "K. Naveed"
            ],
            "title": "Urdu character recognition using principal component analysis",
            "venue": "International Journal of Computer Applications 60",
            "year": 2012
        },
        {
            "authors": [
                "N.H. Khan",
                "A. Adnan"
            ],
            "title": "Urdu optical character recognition systems: Present contributions and future directions",
            "venue": "IEEE Access 6, 46019\u201346046",
            "year": 2018
        },
        {
            "authors": [
                "N.H. Khan",
                "A. Adnan",
                "S. Basar"
            ],
            "title": "An analysis of off-line and on-line approaches in urdu character recognition",
            "venue": "2016 15th International Conference on Artificial Intelligence, Knowledge Engineering and Data Bases (AIKED\u201916)",
            "year": 2016
        },
        {
            "authors": [
                "D. Ko",
                "C. Lee",
                "D. Han",
                "H. Ohk",
                "K. Kang",
                "S. Han"
            ],
            "title": "Approach for machine-printed arabic character recognition: the-state-of-the-art deep-learning method",
            "venue": "electronic imaging 2018, 176\u20131\u2013176\u20138",
            "year": 2018
        },
        {
            "authors": [
                "A. Kolesnikov",
                "A. Dosovitskiy",
                "D. Weissenborn",
                "G. Heigold",
                "J. Uszkoreit",
                "L. Beyer",
                "M. Minderer",
                "M. Dehghani",
                "N. Houlsby",
                "S. Gelly",
                "T. Unterthiner",
                "X. Zhai"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "year": 2021
        },
        {
            "authors": [
                "Y. Lecun",
                "L. Bottou",
                "Y. Bengio",
                "P. Haffner"
            ],
            "title": "Gradient-based learning applied to document recognition",
            "venue": "Proceedings of the IEEE 86(11), 2278\u20132324",
            "year": 1998
        },
        {
            "authors": [
                "C.Y. Lee",
                "S. Osindero"
            ],
            "title": "Recursive recurrent nets with attention modeling for ocr in the wild",
            "year": 2016
        },
        {
            "authors": [
                "M. Li",
                "T. Lv",
                "J. Chen",
                "L. Cui",
                "Y. Lu",
                "D. Florencio",
                "C. Zhang",
                "Z. Li",
                "F. Wei"
            ],
            "title": "Trocr: Transformer-based optical character recognition with pre-trained models (2021)",
            "venue": "https://arxiv.org/abs/",
            "year": 2021
        },
        {
            "authors": [
                "T.Y. Lin",
                "M. Maire",
                "S. Belongie",
                "L. Bourdev",
                "R. Girshick",
                "J. Hays",
                "P. Perona",
                "D. Ramanan",
                "C.L. Zitnick",
                "P. Doll\u00e1r"
            ],
            "title": "Microsoft coco: Common objects in context",
            "year": 2014
        },
        {
            "authors": [
                "W. Liu",
                "C. Chen",
                "K.Y. Wong",
                "Z. Su",
                "J. Han"
            ],
            "title": "Star-net: A spatial attention residue network for scene text recognition",
            "venue": "pp. 43.1\u201343.13",
            "year": 2016
        },
        {
            "authors": [
                "F. Mushtaq",
                "M.M. Misgar",
                "M. Kumar",
                "S.S. Khurana"
            ],
            "title": "UrduDeepNet: offline handwritten urdu character recognition using deep neural network",
            "venue": "Neural Computing and Applications 33(22), 15229\u201315252",
            "year": 2021
        },
        {
            "authors": [
                "S. Naz",
                "S. Ahmed",
                "R. Ahmad",
                "M. Razzak"
            ],
            "title": "Zoning features and 2dlstm for urdu text-line recognition",
            "venue": "Procedia Computer Science 96, 16\u201322",
            "year": 2016
        },
        {
            "authors": [
                "S. Naz",
                "A.I. Umar",
                "R. Ahmad",
                "I. Siddiqi",
                "S.B. Ahmed",
                "M.I. Razzak",
                "F. Shafait"
            ],
            "title": "Urdu nastaliq recognition using convolutional\u2013recursive deep learning",
            "venue": "Neurocomputing 243, 80\u201387",
            "year": 2017
        },
        {
            "authors": [
                "O. Oktay",
                "J. Schlemper",
                "L.L. Folgoc",
                "M. Lee",
                "M. Heinrich",
                "K. Misawa",
                "K. Mori",
                "S. McDonagh",
                "N.Y. Hammerla",
                "B. Kainz",
                "B. Glocker",
                "D. Rueckert"
            ],
            "title": "Attention u-net: Learning where to look for the pancreas",
            "year": 2018
        },
        {
            "authors": [
                "U. Pal",
                "A. Sarkar"
            ],
            "title": "Recognition of printed urdu script",
            "venue": "Seventh International Conference on Document Analysis and Recognition, 2003. Proceedings. pp. 1183\u2013 1187",
            "year": 2003
        },
        {
            "authors": [
                "N.S. Punn",
                "S. Agarwal"
            ],
            "title": "Inception u-net architecture for semantic segmentation to identify nuclei in microscopy cell images",
            "venue": "ACM Trans. Multimedia Comput. Commun. Appl. 16(1)",
            "year": 2020
        },
        {
            "authors": [
                "S.F. Rashid",
                "M.P. Schambach",
                "J. Rottland",
                "S. N\u00fcll"
            ],
            "title": "Low resolution arabic recognition with multidimensional recurrent neural networks",
            "year": 2013
        },
        {
            "authors": [
                "O. Ronneberger",
                "P. Fischer",
                "T. Brox"
            ],
            "title": "U-net: Convolutional networks for biomedical image segmentation",
            "year": 2015
        },
        {
            "authors": [
                "N. Sabbour",
                "F. Shafait"
            ],
            "title": "A segmentation free approach to arabic and urdu ocr",
            "venue": "Proceedings of SPIE - The International Society for Optical Engineering 8658",
            "year": 2013
        },
        {
            "authors": [
                "M.W. Sagheer",
                "C.L. He",
                "N. Nobile",
                "C.Y. Suen"
            ],
            "title": "A new large urdu database for off-line handwriting recognition",
            "venue": "Foggia, P., Sansone, C., Vento, M. (eds.) Image Analysis and Processing \u2013 ICIAP 2009. pp. 538\u2013546. Springer Berlin Heidelberg, Berlin, Heidelberg",
            "year": 2009
        },
        {
            "authors": [
                "S. Sardar",
                "A. Wahab"
            ],
            "title": "Optical character recognition system for urdu",
            "venue": "2010 International Conference on Information and Emerging Technologies pp. 1\u20135",
            "year": 2010
        },
        {
            "authors": [
                "M. Schuster",
                "K.K. Paliwal"
            ],
            "title": "Bidirectional recurrent neural networks",
            "venue": "IEEE Transactions on Signal Processing 45(11), 2673\u20132681",
            "year": 1997
        },
        {
            "authors": [
                "N. Semary",
                "M. Rashad"
            ],
            "title": "Isolated printed arabic character recognition using knn and random forest tree classifiers",
            "venue": "vol. 488, pp. 11\u2013",
            "year": 2014
        },
        {
            "authors": [
                "A. Shahin"
            ],
            "title": "Printed arabic text recognition using linear and nonlinear regression",
            "venue": "International Journal of Advanced Computer Science and Applications 8",
            "year": 2017
        },
        {
            "authors": [
                "M.D. Shaiq",
                "M.D.A. Cheema",
                "A. Kamal"
            ],
            "title": "Transformer based urdu handwritten text optical character reader (2022)",
            "year": 2022
        },
        {
            "authors": [
                "B. Shi",
                "X. Bai",
                "C. Yao"
            ],
            "title": "An end-to-end trainable neural network for imagebased sequence recognition and its application to scene text recognition",
            "year": 2015
        },
        {
            "authors": [
                "B. Shi",
                "X. Wang",
                "P. Lyu",
                "C. Yao",
                "X. Bai"
            ],
            "title": "Robust scene text recognition with automatic rectification",
            "year": 2016
        },
        {
            "authors": [
                "K. Simonyan",
                "A. Zisserman"
            ],
            "title": "Very deep convolutional networks for large-scale image recognition",
            "year": 2014
        },
        {
            "authors": [
                "A. Singh",
                "K. Bacchuwar",
                "A. Bhasin"
            ],
            "title": "A survey of ocr applications",
            "venue": "International Journal of Machine Learning and Computing (IJMLC)",
            "year": 2012
        },
        {
            "authors": [
                "M. Sobhi",
                "Y. Hifny",
                "S. Mesbah Elkaffas"
            ],
            "title": "Arabic optical character recognition using attention based encoder-decoder architecture",
            "venue": "2020 2nd International Conference on Artificial Intelligence, Robotics and Control. p. 1\u20135. AIRC\u201920, Association for Computing Machinery, New York, NY, USA",
            "year": 2021
        },
        {
            "authors": [
                "I. Sutskever",
                "O. Vinyals",
                "Le"
            ],
            "title": "Q.V.: Sequence to sequence learning with neural networks",
            "year": 2014
        },
        {
            "authors": [
                "N. Tabassam",
                "S. Naqvi",
                "H. Rehman",
                "F. Anoshia"
            ],
            "title": "Optical character recognition system for urdu (naskh font) using pattern matching technique",
            "venue": "International Journal of Image Processing 3",
            "year": 2009
        },
        {
            "authors": [
                "J. Wang",
                "X. Hu"
            ],
            "title": "Gated recurrent convolution neural network for ocr",
            "venue": "Proceedings of the 31st International Conference on Neural Information Processing Systems. p. 334\u2013343. NIPS\u201917, Curran Associates Inc., Red Hook, NY, USA",
            "year": 2017
        },
        {
            "authors": [
                "J. Wang",
                "K. Sun",
                "T. Cheng",
                "B. Jiang",
                "C. Deng",
                "Y. Zhao",
                "D. Liu",
                "Y. Mu",
                "M. Tan",
                "X. Wang",
                "W. Liu",
                "B. Xiao"
            ],
            "title": "Deep high-resolution representation learning for visual recognition",
            "year": 2019
        },
        {
            "authors": [
                "W. Wang",
                "E. Xie",
                "X. Li",
                "W. Hou",
                "T. Lu",
                "G. Yu",
                "S. Shao"
            ],
            "title": "Shape robust text detection with progressive scale expansion network (2019)",
            "year": 1903
        },
        {
            "authors": [
                "Y. Wang",
                "H. Xie",
                "S. Fang",
                "J. Wang",
                "S. Zhu",
                "Y. Zhang"
            ],
            "title": "From two to one: A new scene text recognizer with visual language modeling network (2021)",
            "venue": "https://arxiv.org/abs/",
            "year": 2021
        },
        {
            "authors": [
                "Y. Wang",
                "H. Xie",
                "Z. Zha",
                "M. Xing",
                "Z. Fu",
                "Y. Zhang"
            ],
            "title": "Contournet: Taking a further step toward accurate arbitrary-shaped scene text detection",
            "year": 2020
        },
        {
            "authors": [
                "L. Yuan",
                "Y. Chen",
                "T. Wang",
                "W. Yu",
                "Y. Shi",
                "Z. Jiang",
                "F.E. Tay",
                "J. Feng",
                "S. Yan"
            ],
            "title": "Tokens-to-token vit: Training vision transformers from scratch on imagenet (2021)",
            "year": 1986
        },
        {
            "authors": [
                "M.D. Zeiler"
            ],
            "title": "Adadelta: An adaptive learning rate method",
            "year": 2012
        },
        {
            "authors": [
                "S.X. Zhang",
                "X. Zhu",
                "J.B. Hou",
                "C. Liu",
                "C. Yang",
                "H. Wang",
                "X.C. Yin"
            ],
            "title": "Deep relational reasoning graph network for arbitrary shape text detection",
            "year": 2020
        },
        {
            "authors": [
                "Z. Zhang",
                "Q. Liu",
                "Y. Wang"
            ],
            "title": "Road extraction by deep residual u-net",
            "venue": "IEEE Geoscience and Remote Sensing Letters 15(5), 749\u2013753",
            "year": 2018
        },
        {
            "authors": [
                "T. Zheng",
                "Z. Chen",
                "S. Fang",
                "H. Xie",
                "Y.G. Jiang"
            ],
            "title": "Cdistnet: Perceiving multi-domain character distance for robust text recognition (2021)",
            "year": 2021
        },
        {
            "authors": [
                "X. Zhou",
                "C. Yao",
                "H. Wen",
                "Y. Wang",
                "S. Zhou",
                "W. He",
                "J. Liang"
            ],
            "title": "East: An efficient and accurate scene text detector (2017)",
            "year": 2017
        },
        {
            "authors": [
                "Z. Zhou",
                "M.M.R. Siddiquee",
                "N. Tajbakhsh",
                "J. Liang"
            ],
            "title": "Unet++: A nested u-net architecture for medical image segmentation (2018)",
            "year": 2018
        },
        {
            "authors": [
                "A. Zoizou",
                "A. Zarghili",
                "I. Chaker"
            ],
            "title": "A new hybrid method for arabic multi-font text segmentation, and a reference corpus construction",
            "venue": "J. King Saud Univ. Comput. Inf. Sci. 32, 576\u2013582",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "Keywords: Urdu OCR \u00b7 UTRNet\u00b7 UTRSet \u00b7 Printed Text Recognition \u00b7 High-Resolution Feature Extraction"
        },
        {
            "heading": "1 Introduction",
            "text": "Printed text recognition, also known as optical character recognition (OCR), involves converting digital images of text into machine-readable text and is an important topic of research in the realm of document analysis with applications in a wide variety of areas [61]. While OCR has transformed the accessibility & utility\nThis is a pre-print of our paper accepted for presentation at ICDAR 2023. It has not undergone peer review or any post-submission improvements. The Version of the Record of this contribution is published in \u201cDocument Analysis and Recognition - ICDAR 2023\u201d and is available online at https://doi.org/10.1007/ 978-3-031-41734-4_19. We kindly request that readers refer to the published version of the record of our work for the most accurate and authoritative representation.\nar X\niv :2\n30 6.\n15 78\n2v 3\n[ cs\n.C V\n] 2\n3 A\nug 2\nof written/printed information, it has traditionally been focused on Latin languages, leaving non-Latin low-resource languages such as Urdu, Arabic, Pashto, Sindhi and Persian largely untapped. Despite recent developments in Arabic script OCR [18,5,28], research on OCR for Urdu remains limited [34,27,35]. With over 230 million native speakers and a huge literature corpus, including classical prose and poetry, newspapers, and manuscripts, Urdu is the 10th most spoken language in the world [43,30]. Hence the development of a robust OCR system for Urdu remains an open research problem and a crucial requirement for efficient storage, indexing, and consumption of its vast heritage, mainly its classical literature.\nHowever, the intricacies of the Urdu script, which predominantly exists in the Nastaleeq style, present significant challenges. It is primarily cursive, with a wide range of variations in writing style and a high degree of overall complexity, as shown in Fig. 1. Though Arabic script is similar [56], the challenges faced in recognizing Urdu text differ significantly. Arabic text is usually printed in the Naskh style, which is mostly upright and less cursive, and has only 28 alphabets [28], in contrast to the Urdu script, which consists of 45 main alphabets, 26 punctuation marks, 8 honorific marks, and 10 Urdu digits, as well as various special characters from Persian and Arabic, English alphabets, numerals, and punctuation marks, resulting in a total of 181 distinct glyphs [30] that need to be recognized. Furthermore, the lack of large annotated real-world datasets in Urdu compounds these challenges, making it difficult to compare different models\u2019 performance accurately and to continue advancing research in the field (Section 4). The lack of standardization in many Urdu fonts and their rendering schemes (particularly in early Urdu literature) further complicates the generation\nof synthetic data that closely resembles real-world representations. This hinders experiments with more recent transformer-based OCR models that require large training datasets (Table 3B) [57,70,37]. As a result, a naive application of the methods developed for other languages does not result in a satisfactory performance for Urdu (Table 3), highlighting the need for exclusive research in OCR for Urdu.\nThe purpose of our research is to address these long-standing limitations in printed Urdu text recognition through the following key contributions:\n1. We propose a novel approach using high-resolution, multi-scale semantic feature extraction in our UTRNet architecture, a hybrid CNN-RNN model, that demonstrates state-of-the-art performance on benchmark datasets. 2. We create UTRSet-Real, a large-scale annotated real-world dataset comprising over 11,000 lines. 3. We have developed a robust synthetic data generation module and release UTRSet-Synth, a high-quality synthetic dataset of 20,000 lines closely resembling real-world representations of Urdu text. 4. We correct many annotation errors in one of the benchmark datasets [30] for Urdu OCR, thereby elevating its reliability as a valuable resource for future research endeavours and release the corrected annotations publicly. 5. We have curated UrduDoc, a real-world Urdu documents text line detection dataset. The dataset is a byproduct of our efforts towards UTRSet-Real, and contains line segmentation annotation for 478 pages generated from more than 130 books. 6. To make the output of our project available to a larger non-computing research community, as well as lay users, we have developed an online tool for\nend-to-end Urdu OCR, integrating UTRNet with a third-party text detection model.\nIn addition to our key contributions outlined above, we conduct a thorough comparative analysis of state-of-the-art (SOTA) Urdu OCRmodels under similar experimental setups, using a unifying framework introduced by [8] that encompasses feature extraction, sequence modelling and prediction stages and provides a common perspective for all the existing methods. We also examine the contributions of individual modules towards accuracy as an ablation study (Table 2). Finally, we discuss the remaining limitations and potential avenues for future research in Urdu text recognition."
        },
        {
            "heading": "2 Related Work",
            "text": "The study of Urdu OCR has gained attention only in recent years. While the first OCR tools were developed more than five decades back [34], the earliest machine learning based Urdu OCR was developed in 2003 [47]. Since then, the research in this field has progressed from isolated character recognition to word/ligature level recognition to line level recognition (see [34,27,35] for a detailed survey). Early approaches primarily relied on handcrafted features, and traditional machine learning techniques, such as nearest neighbour classification, PCA & HMMs [64,33,53,2], to classify characters after first segmenting individual characters/glyphs from a line image. These techniques often required extensive pre-processing and struggled to achieve satisfactory performance on large, varied datasets.\nRecently segmentation-free end-to-end approaches based on CNN-RNN hybrid networks [30] have been introduced, in which a CNN [38] is used to extract low-level visual features from input data which is then fed to an RNN [63] to get contextual features for the output transcription layer. Among the current SOTA models for Urdu OCR (Table 3C), VGG networks [60] have been used for feature extraction in [30,45,44], whereas ResNet networks [25] have been utilized in [31]. For sequential modelling, BiLSTM networks [54] have been used in [30,31], while MDLSTM networks [13] have been employed in [45,44]. All of these approaches utilize a Connectionist Temporal Classification (CTC) layer [22] for output transcription. In contrast, [6] utilizes a DenseNet [26] and GRU network [16] with an attention-based decoder layer [9] for final transcription. Arabic, like Urdu, shares many similarities in the script as discussed above, and as such, the journey of OCR development has been similar [29,18]. Recent works have shown promising results in recognising Arabic text through a variety of methods, including traditional approaches [55,77], as well as DL-based approaches such as CNN-RNN hybrids [20,29], attention-based models [62,12], and a range of network architectures [49,23,36,32]. However, these approaches still struggle when it comes to recognizing Urdu text, as evident from the low accuracies achieved by SOTA Arabic OCR methods like [20], [62] and [12] in our experimental results presented in Table 3.\nWhile each of the methods proposed so far has claimed to have pushed the boundary of the technology, they often rely on the same methodologies used for other languages without considering the complexities specific to Urdu script and, as such, do not fully utilize the potential in this field. Additionally, a fair comparison among these approaches has been largely missing due to inconsistencies in the datasets used, as described in the dataset section below."
        },
        {
            "heading": "3 Proposed Architecture",
            "text": "Recently, transformer-based models have achieved state-of-the-art performance on various benchmarks [24]. However, these models have the drawback of being highly data-intensive and requiring large amounts of training data, making it difficult to use them for several tasks with limited real-world data, such as\nprinted Urdu text recognition (as discussed in Section 1). In light of this, we propose UTRNet (Figure 3), a novel CNN-RNN hybrid network that offers a viable alternative. The proposed model effectively extracts high-resolution multiscale features while capturing sequential dependencies, making it an ideal fit for Urdu text recognition in real-world scenarios. We have designed UTRNet in two versions: UTRNet-Small (10.7M parameters) and UTRNet-Large (47.3M parameters). The architecture consists of three main stages: feature extraction, sequential modelling, and decoding. The feature extraction stage makes use of convolutional layers to extract feature representations from the input image. These representations are then passed to the sequential modelling stage to learn sequential dependencies among them. Finally, the decoding module converts the sequential data feature thus obtained into the final output sequence."
        },
        {
            "heading": "3.1 High-Resolution Multiscale Feature Extraction",
            "text": "In our proposed method, we address the wide mismatch in the accuracy of different characters observed in existing techniques, as shown in Figure 2. We posit that this is due to the lack of attention given to small features associated with most Urdu characters by the existing methods. These methods rely upon using standard CNNs, such as RCNN[21], VGG[60], ResNet[25], etc., as their backbone. However, a significant drawback of using these CNNs is the low resolution representation generated at the output layer. The representation lacks low-level feature information, such as the dots (called \u201cNuqta\u201d) in Urdu characters. To overcome this limitation, in our proposed method, we propose to use a highresolution multiscale feature extraction technique to extract features from an input image while preserving the small details of the image.\nUTRNet-Small: To address the issue of computational efficiency, we propose a lighter variant of our novel UTRNet architecture, referred to as UTRNet-Small, which employs a standard U-Net model (as shown in Figure 3b), initially proposed in [50] for biomedical image segmentation. The lighter version proposed by us addresses captures high resolution feature maps using the standard U-Net model, originally proposed in [50] for biomedical image segmentation. We first encode the low-resolution representation of input image X, which captured context from a large receptive field. We then recovers the high-resolution representation using learnable convolutions from the previous decoder layer, and skip connections from the encoder layers. For any resolution with index r \u2208 {1, 2, 3, 4, 5}, the feature map at that resolution can be defined as: Fr = downsample(Fr\u22121) during downsampling. Here downsample(Fr\u22121) is the downsampled feature map from the resolution index (r \u2212 1). Similarly, Fr = concat(upsample(Fr+1),Mr) represents feature map during upsampling, where upsample(Fr+1) is the upsampled feature map from the resolution index (r + 1), and Mr is the feature map from the downsampling path corresponding to that resolution). This allows the model to aggregate image features from multiple image scales.\nUTRNet-Large: The model (Figure 3c) maintains high-resolution representation throughout the process and captures the fine-grained details more efficiently,\nusing an HRNet architecture [66]. The resulting network consists of 4 stages, with the Ith stage containing streams coming from #I different resolutions and giving out #I streams corresponding to the different resolutions. Each stream in the Ith stage is represented as RIr , where r \u2208 {1, 2, . . . , I}. These streams which are then inter-fused among themselves to get #(I + 1) final output streams, RI,Or for the next stage:\nRI,Or = I\u2211 i=1 fir(R I i ), r \u2208 {1, 2, . . . , I + 1}\nThe transform function fir is dependent on the input resolution index i and the output resolution index r. If x = r, then fxr(R) = R. However, if x < r, then fxr(R) downsamples the input representation R. Similarly, if x > r, then fxr(R) upsamples the input resolution. UTRNet-Large uses repeated multi-resolution fusions which allows effective exchange of information across multiple resolutions. Thus, giving a multi-dimensional and high-resolution feature representation of the input image, which is semantically richer."
        },
        {
            "heading": "3.2 Sequential Modeling And Prediction",
            "text": "The output from the feature extraction stage is a feature map V = {vi}. To prevent over-fitting, we implement a technique called Temporal Dropout [14], in which we randomly drop half of the visual features before passing them to the next stage. We do this 5 times in parallel and take the average. In order to capture the rich contextual information and temporal relationships between the features thus obtained, we pass it through 2 layers of BiLSTM [54] (DBiLSTM) [58]. Each BiLSTM layer identifies two hidden states, hft and h b t , calculated forward and backward through time, respectively, which are combined to determine one hidden state ht, using an FC layer. The sequence H = {ht} = DBiLSTM(V )) thus obtained has rich contextual information from both directions, which is crucial for Urdu text recognition, especially because the shape of each character depends upon characters around it. The final prediction output Y = {y1, y2, . . .}, a variable-length sequence of characters, is generated by the prediction module from the input sequence H. For this, we use the Connectionist temporal classification (CTC), as described in [22]."
        },
        {
            "heading": "4 Current Publicly Available and Proposed Datasets",
            "text": "The availability of datasets for the study of Urdu Optical Character Recognition (OCR) is limited, with only a total of six datasets currently available: UPTI [51], IIITH [30], UNHD [1], CENPARMI [52], CALAM [17], and PMU-UD [3]. Of these datasets, only IIITH and UPTI contain printed text line samples, out of which only UPTI has a sufficient number of samples for training. However, the UPTI dataset is synthetic in nature, with limited diversity and simplicity in\ncomparison to real-world images (See Figure 4). There is currently no comprehensive real-world printed Urdu OCR dataset available publicly for researchers. As a result, different studies have used their own proprietary datasets, making it difficult to determine the extent to which proposed models improve upon existing approaches. This lack of standardisation and transparency hinders the ability to compare the performance of different models accurately and to continue advancing research in the field, which our work aims to tackle with the introduction of two new datasets. The following are the datasets used in this paper (also summarized in Figure 4):\nIIITH: This dataset was introduced by [30] in 2017, and it contains only the validation data set of 1610 line images in nearly uniform colour and font. No training data set has been provided. We corrected the ground-truth annotations for this dataset, as we found several mistakes, as highlighted in Figure 5.\nUPTI: Unlike the other two, this is a synthetic data set introduced in 2013 by [51]. It consists of a total of 10,063 samples, out of which 2,012 samples are in\nthe validation set. This data set is also uniform in colour and font and has a vocabulary of 12,054 words.\nProposed UTRSet-Real: A comprehensive real-world annotated dataset curated by us, containing a few easy and mostly hard images (as illustrated in Figure 1). To create this dataset, we collected 130 books and old documents, scanned over 500 pages, and manually annotated the scanned images with linewise bounding boxes and corresponding ground truth labels. After cropping the lines and performing data cleaning, we obtained a final dataset of 11,161 lines, with 2,096 lines in the validation set and the remaining in the training set. This dataset stands out for its diversity, with various fonts, text sizes, colours, orientations, lighting conditions, noises, styles, and backgrounds represented in the samples, making it well-suited for real-world Urdu text recognition.\nProposed UTRSet-Synth: To complement the real-world data in UTRSetReal for training purposes, we also present UTRSet-Synth, a high-quality synthetic dataset of 20,000 lines with over 28,000 total unique words, closely resembling real-world representations of Urdu text. This dataset is generated using a custom-designed synthetic data generation module that allows for precise control over variations in font, text size, colour, resolution, orientation, noise, style, background etc. The module addresses the challenge of standardizing fonts by collecting and incorporating over 130 diverse fonts of Urdu after making corrections to their rendering schemes. It also addresses the limitation of current datasets, which have very few instances of Arabic words and numerals, Urdu digits etc., by incorporating such samples in sufficient numbers. Additionally, it generates text samples by randomly selecting words from a vocabulary of 100,000 words. The generated UTRSet-Synth has 28,187 unique words with an average word length of 7 characters. The data generation module has been made publicly available on the project page to facilitate further research.\nProposed UrduDoc: In addition to the recognition datasets discussed above, we also present UrduDoc, a benchmark dataset for Urdu text line detection in scanned documents. To the best of our knowledge, this is the first dataset of its kind [4,15]. It was created as a byproduct of the UTRSet-Real dataset generation process, in which the pages were initially scanned and then annotated with horizontal bounding boxes in COCO format [41] to crop the text lines. Comprising of 478 diverse images collected from various sources such as books, documents,\nmanuscripts, and newspapers, it is split into 358 pages for training and 120 for validation. The images include a wide range of styles, scales, and lighting conditions, making them a valuable resource for the research community. The UrduDoc dataset will serve as a valuable resource for the research community, advancing research in Urdu document analysis. We also provide benchmark results of a few SOTA text detection models on this dataset using precision, recall, and h-mean, as shown in Table 1. The results demonstrate that the ContourNet model [69] outperforms the other models in terms of h-mean. It is worth noting that as text detection was not the primary focus of our research but rather a secondary contribution, a thorough examination of text detection has not been conducted. This aspect can be considered a future work for researchers interested in advancing the field of Urdu document analysis. We will make this dataset publicly available to the research community for non-commercial, academic, and research purposes associated with Urdu document analysis, subject to request and upon execution of a no-cost license agreement."
        },
        {
            "heading": "5 Experiments And Results",
            "text": "Experimental Setup: In order to ensure a fair comparison among the existing models in the field, we have established a consistent experimental setup for evaluating the performance of all the models and report all the results in Table\n3. Specifically, we have fixed the choice of training to the UTRSet-Real training set, the validation set to be the validation sets of the datasets outlined in Figure 4. Further, to compare the different available training datasets, we train our proposed UTRNet models on each of them and present the results in Table 4. We have used the AdaDelta optimizer [71], with a decay rate of 0.95, to train our UTRNet models on an NVIDIA A100 40GB GPU. The batch size and learning rate used were 32 and 1.0, respectively. Gradient clipping was employed at a magnitude of 5, and all parameters were initialized using He\u2019s method [66]. To improve the robustness of the model, we employed a variety of data augmentation techniques during the training process, such as random resizing, stretching/compressing, rotation, and translation, various types of noise, random border crop, contrast stretching, and various image processing techniques, to simulate different types of imaging conditions and improve the model\u2019s ability to generalize to real-world scenarios. The UTRNet-Large model achieved convergence in approximately 7 hours. We utilize the standard character-wise accuracy metric for comparison, which uses the edit distance between the predicted output\n(Pred) and the ground truth (GT):\nAccuracy = \u2211 (length(GT )\u2212 EditDistance(Pred,GT ))\u2211\n(length(GT ))"
        },
        {
            "heading": "5.1 Results And Analysis",
            "text": "In order to evaluate the effectiveness of our proposed architecture, we conducted a series of experiments and compared our results with state-of-the-art (SOTA) models for Urdu OCR, as well as a few for Arabic, including both printed and handwritten ones (Table 3C). Additionally, we evaluated our model against the current SOTA baseline OCR models, primarily developed for Latin-based languages (Tables 3A and 3B). Our proposed model achieves superior performance, surpassing all of the SOTA OCR models in terms of character-wise accuracy on all three validation datasets, achieving a recognition accuracy of 92.97% on the UTRSet-Real validation set. It is worth noting that while Table 3A presents\na comparison of our proposed model against hybrid CNN-RNN models, Table 3B presents a comparison against recent transformer-based models. The results clearly show that transformer-based models perform poorly in comparison to our proposed model and even the SOTA CNN-RNN models for Latin OCR. This can be attributed to the fact that these models, which are designed to be trained on massive datasets when applied to the case of Urdu script recognition, overfit the small-size training data and struggle to generalize, thus resulting in poor validation accuracy.\nIn our analysis of our proposed UTRSet-Real and UTRSet-Synth datasets against the existing UPTI dataset, which is currently the only available training dataset for this purpose, we found that our proposed datasets effectively improve the performance of the UTRNet model. When trained on the UPTI dataset, both UTRNet-Small and UTRNet achieve high accuracy on the UPTI validation set but perform poorly on the UTRSet-Real and IIITH validation sets. This suggests that the UPTI dataset is not representative of real-world scenarios and does not adequately capture the complexity and diversity of printed Urdu text. Our proposed datasets, on the other hand, are specifically designed to address these issues and provide a more comprehensive and realistic representation of the task at hand, as both of them perform significantly well on all datasets, with UTRSet-Real being the best. Furthermore, the results show that combining all three training datasets can further improve the performance, especially on the IIITH and UPTI validation sets.\nOne of the key insights from our results is the significant difference in accuracy when comparing our proposed UTRNet model with the current state-ofthe-art (SOTA) model for printed Urdu OCR [30], as presented in Figure 2. This highlights the complexity of recognizing the intricate features of Urdu characters and the efficacy of our proposed UTRNet model in addressing these challenges. Our results align with our hypothesis that high-resolution multi-scale feature maps are essential for capturing the nuanced details required for accurate Urdu OCR. To further support this claim, we also present a visualization of feature maps generated from our CNN (as depicted in Figure 7), which\nclearly demonstrates the ability of UTRNet to effectively extract and preserve the high-resolution details of the input image. Additionally, we provide a qualitative analysis of the results by comparing our model with the SOTA [30] in Figure 9.\nWe also conducted a series of ablation studies to investigate the impact of various components of our proposed UTRNet model on performance. The results of this study, as shown in Table 2, indicate that each component of our model makes a significant contribution to the overall performance. Specifically, since we observed that incorporating a multi-scale high-resolution feature extraction significantly improves the result for Urdu OCR, we tried various other multiscale CNNs as a part of our ablation study. We also found that the use of generalisation techniques, such as temporal dropout and augmentation, further improved the robustness of our model, making it able to effectively handle a wide range of challenges which are commonly encountered in real-world Urdu OCR scenarios."
        },
        {
            "heading": "6 Web Tool for End-to-End Urdu OCR",
            "text": "We have developed an online website for Urdu OCR that integrates ContourNet [69] model, trained on the Ur-\nduDoc dataset with our proposed UTRNet model. This integration allows for end-toend text recognition in realworld documents, making the website a valuable tool for easy and efficient digitization of a large corpus of available Urdu literature."
        },
        {
            "heading": "7 Conclusion",
            "text": "In this work, we have addressed the limitations of previous works in Urdu OCR, which struggle to generalize to the intricacies of the Urdu script and the lack of large annotated real-world data. We have presented a novel approach through the introduction of a high-resolution, multi-scale semantic feature extractionbased model which outperforms previous SOTA models for Urdu OCR, as well as Latin OCR, by a significant margin. We have also introduced three comprehensive datasets: UTRSet-Real, UTRSet-Synth, and UrduDoc, which are significant contributions towards advancing research in printed Urdu text recognition. Additionally, the corrections made to the ground truth of the existing IIITH dataset have made it a more reliable resource for future research. Furthermore, we\u2019ve also developed a web based tool for end-to-end Urdu OCR which we hope will help in digitizing the large corpus of available Urdu literature. Despite the promising results of our proposed approach, there remains scope for further optimization and advancements in the field of Urdu OCR. A crucial area of focus is harnessing the power of transformer-based models along with large amounts of synthetic data by enhancing the robustness and realism of synthetic data and potentially achieving even greater performance gains. Our work has laid the foundation for continued progress in this field, and we hope it will inspire new and innovative approaches for printed Urdu text recognition."
        },
        {
            "heading": "8 Acknowledgement",
            "text": "We would like to express our gratitude to the Rekhta Foundation and Arjumand Ara for providing us with scanned images, as well as Noor Fatima and Mohammad Usman for their valuable annotations of the UTRSet-Real dataset. Furthermore, we acknowledge the support of a grant from IRD, IIT Delhi, and MEITY, Government of India, through the NLTM-Bhashini project."
        }
    ],
    "title": "UTRNet: High-Resolution Urdu Text Recognition In Printed Documents",
    "year": 2023
}