{
    "abstractText": "Given a graph dataset, how can we augment it for accurate graph classification? Graph augmentation is an essential strategy to improve the performance of graph-based tasks, and has been widely utilized for analyzing web and social graphs. However, previous works for graph augmentation either a) involve the target model in the process of augmentation, losing the generalizability to other tasks, or b) rely on simple heuristics that lead to unreliable results. In this work, we introduce five desired properties for effective augmentation. Then, we propose NodeSam (Node Split and Merge) and SubMix (Subgraph Mix), two model-agnostic algorithms for graph augmentation that satisfy all desired properties with different motivations. NodeSam makes a balanced change of the graph structure to minimize the risk of semantic change, while SubMix mixes random subgraphs of multiple graphs to create rich soft labels combining the evidence for different classes. Our experiments on social networks and molecular graphs show that NodeSam and SubMix outperform existing approaches in graph classification.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jaemin Yoo"
        },
        {
            "affiliations": [],
            "name": "Sooyeon Shim"
        },
        {
            "affiliations": [],
            "name": "U Kang"
        }
    ],
    "id": "SP:36e959e28f0168f85ecc722cfa118581deaf4557",
    "references": [
        {
            "authors": [
                "Filippo Maria Bianchi",
                "Daniele Grattarola",
                "Cesare Alippi"
            ],
            "title": "Spectral Clustering with Graph Neural Networks for Graph Pooling",
            "year": 2020
        },
        {
            "authors": [
                "Ronald V. Book"
            ],
            "title": "Comparing Complexity Classes",
            "venue": "J. Comput. Syst. Sci. 9,",
            "year": 1974
        },
        {
            "authors": [
                "Deli Chen",
                "Yankai Lin",
                "Wei Li",
                "Peng Li",
                "Jie Zhou",
                "Xu Sun"
            ],
            "title": "Measuring and Relieving the Over-Smoothing Problem for Graph Neural Networks from the Topological View",
            "year": 2020
        },
        {
            "authors": [
                "Terrance Devries",
                "Graham W. Taylor"
            ],
            "title": "Improved Regularization of Convolutional Neural Networks with Cutout",
            "year": 2017
        },
        {
            "authors": [
                "Boxin Du",
                "Hanghang Tong"
            ],
            "title": "MrMine: Multi-resolution Multi-network Embedding",
            "venue": "In CIKM",
            "year": 2019
        },
        {
            "authors": [
                "Fuli Feng",
                "Xiangnan He",
                "Jie Tang",
                "Tat-Seng Chua"
            ],
            "title": "Graph Adversarial Training: Dynamically Regularizing Based on Graph Structure",
            "venue": "CoRR abs/1902.08226",
            "year": 2019
        },
        {
            "authors": [
                "Steven Y. Feng",
                "Varun Gangal",
                "Jason Wei",
                "Sarath Chandar",
                "Soroush Vosoughi",
                "Teruko Mitamura",
                "Eduard H. Hovy"
            ],
            "title": "A Survey of Data Augmentation Approaches for NLP",
            "venue": "In Findings of ACL",
            "year": 2021
        },
        {
            "authors": [
                "Maayan Frid-Adar",
                "Idit Diamant",
                "Eyal Klang",
                "Michal Amitai",
                "Jacob Goldberger",
                "Hayit Greenspan"
            ],
            "title": "GAN-based synthetic medical image augmentation for increased CNN performance in liver lesion",
            "venue": "classification. Neurocomputing",
            "year": 2018
        },
        {
            "authors": [
                "Kun Fu",
                "Tingyun Mao",
                "Yang Wang",
                "Daoyu Lin",
                "Yuanben Zhang",
                "Junjian Zhan",
                "Xian Sun",
                "Feng Li"
            ],
            "title": "TS-Extractor: large graph exploration via subgraph extraction based on topological and semantic information",
            "venue": "J. Vis. 24,",
            "year": 2021
        },
        {
            "authors": [
                "Adrian Galdran",
                "Aitor Alvarez-Gila",
                "Maria In\u00eas Meyer",
                "Cristina L\u00f3pez Saratxaga",
                "Teresa Araujo",
                "Est\u00edbaliz Garrote",
                "Guilherme Aresta",
                "Pedro Costa",
                "Ana Maria Mendon\u00e7a",
                "Aur\u00e9lio J.C. Campilho"
            ],
            "title": "Data-Driven Color Augmentation Techniques for Deep Skin Image Analysis",
            "year": 2017
        },
        {
            "authors": [
                "Saehan Jo",
                "Jaemin Yoo",
                "U Kang"
            ],
            "title": "Fast and Scalable Distributed Loopy Belief Propagation on Real-World Graphs. In WSDM",
            "year": 2018
        },
        {
            "authors": [
                "Jang-Hyun Kim",
                "Wonho Choo",
                "Hyun Oh Song"
            ],
            "title": "Puzzle Mix: Exploiting Saliency and Local Statistics for Optimal Mixup",
            "year": 2020
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A Method for Stochastic Optimization",
            "venue": "In ICLR",
            "year": 2015
        },
        {
            "authors": [
                "Thomas N. Kipf",
                "Max Welling"
            ],
            "title": "Semi-Supervised Classification with Graph Convolutional Networks",
            "venue": "In ICLR",
            "year": 2017
        },
        {
            "authors": [
                "Johannes Klicpera",
                "Stefan Wei\u00dfenberger",
                "Stephan G\u00fcnnemann"
            ],
            "title": "Diffusion Improves Graph Learning",
            "venue": "In NeurIPS",
            "year": 2019
        },
        {
            "authors": [
                "Kezhi Kong",
                "Guohao Li",
                "Mucong Ding",
                "Zuxuan Wu",
                "Chen Zhu",
                "Bernard Ghanem",
                "Gavin Taylor",
                "Tom Goldstein"
            ],
            "title": "FLAG: Adversarial Data Augmentation for Graph Neural Networks",
            "year": 2020
        },
        {
            "authors": [
                "Jin-Ha Lee",
                "Muhammad Zaigham Zaheer",
                "Marcella Astrid",
                "Seung-Ik Lee"
            ],
            "title": "SmoothMix: a Simple Yet Effective Data Augmentation to Train Robust Classifiers",
            "year": 2020
        },
        {
            "authors": [
                "Jiaqi Ma",
                "Shuangrui Ding",
                "Qiaozhu Mei"
            ],
            "title": "Towards More Practical Adversarial Attacks on Graph Neural Networks",
            "venue": "NeurIPS",
            "year": 2020
        },
        {
            "authors": [
                "Yao Ma",
                "Suhang Wang",
                "Charu C. Aggarwal",
                "Jiliang Tang"
            ],
            "title": "Graph Convolutional Networks with EigenPooling",
            "year": 2019
        },
        {
            "authors": [
                "Seyed-Mohsen Moosavi-Dezfooli",
                "Alhussein Fawzi",
                "Pascal Frossard"
            ],
            "title": "DeepFool: A Simple and Accurate Method to Fool Deep Neural Networks",
            "year": 2016
        },
        {
            "authors": [
                "Christopher Morris",
                "Nils M. Kriege",
                "Franka Bause",
                "Kristian Kersting",
                "Petra Mutzel",
                "Marion Neumann"
            ],
            "title": "TUDataset: A collection of benchmark datasets for learning with graphs",
            "year": 2020
        },
        {
            "authors": [
                "Annamalai Narayanan",
                "Mahinthan Chandramohan",
                "Lihui Chen",
                "Yang Liu",
                "Santhoshkumar Saminathan"
            ],
            "title": "subgraph2vec: Learning Distributed Representations of Rooted Sub-graphs from Large Graphs",
            "year": 2016
        },
        {
            "authors": [
                "Shirui Pan",
                "Jia Wu",
                "Xingquan Zhu"
            ],
            "title": "CogBoost: Boosting for Fast Cost- Sensitive Graph Classification",
            "venue": "IEEE Trans. Knowl. Data Eng. 27,",
            "year": 2015
        },
        {
            "authors": [
                "Bastian Rieck",
                "Christian Bock",
                "Karsten M. Borgwardt"
            ],
            "title": "A Persistent Weisfeiler-Lehman Procedure for Graph Classification",
            "year": 2019
        },
        {
            "authors": [
                "Ignacio Rocco",
                "Relja Arandjelovic",
                "Josef Sivic"
            ],
            "title": "Convolutional Neural Network Architecture for Geometric Matching",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell. 41,",
            "year": 2019
        },
        {
            "authors": [
                "Yu Rong",
                "Wenbing Huang",
                "Tingyang Xu",
                "Junzhou Huang"
            ],
            "title": "DropEdge: Towards Deep Graph Convolutional Networks on Node Classification",
            "venue": "In ICLR",
            "year": 2020
        },
        {
            "authors": [
                "Connor Shorten",
                "Taghi M. Khoshgoftaar"
            ],
            "title": "A survey on Image Data Augmentation for Deep Learning",
            "venue": "J. Big Data",
            "year": 2019
        },
        {
            "authors": [
                "Krishna Kumar Singh",
                "Yong Jae Lee"
            ],
            "title": "Hide-and-Seek: Forcing a Network to be Meticulous for Weakly-Supervised Object and Action Localization",
            "year": 2017
        },
        {
            "authors": [
                "Ryo Takahashi",
                "Takashi Matsubara",
                "Kuniaki Uehara"
            ],
            "title": "Data Augmentation Using Random Image Cropping and Patching for Deep CNNs",
            "venue": "IEEE Trans. Circuits Syst. Video Technol. 30,",
            "year": 2020
        },
        {
            "authors": [
                "Laurens Van der Maaten",
                "Geoffrey Hinton"
            ],
            "title": "Visualizing data using t-SNE",
            "venue": "Journal of machine learning research 9,",
            "year": 2008
        },
        {
            "authors": [
                "Vikas Verma",
                "Meng Qu",
                "Alex Lamb",
                "Yoshua Bengio",
                "Juho Kannala",
                "Jian Tang"
            ],
            "title": "GraphMix: Regularized Training of Graph Neural Networks for Semi-Supervised Learning",
            "year": 2019
        },
        {
            "authors": [
                "Yiwei Wang",
                "Wei Wang",
                "Yuxuan Liang",
                "Yujun Cai",
                "Bryan Hooi"
            ],
            "title": "GraphCrop: Subgraph Cropping for Graph Classification",
            "year": 2020
        },
        {
            "authors": [
                "Yiwei Wang",
                "Wei Wang",
                "Yuxuan Liang",
                "Yujun Cai",
                "Bryan Hooi"
            ],
            "title": "2021. Cur- Graph: Curriculum Learning for Graph Classification",
            "year": 2021
        },
        {
            "authors": [
                "Yiwei Wang",
                "Wei Wang",
                "Yuxuan Liang",
                "Yujun Cai",
                "Bryan Hooi"
            ],
            "title": "Mixup for Node and Graph Classification",
            "year": 2021
        },
        {
            "authors": [
                "Yiwei Wang",
                "Wei Wang",
                "Yuxuan Liang",
                "Yujun Cai",
                "Juncheng Liu",
                "Bryan Hooi"
            ],
            "title": "NodeAug: Semi-Supervised Node Classification with Data Augmentation",
            "year": 2020
        },
        {
            "authors": [
                "QingsongWen",
                "Liang Sun",
                "Fan Yang",
                "Xiaomin Song",
                "Jingkun Gao",
                "XueWang",
                "Huan Xu"
            ],
            "title": "Time Series Data Augmentation for Deep Learning: A Survey",
            "venue": "In IJCAI",
            "year": 2021
        },
        {
            "authors": [
                "Kaidi Xu",
                "Hongge Chen",
                "Sijia Liu",
                "Pin-Yu Chen",
                "Tsui-Wei Weng",
                "Mingyi Hong",
                "Xue Lin"
            ],
            "title": "Topology Attack and Defense for Graph Neural Networks: An Optimization Perspective",
            "venue": "In IJCAI",
            "year": 2019
        },
        {
            "authors": [
                "Keyulu Xu",
                "Weihua Hu",
                "Jure Leskovec",
                "Stefanie Jegelka"
            ],
            "title": "How Powerful are Graph Neural Networks",
            "venue": "In ICLR",
            "year": 2019
        },
        {
            "authors": [
                "Pinar Yanardag",
                "S.V.N. Vishwanathan"
            ],
            "title": "Deep Graph Kernels",
            "venue": "In KDD",
            "year": 2015
        },
        {
            "authors": [
                "Rex Ying",
                "Ruining He",
                "Kaifeng Chen",
                "Pong Eksombatchai",
                "William L. Hamilton",
                "Jure Leskovec"
            ],
            "title": "Graph Convolutional Neural Networks for Web-Scale Recommender Systems",
            "year": 2018
        },
        {
            "authors": [
                "Zhitao Ying",
                "Jiaxuan You",
                "Christopher Morris",
                "Xiang Ren",
                "William L. Hamilton",
                "Jure Leskovec"
            ],
            "title": "Hierarchical Graph Representation Learning with Differentiable Pooling",
            "venue": "NeurIPS",
            "year": 2018
        },
        {
            "authors": [
                "Jaemin Yoo",
                "Hyunsik Jeon",
                "U Kang"
            ],
            "title": "Belief Propagation Network for Hard Inductive Semi-Supervised Learning",
            "year": 2019
        },
        {
            "authors": [
                "Jaemin Yoo",
                "U Kang",
                "Mauro Scanagatta",
                "Giorgio Corani",
                "Marco Zaffalon"
            ],
            "title": "Sampling Subgraphs with Guaranteed Treewidth for Accurate and Efficient Graphical Inference",
            "venue": "In WSDM",
            "year": 2020
        },
        {
            "authors": [
                "Jaemin Yoo",
                "Junghun Kim",
                "Hoyoung Yoon",
                "Geonsoo Kim",
                "Changwon Jang",
                "U Kang"
            ],
            "title": "Accurate Graph-Based PU Learning without Class Prior. In ICDM",
            "year": 2021
        },
        {
            "authors": [
                "Yuning You",
                "Tianlong Chen",
                "Yang Shen",
                "Zhangyang Wang"
            ],
            "title": "Graph Contrastive Learning Automated",
            "year": 2021
        },
        {
            "authors": [
                "Yuning You",
                "Tianlong Chen",
                "Yongduo Sui",
                "Ting Chen",
                "Zhangyang Wang",
                "Yang Shen"
            ],
            "title": "Graph Contrastive Learning with Augmentations",
            "venue": "NeurIPS",
            "year": 2020
        },
        {
            "authors": [
                "Sangdoo Yun",
                "Dongyoon Han",
                "Sanghyuk Chun",
                "Seong Joon Oh",
                "Youngjoon Yoo",
                "Junsuk Choe"
            ],
            "title": "CutMix: Regularization Strategy to Train Strong Classifiers With Localizable Features",
            "year": 2019
        },
        {
            "authors": [
                "Hanqing Zeng",
                "Hongkuan Zhou",
                "Ajitesh Srivastava",
                "Rajgopal Kannan",
                "Viktor K. Prasanna"
            ],
            "title": "GraphSAINT: Graph Sampling Based Inductive Learning Method",
            "venue": "In ICLR",
            "year": 2020
        },
        {
            "authors": [
                "Hongyi Zhang",
                "Moustapha Ciss\u00e9",
                "Yann N. Dauphin",
                "David Lopez-Paz"
            ],
            "title": "mixup: Beyond Empirical Risk Minimization",
            "venue": "In ICLR",
            "year": 2018
        },
        {
            "authors": [
                "Tong Zhao",
                "Yozen Liu",
                "Leonardo Neves",
                "Oliver J. Woodford",
                "Meng Jiang",
                "Neil Shah"
            ],
            "title": "Data Augmentation for Graph Neural Networks",
            "year": 2020
        },
        {
            "authors": [
                "Zhun Zhong",
                "Liang Zheng",
                "Guoliang Kang",
                "Shaozi Li",
                "Yi Yang"
            ],
            "title": "Random Erasing Data Augmentation",
            "year": 2020
        },
        {
            "authors": [
                "Jiajun Zhou",
                "Jie Shen",
                "Qi Xuan"
            ],
            "title": "Data Augmentation for Graph Classification",
            "venue": "In CIKM",
            "year": 2020
        },
        {
            "authors": [
                "Xinyue Zhu",
                "Yifan Liu",
                "Jiahong Li",
                "Tao Wan",
                "Zengchang Qin"
            ],
            "title": "Emotion Classification with Data Augmentation Using Generative Adversarial Networks. In PAKDD",
            "year": 2018
        }
    ],
    "sections": [
        {
            "heading": "CCS CONCEPTS",
            "text": "\u2022 Computing methodologies\u2192 Supervised learning by classification; \u2022 Information systems\u2192 Social networks."
        },
        {
            "heading": "KEYWORDS",
            "text": "graph classification, data augmentation, model-agnostic methods"
        },
        {
            "heading": "ACM Reference Format:",
            "text": "Jaemin Yoo, Sooyeon Shim, and U Kang. 2022. Model-Agnostic Augmentation for Accurate Graph Classification. In Proceedings of the ACM Web Conference 2022 (WWW \u201922), April 25\u201329, 2022, Virtual Event, Lyon, France. ACM,NewYork, NY, USA, 11 pages. https://doi.org/10.1145/3485447.3512175"
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "How can we augment graphs for accurate graph classification? Data augmentation is an essential strategy to maximize the performance of estimators by enlarging the distribution covered by training data. The technique has been used widely in various data domains such as images [30], time series [39], and language processing [8]. The problem of graph augmentation has also attracted wide attention in the web domain [35, 36], where a community structure works as an essential evidence for classifying graph labels. An augmentation\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. WWW \u201922, April 25\u201329, 2022, Virtual Event, Lyon, France \u00a9 2022 Association for Computing Machinery. ACM ISBN 978-1-4503-9096-5/22/04. . . $15.00 https://doi.org/10.1145/3485447.3512175\n...\nModel-agnostic graph augmentation\nBaseline Rich decision boundary learned by\nFigure 1: Illustration of how model-agnostic augmentation works. Each circle at the bottom represents a graph, whose label is represented as color. The augmented graphs allow a classifier \ud835\udc53 to learn a rich decision boundary.\nmethod provides rich variants of community structures, allowing one to understand the complex relationships between users.\nPrevious approaches on graph augmentation are categorized to model-specific [34, 53] and model-agnostic ones [35, 38, 55]. Modelspecific approaches often make a better performance than modelagnostic ones, because they are designed for specific target models. However, their performance is not generalized to other settings of models and problems, and a careful tuning of hyperparameters is required even with a small change of experimental setups. On the other hand, model-agnostic approaches work generally well with various models and problems, even though their best performance can be worse than that of model-specific ones. Figure 1 illustrates how model-agnostic augmentation works for a graph classifier \ud835\udc53 that classifies each graph into the red or the blue class.\nHowever, previous works on model-agnostic augmentation [35, 38, 55] rely on simple heuristics such as removing random edges or changing node attributes rather than carefully designed operations. Such heuristics provide no theoretical guarantee for essential properties of data augmentation such as the unbiasedness or linear scalability. As a result, previous approaches often make unreliable results, losing the main advantage over model-specific approaches. Moreover, our experiments on benchmark datasets show that they often decrease the accuracy of target models in graph classification, where the structural characteristic of each graph plays an essential role for predicting its label (details are in Section 4).\nIn this work, we first propose five properties that an augmentation algorithm should satisfy to maximize its effectiveness. These properties are designed carefully to include the degree of augmentation, the preservation of graph size, and the scalability to large\nar X\niv :2\n20 2.\n10 10\n7v 1\n[ cs\n.L G\n] 2\n1 Fe\nb 20\n22\ngraphs. We then propose two novel algorithms, NodeSam and SubMix, which satisfy all these properties. NodeSam (Node Split and Merge) performs split and merge operations on nodes, minimizing the degree of structural change while augmenting both the nodeand edge-level information. SubMix (Subgraph Mix) combines multiple graphs by swapping random subgraphs tomaximize the degree of augmentation, generating rich soft labels for classification.\nOur contributions are summarized as follows:\n\u2022 Objective formulation.Wepropose desired properties that are essential for effective graph augmentation, providing a clear objective for augmentation algorithms. \u2022 Algorithms. We propose NodeSam and SubMix, effective model-agnostic algorithms for graph augmentation. NodeSam is a stable and balanced approach that makes a minimal change of the graph structure, while SubMix generates more diverse samples through abundant augmentation. \u2022 Theory. We theoretically analyze the characteristics of our proposed approaches and demonstrate that they satisfy all the desired properties even in the worst cases. \u2022 Experiments. We perform experiments on nine datasets to show the effectiveness of our methods for improving graph classifiers. Our methods make up to 2.1\u00d7 larger improvement of accuracy compared with the best competitors.\nThe rest of this paper is organized as follows. In Section 2, we define the problem of graph augmentation and present the desired properties. In Section 3, we propose our NodeSam and SubMix and discuss their theoretical properties. We show experimental results in Section 4 and introduce related works in Section 5. We conclude in Section 6. All of our implementation and datasets are available at https://github.com/snudatalab/GraphAug.git."
        },
        {
            "heading": "2 PROBLEM AND DESIRED PROPERTIES",
            "text": "We formally define the graph augmentation problem and present desired properties for an effective augmentation algorithm. Table 1 compares various methods for graph augmentation regarding the desired properties that we propose in this section."
        },
        {
            "heading": "2.1 Problem Definition",
            "text": "Given a set of graphs, graph augmentation is to generate a new set of graphs that have similar characteristics to the given graphs. We give the formal definition as Problem 1.\nProblem 1 (Graph augmentation). We have a set G of graphs. Each graph\ud835\udc3a \u2208 G consists of a setV of nodes, a set E of edges, and a feature matrix X \u2208 R |V |\u00d7\ud835\udc51 , where \ud835\udc51 is the number of features. Then, the problem is to make a set G\u0304 of new graphs that are more suitable than G for the training of a model \ud835\udc53 , improving its performance.\nAlthough any task can benefit from graph augmentation, we use graph classification as the target task to solve by a classifier \ud835\udc53 . This is because graph classification is more sensitive to the quality of augmentation than node-level tasks are, such as node classification or link prediction. In graph classification, naive augmentation can easily decrease the accuracy of \ud835\udc53 if it changes the characteristic of a graph that is essential for its classification (details in Section 4). On the other hand, in node-level tasks such as node classification, even a simple heuristic algorithm can improve the accuracy of models by changing the local neighborhood of each target node [28, 38]. Thus, the accuracy of graph classification is a suitable measure for comparing different approaches for graph augmentation."
        },
        {
            "heading": "2.2 Desired Properties",
            "text": "Our goal is to generate a set of augmented graphs that maximize the performance of a graph classifier \ud835\udc53 as presented in Problem 1. The main difficulty of augmentation is that the semantic information of a graph, which means the unique characteristic that determines its label, is not given clearly. For example, in the classification task of molecular graphs, it is difficult even for domain experts to check whether an augmented graph has the same chemical property as in the original graph. This makes it difficult for an augmentation algorithm to safely enlarge the data distribution.\nWe propose five desired properties for an effective augmentation algorithm to maximize the degree of augmentation while minimizing the risk of changing semantic information. Property 1 and 2 are for preserving the basic structural information of a graph in terms of the size and connectivity, respectively.\nProperty 1 (Preserving size). Given a graph \ud835\udc3a = (V, E,X) and an augmentation function \u210e, let\ud835\udc3a = \u210e(\ud835\udc3a). Then, \u210e should make an unbiased change of the graph size by satisfying E[|V\u0304 | \u2212 |V|] = 0 and E[|E\u0304 | \u2212 |E |] = 0, where \ud835\udc3a = (V\u0304, E\u0304, X\u0304).\nProperty 2 (Preserving connectivity). Given a graph \ud835\udc3a = (V, E,X) and an augmentation function \u210e, let \ud835\udc3a = \u210e(\ud835\udc3a). Then, \ud835\udc3a should follow the connectivity information of \ud835\udc3a . In other words, \ud835\udc3a should be connected if and only if \ud835\udc3a is connected.\nAt the same time, it is necessary for an augmentation algorithm to make meaningful changes to the given graph; Property 1 and 2 are satisfied even with the identity function. In this regard, we introduce Property 3 and 4 that force an augmentation algorithm to make node- and edge-level changes at the same time.\nProperty 3 (Changing nodes). Given a graph \ud835\udc3a = (V, E,X) and an augmentation function \u210e, let\ud835\udc3a = \u210e(\ud835\udc3a). Then, \u210e should make a change of nodes inV by satisfying either E[( |V\u0304 | \u2212 |V|)2] > 0 or E[\u2225X\u0304 \u2212 X\u22252F] > 0, where \ud835\udc3a = (V\u0304, E\u0304, X\u0304), and \u2225 \u00b7 \u2225F is the Frobenius norm of a matrix.\nProperty 4 (Changing edges). Given a graph \ud835\udc3a = (V, E,X) and an augmentation function \u210e, let\ud835\udc3a = \u210e(\ud835\udc3a). Then, \u210e should make a change of |E |, i.e., E[( |E\u0304 | \u2212 |E |)2] > 0, where \ud835\udc3a = (V\u0304, E\u0304, X\u0304).\nAlgorithm 1 NodeSam (Node Split and Merge)\nInput: Target graph \ud835\udc3a = (V, E,X) Output: Augmented graph \ud835\udc3a = (V\u0304, E\u0304, X\u0304) 1: \ud835\udc3a \u2032, \ud835\udc63\ud835\udc56 , \ud835\udc63 \ud835\udc57 , \ud835\udc63\ud835\udc58 \u2190 Split(\ud835\udc3a) \u22b2 Algorithm 2 2: \ud835\udc3a \u2032\u2032 \u2190 Adjust(\ud835\udc3a,\ud835\udc3a \u2032, \ud835\udc63\ud835\udc56 , \ud835\udc63 \ud835\udc57 , \ud835\udc63\ud835\udc58 ) \u22b2 Algorithm 4 3: \ud835\udc3a \u2190 Merge(\ud835\udc3a \u2032\u2032) \u22b2 Algorithm 3\nLastly, we require the augmentation to be done in linear time with the graph size to support scalability in large graphs, which is essential for real-world applications [12, 43].\nProperty 5 (Linear complexity). Given a graph\ud835\udc3a = (V, E,X) and an augmentation function \u210e, let \ud835\udc3a = \u210e(\ud835\udc3a). Then, the time and space complexities of \u210e for generating \ud835\udc3a should be \ud835\udc42 (\ud835\udc51 |V| + |E|), where \ud835\udc51 is the number of features, i.e., X \u2208 R |V |\u00d7\ud835\udc51 .\nWe briefly review the previous approaches in Table 1 in terms of the desired properties. DropEdge [28] and GraphCrop [35] make a subgraph of the given graph as a result. This makes a high risk of semantic change, as we have no clue for the essential part that determines the characteristic of the graph. NodeAug [38] also changes the graph properties by adding and removing edges near a random node. MotifSwap [55] preserves the properties of the given graph with respect to both nodes and edges, but fails to make a sufficient amount of change. Moreover, MotifSwap is not scalable to large graphs, as its running time is not linear with the number of edges due to the global enumeration to find all open triangles."
        },
        {
            "heading": "3 PROPOSED METHODS",
            "text": "In this work, we propose two effective algorithms for graph augmentation, which satisfy all the desired properties in Table 1. NodeSam (Node Split and Merge) performs opposite split and merge operations over nodes to make a balanced change of graph properties, while SubMix (Subgraph Mix) combines multiple graphs by mixing random subgraphs to enlarge the space of augmentation."
        },
        {
            "heading": "3.1 NodeSam",
            "text": "The main idea of NodeSam is to perform opposite operations at once to make a balanced change of the graph properties. NodeSam first splits a random node into a pair of adjacent nodes, increasing the number of nodes by one. NodeSam then merges a random pair of nodes into a single node, making the generated graph have the same number of nodes as in the original graph. The combination of these opposite operations allows it to change the node- and edgelevel information at the same time while preserving the structure of the original graph such as the connectivity.\nAlgorithm 1 describes the process of NodeSam, where the split and merge operations are done at lines 1 and 3, respectively. The adjustment operation in line 2 is introduced to make an unbiased change of the number of edges by inserting additional edges to the graph. We first discuss the split and merge operations in detail and then present the adjustment operation in Section 3.1.1.\nGiven a graph\ud835\udc3a = (V, E,X), NodeSam performs the split and merge operations following Algorithms 2 and 3, respectively. The split operation selects a random node \ud835\udc63\ud835\udc56 and splits it into nodes \ud835\udc63 \ud835\udc57 and \ud835\udc63\ud835\udc58 , making an edge (\ud835\udc63 \ud835\udc57 , \ud835\udc63\ud835\udc58 ) and copying its feature as x\ud835\udc56 = x\ud835\udc57 = x\ud835\udc58 . The edges attached to \ud835\udc63\ud835\udc56 are split into \ud835\udc63 \ud835\udc57 and \ud835\udc63\ud835\udc58 following the\nAlgorithm 2 Split in NodeSam\nInput: Target graph \ud835\udc3a = (V, E,X) Output: Intermediate graph\ud835\udc3a \u2032 = (V \u2032, E \u2032,X\u2032), target node \ud835\udc63\ud835\udc56 , and\ngenerated nodes \ud835\udc63 \ud835\udc57 and \ud835\udc63\ud835\udc58 1: \ud835\udc63\ud835\udc56 \u2190 Select a node fromV uniformly at random 2: \ud835\udc63 \ud835\udc57 , \ud835\udc63\ud835\udc58 \u2190Make new nodes to insert to \ud835\udc3a 3: x\ud835\udc57 , x\ud835\udc58 \u2190Make new features such that x\ud835\udc56 = x\ud835\udc57 = x\ud835\udc58 4: \u210e \u2190Make a function that randomly returns \ud835\udc63 \ud835\udc57 or \ud835\udc63\ud835\udc58 5: V \u2032 \u2190 (V \\ {\ud835\udc63\ud835\udc56 }) \u222a {\ud835\udc63 \ud835\udc57 , \ud835\udc63\ud835\udc58 } 6: E \u2032 \u2190 {(\ud835\udc4e, \ud835\udc4f) | (\ud835\udc4e, \ud835\udc4f) \u2208 E \u2227 \ud835\udc4e \u2260 \ud835\udc63\ud835\udc56 \u2227 \ud835\udc4f \u2260 \ud835\udc63\ud835\udc56 } \u222a {(\ud835\udc63 \ud835\udc57 , \ud835\udc63\ud835\udc58 )} 7: E \u2032 \u2190 E \u2032 \u222a {(\u210e(\ud835\udc4e), \ud835\udc4f) | (\ud835\udc4e, \ud835\udc4f) \u2208 E \u2227 \ud835\udc4e = \ud835\udc63\ud835\udc56 } 8: E \u2032 \u2190 E \u2032 \u222a {(\ud835\udc4e, \u210e(\ud835\udc4f)) | (\ud835\udc4e, \ud835\udc4f) \u2208 E \u2227 \ud835\udc4f = \ud835\udc63\ud835\udc56 } 9: X\u2032 \u2190 Remove x\ud835\udc56 from X, and add x\ud835\udc57 and x\ud835\udc58 to it\nAlgorithm 3 Merge in NodeSam\nInput: Graph \ud835\udc3a \u2032\u2032 = (V \u2032\u2032, E \u2032\u2032,X\u2032\u2032) generated from Adjust Output: Augmented graph \ud835\udc3a = (V\u0304, E\u0304, X\u0304) 1: \ud835\udc63\ud835\udc5c , \ud835\udc63\ud835\udc5d \u2190 Select adjacent nodes fromV \u2032\u2032 uniformly at random 2: \ud835\udc63\ud835\udc5e \u2190Make a new node to insert to \ud835\udc3a \u2032\u2032 3: x\ud835\udc5e \u2190Make a new feature such that x\ud835\udc5e = (x\ud835\udc5c + x\ud835\udc5d )/2 4: V\u0304 \u2190 (V \u2032\u2032 \\ {\ud835\udc63\ud835\udc5c , \ud835\udc63\ud835\udc5d }) \u222a {\ud835\udc63\ud835\udc5e} 5: E\u0304 \u2190 Remove all edges from E \u2032\u2032 connected to either \ud835\udc63\ud835\udc5c or \ud835\udc63\ud835\udc5d 6: E\u0304 \u2190 E\u0304 \u222a {(\ud835\udc63\ud835\udc5e, \ud835\udc4f) | (\ud835\udc4e, \ud835\udc4f) \u2208 E \u2032\u2032 \u2227 \ud835\udc4e \u2208 {\ud835\udc63\ud835\udc5c , \ud835\udc63\ud835\udc5d }} 7: E\u0304 \u2190 E\u0304 \u222a {(\ud835\udc4e, \ud835\udc63\ud835\udc5e) | (\ud835\udc4e, \ud835\udc4f) \u2208 E \u2032\u2032 \u2227 \ud835\udc4f \u2208 {\ud835\udc63\ud835\udc5c , \ud835\udc63\ud835\udc5d }} 8: X\u0304\u2190 Remove x\ud835\udc5c and x\ud835\udc5d from X\u2032\u2032, and add x\ud835\udc5e to it\nbinomial distribution B(|N\ud835\udc56 |, 0.5), whereN\ud835\udc56 is the set of neighbors of \ud835\udc63\ud835\udc56 . The merge operation selects a random pair of adjacent nodes \ud835\udc63\ud835\udc5c and \ud835\udc63\ud835\udc5d and merges them into a new single node \ud835\udc63\ud835\udc5e with a feature vector x\ud835\udc5e = (x\ud835\udc5c + x\ud835\udc5d )/2. The edges connected to either \ud835\udc63\ud835\udc5c or \ud835\udc63\ud835\udc5d are connected to \ud835\udc63\ud835\udc5e , while the edge (\ud835\udc63\ud835\udc5c , \ud835\udc63\ud835\udc5d ) is removed.\n3.1.1 Adjustment Operation. The basic version of NodeSam with only the split and merge operations have two limitations. First, the split operation weakens the relationships between the nodes in N\ud835\udc56 . That is, the number of common neighbors between any two nodes in N\ud835\udc56 is likely to decrease in the graph \ud835\udc3a \u2032 generated from the split. This happens if \ud835\udc63\ud835\udc56 forms triangles with its neighbors, since the split eliminates these triangles by changing them into loops of length four. Second, the number of edges tends to decrease in augmented graphs, since the merge operation can remove more than one edge. This happens if there are triangles containing both of the target nodes \ud835\udc63\ud835\udc5c and \ud835\udc63\ud835\udc5d , since the other two edges except (\ud835\udc63\ud835\udc5c , \ud835\udc63\ud835\udc5d ) in each triangle are combined into a single one.\nTo address the two limitations, we propose an adjustment operation of Algorithm 4 that inserts additional edges to nodes \ud835\udc63 \ud835\udc57 and \ud835\udc63\ud835\udc58 , which are generated from the split. First, we randomly select a subset S of nodes from T\ud835\udc56 , which is the set of all nodes that form triangles with \ud835\udc63\ud835\udc56 in the original graph\ud835\udc3a . Then, we add |S| edges to the graph by the following process: for each node \ud835\udc62 \u2208 S, we insert edge (\ud835\udc62, \ud835\udc60) to the graph, where \ud835\udc60 is \ud835\udc63 \ud835\udc57 (or \ud835\udc63\ud835\udc58 ) if \ud835\udc62 is connected with \ud835\udc63\ud835\udc58 (or \ud835\udc63 \ud835\udc57 ). Note that all nodes in S have an edge with either \ud835\udc63 \ud835\udc57 or \ud835\udc63\ud835\udc58 before the adjustment since they are neighbors of \ud835\udc63\ud835\udc56 in \ud835\udc3a .\nFigure 2 illustrates how NodeSam works in an example graph of seven nodes. NodeSam first splits a random node \ud835\udc63\ud835\udc56 into a pair of\nAlgorithm 4 Adjust in NodeSam\nInput: Original graph \ud835\udc3a = (V, E,X), graph \ud835\udc3a \u2032 = (V \u2032, E \u2032,X\u2032) generated from Split, target node \ud835\udc63\ud835\udc56 \u2208 V of Split, and nodes \ud835\udc63 \ud835\udc57 \u2208 V \u2032 and \ud835\udc63\ud835\udc58 \u2208 V \u2032 generated from Split Output: Intermediate graph \ud835\udc3a \u2032\u2032 = (V \u2032\u2032, E \u2032\u2032,X\u2032\u2032) 1: \ud835\udc61\ud835\udc56 \u2190 Count the number of triangles in \ud835\udc3a containing \ud835\udc63\ud835\udc56 2: \ud835\udc51\ud835\udc56 \u2190 Get the degree of \ud835\udc63\ud835\udc56 in \ud835\udc3a 3: \ud835\udc50\ud835\udc56 \u2190 |E| \u2212 3\ud835\udc61\ud835\udc56/\ud835\udc51\ud835\udc56 \u2212 2 4: \u210e\ud835\udc56 \u2190 ((\ud835\udc502\ud835\udc56 + 4\ud835\udc61\ud835\udc56 |V| \u2212 6\ud835\udc61\ud835\udc56 )\n1/2 \u2212 \ud835\udc50\ud835\udc56 )/2 5: \ud835\udc4f \u2190Make a function that returns a random value in [0, 1) 6: T\ud835\udc56 \u2190 Take all nodes included in the triangles containing \ud835\udc63\ud835\udc56 7: S \u2190 {\ud835\udc62 | (\ud835\udc62 \u2208 T\ud835\udc56 \\ {\ud835\udc63\ud835\udc56 }) \u2227 (\ud835\udc4f (\ud835\udc62) < \u210e\ud835\udc56/(|T\ud835\udc56 | \u2212 1))} 8: V \u2032\u2032,X\u2032\u2032 \u2190V \u2032,X\u2032 9: E \u2032\u2032 \u2190 E \u2032 \u222a {(\ud835\udc62, \ud835\udc63 \ud835\udc57 ) | \ud835\udc62 \u2208 S} \u222a {(\ud835\udc62, \ud835\udc63\ud835\udc58 ) | \ud835\udc62 \u2208 S}\nnodes \ud835\udc63 \ud835\udc57 and \ud835\udc63\ud835\udc58 , decreasing the number of triangles from six to four. Then, the adjustment operation selects a random subset S = {\ud835\udc62} of nodes from T\ud835\udc56 , which isV \\ {\ud835\udc63\ud835\udc56 } in this example, and connects \ud835\udc62 with \ud835\udc63 \ud835\udc57 . Lastly, the merge operation combines a random pair of nodes \ud835\udc63\ud835\udc5c and \ud835\udc63\ud835\udc5d into node \ud835\udc63\ud835\udc5e . Note that the numbers of edges and triangles of\ud835\udc3a are preserved in the augmented graph\ud835\udc3a even with a different structure due to edge (\ud835\udc62, \ud835\udc63 \ud835\udc57 ) made by the adjustment.\nThe size of S is determined randomly in line 7 of Algorithm 4 following a binomial distribution whose mean is given as E[|S|] = \u210e\ud835\udc56 , where \u210e\ud835\udc56 is a number chosen to estimate the number of edges removed by the merge operation as follows:\n\u210e\ud835\udc56 = 1 2 ((\ud835\udc502\ud835\udc56 + 4\ud835\udc61\ud835\udc56 |V| \u2212 6\ud835\udc61\ud835\udc56 ) 1/2 \u2212 \ud835\udc50\ud835\udc56 ), (1)\nwhere \ud835\udc61\ud835\udc56 is the number of triangles containing \ud835\udc63\ud835\udc56 in\ud835\udc3a , \ud835\udc51\ud835\udc56 is the node degree of \ud835\udc63\ud835\udc56 in \ud835\udc3a , and \ud835\udc50\ud835\udc56 = |E | \u2212 3\ud835\udc61\ud835\udc56/\ud835\udc51\ud835\udc56 \u2212 2 (see Lemma 1).\nLocal estimation. All variables that compose \u210e\ud835\udc56 in Equation (1) are computed from the direct neighborhood of \ud835\udc63\ud835\udc56 , except for |V| and |E | that are known in advance. This allows NodeSam to be run in linear time with the number of edges, supporting scalability to large real-world graphs. At the same time, since the value of \u210e\ud835\udc56 is positively correlated with the number \ud835\udc61\ud835\udc56 of the triangles containing \ud835\udc63\ud835\udc56 , the adjustment effectively compensates for the triangles that are removed during the split; more triangles are likely to be removed with large \ud835\udc61\ud835\udc56 , but it tends to insert more edges in the adjustment. As a result, we address the two limitations of the naive version of NodeSam with a carefully chosen value of \u210e\ud835\udc56 .\n3.1.2 Satisfying Desired Properties. NodeSam satisfies all the desired properties in Table 1. It is straightforward that Property 3 and 4 are satisfied since NodeSam changes the node features and the\nset of edges at every augmentation. We show in Lemma 1, 2, and 3 that NodeSam also satisfies the rest of the properties.\nLemma 1. Given a graph\ud835\udc3a = (V, E,X), let\ud835\udc3a = (V\u0304, E\u0304, X\u0304) be the result of \ud835\udc41\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc46\ud835\udc4e\ud835\udc5a. Then, E[|V\u0304 | \u2212 |V|] = 0 and E[|E\u0304 | \u2212 |E |] = 0.\nProof. The proof is straightforward for V , since the number of nodes does not change by NodeSam. The proof for E requires a series of estimations for the properties of intermediate graphs, and thus the full proof is given in Appendix A. The idea is that the expected number of edges removed by the merge operation is the same as \u210e\ud835\udc56 , which is the expected number of edges added by the adjustment operation as shown in line 4 of Algorithm 4. \u25a1\nLemma 2. Given a graph \ud835\udc3a = (V, E,X), let \ud835\udc3a = (V\u0304, E\u0304,X) be a graph generated by NodeSam. Then, \ud835\udc3a is connected if and only if \ud835\udc3a is connected.\nProof. The proof is in Appendix B.1. \u25a1 Lemma 3. Given a graph \ud835\udc3a = (V, E,X), the time and space complexities of NodeSam are both \ud835\udc42 (\ud835\udc51 |V| + |E|), where \ud835\udc51 is the number of features.\nProof. The proof is in Appendix B.2. \u25a1"
        },
        {
            "heading": "3.2 SubMix",
            "text": "SubMix aims to make a large degree of augmentation by combining multiple graphs. This is done by swapping subgraphs of different graphs, motivated by mixing approaches in the image domain [14, 50, 52]. The main idea is to treat the adjacency matrix of each graph like an image and replace a random patch of the matrix with that of another graph, as depicted in Figure 3 that compares our SubMix to CutMix [50] in the image domain. The red subgraph corresponds to the head of the fox in the augmented image.\nThe overall process of SubMix is shown as Algorithm 5. Given a graph \ud835\udc3a = (V, E,X) and a set G of all available graphs, SubMix makes an augmented graph by replacing a subgraph of\ud835\udc3a with that of another graph \ud835\udc3a \u2032 chosen from G \\ {\ud835\udc3a}. First, SubMix samples ordered sets \ud835\udc46 and \ud835\udc46 \u2032 of nodes from \ud835\udc3a and \ud835\udc3a \u2032, respectively, where |\ud835\udc46 | = |\ud835\udc46 \u2032 |. Then, SubMix makes a one-to-one mapping \ud835\udf19 from the nodes in \ud835\udc46 \u2032 to those in \ud835\udc46 based on their order in each ordered set, and uses it to transfer the induced subgraph of \ud835\udc46 \u2032 into \ud835\udc3a , replacing the induced subgraph of \ud835\udc46 . The edges that connect \ud835\udc46 to the rest of the graph \ud835\udc3a are then connected to the new nodes.\nAs a result, SubMix makes the set E\u0304 = E1 \u222a E2 of edges for the augmented graph\ud835\udc3a , where E1 and E2 are extracted from\ud835\udc3a and\ud835\udc3a \u2032, respectively. The difference between E1 and E2 is that E1 contains the edges incident to at least one node in \ud835\udc46 , while E2 contains only the edges whose two connected nodes are both in \ud835\udc46 \u2032. The reason is because the main target of augmentation is \ud835\udc3a . The subgraph of \ud835\udc3a \u2032\nis inserted into \ud835\udc3a without changing the edges that connect \ud835\udc46 with V \\ \ud835\udc46 . See Figure 3 for a visual illustration.\nOne notable difference from NodeSam is that SubMix takes the label of \ud835\udc3a as an additional input and changes it. The label y\u0304 of \ud835\udc3a is softly determined as y\u0304 = \ud835\udc5ey + (1 \u2212 \ud835\udc5e)y\u2032, where y and y\u2032 are the one-hot labels of \ud835\udc3a and \ud835\udc3a \u2032, respectively, and \ud835\udc5e = |E1 |/|E\u0304 |. This is based on an assumption that edges are crucial factors to determine the label of a graph, and thus the ratio of included edges quantifies how much it contributes to making y\u0304.\n3.2.1 Selecting Subgraphs with Diffusion. The core part of SubMix is the choice of ordered sets \ud835\udc46 and \ud835\udc46 \u2032. The list of nodes included in each set determines the shape of the subgraph, and their order determines how the nodes in \ud835\udc46 \u2032 are connected to the nodes inV\\\ud835\udc46 . A naive approach is to select a random subset from each graph without considering the structural information, but this is likely to make disconnected subgraphs containing few edges.\nInstead, we utilize graph diffusion to select connected and clustered subgraphs from \ud835\udc3a and\ud835\udc3a \u2032. The process of subgraph sampling is shown as Algorithm 6. Given a root node \ud835\udc5f , a diffusion operator propagates a signal from \ud835\udc5f to all other nodes following the graph structure. This assigns large affinity scores to the nodes close to \ud835\udc5f or having many common neighbors with \ud835\udc5f . Thus, selecting the top \ud835\udc58 nodes having the largest affinity scores provides a meaningful substructure around \ud835\udc5f containing a sufficient number of edges, and the chosen nodes can be used directly as the subset \ud835\udc46 .\nWe use personalized PageRank (PPR) as the diffusion function, which is a popular approach to measure the personalized scores of nodes. PPR [17] converts the adjacency matrix A of each graph to a matrix S of scores by diffusing the graph signals from all nodes: S = \u2211\u221e \ud835\udc58=0 \ud835\udefc (1\u2212 \ud835\udefc)\n\ud835\udc58 (D\u22121/2AD\u22121/2)\ud835\udc58 , where D is the degree matrix such that\ud835\udc37\ud835\udc56\ud835\udc56 = \u2211 \ud835\udc58 \ud835\udc34\ud835\udc56\ud835\udc58 , and \ud835\udefc = 0.15 is the teleport probability. The \ud835\udc56-th column of S contains the affinity scores of nodes with respect to node \ud835\udc56 . Thus, given a root node \ud835\udc5f , we return the \ud835\udc5f -th column of S as the result c of diffusion in line 3 of Algorithm 6.\nConnected subgraphs. In Algorithm 6, it is essential to guarantee the connectivity of nodes \ud835\udc46 and \ud835\udc46 \u2032 to allow SubMix to replace\nAlgorithm 5 SubMix (Subgraph Mix)\nInput: Target graph \ud835\udc3a = (V, E,X) with its label \ud835\udc66, set G of all available graphs with labels Y, and target ratio \ud835\udc5d \u2208 (0, 1) of augmentation Output: Augmented graph \ud835\udc3a = (V\u0304, E\u0304, X\u0304) and its label y\u0304 1: \ud835\udc3a \u2032, \ud835\udc66\u2032 \u2190 Pick a random graph from G \\ {\ud835\udc3a} and its label 2: \ud835\udc46, \ud835\udc46 \u2032 \u2190 Sample(\ud835\udc3a,\ud835\udc3a \u2032, \ud835\udc5d) \u22b2 Algorithm 6 3: \ud835\udf19 \u2190Make the one-to-one mapping from \ud835\udc46 \u2032 to \ud835\udc46 4: E1 \u2190 {(\ud835\udc62, \ud835\udc63) | (\ud835\udc62, \ud835\udc63) \u2208 E \u2227 \u00ac(\ud835\udc62 \u2208 \ud835\udc46 \u2227 \ud835\udc63 \u2208 \ud835\udc46)} 5: E2 \u2190 {(\ud835\udf19 (\ud835\udc62), \ud835\udf19 (\ud835\udc63)) | (\ud835\udc62, \ud835\udc63) \u2208 E \u2032 \u2227 (\ud835\udc62 \u2208 \ud835\udc46 \u2032 \u2227 \ud835\udc63 \u2208 \ud835\udc46 \u2032)} 6: V\u0304, E\u0304, X\u0304\u2190V, E1 \u222a E2,X 7: X\u0304[\ud835\udf19 (\ud835\udc46 \u2032)] \u2190 X\u2032[\ud835\udc46 \u2032] \u22b2 Replace a subset of features 8: y, y\u2032 \u2190 Represent \ud835\udc66 and \ud835\udc66\u2032 as one-hot vectors, respectively 9: y\u0304\u2190 (|E1 |/|E\u0304 |)y + (1 \u2212 |E1 |/|E\u0304 |)y\u2032\nAlgorithm 6 Sample in SubMix\nInput: Target graph \ud835\udc3a = (V, E,X), another graph \ud835\udc3a \u2032 = (V \u2032, E \u2032,X\u2032) selected from G \\ {\ud835\udc3a}, and target ratio \ud835\udc5d \u2208 (0, 1) of augmentation Output: Ordered sets \ud835\udc46 \u2286 V and \ud835\udc46 \u2032 \u2286 V \u2032 of connected nodes 1: \ud835\udc5f, \ud835\udc5f \u2032 \u2190 Pick random nodes from \ud835\udc3a and \ud835\udc3a \u2032, respectively 2: \ud835\udf13 \u2190Make a function that finds the connected component 3: \ud835\udc58 \u2190 uniform(0, \ud835\udc5d) \u00b7min( |\ud835\udf13 (\ud835\udc3a, \ud835\udc5f ) |, |\ud835\udf13 (\ud835\udc3a \u2032, \ud835\udc5f \u2032) |) 4: for (\ud835\udc3a\ud835\udc61 , \ud835\udc5f\ud835\udc61 ) \u2208 {(\ud835\udc3a, \ud835\udc5f ), (\ud835\udc3a \u2032, \ud835\udc5f \u2032)} do 5: c\ud835\udc61 \u2190 Compute the scores of nodes by Diffuse(\ud835\udc3a\ud835\udc61 , \ud835\udc5f\ud835\udc61 ) 6: \ud835\udc46\ud835\udc61 \u2190 Select \ud835\udc58 nodes having the largest scores in c\ud835\udc61 7: if \ud835\udc46\ud835\udc61 contains a disconnected node then 8: \ud835\udc46\ud835\udc61 \u2190 Take the first \ud835\udc58 nodes from BFS(\ud835\udc3a\ud835\udc61 , \ud835\udc5f\ud835\udc61 ) 9: end if 10: \ud835\udc46 \u2190 \ud835\udc46\ud835\udc61 if \ud835\udc3a\ud835\udc61 = \ud835\udc3a otherwise \ud835\udc46 \u2032 \u2190 \ud835\udc46\ud835\udc61 11: end for\nmeaningful substructures. We guarantee the connectivity with two ideas in the algorithm. First, we bound the number of selected nodes by the size of the connected component containing each root node, since the input graphs can be disconnected. Second, if the nodes selected by PPR make disconnected subgraphs, we resample the nodes by the breadth-first search (BFS) that is guaranteed to make connected subgraphs. This is to prevent rare cases where PPR returns a disconnected graph, which has not occurred in all of our experiments but can happen theoretically.\nLemma 4. Each set of nodes returned by Algorithm 6 contains only connected nodes for both \ud835\udc3a and \ud835\udc3a \u2032 and for any value of \ud835\udc58 .\nProof. The proof is straightforward as we run BFS on the connected component of each root if \ud835\udc46 (or \ud835\udc46 \u2032) is not connected. \u25a1\nOrder of nodes. The order of selected nodes determines how the nodes in \ud835\udc46 are matched with those in \ud835\udc46 \u2032, playing an essential role for the result of augmentation. One advantage of diffusion is that the selected nodes are ordered by their affinity scores, making nodes at the same relative position around the root nodes \ud835\udc5f and \ud835\udc5f \u2032 to be matched between \ud835\udc46 and \ud835\udc46 \u2032 in the replacement. The root \ud835\udc5f of \ud835\udc46 is always replaced with \ud835\udc5f \u2032 of \ud835\udc46 \u2032, and the replacement of all remaining nodes is determined by their affinity scores. This makes\nthe generated graph \ud835\udc3a more plausible than in the naive approach that matches the nodes in \ud835\udc46 randomly with those in \ud835\udc46 \u2032.\n3.2.2 Satisfying Desired Properties. SubMix satisfies all the desired properties in Table 1. It is straightforward that Property 3 and 4 are satisfied since SubMix changes the node features and the set of edges at every augmentation. We show in Lemma 1, 2, and 3 that SubMix also satisfies the rest of the properties.\nPreserving size. Since SubMix combines multiple graphs, it is not possible to directly show the satisfaction of Property 1 for any given graph \ud835\udc3a . For instance, the number of edges always increases if \ud835\udc3a is a chain graph and all other graphs in G are cliques. Thus, we assume that the target graph\ud835\udc3a is selected uniformly at random from G and prove the unbiasedness with E[|E | \u2212 |E\u0304 |] = 0.\nLemma 5. Given a set G of graphs, we sample different graphs \ud835\udc3a = (V, E,X) and \ud835\udc3a \u2032 = (V \u2032, E \u2032,X\u2032) from G uniformly at random. Let \ud835\udc3a = (V\u0304, E\u0304) be an augmented graph generated by \ud835\udc46\ud835\udc62\ud835\udc4f\ud835\udc40\ud835\udc56\ud835\udc65 . Then, E[|V| \u2212 |V\u0304 |] = 0 and E[|E | \u2212 |E\u0304 |] = 0.\nProof. The proof is in Appendix B.3. \u25a1\nOther properties. Lemma 6 shows that SubMix preserves the connectivity of the given graph, while Lemma 7 shows that SubMix runs in linear time with respect to the number of edges.\nLemma 6. Given a graph \ud835\udc3a = (V, E,X), let \ud835\udc3a = (V\u0304, E\u0304, X\u0304) be an augmented graph generated by \ud835\udc46\ud835\udc62\ud835\udc4f\ud835\udc40\ud835\udc56\ud835\udc65 . Then, \ud835\udc3a is connected if and only if \ud835\udc3a is connected.\nProof. The proof is in Appendix B.4. \u25a1\nLemma 7. Given graphs \ud835\udc3a = (V, E,X) and \ud835\udc3a \u2032 = (V \u2032, E \u2032,X\u2032), the time and space complexities of SubMix are\ud835\udc42 (\ud835\udc5d\ud835\udc51 |V| + |E| + |E \u2032 |), where \ud835\udc5d is the ratio of sampling, and \ud835\udc51 is the number of features.\nProof. The proof is in Appendix B.5. \u25a1"
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "We perform experiments to answer the following questions: Q1. Accuracy (Section 4.2). Do NodeSam and SubMix improve\nthe accuracy of graph classifiers? Are they better than previous approaches for graph augmentation? Q2. Desired properties (Section 4.3). Do NodeSam and SubMix satisfy the desired properties of Table 1 in real-world graphs as we claim theoretically in Section 3? Q3. Ablation study (Section 4.4). Do our ideas for improving NodeSam and SubMix, such as the adjustment or diffusion operation, increase the accuracy of graph classifiers?"
        },
        {
            "heading": "4.1 Experimental Setup",
            "text": "We introduce our experimental setup including datasets, baseline approaches, hyperparameters, and graph classifiers.\nDatasets. We use 9 benchmark datasets [23, 41] summarized in Table 2, which were used in previous works for graph classification. D&D, ENZYMES, MUTAG, NCI1, NCI109, PROTEINS, and PTCMR [42] are datasets of molecular graphs that represent chemical compounds. COLLAB [42] and Twitter [25] are datasets of social networks. The numbers of nodes and edges in Table 2 are from all the graphs in each dataset. Detailed information of node features in the datasets are described in Appendix C."
        },
        {
            "heading": "Dataset Graphs Nodes Edges Features Labels",
            "text": "Baselines.We consider the following baselines for graph augmentation. DropEdge [28] removes an edge uniformly at random, while DropNode removes a node and all connected edges. AddEdge inserts an edge between a random pair of nodes. ChangeAttr augments the one-hot feature vector of a random node by changing the nonzero index. GraphCrop [35] selects a subgraph by diffusion and uses it as an augmented graph. NodeAug [38] combines ChangeAttr, DropEdge, and AddEdge to change the local neighborhood of a random target node. MotifSwap [55] swaps the two edges in an open triangle to preserve the connectivity during augmentation.\nClassifier. We use GIN [41] as a graph classifier for evaluating the accuracy, which is one of the most popular models for graph classification and shows great performance in many domains. The hyperparameters are searched in the same space as in their original paper [41] to ensure that the improvement of accuracy comes from the augmentation, not from hyperparameter tuning: batch size in {32, 128} and the dropout probability in {0, 0.5}.\nTraining details. Following the experimental process of GIN [41], which we use as the classifier, we run 10-fold cross-validation for evaluation. The indices of chosen graphs for each fold are included in the provided code repository. The Adam optimizer [15] is used, and the learning rate starts from 0.01 and decreases by half at every 50 epochs until it reaches 350 epochs. We set the ratio \ud835\udc5d of augmentation for SubMix, which is the only hyperparameter of our proposed approaches, to 0.4. All of our experiments were done at a workstation with Intel Core i7-8700 and RTX 2080."
        },
        {
            "heading": "4.2 Accuracy of Graph Classification (Q1)",
            "text": "Table 3 compares the accuracy of graph classification with various augmentation methods. The Rank column is made by getting the ranks of methods in each dataset and computing their average and standard deviation over all datasets. For example, if the rank of a method is 1, 3, and 3 in three datasets, respectively, its rank is reported as 2.3 \u00b1 1.2. The rank measures the performance of each method differently from the average accuracy: one can achieve the highest rank even though its average accuracy is low.\nNodeSam and SubMix improve the average accuracy of GIN by 1.71 and 1.75 percent points, which are 2.0\u00d7 and 2.1\u00d7 larger than the improvement of the best competitor, respectively. Note that all approaches except NodeSam and SubMix decrease the accuracy of GIN in some datasets; GraphCrop even decreases the average accuracy of GIN, implying that they give a wrong bias to the classifier by distorting the data distribution with unjustified changes.\nTable 3: Accuracy of graph classification with various graph augmentation methods. The values in parentheses are the ranks of methods in each dataset, and the Rank column shows the average and standard deviation of ranks over all datasets. The proposed methods NodeSam and SubMix achieve the best average accuracy and the highest ranks at the same time.\nMethod D&D ENZY. MUTAG NCI1 N109 PROT. PTC-MR COLLAB Twitter Average Rank\nBaseline 76.40 (4) 50.33 (10) 89.94 (4) 82.68 (9) 81.80 (9) 75.38 (9) 63.94 (7) 82.66 (7) 66.05 (7) 74.35 (8) 7.33 \u00b1 2.18\nGraphCrop 77.08 (2) 51.00 (9) 77.11 (10) 80.46 (10) 79.77 (10) 75.20 (10) 61.87 (10) 83.50 (2) 66.15 (3) 72.46 (10) 7.33 \u00b1 3.77 DropEdge 76.14 (6) 53.67 (6) 81.93 (9) 82.82 (7) 82.60 (7) 75.74 (4) 63.68 (8) 82.50 (9) 66.05 (8) 73.90 (9) 7.11 \u00b1 1.62 NodeAug 76.14 (8) 54.67 (5) 86.14 (7) 83.16 (4) 82.36 (8) 75.56 (6) 66.24 (2) 81.32 (10) 65.98 (9) 74.62 (7) 6.56 \u00b1 2.55 AddEdge 76.14 (7) 55.17 (3) 85.67 (8) 83.99 (2) 83.06 (5) 75.38 (8) 64.27 (5) 82.80 (5) 66.10 (6) 74.73 (6) 5.44 \u00b1 2.07 ChangeAttr 75.72 (9) 53.33 (8) 90.44 (3) 83.02 (5) 83.57 (2) 75.47 (7) 62.47 (9) 82.76 (6) 66.36 (2) 74.79 (5) 5.67 \u00b1 2.83 DropNode 75.55 (10) 55.17 (3) 87.28 (6) 82.85 (6) 83.04 (6) 75.65 (5) 66.59 (1) 82.54 (8) 66.11 (5) 74.97 (4) 5.56 \u00b1 2.60 MotifSwap 76.23 (5) 53.50 (7) 90.47 (2) 82.82 (7) 83.28 (4) 75.92 (3) 65.79 (3) 82.84 (3) 65.93 (10) 75.20 (3) 4.89 \u00b1 2.62\nSubMix 78.10 (1) 57.50 (2) 89.94 (4) 84.33 (1) 84.37 (1) 76.19 (1) 63.97 (6) 83.74 (1) 66.44 (1) 76.06 (2) 2.00 \u00b1 1.80 NodeSam 76.57 (3) 60.00 (1) 90.96 (1) 83.33 (3) 83.52 (3) 76.10 (2) 65.48 (4) 82.82 (4) 66.13 (4) 76.10 (1) 2.78 \u00b1 1.20\n(a) NodeSam (b) SubMix\nFigure 4: Box plots representing the number of edges that our proposed methods change through augmentation. Both methods make unbiased augmentation, where the variance of changes depends on the characteristic of each dataset."
        },
        {
            "heading": "4.3 Preserving Desired Properties (Q2)",
            "text": "The main motivation of our NodeSam and SubMix is to satisfy the desired properties of Table 1. We present empirical results that support our theoretical claims given in Section 3.1.2 and 3.2.2.\nFigure 4 visualizes the change in the number of edges during augmentation as box plots. Both NodeSam and SubMix perform unbiased augmentation as shown in the orange lines that appear in the center of each plot. On the other hand, they make a sufficient change of edges through augmentation, generating diverse examples for the training of a classifier. SubMix makes a larger degree of augmentation than NodeSam, especially in the D&D dataset, as it combines multiple graphs of different structures.\nFigure 5 shows the scalability of NodeSam and SubMix with the number of edges in a graph. We use the Reddit [51] dataset for this experiment, which is large enough to cause scalability issues as it contains 232,965 nodes and 11,606,919 edges. We randomly make nine subgraphs with different sizes to measure the running time of methods. The figure shows that NodeSam and SubMix have linear scalability with similar patterns, while the computational time of MotifSwap increases much faster, causing out-of-memory errors. This supports our claims in Lemma 3 and 7.\nFigure 6 visualizes the space of augmentation for our proposed algorithms and MotifSwap, the strongest baseline. We first use the Weisfeiler-Lehman (WL) kernel [29] to extract embedding vectors\nNodeSam (proposed) SubMix (proposed) MotifSwap\nFigure 5: Computational time of our proposed methods and MotifSwap with respect to the number of edges. Our methods achieve linear scalability, while MotifSwap does not.\nof graphs and then use the t-SNE algorithm [33] to visualize them in the 2D space. The distance value in each figure represents the average distance between each original graph and its augmented graphs in the 2D space. SubMix shows the largest space of augmentation by combining multiple graphs, effectively filling in the space between given examples. MotifSwap makes augmented graphs only near the original graphs, making the smallest average distance."
        },
        {
            "heading": "4.4 Ablation Study (Q3)",
            "text": "We perform an ablation study for both NodeSam and SubMix, and present the result in Figure 7. SplitOnly and MergeOnly refer to the split and merge operations of Algorithm 2 and 3, respectively. NodeSamBase refers to the naive version of NodeSam, which does not perform the adjustment operation of Algorithm 4. SubMixBase refers to the naive version of SubMix, which selects the target sets of nodes for the replacement uniformly at random, without using the diffusion operation of Algorithm 6. The vertical and horizontal axes of the figure are the average accuracy and rank over all datasets, corresponding to the last two columns of Table 3, respectively.\nWe have two observations from Figure 7. First, NodeSam and SubMix with all techniques achieve the best accuracy among all versions, showing that our proposed techniques are effective for improving their performance by satisfying the desired properties. Second, every basic version of our approaches still improves the\nbaseline significantly, which is to use raw graphs without augmentation. This is clear when compared with existing methods shown in Table 3 that make marginal improvements or even decrease the baseline accuracy. Such improvement exhibits the effectiveness of our ideas, which are building blocks of NodeSam and SubMix.\nWe also perform an additional study for NodeSam and its basic versions in Appendix D, which shows that NodeSamBase without the adjustment operation is likely to decrease the number of edges due to the merge operation that eliminates additional triangles."
        },
        {
            "heading": "5 RELATEDWORKS",
            "text": "We review related works for graph augmentation, graph classification, and image augmentation.\nModel-agnostic graph augmentation. Our work focuses on model-agnostic augmentation, which is to perform augmentation independently from target models. Previous works can be categorized by the purpose of augmentation: node-level tasks [28, 38, 46], graph-level tasks [10, 35, 55], or graph contrastive learning [48, 49]. Such methods rely on heuristic operations that make no theoretical guarantee for the degree of augmentation or the preservation of graph properties. We propose five desired properties for effective graph augmentation and show that our approaches satisfy all of them, resulting in the best accuracy in classification.\nModel-specific graph augmentation. Model-specific methods for graph augmentation make changes optimized for the target model. For example, changing the representation vectors of graphs learned by the target classifier by a mixup algorithm [37] is modelspecific. Such model-specific approaches include manifold mixup [34], dynamic augmentation of edges [3, 53], and adversarial perturbations [7, 18]. Methods for graph adversarial training [4, 20, 40, 57] also belong to this category as they require the predictions of target models. Such approaches participate directly in the training process, and thus cannot be used generally in various settings.\nGraph classification. Graph classification is a core problem in the graph domain, which is to predict the label of each graph. Compared to node classification [16] that focuses on the properties of individual nodes, graph classification aims at extracting meaningful substructures of nodes to make a better representation of a graph. A traditional way to solve the problem is to utilize kernel methods [24, 26, 29, 42], while graph neural networks [13, 16, 17, 41, 45, 47] have shown a better performance with advanced pooling methods [1, 21, 44] to aggregate node embeddings. We use graph classification as a downstream task for evaluating augmented graphs, since\naccurate classification requires separating the core information of each graph from random and unimportant components.\nImage augmentation. Data augmentation is studied actively in the image domain. Basic approaches include color and geometric transformations [11, 27], masking [5, 31, 54], adversarial perturbations [22], pixel-basedmixing [19, 52], and generativemodels [9, 56]. Such approaches utilize the characteristic of images where slight changes of pixel values do not change the semantic information of each image. On the other hand, patch-based mixing approaches that replace a random patch of an image to that of another image [14, 32, 50] give us a motivation to propose SubMix, which replaces an induced subgraph of a graph, instead of an image patch. This is based on the idea that images are grid-structured graphs whose pixels correspond to individual nodes."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "We propose NodeSam (Node Split and Merge) and SubMix (Subgraph Mix), effective algorithms for model-agnostic graph augmentation. We first present desired properties for graph augmentation including the preservation of original properties, the guarantee of sufficient augmentation, and the linear scalability. Then, we show that both NodeSam and SubMix achieve all properties unlike all of the existing approaches. NodeSam aims to make a minimal change of the graph structure by performing opposite operations at once: splitting a node and merging two nodes. On the other hand, SubMix aims to increase the degree of augmentation by combining random subgraphs of different graphs with different labels. Experiments on nine datasets show that NodeSam and SubMix achieve the best accuracy in graph classification with up to 2.1\u00d7 larger improvement of accuracy compared with previous approaches."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "This work was supported by Institute of Information & communications Technology Planning & Evaluation(IITP) grant funded by the Korea government(MSIT) [No.2020-0-00894, Flexible and Efficient Model Compression Method for Various Applications and Environments], [No.2021-0-01343, Artificial Intelligence Graduate School Program (Seoul National University)], and [NO.2021-0-0268, Artificial Intelligence Innovation Hub (Artificial Intelligence Institute, Seoul National University)]. The Institute of Engineering Research at Seoul National University provided research facilities for this work. The ICT at Seoul National University provides research facilities for this study. U Kang is the corresponding author."
        },
        {
            "heading": "A UNBIASEDNESS OF NODESAM A.1 Properties of Intermediate Graphs",
            "text": "We have four graphs \ud835\udc3a , \ud835\udc3a \u2032, \ud835\udc3a \u2032\u2032, and \ud835\udc3a presented in Algorithm 1, which denote the original graph, the graph after the split, the graph after the adjustment, and the final graph generated by NodeSam, respectively. Let \ud835\udc63\ud835\udc56 be the target node of the split operation, \ud835\udc51\ud835\udc56 be the degree of \ud835\udc63\ud835\udc56 in\ud835\udc3a , and \u210e\ud835\udc56 be the expected number of edges added in the adjustment operation. Let \ud835\udc47 (\u00b7) be the function that counts the number of triangles in a graph. Then, we analyze the properties of graphs generated during NodeSam in Lemma 8 to 11.\nLemma 8. E[\ud835\udc47 (\ud835\udc3a \u2032)] = \ud835\udc47 (\ud835\udc3a) \u2212 \ud835\udc61\ud835\udc56/2.\nProof. Each triangle in \ud835\udc3a containing \ud835\udc63\ud835\udc56 has a probability of 0.5 to be removed in the split operation, due to the random selection of the target node from \ud835\udc63 \ud835\udc57 and \ud835\udc63\ud835\udc58 . Thus, it is expected that the half of all triangles containing \ud835\udc63\ud835\udc56 are removed in \ud835\udc3a \u2032. \u25a1\nLemma 9. In the adjustment operation of Algorithm 4, let \ud835\udc59 be the number of new triangles created in\ud835\udc3a \u2032\u2032 by connecting an edge between a target node \ud835\udc62 \u2208 S and either \ud835\udc63 \ud835\udc57 or \ud835\udc63\ud835\udc58 . Then, E[\ud835\udc59] = |N\ud835\udc62\ud835\udc56 |/2 + 1, where N\ud835\udc62\ud835\udc56 is the set of common neighbors between \ud835\udc62 and \ud835\udc63\ud835\udc56 in \ud835\udc3a .\nProof. Node \ud835\udc62 is connected to either \ud835\udc63 \ud835\udc57 or \ud835\udc63\ud835\udc58 in\ud835\udc3a \u2032, since it is a neighbor of \ud835\udc63\ud835\udc56 in the the original graph \ud835\udc3a before the split is done. Assume that \ud835\udc62 is connected to \ud835\udc63 \ud835\udc57 in \ud835\udc3a \u2032 without loss of generality. Then, the adjustment for \ud835\udc62 makes an edge (\ud835\udc62, \ud835\udc63\ud835\udc58 ). A single triangle (\ud835\udc62, \ud835\udc63 \ud835\udc57 , \ud835\udc63\ud835\udc58 ) is always created by this operation, since edges (\ud835\udc62, \ud835\udc63 \ud835\udc57 ) and (\ud835\udc63 \ud835\udc57 , \ud835\udc63\ud835\udc58 ) already exist in \ud835\udc3a \u2032. The number of additional triangles created by the adjustment is the same as the number of common neighbors between \ud835\udc62 and \ud835\udc63\ud835\udc58 in\ud835\udc3a \u2032, denoted by |N \u2032\ud835\udc62\ud835\udc58 |. Since we split N\ud835\udc56 by the same probability into \ud835\udc63 \ud835\udc57 and \ud835\udc63\ud835\udc58 during the split operation, E[|N \u2032\n\ud835\udc62\ud835\udc58 |] = |N\ud835\udc62\ud835\udc56 |/2. We prove the lemma by adding one. \u25a1\nLemma 10. E[\ud835\udc47 (\ud835\udc3a \u2032\u2032)] = \ud835\udc47 (\ud835\udc3a) + \u210e\ud835\udc56 (\ud835\udc61\ud835\udc56/\ud835\udc51\ud835\udc56 + 1) \u2212 \ud835\udc61\ud835\udc56/2.\nProof. Following Lemma 9, |N\ud835\udc62\ud835\udc56 |/2 + 1 triangles are expected to be created for each target node \ud835\udc62 \u2208 S during the adjustment. We estimate |N\ud835\udc62\ud835\udc56 | by 2\ud835\udc61\ud835\udc56/\ud835\udc51\ud835\udc56 based on the relation\u2211\ufe01\n\ud835\udc58\u2208N\ud835\udc56 |N\ud835\udc58\ud835\udc56 | = 2\ud835\udc61\ud835\udc56 ,\nwhere N\ud835\udc56 is the set of neighbors of \ud835\udc63\ud835\udc56 in \ud835\udc3a . We get\nE[\ud835\udc47 (\ud835\udc3a \u2032\u2032) \u2212\ud835\udc47 (\ud835\udc3a \u2032)] = \u210e\ud835\udc56 (\ud835\udc61\ud835\udc56/\ud835\udc51\ud835\udc56 + 1)\nby repeating the adjustment \u210e\ud835\udc56 times for every possible \ud835\udc62 \u2208 S. We prove the lemma by combining it with Lemma 8. \u25a1\nLemma 11. E[|E\u0304 |] = |E \u2032\u2032 | \u2212 3\ud835\udc47 (\ud835\udc3a \u2032\u2032)/|E \u2032\u2032 | \u2212 1.\nProof. Let \ud835\udc63\ud835\udc5c and \ud835\udc63\ud835\udc5d be the target nodes of the merge operation, and let \ud835\udc61 \u2032\u2032\ud835\udc5c\ud835\udc5d be the number of triangles containing \ud835\udc63\ud835\udc5c and \ud835\udc63\ud835\udc5d in \ud835\udc3a \u2032\u2032. A single edge (\ud835\udc63\ud835\udc5c , \ud835\udc63\ud835\udc5d ) is always removed by the merge operation as \ud835\udc63\ud835\udc5c and \ud835\udc63\ud835\udc5d are merged into one. The number of additional edges removed by the merge operation is given as \ud835\udc61 \u2032\u2032\ud835\udc5c\ud835\udc5d , which is estimated by 3\ud835\udc47 (\ud835\udc3a \u2032\u2032)/|E \u2032\u2032 | based on the relation\u2211\ufe01\n(\ud835\udc56, \ud835\udc57) \u2208E\u2032\u2032 \ud835\udc61 \u2032\u2032\ud835\udc56 \ud835\udc57 = 3\ud835\udc47 (\ud835\udc3a \u2032\u2032) .\nWe prove the lemma by adding one to 3\ud835\udc47 (\ud835\udc3a \u2032\u2032)/|E \u2032\u2032 |. \u25a1"
        },
        {
            "heading": "A.2 Proof of Lemma 1",
            "text": "We restate Lemma 1 given in Section 3.1.2 and present the full proof based on Lemma 8 to 11.\nLemma 1. Given a graph\ud835\udc3a = (V, E,X), let\ud835\udc3a = (V\u0304, E\u0304, X\u0304) be the result of \ud835\udc41\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc46\ud835\udc4e\ud835\udc5a. Then, E[|V\u0304 | \u2212 |V|] = 0 and E[|E\u0304 | \u2212 |E |] = 0.\nProof. We prove only the edge part, since it is straightforward that NodeSam does not change the number of nodes. Let\ud835\udc5a be the expected number of edges removed by the merge operation:\n\ud835\udc5a \u2261 E[|E \u2032\u2032 | \u2212 |E\u0304 |] . (2) Then,\ud835\udc5a is rewritten as follows by Lemma 11:\n\ud835\udc5a = E[3\ud835\udc47 (\ud835\udc3a \u2032\u2032)/|E \u2032\u2032 |] + 1. (3) If we ignore the dependence between \ud835\udc47 (\ud835\udc3a \u2032\u2032) and |E \u2032\u2032 |, which is negligible in real-world graphs that sastify |E | \u226b \u210e\ud835\udc56 , Equation (3) changes into\n\ud835\udc5a = 3E[\ud835\udc47 (\ud835\udc3a \u2032\u2032)] E[|E \u2032\u2032 |] + 1. (4)\nThe first term E[\ud835\udc47 (\ud835\udc3a \u2032\u2032)] is rewritten by Lemma 10: E[\ud835\udc47 (\ud835\udc3a \u2032\u2032)] = |V|\ud835\udc61\ud835\udc56/3 + \u210e\ud835\udc56 (\ud835\udc61\ud835\udc56/\ud835\udc51\ud835\udc56 + 1) \u2212 \ud835\udc61\ud835\udc56/2, (5)\nwhere the global number \ud835\udc47 (\ud835\udc3a) of triangles is replaced with a local estimation |V|\ud835\udc61\ud835\udc56/3, since we select \ud835\udc63\ud835\udc56 from \ud835\udc3a uniformly at random and the relation E\ud835\udc56 [\ud835\udc61\ud835\udc56 ] = 3\ud835\udc47 (\ud835\udc3a) holds between \ud835\udc47 (\ud835\udc3a) and \ud835\udc61\ud835\udc56 .\nThe second term E[|E \u2032\u2032 |] is given by the definition of \u210e\ud835\udc56 : E[|E \u2032\u2032 |] = |E | + \u210e\ud835\udc56 + 1. (6)\nWe apply Equation (5) and (6) to Equation (4) to get\n\ud835\udc5a = 3(\ud835\udc61\ud835\udc56/\ud835\udc51\ud835\udc56 + 1)\u210e\ud835\udc56 + (|V| \u2212 3/2)\ud835\udc61\ud835\udc56\n\u210e\ud835\udc56 + |E| + 1 + 1. (7)\nIf we denote the first term of the right hand side of Equation (7) as \ud835\udc34, we can verify that Equation (1) is the solution of \u210e\ud835\udc56 = \ud835\udc34:\n\u210e\ud835\udc56 = \ud835\udc34\n\u210e\ud835\udc56 (\u210e\ud835\udc56 + |E| + 1) = 3(\ud835\udc61\ud835\udc56/\ud835\udc51\ud835\udc56 + 1)\u210e\ud835\udc56 + (|V| \u2212 3/2)\ud835\udc61\ud835\udc56 \u210e2\ud835\udc56 + (|E | \u2212 3\ud835\udc61\ud835\udc56/\ud835\udc51\ud835\udc56 \u2212 2)\u210e\ud835\udc56 \u2212 (|V| \u2212 3/2)\ud835\udc61\ud835\udc56 = 0.\n(8)\nThen, the right hand side of Equation (7) changes into \u210e\ud835\udc56 + 1. By the definition of\ud835\udc5a and \u210e\ud835\udc56 , we finally get E[|E \u2032\u2032 | \u2212 |E\u0304 |] = E[|E \u2032\u2032 |] \u2212 |E| \u2212 1 + 1, which changes into E[|E\u0304 | \u2212 |E |] = 0 and proves the lemma. \u25a1"
        },
        {
            "heading": "B PROOFS OF LEMMA 2 TO 7 B.1 Proof of Lemma 2",
            "text": "Proof. The split and merge operations preserve the connectivity, since they are transformations between a single node and a pair of adjacent nodes. The adjustment also preserves the connectivity since it makes new edges only between two-hop neighbors. \u25a1"
        },
        {
            "heading": "B.2 Proof of Lemma 3",
            "text": "Proof. The time complexity of the split operation is \ud835\udc42 (\ud835\udc51 |V| + |E|), and the complexity of the merge operation is the same. The time complexity of the adjustment operation is \ud835\udc42 (V| + |E|) since it does not change X. The space complexities are always smaller than or equal to the time complexities [2]. We prove the lemma by combining the complexities of all these three operations. \u25a1"
        },
        {
            "heading": "B.3 Proof of Lemma 5",
            "text": "Proof. The proof is straightforward forV , since SubMix does not change the number of nodes. For E, let E\ud835\udc60 and E \u2032\ud835\udc60 be the edges of induced subgraphs of \ud835\udc46 and \ud835\udc46 \u2032 in graphs \ud835\udc3a and \ud835\udc3a \u2032, respectively. Then, the following equations hold, since a) SubMix replaces only the edges in E\ud835\udc60 , b) \ud835\udc3a and \ud835\udc3a \u2032 are selected independently, and c) \ud835\udc46 and \ud835\udc46 \u2032 are selected by the same diffusion algorithm:\nE\ud835\udc3a,\ud835\udc3a [|E | \u2212 |E\u0304 |] = E\ud835\udc3a,\ud835\udc3a\u2032 [|E\ud835\udc60 | \u2212 |E \u2032 \ud835\udc60 |]\n= E\ud835\udc3a [|E\ud835\udc60 |] \u2212 E\ud835\udc3a\u2032 [|E \u2032\ud835\udc60 |] = 0.\nThe fact that E[|E | \u2212 |E\u0304 |] = 0 proves the lemma. \u25a1"
        },
        {
            "heading": "B.4 Proof of Lemma 6",
            "text": "Proof. Let \ud835\udc3a \u2032 be a graph chosen for the augmentation of \ud835\udc3a by SubMix. The sets \ud835\udc46 and \ud835\udc46 \u2032 of nodes selected for the replacement are connected due to Lemma 6. If we treat the induced subgraphs of \ud835\udc46 and \ud835\udc46 \u2032 as supernodes, the replacement of \ud835\udc46 with \ud835\udc46 \u2032 does not change the connectivity of \ud835\udc3a , proving the lemma. \u25a1"
        },
        {
            "heading": "B.5 Proof of Lemma 7",
            "text": "Proof. The time complexity of Algorithm 5 without including the sampling function is\ud835\udc42 (\ud835\udc5d\ud835\udc51 |V| + |E| + |E \u2032 |). We assume that the number of labels is negligible. The time complexity of Algorithm 6 is \ud835\udc42 ( |E | + |E \u2032 |), since the PPR diffusion is \ud835\udc42 ( |E |) and \ud835\udc42 ( |E \u2032 |) for \ud835\udc3a and \ud835\udc3a \u2032, respectively. The space complexities are always smaller than or equal to the time complexities [2]. \u25a1"
        },
        {
            "heading": "C NODE FEATURE INFORMATION",
            "text": "We describe detailed information of node features in our datasets, which are summarized in Table 2. \u2022 Molecular graphs. Each node represents an element in a chemical compound, and contains a one-hot feature representing its atomic type, such as carbon or oxygen. \u2022 Twitter. Each graph represents a tweet, and each node is a keyword or a symbol appearing in a tweet. Every node has a one-hot feature representing its keyword or symbol. \u2022 COLLAB. Since the dataset does not contain node features, we compute the degree of every node and assign a one-hot feature vector to each node based on its degree. Thus, the length of feature vectors is the same as the number of unique degrees in the dataset. The same approach was also done in previous work [41] for graph classification."
        },
        {
            "heading": "D ABLATION STUDY FOR NODESAM",
            "text": "We perform an additional ablation study for NodeSam to confirm the effect of the adjustment operation for making unbiased changes of the number of edges. Figure 8 shows box plots for the difference in the number of edges for all variants of NodeSam: (a) SplitOnly, (b) MergeOnly, (c) NodeSamBase, and (d) NodeSam. The detailed information of such variants is discussed in Section 4.4. The figure shows that the split operation increases the number of edges by one in all cases, since it splits a node into two by inserting a single edge, while the merge operation decreases the number of edges by more than one. This is because the merge operation eliminates the triangles that contain both of the target nodes. Our NodeSam with\nthe adjustment operation makes unbiased changes, compensating for the removed edges, compared with NodeSamBase."
        }
    ],
    "title": "Model-Agnostic Augmentation for Accurate Graph Classification",
    "year": 2022
}