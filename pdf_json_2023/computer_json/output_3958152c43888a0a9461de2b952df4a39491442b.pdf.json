{
    "abstractText": "Ever-larger language models with ever-increasing capabilities are by now well-established text processing tools. Alas, information extraction tasks such as named entity recognition are still largely unaffected by this progress as they are primarily based on the previous generation of encoder-only transformer models. Here, we propose a simple yet effective approach, Informed Named Entity Recognition Decoding (iNERD), which treats named entity recognition as a generative process. It leverages the language understanding capabilities of recent generative models in a future-proof manner and employs an informed decoding scheme incorporating the restricted nature of information extraction into open-ended text generation, improving performance and eliminating any risk of hallucinations. We coarse-tune our model on a merged named entity corpus to strengthen its performance, evaluate five generative language models on eight named entity recognition datasets, and achieve remarkable results, especially in an environment with an unknown entity class set, demonstrating the adaptability of the approach.",
    "authors": [
        {
            "affiliations": [],
            "name": "A PREPRINT"
        },
        {
            "affiliations": [],
            "name": "Tobias Deu\u00dfer"
        },
        {
            "affiliations": [],
            "name": "Lars Hillebrand"
        },
        {
            "affiliations": [],
            "name": "Christian Bauckhage"
        },
        {
            "affiliations": [],
            "name": "Rafet Sifa"
        }
    ],
    "id": "SP:95e1470f0c991574d2e7d4e97921fe205f86bfb5",
    "references": [
        {
            "authors": [
                "Ebtesam Almazrouei",
                "Hamza Alobeidli",
                "Abdulaziz Alshamsi",
                "Alessandro Cappelli",
                "Ruxandra Cojocaru",
                "Merouane Debbah",
                "Etienne Goffinet",
                "Daniel Heslow",
                "Julien Launay",
                "Quentin Malartic",
                "Badreddine Noune",
                "Baptiste Pannier",
                "Guilherme Penedo"
            ],
            "title": "Falcon-40B: an open large language model with state-of-the-art performance, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Yejin Bang",
                "Samuel Cahyawijaya",
                "Nayeon Lee",
                "Wenliang Dai",
                "Dan Su",
                "Bryan Wilie",
                "Holy Lovenia",
                "Ziwei Ji",
                "Tiezheng Yu",
                "Willy Chung",
                "Quyet V. Do",
                "Yan Xu",
                "Pascale Fung"
            ],
            "title": "A multitask, multilingual, multimodal evaluation of ChatGPT on reasoning, hallucination, and interactivity, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Daniel M. Bikel",
                "Scott Miller",
                "Richard Schwartz",
                "Ralph Weischedel"
            ],
            "title": "Nymble: a high-performance learning name-finder",
            "venue": "In Proc. Applied Natural Language Processing,",
            "year": 1997
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "In Proc. NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Nigel Collier",
                "Tomoko Ohta",
                "Yoshimasa Tsuruoka",
                "Yuka Tateisi",
                "Jin-Dong Kim"
            ],
            "title": "Introduction to the bio-entity recognition task at JNLPBA",
            "venue": "In Proc. NLPBA/BioNLP,",
            "year": 2004
        },
        {
            "authors": [
                "Michael Collins",
                "Yoram Singer"
            ],
            "title": "Unsupervised models for named entity classification",
            "venue": "In Proc. EMNLP,",
            "year": 1999
        },
        {
            "authors": [
                "Leon Derczynski",
                "Eric Nichols",
                "Marieke van Erp",
                "Nut Limsopatham"
            ],
            "title": "Results of the WNUT2017 shared task on novel and emerging entity recognition",
            "venue": "In Proc. Workshop on Noisy User-generated Text,",
            "year": 2017
        },
        {
            "authors": [
                "Tobias Deu\u00dfer",
                "Maren Pielka",
                "Lisa Pucknat",
                "Basil Jacob",
                "Tim Dilmaghani",
                "Mahdis Nourimand",
                "Bernd Kliem",
                "R\u00fcdiger Loitz",
                "Christian Bauckhage",
                "Rafet Sifa"
            ],
            "title": "Contradiction detection in financial reports",
            "venue": "In Proc. NLDL,",
            "year": 2023
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "In Proc. NAACL-HLT,",
            "year": 2019
        },
        {
            "authors": [
                "Ning Ding",
                "Guangwei Xu",
                "Yulin Chen",
                "Xiaobin Wang",
                "Xu Han",
                "Pengjun Xie",
                "Haitao Zheng",
                "Zhiyuan Liu"
            ],
            "title": "Few-NERD: A few-shot named entity recognition dataset",
            "venue": "In Proc. ACL-IJCNLP,",
            "year": 2021
        },
        {
            "authors": [
                "Chenxiao Dou",
                "Xianghui Sun",
                "Yaoshu Wang",
                "Yunjie Ji",
                "Baochang Ma",
                "Xiangang Li"
            ],
            "title": "Domain-adapted dependency parsing for cross-domain named entity recognition",
            "venue": "In Proc. AAAI,",
            "year": 2023
        },
        {
            "authors": [
                "Rezarta Islamaj Do\u011fan",
                "Robert Leaman",
                "Zhiyong Lu"
            ],
            "title": "Ncbi disease corpus: A resource for disease name recognition and concept normalization",
            "venue": "J. of Biomedical Informatics,",
            "year": 2014
        },
        {
            "authors": [
                "Oren Etzioni",
                "Michael Cafarella",
                "Doug Downey",
                "Ana-Maria Popescu",
                "Tal Shaked",
                "Stephen Soderland",
                "Daniel S. Weld",
                "Alexander Yates"
            ],
            "title": "Unsupervised named-entity extraction from the web: An experimental study",
            "venue": "Artificial Intelligence,",
            "year": 2005
        },
        {
            "authors": [
                "Hao Fei",
                "Shengqiong Wu",
                "Jingye Li",
                "Bobo Li",
                "Fei Li",
                "Libo Qin",
                "Meishan Zhang",
                "Min Zhang",
                "Tat-Seng Chua"
            ],
            "title": "LasUIE: Unifying information extraction with latent adaptive structure-aware generative language model",
            "venue": "In Proc. NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Ralph Grishman",
                "Beth Sundheim"
            ],
            "title": "Message Understanding Conference- 6: A brief history",
            "venue": "In Proc. COLING,",
            "year": 1996
        },
        {
            "authors": [
                "Franz A. Heinsen"
            ],
            "title": "An algorithm for routing vectors in sequences, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Lars Hillebrand",
                "Tobias Deu\u00dfer",
                "Tim Dilmaghani",
                "Bernd Kliem",
                "R\u00fcdiger Loitz",
                "Christian Bauckhage",
                "Rafet Sifa"
            ],
            "title": "KPI-BERT: A joint named entity recognition and relation extraction model for financial reports",
            "venue": "In Proc. ICPR,",
            "year": 2022
        },
        {
            "authors": [
                "Edward J. Hu",
                "Yelong Shen",
                "Phillip Wallis",
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen"
            ],
            "title": "LoRA: Low-rank adaptation of large language models, 2021",
            "year": 2021
        },
        {
            "authors": [
                "Jinhyuk Lee",
                "Wonjin Yoon",
                "Sungdong Kim",
                "Donghyeon Kim",
                "Sunkyu Kim",
                "Chan Ho So",
                "Jaewoo Kang"
            ],
            "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text",
            "venue": "mining. Bioinformatics,",
            "year": 2019
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "In Proc. ICLR,",
            "year": 2019
        },
        {
            "authors": [
                "Ying Luo",
                "Fengshun Xiao",
                "Zhao Hai"
            ],
            "title": "Hierarchical contextualized representation for named entity recognition",
            "venue": "In Proc. AAAI,",
            "year": 2020
        },
        {
            "authors": [
                "Paul McNamee",
                "James Mayfield"
            ],
            "title": "Entity extraction without language-specific resources",
            "venue": "In Proc. COLING,",
            "year": 2002
        },
        {
            "authors": [
                "Ngoc Dang Nguyen",
                "Wei Tan",
                "Wray L. Buntine",
                "Richard Beare",
                "Changyou Chen",
                "Lan Du"
            ],
            "title": "Auc maximization for low-resource named entity recognition",
            "venue": "In Proc. AAAI,",
            "year": 2023
        },
        {
            "authors": [
                "Ildik\u00f3 Pil\u00e1n",
                "Pierre Lison",
                "Lilja \u00d8vrelid",
                "Anthi Papadopoulou",
                "David S\u00e1nchez",
                "Montserrat Batet"
            ],
            "title": "The Text Anonymization Benchmark (TAB): A Dedicated Corpus and Evaluation Framework for Text Anonymization",
            "venue": "Computational Linguistics, 48(4):1053\u20131101,",
            "year": 2017
        },
        {
            "authors": [
                "Sameer Pradhan",
                "Alessandro Moschitti",
                "Nianwen Xue",
                "Hwee Tou Ng",
                "Anders Bj\u00f6rkelund",
                "Olga Uryupina",
                "Yuchen Zhang",
                "Zhi Zhong"
            ],
            "title": "Towards robust linguistic analysis using OntoNotes",
            "venue": "In Proc. CoNLL,",
            "year": 2013
        },
        {
            "authors": [
                "Alec Radford",
                "Karthik Narasimhan",
                "Tim Salimans",
                "Ilya Sutskever"
            ],
            "title": "Improving language understanding by generative pre-training",
            "venue": "https://openai.com/research/language-unsupervised,",
            "year": 2018
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "https://openai.com/research/better-language-models,",
            "year": 2019
        },
        {
            "authors": [
                "Jack W. Rae",
                "Sebastian Borgeaud",
                "Trevor Cai",
                "Katie Millican",
                "Jordan Hoffmann",
                "Francis Song",
                "John Aslanides",
                "Sarah Henderson",
                "Roman Ring",
                "Susannah Young"
            ],
            "title": "Scaling language models: Methods, analysis & insights from training gopher, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Lance Ramshaw",
                "Mitch Marcus"
            ],
            "title": "Text chunking using transformation-based learning",
            "venue": "In Workshop on Very Large Corpora,",
            "year": 1995
        },
        {
            "authors": [
                "Teven Le Scao",
                "Angela Fan",
                "Christopher Akiki",
                "Ellie Pavlick",
                "Suzana Ili\u0107",
                "Daniel Hesslow",
                "Roman Castagn\u00e9",
                "Alexandra Sasha Luccioni",
                "Fran\u00e7ois Yvon",
                "Matthias Gall\u00e9"
            ],
            "title": "BLOOM: A 176b-parameter open-access multilingual language model, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Agam Shah",
                "Ruchit Vithani",
                "Abhinav Gullapalli",
                "Sudheer Chava"
            ],
            "title": "Finer: Financial named entity recognition dataset and weak-supervision model, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Erik F. Tjong Kim Sang",
                "Fien De Meulder"
            ],
            "title": "Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition",
            "venue": "In Proc. Conf. on Natural Language Learning at HLT-NAACL,",
            "year": 2003
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models, 2023a",
            "year": 2023
        },
        {
            "authors": [
                "Hugo Touvron",
                "Louis Martin",
                "Kevin Stone",
                "Peter Albert",
                "Amjad Almahairi",
                "Yasmine Babaei",
                "Nikolay Bashlykov",
                "Soumya Batra",
                "Prajjwal Bhargava",
                "Shruti Bhosale"
            ],
            "title": "Llama 2: Open foundation and fine-tuned chat models, 2023b",
            "year": 2023
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141 ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "In Proc. NeurIPS,",
            "year": 2017
        },
        {
            "authors": [
                "Harsh Verma",
                "Sabine Bergler",
                "Narjesossadat Tahaei"
            ],
            "title": "Comparing and combining some popular NER approaches on biomedical tasks",
            "venue": "In Proc. BioNLP,",
            "year": 2023
        },
        {
            "authors": [
                "Shuhe Wang",
                "Xiaofei Sun",
                "Xiaoya Li",
                "Rongbin Ouyang",
                "Fei Wu",
                "Tianwei Zhang",
                "Jiwei Li",
                "Guoyin Wang"
            ],
            "title": "GPT-NER: Named entity recognition via large language models, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Xinyu Wang",
                "Yong Jiang",
                "Nguyen Bach",
                "Tao Wang",
                "Zhongqiang Huang",
                "Fei Huang",
                "Kewei Tu"
            ],
            "title": "Improving named entity recognition by external context retrieving and cooperative learning",
            "venue": "In Proc. ACL-IJCNLP,",
            "year": 2021
        },
        {
            "authors": [
                "Zihan Wang",
                "Jingbo Shang",
                "Liyuan Liu",
                "Lihao Lu",
                "Jiacheng Liu",
                "Jiawei Han"
            ],
            "title": "CrossWeigh: Training named entity tagger from imperfect annotations",
            "venue": "In Proc. EMNLP-IJCNLP,",
            "year": 2019
        },
        {
            "authors": [
                "Jason Wei",
                "Yi Tay",
                "Rishi Bommasani",
                "Colin Raffel",
                "Barret Zoph",
                "Sebastian Borgeaud",
                "Dani Yogatama",
                "Maarten Bosma",
                "Denny Zhou",
                "Donald Metzler"
            ],
            "title": "Emergent abilities of large language models",
            "venue": "Transactions on Machine Learning Research,",
            "year": 2022
        },
        {
            "authors": [
                "Ronald J. Williams",
                "David Zipser"
            ],
            "title": "A learning algorithm for continually running fully recurrent neural networks",
            "venue": "Neural Computation,",
            "year": 1989
        },
        {
            "authors": [
                "Ikuya Yamada",
                "Akari Asai",
                "Hiroyuki Shindo",
                "Hideaki Takeda",
                "Yuji Matsumoto"
            ],
            "title": "LUKE: Deep contextualized entity representations with entity-aware self-attention",
            "venue": "In Proc. EMNLP,",
            "year": 2020
        },
        {
            "authors": [
                "Hang Yan",
                "Tao Gui",
                "Junqi Dai",
                "Qipeng Guo",
                "Zheng Zhang",
                "Xipeng Qiu"
            ],
            "title": "A unified generative framework for various NER subtasks",
            "venue": "In Proc. ACL-IJCNLP,",
            "year": 2021
        },
        {
            "authors": [
                "Deming Ye",
                "Yankai Lin",
                "Peng Li",
                "Maosong Sun"
            ],
            "title": "Packed levitated marker for entity and relation extraction",
            "venue": "In Proc. ACL,",
            "year": 2022
        },
        {
            "authors": [
                "Shaodian Zhang",
                "No\u00e9mie Elhadad"
            ],
            "title": "Unsupervised biomedical named entity recognition: Experiments with clinical and biological texts",
            "venue": "J. of Biomedical Informatics,",
            "year": 2013
        },
        {
            "authors": [
                "Susan Zhang",
                "Stephen Roller",
                "Naman Goyal",
                "Mikel Artetxe",
                "Moya Chen",
                "Shuohui Chen",
                "Christopher Dewan",
                "Mona Diab",
                "Xian Li",
                "Xi Victoria Lin"
            ],
            "title": "OPT: Open pre-trained transformer language models, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Wayne Xin Zhao",
                "Kun Zhou",
                "Junyi Li",
                "Tianyi Tang",
                "Xiaolei Wang",
                "Yupeng Hou",
                "Yingqian Min",
                "Beichen Zhang",
                "Junjie Zhang",
                "Zican Dong"
            ],
            "title": "A survey of large language models, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Wenxuan Zhou",
                "Muhao Chen"
            ],
            "title": "Learning from noisy labels for entity-centric information extraction",
            "venue": "In Proc. EMNLP,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "Keywords Named Entity Recognition \u00b7 Information Extraction \u00b7 Large Language Models \u00b7 Natural Language Processing \u00b7 Machine Learning"
        },
        {
            "heading": "1 Introduction",
            "text": "Recent public releases of large language models (LLMs) with human-like writing skills have drawn unprecedented attention to natural language processing (NLP). Indeed, the performance of transformer-based LLMs increases notably, and they develop \u201cemergent abilities\u201d, i.e. their performance increases significantly, when their number of parameters exceeds a certain level (Wei et al., 2022).\nOn the other hand, tasks not based on generative transformers, say sentiment analysis, contradiction detection, or named entity recognition, have been relegated to the backseat of this latest push in NLP. As of this writing, they are usually tackled using \u201cencoder-only\u201d1 language models (Heinsen, 2022; Deu\u00dfer et al., 2023; Verma et al., 2023) which are typically much smaller than their \u201cdecoder-only\u201d counterparts.\nHere, we intend to narrow the gap between generative and extractive NLP and introduce a novel named entity recognition (NER) framework. Our Informed Named Entity Recognition Decoding (iNERD) approach has three main features: First, it leverages proven capabilities of \u201cdecoder-only\u201d models. Our current approach works with the latest generative LLMs but can easily incorporate even better models once they become available and thus keep up with rapid release cycles (Zhao et al., 2023) making it future-proof and quick to upgrade.\nSecond, we exploit the extensive pre-training and the resulting language understanding capabilities of state-of-the-art LLMs. Our approach involves an informed decoding algorithm which eliminates any hallucinations current models might suffer from during our approach (Bang et al., 2023) and improves performance by ruling out impossible tokens\n\u2217tdeusser@uni-bonn.de 1\u201cEncoder-only\u201d refers to transformer models which only consist of encoder blocks. This contrasts with the original encoder-\ndecoder structure proposed by Vaswani et al. (2017) or the \u201cdecoder-only\u201d structure of generative models.\nar X\niv :2\n30 8.\n07 79\n1v 1\n[ cs\n.C L\n] 1\n5 A\nug 2\nduring generation. To strengthen the model\u2019s understanding of the NER task, we \u201ccoarse-tune\u201d it on a merged corpus of various task-specific datasets.\nThird, we propose a simple decoding strategy which allows for casting the extractive task of named entity recognition as a generative task. Our idea is to let the model generate extended texts of the following form:\n\u201cEU rejects German call to boycott British lamb. <CT> Organisation <TCS> EU <ES> Location <TCS> German <ES> Location <TCS> British <ES>\u201d,\nHere, the special tokens inside angular brackets signal the start of the entity string (<CombineToken>), separate entity type and entity content (<TypeContentSeparator>), and identify different entities (<EntitySeparator>). During inference, we enforce this structure and thus reduce the complexity of the generation step.\nExtensive evaluations show that this approach achieves remarkable performances in various NER settings ranging from general-purpose over bio-medical to finance.\nIn short, our contributions presented in this paper are the following:\n\u2022 We propose a novel future-proof architecture to cast the extractive process of named entity recognition as a generative one, incorporating natural language understanding capabilities of generative models into the process.\n\u2022 We introduce a novel decoding strategy for such an architecture, which prevents the model from hallucinating and improves performance.\n\u2022 We \u201ccoarse-tune\u201d decoder-only models like Llama (Touvron et al., 2023a) or GPT-2 (Radford et al., 2019) on a merged named entity recognition dataset to further improve the contextual awareness of these models for NER tasks.\n\u2022 We publicly provide our code as well as the weights of our best-performing model2.\nNext, we review recent related work on named entity recognition and generative language models. We then elaborate on our framework, our encoding scheme for named entities, and the corresponding informed decoding. Afterwards, we discuss our experimental protocol and present and discuss the results obtained on eight benchmark datasets. Finally, we summarize our main results and provide an outlook to auspicious future work."
        },
        {
            "heading": "2 Related Work",
            "text": "Named entity recognition (Grishman and Sundheim, 1996) is a fundamental task in text mining and natural language processing. Among others, it allows for anonymization (Pil\u00e1n et al., 2022) or relation extraction (Hillebrand et al., 2022) and, owing to its practical importance, has been studied w.r.t. standardized corpora early on (e.g. the CoNLL-2003 data collected by Tjong Kim Sang and De Meulder (2003)).\nPrior to the deep learning revolution, NER was usually tackled in a rule-based manner (Etzioni et al., 2005) or with unsupervised- or feature-based supervised learning (Collins and Singer, 1999; Zhang and Elhadad, 2013; Bikel et al., 1997; McNamee and Mayfield, 2002).\nIn their seminal paper on BERT, an encoder-only transformer, Devlin et al. (2019) achieved remarkable results on the CoNLL-2003 data by adding a classifier on top of the encoder and fine-tuning the model. Much subsequent work on similar approaches towards NER then focused on improved context awareness. To name but a few, Luo et al. (2020) fused hierarchical contextualized representations with input token embeddings, Lee et al. (2019) applied additional pre-training aimed at biomedical texts, and Wang et al. (2021) added a conditional random field on top of BERT. Going even further, Yamada et al. (2020) forced entity extraction during pre-training and Zhou and Chen (2021) added a co-regularization framework for entity-centric information extraction, to achieve state-of-the-art results. Nevertheless, all of these approaches are built upon an encoder-only transformer model and are unsuited to incorporate the decoder-only transformer architecture powering the recent popularity and success of natural language processing.\nClosest to the ideas proposed in this paper, Yan et al. (2021) formulated NER as an entity span sequence generation task, in which they added special tokens to their vocabulary to then generate entities and their types in an autoregressive fashion. Fei et al. (2022) extended this to cover more tasks in the information extraction field. The advantage of our approach is that we do not add the entity type tokens as special tokens, but as regular tokens already known to the\n2The link to the GitHub repository will be published upon acceptance of this paper.\nmodel. Furthermore, Wang et al. (2023) leveraged the GPT\u20133 (Brown et al., 2020) API to tag entities in a sentence in a zero and few-shot approach.\nGenerative language models gained widespread public interest with the introduction of GPT\u20133 (Brown et al., 2020) and GPT\u20134 (OpenAI, 2023), which both reported impressive language understanding and writing capabilities, but did not make their models and exact architectures known to the research community. On the other hand, their predecessors, GPT\u20132 (Radford et al., 2019) and GPT (Radford et al., 2018), are openly available and were the first to implement a \u201cdecoder-only\u201d architecture, which discarded the Encoder-Decoder structure proposed in Vaswani et al. (2017) in favour of an autoregressive generation process, trained by teacher-forcing.\nIn recent years, this field expanded rapidly, driven by its prominent place in public discourse, and many new models emerged and were studied, e.g. Llama (Touvron et al., 2023a) and its second iteration (Touvron et al., 2023b), RedPajama (Together Computer, 2023), Falcon (Almazrouei et al., 2023), Bloom (Scao et al., 2023), or OPT (Zhang et al., 2022). As already mentioned in the previous section, a critical property of such a large language model (LLM) is that performance experiences a remarkable increase once the model scale, i.e. its parameter size, surpasses a certain threshold, dubbed \u201cemergent abilities of LLMs\u201d, studied in Wei et al. (2022) and Rae et al. (2022). Due to the sheer size of these models, reaching into the hundreds of billions, it is apparent that training and even fine-tuning them is costly and time-consuming. To alleviate this and make the training of pre-trained LLMs accessible to a broader audience, Hu et al. (2021) introduced LoRA, a framework that freezes the pre-trained model weights and injects trainable rank decomposition matrices into the transformer layers.\nRegardless, these LLMs are trained to be capable text generation tools and are, at their current state, mostly incompatible with other NLP tasks like information extraction, a flaw which we alleviate with the iNERD approach introduced in this work."
        },
        {
            "heading": "3 Methodology",
            "text": "Here, we describe how we formulate named entity recognition (NER) as a task suited for generative language models. Following this, we shed light on how our algorithm for Informed Named Entity Recognition Decoding (iNERD) and the complete setup is defined and point out the advantages compared to other approaches."
        },
        {
            "heading": "3.1 Named entity decoding",
            "text": "NER is usually formulated as a \u201ctoken classification\u201d task, as seen in Dou et al. (2023) or Nguyen et al. (2023). In such a setup, an embedding of each token is generated using a text encoder, often an encoder-only transformer model like BERT (Devlin et al., 2019). This embedding is then fed into a classifier, which can be anything from a simple logistic regression to a more involved deep neural network to classify each token as either a part of an entity or not. This prediction generally has to include the entity start and entity end information, which can be achieved with, among others, the IOB tagging scheme (Ramshaw and Marcus, 1995). Figure 1 illustrates this setup.\nIn contrast, we propose to model this task as a generative process, simplifying its machine-learning components to just one building block: a decoder-only transformer model.\nTo formalize this, we define the input I for our generative model during the training phase for n entities e as\nI = Is \u2295 \u03ba\u2295 n\u2223\u2223\u2223\u2223\u2223\u2223 (\u03bee \u2295 \u03c4 \u2295 Ie \u2295 \u03f5)\n= Is \u2295 \u03ba\u2295 E, (1)\nwhere \u2295 is the concatenation operator, Is the actual sentence from which we intend to extract entities, \u03ba the \u201ccombine\u201d token, \u03bee the type of entity e, \u03c4 the \u201ctype-content\u201d separator token, Ie the actual entity string, \u03f5 the \u201centity separator\u201d token, and \u2223\u2223\u2223\u2223n concatenates its input along the number of entities n. This concatenation \u2223\u2223\u2223\u2223n (\u03bee \u2295 \u03c4 \u2295 Ie \u2295 \u03f5) is the entity string E of our input I , i.e. what is unknown during inference and has to be predicted.\nTo make Equation 1 more accessible, we can review the example from the introduction,\n\u201cEU rejects German call to boycott British lamb. <CT> Organisation <TCS> EU <ES> Location <TCS> German <ES> Location <TCS> British <ES>\u201d,\nin which\n\u2022 Is is the input sentence \u201cEU rejects German call to boycott British lamb\u201d, \u2022 \u03ba the string \u201c<CT>\u201d, \u2022 \u03bee the entity types \u201cOrganisation\u201d and \u201cLocation\u201d, \u2022 \u03c4 the string \u201c<TCS>\u201d, \u2022 Ie the actual entity content \u201cEU\u201d, \u201cGerman\u201d and \u201cBritish\u201d, \u2022 \u03f5 the string \u201c<ES>\u201d, \u2022 E the entity string \u201cOrganisation <TCS> EU <ES> Location <TCS> German <ES> Location <TCS> British\n<ES>\u201d\nWe can then fine-tune the pre-trained decoder-only model to predict each token of the input I autoregressively using teacher forcing (Williams and Zipser, 1989), i.e. the causal language modelling task is unchanged for these models. We calculate the loss on all predicted tokens after the \u03ba token.\nCompared to the approach introduced in Yan et al. (2021), the essential advantage of our framework for named entity decoding is that we do not add entity type tokens \u03be as special tokens, but as regular tokens already known to the model. Their approach, where the example sentence above becomes \u201cEU rejects German call to boycott British lamb. <ORG> EU <LOC> German <LOC> British\u201d, loses the meaningful embedding a transformer model has learned for \u03be, i.e. the model has to learn anew what the introduced special tokens mean."
        },
        {
            "heading": "3.2 Informed named entity recognition decoding",
            "text": "In the previous section on Named entity decoding, we only considered the training process, in which we apply teacher forcing to correct the model if it \u201cmakes a mistake\u201d during the generation to accelerate convergence. However, during inference, applying teacher forcing would either be cheating or simply impossible if no ground truth exists.\nNevertheless, we do know quite a bit about what tokens to expect at a certain point during inference, described by these four rules:\n1. After the combine token \u03ba or the entity separator token \u03f5, the entity type token \u03be or the end-of-sequence token has to be predicted.\n2. After predicting the entity type \u03be, the type-content separator \u03c4 has to be predicted. 3. After the type-content separator \u03c4 , any token from the input Is may be predicted (signalling the start of the\nentity e). 4. After a token from the input Is has been predicted, the only allowed tokens for prediction are either the entity\nseparator token \u03f5 (signalling the end of the entity e) or the token following the previous token in the input Is (signalling the continuation of the entity e).\nThese four rules comprise the Informed Named Entity Recognition (iNERD) algorithm, as illustrated in Algorithm 1. This algorithm is implemented as a post-processing step and is executed after the model calculates the score over its vocabulary and before mapping this score to the actual token to be predicted.\nAlgorithm 1 iNERD for a batch size of 1 Input: Scores S with the size of the vocabulary, input IDs I holding the considered sentence and prior predictions Parameters: Combine token \u03ba, entity separator token \u03f5, type content separator token \u03c4 , entity type tokens \u03be Output: Updated scores S with iNERD applied\n1: Let p be the previously predicted token, i.e. the last token in the sequence I . 2: Let g be a boolean value representing if we are in the \u201centity generation phase\u201d, i.e. if in the reversed sequence of I\nwe can find the token \u03c4 before we can find the token \u03f5. 3: Let Is be the sentence considered, i.e. everything of I before the token \u03ba. 4: if p = \u03ba or p = \u03f5 then 5: Mask S to only allow \u03be or the end-of-sequence token. 6: else if p \u2208 \u03be then 7: Mask S to only allow \u03c4 . 8: else if g then 9: if p = \u03c4 then\n10: Mask S to only allow tokens present in Is. 11: else 12: Mask S to only allow the token after p in Is or \u03f5. 13: end if 14: end if 15: return S\nThe advantages of this approach are clear: First, the decoder-only model is unable to hallucinate, as any prediction that does not follow the decoding scheme introduced in Equation 1, is simply masked out, i.e. the score of this token is set to 0. Second, we can apply this model to unseen data and still expect reasonable results if we define our set of entity type tokens \u03be beforehand, as later shown in the Experiments section."
        },
        {
            "heading": "3.3 Complete Model Setup",
            "text": "Now, we can build our model with the blocks introduced in the sections on Named entity decoding and iNERD. First of all, we transform our input to the structure described in Equation 1, which during training contains the entity string E, but becomes\nIInference = Is \u2295 \u03ba (2)\nduring inference. This is passed through the generative language model, which assigns a score s to each token in the vocabulary. The resulting score vector S is the input to the iNERD algorithm, as described in Algorithm 1. This masks out impossible tokens for the current step, resulting in an updated score vector SiNERD, which is then used to calculate the next token by taking the one with the highest score siNERD. This token is concatenated with the input and the whole process is repeated until the model predicts the end-of-sequence token. Figure 2 illustrates this procedure for the first two steps."
        },
        {
            "heading": "4 Experiments",
            "text": "To practically evaluate the merits of our approach, we conducted experiments on eight datasets. Here, we describe our protocol, discuss data and results, and point out the strengths and flaws. We report the performance in micro-F1 in %, which describes the F1 score, the harmonic mean of precision and recall, when we aggregate the results on all samples individually, independent of the classes. On the other hand, the macro-F1 would compute the metric for each class and then average over each of these.\nWe use the model setup introduced in the previous section for all our experiments and compare it to various benchmarks from other approaches. We test various decoder-only language models for this setup, namely the 1.5 billion parameter GPT2\u2013XL (Radford et al., 2019), the 2.7 billion parameter BioMedLM (Bolton et al., 2022), the 3 billion parameter RedPajama (Together Computer, 2023), the 7 billion parameter Falcon (Almazrouei et al., 2023), the 7 and 13 billion parameter Llama (Touvron et al., 2023a), and the 7 billion parameter Llama-2 (Touvron et al., 2023b). Additionally, we apply LoRA (Hu et al., 2021), a framework that freezes the pre-trained model weights while integrating trainable rank decomposition matrices into the transformer layers, to every model with a parameter size above 3 billion.\nOur general approach is as follows. We first coarse-tune (see Section \u201cCoarse-tuning\u201d for more details) each language model on our merged NER dataset. We then evaluate each model without additional fine-tuning on the test set of each dataset, before we fine-tune them on the respective training set and again report the performance on the test set. Additionally, we conduct an ablation study to highlight the improvements of each component of our approach.\nDue to the sheer computational complexity, we only run each experiment once and do not test various seeds to take the average of each run. Furthermore, and for the same reason, we apply no hyperparameter tuning in this scenario. The fixed hyperparameters we used are an (accumulated) batch size of 16, a learning rate of 0.00001 with a weight decay of 0.01 for the adam optimizer with weight decay (Loshchilov and Hutter, 2019). The LoRA configuration, if applicable, is 8 for the rank of the update matrices, 32 for the scaling factor, and 0.1 for dropout. We are certain that the performance of our approach can be further improved if one focuses on a singular dataset and finds the optimal hyperparameter configuration for each dataset, but the computational cost of doing such a hyperparameter search is immense and beyond our financial scope and the general scope of this paper, which aims to point out the general merits of our approach.\nAll experiments were run on a shared GPU cluster outfitted with the 40GB and 80GB versions of the Nvidia A100 GPU, an AMD EPYC 7742 CPU, and 512GB of RAM. The code is implemented in PyTorch and PyTorch Lightning, and the initial model weights were loaded from HuggingFace."
        },
        {
            "heading": "4.1 Data",
            "text": "We train and test on a total of eight datasets to show where our approach demonstrates notable and promising performances. Special attention is places on the most prominent of these eight, the CoNLL\u20132003 dataset (Tjong Kim Sang and De Meulder, 2003) sporting four different entity classes and its second iteration CoNLL++ (Wang et al., 2019), which corrected 5.38% of the apparently wrongly annotated test sentences.\nFurthermore, we include the OntoNotes (Pradhan et al., 2013) and Few\u2013NERD (Ding et al., 2021) datasets, which are similar to CoNLL\u20132003 but have more granular entities (18 and 66 entity classes, respectively). For example, whereas in CoNLL\u20132003, we only have a coarse-grained entity type \u201cPerson\u201d, this is split into eight types in Few-NERD: \u201dActor\u201c, \u201dArtist/Author\u201c, \u201dAthlete\u201c, \u201dDirector\u201c, \u201dPolitician\u201c, \u201dScholar\u201c, \u201dSoldier\u201c, and \u201dOther\u201c.\nGoing a different route, the WNUT-17 (Derczynski et al., 2017) dataset features six different entity classes and focuses on identifying unusual, previously-unseen entities in the context of emerging discussions. We also include three domainspecific datasets, two focusing on biomedical named entities (JNLPBA in Collier et al. (2004) and NCBI-Disease in Dog\u0306an et al. (2014)) and one on financial ones (FiNER-ORD in Shah et al. (2023)). The two bio-medical datasets have five and one different entity classes, respectively, and the financial NER dataset has three.\nThe combined length of this dataset is 290,317 sentences for the training set, 42,016 for the validation set, and 60,477 sentences for the test set."
        },
        {
            "heading": "4.2 Coarse-tuning",
            "text": "As a first step, we merge all training splits of the datasets discussed before and train a language model on the task of predicting the entity string E. We call this step \u201ccoarse-tuning\u201d the pre-trained language model, as we infuse the model with a general sense of \u201cwhat named entities are\u201d. We do not apply iNERD during the validation phase to simplify this step. The results are reported in Table 1.\nIt should be noted that the models have to deal with quite noisy data, as the entity type tokens \u03be are not the same among the datasets. Take the CoNLL-2003 and Few-NERD datasets for example. The former has four different entity type tokens, whereas the latter has 66. Nevertheless, we theorize that by simply letting the model get exposure to the general structure introduced in Equation 1 it can gather valuable insights and might even understand the link between a coarse-grained entity type like \u201cOrganization\u201d (in CoNLL-2003) and its fine-grained subtype \u201cCompany\u201d (in Few-NERD)."
        },
        {
            "heading": "4.3 Results without dataset specific fine-tuning",
            "text": "Looking at Table 2, it becomes apparent that strong performances across datasets are attainable without applying specific fine-tuning on the respective training dataset. An interesting observation is that a larger model size does not consistently yield improved performance outcomes. Our largest studied model, the 13-billion parameter version of Llama, can mostly beat its smaller sister, the 7-billion version, but is largely overcome by the drastically smaller RedPajama (3-billion parameter). We theorize that the most likely explanation for this phenomenon is that during coarse-tuning, we apply LoRA to both Llama models to be able to train them in a reasonable time frame, which reduces the number of trainable parameters drastically. Therefore, for datasets with many entity classes \u03be, like Few-NERD and OntoNotes, models with LoRA applied struggle to learn the subtle nuances between different classes and thus fail to outperform smaller models, likely because their available updateable parameter size is simply too small to fit these nuances.\nAnother insight is that pre-training on a specific domain helps the model during named entity decoding immensely, as shown in the performance of the BioMedLM model. We see this as a vast opportunity for domain-specific pre-training of generative language models to make smaller models usable for the iNERD approach.\nEven though the performances reported are not zero-shot, as a small part of the coarse-tuning dataset consists of the training dataset of the respective dataset, this still demonstrates the impressive capabilities of such a model, the coarse-tuning routine, and the iNERD algorithm, as later shown in the ablation study."
        },
        {
            "heading": "4.4 Fine-tuning results",
            "text": "After evaluating the iNERD approach on its capabilities after coarse-tuning, we further fine-tune it on each dataset. The results of this can be seen in Table 3. In there, we also report various competing approaches and their performances, taken from the respective papers.\nA first observation is that iNERD is capable of performing on par with or better than the standard encoder-only approach reported for the BERT (Devlin et al., 2019) model. A more general observation is that iNERD performs considerably well on datasets with a smaller entity class size, like CoNLL-2003 or NCBI-Disease. For our main focus, the datasets CoNLL-2003 and its corrected version CoNLL++, iNERD is able to be almost on par with competing state-of-the-art encoder-only approaches (Ye et al., 2022; Wang et al., 2019), which are complex implementations and are thus in stark contrast to our simple and still effective approach.\nOn the one hand, it struggles especially on Few-NERD and OntoNotes, where the entity class size is significantly larger. Furthermore, the fine variations of various bio-medical terms in JNLPBA and the novel entities in WNUT-17 seem also to be a considerable hurdle for our approach. Of course, one could have simply excluded these datasets from this study, but we want to point out fields where our approach is struggling, where it might be improved upon with further research, and therefore, not simply ignore possible drawbacks of our method.\nNevertheless, on the other hand, we surpass the current best-performing model on FiNER-ORD, beating it by a considerable margin of more than 4% F1 and establishing a new state-of-the-art for financial named entity recognition on this dataset.\nIn total, the results of our approach are promising for the concept of using generative language models for tasks that they are not originally intended for, as we show that our relatively simple approach can surpass the comparatively simple one proposed in Devlin et al. (2019)."
        },
        {
            "heading": "4.5 Ablation study",
            "text": "To show the advantages of each component of our approach, we conduct an ablation study on the CoNLL-2003 and CoNLL++ datasets. The results are shown in Table 4.\nAs seen there, each component of the iNERD approach adds to the overall performance. If we subtract the coarsetuning as well as informed decoding steps, the micro-F1 score falls to a paltry but expected 0% for the no fine-tuning\nenvironment, similarly when we only exclude the coarse-tuning step. Not so momentous, but still significant, the informed decoding described in Algorithm 1 adds around 4% improvement for both datasets.\nA similar, but not so severe, picture can be observed during fine-tuning, where the distance between each step subtracted shrinks, but is still present. In such a setting, we observe an overall improvement of more than 1% for the CoNLL-2003 and CoNLL++ datasets when we compare the complete approach to the one with all components turned off."
        },
        {
            "heading": "5 Conclusion",
            "text": "We introduced a novel approach for named entity recognition (NER) which leverages the outstanding language understanding capabilities of modern large language models (LLMs). Our Informed Named Entity Recognition Decoding (iNERD) algorithm is easy to implement and arguably as simple as an \u201cencoder-only\u201d transformer plus multilayer-perceptron classifier approach as proposed in the seminal BERT (Devlin et al., 2019) paper. It builds on top of recent LLMs and is thus future-proof, as the employed LLMs can easily be replaced by improved models whenever they become available. It furthermore incorporates an informed decoding scheme which further improves performance, eliminates any risk of hallucinations, and significantly increases the adaptability. This informed scheme leverages the named entity decoding structure proposed herein to mask out disallowed tokens during the prediction phase.\nExtensive experimental validation shows the performance of our framework to be mostly on par with competing \u201cencoder-only\u201d approaches, if not better. Experiments further reveal considerable and outstanding adaptive capabilities and show that iNERD can react to changes in the underlying data distribution without any additional fine-tuning. This contrasts said \u201cencoder-only\u201d approaches, which dominate the current NER landscape, as these have to be retrained whenever their set of entity classes changes.\nAn obvious next step is testing the largest generative language models, like the 70 billion parameter version of Llama, the 40 billion parameter version of Falcon, or even the 176 billion parameter version of Bloom. Using these could improve the performance of the complete iNERD setup on each dataset even further. On the other hand, training these huge model variations is extremely expensive and beyond our current computational capabilities. As already discussed in the Results section, applying LoRA, a method to freeze certain parts of the model to allow training large models, likely leads to a performance decrease. This is yet another interesting path to take for future research, as one could try pre-training large language models without this technique to improve the downstream performance further. Similarly, one could increase the size of the coarse-tuning dataset and include even more datasets.\nAnother promising starting point for future research is investigating how the various highly specialized named entity recognition techniques developed for encoder-only models like PL-Marker (Ye et al., 2022) or Co-Regularization (Zhou and Chen, 2021) can be applied to generative language models and iNERD to improve the performance further.\nDifferent information extraction tasks like relation extraction or event identification are also clear candidates for future research, which we plan to tackle in a similar manner as iNERD, as these tasks are \u201crigid\u201d like NER and would thus profit from an informed approach like the one we propose.\nFrom a more practical standpoint, we plan to implement the iNERD approach in various real-world applications in the world of Financial Auditing and Bio-Medicine, for the advantages of our approach are clear: highly effective on unseen data with a variable entity set \u03be (see Table 2) and easily upgradeable with the newest large language model."
        },
        {
            "heading": "Acknowledgments",
            "text": "This research has been funded by the Federal Ministry of Education and Research of Germany and the state of North-Rhine Westphalia as part of the Lamarr-Institute for Machine Learning and Artificial Intelligence.\nWe would like to thank our colleagues and friends Armin Berger, Kostadin Cvejoski, Leonhard David, Maren Pielka, and Rajkumar Ramamurthy for providing valuable feedback on this paper."
        }
    ],
    "title": "INFORMED NAMED ENTITY RECOGNITION DECODING FOR GENERATIVE LANGUAGE MODELS",
    "year": 2023
}