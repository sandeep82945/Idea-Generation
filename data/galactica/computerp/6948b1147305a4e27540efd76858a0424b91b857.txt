Pre-trained Language Models (PLMs), as parametric-based eager learners, have become the de-facto choice for current paradigms of Natural Language Processing (NLP). In contrast, kNearest-Neighbor (k-NN) classifiers, as the lazy learning paradigm, tend to mitigate over-fitting and isolated noise. In this paper, we revisit k-NN classifiers for augmenting the PLMs-based classifiers. From the methodological level, we propose to adopt k-NN with textual representations of PLMs in two steps: (1) Utilize k-NN as prior knowledge to calibrate the training process. (2) Linearly interpolate the probability distribution predicted by k-NN with that of the PLMs’ classifier. At the heart of our approach is the implementation of k-NN-calibrated training, which treats predicted results as indicators for easy versus hard examples during the training process. From the perspective of the diversity of application scenarios, we conduct extensive experiments on fine-tuning, prompt-tuning paradigms and zero-shot, few-shot and fully-supervised settings, respectively, across eight diverse end-tasks. We hope our exploration will encourage the community to revisit the power of classical methods for efficient NLP1. 2 related work k-NN in the era of PLMs. The k-Nearest Neighbor (kNN) classifier is a classic non-parametric algorithm that predicts based on representation similarities. While kNN has lost some visibility compared to current deep learning approaches in recent years, it has not fallen off the radar completely. In fact, kNN has been used to enhance pre-trained language models (PLMs) in various tasks, such as unconditional language modeling (Khandelwal et al., 2020; He et al., 2021), machine translation (Khandelwal et al., 2021; Gu et al., 2018), and question answering (Kassner and Schütze, 2020). Most recently, (Alon et al., 2022; Meng et al., 2021) further respectively propose automaton-augmented and GNN-augmented retrieval to alleviate the computationally costly datastore search for language modeling. However, previous researchers (He et al., 2021; Khandelwal et al., 2021; Kassner and Schütze, 2020; Li et al., 2021; Meng et al., 2021; Alon et al., 2022; Zhang et al., 2022) mainly focus on generative tasks or adopt simple interpolation strategies to combine k-NN PLMs only at test time. (Shi et al., 2022) propose to leverage k-NN for zero-shot inference. Revisiting k-NN for PLMs. Unlike them, we focus on empirically demonstrating that incorporating k-NN improves PLMs across a wide range of NLP tasks in fine-tuning and prompt-tuning paradigms on various settings, including the fully-supervised, few-shot and zero-shot settings. Note that our work is the first to comprehensively explore k-NN during both the training and inference process further for fruitful pairings: in addition to the approaches mentioned above, we propose to regard the distribution predicted by k-NN as the prior knowledge for calibrating training, so that the PLM will attend more to the examples misclassified by k-NN. 3 methodology The overall framework is presented in Figure 2. We regard the PLM as the feature extractor that transforms the input textual sequence x into an instance representation x with dimensions D. We revisit k-NN in §3.1 and then introduce our method to integrate k-NN with tuning paradigms in §3.2. 3.1 nearest neighbors revisited Given the training set of n labeled sentences {x1, . . . , xn} and a set of target labels {y1, . . . , yn}, y ∈ [1, C], the k-NN classifier can be illustrated in the next three parts:
Feature Representations For k-NN, we firstly have to collect the corresponding set of features D = {x1, . . . ,xn} from the training set. Concretely, we assign x with the embedding of the [CLS] token of the last layer of the PLM for the fine-tuning procedure. More specifically, we define the feature representations as follows:
x = h[CLS], (1)
The feature representation q of a query example xq also follows the above equation. Retrieve k Neighbors Following the commonly practiced in k-NN (Friedman, 2017; Wang et al., 2019), we pre-process both q and features in the training set D with l2-normalization. We then compute the similarity between the query q and each example in D with Euclidean distance as : d(q,x), ∀x ∈ D, where d(·, ·) is the Euclidean distance calculation function. According to the similarity, we select the top-k representations from D, which are the closest in the distance to q in the embedding space. Similarity-based Aggregation Let N donate the set of retrieved top-k neighbors, and Ny be the subset of N where the whole examples have the same class y. Then the k-NN algorithm converts the top-k neighbors to q and the corresponding targets into a distribution over C labels. The probability distribution of q being predicted as c is:
pkNN(c|q) = ∑
x∈Ny exp (−d(q,x)/τ)∑ y∈C ∑ x∈Ny exp (−d(q,x)/τ) , (2)
where τ is the hyper-parameter of temperature. 3.2 Comprehensive Exploiting of k-NN In this section, we propose to comprehensively leverage the k-NN, the representative of lazy learning, to augment the PLM-based classifier. Role of k-NN as Prior Knowledge for Calibrating Training. As k-NN can easily make predictions for each query instance encountered without any training, it is intuitive to regard its predictions as priors to guide the network in focusing on hard examples during the training process of language models. We distinguish between easy and hard examples based on the results of k-NN. Given the probability distribution pkNN of q being predicted as true label y, we propose to adjust the relative loss for the
correctly-classified or misclassified instances identified by k-NN, in order to reweight the cross-entropy loss LCE . Specifically, we define the calibrated training loss LJ as:
LU = (1 + f(pkNN))LCE , (3)
where f(pkNN) donates the modulating factor 1 for calibration. We are inspired by Focal-loss (Lin et al., 2018) to employ the modulating factor, while our focus is on exploring the application of k-NN in the fine-tuning of PLMs. Intergrating k-NN into Inference Let PM denote the class distribution predicted by the PLM, and PkNN be the class distribution predicted by a k-NN classifier. Then, the PM is reformulates by interpolating the non-parametric k nearest neighbor distribution PkNN using parameter λ (Khandelwal et al., 2020) to calculate the final probability PU of the label as:
PU = λPkNN + (1− λ)PM, (4)
where λ ∈ [0, 1] is an adjustable hyper-parameter. 4 experiments  4.1 datasets We choose a variety of NLP tasks to evaluate our proposed methods, including sentiment analysis task (SST-5 (Socher et al., 2013)), question classification task (TREC (Voorhees and Tice, 2000)), NLI tasks (MNLI (Williams et al., 2018) and QNLI (Rajpurkar et al., 2016)), sentence-pair classification task (BoolQ (Clark et al., 2019) and CB (De Marneffe et al., 2019) ), and information extraction tasks (SemEval (Hendrickx et al., 2010) and TACREV (Alt et al., 2020)). We also list a detailed conclusion and future work In this paper, we propose a novel method to enhance PLM-based classifiers using k-NN. Specifically, we introduce a calibration process and linear interpolation of inference phrases to effectively integrate k-NN into the training pipeline. To evaluate the effectiveness of our approach, we conduct a comprehensive and in-depth analysis of the role of k-NN in various NLU tasks and tuning paradigms. Our results demonstrate that the integration of k-NN is flexible and can significantly enhance the performance of large models.