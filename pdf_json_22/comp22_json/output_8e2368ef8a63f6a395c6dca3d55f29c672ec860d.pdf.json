{
    "abstractText": "Solfeggio is an important basic course for music majors, and audio recognition training is one of the important links. With the improvement of computer performance, audio recognition has been widely used in smart wearable devices. In recent years, the development of deep learning has accelerated the research process of audio recognition. However, there is a lot of sound interference in music teaching environment, which leads to the performance of the audio classi\u0085er that cannot meet the actual demand. In order to solve this problem, an improved audio recognition system based on YOLO-v4 is proposed, which mainly improves the network structure. First, Mel frequency cepstrum number is used to process the original audio and extract the corresponding features. \u0090en, try to apply the YOLO-v4 model in the \u0085eld of deep learning to the \u0085eld of audio recognition and improve it by combining with the spatial pyramid pool module to strengthen the generalization ability of data in di\u008cerent audio formats. Second, the stacking method in ensemble learning is used to fuse the independent submodels of two di\u008cerent channels. Experimental results show that compared with other deep learning technologies, the improved YOLO-v4 model can improve the performance of audio recognition, and it has better performance in processing data of di\u008cerent audio formats, which shows better generalization ability.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yun Cui"
        },
        {
            "affiliations": [],
            "name": "Fu Wang"
        }
    ],
    "id": "SP:a86263c3cbf53172a069295951743bb624192454",
    "references": [
        {
            "authors": [
                "S. Wu",
                "D. Zhang",
                "Z. Zhang",
                "N. Yang",
                "M. Li"
            ],
            "title": "and M",
            "venue": "Zhou, \u201cDependency-to-Dependency neural machine translation,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 26, no. 11, pp. 2132\u20132141",
            "year": 2018
        },
        {
            "authors": [
                "J. Zou",
                "W. Li",
                "C. Chen"
            ],
            "title": "and Q",
            "venue": "Du, \u201cScene classification using local and global features with collaborative representation fusion,\u201d Information Sciences, vol. 348, no. 2, pp. 209\u2013226",
            "year": 2016
        },
        {
            "authors": [
                "H. Phan",
                "L. Hertel",
                "M. Maass",
                "P. Koch",
                "R. Mazur"
            ],
            "title": "and A",
            "venue": "Mertins, \u201cImproved audio scene classification based on label-tree embeddings and convolutional neural networks,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 25, no. 6, pp. 1278\u20131290",
            "year": 2017
        },
        {
            "authors": [
                "S. Bayatli"
            ],
            "title": "Unsupervised weighting of transfer rules in rulebased machine translation using maximum-entropy approach,",
            "venue": "Journal of Information Science and Engineering,",
            "year": 2020
        },
        {
            "authors": [
                "A. Rakotomamonjy"
            ],
            "title": "Supervised representation learning for audio scene classification,",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,",
            "year": 2017
        },
        {
            "authors": [
                "W. Yang",
                "S. Krishnan"
            ],
            "title": "Combining temporal features by local binary pattern for acoustic scene classification,",
            "venue": "IEEE/ ACM Transactions on Audio, Speech, and Language Processing,",
            "year": 2017
        },
        {
            "authors": [
                "A.S. Dhanjal",
                "W. Singh"
            ],
            "title": "An automatic machine translation system for multi-lingual speech to Indian sign language,",
            "venue": "Multimedia Tools and Applications,",
            "year": 2021
        },
        {
            "authors": [
                "M.A. . Alamir"
            ],
            "title": "A novel acoustic scene classification model using the late fusion of convolutional neural networks and different ensemble classifiers,",
            "venue": "Applied Acoustics,",
            "year": 2020
        },
        {
            "authors": [
                "J.G. Makin",
                "D.A. Moses"
            ],
            "title": "and E",
            "venue": "F. Chang, \u201cMachine translation of cortical activity to text with an encoder\u2013decoder framework,\u201d Nature Neuroscience, vol. 23, no. 4, pp. 575\u2013582",
            "year": 2020
        },
        {
            "authors": [
                "S. Waldekar",
                "G. Saha"
            ],
            "title": "Two-level fusion-based acoustic scene classification,",
            "venue": "Applied Acoustics,",
            "year": 2020
        },
        {
            "authors": [
                "J. Sangeetha",
                "R. Hariprasad"
            ],
            "title": "and S",
            "venue": "Subhiksha, \u201cAnalysis of machine learning algorithms for audio event classification using Mel-frequency cepstral coefficients,\u201d Applied Speech Processing, vol. 27, no. 3, pp. 175\u2013189",
            "year": 2021
        },
        {
            "authors": [
                "M. Blochberger",
                "F. Zotter"
            ],
            "title": "Particle-filter tracking of sounds for frequency-independent 3D audio rendering from distributed B-format recordings,",
            "venue": "Acta Acustica,",
            "year": 2021
        },
        {
            "authors": [
                "P. Yu",
                "S. Zhang",
                "X. Feng",
                "Z. Liu"
            ],
            "title": "and Y",
            "venue": "Shen, \u201cSelecting program material by audio features for low-frequency perceptual evaluation of loudspeakers,\u201d Applied Sciences, vol. 11, no. 5, pp. 2302\u20132311",
            "year": 2021
        },
        {
            "authors": [
                "Y. Xi",
                "Q. Li",
                "M. Zhang",
                "L. Liu"
            ],
            "title": "and J",
            "venue": "Wu, \u201cCharacterizing the time-varying brain networks of audiovisual integration across frequency bands,\u201d Cognitive Computation, vol. 12, no. 6, pp. 1154\u20131169",
            "year": 2020
        },
        {
            "authors": [
                "C.P. Dadula",
                "E.P. Dadios"
            ],
            "title": "Fuzzy logic system for abnormal audio event detection using Mel frequency cepstral coefficients,",
            "venue": "Journal of Advanced Computational Intelligence and Intelligent Informatics,",
            "year": 2017
        },
        {
            "authors": [
                "K.W. Church"
            ],
            "title": "Emerging trends: APIs for speech and machine translation and more,",
            "venue": "Natural Language Engineering,",
            "year": 2018
        },
        {
            "authors": [
                "L. Yang",
                "H. Zhao"
            ],
            "title": "Sound classification based on multihead attention and support vector machine,",
            "venue": "Mathematical Problems in Engineering,",
            "year": 2021
        },
        {
            "authors": [
                "A. Greco",
                "N. Petkov",
                "A. Saggese"
            ],
            "title": "andM",
            "venue": "Vento, \u201cAReN: a deep learning approach for sound event recognition using a brain inspired representation,\u201d IEEE Transactions on Information Forensics and Security, vol. 15, no. 1, pp. 3610\u20133624",
            "year": 2020
        },
        {
            "authors": [
                "F. Demir",
                "M. Turkoglu",
                "M. Aslan"
            ],
            "title": "and A",
            "venue": "Sengur, \u201cA new pyramidal concatenated CNN approach for environmental sound classification,\u201d Applied Acoustics, vol. 170, no. 6, pp. 107520\u2013108116",
            "year": 2020
        },
        {
            "authors": [
                "Q. Zhu",
                "H. Zheng",
                "Y. Wang",
                "Y. Cao"
            ],
            "title": "and S",
            "venue": "Guo, \u201cStudy on the evaluation method of sound phase cloud maps based on an improved YOLOv4 algorithm,\u201d Sensors, vol. 20, no. 15, pp. 4314\u20134322",
            "year": 2020
        },
        {
            "authors": [
                "A. Venturini",
                "L. Zao",
                "R. Coelho"
            ],
            "title": "On speech features fusion",
            "venue": "\u03b1-integration Gaussian modeling and multi-style training for noise robust speaker classification,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 22, no. 12, pp. 1951\u20131964",
            "year": 2014
        },
        {
            "authors": [
                "J. Zhang",
                "C. Zong"
            ],
            "title": "Deep neural networks in machine translation: an overview,",
            "venue": "IEEE Intelligent Systems,",
            "year": 2015
        },
        {
            "authors": [
                "P.L. Son"
            ],
            "title": "On the design of sparse arrays with frequencyinvariant beam pattern,",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "S. Ketu",
                "P.K. Mishra"
            ],
            "title": "India perspective: CNN-LSTM hybrid deep learning model-based COVID-19 prediction and current status of medical resource availability,",
            "venue": "Soft Computing,",
            "year": 2022
        },
        {
            "authors": [
                "M. Jia",
                "Y. Wu",
                "C. Bao"
            ],
            "title": "and C",
            "venue": "Ritz, \u201cMulti-source DOA estimation in reverberant environments by jointing detection and modeling of time-frequency points,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, no. 2, pp. 379\u2013392",
            "year": 2021
        },
        {
            "authors": [
                "M.S. Akhtar",
                "P. Sawant",
                "S. Sen",
                "A. Ekbal"
            ],
            "title": "and P",
            "venue": "Bhattacharyya, \u201cImproving word embedding coverage in less-resourced languages through multi-linguality and crosslinguality: a case study with aspect-based sentiment analysis,\u201d ACM Transactions on Asian and Low-Resource Language Information Processing, vol. 18, no. 2, pp. 1\u201322",
            "year": 2019
        },
        {
            "authors": [
                "G. Pepe",
                "L. Gabrielli",
                "S. Squartini"
            ],
            "title": "and L",
            "venue": "Cattani, \u201cDesigning audio equalization filters by deep neural networks,\u201d Applied Sciences, vol. 10, no. 7, pp. 2483\u20132491",
            "year": 2020
        },
        {
            "authors": [
                "X. Sun",
                "T. Liu",
                "X. Yu"
            ],
            "title": "and B",
            "venue": "Pang, \u201cUnmanned surface vessel visual object detection under all-weather conditions with optimized feature fusion network in YOLOv4,\u201d Journal of Intelligent and Robotic Systems, vol. 103, no. 3, pp. 55\u201372",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "Research Article",
            "text": ""
        },
        {
            "heading": "Research on Audio Recognition Based on the Deep Neural",
            "text": ""
        },
        {
            "heading": "Network in Music Teaching",
            "text": "Yun Cui 1 and Fu Wang 1,2\n1School of Music and Performing Arts, Mianyang Teachers\u2019 College, Mianyang 621000, China 2College of Management Science, Chengdu University of Technology, Chengdu 610059, China\nCorrespondence should be addressed to Fu Wang; fwang@mtc.edu.cn\nReceived 30 March 2022; Revised 25 April 2022; Accepted 6 May 2022; Published 27 May 2022\nAcademic Editor: Le Sun\nCopyright \u00a9 2022 Yun Cui and Fu Wang. is is an open access article distributed under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.\nSolfeggio is an important basic course for music majors, and audio recognition training is one of the important links. With the improvement of computer performance, audio recognition has been widely used in smart wearable devices. In recent years, the development of deep learning has accelerated the research process of audio recognition. However, there is a lot of sound interference in music teaching environment, which leads to the performance of the audio classi er that cannot meet the actual demand. In order to solve this problem, an improved audio recognition system based on YOLO-v4 is proposed, which mainly improves the network structure. First, Mel frequency cepstrum number is used to process the original audio and extract the corresponding features. en, try to apply the YOLO-v4 model in the eld of deep learning to the eld of audio recognition and improve it by combining with the spatial pyramid pool module to strengthen the generalization ability of data in di erent audio formats. Second, the stacking method in ensemble learning is used to fuse the independent submodels of two di erent channels. Experimental results show that compared with other deep learning technologies, the improved YOLO-v4 model can improve the performance of audio recognition, and it has better performance in processing data of di erent audio formats, which shows better generalization ability."
        },
        {
            "heading": "1. Introduction",
            "text": "Music is an abstract art form with sound as its means of expression. In the process of music teaching, solfeggio can strengthen students\u2019 musical memory ability, enable students to accurately identify music works, and thus obtain better \u201cmusical perception.\u201d As an important link in solfeggio, audio recognition training is very di cult for junior students. is is because students need to master all kinds of clefs, distinguish the length and duration represented by di erent notes, and the pitch di erence between di erent notes.\nAudio signal analysis based on embedded intelligent devices has attracted more and more researchers\u2019 attention [1\u20137]. Intelligent wearable devices with audio recognition function can help students solve the above problems and realize music teaching assistance. e task of audio\nrecognition needs to preprocess the collected audio signals rst, extract useful features for distinguishing music scores from them, and nally classify them according to these features. Classi cation is a very important method of data mining [8\u201310]. Classi cation refers to generating a classication function according to certain rules on the basis of training set data. is function can map the data of the test set to one of the given categories, thus realizing the category prediction of unknown data. At present, common classi ers include decision tree, logistic regression, support vector machine (SVM), Naive Bayes, k-nearest neighbor algorithm (KNN), BP neural network, and deep learning [11\u201313]. e previous machine learning methods often need to manually extract the features that can represent the original data as the input of the classi er. However, deep learning can automatically extract the high-dimensional features of samples (without manual feature extraction), as long as the\nHindawi Computational Intelligence and Neuroscience Volume 2022, Article ID 7055624, 8 pages https://doi.org/10.1155/2022/7055624\ninput data cover the information of the original data as much as possible, which is suitable for large-scale data. *e deep learning method can realize specific audio recognition tasks with the help of a large amount of audio data collected by intelligent devices. *e convolutional neural network (CNN), as a kind of deep learning architecture, is widely used in image classification, speech recognition, natural language processing, and other fields because of its superior performance in local feature learning [14]. Different from other neural network models (such as Boltzmann machine and recurrent neural network), the CNN characterized in that core operation is convolution operation. *e YOLO network draws lessons from the CNN classification network structure and shows good advantages in the field of image recognition, which has attracted the attention of many researchers.\n*erefore, this study tries to apply the YOLO-v4 model to the field of audio recognition and improves its network structure. In addition, the stacking method in ensemble learning is used to fuse two independent submodels of different channels, and the classification performance of the fused system is further improved compared with the single submodel."
        },
        {
            "heading": "2. Related Works",
            "text": "Nowadays, with the emergence of a large number of smart devices, the excellent computer performance and the development of deep learning technology have jointly promoted the research process in the audio field. Combined with the main research contents of this study, the current research status will be introduced from two aspects: convolutional neural network and audio recognition.\n*e convolutional neural network structure originated from a study by Yann LeCun in 1998 is called the Le Net-5 artificial neural network. *e convolutional neural network, like other neural networks, can be trained by the back propagation algorithm [15]. In 2012, Alex Krizhevsky and others adopted CNN technology for the first time in complex computer vision tasks. By using 3 fully connected layers, 5 convolution layers, and Softmax classifier, a convolutional neural network with 8 layers is constructed, which is named AlexNet. AlexNet uses ReLU activation function, and at the same time, it also uses regularization (dropout) to prevent overfitting. In 2014, the Google\u2019 computer vision team puts forward the GoogLeNet network [16], with a network depth of 22 layers, which contains a new structure, incident. It integrates the features of different depths and the same scale, and the detection accuracy is improved. On the basis of the GoogLeNet network, YOLO and SSD algorithms appeared. Both methods are based on a single end-to-end network, which can complete the input from the original image to the output of the object position and category.\nIn the aspect of audio recognition, Yang and Zhao [17] proposed an acoustic scene classification method based on the support vector machine (SVM), which enhanced the sound texture to improve the classification accuracy. Greco et al. [18] proposed a voice recognition system based on the heuristic deep learning method. Demir et al. [19] proposed a\nnew pyramid cascade CNN method for environmental sound classification. Zhu et al. [20] proposed an improved YOLO-v4 algorithm for sound imaging instruments, which effectively improved the accuracy of acoustic phase cloud image detection. *e above methods all show excellent performance in dealing with audio recognition tasks in a single acoustic scene, but there are many sound disturbances in the music teaching environment, and it is necessary to deal with a variety of different audio format data.\n*erefore, this study proposes an audio recognition system based on the improved YOLO-v4 network model. *e main innovations and contributions include the following: (1) try to apply YOLO-v4 network architecture, which is excellent in the field of deep learning, to the field of audio recognition, and improve it by combining the spatial pyramid pool module. *e improved YOLO-v4 network architecture effectively utilizes the spatial information in audio files, thus strengthening the generalization ability of data in different audio formats. (2) *e stacking method in ensemble learning is used to fuse two independent submodels of different channels, and the classification performance of the fused system is improved."
        },
        {
            "heading": "3. Extraction and Processing of Audio Features",
            "text": "Extracting the best parameter representation of audio signal is one of the important tasks to produce better recognition performance. *e feature extraction in this stage is very important for the classifier classification in the next stage because it will directly affect the classification efficiency.\nIn the classification task, especially the audio classification task, the Mel frequency cepstrum coefficient (MFCC) which describes the spectral shape has a long history. Although the MFCC extraction process will cause lossy compression of data, its classification and recognition effect are quite available even when the data rate is very low. In addition, compared with other classification features, MFCC is widely used because it is more in line with the auditory frequency response curve of human ears.\n*e reason why human beings can judge different environments in complex sound environment lies in the credit of the cochlea.*e cochlea can be seen as a filter bank to help people filter 20-20 kHz audio. *e problem is that the sensitivity of the cochlea to frequencies in the auditory range is not linear, but there is a mapping relationship. MFCC can simulate the frequency response of the human ear. MFCC feature extraction consists of seven steps, and the whole process is shown in Figure 1.\nCommon audio signals have the phenomenon that the low-frequency energy is large, but the high-frequency energy is small. If it is transmitted directly, it will lead to high signalto-noise ratio at low frequency and insufficient signal-tonoise ratio at high frequency. In order to make up for this loss of audio signal during transmission, preemphasis is introduced to compensate the input signal, so that the highfrequency characteristics of audio signal can be highlighted. Preemphasis is usually achieved by means of a high-pass filter [21\u201323].\nLet the voice sample value at the nth time beX[n], and the result after preemphasis is\nY[n] \ufffd X[n] \u2212 aX[n \u2212 1], (1)\nwhere a is the preemphasis coefficient, usually within 0.9- 1.0.\nFraming divides audio samples obtained from analog-todigital conversion (ADC) into small frames with a length in the range of 20\u201340 milliseconds. After preemphasis and framing are completed, it is necessary to add a Hamming window to each frame. Windowing is to control the amount of data processing, and only the data in the window are processed at a time. *e frequency range in the fast Fourier transform spectrum is very wide, which leads to the speech signal not following the linear scale [24\u201326]. *erefore, it is necessary to pass the Mel scale filter bank as shown in Figure 2.\nFigure 2 shows a set of triangular filters, which are used to calculate the weighted sum of the spectral components of the filters, so that the processed output approximates Mel scale. *e amplitude-frequency response of each filter is triangular. *e Mel spectrum of a given frequency f is calculated as follows:\nF(Mel) \ufffd 2595 \u00b7 log10 1 + f\n700  . (2)\nDiscrete cosine transform (DCT) transforms the Mel spectrum into time domain. *e result of the transforms is calledMel frequency cepstrum coefficient.*e coefficient set is called acoustic vector. *erefore, each input is converted into an audio vector sequence.\nIn order to improve the signal recognition performance, the differential spectrum based on the static characteristics of audio signals is used to describe the dynamic characteristics of audio signals. 13 first-order difference features and 39 second-order difference features are introduced. *e frame energy of signal x in the window from time t1 to t2 is as follows:\nEnergy \ufffd  t2\nt\ufffdt1\nX 2 (t). (3)\n13 first-order differential features represent the changes between frames of cepstrum in MFCC features, while 39 second-order differential features represent the changes between frames in first-order differential features. *e firstorder difference is calculated as follows:\nd(n) \ufffd c(n + 1) \u2212 c(n \u2212 1)\n2 , (4)\nwhere c(n+ 1) represents the cepstrum coefficient at time n+ 1."
        },
        {
            "heading": "4. SPP-YOLO-v4 Network Structure",
            "text": "4.1. Spatial Pyramid Pool (SPP) Module. SPP can avoid information distortion caused by scaling, stretching, clipping, and other operations and provide output that is not affected by the input size, which cannot be achieved by sliding window pooling technology [27]. Second, SPP can pool with multiple scales, while sliding window pooling only uses one window scale. *e basic structure of the SPP module is shown in Figure 3. It can be seen that because the input size is flexible, SPP can combine the features of data in different audio formats. *e dimension of the transformed feature vector is the same as that of the fully connected layer, while alleviating the generalization problem.\n4.2. SPP-YOLO-v4. YOLO-v4 is a high-precision real-time single-stage detection algorithm integrating YOLO-v1, YOLO-v2, and YOLO-v3. YOLO-v4 constructs the CSP cross-stage partial network (CSPNet) in the residual module, in which the feature layer is the input and the feature information of the higher layer is the output. *is shows that the learning objectives of YOLO-v4 in the ResNet module are different between output and input. *erefore, residual learning is realized, and the model parameters are reduced, so the feature learning ability is enhanced. Considering the application environment of music teaching, some changes are made on the basis of the original network, and the final network structure is shown in Figure 4.\nFirst, the feature layer is convolved three times, and then, the input feature layer is maximally pooled by using the maximum pooled cores of different sizes. After convolution and upsampling, different feature layers are connected in\nseries to realize feature fusion. *en, perform downsampling, compress height and width, and finally stack with the previous feature layer to realize more feature fusion (5 times). *e classification module uses the features extracted from the network to make classification judgment. Take the 13\u00d713 grid as an example, which is equal to dividing the input Mel spectrogram into 13\u00d713 squares; then, each square will be preset with three prior frames. *e classification results of the network will adjust the positions of these three prior boxes and finally filter by the nonmaximum suppression (NMS) algorithm [28], so as to get the final classification results."
        },
        {
            "heading": "5. Audio Recognition System Based on SPPYOLO-v4",
            "text": "5.1. System Architecture. As shown in Figure 5, after audio input, the proposed audio recognition system first divides the audio sequence data into two parts. *e first part comes from stereo channel, while the second part is compressed into mono. *e audio signals of the two channels are extracted by MFCC spectrogram and input into the SPP-YOLO-v4 model as features. *en, two groups of SPP-YOLO-v4 models are integrated, and the stacking method is adopted in the\nintegration. After the integrated learning of the two models, the audio classification results are finally output.*e details of the SPP-YOLO-v4 model are shown in Figure 4.\n5.2. Stacking Integrated Learning. As shown in Figure 5, the system uses ensemble learning technology to get the final classification result. *e basic idea of ensemble learning is to form a strong classifier through the combination of several weak classifiers. Even if some weak classifiers make wrong predictions, they can be corrected by other weak classifiers with correct predictions, thus achieving the effect of improving the system performance.\nAssuming that x is an input,mi (i \ufffd 1, 2, . . . , k) is a group of classifiers and the output of the classifiers is the probability distribution mi(x, cj) of each class cj (i \ufffd 1, 2, . . . , k), the final output y(x) of the integrated classifier can be expressed as\ny(x) \ufffd argmax cj \nk\ni\ufffd1 wimi x, cj , (5)\nwhere wi is the weight of classifier mi. Ensemble is a method to calculate the best weight of each classifier according to the\nclassification target. At present, popular ensemble learning algorithms include stacking, bagging, boosting, ensemble selection, and so on. *e ensemble learning algorithm selected in this study is the stacking method.\nStacking is a process of second-order learning with the output of the first-order learning process as input, also known as \u201cmeta-learning.\u201d*e stacking method has become a popular ensemble learning method, not only because its implementation is quite simple but also because it can significantly improve the generalization ability of the system, which is very consistent with the purpose of this study. *e basic principle of the stacking method is shown in Figure 6."
        },
        {
            "heading": "6. Experiment and Result Analysis",
            "text": "6.1. Experimental Environment and Dataset. *e hardware platform of this study is Intel Core i3-M350 CPU@ Dualcore 2.20GHz, 8GB of DDR2 memory, Nvidia RTX2080Ti GPU, and 11GB of video memory. *e PyCharm integrated\ndevelopment tool is developed in Python 3.5.0 language.*e YOLO annotation framework written in Python is used to convert the numerical format, so that it can be read by YOLO. *e comparison methods are the Gaussian mixture model (GMM), CNN, and R-CNN.\n*e experimental dataset is recorded audio files in the real teaching environment. *e dataset consists of audio types of four different labels (D1, D2, D3, and D4). All audio files are cut into 30-second clips. *ere are 12 audio file formats including MPEG, MP3, and WMA. Each recording is performed at a different location, and the average recording duration is 3\u20135 minutes. *e recording equipment includes two-channel Soundman OKM II Classic/studio A3 in-ear microphone and Roland Edirol R09 waveform recorder with 44.1 kHz sampling rate and 24 bit resolution.\n*e used dataset contains 1404 audio files, and the number of audio files of each type is 351. About 70% of the data is used for training the audio recognition model, and the remaining 30% is used for testing.*e system settings are given in Table 1.\n6.2. Evaluation Criteria. *e mean accuracy (mAP) is calculated as follows:\nmAP \ufffd  1\n0 p(\u03c4)d\u03c4, (6)\nwhere p(\u03c4) is the accuracy of audio classification. Precision and recall are defined as follows:\nPr \ufffd TP\nTP + FP ,\nRecall \ufffd TP\nTP + FN ,\n(7)\nwhere TP is the positive alarm rate, FP is the false alarm rate, and FN is the missed alarm rate.\nF1 score is the harmonic value of precision and recall rate. *e higher the value, the better the performance. It is defined as follows:\nF1 \ufffd 2 Recall \u00d7 Pr Recall + Pr . (8)\n6.3. Verification of SPP-YOLO-v4 Performance. In order to verify the promotion effect of the proposed improved YOLO-v4 (SPP-YOLO-v4) on generalization ability, it is compared with the traditional YOLO-v4 model. In the experiment, 3 of 12 audio file formats were selected: MPEG, MP3, and WMA. *e generalization ability of SPP-YOLOv4 is given in Table 2.\nFrom Table 2, it can be found that the overall accuracy of SPP-YOLO-v4 is higher than that of traditional YOLO-v4, which verifies its generalization ability for data in different\naudio formats. *is is because compared with the original method, SPP of SPP-YOLO-v4 contains more layers, but it also increases the processing time.\n6.4. Comparison of Test Results. Table 3 provides the results of training loss, mAP, and so on for all categories after 8000 rounds of training. It can be seen that the training model of the proposed method can effectively identify audio types. It has certain advantages in accuracy, recall rate, and F1 score, and its loss value is also the lowest of all methods, only 0.0122. *erefore, the stability and accuracy of the proposed method are better. *is is mainly due to the high resolution and receptive field (RF) of SPP-YOLO-v4, and the addition of SPPmodule in the connection layer retains the advantages brought by SPP. In terms of training time, SPP-YOLO-v4 is only slightly more than GMM.*e CNN needs to train a lot of convolution operations, so its training time is longer.\nFinally, the experiment uses data of 12 different audio formats to test and compare the four methods. Table 4 provides the values of test accuracy and test time. It can be seen that the average accuracy of the method proposed in this study is 99.0%, and the average detection time is 0.449ss. *erefore, the proposed method achieves better performance among the four methods compared. It can be concluded that the upsampling and maximum pooling of SPP-YOLO-v4 brought significant benefits. Maximum pooling selects the maximum value from adjacent areas to slightly delete some maximum frequency noise in the audio sequence. *erefore, convolution subsampling can be better operated in the subsequent sampling layer. *rough these advantages, SPP can improve the performance of the backbone network."
        },
        {
            "heading": "7. Conclusions",
            "text": "*is study presents an audio recognition system suitable for music teaching environment. Use SPP to improve YOLO-v4 network architecture, that is to say, use SPP to select local areas on different scales of the same convolution layer to learn the characteristics of the multiscale system. In addition, the stacking method in ensemble learning is used to fuse independent submodels of two different channels. *e experimental results show that the proposed method can improve the recognition accuracy of audio types and has better performance for different audio file formats. Due to the limitation of audio recording conditions, there are few audio types in the experimental dataset and the classification performance of audio files recorded by different devices has yet to be verified. More tests will be conducted on these two issues in the future."
        },
        {
            "heading": "Data Availability",
            "text": "*e data used to support the findings of this study are available from the corresponding author upon request."
        },
        {
            "heading": "Conflicts of Interest",
            "text": "*e authors declare that they have no conflicts of interest."
        }
    ],
    "title": "Research on Audio Recognition Based on the Deep Neural Network in Music Teaching",
    "year": 2022
}